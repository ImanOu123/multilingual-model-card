{"title": "Spectral Learning of General Weighted Automata via Constrained Matrix Completion", "authors": "Borja Balle; Mehryar Mohri", "pub_date": "", "abstract": "Many tasks in text and speech processing and computational biology require estimating functions mapping strings to real numbers. A broad class of such functions can be defined by weighted automata. Spectral methods based on the singular value decomposition of a Hankel matrix have been recently proposed for learning a probability distribution represented by a weighted automaton from a training sample drawn according to this same target distribution. In this paper, we show how spectral methods can be extended to the problem of learning a general weighted automaton from a sample generated by an arbitrary distribution. The main obstruction to this approach is that, in general, some entries of the Hankel matrix may be missing. We present a solution to this problem based on solving a constrained matrix completion problem. Combining these two ingredients, matrix completion and spectral method, a whole new family of algorithms for learning general weighted automata is obtained. We present generalization bounds for a particular algorithm in this family. The proofs rely on a joint stability analysis of matrix completion and spectral learning.", "sections": [{"heading": "Introduction", "text": "Many tasks in text and speech processing, computational biology, or learning models of the environment in reinforcement learning, require estimating a function mapping variable-length sequences to real numbers. A broad class of such functions can be defined by weighted automata. The mathematical and algorithmic properties of weighted automata have been extensively studied in the most general setting where they are defined in terms of an arbitrary semiring [28,9,23]. Weighted automata are widely used in applications ranging from natural text and speech processing [24] to optical character recognition [12] and image processing [1]. This paper addresses the problem of learning weighted automata from a finite set of labeled examples.\nThe particular instance of this problem where the objective is to learn a probabilistic automaton from examples drawn from this same distribution has recently drawn much attention: starting with the seminal work of Hsu et al. [19], the so-called spectral method has proven to be a valuable tool in developing novel and theoretically-sound algorithms for learning HMMs and other related classes of distributions [5,30,31,10,6,4]. Spectral methods have also been applied to other probabilistic models of practical interest, including probabilistic context-free grammars and graphical models with hidden variables [26,22,16,3,2]. The main idea behind these algorithms is that, under an identifiability assumption, the method of moments can be used to formulate a set of equations relating the parameters defining the target to observable statistics. Given enough training data, these statistics can be accurately estimated. Then, solving the corresponding approximate equations yields a model that closely estimates the target distribution. The spectral term takes its origin from the use of a singular value decomposition in solving those equations. This paper tackles a significantly more general and more challenging problem than the specific instance just mentioned. Indeed, in general, there seems to be a large gap separating the scenario of learning a probabilistic automaton using data drawn according to the distribution it generates, from that of learning an arbitrary weighted automaton from labeled data drawn from some unknown distribution. For a start, in the former setting there is only one object to care about because the distribution from which examples are drawn is the target machine. In contrast, the latter involves two distinct objects: a distribution according to which strings are drawn, and a target weighted automaton assigning labels to these strings. It is not difficult in this setting to conceive that, for a particular target, an adversary could find a distribution over strings making the learner's task insurmountably difficult. In fact, this is the core idea behind the cryptography-based hardness results for learning deterministic finite automata given by Kearns and Valiant [20] -these same results apply to our setting as well. But, even in cases where the distribution \"cooperates,\" there is still an obstruction in leveraging the spectral method for learning general weighted automata. The statistics used by the spectral method are essentially the probabilities assigned by the target distribution to each string in some fixed finite set B. In the case where the target is a distribution, increasingly large samples yield uniformly convergent estimates for these probabilities. Thus, it can be safely assumed that the probability of any string from B not present in the sample is zero. When learning arbitrary weighted automata, however, the value assigned by the target to an unseen string is unknown. Furthermore, one cannot expect that a sample would contain the values of the target function for all the strings in B. This observation raises the question of whether it is possible at all to apply the spectral method in a setting with missing data, or, alternatively, whether there is a principled way to \"estimate\" this missing information and then apply the spectral method.\nAs it turns out, the latter approach can be naturally formulated as a constrained matrix completion problem. When applying the spectral method, the (approximate) values of the target on B are arranged in a matrix H. Thus, the main difference between the two settings can be restated as follows: when learning a weighted automaton representing a distribution, unknown entries of H can be filled in with zeros, while in the general setting there is a priori no straightforward method to fill in the missing values. We propose to use a matrix completion algorithm for solving this last problem. In particular, since H is a Hankel matrix whose entries must satisfy some equality constraints, it turns out that the problem of learning weighted automata under an arbitrary distribution leads to what we call the Hankel matrix completion problem. This is essentially a constrained matrix completion problem where entries of valid hypotheses need to satisfy a set of equalities. We give an algorithm for solving this problem via convex optimization. Many existing approaches to matrix completion, e.g., [14,13,27,18], are also based on convex optimization. Since the set of valid hypotheses for our constrained matrix completion problem is convex, many of these algorithms could also be modified to deal with the Hankel matrix completion problem.\nIn summary, our approach leverages two recent techniques for learning a general weighted automaton: matrix completion and spectral learning. It consists of first predicting the missing entries in H and then applying the spectral method to the resulting matrix. Altogether, this yields a family of algorithms parametrized by the choice of the specific Hankel matrix completion algorithm used. These algorithms are designed for learning an arbitrary weighted automaton from samples generated by an unknown distribution over strings and labels.\nWe study a special instance of this family of algorithms and prove generalization guarantees for its performance based on a stability analysis, under mild conditions on the distribution. The proof contains two main novel ingredients: a stability analysis of an algorithm for constrained matrix completion, and an extension of the analysis of spectral learning to an agnostic setting where data is generated by an arbitrary distribution and labeled by a process not necessarily modeled by a weighted automaton.\nThe rest of the paper is organized as follows. Section 2 introduces the main notation and definitions used in subsequent sections. In Section 3, we describe a family of algorithms for learning general weighted automata by combining constrained matrix completion and spectral methods. In Section 4, we give a detailed analysis of one particular algorithm in this family, including generalization bounds.", "publication_ref": ["b27", "b8", "b22", "b23", "b11", "b0", "b18", "b4", "b29", "b30", "b9", "b5", "b3", "b25", "b21", "b15", "b2", "b1", "b13", "b12", "b26", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "This section introduces the main notation used in this paper. Bold letters will be used for vectors v and matrices M. For vectors, v denotes the standard euclidean norm. For matrices, M denotes the operator norm. For p \u2208 [1, +\u221e], M p denotes the Schatten p-norm:\nM p = ( n\u22651 \u03c3 p n (M)) 1/p\n, where \u03c3 n (M) is the nth singular value of M. The special case p = 2 coincides with the Frobenius norm which will be sometimes also written as M F . The Moore-Penrose pseudo-inverse of a matrix M is denoted by M + .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Functions over Strings and Hankel Matrices", "text": "We denote by \u03a3 = {a 1 , . . . , a k } a finite alphabet of size k \u2265 1 and by the empty string. We also write \u03a3 = { } \u222a \u03a3. The set of all strings over \u03a3 is denoted by \u03a3 and the length of a string x denoted by |x|. For any n \u2265 0, \u03a3 \u2264n denotes the set of all strings of length at most n. Given two sets of strings P, S \u2286 \u03a3 we denote by PS the set of all strings uv obtained by concatenation of a string u \u2208 P and a string v \u2208 S. A set of strings P is called \u03a3-complete when P = P \u03a3 for some set P . P is then called the root of P. A pair (P, S) with P, S \u2286 \u03a3 is said to form a basis of \u03a3 if \u2208 P \u2229 S and P is \u03a3-complete. We define the dimension of a basis (P, S) as the cardinality of PS, that is |PS|.\nFor any basis B = (P, S), we denote by H B the vector space of functions R PS whose dimension is the dimension of B. We will simply write H instead of H B when the basis B is clear from the context. The Hankel matrix H \u2208 R P\u00d7S associated to a function h \u2208 H is the matrix whose entries are defined by H(u, v) = h(uv) for all u \u2208 P and v \u2208 S. Note that the mapping h \u2192 H is linear. In fact, H is isomorphic to the vector space formed by all |P| \u00d7 |S| real Hankel matrices and we can thus write by identification\nH = H \u2208 R P\u00d7S : \u2200u 1 , u 2 \u2208 P, \u2200v 1 , v 2 \u2208 S, u 1 v 1 = u 2 v 2 \u21d2 H(u 1 , v 1 ) = H(u 2 , v 2 ) .\nIt is clear from this characterization that H is a convex set because it is a subset of a convex space defined by equality constraints. In particular, a matrix in H contains |P||S| coefficients with |PS| degrees of freedom, and the dependencies can be specified as a set of equalities of the form H(u 1 , v 1 ) = H(u 2 , v 2 ) when u 1 v 1 = u 2 v 2 . We will use both characterizations of H indistinctly for the rest of the paper. Also, note that different orderings of P and S may result in different sets of matrices. For convenience, we will assume for all that follows an arbitrary fixed ordering, since the choice of that order has no effect on any of our results.\nMatrix norms extend naturally to norms in H. For any p \u2208 [1, +\u221e], the Hankel-Schatten p-norm on H is defined as h p = H p . It is straightforward to verify that h p is a norm by the linearity of h \u2192 H. In particular, this implies that the function \u2022 p : H \u2192 R is convex. In the case p = 2, it can be seen that h 2 2 = h, h H , with the inner product on H defined by\nh, h H = x\u2208PS c x h(x)h (x) ,\nwhere c x = |{(u, v) \u2208 P \u00d7 S : x = uv}| is the number of possible decompositions of x into a prefix in P and a suffix in S.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Weighted finite automata", "text": "A widely used class of functions mapping strings to real numbers is that of functions defined by weighted finite automata (WFA) or in short weighted automata [23]. These functions are also known as rational power series [28,9]. A WFA over \u03a3 with n states can be defined as a tuple A = \u03b1, \u03b2, {A a } a\u2208\u03a3 , where \u03b1, \u03b2 \u2208 R n are the initial and final weight vectors, and A a \u2208 R n\u00d7n the transition matrix associated to each alphabet symbol a \u2208 \u03a3. The function f A realized by a WFA A is defined by\nf A (x) = \u03b1 A x1 \u2022 \u2022 \u2022 A xt \u03b2 , for any string x = x 1 \u2022 \u2022 \u2022 x t \u2208 \u03a3 * with t = |x| and x i \u2208 \u03a3 for all i \u2208 [1, t].\nWe will say that a WFA\nA = \u03b1, \u03b2, {A a } is \u03b3-bounded if \u03b1 , \u03b2 , A a \u2264 \u03b3 for all a \u2208 \u03a3.\nThis property is convenient to bound the maximum value assigned by a WFA to any string of a given length. WFAs can be more generally defined over an arbitrary semiring instead of the field of real numbers and are also known as multiplicity automata (e.g., [8]). To any function f : \u03a3 \u2192 R, we can associate its Hankel matrix H f \u2208 R \u03a3 \u00d7\u03a3 with entries defined by H f (u, v) = f (uv). These are just the bi-infinite versions of the Hankel matrices we introduced in the case P = S = \u03a3 . Carlyle and Paz [15] and Fliess [17] gave the following characterization of the set of functions f in R \u03a3 defined by a WFA in terms of the rank of their Hankel matrix rank(H f ). 1 Theorem 1 ( [15,17]) A function f : \u03a3 \u2192 R can be defined by a WFA iff rank(H f ) is finite and in that case rank(H f ) is the minimal number of states of any WFA A such that f = f A .\n1 -1 a, 0 b, 2 3 a, 0 b, 3 4 a, 1 3 b, 1 a, 3 4 b, 6 5 1/2 1/2 \u03b1 = [1/2 1/2] \u03b2 = [1 \u22121] A a = 3/4 0 0 1/3 A b = 6/5 2/3 3/4 1 (a) (b)\nThus, WFAs can be viewed as those functions whose Hankel matrix can be finitely \"compressed\". Since finite sub-blocks of a Hankel matrix cannot have a larger rank than its bi-infinite extension, this justifies the use of a low-rank-enforcing regularization in the definition of a Hankel matrix completion.\nNote that deterministic finite automata (DFA) with n states can be represented by a WFA with at most n states. Thus, the results we present here can be directly applied to classification problems in \u03a3 . However, specializing our results to this particular setting may yield several improvements. By Theorem 1, the Hankel matrix of A has rank at most 2. Given H B , the spectral method described in [19] can be used to recover a WFA\u00c2 equivalent to A, in the sense that A and\u00c2 compute the same function. In general, one may be given a sample of strings labeled using some WFA that does not contain enough information to fully specify a Hankel matrix over a complete basis. In that case, Theorem 1 motivates the use of a low-rank matrix completion algorithm to fill in the missing entries in H B prior to the application of the spectral method. This is the basis of the algorithm we describe in the following section.", "publication_ref": ["b22", "b27", "b8", "b7", "b14", "b16", "b0", "b14", "b16", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Example", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The HMC+SM Algorithm", "text": "In this section we describe our algorithm HMC+SM for learning weighted automata. As input, the algorithm takes a sample\nZ = (z 1 , . . . , z m ) containing m examples z i = (x i , y i ) \u2208 \u03a3 \u00d7 R, 1 \u2264 i \u2264 m, drawn i.i.d. from some distribution D over \u03a3 \u00d7 R.\nThere are three parameters a user can specify to control the behavior of the algorithm: a basis B = (P, S) of \u03a3 , a regularization parameter \u03c4 > 0, and the desired number of states n in the hypothesis. The output returned by HMC+SM is a WFA A Z with n states that computes a function f A Z : \u03a3 \u2192 R.\nThe algorithm works in two stages. In the first stage, a constrained matrix completion algorithm with input Z and regularization parameter \u03c4 is used to return a Hankel matrix H Z \u2208 H B . In the second stage, the spectral method is applied to H Z to compute a WFA A Z with n states. These two steps will be described in detail in the following sections.\nAs will soon become apparent, HMC+SM defines in fact a whole family of algorithms. In particular, by combining the spectral method with any algorithm for solving the Hankel matrix completion problem, one can derive a new algorithm for learning WFAs. For concreteness, in the following, we will only consider the Hankel matrix completion algorithm described in Section 3.1. Through its parametrization by a number 1 \u2264 p \u2264 \u221e and a convex loss : R \u00d7 R \u2192 R + , this completion algorithm already gives rise to a family of learning algorithms that we denote by HMC p, +SM. However, it is important to keep in mind that for each existing matrix completion algorithm that can be modified to solve the Hankel matrix completion problem, a new algorithm for learning WFAs can be obtained via the general scheme we describe below.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Hankel Matrix Completion", "text": "We now describe our Hankel matrix completion algorithm. Given a basis B = (P, S) of \u03a3 and a sample Z over \u03a3 \u00d7 R, the algorithm solves a convex optimization problem and returns a matrix H Z \u2208 H B . We give two equivalent descriptions of this optimization, one in terms of functions h : PS \u2192 R, and another in terms of Hankel matrices H \u2208 R P\u00d7S . While the former is perhaps conceptually simpler, the latter is easier to implement within the existing frameworks of convex optimization.\nWe will denote by Z the subsample of Z formed by examples z = (x, y) with x \u2208 PS and by m its size | Z|. For any p \u2208 [1, +\u221e] and a convex loss function : R \u00d7 R \u2192 R + , we consider the objective function F Z defined for any h \u2208 H by\nF Z (h) = \u03c4 N (h) + R e Z (h) = \u03c4 h 2 p + 1 m (x,y)\u2208 e Z (h(x), y) ,\nwhere \u03c4 > 0 is a regularization parameter. F Z is a convex function, by the convexity of \u2022 p and .\nOur algorithm seeks to minimize this loss function over the finite-dimensional vector space H and returns a function h\nZ satisfying h Z \u2208 argmin h\u2208H F Z (h) . (HMC-h)\nTo define an equivalent optimization over the matrix version of H, we introduce the following notation. For each string x \u2208 PS, fix a pair of coordinate vectors (u x , v x ) \u2208 R P \u00d7 R S such that u x Hv x = H(x) for any H \u2208 H. That is, u x and v x are coordinate vectors corresponding respectively to a prefix u \u2208 P and a suffix v \u2208 S, and such that uv = x. Now, abusing our previous notation, we define the following loss function over matrices:\nF Z (H) = \u03c4 N (H) + R e Z (H) = \u03c4 H 2 p + 1 m (x,y)\u2208 e Z (u x Hv x , y) .\nThis is a convex function defined over the space of all |P| \u00d7 |S| matrices. Optimizing F Z over the convex set of Hankel matrices H leads to an algorithm equivalent to (HMC-h):\nH Z \u2208 argmin H\u2208H F Z (H) . (HMC-H)\nWe note here that our approach shares some common aspects with some previous work in matrix completion. The fact that there may not be a true underlying Hankel matrix makes it somewhat close to the agnostic setting in [18], where matrix completion is also applied under arbitrary distributions. Nonetheless, it is also possible to consider other learning frameworks for WFAs where algorithms for exact matrix completion [14,27] or noisy matrix completion [13] may be useful. Furthermore, since most algorithms in the literature of matrix completion are based on convex optimization problems, it is likely that most of them can be adapted to solve constrained matrix completions problems such as the one we discuss here.", "publication_ref": ["b17", "b13", "b26", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Spectral Method for General WFA", "text": "Here, we describe how the spectral method can be applied to H Z to obtain a WFA. We use the same notation as in [7] and a version of the spectral method working with an arbitrary basis (as in [5,4,7]), in contrast to versions restricted to P = \u03a3 \u22642 and S = \u03a3 like [19].\nWe first need to partition H Z into k + 1 blocks as follows. Since B is a basis, P is \u03a3-complete and admits a root P . We define a block H a \u2208 R P \u00d7S for each a \u2208 \u03a3 , whose entries are given by H a (u, v) = H Z (ua, v), for any u \u2208 P and v \u2208 S. Thus, after suitably permuting the rows of H Z , we can write H Z = [H , H a1 , . . . , H a k ]. We will use the following specific notation to refer to the rows and columns of H corresponding to \u2208 P \u2229 S: h ,S \u2208 R S with h ,S (v) = H ( , v) and h P , (u) \u2208 R P with h P , (u) = H (u, ).\nUsing this notation, the spectral method can be described as follows. Given the desired number of states n, it consists of first computing the truncated SVD of H corresponding to the n largest singular values: U n D n V n . Thus, matrix U n D n V n is the best rank n approximation to H with respect to the Frobenius norm. Then, using the right singular vectors V n of H , the next step consists of computing a weighted automaton A Z = \u03b1, \u03b2, {A a } as follows:\n\u03b1 = h ,S V n \u03b2 = (H V n ) + h P , A a = (H V n ) + H a V n . (SM)\nThe fact that the spectral method is based on a singular value decomposition justifies in part the use of a Schatten p-norm as a regularizer in (HMC-H). In particular, two very natural choices are p = 1 and p = 2. The first one corresponds to a nuclear norm regularized optimization, which is known to enforce a low rank constraint on H Z . In a sense, this choice can be justified in view of Theorem 1 when the target is known to be generated by some WFA. On the other hand, choosing p = 2 also has some effect on the spread of singular values, while at the same time enforcing the coefficients in H Z -especially those that are completely unknown -to be small. As our analysis suggests, this last property is important for preventing errors from accumulating on the values assigned by A Z to long strings.", "publication_ref": ["b6", "b4", "b3", "b6", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Generalization Bound", "text": "In this section, we study the generalization properties of HMC p, +SM. We give a stability analysis for a special instance of this family of algorithms and use it to derive a generalization bound. We study the specific case where p = 2 and (y, y ) = |y \u2212 y | for all (y, y ). But, much of our analysis can be used to derive similar bounds for other instances of HMC p, +SM. The proofs of the technical results presented are given in the Appendix.\nWe first introduce some notation needed for the presentation of our main result. For any \u03bd > 0, let t \u03bd be the function defined by t \u03bd (x) = x for |x| \u2264 \u03bd and t \u03bd (x) = \u03bd sign(x) for |x| > \u03bd. For any distribution D over \u03a3 \u00d7 R, we denote by D \u03a3 its marginal distribution over \u03a3 . The probability that a string x \u223c D \u03a3 belongs to PS is denoted by \u03c0 = D \u03a3 (PS).\nWe assume that the parameters B, n, and \u03c4 are fixed. Two parameters that depend on D will appear in our bound. In order to define these parameters, we need to consider the output H Z of (HMC-H) as a random variable that depends on the sample Z. Writing H Z = [H , H a1 , . . . , H a k ], as in Section 3.2, we define:\n\u03c3 = E Z\u223cD m [\u03c3 n (H )] \u03c1 = E Z\u223cD m \u03c3 n (H ) 2 \u2212 \u03c3 n+1 (H ) 2\n, where \u03c3 n (M) denotes the nth singular value of matrix M. Note that these parameters may vary with m, n, \u03c4 and B.\nIn contrast to previous learning results based on the spectral method, our bound holds in an agnostic setting. That is, we do not require that the data was generated from some (probabilistic) unknown WFA. However, in order to prove our results we do need to make two assumptions about the tails of the distribution. First, we need to assume that there exists a bound on the magnitude of the labels generated by the distribution.\nAssumption 1 There exists a constant \u03bd > 0 such that if (x, y) \u223c D, then |y| \u2264 \u03bd almost surely.\nSecond, we assume that the strings generated by the distribution will not be too long. In particular, that the length of the strings generated by D \u03a3 follows a distribution whose tail is slightly lighter than sub-exponential.\nAssumption 2 There exist constants c, \u03b7 > 0 such that P x\u223cD\u03a3 [|x| \u2265 t] \u2264 exp(\u2212ct 1+\u03b7 ) holds for all t \u2265 0.\nWe note that in the present context both assumptions are quite reasonable. Assumption 1 is equivalent to assumptions made in other contexts where a stability analysis is pursued, e.g., in the analysis of support vector regression in [11]. Furthermore, in our context, this assumption can be relaxed to require only that the distribution over labels be sub-Gaussian, at the expense of a more complex proof.\nAssumption 2 is required by the fact already pointed out in [19] that errors in the estimation of operator models accumulate exponentially with the length of the string. Moreover, it is well known that the tail of any probability distribution generated by a WFA is sub-exponential. Thus, though we do not require D \u03a3 to be generated by a WFA, we do need its distribution over lengths to have a tail behavior similar to that of a distribution generated by a WFA. This seems to be a limitation common to all known learnability proofs based on the spectral method.\nWe can now state our main result, which is a bound on the average loss R\n(f ) = E z\u223cD [ (f (x), y)] in terms of the empirical loss R Z (f ) = |Z| \u22121 z\u2208Z (f (x), y).\nTheorem 2 Let Z be a sample formed by m i.i.d. examples generated from some distribution D satisfying Assumptions 1 and 2. Let A Z be the WFA returned by algorithm HMC p, +SM with p = 2 and loss function (y, y ) = |y \u2212 y |. Then, for any \u03b4 > 0, the following holds with probability at least\n1 \u2212 \u03b4 for f Z = t \u03bd \u2022 f A Z : R(f Z ) \u2264 R Z (f Z ) + O \u03bd 4 |P| 2 |S| 3/2 \u03c4 \u03c3 3 \u03c1\u03c0 ln m m 1/3 ln 1 \u03b4 .\nThe proof of this theorem is based on an algorithmic stability analysis. Thus, we will consider two samples of size m, Z \u223c D m consisting of m i. The first step in the analysis is to bound the stability of the matrix completion algorithm. This is done in the following lemma, that gives a sample-dependent and a sample-independent bound for the stability of H.\nLemma 3 Suppose D satisfies Assumption 1. Then, the following holds:\nH \u2212 H F \u2264 min 2\u03bd |P||S|, 1 \u03c4 min{ m, m } .\nThe standard method for deriving generalization bounds from algorithmic stability results could be applied here to obtain a generalization bound for our Hankel matrix completion algorithm. However, our goal is to give a generalization bound for the full HMC+SM algorithm.\nUsing the bound on the Frobenius norm H \u2212 H F , we are able to analyze the stability of \u03c3 n (H ), \u03c3 n (H ) 2 \u2212 \u03c3 n+1 (H ) 2 , and V n using well-known results on the stability of singular values and singular vectors. These results are used to bound the difference between the operators of WFA A Z and A Z . The following lemma can be proven by modifying and extending some of the arguments of [19,4], which were given in the specific case of WFAs representing a probability distribution.", "publication_ref": ["b10", "b18", "b18", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Lemma 4", "text": "Let \u03b5 = H\u2212H F , \u03c3 = min{\u03c3 n (H ), \u03c3 n (H )}, and \u03c1 = \u03c3 n (H ) 2 \u2212\u03c3 n+1 (H ) 2 . Suppose \u03b5 \u2264 \u03c1/4. Then, there exists some constant C > 0 such that the following three inequalities hold:\n\u2200a \u2208 \u03a3 : A a \u2212 A a \u2264 C\u03b5\u03bd 3 |P| 3/2 |S| 1/2 / \u03c1 \u03c3 2 ; \u03b1 \u2212 \u03b1 \u2264 C\u03b5\u03bd 2 |P| 1/2 |S|/ \u03c1; \u03b2 \u2212 \u03b2 \u2264 C\u03b5\u03bd 3 |P| 3/2 |S| 1/2 / \u03c1 \u03c3 2 .\nThe other half of the proof results from combining Lemmas 15 and 19 to obtain a bound for |f Z (x) \u2212 f Z (x)|. This is a delicate step, because some of the bounds given above involve quantities that are defined in terms of Z. Therefore, all these parameters need to be controlled in order to ensure that the bounds do not grow too large. Furthermore, to obtain the desired bounds we need to extend the usual tools for analyzing spectral methods to the current setting. In particular, these tools need to be adapted to the agnostic settings where there is no underlying true WFA. The analysis is further complicated by the fact that now the functions we are trying to learn and the distribution that generates the data are not necessarily related.\nOnce all this is achieved, it remains to combine these new tools to show an algorithmic stability result for HMC p, +SM. In the following lemma, we first define \"bad\" samples Z and show that bad samples have a very low probability.\nLemma 5 Suppose D satisfies Assumptions 1 and 2. If Z is a large enough i.i.d. sample from D, then with probability at least 1 \u2212 1/m 3 the following inequalities hold simultaneously: |x i | \u2264 ((1/c) ln(4m 4 )) 1/1+\u03b7 for all i, \u03b5 \u2264 4/(\u03c4 \u03c0m), \u03c3 \u2265 \u03c3/2, and \u03c1 \u2265 \u03c1/2.\nAfter that we give two upper bounds for |f Z (x) \u2212 f Z (x)|: a tighter bound that holds for \"good\" samples Z and Z and a another one that holds for all samples. These bounds are combined using a variant of McDiarmid's inequality for dealing with functions that do not satisfy the bounded differences assumption almost surely [21]. The rest of the proof then follows the same scheme as the standard one for deriving generalization bounds for stable algorithms [11,25].", "publication_ref": ["b20", "b10", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We described a new algorithmic solution for learning arbitrary weighted automata from a sample of labeled strings drawn from an unknown distribution. Our approach combines an algorithm for constrained matrix completion with the recently developed spectral learning methods for learning probabilistic automata. Using our general scheme, a broad family of algorithms for learning weighted automata can be obtained. We gave a stability analysis of a particular algorithm in that family and used it to prove generalization bounds that hold for all distributions satisfying two reasonable assumptions. The particular case of Schatten p-norm with p = 1, which corresponds to a regularization with the nuclear norm, can be analyzed using similar techniques. Our results can be further extended by deriving generalization guarantees for all algorithms in the family we introduced. An extensive and rigorous empirical comparison of all these algorithms will be an important complement to the research we presented. Finally, learning DFAs under an arbitrary distribution using the algorithms we presented deserves a specific study since the problem is of interest in many applications and since it may benefit from improved learning guarantees.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Perturbation and stability tools", "text": "In this section, we list a series of known perturbation results for singular values, pseudo-inverses, and singular vectors, and other stability results needed for the proofs given in this appendix. A\n+ \u2212 B + \u2264 1 + \u221a 5 2 max A + 2 , B + 2 A \u2212 B", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Lemma 8 ([33]", "text": ") Let A \u2208 R d\u00d7d be symmetric positive semidefinite matrix and E \u2208 R d\u00d7d a symmetric matrix such that B = A + E is positive semidefinite. Fix n \u2264 rank(A) and suppose that E F \u2264 (\u03bb n (A) \u2212 \u03bb n+1 (A))/4. Then, writing V n for the top n eigenvectors of A and W n for the top n eigenvectors of B, we have\nV n \u2212 W n F \u2264 4 E F \u03bb n (A) \u2212 \u03bb n+1 (A)\n.\nThis last lemma will be most useful to us in the form given in this next corollary.\nCorollary 9 Let A, E \u2208 R d1\u00d7d2 and write\nB = A + E. Suppose n \u2264 rank(A) and E F \u2264 \u03c3 n (A) 2 \u2212 \u03c3 n+1 (A) 2 /4. If V n , W n contain the first n right singular vectors of A and B respec- tively, then V n \u2212 W n F \u2264 8 A F E F + 4 E 2 F \u03c3 n (A) 2 \u2212 \u03c3 n+1 (A) 2 .\nProof. Using that A A \u2212 B B F \u2264 2 A F E F + E 2 F and \u03bb n (A A) = \u03c3 n (A) 2 , we can apply Lemma 8 to get the bound on V n \u2212 W n F under the condition that A A \u2212 B B F \u2264 (\u03c3 n (A) 2 \u2212 \u03c3 n+1 (A) 2 )/4. To see that this last condition is satisfied, observe that for all x, y \u2265 0\none has 1 + \u221a 2 \u221a x + y \u2265 \u221a x + \u221a y. Thus, we get E F \u2264 \u03c3 n (A) 2 \u2212 \u03c3 n+1 (A) 2 4 \u2264 \u03c3 n (A) 2 \u2212 \u03c3 n+1 (A) 2 + 4 A 2 F \u2212 2 A F 2 1 + \u221a 2 \u2264 4 A 2 F + \u03c3 n (A) 2 \u2212 \u03c3 n+1 (A) 2 \u2212 2 A F 2 ,\nand this last inequality implies 2\nA F E F + E 2 F \u2264 (\u03c3 n (A) 2 \u2212 \u03c3 n+1 (A) 2 )/4. 2\nThe next two results give useful extensions of McDiarmid's inequality to deal with functions that do not satisfy the bounded difference assumption almost surely [21].\nDefinition 10 Let X = (X 1 , . . . , X m ) be a random variable on a probability space \u2126 m . We say that a function \u03a6 : \u2126 m \u2192 R is strongly difference-bounded by (b, c, \u03b4) if the following holds: there exists a measurable subset E \u2286 \u2126 m with P[E] \u2264 \u03b4, such that\n\u2022 if X and X differ only by one coordinate and X / \u2208 E, then |\u03a6(X) \u2212 \u03a6(X )| \u2264 c;\n\u2022 for all X, X that differ only by one coordinate |\u03a6(X) \u2212 \u03a6(X )| \u2264 b.\nTheorem 11 Let \u03a6 be a function over a probability space \u2126 m that is strongly difference-bounded by (b, c, \u03b4) with b \u2265 c > 0. Then, for any t > 0,\nP [\u03a6 \u2212 E[\u03a6] \u2265 t] \u2264 exp \u2212t 2 8mc 2 + mb\u03b4 c .\nFurthermore, the same upper bound holds for\nP[E[\u03a6] \u2212 \u03a6 \u2265 t].\nCorollary 12 Let \u03a6 be a function over a probability space \u2126 m that is strongly difference-bounded by (b, \u03b8/m, exp(\u2212Km)). Then, for any 0 < t \u2264 2\u03b8 \u221a K and m \u2265 max{b/\u03b8, (9 + 18/K) ln(3 + 6/K)},\nP [\u03a6 \u2212 E[\u03a6] \u2265 t] \u2264 2 exp \u2212t 2 m 8\u03b8 2 .\nFurthermore, the same upper bound holds for\nP[E[\u03a6] \u2212 \u03a6 \u2265 t].\nThe following is another useful form of the previous Corollary.\nCorollary 13 Let \u03a6 be a function over a probability space \u2126 m that is strongly difference-bounded by (b, \u03b8/m, exp(\u2212Km)). Then, for any \u03b4 > 0 and any m \u2265 max{b/\u03b8, (9 + 18/K) ln(3 + 6/K), (2/K) ln(2/\u03b4)}, each of the following holds with probability at least 1 \u2212 \u03b4:\n\u03a6 \u2265 E[\u03a6] \u2212 8\u03b8 2 m ln 2 \u03b4 , \u03a6 \u2264 E[\u03a6] + 8\u03b8 2 m ln 2 \u03b4 .", "publication_ref": ["b20", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "B Proof of Theorem 2", "text": "To analyze the stability of our algorithm, we consider a sample Z = (z 1 , . . . , z m\u22121 , z m ) that differs from Z only by the last point (z m instead of z m ). Example z m is an arbitrary point in the domain of D. Throughout the analysis, h = h Z and h = h Z denote the functions in H obtained by solving (HMC-h) respectively with training samples Z and Z respectively. We also denote by H = H Z and H = H Z their corresponding Hankel matrices.\nThe following technical lemma will be used to study the algorithmic stability of the optimization problem (HMC-h).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Lemma 14", "text": "The following inequality holds for all samples Z and Z differing by only one point:\n2\u03c4 h \u2212 h 2 2 \u2264 R e Z (h ) \u2212 R e Z (h) + R e Z (h) \u2212 R e Z (h ) .\nProof. The argument is the same as the one presented in [25] to bound the stability of kernel ridge regression. The following inequality is first shown using the expansion of h \u2212 h 2 2 in terms of the corresponding inner product:\n2\u03c4 h \u2212 h 2 2 \u2264 \u03c4 (B N (h h) + B N (h h )) \u2264 B F Z (h h) + B F Z (h h )\n, where B F denotes the Bregman divergence associated to F . Next, using the optimality of h and h , which implies \u2207F Z (h) = 0 and \u2207F Z (h ) = 0, we can write\nB F Z (h h) + B F Z (h h ) = R e Z (h ) \u2212 R e Z (h) + R e Z (h) \u2212 R e Z (h ). 2\nOur next lemma bounds the stability of the first stage of the algorithm using Lemma 14.", "publication_ref": ["b24"], "figure_ref": [], "table_ref": []}, {"heading": "Lemma 15", "text": "Assume that D satisfies Assumption 1. Then, the following holds:\nH \u2212 H F \u2264 min 2\u03bd |P||S|, 1 \u03c4 min{ m, m } .\nProof. Note that by Assumption 1, for all (x, y) in Z, or Z , we have |y| \u2264 \u03bd. Therefore, we must have |H(u, v)| \u2264 \u03bd for all u \u2208 P and v \u2208 S, otherwise the value of F Z (H) is not minimal because decreasing the absolute value of an entry |H(u, v)| > \u03bd decreases the value of F Z (H). The same holds for H . Thus, the first bound follows from H \u2212 H F \u2264 H F + H F \u2264 2\u03bd |P||S|.\nNow we proceed to show the second bound. Since by definition H \u2212 H F = h \u2212 h 2 , it is sufficient to bound this second quantity. By Lemma 14, we have\n2\u03c4 h \u2212 h 2 2 \u2264 R e Z (h ) \u2212 R e Z (h) + R e Z (h) \u2212 R e Z (h ) .(2)\nWe can consider four different situations for the right-hand side of this expression, depending on the membership of x m and x m in the set PS.\nIf\nx m , x m / \u2208 PS, then Z = Z . Therefore, R e Z (h) = R e Z (h), R e Z (h ) = R e Z(\nh ), and h \u2212 h 2 = 0.\nIf x m , x m \u2208 PS, then m = m , and the following equalities hold:\nR e Z (h) \u2212 R e Z (h) = |h(x m ) \u2212 y m | \u2212 |h(x m ) \u2212 y m | m , R e Z (h ) \u2212 R e Z (h ) = |h (x m ) \u2212 y m | \u2212 |h (x m ) \u2212 y m | m .\nThus, in view of (2), we can write\n2\u03c4 h \u2212 h 2 2 \u2264 |h(x m ) \u2212 h (x m )| + |h(x m ) \u2212 h (x m )| m \u2264 2 m h \u2212 h 2 ,\nwhere the first inequality follows from ||h(\nx) \u2212 y| \u2212 |h (x) \u2212 y|| \u2264 |h(x) \u2212 h (x)|, and the second from |h(x) \u2212 h (x)| \u2264 h \u2212 h 2 .\nIf x m \u2208 PS and x m / \u2208 PS, the right-hand side of (2) equals\nz\u2208 e Z |h (x) \u2212 y| m \u2212 |h (x) \u2212 y| m + |h(x) \u2212 y| m \u2212 |h(x) \u2212 y| m + |h (x m ) \u2212 y m | m \u2212 |h(x m ) \u2212 y m | m .\nNow, since m = m + 1 we can write\n2\u03c4 h \u2212 h 2 2 \u2264 z\u2208 e Z |h(x) \u2212 h (x)| m m + |h(x m ) \u2212 h (x m )| m \u2264 2 m h \u2212 h 2 .\nBy symmetry, a similar bound holds in the case where x m / \u2208 PS and x m \u2208 PS. Combining these four bounds yields the desired inequality. 2\nThe next three lemmas contain the main technical tools needed to bound the difference |f A Z (x) \u2212 f A Z (x)| in our agnostic setting.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Lemma 16", "text": "Let A = \u03b1, \u03b2, {A a } and A = \u03b1 , \u03b2 , {A a } be two weighted automata with n states. Let \u03b3 be such that both A and A are \u03b3-bounded. Then, the following inequality holds for any string x \u2208 \u03a3 :\n|f A (x) \u2212 f A (x)| \u2264 \u03b3 |x|+1 \u03b1 \u2212 \u03b1 + \u03b2 \u2212 \u03b2 + |x| i=1 A xi \u2212 A xi .\nProof. Follows by induction on |x| using techniques similar to those used to prove Lemmas 11 and 12 in [19]. Let us define the following quantities in terms of the vectors and matrices that define A and A :\n\u03b5 = H \u2212 H , \u03b5 a = H a \u2212 H a , \u03b5 V = V \u2212 V , \u03b5 S = h \u03bb,S \u2212 h \u03bb,S , \u03b5 P = h P,\u03bb \u2212 h P,\u03bb .\nNow we state a result that will be used in the proof of Lemma 19.", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "Lemma 18", "text": "The following three bounds hold:\nA a \u2212 A a \u2264 \u03b5 a + \u03b5 V H a \u03c3 n (H V) + 1 + \u221a 5 2 H a (\u03b5 + \u03b5 V H ) min{\u03c3 n (H V) 2 , \u03c3 n (H V ) 2 } , \u03b1 \u2212 \u03b1 \u2264 \u03b5 S + \u03b5 V h \u03bb,S , \u03b2 \u2212 \u03b2 \u2264 \u03b5 P \u03c3 n (H V) + 1 + \u221a 5 2 h P,\u03bb (\u03b5 + \u03b5 V H ) min{\u03c3 n (H V) 2 , \u03c3 n (H V ) 2 } .\nProof. Using the triangle inequality, the submultiplicativity of the operator norm, and the properties of the pseudo-inverse, we can write\nA a \u2212 A a = (H V) + (H a V \u2212 H a V ) + ((H V ) + \u2212 (H V) + )H a V \u2264 (H V) + H a V \u2212 H a V + (H V) + \u2212 (H V ) + H a V \u2264 \u03c3 n (H V) \u22121 H a V \u2212 H a V + H a (H V) + \u2212 (H V ) + ,\nwhere we used that (H V) + = \u03c3 n (H V) by the properties of pseudo-inverse and operator norm, and H a V \u2264 H a by sub-multiplactivity and V = 1. Now note that we also have\nH a V \u2212 H a V \u2264 V H a \u2212 H a + H a V \u2212 V \u2264 \u03b5 a + \u03b5 V H a .\nFurthermore, using Lemma 7 we obtain\n(H V) + \u2212 (H V ) + \u2264 1 + \u221a 5 2 H V \u2212 H V max{ (H V) + 2 , (H V ) + 2 } \u2264 1 + \u221a 5 2 H \u2212 H V + H V \u2212 V min{\u03c3 n (H V) 2 , \u03c3 n (H V ) 2 } = 1 + \u221a 5 2 \u03b5 + \u03b5 V H min{\u03c3 n (H V) 2 , \u03c3 n (H V ) 2 } .\nThus we get the first of the bounds. The second bound follows straightforwardly from\nV h \u03bb,S \u2212 V h \u03bb,S \u2264 V \u2212 V h \u03bb,S + V h \u03bb,S \u2212 h \u03bb,S = \u03b5 S + \u03b5 V h \u03bb,S ,\nwhich uses that M = M holds for the operator norm.\nFinally, the last bound follows from the following inequalities, where we use Lemma 7 again:\n\u03b2 \u2212 \u03b2 \u2264 (H V) + h P,\u03bb \u2212 h P,\u03bb + h P,\u03bb (H V) + \u2212 (H V ) + \u2264 h P,\u03bb \u2212 h P,\u03bb \u03c3 n (H V) + 1 + \u221a 5 2 h P,\u03bb H V \u2212 H V min{\u03c3 n (H V) 2 , \u03c3 n (H V ) 2 } \u2264 \u03b5 P \u03c3 n (H V) + 1 + \u221a 5 2 h P,\u03bb (\u03b5 + \u03b5 V H ) min{\u03c3 n (H V) 2 , \u03c3 n (H V ) 2 } . 2\nLemma 19 Let \u03b5 = H \u2212 H F , \u03c3 = min{\u03c3 n (H ), \u03c3 n (H )}, and \u03c1 = \u03c3 n (H ) 2 \u2212 \u03c3 n+1 (H ) 2 . Suppose \u03b5 \u2264 \u03c1/4. There exists a universal constant c 1 > 0 such that the following inequalities hold for all a \u2208 \u03a3:\nA a \u2212 A a \u2264 c 1 \u03b5\u03bd 3 |P| 3/2 |S| 1/2 \u03c1 \u03c3 2 , \u03b1 \u2212 \u03b1 \u2264 c 1 \u03b5\u03bd 2 |P| 1/2 |S| \u03c1 , \u03b2 \u2212 \u03b2 \u2264 c 1 \u03b5\u03bd 3 |P| 3/2 |S| 1/2 \u03c1 \u03c3 2 .\nProof. We begin with a few observations that will help us apply Lemma 18. First note that H a \u2212 H a \u2264 H a \u2212 H a F \u2264 \u03b5 for all a \u2208 \u03a3 , as well as h P,\u03bb \u2212 h P,\u03bb \u2264 \u03b5 and h \u03bb,S \u2212 h \u03bb,S \u2264 \u03b5. Furthermore, H a \u2264 H a F \u2264 \u03bd |P||S| and H a \u2264 \u03bd |P||S| for all a \u2208 \u03a3 . In addition, we have h \u03bb,S \u2264 \u03bd |S| and h P,\u03bb \u2264 \u03bd |P|. Finally, by construction we also have \u03c3 n (H V) = \u03c3 n (H ) and \u03c3 n (H V ) = \u03c3 n (H ). Therefore, it only remains to bound V \u2212 V , which by Corollary 9 is\nV \u2212 V \u2264 4\u03b5 \u03c1 (2\u03bd |P||S| + \u03b5) \u2264 16\u03b5\u03bd |P||S| \u03c1 ,\nwhere the last inequality follows from Lemma 15.\nPlugging all the bounds above in Lemma 18 yields the following inequalities:\nA a \u2212 A a \u2264 \u03b5 \u03c3 1 + 16\u03bd|P| 1/2 |S| 1/2 \u03c1 + 1 + \u221a 5 2 \u03b5\u03bd|P| 1/2 |S| 1/2 \u03c3 2 1 + 16\u03bd 2 |P||S| \u03c1 , \u03b1 \u2212 \u03b1 \u2264 \u03b5 1 + 16\u03bd 2 |P| 1/2 |S| \u03c1 , \u03b2 \u2212 \u03b2 \u2264 \u03b5 \u03c3 + 1 + \u221a 5 2 \u03b5\u03bd|P| 1/2 \u03c3 2 1 + 16\u03bd 2 |P||S| \u03c1 .\nThe result now follows from an adequate choice of c 1 . 2\nWe now define the properties that make Z a good sample and show that for large enough m they are satisfied with high probability.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Definition 20", "text": "We say that a sample Z of m i.i.d. examples from D is good if the following conditions are satisfied for any z m = (x m , y m ) \u2208 supp(D): Thus, for any \u2206 \u2208 (0, 1) the Chernoff bound gives\n\u2022 |x i | \u2264 ((1/c) ln(4m 4 )) 1/(1+\u03b7) for all 1 \u2264 i \u2264 m; \u2022 H \u2212 H F \u2264 4/(\u03c4 \u03c0m); \u2022 min{\u03c3 n (H ), \u03c3 n (H )} \u2265 \u03c3/2; \u2022 \u03c3 n (H ) 2 \u2212 \u03c3 n+1 (H ) 2 \u2265 \u03c1/2.\nP[m < \u03c0(m \u2212 1)(1 \u2212 \u2206)] \u2264 exp \u2212 (m \u2212 1)\u03c0\u2206 2 2 \u2264 exp \u2212 m\u03c0\u2206 2 4 ,\nwhere we have used that (m \u2212 1)/m \u2265 1/2 for m \u2265 2.\nTaking \u2206 = (4/m\u03c0) ln(4m 3 ) above we see that min{ m, m } \u2265 (m \u2212 1)\u03c0(1 \u2212 \u2206) \u2265 m\u03c0(1 \u2212 \u2206)/2 holds with probability at least 1 \u2212 1/(4m 3 ). Now note that m \u2265 (16/\u03c0) ln(4m 3 ) implies \u2206 \u2264 1/2. Therefore, by Lemma 15 we have that m \u2265 max{2, (16/\u03c0) ln(4m 3 ), 2/(\u03c4 \u03c0\u03bd |P||S|)} implies that H \u2212 H F \u2264 4/(\u03c4 \u03c0m) holds with probability at least 1 \u2212 1/(4m 3 ). holds with probability at least 1 \u2212 1/(4m 3 ). Hence, for any sample size such that m \u2265 max{16/(\u03bd\u03c0\u03c3), (2048/\u03c4 2 \u03c0 2 \u03c3 2 ) ln(8m 3 )}, we get\nmin{\u03c3 n (H ), \u03c3 n (H )} \u2265 \u03c3 \u2212 128 \u03c4 2 \u03c0 2 m ln(8m 3 ) \u2212 4 \u03bd\u03c0m \u2265 \u03c3 \u2212 \u03c3 4 \u2212 \u03c3 4 = \u03c3 2 .\nTo prove the fourth bound we shall study the stability of \u03a6(Z) = \u03c3 n (H ) 2 \u2212 \u03c3 n+1 (H ) 2 . We begin with the following chain of inequalities, which follows from Lemma 6 and \u03c3 n (H ) \u2265 \u03c3 n+1 (H ):\n|\u03a6(Z) \u2212 \u03a6(Z )| = (\u03c3 n (H ) 2 \u2212 \u03c3 n+1 (H ) 2 ) \u2212 (\u03c3 n (H ) 2 \u2212 \u03c3 n+1 (H ) 2 ) \u2264 |\u03c3 n (H ) 2 \u2212 \u03c3 n (H ) 2 | + |\u03c3 n+1 (H ) 2 \u2212 \u03c3 n+1 (H ) 2 | = |\u03c3 n (H ) + \u03c3 n (H )||\u03c3 n (H ) \u2212 \u03c3 n (H )| + |\u03c3 n+1 (H ) + \u03c3 n+1 (H )||\u03c3 n+1 (H ) \u2212 \u03c3 n+1 (H )| \u2264 (2\u03c3 n (H ) + H \u2212 H ) H \u2212 H + (2\u03c3 n+1 (H ) + H \u2212 H ) H \u2212 H \u2264 4\u03c3 n (H ) H \u2212 H F + 2 H \u2212 H 2\nF . Now we can use this last bound to show that \u03a6(Z) is strongly difference-bounded by (b \u03c1 , \u03b8 \u03c1 /m, exp(\u2212K \u03c1 m)) with the definitions: b \u03c1 = 16\u03bd 2 |P||S|, \u03b8 \u03c1 = 64\u03c3/(\u03c4 \u03c0) and K \u03c1 = min{\u03c3 2 \u03c4 2 \u03c0 2 /256, \u03c0/64}. For b \u03c1 just observe that from Lemma 15 and \u03c3 \nn (H \u03c3 ) \u2264 H \u03c3 F \u2264 \u03bd |P||S| we get 4\u03c3 n (H ) H \u2212 H F + 2 H \u2212 H 2 F \u2264 16\u03bd 2 |P||S|\n. By the same arguments used above, if m is large enough we have H \u2212 H F \u2264 4/(\u03c4 \u03c0m) with probability at least 1 \u2212 exp(\u2212m\u03c0/16). Furthermore, by taking \u2206 = 1/2 in the stability argument given above for \u03c3 n (H ), and invoking Corollary 13 with \u03b4 = 2 exp(\u2212Km) for some 0 < K \u2264 K \u03c3 /2 = \u03c0/32, we get\n\u03c3 n (H ) \u2264 \u03c3 + 128K \u03c4 2 \u03c0 2\n, with probability at least 1 \u2212 2 exp(\u2212Km). Thus, taking K = min{\u03c0/32, \u03c3 2 \u03c4 2 \u03c0 2 /128} we get \u03c3 n (H ) \u2264 2\u03c3. If we now combine the bounds for H \u2212 H F and \u03c3 n (H ), we get\n4\u03c3 n (H ) H \u2212 H F + 2 H \u2212 H 2 F \u2264 32\u03c3 \u03c4 \u03c0m + 32 \u03c4 2 \u03c0 2 m 2 \u2264 64\u03c3 \u03c4 \u03c0m = \u03b8 \u03c1 m ,\nwhere have assumed that m \u2265 1/(\u03c4 \u03c0\u03c3). To get K \u03c1 note that the above bound holds with probability at least 1 \u2212 e \u2212m\u03c0/16 \u2212 2e \u2212Km \u2265 1 \u2212 3e \u2212Km \u2265 1 \u2212 e \u2212Km/2 = 1 \u2212 e \u2212K\u03c1m , where we have used that K \u2264 \u03c0/16 and assumed that m \u2265 2 ln(3)/K. Finally, applying Corollary 13 to \u03a6(Z) we see that with probability at least 1 \u2212 1/(4m 3 ) one has\n\u03c3 n (H ) 2 \u2212 \u03c3 n+1 (H ) 2 \u2265 \u03c1 \u2212 2 15 \u03c3 2 \u03c4 2 \u03c0 2 m ln(8m 3 ) \u2265 \u03c1 2 ,\nwhenever m \u2265 max{(2 17 \u03c3 2 /\u03c4 2 \u03c0 2 \u03c1 2 ) ln(8m 3 ), \u03bd 2 \u03c4 \u03c0|P||S|/(4\u03c3), (9 + 18/K \u03c1 ) ln(3 + 6/K \u03c1 ), (2/K \u03c1 ) ln(8m 3 )}. 2\nWe can now analyze how the change of one sample point in Z can affect the difference R(f Z ) \u2212 R Z (f Z ). Our main result will be obtained by applying Theorem 11 to this difference. \n| R Z (f ) \u2212 R Z (f )| \u2264 2\u03bd m + 1 m m\u22121 i=1 |f (x i ) \u2212 f (x i )| \u2264 2\u03bd m + \u03b2 2 m \u2212 1 m .\nObserve that for any samples Z and Z we have The following is the proof of our main result. ", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Acknowledgments", "text": "Borja Balle is partially supported by an FPU fellowship (AP2008-02064) and project TIN2011-27479-C04-03 (BASMATI) of the Spanish Ministry of Education and Science, the EU PASCAL2 NoE (FP7-ICT-216886), and by the Generalitat de Catalunya (2009-SGR-1428). The work of Mehryar Mohri was partly funded by the NSF grant IIS-1117591.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Digital image compression", "journal": "Springer", "year": "2009", "authors": "J Albert; J Kari"}, {"ref_id": "b1", "title": "Two SVDs suffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation", "journal": "CoRR", "year": "2012", "authors": "A Anandkumar; D P Foster; D Hsu; S M Kakade; Y-K Liu"}, {"ref_id": "b2", "title": "A method of moments for mixture models and hidden Markov models", "journal": "COLT", "year": "2012", "authors": "A Anandkumar; D Hsu; S M Kakade"}, {"ref_id": "b3", "title": "Quadratic weighted automata: Spectral algorithm and likelihood maximization", "journal": "ACML", "year": "2011", "authors": "R Bailly"}, {"ref_id": "b4", "title": "Grammatical inference as a principal component analysis problem. ICML", "journal": "", "year": "2009", "authors": "R Bailly; F Denis; L Ralaivola"}, {"ref_id": "b5", "title": "A spectral learning algorithm for finite state transducers", "journal": "ECML-PKDD", "year": "2011", "authors": "B Balle; A Quattoni; X Carreras"}, {"ref_id": "b6", "title": "Local loss optimization in operator models: A new insight into spectral learning", "journal": "ICML", "year": "2012", "authors": "B Balle; A Quattoni; X Carreras"}, {"ref_id": "b7", "title": "Learning functions represented as multiplicity automata", "journal": "JACM", "year": "2000", "authors": "A Beimel; F Bergadano; N H Bshouty; E Kushilevitz; S Varricchio"}, {"ref_id": "b8", "title": "Rational Series and Their Languages", "journal": "Springer", "year": "1988", "authors": "J Berstel; C Reutenauer"}, {"ref_id": "b9", "title": "Closing the learning planning loop with predictive state representations", "journal": "I. J. Robotic Research", "year": "2011", "authors": "B Boots; S Siddiqi; G Gordon"}, {"ref_id": "b10", "title": "Stability and generalization", "journal": "JMLR", "year": "2002", "authors": "O Bousquet; A Elisseeff"}, {"ref_id": "b11", "title": "The OCRopus open source OCR system", "journal": "", "year": "2008", "authors": "T M Breuel"}, {"ref_id": "b12", "title": "Matrix completion with noise", "journal": "", "year": "2010", "authors": "E J Candes; Y Plan"}, {"ref_id": "b13", "title": "The power of convex relaxation: Near-optimal matrix completion", "journal": "IEEE Transactions on Information Theory", "year": "2010", "authors": "E J Candes; T Tao"}, {"ref_id": "b14", "title": "Realizations by stochastic finite automata", "journal": "J. Comput. Syst. Sci", "year": "1971", "authors": "W Jack; Azaria Carlyle;  Paz"}, {"ref_id": "b15", "title": "Spectral learning of latent-variable PCFGs. ACL", "journal": "", "year": "2012", "authors": "S B Cohen; K Stratos; M Collins; D P Foster; L Ungar"}, {"ref_id": "b16", "title": "Matrices de Hankel", "journal": "Journal de Math\u00e9matiques Pures et Appliqu\u00e9es", "year": "1974", "authors": "M Fliess"}, {"ref_id": "b17", "title": "Learning with the weighted trace-norm under arbitrary sampling distributions. NIPS", "journal": "", "year": "2011", "authors": "R Foygel; R Salakhutdinov; O Shamir; N Srebro"}, {"ref_id": "b18", "title": "A spectral algorithm for learning hidden Markov models", "journal": "COLT", "year": "2009", "authors": "D Hsu; S M Kakade; T Zhang"}, {"ref_id": "b19", "title": "Cryptographic limitations on learning boolean formulae and finite automata", "journal": "JACM", "year": "1994", "authors": "M Kearns; L Valiant"}, {"ref_id": "b20", "title": "Extensions to McDiarmid's inequality when differences are bounded with high probability", "journal": "", "year": "2002", "authors": "S Kutin"}, {"ref_id": "b21", "title": "Spectral learning in non-deterministic dependency parsing", "journal": "EACL", "year": "2012", "authors": "F M Luque; A Quattoni; B Balle; X Carreras"}, {"ref_id": "b22", "title": "Weighted automata algorithms", "journal": "Springer", "year": "2009", "authors": "M Mohri"}, {"ref_id": "b23", "title": "Speech recognition with weighted finite-state transducers", "journal": "Springer", "year": "2008", "authors": "M Mohri; F C N Pereira; M Riley"}, {"ref_id": "b24", "title": "Foundations of Machine Learning", "journal": "The MIT Press", "year": "2012", "authors": "M Mohri; A Rostamizadeh; A Talwalkar"}, {"ref_id": "b25", "title": "A spectral algorithm for latent tree graphical models. ICML", "journal": "", "year": "2011", "authors": "A P Parikh; L Song; E P Xing"}, {"ref_id": "b26", "title": "A simpler approach to matrix completion", "journal": "JMLR", "year": "2011", "authors": "B Recht"}, {"ref_id": "b27", "title": "Automata-Theoretic Aspects of Formal Power Series", "journal": "Springer-Verlag", "year": "1978", "authors": "Arto Salomaa; Matti Soittola"}, {"ref_id": "b28", "title": "On the definition of a family of automata. Information and Control", "journal": "", "year": "1961", "authors": "M P Sch\u00fctzenberger"}, {"ref_id": "b29", "title": "Reduced-rank hidden Markov models", "journal": "AISTATS", "year": "2010", "authors": "S M Siddiqi; B Boots; G J Gordon"}, {"ref_id": "b30", "title": "Hilbert space embeddings of hidden Markov models", "journal": "", "year": "2010", "authors": "L Song; B Boots; S Siddiqi; G Gordon; A Smola"}, {"ref_id": "b31", "title": "Matrix perturbation theory", "journal": "Academic press", "year": "1990", "authors": "G W Stewart; J Sun"}, {"ref_id": "b32", "title": "On the convergence of eigenspaces in kernel principal component analysis", "journal": "NIPS", "year": "2006", "authors": "L Zwald; G Blanchard"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Example of a weighted automaton over \u03a3 = {a, b} with 2 states: (a) graph representation; (b) algebraic representation.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 11Figure1shows an example of a weighted automaton A = \u03b1, \u03b2, {A a } with two states defined over the alphabet \u03a3 = {a, b}, with both its algebraic representation (Figure1(b)) in terms of vectors and matrices and the equivalent graph representation (Figure1(a)) useful for a variety of WFA algorithms[23]. Let W = { , a, b}, then B = (W\u03a3 , W) is a \u03a3-complete basis. The following is the Hankel matrix of A on this basis shown with three-digit precision entries:", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "i.d. examples drawn from D, and Z differing from Z by just one point: say z m in Z = (z 1 , . . . , z m ) and z m in Z = (z 1 , . . . , z m\u22121 , z m ). The new example z m is an arbitrary point the support of D. Throughout the analysis we use the shorter notation H = H Z and H = H Z for the Hankel matrices obtained from (HMC-H) based on samples Z and Z respectively.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Lemma 6 (6[32]) Let A, B \u2208 R d1\u00d7d2 . Then, for any n \u2208 [1, min{d 1 , d 2 }], the following inequality holds:|\u03c3 n (A) \u2212 \u03c3 n (B)| \u2264 A \u2212 B .Lemma 7 ([32]) Let A, B \u2208 R d1\u00d7d2 . Then the following upper bound holds for the norm of the difference of the pseudo-inverses of matrices A and B:", "figure_data": ""}, {"figure_label": "217", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "2 Lemma 17217Let \u03b3 = \u03bd |P||S|/\u03c3 n (H ). The weighted automaton A Z is \u03b3-bounded. Proof. Since H a \u2264 H a F \u2264 \u03bd |P||S|, simple calculations show that \u03b1 \u2264 \u03bd |S|, \u03b2 \u2264 \u03bd |P|/\u03c3 n (H ), and A a \u2264 \u03bd |P||S|/\u03c3 n (H ). 2", "figure_data": ""}, {"figure_label": "213", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Lemma 21 3 .213Suppose D satisfies Assumptions 1 and 2. There exists a quantity M = poly(\u03bd, \u03c0, \u03c3, \u03c1, \u03c4, |P|, |S|) such that if m \u2265 M , then Z is good with probability at least 1 \u2212 1/m 3 . Proof. First note that by Assumption 2, writing L = ((1/c) ln(4m 4 )) 1/(1+\u03b7) a union bound yields P m i=1 |x i | > L \u2264 m exp(\u2212cL 1+\u03b7 ) = 1 4m Now letm = (x 1 , . . . , x m\u22121 )\u2229(PS). Note that we have min{ m, m } \u2265m and E Z [m] = \u03c0(m\u22121).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "For the third claimnote that by Lemma 6 we have |\u03c3 n (H )\u2212\u03c3 n (H )| \u2264 H \u2212H F \u2264 H\u2212H F . Thus, from the argument we just used in the previous bound we can see that when m \u2265 2 the function \u03a6(Z) = \u03c3 n (H ) is strongly difference-bounded by (b \u03c3 , \u03b8 \u03c3 /m, exp(\u2212K \u03c3 m)) with b \u03c3 = 2\u03bd |P||S|, \u03b8 \u03c3 = 2/(\u03c4 \u03c0(1 \u2212 \u2206)), and K \u03c3 = \u03c0\u2206 2 /4 for any \u2206 \u2208 (0, 1). Now note that by Lemma 6 and the previous goodness condition on H \u2212 H F we have min{\u03c3 n (H ), \u03c3 n (H )} \u2265 \u03c3 n (H ) \u2212 H \u2212 H F \u2265 \u03c3 n (H ) \u2212 4/(\u03bd\u03c0m). Furthermore, taking \u2206 = 1/2 and assuming that 3 ) , we can apply Corollary 13 with \u03b4 = 1/(4m 3 ) to see that \u03c3 n (H ) \u2212 4 \u03bd\u03c0m \u2265 \u03c3 \u2212 128 \u03c4 2 \u03c0 2 m ln(8m 3 ) \u2212 4 \u03bd\u03c0m", "figure_data": ""}, {"figure_label": "22", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Lemma 2222Let \u03b3 1 = 64\u03bd 4 |P| 2 |S| 3/2 /(\u03c4 \u03c3 3 \u03c1\u03c0) and \u03b3 2 = 2\u03bd|P|1/2 |S| 1/2 /\u03c3. If m \u2265 max{M, 16 \u221a 2/(\u03c4 \u03c0 \u221a \u03c1), exp(6 ln \u03b3 2 (1.2c ln \u03b3 2 ) 1/\u03b7 )}, then the function \u03a6(Z) = R(f Z ) \u2212 R Z (f Z )is strongly difference-bounded by (4\u03bd + 2\u03bd/m, c 2 \u03b3 1 m \u22125/6 ln m, 1/m 3 ) for some constant c 2 > 0.Proof. We will write for short f = f Z and f = f Z . Let\u03b2 1 = E x\u223cD\u03a3 [|f (x) \u2212 f (x)|] and \u03b2 2 = max 1\u2264i\u2264m\u22121 |f (x i ) \u2212 f (x i )|. We first show that |\u03a6(Z) \u2212 \u03a6(Z )| \u2264 \u03b2 1 + \u03b2 2 + 2\u03bd/m. By definition of \u03a6 we can write |\u03a6(Z) \u2212 \u03a6(Z )| \u2264 |R(f ) \u2212 R(f )| + | R Z (f ) \u2212 R Z (f )| .By Jensen's inequality, the first term can be upper bounded by E (x,y)\u223cD [||f (x)\u2212y|\u2212|f (x)\u2212y||] \u2264 \u03b2 1 . Now, using the triangle inequality and |f (x m ) \u2212 y m |, |f (x m ) \u2212 y m | \u2264 2\u03bd, the second term can be bounded as follows:", "figure_data": ""}, {"figure_label": "1232", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "\u03b2 1 , \u03b2 2 \u2264 3 and \u03b2 2 \u226412322\u03bd. This provides an almost-sure upper bound needed in the definition of strongly difference-boundedness. We use this bound when the sample Z is not good. By Lemma 21, when m is large enough this event will occur with probability at most 1/m 3 . It remains to bound \u03b2 1 and \u03b2 2 assuming that Z is good. Note that by Lemma 21, m \u2265 max{M, 16 \u221a 2/(\u03c4 \u03c0 \u221a \u03c1)} implies H \u2212 H F \u2264 \u03c1/4. Thus, by combining Lemmas 16, 17,19, and 21, we see that the following holds for any x \u2208 \u03a3 :|f (x) \u2212 f (x)| \u2264 2\u03bd|P| 1/2 |S| 1/2 \u03c3 |x|+1 32c 1 (|x| + 2)\u03bd 3 |P| 3/2 |S| m\u03c4 \u03c0\u03c3 2 \u03c1 = c 1 \u03b3 1 m exp(|x| ln \u03b3 2 + ln(|x| + 2)) .In particular, for |x| \u2264 L = ((1/c) ln(4m 4 )) 1/(1+\u03b7) and m \u2265 exp(6ln \u03b3 2 (1.2c ln \u03b3 2 ) 1/\u03b7 ), a simple calculation shows that |f (x) \u2212 f (x)| \u2264 C\u03b3 1 m \u22125/6ln m for some constant C. Thus, we can write\u03b2 1 \u2264 E x\u223cD\u03a3 [|f (x) \u2212 f (x)| | |x| \u2264 L] + 2\u03bdP x\u223cD\u03a3 [|x| \u2265 L] \u2264 C\u03b3 1 m \u22125/6 ln m + \u03bd/2m C\u03b3 1 m \u22125/6ln m, where the last bound follows from the goodness of Z. Combining these bounds yields the desired result. 2", "figure_data": ""}, {"figure_label": "23", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Proof.[of Theorem 2 ] 3 ,23The result follows from an application of Theorem 11 to \u03a6(Z), defined as in Lemma 22. In particular, for large enough m, the following holds with probability at least 1 \u2212 \u03b4:R(f Z ) \u2264 R Z (f Z ) + E Z\u223cD m [\u03a6(Z)] + C\u03b3 2 1 ln 2 m m 2/3 ln 1 \u03b4 \u2212 6\u03bd C \u03b31 1 m 7/6 ln m , for some constants C, C and \u03b3 1 = \u03bd 4 |P| 2 |S| 3/2 /\u03c4 \u03c3 3 \u03c1\u03c0. Thus, it remains to bound E Z\u223cD m [\u03a6(Z)].First note that we haveE Z\u223cD m [R(f Z )] = E Z,z\u223cD m+1 [|f Z (x) \u2212 y|].On the other hand, we can also writeE Z\u223cD m [ R Z (f Z )] = E Z,z\u223cD m+1 [|f Z (x) \u2212 y|],where Z is a sample of size m containing z and m \u2212 1 other points in Z chosen at random. Thus, by Jensen's inequality we can write| E Z\u223cD m [\u03a6(Z)]| \u2264 E Z,z\u223cD m+1 [|f Z (x) \u2212 f Z (x)|] .Now an argument similar to the one used in Lemma 22 for bounding \u03b2 1 can be used to show that, for large enough m, the following inequality holds: E Z\u223cD m [\u03a6(Z)] \u2264 C\u03b3 1 ln m m 5/6 + 2\u03bd m which completes the proof. 2", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "M p = ( n\u22651 \u03c3 p n (M)) 1/p", "formula_coordinates": [3.0, 112.98, 141.65, 116.21, 12.72]}, {"formula_id": "formula_1", "formula_text": "H = H \u2208 R P\u00d7S : \u2200u 1 , u 2 \u2208 P, \u2200v 1 , v 2 \u2208 S, u 1 v 1 = u 2 v 2 \u21d2 H(u 1 , v 1 ) = H(u 2 , v 2 ) .", "formula_coordinates": [3.0, 115.3, 377.39, 381.39, 11.72]}, {"formula_id": "formula_2", "formula_text": "h, h H = x\u2208PS c x h(x)h (x) ,", "formula_coordinates": [3.0, 246.34, 534.61, 123.19, 20.06]}, {"formula_id": "formula_3", "formula_text": "f A (x) = \u03b1 A x1 \u2022 \u2022 \u2022 A xt \u03b2 , for any string x = x 1 \u2022 \u2022 \u2022 x t \u2208 \u03a3 * with t = |x| and x i \u2208 \u03a3 for all i \u2208 [1, t].", "formula_coordinates": [3.0, 108.0, 686.39, 297.8, 24.4]}, {"formula_id": "formula_4", "formula_text": "A = \u03b1, \u03b2, {A a } is \u03b3-bounded if \u03b1 , \u03b2 , A a \u2264 \u03b3 for all a \u2208 \u03a3.", "formula_coordinates": [3.0, 108.0, 712.07, 283.99, 9.68]}, {"formula_id": "formula_5", "formula_text": "1 -1 a, 0 b, 2 3 a, 0 b, 3 4 a, 1 3 b, 1 a, 3 4 b, 6 5 1/2 1/2 \u03b1 = [1/2 1/2] \u03b2 = [1 \u22121] A a = 3/4 0 0 1/3 A b = 6/5 2/3 3/4 1 (a) (b)", "formula_coordinates": [4.0, 131.13, 82.56, 344.69, 83.29]}, {"formula_id": "formula_6", "formula_text": "Z = (z 1 , . . . , z m ) containing m examples z i = (x i , y i ) \u2208 \u03a3 \u00d7 R, 1 \u2264 i \u2264 m, drawn i.i.d. from some distribution D over \u03a3 \u00d7 R.", "formula_coordinates": [4.0, 227.39, 693.91, 276.61, 9.65]}, {"formula_id": "formula_7", "formula_text": "F Z (h) = \u03c4 N (h) + R e Z (h) = \u03c4 h 2 p + 1 m (x,y)\u2208 e Z (h(x), y) ,", "formula_coordinates": [5.0, 180.9, 426.58, 250.2, 29.91]}, {"formula_id": "formula_8", "formula_text": "Z satisfying h Z \u2208 argmin h\u2208H F Z (h) . (HMC-h)", "formula_coordinates": [5.0, 186.51, 482.93, 317.49, 27.24]}, {"formula_id": "formula_9", "formula_text": "F Z (H) = \u03c4 N (H) + R e Z (H) = \u03c4 H 2 p + 1 m (x,y)\u2208 e Z (u x Hv x , y) .", "formula_coordinates": [5.0, 167.48, 568.16, 277.04, 29.9]}, {"formula_id": "formula_10", "formula_text": "H Z \u2208 argmin H\u2208H F Z (H) . (HMC-H)", "formula_coordinates": [5.0, 257.45, 626.38, 246.55, 16.59]}, {"formula_id": "formula_11", "formula_text": "\u03b1 = h ,S V n \u03b2 = (H V n ) + h P , A a = (H V n ) + H a V n . (SM)", "formula_coordinates": [6.0, 161.35, 282.07, 342.65, 12.69]}, {"formula_id": "formula_12", "formula_text": "\u03c3 = E Z\u223cD m [\u03c3 n (H )] \u03c1 = E Z\u223cD m \u03c3 n (H ) 2 \u2212 \u03c3 n+1 (H ) 2", "formula_coordinates": [6.0, 169.81, 605.14, 258.32, 16.65]}, {"formula_id": "formula_13", "formula_text": "(f ) = E z\u223cD [ (f (x), y)] in terms of the empirical loss R Z (f ) = |Z| \u22121 z\u2208Z (f (x), y).", "formula_coordinates": [7.0, 108.0, 299.9, 396.0, 24.35]}, {"formula_id": "formula_14", "formula_text": "1 \u2212 \u03b4 for f Z = t \u03bd \u2022 f A Z : R(f Z ) \u2264 R Z (f Z ) + O \u03bd 4 |P| 2 |S| 3/2 \u03c4 \u03c3 3 \u03c1\u03c0 ln m m 1/3 ln 1 \u03b4 .", "formula_coordinates": [7.0, 129.31, 369.95, 287.4, 43.09]}, {"formula_id": "formula_15", "formula_text": "H \u2212 H F \u2264 min 2\u03bd |P||S|, 1 \u03c4 min{ m, m } .", "formula_coordinates": [7.0, 203.44, 564.89, 210.1, 22.31]}, {"formula_id": "formula_16", "formula_text": "\u2200a \u2208 \u03a3 : A a \u2212 A a \u2264 C\u03b5\u03bd 3 |P| 3/2 |S| 1/2 / \u03c1 \u03c3 2 ; \u03b1 \u2212 \u03b1 \u2264 C\u03b5\u03bd 2 |P| 1/2 |S|/ \u03c1; \u03b2 \u2212 \u03b2 \u2264 C\u03b5\u03bd 3 |P| 3/2 |S| 1/2 / \u03c1 \u03c3 2 .", "formula_coordinates": [8.0, 204.52, 100.88, 202.96, 44.04]}, {"formula_id": "formula_17", "formula_text": "+ \u2212 B + \u2264 1 + \u221a 5 2 max A + 2 , B + 2 A \u2212 B", "formula_coordinates": [10.0, 201.78, 207.11, 217.1, 30.55]}, {"formula_id": "formula_18", "formula_text": "V n \u2212 W n F \u2264 4 E F \u03bb n (A) \u2212 \u03bb n+1 (A)", "formula_coordinates": [10.0, 230.44, 305.35, 147.17, 23.25]}, {"formula_id": "formula_20", "formula_text": "B = A + E. Suppose n \u2264 rank(A) and E F \u2264 \u03c3 n (A) 2 \u2212 \u03c3 n+1 (A) 2 /4. If V n , W n contain the first n right singular vectors of A and B respec- tively, then V n \u2212 W n F \u2264 8 A F E F + 4 E 2 F \u03c3 n (A) 2 \u2212 \u03c3 n+1 (A) 2 .", "formula_coordinates": [10.0, 108.0, 371.4, 396.0, 60.18]}, {"formula_id": "formula_21", "formula_text": "one has 1 + \u221a 2 \u221a x + y \u2265 \u221a x + \u221a y. Thus, we get E F \u2264 \u03c3 n (A) 2 \u2212 \u03c3 n+1 (A) 2 4 \u2264 \u03c3 n (A) 2 \u2212 \u03c3 n+1 (A) 2 + 4 A 2 F \u2212 2 A F 2 1 + \u221a 2 \u2264 4 A 2 F + \u03c3 n (A) 2 \u2212 \u03c3 n+1 (A) 2 \u2212 2 A F 2 ,", "formula_coordinates": [10.0, 108.0, 478.9, 311.18, 111.26]}, {"formula_id": "formula_22", "formula_text": "A F E F + E 2 F \u2264 (\u03c3 n (A) 2 \u2212 \u03c3 n+1 (A) 2 )/4. 2", "formula_coordinates": [10.0, 242.22, 597.27, 210.94, 12.48]}, {"formula_id": "formula_23", "formula_text": "P [\u03a6 \u2212 E[\u03a6] \u2265 t] \u2264 exp \u2212t 2 8mc 2 + mb\u03b4 c .", "formula_coordinates": [11.0, 216.61, 112.75, 178.79, 23.88]}, {"formula_id": "formula_24", "formula_text": "P[E[\u03a6] \u2212 \u03a6 \u2265 t].", "formula_coordinates": [11.0, 292.93, 145.74, 69.74, 9.3]}, {"formula_id": "formula_25", "formula_text": "P [\u03a6 \u2212 E[\u03a6] \u2265 t] \u2264 2 exp \u2212t 2 m 8\u03b8 2 .", "formula_coordinates": [11.0, 227.63, 201.79, 156.75, 23.88]}, {"formula_id": "formula_26", "formula_text": "P[E[\u03a6] \u2212 \u03a6 \u2265 t].", "formula_coordinates": [11.0, 292.93, 231.88, 69.74, 9.3]}, {"formula_id": "formula_27", "formula_text": "\u03a6 \u2265 E[\u03a6] \u2212 8\u03b8 2 m ln 2 \u03b4 , \u03a6 \u2264 E[\u03a6] + 8\u03b8 2 m ln 2 \u03b4 .", "formula_coordinates": [11.0, 245.9, 322.84, 120.2, 57.42]}, {"formula_id": "formula_28", "formula_text": "2\u03c4 h \u2212 h 2 2 \u2264 R e Z (h ) \u2212 R e Z (h) + R e Z (h) \u2212 R e Z (h ) .", "formula_coordinates": [11.0, 191.12, 535.91, 229.76, 14.67]}, {"formula_id": "formula_29", "formula_text": "2\u03c4 h \u2212 h 2 2 \u2264 \u03c4 (B N (h h) + B N (h h )) \u2264 B F Z (h h) + B F Z (h h )", "formula_coordinates": [11.0, 155.26, 600.38, 293.73, 13.17]}, {"formula_id": "formula_30", "formula_text": "B F Z (h h) + B F Z (h h ) = R e Z (h ) \u2212 R e Z (h) + R e Z (h) \u2212 R e Z (h ). 2", "formula_coordinates": [11.0, 108.0, 631.11, 396.0, 26.63]}, {"formula_id": "formula_31", "formula_text": "H \u2212 H F \u2264 min 2\u03bd |P||S|, 1 \u03c4 min{ m, m } .", "formula_coordinates": [11.0, 202.61, 706.85, 211.76, 22.31]}, {"formula_id": "formula_32", "formula_text": "2\u03c4 h \u2212 h 2 2 \u2264 R e Z (h ) \u2212 R e Z (h) + R e Z (h) \u2212 R e Z (h ) .(2)", "formula_coordinates": [12.0, 191.12, 164.9, 312.88, 14.67]}, {"formula_id": "formula_33", "formula_text": "x m , x m / \u2208 PS, then Z = Z . Therefore, R e Z (h) = R e Z (h), R e Z (h ) = R e Z(", "formula_coordinates": [12.0, 116.98, 214.51, 303.61, 12.59]}, {"formula_id": "formula_34", "formula_text": "R e Z (h) \u2212 R e Z (h) = |h(x m ) \u2212 y m | \u2212 |h(x m ) \u2212 y m | m , R e Z (h ) \u2212 R e Z (h ) = |h (x m ) \u2212 y m | \u2212 |h (x m ) \u2212 y m | m .", "formula_coordinates": [12.0, 190.79, 259.83, 230.42, 47.36]}, {"formula_id": "formula_35", "formula_text": "2\u03c4 h \u2212 h 2 2 \u2264 |h(x m ) \u2212 h (x m )| + |h(x m ) \u2212 h (x m )| m \u2264 2 m h \u2212 h 2 ,", "formula_coordinates": [12.0, 155.87, 329.44, 300.27, 22.31]}, {"formula_id": "formula_36", "formula_text": "x) \u2212 y| \u2212 |h (x) \u2212 y|| \u2264 |h(x) \u2212 h (x)|, and the second from |h(x) \u2212 h (x)| \u2264 h \u2212 h 2 .", "formula_coordinates": [12.0, 108.0, 357.97, 396.0, 20.61]}, {"formula_id": "formula_37", "formula_text": "z\u2208 e Z |h (x) \u2212 y| m \u2212 |h (x) \u2212 y| m + |h(x) \u2212 y| m \u2212 |h(x) \u2212 y| m + |h (x m ) \u2212 y m | m \u2212 |h(x m ) \u2212 y m | m .", "formula_coordinates": [12.0, 108.0, 403.51, 399.28, 29.91]}, {"formula_id": "formula_38", "formula_text": "2\u03c4 h \u2212 h 2 2 \u2264 z\u2208 e Z |h(x) \u2212 h (x)| m m + |h(x m ) \u2212 h (x m )| m \u2264 2 m h \u2212 h 2 .", "formula_coordinates": [12.0, 152.37, 457.03, 307.26, 29.91]}, {"formula_id": "formula_39", "formula_text": "|f A (x) \u2212 f A (x)| \u2264 \u03b3 |x|+1 \u03b1 \u2212 \u03b1 + \u03b2 \u2212 \u03b2 + |x| i=1 A xi \u2212 A xi .", "formula_coordinates": [12.0, 154.22, 602.7, 303.56, 31.18]}, {"formula_id": "formula_40", "formula_text": "\u03b5 = H \u2212 H , \u03b5 a = H a \u2212 H a , \u03b5 V = V \u2212 V , \u03b5 S = h \u03bb,S \u2212 h \u03bb,S , \u03b5 P = h P,\u03bb \u2212 h P,\u03bb .", "formula_coordinates": [13.0, 258.45, 104.37, 95.1, 72.22]}, {"formula_id": "formula_41", "formula_text": "A a \u2212 A a \u2264 \u03b5 a + \u03b5 V H a \u03c3 n (H V) + 1 + \u221a 5 2 H a (\u03b5 + \u03b5 V H ) min{\u03c3 n (H V) 2 , \u03c3 n (H V ) 2 } , \u03b1 \u2212 \u03b1 \u2264 \u03b5 S + \u03b5 V h \u03bb,S , \u03b2 \u2212 \u03b2 \u2264 \u03b5 P \u03c3 n (H V) + 1 + \u221a 5 2 h P,\u03bb (\u03b5 + \u03b5 V H ) min{\u03c3 n (H V) 2 , \u03c3 n (H V ) 2 } .", "formula_coordinates": [13.0, 160.71, 224.18, 295.57, 75.81]}, {"formula_id": "formula_42", "formula_text": "A a \u2212 A a = (H V) + (H a V \u2212 H a V ) + ((H V ) + \u2212 (H V) + )H a V \u2264 (H V) + H a V \u2212 H a V + (H V) + \u2212 (H V ) + H a V \u2264 \u03c3 n (H V) \u22121 H a V \u2212 H a V + H a (H V) + \u2212 (H V ) + ,", "formula_coordinates": [13.0, 148.86, 345.04, 319.27, 44.28]}, {"formula_id": "formula_43", "formula_text": "H a V \u2212 H a V \u2264 V H a \u2212 H a + H a V \u2212 V \u2264 \u03b5 a + \u03b5 V H a .", "formula_coordinates": [13.0, 156.77, 429.65, 303.43, 10.65]}, {"formula_id": "formula_44", "formula_text": "(H V) + \u2212 (H V ) + \u2264 1 + \u221a 5 2 H V \u2212 H V max{ (H V) + 2 , (H V ) + 2 } \u2264 1 + \u221a 5 2 H \u2212 H V + H V \u2212 V min{\u03c3 n (H V) 2 , \u03c3 n (H V ) 2 } = 1 + \u221a 5 2 \u03b5 + \u03b5 V H min{\u03c3 n (H V) 2 , \u03c3 n (H V ) 2 } .", "formula_coordinates": [13.0, 136.32, 461.44, 344.34, 87.15]}, {"formula_id": "formula_45", "formula_text": "V h \u03bb,S \u2212 V h \u03bb,S \u2264 V \u2212 V h \u03bb,S + V h \u03bb,S \u2212 h \u03bb,S = \u03b5 S + \u03b5 V h \u03bb,S ,", "formula_coordinates": [13.0, 121.32, 579.56, 374.34, 10.65]}, {"formula_id": "formula_46", "formula_text": "\u03b2 \u2212 \u03b2 \u2264 (H V) + h P,\u03bb \u2212 h P,\u03bb + h P,\u03bb (H V) + \u2212 (H V ) + \u2264 h P,\u03bb \u2212 h P,\u03bb \u03c3 n (H V) + 1 + \u221a 5 2 h P,\u03bb H V \u2212 H V min{\u03c3 n (H V) 2 , \u03c3 n (H V ) 2 } \u2264 \u03b5 P \u03c3 n (H V) + 1 + \u221a 5 2 h P,\u03bb (\u03b5 + \u03b5 V H ) min{\u03c3 n (H V) 2 , \u03c3 n (H V ) 2 } . 2", "formula_coordinates": [13.0, 108.0, 635.54, 342.35, 87.21]}, {"formula_id": "formula_47", "formula_text": "A a \u2212 A a \u2264 c 1 \u03b5\u03bd 3 |P| 3/2 |S| 1/2 \u03c1 \u03c3 2 , \u03b1 \u2212 \u03b1 \u2264 c 1 \u03b5\u03bd 2 |P| 1/2 |S| \u03c1 , \u03b2 \u2212 \u03b2 \u2264 c 1 \u03b5\u03bd 3 |P| 3/2 |S| 1/2 \u03c1 \u03c3 2 .", "formula_coordinates": [14.0, 237.67, 122.33, 141.64, 80.57]}, {"formula_id": "formula_48", "formula_text": "V \u2212 V \u2264 4\u03b5 \u03c1 (2\u03bd |P||S| + \u03b5) \u2264 16\u03b5\u03bd |P||S| \u03c1 ,", "formula_coordinates": [14.0, 202.13, 292.24, 212.72, 22.5]}, {"formula_id": "formula_49", "formula_text": "A a \u2212 A a \u2264 \u03b5 \u03c3 1 + 16\u03bd|P| 1/2 |S| 1/2 \u03c1 + 1 + \u221a 5 2 \u03b5\u03bd|P| 1/2 |S| 1/2 \u03c3 2 1 + 16\u03bd 2 |P||S| \u03c1 , \u03b1 \u2212 \u03b1 \u2264 \u03b5 1 + 16\u03bd 2 |P| 1/2 |S| \u03c1 , \u03b2 \u2212 \u03b2 \u2264 \u03b5 \u03c3 + 1 + \u221a 5 2 \u03b5\u03bd|P| 1/2 \u03c3 2 1 + 16\u03bd 2 |P||S| \u03c1 .", "formula_coordinates": [14.0, 124.99, 341.35, 367.0, 88.82]}, {"formula_id": "formula_50", "formula_text": "\u2022 |x i | \u2264 ((1/c) ln(4m 4 )) 1/(1+\u03b7) for all 1 \u2264 i \u2264 m; \u2022 H \u2212 H F \u2264 4/(\u03c4 \u03c0m); \u2022 min{\u03c3 n (H ), \u03c3 n (H )} \u2265 \u03c3/2; \u2022 \u03c3 n (H ) 2 \u2212 \u03c3 n+1 (H ) 2 \u2265 \u03c1/2.", "formula_coordinates": [14.0, 133.9, 518.03, 209.36, 66.85]}, {"formula_id": "formula_51", "formula_text": "P[m < \u03c0(m \u2212 1)(1 \u2212 \u2206)] \u2264 exp \u2212 (m \u2212 1)\u03c0\u2206 2 2 \u2264 exp \u2212 m\u03c0\u2206 2 4 ,", "formula_coordinates": [14.0, 153.04, 710.26, 305.92, 23.88]}, {"formula_id": "formula_52", "formula_text": "min{\u03c3 n (H ), \u03c3 n (H )} \u2265 \u03c3 \u2212 128 \u03c4 2 \u03c0 2 m ln(8m 3 ) \u2212 4 \u03bd\u03c0m \u2265 \u03c3 \u2212 \u03c3 4 \u2212 \u03c3 4 = \u03c3 2 .", "formula_coordinates": [15.0, 141.89, 351.46, 328.22, 22.31]}, {"formula_id": "formula_53", "formula_text": "|\u03a6(Z) \u2212 \u03a6(Z )| = (\u03c3 n (H ) 2 \u2212 \u03c3 n+1 (H ) 2 ) \u2212 (\u03c3 n (H ) 2 \u2212 \u03c3 n+1 (H ) 2 ) \u2264 |\u03c3 n (H ) 2 \u2212 \u03c3 n (H ) 2 | + |\u03c3 n+1 (H ) 2 \u2212 \u03c3 n+1 (H ) 2 | = |\u03c3 n (H ) + \u03c3 n (H )||\u03c3 n (H ) \u2212 \u03c3 n (H )| + |\u03c3 n+1 (H ) + \u03c3 n+1 (H )||\u03c3 n+1 (H ) \u2212 \u03c3 n+1 (H )| \u2264 (2\u03c3 n (H ) + H \u2212 H ) H \u2212 H + (2\u03c3 n+1 (H ) + H \u2212 H ) H \u2212 H \u2264 4\u03c3 n (H ) H \u2212 H F + 2 H \u2212 H 2", "formula_coordinates": [15.0, 108.0, 415.5, 455.27, 74.2]}, {"formula_id": "formula_54", "formula_text": "n (H \u03c3 ) \u2264 H \u03c3 F \u2264 \u03bd |P||S| we get 4\u03c3 n (H ) H \u2212 H F + 2 H \u2212 H 2 F \u2264 16\u03bd 2 |P||S|", "formula_coordinates": [15.0, 108.0, 520.59, 396.0, 43.14]}, {"formula_id": "formula_55", "formula_text": "\u03c3 n (H ) \u2264 \u03c3 + 128K \u03c4 2 \u03c0 2", "formula_coordinates": [15.0, 252.31, 617.03, 97.72, 22.31]}, {"formula_id": "formula_56", "formula_text": "4\u03c3 n (H ) H \u2212 H F + 2 H \u2212 H 2 F \u2264 32\u03c3 \u03c4 \u03c0m + 32 \u03c4 2 \u03c0 2 m 2 \u2264 64\u03c3 \u03c4 \u03c0m = \u03b8 \u03c1 m ,", "formula_coordinates": [15.0, 153.19, 672.41, 305.63, 22.31]}, {"formula_id": "formula_57", "formula_text": "\u03c3 n (H ) 2 \u2212 \u03c3 n+1 (H ) 2 \u2265 \u03c1 \u2212 2 15 \u03c3 2 \u03c4 2 \u03c0 2 m ln(8m 3 ) \u2265 \u03c1 2 ,", "formula_coordinates": [16.0, 190.42, 114.87, 231.16, 23.15]}, {"formula_id": "formula_58", "formula_text": "| R Z (f ) \u2212 R Z (f )| \u2264 2\u03bd m + 1 m m\u22121 i=1 |f (x i ) \u2212 f (x i )| \u2264 2\u03bd m + \u03b2 2 m \u2212 1 m .", "formula_coordinates": [16.0, 156.43, 360.01, 299.14, 30.32]}], "doi": ""}