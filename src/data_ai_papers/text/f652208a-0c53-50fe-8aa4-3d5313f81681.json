{"title": "The SoftCumulative Constraint with Quadratic Penalty", "authors": "Yanick Ouellet; Claude-Guy Quimper", "pub_date": "", "abstract": "The CUMULATIVE constraint greatly contributes to the success of constraint programming at solving scheduling problems. SOFTCUMULATIVE, a version of the CUMULATIVE constraint where overloading the resource incurs a penalty is, however, less studied. We introduce a checker and a filtering algorithm for SOFTCUMULATIVE, which are inspired by the energetic reasoning rule for the CUMULATIVE. Both algorithms can be used with a classic linear penalty function, but also with a quadratic penalty function, where the penalty of overloading the resource increases quadratically with the amount of the overload. We show that these algorithms are more general than existing algorithms and outperform a decomposition of SOFTCUMULATIVE in practice.", "sections": [{"heading": "Introduction", "text": "Scheduling problems where tasks share a finite amount of resources are everywhere in the industry. For instance, a university might want to schedule courses in classrooms or a factory might need to plan its production to minimize the peak in the power usage of its machines. The constraint programming community invested significant efforts in developing the CUMULATIVE constraint to help solve problems where the capacity of a resource can never be overloaded. However, a less studied but as important problem is to allow the resource to be overloaded in exchange of a penalty. For instance, a pharmaceutical company could ask its workers to work overtime to ensure that enough doses of a COVID vaccine are produced before a deadline.\nWe present a checker and a filtering algorithm for the SOFTCUMULATIVE constraint, a generalization of the well known CUMULATIVE that allows resource overloads. Our algorithms are based on the energetic reasoning rule used by CUMULATIVE. The algorithms work for both linear and quadratic penalty functions. To the best of our knowledge, a quadratic penalty function cannot currently be modelled by using an existing global constraint.\nSection 2 presents a background from the literature. We introduce our version of SOFTCUMULATIVE (Section 3), present our checker (Section 4) and filtering (Section 5) algorithms. We explain how to use our algorithms with lazy Copyright c 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. clause generation solvers in Section 6. Section 7 shows how relevant our algorithms are in practice before concluding.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Background Scheduling", "text": "Consider a set I of n tasks. Each task i \u2208 I has to be executed between its earliest starting time est i and its latest completion time lct i and has for processing time p i . During its execution, the task requires h i units of a resource at each time point. Let a task i be represented by the tuple est i , lct i , p i , h i . From these parameters, it is possible to compute the earliest completion time ect i = est i +p i and the latest starting time lst i = lct i \u2212p i of a task i. The energy e i = p i \u2022 h i represents the total amount of resource consumed during the execution of the task. The earliest starting time est \u2126 = min i\u2208\u2126 (est i ) of the set of tasks \u2126 \u2286 I is the earliest time point at which any task in \u2126 can start. Similarly, the latest completion time lct \u2126 = max i\u2208\u2126 (lct i ) of \u2126 is the latest time point at which any task in \u2126 can end.\nA task i has a compulsory part in the interval [lst i , ect i ) if lst i < ect i . The task must necessarily execute during its compulsory part since it cannot start later than its latest starting time and cannot end before its earliest completion time.\nUsing constraint programming, one can use the CU-MULATIVE( S, p, h, C) constraint (Aggoun and Beldiceanu 1993) to enforce that, at any time point, the resource consumption of the tasks in execution is less than or equal to the capacity C of a resource. The constraint uses one starting time variable S i \u2208 [est i , lst i ] for each task i in the problem, as well as parameters for their processing times, heights, and the capacity of the resource.\nDeciding whether the CUMULATIVE constraint can be satisfied is NP-Complete (Aggoun and Beldiceanu 1993). Filtering algorithms for the constraint can neither enforce domain or bounds consistency in polynomial time. Instead, it is necessary to rely on detection and filtering rules that work on a relaxation of the constraint. Many such rules have been introduced over the years. Rules relevant to this paper are presented below. Some rules have faster algorithms to apply them while others are slower but produce stronger inconsistency detection and filtering.\nThe Time-Tabling rule (1) (Beldiceanu and Carlsson 2002) ", "publication_ref": ["b0", "b0", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "uses a reasoning based on the compulsory parts of the", "text": "The Thirty-Sixth AAAI Conference on Artificial Intelligence  tasks. At any time point, if the sum of the height of the compulsory parts is greater than the capacity of the resource, the CUMULATIVE constraint cannot be satisfied. This checker rule is sufficient to enforce CUMULATIVE. We refer the reader to (Beldiceanu and Carlsson 2002) for the filtering rule based on the TimeTabling.\n\u2203t i\u2208I : lsti\u2264t<ecti h i > C =\u21d2 fail (1)\nThe EdgeFinding rule (2) (Mercier and Van Hentenryck 2008) checks for precedences between a set of tasks \u2126 and a task i. If the combined energy e \u2126\u222a{i} of the tasks in \u2126 and i is greater than the energy available in the interval [est \u2126\u222a{i} , lct \u2126 ), then i must end after all the tasks in \u2126 have ended. Indeed, i is the only task that can execute outside of [est \u2126\u222a{i} , lct \u2126 ) and prevent an overflow in this interval.\n\u2200i \u2208 I, \u2126 \u2286 I \\ {i} e \u2126\u222a{i} > C \u2022 (lct \u2126 \u2212 est \u2126\u222a{i} ) =\u21d2 i ends after \u2126 (2)\nOf particular interest for this paper is the energetic reasoning rule (Lopez and Esquirol 1996). It is one of the strongest rules, but also one of the slowest to apply. The energetic reasoning is based on the notion of left and right shift in an interval. Let LS(i, l, u) = h i \u2022 (min(u, ect i ) \u2212 max(l, est i )) be the left-shift of task i in the interval [l, u). It represents the amount of energy that the task consumes in the interval if it is scheduled at its earliest. The right-shift\nRS(i, l, u) = h i \u2022 (min(u, lct i ) \u2212 max(l, lst i )) of task i in interval [l, u\n) is symmetric and represents the amount of energy that task i consumes in the interval if it is scheduled at its latest. The minimum intersection MI(i, l, u) = min(LS(i, l, u), RS(i, l, u)) is the minimum between the left-shift and the right-shift. Regardless of when a task is scheduled, it always consumes at least MI(i, l, u) units of energy in a given interval [l, u), as shown on Figure 1.\nl u LS(1, 2, 9) = 4 RS(1, 2, 9) = 6 MI(1, 2, 9) = min(4, 6) = 4 ect1 lst1 0 1 2 3 4 5 6 7 8 9 10 11 12\nFigure 1: Example of the minimum intersection of task with est 1 = 0, lct 1 = 10, p 1 = 4 and h 1 = 2. The left gray area represents the left-shift of 4 while the right gray area represents the right shift of 6. The minimum intersection is the minimum between the left and right shift, which is 4. Regardless of where the task is scheduled, it always consume at least 4 units of energy in that interval.\nLet MI(\u2126, l, u) = i\u2208\u2126 MI(i, l, u) be the sum of the minimum intersection of all the tasks in \u2126. The satisfiability rule for the energetic reasoning (3) states that if there exists an interval for which this sum is greater than the energy available in that interval on the resource, then the constraint cannot be satisfied. \u2203l, \u2203u, MI(I, l, u) > C \u2022(u \u2212 l) =\u21d2 fail\n(3) Baptiste and al. (2001) showed that testing the rule on O(n 2 ) intervals, called intervals of interest, is equivalent to testing the rule on all possible intervals. There are three types of intervals of interest (4). While there are O(n 2 ) intervals of interest, there are also O(n 2 ) distinct lower bounds and O(n 2 ) distinct upper bounds. (Baptiste, Le Pape, and Nuijten 2001) proposed an O(n 2 ) algorithm to apply the rule using the intervals of interest and (Ouellet and Quimper 2018) improved it to O(n log 2 n).", "publication_ref": ["b2", "b8", "b7", "b1", "b1", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Intervals of Interest", "text": "T e ={[l, u) | l \u2208 O 1 , u \u2208 O 2 } \u222a {[l, u) | l \u2208 O 1 , u \u2208 O(l)} \u222a {[l, u) | u \u2208 O 2 , l \u2208 O(u)} where O 1 = {est i | i \u2208 I} \u222a {ect i | i \u2208 I} \u222a {lst i | i \u2208 I} O 2 = {lst i | i \u2208 I} \u222a {ect i | i \u2208 I} \u222a {lct i | i \u2208 I} O(t) = {est i + lct i \u2212t}(4)\nIt is also possible to filter the starting times using a rule similar to the checker rule. Baptiste proposed a \u0398(n 3 ) algorithm to do so. Tesch (2018) and Ouellet and Quimper (2018) independently proposed algorithms in O(n 2 log n) and O(n 2 log 2 n) respectively.", "publication_ref": ["b13", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Lazy Clause Generation", "text": "Several modern constraint programming solvers, such as Chuffed (Chu 2011) and OR-tools (Perron and Furnon 2019), use the lazy clause generation technique (Ohrimenko, Stuckey, and Codish 2009) to enhance their performances. The technique requires that the solver uses a SAT engine as an additional propagator. Each integer variable is also encoded with multiple Boolean variables, for the benefit of the SAT engine. For instance, an integer variable X can be encoded with boolean variables named X \u2265 1 , X \u2265 2 , etc. Furthermore, checker and filtering algorithms must explain the failures and the filtering they produced using SAT clauses. This allows the SAT engine to deduce nogoods, redundant SAT constraints that are added during the search to prevent the solver from making redundant unfruitful exploration. For instance, a checker algorithm that fails because the value of a variable X is too low could explain it with the clause \u00ac X \u2265 5 \u21d2 fail, where \u00ac X \u2265 5 is a literal negating the Boolean variable that encodes the fact that X is greater or equal to 5.\nTo allow the SAT engine to generate nogoods that can be reused more often, one produces the most general clause possible, with as few and as general literals as possible. For instance, the literal X \u2265 2 is more general than X \u2265 3 because the latter implies the former.\nIt is often challenging to generate general explanations for global constraints, since, by their very nature, global constraints work on several variables, often leading to complex explanations. This means that, unlike constraint programming without nogoods, more filtering is not necessarily better, even without considering the running time of the algorithms. A decomposition of a global constraint that uses binary constraints with small and general explanations could be better than a global constraint with poor explanations but better filtering. Without nogoods, one would need to balance the running time of an algorithm with its filtering strength. With lazy clause generation, one must also consider the quality of the explanations. One can use experiments to find out the best balance between these three factors.", "publication_ref": ["b3", "b12", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "De Clercq et al. (2010) introduced a constraint that extends the CUMULATIVE constraint with the following additional parameters and variables: \n\u2022 A sequence J = 0, J 1 , . . . , J k\u22121 , lct I forming consec- utive intervals such that interval i is defined as [J i\u22121 , J i ) \u2022 A sequence L of capacities such that L i \u2264 C is the ca- pacity in the i-th interval of J. \u2022 A sequence Cost of integer variable such that Cost i is the overcost in the i-th interval of J. \u2022 An integer variable Z", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "SoftCumulative", "text": "We define the SOFTCUMULATIVE constraint as follows.\nSOFTCUMULATIVE( S, p, h, C, Z, f ) \u21d0\u21d2 Z \u2265 t f max(0, i\u2208I:Si\u2264t<Si+pi h i \u2212 C (5)\nThe variable S i \u2208 [est i , lst i ] is the starting time of task i. The parameters p, h, and C represent respectively the processing times, the heights, and the capacity of the resource. The variable Z is the overcost variable representing the penalty incurred for overflowing the capacity of the resource. The function f (x) is a non-decreasing function returning the penalty for overflowing the capacity by x units at one time point. We use the terms penalty and cost interchangeably. We consider two cost functions: a linear cost f (x) = x and a quadratic cost f (x) = x 2 . A quadratic cost function is interesting in cases where spreading the overflow over multiple time points is preferable to having few time points with high overflow. For instance, if one unit of the resource represents one employee, it is often more costeffective to hire one additional employee for a few days than hire multiple additional employees for only one day. With a quadratic cost function and a capacity of 0, SOFTCUMULA-TIVE can be used to spread the tasks as equally as possible along the time line. Although we focus on the linear and quadratic cost functions, the algorithms we present can be easily adapted to use other non-decreasing cost functions.\nThe CUMULATIVE constraint is a specific case of the SOFTCUMULATIVE constraint where Z = 0. This means that it is also NP-Complete to decide whether SOFTCUMULATIVE has a solution. This also means that SOFTCUMULATIVE is at least as hard as CUMULATIVE and that filtering algorithms for SOFTCUMULATIVE cannot be faster than their equivalent for the CUMULATIVE.\nWe extend the energetic reasoning to support the cost function f (x). Let overcost(l, u) be the overcost of an interval [l, u). If the amount of energy available in that interval is greater than or equal to the energy consumed by the tasks, the overcost is zero. Otherwise, the overcost is computed as follows.\novercost(l, u) =f (\u03ba) \u2022 (u \u2212 l)+ (f (\u03ba + 1) \u2212 f (\u03ba)) \u2022 (S mod (u \u2212 l)) where S =MI(I, l, u) \u2212 C \u2022(u \u2212 l), \u03ba = S u \u2212 l (6)\nNote that with a linear cost function, the overcost is simply the amount of energy that overflows from the interval.\nC = 1 l u S mod (u \u2212 l) \u03ba = 1 \u03ba + 1 = 2 0 1 2 3 4 5 6 7 8 9 10\nFigure 2: Overcost computation Figure 2 presents an example of how the overcost in the interval [2, 9), with a resource of capacity C = 1, and a minimum intersection MI(I, 2, 9) = 17 is calculated. 7 units of energy available on the resource are represented with diagonal blue lines. 7 units of energy where the capacity of the resource is exceeded by 1 unit is represented with orange dotted points. That leaves 3 units of energy for which the capacity of the resource is exceeded by 2 units. These units span only a part of the interval. They are represented with red vertical lines. For a linear penalty f (x) = x, we obtain overcost(2, 9) = 10 while a quadratic penalty f (x) = x 2 gives overcost(2, 9) = 16.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Checker Algorithm", "text": "We propose a checker algorithm for SOFTCUMULATIVE based on the energetic reasoning. With the CUMULATIVE constraint, it is sufficient to find one interval with a minimum intersection greater than the available energy to detect an inconsistency. With SOFTCUMULATIVE, the checker algorithm instead needs to compute a lower bound on the overcost variable. If it is greater than the upper bound, there is a failure. The approach of the classic energetic checker of finding one interval where there is an overload can work for computing a lower bound. However, this approach considers only one interval at a time. Considering multiple intervals and combining their overcost lead to a better bound. Hence, a better solution is to partition the time line into disjoint intervals such that the sum of the overcost is maximized.\nWe propose to solve this problem using a graph G (see Figure 3). There is one node for each time point. For each pair of time points l and u such that l < u, there is an arc (l, u) with weight overcost(l, u). Each arc (l, u) corresponds to an semi-open interval [l, u) of positive length. The goal is to find the longest path between the first time point and the last. The sum of the weights of the arcs on that longest path gives us a lower bound Z on the overcost variable Z. Since the graph is acyclic, we can solve this problem in polynomial time using dynamic programming. There is C \u2022 (u \u2212 l) = 1 \u2022 2 = 2 units of energy available on the resource in [0, 2). Each task has a minimum intersection of 1 in that interval. This means there is 4 \u2212 2 = 2 units of overcost in the interval [0, 2), making it part of the the longest path (which is in bold).\nThis proposed rule adapts the energetic reasoning to the SOFTCUMULATIVE constraint and Algorithms 1 & 2 apply this rule.\n\u2203 P 0 , . . . , P k , P 0 = 0 \u2227 P k = lct I \u2227P t < P t+1 \u2227 t\u2208{0..k\u22121} overcost(P t , P t+1 ) > Z =\u21d2 fail (7)\nAlgorithm 1 takes as input the set of tasks, the capacity of the resource, and a set of critical time points T . By changing the number of critical time points, we can change the complexity of the algorithm, which is \u0398(|T | 2 ), but also the quality of the resulting lower bound. With more time points,\nAlgorithm 1: OvercostBound(I, C, T) \u03a6 \u2190 0; \u03c0 \u2190 [nil, . . . , nil]; for u = 2..| T | do for l = 1..u \u2212 1 do c \u2190 \u03a6[l] + overcost(T[l], T[u])); if \u03a6[u] < c then \u03a6[u] \u2190 c; \u03c0[u] \u2190 l; return \u03a6[|T |], \u03c0 ;\nAlgorithm 2: SoftEnergeticChecker(I, C, T, Z) \u03b1, \u03c0 \u2190 OvercostBound(I, C, T); if \u03b1 \u2264 Z then return pass else return fail; the algorithm is slower, but the lower bound is better. The OvercostBound algorithm returns the lower bound and the vector \u03c0, a parent vector containing the choices made by the dynamic programming. The vector \u03c0 is not relevant to the checker, but it is later used by the filtering algorithm.\nOnce the lower bound is computed, we can check whether it is greater than the upper bound Z of the overcost (Algorithm 2). If so, the algorithm returns a failure.\nLet T e be the set of O(n 2 ) time points in (4) used by the intervals of interest of Baptiste et al. (2001\n). Let T s = {est i | i \u2208 I} \u222a {ect i | i \u2208 I} \u222a {lst i | i \u2208 I} \u222a {lct i | i \u2208 I} be the set of 4n critical time points of each task.\nTheorem 1. If Z is set to 0 and T to T e , then SoftEnergeticChecker is equivalent to the energetic checker for the CUMULATIVE constraint.\nProof. With an upper bound on 0 for the overcost, SoftEnergeticChecker returns a failure if and only if the computed lower bound is 1 or more. There are two cases.\nIf the classic energetic checker passes, there does not exist an interval for which the capacity is exceeded. This means that the overcost of all intervals is 0. In that case, the lower bound found by our algorithm is 0 and the SoftEnergeticChecker passes.\nIf the classic energetic checker fails, there is at least one interval for which the capacity is exceeded (and for which the overcost is greater than 0) and that interval is one of the intervals of interest. While searching for the longest path, the SoftEnergeticChecker processes all possible intervals of positive length formed by the time points in T. Since T corresponds to the lower bounds and upper bounds of the interval of interest, all intervals of interest (and some intervals that are not of interest) are examined, including the one with a positive overcost. Hence, the longest path has a cost of at least 1 and the SoftEnergeticChecker fails.\nSince there are O(n 2 ) lower bounds and O(n 2 ) up-per bounds in T e , considering them all would lead to a SoftEnergeticChecker with a complexity of O(n 4 ), which is not reasonable for an algorithm that is executed thousands of times during the search. Instead, we propose to use the subset T s that is linear in size. This subset gave us good results while keeping the complexity of the algorithm reasonable. Furthermore, this subset is sufficient to apply two weaker rules, the Time-Tabling and the Edge-Finding, as shown in Theorem 2. Theorem 2. If Z = 0 and the set T = T s , then the SoftEnergeticChecker applies the Time-Tabling.\nProof. The Time-Tabling rule detects a failure if, at any time point, the sum of the compulsory parts of the tasks is greater than the capacity of the resource. Suppose that t is such a time point.\nLet lst i be the latest lst less than or equal to t and let ect j be the earliest ect greater than or equal to t. By definition, the total amount of energy consumed by the compulsory parts can only increase at time points that are lst and it can only decrease at time points that are ect. Thus, the amount of energy consumed by the compulsory parts at each time point in [lst i , ect j ) is the same as the amount at t. Since the amount consumed at t is sufficient to overload the resource, the total amount of energy in [lst i , ect j ) is sufficient to overload the interval.\nOur checker algorithm examines every interval formed from time points in T and all ect and lst are in that set. Thus, our checker raises a failure when examining [lst i , ect j ).", "publication_ref": ["b1"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Filtering Algorithm", "text": "We propose an algorithm based on the checker for filtering the earliest starting time of the tasks (see Algorithm 3). Filtering the latest completion time is symmetric.\nThe idea behind the algorithm is to schedule a task to its earliest and then run the checker algorithm to check if it is a valid solution. If it is not, we can filter the earliest starting time. We use two strategies to save costly calls to the checker. First, we use a constant time check to see whether it is possible that scheduling a task to its earliest makes the checker fail. Second, we use the longest path returned by the checker in case of a failure to filter the earliest starting time of the task as much as possible without recalling the checker. We do this by sliding the execution of the task from its earliest starting time up to a point where the cost of the path no longer causes a failure. Each time the task is shifted by one unit of time, the weight of at most four edges on the path needs to be updated, namely the edges that span the intervals that contain est i , est i +1, ect i , and ect i +1.\nThe algorithm starts by computing a lower bound Z on the overcost using the checker. Then, for each task i, it performs a check, at line 1, to verify whether the task needs to be filtered. The check changes slightly depending on the cost function. In the linear case, if the free energy of the task, which is the portion of the task that is not compulsory, is greater than the difference between the computed lower bound and the upper bound on the overcost, the task might need to be filtered. Otherwise, scheduling the task at its earliest cannot sufficiently increase the lower bound to cause a u] to the front of P ; return P ; failure and the task does not need to be filtered. Indeed, by fixing task i to its earliest, we are effectively only using its left-shift. Thus, the minimum intersection in some intervals can increase, but only by the free energy. The energy from the compulsory part is already included because, by definition, it is both in the left and right shift. This check gains in importance as the search reaches the bottom of the search tree. More tasks are fixed, fewer tasks has free energy, and fewer tasks need to be filtered. For a non-linear cost function, one additional unit of energy can increase the cost by more than one unit. Thus, the check only verifies whether the task has free energy.\nAlgorithm 3: SoftEnergeticFiltering(I, C, T, Z, f ) eMin \u2190 0; if f is a linear function then Z \u2190 OvercostBound(I, C, T); eMin \u2190 Z \u2212 Z; for i \u2208 I do 1 if h i \u2022 (p i \u2212 max(0, ect i \u2212 lst i )) > eMin then t \u2190 lct i ; lct i \u2190 est i +p i ; Z \u2190 OvercostBound(I, C, T); while Z > Z do est i \u2190 est i +1; lct i \u2190 lct i +1; 2 Z \u2190 OvercostBound(I, C, T); lct i \u2190 t; return {est i | i \u2208 I}; Algorithm 4: ComputeLongestPath(I, C, T, \u03c0) u \u2190 |T |; P \u2190 [T [u]]; while u > 1 do u \u2190 \u03c0[u]; Push T [\nIf the algorithm finds that a task needs to be filtered, it fixes the task to its earliest starting time and runs the checker. If there is a failure, the task cannot be executed at its earliest and needs to be filtered. To do so, the algorithm increment its est i by one. To continue the filtering process, lct i is also incremented. The process is repeated until the computed lower bound on the overcost does not exceed the upper bound.\nNote that even if the checker algorithm initially reports a success, it is possible for the filtering algorithm to remove all possible values for the starting time of a task, leading to a failure. This is because the energetic reasoning relaxation allows all tasks to be preempted. However, by fixing a task to its earliest, our filtering algorithm removes the possibility of preemption for the task being filtered.\nTo improve the efficiency of the algorithm, one can compute the longest path P using ComputeLongestPath, Algorithm 5: AdjustCost(I, i, P ) Create a task i with est i +1, lct i +1, p i , h i ; I \u2190 I \\ {i} \u222a {i }; D \u2190 {k | \u2203x \u2208 {est i , est i +1, ect i , ect i +1}, P k \u2264 x < P k+1 }; return k\u2208D overcost(I , P k , P k+1 ) \u2212 k\u2208D overcost(I, P k , P k+1 ); then replace line 2 by Z \u2190 Z + AdjustCost(I, i, P ). This allows filtering task i as much as possible on the longest path without having to re-run the checker. The lower bound Z is instead adjusted incrementally. Algorithm 5 works as follows. Since the increment is only by one, the minimum intersection can change in at most four intervals: the interval containing the current est i , the interval containing the current ect i and the intervals immediately following these two intervals if the est i or ect i is the upper bound on the interval. Computing the change in cost is done in constant time by keeping in cache the minimum intersection for each interval.\nWith this adjustment, the complexity of the algorithm is O(k\n\u2022 | T | 2 + r)\nwhere k \u2264 n is the number of tasks for which the free energy check passes and r is the total number of values removed from the domains. Even if r can be as high as the makespan, it is not necessarily bad since each value removed from the domain of a variable helps reduce the search space. We want to minimize the amount of time spent by the algorithm that leads to no filtering. Spending time if we are sure to filter is acceptable. If we use the set T s of 4n time points as we suggest for the checker algorithm, we have a complexity of O(k \u2022 n 2 + r). If we instead use the set T e , we have a complexity of O(k \u2022 n 4 + r).\nNote that it is also possible to increase the earliest starting time by more than one unit at a time, but such an adjustment depends on the cost function f (x). Theorem 3. If Z = 0 and the set T = T s the SoftEnergeticFiltering algorithm applies the EdgeFinding rule.\nProof. Suppose the EdgeFinding rule (2) finds a precedence stating that a set \u2126 must end before task i ends. The SoftEnergeticFiltering algorithm filters task i until the precedence is no longer detected.\nIndeed, if the EdgeFinding rule holds for i and \u2126, this means that there is not enough energy in [est \u2126\u222a{i} , lct \u2126 ) for both the energy of the tasks in \u2126 and the energy of i, given that i is left-shifted. In other words, when i is leftshifted, overcost(est \u2126\u222a{i} , lct \u2126 ) > 0. Since both the lower bound and the upper bound on that interval are est and lct time points, they are in T s . Thus, when filtering task i, the SoftEnergeticFiltering algorithm finds at least one path with positive overcost. Since Z = 0, task i is filtered by at least 1 unit. This reasoning holds as long as the precedence is detected by the EdgeFinding rule.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Explanations", "text": "We show how to generate explanations to allow our algorithms to be used with lazy clause generation solvers. The goal is to have explanations with as few and as general literals as possible. However, due to the global nature of SOFT-CUMULATIVE, failures or filtering is caused by the interaction between several tasks and the overcost variable. This means that our explanations will necessarily contain multiple variables and will not be as general as explanations from binary constraints.\nTo explain a failure, we use an explanation of the form conditions =\u21d2 fail. To explain filtering, we instead use the form conditions =\u21d2 S i \u2265 est i . The conditions are the same for failure and for filtering since it is caused, in both cases, by an excess in the energy of some tasks in the intervals along a path. For that reason, the rest of the section focuses on the conditions in the left side of the implication.\nIn any explanation for SOFTCUMULATIVE, we must include the literal Z \u2264 Z for the upper bound on the overcost variable.\nThere are several ways to explain the start variables of the tasks. A simple, naive explanation would include the literals S i \u2265 est i \u2227 S i \u2264 lst i for each task i. However, not all the tasks in the scope of SOFTCUMULATIVE contribute to the failure or filtering. In fact, only the tasks that have a positive minimum intersection in one of the intervals with positive overcost on the longest path contribute to the failure/filtering. This means we can exclude the other tasks from the explanation.\nIt is possible to generalize some literals. The minimum intersection of a task in an interval comes from the minimum between the left-shift and the right-shift. We can reduce the maximum between the left and the right shift up to the other value. For instance, if the left-shift of a task is 4 and its rightshift is 3, we can reduce the left-shift to 3 while still keeping our explanation valid. We can do that by increasing the est for the left-shift and reducing the lst for the right-shift, as shown in (8). Note that this rule may produce multiple literals with the same variable. It is possible to simplify the literals S i \u2265 a \u2227 S i \u2265 b to S i \u2265 max(a, b) to keep only the most restrictive literal.\nZ \u2264 Z \u2227 i\u2208I,1\u2264k<|P |: overcost(P k ,P k+1 )>0\u2227MI(i,P k ,P k+1 )>0 S i \u2265 P k+1 \u2212 x \u2227 S i \u2264 P k + x + p i where x = min(LS(i, l, u), RS(i, l, u)) h i (8)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "To ascertain the practical relevance of our SOFTCUMULA-TIVE checker and filtering algorithms, we used an adaptation to the Resource Constrained Project Scheduling Problem (RCPSP), which is a problem often used in the literature to benchmark the CUMULATIVE constraint.\nIn the RCPSP problem, the goal is to schedule n tasks on m machines of various capacities. Each task has a fixed processing time and is executed simultaneously on all machines. The amount of resource a task consumes varies by machine and can be zero. There are precedence constraints between tasks and the goal is to minimize the makespan.\nWith SOFTCUMULATIVE, we instead want to minimize the cost of overloading the resources. It is possible to adapt existing RCPSP instances (that do not allow overloads) by finding the makespan of the optimal solution and then either reducing the capacity of the resources or decreasing the makespan. With this approach, the makespan becomes a parameter of the instance and the goal is now to minimize the sum of overcost over all resources. We tried both approaches, but, with our benchmarks, reducing the capacity of the resources gave us more interesting instances. Reducing the makespan often produced unsatisfiable instances due to the precedence constraints. We used an adapted version of the KSD15 D benchmark (Kon\u00e9 et al. 2011). These instances are highly cumulative (more than one task can often execute at a time). We adapted it by decreasing the capacity of each resource by four units and fixing the makespan to the optimal value of the original instance. This gave us interesting instances, several of which can be solved in a reasonable time. These adapted instances are available in the supplementary material.\nWe implemented our algorithms in C++ and used the lazy clause generation solver Chuffed (Chu 2011). We used the modelling language Minizinc (Nethercote et al. 2007) to implement our models. Our experiments were run on an Intel Xeon 4110 CPU with a speed of 2.10Ghz. We ran all our experiments with a timeout of 20 minutes.\nIn our experiments, we compared our algorithms to the decomposition of SOFTCUMULATIVE (see Equation 5). We tested two configurations of our algorithms: the checker alone and the checker combined with the filtering algorithm. For both algorithms, we set T = T s . In our experiments, the checker and filtering combination always outperformed the checker alone, so we only report the former to save space. The interested reader can find the raw results in the annex, including the checker alone configuration. (De Clercq et al. 2010) kindly gave us access to their Java code that uses the solver Choco 2. We tried to solve our instances with their constraint. However, they only implemented the version of the constraint that computes the sum of the maximum overloads in each interval. To model our problem with this version, we need one interval per time point. Hence, the complexity depends on the makespan. This is not a good match for our instances with hundreds of time points. Even for easy instances, we were not able to find the optimal solution within a few hours with this approach. In addition, their constraint does not support a quadratic cost function, which is the most interesting application of our SOFTCUMULATIVE. Furthermore, naive explanations must be used with their algorithms since no method of generating explanations was proposed for these algorithms. Conversely, our constraint cannot be used to solve their instances since we do not support a maximum penalty function.", "publication_ref": ["b6", "b3", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Linear Cost Function", "text": "Figure 4 (left) compares the time taken by the decomposition and the filtering algorithm when a linear cost function f (x) = x is used. Almost all instances are solved much more quickly with the SOFTCUMULATIVE constraint and its filtering algorithm than by the decomposition. Furthermore, there are many instances for which the decomposition times out at 20 minutes (the points at 1200 seconds) while the filtering algorithm is able to solve them to optimality.\nThe decomposition is better on only 1% of the instances. Both configurations failed to solve 21.5% of the instances to optimality and the filtering algorithm is better on 77.5% of the instances. Furthermore, for nearly 68% of the instances, the filtering algorithm is more than 10 times faster.   The results are similar to the linear case. Neither algorithm proved optimality for 24.6% of the instances. The decomposition is better for 2.7% of the instances while the filtering algorithm is better on 72.7% of the instances. The filtering algorithm is 10 times faster for 60% of the instances. Hence, the filtering algorithm is the method of choice to employ with both linear and quadratic cost functions.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Quadratic Cost Function", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We proposed a checker and a filtering algorithm based on the energetic reasoning for the SOFTCUMULATIVE constraint. Unlike previous work, both algorithms support a quadratic cost function in addition to a linear function. They are parametrable in the sense that their strength and complexity vary depending of the number of time points passed as parameters. With the recommended set of time points T s , the checker has a complexity of O(n 2 ) and the filtering algorithm has a complexity of O(k \u2022 n 2 + r) where k is the number of tasks that pass the free energy check and r is the number of values pruned from the domain of the variables after the execution of the algorithm. We showed how to explain the algorithms to use them with lazy clause generation solvers and presented evidence that, in practice, our algorithms vastly outperformed the decomposition.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Extending CHIP in order to solve complex scheduling and placement problems", "journal": "Mathematical and computer modelling", "year": "1993", "authors": "A Aggoun; N Beldiceanu"}, {"ref_id": "b1", "title": "Constraint-Based Scheduling", "journal": "Kluwer Academic Publishers", "year": "2001", "authors": "P Baptiste; C Le Pape; W Nuijten"}, {"ref_id": "b2", "title": "A new multiresource cumulatives constraint with negative heights", "journal": "Springer", "year": "2002", "authors": "N Beldiceanu; M Carlsson"}, {"ref_id": "b3", "title": "Improving combinatorial optimization", "journal": "", "year": "2011", "authors": "G Chu"}, {"ref_id": "b4", "title": "", "journal": "", "year": "", "authors": "A De Clercq; T Petit; N Beldiceanu; N Jussien"}, {"ref_id": "b5", "title": "A soft constraint for cumulative problems with overloads of resource", "journal": "", "year": "", "authors": ""}, {"ref_id": "b6", "title": "Event-based MILP models for resource-constrained project scheduling problems", "journal": "Computers & Operations Research", "year": "2011", "authors": "O Kon\u00e9; C Artigues; P Lopez; M Mongeau"}, {"ref_id": "b7", "title": "Consistency enforcing in scheduling: A general formulation based on energetic reasoning", "journal": "", "year": "1996", "authors": "P Lopez; P Esquirol"}, {"ref_id": "b8", "title": "Edge finding for cumulative scheduling", "journal": "INFORMS Journal on Computing", "year": "2008", "authors": "L Mercier; P Van Hentenryck"}, {"ref_id": "b9", "title": "MiniZinc: Towards a standard CP modelling language", "journal": "Springer", "year": "2007", "authors": "N Nethercote; P J Stuckey; R Becket; S Brand; G J Duck; G Tack"}, {"ref_id": "b10", "title": "Propagation via lazy clause generation", "journal": "Constraints", "year": "2009", "authors": "O Ohrimenko; P J Stuckey; M Codish"}, {"ref_id": "b11", "title": "A O(n 2 log n) Checker and O(n 2 log n) Filtering Algorithm for the Energetic Reasoning", "journal": "Springer", "year": "2018", "authors": "Y Ouellet; C.-G Quimper"}, {"ref_id": "b12", "title": "", "journal": "", "year": "2019", "authors": "L Perron; V Furnon"}, {"ref_id": "b13", "title": "Improving energetic propagations for cumulative scheduling", "journal": "Springer", "year": "2018", "authors": "A Tesch"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "representing the global penalty of the constraint. \u2022 A function costFunction \u2208 {max, sum} indicating if the Cost variables should be the maximum or the sum of the overcost in the intervals. \u2022 A function penaltyFunction \u2208 {max, sum} indicating if the penalty variable Z should be the maximum between or the sum of the Cost variables. The authors proposed both a O((n + |J|) \u2022 log(n + |J|)) TimeTabling and an O(n \u2022 |J| \u2022 k \u2022 log(n)) EdgeFinding algorithm, where k \u2264 n is the number of distinct heights among the tasks. They do not generate explanations of the filtering.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure3: Example of a graph representing the overcost for a SOFTCUMULATIVE with a capacity of C = 1 with 4 tasks of the form 0, 2, 1, 1 . There is C \u2022 (u \u2212 l) = 1 \u2022 2 = 2 units of energy available on the resource in [0, 2). Each task has a minimum intersection of 1 in that interval. This means there is 4 \u2212 2 = 2 units of overcost in the interval [0, 2), making it part of the the longest path (which is in bold).", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure4: Time in seconds to solve the instances of the adjusted KSD15 D benchmark to optimality with a linear (left) and quadratic (right) cost function for the decomposition and the filtering algorithm. A time of 1200s means that the solver times out without proving the optimal solution.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 44Figure4compares the time taken to solve the instances of the KSD15 D benchmark with the quadratic cost function. The results are similar to the linear case. Neither algorithm proved optimality for 24.6% of the instances. The decomposition is better for 2.7% of the instances while the filtering algorithm is better on 72.7% of the instances. The filtering algorithm is 10 times faster for 60% of the instances. Hence, the filtering algorithm is the method of choice to employ with both linear and quadratic cost functions.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u2203t i\u2208I : lsti\u2264t<ecti h i > C =\u21d2 fail (1)", "formula_coordinates": [2.0, 94.73, 140.05, 197.77, 20.14]}, {"formula_id": "formula_1", "formula_text": "\u2200i \u2208 I, \u2126 \u2286 I \\ {i} e \u2126\u222a{i} > C \u2022 (lct \u2126 \u2212 est \u2126\u222a{i} ) =\u21d2 i ends after \u2126 (2)", "formula_coordinates": [2.0, 61.75, 252.44, 230.75, 23.9]}, {"formula_id": "formula_2", "formula_text": "RS(i, l, u) = h i \u2022 (min(u, lct i ) \u2212 max(l, lst i )) of task i in interval [l, u", "formula_coordinates": [2.0, 54.0, 371.58, 238.5, 19.92]}, {"formula_id": "formula_3", "formula_text": "l u LS(1, 2, 9) = 4 RS(1, 2, 9) = 6 MI(1, 2, 9) = min(4, 6) = 4 ect1 lst1 0 1 2 3 4 5 6 7 8 9 10 11 12", "formula_coordinates": [2.0, 57.32, 473.01, 209.33, 109.36]}, {"formula_id": "formula_4", "formula_text": "T e ={[l, u) | l \u2208 O 1 , u \u2208 O 2 } \u222a {[l, u) | l \u2208 O 1 , u \u2208 O(l)} \u222a {[l, u) | u \u2208 O 2 , l \u2208 O(u)} where O 1 = {est i | i \u2208 I} \u222a {ect i | i \u2208 I} \u222a {lst i | i \u2208 I} O 2 = {lst i | i \u2208 I} \u222a {ect i | i \u2208 I} \u222a {lct i | i \u2208 I} O(t) = {est i + lct i \u2212t}(4)", "formula_coordinates": [2.0, 319.5, 268.7, 238.89, 104.6]}, {"formula_id": "formula_5", "formula_text": "\u2022 A sequence J = 0, J 1 , . . . , J k\u22121 , lct I forming consec- utive intervals such that interval i is defined as [J i\u22121 , J i ) \u2022 A sequence L of capacities such that L i \u2264 C is the ca- pacity in the i-th interval of J. \u2022 A sequence Cost of integer variable such that Cost i is the overcost in the i-th interval of J. \u2022 An integer variable Z", "formula_coordinates": [3.0, 56.99, 295.75, 235.52, 83.09]}, {"formula_id": "formula_6", "formula_text": "SOFTCUMULATIVE( S, p, h, C, Z, f ) \u21d0\u21d2 Z \u2265 t f max(0, i\u2208I:Si\u2264t<Si+pi h i \u2212 C (5)", "formula_coordinates": [3.0, 85.01, 557.35, 207.49, 37.81]}, {"formula_id": "formula_7", "formula_text": "overcost(l, u) =f (\u03ba) \u2022 (u \u2212 l)+ (f (\u03ba + 1) \u2212 f (\u03ba)) \u2022 (S mod (u \u2212 l)) where S =MI(I, l, u) \u2212 C \u2022(u \u2212 l), \u03ba = S u \u2212 l (6)", "formula_coordinates": [3.0, 323.55, 347.25, 234.45, 76.7]}, {"formula_id": "formula_8", "formula_text": "C = 1 l u S mod (u \u2212 l) \u03ba = 1 \u03ba + 1 = 2 0 1 2 3 4 5 6 7 8 9 10", "formula_coordinates": [3.0, 322.82, 469.38, 203.62, 112.14]}, {"formula_id": "formula_9", "formula_text": "\u2203 P 0 , . . . , P k , P 0 = 0 \u2227 P k = lct I \u2227P t < P t+1 \u2227 t\u2208{0..k\u22121} overcost(P t , P t+1 ) > Z =\u21d2 fail (7)", "formula_coordinates": [4.0, 65.0, 603.38, 227.5, 38.26]}, {"formula_id": "formula_10", "formula_text": "Algorithm 1: OvercostBound(I, C, T) \u03a6 \u2190 0; \u03c0 \u2190 [nil, . . . , nil]; for u = 2..| T | do for l = 1..u \u2212 1 do c \u2190 \u03a6[l] + overcost(T[l], T[u])); if \u03a6[u] < c then \u03a6[u] \u2190 c; \u03c0[u] \u2190 l; return \u03a6[|T |], \u03c0 ;", "formula_coordinates": [4.0, 324.48, 59.77, 184.5, 124.25]}, {"formula_id": "formula_11", "formula_text": "). Let T s = {est i | i \u2208 I} \u222a {ect i | i \u2208 I} \u222a {lst i | i \u2208 I} \u222a {lct i | i \u2208 I} be the set of 4n critical time points of each task.", "formula_coordinates": [4.0, 319.5, 375.47, 238.5, 30.87]}, {"formula_id": "formula_12", "formula_text": "Algorithm 3: SoftEnergeticFiltering(I, C, T, Z, f ) eMin \u2190 0; if f is a linear function then Z \u2190 OvercostBound(I, C, T); eMin \u2190 Z \u2212 Z; for i \u2208 I do 1 if h i \u2022 (p i \u2212 max(0, ect i \u2212 lst i )) > eMin then t \u2190 lct i ; lct i \u2190 est i +p i ; Z \u2190 OvercostBound(I, C, T); while Z > Z do est i \u2190 est i +1; lct i \u2190 lct i +1; 2 Z \u2190 OvercostBound(I, C, T); lct i \u2190 t; return {est i | i \u2208 I}; Algorithm 4: ComputeLongestPath(I, C, T, \u03c0) u \u2190 |T |; P \u2190 [T [u]]; while u > 1 do u \u2190 \u03c0[u]; Push T [", "formula_coordinates": [5.0, 321.39, 61.7, 214.2, 292.11]}, {"formula_id": "formula_13", "formula_text": "\u2022 | T | 2 + r)", "formula_coordinates": [6.0, 74.2, 316.81, 48.53, 11.23]}, {"formula_id": "formula_14", "formula_text": "Z \u2264 Z \u2227 i\u2208I,1\u2264k<|P |: overcost(P k ,P k+1 )>0\u2227MI(i,P k ,P k+1 )>0 S i \u2265 P k+1 \u2212 x \u2227 S i \u2264 P k + x + p i where x = min(LS(i, l, u), RS(i, l, u)) h i (8)", "formula_coordinates": [6.0, 318.72, 536.36, 239.28, 67.39]}], "doi": ""}