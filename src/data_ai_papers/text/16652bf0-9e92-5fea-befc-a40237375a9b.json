{"title": "Using natural language and program abstractions to instill human inductive biases in machines", "authors": "Sreejan Kumar; Carlos G Correa; Ishita Dasgupta; Michael Y Hu; Robert D Hawkins; Nathaniel D Daw; Jonathan D Cohen; Karthik Narasimhan; Thomas L Griffiths", "pub_date": "2023-02-05", "abstract": "Strong inductive biases give humans the ability to quickly learn to perform a variety of tasks. Although meta-learning is a method to endow neural networks with useful inductive biases, agents trained by meta-learning may sometimes acquire very different strategies from humans. We show that co-training these agents on predicting representations from natural language task descriptions and programs induced to generate such tasks guides them toward more humanlike inductive biases. Human-generated language descriptions and program induction models that add new learned primitives both contain abstract concepts that can compress description length. Co-training on these representations result in more human-like behavior in downstream meta-reinforcement learning agents than less abstract controls (synthetic language descriptions, program induction without learned primitives), suggesting that the abstraction supported by these representations is key. 36th Conference on Neural Information Processing Systems (NeurIPS 2022).", "sections": [{"heading": "Introduction", "text": "Humans are able to rapidly perform a variety of tasks without extensive experience [1]. This may be because of strong inductive biases towards abstract structured knowledge (e.g. hierarchies, compositionality) [2,3] that act as strong prior knowledge, enabling generalization to novel environments with little new data. These biases present one of the most salient differences between humans and neural network-based learners [4] and may be one of the keys to building artificial agents with human-like generalization capabilities. For this reason, there has been interest among machine learning researchers in identifying and instilling these inductive biases into neural networkbased agents [5][6][7].\nOne emerging approach to implicitly bestowing inductive biases on neural networks is metalearning [8,9]. In meta-learning paradigms, an agent is trained not just on a single task but on a distribution of tasks, with the aim of acquiring the underlying abstractions that these tasks have in common. However, since neural networks are not easily interpretable, it can be difficult to tell if the resulting neural networks actually acquired this abstract knowledge, or whether they have instead learned statistical artifacts correlated with abstract rules [10].\nWhat representations can give our artificial agents access to human inductive biases? Previous work in reinforcement learning has shown that neural networks can improve performance through auxiliary tasks [11][12][13]. We build on this insight to construct auxiliary tasks that require reproduc-+ + synthetic language programs with library learning programs with primitives target board Figure 1: Four repositories of human-like priors. For a stimulus space of 2D binary grids, we investigate two classes of repositories for human-like priors: linguistic descriptions (top row) and program abstractions (bottom row). For linguistic descriptions, we consider both synthetic language (upper-left), and human-generated language (upper-right). For program abstractions, we implemented a simple DSL consisting of an agent that can move around on the grid while filling-in red tiles. We consider both primitive programs written in the base DSL (bottom-left) as well as programs written with a learned library of abstractions (bottom-right).\ning different kinds of representations, and examine what kinds of representations tend to produce agents with human-like inductive biases.\nHuman biases toward abstract knowledge might be linked to the ability to verbalize this knowledge through natural language [14,15]. Human language descriptions can therefore act as a repository of this prior knowledge. Recent work in machine learning has shown that neural network representations can be shaped and structured through natural language supervision for various tasks [16][17][18][19][20][21][22]. Abstract knowledge in humans has also been modeled by program induction [23][24][25][26], where a model directly infers a structured programmatic representation from data. These programmatic representations are constructed from primitive atoms using symbolic rules and procedures. Recent work has leveraged neural networks to make this otherwise expensive and brittle process more scalable [neurosymbolic models; [27][28][29]. In these approaches, abstract knowledge is explicitly built into the artificial agent using a pre-specified Domain Specific Language (DSL), which can be restrictive. In many situations, we would like artificial agents that are implicitly guided towards human-like abstract knowledge [30][31][32][33] rather than explicitly building it in, since it can be hard to determine what specific concepts to build into a DSL for any arbitrary environment. Therefore, in this work, we focus on how to use representations from program induction to guide artificial agents that don't have these built in concepts.\nIn this work, we show that language and programs can be used as repositories of abstract prior knowledge/inductive biases of humans that one can co-train with to elicit human-like biases in other related tasks. The differences in behavior elicited by the use of different inductive biases are subtle, so we use a recent framework from Kumar et al. [10] to build tasks that distinguish these subtle differences. The specific domain we use is a meta-reinforcement learning task where the agent sequentially reveals patterns on 2D binary grids. We find that guiding meta-reinforcement learning agents with representations from language and programs not only increases performance on task distributions humans are adept at, but also decreases performance on control task distributions where humans perform poorly. We also show a correspondence between human-generated language and programs synthesized with library learning. This indicates that humans use higherorder concepts to compress their descriptions in the same way that recent library learning approaches in program induction add new concepts to the DSL to compress program length [24]. Finally, we show evidence that this process of abstraction through compression in these representations (either language or program) is a key driver of human-like behavior in the downstream meta-learning agent, when co-trained on these representations. elicited using Gibbs Sampling with People (GSP) control distribution elicited using neural network with matched statistics human distribution Figure 2: Example grids from human-elicited priors (left) and machine-generated priors (right).", "publication_ref": ["b0", "b1", "b2", "b3", "b4", "b5", "b6", "b7", "b8", "b9", "b10", "b11", "b12", "b13", "b14", "b15", "b16", "b17", "b18", "b19", "b20", "b21", "b22", "b23", "b24", "b25", "b26", "b27", "b28", "b29", "b30", "b31", "b32", "b9", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Dataset and task paradigm", "text": "Our goal is to examine differences in how human-like the behavior of an agent is, as elicited by different co-training objectives. To do this, we first need a distribution that reflects human inductive bias (as well as a control which is closely matched but doesn't directly reflect this inductive bias) -so that human-like behavior is distinguishable. We introduce these distributions in the following sections, as well as how we build an RL task based on these distributions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Eliciting human priors using Gibbs sampling", "text": "We collected a dataset where we directly sample from humans' prior expectations on twodimensional binary grids (we will refer to these as \"boards\"). These data contain the direct human prior knowledge/inductive biases and can therefore be used to study them. To do this, we used a technique called Gibbs Sampling with People [GSP; 34, see Fig. 2] that samples internal prior distributions by putting humans \"in the loop\" of a Gibbs sampler. The stimulus space consisted of the space of 4 \u00d7 4 boards giving 16 stimulus dimensions. Each stimulus dimension (corresponding to a tile on the board) had two possible values and determined the binary color of the tile, namely, red or white. Each GSP trial consisted of a human participant predicting what color a single masked square in the grid should be, conditional on the colors of all other squares on the grid. The specific instructions given were \"What should be the underlying color of the covered greyed tile such that the board is generated by a very simple rule?\" (Fig. 2). Once a decision was made the resulting stimulus was passed on to a new participant, who repeated the task with a different masked tile. When sampling from this distribution, the probability of each board is based on how frequently it occurred during the GSP sampling process, which reflects its probability under human priors over the space of 2D grids. We selected the 500 most probable boards to collect language and program data. We will refer to these as the \"GSP boards\".", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Constructing a matched control distribution", "text": "The control distribution of boards matches the statistics of the GSP boards but is not produced by human decisions (see Fig. 2). A fully connected neural network (3 layers, 16 units each) was trained to encode the conditional distributions of the GSP boards: a random tile is masked out, and the network is trained to predict its value given the other tiles (similar to masked language models; [35]). These conditional distributions contain all the relevant statistical information about the boards. The network achieved an accuracy of above 99% on this task. This network is then used to generate samples, using the same process that was used to generate samples from human priors via Gibbs sampling. A board in which each tile is randomly set to red or white with probability 0.5 is initialized, and this trained network is used to predict masked out tiles. Since the conditional model is trained on the GSP boards, this generates a set of boards with similar statistical properties to the original GSP boards (example: number of red tiles in the GSP boards (Mean=8.4, SD=2.26) do not significantly differ from the control boards (Mean=7.4, SD=2.01), p = 0.12), but also implicitly encode the priors of the conditional model (i.e. a trained neural network instead of humans) as well. The goal of the agent is to sequentially uncover all the red tiles to reveal a picture on a 2D grid while uncovering as few white tiles as possible. A single board is one task and a distribution over boards is a task distribution. (B). Performance of humans and artificial agents on the human-elicited vs machine-generated samples (see Fig 2). The performance metric is based on the number of white tiles revealed, so lower is better.", "publication_ref": ["b34"], "figure_ref": [], "table_ref": []}, {"heading": "A search task for meta-reinforcement learning", "text": "Task. These distributions of boards can be used to construct new reinforcement learning tasks. We use the same task, performance measures, and agent architecture as in Kumar et al. [10]. In this task, the agent starts with an almost fully masked out board (all greyed tiles except for one revealed red), and has to select tiles to reveal. The revealed tiles are red or white depending on the tile colors of a preselected underlying board. One board with a fixed configuration of red tiles defines a single task. The agent's goal is to reveal all the red tiles, while revealing as few white tiles as possible. The episode ends when the agent reveals all the red tiles. There is a reward for each red tile revealed, and a penalty for each white tile revealed. Agents are trained on a set of boards and tested on held out test boards. Doing well on this task requires the agent to learn and represent the distribution over boards in its training distribution. The primary manipulation is whether a board is generated from human priors or from the control distribution (that is closely matched in terms of statistics, but generated from machine priors). Learners that do better on boards that were directly sampled from people's prior distribution on 2D grid stimuli can be said to have a human-like inductive bias, and vice versa for machine-generated boards (see Fig. 2).\nEvaluation metric. The specific metric we use to track performance is as follows. An agent that does well on the task will reveal all the red tiles while revealing as few white tiles as possible.\nWe therefore measure performance by counting the number of white tiles revealed in the episode.\nSince different boards will have different number of red tiles and thus have varying levels of difficulty, we measure the performance relative to a \"nearest neighbor\" heuristic. This heuristic randomly selects covered tiles adjacent to currently uncovered red tiles. We run this heuristic 1000 times on each board and the human/agent's number of white tiles revealed is z-scored according to this distribution. A z-score of below 0 means that the human/agent did better than the mean performance of the nearest neighbor heuristic. The specific value of the z-score reflects how many standard deviations the human/agent did better than the mean performance of the nearest neighbor heuristic. We determine significant differences in performance across different task distributions and models using a non-parametric bootstrap independent samples statistical test.\nBaseline experiments. The agent architecture is an LSTM meta-learner trained with reinforcement learning. This architecture is inspired by Wang et al. [36]. Our LSTM meta-learner takes the full board x t as input, passes it through convolutional and fully connected layers and feeds that, along with the previous action a t\u22121 and reward r t\u22121 , to 120 LSTM cells. The agent had 16 possible actions corresponding to choosing a tile (on the 4 \u00d7 4 board) to reveal. The reward function was: +1 for revealing red tiles, -1 for white tiles, +5 for the last red tile, and -2 for choosing an already revealed tile. The agent was trained using Proximal Policy Optimization [PPO; 37] using the Stable Baselines package [38] for one million episodes.\nWe trained our agent on the distribution of the human-generated GSP samples (see Fig 2). We then evaluated the agent on a set of human-generated samples as well as machine-generated samples, both of which were held out of training. Despite the fact that we did not train the agent on the machine-generated samples, the agent still performs significantly better on the control samples than the human samples (p < 0.0001) (Fig. 3B), which is in stark contrast to humans who perform significantly better on the human-generated samples (p < 0.0001). The fact that the meta-RL agent exhibits the opposite pattern of performance across human and machine-generated tasks (compared to humans) suggests that the agent has learned a different representation than that used by the humans, reflecting a different set of inductive biases than that of the humans. This is a replication of the effect reported in Kumar et al. [10] -using the same architectures and task paradigm, but here the agent is trained with PPO instead of A2C. Having established that humans and agents seem to use different representations to perform these tasks -presumably driven by differences in inductive bias -we now consider how to provide the agent with more human-like inductive bias(es).\n3 Instilling human inductive biases with language and programs", "publication_ref": ["b9", "b35", "b37", "b9"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Natural language representations", "text": "We collected natural-language descriptions of 500 of the most probable GSP boards from a naive group of participants. Each participant wrote descriptions for 25 unique boards in response to the following prompt: \"Your goal is to describe this pattern of red squares in words. Be as detailed as possible. Someone should be able to reproduce the entire board given your description. You may be rewarded based on how detailed your description is.\" Participants were randomly assigned boards such that each board had 9-12 descriptions from different participants. As a control for the language data, we also generated synthetic text descriptions. Synthetic descriptions used a template that said \"The reds are in: row X 1 and column Y 1 ,...,\" for every red tile location (X n , Y n ). We permuted the order in which the red tile locations were verbalized to generate multiple descriptions per board and mimic the multiple human descriptions per board for the human-generated descriptions. To embed each description (both human-generated and synthetic) into a vector space with semantic meaning, we obtained the RoBERTa [39] sentence embedding for each description using the SentenceTransformer package (https://www.sbert.net/, based on Reimers and Gurevych [40]). The SentenceTransformers model maps text into a 768 dimensional dense semantically meaningful vector space (i.e. text samples that are semantically similar are close to each other in vector space). Reimers and Gurevych [40] do this by using a contrastive objective on a dataset of semantically-paired text where embeddings from the same pair are pushed closer and embeddings from different pairs are pushed further apart.", "publication_ref": ["b38", "b39", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "Program representations", "text": "We used DreamCoder [24] to parse the boards from the GSP experiment into programs that can generate them. Given a domain-specific language (DSL) and a way to score a program comprised of DSL functions, DreamCoder enumerates and scores programs during its wake phase. This process is accelerated in two ways during the sleep phase: first, by learning a neural-networkbased recognition model that guides searching the space of programs during wake, and second by augmenting the DSL with subprograms abstracted from repeated expressions in existing programs using library learning. DreamCoder alternates between wake and sleep phases for multiple cycles.\nWe now describe our domain and DSL. In this domain, the goal is to produce the target board on a 4 \u00d7 4 grid by controlling a pointer that marks locations with a pen (see Fig. 1). The pointer state consists of a location on the grid, an orientation in one of the four cardinal directions, and whether the pointer's pen is down (i.e. whether it will mark at subsequent future locations). The DSL is comprised of the following primitives: move which moves the pointer forward from the current location based on orientation; left, right which turn the pointer, changing its orientation; pen-up, pen-down which change the state of the pointer's pen; and fork which stores the pointer state, executes a program, and then restores the agent state (except that grid locations marked in the subroutine are not reset).\nThe score s(\u03c1|l) of a program \u03c1 when executed from an initial location l is the sum of several penalties that encourage the program to replicate the target board faithfully in an efficient manner. First, there is a penalty of \u221210 for each marked location that is in the target board and not in the generated program's board, and a penalty of \u2212\u221e for each location that is in the generated pro-  Example board with corresponding human-generated language descriptions. Some people write more abstract descriptions than others. (B) Examples showcasing compression of programs with learned library concepts. We show the library-learning program (top grids) and non-library learning programs (bottom grids). For each program, we show the execution path (right, star indicates start of path) and the program stack (left). Black indicates main stack whereas blue indicates use of a learned library function. Nonlibrary learning programs do not use learned library functions and thus function only on the base stack. (C) Human-generated language descriptions typically have lower description length than the synthetic descriptions. Analogously, programs with library learning are on average shorter than programs with just the base DSL. (D). Boards that elicit shorter more abstract descriptions from humans also elicit shorter programs, indicating a correspondence between the abstractions human use in language and those from with library learning. gram's board and not in the target board. 1 The program is therefore strongly encouraged to mark all the tiles that are marked in the target, and strongly discouraged from marking additional tiles that are not in the target. Second, there are penalties that encourage the program to replicate the target board efficiently. There is a penalty of \u22121 for each instance of move, left, and right when the pen was down, encouraging programs to draw boards in few steps. The score of a program is set as the best score achieved by the program starting from any initial location max l s(\u03c1|l).\nIn the first sleep phase, DreamCoder trains a neural network that is used to guide program searchthis recognition model does so by taking the board as input and predicting programs that are likely to solve that board based on previous successful solutions. The recognition model consists of a 16-channel 3 \u00d7 3 convolutional layer, followed by two fully-connected layers with 64 units each, and ending with a linear layer. To generate a vector-based representation of the boards that contains the information necessary for program induction, we use the final hidden layer of the recognition model as the representation of the board. This embeds each board into a dense vector, similar to the natural language embedding of each board.\nIn the library learning sleep phase, DreamCoder grows the DSL by adding library functions based on reused components of successful programs found during wake (see Fig. 4B for an example). Specifically, it adds program components that minimize the description length of both the current program library and each successful program, once rewritten to use this new library component. These learned library abstractions serve to compress programs found in previous wake cycles and better enable the search for future programs in future wake cycles. We execute DreamCoder with and without library learning to test the usefulness of program representations with abstractions.", "publication_ref": ["b23", "b0"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Comparing programs and language descriptions", "text": "We first consider the descriptions we obtained from humans. We found a wide range in the level of abstraction in human natural language descriptions. Some are more literal and verbose in their descriptions whereas others use more abstract concepts to compress their descriptions (Fig. 4A). On average, human-generated descriptions were shorter than synthetically-generated descriptions, which listed every red tile location (Fig. 4C). By analogy, library-learned concepts allow for compression of programs (Fig. 4B). Both human-generated language (vs synthetic) and librarylearned programs (vs only primitives) have the same effect on enabling shorter description lengths (Fig. 4C).\nWe further found that the description lengths in human language and library-learned programs are significantly correlated (Fig. 4D) across boards. In other words, a board that is more compressible using human language abstractions was also more compressible with library learning over programs. This indicates that the abstractions used by humans in language overlap with the abstractions learned by Dreamcoder during library learning. This correlation is not significant when comparing human generated language description length with description lengths of programs without library learning (Fig. 4D). This indicates that the correlation may be due to the abstractions captured through library learning. An illustrative example can be seen in Fig. 1, where the program's used library function and the phrase \"U shape\" within in the human language description are actually the same concept. We verify this further by testing for the difference between these two correlations using a non-parametric permutation test. The correlation between human-generated language description length and library learning program description length is statistically significantly higher (p = 0.0034) than the correlation between human-generated description length and non-library learning program description length.\nNot only do the representations between language and programs have correlated description lengths, but the actual representations learned can be similar as well. We consider a sample of three relatively simple boards in Fig. 5 and conduct a representational similarity analysis. We find that the similarity structure of these boards overlap across modalities -i.e they are similar between human-generated language and library-learned programs, as well as between synthetic language and primitive programs. Conversely, these matrices don't look very similar within modality. That is, the matrices for abstract human-generated language don't look very similar to the matrices for non-abstract synthetic language, and similarly for programs. This indicates that modality is not the key driver of similarity in representation space; rather, the level of abstraction can be more salient. This also indicates that the process of abstraction changes the representation in similar ways across language and programs.   Gap between performance on human-generated and machine-generated tasks. Higher gap indicates more human-like behavior of the model.", "publication_ref": [], "figure_ref": ["fig_1", "fig_1", "fig_1", "fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "Representational Similarity (Correlation)", "text": "Language Program More Abstract Less Abstract Human Generated Language", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Grounding agents in language and program representations", "text": "In order to guide the agent to learn a human-like inductive bias, we introduce a task grounding term to the loss function: loss = L P P O (\u03b8) + c task L task (\u03c8 \u03b8 , \u03c8). Here L P P O (\u03b8) is the original PPO loss function, \u03b8 is the current model's parameters, c task is a hyperparameter coefficient that weights the task embedding loss L task (see Fig. 6A for the types of task embeddings we used), \u03c8 is the task embedding, and\u03c8 \u03b8 is the agent's prediction of the task embedding. The task embedding loss L task is the Mean Squared Error (MSE) between the agent's predicted and actual task embedding. This forces the agent to simultaneously predict an embedding of the hidden task state (i.e. the underlying board) as well as come up with the best possible action for the current observation (see Fig. 6A).\nLike the Baseline RL agent in Fig. 3A, we trained our learners on a training set from the humangenerated boards and evaluated its performance on a held-out test set of human-generated and machine-generated boards. We trained different agents to predict different kinds of task embeddings during training (Fig. 6A) based on language (both synthetic and human-generated language) and programs (both no library learning and library learning). We refer to co-training on predicting these auxiliary representations as \"grounding\" the agent on the corresponding representation. The term \"grounding\" is often used in work that trains agents to associate data in one modality (i.e. text instructions) to another (i.e. visual images) [41][42][43][44][45][46][47]. We also co-trained on directly predicting a flattened version of the underlying board stimulus, which is equivalent to co-training the agent on an autoencoder loss.\nWe set out to test whether grounding meta-learning agents on these task embeddings will result in them producing more human-like performance. We know humans perform better on GSP boards than control boards (Fig. 3B), while the generic meta-RL agent does the opposite. Performing better on the GSP boards and worse on the control boards would therefore indicate more humanlike behavior. To compare task performance across different task distributions or different models, we use a non-parametric bootstrap independent samples test to test for significance.\nWe see that grounding with human-generated descriptions leads to a human-like inductive bias (Fig. 6B). Training on this auxillary loss led to an increase in performance at the GSP boards relative to the baseline agent (p < 0.0001). The human-language grounded agent also performed significantly better at the GSP boards than the control boards (p < 0.0001), just like humans do. In contrast, this auxiliary loss significantly reduced performance on the machine-generated control boards (as compared to the baseline agent, p = 0.021). This indicates that the auxiliary loss did not generically improve performance across the board, but rather improves performance selectively in the human-generated boards, and worsens it in the machine generated boards. In other words, it makes the agent more human-like. The human-language grounded agent also does the best at the human-generated tasks across all models.\nThe autoencoder agent is substantially better on the GSP boards than the original agent (p < 0.001). However, its performance does not differ significantly between the human and machinegenerated tasks (p = 0.252), a performance pattern different from that of humans. Similarly, we also find that grounding on synthetic descriptions does not lead to human-like bias and instead has an effect similar to that of the autoencoder loss -it improves performance uniformly rather than selectively. In fact, the agent using synthetic text does better in the control distribution than the GSP distribution (p = 0.0016), which is qualitatively similar to the performance pattern of the original agent (Fig. 3B), and distinctly dissimilar from human behavior.\nWe now move to examining the agents co-trained with program representations. We found that grounding on library learning programs leads to a human-like inductive bias (Fig. 6B). Specifically, although grounding on non-library program task embeddings does numerically better on human-generated tasks than machine-generated tasks, this difference is not significant (p = 0.11). This difference, however, is highly significant when grounding on library-learning program task embeddings (p < 0.0001). In addition, grounding on library learning program task embeddings does significantly better at the GSP tasks than grounding on non-library learning program embeddings (p < 0.0001). Just as grounding on human language produces more human-like inductive bias than synthetic language, grounding on library learning programs produces more human-like inductive bias than grounding on programs without library learning.\nThis points to evidence that the level of abstraction of the task embeddings influences the extent to which the agent acquires a human inductive bias, as indicated by the differences in performance among low (synthetic for language, no library learning for programs) and high-level (human-generated language and library learning programs) agents. In human-generated descriptions, humans write about abstract concepts (e.g. lines, shapes, letters, etc). Similarly abstract concepts can be learned as library functions (see Fig. 4B). The process of abstraction in language and programs allows for compression of the respective description and induced program (Fig. see  4C). The embeddings of these representations that were compressed through abstract concepts may therefore be distilling useful concepts into our meta-learning agent that enables it to acquire a human-like inductive bias.", "publication_ref": ["b40", "b41", "b42", "b43", "b44", "b45", "b46"], "figure_ref": ["fig_3", "fig_3", "fig_0", "fig_3", "fig_0", "fig_3", "fig_0", "fig_3", "fig_1", "fig_1"], "table_ref": []}, {"heading": "Discussion", "text": "In this work, we argue that language descriptions and program abstractions can act as repositories for human inductive biases that may be distilled into artificial neural networks. To test this idea, we considered a tile-revealing task with 2D grids that were directly sampled from human priors. We found that grounding on either human-generated language or programs with library learning not only improves machine performance on tasks that people perform well at, but also impairs performance on tasks that people perform more poorly at. Although the idea of co-training agents on language to shape their representations has been explored before, most works utilize synthetic descriptions [17,21]. Our work suggests human-generated language can lead to more human-like performance than more literal synthetic language descriptions [16], because human-generated descriptions contain information about abstract concepts (e.g. lines, shapes, letters, etc) that compress description length and are reflected in tasks directly sampled from human priors, therefore better capturing human inductive biases. Although we used synthetic descriptions that are as literal/devoid of abstractions as possible to test this point, synthetic descriptions built to contain such abstractions could work as well. The challenge, then, in using such synthetic descriptions would be coming up with the correct abstractions to build in, rather than crowdsourcing such abstractions from human participants as we do in this work.\nWe also show evidence that co-training artificial agents on representations from program induction results in learning human inductive biases just as co-training on language does. Programs induced with library-learned concepts [24] are analogous to language in that adding library-learned con-cepts to the initial DSL enables compression of description length just as how humans use abstract concepts to compress description length. Like human language, programs with library abstractions better enable learning of human-like inductive biases. Program induction can sometimes be difficult due to the combinatorial explosion of the program space [48] and the non-trivial decision of picking a DSL. Co-supervision of tabula rasa artificial agents using representations from program induction can be a useful way to build agents with both the flexibility of neural networks and the human-like priors that program induction can provide. Additionally, although we investigate language and programs as separate but analogous representations that can guide the training of artificial agents, recent work has also learned joint compositional generative models over program libraries and natural language descriptions [18]. An exciting direction for future work is to combine our objectives, utilizing joint program and language representations to instill human inductive biases in artificial agents.\nIn this work, we use a constrained task paradigm on 2D grids, which gives us the advantage of being able to directly study samples of human and machine priors on the task space (Fig. 2). Although some abstractions we found may be specific to this domain, the literature in cognitive science [4] provides good reason to expect that the effectiveness of our approach in this more artificial domain could translate to abstractions in real-world domains that are natural for humans. Some specific domains, such as mathematical reasoning, planning, or puzzle solving, may even more obviously necessitate abstractions [49]. It will be exciting future work to use our approach to empirically validate this conjecture by determining the space of tasks for which co-training with language and program abstractions improves an artificial neural agent's performance. This future work will be an exciting opportunity both for studying human intelligence by getting a better context for what the role of abstraction is in everyday cognition as well as improving machine intelligence by expanding the capabilities of current neural agents. However, there is still work to do to determine how language and programs can influence acquisition of human inductive biases in more scaled up, naturalistic, and real-world settings.\nOn the language side, the main bottleneck is that collecting human descriptions with sufficient coverage can become prohibitively expensive as the state space grows larger. One potential remedy would be to employ active learning methods to focus language elicitation on the most relevant abstractions in the environment and to explore data augmentation approaches to make these descriptions go further. For example, a vision-language model [50] could be fine-tuned on the human descriptions to generate sufficiently high-quality descriptions for new states. There is some new work showing that even a small amount of language can be used for models to perform visual grounding on large environments [51]. This approach to scaling language may require scoring descriptions for their quality in describing the relevant abstractions in the environment, which is less time-consuming for human participants than producing such descriptions.\nOn the program side, the need to define ad hoc base DSLs for each domain is an important limitation of any approach relying on program abstractions. An important direction for this area is to develop a 'canonical' basis for program representations that may apply across many domains. For example, a graphics engine may provide one such general-purpose DSL to synthesize state abstractions for larger and more realistic visual environments. This kind of base DSL could be seen as an extension to the generic 2D vector graphics used by Ellis et al. 52, to induce graphics programs from hand-drawn images. A potential bottleneck here is that library learning can be especially computationally expensive for larger environments, but several algorithmic improvements have already emerged that make this direction more promising [53].\nStrong inductive biases towards abstract structure allow humans to generalize to novel environments without much experience [4]. By instilling these biases in our artificial agents, we can work towards enabling machines to demonstrate human-like general intelligence.\nThe activation dynamics within the LSTM utilizes this prior, along with the history of observations within the episode, to implement a fast algorithm to solve the current grid task for the inner loop. This work's agent's encoder processes the board through a convolutional layer and a fully connected layer and passes this output into the LSTM along with the previous action and reward (see Fig. 3B). To implement the grounding mechanism (Fig. 6A), we take the encoder's output, and pass it through two additional fully connected layers to predict the task embedding. All agents were trained using Proximal Policy Optimization (PPO; Schulman et al. 37 using the Stable Baselines package Raffin et al. 38) for one million episodes. We performed a hyperparameter sweep separately for the agents without the grounding loss (i.e. original agents) and with the grounding loss, since we have to tune the new c task weight on the grounding loss jointly. We performed a hyperparameter sweep for: batch size, n_steps (number of steps to run in an environment update), gamma, learning rate, learning rate schedule (constant or linear), clip range, number of epochs, the \u03bb for Generalized Advantage Estimate (GAE \u03bb), max grad norm, activation function, value loss coefficient, entropy coefficient, and grounding loss coefficient for agents with the grounding loss. The hyperparameter sweep was done by sampling from the space of hyperparameter using the Tree-Structured Parzen Estimator [57]. We evaluated 200 samples of hyperparameters from the space for all agents. Both grounding and non-grounding agents used the same hyperparameter spaces to sweep over. We initially did a separate hyperparameter sweep for different grounding agents, but we found in initial experiments that they all reached similar hyperparameter values and training reward after the search. Hyperparameters were evaluated by training on 100,000 episodes and looking at the training reward. The environments used during test time (Fig. 3B and Fig. 6) were completely held-out during this process. We trained each grounding agent (and the non-grounding agent) fifteen times and ran their policy fifteen times for each held-out test task distribution (GSP and control tasks). Training was run on a university cluster using NVIDIA P100 GPUs with 16 GB of memory. GSP samples internal prior distributions by putting humans \"in the loop\" of a Gibbs sampler. In our case, the stimulus space consisted of the space of 4 \u00d7 4 boards, and each of the 16 stimulus dimensions corresponded to the binary color of each tile, namely, red or blue. One of these dimensions was masked out (i.e. \"greyed\") for the prediction task. Each GSP trial consisted of a prediction task of predicting what color the single masked square is in the grid conditional on the colors of all other squares on the grid. Once a decision is made, the resulting stimulus is passed on to a new participant who repeats the task with another masked square and so on. A sample is generated once a full sweep through the all sixteen squares is completed, similar to the standard procedure of Gibbs sampling. In each trial, participants were presented with a board with one of its tiles covered (indicated by a white tile) as well as the following prompt \"what should be the underlying color of the covered white tile such that the board is described by a very simple rule?\". They then delivered their answer by clicking on a button that corresponded to their color of choice. Overall, we ran 100 GSP chains in parallel for 15 sweeps each, and chains were initialized with randomly sampled boards. The order in which tiles were masked out within each sweep was also randomized across chains to avoid potential biases.\nParticipants were recruited on Amazon Mechanical Turk (AMT) and a total of 272 participants completed the study. To ensure that participants did not suffer from any color perception deficiencies, we ran the Ishihara color blindness test [59] as a pre-screening task. This also helped in screening out automated scripts (\"bots\") that masquerade as participants [60].", "publication_ref": ["b16", "b20", "b15", "b23", "b47", "b17", "b3", "b48", "b49", "b50", "b52", "b3", "b56", "b58", "b59"], "figure_ref": ["fig_0", "fig_3", "fig_0", "fig_3"], "table_ref": []}, {"heading": "A.4 Human Language Description Experiment", "text": "We took 500 of the highest probability GSP boards and randomly assigned subjects 25 of the boards such that each subject gets a unique set of boards and each board gets 9-12 descriptions each from different people. The prompt people were given was: \"Your goal is to describe this pattern of red squares in words. Be as detailed as possible. Someone should be able to reproduce the entire board given your description. You may be rewarded based on how detailed your description is.\" We used Prolific (http://www.prolific.co) for anonymous subject recruiting. Participants were paid at a rate of approximately $12 per hour (with a total study cost of $1844) and were fully anonymized through the whole process. All text data focused on just the board stimuli and did not include any personally-identifiable content. The experiment contained writing about artificial grid stimuli, so potential risks to subjects were very minimal. Subjects gave their consent through a standard consent form at the start of the experiment (where they had to confirm that they agree and give their consent).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.5 DNN Co-training Control Experiment", "text": "For the program results, we co-trained the RL agent with representations from the recognition DNN model used in DreamCoder that proposes programs given a board. The recognition model's architectural modifications, training objective, and data distribution play a critical role in developing useful representations that can be distinguished from a more generic DNN trained on the GSP boards. The DreamCoder DNN's architecture is built to specifically predict a probability distribution over the current routines in the library (this includes high-level routines added to the DSL during library learning). The recognition model is trained to predict the most likely programs for a given grid by balancing a program's description length and its score (its training objective). In addition, the data it trains on comes both from the GSP task dataset, as well as from grids that are randomly sampled from the current DSL/grammar (\"dreams\": its training data).\nAs a comparison point, we provide results (Fig. 8) when cotraining with representations from a \"generic DNN\" that is trained on a more standard task on the GSP boards (predicting randomly held-out tiles). The results show that the selective performance improvement on human-generated tasks vs machine-generated tasks is unique to cotraining representations from the DreamCoder DNN as opposed to a standard DNN. The recognition model's specific architectural modifications, training objective, and data distribution used for program induction within the DreamCoder framework play a critical role in developing representations that can be distinguished from those of a Figure 8: Grounding results for control experiment where we co-train with DNN representations from the hidden layer of a network trained to predict randomly held-out tiles of the GSP boards.\nAlthough co-training with this network does improve performance on human and machine generated boards relative to the original agent, it does not selectively improve performance on humangenerated boards and impair performance on machine-generated boards like co-training with the DreamCoder recognition model does.\nmore 'vanilla' DNN and can serve as a distinct and effective cotraining target that can better instill human inductive biases.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.6 Broader Impact Statement", "text": "This work aims to guide artificial agents through human-like behavior. One way in which this work achieves this goal is by co-training with human-generated language descriptions. Although all language data collected in this study was anonymized and devoid of personally-identifiable information, a larger discussion is to be had in scaling this approach up while respecting the privacy and anonymity of people's data. Should an approach like ours be massively scaled, a conversation about careful checks on data privacy must be had in order to prevent leakage of private information in downstream agent behaviors.\nCurrent artificial systems are not built explicitly to exhibit human-like qualities or behavior. Our work on explicitly training in human-like biases into artificial systems' behavior can be greatly beneficial in terms of being more easily interpretable by humans and being more easily amenable to human-machine collaborations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Appendix", "text": "A.1 Program induction with dreamcoder DreamCoder was run in two modes for the reported analyses: with and without library learning. All parameters matched between the two modes except those exclusive to library learning: 4 iterations of DreamCoder, 10 CPUs, 2 minute timeout for program enumeration, 2 minute timeout for recognition model training, and 50% of tasks used to train the recognition model were randomly sampled \"dreams\". The recognition model was trained to predict a unigram distribution over DSL primitives and library components. No task batching was enabled, so every task was solved at every iteration. Parameters only relevant to library learning were: structure penalty \u03bb = 1.5, refactoring steps n = 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Details of training and hyperparameter tuning of meta-reinforcement learning agents", "text": "Meta-learning can be considered as a bi-level optimization problem where there is an outer-loop of learning in which the model learns a useful inductive bias across different tasks of the same task distribution and an inner-loop of learning which takes that inductive bias and rapidly learns or adapts within a specific task [9]. In recurrent-based meta-reinforcement learning architectures like ours, in which tasks are fed sequentially to the model, the outer loop is explicitly implemented as a reinforcement learning algorithm that updates the weights across tasks and the inner loop is implicitly implemented in the activation dynamics of the recurrent network [54,55] which employs fast adaptation within a specific task. See Ortega et al. [56] for a formal explanation of this adaptation. In our case, the LSTM weights are updated in the outer loop across different grids to give the LSTM a useful prior learned from patterns or abstractions seen across different grid tasks.", "publication_ref": ["b8", "b53", "b54", "b55"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Human Learning, eBook", "journal": "Pearson Higher Ed", "year": "2016", "authors": "Jeanne Ellis Ormrod"}, {"ref_id": "b1", "title": "How to grow a mind: Statistics, structure, and abstraction", "journal": "Science", "year": "2011", "authors": "B Joshua; Charles Tenenbaum;  Kemp; L Thomas; Noah D Griffiths;  Goodman"}, {"ref_id": "b2", "title": "Probabilistic models of cognition: exploring representations and inductive biases", "journal": "Trends in Cognitive Sciences", "year": "2010", "authors": "L Thomas; Nick Griffiths; Charles Chater; Amy Kemp; Joshua B Perfors;  Tenenbaum"}, {"ref_id": "b3", "title": "Building machines that learn and think like people", "journal": "Behavioral and Brain Sciences", "year": "2017", "authors": " Brenden M Lake; D Tomer; Joshua B Ullman; Samuel J Tenenbaum;  Gershman"}, {"ref_id": "b4", "title": "The Need for Biases in Learning Generalizations", "journal": "", "year": "1980", "authors": "M Tom;  Mitchell"}, {"ref_id": "b5", "title": "Relational inductive biases, deep learning, and graph networks", "journal": "", "year": "2018", "authors": "W Peter; Jessica B Battaglia; Victor Hamrick; Alvaro Bapst; Vinicius Sanchez-Gonzalez; Mateusz Zambaldi; Andrea Malinowski; David Tacchetti; Adam Raposo; Ryan Santoro;  Faulkner"}, {"ref_id": "b6", "title": "Inductive biases for deep learning of higher-level cognition", "journal": "", "year": "2020", "authors": "Anirudh Goyal; Yoshua Bengio"}, {"ref_id": "b7", "title": "Doing more with less: meta-reasoning and meta-learning in humans and machines", "journal": "Current Opinion in Behavioral Sciences", "year": "2019", "authors": "L Thomas; Frederick Griffiths;  Callaway; B Michael; Erin Chang;  Grant; M Paul; Falk Krueger;  Lieder"}, {"ref_id": "b8", "title": "Meta-learning in neural networks: A survey", "journal": "", "year": "2020", "authors": "Timothy Hospedales; Antreas Antoniou; Paul Micaelli; Amos Storkey"}, {"ref_id": "b9", "title": "Metalearning of structured task distributions in humans and machines", "journal": "", "year": "2021", "authors": "Sreejan Kumar; Ishita Dasgupta; Jonathan Cohen; Nathaniel Daw; Thomas Griffiths"}, {"ref_id": "b10", "title": "Learning to navigate in complex environments", "journal": "", "year": "2016", "authors": "Piotr Mirowski; Razvan Pascanu; Fabio Viola; Hubert Soyer; J Andrew; Andrea Ballard; Misha Banino; Ross Denil; Laurent Goroshin; Koray Sifre;  Kavukcuoglu"}, {"ref_id": "b11", "title": "Loss is its own reward: Self-supervision for reinforcement learning", "journal": "", "year": "2016", "authors": "Evan Shelhamer; Parsa Mahmoudieh; Max Argus; Trevor Darrell"}, {"ref_id": "b12", "title": "Reinforcement learning with unsupervised auxiliary tasks", "journal": "", "year": "2016", "authors": "Max Jaderberg; Volodymyr Mnih; Wojciech Marian Czarnecki; Tom Schaul; Joel Z Leibo; David Silver; Koray Kavukcuoglu"}, {"ref_id": "b13", "title": "How language programs the mind", "journal": "Topics in Cognitive Science", "year": "2016", "authors": "Gary Lupyan; Benjamin Bergen"}, {"ref_id": "b14", "title": "Language in Mind: Advances in the Study of Language and Thought", "journal": "", "year": "2003", "authors": "S Elizabeth;  Spelke"}, {"ref_id": "b15", "title": "Learning with latent language", "journal": "Long Papers", "year": "2018", "authors": "Jacob Andreas; Dan Klein; Sergey Levine"}, {"ref_id": "b16", "title": "A survey of reinforcement learning informed by natural language", "journal": "", "year": "2019", "authors": "Jelena Luketina; Nantas Nardelli; Gregory Farquhar; Jakob N Foerster; Jacob Andreas; Edward Grefenstette; Shimon Whiteson; Tim Rockt\u00e4schel"}, {"ref_id": "b17", "title": "Leveraging language to learn program abstractions and search heuristics", "journal": "PMLR", "year": "2021", "authors": "Catherine Wong; Kevin M Ellis; Joshua Tenenbaum; Jacob Andreas"}, {"ref_id": "b18", "title": "Grounding language for transfer in deep reinforcement learning", "journal": "Journal of Artificial Intelligence Research", "year": "2018", "authors": "Karthik Narasimhan; Regina Barzilay; Tommi Jaakkola"}, {"ref_id": "b19", "title": "Shaping visual representations with language for few-shot classification", "journal": "", "year": "2020", "authors": "Jesse Mu; Percy Liang; Noah Goodman"}, {"ref_id": "b20", "title": "Tell me why! -explanations support learning of relational and causal structure", "journal": "", "year": "2021", "authors": " Andrew K Lampinen; A Nicholas; Ishita Roy;  Dasgupta; C Y Stephanie; Allison C Chan; James L Tam; Chen Mcclelland; Adam Yan;  Santoro; C Neil; Jane X Rabinowitz;  Wang"}, {"ref_id": "b21", "title": "Semantic supervision: Enabling generalization over output spaces", "journal": "", "year": "2022", "authors": "Ameet Austin W Hanjie; Karthik Deshpande;  Narasimhan"}, {"ref_id": "b22", "title": "Human-level concept learning through probabilistic program induction", "journal": "Science", "year": "2015", "authors": "Ruslan Brenden M Lake; Joshua B Salakhutdinov;  Tenenbaum"}, {"ref_id": "b23", "title": "Dreamcoder: bootstrapping inductive program synthesis with wake-sleep library learning", "journal": "", "year": "2021", "authors": "Kevin Ellis; Catherine Wong; Maxwell Nye; Mathias Sabl\u00e9-Meyer; Lucas Morales; Luke Hewitt; Luc Cary; Armando Solar-Lezama; Joshua B Tenenbaum"}, {"ref_id": "b24", "title": "Identifying concept libraries from language about object structure", "journal": "", "year": "2022", "authors": "Catherine Wong; P William; Gabriel Mccarthy; Yoni Grand; Joshua B Friedman; Jacob Tenenbaum;  Andreas; D Robert; Judith E Hawkins;  Fan"}, {"ref_id": "b25", "title": "Learning to communicate about shared procedural abstractions", "journal": "", "year": "2021", "authors": "P William; Robert D Mccarthy; Haoliang Hawkins; Cameron Wang; Judith E Holdaway;  Fan"}, {"ref_id": "b26", "title": "The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision", "journal": "", "year": "2019", "authors": "Jiayuan Mao; Chuang Gan; Pushmeet Kohli; Joshua B Tenenbaum; Jiajun Wu"}, {"ref_id": "b27", "title": "Learning compositional rules via neural program synthesis", "journal": "", "year": "2020", "authors": "Maxwell Nye; Armando Solar-Lezama; Josh Tenenbaum; M Brenden;  Lake"}, {"ref_id": "b28", "title": "Learning abstract structure for drawing by efficient motor program induction", "journal": "", "year": "2020", "authors": "Lucas Tian; Kevin Ellis; Marta Kryven; Josh Tenenbaum"}, {"ref_id": "b29", "title": "Letting structure emerge: Connectionist and dynamical systems approaches to cognition", "journal": "Trends in Cognitive Sciences", "year": "2010", "authors": "L James;  Mcclelland; M Matthew;  Botvinick; C David;  Noelle; C David; Timothy T Plaut;  Rogers; S Mark; Linda B Seidenberg;  Smith"}, {"ref_id": "b30", "title": "Universal linguistic inductive biases via meta-learning", "journal": "", "year": "2020", "authors": "Thomas Mccoy; Erin Grant; Paul Smolensky; L Thomas; Tal Griffiths;  Linzen"}, {"ref_id": "b31", "title": "Causal reasoning from meta-reinforcement learning", "journal": "", "year": "2019", "authors": "Ishita Dasgupta; Jane Wang; Silvia Chiappa; Jovana Mitrovic; Pedro Ortega; David Raposo; Edward Hughes; Peter Battaglia; Matthew Botvinick; Zeb Kurth-Nelson"}, {"ref_id": "b32", "title": "Machine theory of mind", "journal": "PMLR", "year": "2018", "authors": "Neil Rabinowitz; Frank Perbet; Francis Song; Chiyuan Zhang; Ali Eslami; Matthew Botvinick"}, {"ref_id": "b33", "title": "Gibbs sampling with people", "journal": "Advances in Neural Information Processing Systems", "year": "2020", "authors": "Peter Harrison; Raja Marjieh; Federico Adolfi; Manuel Pol Van Rijn; Ofer Anglada-Tort; Pauline Tchernichovski; Nori Larrouy-Maestri;  Jacoby"}, {"ref_id": "b34", "title": "Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2018", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova Bert"}, {"ref_id": "b35", "title": "Prefrontal cortex as a meta-reinforcement learning system", "journal": "Nature neuroscience", "year": "2018", "authors": "X Jane; Zeb Wang; Dharshan Kurth-Nelson; Dhruva Kumaran; Hubert Tirumala; Joel Z Soyer; Demis Leibo; Matthew Hassabis;  Botvinick"}, {"ref_id": "b36", "title": "Proximal policy optimization algorithms", "journal": "", "year": "2017", "authors": "John Schulman; Filip Wolski; Prafulla Dhariwal; Alec Radford; Oleg Klimov"}, {"ref_id": "b37", "title": "Stable baselines3", "journal": "", "year": "2019", "authors": "Antonin Raffin; Ashley Hill; Maximilian Ernestus; Adam Gleave; Anssi Kanervisto; Noah Dormann"}, {"ref_id": "b38", "title": "A robustly optimized BERT pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov;  Roberta"}, {"ref_id": "b39", "title": "Sentence-BERT: Sentence embeddings using siamese BERT-networks", "journal": "", "year": "2019", "authors": "Nils Reimers; Iryna Gurevych"}, {"ref_id": "b40", "title": "Learning words from sights and sounds: A computational model", "journal": "Cognitive science", "year": "2002", "authors": "K Deb; Alex P Roy;  Pentland"}, {"ref_id": "b41", "title": "A probabilistic approach to learning a visually grounded language model through human-robot interaction", "journal": "IEEE", "year": "2010", "authors": "Haris Dindo; Daniele Zambuto"}, {"ref_id": "b42", "title": "Eye spy: Improving vision through dialog", "journal": "", "year": "2010", "authors": "Adam Vogel; Karthik Raghunathan; Dan Jurafsky"}, {"ref_id": "b43", "title": "Probabilistic labeling for efficient referential grounding based on collaborative discourse", "journal": "Short Papers", "year": "2014", "authors": "Changsong Liu; Lanbo She; Rui Fang; Joyce Chai"}, {"ref_id": "b44", "title": "Grounding referring expressions in images by variational context", "journal": "", "year": "2018", "authors": "Hanwang Zhang; Yulei Niu; Shih-Fu Chang"}, {"ref_id": "b45", "title": "Guided feature transformation (gft): A neural language grounding module for embodied agents", "journal": "PMLR", "year": "2018", "authors": "Haonan Yu; Xiaochen Lian; Haichao Zhang; Wei Xu"}, {"ref_id": "b46", "title": "Multimodal few-shot learning with frozen language models", "journal": "Advances in Neural Information Processing Systems", "year": "2021", "authors": "Maria Tsimpoukelli; L Jacob; Serkan Menick;  Cabi; Oriol Sm Eslami; Felix Vinyals;  Hill"}, {"ref_id": "b47", "title": "Program synthesis. Foundations and Trends\u00ae in Programming Languages", "journal": "", "year": "2017", "authors": "Sumit Gulwani; Oleksandr Polozov; Rishabh Singh"}, {"ref_id": "b48", "title": "Human problem solving", "journal": "Prenticehall", "year": "1972", "authors": "Allen Newell; Herbert Alexander Simon"}, {"ref_id": "b49", "title": "Learning transferable visual models from natural language supervision", "journal": "PMLR", "year": "2021", "authors": "Alec Radford; Jong Wook Kim; Chris Hallacy; Aditya Ramesh; Gabriel Goh; Sandhini Agarwal; Girish Sastry; Amanda Askell; Pamela Mishkin; Jack Clark"}, {"ref_id": "b50", "title": "Intra-agent speech permits zero-shot task acquisition", "journal": "", "year": "2022", "authors": "Chen Yan; Federico Carnevale; Petko Georgiev; Adam Santoro; Aurelia Guy; Alistair Muldal; Chia-Chun Hung; Josh Abramson; Timothy Lillicrap; Gregory Wayne"}, {"ref_id": "b51", "title": "Learning to infer graphics programs from hand-drawn images", "journal": "", "year": "2018", "authors": "Kevin Ellis; Daniel Ritchie; Armando Solar-Lezama; Josh Tenenbaum"}, {"ref_id": "b52", "title": "Top-down synthesis for library learning", "journal": "", "year": "", "authors": "Matthew Bowers; Theo X Olausson; Catherine Wong; Gabriel Grand; Joshua B Tenenbaum; Kevin Ellis; Armando Solar-Lezama"}, {"ref_id": "b53", "title": "Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn", "journal": "", "year": "2016", "authors": "X Jane; Zeb Wang; Dhruva Kurth-Nelson; Hubert Tirumala; Joel Z Soyer; Remi Leibo; Charles Munos;  Blundell"}, {"ref_id": "b54", "title": "Reinforcement learning, fast and slow", "journal": "Trends in cognitive sciences", "year": "2019", "authors": "Matthew Botvinick; Sam Ritter; Jane X Wang; Zeb Kurth-Nelson; Charles Blundell; Demis Hassabis"}, {"ref_id": "b55", "title": "Meta-learning of sequential strategies", "journal": "", "year": "2019", "authors": "A Pedro; Jane X Ortega; Mark Wang; Tim Rowland; Zeb Genewein; Razvan Kurth-Nelson; Nicolas Pascanu; Joel Heess; Alex Veness; Pablo Pritzel;  Sprechmann"}, {"ref_id": "b56", "title": "Algorithms for hyperparameter optimization", "journal": "Advances in neural information processing systems", "year": "2011", "authors": "James Bergstra; R\u00e9mi Bardenet; Yoshua Bengio; Bal\u00e1zs K\u00e9gl"}, {"ref_id": "b57", "title": "Subjective randomness as statistical inference", "journal": "Cognitive psychology", "year": "2018", "authors": "L Thomas; Dylan Griffiths;  Daniels; L Joseph; Joshua B Austerweil;  Tenenbaum"}, {"ref_id": "b58", "title": "The ishihara test for color blindness", "journal": "American Journal of Physiological Optics", "year": "1924", "authors": "J H Clark"}, {"ref_id": "b59", "title": "An mturk crisis? shifts in data quality and the impact on study results", "journal": "Social Psychological and Personality Science", "year": "2020", "authors": "Michael Chmielewski; C Sarah;  Kucker"}, {"ref_id": "b60", "title": "", "journal": "Machine Generated (Control) Human Generated (GSP)", "year": "", "authors": ""}], "figures": [{"figure_label": "3", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 3 :3Figure 3: Meta-RL Task and Architecture. (A)The goal of the agent is to sequentially uncover all the red tiles to reveal a picture on a 2D grid while uncovering as few white tiles as possible. A single board is one task and a distribution over boards is a task distribution. (B). Performance of humans and artificial agents on the human-elicited vs machine-generated samples (seeFig 2). The performance metric is based on the number of white tiles revealed, so lower is better.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 4 :4Figure 4: Properties of Language and Program Data. (A)Example board with corresponding human-generated language descriptions. Some people write more abstract descriptions than others. (B) Examples showcasing compression of programs with learned library concepts. We show the library-learning program (top grids) and non-library learning programs (bottom grids). For each program, we show the execution path (right, star indicates start of path) and the program stack (left). Black indicates main stack whereas blue indicates use of a learned library function. Nonlibrary learning programs do not use learned library functions and thus function only on the base stack. (C) Human-generated language descriptions typically have lower description length than the synthetic descriptions. Analogously, programs with library learning are on average shorter than programs with just the base DSL. (D). Boards that elicit shorter more abstract descriptions from humans also elicit shorter programs, indicating a correspondence between the abstractions human use in language and those from with library learning.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 6 :6Figure6: Grounding Mechanism (A) Mechanism for grounding agents using specific representations. We have the agent's encoder output simultaneously predict a task embedding while performing the task. These task embeddings can be from programs (embeddings from the recognition model of DreamCoder), language (embeddings from RoBERTa), or the flattened board itself (autoencoder baseline). (B). Performance on previously unseen GSP (colored bars) and control tasks (white bars with colored borders) from grounding agents with different task representations; lower is better (since performance is based on white tiles revealed). Error bars are 95% confidence intervals across different 15 trained agents (for humans, across 50 different participants). (C). Gap between performance on human-generated and machine-generated tasks. Higher gap indicates more human-like behavior of the model.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Hyperparameters Chosen Gibbs Sampling with People ExperimentTo generate a task distribution of boards directly from humans we used Gibbs Sampling with People, or GSP (seeHarrison et al. 34  and a similar task for binary sequences in Griffiths et al. 58).", "figure_data": "AgentNo Grounding Loss Grounding Lossbatch_size16256n_steps20488gamma0.90.9learning_rate0.0005165010.000376021lr_schedulelinearlinearent_coef1.3907E-051.45674E-06clip_range0.30.3n_epochs105gae_lambda0.80.95max_grad_norm 20.6vf_coef0.0009143630.016291309activation_fnrelutanhgrounding_coef 00.494866282"}], "formulas": [], "doi": ""}