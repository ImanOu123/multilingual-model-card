{"title": "SemEval-2020 Task 4: Commonsense Validation and Explanation", "authors": "Cunxiang Wang; Shuailong Liang; Yili Jin; Yilong Wang; Xiaodan Zhu; Yue Zhang", "pub_date": "", "abstract": "In this paper, we present SemEval-2020 Task 4, Commonsense Validation and Explanation (ComVE), which includes three subtasks, aiming to evaluate whether a system can distinguish a natural language statement that makes sense to humans from one that does not, and provide the reasons. Specifically, in our first subtask, the participating systems are required to choose from two natural language statements of similar wording the one that makes sense and the one does not. The second subtask additionally asks a system to select the key reason from three options why a given statement does not make sense. In the third subtask, a participating system needs to generate the reason. We finally attracted 39 teams participating at least one of the three subtasks. For Subtask A and Subtask B, the performances of top-ranked systems are close to that of humans. However, for Subtask C, there is still a relatively large gap between systems and human performance. The dataset used in our task can be found at https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation; The leaderboard can be found at https://competitions.codalab.org/competitions/21080#results.", "sections": [{"heading": "Introduction", "text": "In the past decades, computer' ability in processing natural language has significantly improved. However, its intelligence for understanding common sense expressed in language is still limited. For example, it is straightforward for humans to judge that the following sentence is plausible, or makes sense: \"John put a turkey into a fridge\" while \"John put an elephant into the fridge\" does not, but it is non-trivial for a computer to tell the difference. Arguably, commonsense reasoning plays a central role in a natural language understanding system (Davis, 2017). It is essential to gauge how well computers can understand whether a given statement makes sense. In our task, we take an operational definition of making sense by asking human subjects to generate natural language statements that obey or violate their commonsense knowledge about the world. 1 Many existing tasks embed the evaluation of commonsense understanding in other tasks such as coreference resolution (Levesque et al., 2012;Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011), ordinal common-sense inference (Zhang et al., 2017), situations with adversarial generations (Zellers et al., 2018), event validation (Wang et al., 2018), reading comprehension (Mostafazadeh et al., 2016;Ostermann et al., 2018b;Ostermann et al., 2018a), dialogue (Cui et al., 2020) and QA (Davis, 2016;Talmor et al., 2018;Mihaylov et al., 2018). They verify whether a system is equipped with common sense by testing whether the system can give a correct answer when the input does not contain such knowledge. The above tasks do not directly evaluate commonsense validation and they do not explicitly identify the key factor required in a commonsense validation process.\nThe SemEval-2020 Task 4 includes three subtasks on testing whether a system can distinguish natural language statements that make sense from those that do not, and probe the reasons. In the first subtask, a system needs to choose the against-common-sense statement from two natural language statements of similar wordings, e.g., \"John put an elephant into the fridge\" and \"John put a turkey into the fridge\", respectively. The second task aims to find the key reason from three provided options why a given nonsensical statement does not make sense. For example, for the nonsensical statement, \"John put an elephant into the fridge\", the three options are \"An elephant is much bigger than a fridge\", \"Elephants are usually white while fridges are usually white\", and \"An elephant cannot eat a fridge.\" A system needs to identify the correct reason. In addition, the third task requires the participating systems to generate the reason automatically. We hope that the task and datasets can facilitate studies on commonsense validation, its interpretability, and the related natural language understanding and generation problems.\nThere are 39 teams submitting valid systems to at least one subtask. In Subtask A and Subtask B, top-performing systems achieve performances closed to that of human subjects. However, for Subtask C, there is still a relatively large between system and human performances.", "publication_ref": ["b6", "b16", "b30", "b43", "b62", "b61", "b56", "b31", "b35", "b34", "b2", "b5", "b53", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Task Setup", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Task Definition", "text": "Formally, each instance in our dataset is composed of eight sentences:\n{s 1 , s 2 , o 1 , o 2 , o 3 , r 1 , r 2 , r 3 }. s 1\nand s 2 are two similar statements that differ by only a few words; one of them makes sense (i.e., conforms to common sense) while the other does not. They are used in our Subtask A: the Validation subtask, which requires a model to identify which one makes sense. For the statement that does not make sense, we have three candidate reasons, i.e., three options o 1 , o 2 , and o 3 ; one of them explains why the statement does not make sense. So, in our Subtask B, the Explanation (Multi-Choice) subtask, a model is required to find the correct reason from the three options. For the same nonsensical statement, in Subtask C, the Explanation (Generation) subtask, a participating system needs to generate the reason why it does not make sense. Three references, r 1 , r 2 , and r 3 , are used for evaluating Subtask C. Below we give an example for each subtask, in which we introduce some notations we will use in the paper.\n\u2022 Subtask A: Validation Task: Select the statement of the two that does not make sense. s 1 : John put a turkey into a fridge. s 2 : John put an elephant into the fridge.\nIn this example, s 1 is a sensical statement, also denoted as s c , while s 2 is the nonsensical statement, which is also denoted as s n .\n\u2022 Subtask B: Explanation (Multi-Choice) Task: Select the best reason that explains why the given statement does not make sense. Nonsensical statement (s n ): John put an elephant into the fridge. o 1 : An elephant is much bigger than a fridge. o 2 : Elephants are usually white while fridges are usually white. o 3 : An elephant cannot eat a fridge.\nIn this example, the option o 1 is the correct reason, which is also denoted also as o c , while o 2 and o 3 are not the reason, which are also denoted as o n1 and o n2 .\n\u2022 Subtask C: Explanation (Generation) Task: Generate the reason why this statement does not make sense. Nonsensical statement (s n ): John put an elephant into the fridge. Reference reasons (used for calculating the BLEU score): r 1 : An elephant is much bigger than a fridge. r 2 : A fridge is much smaller than an elephant. r 3 : Most of the fridges aren't large enough to contain an elephant.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Score Description", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "0", "text": "The reason is not grammatically correct, or not comprehensible at all, or not related to the statement at all.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "1", "text": "The reason is just the negation of the statement or a simple paraphrase. Obviously, a better explanation can be made.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "2", "text": "The reason is relevant and appropriate, though it may contain a few grammatical errors or unnecessary parts. Or like case 1, but it's hard to write a proper reason.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "3", "text": "The reason is appropriate and is a solid explanation of why the statement does not make sense.\nTable 1: Rubrics used in human evaluation in Subtask C.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation Metrics", "text": "The Subtasks A and B are evaluated using accuracy. Subtask C is evaluated with the BLEU score (Papineni et al., 2002). In addition, for Subtask C, we further perform human evaluation. We randomly select 100 instances from the test set and evaluate system outputs on Amazon Mechanical Turk. We ask three different crowd-sourcing workers to score each generated reason with a scale ranging from 0 to 3, inclusively, according the rubrics listed in Table 1.\nThen we calculate the average score of the three scores as our final human evaluation score. Formally, the human evaluation score of system k is\nscore k = 100 i=1 3 j=1 score ijk 100 * 3 ,(1)\nwhere score ijk means the score from the j th annotator for system k on the i th instance.", "publication_ref": ["b37"], "figure_ref": [], "table_ref": []}, {"heading": "Data Construction", "text": "Our data construction is mainly performed on Amazon Mechanical Turk, which consists of two steps:\n\u2022 Step 1: In this step, we construct datasets for Subtask A and Subtask B. Specifically, we ask a crowd-sourcing worker to write a sensical statement s c and a nonsensical statement s n . For the nonsensical statement s n , the worker further writes three sentences, o 1 , o 2 , o 3 ; one of them, denoted as o c , explains why the nonsensical statement does not make sense; two of them, denoted as o n1 and o n2 , serve as the confusing choices. (Refer to Section 3.1 for details.)\n\u2022\nStep 2: We then make three reference reasons, r 1 , r 2 , r 3 for Subtask C. We use o c as one of three references, and collect two more references in this step. We ask two different crowd-sourcing workers to write each of them. Note that instead of letting the same worker in step 1 to write these two references, we asked two more workers. The reason is to encourage diversity of the reference. (Refer to Section 3.2 for details.)\nFinally, each instance of the dataset have 8 sentences:\n{s 1 , s 2 , o 1 , o 2 , o 3 , r 1 , r 2 , r 3 }. Note that one sentence in o 1 , o 2 , o 3 is repeated in r 1 , r 2 , r 3\n, but for convenience of description, we denote it differently.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "3.1", "text": "Step 1: Collecting Data for Subtask A and B Annotation Guidelines. When writing instances, workers were asked to follow several principles: (1) Try to avoid complex knowledge and focus on daily common sense. Make the questions as understandable as possible, so that a literate person is able to give the right answers.\n(2) The confusing reason options, o n1 and o n2 , should better contain more content words or information such as entities and activities in the nonsensical statements s n . For example, the confusing reasons of \"John put an elephant into the fridge\" should better contain both \"elephant\" and \"fridge\". (3) The confusing reasons, o n1 and o n2 , should be related to the statements s n and the correct reason o c and not deviate from the context; otherwise it may be easily captured by pretrained models like BERT (Talmor et al., 2018). ( 4  and o 3 should only be related to the incorrect statements s n rather than the correct statements s c , because we want further studies to be able to estimate nonsensical statements s n without the correct statement s c . (5) The confusing reasons, o n1 and o n2 , should make sense themselves. Otherwise, the models may simply ignore the incorrect options o n1 , o n2 without considering the casual semantics. This concern is raised from and motivated by the fact that models can achieve high performance in the ROC Story Cloze Task, when only looking at the alternative endings and ignoring the story content (Schwartz et al., 2017). ( 6) We ask the annotators to make the nonsensical statement s n contain about the same number of words as the sensical statement s c , and the correct reason o c have similar length with other two options. We drop the instances which do not meet such requirements.\nUse of Inspirational Materials. It is not easy for all crowd-sourcing workers to write instances from scratch. To address this issue, we also provide them with external reading materials to stimulate inspiration, such as the sentences of the Open Mind Common Sense (OMCS) project (Havasi et al., 2010). For example, \"he was sent to a (restaurant)/(hospital) for treatment after a car crash\" can be inspired by the two sentences \"restaurants provide food\" and \"hospitals provide medical care\".\nQuality Control. To ensure the quality of the data, we manually check the instances and drop or request a rewriting of the low-quality ones. If one worker writes too many low-quality instances, we will remove her or him from our annotator pool. With such process, we finally accept around 30% submitted instances.", "publication_ref": ["b53", "b47", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "3.2", "text": "Step 2: Collecting Data for Subtask C Annotation Guidelines. To collect data for Subtask C, each worker is given a nonsensical statement s n and a sensical statement s c and asked to write a reason to explain why the nonsensical statement s n does not make sense. They shall follow the following rules: (1) Do not explain why the sensical statement s c makes sense. (2) Avoid mentioning the sensical statement s c . (3) Write the reason, rather than simply add the word \"not\" or \"can't\" to the nonsensical statement s n to form an explanation. (4) Write the reason, don't use patterns like \"XXX is not for YYY\" to create an explanation. (5) Do not try to justify why the nonsensical statement s n makes sense. (6) Write only one sentence, do not be overly formal. (7) Refrain from using \"because\" at the beginning of a sentence. (8) Do not try to correct the statement s n , but just give the reason. Quality Control. As the same as in Step 1, after the annotators write the reasons in Step 2, the first two authors of the paper perform the check process again. We reject low-quality reasons (that violate the rules significantly) and low-quality annotators (who write many low-quality reasons with the number above a threshold).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data Summary and Analysis", "text": "For SemEval-2020, we created 11,997 instances (i.e., 11,997 8-sentence tuples). We further split the instances into three subsets with 10,000 (the training set), 997 (the development set), and 1,000 (the test set) instances, respectively. We randomly assign the label of the correct options in subtask A and B to avoid unbalanced correct labels. We conduct three more data analysis experiments to evaluate data quality, including sentence length, common words and repetition.\nAverage Length. In  sensical statements and nonsensical statements almost have the same average lengths in the three sets (the differences are equal or smaller than 1%), which is balanced. However, there is an obvious gap between the correct reasons and confusing reasons in terms of the average lengths (roughly 4% in the training set and 10% in the dev/test set). Common Word Analysis. The most common words are important for showing the differences between sentences. We only present those words which have obvious different frequencies between sensical statements and nonsensical statements or between correct/referential reasons and confusing reasons. So, we skip most uninformative words, including 'a', 'an', 'the', 'to', 'in', 'on', 'of', 'for', 'and', 'is', 'are' and 'be'. After removing those words, we can list the top-5 common words in each type of sentence in the training/dev+test sets. For sensical statements s c and nonsensical statements s n , there are no significant differences between the training, dev, and test set. However, there is an obvious gap in the correct reasons o c and confusing reasons o n in negative words such as \"not\", \"no\", and \"cannot\". In the training data, negative words are about 3 times more common in the correct option o c than in the confusing options o n . In the dev+test data, the gap is about 40%, which indicates that the dev+test data has a higher quality than the training data. However, as discussed in (Niven and Kao, 2019), spurious statistical cues can affect BERT's results. We conjure that the negative words are also spurious effective clues, which make the Subtask B potentially easier. Repetition. The dev+test set have 12 instances (0.6%) that repeat the same nonsensical statements in the training data and 36 instances (1.8%) that repeat the same correct reasons with the training data.", "publication_ref": ["b33"], "figure_ref": [], "table_ref": []}, {"heading": "Cautions of using the data", "text": "The following advice is given to all task participants and future users: (1) Feel free to use whatever additional data they deem appropriate for the tasks to train their model. (2) Do not use the input of Subtask B/C to help Subtask A and do not use the option o of Subtask B to help Subtask C. Otherwise the task will be artificially easy. This is because of two reasons: a) The nonsensical statements s n of Subtask B and Subtask C is exactly the nonsensical statements s c of Subtask A and, participants can use the input of the Subtask B/C to directly obtain the answer of Subtask A and the option answers o of Subtask B will also reduce the difficulty of Subtask A; b) the correct reason o c of Subtask B is also one of the reference reason o c in Subtask C.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Systems and Results", "text": "In this section, we show the evaluation results of all the submitted systems for the three subtasks. Since most systems share similar model architecture for subtasks A and B, we discuss the two subtasks together.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Subtask A and Subtask B", "text": "The formal evaluation results of Subtask A and B are shown in Table 4 and 5. There are in total 39 valid submissions for Subtask A and 27 valid submissions for Subtask B. Most top-performing submissions Figure 1: The most commonly used model architectures used in the three subtasks. This figure is mostly based on Team Solomon's system. For Subtask B and C, the connector can be simply \"No, \", to help in constraining the model to learn a choice that explains the unreasonability of the statement. For Subtask A and B, the pretrained models are finetuned on the task-specific data with MLM-objective, and then trained as a binary classification task to score each input. For Subtask C, the cross-entropy loss of next-token-prediction is used to train the model, and beam search is used at inference. adopted the pretrained language models such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019c), XLNET (Yang et al., 2019) and ALBERT (Lan et al., 2019) as the encoder of the model, and then finetune on the training set of the task. See Figure 1 for the most commonly-used model architectures for Subtask A and B. Also, the top-performing systems take advantage of external knowledge graphs such as ConceptNet (Speer et al., 2017), or unstructured text containing commonsense knowledge. Below we introduce in detail several top-performing systems and their main features.\n\u2022 CN-HIT-IT.NLP  ranks top in Subtask A. They use a variant of K-BERT (Liu et al., 2019a) as the encoder to enhance language representations through knowledge graphs. K-BERT is a Transformer-based model, which enhances the language representations of the text by injecting relevant triples from a knowledge graph to form a knowledge-rich sentence tree, and then uses a mask-Transformer to make the triples visible only to the corresponding entity. They use ConceptNet as the commonsense repository to extract the triples for the statements.\n\u2022 ECNU-SenseMaker (Zhao et al., 2020)    \u2022 Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa.\n\u2022 BUT-FIT (Jon et al., 2020), LMVE , Lijunyi  use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system.\n\u2022 UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020) Table 6: Subtask C results of all the submitted systems. Those marked with * did not submit a system description paper, and those marked with + means they do not include Subtask C in their system description paper.\nIt can be seen from the results that pretrained language models such as RoBERTa can achieve rather high performance, e.g., the team Solomon achieves 96.0% and 94.0% on Subtask A and Subtask B, respectively, without using further resources. This shows that large-scale pretrained language models do contain commonsense knowledge to deal with the Subtask A and the Subtask B in this challenge. Additionally finetuning the pretrained language models on commonsense-related text such as OMCS, which we use as inspirational materials, can push the results even higher, close to human performance. The best-performing teams on Subtask A and Subtask B both adopt K-BERT, which incorporates the external knowledge base (i.e. ConceptNet) to complement the pretrained language models with knowledge triples. This shows that knowledge-graph-enhanced approaches, such as K-BERT can effectively incorporate external knowledge. However, the high number may also indicate data leaking to some extent, since in the data creation stage, both ConceptNet and OMCS are used as references for the annotator to write the data instances.", "publication_ref": ["b7", "b22", "b60", "b15", "b51", "b64", "b24", "b13", "b3", "b0", "b36"], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Subtask C", "text": "The results for Subtask C are shown in Table 6. There are in total 17 valid submissions for Subtask C. There are generally two approaches: (1) sequence-to-sequence approach, where the source side is the non-sensical statement, and the reason is the target sequence. (2) language model generation approach, which uses large-scale pretrained auto-regressive language models such as GPT-2 (Radford et al., 2019) for reason generation, where the non-sensical sentence acts as prompt. An example of the language model generation approach is shown in Figure 1, which is most commonly used and achieves relatively good results. Below we describe in detail the systems and their main features.\n\u2022 BUT-FIT (Jon et al., 2020) experiments with both the sequence-to-sequence approach and the language generation approach. For the sequence-to-sequence approach, they use BART (Lewis et al., 2019) with beam-search decoding to achieves the highest BLEU among all the teams. For the language generation approach, the nonsensical statement is used as a prompt. At the training stage, the statement and the explanation are concatenated together, and a GPT-2 is trained on these sequences with a next token prediction objective. At the test time, based on the statement, the model generates the reason tokens until the end-of-sentence token is generated.\n\u2022 KaLM (Wan and Huang, 2020) uses the sequence-to-sequence architecture BART. To enhance the source side statement, they extract keywords from the statement and search for evidence from Wiktionary. 2 After that, they concatenate the evidence along with the original statement as the source sentence for the generation. This approach proves effective and makes their system second-best for human evaluations.\n\u2022 ANA (Konar et al., 2020) has the highest human evaluation score with a multitask learning framework. Specifically, they use a decoder-only transformer based on GPT-2 as the backbone model, and train the model with two self-attention heads: one for language models and another for classification. They then use data from both task B and task C to calculate language model loss and classification loss. Furthermore, they use OMCS at the pretraining stage and use CoS-E (Rajani et al., 2019) and OpenBook (Mihaylov et al., 2018) at the task-specific training stage.\n\u2022 Solomon (Srivastava et al., 2020), JUSTers (Fadel et al., 2020), SWAGex (Rim and Okazaki, 2020), UI (Doxolodeo and Mahendra, 2020)  Large-scale pretrained language models such as BART and GPT-2 dominates the submissions. The two systems with the highest human evaluations, namely ANA and KaLM, use additional resources such as Wiktionary, OMCS, and other commonsense datasets. This again shows that additional knowledge from structured databases can help with the generation of the reasons. From Table 6 we can see that BLEU does not correlate well with Human Evaluation, especially for the top-performing systems. According to a further experiment of BUT-FIT, the naive baseline of \"copying source sentence as the reason\" can give a BLEU of 17.23, which can rank No. 4 among all the submissions. This indicates that BLEU, which focuses on the surface token overlap, has difficulty in evaluating the generated text reliably. The top-performed system achieves the human evaluation score of 2.10, showing the power of pretrained language models, but considering the human performance of 2.58, we still have a long way to go to generate human acceptable reasons.", "publication_ref": ["b39", "b13", "b17", "b14", "b28", "b52", "b9", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Commonsense reasoning in natural language has been studied in different forms of tasks and has recently attracted extensive attention. In the Winograd Schema Challenge (WSC) (Levesque et al., 2012;Morgenstern and Ortiz, 2015), a model needs to solve hard co-reference resolution problems based on commonsense knowledge. For example, \"The trophy would not fit in the brown suitcase because it was too big. What was too big (trophy or suitcase)?\" The Choice of Plausible Alternatives (COPA) (Roemmele et al., 2011) emphasizes on events and consequences. Each question in COPA aims to find the suitable cause or result of the premise from two given alternatives. All premises and alternatives are simple sentences. For example, the premise can be \"The man broke his toe. What was the CAUSE of this?\" and the two candidate answers are \"(1) He got a hole in his sock.\" and \"(2) He dropped a hammer on his foot.\" Several subsequent datasets are inspired by COPA. The JHU Ordinal Common-sense Inference (JOCI) (Zhang et al., 2017) aims to label the plausibility from 5 (very likely) to 1 (impossible) of human response after a particular situation. Situations with Adversarial Generations (SWAG) (Zellers et al., 2018) request a system to choose the most likely-to-happen alternative after a specific situation. Those datasets emphasize the pre-situations and/or the after-situations of certain situations, but not on the reasons why they occur or are caused. Besides, our dataset is not limited to events or situations. It concerns a broader commonsense setting, which includes events, descriptions, assertion etc. Some datasets are inspired by reading comprehension. The Story Cloze Test and ROCStories Corpora (Mostafazadeh et al., 2016;Sharma et al., 2018) aim to figure out the right ending from two candidate sentences after a four-sentence story. For a narrative text, MCScript (Ostermann et al., 2018a) gives various types of questions and pairs of answer candidates for each question. Most questions require knowledge beyond the facts mentioned in the text. Compared to those reading comprehension tasks, our benchmark encourages people to use any external resources they want. Some other datasets evolve from QA problems and care more about factual commonsense knowledge. SQUABU (Davis, 2016) provides a small hand-constructed test of commonsense and scientific questions. CommonsenseQA (Talmor et al., 2018) asks crowd workers to create questions from ConceptNet (Speer et al., 2017), which is a large graph of commonsense knowledge, where each question discriminates its answer candidates between three target concepts that all share the same relationship to a single source drawn from ConceptNet. OpenBookQA (Mihaylov et al., 2018) provides questions and answer candidates, as well as thousands of diverse facts about elementary level science that are related to the questions. The AI2 Reasoning Challenge (ARC)  gives thousands of questions with different knowledge types, as well as a relevant 14M-sentence corpus, mixed with science facts and other narrative sentences. MuTual provides a dataset for Multi-Turn dialogue reasoning in the commonsense area (Cui et al., 2020). Those questions are not easy to answer without specializing certain domain knowledge, while our questions are based on daily common sense.\nSome datasets focus on non-sentential eventual plausibility (Wang et al., 2018;Porada et al., 2019), such as \"gorilla-ride-camel\". In contrast, our dataset is based on statements which includes events, descriptions, assertion etc, not merely events, such as \"China's territory is larger than Japan's\". And some datasets concentrate on limited attributes or actions of world knowledge, such as physics (Forbes and Choi, 2017). Our dataset concerns general commonsense knowledge beyond just physical common sense, the sentence in our task \"Tom's mom become (happy)/(upset) when Tom gets high grades in the exam\" is about social and emotional common sense. For our first task, those statements that conforms to commonsense can also be phrased as being plausible. Thus our first task is similar to plausibility tests, despite that plausibility has a broader scope while our focus is on commonsense only.\nMore importantly, compared with our work, the above tasks do not directly estimate general common sense or ask the logical reasons behind the correct answers and questions. In recent years, some large-scale commonsense inference knowledge resources have been developed, which may be helpful in commonsense reasoning tasks. Atomic  presents a large-scale everyday commonsense knowledge graph, which has nine if-then relations with variables, including causes, effects, and so on. Event2Mind  proposes a new corpus and task, aiming to find out the mentioned/unmentioned people's intents and reactions under various daily circumstances. These datasets are not directly useful for our benchmark since they focus only on a small domain. ConceptNet is a seminal knowledge graph that has been upgraded over time (Liu and Singh, 2004;Havasi et al., 2007;Speer and Havasi, 2013;Speer et al., 2017). ConceptNet constructs triples using labeled edges as relations and various words and/or phrases as entities. It also has the sentences describing the corresponding triples. In contrast to these datasets, we investigate the evaluation of common sense, rather than building a resource.\nBefore organizing this shared-task, a pilot study (Wang et al., 2019) has been performed, showing that there is still a significant gap between human and machine performance when no training data is provided, despite that the models have already been pretrained with over 100 million natural language sentences. In our task here, we also provide training data with human annotations.", "publication_ref": ["b16", "b30", "b43", "b62", "b61", "b31", "b48", "b34", "b5", "b53", "b51", "b28", "b2", "b56", "b38", "b10", "b19", "b11", "b50", "b51", "b57"], "figure_ref": [], "table_ref": []}, {"heading": "Summary", "text": "This paper summarizes SemEval-2020 Task 4: Commonsense Validation and Explanation. In this task, we construct a dataset that consists of 11,997 instances and 83,986 sentences. The task attracted around 40 participating teams, out of which 31 teams submit their system papers. The pretrained models are shown to be very effective in Subtask A and Subtask B, but there is still a large room to improve system performances in Subtask C. Contextualized embedding such as RoBERTa and BART play a central role in the success of the top-performing models, demonstrating that such methods contain commonsense information to a good extent.\nWe attribute the high performance on Subtask A and B to several main reasons: 1) Subtask A is a relatively easy question by definition: a model needs only to detect a relatively less plausible content among the two candidate sentences. 2) Pretrained models are obtained on billion-words large corpora such as Wikipedia data, which help obtain commonsense knowledge (Zhou et al., 2019), which helps achieve considerably better performance. 3) As described in the annotation process, we use the sentences from OMCS to inspire crowd-sourcing workers. The top-3 systems also use OMCS, which potentially help them to attain better performances. 4) For Subtask B, as discussed in our data analysis section, the data has some flaws in the average length and common words, which reduces the difficulty. 5) Some instances have obvious patterns. For example, there are tens of instances that contain \"put XXX into YYY\", and \"XXX is bigger than YYY\", making the problems simpler. 6) Hundreds of crowd-sourcing workers write instances. It is likely for workers to think about the shared commonsense knowledge, such as \"XXX is bigger/shorter/quicker/slower than YYY\".\nWe consider future works in four directions: 1) We observe that there is still a gap between machine performance and human performance in Subtask C, and the reason generation task still needs further investigation. 2) The artifacts or spurious correlations in the datasets can be further removed, e.g., by making different candidate sentences in subtask B be the same, removing instances with shared commonsense knowledge, removing artifacts in common words, and filtering out common patterns. 3) Subtask A can be turned into a more difficult form. Instead of comparing which statement makes more sense, we can form it into a classification task, validating if one statement makes sense or not. 4) We notice that the BLEU score does not closely align with human evaluation for systems with high performances, and it is desirable to develop an auto-metric for comparing the semantic correlation between two reasons.", "publication_ref": ["b65"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This work is supported by the National Science Foundation of China (Grant No. 61976180), the Westlake University, and the Bright Dream Joint Institute for Intelligent Robotics. The research of Xiaodan Zhu is supported by NSERC. Yue Zhang is the corresponding author.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Deepyang at semeval-2020 task 4: Using the hidden layer of bert model for differentiating common sense", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Yang Bai; Xiaobing Zhou"}, {"ref_id": "b1", "title": "Think you have solved question answering? try arc, the AI2 reasoning challenge", "journal": "CoRR", "year": "2018", "authors": "Peter Clark; Isaac Cowhey; Oren Etzioni; Tushar Khot; Ashish Sabharwal; Carissa Schoenick; Oyvind Tafjord"}, {"ref_id": "b2", "title": "Mutual: A dataset for multi-turn dialogue reasoning", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Leyang Cui; Yu Wu; Shujie Liu; Yue Zhang; Ming Zhou"}, {"ref_id": "b3", "title": "Uaics at semeval-2020 task 4 -using a bidirectional transformer for task a", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Ciprian-Gabriel Cusmuliuc; Lucia-Georgiana Coca; Adrian Iftene"}, {"ref_id": "b4", "title": "Cs-net at semeval-2020 task 4: Siamese bert for comve", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Sandeep Ranjan Soumya Dash; Prateek Routray; Ashutosh Varshney;  Modi"}, {"ref_id": "b5", "title": "How to write science questions that are easy for people and hard for computers", "journal": "AI Magazine", "year": "2016", "authors": "Ernest Davis"}, {"ref_id": "b6", "title": "Logical formalizations of commonsense reasoning: a survey", "journal": "Journal of Artificial Intelligence Research", "year": "2017", "authors": "Ernest Davis"}, {"ref_id": "b7", "title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2018", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b8", "title": "Ui at sem-eval 2020 task 4 : Commonsense validation and explanationby exploiting contradiction", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Kerenza Doxolodeo; Rahmad Mahendra"}, {"ref_id": "b9", "title": "Justers at semeval-2020 task 4: Evaluating transformer models against commonsense validation and explanation", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Ali Fadel; Mahmoud Al-Ayyoub; Erik Cambria"}, {"ref_id": "b10", "title": "Verb physics: Relative physical knowledge of actions and objects", "journal": "", "year": "2017", "authors": "Maxwell Forbes; Yejin Choi"}, {"ref_id": "b11", "title": "Conceptnet 3: a flexible, multilingual semantic network for common sense knowledge", "journal": "Citeseer", "year": "2007", "authors": "Catherine Havasi; Robert Speer; Jason Alonso"}, {"ref_id": "b12", "title": "Open mind common sense: Crowd-sourcing for common sense", "journal": "", "year": "2010", "authors": "Catherine Havasi; Robert Speer; Kenneth Arnold; Henry Lieberman; Jason Alonso; Jesse Moeller"}, {"ref_id": "b13", "title": "But-fit at semeval-2020 task 4: Multilingual commonsense", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Josef Jon; Martin Fajcik; Martin Docekal; Pavel Smrz"}, {"ref_id": "b14", "title": "Ana at semeval-2020 task 4: multitask learning for commonsense reasoning (union)", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Anandh Konar; Chenyang Huang; Amine Trabelsi; Osmar Zaiane"}, {"ref_id": "b15", "title": "Albert: A lite bert for self-supervised learning of language representations", "journal": "", "year": "2019", "authors": "Zhenzhong Lan; Mingda Chen; Sebastian Goodman; Kevin Gimpel; Piyush Sharma; Radu Soricut"}, {"ref_id": "b16", "title": "The winograd schema challenge", "journal": "AAAI Press", "year": "2012", "authors": "Hector J Levesque; Ernest Davis; Leora Morgenstern"}, {"ref_id": "b17", "title": "Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension", "journal": "", "year": "2019", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal; Marjan Ghazvininejad; Abdelrahman Mohamed; Omer Levy; Ves Stoyanov; Luke Zettlemoyer"}, {"ref_id": "b18", "title": "Lijunyi at semeval-2020 task 4: An albert model based on different training sizes and depths for commonsense validation and explanation", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Junyi Li; Wang ; Haiyan Ding"}, {"ref_id": "b19", "title": "Conceptnet-a practical commonsense reasoning tool-kit", "journal": "BT technology journal", "year": "2004", "authors": "Hugo Liu; Push Singh"}, {"ref_id": "b20", "title": "Qi Ju, Haotang Deng, and Ping Wang. 2019a. K-bert: Enabling language representation with knowledge graph", "journal": "", "year": "", "authors": "Weijie Liu; Peng Zhou; Zhe Zhao; Zhiruo Wang"}, {"ref_id": "b21", "title": "Multi-task deep neural networks for natural language understanding", "journal": "", "year": "2019", "authors": "Xiaodong Liu; Pengcheng He; Weizhu Chen; Jianfeng Gao"}, {"ref_id": "b22", "title": "Roberta: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b23", "title": "Lmve at semeval-2020 task 4: Commonsense validation and explanation using pretraining language model", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Shilei Liu; Yu Guo; Bochao Li; Feiliang Ren"}, {"ref_id": "b24", "title": "Qiaoning at semeval-2020 task 4: Commonsense validation and explanation system based on ensemble of language model", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Pai Liu"}, {"ref_id": "b25", "title": "Masked reasoner at semeval-2020 task 4: Fine-tuning roberta for commonsense reasoning", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Daming Lu"}, {"ref_id": "b26", "title": "Uor at semeval-2020 task 4: Pre-trained sentence transformer models for commonsense validation and explanation", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Thanet Markchom; Bhuvana Dhruva; Chandresh Pravin; Huizhi Liang"}, {"ref_id": "b27", "title": "Kde senseforce", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Khanddorj Mendbayar; Masaki Aono"}, {"ref_id": "b28", "title": "Can a suit of armor conduct electricity? a new dataset for open book question answering", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Todor Mihaylov; Peter Clark; Tushar Khot; Ashish Sabharwal"}, {"ref_id": "b29", "title": "Teamjust at semeval-2020 task 4: Commonsense validation and explanation using ensembling techniques", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Roweida Mohammed; Malak Abdullah"}, {"ref_id": "b30", "title": "The winograd schema challenge: Evaluating progress in commonsense reasoning", "journal": "AAAI Press", "year": "2015", "authors": "Leora Morgenstern; Charles L Ortiz"}, {"ref_id": "b31", "title": "A corpus and cloze evaluation for deeper understanding of commonsense stories", "journal": "", "year": "2016", "authors": "Nasrin Mostafazadeh; Nathanael Chambers; Xiaodong He; Devi Parikh; Dhruv Batra; Lucy Vanderwende; Pushmeet Kohli; James F Allen"}, {"ref_id": "b32", "title": "Jbnu at semeval-2020 task 4: Bert and unilm for commonsense validation and explanation", "journal": "", "year": "2020", "authors": "Jong-Hyeon Seung-Hoon Na;  Lee"}, {"ref_id": "b33", "title": "Probing neural network comprehension of natural language arguments", "journal": "", "year": "2019", "authors": "Timothy Niven; Hung-Yu Kao"}, {"ref_id": "b34", "title": "Mcscript: A novel dataset for assessing machine comprehension using script knowledge", "journal": "", "year": "2018", "authors": "Simon Ostermann; Ashutosh Modi; Michael Roth; Stefan Thater; Manfred Pinkal"}, {"ref_id": "b35", "title": "Semeval-2018 task 11: Machine comprehension using commonsense knowledge", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Simon Ostermann; Michael Roth; Ashutosh Modi; Stefan Thater; Manfred Pinkal"}, {"ref_id": "b36", "title": "Ynu-oxz at semeval-2020 task 4: Commonsense validation using bert with bidirectional gru", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Xiaozhi Ou; Xiaobing Zhou;  Li"}, {"ref_id": "b37", "title": "Bleu: a method for automatic evaluation of machine translation", "journal": "Association for Computational Linguistics", "year": "2002-07", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"ref_id": "b38", "title": "Can a gorilla ride a camel? learning semantic plausibility from text", "journal": "", "year": "2019", "authors": "Ian Porada; Kaheer Suleman; Jackie Chi Kit Cheung"}, {"ref_id": "b39", "title": "Language models are unsupervised multitask learners", "journal": "", "year": "2019", "authors": "Alec Radford; Jeff Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"ref_id": "b40", "title": "Explain yourself! leveraging language models for commonsense reasoning", "journal": "", "year": "2019", "authors": "Bryan Nazneen Fatema Rajani; Caiming Mccann; Richard Xiong;  Socher"}, {"ref_id": "b41", "title": "Event2mind: Commonsense inference on events, intents, and reactions", "journal": "Long Papers", "year": "2018", "authors": "Maarten Hannah Rashkin; Emily Sap; Noah A Allaway; Yejin Smith;  Choi"}, {"ref_id": "b42", "title": "Swagex at semeval-2020 task 4: Commonsense explanation as next event prediction", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Wiem Ben Rim; Naoaki Okazaki"}, {"ref_id": "b43", "title": "Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning", "journal": "", "year": "2011-03", "authors": "Melissa Roemmele; Andrew S Cosmin Adrian Bejan;  Gordon"}, {"ref_id": "b44", "title": "Ssn-nlp at semeval-2020 task 4: Text classification and generation on common sense context using neural networks", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "S Kayalvizhi"}, {"ref_id": "b45", "title": "Cs-nlp team at semeval-2020 task 4: Evaluation of state-of-the-art nlp deep learning architectures on commonsense reasoning task", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Sirwe Saeedi; Aliakbar Panahi; Seyran Saeedi; Alvis C Fong"}, {"ref_id": "b46", "title": "ATOMIC: an atlas of machine commonsense for if-then reasoning", "journal": "CoRR", "year": "2018", "authors": "Maarten Sap; Ronan Lebras; Emily Allaway; Chandra Bhagavatula; Nicholas Lourie; Hannah Rashkin; Brendan Roof; Noah A Smith; Yejin Choi"}, {"ref_id": "b47", "title": "The effect of different writing tasks on linguistic style: A case study of the roc story cloze task", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Roy Schwartz; Maarten Sap; Ioannis Konstas; Leila Zilles; Yejin Choi; Noah A Smith"}, {"ref_id": "b48", "title": "Tackling the story ending biases in the story cloze test", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Rishi Sharma; James Allen; Omid Bakhshandeh; Nasrin Mostafazadeh"}, {"ref_id": "b49", "title": "Open mind common sense: Knowledge acquisition from the general public", "journal": "Springer", "year": "2002", "authors": "Push Singh; Thomas Lin; Erik T Mueller; Grace Lim; Travell Perkins; Wan Li Zhu"}, {"ref_id": "b50", "title": "Conceptnet 5: A large semantic network for relational knowledge", "journal": "Springer", "year": "2013", "authors": "Robert Speer; Catherine Havasi"}, {"ref_id": "b51", "title": "Conceptnet 5.5: An open multilingual graph of general knowledge", "journal": "", "year": "2017", "authors": "Robyn Speer; Joshua Chin; Catherine Havasi"}, {"ref_id": "b52", "title": "Team solomon at semeval-2020 task 4: Be reasonable: Exploiting large-scale language models for commonsense reasoning", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Vertika Srivastava; Sudeep Sahoo; Yeon Kumar;  Kim; Rohit R R Hyang; Mayank Raj; Ajay Jaiswal "}, {"ref_id": "b53", "title": "Commonsenseqa: A question answering challenge targeting commonsense knowledge", "journal": "CoRR", "year": "2018", "authors": "Alon Talmor; Jonathan Herzig; Nicholas Lourie; Jonathan Berant"}, {"ref_id": "b54", "title": "Tr at semeval-2020 task 4: Exploring the limits of language-model-based common sense validation", "journal": "", "year": "2020", "authors": "Don Teo"}, {"ref_id": "b55", "title": "Kalm at semeval-2020 task 4: Knowledge-aware language models for comprehension and generation", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Jiajing Wan; Xinting Huang"}, {"ref_id": "b56", "title": "Modeling semantic plausibility by injecting world knowledge", "journal": "", "year": "2018", "authors": "Su Wang; Greg Durrett; Katrin Erk"}, {"ref_id": "b57", "title": "Does it make sense? and why? a pilot study for sense making and explanation", "journal": "Association for Computational Linguistics", "year": "2019-07", "authors": "Cunxiang Wang; Shuailong Liang; Yue Zhang; Xiaonan Li; Tian Gao"}, {"ref_id": "b58", "title": "Cuhk at semeval-2020 task 4: Commonsense explanation, reasoning and prediction with multi-task learning", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Hongru Wang; Xiangru Tang; Sunny Lai; Sak Kwong Leung"}, {"ref_id": "b59", "title": "Iie-nlp-nut at semeval-2020 task 4: Guiding plm with prompt template reconstruction strategy for comve", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Luxi Xing; Yuqiang Xie; Yue Hu; Wei Peng"}, {"ref_id": "b60", "title": "Xlnet: Generalized autoregressive pretraining for language understanding", "journal": "", "year": "2019", "authors": "Zhilin Yang; Zihang Dai; Yiming Yang; Jaime Carbonell; R Russ; Quoc V Salakhutdinov;  Le"}, {"ref_id": "b61", "title": "Swag: A large-scale adversarial dataset for grounded commonsense inference", "journal": "EMNLP", "year": "2018", "authors": "Rowan Zellers; Yonatan Bisk; Roy Schwartz; Yejin Choi"}, {"ref_id": "b62", "title": "Ordinal common-sense inference", "journal": "Transactions of the Association for Computational Linguistics", "year": "2017", "authors": "Sheng Zhang; Rachel Rudinger; Kevin Duh; Benjamin Van Durme"}, {"ref_id": "b63", "title": "Cn-hit-it.nlp at semeval-2020 task 4: Enhanced language representation with multiple knowledge triples", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Yice Zhang; Jiaxuan Lin; Yang Fan; Peng Jin; Yuanchao Liu; Bingquan Liu"}, {"ref_id": "b64", "title": "Ecnu-sensemaker at semeval-2020 task 4: Leveraging heterogeneousknowledge resources for commonsense validation and explanation", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Qian Zhao; Siyu Tao; Jie Zhou; Linlin Wang; Xin Lin"}, {"ref_id": "b65", "title": "Evaluating commonsense in pre-trained language models", "journal": "", "year": "2019", "authors": "Xuhui Zhou; Yue Zhang; Leyang Cui; Dandan Huang"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "and CUHK (Wang et al., 2020) use GPT or GPT-2 finetuned on the task training data. JBNU (Na and Lee, 2020) uses UniLM, which incorporates three LM tasks: unidirectional LM, bidirectional LM and sequence-to-sequence prediction LM, and only use one of the reference correct reasons. UI does not use the training data and treats the generation as a Cloze task. SSN-NLP (S, 2020) uses the seq2seq NMT framework without a pretrained LM.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Average length of different types of sentences of Training/Dev/Test set", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": ", we present the average length of each type of sentence in the training/dev/test set. The sentences in the development and test set have shorter lengths than those in the training set. This is because we check the development and test more carefully and more strictly, thus removing longer and more incomprehensible instances, which lowers the average lengths of the dev/test set. The", "figure_data": "Types of SentencesWord:Word Frequency(\u2030)Sensical Statementscan:1.54his:1.45with:1.285my:1.247 people:0.912Nonsensical Statements can:1.604 his:1.411with:1.299my:1.254 people:0.873Correct Reasonsnot:4.545 cannot:1.731 people:1.579 can:1.389no:1.337Confusing Reasonscan:2.095 people:1.671not:1.49have:1.152than:0.81Referential Reasons not:5.711 cannot:1.65can:1.09people:1.088 have:0.978(a) Training setTypes of SentencesWord:Word Frequency(\u2030)Sensical Statements can:1.343 my:1.322his:1.291put:1.259with:0.978Nonsensical Statements can:1.458 my:1.335put:1.304his:1.15with:0.975Correct Reasonsnot:2.325 can:1.709no:1.614 people:1.362 than:1.31Confusing Reasonscan:2.332 people:1.619 not:1.609have:1.094than:1.079Referential Reasons not:4.264 cannot:1.212 can:1.203it:1.179have:1.093(b) Dev+Test set"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Top-5 common words and their frequencies in different types of sentences in the training and dev+test set. 1.000\u2030 means this word appear once in every 1000 words.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "ranks top in Subtask B. They use Knowledge-enhanced Graph Attention Network to leverage heterogeneous knowledge from both the structured knowledge base (i.e. ConceptNet) and the unstructured text to better improve the commonsense understanding.", "figure_data": "TeamAcc. Rank TeamAcc. Rank TeamAcc. RankHuman99.1-CN-HIT-IT.NLP97.01panaali*92.514Lijunyi83.027ECNU-SenseMaker 96.72ZhengxianFan* 92.415ehsantaher*82.528IIE-NLP-NUT96.43LMVE90.416TakeLab*81.229nlpx*96.43Warren*90.416Vicki*79.830Solomon96.05TMLab*89.218TR79.731Qiaoning95.96UAICS89.119KDE SenseForce79.632BUT-FIT95.87JUST89.119Hitachi*78.433olenet*95.58eggy*89.021CUHK72.434KaLM95.39UI88.222paramitamirza*69.235CS-NET94.810Armins*87.123UoR67.636fkerem*94.411DEEPYANG85.124chenggguang*62.337JUSTers92.912WUY*84.225praveenjoshi007* 55.938CS-NLP92.713YNU-oxz83.626dania*21.639"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Subtask A results of all the submitted systems. Those marked with * did not submit system description paper. Human performance are based on the trial data instead of the test data used by the participating systems.", "figure_data": "TeamAcc. Rank TeamAcc. Rank TeamAcc. RankHuman97.8-ECNU-SenseMaker 95.01JBNU91.410Masked Reasoner 73.519CN-HIT-IT.NLP94.82Qiaoning90.811KDE SenseForce72.820IIE-NLP-NUT94.33CS-NET89.012SSN-NLP68.321Solomon94.04WUY*85.313TakeLab*66.822LMVE93.85SWAGex84.614UoR65.923CS-NLP93.76TMLab*82.015dania*55.524KaLM93.27UI80.516CUHK51.225BUT-FIT93.18ehsantaher* 79.317bhu*36.426JUSTers92.39uzh*75.818praveenjoshi007* 32.627"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "{s 1 , s 2 , o 1 , o 2 , o 3 , r 1 , r 2 , r 3 }. s 1", "formula_coordinates": [2.0, 382.5, 277.29, 143.4, 10.63]}, {"formula_id": "formula_1", "formula_text": "score k = 100 i=1 3 j=1 score ijk 100 * 3 ,(1)", "formula_coordinates": [3.0, 226.91, 348.43, 298.63, 28.02]}, {"formula_id": "formula_2", "formula_text": "{s 1 , s 2 , o 1 , o 2 , o 3 , r 1 , r 2 , r 3 }. Note that one sentence in o 1 , o 2 , o 3 is repeated in r 1 , r 2 , r 3", "formula_coordinates": [3.0, 72.0, 607.21, 453.54, 24.18]}], "doi": ""}