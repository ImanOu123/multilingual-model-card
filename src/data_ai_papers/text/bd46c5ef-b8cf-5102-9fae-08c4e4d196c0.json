{"title": "Improving Training Stability for Multitask Ranking Models in Recommender Systems", "authors": "Jiaxi Tang; Yoel Drori; Daryl Chang; Maheswaran Sathiamoorthy; Justin Gilmer; Wei Li; Xinyang Yi; Lichan Hong; Ed H Chi; Li Wei; Ed H Chi 2023 Hong;  Improving", "pub_date": "2023-06-15", "abstract": "Recommender systems play an important role in many content platforms. While most recommendation research is dedicated to designing better models to improve user experience, we found that research on stabilizing the training for such models is severely under-explored. As recommendation models become larger and more sophisticated, they are more susceptible to training instability issues, i.e., loss divergence, which can make the model unusable, waste significant resources and block model developments. In this paper, we share our findings and best practices we learned for improving the training stability of a real-world multitask ranking model for YouTube recommendations. We show some properties of the model that lead to unstable training and conjecture on the causes. Furthermore, based on our observations of training dynamics near the point of training instability, we hypothesize why existing solutions would fail, and propose a new algorithm to mitigate the limitations of existing solutions. Our experiments on YouTube production dataset show the proposed algorithm can significantly improve training stability while not compromising convergence, comparing with several commonly used baseline methods. We open source our implementation at https:", "sections": [{"heading": "INTRODUCTION", "text": "A good recommender system plays a key factor to user experience. It has become a core technology and even a main user interface in many web applications, including YouTube, one of the largest online video platforms in the world. As a result, many components can be incorporated into recommendation models to capture contexts with different modalities and improve recommendation quality, including audio signals [30], video signals [21], user history sequence [5,28], etc. Besides, the scaling law of recommendation models [3] suggests substantial quality improvements by increasing model capacity in data-rich applications.\nAs recommendation models become larger and more sophisticated, they are more susceptible to training instability issues [14], i.e., the loss diverges (instead of converging), causing the model to be \"broken\" and completely useless. In industry, serving such a \"broken\" model leads to catastrophic user experience (see Section 2.2). Moreover, if we cannot ensure reliable training of recommendation models, a huge amount of resources can be wasted and model development can be blocked. Therefore, we couldn't emphasize more on how essential training stability is. However, very sparse research has been done on the training stability of recommendation models.\nOn one hand, there's a lack of fundamental understanding of why recommendation models are prone to training instability issues. In particular, we observe that ranking models with multiple objectives are more likely to encounter problems than retrieval models with a single objective (e.g. Softmax Cross-Entropy over large output space). In addition to increasing model complexity, we found that simply adding new input features or output tasks can also cause training unstable. To deal with this problem, people mostly rely on empirical solutions and sometimes on luck (when the problem occurs randomly). Developing a fundamental understanding of what causes the problem would allow people to navigate the process more confidently.\nOn the other hand, we found that there's a lack of effective approaches to largely mitigate the training instability problem. There are some widely used methods, such as activation clipping [20], gradient clipping [7,24], learning rate warmup [12,14], and layer normalization [4]. But in practice, we found that these approaches were ad hoc and couldn't completely prevent training instability in our model. Developing an effective method that can significantly improve model training stability accelerates model improvements by addressing concerns about training problems.\nThe focus of this paper is to share the lessons learned from addressing the training instability problems experienced by a multitask ranking model used in production for YouTube recommendations. In Section 2, we show some implications and consequences of unstable model training in real-world recommender systems, suggesting the importance and difficulties of considerably improving model training stability. After introducing some preliminary basics of our model in Section 3, we present some case studies about changes that had led to more training instability problems and provide our understanding on the root cause of the problems. In practice, however, we've found that there's a big gap between knowing the root cause and having an effective solution. Some methods that are supposed to be effective do not work well empirically. Next, in Section 4, we closely examine the training dynamics of our model, which inspired us to propose a more effective approach to overcome the limitations in existing methods. The empirical evidence on a YouTube dataset in Section 5 reveals the effectiveness of the proposed method for improving model training stability, especially when increasing model capacity and using a large learning rate for faster convergence. We hope that these findings can help the community better understand the training instability problem and effectively solve it.", "publication_ref": ["b29", "b20", "b4", "b27", "b2", "b13", "b19", "b6", "b23", "b11", "b13", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "BACKGROUND AND RELATED WORK 2.1 Symptoms", "text": "Training instability is a model property that measures the unsteadiness of model training. It has a common symptom of loss divergence (a.k.a loss blow-up). Based on our observations, we further categorize loss divergence into two types: micro-divergence and full divergence. When a model's loss micro-diverges (see model-a in Figure 1 as an example), we can observe a sudden jump in training loss and a sudden drop in training metrics, although the loss may recover to normal (as shown in the example) as training continues. Usually, we don't need to worry too much about this situation, because the recovered model can have a quality on-par with models that don't suffer from loss divergence. However, if a model's loss fully diverges (see model-b in Figure 1 as an example), we can see that the training loss becomes very high in a few number of training steps, and all training metrics become extremely bad. For example, the binary classification AUC (the metric we mainly look throughout the paper) drops to 0.5 as shown in Figure 1, suggesting the model becomes completely useless, practically giving random results. What's worse, the fully diverged loss cannot recover to its pre-divergence value as training continues.", "publication_ref": [], "figure_ref": ["fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Motivation and Challenges", "text": "We motivate the importance of training stability research, especially for recommender systems in industry, from several aspects. First, the problem of loss divergence, once it occurs regularly, can affect almost all types of model development. This includes, but is not limited to:\n(1) Increasing model complexity: As more modeling techniques are applied and more components are added to the recommendation model (to improve its quality), there's a greater chance that the model will suffer from loss divergence problems. Even simply enlarging the model capacity could put the model in a dangerous state, despite the great benefits in data-rich environments suggested by current scaling laws [3]. (2) Adding more input features or tasks: Typically, the ranking model in a recommendation system uses many input features for multiple tasks [36]. A combination of predictions on all tasks is used to decide the ranking of a candidate item. We found that both adding new input features and adding new tasks can lead to training instability, although they are common ways to improve model quality. (3) Increasing convergence speed: We have found that hyperparameter tuning that facilitate model convergence (such as increasing the learning rate) can significantly increase the likelihood of loss divergence. This forces model designers to use a smaller learning rate which results in slower convergence.\nSecond, as training complex models requires large amounts of resources, loss divergence problems, which block the model from completing their training, waste training resources. Moreover, inadvertently deploying a \"broken\" model for serving also leads to catastrophic user experience. Consequently, we've seen many efforts on alleviating this problem from an engineering perspective, such as ensuring model quality before serving. Nevertheless, given that engineering efforts cannot prevent training instability from occurring, it is clear that drastically improving model training stability is the right path to pursue in the long run.\nIn dealing with the problem of model instability, we experienced the following challenges. ", "publication_ref": ["b2", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Model training stability has been an under-explored research area, not only for recommendation models, but also in general machine learning. Fortunately, with the increasing trend of large models [8,11,29], stabilizing model training has become an emerging research area and attracts more attention in recent years.\nFrom the perspective of optimization theory, Wu et al. [32] first theoretically predicted the training instability for quadratic models with learning rate and the \"sharpness\" (measured by the maximum eigenvalue of the loss Hessian) of the loss curvature. For deep neural networks, Cohen et al. [12], Gilmer et al. [14] confirmed that this prediction is still accurate enough.\nIn terms of techniques, there are some methods widely used in language and vision models, such as activation clipping [20], gradient clipping [24], learning rate warmup [16], and various normalization techniques [4,18]. In addition, You et al. [34] proposed a new optimizer that achieves a better trade-off between convergence and stability for large batch-size training. Brock et al. [7] developed Adaptive Gradient Clipping to improve the stability of ResNet models [17] without Batch Normalization [18].\nHowever, empirically, we found these approaches not effective enough to completely prevent our model from training instability (See Section 5). This may due to some unique properties of the recommendation model. As will be discussed in the next section, these  properties can make multi-task ranking models more susceptible to training instability problems.", "publication_ref": ["b7", "b10", "b28", "b31", "b11", "b13", "b19", "b23", "b15", "b3", "b17", "b33", "b6", "b16", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "UNDERSTANDING THE CAUSE OF THE ISSUE", "text": "In this section, we first describe the model to be studied in this paper and its characteristics. Then, we share our understanding on the root cause of the training instability problems that happened in our model.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model Definition", "text": "YouTube's video recommendation system uses multiple candidate generation algorithms to retrieve a few hundred candidates. This is followed by a ranking system which generates a ranked list from these candidates. This paper mainly focuses on the ranking models in YouTube's recommender system. Different from candidate generation models (a.k.a retrieval models), which are responsible for filtering out the majority of irrelevant items, ranking models aim to provide a ranked list so that items with the highest utility to users are displayed at the top. Therefore, ranking models use more advanced machine learning techniques with more expensive features to have sufficient model expressiveness for learning the association of features and their relationship with utility. Figure 2 depicts a general architecture of the ranking model that we want to study throughout the paper. Below we summarize some important features for our ranking model and how it is trained; one can refer to [36] for more details.\n\u2022 Multitask: As shown in Figure 2, the ranking model has multiple tasks that predict multiple labels. These predictions are combined to form the final ranked list of items. Regardless of different modeling choices [9,22], there are some hidden layers in the middle of the model that are shared by these tasks (either fully shared or softly shared). training. This training scheme has been widely used and is known to be beneficial for many aspects of recommendation quality [2,23,36]. \u2022 Optimization: Large batch-size training is known to have less noise in gradients and thus optimization is more curvature driven [2,34]. We adopt a large batch size with a high learning rate for faster convergence. We found Adagrad [13] to be strong in our case, despite many advances in optimizers (e.g., Adam [19], Adafactor [26]).", "publication_ref": ["b35", "b8", "b21", "b1", "b22", "b35", "b1", "b33", "b12", "b18", "b25"], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "Root Cause and Case Studies", "text": "Regardless of the types of loss divergence, we believe that the intrinsic cause can be summarized as \"step size being too large when loss curvature is steep\". Once a model meets both conditions at a given state, a divergence can easily occur. Intuitively, the step size should be conservative at a steep loss surface (measured by the maximum eigenvalue of the loss Hessian) to ensure that loss decreases instead of increases. For quadratic models, Wu et al. [32] theoretically proves the above argument and suggests 2/ > * to make training stable, where is the learning rate and * is the maximum eigenvalue of the loss Hessian. Cohen et al. [12] gives a nice and straightforward example (in Figure 3) for the proof. For neural networks, this argument still mostly holds [12,14].\nKnowing the root cause of the training instability problem allows us to answer the following research questions: RQ1: Why do recommendation models in general have worse training stability than models in other domains? RQ2: Within recommendation models, why do ranking models typically have worse training stability than retrieval models?\nWe relate the answers to these questions to the following unique properties of our models. Please refer to some empirical evidence in Section A.1 in Supplementary Material.\n\u2022 Data distribution changes (RQ1): Compared to models in other domains, recommendation models use several orders of magnitude more input features (hundreds to thousands). What's worse, with sequential training, the distribution of these input features (and labels) keeps changing. We think a steeper loss curvature can occur when data distribution sudden change, which happens regularly. Also, a model with sequential training will never converge as it has to adapt to newly arriving data points with a changed distribution. Thus, a large learning rate is required to make the adaptation efficient enough. In summary, compared to models in other domains that are trained on a fixed dataset, changes in the training data distribution pose greater challenges for stabilizing the training of recommendation models. \u2022 Larger model size and complexity (RQ2): Compared to retrieval models used for candidate generation, ranking models are usually much larger in capacity to accurately measure the utility of candidate items. With the recent developments of ML hardware (e.g., TPUs), we are able to significantly increase the model size for quality improvements [3]. The empirical studies from Gilmer et al. [14] suggested the increased model capacity and complexity is a contributing factor to steeper loss curvature. \u2022 Multiple objectives vs. Single objective (RQ2): Compared to retrieval models which usually has a single objective (e.g. Softmax Cross-Entropy) [33], ranking models often need to optimize for many objectives at the same time [36]. This causes ranking models to suffer from loss divergence much more easily. Because if there are spurious gradients caused by bad predictions from a particular task, the gradients can backpropagate throughout the model, causing the layers that are shared by multiple tasks to behave (slightly) abnormally. But since the layers are shared by different tasks, other tasks tend to predict irregular values afterwards, reinforcing the instability to a nonrecoverable state. In other words, shared layers (as well as embeddings) can be a double-edged swordthey allow transfer learning from different tasks, but can also exacerbate the training instability problem, making ranking models more vulnerable than retrieval models.\nDespite the recent advances in understanding the root causes of divergence issues, we have found a large gap remains between our current understanding on the cause of the issue and having an effective solution. We have tried many temporary fixes. Some examples are: (1) Using even slower learning rate warmup schedule to pass the initial model state where loss curvature is steep [14]. (2) Enlarging the sequential training moving window to make training data distribution changes smoother. These fixes indeed mitigated training instability issues for a while, but when our model became more complex, loss divergence happened again. After trying many ad-hoc fixes, we believe developing a more principled way that can significantly improve model stability is the long-term solution.", "publication_ref": ["b31", "b11", "b11", "b13", "b2", "b13", "b32", "b35", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "EFFECTIVE METHOD FOR IMPROVING TRAINING STABILITY", "text": "In this section, we first introduce the general direction (Gradient Clipping) for controlling the effective step size while loss curvature  When checking some statistics from the top hidden layer of the model, we found that GC and AGC failed to provide small enough clipping factor. While Clippy's clipping factor can be 2-orders of magnitude smaller than GC and AGC. Section B in Supplementary Material has the statistics for other layers.\nis steep, by presenting some classical methods on this direction, accompanied with notations and denotations. Despite being successful when applied in other domains, we found these classical methods are not effective enough when applied in our model. Based on some observations of training dynamics in our model, we propose a new method and explain why it can be more effective for improving the training stability. We first describe Adagrad [13], the optimizer used in our model. In Adagrad, model parameters are updated by the rule\n= \u22121 + 2 , = \u2022 \u22121/2 , +1 = \u2212 \u2022 ,(1)\nwhere denotes the learning rate at step , is the standard stochastic gradient of the empirical loss with respect to the model parameters and , known as \"accumulator\", is a vector initialized to some small constant value, typically 0.1. In addition, all powers operations are computed element-wise.\nAs mentioned in Section 3, we desire a more principled approach to control the step size when loss curvature is steep. However, the loss curvature measured by the eigenvalue of loss Hessian is very expensive to compute during training. Fortunately, the first-order gradients can be used as a surrogate for the Hessian (c.f. [35]). Consequently, gradient clipping based algorithms become very popular to improve training stability and used in many large models [8,11,29].\nGradient Clipping. Proposed by Pascanu et al. [24], Gradient Clipping (GC) limits the magnitude of gradient (measured by its norm) before applying it to the model. In other words, as gradient magnitude becomes large (loss curvature becomes steeper), Gradient Clipping controls the \"effective step size\" to stabilize model training.\nFormally, Gradient Clipping algorithm clips the gradients (before applying Adagrad update in equation 1) as:\n\u2192 \u2225 \u2225 if \u2225 \u2225 \u2265 , else. Or \u2192 \u2022 , where = min{ \u2225 \u2225 , 1.0} (2)\nThe clipping threshold is a hyperparameter that controls the maximum allowable gradient norm \u2225 \u2225. In other words, if the model gradient has a large magnitude at step , GC will clip its norm to by rescaling gradients with a scalar clipping factor \u2208 R + . In practice, Frobenius norm (or 2 norm) \u2225.\u2225 2 is a common choice for vector norm, and clipping is often applied to each layer independently of the other layers.\nAdaptive Gradient Clipping. Empirically, although GC can improve training stability of the model, training stability is extremely sensitive to the choice of the clipping threshold , requiring finegrained tuning for different layers. What's worse, the threshold need to be re-tuned when model structure, batch size, or learning rate is changed.\nTo overcome this burden, Brock et al. [7] proposed Adaptive Gradient Clipping (AGC). AGC is motivated by the observation that the ratio of the norm of the gradients \u2225 \u2225 to the norm of the model parameters \u2225 \u2225 should not be large, otherwise training is expected to be unstable.\nSpecifically, the gradients is clipped by\n\u2192 \u2225 \u2225 \u2225 \u2225 if \u2225 \u2225 \u2225 \u2225 \u2265 , else . Or \u2192 \u2022 , where = min{ \u2225 \u2225 \u2225 \u2225 , 1.0}(3)\nIntuitively, if at step the gradient norm \u2225 \u2225 is greater than a fraction of the parameter norm \u2022 \u2225 \u2225, AGC will clip the gradient norm to \u2225 \u2225, by rescaling gradients with a scalar clipping factor \u2208 R + . AGC can be viewed as a special case of GC, where the clipping threshold GC is a function of model parameters GC = AGC \u2225 \u2225. So when using AGC, we don't need to fine tune for different layers, this is where the \"adaptiveness\" comes from.", "publication_ref": ["b12", "b34", "b7", "b10", "b28", "b23", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Observations of Training Dynamics", "text": "Despite the success of GC and AGC in various domains, we found that they are not effective enough to prevent loss divergence when being applied in our model. To better understand the limitations of GC/AGC and to propose better solutions, we inspect the training of our model without using any gradient clipping based techniques 1 .\nFigure 4a shows the training loss and AUC for a particular binary classification task. To simplify the illustration, let's look mainly at the 3 most important training steps: step-a, step-b, and step-c 2 . As we can see, this model is training healthily before step-a: the loss is minimized and the AUC has increased rapidly. However, at step-b, the model's training loss started to diverge and AUC began to drop, though relatively unnoticeably. Finally, at step-c, this model was fully diverged with loss become large, and AUC dropped to 0.5.\nIn Figure 4b(left), we take a closer look at some statistics for the top shared layer to understand what happened as loss diverged. The gradient norm \u2225 \u2225 2 is pretty consistent before step-a when model is healthy. Then it grew to a large value at step-b, suggesting the loss curvature is quite steep at that moment. Since we didn't apply any model stability treatments, the model diverged completely at step-c and the gradient norm \u2225 \u2225 2 became a small value. This means that all pre-activations (values before applying nonlinear activation) at this layer already reach a state where gradients are extremely small 3 , causing the loss divergence to become nonrecoverable.\nKnowing what happened, we fabricate how GC/AGC will react in this situation. Figure 4b(left) plots the measurements of \u2225 \u2225 2 (blue) and\n\u2225 \u2225 2 \u2225 \u2225 2\n(orange) that are used to determine clipping factors in GC and AGC. Not surprisingly, both measurements became larger at step-b. However, the relative scale of change for these measurements are different.\n\u2225 \u2225 2 \u2225 \u2225 2\n(orange) is more sensitive to loss 1 Note the model we are inspecting here is the model-b in Figure 1 2 The specific training step numbers are: step-a=198.7k, step-b=198.8k, step-c=198.9k. 3 One typical example is the dying ReLU where most pre-activations are smaller than zero. It is worth noting that other nonlinear activations also have regions where gradients are close to zero, so can suffer from the same issue. +1 = \u2212 \u2192 apply rescaled updates 10: end for 11: Return: curvature changes than \u2225 \u2225 2 (blue). The difference in sensitivity of these measurements can result in different clipping factors , which is the rescaling multiplier to the gradients in different methods. Figure 4b(right) gives the clipping factor for GC and AGC when using GC = 10 \u22121 and AGC = 10 \u22123 as clipping thresholds 4 .\nBy checking the clipping factors, we hypothesize that the reason behind inefficacy of GC/AGC is that they failed to offer enough constraints on gradients (i.e., failed to provide enough control over the \"effective step size\") when gradient norm suddenly increases (i.e., loss curvature becomes steep), due to lack of sensitivity. More specifically, both methods rely on 2 norm, which is not sensitive to drastic gradient changes in only a few coordinates, especially when layer width is large.", "publication_ref": ["b0", "b2", "b0", "b1", "b2", "b3"], "figure_ref": ["fig_5", "fig_5", "fig_5", "fig_5"], "table_ref": []}, {"heading": "Proposed Solution: Clippy", "text": "To alleviate this limitation, we proposed a new algorithm called Clippy. Clippy has two major changes over GC/AGC: First, it uses \u221e norm instead of 2 norm to increase its sensitivity to changes in individual coordinates. Second, it clips based on updates = \u2022 \u22121/2 instead of gradients , since updates are the actual change to model parameters and can be quite different from gradients when using the Adagrad optimizer. Specifically, Clippy controls\n\u221e < ,(4)\nand then rescales updates when the inequality is violated. From Figure 4b, we can see that this measurement has a a more dramatic change at step-b, when loss was diverging. Suppose we use Clippy = 10 \u22121 as the clipping threshold, Clippy results in 2 orders of magnitude smaller clipping factors compared to GC/AGC, thanks to the better sensitivity of the measurement. In other words, we hope Clippy can put larger constraints on the actual updates when loss curvature is steep even in a few coordinates. Formally, we present Clippy in Algorithm 1. As can be seen, there are some minor but important changes in line-8 of the algorithm compared to what we described in equation 4.\n( \n= \u2022 \u22121/2 , +1 = \u2212 ( ) .(5)\nThat is, different algorithms scale down the learning rate with different choices of clipping factor . And the choice of clipping factors by different algorithms are summarized in the table below.\nAlgorithm GC [24] min{1.0, However, compared to GC/AGC, Clippy relies on updates instead of gradients. Moreover, although both Clippy and LAMB use the updates, Clippy does not completely ignore the update magnitude as in LAMB 5 . Finally, Clippy uses \u221e instead of 2 norm to be more sensitive to drastic update changes in a small number of coordinates.\n1 \u2225 \u2225 2 } AGC [7] min{1.0, \u2225 \u2225 2 \u2225 \u2225 2 } LAMB [34] ( \u2225 \u2225 2 ) \u2225 \u2225", "publication_ref": ["b23", "b4"], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "4.3.2", "text": "Clip locally or globally. When using Clippy, we clip the update per each layer (a.k.a locally) instead of per all model parameters as a whole (a.k.a globally), similar to the other methods (like GC/AGC/LAMB). This gives more flexibility on finer-grained control, but results in a biased gradient update. However, in large-batch settings, it can be shown that this bias is small [34]. 5 ", "publication_ref": ["b33", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "LAMB updates parameter by", "text": "+1 = \u2212 \u2225 \u2225 2\n( \u2225 \u2225 2 ), with ( ) = min{max{ , }, } bounds the parameter 2 norm. It uses only the direction of updates and ignores its magnitude. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "EMPIRICAL STUDIES", "text": "Conducted on a YouTube production dataset, experiments in this section are divided into two parts. Firstly, we compare Clippy with other baselines to verify its benefits for improving model stability.\nThen we show some further analyses for Clippy to better understand its strength.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiment Setup", "text": "5.1.1 Model detail. Besides all the model properties that are already covered in Section 3.1, it is worth mentioning that we simplified our ranking model by (1) Only keeping the most important subset of tasks and input features; (2) Using a simple shared bottom structure with several shared hidden layers. Though much simpler than the production model, we found it to be a sufficiently good testbed for studying the training stability problem, as it allows us to train models faster and focus more on research perspectives instead of irrelevant modeling details. The model is built with TensorFlow-2 [1] and is trained using a large batch size of 65k on TPUs.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation protocol.", "text": "Unfortunately, there is no reliable metric to quantify the model's training stability. To precisely measure the benefit from better training stability, we vary model complexity as well as learning rates, then check model's offline quality, measured by AUC for binary classification tasks and RMSE for regression tasks. Presumably, a more complex model gives better offline quality but is more likely to suffer from loss divergence issues. So if an algorithm can significantly improve the model's training stability, we should observe better offline metrics when using it. More specifically, we used first ( \u2212 1) days of data to sequentially train the model and continuously evaluate the model's performance (AUC or RMSE) on the last day (the -th day) of data. If the model does not suffer from any loss divergence issues during training, we should observe the evaluation metrics keep becoming better, as the model is adapting to the data distribution closer to the -th day of data. Whereas if the model's loss diverges during training, either fully-diverge or consistently micro-diverge, the evaluation metrics will be significantly impacted.\nTo explore the effect of model complexities, we consider various model settings summarized in Table 1. Both Small and Large use simple feed-forward networks as the shared bottom, with two 512 layers and four 4096 layers respectively. Large+DCN is built on top instability problems should get worse evaluation metrics. We first find the best learning rate (1x or 2x) for each variant, then repeat the same setting 3 times and report mean and standard deviation. We use underline to denote the best result for each setting.\nof Large and further adds more complexity by having DCN-v2 layers [31] on inputs, followed by a standard Layer Normalization [4].", "publication_ref": ["b30", "b3"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Baselines.", "text": "We apply Clippy and other baselines to non-embedding model parameters and compare their effectiveness. Below are more details about these baselines and Clippy.\n\u2022 Gradient Clipping (GC) [24]: We used layer-wise (local) gradient clipping with clipping threshold searched from GC \u2208 {10 \u22121 , 10 \u22122 , 10 \u22123 }. \u2022 Adaptive Gradient Clipping (AGC) [7]: We used the official implementation 6 provided in the paper and searched clipping threshold from AGC \u2208 {10 \u22122 , 10 \u22123 , 10 \u22124 }. \u2022 LAMB (adapt to Adagrad) [34]: LAMB was originally proposed based on Adam [19], while the authors also provided a general form for the clipping which we introduced in Section 4.3.1. We choose ( ) = as in the official implementation 7 . Since LAMB uses parameter 2 norm \u2225 \u2225 2 as update magnitude that is different from other methods, we have to scale the learning rates by and searched \u2208 {10 \u22121 , 10 \u22122 , 10 \u22123 }. \u2022 Clippy: Clippy has two hyperparameters abs and rel so suppose to be more non-trivial for the tunings, but we found simply setting rel = 0.5 and abs = 10 \u22122 gives decent performance in our experiments. .", "publication_ref": ["b23", "b6", "b5", "b33", "b18", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Overall Performance", "text": "Table-2 presents the overall comparison between Clippy and other baselines on different model settings. Though the model is trained on six tasks, due to space limitations, we only present the metrics from two most representative tasks -one binary classification task evaluated with AUC (in percentage) and another regression task evaluated with RMSE. We not only use the original learning rate but also try to double the learning rate and see if any method can benefit from it. After finalizing the best learning rate, we repeat the same setting 3 times with different random seeds and report the mean and standard deviation.\nLooking at Table 2, we can see the naive method which does not have any treatments on training stability always suffers from loss divergence, even on the Small model. There is a chance for it to survive if we drastically tune down the learning rate (see Section A.1 in Supplementary Material) but we omit its results here as they are bad. GC can survive with 2x learning rate and provide good results on Small and Large model. But in a more complex model with DCN, GC can only use 1x learning rate, otherwise it will suffer from loss divergence issues (see blue line in Figure 5a right). AGC did a reasonable job on Small and Large with 1x learning rate, but became bad with 2x learning rate. On Large+DCN, AGC shows very high variance using either 1x or 2x learning rate (see orange line in Figure 5a), suggesting AGC already reaches its limits on keeping training stable. LAMB successfully trains the model without suffering from training instability problems using 1x learning rate, but the convergence is negatively impacted. On Figure 5a, we found the results from LAMB are always worse than the other methods. We believe this is due to LAMB completely ignoring the update magnitude, causing the convergence at initial training to be very slow when parameter 2 norm is small. Surprisingly, GC performs the best on all settings among all the baselines, this could be because the model is relatively simple thus tuning the clipping threshold for GC is still trivial.\nOn the last column of Table 2, we can see Clippy handles all model settings with 2x learning rate. More importantly, Clippy doesn't compromise convergence, it has comparable results with GC (i.e., the best baseline) on Small and Large model (see Figure 5b), and having significantly better AUC (Note 0.1% AUC improvement in our model is considered very significant and can lead to live metric gains) and RMSE on Large+DCN model compared to GC. One important finding we want to highlight is that Clippy offers larger gains when the model is more complex and trained with a larger learning rate. On Figure 5b, we can see gap between Clippy and GC is getting larger when using a more complex model with 2x learning rate. So we are not surprised Clippy can help in the production model which is much more complex than Large+DCN.     3) bottom hidden layer of shared bottom, and in the output layers of (4) binary classification task and (5) regression task. It is interesting to see more clipping is done on the bottom layers of the model. We think this intuitively makes sense, because bottom layers usually have smaller parameter norm so Clippy's clipping threshold will also be smaller. On the other hand, this could potentially benefit training stability because we know a small change in the bottom layer weights can lead to a large difference in model outputs.", "publication_ref": [], "figure_ref": ["fig_10", "fig_10", "fig_10", "fig_10", "fig_10"], "table_ref": ["tab_3", "tab_3"]}, {"heading": "Closer Look at Clippy's Clipping Factors", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSION AND FUTURE WORK", "text": "In this paper, we present the training instability problem that happen recurrently in ranking models at YouTube. We show the importance and challenges of mitigating this problem in the long run.\nTo better understand the issue, we dive deep into the problem, trying to know the root cause of why conventional methods did not work well in our case. With our understandings, we propose a new clipping based method called Clippy, which has a nice relationship with existing methods but alleviates the limitations of them. From empirical studies on the YouTube production dataset, we found Clippy showed significantly better strength on improving model training stability than other baselines.\nClippy showed significant improvements on training stability in multiple ranking models for YouTube recommendations. It is productionized in some large and complex models. More importantly, it unblocks several ongoing modeling developments and alleviates us from training instability problems that happened recurrently.\nAs for future work, we hope to theoretically justify the effectiveness of Clippy and provide convergence guarantees. In addition, we hope evolution-based AutoML algorithms [10,27] can be applied here to do a better job than the human-designed clipping methods. As a result, we can see that the loss of the model kept diverging if no change is applied: 5 out of 5 runs has loss fully diverged. However, if we reduce the model size (by switching to Small+DCN model) or remove DCN-v2 layers (by switching to Large model), there will be some surviving cases. Moreover, if we remove a subset of input features or one or more output tasks, we can also observe their benefits on training stability. Due to the large training cost for each trial, we cannot offer more data points, but we hope these results can support for the hypotheses and claims in Section 3.2.", "publication_ref": ["b9", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "A.2 A Transformer-based model", "text": "In this section, we report the performance of Clippy in an additional setting. We based our experiment on init2winit 8 's [15] 'trans-late_wmt' dataset with the default 'xformer_translate' model, containing six encoder and six decoder layers for the task of English to German translation. We compared the default AdamW optimizer to Adagrad and to Adagrad with Clippy. For both AdamW and Adagrad optimizers we tested the learning rates [0.01, 0.03, 0.1, 0.3, 1.0], while for Adagrad with Clippy we used learning rate in [0.1, 0.3, 1.0, 3.0, 10.0, 30.0] and set GC = 0.1, AGC = 10 \u22123 . All experiments were executed twice for 500k steps, once a with warm-up period 10k steps and a second time with a 40k step warm-up period.\nWhen the warm-up period was 10k steps, AdamW diverged when the learning rate was 0.1 and diverged for learning rate equal to 1.0 when the warm-up period was at 40k steps. Adagrad diverged for learning rate equal to 0.3 in both cases. On the other hand, Adagrad with Clippy did not diverge for any of our experiments. In 8 https://github.com/google/init2winit a few of our earlier trials, Adagrad with Clippy did start to slowly diverge, however, that divergence was transitory and the model later recovered.\nThe baseline AdamW optimizer attained the best result with a learning rate of 0.1 and 40k warm-up period, reaching a validation error rate of 31.6%. Adagrad's best run was with a learning rate of 0.1 and 10k warm-up period, reaching a validation error rate of 36.9%, while Adagrad with Clippy's best run was with learning rate of 3.0 and 10k warm-up steps, reaching an error rate of 33.4% on the validation set. See Figure 7 for the validation error rate throughout the training process.\nNote that although Adagrad with Clippy did not reach the performance of AdamW, which is considered state-of-the-art for this task, it did show significant improvement over the Adagrad implementation. Furthermore, we did not attempt to tune any of the model's parameters beyond what is reported above, opening the way to further improvements.", "publication_ref": ["b7", "b14"], "figure_ref": ["fig_12"], "table_ref": []}, {"heading": "B STATISTICS FROM OTHER LAYERS", "text": "Besides the top hidden layer weights presented in Figure 4b, we also show statistics from other representative layers in Figure 8. From the figure, we can see other layers behave similarly as the top hidden layer at step-b, except for the binary classification layer. From Figure 8b, we found measurements used by GC/AGC even failed to capture the sudden changes in model parameters at step-b, resulting in no clipping applied at step-b for the binary classification layer weights by GC/AGC.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Tensorflow: a system for large-scale machine learning", "journal": "", "year": "2016", "authors": "Martin Abadi; Paul Barham; Jianmin Chen; Zhifeng Chen; Andy Davis; Jeffrey Dean; Matthieu Devin; Sanjay Ghemawat; Geoffrey Irving; Michael Isard"}, {"ref_id": "b1", "title": "On the Factory Floor: ML Engineering for Industrial-Scale Ads Recommendation Models", "journal": "", "year": "2022", "authors": "Rohan Anil; Sandra Gadanho; Da Huang; Nijith Jacob; Zhuoshu Li; Dong Lin; Todd Phillips; Cristina Pop; Kevin Regan; Gil I Shamir"}, {"ref_id": "b2", "title": "Understanding Scaling Laws for Recommendation Models", "journal": "", "year": "2022", "authors": "Newsha Ardalani; Carole-Jean Wu; Zeliang Chen; Bhargav Bhushanam; Adnan Aziz"}, {"ref_id": "b3", "title": "", "journal": "", "year": "2016", "authors": "Jimmy Lei Ba; Jamie Ryan Kiros; Geoffrey E Hinton"}, {"ref_id": "b4", "title": "Latent Cross: Making Use of Context in Recurrent Recommender Systems", "journal": "ACM", "year": "2018", "authors": "Alex Beutel; Paul Covington; Sagar Jain; Can Xu; Jia Li; Vince Gatto; Ed H Chi"}, {"ref_id": "b5", "title": "Large scale online learning", "journal": "", "year": "2003", "authors": "L\u00e9on Bottou; Yann Cun"}, {"ref_id": "b6", "title": "Highperformance large-scale image recognition without normalization", "journal": "", "year": "2021", "authors": "Andy Brock; Soham De; L Samuel; Karen Smith;  Simonyan"}, {"ref_id": "b7", "title": "Language models are few-shot learners", "journal": "Advances in neural information processing systems", "year": "2020", "authors": "Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell"}, {"ref_id": "b8", "title": "Multitask learning", "journal": "Machine learning", "year": "1997", "authors": "Rich Caruana"}, {"ref_id": "b9", "title": "Evolved Optimizer for Vision", "journal": "", "year": "2022", "authors": "Xiangning Chen; Chen Liang; Da Huang; Esteban Real; Yao Liu; Kaiyuan Wang; Cho-Jui Hsieh; Yifeng Lu; Quoc V Le"}, {"ref_id": "b10", "title": "Palm: Scaling language modeling with pathways", "journal": "", "year": "2022", "authors": "Aakanksha Chowdhery; Sharan Narang; Jacob Devlin; Maarten Bosma; Gaurav Mishra; Adam Roberts; Paul Barham;  Hyung Won; Charles Chung; Sebastian Sutton;  Gehrmann"}, {"ref_id": "b11", "title": "Gradient descent on neural networks typically occurs at the edge of stability", "journal": "", "year": "2021", "authors": "Simran Jeremy M Cohen; Yuanzhi Kaur; Zico Li; Ameet Kolter;  Talwalkar"}, {"ref_id": "b12", "title": "Adaptive subgradient methods for online learning and stochastic optimization", "journal": "Journal of Machine Learning Research", "year": "2011-07", "authors": "John Duchi; Elad Hazan; Yoram Singer"}, {"ref_id": "b13", "title": "A Loss Curvature Perspective on Training Instability in Deep Learning", "journal": "", "year": "2021", "authors": "Justin Gilmer; Behrooz Ghorbani; Ankush Garg; Sneha Kudugunta; Behnam Neyshabur; David Cardoze; George Dahl; Zachary Nado; Orhan Firat"}, {"ref_id": "b14", "title": "init2winit: a JAX codebase for initialization, optimization, and tuning research", "journal": "", "year": "2021", "authors": "Justin M Gilmer; George E Dahl; Zachary Nado"}, {"ref_id": "b15", "title": "Yangqing Jia, and Kaiming He. 2017. Accurate, large minibatch sgd: Training imagenet in 1 hour", "journal": "", "year": "2017", "authors": "Priya Goyal; Piotr Doll\u00e1r; Ross Girshick; Pieter Noordhuis; Lukasz Wesolowski; Aapo Kyrola; Andrew Tulloch"}, {"ref_id": "b16", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b17", "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "journal": "", "year": "2015", "authors": "Sergey Ioffe; Christian Szegedy"}, {"ref_id": "b18", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "Diederik Kingma; Jimmy Ba"}, {"ref_id": "b19", "title": "Convolutional deep belief networks on cifar-10. Unpublished manuscript", "journal": "", "year": "2010", "authors": "Alex Krizhevsky; Geoff Hinton"}, {"ref_id": "b20", "title": "Large scale video representation learning via relational graph clustering", "journal": "", "year": "2020", "authors": "Hyodong Lee; Joonseok Lee; Joe Yue-Hei; Paul Ng;  Natsev"}, {"ref_id": "b21", "title": "Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts", "journal": "", "year": "2018", "authors": "Jiaqi Ma; Zhe Zhao; Xinyang Yi; Jilin Chen; Lichan Hong; Ed H Chi"}, {"ref_id": "b22", "title": "Ad click prediction: a view from the trenches", "journal": "", "year": "2013", "authors": "Gary H Brendan Mcmahan; David Holt; Michael Sculley; Dietmar Young; Julian Ebner; Lan Grady; Todd Nie; Eugene Phillips; Daniel Davydov;  Golovin"}, {"ref_id": "b23", "title": "On the difficulty of training recurrent neural networks", "journal": "", "year": "2013", "authors": "Razvan Pascanu; Tomas Mikolov; Yoshua Bengio"}, {"ref_id": "b24", "title": "Online learning and online convex optimization. Foundations and Trends\u00ae in", "journal": "Machine Learning", "year": "2012", "authors": "Shai Shalev-Shwartz"}, {"ref_id": "b25", "title": "Adafactor: Adaptive learning rates with sublinear memory cost", "journal": "", "year": "2018", "authors": "Noam Shazeer; Mitchell Stern"}, {"ref_id": "b26", "title": "Primer: Searching for efficient transformers for language modeling", "journal": "", "year": "2021", "authors": "Wojciech David R So; Hanxiao Ma\u0144ke; Zihang Liu; Noam Dai; Quoc V Shazeer;  Le"}, {"ref_id": "b27", "title": "Towards neural mixture recommender for long range dependent user sequences", "journal": "", "year": "2019", "authors": "Jiaxi Tang; Francois Belletti; Sagar Jain; Minmin Chen; Alex Beutel; Can Xu; Ed H Chi"}, {"ref_id": "b28", "title": "Lamda: Language models for dialog applications", "journal": "", "year": "2022", "authors": "Romal Thoppilan; Daniel De Freitas; Jamie Hall; Noam Shazeer; Apoorv Kulshreshtha;  Heng-Tze; Alicia Cheng; Taylor Jin; Leslie Bos; Yu Baker;  Du"}, {"ref_id": "b29", "title": "Deep content-based music recommendation", "journal": "Advances in neural information processing systems", "year": "2013", "authors": "Aaron Van Den Oord; Sander Dieleman; Benjamin Schrauwen"}, {"ref_id": "b30", "title": "Dcn v2: Improved deep & cross network and practical lessons for web-scale learning to rank systems", "journal": "", "year": "", "authors": "Ruoxi Wang; Rakesh Shivanna; Derek Cheng; Sagar Jain; Dong Lin"}, {"ref_id": "b31", "title": "How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective", "journal": "", "year": "2018", "authors": "Lei Wu; Chao Ma; Weinan E "}, {"ref_id": "b32", "title": "Sampling-bias-corrected neural modeling for large corpus item recommendations", "journal": "", "year": "2019", "authors": "Xinyang Yi; Ji Yang; Lichan Hong; Derek Zhiyuan Cheng; Lukasz Heldt; Aditee Kumthekar; Zhe Zhao; Li Wei; Ed Chi"}, {"ref_id": "b33", "title": "Large batch optimization for deep learning: Training bert in 76 minutes", "journal": "", "year": "2019", "authors": "Yang You; Jing Li; Sashank Reddi; Jonathan Hseu; Sanjiv Kumar; Srinadh Bhojanapalli; Xiaodan Song; James Demmel; Kurt Keutzer; Cho-Jui Hsieh"}, {"ref_id": "b34", "title": "Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity", "journal": "", "year": "2020", "authors": "Jingzhao Zhang; Tianxing He; Suvrit Sra; Ali Jadbabaie"}, {"ref_id": "b35", "title": "Recommending what video to watch next: a multitask ranking system", "journal": "", "year": "", "authors": "Zhe Zhao; Lichan Hong; Li Wei; Jilin Chen; Aniruddh Nath; Shawn Andrews; Aditee Kumthekar; Maheswaran Sathiamoorthy"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Example of loss divergence in our model and its impact on training loss (top) and AUC (bottom). In this example, model-a's loss micro-diverged then recovered, whereas model-b's loss fully-diverged.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: An general illustration of the ranking model used in recommender systems. The model has one or more layers that are (softly or fully) shared by multiple tasks.", "figure_data": ""}, {"figure_label": "12113", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "\u2022 1 q 2 ( 11 Figure 3 :12113Figure 3: From [12, Figure 2]. Gradient descent on a quadratic model with eigenvalues 1 = 20 and 2 = 1. We can clearly observe training instability problems starting to occur when learning rate > 2/ * = 2/ 1 = 0.1.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "(a) Training loss (left) and AUC (right) at different steps. (b) (left) Measurements used by different methods to determine clipping factors. (right) The corresponding clipping factors.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 :4Figure 4: (a) We dive into three typical moments in model training: The model was training healthily before step-a. Then at step-b, model's loss aroused and AUC dropped. Finally at step-c, the loss is fully diverged and AUC dropped to 0.5. (b)When checking some statistics from the top hidden layer of the model, we found that GC and AGC failed to provide small enough clipping factor. While Clippy's clipping factor can be 2-orders of magnitude smaller than GC and AGC. Section B in Supplementary Material has the statistics for other layers.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Algorithm 1 =1Adagrad with Clippy 1: Input: Parameter vector to optimize ; objective function L; learning rate schedule . 2: Input: Clippy hyperarameters: relative threshold rel and absolute threshold abs . 3: Initialize parameter vector 0 . 4: for = 0 to \u2212 1 do min{1.0, min( rel | |+ abs * | | )} \u2192 get clipping factor 9:", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "2 Clippy(2Ours) min{1.0, min( rel | |+ abs * | | )} Clippy is a combination of GC/AGC/LAMB: First of all, Clippy switches from GC-style to AGC-style during training. During initial model training when | | \u2248 0, abs dominates the clipping threshold rel | |+ abs * | | \u2248 abs * | | and makes Clippy close to GC. In later training, when rel | | \u226b abs , Clippy acts more like AGC rel | |+ abs * | | \u2248 rel | | * | | .", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 6 shows6Figure 6 shows Clippy's clipping factor on different layers during training the Large+DCN model. As introduced in Section 4, the", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "(a) AUC for different methods during the training on Large+DCN model. (b) AUC of Clippy and GC (the best baseline) during the training on different model settings.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 5 :5Figure 5: Evaluation AUC vs. Training steps for different methods in different model settings.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 6 :6Figure 6: Clippy's clipping factor on different layers during training the Large+DCN model.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 7 :7Figure 7: A comparison of AdamW, Adagrad and Adagrad with Clippy on the task for English to German translation.A ADDITIONAL EMPIRICAL STUDIES A.1 Empirical Evidence for Section 3.2In Table3, we show more empirical evidence on how the unique properties of multitask ranking models in the recommendation domain can affect training stability. All the experiments are performed on the Large+DCN model without any treatments on training stability. We used 0.4x learning rate to make the training at the edge of instability and to see the benefits from other changes on improving training stability.As a result, we can see that the loss of the model kept diverging if no change is applied: 5 out of 5 runs has loss fully diverged. However, if we reduce the model size (by switching to Small+DCN model) or remove DCN-v2 layers (by switching to Large model), there will be some surviving cases. Moreover, if we remove a subset of input features or one or more output tasks, we can also observe their benefits on training stability. Due to the large training cost for each trial, we cannot offer more data points, but we hope these results can support for the hypotheses and claims in Section 3.2.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Introducing absolute threshold. In Clippy, we use two hyperparameters: The relative threshold rel that is similar to GC/AGC, and another absolute threshold abs . With the absolute threshold abs introduced, we can avoid aggressive clipping when model parameters are zero (e.g., biases that are initialized to zeros) or have very small values. As will be discussed in Section 4.3.1, this allows Clippy to switch from GC-style to AGC-style during training.(2) Considering learning rate. We have learning rate in the denominator when calculating the clipping factor to account for different learning rate schedules. If the learning rate slowly ramps up, this will loosen the clipping threshold at initial training, avoiding a slow pace of convergence in the initial phases of training.", "figure_data": "4.3 Additional Discussions"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "The configuration of each model setting.", "figure_data": "Model NameNon-Embedding Model ParametersShared bottom ArchitectureSmall7.5MFFN: 512 \u00d7 2Large57.0MFFN: 4096 \u00d7 4Large+DCN68.0MDCN + LN \u2192 4096 \u00d7 44.3.3 Adapting to other optimizers. One can easily adapt Clippy tooptimizers other than Adagrad by using the optimizer-dependentupdate . Empirically, we have also observed clear benefits intraining stability when applying Clippy on Adam [19], withoutcompromising convergence. But we leave the theoretical conver-gence analysis of Clippy to future work."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "\u00b1 0.13 71.73 \u00b1 0.00 71.56 \u00b1 0.01 71.79 \u00b1 0.00 RMSE (lower is better) 1.058 \u00b1 0.002 1.059 \u00b1 0.003 1.063 \u00b1 0.001 1.056 \u00b1 0.000 Evaluation of training stability treatments on different model settings. Methods suffering from training", "figure_data": "Model NameMetricsNaiveGCMethods AGCLAMBClippySmallAUC (higher is better) 71.68 Best learning rate diverged 2x1x1x2xAUC (higher is better)72.07 \u00b1 0.05 72.09 \u00b1 0.02 72.01 \u00b1 0.09 72.16 \u00b1 0.02LargeRMSE (lower is better)diverged1.053 \u00b1 0.003 1.051 \u00b1 0.001 1.054 \u00b1 0.002 1.051 \u00b1 0.000Best learning rate2x1x1x2xAUC (higher is better)72.27 \u00b1 0.03 72.06 \u00b1 0.08 72.05 \u00b1 0.11 72.37 \u00b1 0.01Large+DCNRMSE (lower is better)diverged1.049 \u00b1 0.001 1.051 \u00b1 0.001 1.057 \u00b1 0.001 1.047 \u00b1 0.001Best learning rate1x2x1x2x"}], "formulas": [{"formula_id": "formula_0", "formula_text": "= \u22121 + 2 , = \u2022 \u22121/2 , +1 = \u2212 \u2022 ,(1)", "formula_coordinates": [5.0, 147.96, 533.51, 146.63, 41.86]}, {"formula_id": "formula_1", "formula_text": "\u2192 \u2225 \u2225 if \u2225 \u2225 \u2265 , else. Or \u2192 \u2022 , where = min{ \u2225 \u2225 , 1.0} (2)", "formula_coordinates": [5.0, 364.88, 537.94, 193.86, 47.63]}, {"formula_id": "formula_2", "formula_text": "\u2192 \u2225 \u2225 \u2225 \u2225 if \u2225 \u2225 \u2225 \u2225 \u2265 , else . Or \u2192 \u2022 , where = min{ \u2225 \u2225 \u2225 \u2225 , 1.0}(3)", "formula_coordinates": [6.0, 98.13, 178.01, 196.45, 50.24]}, {"formula_id": "formula_3", "formula_text": "\u2225 \u2225 2 \u2225 \u2225 2", "formula_coordinates": [6.0, 92.77, 608.17, 16.06, 16.78]}, {"formula_id": "formula_4", "formula_text": "\u2225 \u2225 2 \u2225 \u2225 2", "formula_coordinates": [6.0, 157.41, 644.92, 16.06, 16.78]}, {"formula_id": "formula_5", "formula_text": "\u221e < ,(4)", "formula_coordinates": [6.0, 435.48, 542.99, 123.26, 15.78]}, {"formula_id": "formula_6", "formula_text": "= \u2022 \u22121/2 , +1 = \u2212 ( ) .(5)", "formula_coordinates": [7.0, 143.23, 321.66, 151.36, 26.05]}, {"formula_id": "formula_7", "formula_text": "1 \u2225 \u2225 2 } AGC [7] min{1.0, \u2225 \u2225 2 \u2225 \u2225 2 } LAMB [34] ( \u2225 \u2225 2 ) \u2225 \u2225", "formula_coordinates": [7.0, 101.88, 405.97, 130.94, 44.96]}, {"formula_id": "formula_8", "formula_text": "+1 = \u2212 \u2225 \u2225 2", "formula_coordinates": [7.0, 154.76, 683.7, 61.22, 10.49]}], "doi": "10.1145/3580305.3599846"}