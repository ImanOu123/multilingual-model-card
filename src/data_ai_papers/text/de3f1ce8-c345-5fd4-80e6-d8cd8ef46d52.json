{"title": "Aligning Non-Overlapping Sequences * , \u2020", "authors": "Yaron Caspi; Michal Irani", "pub_date": "", "abstract": "This paper shows how two image sequences that have no spatial overlap between their fields of view can be aligned both in time and in space. Such alignment is possible when the two cameras are attached closely together and are moved jointly in space. The common motion induces \"similar\" changes over time within the two sequences. This correlated temporal behavior, is used to recover the spatial and temporal transformations between the two sequences. The requirement of \"consistent appearance\" in standard image alignment techniques is therefore replaced by \"consistent temporal behavior\", which is often easier to satisfy. This approach to alignment can be used not only for aligning non-overlapping sequences, but also for handling other cases that are inherently difficult for standard image alignment techniques. We demonstrate applications of this approach to three real-world problems: (i) alignment of non-overlapping sequences for generating wide-screen movies, (ii) alignment of images (sequences) obtained at significantly different zooms, for surveillance applications, and, (iii) multi-sensor image alignment for multi-sensor fusion.", "sections": [{"heading": "Introduction", "text": "The problem of image alignment (or registration) has been extensively researched, and successful approaches have been developed for solving this problem. Some of these approaches are based on matching extracted local image features, other approaches are based on directly matching image intensities. A review of some of these methods can be found in Torr and Zisserman (1999) and Irani and Anandan(1999). However, all these approaches share one basic assumption: that there is sufficient overlap between the two images to allow extraction of common image properties, namely, that there is sufficient \"similarity\" between the two images (\"Similarity\" of images is used here in the broadest sense. It could range from gray-level similarity, to feature similarity, to similarity of frequencies, and all the way to statistical similarity such as mutual information (Viola and Wells III, 1995).\nIn this paper the following question is addressed: Can two images be aligned when there is very little similarity between them, or even more extremely, when there is no spatial overlap at all between the two images? When dealing with individual images, the answer tends to be \"No\". However, this is not the case when dealing with image sequences. An image sequence contains much more information than any individual frame does. In particular, temporal changes (such as dynamic changes in the scene, or the induced image motion) are encoded between video frames, but do not appear in any individual frame. Such information can form a powerful cue for alignment of two (or more) sequences. Caspi and Irani (2000) and Stein (1998) have illustrated an applicability of such an approach for aligning two sequences based on common dynamic scene information. However, they assumed that the same temporal changes in the scene (e.g., moving objects) are visible to both video cameras, leading to the requirement that there must be significant overlap in the FOVs (fields-of-view) of the two cameras.\nIn this paper we show that when two cameras are attached closely to each other (so that their centers of projections are very close), and move jointly in space, then the induced frame-to-frame transformations within each sequence have correlated behavior across the two sequences. This is true even when the sequences have no spatial overlap. This correlated temporal behavior is used to recover both the spatial and temporal transformations between the two sequences.\nUnlike carefully calibrated stereo-rigs (Slama, 1980), our approach does not require any prior internal or external camera calibration, nor any sophisticated hardware. Our approach bears resemblance to the approaches suggested by Demirdijian et al. (2000) Horaud and Csurka (1998) and Zisserman et al. (1995) for auto-calibration of stereo-rigs. But unlike these methods, we do not require that the two cameras observe and match the same scene features, nor that their FOVs will overlap.\nThe need for \"consistent appearance\", which is a fundamental assumption in image alignment or calibration methods, is replaced here with the requirement of \"consistent temporal behavior\". Consistent temporal behavior is often easier to satisfy (e.g., by moving the two cameras jointly in space). A similar idea was used for \"hand-eye calibration\" in robotics research (e.g., Tsai and Lenz (1989) and Horaud and Dornaika (1995)).\nOur approach is useful not only in the case of nonoverlapping sequences, but also in other cases where there is very little common appearance information between images, and are therefore inherently difficult for standard image alignment techniques. This gives rise to a variety of real-world applications, including: (i) Multi-sensor alignment for image fusion. This requires accurate alignment of images (sequences) obtained by sensors of different sensing modalities (such as Infra-Red and visible light). Such images differ significantly in their appearance due to different sensor properties (Viola and Wells III, 1995). (ii) Alignment of images (sequences) obtained at different zooms. The problem here is that different image features are prominent at different image resolutions (Dufournaud et al., 2000). Alignment of a wide-FOV sequence with a narrow-FOV sequence is useful for detecting small zoomed-in objects in (or outside) a zoomed-out view of the scene. This can be useful in surveillance appli-cations. (iii) Generation of wide-screen movies from multiple non-overlapping narrow FOV movies (such as in IMAX movies).\nOur approach can handle such cases. Results are demonstrated in the paper on complex real-world sequences, as well as on manipulated sequences with ground truth.", "publication_ref": ["b23", "b14", "b25", "b4", "b22", "b21", "b6", "b26", "b24", "b13", "b25", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Problem Formulation", "text": "We examine the case when two video cameras having (approximately) the same center of projection but different 3D orientation, move jointly in space (see Fig. 1). The fields of view of the two cameras do not necessarily overlap. The internal parameters of the two cameras are different and unknown, but fixed along the sequences. The external parameters relating the two cameras (i.e., the relative 3D orientation) are also unknown but fixed. Let S = I 1 , . . . , I n+1 and S = I 1 , . . . , I m+1 be the two sequences of images recorded by the two cameras. 1 When temporal synchronization (e.g., time stamps) is not available, then I i and I i may not be corresponding frames in time. Our goal is to recover the transformation that aligns the two sequences both in time and in space. Note the term \"alignment\" here has a broader meaning than the usual one, as the sequences may not overlap in space, and may not be synchronized in time. Here we refer to alignment as displaying one sequence in the spatial coordinate system of the other sequence, and at the correct time shift, as if obtained by the other camera.\nWhen the two cameras have the same center of projection (and differ only in their 3D orientation and their internal calibration parameters), then a simple fixed homography H (a 2D projective transformation) describes the spatial transformation between temporally Figure 1. Two video cameras are attached to each other, so that they have the same center of projection, but non-overlapping fieldsof-view. The two cameras are moved jointly in space, producing two separate video sequences I 1 , . . . , I n+1 and I 1 , . . . , I n+1 .\nFigure 2. Problem formulation. The two sequences are spatially related by a fixed but unknown inter-camera homography H , and temporally related by a fixed and unknown time shift t. Given the frame-to-frame transformations T 1 , . . . , T n and T 1 , . . . , T m , we want to recover H and t. corresponding pairs of frames across the two sequences (Hartley and Zisserman, 2000).\nIf there were enough common features (e.g., p and p ) between temporally corresponding frames (e.g., I i and I i ), then it would be easy to recover the intercamera homography H , as each such pair of corresponding image points would provide linear constrains on H : p \u223c = H p. This, in fact, is how most image alignment techniques work (Hartley and Zisserman, 2000). However, this is not the case here. The two sequence do not share common features, because there is no spatial overlap between the two sequences. Instead, the homography H is recovered from the induced frameto-frame transformations within each sequence.\nLet T 1 , . . . , T n and T 1 , . . . , T m be the sequences of frame-to-frame transformations within the video sequences S and S , respectively. T i is the transformation relating frame I i to I i+1 . These transformations can be either 2D parametric transformations (e.g., homographies or affine transformations) or 3D transformations/relations (e.g., fundamental matrices). We next show how we can recover the spatial transformation H and the temporal shift t between the two video sequences directly from the two sequences of transformations T 1 , . . . , T n and T 1 , . . . , T m . The problem formulated above is illustrated in Fig. 2.", "publication_ref": ["b11", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Recovering Spatial Alignment Between Sequences", "text": "Let us first assume that the temporal synchronization is known. Such information is often available (e.g., from time stamps encoded in each of the two sequences).\nSection 4 shows how we can recover the temporal shift between the two sequences when that informa-tion is not available. Therefore, without loss of generality, it is assumed that I i and I i are corresponding frames in time in sequences S and S , respectively. Two cases are examined: (i) The case when the scene is planar or distant from the cameras. We refer to these scenes as \"2D scenes\". In this case the frame-to-frame transformations T i can be modeled by homographies (Section 3.1). (ii) The case of a non-planar scene. We refer to these scenes as \"3D scenes\". In this case the frame-to-frame relations can be modeled by fundamental matrices (Section 3.2).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Planar or Distant (2D) Scenes", "text": "When the scene is planar or distant from the cameras, or when the joint 3D translations of the two cameras are negligible relative to the distance of the scene, then the induced image motions within each sequence (i.e., T 1 , . . . , T n and T 1 , . . . , T n ) can be described by 2D parametric transformations (Hartley and Zisserman, 2000). T i thus denotes the homography between frame I i and I i+1 , represented by 3 \u00d7 3 non-singular matrix.\nWe next show that temporally corresponding transformations T i and T i are related by the same fixed intercamera homography H (which relates frames I i and I i ).\nLet P be a 3D point in the planar (or the remote) scene. Denote by p i and p i its image coordinates in frames I i and I i , respectively (the point P need not be visible in the two frames, i.e., P need not be within the FOV of the cameras). Let p i+1 and p i+1 be its image coordinates in frames I i+1 and I i+1 , respectively. Then, p i+1 \u223c = T i p i and p i+1 \u223c = T i p i . Because the coordinates of the video sequences S and S are related by a fixed homography H , then: p \u223c = H p and p i+1 \u223c = H p i+1 .\nTherefore:\nH T i p i \u223c = H p i+1 \u223c = p i+1 \u223c = T i p i \u223c = T i H p i (1)\nEach p i could theoretically have a different scalar associated with the equality in Eq. (1). However, it is easy to show that because the relation in Eq. ( 1) holds for all points p i , therefore all these scalars are equal, and hence:\nH T i \u223c = T i H. (2\n)\nBecause H is non-singular we may write\nT i \u223c = H T i H \u22121 , or T i = s i H T i H \u22121 (3)\nwhere s i is a (frame-dependent) scale factor. Equation ( 3) is true for all frames, i.e., for any pair of corresponding transformations T i and T i (i = 1..n) there exists a scalar s i such that T i = s i H T i H \u22121 . It shows that there is a similarity relation 2 (or a \"conjugacy relation\") between the two matrices T i and T i (up to a scale factor). A similar observation was made for case of hand-eye calibration (e.g., Tsai and Lenz (1989) and Horaud and Dornaika (1995)), and for autocalibration of a stereo-rig (e.g. Zisserman et al. (1995)). Denote by eig(A) = [\u03bb 1 , \u03bb 2 , \u03bb 3 ] t a 3 \u00d7 1 vector containing the eigenvalues of a 3 \u00d7 3 matrix A (in decreasing order). Then it is known (Pearson (1983) p. 898.) that: (i) If A and B are similar (conjugate) matrices, then they have the same eigenvalues: eig(A) = eig(B), and, (ii) The eigenvalues of a scaled matrix are scaled: eig(s A) = s(eig(A)). Using these two facts and Eq. ( 3) we obtain:\neig(T i ) = s i eig(T i ) (4)\nwhere s i is the scale factor defined by Eq. (3). Equation ( 4) implies that the two vectors eig(T i ) and eig(T i ) are \"parallel\". This gives rise to a measure of similarity between two matrices T i and T i :\nsim(T i , T i ) = eig(T i ) t eig(T i ) eig(T i ) eig(T i ) , (5\n)\nwhere \u2022 is the vector norm. For real valued eigenvalues, Eq. (5) provides the cosine of the angle between the two vectors eig(T i ) and eig(T i ). This property will be used later for obtaining the temporal synchronization between the two sequences (Section 4). This measure is also used for outlier rejection of bad frame-toframe transformation pairs, T i and T i (Section 6.3). The remainder of this section explains how the fixed inter-camera homography H is recovered from the list of frame-to-frame transformations T 1 , . . . , T n and T 1 , . . . , T n .\nFor each pair of temporally corresponding transformations T i and T i in sequences S and S , we first compute their eigenvalues eig(T i ) and eig(T i ). The scale factor s i which relates them is then estimated from Eq. (4) using least squares minimization (three equations, one unknown). 3 Once s i is estimated, Eq. (3) (or Eq. (2)) can be rewritten as:\ns i H T i \u2212 T i H = 0 (6)\nEq. ( 6) is linear in the unknown components of H . Rearranging the components of H in a 9 \u00d7 1 column vector \nM i h = 0 (7)\nwhere M i is a 9 \u00d7 9 matrix defined by T i , T i and s i :\nM i = \uf8ee \uf8f0 s i T i t \u2212 T i 11 I \u2212T i 12 I \u2212T i 13 I \u2212T i 21 I s i T t \u2212 T i 22 I \u2212T i 23 I \u2212T i 31 I \u2212T i 32 I s i T t \u2212 T i 33 I \uf8f9 \uf8fb 9\u00d79\nwhere I is the 3\u00d73 identity matrix. Equation ( 7) implies that each pair of corresponding transformations T i and T i contributes 9 linear constrains in the unknown homography H (i.e., h), out of which at most 6 constraints are linearly independent (see Section 6). Therefore, in theory, at least two such pairs of independent transformations are needed to uniquely determine the homography H (up to a scale factor). In practice, we use all available constraints from all pairs of transformations to compute H . The constraints from all the transformations T 1 , . . . , T n and T 1 , . . . , T n can be combined into a single set of linear equations in h:\nA h = 0 (8)\nwhere A is a 9n \u00d7 9 matrix:\nA = [ M 1 . . . M n\n]. Equation ( 8) is a homogeneous set of linear equations in h, that can be solved in a variety of ways (Bjorck, 1996). In particular, h may be recovered up to scale by computing the eigenvector which corresponds to the smallest eigenvalue of the matrix A t A.", "publication_ref": ["b11", "b24", "b13", "b26", "b18", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "3D Scenes", "text": "When the scene is neither planar nor distant, the relation between two consecutive frames of an uncalibrated camera is described by the fundamental matrix (Hartley and Zisserman, 2000). In this case the input to our algorithm is two sequences of fundamental matrices between successive frames, denoted by F 1 , . . . , F n and F 1 , . . . , F n . Namely, if p i \u2208 I i and p i+1 \u2208 I i+1 are corresponding image points, then: p t i+1 F i p i = 0. Although the relations within each sequence are characterized by fundamental matrices, the inter-camera transformation remains a homography H . This is because the two cameras still share the same center of projection (Section 2).\nEach fundamental matrix F i can be decomposed into a homography + epipole (Hartley and Zisserman, 2000) as follows:\nF i = [e i ] \u00d7 T i\nwhere e i is the epipole relating frames I i and I i+1 , the matrix T i is the induced homography from I i to I i+1 via any plane (real or virtual). [\u2022] \u00d7 is the cross product matrix ([v] \u00d7 w = v \u00d7 w).\nThe homographies, T 1 , . . . , T n and T 1 , . . . , T n , and the epipoles e 1 , . . . , e n and e 1 , . . . , e n , impose separate constraints on the inter-camera homography H . These constraints can be used separately or jointly to recover H .", "publication_ref": ["b11", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Homography-Based Constraints.", "text": "The homographies T 1 , . . . , T n and T 1 , . . . , T n (extracted from the fundamental matrices F 1 , . . . , F n and F 1 , . . . , F n , respectively), may correspond to different 3D planes. In order to apply the algorithm of Section 3.1 using these homographies, we need to impose plane-consistency across the two sequences (to guarantee that temporally corresponding homographies correspond to the same plane in the 3D world). One possible way for imposing plane-consistency across (and within) the two sequences is by using the \"Plane + Parallax\" approach (Kumar et al., 1994;Irani et al., 1998;Shashua and Navab, 1994;Sawhney, 1994). However, this approach requires that a real physical planar surface be visible in all video frames. Alternatively, the \"threading\" method of Avidan and Shashua (1998) or other methods for computing consistent set of camera matrices (e.g., Beardsley et al., 1998), can impose plane-consistency within each sequence, even if no real physical plane is visible in any of the frames. Plane consistency across the two sequences can be obtained, e.g., if Avidan and Shashua (1998) is initiated at frames which are known to simultaneously view the same real plane in both sequences. This can be done even if the two cameras see different portions of the plane (allowing for nonoverlapping FOVs), and do not see that plane at any of the other frames. This approach is therefore less restrictive than the Plane + Parallax approach.", "publication_ref": ["b17", "b15", "b20", "b19", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Epipole-Based Constraints.", "text": "The fundamental matrices F 1 ..F n and F 1 ..F n also provide a list of epipoles e 1 , . . ., e n and e 1 , . . ., e n . These epipoles are uniquely defined (there is no issue of plane consistency here).\nSince the two cameras have the same center of projection, then for any frame i: e i \u223c = He i , or more specifically:\n(e i ) x = [h 1 h 2 h 3 ] e i [h 7 h 8 h 9 ] e i (e i ) y = [h 4 h 5 h 6 ] e i [h 7 h 8 h 9 ] e i (9\n)\nMultiplying by the dominator and rearranging terms yields two new linear constrains on H for every pair of corresponding epipoles e i and e i :\ne t i 0 t \u2212 (e i ) x e t i 0 t e t i \u2212 (e i ) y e t i 2\u00d79 h = 0 (10\n)\nwhere 0 t = [0, 0, 0]. Every pair of temporally corresponding epipoles, e i and e i , thus imposes two linear constraints on H . These 2n constraints (i = 1, . . . , n) can be added to the set of linear equations in Eq. ( 8) which are imposed by the homographies. Alternatively, the epipole-related constraints can be used alone to solve for H , thus avoiding the need to enforce planeconsistency on the homographies. Theoretically, four pairs of corresponding epipoles e i and e i in general position (no 3 on the same line) are sufficient.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Recovering Temporal Synchronization Between Sequences", "text": "So far we have assumed that the temporal synchronization between the two sequences is known and given. Namely, that frame I i in sequence S corresponds to frame I i in sequence S , and therefore the transformation T i corresponds to transformation T i . Such information is often available from time stamps. However, when such synchronization is not available, we can recover it. Given two unsynchronized sequences of transformations T 1 , . . . , T n and T 1 , . . . , T m , we wish to recover the unknown temporal shift t between them.\nLet T i and T i+ t be temporally corresponding transformations (namely, they occurred at the same time instance). Then from Eq. ( 4) we know that they should satisfy eig(T i ) eig(T i+ t ) (i.e., the 3 \u00d7 1 vectors of eigenvalues should be parallel). In other words, the similarity measure sim(T t i , T t i + t ) of Eq. (5) should equal 1 (corresponding to cos(0), i.e., an angle of 0 \u2022 between the two vectors). All pairs of corresponding transformations T i and T i+ t must simultaneously satisfy this constraint for the correct time shift t. Therefore, we recover the unknown temporal time shift t by maximizing the following objective function:\nSIM( t) = i sim(T i , T i+ t ) 2 (11\n)\nThe maximization is currently performed by an exhaustive search over a finite range of valid time shifts t.\nTo address larger temporal shifts, we apply a hierarchical search. Coarser temporal levels are constructed by composing transformations to obtain fewer transformation between more distant frames. The objective function of Eq. (11) can be generalized to handle sequences of different frame rates, such as sequences obtained by NTSC cameras (30 frame/sec) vs. PAL cameras (25 frames/sec). The ratio between frames corresponding to equal time steps in the two sequences is 25:30 = 5:6. Therefore, the objective function that should be maximized for an NTSC-PAL pair of sequences is:\nSIM( t) = i sim T 5(i+1) 5i , T 6(i+1)+ t 6i+ t 2 (12)\nWhere T j i is the transformation from frame I i to frame I j . In our experiments, all sequences were obtained by PAL video cameras. Therefore only the case of equal frame-rate (Eq. ( 11)) was experimentally verified. We found this method to be very robust. It successfully recovered the temporal shift up to field (half-frame) accuracy. Sub-field accuracy may be further recovered by interpolating the values of SIM( t) obtained at discrete time shifts.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Applications", "text": "This section illustrates the applicability of our method to solving some real-world problems, which are particularly difficult for standard image alignment techniques. These include: (i) Alignment of nonoverlapping sequences for generation of wide-screen movies from multiple narrow-screen movies (such as in IMAX films), (ii) Alignment of sequences obtained at significantly different zooms (e.g., for surveillance applications), and (iii) Alignment of multi-sensor sequences for multi-sensor fusion. We show results of applying the method to complex real-world sequences. All sequences which we experimented with, were captured by \"off-the-shelf\" consumer CCD cameras. The cameras were attached to each other to minimize the distance between their centers of projections. The joint camera motion was performed manually (i.e., a person would manually hold and rotate the two attached cameras). No temporal synchronization tool was used.\nThe frame-to-frame input transformations within each sequence (homographies T 1 , . . . , T n and T 1 , . . . , T n ) were extracted using the method described in Irani et al. (1994). Inaccurate frame-to-frame transformations T i are pruned out by using two outlier detection mechanisms (see Section 6.3). The input sequences were usually several seconds long to guaranty significant enough motion. The temporal time shift was recovered using the algorithm described in Section 4 up to field accuracy. Finally, the best thirty or so transformations were used in the estimation of the inter-camera homography H (using the algorithm described in Section 3.1).", "publication_ref": ["b16"], "figure_ref": [], "table_ref": []}, {"heading": "Alignment of Non-Overlapping Sequences", "text": "Figure 3 shows an example of alignment of nonoverlapping sequences. The left camera is zoomed-in and rotated relative to the right camera. The correct spatio-temporal alignment can be seen in Fig. 3(c). Note the accurate alignment of the running person both in time and in space.\nOur approach to sequence alignment can be used to generate wide-screen movies from two (or more) narrow field-of-view movies (such as in IMAX movies). Such an example is shown in Fig. 4. To verify the accuracy of alignment (both in time and in space), we allowed for a very small overlap between the two sequences. However, this image region was not used in the estimation process, to imitate the case of truly non-overlapping sequences. The overlapping region was used only for display and verification purposes. Figure 4(c) shows the result of combining the two sequences (by averaging corresponding frames) after spatio-temporal alignment. Note the accurate spatial as well as temporal alignment of the soccer player in the averaged overlapping region.", "publication_ref": [], "figure_ref": ["fig_0", "fig_0", "fig_2", "fig_2"], "table_ref": []}, {"heading": "Alignment of Sequences Obtained at Different Zooms", "text": "Often in surveillance applications two cameras are used, one with a wide FOV (field-of-view) for observing large scene regions, and the other camera with a narrow FOV (zoomed-in) for detecting small objects. Matching two such images obtained at significantly different zooms is a difficult problem for standard image alignment methods, since the two images display different features which are prominent at the different resolutions. Our sequence alignment approach may be used for such scenarios. Figure 5 shows three such examples. The results of the spatio-temporal alignment (right column of Fig. 5) are displayed in the form of averaging temporally corresponding frames after alignment according to the computed homography and the computed time shift. In the first example (top row of Fig. 5) the zoom difference between the two cameras was approximately 1:3. In the second example (second row) it was \u22481:4, and in the third example (bottom row) it was \u22481:8. Note the small red flowers in the zoomed view (Fig. ", "publication_ref": [], "figure_ref": ["fig_3", "fig_3", "fig_3"], "table_ref": []}, {"heading": "Multi-Sensor Alignment", "text": "Images obtained by sensors of different modalities, e.g., IR (Infra-Red) and visible light, can vary significantly in their appearance. Features appearing in one image may not appear in the other, and vice versa. This poses a problem for image alignment methods. Our se-quence alignment approach, however, does not require consistent appearance between the two sequences, and can therefore be applied to solve the problem. Figure 6 shows an example of two such sequences, one captured by a near IR camera, while the other by a regular video (visible-light) camera. The scene was shot in twilight. In the sequence obtained by the regular video camera (Fig. 6(a)), the outdoor scene is barely visible, while the inside of the building is clearly visible. The IR camera, on the other hand, captures the outdoor scene in great detail, while the indoor part (illuminated by \"cold\" neon light) was invisible to the IR camera (Fig. 6(b)). The result of the spatio-temporal alignment is illustrated by fusing temporally corresponding frames. The IR camera provides only intensity information, and was therefore fused only with the intensity (Y) component of the visible-light camera (using the image-fusion method of Burt and Kolczynski (1993)).\nThe chrome components (I and Q) of the visible-light camera supply the color information.\nThe reader is encouraged to view color sequences at www.wisdom.weizmann.ac.il/NonOverlappingSeqs. ", "publication_ref": ["b3"], "figure_ref": ["fig_4", "fig_4", "fig_4"], "table_ref": []}, {"heading": "Analysis", "text": "In this section we evaluated the effectiveness and stability, of the presented approach empirically (Section 6.1) and theoretically (Section 6.2) and numerically (Section 6.3).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Empirical Evaluation", "text": "In order to empirically verify the accuracy of our method, we took a real video sequence (see Fig. 7) and generated from it pairs of sequences with known (ground truth) spatial transformation H and temporal shift t. We then applied our algorithm and compared the recovered H and t with the ground truth.\nFor the case of non overlapping sequences, the real sequence of Fig. 7 was split in the middle, producing two non-overlapping sub-sequences of half-a-frame width each. The true (ground truth) homography H therefore corresponds to a horizontal shift by the width of a halved frame (352 pixels), and t in this case is 0. The \"inter-camera\" homography H was recovered up to a misalignment error of less than 0.7 pixel over the entire image. The temporal shift ( t = 0) was recovered accurately from the frame-to-frame transformations.\nTo empirically verify the accuracy of our method in the presence of large zooms and large rotations, we ran the algorithm on following three manipulated sequences with known (ground truth) manipulations: We warped the sequence of Fig. 7 (once by a zoom factor of 2, once by a zoom factor of 4, and once rotated it by 180 \u2022 ) to generate the second sequence.\nThe results are summarized in Table 1. The reported residual misalignment was measured as follows: The recovered homography was composed with the inverse of the ground-truth homography: H \u22121 true H recovored . Ideally, the composed homography should be the identity   b) are temporally corresponding frames from the visible-light and near-IR sequences, respectively (the temporal alignment was automatically detected). The inside of the building is visible only in the visible-light sequence, while the IR sequence captures the details outdoors (e.g., the dark trees, the sign, the bush). (c) shows the results of fusing the two sequences after spatio-temporal alignment. The fused sequence preserves the details from both sequences. Note the high accuracy of alignment (both in time and in space) of the walking lady. For more details see text. For color sequences see www.wisdom.weizmann.ac.il/NonOverlappingSeqs. Figure 7. The sequence used for empirical evaluation. (a, b, c) are three frames (0, 150, 300) out of the original 300 frames. This sequence was used as the base sequence for the quantitative experiments summarized in Table 1. matrix. The errors reported in Table 1 are the maximal residual misalignment induced by the composed homography over the entire image. In all the cases the correct t was recovered (not shown in the table ). In each experiment a real video sequence (Fig. 7) was warped (\"manipulated\") by a known homography, to generate a second sequence. The left column describes the type of spatial transformation applied to the sequence, the center column describes the recovered transformation, and the right column describes the residual error between the ground-truth homography and the recovered homography (measured in maximal residual misalignment in the image space). In all 4 cases the correct temporal shift was recovered accurately. See text for further details.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1", "tab_1", "tab_1"]}, {"heading": "Uniqueness of Solution", "text": "This section studies how many pairs of corresponding transformations T i and T i are required in order to uniquely resolve the inter-camera homography H . To do so we examine the number of constraints imposed on H by a single pair of transformations via the similarity equation Eq. (3). Since we can extract the scale factor s i directly from T i and T i (see Section 3.1) we can omit the scale factor s i and study the following question: How many constraints does an equation of the form G = HBH \u22121 impose on H ? (e.g., B = T i and G = T i ). 4 The following notations are used: Denote by \u03bb 1 , \u03bb 2 , \u03bb 3 the eigenvalues of the matrix B in decreasing order (|\u03bb\n1 | \u2265 |\u03bb 2 | \u2265 |\u03bb 3 |). Denote by u b 1 , u b 2 , u b 3 the corresponding eigenvectors with unit norm ( u b 1 = u b 2 = u b 3 = 1)\n. Denote by r j the algebraic multiplicity 5 of the eigenvalue \u03bb j , and denote by V j = { v \u2208 R n : B v = \u03bb j v} the corresponding eigen subspace. The same holds for eigen subspaces. If V is an eigen subspace of B corresponding to an eigenvalue \u03bb, then H (V ) is an eigen subspace of G with the same eigenvalue \u03bb. We investigate the number of constraints imposed on H by B and G according to the dimensionality of their eigen subspaces. Let V be the eigen subspace corresponding to an eigenvector u b of B. We investigate three possible cases, one for each possible dimensionality of V , i.e., dim(V ) = 1, 2, 3.\nCase I. dim(V ) = 1. This case mostly occurs when all three eigenvalues are distinct, but can also occur if some eigenvalues have algebraic multiplicity two or even three. In all these cases, V is spanned by the single eigenvector u b . Similarly H (V ) is spanned by the eigenvector u g of G. Therefore:\nH u b = \u03b1u g (13\n)\nwith an unknown scale factor \u03b1. Equation ( 13) provides 3 linear equations in H and one new unknown \u03b1, thus in total it provides two new linearly independent constraints on H .\nCase II. dim(V ) = 2. This occurs in one of the following two cases: (a) when there exists an eigenvalue with algebraic multiplicity two, or (b) when there is only one eigenvalue with algebraic multiplicity three, but the eigen subspace spanned by all eigenvectors has dimensionality of two. 6 When dim(V ) = 2 then two eigenvectors span V (w.l.o.g., u b1 and u b2 ). Then every linear combination of u b1 and u b2 is also an eigenvector of B with the same eigenvalue. Similarly, every linear combination of u g 1 and u g 2 is an eigenvector of G with the same eigenvalue. Therefore:\nH u b j = \u03b1 j u g 1 + \u03b2 j u g 2 (14)\nwhere \u03b1 j and \u03b2 j are unknown scalars ( j = 1, 2). Hence, each of the two eigenvectors u b1 and u b2 provides 3 linear equations and 2 new unknowns. Therefore, in total, together they provide 2 new linear constraints on H .\nCase III. dim(V ) = 3. In this case any vector is an eigenvector (all with the same eigenvalue \u03bb). This is the case when B \u223c = G \u223c = \u03bbI are the identity transformation up to scale, i.e., no camera motion. In this case (as expected) B and G provide no additional constraints on H .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Counting Constrains.", "text": "So far we counted the number of constraints imposed on H by a single eigen subspace. In order to count the total number of linear constraints that B and G impose on H , we analyze every possible combination of eigen subspaces according to the algebraic multiplicity of their eigenvalues:\n1. \u03bb i = \u03bb j = \u03bb k . This implies V i = V j = V k and dim(V i ) = dim(V j ) = dim(V k ) = 1. 2. \u03bb i = \u03bb j = \u03bb k (V i = V j = V k ).\nThere are two such cases:\n(a) dim(V i = V j ) = 2, and dim(V k ) = 1. (b) dim(V i = V j ) = 1, and dim(V k ) = 1. 3. \u03bb i = \u03bb j = \u03bb k .\nIn this case there is only a single eigen subspace V = V i = V j = V k . Its dimensionality may be 1, 2, or 3.\nThe following table summarizes the number of linearly independent constraints for each of the above cases: \n\u03bb i = \u03bb j = \u03bb k |V i | = |V j | = |V k | = 1 6 (2.a) \u03bb i = \u03bb j = \u03bb k |V i = V j | = 2, |V k | = 1 4 (2.b) \u03bb i = \u03bb j = \u03bb k |V i = V j | = 1, |V k | = 1 4 (3.a) \u03bb i = \u03bb j = \u03bb k |V i = V j = V k | = 1 2 (3.b) \u03bb i = \u03bb j = \u03bb k |V i = V j = V k | = 2 2 (3.c) \u03bb i = \u03bb j = \u03bb k |V i = V j = V k | = 3 0\nTo summarize: When B (and G) have either two or three distinct eigenvalues (which is typical of general frame-to-frame transformations), then two independent pairs of transformations suffice to uniquely determine H . This is because each pair of transformations imposes 4 to 6 linearly independent constraints, and in theory 8 independent linear constraints suffice to uniquely resolve H (up to arbitrary scale factor).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Numerical Stability", "text": "The final step in our algorithm is to solve a homogeneous set of linear equations (Eq. ( 8)). Care has to be taken when solving this system. For example, inaccuracies in the estimated frame-to-frame transformations decrease the accuracy of the final output. The previous section showed that two independent pairs of transformations may suffice to uniquely determine H . In practice, however, to increase numerical stability, we use all available constraints from all pairs of reliable transformations after subsampling of the sequences, outlier rejection and normalization. These are explained next: Temporal Subsampling. When the frame-to-frame transformations are too small, we often temporally subsample the sequences to obtain more significant transformations between successive frames. In our experiments where video clips were a couple of hundred frames long, we usually used 30 reliable transformations between distant (non-successive) frames. Such temporal subsampling should be done after recovering the temporal synchronization, to assure that it is done in a temporally synchronized manner across the two sequences.\nOutlier Rejection. Inaccurate frame-to-frame transformations T i are pruned out by using two outlier detection mechanisms:\n(i) The transformation between successive frames within each sequence are computed in both directions. Let T i be the transformation from I i to I i+1 , and T Reverse i the transformation from I i+1 to I i . Then we measure the deviation of the composed matrix T i T Reverse i from the identity matrix in terms of the maximal induced residual misalignment of pixels, i.e.,", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Reliability(", "text": "T i ) = max p\u2208I i T i T Reverse i p \u2212 p (15\n)\n(ii) The similarity criterion of Eq. ( 5) can also be used to verify the degree of \"similarity\" between a pair of transformations T i and T i . After t has been estimated and before H is estimated, an unreliable pair of transformations can be detected and pruned out by measuring the deviation of Sim(T i , T i ) from 1. However, the first outlier criterion (that of Eq. ( 15)) proved to be more powerful.\nMatrix Normalization. Using the heuristic provided in Golub and Van Loan (1989) (originly derived for Gaussian elimination) we normalize (scale) components of the input matrices T i and T i in a way that the rows of the matrix A of Eq. (8) will have approximately the same norm. This is an equivalent step to the scaling proposed by Hartley (1997) for recovering fundamental matrices. This step indeed improve the results.\nPreferred Camera Motions. When acquiring the sequences of input transformations, we usually have control over the camera motion. In general, any type of camera motion provides a frame-to-frame transformation which induces constraints on the inter-camera homography H . However, some transformations provide more stable sets of equations than others. In particular, we would like to generate sequences of transformations which provide more reliable components in each column of the matrix A in Eq. (8). For example, imageplane rotations (i.e., rotations about the optical axis of one of the cameras) usually provide reliable entries in all columns of M i (a block of A), thus impose stable constraints on H . To conclude, the camera rig can (and should) be moved freely, however, it is recommended that a few of the camera movements include non-negligible image-plane rotations.", "publication_ref": ["b9", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "This paper presents an approach for aligning two sequences (both in time and in space), even when there is no common spatial information between the sequences. This was made possible by replacing the need for \"consistent appearance\" (which is a fundamental requirement in standard images alignment techniques), with the requirement of \"consistent temporal behavior\", which is often easier to satisfy. We demonstrated applications of this approach to real-world problems, which are inherently difficult for regular image alignment techniques.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgment", "text": "The authors would like to thank R. Basri and L. Zelnik-Manor for their helpful comments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Notes", "text": "1. The subscript i is used to represent the frame time index, and the superscript prime is used to distinguish between the two sequences S and S . 2. A matrix A is said to be \"similar\" to a matrix B if there exists an invertible matrix M such that A = MBM \u22121 (see Pearson (1983)). The term \"conjugate matrices\" is also often used. 3. Alternatively, the input homographies can be normalized to have determinant equal to 1, to avoid the need to compute s i . 4. A general analysis of matrix equations of the form GH = HB may be found in Gantmakher (1959). 5. If \u03bb 1 = \u03bb 2 = \u03bb 3 then the algebraic multiplicity of all eigenvalues is 1 (r j = 1). If \u03bb 1 = \u03bb 2 = \u03bb 3 then the algebraic multiplicity of \u03bb 1 and \u03bb 2 is 2, and the algebraic multiplicity of \u03bb 3 is 1 (r 1 = r 2 = 2 and r 3 = 1). If \u03bb 1 = \u03bb 2 = \u03bb 3 then the algebraic multiplicity of \u03bb 1 , \u03bb 2 , and \u03bb 2 is 3 (r 1 = r 2 = r 3 = 3). 6. Eigenvalues with algebraic multiplicity 2 and 3 are not rare.\nFor example a homography defined by pure shift ( x, y) has the form:\n]. This matrix has a single eigenvalue \u03bb 1 = \u03bb 2 = \u03bb 3 = 1 with algebraic multiplicity three. The corresponding eigen subspace has dimensionality 2. It is spanned by two linearly independent eigenvectors [1, 0, 0] t and [0, 1, 0] t .", "publication_ref": ["b18", "b8"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Threading fundamental matrices", "journal": "", "year": "1998", "authors": "S Avidan; A Shashua"}, {"ref_id": "b1", "title": "3D model aquisition from extended image sequences", "journal": "", "year": "1996", "authors": "P A Beardsley; P H S Torr; A Zisserman"}, {"ref_id": "b2", "title": "Numerical Methodes for Least Squares Problems", "journal": "SIAM: Philadelphia", "year": "1996", "authors": "A Bjorck"}, {"ref_id": "b3", "title": "Enhanced image capture through fusion", "journal": "", "year": "1993", "authors": "P R Burt; R J Kolczynski"}, {"ref_id": "b4", "title": "A step towards sequence-to-sequence alignment", "journal": "", "year": "2000-06", "authors": "Y Caspi; M Irani"}, {"ref_id": "b5", "title": "Alignment of non-overlaping sequences", "journal": "", "year": "2001", "authors": "Y Caspi; M Irani"}, {"ref_id": "b6", "title": "Stereo autocalibration from one plane", "journal": "", "year": "2000", "authors": "D Demirdijian; A Zisserman; R Horaud"}, {"ref_id": "b7", "title": "Matching images with different resolutions", "journal": "", "year": "2000-06", "authors": "Y Dufournaud; C Schmid; R Horaud"}, {"ref_id": "b8", "title": "The Theory of Matrices", "journal": "Chelsea Pub", "year": "1959", "authors": "F R Gantmakher"}, {"ref_id": "b9", "title": "Matrix Computations", "journal": "The Johns Hopkins University Press", "year": "1989", "authors": "Gene Golub; Van Loan"}, {"ref_id": "b10", "title": "In defence of the 8-point algorithm", "journal": "IEEE Trans. on Pattern Analysis and Machine Intelligence", "year": "1997", "authors": "R I Hartley"}, {"ref_id": "b11", "title": "Multiple View Geometry in Computer Vision", "journal": "Cambridge University Press", "year": "2000", "authors": "R Hartley; A Zisserman"}, {"ref_id": "b12", "title": "Reconstruction using motions of a stereo rig", "journal": "", "year": "1998", "authors": "R Horaud; G Csurka"}, {"ref_id": "b13", "title": "Hand-eye calibration", "journal": "International Journal of Robotics Research", "year": "1995", "authors": "R Horaud; F Dornaika"}, {"ref_id": "b14", "title": "About direct methods", "journal": "", "year": "1999", "authors": "M Irani; P Anandan"}, {"ref_id": "b15", "title": "From reference frames to reference planes: Multi-view parallax geometry and applications", "journal": "", "year": "1998-06", "authors": "M Irani; P Anandan; D Weinshall"}, {"ref_id": "b16", "title": "Computing occluding and transparent motions", "journal": "International Journal of Computer Vision", "year": "1994", "authors": "M Irani; B Rousso; S Peleg"}, {"ref_id": "b17", "title": "Direct recovery of shape from multiple views: Parallax based approach", "journal": "", "year": "1994", "authors": "R Kumar; P Anandan; K Hanna"}, {"ref_id": "b18", "title": "Handbook of Applied Mathematics, 2nd edn", "journal": "Van Nostrand Reinhold Company", "year": "1983", "authors": "C E Pearson"}, {"ref_id": "b19", "title": "3D geometry from planar parallax", "journal": "", "year": "1994-06", "authors": "H Sawhney"}, {"ref_id": "b20", "title": "Relative affine structure: Theory and application to 3D reconstruction from perspective views", "journal": "", "year": "1994-06", "authors": "A Shashua; N Navab"}, {"ref_id": "b21", "title": "Manual of Photogrammetry. American Society of Photogrammetry and Remote Sensing", "journal": "", "year": "1980", "authors": "C C Slama"}, {"ref_id": "b22", "title": "Tracking from multiple view points: Selfcalibration of space and time", "journal": "", "year": "1998", "authors": "G P Stein"}, {"ref_id": "b23", "title": "Feature based methods for structure and motion estimation", "journal": "", "year": "1999", "authors": "P H S Torr; A Zisserman"}, {"ref_id": "b24", "title": "A new technique for full autonomous and efficient 3D robotics hand/eye calibration", "journal": "IEEE Journal of Robotics and Automation", "year": "1989", "authors": "R Y Tsai; R K Lenz"}, {"ref_id": "b25", "title": "Alignment by maximization of mutual information", "journal": "", "year": "1995", "authors": "P Viola; Iii Wells; W "}, {"ref_id": "b26", "title": "Metric calibration of a stereo rig", "journal": "", "year": "1995", "authors": "A Zisserman; P A Beardsley; Reid ; I D "}], "figures": [{"figure_label": "3", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 3 .3Figure 3. Alignment of non-overlapping sequences. (a) and (b) are temporally corresponding frames from sequences S and S . The correct time shift was automatically detected. (c) shows one frame in the combined sequence after spatio-temporal alignment. Note the accuracy of the spatial and temporal alignment of the running person. For color sequences see www.wisdom.weizmann.ac.il/NonOverlappingSeqs.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "5.2.(b)), that can barely be seen in the corresponding low resolution wide-view frame (Fig. 5.2 (a)). The same holds for the Pagoda in Fig. 5.3 (b).", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 .4Figure 4. Wide-screen movies generation. (a) and (b) are temporally corresponding frames from sequences S and S . The correct time shift was automatically detected. (c) shows one frame in the combined sequence. Corresponding video frames were averaged after spatio-temporal alignment. The small overlapping area was not used in the estimation process, but only for verification (see text). Note the accuracy of the spatial and temporal alignment of the soccer player in the overlapping region. For color sequences see www.wisdom.weizmann.ac.il/NonOverlappingSeqs.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 .5Figure 5. Finding zoomed region. This figure displays three different examples (one at each row), each one with different zoom factor. The left column (1.a, 2.a, 3.a) display one frame from each of the three wide-FOV sequences. The temporally corresponding frames from the corresponding narrow-FOV sequences are displayed in the center column (1.b, 2.b, 3.b). The correct time shift was automatically detected for each pair of narrow/wide FOV sequences. Each image on the right column shows super-position of corresponding frames of the two sequences after spatiotemporal alignment, displayed by color averaging (1.c, 2.c, 3.c). For color sequences see www.wisdom.weizmann.ac.il/NonOverlappingSeqs.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 .6Figure6. Multi-sensor alignment. (a) and (b) are temporally corresponding frames from the visible-light and near-IR sequences, respectively (the temporal alignment was automatically detected). The inside of the building is visible only in the visible-light sequence, while the IR sequence captures the details outdoors (e.g., the dark trees, the sign, the bush). (c) shows the results of fusing the two sequences after spatio-temporal alignment. The fused sequence preserves the details from both sequences. Note the high accuracy of alignment (both in time and in space) of the walking lady. For more details see text. For color sequences see www.wisdom.weizmann.ac.il/NonOverlappingSeqs.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "6.2.1. Basic Constraints. Similar (conjugate) matrices (e.g., B and G) have the same eigenvalues but different eigenvectors. Their eigenvectors are related by H . If u b is an eigenvector of B with corresponding eigenvalue \u03bb, then H u b is an eigenvector of G with the same eigenvalue \u03bb: G(H u b ) = \u03bb(H u b ).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "h = [H 11 H 12 H 13 H 21 H 22 H 23 H 31 H 32 H 33 ] t , Eq. (6) can be rewritten as a set of linear equations in h:", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Quantitative results.", "figure_data": "AppliedRecoveredMax residualtransformationtransformationmisalignmentHorizontal shiftHorizontal shift0.7 pixelsof 352 pixelsof 351.6 pixelsZoom factor = 2Zoom factor = 1.99920.4 pixelsZoom factor = 4Zoom factor = 4.00480.4 pixelsRotation by 180 \u2022Rotation by 180.00 \u20220.01 pixelsThis table summarizes the quantitative results with respectto ground truth. Each row corresponds to one experiment."}], "formulas": [{"formula_id": "formula_0", "formula_text": "H T i p i \u223c = H p i+1 \u223c = p i+1 \u223c = T i p i \u223c = T i H p i (1)", "formula_coordinates": [3.0, 337.07, 561.88, 187.49, 14.41]}, {"formula_id": "formula_1", "formula_text": "H T i \u223c = T i H. (2", "formula_coordinates": [3.0, 392.99, 658.35, 127.69, 14.41]}, {"formula_id": "formula_2", "formula_text": ")", "formula_coordinates": [3.0, 520.69, 661.88, 3.87, 8.92]}, {"formula_id": "formula_3", "formula_text": "T i \u223c = H T i H \u22121 , or T i = s i H T i H \u22121 (3)", "formula_coordinates": [3.0, 310.12, 682.67, 214.44, 50.7]}, {"formula_id": "formula_4", "formula_text": "eig(T i ) = s i eig(T i ) (4)", "formula_coordinates": [4.0, 138.94, 352.15, 146.51, 11.8]}, {"formula_id": "formula_5", "formula_text": "sim(T i , T i ) = eig(T i ) t eig(T i ) eig(T i ) eig(T i ) , (5", "formula_coordinates": [4.0, 107.48, 428.99, 174.1, 26.8]}, {"formula_id": "formula_6", "formula_text": ")", "formula_coordinates": [4.0, 281.58, 437.68, 3.87, 8.92]}, {"formula_id": "formula_7", "formula_text": "s i H T i \u2212 T i H = 0 (6)", "formula_coordinates": [4.0, 142.17, 687.99, 143.29, 11.8]}, {"formula_id": "formula_8", "formula_text": "M i h = 0 (7)", "formula_coordinates": [4.0, 399.66, 165.59, 124.9, 10.62]}, {"formula_id": "formula_9", "formula_text": "M i = \uf8ee \uf8f0 s i T i t \u2212 T i 11 I \u2212T i 12 I \u2212T i 13 I \u2212T i 21 I s i T t \u2212 T i 22 I \u2212T i 23 I \u2212T i 31 I \u2212T i 32 I s i T t \u2212 T i 33 I \uf8f9 \uf8fb 9\u00d79", "formula_coordinates": [4.0, 311.17, 206.72, 211.75, 46.52]}, {"formula_id": "formula_10", "formula_text": "A h = 0 (8)", "formula_coordinates": [4.0, 403.5, 416.48, 121.06, 9.96]}, {"formula_id": "formula_11", "formula_text": "A = [ M 1 . . . M n", "formula_coordinates": [4.0, 424.94, 432.33, 31.1, 22.76]}, {"formula_id": "formula_12", "formula_text": "F i = [e i ] \u00d7 T i", "formula_coordinates": [5.0, 152.96, 153.74, 49.33, 10.71]}, {"formula_id": "formula_13", "formula_text": "(e i ) x = [h 1 h 2 h 3 ] e i [h 7 h 8 h 9 ] e i (e i ) y = [h 4 h 5 h 6 ] e i [h 7 h 8 h 9 ] e i (9", "formula_coordinates": [5.0, 321.79, 148.61, 198.9, 23.76]}, {"formula_id": "formula_14", "formula_text": ")", "formula_coordinates": [5.0, 520.69, 155.6, 3.87, 8.92]}, {"formula_id": "formula_15", "formula_text": "e t i 0 t \u2212 (e i ) x e t i 0 t e t i \u2212 (e i ) y e t i 2\u00d79 h = 0 (10", "formula_coordinates": [5.0, 365.94, 226.54, 154.47, 30.95]}, {"formula_id": "formula_16", "formula_text": ")", "formula_coordinates": [5.0, 520.41, 236.47, 4.15, 8.92]}, {"formula_id": "formula_17", "formula_text": "SIM( t) = i sim(T i , T i+ t ) 2 (11", "formula_coordinates": [5.0, 354.02, 712.18, 166.39, 21.28]}, {"formula_id": "formula_18", "formula_text": ")", "formula_coordinates": [5.0, 520.41, 714.33, 4.15, 8.92]}, {"formula_id": "formula_19", "formula_text": "SIM( t) = i sim T 5(i+1) 5i , T 6(i+1)+ t 6i+ t 2 (12)", "formula_coordinates": [6.0, 91.7, 303.06, 193.76, 25.93]}, {"formula_id": "formula_20", "formula_text": "1 | \u2265 |\u03bb 2 | \u2265 |\u03bb 3 |). Denote by u b 1 , u b 2 , u b 3 the corresponding eigenvectors with unit norm ( u b 1 = u b 2 = u b 3 = 1)", "formula_coordinates": [10.0, 309.37, 278.73, 215.19, 35.1]}, {"formula_id": "formula_21", "formula_text": "H u b = \u03b1u g (13", "formula_coordinates": [10.0, 394.89, 654.47, 125.52, 10.62]}, {"formula_id": "formula_22", "formula_text": ")", "formula_coordinates": [10.0, 520.41, 655.39, 4.15, 8.92]}, {"formula_id": "formula_23", "formula_text": "H u b j = \u03b1 j u g 1 + \u03b2 j u g 2 (14)", "formula_coordinates": [11.0, 132.49, 273.74, 152.96, 12.4]}, {"formula_id": "formula_24", "formula_text": "1. \u03bb i = \u03bb j = \u03bb k . This implies V i = V j = V k and dim(V i ) = dim(V j ) = dim(V k ) = 1. 2. \u03bb i = \u03bb j = \u03bb k (V i = V j = V k ).", "formula_coordinates": [11.0, 70.26, 548.71, 215.19, 34.53]}, {"formula_id": "formula_25", "formula_text": "(a) dim(V i = V j ) = 2, and dim(V k ) = 1. (b) dim(V i = V j ) = 1, and dim(V k ) = 1. 3. \u03bb i = \u03bb j = \u03bb k .", "formula_coordinates": [11.0, 70.26, 605.5, 177.97, 43.5]}, {"formula_id": "formula_26", "formula_text": "\u03bb i = \u03bb j = \u03bb k |V i | = |V j | = |V k | = 1 6 (2.a) \u03bb i = \u03bb j = \u03bb k |V i = V j | = 2, |V k | = 1 4 (2.b) \u03bb i = \u03bb j = \u03bb k |V i = V j | = 1, |V k | = 1 4 (3.a) \u03bb i = \u03bb j = \u03bb k |V i = V j = V k | = 1 2 (3.b) \u03bb i = \u03bb j = \u03bb k |V i = V j = V k | = 2 2 (3.c) \u03bb i = \u03bb j = \u03bb k |V i = V j = V k | = 3 0", "formula_coordinates": [11.0, 309.37, 171.63, 193.88, 73.35]}, {"formula_id": "formula_27", "formula_text": "T i ) = max p\u2208I i T i T Reverse i p \u2212 p (15", "formula_coordinates": [12.0, 141.76, 238.64, 139.55, 17.37]}, {"formula_id": "formula_28", "formula_text": ")", "formula_coordinates": [12.0, 281.31, 240.79, 4.15, 8.92]}], "doi": ""}