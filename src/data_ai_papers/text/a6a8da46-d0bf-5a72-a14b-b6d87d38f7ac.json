{"title": "Solving high-dimensional parabolic PDEs using the tensor train format", "authors": "Lorenz Richter; Leon Sallandt; Nikolas N\u00fcsken", "pub_date": "", "abstract": "High-dimensional partial differential equations (PDEs) are ubiquitous in economics, science and engineering. However, their numerical treatment poses formidable challenges since traditional gridbased methods tend to be frustrated by the curse of dimensionality. In this paper, we argue that tensor trains provide an appealing approximation framework for parabolic PDEs: the combination of reformulations in terms of backward stochastic differential equations and regression-type methods in the tensor format holds the promise of leveraging latent low-rank structures enabling both compression and efficient computation. Following this paradigm, we develop novel iterative schemes, involving either explicit and fast or implicit and accurate updates. We demonstrate in a number of examples that our methods achieve a favorable trade-off between accuracy and computational efficiency in comparison with state-of-the-art neural network based approaches.", "sections": [{"heading": "Introduction", "text": "While partial differential equations (PDEs) offer one of the most elegant frameworks for modeling in economics, science and engineering, their practical use is often limited by the fact that solving those equations numerically becomes notoriously difficult in high-dimensional settings. The socalled \"curse of dimensionality\" refers to the phenomenon that the computational effort scales exponentially in the dimension, rendering classical grid based methods infeasible. In recent years there have been fruitful developments in combining Monte Carlo based algorithms with neural networks in order to tackle high-dimensional problems in a way that seemingly does not suffer from this curse, resting primarily on stochastic representations of the PDEs under consideration (E et al., 2017;Raissi et al., 2019;E et al., 2019;Hur\u00e9 et al., 2020;N\u00fcsken & Richter, 2020). Many of the suggested algorithms perform remarkably well in practice and some theoretical results proving beneficial approximation properties of neural networks in the PDE setting are now available (Jentzen et al., 2018). Still, a complete picture remains elusive, and the optimization aspect in particular continues to pose challenging and mostly open problems, both in terms of efficient implementations and theoretical understanding. Most importantly for practical applications, neural network training using gradient descent type schemes may often take a very long time to converge for complicated PDE problems.\nInstead of neural networks (NN), we propose relying on the tensor train (TT) format (Oseledets, 2011) to approximate the solutions of high-dimensional PDEs. As we argue in the course of this article, the salient features of tensor trains make them an ideal match for the stochastic methods alluded to in the previous paragraph: First, tensor trains have been designed to tackle high-dimensional problems while still being computationally cheap by exploiting inherent lowrank structures (Kazeev & Khoromskij, 2012;Kazeev et al., 2016;Dolgov et al., 2012) typically encountered in physically inspired PDE models. Second, built-in orthogonality relations allow fast and robust optimization in regression type problems arising naturally in stochastic backward formulations of parabolic PDEs. Third, the function spaces corresponding to tensor trains can be conveniently extended to incorporate additional information such as initial or final conditions imposed on the PDE to be solved. Last but not least, tensor trains allow for extremely efficient and explicit computation of first and higher order derivatives.\nTo develop TT-based solvers for parabolic PDEs, we follow (Bouchard & Touzi, 2004;Hur\u00e9 et al., 2020) and first identify a backward stochastic differential equation (BSDE) representation of the PDE, naturally giving rise to iterative backward schemes for a numerical treatment. We suggest two versions of our algorithm, allowing to adjust the tradeoff between accuracy and speed according to the application: The first scheme is explicit, relying on L 2 projections (Gobet et al., 2005) that can be solved efficiently using an alternating least squares algorithm and explicit expressions for the minimizing parameters (see Section 3.1). The second arXiv:2102.11830v2 [stat.ML] 17 Jul 2021 scheme is implicit and involves a nested iterative procedure, holding the promise of more accurately resolving highly nonlinear relationships at the cost of an increased computational load. For theoretical underpinning, we prove the convergence of the nested iterative scheme in Section 3.2.\nTo showcase the performance of the TT-schemes, we evaluate their outputs on various high-dimensional PDEs (including toy examples and real-world problems) in comparison with NN-based approximations. In all our examples, the TT results prove competitive, and often considerably more accurate when low-rank structures can be identified and captured by the underlying ansatz spaces. At the same time, the runtimes of the TT-schemes are usually significantly smaller, with the explicit L 2 -projection-based algorithm beating the corresponding NN alternative by orders of magnitude in terms of computational time. Even the more accurate algorithm based on nested nonlinear iterations often proves to be substantially faster than NN training.", "publication_ref": ["b15", "b60", "b16", "b37", "b53", "b39", "b54", "b43", "b42", "b13", "b6", "b37", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Previous work", "text": "Using numerical discretizations of BSDEs to solve PDEs originated in (Bouchard & Touzi, 2004;Gobet et al., 2005), while regression based methods for PDE-related problems in mathematical finance have already been proposed in (Longstaff & Schwartz, 2001). An iterative method motivated by BSDEs and approached with neural networks has been introduced in (E et al., 2017), making the approximation of high-dimensional PDE problems feasible. Solving explicit backwards schemes with neural networks has been suggested in (Beck et al., 2019) and an implicit method similar to the one developed in this paper has been suggested in (Hur\u00e9 et al., 2020). Another interesting method to approximate PDE solutions relies on minimizing a residual term on uniformly sampled data points as suggested in (Sirignano & Spiliopoulos, 2018;Raissi et al., 2019). Rooted in quantum physics under the name matrix product states, tensor trains have been introduced to the mathematical community in (Oseledets, 2011) to tackle the curse of dimensionality. Note that tensor trains are a special case of hierarchical tensor networks, which have been developed in (Hackbusch & K\u00fchn, 2009). For good surveys and more details, see (Hackbusch, 2014;Hackbusch & Schneider, 2014;Szalay et al., 2015;Bachmayr et al., 2016). Tensor trains have already been applied to parametric PDEs, see e.g. (Dolgov et al., 2015;Eigel et al., 2017;Dektor et al., 2020), Hamilton-Jacobi-Bellman PDEs (Horowitz et al., 2014;Stefansson & Leong, 2016;Gorodetsky et al., 2018;Dolgov et al., 2019;Oster et al., 2019;Fackeldey et al., 2020;Chen & Lu, 2021), and PDEs of other types, see e.g. (Khoromskij, 2012;Kormann, 2015;Lubasch et al., 2018).\nThe paper is organized as follows: In Section 2 we motivate our algorithm by recalling the stochastic PDE representation in terms of BSDEs as well as two appropriate discretization schemes. In Section 3 we review the tensor train format as a highly efficient framework for approximating highdimensional functions by detecting low-rank structures and discuss how those structures can be exploited in the numerical solution of BSDEs. Finally, in Section 4 we provide multiple high-dimensional numerical examples to illustrate our claims.", "publication_ref": ["b6", "b22", "b50", "b4", "b37", "b62", "b60", "b54", "b26", "b25", "b27", "b64", "b2", "b11", "b17", "b10", "b34", "b63", "b23", "b12", "b56", "b18", "b8", "b44", "b47", "b51"], "figure_ref": [], "table_ref": []}, {"heading": "Solving PDEs via BSDEs", "text": "In this section we recall how backward stochastic differential equations (BSDEs) can be used to design iterative algorithms for approximating the solutions of high-dimensional PDEs. Throughout this work, we consider parabolic PDEs of the form\n(\u2202 t +L)V (x, t)+h(x, t, V (x, t), (\u03c3 \u2207V )(x, t)) = 0 (1) for (x, t) \u2208 R d \u00d7 [0, T ], a nonlinearity h : R d \u00d7[0, T ] \u00d7 R \u00d7 R d \u2192 R, and a differential operator L = 1 2 d i,j=1 (\u03c3\u03c3 ) ij (x, t)\u2202 xi \u2202 xj + d i=1 b i (x, t)\u2202 xi , (2) with coefficient functions b : R d \u00d7[0, T ] \u2192 R d and \u03c3 : R d \u00d7[0, T ] \u2192 R d\u00d7d . The terminal value is given by V (x, T ) = g(x),(3)\nfor a specified function g : R d \u2192 R. Note that by using the time inversion t \u2192 T \u2212t, the terminal value problem (1)-(3) can readily be transformed into an initial value problem.\nBSDEs were first introduced in (Bismut, 1973) and their systematic study began with (Pardoux & Peng, 1990). Loosely speaking, they can be understood as nonlinear extensions of the celebrated Feynman-Kac formula (Pardoux, 1998), relating the PDE (1) to the stochastic process X s defined by\ndX s = b(X s , s) ds + \u03c3(X s , s) dW s , X 0 = x 0 , (4)\nwhere b and \u03c3 are as in (2) and W s is a standard ddimensional Brownian motion. The key idea is then to define the processes\nY s = V (X s , s), Z s = (\u03c3 \u2207V )(X s , s) (5)\nas representations of the PDE solution and its gradient, and apply It\u00f4's lemma to obtain\ndY s = \u2212h(X s , s, Y s , Z s ) ds + Z s \u2022 dW s ,(6)\nwith terminal condition Y T = g(X T ). Noting that the processes Y s and Z s are adapted 1 to the filtration generated by the Brownian motion W s , they should indeed be understood as backward processes and not be confused with time-reversed processes. A convenient interpretation of the relations in ( 5) is that solving for the processes Y s and Z s under the constraint (6) corresponds to determining the solution of the PDE (1) (and its gradient) along a random grid which is provided by the stochastic process X s defined in (4).", "publication_ref": ["b5", "b58", "b57"], "figure_ref": [], "table_ref": []}, {"heading": "Numerical approximation of BSDEs", "text": "The BSDE formulation (6) opens the door for Monte Carlo algorithms aiming to numerically approximate Y s and Z s , and hence yielding approximations of solutions to the PDE (1) according to (5), see (Bouchard & Touzi, 2004;Gobet et al., 2005). In this section we discuss suitable discretizations of ( 6) and corresponding optimization problems that will provide the backbone for TT-schemes to be developed in Section 3.\nTo this end, let us define a discrete version of the process (4) on a time grid\n0 = t 0 < t 1 < \u2022 \u2022 \u2022 < t N = T by X n+1 = X n + b( X n , t n )\u2206t + \u03c3( X n , t n )\u03be n+1 \u221a \u2206t, (7)\nwhere n \u2208 {0, . . . , N \u2212 1} enumerates the steps, \u2206t = t n+1 \u2212 t n is the stepsize, \u03be n+1 \u223c N (0, Id d\u00d7d ) are normally distributed random variables and X 0 = x 0 provides the initial condition. Two 2 discrete versions of the backward process (6) are given by\nY n+1 = Y n \u2212 h n+1 \u2206t + Z n \u2022 \u03be n+1 \u221a \u2206t,(8a)\nY n+1 = Y n \u2212 h n \u2206t + Z n \u2022 \u03be n+1 \u221a \u2206t,(8b)\nwhere we have introduced the shorthands\nh n = h( X n , t n , Y n , Z n ),(9a)\nh n+1 = h( X n+1 , t n+1 , Y n+1 , Z n+1 ).(9b)\nFinally, we complement (8a) and (8b) by specifying the terminal condition Y N = g( X N ). The reader is referred to Appendix E for further details.\nBoth of our schemes solve the discrete processes (8a) and (8b) backwards in time, an approach which is reminiscent of the dynamic programming principle in optimal control theory (Fleming & Rishel, 2012), where the problem is divided into a sequence of subproblems. To wit, we start with the known terminal value Y N = g( X N ) and move backwards in iterative fashion until reaching Y 0 . Throughout this procedure, we posit functional approximations\nV n ( X n ) \u2248 Y n \u2248 V ( X n , t n )\nto be learnt in the update step n + 1 \u2192 n which can either be based on (8a) or on (8b):\nStarting with the former, it can be shown by leveraging the relationship between conditional expectations and L 2projections (see Appendix E) that solving (8a) is equivalent to minimizing\nE V n ( X n ) \u2212 h n+1 \u2206t \u2212 V n+1 ( X n+1 ) 2(10)\nwith respect to V n . Keeping in mind that V n+1 is known from the previous step this results in an explicit scheme. Methods based on (10) have been extensively analyzed in the context of linear ansatz spaces for V n and we refer to (Zhang, 2004;Gobet et al., 2005) as well as to Appendix E.\nMoving on to (8b), we may as well penalize deviations in this relation by minimizing the alternative loss\nE[( V n ( X n ) \u2212 h n \u2206t \u2212 V n+1 ( X n+1 ) + \u03c3 ( X n , t n )\u2207 V n ( X n ) \u2022 \u03be n+1 \u221a \u2206t) 2 ],(11)\nwith respect to V n , see (Hur\u00e9 et al., 2020). In analogy to (9a) we use the shorthand notation\nh n = h( X n , t n , V n ( X n ), \u03c3 ( X n , t n )\u2207 V n ( X n )),(12)\nnoting that since h n depends on V n , approaches based on (11) will necessarily lead to implicit schemes. At the same time, we expect algorithms based on (11) to be more accurate in highly nonlinear scenarios as the dependence in h is resolved to higher order.", "publication_ref": ["b6", "b22", "b19", "b66", "b22", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "Solving BSDEs via tensor trains", "text": "In this section we discuss the functional approximations V n in terms of the tensor train format, leading to efficient optimization procedures for (10) and (11). Encoding functions defined on high-dimensional spaces using traditional methods such as finite elements, splines or multi-variate polynomials leads to a computational complexity that scales exponentially in the state space dimension d. However, interpreting the coefficients of such ansatz functions as entries in a high-dimensional tensor allows us to use tensor compression methods to reduce the number of parameters. To this end, we define a set of functions {\u03c6 1 , . . . , \u03c6 m } with \u03c6 i : R \u2192 R , e.g. one-dimensional polynomials or finite elements. The approximation V of V : R d \u2192 R takes the form\nV (x 1 , . . . , x d ) = m i1=1 \u2022 \u2022 \u2022 m i d =1 c i1,...,i d \u03c6 i1 (x 1 ) \u2022 \u2022 \u2022 \u03c6 i d (x d ),(13)\nmotivated by the fact that polynomials and other tensor product bases are dense in many standard function spaces (Sickel & Ullrich, 2009). Note that for the sake of simplicity we choose the set of ansatz functions to be the same in every dimension (see Appendix A for more general statements). As expected, the coefficient tensor c \u2208 R m\u00d7m\u00d7\u2022\u2022\u2022\u00d7m \u2261 R m d suffers from the curse of dimensionality since the number of entries increases exponentially in the dimension d. In what follows, we review the tensor train format to compress the tensor c.\nFor the sake of readability we will henceforth write c i1,...,i d = c[i 1 , . . . , i d ] and represent the contraction of the last index of a tensor w 1 \u2208 R r1\u00d7m\u00d7r2 with the first index of another tensor w 2 \u2208 R r2\u00d7m\u00d7r3 by\nw = w 1 \u2022 w 2 \u2208 R r1\u00d7m\u00d7m\u00d7r3 ,(14a)\nw[i 1 , i 2 , i 3 , i 4 ] = r2 j=1 w 1 [i 1 , i 2 , j]w 2 [j, i 3 , i 4 ].(14b)\nIn the literature on tensor methods, graphical representations of general tensor networks are widely used. In these pictorial descriptions, the contractions \u2022 of the component tensors are indicated as edges between vertices of a graph.\nAs an illustration, we provide the graphical representation of an order-4 tensor and a tensor train representation (see Definition 1 below) in Figure 1. Further examples can be found in Appendix A. Tensor train representations of c can now be defined as follows (Oseledets, 2011).\nu 1 u 2 u 3 u 4 c = r 1 r 2 r 3 m m m m m m m m\nDefinition 1 (Tensor Train). Let c \u2208 R m\u00d7\u2022\u2022\u2022\u00d7m . A factor- ization c = u 1 \u2022 u 2 \u2022 \u2022 \u2022 \u2022 \u2022 u d ,(15)\nwhere\nu 1 \u2208 R m\u00d7r1 , u i \u2208 R ri\u22121\u00d7m\u00d7ri , 2 \u2264 i \u2264 d \u2212 1, u d \u2208 R r d\u22121 \u00d7m\n, is called tensor train representation of c. We say that u i are component tensors. The tuple of the dimensions (r 1 , . . . , r d\u22121 ) is called the representation rank and is associated with the specific representation (15). In contrast to that, the tensor train rank (TT-rank) of c is defined as the minimal rank tuple r = (r 1 , . . . , r d\u22121 ), such that there exists a TT representation of c with representation rank equal to r. Here, minimality of the rank is defined in terms of the partial order relation on N d given by\ns t \u21d0\u21d2 s i \u2264 t i for all 1 \u2264 i \u2264 d, for r = (r 1 , . . . , r d ), s = (s 1 , . . . , s d ) \u2208 N d .\nIt can be shown that every tensor has a TT-representation with minimal rank, implying that the TT-rank is well defined (Holtz et al., 2012b). An efficient algorithm for computing a minimal TT-representation is given by the Tensor-Train-Singular-Value-Decomposition (TT-SVD) (Oseledets & Tyrtyshnikov, 2009). Additionally, the set of tensor trains with fixed TT-rank forms a smooth manifold, and if we include lower ranks, an algebraic variety is formed (Landsberg, 2012;Kutschan, 2018).\nIntroducing the compact notation\n\u03c6 : R \u2192 R m , \u03c6(x) = [\u03c6 1 (x), . . . , \u03c6 m (x)],\nthe TT-representation of ( 13) is then given as\nV (x) = m i1 \u2022 \u2022 \u2022 m i d r1 j1 \u2022 \u2022 \u2022 r d\u22121 j d\u22121 u 1 [i 1 , j 1 ]u 2 [j 1 , i 2 , j 2 ] \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 u d [j d\u22121 , i d ]\u03c6(x 1 )[i 1 ] \u2022 \u2022 \u2022 \u03c6(x d )[i d ]. (16\n)\nThe corresponding graphical TT-representation (with d = 4 for definiteness) is then given as follows:\nu 1 u 2 u 3 u 4 \u03c6(x 1 ) \u03c6(x 2 ) \u03c6(x 3 ) \u03c6(x 4 ) V (x) = r 1 r 2 r 3 m m m m Figure 2. Graphical representation of V : R 4 \u2192 R.", "publication_ref": ["b61", "b54", "b33", "b55", "b49", "b48"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Optimization on the TT manifold", "text": "The multilinear structure of the tensor product enables efficient optimization of ( 10) and ( 11) within the manifold structure by means of reducing a high-dimensional linear equation in the coefficient tensor to small linear subproblems on the component tensors 3 . For this, we view ( 10) and ( 11) abstractly as least squares problems on a linear space U \u2282 L 2 (\u2126), where \u2126 \u2282 R d is a bounded Lipschitz domain.\nOur objective is then to find\narg min V \u2208U J j=1 | V (x j ) \u2212 R(x j )| 2 ,(17)\nwhere {x 1 , . . . , x J } \u2282 \u2126 are data points obtained from samples of X n , and R : \u2126 \u2192 R stands for the terms in ( 10) and ( 11 \nc = (A A) \u22121 A r \u2208 R M ,(18)\nwhere A = [a ij ] \u2208 R J\u00d7M with a ij = b j (x i ) and r j = R(x j ) \u2208 R J .\nThe alternating least-squares (ALS) algorithm (Holtz et al., 2012a) reduces the high-dimensional system (18) in the coefficient tensor c to small linear subproblems in the component tensors u i as follows: Since the tensor train format ( 15) is a multilinear parametrization of c, fixing every component tensor but one (say u i ) isolates a remaining lowdimensional linear parametrization with associated local linear subspace U loc,i . The number M i of remaining parameters (equivalently, the dimension of U loc,i ) is given by the number of coefficients in the component tensor u i , i.e. M i = r i\u22121 m r i . If the ranks r i , r i\u22121 are significantly smaller than M , this results in a low-dimensional hence efficiently solvable least-squares problem. Iterating over the component tensors u i then leads to an efficient scheme for solving high-dimensional least-squares problems with low rank structure. Basis functions in U loc,i are obtained from the order 3 tensor b loc depicted in Figure 3 (note the three open edges). A simple reshape to an order one tensor then yields the desired basis functions, stacked onto each other, i.e. b\nloc,i (x) = [b loc,i 1 (x), b loc,i 2 (x), . . . , b loc,i Mi (x)\n]. More precisely, the local basis functions can be identified using the open edges in Figure 3 as follows. Assuming u 2 is being optimized, we notice that the tensor \u03c6(x 1 ) \u2022 u 1 is a mapping from R \u2192 R r1 , which means that we can identify r 1 many one-dimensional functions. Note that this corresponds to the left part of the tensor picture in Figure 3. Further, we have that \u03c6(x 2 ) is a vector consisting of m one-dimensional functions, which is the middle part of the above tensor picture. The right part, consisting of the contractions between \u03c6(x 2 ), u 3 , u 4 , and \u03c6(x 4 ), is a set of two-dimensional functions with cardinality r 2 . Taking the tensor product of the above functions yields an r 1 mr 2 dimensional function space of four-dimensional functions, which is exactly the span of the local basis functions.\nFurther details as well as explicit formulas are given in Appendix A.1. In many situations the terminal condition g, defined in (3), is not part of the ansatz space just defined. This is always the case if g is not in tensor-product form. However, as the ambient space R m d is linear, g can be straightforwardly added 4 4 We note that the idea of enhancing the ansatz space has been to the ansatz space, potentially increasing its dimension to m d + 1. Whenever a component tensor u i is optimized in the way described above, we simply add g to the set of local basis functions, obtaining as a new basis\nu 1 u 3 u 4 \u03c6(x 1 ) \u03c6(x 2 ) \u03c6(x 3 ) \u03c6(x 4 ) b loc,i (x) = r 1 r 2 r 3 m m m m\nb loc,i g = {b loc,i 1 , . . . , b loc,i m , g},(19)\nonly marginally increasing the complexity of the leastsquares problem. In our numerical tests we have noticed substantial improvements using the extension (19). Incorporating the terminal condition, the representation of the PDE solution takes the form depicted in Figure 4, for some c g \u2208 R.\nu 1 u 2 u 3 u 4 \u03c6(x 1 ) \u03c6(x 2 ) \u03c6(x 3 ) \u03c6(x 4 ) V (x) = + c g g(x) r 1 r 2 r 3 m m m m Figure 4. Graphical representation of V : R 4 \u2192 R.\nSumming up, we briefly state a basic ALS algorithm with our adapted basis b loc,i :\nAlgorithm 1 simple ALS algorithm Input: initial guess u 1 \u2022 u 2 \u2022 \u2022 \u2022 \u2022 \u2022 u d . Output: result u 1 \u2022 u 2 \u2022 \u2022 \u2022 \u2022 \u2022 u d . repeat for i = 1\nto d do identify the local basis functions (19), parametrized by u k , k = j optimize u i using the local basis by solving the local least squares problem end for until noChange is true The drawback of Algorithm 1 is that the ranks of the tensor approximation have to be chosen in advance. However, there are more involved rank-adaptive versions of the ALS algorithm, providing a convenient way of finding suitable ranks. In this paper we make use of the rank-adaptive stable alternating least-squares algorithm (SALSA) (Grasedyck & Kr\u00e4mer, 2019). However, as we will see in Section 4, we can in fact oftentimes find good solutions by setting the rank to be (1, . . . , 1) \u2208 N d\u22121 , enabling highly efficient computations.\nBy straightforward extensions, adding the terminal condition g to to set of local ansatz functions can similarly be implemented into more advanced, rank adaptive ALS algorithms, which is exactly what we do for our version of SALSA.\nsuggested in (Zhang, 2017) in the context of linear parametrizations.", "publication_ref": ["b32", "b24", "b67"], "figure_ref": ["fig_3", "fig_3", "fig_3"], "table_ref": []}, {"heading": "Handling implicit regression problems", "text": "The algorithms described in the previous section require the regression problem to be explicit such as in (10). In contrast, the optimization in ( 11) is of implicit type, as h n contains the unknown V n . In order to solve (11), we therefore choose an initial guess V 0 n and iterate the optimization of\nE[( V k+1 n ( X n ) \u2212 h( X n , t n , Y k n , Z k n )\u2206t+ Z k n \u2022 \u03be n+1 \u221a \u2206t \u2212 V n+1 ( X n+1 )) 2 ] (20)\nwith respect to V k+1 n until convergence (see Appendix C for a discussion of appropriate stopping criteria). In the above display,\nY k n = V k n ( X n ) and Z k n = (\u03c3 \u2207 V k n )( X n\n) are computed according to (5). For theoretical foundation, we guarantee convergence of the proposed scheme when the step size \u2206t is small enough.\nTheorem 3.1. Assume that U \u2282 L 2 (\u2126) \u2229 C \u221e b (\u2126) is a finite dimensional linear subspace, that \u03c3(x, t) is nondegenerate for all (x, t) \u2208 [0, T ] \u00d7 R d ,\nand that h is globally Lipschitz continuous in the last two arguments. Then there exists \u03b4 > 0 such that the iteration (20) converges for all \u2206t \u2208 (0, \u03b4).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof. See Appendix B.", "text": "Remark 2. In order to ensure the boundedness assumption in Theorem 3.1 and to stabilize the computation we add a regularization term involving the Frobenius norm of the coefficient tensor to the objective in (20). Choosing an orthonormal basis we can then relate the Frobenius norm to the associated norm in the function space by Parseval's identity. In our numerical tests we set our one-dimensional ansatz functions to be H 2 (a, b)-orthonormal 5 , where a and b are set to be approximately equal to the minimum and maximum of the samples X n , respectively. In Appendix D.1 we state the exact choices of a and b for the individual numerical tests. The corresponding tensor space (H 2 (a, b)) \u2297d = H 2 mix ([a, b]) d can be shown to be continuously embedded in W 1,\u221e (\u2126), guaranteeing boundedness of the approximations and their derivatives (Sickel & Ullrich, 2009). Remark 3 (Parameter initializations). Since we expect V (\u2022, t n ) to be close to V (\u2022, t n+1 ) for any n \u2208 {0, . . . , N \u2212 1}, we initialize the parameters of V 0 n as those obtained for V n+1 identified in the preceding time step.\nClearly, the iterative optimization of (20) is computationally more costly than the explicit scheme described in Section 3.1 that relies on a single optimization of the type (17) per time step. However, implicit schemes typically ensure improved convergence orders as well as robustness (Kloeden & Platen, 1992) and therefore hold the promise of more accurate approximations (see Section 4 for experimental confirmation). We note that the NN based approaches considered as baselines in Section 4 perform gradient descent for both the explicit and implicit schemes and therefore no significant differences in the corresponding runtimes are expected. For convenience, we summarize the developed methods in Algorithm 2.", "publication_ref": ["b61", "b46"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 2 PDE approximation", "text": "Input: initial parametric choice for the functions 10) or (11) (both depending on V n+1 ) using Monte Carlo minimize this quantity (explicitly or by iterative schemes) set V n to be the minimizer end for\nV n for n \u2208 {0, . . . , N \u2212 1} Output: approximation of V (\u2022, t n ) \u2248 V n along the tra- jectories for n \u2208 {0, . . . , N \u2212 1} Simulate K samples of the discretized SDE (7). Choose V N = g. for n = N \u2212 1 to 0 do approximate either (", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Numerical examples", "text": "In this section we consider some examples of highdimensional PDEs that have been addressed in recent articles and treat them as benchmark problems in order to compare against our algorithms with respect to approximation accuracy and computation time. We refer to Appendix C for implementation details and to Appendix D for additional experiments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Hamilton-Jacobi-Bellman equation", "text": "The Hamilton-Jacobi-Bellman equation (HJB) is a PDE for the so-called value function that represents the minimal costto-go in stochastic optimal control problems from which the optimal control policy can be deduced. As suggested in (E et al., 2017), we consider the HJB equation\n(\u2202 t + \u2206) V (x, t) \u2212 |\u2207V (x, t)| 2 = 0, (21a) V (x, T ) = g(x), (21b\n)\nwith g(x) = log 1 2 + 1 2 |x| 2 , leading to b = 0, \u03c3 = \u221a 2 Id d\u00d7d , h(x, s, y, z) = \u2212 1 2 |z| 2 (22)\nin terms of the notation established in Section 2. One appealing property of this equation is that (up to Monte Carlo approximation) a reference solution is available:\nV (x, t) = \u2212 log E e \u2212g(x+ \u221a T \u2212t\u03c3\u03be) ,(23)\nwhere \u03be \u223c N (0, Id d\u00d7d ) is a normally distributed random variable (see Appendix D.1 for further details).\nIn our experiments we consider d = 100, T = 1, \u2206t = 0.01, x 0 = (0, . . . , 0) and K = 2000 samples. In Table 1 we compare the explicit scheme stated in (10) with the implicit scheme from (11), once with TTs and once with NNs.\nFor the tensor trains we try different polynomial degrees, and it turns out that choosing constant ansatz functions is the best choice, while fixing the rank to be 1. For the NNs we use a DenseNet like architecture with 4 hidden layers (all the details can be found in Appendices C and D).\nWe display the approximated solutions at (x 0 , 0), the corresponding relative errors\nVn(x0)\u2212V ref (x0,0) V ref (x0,0)\nwith V ref (x 0 , 0) = 4.589992 being provided in (E et al., 2017), their computation times, as well as PDE and reference losses, which are specified in Appendix C. We can see that the TT approximation is both more accurate and much faster than the NN-based approaches, improving also on the results in (E et al., 2017;Beck et al., 2019). As it turns out that the explicit scheme for NNs is worse in terms of accuracy than its implicit counterpart in all our experiments, but takes a very similar amount of computation time we will omit reporting it for the remaining experiments. In Figures 5 and 6 we plot the reference solutions computed by (23) along two trajectories of the discrete forward process (7) in dimensions d = 10 and d = 100 and compare to the implicit TT and NN-based approximations. We can see that the TT approximations perform particularly well in the higher dimensional case d = 100. In Figure 7 we plot the mean relative error over time, as defined in Appendix C, indicating that both schemes are stable and where again the implicit TT scheme yields better results than the NN scheme.\nThe accuracy of the TT approximations is surprising given that the ansatz functions are constant in space. We further investigate this behavior in Table 2 and observe that the required polynomial degree decreases with increasing dimension. While similar \"blessings of dimensionality\" have been reported and discussed (see, for instance, Figure 3 in (Bayer et al., 2021) and Section 1.3 in (Khoromskij, 2012)), a thorough theoretical understanding is still lacking. To  guide intuition, we would like to point out that the phenomenon that high-dimensional systems become in some sense simpler is well known from the theory of interacting particle systems (\"propagation of chaos\", see (Sznitman, 1991)): In various scenarios, the joint distribution of a large number of particles tends to approximately factorize as the number of particles increases (that is, as the dimensionality of the joint state space grows large). It is plausible that similar approximate factorizations are relevant for highdimensional PDEs and that tensor methods are useful (i) to detect this effect and (ii) to exploit it. In this experiment, the black-box nature of neural networks does not appear to reveal such properties.\nd Polynomial degree 0 1 2 3 4 1 3.62e \u22121 3.60e \u22121 2.47e \u22123 3.86e \u22124 4.27e \u22122 2 1.03e \u22121 1.02e \u22121 1.87e \u22122 1.79e \u22122 1.79e \u22122 5\n1.55e \u22122 1.54e \u22122 1.03e \u22123 9.52e \u22124 1.96e \u22122 10 2.84e \u22123 2.86e \u22123 1.37e \u22123 1.34e \u22123 1.10e \u22121 50 1.17e \u22124 1.29e \u22124 2.79e \u22124 3.35e \u22124 6.96e \u22125 100 5.90e \u22125 4.99e \u22125 8.65e \u22125 1.23e \u22124 3.62e \u22125 ", "publication_ref": ["b15", "b4", "b3", "b44", "b65"], "figure_ref": ["fig_3"], "table_ref": ["tab_0", "tab_2"]}, {"heading": "HJB with double-well dynamics", "text": "In another example we consider again an HJB equation, however this time making the drift in the dynamics nonlinear, as suggested in (N\u00fcsken & Richter, 2020). The PDE becomes\n(\u2202 t + L) V (x, t) \u2212 1 2 |(\u03c3 \u2207V )(x, t)| 2 = 0, (24a) V (x, T ) = g(x), (24b)\nwith L as in ( 2), where now the drift is given as the gradient of the double-well potential\nb = \u2212\u2207\u03a8, \u03a8(x) = d i,j=1 C ij (x 2 i \u2212 1)(x 2 j \u2212 1) (25)\nand the terminal condition is g(\nx) = d i=1 \u03bd i (x i \u2212 1) 2 for \u03bd i > 0. Similarly as before a reference solution is available, V (x, t) = \u2212 log E e \u2212g(X T ) X t = x ,(26)\nwhere X t is the forward diffusion as specified in (4) (see again Appendix D.1 for details).\nFirst, we consider diagonal matrices C = 0.1 Id d\u00d7d , \u03c3 = \u221a 2 Id d\u00d7d , implying that the dimensions do not interact, and take T = 0.5, d = 50, \u2206t = 0.01, K = 2000, \u03bd i = 0.05. We set the TT-rank to 2, use polynomial degree 3 and refer to Appendix D for further details on the TT and NN configurations. Since in the solution of the PDE the dimensions do not interact either, we can compute a reference solution with finite differences. In Table 3 we see that the TT and NN approximations are compatible with TTs having an advantage in computational time.\nLet us now consider a non-diagonal matrix C = Id d\u00d7d + (\u03be ij ), where \u03be ij \u223c N (0, 0.1) are sampled once at the beginning of the experiment and further choose \u03c3 = \u221a 2 Id d\u00d7d , \u03bd i = 0.5, T = 0.3. We aim at the solution at TT impl TT expl NN impl V 0 (x 0 ) 9.6876 9.6865 9.6942 relative error 1.41e \u22123 1.53e \u22123 7.27e \u22124 reference loss 1.36e \u22123 3.25e \u22123 4.25e \u22123 PDE loss 3.62e \u22122 11.48 2.66e \u22121 computation time 95 16 1987 x 0 = (\u22121, . . . , \u22121) and compute a reference solution with (26) using 10 7 samples. We see in Table 4 that TTs are much faster than NNs, while yielding a similar performance.\nNote that due to the non-diagonality of C it is expected that the TTs are of rank larger than 2. For the explicit case we do not cap the ranks of the TT and the rank-adaptive solver finds ranks of mostly 4 and never larger than 6. Motivated by these results we cap the ranks at r i \u2264 6 in the implicit case and indeed they are obtained for nearly every dimension, as seen from the ranks below, [5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,5].\nThe results were obtained with polynomial degree 7. ", "publication_ref": ["b53"], "figure_ref": [], "table_ref": ["tab_3", "tab_4"]}, {"heading": "Cox-Ingersoll-Ross model", "text": "Our last example is taken from financial mathematics. As suggested in (Jiang & Li, 2021) we consider a bond price in a multidimensional Cox-Ingersoll-Ross (CIR) model, see also (Hyndman, 2007;Alfonsi et al., 2015). The underlying PDE is specified as\n\u2202 t V (x, t) + 1 2 d i,j=1 \u221a x i x j \u03b3 i \u03b3 j \u2202 xi \u2202 xj V (x, t) + d i=1 a i (b i \u2212 x i )\u2202 xi V (x, t) \u2212 max 1\u2264i\u2264d x i V (x, t) = 0. (27\n)\nHere, a i , b i , \u03b3 i \u2208 [0, 1] are uniformly sampled at the beginning of the experiment and V (T, x) = 1. We set d = 100.\nWe aim to estimate the bond price at the initial condition x 0 = (1, . . . , 1) . As there is no reference solution known, we rely on the PDE loss to compare our results. Table 5 shows that all three approaches yield similar results, while having a rather small PDE loss. For this test it is again sufficient to set the TT-rank to 1 and the polynomial degree to 3. The TT approximations seem to be slightly better and we note that the explicit TT scheme is again much faster.\nTT impl TT expl NN impl V 0 (x 0 ) 0.312 0.306 0.31087 PDE loss 5.06e \u22124 5.04e \u22124 7.57e \u22123 computation time 5281 197 9573\nTable 5. K = 1000, d = 100, x0 = [1, 1, . . . , 1]\nIn Table 6 we compare the PDE loss using different polynomial degrees for the TT ansatz function and see that we do not get any improvements with polynomials of degree larger than 1. Noticing the similarity between the results for polynomial degrees 1, 2, and 3, we further investigate by computing the value function along a sample trajectory in Figure 8, where we see that indeed the approximations with those polynomial degrees are indistinguishable. \nPolynom. degree 0 1 2 3 V 0 (x 0 ) 0.", "publication_ref": ["b40", "b38", "b1"], "figure_ref": ["fig_5"], "table_ref": ["tab_5"]}, {"heading": "Conclusions and outlook", "text": "In this paper, we have developed tensor train based approaches towards solving high-dimensional parabolic PDEs, relying on reformulations in terms of BSDEs. For the discretization of the latter, we have considered both explicit and implicit schemes, allowing for a trade-off between approximation accuracy and computational cost. Notably, the tensor train format specifically allows us to take advantage of the additional structure inherent in least-squares based formulations, particularly in the explicit case.\nMore elaborate numerical treatments for BSDEs (involving, for instance, multi-step and/or higher-order discretizations) have been put forward in the literature (Chassagneux, 2014;Crisan et al., 2014;Macris & Marino, 2020). Combining these with tensor based methods remains a challenging and interesting topic for future research. Finally, we believe that the \"blessing of dimensionality\" observed in Section 4.1 deserves a mathematically rigorous explanation; progress in this direction may further inform the design of scalable schemes for high-dimensional PDEs.", "publication_ref": ["b7", "b9", "b52"], "figure_ref": [], "table_ref": []}, {"heading": "A. Graphical notation for tensor trains", "text": "In this section we provide some further material on tensor networks and their graphic notation. Let us start by noting that a vector x \u2208 R n can be interpreted as a tensor.\nx n\nIn the graphic representation contractions between indices are denoted by a line between the tensors. Below we contract a tensor A \u2208 R n\u00d7m and x \u2208 R n , which results in an element of R m , representing the usual matrix-vector product.\nx A n m\nIn Figure 9 an order 3 tensor B \u2208 R n1\u00d7n2\u00d7n3 is represented with three lines, not connected to any other tensor. As\nB n 1 n 2 n 3 Figure 9\n. Graphical notation of simple tensors and tensor networks another example, we can write the compact singular value decomposition in matrix form as A = U \u03a3V , with U \u2208 R n,r , \u03a3 \u2208 R r,r , V \u2208 R r,m , which we represent as a tensor network in Figure 10.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "A.1. The local basis functions", "text": "Following the inexact description of the local basis functions we now give a precise formula. When optimizing the k-th component tensor, the local basis functions are given by setting\n1 \u2264 j k\u22121 \u2264 r k\u22121 , 1 \u2264 i k \u2264 m, and 1 \u2264 j k \u2264 r k within the following formula: b j k\u22121 ,i k ,j k (x) = m,...,m i1,...,i k\u22121 r1,...,r k\u22122 j1,...,j k\u22122 u 1 [i 1 , j 1 ] . . . u k\u22121 [j k\u22122 , i k\u22121 , j k\u22121 ] \u03c6(x 1 )[i 1 ] . . . \u03c6(x k\u22121 )[i k\u22121 ] \u03c6(x k )[i k ] m,...,m i k+1 ,...,i d r k ,...,r d\u22121 j k ,...,j d\u22121 u k+1 [j k , i k+1 , j k+1 ] . . . u d [j d\u22121 , i d ] \u03c6(x k+1 )[i k+1 ] . . . \u03c6(x d )[i d ] .(28)\nU \u03a3 V A = r r n m n m\nFigure 10. Graphical notation of simple tensors and tensor networks.\nNote that in the above formula, every index except j k\u22121 , i k and j k is contracted, leaving an order three tensor. A simple reshape into one index then yields the local basis functions as used in this paper.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "B. Proof of Theorem 3.1", "text": "Proof of Theorem 3.1. In this proof, we denote the underlying probability measure by P, and the corresponding Hilbert space of random variables with finite second moments by L 2 (P). We define the linear subspace U \u2282 L 2 (P) by\nU = f ( X n ) : f \u2208 U ,(29)\nnoting that U is finite-dimensional by the assumption on U, hence closed. The corresponding L 2 (P)-orthogonal projection onto U will be denoted by \u03a0 U . By the nondegeneracy of \u03c3, the law of X n has full support on \u2126, and so\n\u2022 L 2 (P) is indeed a norm on U. Since U is finite-dimensional, the linear operators U f ( X n ) \u2192 \u2202f \u2202x i ( X n ) \u2208 L 2 (P)(30)\nare bounded, and consequently there exists a constant C 1 > 0 such that\n\u2202f \u2202x i ( X n ) L 2 (P) \u2264 C 1 f ( X n ) L 2 (P) ,(31)\nfor all i = 1, . . . , d and f \u2208 U. Furthermore, there exists a constant\nC 2 > 0 such that E f 4 ( X n ) 1/4 := f ( X N ) L 4 (P) \u2264 C 2 f ( X n ) L 2 (P)\n, for all f \u2208 U, again by the finite-dimensionality of U and the fact that on finite dimensional vector spaces, all norms are equivalent. By standard results on orthogonal projections, the solution to the iteration ( 20) is given by\nV k+1 n ( X n ) = \u03a0 U \u2212 h( X n , t n , Y k n , Z k n )\u2206t+ Z k n \u2022 \u03be n+1 \u221a \u2206t \u2212 V n+1 ( X n+1 ) .\nWe now consider the map \u03a8 : U \u2192 U defined by\nf ( X n ) \u2192 \u03a0 U \u2212 h( X n , t n , f ( X n ), \u03c3 \u2207f ( X n ))\u2206t+ \u03c3 \u2207f ( X n ) \u2022 \u03be n+1 \u221a \u2206t \u2212 V n+1 ( X n+1 ) . For F 1 , F 2 \u2208 U with F i = f i ( X n ), f i \u2208 U, we see that \u03a8F 1 \u2212 \u03a8F 2 L 2 (P) = \u03a0 U \u2212 h( X n , t n , f 1 ( X n ), \u03c3 \u2207f 1 ( X n ))\u2206t + h( X n , t n , f 2 ( X n ), \u03c3 \u2207f 2 ( X n ))\u2206t + \u221a \u2206t \u03c3 \u2207f 1 ( X n ) \u2212 \u03c3 \u2207f 2 ( X n ) \u2022 \u03be n+1 L 2 (P) \u2264 C 3 \u03a0 U L 2 (P)\u2192L 2 (P) \u2206t F 1 \u2212 F 2 L 2 (P) + \u221a \u2206t \u03c3 \u2207f 1 ( X n ) \u2212 \u03c3 \u2207f 2 ( X n ) \u2022 \u03be n+1 L 2 (P)\nfor some constant C 3 that does not depend on \u2206t, where we have used the triangle inequality, the Lipschitz assumption on h, the boundedness of \u03c3, and the estimate (31). Using the Cauchy-Schwarz inequality, boundedness of \u03c3 as well as ( 31) and ( 32), the last term can be estimated as follows,\n\u03c3 \u2207f 1 ( X n ) \u2212 \u03c3 \u2207f 2 ( X n ) \u2022 \u03be n+1 L 2 (P) \u2264 \u03c3 \u2207f 1 ( X n ) \u2212 \u03c3 \u2207f 2 ( X n ) 2 1/2 L 2 (P) \u03be 2 n+1 1/2 L 2 (P) \u2264 C 4 F 1 \u2212 F 2 L 2 (P) ,\nwhere C 4 is a constant independent of \u2206t. Collecting the previous estimates, we see that \u03b4 > 0 can be chosen such that for all t \u2208 (0, \u03b4), the mapping \u03a8 is a contraction on U when equipped with the norm \u2022 L 2 (P) , that is,\n\u03a8F 1 \u2212 \u03a8F 2 \u2264 \u03bb F 1 \u2212 F 2 ,(35)\nfor some \u03bb < 1 and all F 1 , F 2 \u2208 U. Finally, the statement follows from the Banach fixed point theorem.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C. Implementation details", "text": "For the evaluation of our approximations we rely on reference values of V (x 0 , 0) and further define the following two loss metrics, which are zero if and only if the PDE is fulfilled along the samples generated by the discrete forward SDE (7). In the spirit of (Raissi et al., 2019), we define the PDE loss as\nL PDE = 1 KN N n=1 K k=1 (\u2202 t + L)V ( X (k) n , t n ) + h( X (k) n , t n , V ( X (k) n , t n ), (\u03c3 \u2207V )( X (k) n , t n )) 2 ,(36)\nwhere\nX (k)\nn are realizations of ( 7), the time derivative is approximated with finite differences and the space derivatives are computed analytically (or with automatic differentiation tools). We leave out the first time step n = 0 since the regression problem within the explicit and the implicit schemes for the tensor trains are not well-defined due to the fact that X k 0 = x 0 has the same value for all k. We still obtain a good approximation since the added regularization term brings a minimum norm solution with the correct point value V (x 0 , 0). Still, this does not aim at the PDE being entirely fulfilled at this point in time.\nFurther, we define the relative reference loss as\nL ref = 1 K(N + 1) N n=0 K k=1 V ( X (k) n , t n ) \u2212 V ref ( X (k) n , t n ) V ref ( X (k) n , t n ) ,(37)\nwhenever a reference solution for all x and t is available.\nAll computation times in the reported tables are measured in seconds.\nOur experiments have been performed on a desktop computer containing an AMD Ryzen Threadripper 2990 WX 32x 3.00 GHz mainboard and an NVIDIA Titan RTX GPU, where we note that only the NN optimizations were run on this GPU, since our TT framework does not include GPU support. It is expected that running the TT approximations on a GPU will improve time performances in the future (Abdelfattah et al., 2016).\nAll our code is available under https://github.com/ lorenzrichter/PDE-backward-solver.", "publication_ref": ["b60", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "C.1. Details on neural network approximation", "text": "For the neural network architecture we rely on the DenseNet, which consists of fully-connected layers with additional skip connections as for instance suggested in (E & Yu, 2018) and being rooted in (Huang et al., 2017). To be precise, we define a version of the DenseNet that includes the terminal condition of the PDE (1) as an additive extension by\n\u03a6 (x) = A L x L + b L + \u03b8g(x),(38)\nwhere x L is specified recursively as\ny l+1 = (A l x l + b l ), x l+1 = (x l , y l+1 ) (39) for 1 \u2264 l \u2264 L \u2212 1 with A l \u2208 R r l \u00d7 l\u22121 i=0 ri , b l \u2208 R l , \u03b8 \u2208 R and x 1 = x.\nThe collection of matrices A l , vectors b l and the coefficient \u03b8 comprises the learnable parameters, and we introduce the vector r := (d in , r 1 , . . . , r L\u22121 , d out ) to represent a certain choice of a DenseNet architecture, where in our setting d in = d and d out = 1. If not otherwise stated we fix the parameter \u03b8 to be 1. For the activation function : R \u2192 R, that is to be applied componentwise, we choose tanh.\nFor the gradient descent optimization we choose the Adam optimizer with the default parameters \u03b2 1 = 0.9, \u03b2 2 = 0.999, \u03b5 = 10 \u22128 (Kingma & Ba, 2014). In most of our experiments we chose a fixed learning rate \u03b7 N \u22121 for the approximation of the first backward iteration step to approximate V N \u22121 and another fixed learning rate \u03b7 n for all the other iteration steps to approximate V n for 0 \u2264 n \u2264 N \u2212 2 (cf. Remark 3). Similarly, we denote with G N \u22121 and G n the amount of gradient descent steps in the corresponding optimizations.\nIn Tables 7 and 8 we list our hyperparameter choices for the neural network experiments that we have conducted.", "publication_ref": ["b14", "b35", "b45"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "C.2. Details on tensor train approximation", "text": "For the implementation of the tensor networks we rely on the C++ library xerus (Huber & Wolf, 2014-2017 and the Python library numpy (Harris et al., 2020).\nWithin the optimization we have to specify the regularization parameter as noted in Remark 2, which we denot here by \u03b7 > 0. We adapt this parameter in dependence of the current residual in the regression problem (20), i.e. \u03b7 = cw, where c > 0 and w is the residual from the previous sweep of SALSA. In every all our experiments we set c \u03b7 = 1. Further, we have to specify the condition \"noChange is true\" within Algorithm 1. To this end we introduce a test set with equal size as our training set. We measure the residual within a single run of SALSA on the test set and the training set. If the change of the residual on either of this sets is below \u03b4 = 0.0001 we set noChange = true. For the fixed-point iteration we have a two-fold stopping condition. We stop the iteration if either the Frobenius norm of the coefficients has a smaller relative difference than \u03b3 1 < 0.0001 or if the values V k+1 n and V k n and their gradients, evaluated at the points of the test set, have a relative difference smaller than \u03b3 2 < 0.00001. Note that the second condition is essentially a discrete H 1 norm, which is necessary since by adding the final condition into the ansatz space the orthonormal basis property is violated.\nFinally, we comment on the area [a, b] where the 1dimensional polynomials are orthonormalized w.r.t. the H 2 (a, b) norm, c.f. Remark 2. We obtain these polynomials by performing a Gram-Schmidt process starting with one-dimensional monomials. Thus, we have to specify the integration area [a, b] \nG n = 2000 for 0 \u2264 n \u2264 15 G n = 300 for 16 \u2264 n \u2264 N \u2212 2 G N \u22121 = 10000 \u03b7 n = 0.00005, \u03b7 N \u22121 = 0.0001", "publication_ref": ["b67", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "D. Further numerical examples", "text": "In this section we elaborate on some of the numerical examples from the paper and provide two additional problems. PDE with unbounded solution d = 10, NN impl , Table 9 K = 1000, \u2206t = 0.001 r = (10, 30, 30, 1) G n = 100, G N \u22121 = 10000 \u03b7 n = 0.0001, \u03b7 N \u22121 = 0.0001 Allen-Cahn d = 100, NN impl , Table 10 K = 8000, \u2206t = 0.01 r = (10, 30, 30, 1) \nG n = 10000 for 0 \u2264 n \u2264 5 G n = 6000 for 6 \u2264 n \u2264 N \u2212 2 G N \u22121 = 15000 \u03b7 n = 0.0002, \u03b7 N \u22121 = 0.001\n(\u2202 t + L) V (x, t) \u2212 1 2 |(\u03c3 \u2207V )(x, t)| 2 = 0,(40a)\nV (x, T ) = g(x), (40b)\nin a generic form with the differential operator L being defined in (2). We can introduce the exponential transformation \u03c8 := e \u2212V and with the chain rule find that the transformed function fulfills the linear PDE\n(\u2202 t + L) \u03c8(x, t) = 0, (41a) \u03c8(x, T ) = e \u2212g(x) .(41b)\nThis is known as Hopf-Cole transformation, see also (Fleming & Soner, 2006;Hartmann et al., 2017). It is known that via the Feynman-Kac theorem (Karatzas & Shreve, 1998) the solution to this PDE has the stochastic representation\n\u03c8(x, t) = E e \u2212g(X T ) X t = x ,(42)\nsuch that we readily get\nV (x, t) = \u2212 log E e \u2212g(X T ) X t = x ,(43)\nwhich we can use as a reference solution by approximating the expectation value via Monte Carlo simulation, however keeping in mind that in high dimensions corresponding estimators might have high variances (Hartmann & Richter, 2021).\nLet us stress again that our algorithms only aim to provide a solution of the PDE along the trajectories of the forward process (4). Still, there is hope that our approximations generalize to regions \"close\" to where samples are available.\nTo illustrate this, consider for instance the d-dimensional forward process\nX s = x 0 + \u03c3W s ,(44)\nas for instance in Section 4.1, where now \u03c3 > 0 is onedimensional for notational convenience. We know that X t \u223c N (x 0 , \u03c3 2 t Id d\u00d7d ) and therefore note that for the expected distance to the origin it holds\nE [|X t \u2212 x 0 |] < E [|X t \u2212 x 0 | 2 ] = \u03c3 \u221a dt. (45\n)\nThis motivates evaluating the approximations along the curve\nX t = x 0 + \u03c3 \u221a t1,(46)\nwhere 1 = (1, . . . , 1) . Figure 11 shows that in this case we indeed have good agreement of the approximation with the reference solution when using TTs and that for NNs the deep neural network that we have specified in Table 7 generalizes worse than a shallower network with only two hidden layers consisting of 30 neurons each. ", "publication_ref": ["b20", "b30", "b41", "b29"], "figure_ref": ["fig_1"], "table_ref": ["tab_0", "tab_6"]}, {"heading": "D.2. PDE with unbounded solution", "text": "As an additional problem, we choose an example from (Hur\u00e9 et al., 2020) which offers an analytical reference solution. For the PDE as defined in (1) we consider the coefficients b(x, t) = 0, \u03c3(x, t) =\nId d\u00d7d \u221a d , g(x) = cos d i=1 ix i ,(47)\nh(x, t, y, z\n) = k(x) + y 2 \u221a d d i=1 z i + y 2 2 ,(48)\nwhere, with an appropriately chosen k, a solution can shown to be\nV (x, t) = T \u2212 t d d i=1 (sin(x i )1 xi<0 + x i 1 xi\u22650 ) + cos d i=1 ix i .(49)\nIn Table 9 we compare the results for d = 10, K = 1000, T = 1, \u2206t = 0.001, x 0 = (0.5, . . . , 0.5) . For the TT case it was sufficient to set the ranks to 1 and the polynomial degree to 6. We see that the results are improved significantly if we increase the sample size K from 1000 to 20000. Note that even when increasing the sample size by a factor 20, the computational time is still lower than the NN implementation. It should be highlighted that adding the function g to the neural network (as explained in Appendix C) is essential for its convergence in higher dimensions and thereby mitigates the observed difficulties in (Hur\u00e9 et al., 2020)). Table 9. Approximation results for the PDE with an unbounded analytic solution. For TT * impl we choose K = 20000, for the others we choose K = 1000.", "publication_ref": ["b37", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "D.3. Allen-Cahn like equation", "text": "Finally, let us consider the following Allen-Cahn like PDE with a cubic nonlinearity in d = 100:\n(\u2202 t + \u2206)V (x, t) + V (x, t) \u2212 V 3 (x, t) = 0, (50a) V (x, T ) = g(x), (50b)\nwhere we choose g(x) = 2 + 2 5 |x| 2 \u22121 , T = 3 10 and are interested in an evaluation at x 0 = (0, . . . , 0) . This problem has been considered in (E et al., 2017), where a reference solution of V (x 0 , 0) = 0.052802 calculated by means of the branching diffusion method is provided. We consider a sample size of K = 1000 and a stepsize \u2206t = 0.01 and provide our approximation results in Table 10. Note that for this example it is again sufficient to use a TT-rank of 1 and a polynomial degree of 0.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "TT impl", "text": "TT expl NN impl NN * impl V 0 (x 0 ) 0.052800 0.05256 0.04678 0.05176 relative error 4.75e \u22125 4.65e \u22123 1.14e \u22121 1.97e \u22122 PDE loss 2.40e \u22124 2.57e \u22124 9.08e \u22121 6.92e \u22121 comp. time 24 10 23010 95278\nTable 10. Approximations for Allen-Cahn PDE, where NN * impl uses K = 8000 and the others K = 1000 samples. E. Some background on BSDEs and their numerical discretizations\nBSDEs have been studied extensively in the last three decades and we refer to (Pardoux, 1998;Pham, 2009;Gobet, 2016;Zhang, 2017) for good introductions to the topic.\nLet us note that given some assumptions on the coefficients b, \u03c3, h and g one can prove existence and uniqueness of a solution to the BSDE system as defined in ( 4) and ( 6), see for instance Theorem 4.3.1 in (Zhang, 2017).\nWe note that the standard BSDE system can be generalized to dX s = (b(X s , s) + v(X s , s)) ds + \u03c3(X s , s)dW s , (51a)\nX 0 = x,(51b)\ndY s = (\u2212h(X s , s, Y s , Z s ) + v(X s , s) \u2022 Z s )ds + Z s \u2022 dW s ,(51c)\nY T = g(X T ),\nwhere v : R d \u00d7[0, T ] \u2192 R d is any suitable control vector field that can be understood as pushing the forward trajectories into desired regions of the state space, noting that the relations Y s = V (X s , s), Z s = (\u03c3 \u2207V )(X s , s),\nwith V : R d \u00d7[0, T ] \u2192 R being the solution to the parabolic PDE (1), hold true independent of the choice of v (Hartmann et al., 2019). Our algorithms readily transfer to this change in sampling the forward process by adapting the backward process and the corresponding loss functionals ( 10) and ( 11) accordingly.\nIn order to understand the different numerical discretization schemes in Section 2.1, let us note that we can write the backward process (5) in its integrated form for the times t n < t n+1 as (53) In a discrete version we have to replace the integrals with suitable discretizations, where for the deterministic integral we can decide which endpoint to consider, leading to either\nY tn+1 = Y tn \u2212", "publication_ref": ["b57", "b59", "b21", "b67", "b67", "b31"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "", "text": "We would like to thank Reinhold Schneider for giving valuable input and for sharing his broad insight in tensor methods and optimization.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "of the following two discretization schemes\nas defined in ( 8), where we recall the shorthands\nThe L 2 -projection scheme ( 10) can be motivated as follows.\nConsider the explicit discrete backward scheme as in ( 54b)\nTaking conditional expectations w.r.t. to the \u03c3-algebra generated by the discrete Brownian motion at time step n, denoted by F n , yields\n(57) We can now recall that a conditional expectation can be characterized as a best approximation in L 2 , namely\nfor any random variable B \u2208 L 2 , which brings\nThis then yields the explicit scheme depicted in (10). We refer once more to (Gobet et al., 2005) for extensive numerical analysis, essentially showing that the proposed scheme is of order 1 2 in the time step \u2206t.", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "High-performance tensor contractions for GPUs", "journal": "Procedia Computer Science", "year": "2016", "authors": "A Abdelfattah; M Baboulin; V Dobrev; J Dongarra; C Earl; J Falcou; A Haidar; I Karlin; T Kolev; I Masliah"}, {"ref_id": "b1", "title": "Affine diffusions and related processes: simulation, theory and applications", "journal": "Springer", "year": "2015", "authors": "A Alfonsi"}, {"ref_id": "b2", "title": "Tensor networks and hierarchical tensors for the solution of high-dimensional partial differential equations", "journal": "Found. Comput. Math", "year": "2016", "authors": "M Bachmayr; R Schneider; A Uschmajew"}, {"ref_id": "b3", "title": "Pric-Solving high-dimensional parabolic PDEs using the tensor train format ing high-dimensional bermudan options with hierarchical tensor formats", "journal": "", "year": "2021", "authors": "C Bayer; M Eigel; L Sallandt; P Trunschke"}, {"ref_id": "b4", "title": "Deep splitting method for parabolic PDEs", "journal": "", "year": "2019", "authors": "C Beck; S Becker; P Cheridito; A Jentzen; A Neufeld"}, {"ref_id": "b5", "title": "Conjugate convex functions in optimal stochastic control", "journal": "Journal of Mathematical Analysis and Applications", "year": "1973", "authors": "J.-M Bismut"}, {"ref_id": "b6", "title": "Discrete-time approximation and Monte-Carlo simulation of backward stochastic differential equations", "journal": "", "year": "2004", "authors": "B Bouchard; N Touzi"}, {"ref_id": "b7", "title": "Linear multistep schemes for bsdes", "journal": "SIAM Journal on Numerical Analysis", "year": "2014", "authors": "J.-F Chassagneux"}, {"ref_id": "b8", "title": "Tensor decomposition and highperformance computing for solving high-dimensional stochastic control system numerically", "journal": "Journal of Systems Science and Complexity", "year": "2021", "authors": "Y Chen; Z Lu"}, {"ref_id": "b9", "title": "Second order discretization of backward sdes and simulation with the cubature method", "journal": "Annals of Applied Probability", "year": "2014", "authors": "D Crisan; K Manolarakis"}, {"ref_id": "b10", "title": "Rank-adaptive tensor methods for high-dimensional nonlinear pdes", "journal": "", "year": "2020", "authors": "A Dektor; A Rodgers; D Venturi"}, {"ref_id": "b11", "title": "Polynomial chaos expansion of random coefficients and the solution of stochastic partial differential equations in the tensor train format", "journal": "SIAM/ASA Journal on Uncertainty Quantification", "year": "2015", "authors": "S Dolgov; B N Khoromskij; A Litvinenko; H G Matthies"}, {"ref_id": "b12", "title": "Tensor decompositions for high-dimensional Hamilton-Jacobi-Bellman equations", "journal": "", "year": "2019", "authors": "S Dolgov; D Kalise; K Kunisch"}, {"ref_id": "b13", "title": "Fast solution of parabolic problems in the tensor train/quantized tensor train format with initial application to the Fokker-Planck equation", "journal": "SIAM Journal on Scientific Computing", "year": "2012", "authors": "S V Dolgov; B N Khoromskij; I V Oseledets"}, {"ref_id": "b14", "title": "The deep Ritz method: a deep learningbased numerical algorithm for solving variational problems", "journal": "Communications in Mathematics and Statistics", "year": "2018", "authors": "E ; W Yu; B "}, {"ref_id": "b15", "title": "Deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations", "journal": "Communications in Mathematics and Statistics", "year": "2017", "authors": "E ; W Han; J ; Jentzen ; A "}, {"ref_id": "b16", "title": "On multilevel picard numerical approximations for highdimensional nonlinear parabolic partial differential equations and high-dimensional nonlinear backward stochastic differential equations", "journal": "Journal of Scientific Computing", "year": "2019", "authors": "E ; W Hutzenthaler; M Jentzen; A Kruse; T "}, {"ref_id": "b17", "title": "Adaptive stochastic galerkin fem with hierarchical tensor representations", "journal": "Numerische Mathematik", "year": "2017", "authors": "M Eigel; M Pfeffer; R Schneider"}, {"ref_id": "b18", "title": "Approximative policy iteration for exit time feedback control problems driven by stochastic differential equations using tensor train format", "journal": "", "year": "2020", "authors": "K Fackeldey; M Oster; L Sallandt; R Schneider"}, {"ref_id": "b19", "title": "Deterministic and stochastic optimal control", "journal": "Springer Science & Business Media", "year": "2012", "authors": "W H Fleming; R W Rishel"}, {"ref_id": "b20", "title": "Controlled Markov processes and viscosity solutions", "journal": "Springer Science & Business Media", "year": "2006", "authors": "W H Fleming; H M Soner"}, {"ref_id": "b21", "title": "Monte-Carlo methods and stochastic processes: from linear to non-linear", "journal": "CRC Press", "year": "2016", "authors": "E Gobet"}, {"ref_id": "b22", "title": "A regressionbased Monte Carlo method to solve backward stochastic differential equations", "journal": "The Annals of Applied Probability", "year": "2005", "authors": "E Gobet; J.-P Lemor; X Warin"}, {"ref_id": "b23", "title": "Highdimensional stochastic optimal control using continuous tensor decompositions", "journal": "The International Journal of Robotics Research", "year": "2018", "authors": "A Gorodetsky; S Karaman; Y Marzouk"}, {"ref_id": "b24", "title": "Stable als approximation in the tt-format for rank-adaptive tensor completion", "journal": "Numerische Mathematik", "year": "2019", "authors": "L Grasedyck; S Kr\u00e4mer"}, {"ref_id": "b25", "title": "", "journal": "Numerical tensor calculus. Acta numerica", "year": "2014", "authors": "W Hackbusch"}, {"ref_id": "b26", "title": "A new scheme for the tensor representation", "journal": "Journal of Fourier Analysis and Applications", "year": "2009", "authors": "W Hackbusch; S K\u00fchn"}, {"ref_id": "b27", "title": "Tensor Spaces and Hierarchical Tensor Representations", "journal": "Springer International Publishing", "year": "2014", "authors": "W Hackbusch; R Schneider"}, {"ref_id": "b28", "title": "Array programming with NumPy", "journal": "Nature", "year": "2020-09", "authors": "C R Harris; K J Millman; S J Van Der Walt; R Gommers; P Virtanen; D Cournapeau; E Wieser; J Taylor; S Berg; N J Smith; R Kern; M Picus; S Hoyer; M H Van Kerkwijk; M Brett; A Haldane; J F Del R'\u0131o; M Wiebe; P Peterson; P Marchant; K Sheppard; T Reddy; W Weckesser; H Abbasi; C Gohlke; T E Oliphant"}, {"ref_id": "b29", "title": "Nonasymptotic bounds for suboptimal importance sampling", "journal": "", "year": "2021", "authors": "C Hartmann; L Richter"}, {"ref_id": "b30", "title": "Variational characterization of free energy: Theory and algorithms", "journal": "Entropy", "year": "2017", "authors": "C Hartmann; L Richter; C Sch\u00fctte; W Zhang"}, {"ref_id": "b31", "title": "Variational approach to rare event simulation using leastsquares regression", "journal": "Chaos: An Interdisciplinary Journal of Nonlinear Science", "year": "2019", "authors": "C Hartmann; O Kebiri; L Neureither; L Richter"}, {"ref_id": "b32", "title": "The alternating linear scheme for tensor optimization in the tensor train format", "journal": "SIAM J. Sci. Comput", "year": "2012", "authors": "S Holtz; T Rohwedder; R Schneider"}, {"ref_id": "b33", "title": "On manifolds of tensors of fixed tt-rank", "journal": "Numerische Mathematik", "year": "2012", "authors": "S Holtz; T Rohwedder; R Schneider"}, {"ref_id": "b34", "title": "Linear hamilton jacobi bellman equations in high dimensions", "journal": "IEEE", "year": "2014", "authors": "M B Horowitz; A Damle; J W Burdick"}, {"ref_id": "b35", "title": "Densely connected convolutional networks", "journal": "", "year": "2017", "authors": "G Huang; Z Liu; L Van Der Maaten; K Q Weinberger"}, {"ref_id": "b36", "title": "Xerus -a general purpose tensor library", "journal": "", "year": "2014", "authors": "B Huber; S Wolf"}, {"ref_id": "b37", "title": "Deep backward schemes for high-dimensional nonlinear PDEs", "journal": "Mathematics of Computation", "year": "2020", "authors": "C Hur\u00e9; H Pham; X Warin"}, {"ref_id": "b38", "title": "Forward-backward SDEs and the CIR model", "journal": "Statistics & probability letters", "year": "2007", "authors": "C B Hyndman"}, {"ref_id": "b39", "title": "A proof that deep artificial neural networks overcome the curse of dimensionality in the numerical approximation of Kolmogorov partial differential equations with constant diffusion and nonlinear drift coefficients", "journal": "", "year": "2018", "authors": "A Jentzen; D Salimova; T Welti"}, {"ref_id": "b40", "title": "Convergence of the deep BSDE method for FBSDEs with non-lipschitz coefficients", "journal": "", "year": "2021", "authors": "Y Jiang; J Li"}, {"ref_id": "b41", "title": "Brownian Motion and Stochastic Calculus", "journal": "Springer", "year": "1998", "authors": "I Karatzas; S E Shreve"}, {"ref_id": "b42", "title": "QTT-finite-element approximation for multiscale problems", "journal": "", "year": "2016-06", "authors": "V Kazeev; I Oseledets; M Rakhuba; C Schwab"}, {"ref_id": "b43", "title": "Low-rank explicit QTT representation of the laplace operator and its inverse", "journal": "SIAM journal on matrix analysis and applications", "year": "2012", "authors": "V A Kazeev; B N Khoromskij"}, {"ref_id": "b44", "title": "Tensors-structured numerical methods in scientific computing: Survey on recent advances. Chemometrics and Intelligent Laboratory Systems", "journal": "", "year": "2012", "authors": "B N Khoromskij"}, {"ref_id": "b45", "title": "A method for stochastic optimization", "journal": "", "year": "2014", "authors": "D P Kingma; J Ba;  Adam"}, {"ref_id": "b46", "title": "Stochastic differential equations", "journal": "Springer", "year": "1992", "authors": "P E Kloeden; E Platen"}, {"ref_id": "b47", "title": "A semi-Lagrangian Vlasov solver in tensor train format", "journal": "SIAM Journal on Scientific Computing", "year": "2015", "authors": "K Kormann"}, {"ref_id": "b48", "title": "Tangent cones to tensor train varieties", "journal": "Linear Algebra and its Applications", "year": "2018", "authors": "B Kutschan"}, {"ref_id": "b49", "title": "Tensors: geometry and applications", "journal": "", "year": "2012", "authors": "J M Landsberg"}, {"ref_id": "b50", "title": "Valuing American options by simulation: a simple least-squares approach. The review of financial studies", "journal": "", "year": "2001", "authors": "F A Longstaff; E S Schwartz"}, {"ref_id": "b51", "title": "Multigrid renormalization", "journal": "Journal of Computational Physics", "year": "2018", "authors": "M Lubasch; P Moinier; D Jaksch"}, {"ref_id": "b52", "title": "Solving non-linear kolmogorov equations in large dimensions by using deep learning: a numerical comparison of discretization schemes", "journal": "", "year": "2020", "authors": "N Macris; R Marino"}, {"ref_id": "b53", "title": "Solving high-dimensional Hamilton-Jacobi-Bellman PDEs using neural networks: perspectives from the theory of controlled diffusions and measures on path space", "journal": "", "year": "2020", "authors": "N N\u00fcsken; L Richter"}, {"ref_id": "b54", "title": "Tensor-train decomposition", "journal": "SIAM Journal on Scientific Computing", "year": "2011", "authors": "I V Oseledets"}, {"ref_id": "b55", "title": "Breaking the curse of dimensionality, or how to use SVD in many dimensions", "journal": "SIAM Journal on Scientific Computing", "year": "2009", "authors": "I V Oseledets; E E Tyrtyshnikov"}, {"ref_id": "b56", "title": "Approximating the stationary Hamilton-Jacobi-Bellman equation by hierarchical tensor products", "journal": "", "year": "2019", "authors": "M Oster; L Sallandt; R Schneider"}, {"ref_id": "b57", "title": "Backward stochastic differential equations and viscosity solutions of systems of semilinear parabolic and elliptic PDEs of second order", "journal": "Springer", "year": "1998", "authors": "\u00c9 Pardoux"}, {"ref_id": "b58", "title": "Adapted solution of a backward stochastic differential equation", "journal": "Systems & Control Letters", "year": "1990", "authors": "E Pardoux; S Peng"}, {"ref_id": "b59", "title": "Continuous-time stochastic control and optimization with financial applications", "journal": "Springer Science & Business Media", "year": "2009", "authors": "H Pham"}, {"ref_id": "b60", "title": "Physicsinformed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations", "journal": "Journal of Computational Physics", "year": "2019", "authors": "M Raissi; P Perdikaris; G E Karniadakis"}, {"ref_id": "b61", "title": "Tensor products of Sobolev-Besov spaces and applications to approximation from the hyperbolic cross", "journal": "Journal of Approximation Theory", "year": "2009", "authors": "W Sickel; T Ullrich"}, {"ref_id": "b62", "title": "DGM: A deep learning algorithm for solving partial differential equations", "journal": "Journal of computational physics", "year": "2018", "authors": "J Sirignano; K Spiliopoulos"}, {"ref_id": "b63", "title": "Sequential alternating least squares for solving high dimensional linear hamiltonjacobi-bellman equation", "journal": "IEEE", "year": "2016", "authors": "E Stefansson; Y P Leong"}, {"ref_id": "b64", "title": "Tensor product methods and entanglement optimization for ab initio quantum chemistry. International j. of quantum chemistry", "journal": "", "year": "2015", "authors": "S Szalay; M Pfeffer; V Murg; G Barcza; F Verstraete; R Schneider; Legeza And\u00f6rs"}, {"ref_id": "b65", "title": "Topics in propagation of chaos", "journal": "Springer", "year": "1991", "authors": "A.-S Sznitman"}, {"ref_id": "b66", "title": "A numerical scheme for BSDEs. The annals of applied probability", "journal": "", "year": "2004", "authors": "J Zhang"}, {"ref_id": "b67", "title": "Backward stochastic differential equations", "journal": "Springer", "year": "2017", "authors": "J Zhang"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 .1Figure 1. An order 4 tensor and a tensor train representation.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": ") that are not varied in the optimization. Choosing a basis {b 1 , . . . , b M } of U we can represent any function w \u2208 U by w(x) = M m=1 c m b m (x) and it is well known that the solution to (17) is given in terms of the coefficient vector", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 .3Figure 3. Graphical representation of the local basis functions for i = 2.", "figure_data": ""}, {"figure_label": "67", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 .Figure 7 .67Figure 6. Reference solutions compared with implicit TT and NN approximations along two trajectories in d = 100.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 8 .8Figure 8. Reference trajectory for different polynomial degrees.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "for the different tests. In Section 4.1 we set a = \u22126 and b = 6. In Section 4.2 we set a = \u22123 and b = 3 for the case C diagonal and for the interacting case, where C is non-diagonal, we set a = \u22128 and b = 2. In Section 4.3 we choose a = \u22120.2 and b = 6. HJB, d = 10, NN impl Figure 5 K = 2000, \u2206t = 0.01 r = (100,110, 110, 50, 50, 1) G n = 8000, G N \u22121 = 40000 \u03b7 n = 0.0001, \u03b7 N \u22121 = 0.0001 HJB, d = 100, NN impl Table 1, Figures 6, 7 K = 2000, \u2206t = 0.01 r = (100, 130, 130, 70, 70, 1) G n = 5000, G N \u22121 = 40000 \u03b7 n = 0.0001, \u03b7 N \u22121 = 0.0003 HJB, d = 100, NN explTable 1, Figures 6, 7 K = 2000, \u2206t = 0.01 r = (100, 110, 110, 50, 50, 1)G n = 500, G N \u22121 = 7000 \u03b7 n = 0.00005, \u03b7 N \u22121 = 0.0003 HJB double well d = 50, NN impl , Table 3 K = 2000, \u2206t = 0.01 r = (50, 30, 30, 1) G n = 2000, G N \u22121 = 25000 \u03b7 n = 0.0002, \u03b7 N \u22121 = 0.0005HJB interacting double well d = 20, NN impl , Table 4 K = 2000, \u2206t = 0.01 r = (50, 20, 20, 20, 20, 1) G n = 3000, G N \u22121 = 30000 \u03b7 n = 0.0007, \u03b7 N \u22121 = 0.001 CIR, d = 100, NN implTable 5 K = 1000, \u2206t = 0.01 r =(100, 110, 110, 50, 50, 1)   ", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 11 .11Figure 11. Approximations of the HJB equation in d = 100 evaluated along a representative curve.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "s , s, Y s , Z s )ds+ tn+1 tn Z s \u2022dW s .", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Comparison of approximation results for the HJB equation in d = 100.", "figure_data": "TT implTT explNN implNN explV 0 (x 0 )4.59034.59094.58224.4961relative error 5.90e \u22125 3.17e \u22124 1.71e \u22123 2.05e \u22122reference loss 3.55e \u22124 5.74e \u22124 4.23e \u22123 1.91e \u22122PDE loss1.99e \u22123 3.61e \u2212390.8991.12comp. time41254471225178"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Reference solutions compared with implicit TT and NN approximations along two trajectories in d = 10.", "figure_data": "Evaluation along trajectoriesTT\u015dNNs2.75V(Xt, t)2.752.50Vref(Xt, t)2.502.252.252.002.001.751.750.000.250.500.751.000.000.250.500.751.00tt4.4 4.6 4.8 Figure 5. 0.000.25 V(Xt, t) Vref(Xt, t) 0.50 TT\u015d Evaluation along trajectories 0.75 1.00 0.00 0.25 4.4 4.6 4.80.50 NNs0.751.00tt"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Relative errors of the TT approximations Vn(x0) for different dimensions and polynomial degrees.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Approximation results for the HJB equation with noninteracting double well potential in d = 50.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Approximation results for the HJB equation with interacting double well potential in d = 20.", "figure_data": "TT implTT explNN implV 0 (x 0 )35.01534.75634.917relative error1.52e \u22123 2.82e \u22123 4.24e \u22123reference loss1.30e \u22122 1.59e \u22122 6.38e \u22122PDE loss79.9341170.64computation time4601516991"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "PDE loss and computation time for TTs with different polynomial degrees", "figure_data": "2940.3120.3120.312PDE loss9.04e \u22122 7.80e \u22124 1.05e \u22123 5.06e \u22124comp. time110360942195281"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Neural network hyperparameters for the experiments in paper.", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Neural network hyperparameters for the additional experiments.", "figure_data": "D.1. Hamilton-Jacobi-Bellman equationLet us consider the HJB equation from Sections 4.1 and 4.2,which we can write as"}], "formulas": [{"formula_id": "formula_0", "formula_text": "(\u2202 t +L)V (x, t)+h(x, t, V (x, t), (\u03c3 \u2207V )(x, t)) = 0 (1) for (x, t) \u2208 R d \u00d7 [0, T ], a nonlinearity h : R d \u00d7[0, T ] \u00d7 R \u00d7 R d \u2192 R, and a differential operator L = 1 2 d i,j=1 (\u03c3\u03c3 ) ij (x, t)\u2202 xi \u2202 xj + d i=1 b i (x, t)\u2202 xi , (2) with coefficient functions b : R d \u00d7[0, T ] \u2192 R d and \u03c3 : R d \u00d7[0, T ] \u2192 R d\u00d7d . The terminal value is given by V (x, T ) = g(x),(3)", "formula_coordinates": [2.0, 307.08, 269.2, 235.74, 132.88]}, {"formula_id": "formula_1", "formula_text": "dX s = b(X s , s) ds + \u03c3(X s , s) dW s , X 0 = x 0 , (4)", "formula_coordinates": [2.0, 319.33, 533.8, 222.11, 9.65]}, {"formula_id": "formula_2", "formula_text": "Y s = V (X s , s), Z s = (\u03c3 \u2207V )(X s , s) (5)", "formula_coordinates": [2.0, 336.63, 596.78, 204.81, 9.65]}, {"formula_id": "formula_3", "formula_text": "dY s = \u2212h(X s , s, Y s , Z s ) ds + Z s \u2022 dW s ,(6)", "formula_coordinates": [2.0, 341.48, 647.8, 199.97, 9.65]}, {"formula_id": "formula_4", "formula_text": "0 = t 0 < t 1 < \u2022 \u2022 \u2022 < t N = T by X n+1 = X n + b( X n , t n )\u2206t + \u03c3( X n , t n )\u03be n+1 \u221a \u2206t, (7)", "formula_coordinates": [3.0, 61.69, 311.26, 227.75, 30.01]}, {"formula_id": "formula_5", "formula_text": "Y n+1 = Y n \u2212 h n+1 \u2206t + Z n \u2022 \u03be n+1 \u221a \u2206t,(8a)", "formula_coordinates": [3.0, 89.67, 412.47, 199.77, 18.57]}, {"formula_id": "formula_6", "formula_text": "Y n+1 = Y n \u2212 h n \u2206t + Z n \u2022 \u03be n+1 \u221a \u2206t,(8b)", "formula_coordinates": [3.0, 89.67, 429.76, 199.77, 18.57]}, {"formula_id": "formula_7", "formula_text": "h n = h( X n , t n , Y n , Z n ),(9a)", "formula_coordinates": [3.0, 107.25, 479.39, 182.19, 9.65]}, {"formula_id": "formula_8", "formula_text": "h n+1 = h( X n+1 , t n+1 , Y n+1 , Z n+1 ).(9b)", "formula_coordinates": [3.0, 97.17, 496.68, 192.28, 9.65]}, {"formula_id": "formula_9", "formula_text": "V n ( X n ) \u2248 Y n \u2248 V ( X n , t n )", "formula_coordinates": [3.0, 55.44, 655.76, 123.1, 9.65]}, {"formula_id": "formula_10", "formula_text": "E V n ( X n ) \u2212 h n+1 \u2206t \u2212 V n+1 ( X n+1 ) 2(10)", "formula_coordinates": [3.0, 327.05, 126.69, 214.39, 16.6]}, {"formula_id": "formula_11", "formula_text": "E[( V n ( X n ) \u2212 h n \u2206t \u2212 V n+1 ( X n+1 ) + \u03c3 ( X n , t n )\u2207 V n ( X n ) \u2022 \u03be n+1 \u221a \u2206t) 2 ],(11)", "formula_coordinates": [3.0, 319.06, 267.42, 222.38, 26.94]}, {"formula_id": "formula_12", "formula_text": "h n = h( X n , t n , V n ( X n ), \u03c3 ( X n , t n )\u2207 V n ( X n )),(12)", "formula_coordinates": [3.0, 317.16, 348.82, 224.28, 9.65]}, {"formula_id": "formula_13", "formula_text": "V (x 1 , . . . , x d ) = m i1=1 \u2022 \u2022 \u2022 m i d =1 c i1,...,i d \u03c6 i1 (x 1 ) \u2022 \u2022 \u2022 \u03c6 i d (x d ),(13)", "formula_coordinates": [3.0, 308.03, 640.38, 233.41, 40.97]}, {"formula_id": "formula_14", "formula_text": "w = w 1 \u2022 w 2 \u2208 R r1\u00d7m\u00d7m\u00d7r3 ,(14a)", "formula_coordinates": [4.0, 120.92, 215.89, 168.52, 11.72]}, {"formula_id": "formula_15", "formula_text": "w[i 1 , i 2 , i 3 , i 4 ] = r2 j=1 w 1 [i 1 , i 2 , j]w 2 [j, i 3 , i 4 ].(14b)", "formula_coordinates": [4.0, 70.5, 231.58, 218.94, 30.43]}, {"formula_id": "formula_16", "formula_text": "u 1 u 2 u 3 u 4 c = r 1 r 2 r 3 m m m m m m m m", "formula_coordinates": [4.0, 62.8, 384.27, 225.19, 43.12]}, {"formula_id": "formula_17", "formula_text": "Definition 1 (Tensor Train). Let c \u2208 R m\u00d7\u2022\u2022\u2022\u00d7m . A factor- ization c = u 1 \u2022 u 2 \u2022 \u2022 \u2022 \u2022 \u2022 u d ,(15)", "formula_coordinates": [4.0, 55.44, 515.96, 235.65, 35.14]}, {"formula_id": "formula_18", "formula_text": "u 1 \u2208 R m\u00d7r1 , u i \u2208 R ri\u22121\u00d7m\u00d7ri , 2 \u2264 i \u2264 d \u2212 1, u d \u2208 R r d\u22121 \u00d7m", "formula_coordinates": [4.0, 55.44, 557.21, 235.25, 23.18]}, {"formula_id": "formula_19", "formula_text": "s t \u21d0\u21d2 s i \u2264 t i for all 1 \u2264 i \u2264 d, for r = (r 1 , . . . , r d ), s = (s 1 , . . . , s d ) \u2208 N d .", "formula_coordinates": [4.0, 55.44, 687.29, 192.88, 30.62]}, {"formula_id": "formula_20", "formula_text": "\u03c6 : R \u2192 R m , \u03c6(x) = [\u03c6 1 (x), . . . , \u03c6 m (x)],", "formula_coordinates": [4.0, 334.45, 201.12, 179.98, 11.72]}, {"formula_id": "formula_21", "formula_text": "V (x) = m i1 \u2022 \u2022 \u2022 m i d r1 j1 \u2022 \u2022 \u2022 r d\u22121 j d\u22121 u 1 [i 1 , j 1 ]u 2 [j 1 , i 2 , j 2 ] \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 u d [j d\u22121 , i d ]\u03c6(x 1 )[i 1 ] \u2022 \u2022 \u2022 \u03c6(x d )[i d ]. (16", "formula_coordinates": [4.0, 317.4, 242.22, 227.3, 48.06]}, {"formula_id": "formula_22", "formula_text": ")", "formula_coordinates": [4.0, 537.29, 280.94, 4.15, 8.64]}, {"formula_id": "formula_23", "formula_text": "u 1 u 2 u 3 u 4 \u03c6(x 1 ) \u03c6(x 2 ) \u03c6(x 3 ) \u03c6(x 4 ) V (x) = r 1 r 2 r 3 m m m m Figure 2. Graphical representation of V : R 4 \u2192 R.", "formula_coordinates": [4.0, 330.1, 332.69, 186.64, 72.03]}, {"formula_id": "formula_24", "formula_text": "arg min V \u2208U J j=1 | V (x j ) \u2212 R(x j )| 2 ,(17)", "formula_coordinates": [4.0, 361.5, 552.28, 179.94, 30.32]}, {"formula_id": "formula_25", "formula_text": "c = (A A) \u22121 A r \u2208 R M ,(18)", "formula_coordinates": [4.0, 370.61, 676.88, 170.83, 11.37]}, {"formula_id": "formula_26", "formula_text": "loc,i (x) = [b loc,i 1 (x), b loc,i 2 (x), . . . , b loc,i Mi (x)", "formula_coordinates": [5.0, 74.98, 324.51, 169.66, 13.83]}, {"formula_id": "formula_27", "formula_text": "u 1 u 3 u 4 \u03c6(x 1 ) \u03c6(x 2 ) \u03c6(x 3 ) \u03c6(x 4 ) b loc,i (x) = r 1 r 2 r 3 m m m m", "formula_coordinates": [5.0, 75.12, 554.26, 192.15, 45.7]}, {"formula_id": "formula_28", "formula_text": "b loc,i g = {b loc,i 1 , . . . , b loc,i m , g},(19)", "formula_coordinates": [5.0, 365.68, 122.05, 175.76, 13.56]}, {"formula_id": "formula_29", "formula_text": "u 1 u 2 u 3 u 4 \u03c6(x 1 ) \u03c6(x 2 ) \u03c6(x 3 ) \u03c6(x 4 ) V (x) = + c g g(x) r 1 r 2 r 3 m m m m Figure 4. Graphical representation of V : R 4 \u2192 R.", "formula_coordinates": [5.0, 310.76, 224.42, 229.27, 83.98]}, {"formula_id": "formula_30", "formula_text": "Algorithm 1 simple ALS algorithm Input: initial guess u 1 \u2022 u 2 \u2022 \u2022 \u2022 \u2022 \u2022 u d . Output: result u 1 \u2022 u 2 \u2022 \u2022 \u2022 \u2022 \u2022 u d . repeat for i = 1", "formula_coordinates": [5.0, 307.08, 361.42, 163.94, 58.16]}, {"formula_id": "formula_31", "formula_text": "E[( V k+1 n ( X n ) \u2212 h( X n , t n , Y k n , Z k n )\u2206t+ Z k n \u2022 \u03be n+1 \u221a \u2206t \u2212 V n+1 ( X n+1 )) 2 ] (20)", "formula_coordinates": [6.0, 67.06, 162.08, 222.38, 29.98]}, {"formula_id": "formula_32", "formula_text": "Y k n = V k n ( X n ) and Z k n = (\u03c3 \u2207 V k n )( X n", "formula_coordinates": [6.0, 116.78, 228.36, 169.46, 12.2]}, {"formula_id": "formula_33", "formula_text": "Theorem 3.1. Assume that U \u2282 L 2 (\u2126) \u2229 C \u221e b (\u2126) is a finite dimensional linear subspace, that \u03c3(x, t) is nondegenerate for all (x, t) \u2208 [0, T ] \u00d7 R d ,", "formula_coordinates": [6.0, 55.11, 280.32, 234.32, 34.78]}, {"formula_id": "formula_34", "formula_text": "V n for n \u2208 {0, . . . , N \u2212 1} Output: approximation of V (\u2022, t n ) \u2248 V n along the tra- jectories for n \u2208 {0, . . . , N \u2212 1} Simulate K samples of the discretized SDE (7). Choose V N = g. for n = N \u2212 1 to 0 do approximate either (", "formula_coordinates": [6.0, 317.4, 193.09, 225.69, 93.89]}, {"formula_id": "formula_35", "formula_text": "(\u2202 t + \u2206) V (x, t) \u2212 |\u2207V (x, t)| 2 = 0, (21a) V (x, T ) = g(x), (21b", "formula_coordinates": [6.0, 332.27, 574.36, 209.17, 25.97]}, {"formula_id": "formula_36", "formula_text": ")", "formula_coordinates": [6.0, 537.12, 591.69, 4.32, 8.64]}, {"formula_id": "formula_37", "formula_text": "with g(x) = log 1 2 + 1 2 |x| 2 , leading to b = 0, \u03c3 = \u221a 2 Id d\u00d7d , h(x, s, y, z) = \u2212 1 2 |z| 2 (22)", "formula_coordinates": [6.0, 307.08, 610.43, 234.36, 42.98]}, {"formula_id": "formula_38", "formula_text": "V (x, t) = \u2212 log E e \u2212g(x+ \u221a T \u2212t\u03c3\u03be) ,(23)", "formula_coordinates": [6.0, 348.69, 699.19, 192.75, 16.86]}, {"formula_id": "formula_39", "formula_text": "Vn(x0)\u2212V ref (x0,0) V ref (x0,0)", "formula_coordinates": [7.0, 194.37, 226.01, 64.98, 15.05]}, {"formula_id": "formula_40", "formula_text": "d Polynomial degree 0 1 2 3 4 1 3.62e \u22121 3.60e \u22121 2.47e \u22123 3.86e \u22124 4.27e \u22122 2 1.03e \u22121 1.02e \u22121 1.87e \u22122 1.79e \u22122 1.79e \u22122 5", "formula_coordinates": [8.0, 66.3, 116.21, 233.9, 56.96]}, {"formula_id": "formula_41", "formula_text": "(\u2202 t + L) V (x, t) \u2212 1 2 |(\u03c3 \u2207V )(x, t)| 2 = 0, (24a) V (x, T ) = g(x), (24b)", "formula_coordinates": [8.0, 67.09, 338.28, 222.36, 34.88]}, {"formula_id": "formula_42", "formula_text": "b = \u2212\u2207\u03a8, \u03a8(x) = d i,j=1 C ij (x 2 i \u2212 1)(x 2 j \u2212 1) (25)", "formula_coordinates": [8.0, 61.59, 417.1, 227.85, 30.32]}, {"formula_id": "formula_43", "formula_text": "x) = d i=1 \u03bd i (x i \u2212 1) 2 for \u03bd i > 0. Similarly as before a reference solution is available, V (x, t) = \u2212 log E e \u2212g(X T ) X t = x ,(26)", "formula_coordinates": [8.0, 55.44, 458.48, 235.25, 48.48]}, {"formula_id": "formula_44", "formula_text": "\u2202 t V (x, t) + 1 2 d i,j=1 \u221a x i x j \u03b3 i \u03b3 j \u2202 xi \u2202 xj V (x, t) + d i=1 a i (b i \u2212 x i )\u2202 xi V (x, t) \u2212 max 1\u2264i\u2264d x i V (x, t) = 0. (27", "formula_coordinates": [8.0, 308.76, 576.65, 228.53, 77.87]}, {"formula_id": "formula_45", "formula_text": ")", "formula_coordinates": [8.0, 537.29, 645.88, 4.15, 8.64]}, {"formula_id": "formula_46", "formula_text": "Table 5. K = 1000, d = 100, x0 = [1, 1, . . . , 1]", "formula_coordinates": [9.0, 84.47, 212.34, 175.94, 8.06]}, {"formula_id": "formula_47", "formula_text": "Polynom. degree 0 1 2 3 V 0 (x 0 ) 0.", "formula_coordinates": [9.0, 70.04, 294.92, 202.43, 34.99]}, {"formula_id": "formula_48", "formula_text": "B n 1 n 2 n 3 Figure 9", "formula_coordinates": [13.0, 54.94, 311.12, 136.82, 70.68]}, {"formula_id": "formula_49", "formula_text": "1 \u2264 j k\u22121 \u2264 r k\u22121 , 1 \u2264 i k \u2264 m, and 1 \u2264 j k \u2264 r k within the following formula: b j k\u22121 ,i k ,j k (x) = m,...,m i1,...,i k\u22121 r1,...,r k\u22122 j1,...,j k\u22122 u 1 [i 1 , j 1 ] . . . u k\u22121 [j k\u22122 , i k\u22121 , j k\u22121 ] \u03c6(x 1 )[i 1 ] . . . \u03c6(x k\u22121 )[i k\u22121 ] \u03c6(x k )[i k ] m,...,m i k+1 ,...,i d r k ,...,r d\u22121 j k ,...,j d\u22121 u k+1 [j k , i k+1 , j k+1 ] . . . u d [j d\u22121 , i d ] \u03c6(x k+1 )[i k+1 ] . . . \u03c6(x d )[i d ] .(28)", "formula_coordinates": [13.0, 55.08, 510.53, 238.14, 206.68]}, {"formula_id": "formula_50", "formula_text": "U \u03a3 V A = r r n m n m", "formula_coordinates": [13.0, 324.45, 67.87, 199.01, 15.86]}, {"formula_id": "formula_51", "formula_text": "U = f ( X n ) : f \u2208 U ,(29)", "formula_coordinates": [13.0, 374.82, 281.42, 166.62, 9.65]}, {"formula_id": "formula_52", "formula_text": "\u2022 L 2 (P) is indeed a norm on U. Since U is finite-dimensional, the linear operators U f ( X n ) \u2192 \u2202f \u2202x i ( X n ) \u2208 L 2 (P)(30)", "formula_coordinates": [13.0, 307.44, 345.83, 234.44, 67.49]}, {"formula_id": "formula_53", "formula_text": "\u2202f \u2202x i ( X n ) L 2 (P) \u2264 C 1 f ( X n ) L 2 (P) ,(31)", "formula_coordinates": [13.0, 351.26, 449.72, 190.18, 24.86]}, {"formula_id": "formula_54", "formula_text": "C 2 > 0 such that E f 4 ( X n ) 1/4 := f ( X N ) L 4 (P) \u2264 C 2 f ( X n ) L 2 (P)", "formula_coordinates": [13.0, 310.04, 496.02, 223.87, 39.83]}, {"formula_id": "formula_55", "formula_text": "V k+1 n ( X n ) = \u03a0 U \u2212 h( X n , t n , Y k n , Z k n )\u2206t+ Z k n \u2022 \u03be n+1 \u221a \u2206t \u2212 V n+1 ( X n+1 ) .", "formula_coordinates": [13.0, 333.36, 601.88, 182.16, 29.98]}, {"formula_id": "formula_56", "formula_text": "f ( X n ) \u2192 \u03a0 U \u2212 h( X n , t n , f ( X n ), \u03c3 \u2207f ( X n ))\u2206t+ \u03c3 \u2207f ( X n ) \u2022 \u03be n+1 \u221a \u2206t \u2212 V n+1 ( X n+1 ) . For F 1 , F 2 \u2208 U with F i = f i ( X n ), f i \u2208 U, we see that \u03a8F 1 \u2212 \u03a8F 2 L 2 (P) = \u03a0 U \u2212 h( X n , t n , f 1 ( X n ), \u03c3 \u2207f 1 ( X n ))\u2206t + h( X n , t n , f 2 ( X n ), \u03c3 \u2207f 2 ( X n ))\u2206t + \u221a \u2206t \u03c3 \u2207f 1 ( X n ) \u2212 \u03c3 \u2207f 2 ( X n ) \u2022 \u03be n+1 L 2 (P) \u2264 C 3 \u03a0 U L 2 (P)\u2192L 2 (P) \u2206t F 1 \u2212 F 2 L 2 (P) + \u221a \u2206t \u03c3 \u2207f 1 ( X n ) \u2212 \u03c3 \u2207f 2 ( X n ) \u2022 \u03be n+1 L 2 (P)", "formula_coordinates": [13.0, 315.53, 663.47, 217.83, 38.9]}, {"formula_id": "formula_57", "formula_text": "\u03c3 \u2207f 1 ( X n ) \u2212 \u03c3 \u2207f 2 ( X n ) \u2022 \u03be n+1 L 2 (P) \u2264 \u03c3 \u2207f 1 ( X n ) \u2212 \u03c3 \u2207f 2 ( X n ) 2 1/2 L 2 (P) \u03be 2 n+1 1/2 L 2 (P) \u2264 C 4 F 1 \u2212 F 2 L 2 (P) ,", "formula_coordinates": [14.0, 58.89, 330.2, 227.11, 65.12]}, {"formula_id": "formula_58", "formula_text": "\u03a8F 1 \u2212 \u03a8F 2 \u2264 \u03bb F 1 \u2212 F 2 ,(35)", "formula_coordinates": [14.0, 114.85, 459.49, 174.59, 9.65]}, {"formula_id": "formula_59", "formula_text": "L PDE = 1 KN N n=1 K k=1 (\u2202 t + L)V ( X (k) n , t n ) + h( X (k) n , t n , V ( X (k) n , t n ), (\u03c3 \u2207V )( X (k) n , t n )) 2 ,(36)", "formula_coordinates": [14.0, 58.02, 616.84, 231.42, 65.66]}, {"formula_id": "formula_60", "formula_text": "X (k)", "formula_coordinates": [14.0, 81.63, 693.17, 19.67, 11.87]}, {"formula_id": "formula_61", "formula_text": "L ref = 1 K(N + 1) N n=0 K k=1 V ( X (k) n , t n ) \u2212 V ref ( X (k) n , t n ) V ref ( X (k) n , t n ) ,(37)", "formula_coordinates": [14.0, 307.44, 204.21, 240.82, 41.71]}, {"formula_id": "formula_62", "formula_text": "\u03a6 (x) = A L x L + b L + \u03b8g(x),(38)", "formula_coordinates": [14.0, 362.18, 544.36, 179.26, 9.65]}, {"formula_id": "formula_63", "formula_text": "y l+1 = (A l x l + b l ), x l+1 = (x l , y l+1 ) (39) for 1 \u2264 l \u2264 L \u2212 1 with A l \u2208 R r l \u00d7 l\u22121 i=0 ri , b l \u2208 R l , \u03b8 \u2208 R and x 1 = x.", "formula_coordinates": [14.0, 307.44, 587.38, 234.0, 46.84]}, {"formula_id": "formula_64", "formula_text": "G n = 2000 for 0 \u2264 n \u2264 15 G n = 300 for 16 \u2264 n \u2264 N \u2212 2 G N \u22121 = 10000 \u03b7 n = 0.00005, \u03b7 N \u22121 = 0.0001", "formula_coordinates": [15.0, 361.02, 556.42, 126.84, 45.52]}, {"formula_id": "formula_65", "formula_text": "G n = 10000 for 0 \u2264 n \u2264 5 G n = 6000 for 6 \u2264 n \u2264 N \u2212 2 G N \u22121 = 15000 \u03b7 n = 0.0002, \u03b7 N \u22121 = 0.001", "formula_coordinates": [16.0, 107.78, 204.76, 126.84, 45.52]}, {"formula_id": "formula_66", "formula_text": "(\u2202 t + L) V (x, t) \u2212 1 2 |(\u03c3 \u2207V )(x, t)| 2 = 0,(40a)", "formula_coordinates": [16.0, 67.09, 350.86, 222.36, 22.31]}, {"formula_id": "formula_67", "formula_text": "V (x, T ) = g(x), (40b)", "formula_coordinates": [16.0, 188.51, 376.79, 100.93, 8.96]}, {"formula_id": "formula_68", "formula_text": "(\u2202 t + L) \u03c8(x, t) = 0, (41a) \u03c8(x, T ) = e \u2212g(x) .(41b)", "formula_coordinates": [16.0, 118.55, 456.99, 170.89, 25.87]}, {"formula_id": "formula_69", "formula_text": "\u03c8(x, t) = E e \u2212g(X T ) X t = x ,(42)", "formula_coordinates": [16.0, 106.66, 554.69, 182.78, 11.72]}, {"formula_id": "formula_70", "formula_text": "V (x, t) = \u2212 log E e \u2212g(X T ) X t = x ,(43)", "formula_coordinates": [16.0, 94.1, 604.0, 195.34, 11.72]}, {"formula_id": "formula_71", "formula_text": "X s = x 0 + \u03c3W s ,(44)", "formula_coordinates": [16.0, 389.13, 132.96, 152.31, 9.65]}, {"formula_id": "formula_72", "formula_text": "E [|X t \u2212 x 0 |] < E [|X t \u2212 x 0 | 2 ] = \u03c3 \u221a dt. (45", "formula_coordinates": [16.0, 329.22, 214.59, 208.07, 18.63]}, {"formula_id": "formula_73", "formula_text": ")", "formula_coordinates": [16.0, 537.29, 223.88, 4.15, 8.64]}, {"formula_id": "formula_74", "formula_text": "X t = x 0 + \u03c3 \u221a t1,(46)", "formula_coordinates": [16.0, 387.53, 273.19, 153.92, 18.23]}, {"formula_id": "formula_75", "formula_text": "Id d\u00d7d \u221a d , g(x) = cos d i=1 ix i ,(47)", "formula_coordinates": [16.0, 407.38, 637.11, 134.06, 41.48]}, {"formula_id": "formula_76", "formula_text": ") = k(x) + y 2 \u221a d d i=1 z i + y 2 2 ,(48)", "formula_coordinates": [16.0, 384.17, 685.09, 157.27, 30.32]}, {"formula_id": "formula_77", "formula_text": "V (x, t) = T \u2212 t d d i=1 (sin(x i )1 xi<0 + x i 1 xi\u22650 ) + cos d i=1 ix i .(49)", "formula_coordinates": [17.0, 67.79, 107.36, 221.65, 65.35]}, {"formula_id": "formula_78", "formula_text": "(\u2202 t + \u2206)V (x, t) + V (x, t) \u2212 V 3 (x, t) = 0, (50a) V (x, T ) = g(x), (50b)", "formula_coordinates": [17.0, 67.19, 564.32, 222.25, 25.97]}, {"formula_id": "formula_79", "formula_text": "X 0 = x,(51b)", "formula_coordinates": [17.0, 318.83, 361.1, 222.61, 9.65]}, {"formula_id": "formula_80", "formula_text": "dY s = (\u2212h(X s , s, Y s , Z s ) + v(X s , s) \u2022 Z s )ds + Z s \u2022 dW s ,(51c)", "formula_coordinates": [17.0, 315.97, 376.04, 237.0, 20.91]}, {"formula_id": "formula_83", "formula_text": "Y tn+1 = Y tn \u2212", "formula_coordinates": [17.0, 307.44, 654.53, 57.94, 9.65]}], "doi": "10.1007/s10208-016-9317-9"}