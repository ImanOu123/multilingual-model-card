{"title": "The Validity of Evaluation Results: Assessing Concurrence Across Compositionality Benchmarks", "authors": "Kaiser Sun; Adina Williams; Dieuwke Hupkes; Meta Ai", "pub_date": "", "abstract": "NLP models have progressed drastically in recent years, according to numerous datasets proposed to evaluate performance. Questions remain, however, about how particular dataset design choices may impact the conclusions we draw about model capabilities. In this work, we investigate this question in the domain of compositional generalization. We examine the performance of six modeling approaches across 4 datasets, split according to 8 compositional splitting strategies, ranking models by 18 compositional generalization splits in total. Our results show that: i) the datasets, although all designed to evaluate compositional generalization, rank modeling approaches differently; ii) datasets generated by humans align better with each other than they with synthetic datasets, or than synthetic datasets among themselves; iii) generally, whether datasets are sampled from the same source is more predictive of the resulting model ranking than whether they maintain the same interpretation of compositionality; and iv) which lexical items are used in the data can strongly impact conclusions. Overall, our results demonstrate that much work remains to be done when it comes to assessing whether popular evaluation datasets measure what they intend to measure, and suggests that elucidating more rigorous standards for establishing the validity of evaluation sets could benefit the field. 1   ", "sections": [{"heading": "Introduction", "text": "Over the past few years, NLP has made astonishing progress on almost all language-related tasks proposed by the community. Concurrently, a plethora of benchmark datasets has emerged for evaluating the skills of NLP models and exposing their strengths and weaknesses (Chowdhery et al. 2022, inter alia). These datasets focus on a variety of Figure 1: Pairwise concurrence values averaged across models for each dataset-split pair. Values closer to 1.0 (blue) denote a more similar ranking of models according to their performance on the dataset and split. The dataset and split font color indicate whether the data was generated by humans (purple) or synthetically using rules (green). different aspects of model capabilities, that are increasingly not mutually exclusive: oftentimes, multiple benchmarks are available that target the same capability or skill, using (slightly) different metrics, design choices, and/or conceptual approaches. For instance, Hupkes et al. (2023) report that many recent studies on generalization used different shift sources to study the same types of generalization (see Figure 2). 2 However, somewhat surprisingly, despite a wealth of work in the domain of evaluation and generalization, there is very little research that assesses whether multiple datasets designed to measure the same ability also yield the same conclusions. This makes it difficult for practitioners to conduct informed evaluation dataset selection and, perhaps even more concerning, impedes our understanding of how well different datasets measure what they intend to measure. While establishing construct validity and construct reliability -for instance through comparing the results of tests with other tests that intend to measure the same thingis common practice in the social sciences (Westen and Rosenthal, 2003;Jacobs and Wallach, 2021), it is not the standard in the field of NLP.\nIn this work, we argue that establishing such standards is much needed in our field, and we present a detailed set of experiments that assesses construct validity in the domain of compositional generalization. Following Liu et al. ( 2021), we use concurrence to measure the extent to which 8 different compositional splitting strategies for 4 different datasets -SCAN, GeoQuery, COGS, and Spider -provide similar rankings for 6 different modeling approaches -BART, T5, Transformer, uni-and biLSTMS, and Neural-BTG. We find that, in general, the conclusions drawn from one dataset split typically do not align with the results from another dataset split. In a range of experiments, we explore if that could be attributed to whether the underlying data are synthetic or human-generated, to the compositional splitting strategy is used to create the data (a.k.a. what interpretation of compositionality), or to uncontrolled exposure to lexical items that also occurred during pretraining.\nWe find that concurrence values are generally low: only 10 out of 153 pairs of dataset splits have a concurrence value that surpasses the threshold for high concurrence. Furthermore, results from human-authored datasets concur much more than results from synthetic datasets. On the contrary, dataset splits that share the same interpretation of compositionality -as defined by their splitting strategy -hardly concur with each other: the underlying data plays a more important role in model rankings. Lastly, aligned with the findings of , we find that carefully controlling the lexical items in a compositional split has a positive impact on concurrence. Overall, our results suggest that much work remains to be done to evaluate compositional generalization, and more generally that having more rigorous standards for establishing the validity of evaluation sets should be prioritized in the future. ", "publication_ref": ["b43", "b11"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Related Work", "text": "In this section, we provide an overview of datasets commonly used for assessing compositional generalization, and we discuss previous attempts to compare performance across benchmarks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Datasets for Compositional Generalization", "text": "Since the introduction of SCAN in 2018 , many datasets have been proposed to assess compositional generalization in neural networks. Several of them were direct followups to SCAN that aimed to extend the original dataset or mitigate various issues perceived with it. For instance, Bastings et al. (2018) introduced NACS, a 'reversed' version of SCAN; Loula et al. (2018) introduced new splits using the original dataset; Ruis et al. (2020) introduced a multimodal, grounded version of the benchmark; and Patel et al. (2022) increased the number of primitives. Recently, Valvoda et al. (2022) proposed a transducerbased procedure for generating myriad synthetic datasets similar to SCAN to investigate which formal properties impact the results. Other artificially generated datasets available to evaluate compositionality are PCFG SET (Hupkes et al., 2020), COGS (Kim and Linzen, 2020), and the dataset proposed by Oren et al. (2021). Datasets that use more natural (but often still templated) data are typically situated in the domain of machine translation -such as Li et al. (2021), Dankers et al. (2022) and Raunak et al. (2019) or semantic parsing -e.g. Finegan-Dollak et al. (2018); Keysers et al. (2019); Shaw et al. (2021); Cui et al. (2022). Finally, Thrush et al. (2022) introduce Winoground, aimed to assess compositionality in text-to-image models. In our work, we focus on datasets that target compositionality in the domain of semantic parsing, with the addition of SCAN for its sheer popularity.\nPerformance across benchmarks Several recent works across NLP have been interested in the extent to which strong performance on one task, setting, or dataset transfers to strong performance on another. Typically, such experiments are motivated by transfer learning, rather than establishing the validity of evaluation results. For instance, Vu et al. (2020), Ye et al. (2021), Luo et al. (2022), Padmakumar et al. (2022), andWeber et al. (2021) all investigate to what extent performance transfers across tasks. More closely related to our study, is the work presented by Liu et al. (2021), who quantify the measurement of benchmark agreement on model rankings and compare it in question answering. In our work, we adopt their definition of comparability across datasets.\nIn the context of compositional generalization, the work most closely related to ours is the study presented by Chaabouni et al. (2021), in which they investigate whether the performance improvements on the synthetic dataset SCAN transfer to the naturalistic setting. We largely confirm their results, but consider compositionality benchmarks more broadly, not only considering the synthetic v.s natural dimension, but also interpretations of compositionality and lexical items exposed during pretraining.", "publication_ref": ["b0", "b23", "b32", "b28", "b38", "b9", "b14", "b25", "b20", "b5", "b30", "b6", "b13", "b33", "b4", "b37", "b40", "b46", "b24", "b27", "b42", "b22", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "We compare how the conclusions drawn from 18 different compositional generalization splits -defined over 4 different datasets with 8 compositional splitting strategies -compare across 6 modeling approaches. In this section, we describe the datasets and modeling approaches we consider and provide details on training and hyperparameter selection.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Models", "text": "For our experiments, we consider both pretrained and train-from-scratch approaches that have previously been considered in the context of compositional generalization.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "BART & T5", "text": "We use the pretrained seq2seq models BART (Lewis et al., 2020) and T5 (Raffel et al., 2020) to enable easy comparison with prior work. In the case of BART, order-based noising strategies are used, which may encourage the model to learn to better represent linguistic structure.  LSTM & Transformer To ensure coverage of models without pre-trained knowledge, we use a uni-directional LSTM (Hochreiter and Schmidhuber, 1997), a bi-directional LSTM, and a vanilla transformer (Vaswani et al., 2017).\nNeural-BTG We include one modeling approach specifically designed to address compositionality: Neural-BTG (Wang et al., 2022), composed of a discriminative parser based on a bracketing transduction grammar (BTG; Wu, 1997) and a neural seq2seq model.", "publication_ref": ["b19", "b29", "b7", "b39", "b41", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "Data", "text": "We consider four different datasets designed to test compositional generalization. We focus on datasets for semantic parsing and include SCAN as the most commonly used dataset for compositionality overall. Three of these datasets contain different curated splits that target different interpretations of compositionality. Two of the datasets (SCAN and COGS) are synthetic datasets that are generated with rules, while the other two (Spider and GeoQuery) are natural datasets, authored by humans. Examples for all datasets and descriptions of all curated splits can be found in Appendix A.\nSCAN Consisting of a set of commands and the corresponding action sequences, SCAN ) is one of the most popular synthetic datasets to study compositional generalization. We include the simple, length, add primitive, template splits from . In addition to original SCAN splits, we also use the maximum compound divergence (MCD) splits of SCAN proposed by Keysers et al. (2020).\nCOGS Kim and Linzen (2020) introduced COGS, a synthetic semantic parsing dataset generated by a rule-based approach, which covers a larger variety of grammar rules than SCAN does. The inputs in COGS are English sentences, generated by a probabilistic context-free grammar. The corresponding output, which is the semantic interpretation of the input, is annotated with the logical formalism of Reddy et al. (2017). COGS includes a randomly sampled test set and an out-ofdistribution compositional generalization set. Spider Spider (Yu et al., 2018) is originally designed for cross-domain semantic parsing. We use the compositional generalization splits for Spider defined by Shaw et al. (2021), which match their splits for GeoQuery: random/standard, length, template, and TMCD.", "publication_ref": ["b12", "b14", "b31", "b47", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "Training Setup", "text": "We train/fine-tune the models on the train partition of each dataset described above and evaluate them on the corresponding test set. For T5 on GeoQuery and Spider as well as LSTM and Transformers on COGS, we use the hyperparameters provided in Shaw et al. (2021) and Kim and Linzen (2020), respectively. We followed Orhan (2021) to train T5 and Yao and Koller (2022) to train BART on COGS.\nFor the remaining model-dataset combinations, we perform a hyperparameter search for each dataset, with 10% of instances randomly chosen to be used for tuning. Details can be found in Appendix C. We use three different random seeds for each training run and use five random seeds for each training run of LSTM, to compensate for LSTM's higher variation in performance across seeds. For models with existing evaluations on a dataset, we compare to these previous measures of performance to ensure that our replication results align with previously reported numbers (Keysers et al., 2020;Kim and Linzen, 2020;Orhan, 2021;Shaw et al., 2021;Yao and Koller, 2022;Sun et al., 2023b).", "publication_ref": ["b33", "b14", "b45", "b12", "b14", "b26", "b33", "b45", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "For most datasets, we use exact match (EM) accuracy. EM is a binary metric that only counts an output as correct if it matches the target output exactly, and is most frequently used for the datasets we consider. During initial experiments, we found that, in many cases, EM accuracy may be too strict for our purposes. In some cases, models' tokenizers may prefer slightly different spacing -a phenomenon also reported by Sun et al. (2023a) in others, models lack specific tokens in their vocabulary. Neither of these things is indicative of a model's compositional generalization capability, and we therefore choose to normalize model outputs before applying EM accuracy. In Appendix D, we include examples of such cases, and we report the differences between EM scores with and without our normalization step. For Spider, the original dataset also uses a more lenient EM implementation. For consistency reasons, we use the same implementation across all datasets, but we report Spider EM scores in Appendix E to compare with previous work.", "publication_ref": ["b34"], "figure_ref": [], "table_ref": []}, {"heading": "Measuring Concurrence", "text": "To measure how similarly different dataset splits rank different modeling approaches, we use the concept of concurrence introduced by Liu et al. (2021). The concurrence between two dataset splits is defined as the correlation between the performances of different modeling approaches for those splits. More specifically, the concurrence CONCUR(D 1 , D 2 ; A, Eval) between two dataset splits D 1 and D 2 , given a set of modeling approaches A and evaluation function Eval, is defined as:\nCONCUR(D 1 , D 2 ; A, Eval) = CORR(P 1 , P 2 ),\nwhere CORR is some correlation function and P i is the variable that holds the scores of Eval(a, D i ) for all a \u2208 A. For CORR, Liu et al. ( 2021) considered both Pearson (r) and Kendall rank (\u03c4 ). Because we are interested in how benchmarks rank model performance, we report the concurrence values under Kendall's \u03c4 unless specified otherwise. We refer to the concurrence between the dataset split and itself as self-concurrence, the value of which is purely affected by seed variation across training runs. We see self-concurrence, which would be 1.0 if there is no variation across seeds, as an upper bound for the concurrence values across dataset splits.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We now present our results, starting with a discussion of the performance of models on the datasets ( \u00a74.1) and the concurrence scores between the performances ( \u00a74.2), we then proceed to look at the relationship between synthetic and natural compositionality datasets ( \u00a74.3), and how this interacts with the choice of definition of compositionality and underlying dataset ( \u00a74.4). We finish our results section with a short investigation into the impact of the choice of lexical items in data ( \u00a74.5).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Overall Performance", "text": "In Table 2, we show the performance of all models on all dataset splits under consideration, as well as the average performance per dataset split (last column). Our scores are generally close to the scores reported in previous work, for the (dataset split, architecture) combinations for which previous results exist (Sun et al., 2023b), with the exception of the results for Spider, for which we use a different metric. All models perform reasonably well on the random splits of each datasets (first row for each dataset in Table 2), but most struggle with various generalization splits. While some splits are difficult across the board, other difficulties appear more model-dependent. For instance, while all models are weak on the length and MCD splits of SCAN and length split of Spider, COGS is difficult for some models (e.g., BTG) but much less for others (e.g., T5). Similarly, some models perform well on one of the datasets or one of the splits, but perform poorly on the others. BART, for instance, maintains high performance on GeoQuery and COGS, but performs even worse than non-pretrained models on some splits of SCAN, while BTG performs well on GeoQuery but fails on many splits of SCAN. T5 has high performance on most datasets, but is outperformed by the unidirectional LSTM on the length split of SCAN. SCAN, in particular, appears to be challenging for all models, with the TurnLeft split being the only exception. 3", "publication_ref": ["b35"], "figure_ref": [], "table_ref": ["tab_4", "tab_4"]}, {"heading": "Overall Concurrence", "text": "It is not difficult to tell from Table 2 that the performance of a model on one dataset is not predictive of its performance on the others. To quantitatively substantiate this observation, we compute the concurrences between the different dataset splits, which we visualize in Figure 1. On average, the concurrence between dataset splits is low: a mere 0.22, far below the average self-concurrence of 0.76 that (model, split) combinations have across different seeds. Interestingly, even these average self-concurrence values are lower than the 0.8 that Liu et al. ( 2021) used as a threshold for \"high\" concurrence, indicating that performance on the same compositional dataset is not very stable across runs. 4 Consequently, we lower the threshold to 0.7 here, which is approximately 90% of the average self-concurrence. Of the 153 pairs of dataset split we compare in this experiment, only 10 pairs surpass this threshold. Somewhat surprisingly, perhaps, many of the highest values (reported in Table 3), are concurrences between i.i.d. splits and compositional splits.\nConsidering the concurrence of each dataset with all other datasets (excluding self-concurrence, values are reported below Figure 1), we can see that performance COGS, with an average \u03c4 of 0.36 is most predictive of performance on other datasets. Furthermore, the three semantic parsing datasets have much higher average concurrence than SCAN, suggesting that compositionality on one task may not be predictive of compositionality on another.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Synthetic vs natural data", "text": "Why are these concurrence values so low? The first hypothesis that we explore is that performance on strongly structured templated data may not correlate with performance on datasets that are authored by humans. To this end, we compute the average concurrence values of three combinations of dataset split pairs, natural-natural, natural-synthetic and synthetic-synthetic, and include an example of each pair type in Figure 3. We find that splits of natural datasets concur much better than splits of synthetic datasets (0.54 v.s. 0.22); the worst is concurrence between synthetic and natural dataset splits (0.19). The same finding can be observed in Figure 6, which we will use later to explore the relationship between concurrence values and performance in \u00a74.6.\nThese results are in line with earlier studies that suggested that performance on synthetic compositionality datasets may not transfer to more re-   alistic scenarios (Chaabouni et al., 2021;Shaw et al., 2021), and underline the point made by Dankers et al. (2022), who argue that compositionality should be studied in its natural habitat. Also the concurrence between dataset splits with naturalistic data is well below the threshold for high concurrence, suggesting that there exist factors beyond dataset creation strategy that can affect how compositionality benchmarks rank modeling approaches.", "publication_ref": ["b2", "b33", "b5"], "figure_ref": ["fig_1", "fig_4"], "table_ref": []}, {"heading": "Interpretations of compositionality", "text": "The next hypothesis that we consider is that concurrence values are low because different dataset splits investigate different types of compositionality (cf. Hupkes et al., 2020). In compositional evaluation datasets, the interpretation of compositionality is operationalized through its splitting strategy. One splitting strategy may, for instance, define compositional generalization as generalization to longer lengths, whereas another instead focuses on generalization to novel vocabulary items. These different interpretations of compositionality could potentially require different model capabilities. Could it be that our concurrence values are low because different splits in fact focus on different types of compositional generalization?\nTo investigate this, we group the concurrence values by four dataset pair types -different datasets with the same splitting strategy, the same dataset with different splitting strategies, different datasets with different splitting strategies, and the same dataset with the same splitting strategy -and plot them in Figure 4. Predictably, datasets concur most with themselves (red line). We also see that which data a splitting approach is applied to is more important than the interpretation of compositionality (cyan and dark blue lines, respectively): concur-   rence between experiments that share the same source of data averages at 0.38, whereas different data but the same splitting strategy results in an average concurrence of 0.32. However, when both the source of data and splitting strategy are different (yellow line), the concurrence values shift leftward, suggesting that the data type and splitting strategy pose different kinds of difficulties for the modelling approaches considered.\nLength Generalization Because not every dataset in previous work applied all the splitting strategies, we follow-up with a small experiment in a split shared across all datasets: length generalization splits. 5 The concurrence values between the different length splits, shown in Table 4, are generally low, ranging from \u22120.09 to 0.54 and averaging at 0.16. This additional experiment confirms that even when benchmarks maintain the same interpretation of compositionality, there may still be substantial differences in model rankings, depending on the underlying data.", "publication_ref": ["b9"], "figure_ref": ["fig_2"], "table_ref": ["tab_7"]}, {"heading": "The influence of lexical items", "text": "In Table 2, we can see that pretrained models achieve the highest accuracies and in Table 3 that the highest concurrence values are between two natural datasets. In this section, we dive into the 5 As the original COGS dataset did not come with a length generalization split, we generate one ourselves. differences between pretrained and trained-fromscratch models, and investigate the extent to which those differences affect the concurrence results. In particular, we investigate whether the presence of uncontrolled lexical exposure during pretraining may impact the performance of pretrained models, implying their accuracy numbers may not solely reflect their compositional abilities, as suggested by . Were this to happen, a misalignment in the evaluation between pretrained and nonpretrained models would contribute to variation in the concurrence values, where the performance of pretrained models is overestimated due to lexical exposure in pretraining.\nTo test for possible effects of lexical exposure, we extend the experiment from  -who conducted it for COGS -to the TMCD and Std split of GeoQueory, and the TurnLeft split of SCAN 6 In both cases, we swap out lexical items with strings of similar length that act as \"wug words\" (Berko, 1958), or, in other words, previously unattested and therefore meaningless lexical items. Following , we generate the strings in two ways:\n\u2022 Rstr: We randomly sample lowercase characters from the Latin script with replacements. \u2022 Rcvcv: We alternately sample a vowel after a consonant from the Latin script. We train the models on all modified splits and compute the performance (Figure 5). We also compute the concurrence between the original split and the modified split (Table 5a and Table 5b).  In Figure 5, we see that the performance of the pretrained models drops drastically when the lexical items are replaced, while the non-pretrained models' performance does not, confirming the results of . In addition, the concurrence between the original splits and the modified splits for all datasets is below our set thresholdalbeit higher than other comparisons we have seen before (Table 5a) -implying that replacing lexical items results in yet another new ranking of modeling approaches for compositionality.\nWe then compute the concurrence between the same set of splits before and after the lexical exposure edits: within the group of splits that are selected for the lexical changes, the concurrence values decrease from 0.49 to 0.41, while the average concurrence values of these splits with other splits that haven't undergone lexical edits slightly increase from 0.25 to 0.26 (e.g. concurrence between GeoQuery and Spider TMCD splits increases when GeoQuery TMCD split applies the lexical changes), with many more dataset split pairs surpassing the \u03c4 = 0.7 bar for high concurrence (Table 5b).\nA closer look explains this apparent contrast: the overall low-concurring dataset SCAN -which makes up 12.5% of the lexically edited splits, drags down the concurrence values within that group. Excluding SCAN, the within-group concurrence values also increase, from 0.63 to 0.66. These results do thus not only confirm that controlling lexical exposure is important when evaluating compositionality in pretrained models, but also further exemplify our earlier finding that compositionality scores -for neural models -strongly depend task and dataset. We further analyze the influence of tasks to compositionality results in Appendix F.", "publication_ref": ["b1"], "figure_ref": ["fig_3", "fig_3"], "table_ref": ["tab_4", "tab_5", "tab_9", "tab_9", "tab_9"]}, {"heading": "Other confounding factors", "text": "We have explored a range of factors that may impact the evaluation of compositionality, such as the nature of the underlying data and task, the interpretation of compositionality, and the choice of lexical items. We wrap up our analysis by verifying that our results are not driven by specific performance scores: we verify that concurrence values are not skewed by datasets for which performances are saturated or close to random. To assess this, we compute the correlation between the average performance between two datasets and their concurrence, as plotted in Figure 6. As can be seen, there is no apparent relation between average performance and concurrence: difficult datasets do not concur less or more than easier ones, and dataset saturation (or the opposite: random performance) appears not to impact the results. A correlation test confirms this visually observed pattern: the Pearson correlation coefficient between performance and concurrence is near zero (r = 0.026).", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we explored how different evaluation choices impact the conclusions drawn from the experiments evaluating compositionality. Using compositional generalization datasets and models ranging from trained-from-scratch to pretrained, we conduct a series of experiments to understand whether datasets consistently rank models in terms of their generalizability, and we find little consistency. When we perform further analysis to try to better understand this inconsistency, we find that comparing within the training setting (pretrained v.s. trained-from-scratch) or data creation type (synthetically generated v. naturally generated) does not increase consistency. However, better controlling the lexical items can help us draw more consistent conclusions, at least for datasets that share the same notion of compositionality. We leave the investigation into how task selection might affect evaluation results for compositional generalization to further research. Overall, our results suggest that to evaluate compositional generalization consistently, clearer definitions of compositionality are needed, as well as more careful consideration of evaluation design and more thorough dataset evaluations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Dataset examples", "text": "For convenience, we include a brief description with examples of all datasets we consider in our experiments in Table 6. The description of each split and the number of instances in each dataset split is shown in Table 7 and Table 8.\nSCAN Consisting of a set of commands and the corresponding action sequences, SCAN ) is one of the most popular synthetic datasets to study compositional generalization. The model is given commands like jump left and is expected to predict action sequences like LTURN JUMP. We include the simple, length, add primitive, template splits from .\nIn addition to original SCAN splits, we also use maximum compound divergence (MCD) splits of SCAN proposed by Keysers et al. (2020).\nCOGS Kim and Linzen (2020) introduce COGS, a synthetic semantic parsing dataset generated by a rule-based approach, which covers a larger variety of grammar rules than SCAN does. The inputs in COGS are English sentences, generated by a probabilistic context-free grammar. The corresponding output, which is the semantic interpretation of the input, is annotated with the logical formalism in Reddy et al. (2017). COGS includes a randomly sampled test set and an out-of-distribution compositional generalization set.\nGeoQuery GeoQuery (Tang and Mooney, 2001;Zelle and Mooney, 1996) is a text-to-QL dataset containing naturalistic examples. We use the four compositional generalization splits defined on this dataset by Shaw et al. (2021): We use the splits in Shaw et al. (2021), in which all entity mentions are converted with placeholders and use Functional Query Language (FunQL) as the target representation. random/standard, length, template, and Target Maximum Compound Divergence (TMCD).\nThe TMCD split is an extension of MCD splits in SCAN, with the capability to be applied to nonsynthetic datasets.\nSpider Spider (Yu et al., 2018) is originally designed for cross-domain semantic parsing, and targets a challenging kind of generalization, generalization to new database schemata, using different databases for the training and test set. It also uses SQL for a more complex syntax. We use the compositional generalization splits for Spider defined by Shaw et al. (2021), which match their splits for GeoQuery: random/standard, length, template, and TMCD. In the same paper, Shaw et al. (2021) split Spider into the same four splits as GeoQuery and adopt a setting where databases are shared between train and test examples so that the dataset splits can be dedicated to evaluating compositional generalization.", "publication_ref": ["b12", "b14", "b31", "b36", "b48", "b33", "b33", "b47", "b33", "b33"], "figure_ref": [], "table_ref": ["tab_11", "tab_12"]}, {"heading": "B License of Artifacts", "text": "We include the licenses and intended usage of artifacts used in this work in Table 9.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_17"]}, {"heading": "C Hyperparameters", "text": "For the models and dataset combinations that have already been trained by prior works, we adopt the same set of hyperparameters. For the remaining combinations, we tune the hyperparameters on a random split of the original dataset, with 90% data in the training set and 10% data in the test set. We describe the final hyperparamters below.\nFor T5 with GEOQUERY and SPIDER, we follow the same hyperparameter setup as Shaw et al., 2021. For LSTM and Transformer with COGS, we follow the same hyperparameter setup as in Kim and Linzen, 2020. For T5 with COGS, we follow the training strategy from (Orhan, 2021).\nFor other datasets, we tune the learning rate of T5 and BART in [10 \u22125 , 10 \u22124 , 10 \u22123 ]. We tune the dropout rate in [0.0, 0.1, 0.5] and layers in [1,2] for LSTMs; dropout rate in [0.0, 0.1, 0.5] and layers in [2,4,8] for Transformer. For BTG, we tune the vocabulary size between 200 and 800, as well as the learning rate in [1.0 \u00d7 10 \u22124 , 3.0 \u00d7 10 \u22124 ].", "publication_ref": ["b33"], "figure_ref": [], "table_ref": []}, {"heading": "COGS Input:", "text": "Mila liked that the cake was offered to Emma . Output:\n* cake ( x _ 4 ) ; like . agent ( x _ 1 , Mila ) AND like . ccomp ( x _ 1 , x _ 6 ) AND offer . theme ( x _ 6 , x _ 4 ) AND offer . recipient ( x _ 6 , Emma ) answer ( intersection ( river , loc_2 ( m0 ) ) )", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Spider", "text": "Input: flight_1: what is the average distance and price for all flights from la? Output:\nselect avg(distance) , avg(price) from flight where origin = \"los angeles\"    The most intuitive implementation of exact match accuracy is directly comparing the output text string with the gold sequence, without any postprocessing. However, we found this to be unnecessarily strict for some models, such as T5, which does not have the \"<\" symbol, which appears in a large number of instances, in the vocabulary and required post-processing to replace the UNK tokens with \"<\". In addition, although the location of space should not change the correctness of a prediction for our evaluated datasets, often incorrect spaces led to wrong evaluation when direct text comparison is used. Table 11 shows an example of such an instance. With the leniency on spaces, T5's exact match value changed from zero accuracy on a whole dataset (COGS) to performing among the best on all datasets (Table 10); this is likely due to the tokenization of special tokens with space, as noted in Sun et al. (2023a).   ", "publication_ref": ["b34"], "figure_ref": [], "table_ref": ["tab_1", "tab_1"]}, {"heading": "Neural-BTG MIT", "text": "A neural transducer for sequence-to-sequence tasks. LSTM, Transformer (OpenNMT-py (Klein et al., 2017))\nMIT Models for sequence-to-sequence tasks.\nT5 Apache-2.0 A pre-trained model for sequence-to-sequence tasks. BART Apache-2.0 A pre-trained model for sequence-to-sequence tasks.  ", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "F The influence of task similarity", "text": "As briefly mentioned in \u00a74.5, task formulation can be another factor that affects the agreement between datasets. To understand the effect of task similarity on the conclusion obtained from compositionality benchmarks, we add in the NACS dataset (Bastings et al., 2018) for existing experiments, as all three datasets except for SCAN are semantic parsing tasks, while SCAN falls under a navigation task. NACS is introduced as a dataset that is similar to SCAN but requires mapping actions back to the original commands, and it is thus more complex for models compared to SCAN and will not allow simple models to gain unintended high performance. We train models on NACS with the same hyperparameter tuning and training strategy as in \u00a73, compute the concurrence between NACS and other datasets, and look at the effect of different splitting strategy between SCAN and NACS. The results are discussed below.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "F.1 Overall Performance and Concurrence", "text": "The overall performance and concurrence including NACS are shown in Table 15 and Figure 7. The concurrence values between NACS and SCAN is surprisingly low compared to the concurrence values between NACS and other datasets, with the length split being the only exception, suggesting that even when the underlying tasks are the same, the datasets may provide very different model rankings. In terms of the distribution of concurrence values by type of data split pairs (Figure 8), the conclusion in \u00a74.4 persists: the source of the dataset matters more than the interpretation of compositonality (splitting strategy).", "publication_ref": [], "figure_ref": ["fig_6"], "table_ref": ["tab_1"]}, {"heading": "F.2 Length Split of NACS", "text": "Out of the four splits of NACS, the length split is the only split that results in a high concurrence with tsplits of SCAN (Figure 7). The length split of SCAN and NACS is also the only length splits pair   H Mistakes that model make in both random splits and generalization splits\nThe in-distribution performance may also be a confounder when at least one of the models does not perform as well on an in-distribution test set, or in a random split of the data. Qualitatively, we observe that models sometimes make the same trivial mistakes in both a random split and a generalization split, making the resulting raw metric unrepresentative of compositionality. For example, BART makes mistakes on parentheses, adding or dropping them on both standard split and generalization splits of GeoQuery (  make the mistakes in the standard split again in the generalization splits requires further research. We also include a Genbench evaluation card (Hupkes et al., 2023) in Table 19.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "I Limitations", "text": "While we explore the consequences of the modeling approach on concurrence, we have focused mainly on models trained from scratch to perform compositional generalization or pretrained models which have been finetuned. Another possible area of investigation would be to explore the extent to which a model's compositional generalization abilities also transfer to in-context evaluations (Hosseini et al., 2022). We leave this question for future work.\nExample 1.\nBART on GeoQuery standard and template Input:\nwhat are the highest points of all the states Output:\nanswer ( highest ( intersection ( place , loc_2 ( state ) ) ) ) Prediction:\nanswer ( highest ( intersection ( place , loc_2 ( state ) ) ) ) ) Input:\nwhat is the adjacent state of m0 Output:\nanswer ( intersection ( state , next_to_2 ( m0 ) ) ) Prediction:\nanswer ( intersection ( state , next_to_2 ( m0 ) ) ) )\nExample 2. BTG on GeoQuery simple and TurnLeft Input:\nrun left thrice and look opposite right thrice Output:\nTURN_LEFT RUN TURN_LEFT RUN TURN_LEFT RUN TURN_RIGHT TURN_RIGHT LOOK TURN_RIGHT TURN_RIGHT LOOK TURN_RIGHT TURN_RIGHT I_LOOK Prediction:\nTURN_LEFT RUN TURN_LEFT RUN TURN_LEFT RUN TURN_LEFT TURN_LEFT LOOK TURN_LEFT TURN_LEFT LOOK TURN_LEFT TURN_LEFT LOOK Input:\nlook right after turn left Output:\nTURN_LEFT TURN_RIGHT LOOK Prediction:\nTURN_LEFT TURN_LEFT LOOK  ", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Input:", "text": "Zoe thought that a hippo cleaned . Output:\nthink . agent ( x _ 1 , Zoe ) AND think . ccomp ( x _ 1 , x _ 5 ) AND hippo ( x _ 4 ) AND clean . agent ( x _ 5 , x _ 4 ) Prediction:\nthink. agent ( x _ 1, Zoe ) AND think. ccomp ( x _ 1, x _ 5 ) AND hippo ( x _ 4 ) AND clean. agent ( x _ 5, x _ 4 )  that exceed the boundary set for high concurrence (Table 14). It is likely because that both length split of NACS and the splits that it has high concurrence with are extremely difficult split that many models fail on.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G Performance and concurrence across all setups", "text": "The performance of all models on all the curated splits for each dataset is shown in Table 15. The concurrence between all datasets and split pairs in this work is shown in Figure 9 and the exact values are included in Table 17.   ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "COGS", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Jump to better conclusions: SCAN both left and right", "journal": "", "year": "2018", "authors": "Jasmijn Bastings; Marco Baroni; Jason Weston; Kyunghyun Cho; Douwe Kiela"}, {"ref_id": "b1", "title": "The child's learning of english morphology", "journal": "Word", "year": "1958", "authors": "Jean Berko"}, {"ref_id": "b2", "title": "Can transformers jump around right in natural language? assessing performance transfer from SCAN", "journal": "", "year": "2021", "authors": "Rahma Chaabouni; Roberto Dess\u00ec; Eugene Kharitonov"}, {"ref_id": "b3", "title": "PaLM: Scaling language modeling with pathways", "journal": "", "year": "2022", "authors": "Aakanksha Chowdhery; Sharan Narang; Jacob Devlin; Maarten Bosma; Gaurav Mishra"}, {"ref_id": "b4", "title": "Compositional generalization in multilingual semantic parsing over Wikidata", "journal": "Transactions of the Association for Computational Linguistics", "year": "2022", "authors": "Ruixiang Cui; Rahul Aralikatte; Heather Lent; Daniel Hershcovich"}, {"ref_id": "b5", "title": "The paradox of the compositionality of natural language: A neural machine translation case study", "journal": "Long Papers", "year": "2022", "authors": "Verna Dankers; Elia Bruni; Dieuwke Hupkes"}, {"ref_id": "b6", "title": "Improving textto-SQL evaluation methodology", "journal": "Long Papers", "year": "2018", "authors": "Catherine Finegan-Dollak; Jonathan K Kummerfeld; Li Zhang; Karthik Ramanathan; Sesh Sadasivam; Rui Zhang; Dragomir Radev"}, {"ref_id": "b7", "title": "Long short-term memory", "journal": "Neural computation", "year": "1997", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"ref_id": "b8", "title": "On the compositional generalization gap of in-context learning", "journal": "", "year": "2022", "authors": "Arian Hosseini; Ankit Vani; Dzmitry Bahdanau; Alessandro Sordoni; Aaron Courville"}, {"ref_id": "b9", "title": "Compositionality decomposed: How do neural networks generalise", "journal": "Journal of Artificial Intelligence Research", "year": "2020", "authors": "Dieuwke Hupkes; Verna Dankers; Mathijs Mul; Elia Bruni"}, {"ref_id": "b10", "title": "Cotterell, and Zhijing Jin. 2023. A taxonomy and review of generalization research in nlp", "journal": "Nature Machine Intelligence", "year": "", "authors": "Dieuwke Hupkes; Mario Giulianelli; Verna Dankers; Mikel Artetxe; Yanai Elazar; Tiago Pimentel; Christos Christodoulopoulos; Karim Lasri; Naomi Saphra; Arabella Sinclair; Dennis Ulmer; Florian Schottmann; Khuyagbaatar Batsuren; Kaiser Sun; Koustuv Sinha; Leila Khalatbari"}, {"ref_id": "b11", "title": "Measurement and fairness", "journal": "", "year": "2021", "authors": "Z Abigail; Hanna Jacobs;  Wallach"}, {"ref_id": "b12", "title": "Measuring compositional generalization: A comprehensive method on realistic data", "journal": "", "year": "2020", "authors": "Daniel Keysers; Nathanael Sch\u00e4rli; Nathan Scales; Hylke Buisman; Daniel Furrer; Sergii Kashubin; Nikola Momchev; Danila Sinopalnikov; Lukasz Stafiniak; Tibor Tihon; Dmitry Tsarkov; Xiao Wang; Olivier Marc Van Zee;  Bousquet"}, {"ref_id": "b13", "title": "Measuring compositional generalization: A comprehensive method on realistic data", "journal": "", "year": "2019", "authors": "Daniel Keysers; Nathanael Sch\u00e4rli; Nathan Scales; Hylke Buisman; Daniel Furrer; Sergii Kashubin; Nikola Momchev; Danila Sinopalnikov; Lukasz Stafiniak; Tibor Tihon"}, {"ref_id": "b14", "title": "COGS: A compositional generalization challenge based on semantic interpretation", "journal": "", "year": "2020", "authors": "Najoung Kim; Tal Linzen"}, {"ref_id": "b15", "title": "Uncontrolled lexical exposure leads to overestimation of compositional generalization in pretrained models", "journal": "", "year": "2022", "authors": "Najoung Kim; Tal Linzen; Paul Smolensky"}, {"ref_id": "b16", "title": "Sequence-to-sequence learning with latent neural grammars", "journal": "", "year": "2021", "authors": "Yoon Kim"}, {"ref_id": "b17", "title": "OpenNMT: Opensource toolkit for neural machine translation", "journal": "", "year": "2017", "authors": "Guillaume Klein; Yoon Kim; Yuntian Deng; Jean Senellart; Alexander Rush"}, {"ref_id": "b18", "title": "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks", "journal": "PMLR", "year": "2018", "authors": "Brenden Lake; Marco Baroni"}, {"ref_id": "b19", "title": "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension", "journal": "", "year": "2020", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal; Marjan Ghazvininejad; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"ref_id": "b20", "title": "On compositional generalization of neural machine translation", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Yafu Li; Yongjing Yin; Yulong Chen; Yue Zhang"}, {"ref_id": "b21", "title": "Memorize or generalize? searching for a compositional RNN in a haystack", "journal": "", "year": "2018", "authors": "Adam Liska; Germ\u00e1n Kruszewski; Marco Baroni"}, {"ref_id": "b22", "title": "Do question answering modeling improvements hold across benchmarks? arXiv preprint", "journal": "", "year": "2021", "authors": "F Nelson; Tony Liu; Robin Lee; Percy Jia;  Liang"}, {"ref_id": "b23", "title": "Rearranging the familiar: Testing compositional generalization in recurrent networks", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Jo\u00e3o Loula; Marco Baroni; Brenden Lake"}, {"ref_id": "b24", "title": "Cog-Taskonomy: Cognitively inspired task taxonomy is beneficial to transfer learning in NLP", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Yifei Luo; Minghui Xu; Deyi Xiong"}, {"ref_id": "b25", "title": "Finding needles in a haystack: Sampling structurallydiverse training sets from synthetic data for compositional generalization", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Inbar Oren; Jonathan Herzig; Jonathan Berant"}, {"ref_id": "b26", "title": "Compositional generalization in semantic parsing with pretrained transformers", "journal": "", "year": "2021", "authors": " A Emin Orhan"}, {"ref_id": "b27", "title": "Exploring the role of task transferability in largescale multi-task learning", "journal": "", "year": "2022", "authors": "Vishakh Padmakumar; Leonard Lausen; Miguel Ballesteros; Sheng Zha; He He; George Karypis"}, {"ref_id": "b28", "title": "Revisiting the compositional generalization abilities of neural sequence models", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Arkil Patel; Satwik Bhattamishra; Phil Blunsom; Navin Goyal"}, {"ref_id": "b29", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "J. Mach. Learn. Res", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; J Peter;  Liu"}, {"ref_id": "b30", "title": "On compositionality in neural machine translation", "journal": "", "year": "2019", "authors": "Vikas Raunak; Vaibhav Kumar; Florian Metze; Jaimie Callan"}, {"ref_id": "b31", "title": "Mark Steedman, and Mirella Lapata", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Siva Reddy; Oscar T\u00e4ckstr\u00f6m; Slav Petrov"}, {"ref_id": "b32", "title": "A benchmark for systematic generalization in grounded language understanding", "journal": "", "year": "2020-12-06", "authors": "Laura Ruis; Jacob Andreas; Marco Baroni; Diane Bouchacourt; Brenden M Lake"}, {"ref_id": "b33", "title": "Compositional generalization and natural language variation: Can a semantic parsing approach handle both?", "journal": "", "year": "2021", "authors": "Peter Shaw; Ming-Wei Chang; Panupong Pasupat; Kristina Toutanova"}, {"ref_id": "b34", "title": "Tokenization consistency matters for generative models on extractive NLP tasks", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Kaiser Sun; Peng Qi; Yuhao Zhang; Lan Liu; William Yang Wang; Zhiheng Huang"}, {"ref_id": "b35", "title": "A replication study of compositional generalization works on semantic parsing", "journal": "ReScience C", "year": "2023", "authors": "Kaiser Sun; Adina Williams; Dieuwke Hupkes"}, {"ref_id": "b36", "title": "Using multiple clause constructors in inductive logic programming for semantic parsing", "journal": "Springer", "year": "2001", "authors": "R Lappoon; Raymond J Tang;  Mooney"}, {"ref_id": "b37", "title": "Winoground: Probing vision and language models for visio-linguistic compositionality", "journal": "", "year": "2022", "authors": "Tristan Thrush; Ryan Jiang; Max Bartolo; Amanpreet Singh; Adina Williams; Douwe Kiela; Candace Ross"}, {"ref_id": "b38", "title": "Benchmarking compositionality with formal languages", "journal": "", "year": "2022", "authors": "Josef Valvoda; Naomi Saphra; Jonathan Rawski; Adina Williams; Ryan Cotterell"}, {"ref_id": "b39", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b40", "title": "Exploring and predicting transferability across NLP tasks", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Tu Vu; Tong Wang; Tsendsuren Munkhdalai; Alessandro Sordoni; Adam Trischler; Andrew Mattarella-Micke; Subhransu Maji; Mohit Iyyer"}, {"ref_id": "b41", "title": "Hierarchical phrase-based sequence-tosequence learning", "journal": "", "year": "2022", "authors": "Bailin Wang; Ivan Titov; Jacob Andreas; Yoon Kim"}, {"ref_id": "b42", "title": "Language modelling as a multi-task problem", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Lucas Weber; Jaap Jumelet; Elia Bruni; Dieuwke Hupkes"}, {"ref_id": "b43", "title": "Quantifying construct validity: two simple measures", "journal": "Journal of personality and social psychology", "year": "2003", "authors": "Drew Westen; Robert Rosenthal"}, {"ref_id": "b44", "title": "Stochastic inversion transduction grammars and bilingual parsing of parallel corpora", "journal": "Computational Linguistics", "year": "1997", "authors": "Dekai Wu"}, {"ref_id": "b45", "title": "Structural generalization is hard for sequence-to-sequence models", "journal": "", "year": "2022", "authors": "Yuekun Yao; Alexander Koller"}, {"ref_id": "b46", "title": "CrossFit: A few-shot learning challenge for crosstask generalization in NLP", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Qinyuan Ye; Xiang Bill Yuchen Lin;  Ren"}, {"ref_id": "b47", "title": "Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Tao Yu; Rui Zhang; Kai Yang; Michihiro Yasunaga; Dongxu Wang; Zifan Li; James Ma; Irene Li; Qingning Yao; Shanelle Roman; Zilin Zhang; Dragomir Radev"}, {"ref_id": "b48", "title": "Learning to parse database queries using inductive logic programming", "journal": "", "year": "1996", "authors": "M John; Raymond J Zelle;  Mooney"}, {"ref_id": "b49", "title": "Amount of Pairs over All Instances DiffData, SameSplit", "journal": "", "year": "", "authors": ""}, {"ref_id": "b50", "title": "", "journal": "", "year": "", "authors": "Diffsplit Samedata"}, {"ref_id": "b51", "title": "", "journal": "", "year": "", "authors": "Diffsplit Diffdata"}, {"ref_id": "b52", "title": "", "journal": "", "year": "", "authors": "Samesplit Samedata"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "FuFigure 2 :2Figure 2: Generalization studies published in the ACL anthology (2015-2022), across different shift sources.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure3: Performance of one dataset split versus another. Upper left is an example of high concurrence pair between a synthetic and a natural dataset; upper right is an example of low concurrence within synthetic datasets; lower left is an example of high concurrence within natural datasets; lower right is an example of low concurrence between natural and synthetic datasets.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Distribution of concurrence values among all dataset splits. The color of the bar indicates whether the splits in the pair share the same dataset origin and/or the same splitting strategy.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure5: Performance of the original split versus the splits with lexical items replaced. Performance of pretrained models decreases when train on the splits with lexical items that are not previously seen in pretraining.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure 6: Values of concurrences with respect to pairwise averaged performance among the splits shown in Table 2. The color of dots indicates the type of split pairs. The triangle-shape dots indicates the values of self-concurrence.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 8 :8Figure 8: Distribution of concurrence values among all dataset splits. The color of the bar indicates whether the splits in the pair share the same dataset origin and/or the same splitting strategy.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ": Examples of instances in each dataset used inour experiments."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Model exact-match accuracy on datasets averaged across random seeds, with standard deviation.", "figure_data": "Dataset ADataset BSplit ASplit BConcurSpiderSpiderTemplateTMCD0.88GeoQuerySpiderStdTemplate0.84GeoQuerySpiderStdTMCD0.83SCANSpiderTemplateRand0.76SCANSpiderTemplateLength0.76SpiderSpiderRandLength0.75GeoQuerySpiderTemplateTemplate0.74GeoQuerySpiderTemplateTMCD0.73GeoQueryGeoQueryStdTemplate0.73SCANSCANLengthMCD30.72"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "High concurrence values (\u2265 0.7) among all pairs of dataset splits, excluding self-concurrence.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Performance and Concurrence between the lexically-processed splits of datasets.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_RUN I_RUN I_RUN", "figure_data": "SCANInput: Output:turn left after jump twice I_JUMP I_JUMP I_TURN_LEFTNACSInput: Output:run thrice after jump around leftGeoQueryInput: Output:how much population does m0 have"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Examples of instances in each dataset used in our experiments.", "figure_data": "SplitDatasetDescriptionrandom/standard/simple COGS, SCAN, GeoQuery, Spider Split the dataset randomly.lengthCOGS, SCAN, GeoQuery, Spider Split the dataset according to the input length.templateSCAN, GeoQuery, SpiderSplit the dataset based on a given string template.TurnLeftSCANCompositional commands of TurnLeft are isolated in training set.JumpSCANCompositional commands of Jump are isolated in training set.MCDSCANSplit according to maximum compound divergence.TMCDGeoQuery, SpiderNatural counterpart of MCD, split the data based on target MCD.GenCOGSNot a splitting strategy, but a collection of specially generated samples designed to test 21 cases of generalization in COGS."}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Summary of each split and their designated dataset we use.", "figure_data": ""}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "", "figure_data": ": Percentage difference between raw EM imple-mentation and EM implementation that ignore harmlessspace (space-lenient EM -raw EM). SCAN and NACSare omitted because models do not have this issue onthem. LSTMs do not display this issue; the differencefor Transformer is under 0.1% for each datset."}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Model exact-match accuracy with Spider EM. A large amount of output of LSTM and Transformer are deemed as invalid SQL due to special tokens.", "figure_data": "The official release of Spider (Yu et al., 2018) usesa different variant of exact match accuracy, whichis more lenient than the version we used. We in-clude a table of model performance on splits ofSpider, evaluated with the official Spider metric in"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "Number of instances for each dataset in each optimization split.", "figure_data": "ArtifactLicenseIntended UsageCOGSMITA dataset focuses on compositional generalizationSCANBSDA dataset focuses on compositional generalization.GeoQueryODC-BY 1.0 licenseA database query datasets for U.S. geography.SpiderCC BY-SA 4.0A cross-domain semantic parsing and text-to-SQL dataset.NACSCC-NCA dataset focuses on compositional generalization."}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "License and intended usage for the artifacts we used.", "figure_data": ""}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "", "figure_data": ""}, {"figure_label": "14", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "Concurrence between length splits of datasets.", "figure_data": ""}, {"figure_label": "18", "figure_type": "table", "figure_id": "tab_21", "figure_caption": "); BTG cannot tell left from right in the simple split of SCAN, and the same type of mistake continues to appear in the template split. While simple mistakes like these and the space tokenization issue mentioned in Section 3.4 can be easily resolved by adopting a post-processing protocol or rules to ignore when computing EM, other types of less identifiable errors may also be present and harder to patch. Since many of the models do not achieve near-perfect performance on the random splits, to what extent they", "figure_data": "DatasetSplitLSTM UniLSTM BiTransformerT5BARTBTGAvgStd-Test99.3\u00b1.099.1\u00b1.0199.5\u00b1.099.7\u00b1.099.7\u00b1.068.8\u00b1.0194.3Rcvcv-Test99.4\u00b1.099.1\u00b1.099.5\u00b1.099.7\u00b1.099.7\u00b1.068.1\u00b1.094.2Rstr-Test99.4\u00b1.099.0\u00b1.0199.6\u00b1.099.8\u00b1.099.7\u00b1.068.4\u00b1.094.3COGSStd-Gen21.3\u00b1.0514.8\u00b1.0856.1\u00b1.0682.9\u00b1.078.6\u00b1.02.8\u00b1.0142.8Rcvcv-Gen22.6\u00b1.0410.1\u00b1.0257.6\u00b1.0250.0\u00b1.0244.5\u00b1.070.0\u00b1.030.8Rstr-Gen22.3\u00b1.0714.7\u00b1.0356.6\u00b1.0348.0\u00b1.0133.5\u00b1.030.0\u00b1.029.2Length20.7\u00b1.0124.9\u00b1.0128.7\u00b1.0237.9\u00b1.034.1\u00b1.0120.5\u00b1.027.8Simple99.9\u00b1.099.9\u00b1.0100.0\u00b1.094.9\u00b1.0199.1\u00b1.0112.3\u00b1.0184.4Jump0.4\u00b1.010.0\u00b1.00.1\u00b1.095.0\u00b1.010.4\u00b1.010.0\u00b1.016.0Template0.2\u00b1.00.3\u00b1.011.1\u00b1.034.3\u00b1.030.0\u00b1.00.9\u00b1.016.1MCD15.9\u00b1.0612.2\u00b1.071.1\u00b1.024.6\u00b1.010.4\u00b1.011.8\u00b1.017.7SCANMCD2 MCD36.7 8.7\u00b1.03 \u00b1.045.8 7.8\u00b1.03 \u00b1.021.2 0.7\u00b1.0 \u00b1.034.1 11.1\u00b1.01 \u00b1.011.6 1.2\u00b1.0 \u00b1.010.5 0.8\u00b1.0 \u00b1.018.3 5.0Length15.3\u00b1.0411.8\u00b1.010.0\u00b1.014.1\u00b1.010.7\u00b1.010.0\u00b1.07.0TurnLeft61.1\u00b1.1334.1\u00b1.0664.8\u00b1.1170.3\u00b1.1263.1\u00b1.198.9\u00b1.0150.4TurnLeftRcvcv69.4\u00b1.1442.8\u00b1.1460.4\u00b1.1220.0\u00b1.0337.7\u00b1.153.5\u00b1.0139.0TurnLeftRStr59.0\u00b1.1843.5\u00b1.161.9\u00b1.117.7\u00b1.0223.9\u00b1.172.4\u00b1.034.7Simple100.0\u00b1.0100.0\u00b1.0100.0\u00b1.094.6\u00b1.0100.0\u00b1.06.1\u00b1.0183.5NACSJump TurnLeft0.1 63.3\u00b1.0 \u00b1.120.2 62.0\u00b1.0 \u00b1.130.2 54.4\u00b1.0 \u00b1.1195.8 64.9\u00b1.01 \u00b1.0467.6 82.4\u00b1.04 \u00b1.130.0 9.2\u00b1.0 \u00b1.0127.3 56.0Length12.7\u00b1.0213.2\u00b1.010.0\u00b1.014.3\u00b1.09.3\u00b1.020.0\u00b1.08.2Rand33.4\u00b1.0236.9\u00b1.0142.5\u00b1.0168.0\u00b1.032.7\u00b1.0140.1\u00b1.0142.3SpiderTemplate TMCD1.0 4.6\u00b1.0 \u00b1.012.2 6.0\u00b1.01 \u00b1.014.6 7.5\u00b1.0 \u00b1.0139.6 47.2\u00b1.01 \u00b1.0121.6 31.2\u00b1.01 \u00b1.031.9 5.5\u00b1.0 \u00b1.011.8 17.0Length12.7\u00b1.0114.0\u00b1.0117.5\u00b1.0135.4\u00b1.017.4\u00b1.014.0\u00b1.0116.8Std74.0\u00b1.0678.9\u00b1.0482.3\u00b1.0292.5\u00b1.0189.2\u00b1.0179.0\u00b1.0182.6Std-Rcvcv76.7\u00b1.0378.9\u00b1.0280.5\u00b1.0189.4\u00b1.084.2\u00b1.069.0\u00b1.0379.8Std-Rstr77.1\u00b1.0178.6\u00b1.0282.7\u00b1.0188.8\u00b1.0179.9\u00b1.065.8\u00b1.0178.8GeoQueryTemplate Length46.5 18.5\u00b1.06 \u00b1.0355.9 16.2\u00b1.07 \u00b1.0256.7 22.0\u00b1.04 \u00b1.0191.0 41.1\u00b1.0 \u00b1.0177.1 36.1\u00b1.06 \u00b1.0153.5 20.7\u00b1.06 \u00b1.0263.5 25.8TMCD35.8\u00b1.0237.1\u00b1.0237.9\u00b1.0154.1\u00b1.048.2\u00b1.036.9\u00b1.041.7TMCD-Rcvcv35.9\u00b1.0136.7\u00b1.0137.5\u00b1.043.3\u00b1.040.8\u00b1.0134.3\u00b1.038.1TMCD-Rstr35.5\u00b1.0137.7\u00b1.0137.6\u00b1.043.1\u00b1.041.4\u00b1.035.3\u00b1.0138.4"}, {"figure_label": "15", "figure_type": "table", "figure_id": "tab_22", "figure_caption": "Model exact-match accuracy on datasets averaged across random seeds, with standard deviation.", "figure_data": ""}, {"figure_label": "18", "figure_type": "table", "figure_id": "tab_23", "figure_caption": "Examples of instance where the model makes both mistakes in random split and generalization split. The first instance is the output of BART on standard split of GeoQuery, and the second entry is BART making a similar mistake on template split of GeoQuery; the second instance is output of BTG on simple split of SCAN, and a similar instance making the same directional mistake on the TurnLeft split.", "figure_data": "MotivationPracticalCognitiveIntrinsicFairness\u25a1 \u25b3 \u20dd \u2299Generalisation typeCompositionalStructuralCross Task Cross Language Cross DomainRobustness\u25a1 \u25b3 \u20dd \u2299Shift typeCovariateLabelFullAssumed\u25a1 \u25b3 \u20dd \u2299Shift sourceNaturally occuringPartitioned naturalGenerated shiftFully generated\u25a1 \u25b3\u20dd \u2299Shift locusTrain-testFinetune train-testPretrain-trainPretrain-test\u25a1 \u20dd\u25b3 \u2299"}, {"figure_label": "19", "figure_type": "table", "figure_id": "tab_24", "figure_caption": "A GenBench evaluation card(Hupkes et al., 2023)  that summarizes our experiments. \u25a1= Experiments of LSTM and Transformer on GeoQuery and Spider; \u25b3= Experiments of T5 and BART on GeoQuery and Spider; \u20dd= Experiments of LSTM and Transformer on COGS and SCAN; \u2299= Experiments of T5 and BART on COGS and SCAN.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "CONCUR(D 1 , D 2 ; A, Eval) = CORR(P 1 , P 2 ),", "formula_coordinates": [4.0, 311.01, 576.42, 208.54, 10.63]}], "doi": "10.18653/v1/W18-5407"}