{"title": "A Tensor-Based Algorithm for High-Order Graph Matching", "authors": "Olivier Duchenne; Francis Bach; Inso Kweon; Jean Ponce", "pub_date": "", "abstract": "This paper addresses the problem of establishing correspondences between two sets of visual features using higher-order constraints instead of the unary or pairwise ones used in classical methods. Concretely, the corresponding hypergraph matching problem is formulated as the maximization of a multilinear objective function over all permutations of the features. This function is defined by a tensor representing the affinity between feature tuples. It is maximized using a generalization of spectral techniques where a relaxed problem is first solved by a multi-dimensional power method, and the solution is then projected onto the closest assignment matrix. The proposed approach has been implemented, and it is compared to state-of-the-art algorithms on both synthetic and real data.", "sections": [{"heading": "Introduction", "text": "Establishing correspondences between two sets of visual features is a key problem in computer vision tasks as diverse as feature tracking [5], image classification [15] or retrieval [23], object detection [4], shape matching [28,16], or wide-baseline stereo fusion [21]. Different image cues may lead to very different matching strategies. At one end of the spectrum, geometric matching techniques such as RANSAC [8], interpretation trees [11], or alignment [12] can be used to efficiently explore consistent correspondence hypotheses when the mapping between image features is assumed to have some parametric form (e.g., a planar affine transformation), or obey some parametric constraints (e.g., epipolar ones). At the other end of the spectrum, visual appearance alone can be used to find matching features when such an assumption does not hold: For example, bagsof-features methods that discard all spatial information to build some invariance to intra-class variations and viewpoint changes have been applied quite successfully in image classification tasks [26,27]. Modern methods for image matching now tend to mix both geometric and appearance cues to guide the search for correspondences (see, for example, [15,18]).\nMany matching algorithms proposed in the 80s and 90s have an iterative form but are not explicitly aimed as optimizing a well-defined objective function (this is the case for RANSAC and alignment methods for example). The situation has changed in the past few years, with the advent of combinatorial or mixed continuous/combinatorial optimization approaches to feature matching (see, for example [4,19,20,28,16]). 1 This paper builds on this work in a framework that can accommodate both (mostly local) geometric invariants and image descriptors. Concretely, the search for correspondences is cast as a hypergraph matching problem using higher-order constraints instead of the unary or pairwise ones used by previous methods: Firstorder methods based (for example) on local image descriptions are susceptible to image ambiguities due to repeated patterns, textures or non-discriminative local appearance for example. Geometric consistency is normally enforced using pairwise relationships between image features. In contrast, we propose in this paper to use higher-order (mostly thirdorder) constraints to enforce feature matching consistency (see Figure 1) This work generalizes the spectral matching method of [16] to higher-order potentials: The corresponding hypergraph matching problem is formulated as the maximization of a multilinear objective function over all permutations of the features. This function is defined by a tensor representing the affinity between feature tuples. It is maximized by first using a multi-dimensional power method to solve a relaxed version of the problem, whose solution is then projected onto the closest assignment matrix.\nThe three main contributions are (1) the application of the tensor power iteration for the high-order matching task, used with a tighter relaxation based on constraints on the column norms of assignment matrices, (2) the design of appropriate similarity measures which can be chosen ei-Figure 1. Left: second-order potentials can be rotation-invariant by comparing distances between matched points. Right: Thirdorder potentials can be similarity-invariant by comparing angles of triangles.\nther to improve invariance of the matching, or to improve the expressivity of the model (see Section 5), and (3) an 1 -norm relaxation instead of the classical 2 -norm relaxation, that allows solutions which are more discriminative but still allows efficient power iteration solutions (see Section 4). The proposed approach has been implemented, and it is compared to state-of-the-art algorithms on both synthetic and real data. As shown by our experiments (Section 7), our implementation is, overall, as fast as these methods in spite of the higher complexity of the underlying model, with better accuracy on standard databases. The source code of our software is available online at http: //www.di.ens.fr/\u02dcduchenne.", "publication_ref": ["b4", "b14", "b22", "b3", "b27", "b15", "b20", "b7", "b10", "b11", "b25", "b26", "b14", "b17", "b3", "b18", "b19", "b27", "b15", "b0", "b15", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Graph matching for computer vision 2.1. Previous work", "text": "As noted earlier, finding correspondences between visual features (such as interest points, edges, or even raw pixels) is a key problem in many computer vision tasks. The simplest approach to this problem is to define some measure of similarity between two features (e.g., the Euclidean distance between Sift descriptors of small image patches [18]), and match each feature in the first image to its nearest neighbor in the second one. This naive approach will fail in the presence of ambiguities such as repeated patterns, textures or non-discriminative local appearance. To handle this difficulty, some papers try to enforce some geometric consistency between pairs of feature correspondences. The basic idea is that if the points p 1 and p 1 of image 1 are matched to points p 2 and p 2 of image 2, then the geometric relation between p 1 and p 1 , and the ones between p 2 and p 2 should be similar.\nSeveral pairwise geometric relations have been used. Leordanu and Hebert [16] use only the distance between two points, leading to a potential which is invariant to rotation. Berg et al. [4] use a combination of potential based on distance and based on angle (scale invariant), to find a trade-off between rotation and scale invariance. Some other methods (e.g., [23,28]) use neighborhoods, by only assuming that neighbor points should be matched to neigh-bor points. One difficulty here is to define an appropriate notion of neighborhood.\nRecently, the computer vision community has put much effort in increasing the order of complexity of the models used: For example, Kohli et al. [13] introduce a highorder clique potential for segmentation, but the type of energy is limited to specific types of functions, and the alpha-expansion framework used there leads to a local optimum. Moreover, Zass and Sashua [25] formulate the search for higher-order feature correspondences as a hypergraph matching problem. We will use the same formulation but a different optimization setup. In addition, unlike these authors, we will refrain from using independence assumptions (that may or may not be justified depending on the situation) to factor our model into first-order interactions. As will be shown in the comparative experiments of Section 7, explicitly maintaining higher-order interactions in the optimization process will lead to superior performance.", "publication_ref": ["b17", "b15", "b3", "b22", "b27", "b12", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Mathematical Formulation", "text": "We consider two images, and assume that we have extracted N 1 features from image 1, and N 2 features from image 2. We do not assume that N 1 = N 2 , i.e., there may be different numbers of points in the two images to be matched. Throughout this paper, for s = 1, 2, all indices i s , j s , k s will be assumed to vary from 1 to N s . We will also note i = (i 1 , i 2 ), j = (j 1 , j 2 ), k = (k 1 , k 2 ) pairs of potentially matched points.\nLet P s i be the i th point of image s. The problem of matching points from image 1 to points from image 2 is equivalent to looking for an N 1 \u00d7 N 2 assignment matrix X such that X i1,i2 is equal to 1 when P 1 i1 is matched to P 2 i2 , and to 0 otherwise. In this paper, we assume that a point in the first image is matched to exactly one point in the second image, but that two points in the second image may be matched to an arbitrary number of points in the first image, i.e., we assume that the sums of each column is equal to one, but put no with constraints on the row sums. (this framework can easily be extended to allow matching points from the first image to no points in the second image using dummy nodes such as in [4]). Thus, we consider the set A of assignment matrices:\nA = {X \u2208 {0, 1} N1\u00d7N2 , i1 X i1,i2 = 1}.\nNote that our definition is not symmetric (i.e., if we switch the two images, we obtain different matchings). It can simply be made symmetric by considering the two possible matchings and combining them in a (mostly applicationdependent way), e.g., by taking the union or intersection of matchings.\nIn [4,7,16], the matching problem is formulated as the maximization of the following score on A:\nscore(X) = i1,i2,j1,j2 H i1,i2,j1,j2 X i1,i2 X j1,j2 ,\nwhere H i1,i2,j1,j2 (which is equal to H i,j with our notations for pairs) is a binary potential corresponding to the pairs of points (P i1 , P j1 ) of image 1, and (P i2 , P j2 ) of image 2.\nHigh values of H correspond to similar pairs. This graph matching problem is actually an integer quadratic programming problem, with no known polynomial-time algorithm. Approximate methods may be divided into two groups of algorithms. The first group is composed of methods which use spectral representations of adjacency matrices (see, e.g., [24,16]). The second group is composed of algorithms which work directly with graph adjacency matrices, and typically involve a relaxation of the complex discrete optimization problem (see, e.g., [2]). In this paper, we focus on improvements on spectral methods.\nIn [7,16], the set of matrices over which the optimization is performed is thus relaxed to the set of matrices with Frobenius norm X F equal to N 1/2 2 , leading to the simpler problem:\nmax X F =N 1/2 2 i1,i2,j1,j2 H i1,i2,j1,j2 X i1,i2 X j1,j2 .(1)\nIn turn, this can be rewritten as max X F =N 1/2 2 X T HX where, abusing the notation, X is this time considered as a N 1 N 2 vector, and H as an N 1 N 2 by N 1 N 2 symmetric matrix. This is a classical Rayleigh quotient problem, whose solution is N 1/2 2 times the eigenvector X * associated with the largest eigenvalue (which we refer to as the main eigenvalue) of the matrix X [9], and can be computed efficiently by the power iteration method described in the next section.\nAn important constraint that is put on H is that it is pointwise non-negative. In this situation, the Perron-Frobenius theorem [9] ensures that X * only has non-negative coefficients, which simplifies the interpretation of the result: In order to obtain an assignment matrix in A, i.e., a matrix with elements in {0, 1} and proper column sums, the matching is discretized using a greedy algorithm.", "publication_ref": ["b3", "b3", "b6", "b15", "b23", "b15", "b1", "b6", "b15", "b8", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Power iteration for eigenvalue problem", "text": "The power iteration method is a very simple algorithm for computing the main eigenvector of a matrix, which is needed for matching. This algorithm converges geometrically to the largest eigenvalue of the input matrix [9]. In our situation, H is very sparse and we want to take advantage of it. Each step of the power iteration algorithm requires only O(m) operations, where m is the number of non-zero elements of H. Typically, in our situations, the algorithm converges in around 20 steps.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Input: matrix H", "text": "Output: V main eigenvector of H initialize V randomly ; 1 repeat 2 V \u2190 HV ; 3 V \u2190 V / V 2 ;", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Tensor formulation", "text": "We propose to use tensors to solve the high-order feature matching problem. Indeed, using tensors is quite natural to generalize the idea of spectral matching [16] which deals with a matrix. Previous works only use point-to-point and pair-to-pair comparisons for their matching. In this paper, we want to compare tuples of points. So, we now want to add higher-order terms to the score function defined in Eq. (1). For simplicity, we will focus from now on thirdorder interactions. Generalizations to higher-order potentials is (in theory at least) straightforward.\nWe define a new high-order score:\nscore(X) = i1,i2,j1,j2,k1,k2 H i1,i2,j1,j2,k1,k2 X i1,i2 X j1,j2 X k1,k2 , (2)\nwhere we assume that we have a super-symmetric tensor, i.e., invariant by permutation of indices in\n{i 1 , j 1 , k 1 } or {i 2 , j 2 , k 2 }.\nHere, the product X i1,i2 X j1,j2 X k1,k2 will be equal to 1 if the points {i 1 , j 1 , k 1 } are all matched to the points {i 2 , j 2 , k 2 }, and 0 otherwise. In the first case, it will add H i1,i2,j1,j2,k1,k2 to the total score function. This is a similarity measure, which will be high if the sets of features\n{i 1 , j 1 , k 1 } is similar to the set {i 2 , j 2 , k 2 }.\nNote that we can rewrite the score compactly using tensor notation as (see [22] for more details): score(X) = H \u2297 1 X \u2297 2 X \u2297 3 X. In section 5, we will explain how the higher-orders potentials can be used to have more invariant or more expressive features.", "publication_ref": ["b15", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Tensor power iteration", "text": "To find the optimum of the new high-order score of Eq. (2), we want to use a generalization of the previously mentioned power iteration, as proposed in [14], for the equivalent problem of computing the rank-1 approximations of the tensor H. Their algorithm presented below extends Algorithm 1.\nThis method does not reach a global optimum. However, it converges to a local maximum for tensors which leads to convex functions of X [22] In our experiments, it converges almost always to a very satisfactory solution. Also, the authors of [22] propose a smart way to initialize it, to lead to a quantifiable proximity to the optimal solution.\nInput: supersymmetric tensor H Output: V main eigenvector of H initialize V randomly ; We can also see that as in the matrix case, the resulting vector will have only positive values. That is required to have a meaningful result. Indeed, negative values of X in the score in Eq. (2) would prevent us from interpreting H i1,i2,j1,j2,k1,k2 as a similarity potential activated only when all corresponding pairs are matched.\n1 repeat 2 V \u2190 H \u2297 1 V \u2297 2 V ; 3 ( i.e. \u2200i, v i \u2190 i,j,k h i,j,k v j v k ) 4 V \u2190 V / V 2 ;", "publication_ref": ["b13", "b21", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Tensor power iteration for unit norm columns", "text": "In our context of only constraining the sums of the columns, we can design a tighter relaxation of A than matrices of fixed Frobenius norm, namely as the set C 2 of matrices whose Euclidean norms of each of the N 2 columns are equal to one.\nWe can extend the previous algorithm to the following:\nInput: supersymmetric tensor H Output: V = [v 1 , . . . , v N2 ] stationary point initialize V randomly ; 1 repeat 2 V \u2190 H \u2297 1 V \u2297 2 V ; 3 ( i.e. \u2200i, v i \u2190 i,j,k h i,j,k v j v k ) 4 \u2200i 2 , V (:, i 2 ) \u2190 V (:, i 2 )/ V (:, i 2 ) 2 ;\n5 until convergence ;\n6\nAlgorithm 3: Supersymmetric tensor power iteration (third order) with unit norm constraints.\nWe extended the proof of [22], to our new constraints. We can thus prove that this algorithm has the same nice properties of the previous one: if the score is a convex function of X, then Algorithm 3 converges to a stationary point (see proof in Appendix). Note that we can always make the score convex by adding a multiple of the function X X, which does not change the value on our set of constant unit column matrices and thus does not change the optima of the score on C 2 .\nFinally, we have a natural projection step here to the set A by considering, for each column, the index that is maximum in the solution obtained from the previous algorithm.", "publication_ref": ["b21"], "figure_ref": [], "table_ref": []}, {"heading": "Merging potentials of different orders", "text": "It could be interesting to include in the matching process, in the same time, information about different orders (e.g. considering in the same time pair similarities, and triplet similarities). To do this a first solution is to include the loworder information into the tensor of the highest-order potential. Cour and Shi [7] presented a method to do this in the second order case. The generalization is straightforward for our setting. However, in our power iteration framework, it is equivalent to use the simple following algorithm (which could also be extended to constrain columns to have unit norms):\nInput: several supersymmetric tensors H t of order t Output: V main eigenvector of H initialize V randomly ; \n1 repeat 2 V \u2190 H 4 \u2297 1 V \u2297 2 V \u2297 3 V + 3 H 3 \u2297 1 V \u2297 2 V + H 2 \u2297 1 V + H 1 ; 4 ( i.e. \u2200i, v i \u2190 i,j,k,l h 4 i,j,k,l v j v k v l + 5 i,j,k h 3 i,j,k v j v k + i,j,k h 2 i,j v j + h 1 i ) V \u2190 V / V 2 ;", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "1 -norms vs. 2 -norms of columns", "text": "One of the main problems of spectral relaxations is that the solution is often uniform, which means that it is hard to extract from it an assignment matrix with values in {0, 1}. We claim that this is due to the relaxation of the set A of assignment matrices to matrices in C 2 with all columns with unit 2 -norm. In fact, we can also relax the set A to the matrices in C 1 with all columns with unit 1 -norm (i.e., sum of absolute values), and as we show in Figure 2, it leads to results that are more easily interpretable.\nIn the context of second-order interactions, solving the 1 -norm problem cannot be done by power iterations. However, in our higher-order context, this can seamlessly be done. Indeed, solving with the change of variable, \u2200i, Y 2 i = X i . The order of this new problem is 4, but using the tensor power iteration algorithm, the complexity is still as low as the first problem. Using this algorithm we obtain in practice an almost completely binary solution, as shown in Figure 2. This method is easily extended to solve any high-order matching problem.\nmax X\u2208C1,X 0 i,j H i,j X i X j is equivalent to solving max Y \u2208C2 i,j H i,j Y 2 i Y 2", "publication_ref": [], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Building tensors for computer vision", "text": "We can use higher-order potentials to increase either the geometric invariance of image features, or the expressivity of the models. We describe here a few possible potentials. They are all based on computing a Gaussian kernel between appropriate invariant features. Clearly, many other potentials are possible.\nIn this section we will only consider third-order potentials. As shown in Figure 1, classical methods try to remove ambiguities by looking for matches that preserve some properties of point pairs. Here, we will try to conserve properties of point triplets. In particular, in most of the cases, we will use the properties of the triangle formed by three points. Basically, if the points (P 1 1 , P 1 2 , P 1 3 ) are matched to the points (P 2 1 , P 2 2 , P 2 3 ), the corresponding triangles should be similar. In [16] rotation and translation-invariant potentials based on edge lengths and angles are used since it is impossible to build invariants to larger classes of transformations from feature pairs alone. Here, we propose using potentials based on triplets of points, which can be made invariant to richer classes of transformations, including (planar) similarities, affine transformations, and projective ones.", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "Similarity-invariant potentials", "text": "The basic idea is that under a similarity transformation the angles of a triangle are unchanged. In practice, we describe each triangle by the sines of its three angles (see figure 1).\nAffine-invariant potentials As the weighted mean is conserved by affine transforms, we can design a descriptor of the image inside a triangle. We normalize the triangle as an equilateral one, and then compare the intensity patterns of normalized triangles by normalized cross correlation.\nProjective-invariant potentials Inspired by [17], we can also develop higher-order potentials invariant to projective transforms. If we sample only feature points on lines, we can use the edge direction as an additional feature, and focus on properties of three points and three directions that are conserved under projective transforms. The main property conserved by a projective transform is the cross ratio. So if we suppose that the triangle we are looking at is flat, we can geometrically build three lines with four points on each (see figure 3). We will use the three cross-ratios defined by those points to make a perspective-invariant potential.", "publication_ref": ["b16"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Implementation", "text": "In the case of d-th order potentials, the brute force algorithm has a complexity O(n 2d ), where n = max{N 1 , N 2 }. Previous algorithms use 50 -100 points and require approximately O(n 3 ) operations, for second order potentials. We developed an efficient algorithm which has in the general case a complexity of approximately O(n 3 + n d log(n))\nFor clarity reasons, we will explain the algorithm in the case d = 3, but it is straightforward to generalize it. First, it is very time consuming and not very meaningful to use all the triangles, so as in [25] we only sample t triangles per points in image 1. We take t = 20 in our experiments. We believe that this number of triangles is more than enough to obtain a robust matching. Then, we sample all the possible triangles of image 2, and compute their descriptors. We use a kd-tree to store them efficiently. For each of the selected triangles of image 1, we find the k (500 in our implementation) nearest neighbors of image 2. Then we start the power iteration with h i1,j1,k1,i2,j2,k2 = exp(\u2212\u03b3 triangle(i 1 , j 1 , k 1 ) \u2212 triangle(i 2 , j 2 , k 2 ) 2 ) if triangle(i 1 , j 1 , k 1 ) is the set of features computed from Section 5 from image 1 and triangle(i 2 , j 2 , k 2 ) for one of its nearest neighbors in image 2. We take \u03b3 = 2 in our experiments.\nThe total complexity of the algorithm is O(n d log(n) + ntk log(n)). The final algorithm typically last one second for 80 points.\nSmart selections of triangles There exist several strategies to select triangles depending of your final goal. If one wants to match and allow deformations, the triangle should be selected at small scales. On the other hand, if one wants to capture the global property of a shape, he should select big triangles.", "publication_ref": ["b24"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Artificial data", "text": "Following [25,16], we first used artificial data in order to easily compare quantitatively our algorithm to the stateof-the-art. We sampled randomly and uniformly points on the 2D plane. We created a second set of points by perturbing the first one. Then, we compared different algorithms trying to match those two sets. The accuracy of the algorithm is computed as the proportion of good matches. In all the experiments presented here, we only use the simple similarity-invariant potential presented in section 5.\nIn order to have a fair comparison between our method and the probabilistic hypergraph matching [25], we first compute the tensor as described earlier. Then, we marginalize it as explained in [25]. Then we use the resulting vector with the algorithm provided online. We also compare to spectral matching [16] to show the improvement of using higher-order potentials.\nFirst, we added a Gaussian noise to the position of the second set, rotate them, and add outliers. The results are shown in Figure 4 (top). We can see that our method outperforms the others. Our interpretation is that when many outliers are added, the ambiguities of pairwise methods [16] increase, because many pairs become similar, while triplets are less likely to become similar. Moreover, probabilistic hypergraph matching [25] reduces the high-order problem  to a first-order one, so that it is likely to match points which have the same neighborhoods. Such a method thus become ambiguous when there are many outliers.\nSecond, we added Gaussian noise, rotation, and rescaling. Indeed, the low-order matching methods, such as the spectral method, cannot handle those transformations. In Figure 4 (bottom), we can see that our method and the one of [25] are indifferent to those transformations, but performance of [16] drops to 50% after a scaling of only 1.1 or 0.9, and quickly reach the chance level at 1.2 or 0.8.", "publication_ref": ["b24", "b15", "b24", "b24", "b15", "b15", "b24", "b24", "b15"], "figure_ref": ["fig_6", "fig_6"], "table_ref": []}, {"heading": "House Dataset", "text": "The House dataset is a commonly used dataset to test performance of matching algorithm. Some objects are taken from different viewpoints and some keypoints which are present on every frame are labeled. The scale is always roughly the same, but the transformation is now projective. As the ground truth is provided, it is also easy to compute the accuracy of the algorithm. In Figure 5, we can see that the low-order algorithm cannot handle the fact that in perspective transforms, some points move more than some others. ", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "Natural images", "text": "We took images from the Caltech-256 image database [10] which are objects on a clear background. We extracted from those the silhouettes and subsample them. We can then match images from the same class using our algorithm; results are presented in Figure 6. Our tensor-based algorithm is able to match objects with slightly different visual appearances and in the presence of strong deformations.", "publication_ref": ["b9"], "figure_ref": ["fig_9"], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we have proposed a tensor-based algorithm for high-order graph matching. We have reached state-ofthe-art performance by using simple potentials which are invariant to rigid, affine or projective transformations. This work can extended in a number of ways, by considering more complex features, either on triplets, or potentially quadruplet or more to be fully invariant to richer classes of transforms. Moreover, it is natural to follow the approach of [6] to learn potentials automatically from labelled or partially labelled data.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "A. Power iterations for unit norm columns", "text": "In this appendix, we consider the problem of maximizing a convex function on V on a product of spheres in dimension N 1 , i.e., on the set C 2 . The following proposition extends the result of [22] from spheres to products of spheres. We consider the general algorithm:  Proposition A.1 If the function f is differentiable on R N1\u00d7N2 and strictly convex, Algorithm 5 is an ascent method and converges to a stationary point of f on C 2 .\nProof In this proof, we refer to the i 2 -th column of any matrix v as v i2 . Given v 0 in C 2 , one iteration of Algorithm 5 applied to v 0 leads to the matrix v 1 with i 2 -th column equal to v 1 i2 = \u2207f (v 0 ) i2 / \u2207f (v 0 ) i2 2 . Since f is strictly convex, for all w in R N1\u00d7N2 , f (w) f (v 0 ) + i2 \u2207f (v 0 ) i2 (w i2 \u2212 v 0 i2 ), with equality if and only if w = v 0 . We thus have:\nf (v 1 ) f (v 0 ) + i2 \u2207f (v 0 ) i2 (v 1 i2 \u2212 v 0 i2 ) f (v 0 ) + i2 \u2207f (v 0 ) i2 (v 0 i2 \u2212 v 0 i2 ) = f (v 0 ), because for each i 1 , \u2207f (v 0 ) i2 v 1 i2 = \u2207f (v 0 ) i2 2 \u2207f (v 0 ) i2 v 0\ni2 . We have equalities above if and only if v 1 = v 0 and, for each i 2 , \u2207f (v 0 ) i2 is equal to a positive constant times v 0 i2 . This shows that each iteration is increasing the cost function. Since f is continuous and C 2", "publication_ref": ["b21"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "is compact, if we denote by v t the sequence of iterates, the sequence f (v t ) is non-decreasing and bounded, hence convergent. Since having f (v 0 ) = f (v 1 ) implies v 1 = v 0 , the sequence v t is also converging, and its limit v \u221e is such that for each i 2 , \u2207f (v \u221e ) i2 is equal to a positive constant times v \u221e i2 , i.e., v \u221e is a stationary point of f on the product of spheres C 2 [1].", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "B. Acknowledgments", "text": "This paper was supported in part by a grant from the Agence Nationale de la Recherche (MGA Project), by the Defense Acquisition Program Administration and Agency for Defense Development, Korea, through the Image Information Research Center at KAIST under the contract UD070007AD and by the IT R&D program of MCST.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Optimization Algorithms on Matrix Manifolds", "journal": "Princeton Univ. Press", "year": "2008", "authors": "P.-A Absil; R Mahony; R Sepulchre"}, {"ref_id": "b1", "title": "A linear programming approach for the weighted graph matching problem", "journal": "IEEE PAMI", "year": "1993", "authors": "H A Almohamad; S O Duffuaa"}, {"ref_id": "b2", "title": "Computer Vision", "journal": "Prentice-Hall", "year": "1982", "authors": "D H Ballard; C M Brown"}, {"ref_id": "b3", "title": "Shape Matching and Object Recognition Using Low Distortion Correspondences", "journal": "", "year": "2005", "authors": "A C Berg; T L Berg; J Malik"}, {"ref_id": "b4", "title": "KLT: An implementation of the Kanade-Lucas-Tomasi feature tracker", "journal": "", "year": "1998", "authors": "S Birchfield"}, {"ref_id": "b5", "title": "Learning graph matching", "journal": "", "year": "2007", "authors": "T Caetano; L Cheng; Q V Le; A J Smola"}, {"ref_id": "b6", "title": "Balanced graph matching", "journal": "", "year": "2004", "authors": "T Cour; P Srinivasan; J Shi"}, {"ref_id": "b7", "title": "Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography", "journal": "Comm. of the ACM", "year": "1981", "authors": "M Fischler; R Bolles"}, {"ref_id": "b8", "title": "Matrix computations", "journal": "Johns Hopkins University Press", "year": "1996", "authors": "G H Golub; C F Van Loan"}, {"ref_id": "b9", "title": "Caltech-256 object category dataset", "journal": "", "year": "2007", "authors": "G Griffin; A Holub; P Perona"}, {"ref_id": "b10", "title": "Localizing overlapping parts by searching the interpretation tree", "journal": "PAMI", "year": "1987", "authors": "W E L Grimson; T Lozano-P\u00e9rez"}, {"ref_id": "b11", "title": "Object recognition using alignment", "journal": "", "year": "1987", "authors": "D P Huttenlocher; S Ullman"}, {"ref_id": "b12", "title": "Solving energies with higher order cliques. CVPR", "journal": "", "year": "2007", "authors": "P Kohli; P Mudigonda; P Torr"}, {"ref_id": "b13", "title": "On the best rank-1 and rank-(r1,r2,. . .,rn) approximation of higherorder tensors", "journal": "SIAM J. Matrix Anal. Appl", "year": "2000", "authors": "L De Lathauwer; B De Moor; J Vandewalle"}, {"ref_id": "b14", "title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "journal": "", "year": "2006", "authors": "S Lazebnik; C Schmid; J Ponce"}, {"ref_id": "b15", "title": "A spectral technique for correspondence problems using pairwise constraints", "journal": "", "year": "2005", "authors": "M Leordeanu; M Hebert"}, {"ref_id": "b16", "title": "Beyond local appearance: Category recognition from pairwise interactions of simple features", "journal": "", "year": "2007", "authors": "M Leordeanu; M Hebert; R Sukthankar"}, {"ref_id": "b17", "title": "Distinctive image features from scale-invariant keypoints", "journal": "IJCV", "year": "2004", "authors": "D Lowe"}, {"ref_id": "b18", "title": "A global solution to sparse correspondence problems", "journal": "PAMI", "year": "2003", "authors": "J Maciel; J Costeira"}, {"ref_id": "b19", "title": "Optimal multi-frame correspondence with assignment tensors", "journal": "", "year": "2006", "authors": "R Oliveira; R Ferreira; J Costeira"}, {"ref_id": "b20", "title": "Wide baseline stereo matching", "journal": "", "year": "1998", "authors": "P Pritchett; A Zisserman"}, {"ref_id": "b21", "title": "The higher-order power method revisited: convergence proofs and effective initialization", "journal": "ICASSP", "year": "2000", "authors": "P A Regalia; E Kofidis"}, {"ref_id": "b22", "title": "Local grayvalue invariants for image retrieval", "journal": "PAMI", "year": "1997", "authors": "C Schmid; R Mohr"}, {"ref_id": "b23", "title": "An eigendecomposition approach to weighted graph matching problems", "journal": "PAMI", "year": "1988", "authors": "S Umeyama"}, {"ref_id": "b24", "title": "Probabilistic graph and hypergraph matching", "journal": "CVPR", "year": "2006", "authors": "Ron Zass; Amnon Shashua"}, {"ref_id": "b25", "title": "SVM-KNN: Discriminative nearest neighbor classification for visual category recognition", "journal": "", "year": "2006", "authors": "H Zhang; A C Berg; M Maire; J Malik"}, {"ref_id": "b26", "title": "Local features and kernels for classifcation of texture and object categories: An in-depth study", "journal": "Int. J. of Comp. Vision", "year": "2007", "authors": "J Zhang; M Marszalek; S Lazebnik; C Schmid"}, {"ref_id": "b27", "title": "Robust point matching for nonrigid shapes by preserving local neighborhood structures", "journal": "PAMI", "year": "2006", "authors": "Y Zheng; D Doermann"}], "figures": [{"figure_label": "51", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "4 until convergence ; 5 Algorithm 1 :51Power iteration for eigenvalue problem.", "figure_data": ""}, {"figure_label": "62", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "5 until convergence ; 6 Algorithm 2 :62Supersymmetric tensor power iteration (third order).", "figure_data": ""}, {"figure_label": "74", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "6 until convergence ; 7 Algorithm 4 :74Multiple Order Supersymmetric tensor power iteration (fourth order).", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 .2Figure2. Top: Values of the assignment matrix, when the 2 -norm is used. (hard to project). Bottom: When using the 1 -norm, we obtain directly a very clear assignment matrix with minor adjustments.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 .3Figure 3. Left: Diagram illustrating the features used in the proposed projective-invariant potential. Right: the three sets of four aligned points used to compute the cross ratio.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 4 .4Figure 4. Top: Accuracy as a function of the number of added outliers. Bottom: Accuracy, as a function of the rescaling. (e.g. x = 2, correspond to a scaling of 1.1 2 ).", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 5 .5Figure 5. House data.", "figure_data": ""}, {"figure_label": "12355", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Input: 1 repeat 2 V 3 V 5 Algorithm 5 :12355Convex function fOutput: V = [v 1 , . . . , v N2 ] stationary point of f in C 2 . initialize V randomly ; \u2190 \u2207f (V ) ; \u2190 [v 1 / v 1 2 , . . . , v N2 / v N2 2 ] ;4 until convergence ; Power iteration for maximizing a convex function f in X \u2208 C2 .", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 6 .6Figure 6. Matching silhouettes from the Caltech-256 database.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "A = {X \u2208 {0, 1} N1\u00d7N2 , i1 X i1,i2 = 1}.", "formula_coordinates": [2.0, 339.11, 603.48, 175.75, 13.22]}, {"formula_id": "formula_1", "formula_text": "score(X) = i1,i2,j1,j2 H i1,i2,j1,j2 X i1,i2 X j1,j2 ,", "formula_coordinates": [3.0, 75.87, 111.3, 184.73, 19.91]}, {"formula_id": "formula_2", "formula_text": "max X F =N 1/2 2 i1,i2,j1,j2 H i1,i2,j1,j2 X i1,i2 X j1,j2 .(1)", "formula_coordinates": [3.0, 83.3, 403.02, 203.06, 19.91]}, {"formula_id": "formula_3", "formula_text": "Output: V main eigenvector of H initialize V randomly ; 1 repeat 2 V \u2190 HV ; 3 V \u2190 V / V 2 ;", "formula_coordinates": [3.0, 312.35, 103.37, 143.83, 57.54]}, {"formula_id": "formula_4", "formula_text": "score(X) = i1,i2,j1,j2,k1,k2 H i1,i2,j1,j2,k1,k2 X i1,i2 X j1,j2 X k1,k2 , (2)", "formula_coordinates": [3.0, 305.54, 454.32, 247.41, 20.14]}, {"formula_id": "formula_5", "formula_text": "{i 1 , j 1 , k 1 } or {i 2 , j 2 , k 2 }.", "formula_coordinates": [3.0, 308.86, 496.66, 236.25, 21.61]}, {"formula_id": "formula_6", "formula_text": "{i 1 , j 1 , k 1 } is similar to the set {i 2 , j 2 , k 2 }.", "formula_coordinates": [3.0, 308.86, 580.34, 172.64, 9.65]}, {"formula_id": "formula_7", "formula_text": "1 repeat 2 V \u2190 H \u2297 1 V \u2297 2 V ; 3 ( i.e. \u2200i, v i \u2190 i,j,k h i,j,k v j v k ) 4 V \u2190 V / V 2 ;", "formula_coordinates": [4.0, 53.6, 214.85, 158.34, 57.23]}, {"formula_id": "formula_8", "formula_text": "Input: supersymmetric tensor H Output: V = [v 1 , . . . , v N2 ] stationary point initialize V randomly ; 1 repeat 2 V \u2190 H \u2297 1 V \u2297 2 V ; 3 ( i.e. \u2200i, v i \u2190 i,j,k h i,j,k v j v k ) 4 \u2200i 2 , V (:, i 2 ) \u2190 V (:, i 2 )/ V (:, i 2 ) 2 ;", "formula_coordinates": [4.0, 53.6, 501.72, 183.89, 83.21]}, {"formula_id": "formula_9", "formula_text": "1 repeat 2 V \u2190 H 4 \u2297 1 V \u2297 2 V \u2297 3 V + 3 H 3 \u2297 1 V \u2297 2 V + H 2 \u2297 1 V + H 1 ; 4 ( i.e. \u2200i, v i \u2190 i,j,k,l h 4 i,j,k,l v j v k v l + 5 i,j,k h 3 i,j,k v j v k + i,j,k h 2 i,j v j + h 1 i ) V \u2190 V / V 2 ;", "formula_coordinates": [4.0, 312.35, 326.16, 214.91, 82.64]}, {"formula_id": "formula_10", "formula_text": "max X\u2208C1,X 0 i,j H i,j X i X j is equivalent to solving max Y \u2208C2 i,j H i,j Y 2 i Y 2", "formula_coordinates": [4.0, 308.86, 653.1, 169.83, 62.24]}, {"formula_id": "formula_11", "formula_text": "f (v 1 ) f (v 0 ) + i2 \u2207f (v 0 ) i2 (v 1 i2 \u2212 v 0 i2 ) f (v 0 ) + i2 \u2207f (v 0 ) i2 (v 0 i2 \u2212 v 0 i2 ) = f (v 0 ), because for each i 1 , \u2207f (v 0 ) i2 v 1 i2 = \u2207f (v 0 ) i2 2 \u2207f (v 0 ) i2 v 0", "formula_coordinates": [7.0, 308.86, 614.59, 232.23, 64.49]}], "doi": ""}