{"title": "Kernel Choice and Classifiability for RKHS Embeddings of Probability Distributions", "authors": "Bharath K Sriperumbudur; Kenji Fukumizu; Arthur Gretton; Gert R G Lanckriet; Bernhard Sch\u00f6lkopf", "pub_date": "", "abstract": "Embeddings of probability measures into reproducing kernel Hilbert spaces have been proposed as a straightforward and practical means of representing and comparing probabilities. In particular, the distance between embeddings (the maximum mean discrepancy, or MMD) has several key advantages over many classical metrics on distributions, namely easy computability, fast convergence and low bias of finite sample estimates. An important requirement of the embedding RKHS is that it be characteristic: in this case, the MMD between two distributions is zero if and only if the distributions coincide. Three new results on the MMD are introduced in the present study. First, it is established that MMD corresponds to the optimal risk of a kernel classifier, thus forming a natural link between the distance between distributions and their ease of classification. An important consequence is that a kernel must be characteristic to guarantee classifiability between distributions in the RKHS. Second, the class of characteristic kernels is broadened to incorporate all strictly positive definite kernels: these include non-translation invariant kernels and kernels on non-compact domains. Third, a generalization of the MMD is proposed for families of kernels, as the supremum over MMDs on a class of kernels (for instance the Gaussian kernels with different bandwidths). This extension is necessary to obtain a single distance measure if a large selection or class of characteristic kernels is potentially appropriate. This generalization is reasonable, given that it corresponds to the problem of learning the kernel by minimizing the risk of the corresponding kernel classifier. The generalized MMD is shown to have consistent finite sample estimates, and its performance is demonstrated on a homogeneity testing example.", "sections": [{"heading": "Introduction", "text": "Kernel methods are broadly established as a useful way of constructing nonlinear algorithms from linear ones, by embedding points into higher dimensional reproducing kernel Hilbert spaces (RKHSs) [12]. A generalization of this idea is to embed probability distributions into RKHSs, giv-ing us a linear method for dealing with higher order statistics [8,15,17]. More specifically, suppose we are given the set P of all Borel probability measures defined on the topological space M , and the RKHS (H, k) of functions on M with k as its reproducing kernel (r.k.). For P \u2208 P, denote by Pk := M k(., x) dP(x). If k is measurable and bounded, then we may define the embedding of P in H as Pk \u2208 H. The RKHS distance between two such mappings associated with P, Q \u2208 P is called the maximum mean discrepancy (MMD) [8,17], and is written \u03b3 k (P, Q) = Pk \u2212 Qk H .\n(1)\nWe say that k is characteristic [6,17] if the mapping P \u2192 Pk is injective, in which case ( 1) is zero if and only if P = Q, i.e., \u03b3 k is a metric on P. An immediate application of the MMD is to problems of comparing distributions based on finite samples: examples include tests of homogeneity [8], independence [9], and conditional independence [6]. In this application domain, the question of whether k is characteristic is key: without this property, the algorithms can fail through inability to distinguish between particular distributions. Characteristic kernels are important in binary classification: The problem of distinguishing distributions is strongly related to binary classification: indeed, one would expect easily distinguishable distributions to be easily classifiable. 1 The link between these two problems is especially direct in the case of the MMD: in Section 2, we show that \u03b3 k is the negative of the optimal risk (corresponding to a linear loss function) associated with the Parzen window classifier [12,14] (also called kernel classification rule [4,Chapter 10]), where the Parzen window turns out to be k. We also show that \u03b3 k is an upper bound on the margin of a hard-margin support vector machine (SVM). The importance of using characteristic RKHSs is further underlined by this link: if the property does not hold, then there exist distributions that are unclassifiable in the RKHS H. We further strengthen this by showing that characteristic kernels are necessary (and sufficient under certain conditions) to achieve Bayes risk in the kernel-based classification algorithms. Characterization of characteristic kernels: Given the centrality of the characteristic property to both RKHS classification and RKHS distribution testing, we should take particular care in establishing which kernels satisfy this requirement. Early results in this direction include [8], where k is shown to be characteristic on compact M if it is universal in the sense of Steinwart [18,Definition 4]; and [6,7], which address the case of non-compact M , and show that k is characteristic if and only if H + R is dense in the Banach space of p-power (p \u2265 1) integrable functions. The conditions in both these studies can be difficult to check and interpret, however, and the restriction of the first to compact M is limiting. In the case of translation invariant kernels, [17] proved the kernel to be characteristic if and only if the support of the Fourier transform of k is the entire R d , which is a much easier condition to verify. Similar sufficient conditions are obtained by [7] for translation invariant kernels on groups and semi-groups. In Section 3, we expand the class of characteristic kernels to include kernels that may or may not be translation invariant, with the introduction of a novel criterion: strictly positive definite kernels (see Definition 3) on M are characteristic. Choice of characteristic kernels: In expanding the families of allowable characteristic kernels, we have so far neglected the question of which characteristic kernel to choose. A practitioner asking by how much two samples differ does not want to receive a blizzard of answers for every conceivable kernel and bandwidth setting, but a single measure that satisfies some \"reasonable\" notion of distance across the family of kernels considered. Thus, in Section 4, we propose a generalization of the MMD, yielding a new distance measure between P and Q defined as\n\u03b3(P, Q) = sup{\u03b3 k (P, Q) : k \u2208 K} = sup{ Pk \u2212 Qk H : k \u2208 K},(2)\nwhich is the maximal RKHS distance between P and Q over a family, K of positive definite kernels. For example, K can be the family of Gaussian kernels on R d indexed by the bandwidth parameter. This distance measure is very natural in the light of our results on binary classification (in Section 2): most directly, this corresponds to the problem of learning the kernel by minimizing the risk of the associated Parzen-based classifier. As a less direct justification, we also increase the upper bound on the margin allowed for a hard margin SVM between the samples. To apply the generalized MMD in practice, we must ensure its empirical estimator is consistent. In our main result of Section 4, we provide an empirical estimate of \u03b3(P, Q) based on finite samples, and show that many popular kernels like the Gaussian, Laplacian, and the entire Mat\u00e9rn class on R d yield consistent estimates of \u03b3(P, Q). The proof is based on bounding the Rademacher chaos complexity of K, which can be understood as the U-process equivalent of Rademacher complexity [3]. Finally, in Section 5, we provide a simple experimental demonstration that the generalized MMD can be applied in practice to the problem of homogeneity testing. Specifically, we show that when two distributions differ on particular length scales, the kernel selected by the generalized MMD is appropriate to this difference, and the resulting hypothesis test outperforms the heuristic kernel choice employed in earlier studies [8]. The proofs of the results in Sections 2-4 are provided in the appendix.", "publication_ref": ["b11", "b7", "b14", "b16", "b7", "b16", "b5", "b16", "b7", "b8", "b5", "b0", "b11", "b13", "b3", "b7", "b17", "b5", "b6", "b16", "b6", "b2", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Characteristic Kernels and Binary Classification", "text": "One of the most important applications of the maximum mean discrepancy is in nonparametric hypothesis testing [8,9,6], where the characteristic property of k is required to distinguish between probability measures. In the following, we show how MMD naturally appears in binary classification, with reference to the Parzen window classifier and hard-margin SVM. This motivates the need for characteristic k to guarantee that classes arising from different distributions can be classified by kernel-based algorithms.\nTo this end, let us consider the binary classification problem with X being a M -valued random variable, Y being a {\u22121, +1}-valued random variable and the product space, M \u00d7 {\u22121, +1}, being endowed with an induced Borel probability measure \u00b5. A discriminant function, f is a real valued measurable function on M , whose sign is used to make a classification decision. Given a loss function L : {\u22121, +1} \u00d7 R \u2192 R, the goal is to choose an f that minimizes the risk associated with L, with the optimal L-risk being defined as\nR L F = inf f \u2208F M L(y, f (x)) d\u00b5(x, y) = inf f \u2208F \u03b5 M L 1 (f ) dP + (1 \u2212 \u03b5) M L \u22121 (f ) dQ , (3\n)\nwhere F is the set of all measurable functions on M , L 1 (\u03b1\n) := L(1, \u03b1), L \u22121 (\u03b1) := L(\u22121, \u03b1), P(X) := \u00b5(X|Y = +1), Q(X) := \u00b5(X|Y = \u22121), \u03b5 := \u00b5(M, Y = +1).\nHere, P and Q represent the class-conditional distributions and \u03b5 is the prior distribution of class +1. Now, we present the result that relates \u03b3 k to the optimal risk associated with the Parzen window classifier.\nTheorem 1 (\u03b3 k and Parzen classification). Let L 1 (\u03b1) = \u2212 \u03b1 \u03b5 and L \u22121 (\u03b1) = \u03b1 1\u2212\u03b5 . Then, \u03b3 k (P, Q) = \u2212R L F k , where F k = {f : f H \u2264 1} and H is an RKHS with a measurable and bounded k.\nSuppose {(X i , Y i )} N i=1 , X i \u2208 M , Y i \u2208 {\u22121, +1}, \u2200 i is a training sample drawn i.i.d. from \u00b5 and m = |{i : Y i = 1}|. If f \u2208 F k is an empirical minimizer of (3) (where F is replaced by F k in (3)), then sign( f (x)) = 1, 1 m Y i =1 k(x, X i ) > 1 N \u2212m Y i =\u22121 k(x, X i ) \u22121, 1 m Y i =1 k(x, X i ) \u2264 1 N \u2212m Y i =\u22121 k(x, X i ) ,(4)\nwhich is the Parzen window classifier.\nTheorem 1 shows that \u03b3 k is the negative of the optimal L-risk (where L is the linear loss as defined in Theorem 1) associated with the Parzen window classifier. Therefore, if k is not characteristic, which means \u03b3 k (P, Q) = 0 for some P = Q, then R L F k = 0, i.e., the risk is maximum (note that since 0 \u2264 \u03b3 k (P, Q) = \u2212R L F k , the maximum risk is zero). In other words, if k is characteristic, then the maximum risk is obtained only when P = Q. This motivates the importance of characteristic kernels in binary classification. In the following, we provide another result which provides a similar motivation for the importance of characteristic kernels in binary classification, wherein we relate \u03b3 k to the margin of a hard-margin SVM.\nTheorem 2 (\u03b3 k and hard-margin SVM). Suppose {(\nX i , Y i )} N i=1 , X i \u2208 M , Y i \u2208 {\u22121, +1}, \u2200 i is a training sample drawn i.i.d. from \u00b5. Assuming the training sample is separable, let f svm be the solution to the program, inf{ f H : Y i f (X i ) \u2265 1, \u2200 i},\nwhere H is an RKHS with measurable and bounded k. If k is characteristic, then\n1 f svm H \u2264 \u03b3 k (P m , Q n ) 2 , (5\n)\nwhere\nP m := 1 m Y i =1 \u03b4 Xi , Q n := 1 n Y i =\u22121 \u03b4 Xi , m = |{i : Y i = 1}| and n = N \u2212 m. \u03b4 x represents the Dirac measure at x.\nTheorem 2 provides a bound on the margin of hard-margin SVM in terms of MMD. (5) shows that a smaller MMD between P m and Q n enforces a smaller margin (i.e., a less smooth classifier, f svm , where smoothness is measured as f svm H ). We can observe that the bound in (5) may be loose if the number of support vectors is small. Suppose k is not characteristic, then \u03b3 k (P m , Q n ) can be zero for P m = Q n and therefore the margin is zero, which means even unlike distributions can become inseparable in this feature representation. Another justification of using characteristic kernels in kernel-based classification algorithms can be provided by studying the conditions on H for which the Bayes risk is realized for all \u00b5. Steinwart and Christmann [19,Corollary 5.37] have showed that under certain conditions on L, the Bayes risk is achieved for all \u00b5 if and only if H is dense in L p (M, \u03b7) for all \u03b7, where \u03b7 = \u03b5P + (1 \u2212 \u03b5)Q. Here, L p (M, \u03b7) represents the Banach space of p-power integrable functions, where p \u2208 [1, \u221e) is dependent on the loss function, L. Denseness of H in L p (M, \u03b7) implies H + R is dense L p (M, \u03b7), which therefore yields that k is characteristic [6,7]. On the other hand, if constant functions are included in H, then it is easy to show that the characteristic property of k is also sufficient to achieve the Bayes risk. As an example, it can be shown that characteristic kernels are necessary (and sufficient if constant functions are in H) for SVMs to achieve the Bayes risk [19,Example 5.40]. Therefore, the characteristic property of k is fundamental in kernel-based classification algorithms. Having showed how characteristic kernels play a role in kernel-based classification, in the following section, we provide a novel characterization for them.", "publication_ref": ["b7", "b8", "b5", "b4", "b18", "b5", "b6", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Novel Characterization for Characteristic Kernels", "text": "A positive definite (pd) kernel, k is said to be characteristic to P if and only if \u03b3 k (P, Q) = 0 \u21d4 P = Q, \u2200 P, Q \u2208 P. The following result provides a novel characterization for characteristic kernels, which shows that strictly pd kernels are characteristic to P. An advantage with this characterization is that it holds for any arbitrary topological space M unlike the earlier characterizations where a group structure on M is assumed [17,7]. First, we define strictly pd kernels as follows. k(x, y)f (x)f (y) dx dy > 0 for all f \u2208 L 2 (R d ), which is the strictly positive definiteness of the integral operator given by the kernel. Definition 3 is stronger than the finite sum definition as [19,Theorem 4.62] shows a kernel that is strictly pd in the finite sum sense but not in the integral sense.\nTheorem 4 (Strictly pd kernels are characteristic). If k is strictly positive definite on M , then k is characteristic to P.\nThe proof idea is to derive necessary and sufficient conditions for a kernel not to be characteristic. We show that choosing k to be strictly pd violates these conditions and k is therefore characteristic to P. Examples of strictly pd kernels on R\nd include exp(\u2212\u03c3 x\u2212y 2 2 ), \u03c3 > 0, exp(\u2212\u03c3 x\u2212y 1 ), \u03c3 > 0, (c 2 + x \u2212 y 2 2 ) \u2212\u03b2 , \u03b2 > 0, c > 0, B 2l+1 -splines etc. Note thatk(x, y) = f (x)k(x, y)f (y)\nis a strictly pd kernel if k is strictly pd, where f : M \u2192 R is a bounded continuous function. Therefore, translation-variant strictly pd kernels can be obtained by choosing k to be a translation invariant strictly pd kernel. A simple example of a translation-variant kernel that is a strictly pd kernel on compact sets of R d isk(x, y) = exp(\u03c3x T y), \u03c3 > 0, where we have chosen f (.) = exp(\u03c3 . 2 2 /2) and k(x, y) = exp(\u2212\u03c3 x \u2212 y 2 2 /2), \u03c3 > 0. Therefore,k is characteristic on compact sets of R d , which is the same result that follows from the universality ofk [18, Section 3, Example 1]. The following result in [13], which is based on the usual definition of strictly pd kernels, can be obtained as a corollary to Theorem 4.", "publication_ref": ["b16", "b6", "b18", "b1", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Corollary 5 ([13]). Let", "text": "X = {x i } m i=1 \u2282 M , Y = {y j } n j=1 \u2282 M and assume that x i = x j , y i = y j , \u2200 i, j. Suppose k is strictly positive definite. Then m i=1 \u03b1 i k(., x i ) = n j=1 \u03b2 j k(., y j ) for some \u03b1 i , \u03b2 j \u2208 R\\{0} \u21d2 X = Y .\nSuppose we choose \u03b1 i = 1 m , \u2200 i and \u03b2 j = 1 n , \u2200 j in Corollary 5. Then m i=1 \u03b1 i k(., x i ) and n j=1 \u03b2 j k(., y j ) represent the mean functions in H. Note that the Parzen classifier in ( 4) is a mean classifier (that separates the mean functions) in H, i.e., sign( k(., x), w H ), where\nw = 1 m m i=1 k(., x i ) \u2212 1 n n i=1 k(., y i ).\nSuppose k is strictly pd (more generally, suppose k is characteristic). Then, by Corollary 5, the normal vector, w to the hyperplane in H passing through the origin is zero, i.e., the mean functions coincide (and are therefore not classifiable) if and only if X = Y .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Generalizing the MMD for Classes of Characteristic Kernels", "text": "The discussion so far has been related to the characteristic property of k that makes \u03b3 k a metric on P. We have seen that this characteristic property is of prime importance both in distribution testing, and to ensure classifiability of dissimilar distributions in the RKHS. We have not yet addressed how to choose among a selection/family of characteristic kernels, given a particular pair of distributions we wish to discriminate between. We introduce one approach to this problem in the present section.\nLet M = R d and k \u03c3 (x, y) = exp(\u2212\u03c3 x \u2212 y 2\n2 ), \u03c3 \u2208 R + , where \u03c3 represents the bandwidth parameter. {k \u03c3 : \u03c3 \u2208 R + } is the family of Gaussian kernels and {\u03b3 k\u03c3 : \u03c3 \u2208 R + } is the family of MMDs indexed by the kernel parameter, \u03c3. Note that k \u03c3 is characteristic for any \u03c3 \u2208 R ++ and therefore \u03b3 k\u03c3 is a metric on P for any \u03c3 \u2208 R ++ . However, in practice, one would prefer a single number that defines the distance between P and Q. The question therefore to be addressed is how to choose appropriate \u03c3. The choice of \u03c3 has important implications on the statistical aspect of \u03b3 k \u03c3 . Note that as \u03c3 \u2192 0, k \u03c3 \u2192 1 and as \u03c3 \u2192 \u221e, k \u03c3 \u2192 0 a.e., which means \u03b3 k\u03c3 (P, Q) \u2192 0 as \u03c3 \u2192 0 or \u03c3 \u2192 \u221e for all P, Q \u2208 P (this behavior is also exhibited by\nk \u03c3 (x, y) = exp(\u2212\u03c3 x \u2212 y 1 ) and k \u03c3 (x, y) = \u03c3 2 /(\u03c3 2 + x \u2212 y 2\n2 ), which are also characteristic). This means choosing sufficiently small or sufficiently large \u03c3 (depending on P and Q) makes \u03b3 k \u03c3 (P, Q) arbitrarily small. Therefore, \u03c3 has to be chosen appropriately in applications to effectively distinguish between P and Q. Presently, the applications involving MMD set \u03c3 heuristically [8,9].\nTo generalize the MMD to families of kernels, we propose the following modification to \u03b3 k , which yields a pseudometric on P,\n\u03b3(P, Q) = sup{\u03b3 k (P, Q) : k \u2208 K} = sup{ Pk \u2212 Qk H : k \u2208 K}.(6\n) Note that \u03b3 is the maximal RKHS distance between P and Q over a family, K of positive definite kernels. It is easy to check that if any k \u2208 K is characteristic, then \u03b3 is a metric on P. Examples for K include:\nK g := {e \u2212\u03c3 x\u2212y 2 2 , x, y \u2208 R d : \u03c3 \u2208 R + }; K l := {e \u2212\u03c3 x\u2212y 1 , x, y \u2208 R d : \u03c3 \u2208 R + }; K \u03c8 := {e \u2212\u03c3\u03c8(x,y) , x, y \u2208 M : \u03c3 \u2208 R + }, where \u03c8 : M \u00d7 M \u2192 R is a negative definite kernel; K rbf := { \u221e 0 e \u2212\u03bb x\u2212y 2 2 d\u00b5 \u03c3 (\u03bb), x, y \u2208 R d , \u00b5 \u03c3 \u2208 M + : \u03c3 \u2208 \u03a3 \u2282 R d },\nwhere M + is the set of all finite nonnegative Borel measures, \u00b5 \u03c3 on R + that are not concentrated at zero, etc. The proposal of \u03b3(P, Q) in ( 6) can be motivated by the connection that we have established in Section 2 between \u03b3 k and the Parzen window classifier. Since the Parzen window classifier depends on the kernel, k, one can propose to learn the kernel like in support vector machines [10], wherein the kernel is chosen such that R L F k in Theorem 1 is minimized over k \u2208 K, i.e., inf k\u2208K R L F k = \u2212 sup k\u2208K \u03b3 k (P, Q) = \u2212\u03b3(P, Q). A similar motivation for \u03b3 can be provided based on (5) as learning the kernel in a hard-margin SVM by maximizing its margin.\nAt this point, we briefly discuss the issue of normalized vs. unnormalized kernel families, K in (6). We say a translation-invariant kernel, k on R d is normalized if M \u03c8(y) dy = c (some positive constant independent of the kernel parameter), where k(x, y) = \u03c8(x \u2212 y). K is a normalized kernel family if every kernel in K is normalized. If K is not normalized, we say it is unnormalized. For example, it is easy to see that K g and K l are unnormalized kernel families. Let us consider the normalized Gaussian family,\nK n g = {(\u03c3/\u03c0) d/2 e \u2212\u03c3 x\u2212y 2 2 , x, y \u2208 R d : \u03c3 \u2208 [\u03c3 0 , \u221e)}.\nIt can be shown that for any k \u03c3 , k \u03c4 \u2208 K n g , 0 < \u03c3 < \u03c4 < \u221e, we have \u03b3 k \u03c3 (P, Q) \u2265 \u03b3 k \u03c4 (P, Q), which means, \u03b3(P, Q) = \u03b3 \u03c30 (P, Q). Therefore, the generalized MMD reduces to a single kernel MMD. A similar result also holds for the normalized inverse-quadratic kernel family, { 2\u03c3 2 /\u03c0(\n\u03c3 2 + x \u2212 y 2 2 ) \u22121 , x, y \u2208 R : \u03c3 \u2208 [\u03c3 0 , \u221e)}.\nThese examples show that the generalized MMD definition is usually not very useful if K is a normalized kernel family. In addition, \u03c3 0 should be chosen beforehand, which is equivalent to heuristically setting the kernel parameter in \u03b3 k . Note that \u03c3 0 cannot be zero because in the limiting case of \u03c3 \u2192 0, the kernels approach a Dirac distribution, which means the limiting kernel is not bounded and therefore the definition of MMD in (1) does not hold. So, in this work, we consider unnormalized kernel families to render the definition of generalized MMD in (6) useful.\nTo use \u03b3 in statistical applications where P and Q are known only through i.i.d. samples {X i } m i=1 and {Y i } n i=1 respectively, we require its estimator \u03b3(P m , Q n ) to be consistent, where P m and Q n represent the empirical measures based on {X i } m i=1 and {Y j } n j=1 . For k measurable and bounded, [8,15] have shown that \u03b3 k (P m , Q n ) is a mn/(m + n)-consistent estimator of \u03b3 k (P, Q). The statistical consistency of \u03b3(P m , Q n ) is established in the following theorem, which uses tools from U-process theory [3,Chapters 3,5]. We begin with the following definition. Definition 6 (Rademacher chaos). Let G be a class of functions on M \u00d7 M and {\u03c1 i } n i=1 be independent Rademacher random variables, i.e., Pr(\u03c1 i = 1) = Pr(\u03c1 i = \u22121) = 1 2 . The homogeneous Rademacher chaos process of order two with respect to {\u03c1 i } n i=1 is defined as {n \u22121 n i<j \u03c1 i \u03c1 j g(x i , x j ) : g \u2208 G} for some {x i } n i=1 \u2282 M . The Rademacher chaos complexity over G is defined as\nU n (G; {x i } n i=1 ) := E \u03c1 sup g\u2208G 1 n n i<j \u03c1 i \u03c1 j g(x i , x j ) . (7\n)\nWe now provide the main result of the present section. Theorem 7 (Consistency of \u03b3(P m , Q n )). Let every k \u2208 K be measurable and bounded with \u03bd := sup k\u2208K,x\u2208M k(x, x) < \u221e. Then, with probability at least 1 \u2212 \u03b4, |\u03b3(P m , Q n ) \u2212 \u03b3(P, Q)| \u2264 A, where\nA = 16U m (K; {X i }) m + 16U n (K; {Y i }) n + ( \u221a 8\u03bd + 36\u03bd log 4 \u03b4 ) \u221a m + n \u221a mn . (8\n)\nFrom (8), it is clear that if U m (K; {X i }) = O P (1) and U n (K; {Y i }) = O Q (1), then \u03b3(P m , Q n ) a.s. \u2192 \u03b3(P, Q).\nThe following result provides a bound on U m (K; {X i }) in terms of the entropy integral. Lemma 8 (Entropy bound). For any K as in Theorem 7 with 0 \u2208 K, there exists a universal constant C such that \nU m (K; {X i } m i=1 ) \u2264 C \u03bd 0 log N (K, D, ) d , (9\n)\nwhere D(k 1 , k 2 ) = 1 m m i<j (k 1 (X i , X j ) \u2212 k 2 (X i , X j )) 2", "publication_ref": ["b7", "b8", "b9", "b5", "b7", "b14", "b2", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "< g(x)}. A collection G of measurable functions on a sample space is called a VC-subgraph class, if the collection of all subgraphs of the functions in G forms a VC-class of sets (in M \u00d7 R).", "text": "The VC-index (also called the VC-dimension) of a VC-subgraph class, G is the same as the pseudodimension of G. See [1, Definition 11.1] for details. Corollary 10 (U m (K; {X i }) for VC-subgraph, K). Suppose K is a VC-subgraph class with V (K) being the VC-index. Assume K satisfies the conditions in Theorem 7 and 0 \u2208 K. Then\nU m (K; {X i }) \u2264 C\u03bd log(C 1 V (K)(16e 9 ) V (K) ),(10)\nfor some universal constants C and \u2192 0. Now, the question reduces to which of the kernel classes, K have V (K) < \u221e. [22,Lemma 12] showed that V (K g ) = 1 (also see [23]) and\nU m (K rbf ) \u2264 C 2 U m (K g ), where C 2 < \u221e. It can be shown that V (K \u03c8 ) = 1 and V (K l ) = 1.\nAll these classes satisfy the conditions of Theorem 7 and Corollary 10 and therefore provide consistent estimates of \u03b3(P, Q) for any P, Q \u2208 P. Examples of kernels on R d that are covered by these classes include the Gaussian, Laplacian, inverse multiquadratics, Mat\u00e9rn class etc. Other choices for K that are popular in machine learning are the linear combination of kernels, [16,Lemma 7] have shown that V (K con ) \u2264 V (K lin ) \u2264 l. Therefore, instead of using a class based on a fixed, parameterized kernel, one can also use a finite linear combination of kernels to compute \u03b3.\nK lin := {k \u03bb = l i=1 \u03bb i k i | k \u03bb is pd, l i=1 \u03bb i = 1} and K con := {k \u03bb = l i=1 \u03bb i k i | \u03bb i \u2265 0, l i=1 \u03bb i = 1}.\nSo far, we have presented the metric property and statistical consistency (of the empirical estimator) of \u03b3. Now, the question is how do we compute \u03b3(P m , Q n ) in practice. To show this, in the following, we present two examples. Example 11. Suppose K = K g . Then, \u03b3(P m , Q n ) can be written as\n\u03b3 2 (P m , Q n ) = sup \u03c3\u2208R + \uf8ee \uf8f0 m i,j=1 e \u2212\u03c3 X i \u2212X j 2 m 2 + n i,j=1 e \u2212\u03c3 Y i \u2212Y j 2 n 2 \u2212 2 m,n i,j=1 e \u2212\u03c3 X i \u2212Y j 2 mn \uf8f9 \uf8fb . (11\n)\nThe optimum \u03c3 * can be obtained by solving (11) and \u03b3(P m ,\nQ n ) = P m k \u03c3 * \u2212 Q n k \u03c3 * H \u03c3 . Example 12. Suppose K = K con . Then, \u03b3(P m , Q n ) becomes \u03b3 2 (P m , Q n ) = sup k\u2208K con P m k \u2212 Q n k 2 H = sup k\u2208K con k d(P m \u2212 Q n ) \u2297 (P m \u2212 Q n ) = sup{\u03bb T a : \u03bb T 1 = 1, \u03bb 0}, (12\n)\nwhere we have replaced k by\nl i=1 \u03bb i k i .\nHere \u03bb = (\u03bb 1 , . . . , \u03bb l ) and (a\n) i = P m k i \u2212 Q n k i 2 H i = 1 m 2 m a,b=1 k i (X a , X b ) + 1 n 2 n a,b=1 k i (Y a , Y b ) \u2212 2 mn m,n a,b=1 k i (X a , Y b ). It is easy to see that \u03b3 2 (P m , Q n ) = max 1\u2264i\u2264l (a) i .\nSimilar examples can be provided for other K, where \u03b3(P m , Q n ) can be computed by solving a semidefinite program (K = K lin ) or by the constrained gradient descent ( K = K l , K rbf ).\nFinally, while the approach in (6) to generalizing \u03b3 k is our focus in this paper, an alternative Bayesian strategy would be to define a non-negative finite measure \u03bb over K, and to average \u03b3 k over that measure, i.e., \u03b2(P, Q) := K \u03b3 k (P, Q) d\u03bb(k). This also yields a pseudometric on P. That said, \u03b2(P, Q) \u2264 \u03bb(K)\u03b3(P, Q), \u2200 P, Q, which means if P and Q can be distinguished by \u03b2, they can be distinguished by \u03b3, but not vice-versa. In this sense, \u03b3 is stronger than \u03b2. One further complication with the Bayesian approach is in defining a sensible \u03bb over K. Note that \u03b3 k 0 (single kernel MMD based on k 0 ) can be obtained by defining \u03bb(k) = \u03b4(k \u2212 k 0 ) in \u03b2(P, Q).", "publication_ref": ["b21", "b22", "b15", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section, we present a benchmark experiment that illustrates the generalized MMD proposed in Section 4 is preferred above the single kernel MMD where the kernel parameter is set heuristically. The experimental setup is as follows.\nLet p = N (0, \u03c3 2 p ), a normal distribution in R with zero mean and variance, \u03c3 2 p . Let q be the perturbed version of p, given as q(x) = p(x)(1 + sin \u03bdx). Here p and q are the densities associated with P and Q respectively. It is easy to see that q differs from p at increasing frequencies with increasing \u03bd. Let k(x, y) = exp(\u2212(x \u2212 y) 2 /\u03c3). Now, the goal is that given random samples drawn i.i.d. from P and Q (with \u03bd fixed), we would like to test H 0 : P = Q vs. H 1 : P = Q. The idea is that as \u03bd increases, it will be harder to distinguish between P and Q for a fixed sample size. Therefore, using this setup we can verify whether the adaptive bandwidth selection achieved by \u03b3 (as the test statistic) helps to distinguish between P and Q at higher \u03bd compared to \u03b3 k with a heuristic \u03c3. To this end, using \u03b3(P m , Q n ) and \u03b3 k (P m , Q n ) (with various \u03c3) as test statistics T mn , we design a test that returns H 0 if T mn \u2264 c mn , and H 1 otherwise. The problem therefore reduces to finding c mn . c mn is determined as the (1 \u2212 \u03b1) quantile of the asymptotic distribution of T mn under H 0 , which therefore fixes the type-I error (the probability of rejecting H 0 when it is true) to \u03b1. The consistency of this test under \u03b3 k (for any fixed \u03c3) is proved in [8]. A similar result can be shown for \u03b3 under some conditions on K. We skip the details here.\nIn our experiments, we set m = n = 1000, \u03c3 2 p = 10 and draw two sets of independent random samples from Q. The distribution of T mn is estimated by bootstrapping on these samples (250 bootstrap iterations are performed) and the associated 95 th quantile (we choose \u03b1 = 0.05) is computed. Since the performance of the test is judged by its type-II error (the probability of accepting H 0 when H 1 is true), we draw a random sample, one each from P and Q and test whether P = Q. This process is repeated 300 times, and estimates of type-I and type-II errors are obtained for both \u03b3 and \u03b3 k . 14 different values for \u03c3 are considered on a logarithmic scale of base 2 with exponents (\u22123, \u22122, \u22121, 0, 1, 3 2 , 2, 5 2 , 3, 7 2 , 4, 5, 6) along with the median distance between samples as one more choice. 5 different choices for \u03bd are considered: ( 1 2 , 3 4 , 1, 5 4 , 3 2 ). Figure 1(a) shows the estimated type-I and type-II errors using \u03b3 as the test statistic for varying \u03bd. Note that the type-I error is close to its design value of 5%, while the type-II error is zero for all \u03bd, which means \u03b3 distinguishes between P and Q for all perturbations. Figures 1(b,c) show the estimates of type-I and type-II errors using \u03b3 k as the test statistic for different \u03c3 and \u03bd. Figure 1(d) shows the box plot for log \u03c3, grouped by \u03bd, where \u03c3 is the bandwidth selected by \u03b3. Figure 1(e) shows the box plot of the median distance between points (which is also a choice for \u03c3), grouped by \u03bd. From Figures 1(c) and (e), it is easy to see that the median heuristic exhibits high type-II error for \u03bd = 3 2 , while \u03b3 exhibits zero type-II error (from Figure 1(a)). Figure 1(c) also shows that heuristic choices of \u03c3 can result in high type-II errors. It is intuitive to note that as \u03bd increases, (which means the characteristic function of Q differs from that of P at higher frequencies), a smaller \u03c3 is needed to detect these changes. The advantage of using \u03b3 is that it selects \u03c3 in a distribution-dependent fashion and its behavior in the box plot shown in Figure 1(d) matches with the previously mentioned intuition about the behavior of \u03c3 with respect to \u03bd. These results demonstrate the validity of using \u03b3 as a distance measure in applications.", "publication_ref": ["b7", "b2", "b2"], "figure_ref": ["fig_2", "fig_2", "fig_2", "fig_2", "fig_2", "fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "Conclusions", "text": "In this work, we have shown how MMD appears in binary classification, and thus that characteristic kernels are important in kernel-based classification algorithms. We have broadened the class of characteristic RKHSs to include those induced by strictly positive definite kernels (with particular application to kernels on non-compact domains, and/or kernels that are not translation invariant). We have further provided a convergent generalization of MMD over families of kernel functions, which becomes necessary even in considering relatively simple families of kernels (such as the Gaussian kernels parameterized by their bandwidth). The usefulness of the generalized MMD is illustrated experimentally with a two-sample testing problem.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Proofs", "text": "We provide proofs for the results in Sections 2-4.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Proof of Theorem 1", "text": "To prove Theorem 1, we need the following result from [17]. Theorem 13 ([17]). Let F k := {f : f H \u2264 1}, where (H, k) is an RKHS defined on a measurable space M with k measurable and bounded. Then,\n\u03b3 k (P, Q) = sup f \u2208F k |Pf \u2212 Qf | = Pk \u2212 Qk H , (13\n)\nwhere \u2022 H represents the RKHS norm.\nProof of Theorem 1: From (3), we have\n\u03b5 M L 1 (f ) dP + (1 \u2212 \u03b5) M L \u22121 (f ) dQ = M f dQ \u2212 M f dP = Qf \u2212 Pf.\nTherefore,\nR L F k = inf f \u2208F k (Qf \u2212 Pf ) = \u2212 sup f \u2208F k (Pf \u2212 Qf ) = \u2212 sup f \u2208F k |Pf \u2212 Qf | = \u2212\u03b3 k (P, Q),\nwhich follows from Theorem 13. Given {(X i , Y i )} N i=1 drawn i.i.d. from \u00b5, the empirical equivalent of ( 3) is given by\ninf \u2212 1 m Y i =1 f (X i ) + 1 N \u2212 m Y i =\u22121 f (X i ) : f \u2208 F k .\nSolving this for f gives\nf = 1 m Yi=1 k(., X i ) \u2212 1 N \u2212m Yi=\u22121 k(., X i ) 1 m Y i =1 k(., X i ) \u2212 1 N \u2212m Y i =\u22121 k(., X i ) H\n, and the result in (4) follows.", "publication_ref": ["b16"], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Proof of Theorem 2", "text": "Before we prove Theorem 2, we present a lemma which we will use to prove Theorem 2. Lemma 14. Let \u03b8 : V \u2192 R and \u03c8 : V \u2192 R be convex functions on a real vector space V . Suppose\na = sup{\u03b8(x) : \u03c8(x) \u2264 b}, (14\n)\nwhere \u03b8 is not constant on {x : \u03c8(x) \u2264 b} and a < \u221e. Then\nb = inf{\u03c8(x) : \u03b8(x) \u2265 a}. (15\n)\nWe need the following result from [11,Theorem 32.1] to prove Lemma 14. Theorem 15 ([11]). Let f be a convex function, and let C be a convex set contained in the domain of f . If f attains its supremum relative to C at some point of relative interior of C, then f is actually constant throughout C. \n\u03c1 i \u03c1 j k(X i , X j ) \u2264 \u221a 2 m m i<j \u03c1 i \u03c1 j k(X i , X j ) + \u221a \u03bd \u221a m ,(22)\nwe have with probability at least 1 \u2212 \u03b4 4 , the following holds:\nEE \u03c1 sup k\u2208K 1 m m i=1 \u03c1 i k(., X i ) H \u2264 2U m (K; {X i }) m + \u221a \u03bd \u221a m + 2\u03bd m log 4 \u03b4 . (23\n)\nTying ( 19)-( 23), we have that w.p. at least 1 \u2212 \u03b4 2 over the choice of {X i }, the following holds: \nsup k\u2208K P m k \u2212 Pk H \u2264 8U m (K; {X i }) m + 2 \u221a \u03bd \u221a m + 18\u03bd m log 4 \u03b4 . (24\nD 2 (k 1 , k 2 ) = E \u03c1 \uf8ee \uf8f0 1 m n i<j \u03c1 i \u03c1 j h(X i , X j ) \uf8f9 \uf8fb 2 = 1 m 2 E \u03c1 \uf8ee \uf8f0 m i<j,r<s \u03c1 i \u03c1 j \u03c1 r \u03c1 s h(X i , X j )h(X r , X s ) \uf8f9 \uf8fb = 1 m 2 m i<j h 2 (X i , X j ),\nwhere h(X i , X j ) = k 1 (X i , X j ) \u2212 k 2 (X i , X j ).", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "A.7 Proof of Corollary 10", "text": "The result follows by bounding the uniform covering number of the VC-subgraph class, K. By [21, Theorem 2.6], we have N (K, D, ) \u2264 C 1 V (K)(16\u03bd 2 \u22122 e) V (K) , ( ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "The authors thank anonymous reviewers for their constructive comments and especially the reviewer who pointed out the connection between characteristic kernels and the achievability of Bayes risk. B. K. S. was supported by the MPI for Biological Cybernetics, National Science Foundation (grant DMS-MSPA 0625409), the Fair Isaac Corporation and the University of California MI-CRO program. A. G. was supported by grants DARPA IPTO FA8750-09-1-0141, ONR MURI N000140710747, and ARO MURI W911NF0810242.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Proof of Theorem 2: By Theorem 13, \u03b3 k (P, Q) = sup{Pf \u2212 Qf : f H \u2264 1}. Note that Pf \u2212 Qf and f H are convex functionals on H. For P = Q, Pf \u2212 Qf is not constant on F k , since k is characteristic. Therefore, by Lemma 14, we have\nSince this holds for all P = Q, it holds for P m and Q n . Therefore, we have", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.3 Proof of Theorem 4", "text": "To prove Theorem 4, we need the following lemma that provides necessary and sufficient conditions for a kernel not to be characteristic.\nLemma 16. Let k be measurable and bounded on M . Then \u2203 P = Q, P, Q \u2208 P such that \u03b3 k (P, Q) = 0 if and only if there exists a finite non-zero signed Borel measure \u00b5 that satisfies: Then,\nFrom the proof of Theorem 13 (see Theorem 3 in [17]), we have \u00b5k, \u00b5k H = k(x, y) d\u00b5(x) d\u00b5(y) and therefore, by (i), \u03b3 k (P, Q) = 0. So, we have constructed\n, and therefore (i) follows.", "publication_ref": ["b16"], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Theorem 4:", "text": "Since k is strictly pd on M , we have k(x, y) d\u03b7(x)d\u03b7(y) > 0 for any finite non-zero signed Borel measure \u03b7. This means there does not exist a finite non-zero signed Borel measure that satisfies (i) in Lemma 16. Therefore, by Lemma 16, there does not exist P = Q, P, Q \u2208 P such that \u03b3 k (P, Q) = 0, which implies k is characteristic to P.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.4 Proof of Corollary 5", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Consider", "text": "Here \u03b4 x i represents the Dirac measure at x i \u2208 M . So, we have \u00b5 X k = \u00b5 Y k, which is equivalent to\nSince k is strictly pd, by Lemma 16, we have", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Neural Network Learning: Theoretical Foundations", "journal": "Cambridge University Press", "year": "1999", "authors": "M Anthony; P L Bartlett"}, {"ref_id": "b1", "title": "Limit theorems for U-processes", "journal": "Annals of Probability", "year": "1993", "authors": "M A Arcones; E Gin\u00e9"}, {"ref_id": "b2", "title": "Decoupling: From Dependence to Independence", "journal": "Springer-Verlag", "year": "1999", "authors": "V H De La Pe\u00f1a; E Gin\u00e9"}, {"ref_id": "b3", "title": "A Probabilistic Theory of Pattern Recognition", "journal": "Springer-Verlag", "year": "1996", "authors": "L Devroye; L Gyorfi; G Lugosi"}, {"ref_id": "b4", "title": "Real Analysis and Probability", "journal": "Cambridge University Press", "year": "2002", "authors": "R M Dudley"}, {"ref_id": "b5", "title": "Kernel measures of conditional dependence", "journal": "MIT Press", "year": "2008", "authors": "K Fukumizu; A Gretton; X Sun; B Sch\u00f6lkopf"}, {"ref_id": "b6", "title": "Characteristic kernels on groups and semigroups", "journal": "", "year": "2009", "authors": "K Fukumizu; B K Sriperumbudur; A Gretton; B Sch\u00f6lkopf"}, {"ref_id": "b7", "title": "A kernel method for the two sample problem", "journal": "MIT Press", "year": "2007", "authors": "A Gretton; K M Borgwardt; M Rasch; B Sch\u00f6lkopf; A Smola"}, {"ref_id": "b8", "title": "A kernel statistical test of independence", "journal": "MIT Press", "year": "2008", "authors": "A Gretton; K Fukumizu; C.-H Teo; L Song; B Sch\u00f6lkopf; A Smola"}, {"ref_id": "b9", "title": "Learning the kernel matrix with semidefinite programming", "journal": "Journal of Machine Learning Research", "year": "2004", "authors": "G R G Lanckriet; N Christianini; P Bartlett; L El Ghaoui; M I Jordan"}, {"ref_id": "b10", "title": "Convex Analysis", "journal": "Princeton University Press", "year": "1970", "authors": "R T Rockafellar"}, {"ref_id": "b11", "title": "Learning with Kernels", "journal": "MIT Press", "year": "2002", "authors": "B Sch\u00f6lkopf; A J Smola"}, {"ref_id": "b12", "title": "RKHS representation of measures", "journal": "", "year": "2008", "authors": "B Sch\u00f6lkopf; B K Sriperumbudur; A Gretton; K Fukumizu"}, {"ref_id": "b13", "title": "Kernel Methods for Pattern Analysis", "journal": "Cambridge University Press", "year": "2004", "authors": "J Shawe-Taylor; N Cristianini"}, {"ref_id": "b14", "title": "A Hilbert space embedding for distributions", "journal": "Springer-Verlag", "year": "2007", "authors": "A J Smola; A Gretton; L Song; B Sch\u00f6lkopf"}, {"ref_id": "b15", "title": "Learning bounds for support vector machines with learned kernels", "journal": "", "year": "2006", "authors": "N Srebro; S Ben-David"}, {"ref_id": "b16", "title": "Injective Hilbert space embeddings of probability measures", "journal": "", "year": "2008", "authors": "B K Sriperumbudur; A Gretton; K Fukumizu; G R G Lanckriet; B Sch\u00f6lkopf"}, {"ref_id": "b17", "title": "On the influence of the kernel on the consistency of support vector machines", "journal": "Journal of Machine Learning Research", "year": "2002", "authors": "I Steinwart"}, {"ref_id": "b18", "title": "Support Vector Machines", "journal": "Springer", "year": "2008", "authors": "I Steinwart; A Christmann"}, {"ref_id": "b19", "title": "Positive definite functions and generalizations, an historical survey", "journal": "Rocky Mountain Journal of Mathematics", "year": "1976", "authors": "J Stewart"}, {"ref_id": "b20", "title": "Weak Convergence and Empirical Processes", "journal": "Springer-Verlag", "year": "1996", "authors": "A W Van Der Vaart; J A Wellner"}, {"ref_id": "b21", "title": "Generalization bounds for learning the kernel", "journal": "", "year": "2009", "authors": "Y Ying; C Campbell"}, {"ref_id": "b22", "title": "Learnability of Gaussians with flexible variances", "journal": "Journal of Machine Learning Research", "year": "2007", "authors": "Y Ying; D X Zhou"}], "figures": [{"figure_label": "3", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Definition 3 (3Strictly positive definite kernels). Let M be a topological space. A measurable and bounded kernel, k is said to be strictly positive definite if and only if M M k(x, y) d\u00b5(x) d\u00b5(y) > 0 for all finite non-zero signed Borel measures, \u00b5 defined on M .Note that the above definition is not equivalent to the usual definition of strictly pd kernels that involves finite sums[19, Definition 4.15]. The above definition is a generalization of integrally strictly positive definite functions[20, Section 6]:", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "1 2 .2N (K, D, ) represents thecovering number of K with respect to the metric D. Assuming K to be a VC-subgraph class, the following result, as a corollary to Lemma 8 provides an estimate of U m (K; {X i } m i=1 ). Before presenting the result, we first provide the definition of a VC-subgraph class. Definition 9 (VC-subgraph class). The subgraph of a function g : M \u00d7 R is the subset of M \u00d7 R given by {(x, t) : t", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "C 1 .1Using (10) in (8), we have |\u03b3(P m , Q n ) \u2212 \u03b3(P, Q)| = O P,Q ( (m + n)/mn) and by the Borel-Cantelli lemma, |\u03b3(P m , Q n ) \u2212 \u03b3(P, Q)| a.s.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 1 :1Figure 1: (a) Type-I and Type-II errors (in %) for \u03b3 for varying \u03bd. (b,c) Type-I and type-II error (in %) for \u03b3 k (with different \u03c3) for varying \u03bd. The dotted line in (c) corresponds to the median heuristic, which shows that its associated type-II error is very large at large \u03bd. (d) Box plot of log \u03c3 grouped by \u03bd, where \u03c3 is selected by \u03b3. (e) Box plot of the median distance between points (which is also a choice for \u03c3), grouped by \u03bd. Refer to Section 5 for details.", "figure_data": ""}, {"figure_label": "14", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Proof of Lemma 14 :14Note that A := {x : \u03c8(x) \u2264 b} is a convex subset of V . Since \u03b8 is not constant on A, by Theorem 15, \u03b8 attains its supremum on the boundary of A. Therefore, any solution, x * to (14) satisfies \u03b8(x * ) = a and \u03c8(x * ) = b. Let G := {x : \u03b8(x) > a}. For any x \u2208 G, \u03c8(x) > b. If this were not the case, then x * is not a solution to(14). Let H := {x : \u03b8(x) = a}. Clearly, x * \u2208 H and so there exists an x \u2208 H for which \u03c8(x) = b. Suppose inf{\u03c8(x) : x \u2208 H} = c < b, which means for some x * \u2208 H, x * \u2208 A. From (14), this implies \u03b8 attains its supremum relative to A at some point of relative interior of A. By Theorem 15, this implies \u03b8 is constant on A leading to a contradiction. Therefore, inf{\u03c8(x) : x \u2208 H} = b and the result in (15) follows.A.5 Proof of Theorem 7Consider|\u03b3(P m , Q n ) \u2212 \u03b3(P, Q)| = | sup k\u2208K P m k \u2212 Q n k H \u2212 sup k\u2208K Pk \u2212 Qk H | \u2264 sup k\u2208K | P m k \u2212 Q n k H \u2212 Pk \u2212 Qk H | \u2264 sup k\u2208K P m k \u2212 Q n k \u2212 Pk + Qk H \u2264 sup k\u2208K [ P m k \u2212 Pk H + Q n k \u2212 Qk H ] \u2264 sup k\u2208K P m k \u2212 Pk H + sup k\u2208K Q n k \u2212 Qk H . (18)We now bound the terms sup k\u2208K P m k\u2212Pk H and sup k\u2208K Q n k\u2212Qk H . Since sup k\u2208K P m k\u2212 Pk H satisfies the bounded difference property, using McDiarmid's inequality gives that with probability at least 1 \u2212 \u03b4 4 over the choice of {X i }, we havesup k\u2208K P m k \u2212 Pk H \u2264 E sup k\u2208K P m k \u2212 Pk H + 2\u03bd m invoking symmetrization for E sup k\u2208K P m k \u2212 Pk H , we have E sup k\u2208K P m k \u2212 Pk H \u2264 2EE \u03c1 sup k\u2208K {\u03c1 i } m i=1 represent i.i.d. Rademacher random variables and E \u03c1 represents the expectation w.r.t. {\u03c1 i } conditioned on {X i }. Since E \u03c1 sup k\u2208K 1 m m i=1 \u03c1 i k(., X i ) H satisfies the bounded difference property, by McDiarmid's inequality, with probability at least 1 \u2212 \u03b4 4 over the choice of the random samples of size m, we have EE \u03c1 sup k\u2208K", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "\u221awhere V (K) is the VC-index of K. Therefore,U m (K; {X i }) \u2264 C \u03bd 0 log N (K, D, ) d \u2264 4V (K)C \u03bd 0 log( \u221a \u03bd \u221a ) d + \u03bdV (K)C log(16e) + C\u03bd log(C 1 V (K)). ) d \u2264 2\u03bd.Using this in (27) and rearranging the terms provides the result.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Performing similar analysis for sup k\u2208K Q n k \u2212 Qk H , we have that w.p. at least 1 \u2212 \u03b4 2 over the choice of {Y i },supFrom [2, Proposition 2.2, Proposition 2.6] (also see [3, Corollary 5.1.8]), we have that there exists a universal constant C < \u221e such that U m (K; {X i }) \u2264 C \u03bd 0 log N (K, D, ) d , where", "figure_data": "A.6 Proof of Lemma 8Using (24) and (25) with\u221a a +\u221a}) b \u2264 2(a + b) provides the result. n + 2 \u221a \u03bd \u221a n +18\u03bd nlog4 \u03b4.(25)"}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u03b3(P, Q) = sup{\u03b3 k (P, Q) : k \u2208 K} = sup{ Pk \u2212 Qk H : k \u2208 K},(2)", "formula_coordinates": [2.0, 170.81, 585.88, 333.2, 11.36]}, {"formula_id": "formula_1", "formula_text": "R L F = inf f \u2208F M L(y, f (x)) d\u00b5(x, y) = inf f \u2208F \u03b5 M L 1 (f ) dP + (1 \u2212 \u03b5) M L \u22121 (f ) dQ , (3", "formula_coordinates": [3.0, 118.09, 353.24, 382.04, 20.51]}, {"formula_id": "formula_2", "formula_text": ")", "formula_coordinates": [3.0, 500.13, 354.81, 3.87, 10.46]}, {"formula_id": "formula_3", "formula_text": ") := L(1, \u03b1), L \u22121 (\u03b1) := L(\u22121, \u03b1), P(X) := \u00b5(X|Y = +1), Q(X) := \u00b5(X|Y = \u22121), \u03b5 := \u00b5(M, Y = +1).", "formula_coordinates": [3.0, 108.0, 380.41, 396.0, 21.41]}, {"formula_id": "formula_4", "formula_text": "Suppose {(X i , Y i )} N i=1 , X i \u2208 M , Y i \u2208 {\u22121, +1}, \u2200 i is a training sample drawn i.i.d. from \u00b5 and m = |{i : Y i = 1}|. If f \u2208 F k is an empirical minimizer of (3) (where F is replaced by F k in (3)), then sign( f (x)) = 1, 1 m Y i =1 k(x, X i ) > 1 N \u2212m Y i =\u22121 k(x, X i ) \u22121, 1 m Y i =1 k(x, X i ) \u2264 1 N \u2212m Y i =\u22121 k(x, X i ) ,(4)", "formula_coordinates": [3.0, 108.0, 454.65, 396.0, 62.69]}, {"formula_id": "formula_5", "formula_text": "X i , Y i )} N i=1 , X i \u2208 M , Y i \u2208 {\u22121, +1}, \u2200 i is a training sample drawn i.i.d. from \u00b5. Assuming the training sample is separable, let f svm be the solution to the program, inf{ f H : Y i f (X i ) \u2265 1, \u2200 i},", "formula_coordinates": [3.0, 108.0, 630.83, 396.0, 34.35]}, {"formula_id": "formula_6", "formula_text": "1 f svm H \u2264 \u03b3 k (P m , Q n ) 2 , (5", "formula_coordinates": [3.0, 259.74, 680.52, 240.39, 24.93]}, {"formula_id": "formula_7", "formula_text": ")", "formula_coordinates": [3.0, 500.13, 687.27, 3.87, 10.46]}, {"formula_id": "formula_8", "formula_text": "P m := 1 m Y i =1 \u03b4 Xi , Q n := 1 n Y i =\u22121 \u03b4 Xi , m = |{i : Y i = 1}| and n = N \u2212 m. \u03b4 x represents the Dirac measure at x.", "formula_coordinates": [3.0, 108.0, 712.66, 395.51, 22.8]}, {"formula_id": "formula_9", "formula_text": "d include exp(\u2212\u03c3 x\u2212y 2 2 ), \u03c3 > 0, exp(\u2212\u03c3 x\u2212y 1 ), \u03c3 > 0, (c 2 + x \u2212 y 2 2 ) \u2212\u03b2 , \u03b2 > 0, c > 0, B 2l+1 -splines etc. Note thatk(x, y) = f (x)k(x, y)f (y)", "formula_coordinates": [4.0, 108.0, 541.71, 396.0, 26.18]}, {"formula_id": "formula_10", "formula_text": "X = {x i } m i=1 \u2282 M , Y = {y j } n j=1 \u2282 M and assume that x i = x j , y i = y j , \u2200 i, j. Suppose k is strictly positive definite. Then m i=1 \u03b1 i k(., x i ) = n j=1 \u03b2 j k(., y j ) for some \u03b1 i , \u03b2 j \u2208 R\\{0} \u21d2 X = Y .", "formula_coordinates": [4.0, 108.0, 665.44, 396.0, 38.2]}, {"formula_id": "formula_11", "formula_text": "w = 1 m m i=1 k(., x i ) \u2212 1 n n i=1 k(., y i ).", "formula_coordinates": [5.0, 108.0, 91.85, 169.26, 15.76]}, {"formula_id": "formula_12", "formula_text": "Let M = R d and k \u03c3 (x, y) = exp(\u2212\u03c3 x \u2212 y 2", "formula_coordinates": [5.0, 108.0, 224.91, 202.52, 12.43]}, {"formula_id": "formula_13", "formula_text": "k \u03c3 (x, y) = exp(\u2212\u03c3 x \u2212 y 1 ) and k \u03c3 (x, y) = \u03c3 2 /(\u03c3 2 + x \u2212 y 2", "formula_coordinates": [5.0, 108.0, 302.7, 396.0, 22.32]}, {"formula_id": "formula_14", "formula_text": "\u03b3(P, Q) = sup{\u03b3 k (P, Q) : k \u2208 K} = sup{ Pk \u2212 Qk H : k \u2208 K}.(6", "formula_coordinates": [5.0, 170.81, 386.0, 329.32, 11.36]}, {"formula_id": "formula_15", "formula_text": "K g := {e \u2212\u03c3 x\u2212y 2 2 , x, y \u2208 R d : \u03c3 \u2208 R + }; K l := {e \u2212\u03c3 x\u2212y 1 , x, y \u2208 R d : \u03c3 \u2208 R + }; K \u03c8 := {e \u2212\u03c3\u03c8(x,y) , x, y \u2208 M : \u03c3 \u2208 R + }, where \u03c8 : M \u00d7 M \u2192 R is a negative definite kernel; K rbf := { \u221e 0 e \u2212\u03bb x\u2212y 2 2 d\u00b5 \u03c3 (\u03bb), x, y \u2208 R d , \u00b5 \u03c3 \u2208 M + : \u03c3 \u2208 \u03a3 \u2282 R d },", "formula_coordinates": [5.0, 108.0, 420.55, 396.01, 42.17]}, {"formula_id": "formula_16", "formula_text": "K n g = {(\u03c3/\u03c0) d/2 e \u2212\u03c3 x\u2212y 2 2 , x, y \u2208 R d : \u03c3 \u2208 [\u03c3 0 , \u221e)}.", "formula_coordinates": [5.0, 227.2, 603.74, 235.37, 14.71]}, {"formula_id": "formula_17", "formula_text": "\u03c3 2 + x \u2212 y 2 2 ) \u22121 , x, y \u2208 R : \u03c3 \u2208 [\u03c3 0 , \u221e)}.", "formula_coordinates": [5.0, 108.0, 642.43, 396.0, 25.56]}, {"formula_id": "formula_18", "formula_text": "U n (G; {x i } n i=1 ) := E \u03c1 sup g\u2208G 1 n n i<j \u03c1 i \u03c1 j g(x i , x j ) . (7", "formula_coordinates": [6.0, 207.03, 211.73, 293.1, 31.53]}, {"formula_id": "formula_19", "formula_text": ")", "formula_coordinates": [6.0, 500.13, 221.65, 3.87, 10.46]}, {"formula_id": "formula_20", "formula_text": "A = 16U m (K; {X i }) m + 16U n (K; {Y i }) n + ( \u221a 8\u03bd + 36\u03bd log 4 \u03b4 ) \u221a m + n \u221a mn . (8", "formula_coordinates": [6.0, 148.56, 286.84, 351.57, 35.5]}, {"formula_id": "formula_21", "formula_text": ")", "formula_coordinates": [6.0, 500.13, 305.0, 3.87, 10.46]}, {"formula_id": "formula_22", "formula_text": "From (8), it is clear that if U m (K; {X i }) = O P (1) and U n (K; {Y i }) = O Q (1), then \u03b3(P m , Q n ) a.s. \u2192 \u03b3(P, Q).", "formula_coordinates": [6.0, 108.0, 326.37, 396.0, 24.52]}, {"formula_id": "formula_23", "formula_text": "U m (K; {X i } m i=1 ) \u2264 C \u03bd 0 log N (K, D, ) d , (9", "formula_coordinates": [6.0, 213.88, 372.5, 286.25, 27.5]}, {"formula_id": "formula_24", "formula_text": ")", "formula_coordinates": [6.0, 500.13, 381.05, 3.87, 10.46]}, {"formula_id": "formula_25", "formula_text": "where D(k 1 , k 2 ) = 1 m m i<j (k 1 (X i , X j ) \u2212 k 2 (X i , X j )) 2", "formula_coordinates": [6.0, 108.0, 406.75, 247.79, 15.76]}, {"formula_id": "formula_26", "formula_text": "U m (K; {X i }) \u2264 C\u03bd log(C 1 V (K)(16e 9 ) V (K) ),(10)", "formula_coordinates": [6.0, 210.78, 573.78, 293.23, 12.93]}, {"formula_id": "formula_27", "formula_text": "U m (K rbf ) \u2264 C 2 U m (K g ), where C 2 < \u221e. It can be shown that V (K \u03c8 ) = 1 and V (K l ) = 1.", "formula_coordinates": [6.0, 108.0, 642.3, 396.0, 11.35]}, {"formula_id": "formula_28", "formula_text": "K lin := {k \u03bb = l i=1 \u03bb i k i | k \u03bb is pd, l i=1 \u03bb i = 1} and K con := {k \u03bb = l i=1 \u03bb i k i | \u03bb i \u2265 0, l i=1 \u03bb i = 1}.", "formula_coordinates": [6.0, 118.51, 686.14, 385.48, 26.2]}, {"formula_id": "formula_29", "formula_text": "\u03b3 2 (P m , Q n ) = sup \u03c3\u2208R + \uf8ee \uf8f0 m i,j=1 e \u2212\u03c3 X i \u2212X j 2 m 2 + n i,j=1 e \u2212\u03c3 Y i \u2212Y j 2 n 2 \u2212 2 m,n i,j=1 e \u2212\u03c3 X i \u2212Y j 2 mn \uf8f9 \uf8fb . (11", "formula_coordinates": [7.0, 117.4, 129.12, 382.45, 41.64]}, {"formula_id": "formula_30", "formula_text": ")", "formula_coordinates": [7.0, 499.85, 149.14, 4.15, 10.46]}, {"formula_id": "formula_31", "formula_text": "Q n ) = P m k \u03c3 * \u2212 Q n k \u03c3 * H \u03c3 . Example 12. Suppose K = K con . Then, \u03b3(P m , Q n ) becomes \u03b3 2 (P m , Q n ) = sup k\u2208K con P m k \u2212 Q n k 2 H = sup k\u2208K con k d(P m \u2212 Q n ) \u2297 (P m \u2212 Q n ) = sup{\u03bb T a : \u03bb T 1 = 1, \u03bb 0}, (12", "formula_coordinates": [7.0, 108.0, 178.36, 391.85, 70.46]}, {"formula_id": "formula_32", "formula_text": ")", "formula_coordinates": [7.0, 499.85, 238.36, 4.15, 10.46]}, {"formula_id": "formula_33", "formula_text": "l i=1 \u03bb i k i .", "formula_coordinates": [7.0, 236.66, 254.56, 35.19, 15.31]}, {"formula_id": "formula_34", "formula_text": ") i = P m k i \u2212 Q n k i 2 H i = 1 m 2 m a,b=1 k i (X a , X b ) + 1 n 2 n a,b=1 k i (Y a , Y b ) \u2212 2 mn m,n a,b=1 k i (X a , Y b ). It is easy to see that \u03b3 2 (P m , Q n ) = max 1\u2264i\u2264l (a) i .", "formula_coordinates": [7.0, 108.0, 255.95, 396.0, 39.17]}, {"formula_id": "formula_35", "formula_text": "\u03b3 k (P, Q) = sup f \u2208F k |Pf \u2212 Qf | = Pk \u2212 Qk H , (13", "formula_coordinates": [9.0, 213.29, 195.86, 286.57, 19.02]}, {"formula_id": "formula_36", "formula_text": ")", "formula_coordinates": [9.0, 499.85, 195.86, 4.15, 10.46]}, {"formula_id": "formula_37", "formula_text": "\u03b5 M L 1 (f ) dP + (1 \u2212 \u03b5) M L \u22121 (f ) dQ = M f dQ \u2212 M f dP = Qf \u2212 Pf.", "formula_coordinates": [9.0, 148.1, 263.1, 315.8, 18.95]}, {"formula_id": "formula_38", "formula_text": "R L F k = inf f \u2208F k (Qf \u2212 Pf ) = \u2212 sup f \u2208F k (Pf \u2212 Qf ) = \u2212 sup f \u2208F k |Pf \u2212 Qf | = \u2212\u03b3 k (P, Q),", "formula_coordinates": [9.0, 139.64, 302.7, 332.73, 20.6]}, {"formula_id": "formula_39", "formula_text": "inf \u2212 1 m Y i =1 f (X i ) + 1 N \u2212 m Y i =\u22121 f (X i ) : f \u2208 F k .", "formula_coordinates": [9.0, 188.6, 362.66, 234.79, 29.11]}, {"formula_id": "formula_40", "formula_text": "f = 1 m Yi=1 k(., X i ) \u2212 1 N \u2212m Yi=\u22121 k(., X i ) 1 m Y i =1 k(., X i ) \u2212 1 N \u2212m Y i =\u22121 k(., X i ) H", "formula_coordinates": [9.0, 195.24, 414.75, 216.94, 30.3]}, {"formula_id": "formula_41", "formula_text": "a = sup{\u03b8(x) : \u03c8(x) \u2264 b}, (14", "formula_coordinates": [9.0, 250.59, 528.26, 249.26, 10.46]}, {"formula_id": "formula_42", "formula_text": ")", "formula_coordinates": [9.0, 499.85, 528.26, 4.15, 10.46]}, {"formula_id": "formula_43", "formula_text": "b = inf{\u03c8(x) : \u03b8(x) \u2265 a}. (15", "formula_coordinates": [9.0, 252.03, 563.53, 247.82, 10.46]}, {"formula_id": "formula_44", "formula_text": ")", "formula_coordinates": [9.0, 499.85, 563.53, 4.15, 10.46]}, {"formula_id": "formula_45", "formula_text": "\u03c1 i \u03c1 j k(X i , X j ) \u2264 \u221a 2 m m i<j \u03c1 i \u03c1 j k(X i , X j ) + \u221a \u03bd \u221a m ,(22)", "formula_coordinates": [11.0, 274.94, 475.98, 229.07, 67.84]}, {"formula_id": "formula_46", "formula_text": "EE \u03c1 sup k\u2208K 1 m m i=1 \u03c1 i k(., X i ) H \u2264 2U m (K; {X i }) m + \u221a \u03bd \u221a m + 2\u03bd m log 4 \u03b4 . (23", "formula_coordinates": [11.0, 152.88, 566.97, 346.97, 35.53]}, {"formula_id": "formula_47", "formula_text": ")", "formula_coordinates": [11.0, 499.85, 580.89, 4.15, 10.46]}, {"formula_id": "formula_48", "formula_text": "sup k\u2208K P m k \u2212 Pk H \u2264 8U m (K; {X i }) m + 2 \u221a \u03bd \u221a m + 18\u03bd m log 4 \u03b4 . (24", "formula_coordinates": [11.0, 173.05, 620.91, 326.81, 32.28]}, {"formula_id": "formula_49", "formula_text": "D 2 (k 1 , k 2 ) = E \u03c1 \uf8ee \uf8f0 1 m n i<j \u03c1 i \u03c1 j h(X i , X j ) \uf8f9 \uf8fb 2 = 1 m 2 E \u03c1 \uf8ee \uf8f0 m i<j,r<s \u03c1 i \u03c1 j \u03c1 r \u03c1 s h(X i , X j )h(X r , X s ) \uf8f9 \uf8fb = 1 m 2 m i<j h 2 (X i , X j ),", "formula_coordinates": [12.0, 172.15, 128.77, 266.04, 117.37]}], "doi": ""}