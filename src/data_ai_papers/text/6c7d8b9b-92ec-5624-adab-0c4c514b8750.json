{"title": "LECO: Improving Early Exiting via Learned Exits and Comparison-based Exiting Mechanism", "authors": "Jingfan Zhang; Ming Tan; Pengyu Dai; Wei Zhu", "pub_date": "", "abstract": "Recently, dynamic early exiting has attracted much attention since it can accelerate the inference speed of pre-trained models (PTMs). However, previous work on early exiting has neglected the intermediate exits' architectural designs. In this work, we propose a novel framework, Learned Exits and COmparison-based early exiting (LECO) to improve PTMs' early exiting performances. First, to fully uncover the potentials of multi-exit BERT, we design a novel search space for intermediate exits and employ the idea of differentiable neural architecture search (DNAS) to design proper exit architectures for different intermediate layers automatically. Second, we propose a simpleyet-effective comparison-based early exiting mechanism (COBEE), which can help PTMs achieve better performance and speedup tradeoffs. Extensive experiments show that our LECO achieves the SOTA performances for multi-exit BERT training and dynamic early exiting.", "sections": [{"heading": "Introduction", "text": "Despite achieving state-of-the-art (SOTA) performances on almost all the natural language processing (NLP) tasks (Lin et al., 2021), large pre-trained language models (PLMs) still have difficulty being applied to many industrial scenarios with low latency requirements. Many research works are devoted to speeding up the inference of BERT or other PLMs, such as network pruning (Zhu and Gupta, 2017;Xu et al., 2020a;Fan et al., 2019;Gordon et al., 2020), student network distillation (Sun et al., 2019;Sanh et al., 2019;Jiao et al., 2020), and early exiting (Teerapittayanon et al., 2016;Kaya et al., 2019;. Due to its potential in applications, early exiting has attracted much attention in the research field (Xu et al., 2021a). Early exiting requires a multi-exit BERT, a BERT backbone with an intermediate classifier (or exit) installed on each layer. And then, a dynamic early exiting mechanism is applied during the forward pass to ensure efficient inference. Early exiting is in parallel with and can work together with static model compression methods (Tambe et al., 2020). However, the literature focuses less on the training of multi-exit BERT (Teerapittayanon et al., 2016; and there is no literature systematically discussing the architectural design of the intermediate exits.\nIn this work, we propose a novel framework, Learned Exits and COmparison-based Early exiting (LECO), designated to discover the full potentials of multi-exit BERT in early exiting. First, we design a suitable and comprehensive search space for architectural learning of the intermediate exits (see Figure 1). Our search space contains candidate activation functions, encoding operations, and pooling operations. We follow the differentiable neural architecture search (DNAS) framework like Liu et al. (2019a); Xie et al. (2019); Chen et al. (2021) to learn a set of intermediate exits with different architectures automatically. Second, reflecting on the limitations of the patience-based early exiting method PABEE , we propose a comparison-based early exiting (COBEE) mechanism. COBEE makes early exiting decisions by comparing the predicted distributions of adjacent intermediate layers.\nWe conduct extensive experiments and ablation studies on the GLUE benchmark . We show that learned intermediate exits of LECO outperform the previous SOTA multi-exiting BERT training methods while adding fewer trainable parameters. Furthermore, our novel dynamic early exiting mechanism COBEE outperforms the previous SOTA early exiting mechanisms. Further analysis shows that: (a) our LECO framework can help to boost the performance of multi-exiting  2 Related Work", "publication_ref": ["b47", "b55", "b8", "b11", "b37", "b33", "b17", "b40", "b18", "b50", "b39", "b40", "b26", "b45", "b4"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Inference acceleration methods", "text": "Since the rise of BERT, there are quite large numbers of literature devoting themselves to speeding up the inference of BERT. Standard method include direct network pruning (Zhu and Gupta, 2017;Xu et al., 2020a;Fan et al., 2019;Gordon et al., 2020), distillation (Sun et al., 2019;Sanh et al., 2019;Jiao et al., 2020), Weight quantization (Zhang et al., 2020b;Kim et al., 2021) and Adaptive inference . Among them, adaptive inference has drawn much attention. Adaptive inference aims to deal with simple examples with only shallow layers of PLMs, thus speeding up inference time on average.\nEarly exiting requires a multi-exit model, like a BERT backbone with an intermediate classifier (or exit) installed on each layer. Early exiting literature mainly focuses on the development of the early exiting strategies, that is, determining when an intermediate exit's prediction is suitable as the final model prediction. Score based strategies (Teerapittayanon et al., 2016;Kaya et al., 2019;, prior based strategies (Sun et al., 2022) and patience based strategies  have been proposed. Teerapittayanon et al. (2016) uses the entropy of an intermediate layer's predicted distribution to measure the in-confidence level and decide whether to exit early. PABEE asks the model to exit when the current layer's prediction is the same with the previous layers.\nOur work complements the literature on early exiting by proposing the LECO framework to improve early exiting performance via the automatic architectural design of exit architectures and a novel early exiting mechanism.", "publication_ref": ["b55", "b8", "b11", "b37", "b33", "b17", "b53", "b40", "b18", "b38", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "Neural architecture search", "text": "With the rapid development and wide industrial applications, researchers have devoted great effect in manually designing neural netoweks (Krizhevsky et al., 2012;Simonyan and Zisserman, 2015;He et al., 2016;Huang et al., 2017;Wang et al., 2022). The trend is to stack more and more convolutional or transformer layers to construct a deep network. Recently, when trying to avoid manual architecture design, researchers started considering developing algorithms to design neural networks automatically. Thus, a new research sub-field of automated machine learning (AutoML)  called neural architecture search is established .\nIn the early attempts, NAS requires massive computations, like thousands of GPU days Liu et al., 2018). Recently, a particular group of one-shot NAS, led by the seminal work DARTS (Liu et al., 2019a) has attracted much attention. DARTS formulates the search space into a super-network that can adjust itself in a continuous space so that the network and architectural parameters can be optimized alternately (bi-level optimization) using gradient descent. A series of literature try to improve the performance and efficiency of DARTS. SNAS (Xie et al., 2019) reformulate DARTS as a credit assignment task while maintaining the differentiability. P-DARTS (Chen et al., 2021) analyze the issues during the DARTS bi-level optimization, and propose a series of modifications. PC-DARTS (Xu et al., 2021b) reduces the memory cost during search by sampling partial channels in super-networks. Fair-DARTS  change the softmax operations in DARTS into sigmoid and introduce a penalty term to prune the architectural parameters according to the demand.  make the hyper-network more close to the discretized sub-network by penalizing the entropy of the architecture parameters.\nOur work contributes to the NAS literature by investigate the architectural search of intermediate exits to improve the early exiting performances.", "publication_ref": ["b20", "b35", "b12", "b16", "b43", "b25", "b26", "b45", "b4", "b51"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "In this section, we introduce the necessary background for BERT early exiting. we consider the case of multi-class classification with K classes, K = {1, 2, ..., K}. The dataset consists of N samples {(x i , y i ), i \u2208 I = {1, 2, ..., N }} , where x i is an input sentence consisting of L words, and y i \u2208 K is the label.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Early Exiting", "text": "Multi-exit PTM Early exiting is based on multiexit PTM, which is a PTM backbone with classifiers (or exits) at each layer. With M layers, M classifiers f m (x; \u03b8 m ) are designated at M layers of the PTM, each of which maps its input to the prob-ability distribution on K classes. f m (x; \u03b8 m ) can take the form of a simple linear layer (linear exit) following . However, as is shown in , adding an encoding operation like the multi-head self-attention layer (Vaswani et al., 2017) to the intermediate exits (MHA exits) can significantly boost the performance of intermediate layers, demonstrating the importance of architectural design.\nTraining We now introduce the three main multiexit BERT training methods widely adopted in the literature.\nJT. Perhaps the most straightforward fine-tuning strategy is to minimize the sum of all classifiers' loss functions and jointly update all parameters in the process. We refer to this strategy as JT. The loss function is:\nL JT = M m=1 L CE m (1)\nwhere\nL CE m = L CE m (y, f m (x; \u03b8 m ))\ndenotes the cross-entropy loss of the m-th exit. This method is adopted by Teerapittayanon et al. (2016); Kaya et al. (2019); ; Zhu (2021).\n2ST. The two-stage (2ST)  training strategy divides the training procedure into two stages. The first stage is identical to the vanilla BERT fine-tuning, updating the backbone model and only the final exit. In the second stage, we freeze all parameters updated in the first stage and fine-tune the remaining exits separately:\nStage1 : L stage1 = L CE M (y i , f M (x i ; \u03b8 M ))(2)\nStage2 : L stage2 = L CE m , m = 1, ..., M \u2212 1. (3)\nwhere\nL CE m = L CE m (y i , f m (x i ; \u03b8 m ))\ndenotes the cross-entropy loss of m-th exit.\nALT. It alternates between two objectives (taken from Equation 1 and 2) across different epochs, and it was proposed by BERxiT :\nOdd : L stage1 = L CE M (y i , f M (x i ; \u03b8 M ))(4)\nEven : L joint = M m=1 L CE m (5)\nFor the search and training of our LECO method, we adopt the joint training (JT) method, following Teerapittayanon et al. (2016); Kaya et al. (2019); ; Zhu (2021). LECO mainly employs JT to fine-tune the PTM backbone and simultaneously learn the best exit architectures for all intermediate layers under a differentiable NAS framework. Early exiting inference At inference, the multiexit PLM can operate in two different modes: (a) static early exiting, that is, a suitable exit m * is appointed to predict all queries. (b) Dynamic early exiting, the model starts to predict on the classifiers f (1) , f (2) , ..., in turn in a forward pass, until it receives a signal to stop early at an exit m * < M , or arrives at the last exit M .", "publication_ref": ["b41", "b40", "b18", "b56", "b40", "b18", "b56"], "figure_ref": [], "table_ref": []}, {"heading": "Inference speedup ratio", "text": "During inference, we will run the test samples with batch size one following ; Teerapittayanon et al. (2016). We report the actual wall-clock run-time reduction as the efficiency metric. For each test sample x i , denote the inference time cost under early exiting as t i , and time cost under no early exiting as T i . Then the average speedup ratio on the test set is calculated by\nSpeedup = 1\u2212 Ntest 1 t i Ntest 1 T i\n, where N test is the number of samples on the test set. We will run the test set ten times and report the average speedup ratio to avoid randomness of run-time.", "publication_ref": ["b40"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries on DARTS", "text": "Assume there is a pre-defined space of operations denoted by O, where each element, o(\u2022), denotes a neural network operation, such as convolutional operation, self-attention, and activation. DARTS (Liu et al., 2019a) operates on a search cell, a fully connected directed acyclic graph (DAG) with N nodes. Let (i, j) denote a pair of nodes. The core idea of DARTS is to initialize a supernetwork stacked with blocks with the same architecture as the DAG. During the search, each edge in the DAG is a weighted sum including all\n|O| op- erations in O, f i,j (z i ) = o\u2208O a o i,j \u2022 o(z i ),where\na o i,j = exp \u03b1 o i,j o \u2032 \u2208O exp \u03b1 o \u2032 i,j\n, z i denotes the output of the i-th node, and \u03b1 o i,j is the architectural parameters that represent the weight (or the importance score) of o(\u2022) in edge (i, j). The output of a node is the sum of all input flow, i.e., z j = i<j f i,j (z i ). The output of the entire cell is formed by summing the last two nodes.\nThis design makes the entire framework differentiable to layer weights and architectural parameters \u03b1 o i,j so that it can perform architecture searches in an end-to-end fashion. The standard optimization method is the bi-level optimization proposed in DARTS. After the search process is completed, the discretization procedure extracts the final subnetwork by dropping the operations receiving lower scores. Pooling cell It is also a one-step DAG for selecting the proper pooling layer. The most commonly used pooling operation for PTM-based models is to extract the representations of the [CLS] token (denoted as cls_pool). As is summarized in Gong et al. (2018), other commonly used pooling operations are: max pooling (max_pool); average pooling (avg_pool); self-attention based pooling (sa_pool).\nNote that our search space contains the MHA exit (introduced in Section 3.1) as a special case. The above search space can result in 6.87e+34 combinations of different multi-exit BERT. We will mainly follow DARTS (Liu et al., 2019a) to search for the optimal architecture designs of exits. But different from (Liu et al., 2019a), we adopt a macro search space, that is, the exits from different layers have different architectural parameters, thus resulting different architectures for different layers.", "publication_ref": ["b26", "b10", "b26", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Comparison-based Early Exiting", "text": "The patience-based mechanism  validates the early exiting decisions among the previous layers, providing a promising direction for designing early exiting mechanisms. the early exiting condition in PABEE is coarse: it directly compares the predicted labels. However, tt is common for BERT to change its predictions after a few intermediate layers. Thus, PABEE's early exiting performances with low patience parameters may not be reliable. To summarize, we need a more fine-grained criterion to generate more reliable early exiting signals.\nWe now introduce our Comparison-based early exiting method, COBEE. The inference procedure is illustrated in Figure 1. Assume the forward pass has reached layer m < M . We now compare the predicted distributions of layer m and layer m \u2032 (m > m \u2032 ) as follows. Denote the label that receives the highest probability mass at layer m as k * m , and the probability distribution of exit m is denoted as Pr m , then the disagreement between layer m and layer m \u2032 is calculated as:\nDi(Pr m , Pr m \u2032 ) = |Pr m (k * m )\u2212Pr m \u2032 (k * m )|. (6)\nFor simplicity, we denote di m,m \u2032 = Di(Pr m , Pr m \u2032 ) \u2208 R. The smaller the value of di m,m \u2032 , the predicted distributions Pr m and Pr m \u2032 are more consistent with each other. We use a counter cnt to store the number of times the disagreement scores between adjacent layers are less than the pre-defined exiting threshold \u03c4 . At layer m, cnt m is calculated as:\ncnt m = cnt m\u22121 + 1, if di m,m\u22121 < \u03c4, 0, otherwise. (7)\nIf di m,m\u22121 is less than the pre-defined threshold, then the patience counter is increased by 1. Otherwise, the patience counter is reset to 0. If cnt m reaches the pre-defined patience value t, the model stops inference and exits early. Otherwise, the model goes to the next layer. However, if the model does not exit early at intermediate layers, the model uses the final classifier f M for prediction.\n6 Experiments", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Datasets", "text": "We evaluate our proposed approach to the classification tasks on GLUE benchmark . We only exclude the STS-B task since it is a regression task, and we exclude the WNLI task following previous work (Devlin et al., 2019;Jiao et al., 2020;Xu et al., 2020b). Since the original test sets are not publicly available, we follow Zhang et al. (2020a) and Mahabadi et al. (2021) to construct the train/dev/test splits as follows: (a) for datasets with fewer than 10k samples (RTE, MRPC, CoLA), we divide the original validation set in half, using one half for validation and the other for testing. (b) for larger datasets, we split 1k samples from the training set as the development set, and use the original development set as the test set. The detailed dataset statistics are presented in Table 1.\nFor MNLI, we report acc, which is the average of the accuracy scores on the matched and mismatched test set. For MRPC and QQP, we report acc-f1, which is the average of accuracy and F1 scores. For CoLA, we report mcc, which is the Matthews correlation. For all other tasks, we report accuracy (acc).", "publication_ref": ["b7", "b17", "b49", "b52", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Baseline methods", "text": "We compare our LECO framework with the following baselines: Multi-exiting model training For multi-exit model training, we compare: (a) Joint training (JT) Teerapittayanon et al., 2016), with both a linear exit and an MHA exit (d e = 64); (b) two-stage training (2ST)  Gradient Equilibrium technique (GradEquil) (Li et al., 2019), which incorporates JT with gradient adjustments and is adopted by Liu et al. (2021);  2021), which train an extra meta-classifier to estimate the confidence on a sample and achieves the SOTA performances of early exiting. For comparison, we also run the patience-based method on the backbone obtained by the JT method with linear exits.", "publication_ref": ["b40", "b22", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental settings", "text": "Devices We implement LECO on the base of HuggingFace's Transformers. We conduct our experiments on Nvidia V100 16GB GPUs. PTM models. We mainly adopt the ALBERT base (Lan et al., 2019) backbone. We will also include RoBERTa-base (Liu et al., 2019b), and DeBERTa-base (He et al., 2020) in the ablation studies.\nSettings for Architecture search We add a LECO search cell (Figure 1) with dimension d e equal to 32 on each intermediate layer of the PTM and adopt the DARTS (Liu et al., 2019a) method to learn the best exit architecture for each layer. AdamW optimizer (Loshchilov and Hutter, 2019) is used for both the model and architecture parameters. At the beginning of each epoch, the training set is randomly split into D 1 (for updating model parameters) and D 2 (for updating architecture parameters) with a ratio of 1 : 1. The search will last for 30 epochs. The learning rate is 2e-5 for model parameters and 2e-4 for architectural parameters. The search procedure is run once on each GLUE task.\nSettings for Architecture evaluation After the search procedure ends, the top-scored sub-network is discretized from the super-network at each layer and will be trained from scratch as the final learned exit. The learning rate is 2e-5, and AdamW optimizer (Loshchilov and Hutter, 2019) is used for optimization. We evaluate the dev set and save the checkpoint after each epoch. After training ends, we evaluate the best checkpoint on the test set. We train the final learned exits under 5 random seeds to obtain its average test performance.  ) and JT + MHA exit introduce 66k parameters per exit, while the LECO method adds 25k-26k parameters per exit. The comparison among the three methods demonstrates that our LECO method does not rely only on adding more parameters to obtain performance improvements. The improvements of LECO result from better architectural designs for Table 2: Average test performance of methods with ALBERT backbone on GLUE tasks across 5 random seeds. AVG represents cross-layer average score, and BEST represents best score among all layers. The * symbol on the AVG scores means the results surpass the baseline method with statistical significance (by the Wilcoxon signed-rank test). exits of different depths.", "publication_ref": ["b21", "b29", "b13", "b26", "b30", "b30"], "figure_ref": ["fig_0"], "table_ref": ["tab_2"]}, {"heading": "Main results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Comparison of multi-exit model training methods", "text": "Comparison of dynamic early exiting mechanisms We compare our COBEE method with the previous best-performing early exiting methods on the multi-exit ALBERT-base backbone trained under our LECO framework (as reported in Table 2). We also run the patience-based early exiting with the multi-exit ALBERT-base trained with the JT method. For the patience-based method , early exiting is run on different patience parameters. For the other methods, we run early exiting under different confidence thresholds or patience parameters so that the speedup-performance curves consist of at least 20 points evenly distributed across the interval (0, 1) of speedup ratios. The speedup-performance curves for the RTE and SST-2 tasks are plotted in Figure 2.\nThe following takeaways can also be made from Figure 2: (a) With the same backbone model, our COBEE method achieves better speedupperformance trade-offs than the previous SOTA early exiting methods, especially when the speedup ratio is large. (b) The comparison between Patience and JT+linear exit: Patience demonstrates that our LECO method can provide superior backbones for early exiting and consistently result in superior performances under different speedup ratios, even though introducing a more complex exit architecture. The learned exit architecture constitutes 0.25% of the parameters on each intermediate layer and increases 0.6% inference latency on average. However, the performance gains on the intermediate layers clearly out-weights the increased latency.", "publication_ref": [], "figure_ref": ["fig_3", "fig_3"], "table_ref": ["tab_2"]}, {"heading": "Discussions and ablation studies", "text": "Discussion on the learned architectures Table 6 of the Appendix A presents the best-learned exit architectures on each layer of ALBERT when the downstream task is MRPC or RTE. Three observations can be made: (a) although we allow at most two encoder operations in the encoder search cell, more than half of the learned exits include one valid encoding operation, making the exits more parameter efficient. (b) The learned archi- tectures tend to use a pair of different activation functions, which is different from the combination of the Tanh-Tanh activation functions applied in the MHA exit . (c) Most exits do not select the cls_pool pooling operation, validating the necessity of our pooler search cell. LECO works well with other multi-exit training strategies In the main experiments, we train LECO with the JT method.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitation", "text": "Although our LECO framework is shown to be effective in improving the multi-exit BERT training, it still has certain limitations that need to be addressed in the future: (a) MHA exits and our learned exits indeed introduce new parameters and additional flops. We would like to explore more parameter-efficient methods to improve multi-exit BERT training in future works. (b) In this work, we demonstrate our framework's performance on sentence classification or pair classification tasks. In future works, we would like to extend our work to broader tasks such as sequence labeling, relation extraction, and text generation. We would like to explore this aspect in the future.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "Our LECO framework is designated to improve the training of multi-exit BERT and dynamic early exiting performances. Our work can facilitate the deployment and applications of pre-trained models on devices with less powerful computation capabilities, making the state-of-the-art models accessible for everyone. In addition, we hope this technology can help reduce the carbon footprints of NLP-based applications. Furthermore, the datasets we experiment with are widely used in previous work and, to our knowledge, does not introduce new ethical concerns.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "", "journal": "", "year": "", "authors": "Sst-2 Qnli Qqp Rte Mrpc Cola;  Mnli"}, {"ref_id": "b1", "title": "Deep learning using rectified linear units (relu). ArXiv", "journal": "References Abien Fred Agarap", "year": "2018", "authors": ""}, {"ref_id": "b2", "title": "Binarybert: Pushing the limit of bert quantization", "journal": "", "year": "2020", "authors": "Haoli Bai; Wei Zhang; Lu Hou; Lifeng Shang; Jing Jin; Xin Jiang; Qun Liu; Michael Lyu; Irwin King"}, {"ref_id": "b3", "title": "Adabert: Taskadaptive bert compression with differentiable neural architecture search", "journal": "", "year": "2020", "authors": "Daoyuan Chen; Yaliang Li; Minghui Qiu; Zhen Wang; Bofang Li; Bolin Ding; Hongbo Deng; Jun Huang; Wei Lin; Jingren Zhou"}, {"ref_id": "b4", "title": "Progressive darts: Bridging the optimization gap for nas in the wild", "journal": "ArXiv", "year": "2021", "authors": "Xin Chen; Lingxi Xie; Jun Wu; Qi Tian"}, {"ref_id": "b5", "title": "Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search", "journal": "", "year": "2021", "authors": "Xiangxiang Chu; Bo Zhang; Ruijun Xu; Jixiang Li"}, {"ref_id": "b6", "title": "IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "", "year": "", "authors": ""}, {"ref_id": "b7", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b8", "title": "Reducing transformer depth on demand with structured dropout", "journal": "", "year": "2019", "authors": "Angela Fan; Edouard Grave; Armand Joulin"}, {"ref_id": "b9", "title": "Mtl-nas: Task-agnostic neural architecture search towards general-purpose multitask learning", "journal": "", "year": "2020", "authors": "Yuan Gao; Haoping Bai; Zequn Jie; Jiayi Ma; Kui Jia; Wei Liu"}, {"ref_id": "b10", "title": "Information aggregation via dynamic routing for sequence encoding", "journal": "", "year": "2018", "authors": "Jingjing Gong; Xipeng Qiu; Shaojing Wang; Xuanjing Huang"}, {"ref_id": "b11", "title": "Compressing bert: Studying the effects of weight pruning on transfer learning", "journal": "", "year": "2020", "authors": "Kevin Mitchell A Gordon; Nicholas Duh;  Andrews"}, {"ref_id": "b12", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "X Kaiming He; Shaoqing Zhang; Jian Ren;  Sun"}, {"ref_id": "b13", "title": "Deberta: Decodingenhanced bert with disentangled attention. ArXiv, abs", "journal": "", "year": "2006", "authors": "Pengcheng He; Xiaodong Liu; Jianfeng Gao; Weizhu Chen"}, {"ref_id": "b14", "title": "Automl: A survey of the state-of-the-art", "journal": "Knowl. Based Syst", "year": "2021", "authors": "Xin He; Kaiyong Zhao; Xiaowen Chu"}, {"ref_id": "b15", "title": "Gaussian error linear units (gelus)", "journal": "", "year": "2016", "authors": "Dan Hendrycks; Kevin Gimpel"}, {"ref_id": "b16", "title": "Densely connected convolutional networks", "journal": "", "year": "2017", "authors": "Gao Huang; Zhuang Liu; Kilian Q Weinberger"}, {"ref_id": "b17", "title": "Tinybert: Distilling bert for natural language understanding", "journal": "ArXiv", "year": "1909", "authors": "Xiaoqi Jiao; Yichun Yin; Lifeng Shang; Xin Jiang; Xiao Chen; Linlin Li; Fang Wang; Qun Liu"}, {"ref_id": "b18", "title": "Shallow-deep networks: Understanding and mitigating network overthinking", "journal": "", "year": "2019", "authors": "Sanghyun Kaya; T Hong;  Dumitras"}, {"ref_id": "b19", "title": "2021. I-bert: Integeronly bert quantization", "journal": "PMLR", "year": "", "authors": "Sehoon Kim; Amir Gholami; Zhewei Yao; W Michael; Kurt Mahoney;  Keutzer"}, {"ref_id": "b20", "title": "Imagenet classification with deep convolutional neural networks", "journal": "Communications of the ACM", "year": "2012", "authors": "Alex Krizhevsky; Ilya Sutskever; Geoffrey E "}, {"ref_id": "b21", "title": "Albert: A lite bert for self-supervised learning of language representations", "journal": "", "year": "2019", "authors": "Zhenzhong Lan; Mingda Chen; Sebastian Goodman; Kevin Gimpel; Piyush Sharma; Radu Soricut"}, {"ref_id": "b22", "title": "Improved techniques for training adaptive deep networks", "journal": "", "year": "2019", "authors": "Hao Li; Hong Zhang; Xiaojuan Qi; Ruigang Yang; Gao Huang"}, {"ref_id": "b23", "title": "Xu Sun, and Bin He. 2021. A global past-future early exit method for accelerating inference of pretrained language models", "journal": "", "year": "", "authors": "Kaiyuan Liao; Yi Zhang; Xuancheng Ren; Qi Su"}, {"ref_id": "b24", "title": "", "journal": "", "year": "", "authors": "Tianyang Lin; Yuxin Wang; Xiangyang Liu"}, {"ref_id": "b25", "title": "Progressive neural architecture search", "journal": "", "year": "2018", "authors": "Chenxi Liu; Barret Zoph; Jonathon Shlens; Wei Hua; Li-Jia Li; Li Fei-Fei; Alan Loddon Yuille; Jonathan Huang; Kevin P Murphy"}, {"ref_id": "b26", "title": "Darts: Differentiable architecture search", "journal": "ArXiv", "year": "2019", "authors": "Hanxiao Liu; Karen Simonyan; Yiming Yang"}, {"ref_id": "b27", "title": "Fastbert: a selfdistilling bert with adaptive inference time", "journal": "", "year": "2020", "authors": "Weijie Liu; Peng Zhou; Zhe Zhao; Zhiruo Wang; Haotang Deng; Qi Ju"}, {"ref_id": "b28", "title": "Towards efficient nlp: A standard evaluation and a strong baseline", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Xiangyang Liu; Tianxiang Sun; Junliang He; Lingling Wu; Xinyu Zhang; Hao Jiang; Zhao Cao; Xuanjing Huang; Xipeng Qiu"}, {"ref_id": "b29", "title": "Roberta: A robustly optimized bert pretraining approach. ArXiv, abs", "journal": "", "year": "1907", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b30", "title": "Decoupled weight decay regularization", "journal": "", "year": "2019", "authors": "Ilya Loshchilov; Frank Hutter"}, {"ref_id": "b31", "title": "Compacter: Efficient low-rank hypercomplex adapter layers", "journal": "", "year": "2021", "authors": "James Rabeeh Karimi Mahabadi; Sebastian Henderson;  Ruder"}, {"ref_id": "b32", "title": "Swish: a self-gated activation function. arXiv: Neural and Evolutionary Computing", "journal": "", "year": "2017", "authors": "Prajit Ramachandran; Barret Zoph; V Quoc;  Le"}, {"ref_id": "b33", "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter", "journal": "", "year": "2019", "authors": "Victor Sanh; Lysandre Debut; Julien Chaumond; Thomas Wolf"}, {"ref_id": "b34", "title": "The right tool for the job: Matching model and instance complexities", "journal": "", "year": "2020", "authors": "Roy Schwartz; Gabriel Stanovsky; Swabha Swayamdipta; Jesse Dodge; Noah A Smith"}, {"ref_id": "b35", "title": "Very deep convolutional networks for large-scale image recognition", "journal": "", "year": "2015", "authors": "Karen Simonyan; Andrew Zisserman"}, {"ref_id": "b36", "title": "The evolved transformer. ArXiv, abs", "journal": "", "year": "1901", "authors": "David R So; Chen Liang; V Quoc;  Le"}, {"ref_id": "b37", "title": "Patient knowledge distillation for bert model compression", "journal": "", "year": "2019", "authors": "Siqi Sun; Yu Cheng; Zhe Gan; Jingjing Liu"}, {"ref_id": "b38", "title": "A simple hash-based early exiting approach for language understanding and generation", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Tianxiang Sun; Xiangyang Liu; Wei Zhu; Zhichao Geng; Lingling Wu; Yilong He; Yuan Ni; Guotong Xie; Xuanjing Huang; Xipeng Qiu"}, {"ref_id": "b39", "title": "Edgebert: Optimizing on-chip inference for multitask nlp. ArXiv, abs", "journal": "", "year": "2011", "authors": "Thierry Tambe; Coleman Hooper; Lillian Pentecost; En-Yu Yang; Marco Donato; Victor Sanh; Alexander M Rush; David M Brooks; Gu-Yeon Wei"}, {"ref_id": "b40", "title": "Branchynet: Fast inference via early exiting from deep neural networks", "journal": "", "year": "2016", "authors": "Surat Teerapittayanon; Bradley Mcdanel; H T Kung"}, {"ref_id": "b41", "title": "", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam M Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b42", "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding", "journal": "", "year": "2018", "authors": "Alex Wang; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel R Bowman"}, {"ref_id": "b43", "title": "Deepnet: Scaling transformers to 1, 000 layers", "journal": "ArXiv", "year": "2022", "authors": "Hongyu Wang; Shuming Ma; Li Dong; Shaohan Huang; Dongdong Zhang; Furu Wei"}, {"ref_id": "b44", "title": "Textnas: A neural architecture search space tailored for text representation", "journal": "", "year": "2020", "authors": "Yujing Wang; Yaming Yang; Yiren Chen; Jing Bai; Ce Zhang; Guinan Su; Xiaoyu Kou; Yunhai Tong; Mao Yang; Lidong Zhou"}, {"ref_id": "b45", "title": "Snas: Stochastic neural architecture search", "journal": "ArXiv", "year": "2019", "authors": "Sirui Xie; Hehui Zheng; Chunxiao Liu; Liang Lin"}, {"ref_id": "b46", "title": "Deebert: Dynamic early exiting for accelerating bert inference", "journal": "", "year": "2020", "authors": "Ji Xin; Raphael Tang; Jaejun Lee; Yaoliang Yu; Jimmy Lin"}, {"ref_id": "b47", "title": "Berxit: Early exiting for bert with better finetuning and extension to regression", "journal": "", "year": "2021", "authors": "Ji Xin; Raphael Tang; Yaoliang Yu; Jimmy Lin"}, {"ref_id": "b48", "title": "2020a. Bert-of-theseus: Compressing bert by progressive module replacing", "journal": "", "year": "", "authors": "Canwen Xu; Wangchunshu Zhou; Tao Ge; Furu Wei; Ming Zhou"}, {"ref_id": "b49", "title": "Bert-of-theseus: Compressing bert by progressive module replacing", "journal": "", "year": "2020", "authors": "Canwen Xu; Wangchunshu Zhou; Tao Ge; Furu Wei; Ming Zhou"}, {"ref_id": "b50", "title": "A survey on green deep learning", "journal": "ArXiv", "year": "2021", "authors": "Jingjing Xu; Wangchunshu Zhou; Zhiyi Fu; Hao Zhou; Lei Li"}, {"ref_id": "b51", "title": "Partially-connected neural architecture search for reduced computational redundancy", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2021", "authors": "Yuhui Xu; Lingxi Xie; Wenrui Dai; Xiaopeng Zhang; Xin Chen; Guo-Jun Qi; Hongkai Xiong; Qi Tian"}, {"ref_id": "b52", "title": "Revisiting fewsample bert fine-tuning. ArXiv, abs", "journal": "", "year": "2006", "authors": "Tianyi Zhang; Felix Wu; Arzoo Katiyar; Q Kilian; Yoav Weinberger;  Artzi"}, {"ref_id": "b53", "title": "Ternarybert: Distillation-aware ultra-low bit bert", "journal": "", "year": "2020", "authors": "Wei Zhang; Lu Hou; Yichun Yin; Lifeng Shang; Xiao Chen; Xin Jiang; Qun Liu"}, {"ref_id": "b54", "title": "Bert loses patience: Fast and robust inference with early exit", "journal": "Advances in Neural Information Processing Systems", "year": "2020", "authors": "Wangchunshu Zhou; Canwen Xu; Tao Ge; Julian Mcauley; Ke Xu; Furu Wei"}, {"ref_id": "b55", "title": "To prune, or not to prune: exploring the efficacy of pruning for model compression", "journal": "", "year": "2017", "authors": "Michael Zhu; Suyog Gupta"}, {"ref_id": "b56", "title": "Leebert: Learned early exit for bert with cross-level optimization", "journal": "", "year": "2021", "authors": "Wei Zhu"}, {"ref_id": "b57", "title": "Discovering better model architectures for medical query understanding", "journal": "", "year": "2021", "authors": "Wei Zhu; Yuan Ni; Xiaoling Wang; Guo Tong Xie"}, {"ref_id": "b58", "title": "Gaml-bert: Improving bert early exiting by gradient aligned mutual learning", "journal": "", "year": "2021", "authors": "Wei Zhu; Xiaoling Wang; Yuan Ni; Guo Tong Xie"}, {"ref_id": "b59", "title": "Neural architecture search with reinforcement learning", "journal": "ArXiv", "year": "2017", "authors": "Barret Zoph; V Quoc;  Le"}, {"ref_id": "b60", "title": "Learning transferable architectures for scalable image recognition", "journal": "", "year": "2018", "authors": "Barret Zoph; Vijay Vasudevan; Jonathon Shlens; Quoc V Le"}, {"ref_id": "b61", "title": "IEEE/CVF Conference on Computer Vision and Pattern Recognition", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: The overall framework of our LECO framework. Left: We compare the predicted distributions of adjacent PTMs' intermediate layers for mining exiting signals. Middle: the general architecture of intermediate exits. Right: Each edge in the the search cell is a weighted sum of multiple operations under the DNAS framework.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "4Search space of LECO As depicted in Figure 1, we construct the search space of a LECO intermediate exit mimicking the MHA exit. Representations of the current BERT layer, H (m) i , will first be down-sampled to a smaller dimension R de (e.g., 64) to keep the intermediate exit parameter-efficient. 1 Then, it will go through an activation cell, an encoder cell, a pooling cell, and finally, another activation cell. The whole DAG of the intermediate exit consists of 7 edges. Activation cell Both activations cells are onestep DAGs (Figure 1), designated to choose the proper activation function from several candidates. Similar to So et al. (2019), the collection of activation functions we consider is: (a) ReLU (Agarap, 2018); (b) GeLU (Hendrycks and Gimpel, 2016); (c) SWISH (Ramachandran et al., 2017); (d) Tanh (Krizhevsky et al., 2012); (e) NullAct, which means making no changes to the input. Encoder cell As is shown in Figure 1, different from Wang et al. (2020); Zhu et al. (2021a), we construct our encoder cell as a simple DAG, which consists of at most two encoder operations.Encoder operations 1 and 2 will encode the cell's input, and their outputs will be summed to be the output of the encoder cell. As an extension to the encoder search space of;Zhu et al. (2021a);, our collection of encoder operations consists of the following commonly used encoding operations: (a) 1-d convolutional layers, with stride 1, same padding, output filters equal to the input's dimension, and kernel size equal to 1, 3, or 5 (denoted as conv_k, k = 1, 3, 5); (b) multi-head self-attention layer(Vaswani et al., 2017), with k = 2, 4, 8 attention heads, head size equaling d e /k (denoted as mha_k, k = 2, 4, 8); (c) skip-connection, denoted as skip-connect; (d) the null encoding operation that multiply zero tensors to the input (null).2   ", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "(e) Global Past Future(Liao et al., 2021) (Global-PF) which asks the lower layers to imitate the deeper layers; (f) GAML-BERT(Zhu et al., 2021b), which employs a mutual learning strategy to improve the performances of shallow exits. Early exiting methods We compare the early exiting performances of our COBEE method on the multi-exit backbone trained under the LECO framework with the following methods: (a) Entropybased method (Entropy) originated from(Teerapittayanon et al., 2016), which is equivalent to the maximum-probability based method Schwartz et al. (2020); (b) Patience-based method (Patience) (Zhou et al., 2020); (c) learning-to-exit based method (LTE) proposed by Xin et al. (", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: The speedup-score curves with different dynamic early exiting methods, on the RTE and SST-datasets.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "The statistics of datasets evaluated in this work. For MNLI task, the number of samples in the test set is summed by matched and mismatched samples. |Y| is the number of classes for a dataset.", "figure_data": "Category Single-sentenceDatasets |train| SST-2 66349 1000 |dev| CoLA 8551 521Type sentiment 2 linguistic acceptability |test| |Y| 872 2 522Labels positive, negative acceptable, not acceptableMNLI391702 1000 19647 3NLIentailment, neutral, contradictionMRPC36682042042paraphraseequivalent, not equivalentSentence-pairQNLI103743 1000 54632NLIentailment, not entailmentQQP362846 1000 40430 2paraphraseequivalent, not equivalentRTE24901381392NLIentailment, not entailmentTable 1:, with an MHA exit (d e = 64); (c) alter-nating training (ALT) in Xin et al. (2021); (d) the"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": "reports the main results on the GLUEbenchmark with ALBERT as the backbone model.All baseline models are run with the original au-thors' open-sourced codes. We report AVG, thecross-layer average score, and BEST, the best scoreamong all the intermediate layers. From Table 2,Our LECO method outperforms the previous multi-exit model training methods in terms of the AVGscores (with statistical significance), demonstratingthat our LECO framework effectively boosts theoverall performances of intermediate exits and thusproviding stronger backbones for early exiting.Note that both 2ST + MHA exit"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "demonstrates the results of LECO when trained with 2ST and ALT. The results show that LECO can effectively improve the performances of 2ST and ALT, and achieve comparable results with LECO combined with JT. However, the JT method is more convenient and takes less training time.", "figure_data": "LECO works well with other pretrained back-bones We now substitute the pretrained back-bone to RoBERTa-base (Liu et al., 2019b) andDeBERTa-base (He et al., 2020), and the resultsare reported in Table 4. We can see that ourLECO framework can also help to improve the aver-age performance of multi-exit RoBERTa/DeBERTamodel. An interesting take-away is that RoBERTaand DeBERTa can not outperform ALBERT interms of AVG scores. We hypothesis that ALBERTshares parameters across transformer layers, thusthe difference between shallow and deep layers aresmaller than the other models.Ablation on the search space We now conduct an ablation study to show the validity of our searchspace design. We consider reducing our searchspace O to a singleton step-by-step: (a) reduce the activation cells by only keeping the Tanh activation (O 1 ); (b) further reduce the pooler cell to only in-clude cls_pool (O 2 ); (c) further reduce the encoder cell to only include mha_dot, and now the search space only contains the MHA exit. Table 5 reportsthe search results on different search spaces. FromTable 5, we can see that dropping any componentsof the whole search space results in performance"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Experimental results for the ablation study of our LECO search space. Cross-layer average (AVG) performance scores are reported. Our contributions are three-fold. First, LECO designs a unified search space for architectural designs of intermediate exits. Second, we apply the differentiable NAS framework of DARTS to learn the optimal exit architectures automatically. Third, we propose a novel comparison based early exiting mechanism, COBEE. Experiments on the GLUE benchmark and ablation studies demonstrate that our LECO framework can achieve SOTA on multi-exit BERT training and outperforms the previously SOTA dynamic early exiting methods.", "figure_data": "losses, demonstrating that our search space designis necessary and beneficial.7 ConclusionIn this work, we propose a novel framework,LECO."}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "The best architectures learned via our LECO framework. We can see that on the same task, BERT requires different intermediate exits to better exploit the representation capabilities on different layers.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "L JT = M m=1 L CE m (1)", "formula_coordinates": [3.0, 377.56, 310.26, 147.6, 34.56]}, {"formula_id": "formula_1", "formula_text": "L CE m = L CE m (y, f m (x; \u03b8 m ))", "formula_coordinates": [3.0, 337.68, 354.38, 129.66, 20.41]}, {"formula_id": "formula_2", "formula_text": "Stage1 : L stage1 = L CE M (y i , f M (x i ; \u03b8 M ))(2)", "formula_coordinates": [3.0, 311.86, 526.42, 213.29, 20.96]}, {"formula_id": "formula_3", "formula_text": "Stage2 : L stage2 = L CE m , m = 1, ..., M \u2212 1. (3)", "formula_coordinates": [3.0, 311.86, 544.42, 213.29, 20.96]}, {"formula_id": "formula_4", "formula_text": "L CE m = L CE m (y i , f m (x i ; \u03b8 m ))", "formula_coordinates": [3.0, 336.73, 568.51, 132.51, 20.41]}, {"formula_id": "formula_5", "formula_text": "Odd : L stage1 = L CE M (y i , f M (x i ; \u03b8 M ))(4)", "formula_coordinates": [3.0, 323.5, 659.25, 201.65, 20.96]}, {"formula_id": "formula_6", "formula_text": "Even : L joint = M m=1 L CE m (5)", "formula_coordinates": [3.0, 323.5, 678.34, 201.65, 34.56]}, {"formula_id": "formula_7", "formula_text": "Speedup = 1\u2212 Ntest 1 t i Ntest 1 T i", "formula_coordinates": [4.0, 70.86, 370.67, 112.0, 32.98]}, {"formula_id": "formula_8", "formula_text": "|O| op- erations in O, f i,j (z i ) = o\u2208O a o i,j \u2022 o(z i ),where", "formula_coordinates": [4.0, 70.86, 606.83, 220.09, 34.1]}, {"formula_id": "formula_9", "formula_text": "a o i,j = exp \u03b1 o i,j o \u2032 \u2208O exp \u03b1 o \u2032 i,j", "formula_coordinates": [4.0, 70.86, 633.52, 101.84, 39.82]}, {"formula_id": "formula_10", "formula_text": "Di(Pr m , Pr m \u2032 ) = |Pr m (k * m )\u2212Pr m \u2032 (k * m )|. (6)", "formula_coordinates": [5.0, 76.31, 654.8, 213.55, 21.19]}, {"formula_id": "formula_11", "formula_text": "cnt m = cnt m\u22121 + 1, if di m,m\u22121 < \u03c4, 0, otherwise. (7)", "formula_coordinates": [5.0, 317.93, 97.68, 207.22, 29.68]}], "doi": "10.18653/v1/N19-1423"}