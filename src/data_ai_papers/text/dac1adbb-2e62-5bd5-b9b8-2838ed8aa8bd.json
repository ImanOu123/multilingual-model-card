{"title": "F1/10: An Open-Source Autonomous Cyber-Physical Platform", "authors": "Matthew O'kelly; Houssam Abbas; Jack Harkins; Chris Kao; Yash Vardhan; Rahul Mangharam; Varundev Suresh Babu; Dipshil Agarwal; Madhur Behl; Paolo Burgio; Marko Bertogna", "pub_date": "2019-01-24", "abstract": "In 2005 DARPA labeled the realization of viable autonomous vehicles (AVs) a grand challenge; a short time later the idea became a moonshot that could change the automotive industry. Today, the question of safety stands between reality and solved. Given the right platform the CPS community is poised to o er unique insights. However, testing the limits of safety and performance on real vehicles is costly and hazardous. e use of such vehicles is also outside the reach of most researchers and students. In this paper, we present F1/10: an open-source, a ordable, and high-performance 1/10 scale autonomous vehicle testbed. e F1/10 testbed carries a full suite of sensors, perception, planning, control, and networking so ware stacks that are similar to full scale solutions. We demonstrate key examples of the research enabled by the F1/10 testbed, and how the platform can be used to augment research and education in autonomous systems, making autonomy more accessible.", "sections": [{"heading": "INTRODUCTION", "text": "Progress in cyber-physical systems (CPS) requires the availability of robust platforms on which researchers can conduct real-world experiments and testing. Unfortunatley, a vast majority of CPS experiments are done in isolation -either completely in simulation, or on proprietary hardware designs. In either case, researchers are limited by the inability to deploy their methodologies in realistic environments without solving a host of unrelated problems. In many cases, due to these challenges, the research becomes less reproducible. In contrast, open source tools, and platforms, which can be commonly used across di erent CPS disciplines and by multiple research groups can be a primary driver in enabling highimpact research and teaching.\nis lack of commonly available CPS testbeds is especially signi cant in the rapidly growing eld of connected, and autonomous vehicles (AVs). Modern full-scale automotive platforms are some of the most complex cyber-physical systems ever designed. From realtime and embedded systems, to machine learning and AI, sensor networks, to predictive control, formal methods, security & privacy, to infrastructure planning, and transportation -the design of trustworthy, safe AVs is a truly interdisciplinary endeavour that has captured the imagination of researchers in both academia and industry. Auto companies are joining with tech giants like Google, Uber, and prominent start-ups to develop next-generation autonomous Figure 1: It takes only a couple of hours fully to assemble a F1/10 autonomous racecar, using detailed instructions available at h p://f1tenth.org/ vehicles that will alter our roads and lay the groundwork for future smart cities.\nToday, conducting research in autonomous systems and AVs requires building one's own automotive testbed from scratch. Sometimes researchers must enter into restrictive agreements with automotive manufactures to obtain access to the documentation necessary to build such a testbed, thus preventing the release of their testbed so ware and hardware.\nis paper presents the F1/10 Autonomous Racing Cyber-Physical platform and summarizes the use of this testbed technology as the common denominator and key enabler to address the research and education needs of future autonomous systems and automotive Cyber-Physical Systems.\nere are no a ordable, open-source, and integrated autonomous vehicles test-beds available today that would t in a typical indoor CPS lab. Our goal is not to provide yet another isolated vehicle testbed. Instead, we aim to relieve researchers around the world of the requirement to set up their own facilities for research in autonomous vehicles. e F1/10 research instrument has the potential to build stronger networks of collaborative research. Since the platform is 1/10 the the scale of a real vehicle we call it F1/10 (inspired from Formula 1 (F1)). Kick-started through a joint e ort by University of Pennsylvania (USA), University of Virginia (USA), and UNIMORE (Italy), the F1/10 community is rapidly growing with about 20+ institutions utilizing the test-bed. F1/10 enables researchers and students to rapidly explore automotive cyber-physical systems by providing a platform for realworld experimentation. F1/10's biggest value is in taking care of the most tedious aspects of pu ing together an autonomous vehicle testbed so that the user can focus directly on the research and learning goals.\nWhile commercially available mobile platforms like TurtleBot2 [43], and Jackal UGV [42] can be used as a research testbed, they lack realistic dynamics like Ackermann steering, and the ability to travel at high speeds -a characteristic which is essential for any autonomous vehicle testbed. In contrast, the F1/10 platform is designed to address the issues of realistic vehicle dynamics, and drive-trains. We have designed the F1/10 platform using fully open-source and standardized systems that take advantage of ROS [38] and its associated libraries. On our website h p://f1tenth.org/, detailed, and free instructions are available on how to build, and drive the platform.\nere is an active community of researchers who contribute to both the open-source hardware and so ware design.\nWe present the following open-source capabilities of the F1/10 Autonomous Cyber-Physical Platform: (i) Open-source mechanical design (chassis, development circuit boards, programmable hardware) and open-source kits for assembling a 1/10-scale autonomous racing car. (ii) A suite of AV so ware libraries for perception, planning, control and coordinated autonomy research. (iii) F1/10 simulator and virtual race track. (iv) Multiple annual autonomous racing competitions, hackathons, and high-school education programs. (v) Online course material and data sets. is paper has the following research contributions:\n(1) e design and implementation of F1/10, an open-source autonomous testbed for research and education in autonomy, (2) Modular hardware and so ware stacks that make the F1/10 testbed an accessible, AV vehicle research tool, (3) More than a dozen representative examples of the types of research enabled by the F1/10 platform, particularly those that can be used to test AV algorithms and so ware pipelines with realistic dynamics on a physical and a ordable testbed, (4) A case study of going from 1/10 scale F1/10 cars to full scale autonomous vehicles, (5) Overview of the widely successfully and exciting F1/10 Autonomous Racing Competitions being held at premier CPS and Embedded Systems venues over the last 3 years.\n2 F1/10 TESTBED e F1/10 platform is designed to meet the following requirements: (a) e platform must be able to capture the dynamics of a full scaled autonomous car; (b) e platform's hardware and so ware stack must be modular so as to enable easy upgrades, maintenance and repairs; and (c) e platform must be self-sustaining in terms of power, computation and sensors, i.e, it need not use any external localization (VICON cameras).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "System Architecture", "text": "Figure 2 shows an overview of the F1/10 platform. e perception module interfaces and controls the various sensors including scanning LiDARs, monocular & stereo cameras, inertial sensors, etc.\ne sensors provide the platform with the ability to navigate and localize in the operating environment. e planning pipeline (in ROS) helps process the sensor data, and run mapping, and path planning algorithms to determine the trajectory of the car. Finally, the control module determines the steering and acceleration commands to follow the trajectory in a robust manner.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "F1/10 Build", "text": "In this section we provide a brief description of how the F1/10 autonomous race is built. Detailed instructions and assembly videos can be found at f1tenth.org.\nChassis: e chassis consists of two parts. e bo om chassis is a 1/10 scale race car chassis available from Traxxas [53]. e top chassis is a custom laser-cut ABS plate that our team has developed and to which all the electronic components are a ached. e CAD and laser cut les for the top plate are open-sourced. e Traxxas bo om chassis is no ordinary racing toy: it is a very realistic representation of a real car. It has 4-wheel drive and can reach a top speed of 40mph, which is extremely fast for a car this size. Tire designs replicate the racing rubber used on tarmac circuits. e turnbuckles have broad ats that make it easy to set toe-in and camber, just like in a real race car. e bo om chassis has a high RPM brush-less DC motor to provide the drive to all the wheels, an Electronic Speed Controller (ESC) to controls the main drive motor using pulse-width modulation (PWM), a servo motor for controlling the Ackermann steering, and a ba ery pack; which provides power to all these systems. All the sensors and the on-board computer are powered by a separate power source (lithium-ion ba ery). e F1/10 platform components are a ordable and widely available across the world making it accessible for research groups at most institutions. ese components are properly documented and supported by the manufacturer and the open-source community. Sensors and Computation: e F1/10 platform uses an NVIDIA Jetson TX2 [15] GPU computer. e Jetson is housed on a carrier board [24] to reduce the form factor and power consumption. e Jetson computer hosts the F1/10 so ware stack built on Robot Operating System (ROS). e entire so ware stack, compatible with the sensors listed below, is available as an image that can be ashed onto the Jetson, enabling a plug-and-play build. e default sensor con guration includes a monocular USB web cam, a ZED depth camera, Hokuyo 10LX scanning LiDAR, and a MPU-9050 inertial measurement unit (IMU). ese sensors connect to the Jetson computer over a USB3 hub. Since the underpinnings of the so ware stack is in ROS, many other user preferred sensors can also be integrated/replaced. Power Board: In order to enable high performance driving and computing the F1/10 platform utilizes Lithium Polymer ba eries.\ne power board is used to provide a stable voltage source for the car and its peripherals since the ba ery voltage varies as the vehicle is operated. e power board also greatly simpli es wiring of peripherals such as the LIDAR and wi antennas. Lastly the power board includes a Teensy MCU in order to provide a simple interface to sensors such as wheel encoders and add-ons such as RF receivers for long range remote control. Odometry: Precise odometry is critical for path planing, mapping, and localization. Odometry is provided by the on board VESC as an estimate of the steering angle and the position of the vehicle. e open-source F1/10 so ware stack includes the custom ROS nodes, and a con guration le required to interface with the VESC and obtain the odometry information. Communication architecture: e F1/10 testbed includes a wireless access point which is used to remotely connect (ssh) into the Jetson board. e so ware stack is con gured to use ROS-Over-Network used for both sending commands to the car and obtaining telemetry data from the car in real-time. In addition we have created so ware which supplies a socket which enables communication between multiple F1/10 vehicles operating under di erent ROS master nodes.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "RESEARCH: PLANNING AND CONTROL", "text": "e decision making systems utilized on AVs have progressed signi cantly in recent years; however they still remain a key challenge in enabling AV deployment [48]. While AVs today can perform well in simple scenarios such as highway driving; they o en struggle in scenarios such as merges, pedestrian crossings, roundabouts, and unprotected le -turns. Conducting research in di cult scenarios using full-size vehicles is both expensive and risky. In this section we highlight how the F1/10 platform can enable research on algorithms for obstacle avoidance, end-to-end driving, model predictive control, and vehicle-to-vehicle communication.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Obstacle avoidance", "text": "Obstacle avoidance and forward collision assist are essential to the operation of an autonomous vehicle.\ne AV is required to scan the environment for obstacles and safely navigate around them. For this reason, many researchers have developed interesting real-time approaches for avoiding unexpected static and dynamic obstacles [22,50]. To showcase the capability of the F1/10 testbed, we implement one such algorithm known as Follow e Gap (FTG) method [47]. e Follow the Gap method is based on the construction of a gap array around the vehicle and calculation of the best heading angle for moving the robot into the center of the maximum gap in front, while simultaneously considering its goal. ese two objectives are considered simultaneously by using a fusing function. Figure 3[Le ] shows an overview and the constraints of FTG method. e three steps involved in FTG are: (a) Calculating the gap array using vector eld histogram, and nding the maximum gap in the LIDAR point cloud using an e cient sorting algorithm, (b) Calculating the center of the largest gap, and (c) Calculating the heading angle to the centre of the largest gap in reference to the orientation of the car, and generating a steering control value for the car.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "End-to-end driving", "text": "Some recent research replaces the classic chain of perception, planning, and control with a neural network that directly maps sensor input to control output [7,10,14], a methodology known as end-toend driving. Despite the early interest in end-to-end driving [37], most self-driving cars still use the perception-planning-control paradigm. is slow development can be explained by the challenges of verifying system performance; however, new approaches based on reinforcement learning are being actively developed [25]. e F1/10 testbed is a well suited candidate for experimentation with end-to-end driving pipelines, from data gathering and annotation, to inference, and in some cases even training. Data gathering and annotation for deep learning: As shown in Figure 3[Right], we are able to integrate a First Person View (FPV) camera and headset with the F1/10 car. We are also able to drive the car manually with a USB steering wheel and pedals instead of the RC remote controller which comes with the Traxxas car. e setup consists of a Fat Shark FSV1204 -700TVL CMOS Fixed Mount FPV Camera, 5.8GHz spiroNET Cloverleaf Antenna Set, 5.8GhZ ImmersionRC receiver, and Fat Shark FSV1076-02 Dominator HD3 Core Modular 3D FPV Goggles Headset.\ne FPV setup easily enables teleoperation for the purposes of collecting data to train the end-to-end deep neural netowrks (DNNs). Each training data consists of an input, in this case an image from the front facing camera, and a label a vector containing the steering angle and requested acceleration. Practical issues arise due to the fact that the label measurements (50 Hz) must be synchronized with the acquired camera images (30 Hz). Included in this portion of the stack is a ROS node which aligns the measurements and the labels. As part of this research we are releasing over 40,000 labeled images collected from multiple builds at the University of Pennsylvania and the University of Virginia. End-to-End driving: Partly inspired by Pilotnet [7] end-to-end work, we implemented a combination of a LSTM [27] and a Convolutional Neural Network(CNN) [20] cell. ese units are then used in the form of a recurrent neural network (RNN). is setup uses the bene ts of LSTMs in maintaining temporal information (critical to driving) and utilizes the ability of CNN's to extract high level features from images.\nTo evaluate the performance of the model we use the normalized root mean square error (NRMSE) metric between the ground truth steering value and the predicted value from the DNN. As can be seen in the point-of-view (PoV) image in Figure 3[Le ], our DNN is able to accurately predict the steering angle with an NRMSE of 0.14.", "publication_ref": [], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Global & local approaches to path planning", "text": "AVs operate in relatively structured environments. Most scenarios an AV might face feature some static structure. O en this is the road geometry, lane connectivity, locations of tra c signals, buildings, etc. Many AVs exploit the static nature of these elements to increase their robustness to sensing errors or uncertainty. In the context of F1/10, it may be convenient to exploit some information known a priori about the environment, such as the track layout and oor friction.\nese approaches are called static, or global, and they typically imply building a map of the track, simulating the car in the map, and computing o ine a suitable nominal path which the vehicle will a empt to follow. Valuable data related to friction and dri may also be collected to re ne the vehicle dynamics model. More re ned models can be adopted o -line to compute optimal paths and target vehicle speeds, adopting more precise optimization routines that have a higher computational complexity to minimize the lap time.\nOnce the desired global path has been de ned, the online planner must track it. To do that, there are two main activities must be accomplished on-line, namely localization and vehicle dynamics control. Once the vehicle has been properly localized within a map, a local planner is adopted to send longitudinal and transversal control signals to follow the precomputed optimal path. As the local planner needs to run in real-time, simpler controllers are adopted to decrease the control latency as much as possible. Convenient online controllers include pure pursuit path geometric tracking [11]. e F1/10 so ware distribution includes an implementation of pure pursuit, nodes for creating and loading waypoints, and path visualization tools. For the interested reader we recommend this comprehensive survey of classical planning methods employed on AVs [34].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model Predictive Control", "text": "While data annotation for training end-to-end networks is relatively easy, the performance of such methods is di cult to validate empirically [49] especially relative to approaches which decompose functionality into interpret-able modules. In this section we outline both a local planner which utilizes a model predictive controller (MPC) and a learned approximation of the policy it generates detailing one way planning components can be replaced with e cient learned modules.\nComponents: e F1/10 platform includes a MPC wri en in C++ comprised of the vehicle dynamics model, an optimization routine which performs gradient descent on the spline parameters. Peripheral support nodes provide an interface to road center line information, a multi-threaded goal sampler, a 2D occupancy grid, and a trajectory evaluation module. Additionally, we include a CUDA implementation of a learned approximation of the MPC which utilizes the same interface as described above. Cubic Spline Trajectory Generation: One local planner available on the F1/10 vehicle utilizes the methods outlined in [29] and [21] and rst described in [32]. is approach is commonly known as state-la ice planning with cubic spline trajectory generation. Each execution of the planner requires the current state of the vehicle and a goal state. Planning occurs in a local coordinate frame. e vehicle state x is de ned in the local coordinate system, a subscript indicates a particular kind of state (i.e. a goal) In this implementation we de ne x as: \u00ec\nx = [s x s \u03a8 \u03ba] T , where s x and s are the x and y positions of the center of mass, is the velocity, \u03a8 is the heading angle, and \u03ba is the curvature.\nIn this formulation, trajectories are limited to a speci c class of parameterized curves known as cubic splines which are dense in the robot workspace. We represent a cubic spline as a function of arc length such that the parameters \u00ec\np = [s a b c d] T\nwhere s is the total curve length and (a, b, c, d) are equispaced knot points representing the curvature at a particular arc length. When these parameters are used to de ne the expression of \u03ba(s) which can be used to steer the vehicle directly. e local planner's objective is then to nd a feasible trajectory from the initial state de ned by the tuple \u00ec\nx to a goal pose \u00ec\nx . We use a gradient descent algorithm and forward simulation models which limit the ego-vehicle curvature presented in [21].\nese methods ensure that the path generated is kinematically and dynamically feasible up to a speci ed velocity.\nLearning an Approximation: Recall that \u00ec\nx, the current state of the AV, can be expressed as the position of a moving reference frame a ached to the vehicle. O ine, a region in front of the AV is sampled, yielding a set of M possible goals { \u00ec\nx i } M i=1 , each expressed in relative coordinates. en for each goal \u00ec\nx ,i the reference trajectory connecting them is computed by the original MPC algorithm. Denote the computed reference trajectory by \u00ec\np i = [s a b c d] T us we now have a training set {(\u00ec x ,i , \u00ec p i )} M i=1 . A neural network N N T P is used to t the function x oal,i \u2192 \u00ec p i . Online, given an actual target state \u00ec x in relative coordinates, the AV computes N N T P (\u00ec x )\nto obtain the parameters of the reference trajectory \u00ec p leading to \u00ec x . Our implementation utilizes a radial basis function network architecture, the bene ts of this decision is that the weights can be trained algebraically (via a pseudo-inverse) and each data point is guaranteed to be interpolated exactly. On 145,824 samples in the test set our methodology exhibits a worst-case test error of 0.1% and is capable of generating over 428,000 trajectories per-second.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Vehicle-to-Vehicle Communication,", "text": "Cooperation, and Behavioral Planning e development of autonomous vehicles has been propelled by an idealistic notion that the technology can nearly eliminate accidents.\ne general public expects AVs to exhibit what can best be described as superhuman performance; however, a key component of human driving is the ability to communicate intent via visual, auditory, and motion based cues. Evidence suggests that these communication channels are developed to cope with scenarios in which the fundamental limitations of human senses restrict drivers to cautious operations which anticipate dangerous phenomena before they can be identi ed or sensed. Components: In order to carry out V2V communication experiments we augment the F1/10 planning stack with ROS nodes which contain push/pull TCP clients and servers, these nodes extract user de ned state and plan information so that it may be transmi ed to other vehicles.\nIn this research we construct an AV 'roundabout' scenario where the center-island obstructs the ego-vehicles view of the other tra c participants. A communication protocol which transmits an object list describing the relative positions of participating vehicles, and a simple indicator function encodes whether given each vehicles preferred next action it is safe to proceed into the roundabout is implemented. Alternative scenarios such as a high-speed merge or highway exit maneuver can also easily be constructed at significantly less cost and risk than real world experiments. e F1/10 platform enables an intermediate step between simulation and realworld testing such that the e ects of sensor noise, wireless channel degradation, and actuation error may be studied in the context of new V2V protocols.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "RESEARCH: PERCEPTION", "text": "In this section we highlight how the F1/10 vehicle enables a novel mode of research relative to perception tasks. Although there has been huge progress in low-level vision tasks such as object detection due to e ectiveness of deep learning, AVs only perform such tasks in order to enable decisions which lead to safe mobility. In this context the F1/10 vehicle is a unique tool because it allows researchers to measure not just performance of a perception subsystem in isolation, but rather the capabilities of the whole system within its operating regime. Due to the extensive planning and state estimation capabilities already reliably enabled on the car new researchers focused on perception subsystems can enable comparison of a variety of methods on uniform platform in the context of speci c driving tasks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Simultaneous Localization and Mapping", "text": "e ability for a robot to create a map of a new environment without knowing its precise location (SLAM) is a primary enabler for the use of the F1/10 platform in a variety of locations and environments. Moreover, although SLAM is a well understood problem it is still challenging to create reliable real-time implementations. In order to allow the vehicle to drive in most indoor environments we provide interface to a state of the art LIDAR-based SLAM package which provides loop-closures, namely Google Cartographer [19]. Included in our base so ware distribution are local and global se ings which we have observed to work well empirically through many trials in the classroom and at outreach events. In addition we include a description of the robots geometry in an appropriate format which enables plug-and-play operation. For researchers interested primarily in new approaches to SLAM the F1/10 platform is of interest due to its non-trivial dynamics, modern sensor payload, and the ability to test performance of the algorithm in motion capture spaces (due to the small size of vehicle). In addition to SLAM packages we also provide an interface to an e cient, parallel localization package which utilizes a GPU implementation of raymarching to simulate the observations of random particles in a known 2D map [54]. e inclusion of this package enables research on driving at the limits of control even without a motion capture system for state estimation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Computer Vision", "text": "Our distribution of F1/10 so ware includes the basic ingredients necessary to explore the use of deep learning for computer vision. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Lane keep assist", "text": "e F1/10 platform is designed to work with a wide array of sensors and, among them are USB cameras which enable implementation of lane tracking, and lane keep assist algorithms [16,46]. Utilizing the OpenCV [8] libraries. We implemented a lane tracking algorithm [45] to run in real-time on the F1/10 on-board computer. To do so, we created an image processing pipeline to capture, lter, process, and analyze the image stream using the ROS image transport package, and designed a ROS node to keep track of the le and right lanes and calculate the geometric center of the lane in the current frame. e F1/10 steering controller was modi ed to keep track of the lane center using a proportional-derivative-integral (PID) controller. e image pipeline detailed in Fig. 4 [Le ] is comprised of the following tasks: (a) e raw RGB camera image, in which the lane color was identied by its hue and saturation value, is converted to greyscale and subjected to a color lter designed to set the lane color to white and everything else to black, (b) e masked image from the previous step is sent through a canny edge detector and then through a logical AND mask whose parameters ensured that the resulting image contains only the information about the path, (c) e output from the second step is ltered using a Gaussian lter that reduces noise and is sent through a Hough transformation, resulting in the lane markings contrasting a black background. e output of the image pipeline contains only the lane markings. e lane center is calculated and the F1/10 current heading is compared to the lane center to generate the error in heading. e heading of the car is updated to re ect the new heading generated by the ROS node using a PID controller.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "RESEARCH: SYSTEMS, SIMULATION, AND VERIFICATION", "text": "Safety and robustness are key research areas which must make progress in order to deploy commercial AVs. In this section we highlight the tools which we are using to enable simulation, realtime systems research, and veri cation e orts.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Gazebo Racing Simulators", "text": "Why would we want to use a simulator if you have the F1/10 platform? We want to test the car's algorithms in a controlled environment before we bring it into the real world so that we minimize risk of crashing. For instance, if a front steering servo plastic piece were to break, it is necessary to disassemble 20 parts in order to replace it. In fact each of the labs taught in our courses can be completed entirely in simulation rst. e added bene t is that researchers and students with resource constraints can still utilize the so ware stack that we have built.\nWe use the ROS Gazebo simulator so ware [26]. From a high level, Gazebo loads a world as a .DAE le and loads the car. Gazebo also includes a physics engine that can determine how the F1/10 car will respond to control inputs, friction forces, and collisions with other obstacles in the environment. e F1/10 simulation package currently provides four tracks, each of which have real world counterparts. It is also possible to create custom environments. In the F1/10 reference manual we provide a tutorial on the use of Sketchup to create simple 3D models. More advanced 3D modeling tools such as 3DS Max and Solid Works will also work. Our future work includes a cloud based simulation tool which utilizes the Py-Bullet [12] physics engine and Kubernetes [9] containers for ease of deployment and large scale reinforcement learning experiments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Real-time Systems Research", "text": "Autonomous driving is one of the most challenging engineering problems posed to modern embedded computing systems. It entails processing and interpreting a wide amount of data, in order to make prompt planning decisions and execute them in real-time. Complex perception and planning routines impose a heavy computing workload to the embedded platform, requiring multi-core computing engines and parallel accelerators to satisfy the challenging timing requirements induced by high-speed driving. Inaccuracy in the localization of the vehicles as well as delays in the perception and control loop may signi cantly a ect the stability of the vehicle, and result in intolerable deviations from safe operating conditions. Due to the safety-critical nature of such failures, the F1/10 stack is an ideal platform for testing the e ectiveness of new real-time scheduling and task partitioning algorithms which e ciently exploit the heterogeneous parallel engines made available on the vehicle. One example of such research implemented on the F1/10 platform is the AutoV project [55] which explores whether safety critical vehicle control algorithms can be safely run within a virtual environment. e F1/10 platform also enables real-time systems research which explicitly consider the problem of co-design at the application layer. Speci cally the goal is to create planning, perception, and scheduling algorithms which adapt to the context of the vehicle's operating environment. is regime was explored in a study on CPU/GPU resource allocation for camera-based perception and control [35].\nIn the experiments performed on the F1/10 platform the objective was to obtain energy-e cient computations for the perception and estimation algorithms used in autonomous systems by manipulating the clock of each CPU core and the portion of the computation which would be o oaded to the a GPU. ese knobs allow us to leverage a trade-o between computation time, power consumption and output quality of the perception and estimation algorithms. In this experiment, a vanishing point algorithm is utilized to navigate a corridor. e computation is decomposed into three sequential components, and we study how its runtime and power consumption are a ected by whether each component is run on a GPU or CPU, and the frequency at which it is executed. Results highlight CPU/GPU allocation and execution frequencies which achieve either be er throughput or lower energy consumption without sacri cing control performance. e possible set of operating points and their e ect on the update rate and power consumption for the vanishing point algorithm are shown in Fig. 5 [Middle].", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Monitoring, Testing, & Veri cation", "text": "F1/10 can be used to support and demonstrate advances in formal veri cation and runtime monitoring.\nReal-time veri cation. Rechability analysis is a technique for rigorously bounding a system's future state evolution, given that its current state x(t) is known to be in some set X (t). e uncertainty about the system's current state is due to measurement noise and actuation imperfections. Being able to ascertain, rigorously, bounds on the system state over [t, t + T ] despite current uncertainty allows the car to avoid unsafe plans. Calculating the system's reach set, however, can be computationally expensive and various techniques are proposed to deal with this issue, but very few have been explicitly aimed at real-time operation, or tested in a real-life situation. e F1/10 platform enables such testing of reachability so ware in a real-world setup, with the code running along with other loads on the target hardware. Runtime monitoring Good design practice requires the creation of runtime monitors, which are so ware functions that monitor key properties of the system's behavior in real-time, report any violations, and possibly enforce fail-safe behavior. Increased sophistication in the perception and control pipelines necessitates the monitoring of complex requirements, which range from enforcing safety and security properties to pa ern matching over sensor readings to help perception [2]. A promising direction is to generate these complex monitors automatically from their highlevel speci cation [4,6,13,17,18,44,52]. ese approaches have been implemented in standalone tools such as [3,5,31,41,51]. For robotic applications, it will be necessary to develop a framework that handles speci cations in a uni ed manner and generates efcient monitoring ROS nodes to be deployed quickly in robotic applications. Steps in this direction appear in ROSMOP 1 [31], and in REELAY 2 . e F1/10 platform is ideal for testing the generated monitors' e ciency. Its hardware architecture could guide low-level details of code generation and deployment over several processors. e distributed nature of ROS also raises questions in distributed monitoring. Finally, F1/10 competitions could be a proving ground for ease-of-use: based on practice laps, new conditions need to be monitored and the corresponding code needs to be created and deployed quickly before the next round. is would be the ultimate test of user-friendliness. Generating Adversarial Tra c Because F1/10 cars are reducedscale, cheaper and safer to operate than full-scale cars, they are a good option for testing new algorithms in tra c, where the F1/10 cars provide the tra c. E.g. if one has learned a dynamic model of tra c in a given area, as done in [33] then that same model can drive a eet of F1/10 cars, thus providing a convincing setup for testing new navigation algorithms. is eet of cars can also allow researchers to evaluate statistical claims of safety, since it can generate more data, cheaply, than a full-scale eet.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "FROM F1/10 TO FULL-SCALE AVS", "text": "e open source AV stack provided by the F1/10 project represents an excellent starting point to implement the perception-planningactuation pipeline of a full scale vehicle. Fig. 6 shows a vehicle prototype realized by the HiPeRT Lab of the University of Modena which extends the F1/10 stack with the required drivers and routines to process data from six (Sekonix) cameras, a 3D Lidar (Velodyne VLP-16) and a di erential GPS receiver. e primary controller is based on NVIDIA's Drive PX Autocruise platform, the 1 h ps://github.com/Formal-Systems-Laboratory/rosmop 2 h ps://github.com/doganulus/reelay Figure 6: Open source perception, planning and control pipeline of F1/10 platform has been successfully applied in the design of full-scale autonomous racecars at UNIMORE, Italy automotive-grade version of the Jetson TX2 board adopted in the F1/10 project. e car is able to automatically exit a parking lot, navigate autonomously in roundabouts and line-marked paths while avoiding detected obstacles, stop at tra c signals when required, and park itself in a user-de ned slot. e rst version of the HiPeRT autopilot utilzes a pure pursuit trajectory tracker developed within the F1/10 framework, and a model predictive controller for trajectory generation. e Gazebo simulator (and an alternative version based on Unity) have been adopted to test the HiPeRT autopilot before deploying it to the real car controller. e initial prototype, like F1/10, includes modules such as lane keep assist, DNNs for object detection, a basic obstacle avoidance planner, SLAM algorithms (HectorSLAM, Cartographer, GraphSLAM, particle lters, etc.), Camera/Lidar sensor fusion, and V2I communication support. In the current version of the car the ROS-based meta operating system is replaced by a stack with be er real-time guarantees. Nevertheless, the routines made available by the F1/10 project were instrumental in enabling the deployment of a working AV prototype in a very limited time. Additionally the F1/10 platform proved to be an ideal sandbox in which to incrementally iterate on the development of the various components.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F1/10 EDUCATION AND COMPEITIONS", "text": "e F1/10 platform is the basis of a full-semester class on autonomous driving at the University of Virginia, University of Pennsylvania and at Oregon State University. In addition, our open source course material has been taught at UT Austin (2016) and Clemson University (2017). e online videos from UVA have been viewed thousands of times since launch. e courses are o ered to graduate and advanced undergraduate students with backgrounds in any of the following: electrical engineering, mechanical engineering, robotics, embedded systems, and computer science. Our goal is to present an example of a class which teaches the entire stack for autonomous driving: from assembling the electrical components and sensors, to programming the car at two levels of complexity. Students who enroll in these courses will learn technologies that drive today's research AVs, and have the con dence to develop more sophisticated approaches to these problems on their own. Importantly, the students become familiar with the system as a whole, and encounter integration problems due to non-real-time performance, mechanical limitations, and sensor choices -this is why the students are divided into inter-disciplinary teams.\ne class is project-based, with the formal lectures introducing the techniques that students will code and implement on the car. In the rst half of the semester, the students are guided to the point where their car can navigate an environment with static obstacles. In the rst week they build the car and can control it manually. en they successively tackle LiDAR data processing with gap-nding, coordinate transformations, reference tracking, Electronic Speed Control, localization with scan matching, Simultaneous Localization and Mapping, and path planning. We also tackle the thorny question of moral decision-making for autonomous systems. For this, we assign readings from the humanities and sciences on topics like responsibility and moral agency, and have in-class guided discussions on the possibility of ethics for autonomous robots. e course culminates in a F1/10 'ba le of algorithms' race among the teams.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "e F1/10 Competition", "text": "Few things focus the mind and excite the spirit like a competition. In the early days of racing, competitors rst had to build their vehicles before they could race them. It was thus as much an engineering as a racing competition. We want to rekindle that competitive spirit. For the past three years, we have been organizing the F1/10 International Autonomous Racing Competition, the rst ever event of its kind. e inaugural race was held at the 2016 ES-Week in Pi sburgh, USA; followed by another race held during Cyber-Physical Systems (CPS) Week in April 2018, in Porto, Portugal. e third race was held at the 2018 ES-Week in October in Turin, Italy, Figure 7 [Right]). Every team builds the same baseline car, following the speci cations on f1tenth.org. From there, they have the freedom to deploy any algorithms they want to complete a loop around the track in the fastest time, and to complete the biggest number of laps in a xed duration. Future editions of the race will feature car-vs-car racing.\nSo far, teams from more than 12 universities have participated in the F1/10 competition, including teams from KAIST (Korea), KTH (Sweden), Czech Technical University, University of Connecticut, Seoul National University, University of New Mexico, Warsaw university of Technology, ETH Zurich, Arizona State University, and Da code (a Polish venture building company).", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "CONCLUSION AND DISCUSSION", "text": "e paper presents a new open-source and widely used 1/10 scale autonomous vehicle testbed called F1/10. All the instructions to build, drive, and race the F1/10 car are freely available on f1tenth.org. F1/10 uses a modular hardware and so ware design enabling researches to shape and use the platform to t their needs. e default con guration houses several sensors and a powerful on-board GPU -similar to a full scale car. e chassis of the F1/10 platform provides realistic dynamics so that researchers can test their algorithms on the 1/10 scale safely and cost-e ectively.\ne open-source ROS based so ware stack makes it very easy for beginners to get up to speed with autonomous driving behavior and build on existing capabilities. We show three representative examples of the kind of research that is easily enabled by the F1/10 platform -obstacle avoidance, land keep assist, and end-to-end autonomous driving. F1/10 has slowly become a popular instrument to make autonomy accessible and bring it to the classroom. Dozens of research groups have built their cars using instructions and videos available on the F1/10 web page. We also present highlights from the International F1/10 Autonomous Racing Competitions, which have been previously held at prominent CPS and Embedded Systems venues. e F1/10 autonomous platform is the building block and the vehicle for educating tomorrow's engineers on the interlocking concerns of performance, control, and safety for autonomous systems, and in particular for autonomous vehicles. It will also be the meeting point where a community of researchers from di erent backgrounds can develop their ideas in a way that emphasizes prototyping and realworld testing. As the community continues to grow, so does the range of possibilities of what we can discover and create.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Overview of the F1/10 research instrument -Modular chassis and system design with detailed instructions; open-source so ware stack in ROS; and a wide variety of AV research enabled in the lab on a real testbed.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Planing and control research enabled by the F1/10 platform: (Bo om Le ) Reactive Obstacle Avoidance, (Top Le ) End-to-End Driving, (Top Right) Model Predictive Control, (Bo om Right) V2V Collaboration", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Some perception research enabled by the F1/10 platform (clockwise, starting le ); (a) lane following using a monocular camera, (b) optical ow computation using Farenback's method and FlowNet 2.0 and, (c) localization and mapping", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: Figure (le ) shows an F1/10 car in a simulated environment generated using data from the real world, (right, top) real time scheduling of vanishing point algorithm on the F1/10 onboard computer and, (right, bottom) verifying tra c behavior", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 77Figure7: e F1/10 testbed instrument is enabling K-12, undergrad, and graduate outreach through our online courses and MOOCs, autonomous racing competitions, summer schools, and hackathaons.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "It includes CUDA enabled versions of PyTorch [36], Tensor ow [1], and Darknet [39]. We include example networks for semantic segmentation [30], object detection [40], and optical ow [23]; we focus on e cient variants of the state-of-the-art that can run at greater than 10 FPS on the TX2. Recently, it has come to light that many DNNs used on vision tasks are susceptible to so called adversarial examples, subtle perturbations of a few pixels which to the human eye are meaningless but when processed by a DNN result in gross errors in classi cation. Recent work has suggested that such adversarial examples are not invariant to viewpoint transformations [28], and hence not a concern. e F1/10 platform can help to enable principled investigations into how errors in DNN vision systems a ect vehicle level performance.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "p = [s a b c d] T", "formula_coordinates": [5.0, 168.89, 319.18, 53.93, 10.95]}, {"formula_id": "formula_1", "formula_text": "p i = [s a b c d] T us we now have a training set {(\u00ec x ,i , \u00ec p i )} M i=1 . A neural network N N T P is used to t the function x oal,i \u2192 \u00ec p i . Online, given an actual target state \u00ec x in relative coordinates, the AV computes N N T P (\u00ec x )", "formula_coordinates": [5.0, 53.47, 501.56, 240.58, 46.39]}], "doi": ""}