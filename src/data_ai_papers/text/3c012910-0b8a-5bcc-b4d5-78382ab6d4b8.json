{"title": "PaSca: a Graph Neural Architecture Search System under the Scalable Paradigm", "authors": "Wentao Zhang; Yu Shen; Zheyu Lin; Yang Li; Xiaosen Li; Wen Ouyang; Yangyu Tao; Zhi Yang; Bin Cui", "pub_date": "2022-03-01", "abstract": "Graph neural networks (GNNs) have achieved state-of-the-art performance in various graph-based tasks. However, as mainstream GNNs are designed based on the neural message passing mechanism, they do not scale well to data size and message passing steps. Although there has been an emerging interest in the design of scalable GNNs, current researches focus on specific GNN design, rather than the general design space, limiting the discovery of potential scalable GNN models. This paper proposes PaSca, a new paradigm and system that offers a principled approach to systemically construct and explore the design space for scalable GNNs, rather than studying individual designs. Through deconstructing the message passing mechanism, PaSca presents a novel Scalable Graph Neural Architecture Paradigm (SGAP), together with a general architecture design space consisting of 150k different designs. Following the paradigm, we implement an auto-search engine that can automatically search well-performing and scalable GNN architectures to balance the trade-off between multiple criteria (e.g., accuracy and efficiency) via multi-objective optimization. Empirical studies on ten benchmark datasets demonstrate that the representative instances (i.e., PaSca-V1, V2, and V3) discovered by our system achieve consistent performance among competitive baselines. Concretely, PaSca-V3 outperforms the state-of-the-art GNN method JK-Net by 0.4% in terms of predictive accuracy on our large industry dataset while achieving up to 28.3\u00d7 training speedups.", "sections": [{"heading": "INTRODUCTION", "text": "Graph neural networks (GNNs) [56] have become the state-of-theart methods in many graph representation learning scenarios such as node classification [8,31,32,61], link prediction [3,14,47,54], recommendation [17,34,53,58], and knowledge graphs [1,48,49,51]. Most GNN pipelines can be described in terms of the neural message passing (NMP) framework [15], which is based on the core idea of recursive neighborhood aggregation and transformation. Specifically, during each iteration, the representation of each node is updated (with neural networks) based on messages received from its neighbors. Since they typically need to perform a recursive neighborhood expansion to gather neural messages repeatedly, this process leads to an expensive neighborhood expansion, which grows exponentially with layers. The exponential growth of neighborhood size corresponds to an exponential IO overhead, which is the major challenge of large-scale GNN computation.\nTo scale up GNNs to web-scale graphs, recent work focuses on designing training frameworks with sampling approaches (e.g., DistDGL [66], NextDoor [19], SeaStar [55], FlexGraph [46], Dorylus [43], GNNAdvisor [50], etc.). Although distributed training is applied in these frameworks, they still suffer from high communication costs due to the recursive neighborhood aggregation during the training process. To demonstrate this issue, we utilize distributed training functions provided by DGL [2] to execute the train pipeline of GraphSAGE [15]. We partition the Reddit dataset across multiple machines and treat each GPU as a worker, and then calculate the speedup relative to the runtime of two workers. Figure 1 illustrates the training speedup along with the number of workers and the bottleneck in distributed settings. In particular, Figure 1(a) shows that the scalability of GraphSAGE is limited even when the mini-batch training and graph sampling method are adopted. Figure 1(b) further shows that the scalability is mainly bottlenecked by the aggregation procedure in which high data loading cost is incorporated to gather neighborhood information.\nDifferent from the recently developed GNN systems [43,50], we address the scalability challenges from an orthogonal perspective: re-designing the GNN pipeline to make the computing naturally scalable. To ensure scalability, we consider a different GNN training pipeline from most existing work: treating data aggregation over the graph as pre/post-processing stages that are separate from training. While there has been an emerging interest in specific architectural designs with decoupled pipeline [12,52,63,68], current researches  only focus on specific GNN instances, rather than the general design space, which limits the discovery of potential scalable GNN variants.\nIn addition, new architecture search systems are required to perform extensive exploration over the design space for scalable GNNs, which is also a major motivation to our work.\nTo the best of our knowledge, we propose the first paradigm and system -PaSca to explore the designs of scalable GNN, which makes the following contributions: Scalable Paradigm. We introduce the Scalable Graph Neural Architecture Paradigm (SGAP) with three operation abstractions: (1) graph_aggregator captures the structural information via graph aggregation operations, (2) message_aggregator combines different levels of structural information, and (3) message_updater generates the prediction based on the multi-scale features. Compared with the recently published scalable GNN systems, the SGAP interfaces in PaSca are motivated and implemented differently: (1) The APIs of GNN systems are used to express existing GNNs, whereas we propose a novel GNN pipeline abstraction to define the general design space for scalable GNN architectures. (2) The existing system contains two stages -sampling and training, where sampling is not a decoupled pre-processing stage and needs to be performed for each training iteration. By contrast, the SGAP paradigm considers propagation as pre/post-processing and does not require the expensive neighborhood expansion during training.\nDesign Space. Based on the proposed SGAP paradigm, we further propose a general design space consisting of 6 design dimensions, resulting in 150k possible designs of scalable GNN. We find that recently emerging scalable GNN models, such as SGC [52], SIGN [12], S 2 GC [68] and GBP [6] are special instances in our design space. Instead of simply generalizing existing specific GNN designs, we propose a design space with adaptive aggregation and a complementary post-processing stage beyond what is typically considered in the literature. The extension is motivated by the observation that previous GNNs (e.g., GCN [20] and SGC) suffer from model scalability issue, as shown in Figure 2. Here we use model scalability to describe its capability to cope with the large-scale neighborhood with increased aggregation step . We find the underlying reason is that their aggregation processes are restricted to a fixed-hop neighborhood and are insensitive to the actual demands of different nodes, which may lead to two limitations preventing them from unleashing their full potential: (1) long-range dependencies cannot be fully leveraged due to limited hops/layers, and (2) local information are lost due to the introduction of irrelevant nodes and unnecessary messages when the number of hops increases (i.e., over-smoothing issue [23,33,62]). Through extending design space with adaptive Auto-search Engines. We design and implement a search system that automates the search procedure for well-performing scalable GNN architectures to explore the proposed design space instead of the manual design. Our search system contains the following two engines: (1) Suggestion engine that implements a multi-objective search algorithm, which aims to find Pareto-optimal GNN instances given multiple criteria (e.g., predictive performance, inference time, resource consumption), allowing for a designer to select the best Pareto-optimal solution based on specific requirements; (2) Evaluation engine that evaluates the GNN instances from the search engine in a distributed manner. Due to the repetitive expansion in the training stage of GNNs, it is hard for existing GNN systems to scale to increasing workers. Based on the SGAP paradigm, the evaluation engine in PaSca involves the expensive neighborhood expansion only once in the pre/post-processing stages, and thus ensuring the scalability upon the number of training workers. To support the new pipeline, we implement two components: (1) the distributed graph data aggregator to pre/post-process data over graph structure, and (2) the distributed trainer where workers only need to exchange neural parameters.\nBased on our auto-search system PaSca, we discover new scalable GNN instances from the proposed design space for different accuracy-efficiency requirements. Extensive experiments on ten graph datasets demonstrate the superior training scalability/efficiency and performance of searched representatives given by PaSca among competitive baselines. Concretely, the representatives (i.e., PaSca-V2 and PaSca-V3) outperform the state-of-the-art JK-Net by 0.2% and 0.4% in predictive accuracy on our industry dataset, while achieving up to 56.6\u00d7 and 28.3\u00d7 training speedups, respectively.\nRelevance to Web. GNNs have recently been applied to a broad spectrum of web research such as social influence prediction [37,38], network role discovery [10,39], recommendation system [17,54,58], and fraud/spam detection [22,30]. However, scalability is a major challenge that precludes GNN-based methods in practical web-scale graphs. Moreover, manually designing the well-behaved GNNs for web tasks requires immense human expertise. To bridge this gap, we highlight the relevance of the proposed PaSca platform to GNN-based web research. First, PaSca provides easier support for experts in solving their web problems via scalable GNN paradigm. Domain experts only need to provide properly formatted datasets, and PaSca can automatically search suitable and scalable GNN designs to the web-scale graphs. So PaSca permits a transition from particular GNN instances to GNN design space, which offers exciting opportunities for scalable GNN architecture innovation. Second, PaSca provides the dis-aggregated execution pipeline for efficiently training and evaluating searched GNN models, without resorting to any approximation technique (e.g., graph sampling). Given these advancements in scalability and automaticity, our PaSca system enables practical and scalable GNN-based implementation for web-scale tasks, and thus significantly reducing the barrier when applying GNN models in web research.", "publication_ref": ["b55", "b7", "b30", "b31", "b60", "b2", "b13", "b46", "b53", "b16", "b33", "b52", "b57", "b0", "b47", "b48", "b50", "b14", "b65", "b18", "b54", "b45", "b42", "b49", "b1", "b14", "b42", "b49", "b11", "b51", "b62", "b67", "b0", "b0", "b51", "b11", "b67", "b5", "b19", "b22", "b32", "b61", "b36", "b37", "b9", "b38", "b16", "b53", "b57", "b21", "b29"], "figure_ref": ["fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "PRELIMINARY", "text": "GNN Pipelines. Considering a graph G = (V, E) with nodes V, edges E and features for all nodes x \u2208 R , \u2200 \u2208 V, Most GNNs can be formulated using the neural message passing framework, like GCN [20], GraphSAGE [15], GAT [44], and GraphSAINT [60], where each layer adopts one aggregation and an update function. At time step , a message vector m is computed with the representations of its neighbors N using an aggregate function, and m is then updated by a neural-network based update function:\nm \u2190 aggregate h \u22121 | \u2208 N , h \u2190 update(m ).(1)\nIn this way, messages are passed for time steps in a -layer GNN so that the steps of message passing correspond to the GNN depth.\nTaking the vanilla GCN [20] as an example, we have:\nGCN-aggregate h \u22121 | \u2208 N = \u2211\ufe01 \u2208N h \u22121 / \u221a\ufe03\u02dc\u02dc, GCN-update(m ) = ( m ),\nwhere\u02dcis the degree of node obtained from the adjacency matrix with self-connections\u02dc= + . Recently, some GNN variants adopt the decoupled neural message passing (DNMP) for better graph learning. More details can be found in Appendix A.2.\nScalable GNN Instances. Following SGC [52], a recent direction for scalable GNN is to remove the non-linearity between each layer in the forward aggregation, and models in this direction have achieved state-of-the-art performance in leaderboards of Open Graph Benchmark [18]. Concretely, SIGN [12] proposes to concatenate different iterations of aggregated feature messages, while S 2 GC [68] proposes a simple spectral graph convolution to average them. In addition, GBP [6] applies constants to weight aggregated feature messages of different layers. As current researches focus on studying specific architectural designs, we systematically study the architectural design space for scalable GNNs.\nGraph Neural Architecture Search. As a popular direction of AutoML [16,24,26], neural architecture search [11,41,65] has been proposed to solve the labor-intensive problem of neural architecture design. Auto-GNN [67] and GraphNAS [13] are early approaches that apply reinforcement learning with RNN controllers on a fixed search space. You et al. [59] define a similarity metric and search for the best transferable architectures across tasks via random search. Based on DARTS [29], GNAS [4] proposes the differentiable searching strategy to search for GNNs with optimal message-passing step. DSS [27] also adopts the differentiable strategy but optimizes over a dynamically updated search space. PaSca differs from these works in two aspects: (1) To pursue efficiency and scalability on large graphs, PaSca searches for scalable architectures under the novel Algorithm 1 Scalable graph neural architecture paradigm.\nInput: Graph G = (V, E), maximum aggregation steps for preprocessing and post-processing , , feature x .\nOutput: Prediction message m , \u2200 \u2208 V. ", "publication_ref": ["b19", "b14", "b43", "b59", "b19", "b51", "b17", "b11", "b67", "b5", "b15", "b23", "b25", "b10", "b40", "b64", "b66", "b12", "b58", "b28", "b3", "b26", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "PASCA ABSTRACTION", "text": "To address data and model scalability issues mentioned in Section 1, we propose a novel abstraction under which more scalable GNNs can be derived. Then we define the general design space for scalable GNNs based on the proposed abstraction.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SGAP Paradigm", "text": "Our PaSca system introduces a Scalable Graph Neural Architecture Paradigm (SGAP) for designing scalable GNNs. As shown in Algorithm 1, it consists of the following three decoupled stages:\nPre-processing. For each node , we range the step from 1 to , where is the maximum feature aggregation step. At each step , we use an operator, namely graph_aggregator, to aggregate the message vector collected from the neighbors N :\nm \u2190 graph_aggregator m \u22121 | \u2208 N ,(2)\nwhere m 0 = x . The messages are passed for steps in total during pre-processing, and m at step can gather the neighborhood information from nodes that are -hop away (lines 4-9). \nNote that, if the message_aggregator is not applied, the combined message vector c is set to the message of the last step m . We then use the message_updater to learn the class distribution of all nodes, i.e., the soft predictions (softmax outputs) predicted by the updater (line 13). Specifically, PaSca applies the Multi-layer Perceptron (MLP) as the updater, and we denote the depth of MLP as the transformation step . It learns node embedding h from the combined message vector c :\nh \u2190 message_updater(c ).(4)\nPost-processing. Motivated by Label Propagation [45] which aggregates the node labels, we regard the soft predictions as new features (line 16). Then, we use the graph_aggregator again at each step to aggregate the adjacent node predictions and make the final prediction (lines 17-21) as:\nm \u2190 graph_aggregator m \u22121 | \u2208 N ,(5)\nwhere m 0 = h is the original node prediction. We introduce SGAP to address both training and model scalability challenges. Specifically, it differs from the previous NMP and DNMP framework in terms of message type, message scale, and pipeline: (1) To perform the aggregate function for the next step, existing GNNs in NMP and DNMP update the hidden state h by applying the message vector m with neural networks. By contrast, SGAP allows passing node feature messages without applying graph_aggregator on the hidden states. As a result, this message passing procedure is independent of learnable model parameters and can be easily pre-computed, thus leading to high scalability and speedups. (2) Most GNNs in NMP and DNMP only utilizes the last message vector m to compute the final hidden state h . SGAP assumes that the optimal neighborhood expansion size should be different for each node , and thus we retain all the messages {m | \u2208 [1, ]} that a node receives over different steps (i.e., localities). The multi-scale messages are then aggregated per node into a single vector via message_aggregator, such that we could balance the preservation of information from both local and extended (multi-hop) neighborhoods for each node. (3) Besides feature aggregation, we propose a complementary post-processing stage to aggregate predictions (soft labels), which is not typically considered in the existing literature.", "publication_ref": ["b44"], "figure_ref": [], "table_ref": []}, {"heading": "Design Space under SGAP", "text": "Following the SGAP paradigm, we propose a general design space for scalable GNNs, as shown in Table 1. The design space contains three integer and three categorical parameters, which are responsible for the choice of aggregators and the steps of aggregation and transformation. Each configuration sampled from the search space represents a unique scalable architecture, resulting in 150k possible designs in total. One can also include more aggregators in the current space with future state-of-the-arts. In the following, we first introduce the aggregators used in our design space, and then explore interesting GNN instances in our defined space.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Graph Aggregators.", "text": "To capture the information of nodes that are several hops away, PaSca adopts a graph_aggregator to combine the nodes with their neighbors during each time step. Intuitively, it is unsuitable to use a fixed graph_aggregator for each task since the choice of graph aggregators depends on the graph structure and features. Thus PaSca provides three different graph aggregators to cope with different scenarios, and one could add more aggregators following the semantic of graph_aggregator.\nAugmented normalized adjacency (Aug. NA) [20]. It applies the random walk normalization on the augmented adjacency matrix = + , which is simple yet effective on a range of GNNs. The normalized graph_aggregator is:\nm = \u2211\ufe01 \u2208N 1 m \u22121 .(6)\nPersonalized PageRank (PPR). It focuses on its local neighborhood using a restart probability \u2208 (0, 1] and performs well on graphs with noisy connectivity. While the calculation of the fully personalized PageRank matrix is computationally expensive, we apply its approximate computation [21]:\nm = m 0 + (1 \u2212 ) \u2211\ufe01 \u2208N 1 \u221a\ufe03\u02dc\u02dcm \u22121 ,(7)\nwhere the restart probability allows to balance preserving locality (i.e., staying close to the root node to avoid over-smoothing) and leveraging the information from a large neighborhood.\nTriangle-induced adjacency (Triangle. IA) [35]. It accounts for the higher-order structures and helps distinguish strong and weak ties on complex graphs like social graphs. We assign each edge a weight representing the number of different triangles it belongs to, which forms a weight matrix . We denote as the degree of node from the weighted adjacency matrix . The aggregator is then calculated by applying a row-wise normalization:\nm = \u2211\ufe01 \u2208N 1 m \u22121 .(8)\n3.2.2 Message Aggregators. Before updating the hidden state of each node, PaSca proposes to apply a message_aggregator to combine messages obtained by graph_aggregator per node into a single vector, such that the subsequent model learns from the multiscale neighborhood of a given node. We summarize the different message aggregators PaSca as follows,\nNon-adpative aggregator. This type of aggregator does not consider the correlation between messages and the center node. The messages are directly concatenated or summed up with weights to obtain the combined message vector as,\n\u2190 \u2295 m \u2208 (m ),(9)\nwhere is a function used to reduce the dimension of message vectors, and \u2295 can be concatenating or pooling operators including average pooling or max pooling. Note that, for aggregator type \"Mean\", \"Max\" and \"Concatenate\", each weight is set to 1 for each message. For aggregator type \"Weighted\", we set the weight to constants following GBP [6]. Compared with pooling operators, though the concatenating operator keeps all the input message  information, the dimension of its outputs increases as grows, leading to additional computational cost in the downstream updater.\nAdpative aggregators. The messages of different hops make different contributions to the final performance. As shown in Figure 3, we apply GCN with different layers to conduct node classification on Citeseer. Note that the X-axis is the node id and Y-axis is the aggregation steps (number of layers in GCN). The color from white to blue represents the ratio of being predicted correctly in 50 different runs. We observe that most nodes are well classified with two steps, and as a result, most carefully designed GNN models are set with two layers (i.e., steps). In addition, the predictive accuracy on 13 of the 20 sampled nodes increases with a certain step larger than two. This motivates the design of node-adaptive aggregation functions, which determines the importance of a node's message at different ranges rather than fixing the same weights for all nodes.\nTo this end, we propose the gating aggregator, which generates retainment scores that indicate how much the corresponding messages should be retained in the final combined message.\nc msg \u2190 \u2211\ufe01 m \u2208 m , = (sm ),(10)\nwhere s is a trainable vector to generate gating scores, and is the sigmoid function. With the adaptive message_aggregator, the model can balance the messages from the multi-scale neighborhoods for each node at the expense of training extra parameters.", "publication_ref": ["b19", "b20", "b34", "b5"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "SGAP Instances", "text": "Recent scalable GNN models, such as SGC [52], SIGN [12], S 2 GC [68] and GBP [6], can be considered as specific instances in our defined design space, as shown in Table 2. We see that most current scalable GNNs ignore the post-processing stages, which can boost the model performance demonstrated by our experiments.\nBesides, various scalable GNNs can be obtained by using different design choices under SGAP. For example, the current state-of-theart scalable model GBP sets the graph_aggregator as Aug.NA and uses the weighted strategy in the message_aggregator. We decouple MLP training and information propagation in APPNP [21] into two individual processes and get a new scalable model PaSca-APPNP. To effectively explore the large design space, we implement an auto-search system engine below to automate the search procedure of scalable GNN architecture instead of manual design.", "publication_ref": ["b51", "b11", "b67", "b5", "b20"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "PASCA ENGINES", "text": "Figure 4 shows the overview of our proposed auto-search system to explore GNN designs under PaSca abstraction. It consists of two engines: the search engine and the evaluation engine. The search engine includes the proposed designed search space for scalable GNNs and implements a suggestion server that is responsible for suggesting architectures to evaluate. The evaluation engine receives an instance and trains the corresponding architecture in a distributed fashion. An iteration of the searching process is as follows: 1) The suggestion server samples an architecture instance based on its built-in optimization algorithm and sends it to the evaluation engine; 2) The evaluation engine evaluates the architecture and updates the suggestion server with its observed performance.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Search Engine", "text": "While prior researches on scalable GNNs [6,12] focus on optimizing the classification error, recent applications not only require high predictive performance but also low resource-consumption, e.g. model size or inference time. In addition, there is typically an implicit trade-off between predictive performance and resource consumption. To this end, the suggestion server implements a multi-objective search algorithm to tackle this trade-off. Concretely, we use the Bayesian optimization based on EHVI [9], a widely-used algorithm that maximizes the predicted improvement of hypervolume indicator of Pareto-optimal points relative to a given reference point. The suggestion server then optimizes over the search space following a typical Bayesian optimization as 1) Based on the observation history, the server trains multiple surrogates, namely the Gaussian Process, to model the relationships between each architecture instance and its objective values; 2) The server randomly samples a number of new instances, and suggests the best one which maximizes the EHVI based on the predicted outputs of trained surrogates; 3) The server receives the results of the suggested instance and updates its observation history.  ", "publication_ref": ["b5", "b11", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation Engine", "text": "Different from the sampling-training process of existing GNN systems (e.g., DistDGL [66], NextDoor [19], and FlexGraph [46]), the process of PaSca evaluation engine is decoupled into pre-processing, training and post-processing as illustrated in Figure 4: PaSca first pre-computes the feature messages for each node over the graph, and then it combines the messages and trains the model parameters with parameter sever. Finally, PaSca post-computes the prediction messages for each node over the graph. All the messages are partitioned and stored in a distributed storage system, and the stages can be implemented in a distributed fashion. Specifically, the engine consists of the following two components:\nGraph Data Aggregator. This component handles pre-processing and post-processing stages on data aggregation over graph structure. The two stages share the same pipeline but take different messages as inputs (features for pre-processing and predictions for post-processing). We implement an efficient batch processing pipeline over distributed graph storage: The nodes are partitioned into batches, and the computation of each batch is implemented by workers in parallel with matrix multiplication. As shown in Figure 4, for each node in a batch, we firstly pull all the -th step messages of its 1-hop neighbors from the message distributed storage and then compute the ( + 1)-th step messages of the batch in parallel.\nNext, We push these aggregated messages back for reuse in the calculation of the ( + 2)-th step messages. In our implementation, we treat GPUs as workers for fast processing, and the graph data are partitioned and stored on host memory across machines. Given the parallel message computation, our implementation could scale to large graphs and significantly reduce the runtime.\nNeural Architecture Trainer. This component handles the training of neural networks. We optimize the parameters of each architecture with distributed SGD. The model parameters are stored on a parameter server, and multiple workers (GPUs) process the data in parallel. We adopt asynchronous training to avoid the communication overhead between workers. Each worker fetches the most up-to-date parameters and computes the gradients for a mini-batch of data, independent of the other workers.", "publication_ref": ["b65", "b18", "b45"], "figure_ref": ["fig_5", "fig_5"], "table_ref": []}, {"heading": "EXPERIMENTS 5.1 Experimental Settings", "text": "Datasets. We conduct the experiments on three citation networks (Citeseer, Cora, and PubMed) in [20], two social networks (Flickr and Reddit) in [60], four co-authorship graphs (Amazon and Coauthor) in [36], the co-purchasing network (ogbn-products) in [18] and one short-form video recommendation graph (Industry) from our industrial cooperative enterprise. Table 6 in Appendix A.1 provides the overview of the used graph datasets.\nParameters and Environment. To eliminate random factors, we run each method 20 times and report the mean and variance of the performance. More details for experimental setups and reproduction are provided in Appendix A.4 and A.5.\nBaselines. In the transductive settings, we compare the searched scalable GNNs with GCN [20], GAT [44], JK-Net [57], Res-GCN [20], APPNP [21], AP-GCN [42], SGC [52], SIGN [12], S 2 GC [68] and GBP [6], which are SOTA models of different message passing types. In the inductive settings, the compared baselines are Graph-SAGE [15], FastGCN [5], ClusterGCN [7] and GraphSAINT [60]. More descriptions about the baselines are provided in Appendix A.3.\nIn the following, we first analyze the superiority of representative instances searched by PaSca. Then we evaluate the transferability, training efficiency, and model scalability of PaSca representatives compared with competitive state-of-the-art baselines.", "publication_ref": ["b19", "b59", "b35", "b17", "b19", "b43", "b56", "b19", "b20", "b41", "b51", "b11", "b67", "b5", "b14", "b4", "b6", "b59"], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "Searched Representatives", "text": "We apply the multi-objective optimization targeting at classification error and inference time on Cora. Figure 6 demonstrates the Pareto Front found by PaSca with a budget of 2000 evaluations, together with the results of several manually designed scalable GNNs. The inference time has been normalized based on instances with the minimum and maximum inference time in our design space. Interestingly, we observe that GBP and PaSca-APPNP, our extended variant of APPNP (see Table 2), falls on the Pareto Front, which indicates the superior design of the \"Weighted\" message_aggregator and \"PPR\" graph_aggregator. We also choose other three instances from the Pareto Front as PaSca-V1 to V3 with different accuracyefficiency requirements as searched representatives of SGAP for the following evaluations. The corresponding parameters of each architecture are shown in Table 3. Among the three architectures, PaSca-V1 Pareto-dominates the other baselines except GBP, and   PaSca-V3 is the architecture with the best predictive performance found by the search engine.", "publication_ref": [], "figure_ref": ["fig_8"], "table_ref": ["tab_2", "tab_4"]}, {"heading": "Training Scalability", "text": "The main characteristic of our proposed design space is that the architectures sampled from the space, namely PaSca-SGAP, share high scalability upon workers. To examine the scalability of PaSca-SGAP, we choose PaSca-APPNP as a representative and compare it with GraphSAGE, a widely-used method in industry on two largescale datasets. We train GraphSAGE with DGL and PaSca-APPNP with the evaluation engine of PaSca, respectively. We train both methods in stand-alone and distributed scenarios and then measure their corresponding speedups. The batch size is 8192 for Reddit and 16384 for ogbn-product, and the speedup is calculated by runtime per epoch relative to that of one worker in the stand-alone scenario and two workers in the distributed scenario. Without considering extra cost, the speedup will increase linearly in an ideal condition.\nThe corresponding results are shown in Figure 5. Since Graph-SAGE requires aggregating the neighborhood nodes during training, GraphSAGE trained with DGL meets the I/O bottleneck when transmitting a large number of required neural messages. Thus, the speedup of GraphSAGE training increases slowly as the number of workers grows, which is less than 2\u00d7 even with four workers in the stand-alone scenario and eight workers in the distributed scenario. Recall that the only communication cost of PaSca-SGAP is to synchronize parameters among different workers, which is essential to all distributed training methods. As a result, PaSca-SGAP trained with the evaluation engine scales up close to the ideal circumstance in both scenarios, indicating the superiority of PaSca.  ", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "Performance-Efficiency Analysis", "text": "To test the transferability and training efficiency of PaSca models, we further evaluate them on more datasets compared with competitive baselines. The results are summarized in Table 4 and 5.\nWe observe that PaSca models obtain quite competitive performance in both transductive and inductive settings. In transductive settings, our simplified variant PaSca-V1 also achieves the best performance among Non-SGAP baselines on most datasets, which shows the superiority of SGAP design. In addition, PaSca-V2 and V3 outperform the best baseline GBP by a margin of 0.1%\u223c0.6% and 0.2%\u223c1.3% on each dataset. We attribute this improvement to the application of the adaptive message_aggregator. In inductive settings, Table 5 shows that PaSca-V3 outperforms the best baseline GraphSAINT by a margin of 1.1% on Flickr and 0.1% on Reddit.\nWe also evaluate the training efficiency of each method in the real production environment. Figure 7 illustrates the performance over training time on Industry. In particular, we pre-compute the feature messages of each scalable method, and the training time takes into account the pre-computation time. We observe that NMP architectures require at least a magnitude of training time than PaSca-SGAP. Among considered baselines, PaSca-V3 achieves the best performance with 4\u00d7 training time compared with GBP and PaSca-V1. Note that, though PaSca-V1 requires the same training time as GBP, its inference time is less than GBP. Therefore, we recommend choosing PaSca-V1 to V3, along with GBP, according to different requirements of predictive performance, training efficiency, and inference time.", "publication_ref": [], "figure_ref": ["fig_9"], "table_ref": ["tab_6"]}, {"heading": "Model Scalability", "text": "We observe that both PaSca-V2 and V3 found by the search engine contain the \"Adaptive\" message_aggregator. In this subsection, we aim to explain the advantage of adaptive message_aggregator in the perspective of model scalability on message passing steps.\nWe plot the changes of model performance along with the message passing steps in the left subfigure of Figure 8. For a fair comparison, we use PaSca-V2 which does not include post-processing. The vanilla GCN gets the best results with two aggregation steps, but its performance drops rapidly along with the increased steps due to the over-smoothing issue. Both Res-GCN and SGC show better performance than GCN with larger aggregation steps. Take Res-GCN as an example, it carries information from the previous step by introducing the residual connections and thus alleviates this problem. However, these two methods cannot benefit from deep GNN architecture since they are unable to balance the needs of preserving locality (i.e., staying close to the root node to avoid over-smoothing) and leveraging the information from a large neighborhood. In contrast, PaSca-V2 achieves consistent improvement and remains nondecreasing across steps, which indicates that PaSca can scales to large depth. The reason is that the adaptive message_aggregator in PaSca-V2 is able to adaptively and effectively combine multiscale neighborhood messages for each node.\nTo demonstrate this, the right subfigure of Figure 8 shows PaSca-V2's average gating weights of feature messages according to the number of steps and degrees of input nodes, where the maximum step is 6. In this experiment, we randomly select 20 nodes for each degree range (1-4, 5-8, 9-12) and plot the relative weight based on the maximum value. We get two observations from the heat map: 1) The 1-step and 2-step graph messages are always of great importance, which shows that the adaptive message_aggregator captures the local information as those widely used 2-layer GNNs do; 2) The weights of graph messages with larger steps drop faster as the degree grows, which indicates that the attention-based aggregator could prevent high-degree nodes from including excessive irrelevant nodes which lead to over-smoothing. From the two observations, we conclude that the adaptive message_aggregator can identify the different message-passing demands of nodes and explicitly weight each graph message.", "publication_ref": [], "figure_ref": ["fig_10", "fig_10"], "table_ref": []}, {"heading": "CONCLUSION", "text": "In this paper, we proposed PaSca, a new auto-search system that offers a principled approach to systemically construct and explore the design space for scalable GNNs, rather than studying individual designs. Experiments on ten real-world benchmarks demonstrate that the representative instances searched by PaSca outperform SOTA GNNs in terms of performance, efficiency, and scalability. PaSca can help researchers understand design choices when developing new scalable GNN models, and serve as a system to support extensive exploration over the design space for scalable GNNs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A APPENDIX A.1 Dataset description", "text": "Cora, Citeseer, and Pubmed 1 are three well-known citation network datasets, and we follow the same training/validation/test split as GCN [20].\nReddit is a social network modeling the community structure of Reddit posts. This dataset is often used for inductive training, and the training/validation/test split is coherent with that of Graph-SAGE [15].\nFlickr originates from NUS-wide 2 and contains different types of images based on the descriptions and common properties of online images. We use a public version of Reddit and Flickr provided by GraphSAINT 3 .\nAmazon Computers and Amazon Photo are segments of the Amazon co-purchase graph [40], where nodes represent goods, edges indicate that two goods are frequently bought together, node features are bag-of-words encoded product reviews, and class labels are given by the product category.\nCoauthor CS and Coauthor Physics are co-authorship graph based on the Microsoft Academic Graph from the KDD Cup 2016 challenge 4 . Here, nodes are authors, that are connected by an edge if they co-authored a paper; node features represent paper keywords for each author's papers, and class labels indicate the most active fields of study for each author. We use a pre-divided version of these datasets through the Deep Graph Library (DGL) 5 .\nogbn-products is an unweighted graph representing an Amazon product co-purchase network. Each node represents a product sold on Amazon, and edges between two products indicate that the products are purchased together. We use the public data split for this dataset as in Open Graph Benchmark 6 .\nIndustry is a user-video graph collected from a real-world mobile application from our industry partner. We sampled 1,000,000 users and videos from the app, and treat these items as nodes. The edges in the generated bipartite graph represent that the user clicks the short videos. Each user has 64 features, and the target is to category these short videos into 253 different classes.", "publication_ref": ["b19", "b14", "b2", "b39", "b3", "b4", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Decoupled Neural Message Passing", "text": "Note that the aggregate and update operations are inherently intertwined in Equation (1), i.e., each aggregate operation requires a neural layer to update the node's hidden state in order to generate a new message for the next step. Recently, some researches show that such entanglement could compromise performance on a range of benchmark tasks [12,52,64], and suggest separating GCN from the aggregation scheme. We reformulate these models into a single Decoupled Neural Message Passing (DNMP) framework: Neural prediction messages are first generated (with update function) for each node utilizing only that node's own features, and then aggregated using aggregate function.  [21], APPNP [21], AP-GCN [42] and etc., follows this decoupled MP. Taking APPNP as an example:\nh 0 \u2190 update(x ), h \u2190 aggregate h \u22121 | \u2208 N . (11\nAPPNP-update(x v ) = ( x ), APPNP-aggregate h \u22121 | \u2208 N = h 0 + (1 \u2212 ) \u2211\ufe01 \u2208N h \u22121 \u221a\ufe03\u02dc\u02dc,\nwhere aggregate function adopts personalized PageRank with the restart probability \u2208 (0, 1] controlling the locality.", "publication_ref": ["b11", "b51", "b63", "b20", "b20", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "A.3 More details about the compared baselines", "text": "The main characteristic of all baselines are listed as follows:\n\u2022 GCN [20] produces node embedding vectors by truncating the Chebyshev polynomial to the first-order neighborhoods.\n\u2022 ResGCN [20] adopts the residual connections between hidden layers to facilitate the training of deeper models by enabling the model to carry over information from the previous layer's input. \u2022 JK-Net [57] proposes a new aggregation scheme for node representation learning that can adapt neighborhood ranges to nodes individually. \u2022 APPNP [21] uses the relationship between GCN and PageRank to derive an improved propagation scheme based on personalized PageRank. \u2022 AP-GCN [42] is a variation of GCN wherein each node selects automatically the number of propagation steps performed across the graph. \u2022 SGC [52] reduces the excess complexity of GCN through successively removing non-linearities and collapsing weight matrices between consecutive layers. \u2022 SIGN [12] is a sampling-free Graph Neural Network model that is able to easily scale to gigantic graphs while retaining enough expressive power. \u2022 GraphSAGE [15] is an inductive framework that leverages node attribute information to efficiently generate representations on previously unseen data.\n\u2022 GAT [28] leverages masked self-attentional layers to address the shortcomings of prior GNNs based on graph convolutions or their approximations, and enables specifying different weights to different nodes in a neighborhood. \u2022 S 2 GC [68]: S 2 GC uses a modified Markov Diffusion Kernel to derive a variant of GCN, and it can be used as a trade-off of low-pass and high-pass filter which captures the global and local contexts of each node. \u2022 FastGCN [5] interprets graph convolutions as integral transforms of embedding functions under probability measures, and enhances GCN with importance sampling. \u2022 ClusterGCN [7] designs the batches based on efficient graph clustering algorithms, and it proposes a stochastic multiclustering framework to improve the convergence. \u2022 GBP [6]: GBP utilizes a localized bidirectional propagation process to further improve SGC. ", "publication_ref": ["b19", "b19", "b56", "b20", "b41", "b51", "b11", "b14", "b27", "b4", "b6", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "A.4 Experiments setup", "text": "We use PyTorch 7 and DGL to implement the models, and we train them using Adam optimizer. To evaluate the scalability of Graph-SAGE, we implement GraphSAGE via DistDGL. Besides, we train each model 400 epochs and terminate the training process if the validation accuracy does not improve for 20 consecutive steps. Note that both GraphSAGE and JKNet have three aggregators, and we choose the concatenation and mean as their aggregator, respectively, since these two aggregators perform best in most datasets.\nFor GAT, the number of attention heads is fixed to 8. For Graph-SAGE, we use the results on Flickr and Reddit as reported in [15] and [60]. For ClusterGCN, we use the results on Reddit as reported in [7] and run our own implementation on Flickr. The hyperparameters are selected from random search. The random search was performed over the following search space: hidden size \u2208 {8, 16, 32, 64, 128, 256, 512}, learning rate \u2208 {1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 2e-1}, dropout rate \u2208 {0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9]}, regularization strength \u2208 {1e-4, 5e-4,1e-3, 5e-3, 1e-2, 5e-2, 1e-1}. Note that both Res-GCN and JK-Net will degrade into GCN if they have only two layers, so we set their aggregation steps \u2208 [3,20] in all datasets.", "publication_ref": ["b6", "b14", "b59", "b6", "b2", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "A.5 Experiment Environment and Reproduction Instructions", "text": "The experiments are implemented on 4 machines with 14 Intel(R) Xeon(R) CPUs (Gold 5120 @ 2.20GHz) and four NVIDIA TITAN RTX GPUs. The code is written in Python 3.6, and the multi-objective algorithm is implemented based on OpenBox [25]. We use Pytorch 1.7.1 on CUDA 10.1 to train the model on GPU.", "publication_ref": ["b24"], "figure_ref": [], "table_ref": []}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is supported by NSFC (No. 61832001, 61972004), Beijing Academy of Artificial Intelligence (BAAI), PKU-Baidu Fund 2019BD006, and PKU-Tencent Joint Research Lab. Zhi Yang and Bin Cui are the corresponding authors.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "RECON: relation extraction using knowledge graph context in a graph neural network", "journal": "", "year": "2021", "authors": "Anson Bastos; Abhishek Nadgeri; Kuldeep Singh; Isaiah Onando Mulang; Saeedeh Shekarpour; Johannes Hoffart; Manohar Kaul"}, {"ref_id": "b1", "title": "Geometric deep learning: going beyond euclidean data", "journal": "IEEE Signal Processing Magazine", "year": "2017", "authors": "Joan Michael M Bronstein; Yann Bruna; Arthur Lecun; Pierre Szlam;  Vandergheynst"}, {"ref_id": "b2", "title": "A multi-scale approach for graph link prediction", "journal": "", "year": "2020", "authors": "Lei Cai; Shuiwang Ji"}, {"ref_id": "b3", "title": "Rethinking graph neural architecture search from message-passing", "journal": "", "year": "2021", "authors": "Shaofei Cai; Liang Li; Jincan Deng; Beichen Zhang; Zheng-Jun Zha; Li Su; Qingming Huang"}, {"ref_id": "b4", "title": "FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling", "journal": "", "year": "2018", "authors": "Jie Chen; Tengfei Ma; Cao Xiao"}, {"ref_id": "b5", "title": "Scalable graph neural networks via bidirectional propagation", "journal": "Advances in neural information processing systems", "year": "2020", "authors": "Ming Chen; Zhewei Wei; Bolin Ding; Yaliang Li; Ye Yuan; Xiaoyong Du; Ji-Rong Wen"}, {"ref_id": "b6", "title": "Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks", "journal": "", "year": "2019", "authors": "Wei-Lin Chiang; Xuanqing Liu; Si Si; Yang Li; Samy Bengio; Cho-Jui Hsieh"}, {"ref_id": "b7", "title": "On the equivalence of decoupled graph convolution network and label propagation", "journal": "", "year": "2021", "authors": "Hande Dong; Jiawei Chen; Fuli Feng; Xiangnan He; Shuxian Bi; Zhaolin Ding; Peng Cui"}, {"ref_id": "b8", "title": "Single-and multi-objective evolutionary design optimization assisted by gaussian random field metamodels", "journal": "", "year": "2005", "authors": "Michael Emmerich"}, {"ref_id": "b9", "title": "Learning to Identify High Betweenness Centrality Nodes from Scratch: A Novel Graph Neural Network Approach", "journal": "", "year": "2019-11-03", "authors": "Changjun Fan; Li Zeng; Yuhui Ding; Muhao Chen; Yizhou Sun; Zhong Liu"}, {"ref_id": "b10", "title": "EAT-NAS: Elastic architecture transfer for accelerating large-scale neural architecture search", "journal": "Science China Information Sciences", "year": "2021", "authors": "Jiemin Fang; Yukang Chen; Xinbang Zhang; Qian Zhang; Chang Huang; Gaofeng Meng; Wenyu Liu; Xinggang Wang"}, {"ref_id": "b11", "title": "SIGN: Scalable Inception Graph Neural Networks", "journal": "", "year": "2020", "authors": "Fabrizio Frasca; Emanuele Rossi; Davide Eynard; Benjamin Chamberlain; Michael Bronstein; Federico Monti"}, {"ref_id": "b12", "title": "Graph Neural Architecture Search", "journal": "", "year": "2020", "authors": "Yang Gao; Hong Yang; Peng Zhang; Chuan Zhou; Yue Hu"}, {"ref_id": "b13", "title": "Syntax-guided text generation via graph neural network", "journal": "Sci. China Inf. Sci", "year": "2021", "authors": "Qipeng Guo; Xipeng Qiu; Xiangyang Xue; Zheng Zhang"}, {"ref_id": "b14", "title": "Inductive representation learning on large graphs. Advances in neural information processing systems", "journal": "", "year": "2017", "authors": "Will Hamilton; Zhitao Ying; Jure Leskovec"}, {"ref_id": "b15", "title": "AutoML: A survey of the stateof-the-art", "journal": "Knowledge-Based Systems", "year": "2021", "authors": "Xin He; Kaiyong Zhao; Xiaowen Chu"}, {"ref_id": "b16", "title": "RetaGNN: Relational temporal attentive graph neural networks for holistic sequential recommendation", "journal": "", "year": "2021", "authors": "Cheng Hsu; Cheng-Te Li"}, {"ref_id": "b17", "title": "Open graph benchmark: Datasets for machine learning on graphs", "journal": "Advances in neural information processing systems", "year": "2020", "authors": "Weihua Hu; Matthias Fey; Marinka Zitnik; Yuxiao Dong; Hongyu Ren; Bowen Liu; Michele Catasta; Jure Leskovec"}, {"ref_id": "b18", "title": "Accelerating graph sampling for graph machine learning using GPUs", "journal": "", "year": "2021", "authors": "Abhinav Jangda; Sandeep Polisetty; Arjun Guha; Marco Serafini"}, {"ref_id": "b19", "title": "Semi-Supervised Classification with Graph Convolutional Networks", "journal": "", "year": "2017", "authors": "N Thomas; Max Kipf;  Welling"}, {"ref_id": "b20", "title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "journal": "", "year": "2019", "authors": "Johannes Klicpera; Aleksandar Bojchevski; Stephan G\u00fcnnemann"}, {"ref_id": "b21", "title": "Spam Review Detection with Graph Convolutional Networks", "journal": "", "year": "2019-11-03", "authors": "Ao Li; Runshi Zhou Qin; Yiqun Liu; Dong Yang;  Li"}, {"ref_id": "b22", "title": "Deeper insights into graph convolutional networks for semi-supervised learning", "journal": "", "year": "2018", "authors": "Qimai Li; Zhichao Han; Xiao-Ming Wu"}, {"ref_id": "b23", "title": "MFES-HB: Efficient Hyperband with Multi-Fidelity Quality Measurements", "journal": "AAAI Press", "year": "2021", "authors": "Yang Li; Yu Shen; Jiawei Jiang; Jinyang Gao; Ce Zhang; Bin Cui"}, {"ref_id": "b24", "title": "Openbox: A generalized black-box optimization service", "journal": "", "year": "2021", "authors": "Yang Li; Yu Shen; Wentao Zhang; Yuanwei Chen; Huaijun Jiang; Mingchao Liu; Jiawei Jiang; Jinyang Gao; Wentao Wu; Zhi Yang"}, {"ref_id": "b25", "title": "VolcanoML: speeding up end-to-end AutoML via scalable search space decomposition", "journal": "Proceedings of the VLDB Endowment", "year": "2021", "authors": "Yang Li; Yu Shen; Wentao Zhang; Jiawei Jiang; Bolin Ding; Yaliang Li; Jingren Zhou; Zhi Yang; Wentao Wu; Ce Zhang"}, {"ref_id": "b26", "title": "One-shot graph neural architecture search with dynamic search space", "journal": "", "year": "2021", "authors": "Yanxi Li; Zean Wen; Yunhe Wang; Chang Xu"}, {"ref_id": "b27", "title": "Graph Partition Neural Networks for Semi-Supervised Classification", "journal": "", "year": "2018", "authors": "Renjie Liao; Marc Brockschmidt; Daniel Tarlow; Alexander Gaunt; Raquel Urtasun; Richard Zemel"}, {"ref_id": "b28", "title": "DARTS: Differentiable Architecture Search", "journal": "", "year": "2019", "authors": "Hanxiao Liu; Karen Simonyan; Yiming Yang"}, {"ref_id": "b29", "title": "Pick and choose: a GNN-based imbalanced learning approach for fraud detection", "journal": "", "year": "2021", "authors": "Yang Liu; Xiang Ao; Zidi Qin; Jianfeng Chi; Jinghua Feng; Hao Yang; Qing He"}, {"ref_id": "b30", "title": "Improving Graph Neural Networks with Structural Adaptive Receptive Fields", "journal": "", "year": "2021", "authors": "Xiaojun Ma; Junshan Wang; Hanyue Chen; Guojie Song"}, {"ref_id": "b31", "title": "Degnn: Improving graph neural networks with graph decomposition", "journal": "", "year": "2021", "authors": "Xupeng Miao; Wentao Nezihe Merve G\u00fcrel; Zhichao Zhang; Bo Han; Wei Li; Susie Xi Min; Hansheng Rao; Yinan Ren; Yingxia Shan;  Shao"}, {"ref_id": "b32", "title": "Lasagne: A multi-layer graph convolutional network framework via node-aware deep architecture", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2021", "authors": "Xupeng Miao; Wentao Zhang; Yingxia Shao; Bin Cui; Lei Chen; Ce Zhang; Jiawei Jiang"}, {"ref_id": "b33", "title": "Geometric matrix completion with recurrent multi-graph neural networks. Advances in neural information processing systems", "journal": "", "year": "2017", "authors": "Federico Monti; Michael Bronstein; Xavier Bresson"}, {"ref_id": "b34", "title": "Motifnet: a motifbased graph convolutional network for directed graphs", "journal": "IEEE Data Science Workshop", "year": "2018", "authors": "Federico Monti; Karl Otness; Michael M Bronstein"}, {"ref_id": "b35", "title": "Geom-GCN: Geometric Graph Convolutional Networks", "journal": "", "year": "2020", "authors": "Hongbin Pei; Bingzhe Wei; Kevin Chen-Chuan; Yu Chang; Bo Lei;  Yang"}, {"ref_id": "b36", "title": "Predicting Customer Value with Social Relationships via Motif-based Graph Attention Networks", "journal": "", "year": "2021-04-19", "authors": "Jinghua Piao; Guozhen Zhang; Fengli Xu; Zhilong Chen; Yong Li"}, {"ref_id": "b37", "title": "Deepinf: Social influence prediction with deep learning", "journal": "", "year": "2018", "authors": "Jiezhong Qiu; Jian Tang; Hao Ma; Yuxiao Dong; Kuansan Wang; Jie Tang"}, {"ref_id": "b38", "title": "Pathfinder Discovery Networks for Neural Message Passing", "journal": "", "year": "2021", "authors": "Peter Benedek Rozemberczki; Amol Englert; Martin Kapoor; Bryan Blais;  Perozzi"}, {"ref_id": "b39", "title": "Pitfalls of graph neural network evaluation", "journal": "", "year": "2018", "authors": "Oleksandr Shchur; Maximilian Mumme; Aleksandar Bojchevski; Stephan G\u00fcnnemann"}, {"ref_id": "b40", "title": "ProxyBO: Accelerating Neural Architecture Search via Bayesian Optimization with Zero-cost Proxies", "journal": "", "year": "2021", "authors": "Yu Shen; Yang Li; Jian Zheng; Wentao Zhang; Peng Yao; Jixiang Li; Sen Yang; Ji Liu; Cui Bin"}, {"ref_id": "b41", "title": "Adaptive propagation graph convolutional network", "journal": "IEEE Transactions on Neural Networks and Learning Systems", "year": "2020", "authors": "Indro Spinelli; Simone Scardapane; Aurelio Uncini"}, {"ref_id": "b42", "title": "Dorylus: Affordable, Scalable, and Accurate {GNN} Training with Distributed {CPU} Servers and Serverless Threads", "journal": "", "year": "2021", "authors": "John Thorpe; Yifan Qiao; Jonathan Eyolfson; Shen Teng; Guanzhou Hu; Zhihao Jia; Jinliang Wei; Keval Vora; Ravi Netravali; Miryung Kim"}, {"ref_id": "b43", "title": "Graph Attention Networks", "journal": "", "year": "2018", "authors": "Petar Veli\u010dkovi\u0107; Guillem Cucurull; Arantxa Casanova; Adriana Romero; Pietro Li\u00f2; Yoshua Bengio"}, {"ref_id": "b44", "title": "Label propagation through linear neighborhoods", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2007", "authors": "Fei Wang; Changshui Zhang"}, {"ref_id": "b45", "title": "FlexGraph: a flexible and efficient distributed framework for GNN training", "journal": "", "year": "2021", "authors": "Lei Wang; Qiang Yin; Chao Tian; Jianbang Yang; Rong Chen; Wenyuan Yu; Zihang Yao; Jingren Zhou"}, {"ref_id": "b46", "title": "Graph structure estimation neural networks", "journal": "", "year": "2021", "authors": "Ruijia Wang; Shuai Mou; Xiao Wang; Wanpeng Xiao; Qi Ju; Chuan Shi; Xing Xie"}, {"ref_id": "b47", "title": "Mixed-Curvature Multi-Relational Graph Neural Network for Knowledge Graph Completion", "journal": "", "year": "2021-04-19", "authors": "Shen Wang; Xiaokai Wei; C\u00edcero Nogueira; Zhiguo Santos; Ramesh Wang; Andrew O Nallapati; Bing Arnold; Philip S Xiang; Isabel F Yu;  Cruz"}, {"ref_id": "b48", "title": "Zero-shot recognition via semantic embeddings and knowledge graphs", "journal": "", "year": "2018", "authors": "Xiaolong Wang; Yufei Ye; Abhinav Gupta"}, {"ref_id": "b49", "title": "GNNAdvisor: An Adaptive and Efficient Runtime System for GNN Acceleration on GPUs", "journal": "", "year": "2021", "authors": "Yuke Wang; Boyuan Feng; Gushu Li; Shuangchen Li; Lei Deng; Yuan Xie; Yufei Ding"}, {"ref_id": "b50", "title": "Deep reasoning with knowledge graph for social relationship understanding", "journal": "", "year": "2018", "authors": "Zhouxia Wang; Tianshui Chen; Jimmy Ren; Weihao Yu; Hui Cheng; Liang Lin"}, {"ref_id": "b51", "title": "Simplifying graph convolutional networks", "journal": "", "year": "2019", "authors": "Felix Wu; Amauri Souza; Tianyi Zhang; Christopher Fifty; Tao Yu; Kilian Weinberger"}, {"ref_id": "b52", "title": "Graph neural networks in recommender systems: a survey", "journal": "", "year": "2020", "authors": "Shiwen Wu; Fei Sun; Wentao Zhang; Bin Cui"}, {"ref_id": "b53", "title": "Hashing-accelerated graph neural networks for link prediction", "journal": "", "year": "2021", "authors": "Wei Wu; Bin Li; Chuan Luo; Wolfgang Nejdl"}, {"ref_id": "b54", "title": "Seastar: vertex-centric programming for graph neural networks", "journal": "", "year": "2021", "authors": "Yidi Wu; Kaihao Ma; Zhenkun Cai; Tatiana Jin; Boyang Li; Chenguang Zheng; James Cheng; Fan Yu"}, {"ref_id": "b55", "title": "A comprehensive survey on graph neural networks", "journal": "", "year": "2020", "authors": "Zonghan Wu; Shirui Pan; Fengwen Chen; Guodong Long; Chengqi Zhang; S Yu Philip"}, {"ref_id": "b56", "title": "Representation learning on graphs with jumping knowledge networks", "journal": "", "year": "2018", "authors": "Keyulu Xu; Chengtao Li; Yonglong Tian; Tomohiro Sonobe; Ken-Ichi Kawarabayashi; Stefanie Jegelka"}, {"ref_id": "b57", "title": "Graph convolutional neural networks for web-scale recommender systems", "journal": "", "year": "2018", "authors": "Rex Ying; Ruining He; Kaifeng Chen; Pong Eksombatchai; L William; Jure Hamilton;  Leskovec"}, {"ref_id": "b58", "title": "Design space for graph neural networks", "journal": "Advances in Neural Information Processing Systems", "year": "2020", "authors": "Jiaxuan You; Zhitao Ying; Jure Leskovec"}, {"ref_id": "b59", "title": "GraphSAINT: Graph Sampling Based Inductive Learning Method", "journal": "", "year": "2020", "authors": "Hanqing Zeng; Hongkuan Zhou; Ajitesh Srivastava; Rajgopal Kannan; Viktor Prasanna"}, {"ref_id": "b60", "title": "ROD: reception-aware online distillation for sparse graphs", "journal": "", "year": "2021", "authors": "Wentao Zhang; Yuezihan Jiang; Yang Li; Zeang Sheng; Yu Shen; Xupeng Miao; Liang Wang; Zhi Yang; Bin Cui"}, {"ref_id": "b61", "title": "Zhi Yang, and Bin Cui. 2021. Evaluating deep graph neural networks", "journal": "", "year": "2021", "authors": "Wentao Zhang; Zeang Sheng; Yuezihan Jiang; Yikuan Xia; Jun Gao"}, {"ref_id": "b62", "title": "Node Dependent Local Smoothing for Scalable Graph Learning", "journal": "Advances in Neural Information Processing Systems", "year": "2021", "authors": "Wentao Zhang; Mingyu Yang; Zeang Sheng; Yang Li; Wen Ouyang; Yangyu Tao; Zhi Yang; Bin Cui"}, {"ref_id": "b63", "title": "Graph attention multi-layer perceptron", "journal": "", "year": "2021", "authors": "Wentao Zhang; Ziqi Yin; Zeang Sheng; Wen Ouyang; Xiaosen Li; Yangyu Tao; Zhi Yang; Bin Cui"}, {"ref_id": "b64", "title": "Automated Machine Learning on Graphs: A Survey", "journal": "", "year": "2021", "authors": "Ziwei Zhang; Xin Wang; Wenwu Zhu"}, {"ref_id": "b65", "title": "Distdgl: distributed graph neural network training for billion-scale graphs", "journal": "IEEE", "year": "2020", "authors": "Da Zheng; Chao Ma; Minjie Wang; Jinjing Zhou; Qidong Su; Xiang Song; Quan Gan; Zheng Zhang; George Karypis"}, {"ref_id": "b66", "title": "Auto-gnn", "journal": "", "year": "2019", "authors": "Kaixiong Zhou; Qingquan Song; Xiao Huang; Xia Hu"}, {"ref_id": "b67", "title": "Simple spectral graph convolution", "journal": "", "year": "2021", "authors": "Hao Zhu; Piotr Koniusz"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: The speedup and bottleneck of a two-layer Graph-SAGE along with the increased workers on Reddit dataset.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "1 :1Initialize message set M = {x }, \u2200 \u2208 V; 2: // Stage 1: Pre-processing 3: Initialize feature message m 0 = x , \u2200 \u2208 V; 4: for 1 \u2264 \u2264 do end for 10: // Stage 2: Model-training 11: for \u2208 V do 12: c \u2190message_aggregator(M ); 13: h \u2190message_updater(c ); 14: end for 15: // Stage 3: Post-processing 16: Initialize feature message m 0 = h , \u2200 \u2208 V; 17: for 1 \u2264 \u2264 do end for 22: return m , \u2200 \u2208 V; SGAP paradigm instead of classic architectures under the message passing framework; (2) Rather than optimizing the predictive performance alone, PaSca tackles the accuracy-efficiency trade-off through multi-objective optimization, and provides architectures to meet different needs of performance and inference time.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Model-training. The multi-hop messages M = {m | 0 \u2264 \u2264 } are then aggregated into a single combined message vector c for each node (line 12) as: c \u2190 message_aggregator(M ).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: The influence of aggregation steps on 20 randomly sampled nodes on Citeseer dataset.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 :4Figure 4: The workflow of PaSca, which consists of the searching and evaluation engine.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Distributed on ogbn-product", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 5 :5Figure5: Scalability comparison on Reddit and ogbn-product datasets. The stand-alone scenario means the graph has only one partition stored on a multi-GPU server, whereas the distributed scenario means the graph is partitioned and stored on multi-servers. In the distributed scenario, we run two workers per machine.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 6 :6Figure 6: Pareto Front found on Cora.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 7 :7Figure 7: Test accuracy over training time on Industry.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 8 :8Figure 8: Left: Test accuracy of different models along with the increased aggregation steps on PubMed. Right: PaSca-V2's average gating weights of graph messages of different steps on 60 randomly selected nodes from PubMed.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "The search space for scalable GNNs in our PaSca system.", "figure_data": "StagesNameRange/ChoicesTypePre-processingAggregation steps ( Graph aggregators ())[0, 10] {Aug.NA, PPR( = 0.1), PPR( = 0.2), PPR( = 0.3), Triangle. IA} Categorical IntegerModel trainingMessage aggregators ( Transformation steps ()){None, Mean, Max, Concatenate, Weighted, Adaptive} [1, 10]Categorical Integer"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Current scalable GNNs in our design space.", "figure_data": "Pre-processingModel trainingPost-processingModelsSGCAug.NANone1/SIGNOptionalConcatenate1/S 2 GCPPRMean1/GBPAug.NAWeighted\u2265 2/PaSca-APPNP//\u2265 2PPR"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": "Search EngineMessage Distributed StorageMessage Distributed StorageDesign Space\u2026 -th -th -th\u2026-th -th\u2026-th\u2026\u2026 -th -th -th\u2026-th -th\u2026-thSuggestion ServerStructureFeatures\u2026 StructureFeaturesStructurePredictions\u2026 StructurePredictions1) Sample2) Update1) -step messages of node 's neighborhood2) ( +1)-step message of node1) -step messages of node 's neighborhood2) ( +1)-step message of nodearchitecturesobservations-th -th-th-th-th -th-th-thEvaluation EngineWorker 1Worker 2\u2026Worker N\u2026\u2026BatchWorker 1Worker 2\u2026Worker NFeaturesFeaturesSearchingPre-processing FeaturesPost-processing Predictions"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Scalable GNNs found by PaSca.", "figure_data": "Pre-processingModel trainingPost-processingModelsPaSca-V1 PPR( = 0.1) Weighted32//PaSca-V2Aug.NAAdaptive62//PaSca-V3Aug.NAAdaptive63PPR ( = 0.3)4"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Test accuracy (%) in transductive settings. \"NMP\" and \"DNMP\" refer to architectures following NMP and DNMP paradigm. \"SGAP\" refers to architectures following the scalable graph archtecture paradigm proposed in Section 3.1.", "figure_data": "AmazonAmazonCoauthorCoauthorTypeModelsCoraCiteseer PubMedIndustryComputerPhotoCSPhysicsGCN81.8\u00b10.5 70.8\u00b10.5 79.3\u00b10.782.4\u00b10.491.2\u00b10.690.7\u00b10.292.7\u00b11.145.9\u00b10.4NMPGAT JK-Net83.0\u00b10.7 72.5\u00b10.7 79.0\u00b10.3 81.8\u00b10.5 70.7\u00b10.7 78.8\u00b10.780.1\u00b10.6 82.0\u00b10.690.8\u00b11.0 91.9\u00b10.787.4\u00b10.2 89.5\u00b10.690.2\u00b11.4 92.5\u00b10.446.8\u00b10.7 47.2\u00b10.3ResGCN82.2\u00b10.6 70.8\u00b10.7 78.3\u00b10.681.1\u00b10.791.3\u00b10.987.9\u00b10.692.2\u00b11.546.8\u00b10.5DNMPAPPNP AP-GCN 83.4\u00b10.3 71.3\u00b10.5 79.7\u00b10.3 83.3\u00b10.5 71.8\u00b10.5 80.1\u00b10.281.7\u00b10.3 83.7\u00b10.691.4\u00b10.3 92.1\u00b10.392.1\u00b10.4 91.6\u00b10.792.8\u00b10.9 93.1\u00b10.946.7\u00b10.6 46.9\u00b10.7SGC81.0\u00b10.2 71.3\u00b10.5 78.9\u00b10.582.2\u00b10.991.6\u00b10.790.3\u00b10.591.7\u00b11.145.2\u00b10.3SIGN82.1\u00b10.3 72.4\u00b10.8 79.5\u00b10.583.1\u00b10.891.7\u00b10.791.9\u00b10.392.8\u00b10.846.3\u00b10.5S 2 GC82.7\u00b10.3 73.0\u00b10.2 79.9\u00b10.383.1\u00b10.791.6\u00b10.691.6\u00b10.693.1\u00b10.845.9\u00b10.4SGAPGBP83.9\u00b10.7 72.9\u00b10.5 80.6\u00b10.483.5\u00b10.892.1\u00b10.892.3\u00b10.493.3\u00b10.747.1\u00b10.6PaSca-V1 83.4\u00b10.5 72.2\u00b10.5 80.5\u00b10.483.7\u00b10.792.1\u00b10.791.9\u00b10.393.2\u00b10.646.3\u00b10.4PaSca-V2 84.4\u00b10.3 73.1\u00b10.3 80.7\u00b10.784.1\u00b10.792.4\u00b10.792.6\u00b10.493.6\u00b10.847.4\u00b10.6PaSca-V3 84.6\u00b10.6 73.4\u00b10.5 80.8\u00b10.684.8\u00b10.792.7\u00b10.8 92.8\u00b10.593.8\u00b10.947.6\u00b10.3Table 5: Test accuracy (%) in inductive settings.ModelsFlickrRedditGraphSAGE 50.1\u00b11.3 95.4\u00b10.0FastGCN50.4\u00b10.1 93.7\u00b10.0ClusterGCN 48.1\u00b10.5 95.7\u00b10.0GraphSAINT 51.1\u00b10.1 96.6\u00b10.1PaSca-V151.2\u00b10.3 95.8\u00b10.1PaSca-V251.8\u00b10.3 96.3\u00b10.0PaSca-V352.1\u00b10.2 96.7\u00b10.1"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "https://github.com/snap-stanford/ogb where is the input feature of node . Existing methods, such as PPNP", "figure_data": ")"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Overview of the Graph Datasets", "figure_data": "Dataset#Nodes #Features#Edges#Classes#Train/Val/TestTask typeDescriptionCora2,7081,4335,4297140/500/1000Transductivecitation networkCiteseer3,3273,7034,7326120/500/1000Transductivecitation networkPubmed19,71750044,338360/500/1000Transductivecitation networkAmazon Computer13,381767245,77810200/300/12881Transductiveco-purchase graphAmazon Photo7,487745119,0438160/240/7,087Transductiveco-purchase graphogbn-products2,449,02910061,859,14047195922/489811/204126 Transductive co-purchase networkCoauthor CS18,3336,80581,89415300/450/17,583Transductive co-authorship graphCoauthor Physics34,4938,415247,9625100/150/34,243Transductive co-authorship graphFlickr89,250500899,756744,625/22,312/22,312Inductiveimage networkReddit232,96560211,606,91941155,310/23,297/54,358Inductivesocial networkIndustry1,000,000641,434,3822535,000/10,000/30,000Transductiveuser-video graph"}], "formulas": [{"formula_id": "formula_0", "formula_text": "m \u2190 aggregate h \u22121 | \u2208 N , h \u2190 update(m ).(1)", "formula_coordinates": [3.0, 60.61, 294.6, 233.44, 11.09]}, {"formula_id": "formula_1", "formula_text": "GCN-aggregate h \u22121 | \u2208 N = \u2211\ufe01 \u2208N h \u22121 / \u221a\ufe03\u02dc\u02dc, GCN-update(m ) = ( m ),", "formula_coordinates": [3.0, 74.43, 351.74, 198.9, 39.23]}, {"formula_id": "formula_2", "formula_text": "m \u2190 graph_aggregator m \u22121 | \u2208 N ,(2)", "formula_coordinates": [3.0, 354.97, 633.49, 203.23, 11.09]}, {"formula_id": "formula_4", "formula_text": "h \u2190 message_updater(c ).(4)", "formula_coordinates": [4.0, 119.97, 217.05, 174.07, 8.97]}, {"formula_id": "formula_5", "formula_text": "m \u2190 graph_aggregator m \u22121 | \u2208 N ,(5)", "formula_coordinates": [4.0, 90.81, 297.29, 203.23, 11.09]}, {"formula_id": "formula_6", "formula_text": "m = \u2211\ufe01 \u2208N 1 m \u22121 .(6)", "formula_coordinates": [4.0, 404.76, 220.86, 153.44, 21.87]}, {"formula_id": "formula_7", "formula_text": "m = m 0 + (1 \u2212 ) \u2211\ufe01 \u2208N 1 \u221a\ufe03\u02dc\u02dcm \u22121 ,(7)", "formula_coordinates": [4.0, 373.15, 319.48, 185.05, 21.87]}, {"formula_id": "formula_8", "formula_text": "m = \u2211\ufe01 \u2208N 1 m \u22121 .(8)", "formula_coordinates": [4.0, 402.27, 459.94, 155.93, 21.87]}, {"formula_id": "formula_9", "formula_text": "\u2190 \u2295 m \u2208 (m ),(9)", "formula_coordinates": [4.0, 412.32, 618.02, 145.88, 10.17]}, {"formula_id": "formula_10", "formula_text": "c msg \u2190 \u2211\ufe01 m \u2208 m , = (sm ),(10)", "formula_coordinates": [5.0, 109.57, 487.84, 184.47, 20.99]}, {"formula_id": "formula_11", "formula_text": "h 0 \u2190 update(x ), h \u2190 aggregate h \u22121 | \u2208 N . (11", "formula_coordinates": [12.0, 60.9, 637.6, 229.72, 11.4]}, {"formula_id": "formula_12", "formula_text": "APPNP-update(x v ) = ( x ), APPNP-aggregate h \u22121 | \u2208 N = h 0 + (1 \u2212 ) \u2211\ufe01 \u2208N h \u22121 \u221a\ufe03\u02dc\u02dc,", "formula_coordinates": [12.0, 317.96, 122.52, 240.74, 39.99]}], "doi": "10.1145/3485447.3511986"}