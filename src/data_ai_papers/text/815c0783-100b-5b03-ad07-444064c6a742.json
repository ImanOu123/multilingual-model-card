{"title": "Real-Time High-Resolution Background Matting", "authors": "Shanchuan Lin; Andrey Ryabtsev; Soumyadip Sengupta; Brian Curless; Steve Seitz; Ira Kemelmacher-Shlizerman", "pub_date": "2020-12-14", "abstract": "Zoom with new background Our Zoom plugin with new background Figure 1: Current video conferencing tools like Zoom can take an input feed (left) and replace the background, often introducing artifacts, as shown in the center result with close-ups of hair and glasses that still have the residual of the original background. Leveraging a frame of video without the subject (far left inset), our method produces real-time, high-resolution background matting without those common artifacts. The image on the right is our result with the corresponding close-ups, screenshot from our Zoom plugin implementation.", "sections": [{"heading": "Introduction", "text": "Background replacement, a mainstay in movie special effects, now enjoys wide-spread use in video conferencing tools like Zoom, Google Meet, and Microsoft Teams. In addition to adding entertainment value, background replace-* Equal contribution. ment can enhance privacy, particularly in situations where a user may not want to share details of their location and environment to others on the call. A key challenge of this video conferencing application is that users do not typically have access to a green screen or other physical props used to facilitate background replacement in movie special effects.\nWhile many tools now provide background replacement functionality, they yield artifacts at boundaries, particularly in areas where there is fine detail like hair or glasses (Figure 1). In contrast, traditional image matting methods [6,16,17,30,9,2,7] provide much higher quality results, but do not run in real-time, at high resolution, and frequently require manual input. In this paper, we introduce the first fully-automated, real-time, high-resolution matting technique, producing state-of-the-art results at 4K (3840\u00d72160) at 30fps and HD (1920\u00d71080) at 60fps. Our method relies on capturing an extra background image to compute the alpha matte and the foreground layer, an approach known as background matting.\nDesigning a neural network that can achieve realtime matting on high-resolution videos of people is extremely challenging, especially when fine-grained details like strands of hair are important; in contrast, the previous state-of-the-art method [28] is limited to 512\u00d7512 at 8fps. Training a deep network on such a large resolution is extremely slow and memory intensive. It also requires large volumes of images with high-quality alpha mattes to generalize; the publicly available datasets [33,25] are too limited.\nSince it is difficult to collect a high-quality dataset with manually curated alpha mattes in large quantities, we propose to train our network with a series of datasets, each with different characteristics. To this end, we introduce Video-Matte240K and PhotoMatte13K/85 with high-resolution alpha mattes and foreground layers extracted with chromakey software. We first train our network on these larger databases of alpha mattes with significant diversity in human poses to learn robust priors. We then train on publicly available datasets [33,25] that are manually curated to learn fine-grained details.\nTo design a network that can handle high-resolution images in real-time, we observe that relatively few regions in the image require fine-grained refinement. Therefore, we introduce a base network that predicts the alpha matte and foreground layer at lower resolution along with an error prediction map which specifies areas that may need highresolution refinement. A refinement network then takes the low-resolution result and the original image to generate high-resolution output only at select regions.\nWe produce state-of-the-art background matting results in real-time on challenging real-world videos and images of people. We will release our VideoMatte240K and Pho-toMatte85 datasets and our model implementation.", "publication_ref": ["b5", "b15", "b16", "b29", "b8", "b1", "b6", "b27", "b32", "b24", "b32", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Background replacement can be achieved with segmentation or matting. While binary segmentation is fast and efficient, the resulting composites have objectionable artifacts. Alpha matting can produce visually pleasing composites but often requires either manual annotations or a known background image. In this section, we discuss related works that perform background replacement with segmentation or matting.\nSegmentation. The literature in both instance and semantic segmentation is vast and out of scope for this paper, so we will review the most relevant works. Mask RCNN [11] is still a top choice for instance segmentation while DeepLabV3+ [5] is a state-of-the-art semantic segmentation network. We incorporate the Atrous Spatial Pyramid Pooling (ASPP) module from DeepLabV3 [4] and DeepLabV3+ within our network. Since segmentation algorithms tend to produce coarse boundaries especially at higher resolutions, Kirillov et al. presented PointRend [15] which samples points near the boundary and iteratively refines the segmentation. This produces high-quality segmentation for large image resolutions with significantly cheaper memory and computation. Our method adopts this idea to the matting domain via learned refinement-region selection and a convolutional refinement architecture that improves the receptive field. Specific applications of human segmentation and parsing have also received considerable attention in recent works [34,19].", "publication_ref": ["b10", "b4", "b3", "b33", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Trimap-based matting.", "text": "Traditional (non-learning based) matting algorithms [6,16,17,30,9,2,7] require manual annotation (a trimap) and solve for the alpha matte in the 'unknown' region of the trimap. Different matting techniques are reviewed in the survey by Wang and Cohen [32]. Xu et al. [33] introduced a matting dataset and used a deep network with a trimap input to predict the alpha matte. Many recent approaches rely on this dataset to learn matting, e.g., Context-Aware Matting [13], Index Matting [21], sampling-based matting [31] and opacity propagationbased matting [18]. Although the performance of these methods depends on the quality of the annotations, some recent methods consider coarse [20] or faulty human annotations [3] to predict the alpha matte.\nMatting without any external input. Recent approaches have also focused on matting humans without any external input. Portrait matting without a trimap [36,29] is one of the more successful applications due to less variability among portrait images compared to full body humans. Soft segmentation for natural images had also been explored in [1]. Recent approaches like Late Fusion Matting [35] and HAttMatting [25] aim to solve for the alpha matte directly from the image, but these approaches can often fail to generalize as shown in [28].\nMatting with a known natural background. Matting with known natural background had been previously explored in [24], Bayesian matting [7] and Poisson matting [30,10] which also requires a trimap. Recently Sengupta et al. [28] introduced Background Matting (BGM) where an additional background image is captured and it provides a significant cue to predict the alpha matte and the foreground layer. Although this method showed high-quality matting results, the architecture is limited to 512\u00d7512 resolution and runs only at 8fps. In contrast, we introduce a real-time unified matting architecture that operates on 4K videos at 30fps and HD videos at 60fps, and produces higher quality results than BGM.", "publication_ref": ["b5", "b15", "b16", "b29", "b8", "b1", "b6", "b31", "b32", "b12", "b20", "b30", "b17", "b19", "b2", "b35", "b28", "b0", "b34", "b24", "b27", "b23", "b6", "b29", "b9", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Our Dataset", "text": "Since it is extremely difficult to obtain a large-scale, high-resolution, high-quality matting dataset where the alpha mattes are cleaned by human artists, we rely on multiple datasets including our own collections and publicly available datasets.\nPublicly available datasets. The Adobe Image Matting (AIM) dataset [33] provides 269 human training samples and 11 test samples, averaging around 1000\u00d71000 resolution. We also use a humans-only subset of Distinctions-646 [25] consisting of 362 training and 11 test samples, averaging around 1700\u00d72000 resolution. The mattes were created manually and are thus high-quality. However 631 training images are not enough to learn large variations in human poses and finer details at high resolution, so we in- VideoMatte240K. We collect 484 high-resolution green screen videos and generate a total of 240,709 unique frames of alpha mattes and foregrounds with chroma-key software Adobe After Effects. The videos are purchased as stock footage or found as royalty-free materials online. 384 videos are at 4K resolution and 100 are in HD. We split the videos by 479 : 5 to form the train and validation sets. The dataset consists of a vast amount of human subjects, clothing, and poses that are helpful for training robust models. We are releasing the extracted alpha mattes and foregrounds as a dataset to the public. To our knowledge, our dataset is larger than all existing matting datasets publicly available by far, and it is the first public video matting dataset that contains continuous sequences of frames instead of still images, which can be used in future research to develop models that incorporate motion information.\nPhotoMatte13K/85. We acquired a collection of 13,665 images shot with studio-quality lighting and cameras in front of a green-screen, along with mattes extracted via chroma-key algorithms with manual tuning and error repair. We split the images by 13,165 : 500 to form the train and validation sets. These mattes contain a narrow range of poses but are high resolution, averaging around 2000\u00d72500, and include details such as individual strands of hair. We refer to this dataset as PhotoMatte13K. However privacy and licensing issues prevent us from sharing this set; thus, we also collected an additional set of 85 mattes of similar quality for use as a test set, which we are releasing to the public as PhotoMatte85. In Figure 2 we show examples from the VideoMatte240K and PhotoMatte13K/85 datasets.\nWe crawl 8861 high-resolution background images from Flickr and Google and split them by 8636 : 200 : 25 to use when constructing the train, validation, and test sets. We will release the test set in which all images have a CC license (see appendix for details).", "publication_ref": ["b32", "b24"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Our Approach", "text": "Given an image I and the captured background B we predict the alpha matte \u03b1 and the foreground F , which will allow us to composite over any new background by I = \u03b1F + (1 \u2212 \u03b1)B , where B is the new background. Instead of solving for the foreground directly, we solve for foreground residual F R = F \u2212 I. Then, F can be recovered by adding F R to the input image I with suitable clamping: F = max(min(F R + I, 1), 0). We find this formulation improves learning, and allows us to apply a lowresolution foreground residual onto a high-resolution input image through upsampling, aiding our architecture as described later.\nMatting at high resolution is challenging, as applying a deep network directly incurs impractical computation and memory consumption. As Figure 4 shows, human mattes are usually very sparse, where large areas of pixels belong to either background (\u03b1 = 0) or foreground (\u03b1 = 1), and only a few areas involve finer details, e.g., around the hair, glasses, and person's outline. Thus instead of designing one network that operates on high-resolution images, we introduce two networks; one operates at lower-resolution and another only operates on selected patches at the original resolution based on the prediction of the previous network.\nThe architecture consists of a base network G base and a refinement network G refine . Given the original image I and the captured background B, we first downsample by a factor of c to I c and B c . The base network G base takes I c and B c as input and predicts coarse-grained alpha matte \u03b1 c , foreground residual F R c , an error prediction map E c , and hidden features H c . Then, the refinement network G refine employs H c , I, and B to refine \u03b1 c and F R c only in regions where the predicted error E c is large, and produces alpha \u03b1 and foreground residual F R at the original resolution. Our model is fully-convolutional and is trained to work on arbitrary sizes and aspect ratios.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Base Network", "text": "The base network is a fully-convolutional encoderdecoder network inspired by the DeepLabV3 [4] and DeepLabV3+ [5] architectures, which achieved state-ofthe-art performance on semantic segmentation tasks in 2017 and 2018. Our base network consists of three modules: Backbone, ASPP, and Decoder.\nWe adopt ResNet-50 [12] for our encoder backbone, which can be replaced by ResNet-101 and MobileNetV2 [27] to trade-off between speed and quality. We adopt the  ASPP (Atrous Spatial Pyramid Pooling) module after the backbone following the DeepLabV3 approach. The ASPP module consists of multiple dilated convolution filters with different dilation rates of 3,6 and 9. Our decoder network applies bilinear upsampling at each step, concatenated with the skip connection from the backbone, and followed by a 3\u00d73 convolution, Batch Normalization [14], and ReLU activation [22] (except the last layer). The decoder network outputs coarse-grained alpha matte \u03b1 c , foreground residual F R c , error prediction map E c and a 32-channel hidden features H c . The hidden features H c contain global contexts that will be useful for the refinement network.", "publication_ref": ["b3", "b4", "b11", "b26", "b13", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Refinement Network", "text": "The goal of the refinement network is to reduce redundant computation and recover high-resolution matting details. While the base network operates on the whole image, the refinement network operates only on patches selected based on the error prediction map E c . We perform a two-stage refinement, first at 1 2 of the original resolution and then at the full resolution. During inference, we refine k patches, with k either set in advance or set based on a threshold that trades off between quality and computation time.\nGiven the coarse error prediction map E c at 1 c of the original resolution, we first resample it to 1 4 of the original resolution E 4 , s.t. each pixel on the map corresponds to a 4\u00d74 patch on the original resolution. We select the top k pixels with the highest predicted error from E 4 to denote the k 4\u00d74 patch locations that will be refined by our refinement module. The total number of refined pixels at the original resolution is 16k.\nWe perform a two-stage refinement process. First, we bilinearly resample the coarse outputs, i.e., alpha matte \u03b1 c , foreground residual F R c and hidden features H c , as well as the input image I and background B to 1 2 of the original resolution and concatenate them as features. Then we crop out 8\u00d78 patches around the error locations selected from E 4 , and pass each through two layers of 3\u00d73 convolution with valid padding, Batch Normalization, and ReLU, which reduce the patch dimension to 4\u00d74. These intermediate features are then upsampled to 8 \u00d7 8 again and concatenated with the 8\u00d78 patches extracted from the original-resolution input I and background B at the corresponding location. We then apply an additional two layers of 3\u00d73 convolution with valid padding, Batch Normalization and ReLU (except the last layer) to obtain 4\u00d74 alpha matte and foreground residuals results. Finally, we upsample the coarse alpha matte \u03b1 c and foreground residual F R c to the original resolution and swap in the respective 4\u00d74 patches that have been refined to obtain the final alpha matte \u03b1 and foreground residual F R . The entire architecture is illustrated in Figure 3. See appendix for the details of implementation.", "publication_ref": ["b0", "b0", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Training", "text": "All matting datasets provide an alpha matte and a foreground layer, which we compose onto multiple highresolution backgrounds. We employ multiple data augmentation techniques to avoid overfitting and help the model generalize to challenging real-world situations. We apply affine transformation, horizontal flipping, brightness, hue, and saturation adjustment, blurring, sharpening, and random noise as data augmentation to both the foreground and background layer independently. We also slightly translate the background to simulate misalignment and create artificial shadows to simulate how the presence of a subject can cast shadows in real-life environments (see appendix for more details). We randomly crop the images in every minibatch so that the height and width are each uniformly distributed between 1024 and 2048 to support inference at any resolution and aspect ratio.\nTo learn \u03b1 w.r.t. ground-truth \u03b1 * , we use an L1 loss over the whole alpha matte and its (Sobel) gradient:\nL \u03b1 = ||\u03b1 \u2212 \u03b1 * || 1 + ||\u2207\u03b1 \u2212 \u2207\u03b1 * || 1 .\n(1) We obtain the foreground layer from predicted foreground residual F R , using F = max(min(F R + I, 1), 0). We compute L1 loss only on the pixels where \u03b1 * > 0:\nL F = ||(\u03b1 * > 0) * (F \u2212 F * ))|| 1 .\n(2) where that (\u03b1 * > 0) is a Boolean expression.\nFor refinement region selection, we define the ground truth error map as E * = |\u03b1 \u2212 \u03b1 * |. We then compute mean squared error between the predicted error map and the ground truth error map as the loss:\nL E = ||E \u2212 E * || 2 .\n(3) This loss encourages the predicted error map to have larger values where the difference between the predicted alpha and the ground-truth alpha is large. The ground-truth error map changes over iterations during training as the predicted alpha improves. Over time, the error map converges and predicts high error in complex regions, e.g. hair, that would lead to poor composites if simply upsampled.\nThe base network\n(\u03b1 c , F R c , E c , H c ) = G base (I c , B c ) op- erates at 1\nc of the original image resolution, and is trained with the following loss function:\nL base = L \u03b1c + L Fc + L Ec . (4\n) The refinement network (\u03b1, F R ) = G refine (\u03b1 c , F R c , E c , H c , I, B\n) is trained using: We train our model on multiple datasets in the following order. First, we train only the base network G base and then the entire model G base and G refine jointly on Video-Matte240K, which makes the model robust to a variety of subjects and poses. Next, we train our model jointly on Pho-toMatte13K to improve the high-resolution details. Finally, we train our model jointly on Distinctions-646. The dataset has only 362 unique training samples, but it is of the highest quality and contains human-annotated foregrounds that are very helpful for improving the foreground quality produced by our model. We omit training on the AIM dataset as a possible 4th stage and only use it for testing because it causes a degradation in quality as shown in our ablation study in Section 6.  \nL refine = L \u03b1 + L F .(5", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Alpha", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Evaluation", "text": "We compare our approach to two trimap-based methods, Deep Image Matting (DIM) [33] and FBA Matting (FBA) [8], and one background-based method, Background Matting (BGM) [28]. The input resolution to DIM was fixed at 320\u00d7320 by the implementation, while we set the FBA input resolution to approximately HD due to memory limits. We additionally train the BGM model on our datasets and denote it as BGM a (BGM adapted).\nOur evaluation uses c = 4, k = 20, 000 for photos, c = 4, k = 5, 000 for HD videos, and c = 8, k = 20, 000 for 4K videos, where c is the downsampling factor for the base network and k is the number of patches that get refined.", "publication_ref": ["b32", "b7", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation on composition datasets", "text": "We construct test benchmarks by separately compositing test samples from AIM, Distinctions, and PhotoMatte85 datasets onto 5 background images per sample. We apply minor background misalignment, color adjustment, and noise to simulate flawed background capture. We generate trimaps from ground-truth alpha using thresholding and morphological operations. We evaluate matte outputs using metrics from [26]: MSE (mean squared error) for alpha and foreground, SAD (sum of absolute difference), Grad (spatial-gradient metric), and Conn (connectivity) for alpha only. All MSE values are scaled by 10 3 and all metrics are only computed over the unknown region of trimaps generated as described above. Foreground MSE is additionally only measured where the grouth-truth alpha \u03b1 * > 0.\nTable 1 shows that our approach outperforms the existing background-based BGM method across all datasets.  Our approach is slightly worse than the state-of-the-art trimap-based FBA method, which requires carefully annotated manual trimaps and is much slower than our approach, as shown later in the performance comparison.", "publication_ref": ["b25"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Evaluation on captured data", "text": "Although quantitative evaluation on the abovementioned datasets serves the purpose of quantifying the performance of different algorithms, it is important to evaluate how these methods perform on unconstrained real data. To evaluate on real data, we capture a number of photos and videos containing subjects in varying poses and surroundings. The videos are captured on a tripod with consumer smartphones (Samsung S10+ and iPhone X) and a professional camera (Sony \u03b17s II), in both HD and 4K resolution. The photos are captured in 4000\u00d76000 resolution. We also use some HD videos presented in the BGM paper that are made public to compare with our method.\nFor fair comparison in the real-time scenario, where manual trimaps cannot be crafted, we construct trimaps by morphing segmentation result from DeepLabV3+, as suggested in [28]. We show results on both trimaps, denoting FBA using this fully automatic trimap as FBA auto .\nFigure 5 shows our method produces much sharper and more detailed results around hair and edges compared to other methods. Since our refinement operates at the native resolution, the quality is far superior relative to BGM, which resizes the images and only processes them at 512\u00d7512 resolution. FBA, with manual trimap, produces excellent results around hair details, however cannot be evaluated at resolutions above around HD on standard GPUs. When FBA is applied on automatic trimaps generated with segmentation, it often shows large artifacts, mainly due to faulty segmentation.\nWe extract 34 frames from both the test videos shared by the BGM paper and our captured videos and photos to create a user study. 40 participants were presented with an interactive interface showing each input image as well as the mattes produced by BGM and our approach, in random order. They were encouraged to zoom in on details and asked to rate one of the mattes as \"much better\", \"slightly better\", or \"similar\". The results, shown in Table 2, demonstrate significant qualitative improvement over BGM. 59% of the time participants perceive our algorithm to be better, com-pared to 23% for BGM. For sharp samples in 4K and larger, our method is preferred 75% of the time to BGM's 15%.  ", "publication_ref": ["b27"], "figure_ref": ["fig_3"], "table_ref": ["tab_5"]}, {"heading": "Performance comparison", "text": "Table 3 and 4 show that our method is smaller and much faster than BGM. Our method contains only 55.7% of the parameters compared to BGM. Our method can achieve HD 60fps and 4K 30fps at batch size 1 on an Nvidia RTX 2080 TI GPU, considered to be real-time for many applications. It is a significant speed-up compared to BGM which can only handle 512\u00d7512 resolution at 7.8fps. The performance can be further improved by switching to Mo-bileNetV2 backbone, which achieves 4K 45fps and HD 100fps. More performance results, such as adjusting the refinement selection parameter k and using a larger batch size, are included in the ablation studies and in the appendix.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Method Backbone", "text": "Resolution   Figure 6: We produce better results than a chroma-keying software, when an amateur green-screen setup is used.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Practical use", "text": "Zoom implementation We build a Zoom plugin that intercepts the webcam input, collects one no-person (background) shot, then performs real-time video matting and compositing, streaming the result back into the Zoom call. We test with a 720p webcam in Linux. The upgrade elicits praise in real meetings, demonstrating its practicality in a real-world setting.\nComparison to green-screen Chroma keying with a green screen is the most popular method for creating highquality mattes. However, it requires even lighting across the screen and background-subject separation to avoid cast shadows. In Figure 6, we compare our method against chroma-keying under the same lighting with an amateur green-screen setup. We find that in the unevenly lit setting, our method outperforms approaches designed for the green screen.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ablation Studies", "text": "Role of our datasets We train on multiple datasets, each of which brings unique characteristics that help our network produce high-quality results at high-resolution. Table 5 shows the metrics of our method by adding or removing a dataset from our training pipeline. We find adding the AIM dataset as a possible 4th stage worsens the metrics even on the AIM test set itself. We believe it is because samples in the AIM dataset are lower in resolution and quality compared to Distinctions and the small number of samples may have caused overfitting. Removal of VideoMatte240K, PhotoMatte13K, and Distinctions datasets from the training pipeline all result in worse metrics, proving that those datasets are helpful in improving the model's quality.\nRole of the base network We experiment with replacing ResNet-50 with ResNet-101 and MobileNetV2 as our encoder backbone in the base network. The metrics in Table 6 show that ResNet-101 has slight improvements over ResNet-50 on some metrics while doing worse on others. This indicates that ResNet-50 is often sufficient for obtaining the best quality. MobileNetV2 on the other hand is worse than ResNet-50 on all metrics, but it is significantly faster and smaller than ResNet-50 as shown in Tables 3 and  4   Role of the refinement network Our refinement network improves detail sharpness over the coarse results in Figure 7, and is effective even in 4K resolution. Figure 8 shows the effects of increasing and decreasing the refinement area. Most improvement can be achieved by refining over only 5% to 10% of the image resolution. Table 7 shows that refining only the selected patches provides significant speedup compared to refining the full image.    Patch-based refinement vs. Point-based refinement Our refinement module uses a stack of 3\u00d73 convolution kernels, creating a 13\u00d713 receptive field for every output pixel. An alternative is to refine only on points using 1\u00d71 convolution kernels, which would result in a 2\u00d72 receptive field with our method. Table 6 shows that the 3 \u00d7 3 kernel can achieve better metrics than point-based kernels, due to a larger receptive field.\nLimitations Our method can be used on handheld input by applying homography alignment to the background on every frame, but it is limited to small motion. Other common limitations are indicated in Figure 9. We recommend using our method with a simple-textured background, fixed exposure/focus/WB setting, and a tripod for the best result.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": ["tab_10", "tab_11", "tab_7", "tab_13", "tab_11"]}, {"heading": "Conclusion", "text": "We have proposed a real-time, high-resolution background replacement technique that operates at 4K 30fps and HD 60fps. Our method only requires an input image and an pre-captured background image, which is easy to obtain in many applications. Our proposed architecture efficiently refines only the error-prone regions at high-resolution, which reduces redundant computation and makes real-time highresolution matting possible. We introduce two new largescale matting datasets that help our method generalize to real-life scenarios. Our experiment shows our method sets new state-of-the-art performance on background matting. We demonstrate the practicality of our method by streaming our results to Zoom and achieve a much more realistic virtual conference call.\nEthics Our primary goal is to enable creative applications and give users more privacy options through background replacement in video calls. However, we recognize that image editing can also be used for negative purposes, which can be mitigated through watermarking and other security techniques in commercial applications of this work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Overview", "text": "We provide additional details in this appendix. In Sec. B, we describe the details of our network architecture and implementation. In Sec. C, we clarify our use of keywords for crawling background images. In Sec. D, we explain how we train our model and show details of our data augmentations. In Sec. E, we show additional metrics about our method's performance. In Sec. F, we show all the qualitative results used in our user study along with the average score per sample.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B. Network", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1. Architecture", "text": "Backbone Both ResNet and MobileNetV2 are adopted from the original implementation with minor modifications. We change the first convolution layer to accept 6 channels for both the input and the background images. We follow DeepLabV3's approach and change the last downsampling block with dilated convolutions to maintain an output stride of 16. We do not use the multi-grid dilation technique proposed in DeepLabV3 for simplicity.\nASPP We follow the original implementation of ASPP module proposed in DeepLabV3. Our experiment suggests that setting dilation rates to (3,6,9) produces the better results. Decoder", "publication_ref": ["b2", "b5", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "CBR128 -CBR64 -CBR48 -C37", "text": "\"CBRk\" denotes k 3\u00d73 convolution filters with same padding without bias followed by Batch Normalization and ReLU. \"Ck\" denotes k 3\u00d73 convolution filters with same padding and bias. Before every convolution, decoder uses bilinear upsampling with a scale factor of 2 and concatenates with the corresponding skip connection from the backbone. The 37-channel output consists of 1 channel of alpha \u03b1 c , 3 channels of foreground residual F R c , 1 channel of error map E c , and 32 channels of hidden features H c . We clamp \u03b1 c and E c to 0 and 1. We apply ReLU on H c . Refiner features. After the second stage, the patch dimension becomes [nk \u00d7 4 \u00d7 4 \u00d7 4]. The 4 channels are alpha and foreground residual. Finally, we bilinearly upsample the coarse \u03b1 c and F R c to full resolution and replace the refined patches to their corresponding location to form the final output \u03b1 and F R .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.2. Implementation", "text": "We implement our network in PyTorch [23]. The patch extraction and replacement can be achieved via the native vectorized operations for maximum performance. We find that PyTorch's nearest upsampling operation is much faster on small-resolution patches than bilinear upsampling, so we use it when upsampling the patches.", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}, {"heading": "C. Dataset", "text": "VideoMatte240K The dataset contains 484 video clips, which consists a total of 240,709 frames. The average frames per clip is 497.3 and the median is 458.5. The longest clip has 1500 frames while the shortest clip has 124 frames. Figure 10 shows more examples from Video-Matte240K dataset.    Additionally, we use mixed precision training for faster computation and less memory consumption. When using multiple GPUs, we apply data parallelism to split the minibatch across multiple GPUs and switch to use PyTorch's Synchronized Batch Normalization to track batch statistics across GPUs.", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "D. Training", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.1. Training augmentation", "text": "For every alpha and foreground training sample, we rotate to composite with backgrounds in a \"zip\" fashion to form a single epoch. For example, if there are 60 training samples and 100 background images, a single epoch is 100 images, where the 60 samples first pair with the first 60 background images, then the first 40 samples pair with the rest of the 40 background images again. The rotation stops when one set of images runs out. Because the datasets we use are very different in sizes, this strategy is used to generalize the concept of an epoch.\nWe apply random rotation (\u00b15deg), scale (0.3\u223c1), translation (\u00b110%), shearing (\u00b15deg), brightness (0.85\u223c1.15), contrast (0.85\u223c1.15), saturation (0.85\u223c1.15), hue (\u00b10.05), gaussian noise (\u03c3 2 \u22640.03), box blurring, and sharpening independently to foreground and background on every sample. We then composite the input image using\nI = \u03b1F + (1 \u2212 \u03b1)B.\nWe additionally apply random rotation (\u00b11deg), translation (\u00b11%), brightness (0.82\u223c1.18), contrast (0.82\u223c1.18), saturation (0.82\u223c1.18), and hue (\u00b10.1) only on the background 30% of the time. This small misalignment between input I and background B increases model's robustness on real-life captures.\nWe also find creating artificial shadows increases model's robustness because subjects in real-life often cast shadows on the environment. Shadows are created on I by darkening some areas of the image behind the subject following the subject's contour 30% of the time. Examples of composited images are shown in Figure 11. The bottom row shows examples of shadow augmentation. ", "publication_ref": [], "figure_ref": ["fig_8"], "table_ref": []}, {"heading": "D.2. Testing augmentation", "text": "For AIM and Distinctions, which have 11 human test samples each, we pair every sample with 5 random backgrounds from the background test set. For PhotoMatte85, which has 85 test samples, we pair every sample with only 1 background. We use the method and metrics described in [26] to evaluate the resulting sets of 55, 55, and 85 images.\nWe apply a random subpixel translation (\u00b10.3 pixels), random gamma (0.85\u223c1.15), and gaussian noise (\u00b5 = \u00b10.02, 0.08 \u2264 \u03c3 2 \u2264 0.15) to background B only, to simulate misalignment.\nThe trimaps used as input for trimap-based methods and for defining the error metric regions are obtained by thresholding the grouth-truth alpha between 0.06 and 0.96, then applying 10 iterations of dilation followed by 10 iterations of erosion using a 3\u00d73 circular kernel.", "publication_ref": ["b25"], "figure_ref": [], "table_ref": []}, {"heading": "E. Performance", "text": "Table 9 shows the performance of our method on two Nvidia RTX 2000 series GPUs: the flagship RTX 2080 TI and the entry-level RTX 2060 Super. The entry-level GPU yields lower FPS but is still within an acceptable range for many real-time applications. Additionally, Table 10 shows that switching to a larger batch size and a lower precision can increase the FPS significantly.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_18", "tab_2"]}, {"heading": "F. Additional Results", "text": "In Figures 13, 14, 15, we show all 34 examples in the user study, along with their average rating and results by different methods. Figure 12 shows the web UI for our userstudy.\nFigure 12: The web UI for our user study. Users are shown the original image and two result images from Ours and BGM methods. Users are given the instruction to rate whether one algorithm is \"much better\", \"slightly better\", or both as \"similar\".    A score of -10 denotes BGM is \"much better\", -5 denotes BGM is \"slightly better\", 0 denotes \"similar\", +5 denotes Ours is \"slightly better\", +10 denotes Ours is \"much better\". Our method receives an average 3.1 score.  ", "publication_ref": [], "figure_ref": ["fig_9", "fig_0", "fig_0"], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Semantic soft segmentation", "journal": "ACM Transactions on Graphics (TOG)", "year": "2018", "authors": "Yagiz Aksoy; Tae-Hyun Oh; Sylvain Paris; Marc Pollefeys; Wojciech Matusik"}, {"ref_id": "b1", "title": "Designing effective inter-pixel information flow for natural image matting", "journal": "", "year": "2017", "authors": "Yagiz Aksoy; Marc Tunc Ozan Aydin;  Pollefeys"}, {"ref_id": "b2", "title": "Disentangled image matting. International Conference on Computer Vision (ICCV)", "journal": "", "year": "2019", "authors": "Shaofan Cai; Xiaoshuai Zhang; Haoqiang Fan; Haibin Huang; Jiangyu Liu; Jiaming Liu; Jiaying Liu; Jue Wang; Jian Sun"}, {"ref_id": "b3", "title": "Rethinking atrous convolution for semantic image segmentation", "journal": "", "year": "2017", "authors": "Liang-Chieh Chen; George Papandreou; Florian Schroff; Hartwig Adam"}, {"ref_id": "b4", "title": "Encoder-decoder with atrous separable convolution for semantic image segmentation", "journal": "", "year": "2018", "authors": "Yukun Liang-Chieh Chen; George Zhu; Florian Papandreou; Hartwig Schroff;  Adam"}, {"ref_id": "b5", "title": "Knn matting", "journal": "", "year": "2013", "authors": "Qifeng Chen; Dingzeyu Li; Chi-Keung Tang"}, {"ref_id": "b6", "title": "A bayesian approach to digital matting", "journal": "", "year": "2001", "authors": "Yung-Yu Chuang; Brian Curless; H David; Richard Salesin;  Szeliski"}, {"ref_id": "b7", "title": "", "journal": "", "year": "2020", "authors": "Marco Forte; Fran\u00e7ois Piti\u00e9; . F Matting"}, {"ref_id": "b8", "title": "Shared sampling for real-time alpha matting", "journal": "Wiley Online Library", "year": "2010", "authors": "S L Eduardo; Manuel M Gastal;  Oliveira"}, {"ref_id": "b9", "title": "Near-real-time image matting with known background", "journal": "Canadian Conference on Computer and Robot Vision", "year": "2009", "authors": "Minglun Gong; Yee-Hong Yang"}, {"ref_id": "b10", "title": "Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn", "journal": "", "year": "2017", "authors": "Kaiming He; Georgia Gkioxari"}, {"ref_id": "b11", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "X Kaiming He; Shaoqing Zhang; Jian Ren;  Sun"}, {"ref_id": "b12", "title": "Context-aware image matting for simultaneous foreground and alpha estimation", "journal": "", "year": "2019", "authors": "Qiqi Hou; Feng Liu"}, {"ref_id": "b13", "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "journal": "ArXiv", "year": "2015", "authors": "S Ioffe; Christian Szegedy"}, {"ref_id": "b14", "title": "Kaiming He, and Ross Girshick. Pointrend: Image segmentation as rendering", "journal": "", "year": "2020", "authors": "Alexander Kirillov; Yuxin Wu"}, {"ref_id": "b15", "title": "A closed-form solution to natural image matting. IEEE transactions on pattern analysis and machine intelligence", "journal": "", "year": "2007", "authors": "Anat Levin; Dani Lischinski; Yair Weiss"}, {"ref_id": "b16", "title": "Spectral matting. IEEE transactions on pattern analysis and machine intelligence", "journal": "", "year": "2008", "authors": "Anat Levin; Alex Rav-Acha; Dani Lischinski"}, {"ref_id": "b17", "title": "Hierarchical opacity propagation for image matting", "journal": "", "year": "2020", "authors": "Yaoyi Li; Qingyao Xu; Hongtao Lu"}, {"ref_id": "b18", "title": "Look into person: Joint body parsing & pose estimation network and a new benchmark", "journal": "", "year": "2018", "authors": "Xiaodan Liang; Ke Gong; Xiaohui Shen; Liang Lin"}, {"ref_id": "b19", "title": "Boosting semantic human matting with coarse annotations", "journal": "", "year": "2020", "authors": "Jinlin Liu; Yuan Yao; Wendi Hou; Miaomiao Cui; Xuansong Xie; Changshui Zhang; Xian-Sheng Hua"}, {"ref_id": "b20", "title": "Indices matter: Learning to index for deep image matting", "journal": "", "year": "2019", "authors": "Hao Lu; Yutong Dai; Chunhua Shen; Songcen Xu"}, {"ref_id": "b21", "title": "Rectified linear units improve restricted boltzmann machines", "journal": "", "year": "2010", "authors": "V Nair; Geoffrey E Hinton"}, {"ref_id": "b22", "title": "Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style", "journal": "", "year": "2019", "authors": ", S Adam Paszke; Francisco Gross; A Massa; J Lerer; G Bradbury; T Chanan; Z Killeen; N Lin; L Gimelshein; Alban Antiga; Andreas Desmaison; E K\u00f6pf; Zach Yang;  De-Vito"}, {"ref_id": "b23", "title": "Video background replacement without a blue screen", "journal": "", "year": "1999", "authors": "J Richard; M Ibrahim Qian;  Sezan"}, {"ref_id": "b24", "title": "Attention-guided hierarchical structure aggregation for image matting", "journal": "", "year": "2020", "authors": "Yu Qiao; Yuhao Liu; Xin Yang; Dongsheng Zhou; Mingliang Xu; Qiang Zhang; Xiaopeng Wei"}, {"ref_id": "b25", "title": "A perceptually motivated online benchmark for image matting", "journal": "IEEE", "year": "2009", "authors": "Christoph Rhemann; Carsten Rother; Jue Wang; Margrit Gelautz; Pushmeet Kohli; Pamela Rott"}, {"ref_id": "b26", "title": "Mobilenetv2: Inverted residuals and linear bottlenecks", "journal": "", "year": "2018", "authors": "Mark Sandler; Andrew Howard; Menglong Zhu; Andrey Zhmoginov; Liang-Chieh Chen"}, {"ref_id": "b27", "title": "Background matting: The world is your green screen", "journal": "", "year": "2006", "authors": "Soumyadip Sengupta; Vivek Jayaram; Brian Curless; M Steven; Ira Seitz;  Kemelmacher-Shlizerman"}, {"ref_id": "b28", "title": "Deep automatic portrait matting", "journal": "Springer", "year": "2016", "authors": "Xiaoyong Shen; Xin Tao; Hongyun Gao; Chao Zhou; Jiaya Jia"}, {"ref_id": "b29", "title": "", "journal": "Poisson matting. In ACM Transactions on Graphics", "year": "2002", "authors": "Jian Sun; Jiaya Jia; Chi-Keung Tang; Heung-Yeung Shum"}, {"ref_id": "b30", "title": "Learning-based sampling for natural image matting", "journal": "", "year": "2002", "authors": "Jingwei Tang; Yagiz Aksoy; Cengiz Oztireli; Markus Gross; Tunc Ozan Aydin"}, {"ref_id": "b31", "title": "Image and video matting: a survey. Foundations and Trends\u00ae in Computer Graphics and Vision", "journal": "", "year": "2008", "authors": "Jue Wang;  Michael F Cohen"}, {"ref_id": "b32", "title": "Deep image matting", "journal": "", "year": "2005", "authors": "Ning Xu; Brian Price; Scott Cohen; Thomas Huang"}, {"ref_id": "b33", "title": "Pose2seg: Detection free human instance segmentation", "journal": "", "year": "2019", "authors": "Song-Hai Zhang; Ruilong Li; Xin Dong; Paul Rosin; Zixi Cai; Xi Han; Dingcheng Yang; Haozhi Huang; Shi-Min Hu"}, {"ref_id": "b34", "title": "A late fusion cnn for digital matting", "journal": "", "year": "2019", "authors": "Yunke Zhang; Lixue Gong; Lubin Fan; Peiran Ren; Qixing Huang; Hujun Bao; Weiwei Xu"}, {"ref_id": "b35", "title": "Fast deep matting for portrait animation on mobile phone", "journal": "ACM", "year": "2017", "authors": "Bingke Zhu; Yingying Chen; Jinqiao Wang; Si Liu; Bo Zhang; Ming Tang"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: We introduce two large-scale matting datasets containing 240k unique frames and 13k unique photos. troduce 2 additional datasets.VideoMatte240K. We collect 484 high-resolution green screen videos and generate a total of 240,709 unique frames of alpha mattes and foregrounds with chroma-key software Adobe After Effects. The videos are purchased as stock footage or found as royalty-free materials online. 384 videos are at 4K resolution and 100 are in HD. We split the videos by 479 : 5 to form the train and validation sets. The dataset consists of a vast amount of human subjects, clothing, and poses that are helpful for training robust models. We are releasing the extracted alpha mattes and foregrounds as a dataset to the public. To our knowledge, our dataset is larger than all existing matting datasets publicly available by far, and it is the first public video matting dataset that contains continuous sequences of frames instead of still images, which can be used in future research to develop models that incorporate motion information.PhotoMatte13K/85. We acquired a collection of 13,665 images shot with studio-quality lighting and cameras in front of a green-screen, along with mattes extracted via chroma-key algorithms with manual tuning and error repair. We split the images by 13,165 : 500 to form the train and validation sets. These mattes contain a narrow range of poses but are high resolution, averaging around 2000\u00d72500, and include details such as individual strands of hair. We refer to this dataset as PhotoMatte13K. However privacy and licensing issues prevent us from sharing this set; thus, we also collected an additional set of 85 mattes of similar quality for use as a test set, which we are releasing to the public as PhotoMatte85. In Figure2we show examples", "figure_data": ""}, {"figure_label": "34", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :Figure 4 :34Figure3: The base network Gbase (blue) operates on the downsampled input to produce coarse-grained results and an error prediction map. The refinement network Grefine (green) selects error-prone patches and refines them to the full resolution.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": ") We initialize our model's backbone and ASPP module with DeepLabV3 weights pre-trained for semantic segmentation on ImageNet and Pascal VOC datasets. We first train the base network till convergence and then add the refinement module and train it jointly. We use Adam optimizer and c = 4, k = 5, 000 during all the training. For training only the base network, we use batch size 8 and learning rate [1e-4, 5e-4, 5e-4] for backbone, ASPP, and decoder. When training jointly, we use batch size 4 and learning rate [5e-5, 5e-5, 1e-4, 3e-4] for backbone, ASPP, decoder, and refinement module respectively.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure5: Qualitative comparison on real images. We produce superior results at high-resolution with minimal user input. While FBA is competitive, it fails in a fully automatic application where the segmentation-based trimap is faulty.", "figure_data": ""}, {"figure_label": "78", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 7 :Figure 8 :78Figure 7: Effect of refinement, from coarse to HD and 4K.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 9 :9Figure9: Failure cases. Our method fails when the subject casts a substantial shadow on, or matches color with, the background (top) and when the background is highly textured (bottom).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "BRk\" and \"C*k\" follow the same definition above except that the convolution does not use padding.Refiner first resamples coarse outputs \u03b1 c , F R c , H c , and input images I, B to1 2 resolution and concatenates them as[n \u00d7 42 \u00d7 h 2 \u00d7 w 2 ]features. Based on the error prediction E c , we crop out top k most error-prone patches [nk\u00d742\u00d78\u00d78]. After applying the first stage, the patch dimension becomes [nk \u00d7 16 \u00d7 4 \u00d7 4]. We upsample the patches with nearest upsampling and concatenate them with patches at the corresponding location from I and B to form [nk \u00d7 22 \u00d7 8 \u00d7 8]", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 10 :10Figure 10: More examples from VideoMatte240K dataset.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 11 :11Figure 11: Training samples with augmentations. Bottom row are samples with shadow augmentation. Actual samples have different resolutions and aspect ratios.", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 13 :13Figure13: Additional qualitative comparison (1/3). Average user ratings between Ours and BGM are included. A score of -10 denotes BGM is \"much better\", -5 denotes BGM is \"slightly better\", 0 denotes \"similar\", +5 denotes Ours is \"slightly better\", +10 denotes Ours is \"much better\". Our method receives an average 3.1 score.", "figure_data": ""}, {"figure_label": "15", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 15 :15Figure 15: Additional qualitative comparisons (3/3)", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "User study results: Ours vs BGM", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Speed measured on Nvidia RTX 2080 TI as PyTorch model pass-through without data transferring at FP32 precision and with batch size 1. GMac does not account for interpolation and cropping operations. For the ease of measurement, BGM and FBAauto use adapted PyTorch DeepLabV3+ implementation with ResNet101 backbone as segmentation.", "figure_data": "Method BackboneParametersSizeFBA34,693,031 138.80 MBFBA auto89,398,348 347.48 MBBGM72,231,209 275.53 MBResNet-50*40,247,703 153.53 MBOursResNet-10159,239,831 225.98 MBMobileNetV25,006,83919.10 MB"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Model size comparison. BGM and FBAauto use DeepLabV3+ with Xception backbone for segmentation.", "figure_data": "Natural captureBGMOursGreen screen captureDaVinciOurs"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": ", and still obtains better metrics than BGM.", "figure_data": "AlphaFGMethodSAD MSE Grad Conn MSEOurs*12.86 12.01 8426 11116 5.31+ AIM14.19 14.70 9629 12648 6.34-PhotoMatte13K 14.05 14.10 10102 12749 6.53-VideoMatte240K 15.17 17.31 11907 13827 7.04-Distinctions15.95 19.51 11911 14909 14.36BGM16.07 21.00 15371 14123 42.84"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Effect of removing or appending a dataset in the training pipeline, evaluated on the AIM test set.", "figure_data": "BaseRefineAlphaFGBackboneKernel SAD MSE Grad Conn MSEBGM a16.02 20.18 24845 14900 43.00MobileNetV2 3\u00d73 10.53 9.62 7904 8808 8.19ResNet-50*3\u00d73 9.19 7.08 6345 7216 6.10ResNet-1013\u00d73 9.30 6.82 6191 7128 7.68ResNet-501\u00d71 9.36 8.06 7319 7395 6.92"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Comparison of backbones and refinement kernels on the Distinctions test set", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Performance with different k values. Measured on our method with ResNet-50 backbone at HD.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Background The keywords we use for crawling background images are:", "figure_data": "airport interioratticbar interiorbathroombeachcitychurch interior classroom interiorempty cityforestgarage interiorgym interiorhouse outdoorinteriorkitchenlab interiorlandscapelecture hallmall interior night club interiorofficerainy woodsrooftopstadium interiortheater interiortrain stationwarehouse interiorworkplace interior"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "the training order, epochs, and hours of our final model on different datasets. We use 1\u00d7RTX 2080 TI when training only the base network and 2\u00d7RTX 2080 TI when training the network jointly.", "figure_data": "DatasetNetworkEpochs HoursVideoMatte240K G base824VideoMatte240K G base + G refine112PhotoMatte13KG base + G refine258DistinctionsG base + G refine308"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "Training epochs and hours on different datasets. Time measured on model with ResNet-50 backbone.", "figure_data": ""}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "Performance on different GPUs. Measured with batch size 1 and FP32 precision.", "figure_data": "BackboneReso Batch PrecisionFPS1FP32100.6HD8FP32138.4MobileNetV28FP16200.04K8FP1664.2"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "Performance using different batch sizes and precisions. Measured on RTX 2080 TI.", "figure_data": "+2.0-3.3+1.8+0.1+3.3+9.0+7.5+7.1-2.6-1.3-0.82.4"}], "formulas": [{"formula_id": "formula_0", "formula_text": "L \u03b1 = ||\u03b1 \u2212 \u03b1 * || 1 + ||\u2207\u03b1 \u2212 \u2207\u03b1 * || 1 .", "formula_coordinates": [5.0, 94.43, 98.93, 147.61, 11.72]}, {"formula_id": "formula_1", "formula_text": "L F = ||(\u03b1 * > 0) * (F \u2212 F * ))|| 1 .", "formula_coordinates": [5.0, 100.51, 150.61, 135.46, 11.72]}, {"formula_id": "formula_2", "formula_text": "L E = ||E \u2212 E * || 2 .", "formula_coordinates": [5.0, 129.33, 226.21, 77.81, 11.72]}, {"formula_id": "formula_3", "formula_text": "(\u03b1 c , F R c , E c , H c ) = G base (I c , B c ) op- erates at 1", "formula_coordinates": [5.0, 50.11, 324.28, 236.25, 22.49]}, {"formula_id": "formula_4", "formula_text": "L base = L \u03b1c + L Fc + L Ec . (4", "formula_coordinates": [5.0, 113.46, 363.65, 169.03, 9.81]}, {"formula_id": "formula_5", "formula_text": ") The refinement network (\u03b1, F R ) = G refine (\u03b1 c , F R c , E c , H c , I, B", "formula_coordinates": [5.0, 50.11, 363.97, 236.25, 36.14]}, {"formula_id": "formula_6", "formula_text": "L refine = L \u03b1 + L F .(5", "formula_coordinates": [5.0, 129.48, 403.38, 153.01, 9.81]}, {"formula_id": "formula_7", "formula_text": "I = \u03b1F + (1 \u2212 \u03b1)B.", "formula_coordinates": [12.0, 50.11, 548.59, 86.68, 8.74]}], "doi": ""}