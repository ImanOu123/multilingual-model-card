{"title": "Deformable Markov Model Templates for Time-Series Pattern Matching", "authors": "", "pub_date": "2000-03", "abstract": "This paper addresses the problem of automatically detecting specific patterns or shapes in timeseries data. A novel and flexible approach is proposed based on segmental semi-Markov models. Unlike dynamic time-warping or template-matching, the proposed framework provides a systematic and coherent framework for leveraging both prior knowledge and training data. The pattern of interest is modeled as a K-state segmental hidden Markov model where each state is responsible for the generation of a component of the overall shape using a state-based regression function. The distance (in time) between segments is modeled as a semi-Markov process, allowing flexible deformation of time. The model can be constructed from a single training example. Recognition of a pattern in a new time series is achieved by a recursive Viterbi-like algorithm which scales linearly in the length of the sequence. The method is successfully demonstrated on real data sets, including an application to end-point detection in semiconductor manufacturing.", "sections": [{"heading": "Introduction", "text": "A fundamental problem in pattern recognition and data mining is the problem of automatically recognizing specific waveforms in time-series based on their shapes. Applications in the context of time-series data mining include exploratory data analysis of time-series, monitoring and diagnosis of critical systems, classification of time-series, and unsupervised discovery of recurrent patterns. We propose a novel Markov-based representation for waveform shapes and couple this to an efficient and optimal Viterbi-like algorithm for online waveform detection. The detection algorithm is optimal in the maximum likelihood sense of detecting the time-series segment which is most likely to have been generated by the waveform model.\nMuch work on this problem in the data mining literature has emphasized the issue of scalability in this context Agrawal et al. (1993); Faloutsos et al. (1994); Agrawal et al. (1995); Chan and Fu (1999), i.e., being able to scale one's representation method and matching algorithm to massive time-series archives. In this paper we explicitly focus on fundamental signal representation and matching aspects of the problem, rather than scalability. We believe that the representation and matching problems are still not adequately solved (we explain why in Section 3) and, thus, our philosophy is that these issues need to be addressed first before scalability is considered. Having said all of this, we will discuss in section 5 how our approach can be scaled up in an efficient fashion. However, the main focus is on representation and matching.\nIn the following sections of the paper, we begin by providing a general definition of the problem (Section 2). In Section 3 we then discuss related prior work on this problem. In Section 4 we propose our new segmental semi-Markov model framework for pattern representation. A specific pattern-detection algorithm is described in Section 5, followed by results and evaluation in Section 6.", "publication_ref": ["b0", "b6", "b1", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Statement of Problem", "text": "Consider that we have a waveform pattern Q of interest and we wish to detect any occurrences of this pattern Q in a (potentially much longer) time-series R. We will assume that Q (for \"query\" pattern) is a univariate waveform, uniformly sampled in time, with a distinct \"shape.\" Generalizations to multivariate and non-uniformly-sampled waveforms are relatively straightforward and not discussed in this paper due to space limitations.\nFigure 1(a) provides a simple example of such a pattern Q and its shape. This particular pattern signals the end of what is known as the plasma etch process in semiconductor manufacturing. The end of the process is referred to as the end-point. A skilled semiconductor engineer can manually interpret and detect this pattern offline after the entire process is complete. However, in a real operational manufacturing environment the pattern must be detected online in real-time, as sensor data is measured (this automated detection enables automated control of the process). Current detection technology using existing sensors relies on threshold-based techniques for endpoint detection. This approach typically only works well if the end-point pattern consists of a simple increase or decrease in the level of the sensor. However, more sophisticated sensors can generate relatively complex endpoint patterns, such as the waveform in Figure 1(a), and thresholding is no longer applicable. An example of a new run of the process is provided in Figure 1(b), where the estimated location of the pattern Q has been determined (manually) in the indicated window. Accurate online detection of the pattern Q is critical for automatic control of the etch step in the process: if the etch-step ends too early or too late the semiconductor wafer will not be etched properly, resulting in significant financial loss. We will return to this particular semiconductor manufacturing problem in more detail in Section 6 where we discuss experimental results.\nThis same type of waveform recognition problem (also sometimes referred to as \"subsequence matching\") occurs in a variety of data mining contexts. In interactive data exploration of time-series archives (for example in finance or marketing) a data analyst may wish to know if this week's pattern has ever occurred before, or if the weekly pattern of customer-visits over time at one particular store is common to other stores. In diagnosis and fault detection an engineer may wish to query an archival database in real-time to determine what past situations (contexts) are most similar to the current sensor pattern Q (e.g., see Keogh and Smyth (1997) for an example from space shuttle sensor monitoring). A third data mining task is unsupervised discovery of patterns in time-series; any data mining algorithm that tries to discover recurring (previously unknown) patterns in a data set will need to be able to solve this \"waveform matching\" problem as a primitive operation to support such unsupervised discovery (e.g., Das et al. (1998)). Thus, the problem of detecting waveforms in time-series has broad applicability and relevance to data mining of time-series data.\nThe inherent difficulty of this type of recognition problem typically stems from the inherent variability in both the waveform Q to be detected and the \"background\" variability in the timeseries R i . For example, based on the physics of the associated plasma etching process, it is known that the specific shape of the \"end-point pattern\" Q in Figure 1(a) can vary substantially from run to run. Nonetheless, each realization of the pattern still possesses the same inherent general shape characteristics that allows a human observer to detect it in a relatively straightforward, almost gestalt, manner.\nA key point here is the notion of shape variability. If we can characterize systematically the manner in which a shape can vary in \"shape-space\" then in principle we have a sound footing from which to engineer a detection system since we can characterize which types of variations are expected and which are not. This point has not escaped researchers in the computer vision and image analysis community, who have pioneered the notion of shape-space variability for the potentially more difficult problem of two-dimensional pattern detection in recent years. Work such as Amit et al. (1991); Mardia and Dryden (1998) is seminal in this context, using the notion of identifying landmark points to represent the shape of an outline in 2d, and then characterizing shape variability among a population of such outlines in terms of a shape-space distributions on the vector of landmarks. Our work here is inspired by this general shape-space framework, but applied to 1-dimensional waveform shapes. By leveraging the natural constraints imposed by a 1dimensional time axis (rather than 2d images), we are able to use a richer underlying representation for our shapes than simply landmark-based models (namely generative state-based Markov models), in turn leading to efficient and accurate detection algorithms. ", "publication_ref": ["b11", "b5", "b2", "b14"], "figure_ref": ["fig_0", "fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Background and Motivation", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "The simplest (and weakest) approach to this problem is to use direct template-matching (aka sequential scanning), i.e., compute the amplitude distance, point by point, between the pattern waveform Q and subwindows within R, and calculate the mean-squared error. If the error is below some threshold a detection is declared. Figure 2 illustrates the limitations of this approach, using our plasma etch data as an example. There are several minima of the root mean square error (RMSE) for different subwindows in R, several of which are below the RMSE value at the \"obvious\" location based on human judgment at about time point 240, where this time point was determined by an engineer familiar with the process. A number of variations on this theme allow for some slack in the matching process. However, an underlying problem is that there is no notion of variability in the template Q. Thus, even small variations in a new occurrence of the waveform may lead to large RMSE distances and large variations in new waveforms may produce small RMSE distances (e.g., the local minima around times 40, 95,175 and 275 in the bottom portion of Figure 2). Dynamic time-warping (DTW) generalizes the templatematching approach to explicitly allow for some slack in the matching of the time-axes of Q and R. How much slack is tolerated is encoded by the choice of distance function in the DTW matching algorithm. Berndt and Clifford (1994) introduced the concept to time-series data mining. The main problem with DTW in a data mining context is that construction of an effective distance measure can be highly non-trivial and very problem-dependent. A second general problem is that DTW focuses only on one specific type of pattern variability, namely elasticity in time, whereas in practice other deformations may also be present.\nThere has bee substantial interest in this problem in the data mining literature (e.g., Agrawal et al. (1993); Faloutsos et al. (1994); Agrawal et al. (1995); Shatkay and Zdonik (1996); Yi et al. (1998); Chan and Fu (1999); Huang and Yu (1999); Keogh and Pazzani (1999)). Much of this work can be characterized procedurally in the following general manner: (1) find an approximate and robust representation for the time-series (e.g., Fourier coefficients, piecewise linear models, etc.), (2) define a flexible matching function which can handle various pattern deformations (scalings, transformations, don't cares, etc), and (3) provide an efficient scalable algorithm, using this representation and this matching function, for massive time-series data sets. While these approaches in general provide a wealth of useful heuristics for waveform matching, they do not explicitly account for uncertainty in the matching process, i.e., there are no probabilistic semantics associated with the matching process, Consequently, one cannot quantify in general the inherent uncertainty associated with any detection decision. A second (related) limitation is that they do not provide a coherent quantitative mechanism for adaptation, i.e., either adapting the detection algorithm on the basis of training data and/or via prior knowledge. As we will see in later sections, the probabilistic probabilistic framework in this paper can handle these issues in a systematic and straightforward manner.", "publication_ref": ["b3", "b0", "b6", "b1", "b16", "b19", "b4", "b9", "b12"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Motivation for our Approach", "text": "All of the work above can be characterized as being distance-based (where we use the word \"distance\" in the loose sense of computing some scalar dissimilarity measure between two waveforms, rather than a formal distance metric). In other words, the focus of these approaches is largely based on defining flexible distance measures for matching two waveforms Q and Q , given some suitable underlying representation for the waveforms. A significant practical problem here is that a distance measure that is optimized for one type of pattern and application domain (e.g., detecting arrythmia patterns in cardiac monitoring) may be entirely inappropriate (and ineffective) when applied to a different domain (e.g., financial data analysis). Thus, we question whether it is really feasible to find \"general-purpose\" universal distance metrics which are broadly useful. The question remains of course how then should one invent a new distance metric for every new application?\nOne answer is provided by the general approach of probabilistic generative modeling, which is fundamentally different to the distance based concept. In simple terms, this means that we construct a model for Q (call it M Q , typically it defines a probability distribution on waveforms). From this model we can generate or simulate sample waveforms (i.e., other realizations of waveforms Q from an assumed data generating process M Q ). Typically this model consists of a mean shape and a distribution function which describes variation about this mean shape.\nFrom a matching perspective an important point is that we can measure the \"similarity\" of any new pattern Q to our model M Q simply by computing p(Q |M Q ), the likelihood that Q came from M Q . (Usually we take negative log-likelihood to be a distance function, \u2212 log p(Q |M Q ), which will be smaller the closer Q is to the model M Q ). The generality and simplicity of the likelihood definition underlies both the elegance and power of the approach. The distance measure (between Q and Q) is implicitly specified by \u2212 log p(Q |M Q ) once the model M Q is defined. In other words there is no need to construct an ad hoc distance measure between patterns, since it is automatically defined by the likelihood of the model.\nFor example, if M Q generates patterns of fixed length with a specific mean shape and additive Gaussian noise, our likelihood measure (more specifically the negative of the log of the Gaussian likelihood, log p(Q |M Q )) simply reduces to the Euclidean distance between Q and Q . The power of the approach lies in how we can generalize this concept to much more expressive models M Q . Indeed this general idea is not new and implicitly permeates much work in statistical pattern recognition. The hidden Markov model (HMM) approach to speech recognition uses precisely this framework where Q is an observed acoustic waveform and we have a set of M Q 's for different words.\nIn a data mining context, Keogh and Smyth (1997) proposed a version of probabilistic generative models, but where the probabilistic model was defined on the observed deformations between two waveforms, i.e., the method as proposed still required a definition of a distance measure. The flexible Markov generative model we propose in this paper is a more direct approach, as well as providing a more flexible modeling framework.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Semi-Markov and Segmental Markov Models for Waveform Representation", "text": "An influential idea in pattern recognition is recognition by parts, i.e., decomposing an object into a model composed of (a) individual components, and (b) the relations (temporal or spatial) between these components. Recognition then becomes a matter of detecting individual components and then \"parsing\" their likely configurations relative to each other. Following this line of thought, we choose to model a waveform as K distinct segments with constraints on how the segments are \"linked.\"", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Segmental Observation Models", "text": "We begin our discussion with a standard discrete-time finite-state Markov model where each segment in the data corresponds to a state of the Markov model. Let the number of states be K.\nThe parameters of the model include \u03c0, the initial state distribution (typically we will constrain the waveform to always begin with the same segment), and A, the K \u00d7 K state transition matrix (again, this transition matrix can be constrained to be a \"left-to-right\" model which enforces a strict ordering in time of the segments). Let y = y 1 y 2 . . . y t . . . y T be the observed waveform measurements. The corresponding states are defined as s = s 1 s 2 . . . s t . . . s T (i.e., segment labels) and are hidden (not observed directly). In this hidden Markov model, the joint distribution of the observed data sequence y and a state sequence s, can be factored as:\np(y, s) = T t=2 p(y t |s t )p(s t |s t\u22121 ) p(y 1 |s 1 )\u03c0(s 1 ),(1)\nWe have not yet described the functional form of the conditional densities p(y t |s t ) which relate the observed data to the hidden states. In the standard HMM framework (e.g., in speech recognition) the real-valued y t 's are often modeled as Gaussians or mixtures of Gaussians. For Gaussians, this implies a piecewise constant process with one mean \u00b5 i per state i with additive Gaussian noise. Mixtures allow switching between multiple means per state, but still imply a constant \"shape\" process within each state as a function of time.\nFor waveform modeling this assumption of a constant shape plus noise is often inappropriate, e.g., the waveform pattern in Figure 1 could not be modeled parsimoniously using a piecewise constant model. A natural generalization of the constant model is to allow each state (or segment) to generate data in the form of a regression curve, i.e.,\ny t = f i (t|\u03b8 i ) + e t (2\n)\nwhere f i (t|\u03b8 i ) is a state-dependent regression function with parameters \u03b8 i and e t is additive independent noise (often assumed Gaussian, but not necessarily so). For Gaussian noise we have that p(y t |s t = i) is Gaussian with a mean f i (t) which is a function of time and with variance \u03c3 2 . Note that conditioned on the regression parameters \u03b8 i , the y t 's only depend on the current state s t , as in the standard regression framework (i.e., observations are conditionally independent of everything else given the current state and state regression parameters). Thus, the likelihood of the data can be still be expressed in the simple product form of Equation 1, but now the p(y t |s t ) terms are dependent on the time t (as in Equation 2) rather than being constant. An important point is that this product form for the likelihood makes both parameter estimation (model learning) and inference (pattern detection) relatively straightforward-we will take advantage of this fact when we apply this technique to waveform detection later in this paper. This segmental Markov model (Holmes and Russell, 1999) is a natural one for modeling waveform shapes. It decomposes the waveform Q into local segments, each of which consists of a parametric functional \"shape\" with additive noise, and the segments are \"linked\" in a Markov manner. The problem of finding the best-fitting parameters for this model, given observed data in the form of a particular waveform Q (or set of waveforms), is quite straightforward as long as the parameters \u03b8 i appear linearly in the shape functions f i (t|\u03b8 i ). Figure 3 shows a simple example of the output of a simulated segmental Markov model. The process begins in state A which produces observations according to a noisy linear regression model. After 30 time steps or so it transitions to state B and produces observations according to a noisy exponential decay model.", "publication_ref": ["b8"], "figure_ref": ["fig_0", "fig_2"], "table_ref": []}, {"heading": "Duration Modeling with Semi-Markov Processes", "text": "In the standard Markov framework the distribution of the durations of the system in state i is given by\np i (t d = d) = a d\u22121 ii (1 \u2212 a ii ) (3)\nwhere a ii is the self-loop transition probability of state i and d is the number of time-steps spent in state i. In other words, the Markov assumption constrains the state-duration distributions to be geometric in form. In reality we will want a more flexible way to model duration distributions to reflect the fact that each segment of the waveform being modeled has a typical duration length (mean time) and some variability around that mean time. The problem of modifying the standard Markov model to allow for arbitrary state-durations can be addressed by the use of semi-Markov models (e.g., Ferguson 1980). A semi-Markov model has the following generative description:\n\u2022 On entering state i a duration time t d is drawn from a state-duration distribution p i (t d ).\n\u2022 The process remains in state i for time t d .\n\u2022 At time t d the process transitions to another state according to a transition matrix A, and the process repeats.\nThe state-duration distributions, p i (t d ), 1 \u2264 i \u2264 M , can be modeled using parametric distributions (such as log-normal, Gamma, etc) or non-parametrically by mixtures, kernel densities, etc. If t d is constrained to take only integer values we get a discrete-time semi-Markov model. For waveform modeling, by including the state-duration distributions in the model, we can encode a prior on how long we expect the process to remain in each state. We can combine this semi-Markov approach with the segmental hidden Markov model described in the last section. This gives us a flexible framework for defining distributions p(Q) over waveforms Q. This waveform model allows us to specify shape of the waveform within each segment, the mean and variance of the duration length for each segment, and the ordering of the segments. Having a probabilistic model for our waveforms has certain distinct advantages that are worth mentioning explicitly:\n\u2022 we can estimate the parameters of our model from data,\n\u2022 we can combine these estimates with prior knowledge using Bayesian priors, and\n\u2022 we can quantify the likelihood that the waveform Q occurs in any arbitrary position in a time-series R.\nWe elaborate further on these issues in the discussion in the next section below. ", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "Waveform Pattern Matching", "text": "Given the general framework presented above we now formulate a specific algorithm for detecting a waveform pattern Q in a time-series R. Specifically, given an example of the waveform Q, we show how to construct a semi-Markov segmental hidden Markov model to fit this waveform, and then give a computationally efficient solution for detecting any instances of this waveform which are embedded in a time-series R.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Constructing a Waveform Model from Data", "text": "The construction of the model begins with a piecewise linear representation of the example waveform pattern using a standard piecewise linear segmentation algorithm (Imai and Iri, 1986;Zhu and Seneviratne, 1997). For illustration, we show in Figure 4 the piecewise linear representation of the example waveform pattern Figure 1(a), where we arbitrarily set the error tolerance = 1.0 for the segmentation algorithm so that the approximating error at each point will not exceed in amplitude on this data. We could also automatically set the error tolerance by calculating the noise scale in the data, e.g., by filtering, or Fourier transformations. We are assuming each segment to be linear, although polynomials, splines, etc., could also be used.\nLet the number of segments in the pattern be K. We construct a K-state segmental HMM each state of which corresponds to one segment in the piecewise linear representation. Because one can only start from segment 1, then go to segment 2, etc., the transition matrix A will be left-to-right, i.e., A i,i+1 = 1, A i,j = 0 if j = i + 1 and A i,j is the probability of going to state j given that the process is in state i. The initial state distribution will be \u03c0 = [1, 0, . . . , 0]. The output probability distribution of state i will be of the form\np(y m+1 y m+2 . . . y m+d i |s i ) = p(d i |s i )p(\u03b8 i |s i ) m+d i t=m+1 p(y t |f i (\u03b8 i , t))(4)\nwhere\n\u2022 p(d i )\nis the probability of the duration d i (i.e., the length of the segment in time),\n\u2022 \u03b8 i is state i's parameter of the regression function which has functional form f i (\u03b8 i , t),\n\u2022 p(y t |f i (\u03b8 i , t)) is assumed to be a Gaussian distribution with mean f i (\u03b8 i , t) and variance \u03c3 2 y .\nSince we are using piecewise linear representation of the example waveform pattern here, the regression function f i (\u03b8 i , t) will be a linear function f i (\u03b8 i , t) = b i t + c i . Of the two parameters, the intercept c i is ignored in the model and allowed to be freely fit in the detection process to allow shifting in time. Thus, \u03b8 i includes only b i which is set to be the slope of the segment in the example waveform pattern. In the absence of prior knowledge about how this slope will change, for simplicity, we assume \u03b8 i to be fixed (non-random) in Equation 4. The state duration distribution p(d i |s i ) for state i is set to be a left-truncated Gaussian distribution with mean being l i , the length (in time) of the corresponding segment in the example pattern, and standard deviation being l i \u00d7 k% (where k was set to 20 for the results reported below).\nThe variance of the additive noise, \u03c3 2 y , is set to be the mean squared error of the piecewise linear representation (as fitted to the original data).\nAs described here the training procedure essentially amounts to setting the means and variances in an appropriate data-dependent manner based on a single waveform observation. The generalization to training a model from multiple waveforms is straightforward, if all waveforms are represented by the same number of linear segments. A completely unsupervised approach (using multiple waveforms) is to directly train a semi-Markov segmental hidden Markov model using the EM algorithm. Here the number of segments (states) is fixed a priori and EM determines the mean lengths and shapes for each segment by maximizing the overall likelihood of the observed waveform data.", "publication_ref": ["b10", "b20"], "figure_ref": ["fig_3", "fig_0"], "table_ref": []}, {"heading": "A Viterbi-like Algorithm to Compute the Most Likely State Sequence", "text": "Once we have a semi-Markov segmental hidden Markov model M Q for our waveform(s) Q, a basic task is to find the most likely state (i.e., segment label) sequence\u015d = s 1 s 2 . . . s t . . . for a data sequence y = y 1 y 2 . . . y t . . ..\nHere we give a recursive Viterbi-like algorithm based on dynamic programming. At each time t, this algorithm calculates the quantityp\n(t) i for each for each state i, 1 \u2264 i \u2264 K, wherep (t) i is defined asp (t) i = max s {p(s|y 1 y 2 . . . y t )|s = s 1 s 2 . . . s t , s t = i}.(5)\nIn other words,p\ni is the likelihood of the most likely state sequence that ends with state i (i.e., y t is the last point of segment i). function s 1 s 2 . . . s t = MLSS(y 1 y 2 . . . y t ) 1. for each state i 2.\nComputep (t) i , P REV (i, t); 3. end for 4. j = argmax ip (t) i ; 5. [j , t ] = P REV (j, t); 6. for k = t + 1 to t 7.\ns k = j; 8. end for 9. if (t > 0) 10.\n[j, t] = [j , t ]; 11. goto 3; 12. else 13.\nreturn; 14. end if procedure DETECT(y 1 y 2 . . . y t ...) 1. t = 1; 2. s 1 s 2 . . . s t = MLSS(y 1 y 2 . . . y t ); 3. if (s t == K) 4.\ndeclare 'found'; 5.\nstop; 6. else 7.\nt = t + 1; 8. goto 2; 9. end if Figure 5: Pseudo-code for MLSS (finding most likely state sequence s 1 s 2 . . . s t for data sequence y 1 y 2 . . . y t ), and DETECT (online detection of waveform).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The recursive function for calculatingp", "text": "(t) i i\u015d p (t) i = max d i max jp (t\u2212d i ) j A ji p(d i )p(y t\u2212d i +1 . . . y t |\u03b8 i )(6)\nIn the above equation, the outer maximization (max d i ) is over all possible values of the duration d i of state i. Recall that y t is now fixed to be the last point of segment i, the last point of the previous segment will be t \u2212 d i . For a given d i , the inner maximization (max j ) is over all possible previous states j that transitions to state i at time t \u2212 d i . The state j and the time t \u2212 d i for the maximum valuep (t)\ni are recorded in P REV (i, t). Obviously, the overall most likely state sequence for the data sequence y 1 y 2 . . . y t will be the state sequence with the likelihood max ip (t) i , and can be found by tracing back using P REV (i, t) (see the pseudocode \"MLSS\" in Figure 5). Note that this detection procedure is optimal in a maximum likelihood sense, i.e., it finds the state-sequence which is most likely to account for the observed data.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Online Detection of the Waveform", "text": "To detect a waveform inside a (much longer) time series y 1 y 2 . . . y t . . ., an obvious approach would be to match the model against every subwindow y i y i+1 . . . y j , find the most likely state sequence s i s i+1 . . . s j , and declare \"found\" if the likelihood is above a certain threshold. The problems with this approach are (1) how to set the likelihood threshold and (2) the redundant computation from the fact that the computation for every subwindow is carried out from scratch, even if a subwindow overlaps with another subwindow.\nTo deal with these problems, we augment the model with two extra \"background\" states: a pre-pattern background state (state 0) to model the data before the pattern, and a post-pattern background state (state K + 1) for the data after the pattern. This augmented model may be seen as a \"global\" model that can be matched directly against the whole time series y i y i+1 . . . y j (instead of the subwindows). These background states are also called garbage states in the speech recognition literature (Wilpon et al., 1990). The parameters of the background states (e.g., the state duration distribution, the noise variance, parameter \u03b8 of the regression function, etc.) are estimated in a similar way to other (ordinary) states in the model. They could instead be set according to prior knowledge. For example, the probability distribution on the state duration of the pre-pattern background state (i.e., its length in time) can be set to reflect a prior probability distribution of the starting time of the pattern.\nWith this augmented model, we run the MLSS algorithm in Figure 5 online as new data points y 1 y 2 . . . y t . . . are coming in. If, at time t, s t = K in the most likely state sequence where K is the last segment in the waveform, we declare that the waveform is detected with end-time y t . See the pseudocode DETECT in Figure 5.", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "Complexity of the Method", "text": "The complexity of the above detection algorithm will be O(|R| \u00d7 (K + 2) \u00d7 C) where |R| is the size of the new time series R, K + 2 is the number of states (K pattern-states and 2 background states), and C is the complexity of evaluating Equation 6. Generally speaking, C will be O(v i \u00d7 (K + 1)) where v i is the number of possible values for d i (the duration of state i), K + 1 is the maximum number of predecessor states for any state. In the current setting where the transition matrix A is left-to-right, each state i has only one predecessor state, so C = O(v i ). Letting V = max i v i , the overall complexity of the algorithm is O(|R| \u00d7 K \u00d7 V ). Thus, this method is essentially linear in |R|.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results on Plasma Etch Process Data", "text": "Plasma etch (Manos and Flamm, 1989;Williams, 1997) is a critical process in semiconductor manufacturing. A semiconductor wafer is bombarded with a gas containing various chemical components within a plasma gas chamber. Different gas combinations are used in sequence to remove different layers from the wafer. Since there is no direct way of measuring when a layer has been etched through, control of the plasma process (i.e., when to halt gas flow so as to stop etching) is achieved by inferring the nature of the material being etched indirectly from the composition of the gas within the chamber. For example, from the interferometry data in Figure 1(a), an engineer can manually detect the endpoint by seeing the pattern (enclosed in the dotted rectangle). Here is where the pattern matching algorithm fits in. We would like to detect the same pattern in the interferometry data of future runs, e.g., Figure 1(b).\nFor comparison, we ran both the template-matching (Figure 2) and dynamic time warping (Figure 6) techniques. The global minima in the two resulting error curves do not correspond to    a pattern in the NYSE Composite Index (top), building the semi-Markov segmental model, then searching for this pattern in the NYSE Utility Index (bottom). The algorithm correctly finds a similar pattern.", "publication_ref": ["b13", "b17"], "figure_ref": ["fig_0", "fig_0", "fig_1", "fig_5"], "table_ref": []}, {"heading": "Results on ECG Time Series", "text": "The third data set (Figure 9) is an ECG time series (http://www.ms.washington.edu/~s530/ data.html) from Percival and Walden (2000) . A heart-beat pattern was selected from an early part of the time-series (Figure 10, top) and used to build a semi-Markov segmental model using the default method described in Section 5.1. The online detection algorithm of Section 5.3 was then used to detect later occurrences of the pattern as in Figure 10 (bottom). Note that the general method described in Section 5.3 for online detection of a single pattern can easily be generalized to the problem of finding multiple repeating patterns (e.g., multiple repeating heart-beats as in this data set) by allowing transitions in the Markov model from the end state back to the start state, i.e., allowing repeated transitions through the state sequence.", "publication_ref": ["b15"], "figure_ref": ["fig_7", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Discussion and Conclusions", "text": "Overall we have found the proposed technique to be quite robust. The main parameter that is manually chosen is the standard deviation in the state duration models, which was set to 20% of the mean duration in the results described above. Generally speaking this should be a function of prior knowledge, i.e., how much variation in time do we expect in waveforms? On the data sets above we have found empirically that the method finds the ideal detection over a broad range of values both above and below 20%, indicating that the method is not overly sensitive to this parameter. The degree of sensitivity will of course in general be dependent on the nature of the both the waveform Q and the background time series R. Of course if one is training on multiple waveforms this variance term can be estimated directly.\nThere are a number of natural extensions of the proposed framework which we do not discuss in detail here due to space limitations. For example, in the semiconductor manufacturing process there are multiple runs (and associated endpoints) over time. One can extend the framework proposed here to include an online adaptive Bayesian component, where the model M Q is adapted over time as new time series are added to the archive. An interesting issue here is how to incorporate both subjective human judgement (supervised labeling of waveforms) with the type of unsupervised detection demonstrated here. Another research direction is to embed this framework within a hierarchical Bayesian model with random variation in the segment coefficients rather than keeping them fixed. The underlying semi-Markov segmental hidden Markov model for waveforms can also be generalized, allowing for further modeling flexibility such as stochastic grammars, hierarchical hidden Markov models, and so forth.\nIn summary, we have proposed a general model-based framework for waveform matching, using generalized Markov models to encode the shape of a waveform. We believe that this can provide a more flexible and practical methodology than competing distance-based methods. For example, we illustrate how the model can be learned from data and how we can perform detection in a relatively automatic fashion using basic likelihood concepts. Based on these results we conclude that the proposed semi-Markov segmental HMM appears to be quite a useful, flexible, and accurate framework for time-series waveform pattern detection.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We would like to thank Wenli Collison, Tom Ni, and David Hemker of LAM Research for providing the plasma etch data and for discussions on change-point detection in plasma etch processes. The research described in this paper was supported by NSF CAREER award IRI-9703120 and by the NIST Advanced Technology Program and KLA-Tencor.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "the best match, i.e., both minima correspond to false alarms around time t = 90 seconds.\nWe built a semi-Markov segmental model for the pattern in Figure 1(a), and ran the pattern matching algorithm (the DETECT procedure in Figure 5) on the new time series in Figure 1(b). At time t = 252, the DETECT procedure correctly detected the end of the pattern, i.e., the maximum of the score function agrees precisely with the engineer's subjective judgement on the location of the end-point.\nIt is also interesting to look at the normalized likelihood [p  7. It can be seen that the peak of the SCORE curve is at t = 252 which is when the online DETECT procedure finds the pattern.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results on Financial Data", "text": "A second dataset is the New York Stock Exchange (NYSE) daily index closes for 1999 (available online at http://www.nyse.com/marketinfo/stats/Nya99.prn.) This dataset contains, among others, the NYSE Composite Index, and the Utility Index, as shown in Figure 8. These two indices, like the two semiconductor manufacturing sensor runs, share some common patterns, but are still quite different from each other (e.g., the amplitude of the Composite Index is between 560 and 680, while the Utility Index is between 450 and 600.) In Figure 8, we show the results of selecting", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Efficient similarity search in sequence databases", "journal": "", "year": "1993-10", "authors": "R Agrawal; C Faloutsos; A Swami"}, {"ref_id": "b1", "title": "Fast similarity search in the presence of noise, scaling, and translation in time-series databases", "journal": "", "year": "1995-09", "authors": "R Agrawal; K.-I Lin; H S Sawhney; K Shim"}, {"ref_id": "b2", "title": "Structural image restoration through deformable templates", "journal": "Journal of the American Statistical Association", "year": "1991", "authors": "Y Amit; U Grenander; M Piccioni"}, {"ref_id": "b3", "title": "Using dynamic time warping to find patterns in time series", "journal": "", "year": "1994-07", "authors": "D J Berndt; J Clifford"}, {"ref_id": "b4", "title": "Efficient time series matching by wavelets", "journal": "", "year": "1999-03", "authors": "K.-P Chan; A W ; -C Fu"}, {"ref_id": "b5", "title": "Rule discovery from time series", "journal": "AAAI Press", "year": "1998", "authors": "G Das; K Lin; H Mannila; G Rengenathan; P Smyth"}, {"ref_id": "b6", "title": "Fast subsequence matching in time-series databases", "journal": "", "year": "1994-05", "authors": "C Faloutsos; M Ranganathan; Y Manolopoulos"}, {"ref_id": "b7", "title": "Variable duration models for speech", "journal": "", "year": "1980-10", "authors": "J D Ferguson"}, {"ref_id": "b8", "title": "Probabilistic-trajectory segmental HMMs", "journal": "Computer Speech and Language", "year": "1999", "authors": "W J Holmes; M J Russell"}, {"ref_id": "b9", "title": "Adaptive query processing for time-series data", "journal": "", "year": "1999", "authors": "Y.-W Huang; P S Yu"}, {"ref_id": "b10", "title": "An optimal algorithm for approximating a piecewise linear function", "journal": "Journal of Information Processing", "year": "1986", "authors": "H Imai; M Iri"}, {"ref_id": "b11", "title": "A probabilistic approach to fast pattern matching in time series databases", "journal": "", "year": "1997-08", "authors": "E Keogh; P Smyth"}, {"ref_id": "b12", "title": "An indexing scheme for fast similarity search in large time series databases", "journal": "", "year": "1999-07", "authors": "E J Keogh; M J Pazzani"}, {"ref_id": "b13", "title": "Plasma Etching, An Introduction", "journal": "Academic Press, Inc", "year": "1989", "authors": "D M Manos; D L Flamm"}, {"ref_id": "b14", "title": "Statistical Shape Analysis", "journal": "John Wiley & Sons", "year": "1998", "authors": "K V Mardia; I L Dryden"}, {"ref_id": "b15", "title": "Wavelet Methods for Time Series Analysis", "journal": "Cambridge University Press", "year": "2000", "authors": "D B Percival; A T Walden"}, {"ref_id": "b16", "title": "Approximate queries and representations for large data sequences", "journal": "", "year": "1996-02", "authors": "H Shatkay; S B Zdonik"}, {"ref_id": "b17", "title": "Plasma Processing of Semiconductors", "journal": "Kuwer Academic Publishers", "year": "1997", "authors": "P F Williams"}, {"ref_id": "b18", "title": "Automatic recognition of keywords in unconstrained speech using hidden Markov models", "journal": "IEEE Transactions on Acoustics, Speech and Signal Processing", "year": "1990-11", "authors": "J G Wilpon; L R Rabiner; C.-H Lee; E R Goldman"}, {"ref_id": "b19", "title": "Efficient retrieval of similar time sequences under time warping", "journal": "", "year": "1998-02", "authors": "B.-K Yi; H V Jagadish; C Faloutsos"}, {"ref_id": "b20", "title": "Optimal polygonal approximation of digitized curves. IEE proceedings. Vision, image, and signal processing", "journal": "", "year": "1997-02", "authors": "Y Zhu; L D Seneviratne"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: An example of an interferometry sensor from a semiconductor manufacturing process: (a) (top) a waveform pattern indicating the end of the plasma etch process is indicated with dotted line, (b) (bottom) another run of the same process where we wish to detect a similar pattern (indicated by dotted lines).", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Results of applying a simple template matching technique to the data in Figure 1.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: A simple illustration of the output of a simulated segmental Markov model. The solid lines show the underlying deterministic components of the regression models within each state, and the dotted lines show the actual noisy observations.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: The example waveform pattern Figure 1(a), and its piecewise linear representation.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 6 :6Figure 6: Results of applying dynamic time-warping to the data in Figure 1.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 7 :7Figure 7: Results of applying the semi-Markov segmental model to the data in Figure 1. The SCORE is [p (t) K ] 1 t , the normalized likelihood for state K (the last segment of the pattern).", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 9 :9Figure 9: ECG time series.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 10 :10Figure10: Results of applying the semi-Markov segmental model to recognize the \"heart beat\" pattern in the ECG time series. Top: an example pattern. Bottom: a similar pattern found by the pattern matching algorithm.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "p(y, s) = T t=2 p(y t |s t )p(s t |s t\u22121 ) p(y 1 |s 1 )\u03c0(s 1 ),(1)", "formula_coordinates": [8.0, 193.57, 612.37, 346.4, 31.97]}, {"formula_id": "formula_1", "formula_text": "y t = f i (t|\u03b8 i ) + e t (2", "formula_coordinates": [9.0, 266.95, 469.75, 268.38, 11.45]}, {"formula_id": "formula_2", "formula_text": ")", "formula_coordinates": [9.0, 535.33, 469.75, 4.64, 10.91]}, {"formula_id": "formula_3", "formula_text": "p i (t d = d) = a d\u22121 ii (1 \u2212 a ii ) (3)", "formula_coordinates": [10.0, 245.17, 240.18, 294.8, 14.93]}, {"formula_id": "formula_4", "formula_text": "p(y m+1 y m+2 . . . y m+d i |s i ) = p(d i |s i )p(\u03b8 i |s i ) m+d i t=m+1 p(y t |f i (\u03b8 i , t))(4)", "formula_coordinates": [12.0, 159.64, 132.56, 380.33, 32.3]}, {"formula_id": "formula_5", "formula_text": "\u2022 p(d i )", "formula_coordinates": [12.0, 88.36, 198.67, 33.93, 11.45]}, {"formula_id": "formula_6", "formula_text": "(t) i for each for each state i, 1 \u2264 i \u2264 K, wherep (t) i is defined asp (t) i = max s {p(s|y 1 y 2 . . . y t )|s = s 1 s 2 . . . s t , s t = i}.(5)", "formula_coordinates": [12.0, 72.0, 611.63, 467.97, 44.86]}, {"formula_id": "formula_8", "formula_text": "(t) i i\u015d p (t) i = max d i max jp (t\u2212d i ) j A ji p(d i )p(y t\u2212d i +1 . . . y t |\u03b8 i )(6)", "formula_coordinates": [13.0, 184.13, 378.29, 355.84, 49.67]}], "doi": ""}