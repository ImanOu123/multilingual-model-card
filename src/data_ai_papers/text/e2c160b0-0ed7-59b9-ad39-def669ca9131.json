{"title": "Planning-oriented Autonomous Driving", "authors": "Yihan Hu; Jiazhi Yang; Chen Li; Keyu Li; Chonghao Sima; Xizhou Zhu; Siqi Chai; Senyao Du; Tianwei Lin; Wenhai Wang; Lewei Lu; Xiaosong Jia; Qiang Liu; Jifeng Dai; Yu Qiao; Hongyang Li", "pub_date": "2023-03-23", "abstract": "Modern autonomous driving system is characterized as modular tasks in sequential order, i.e., perception, prediction, and planning. In order to perform a wide diversity of tasks and achieve advanced-level intelligence, contemporary approaches either deploy standalone models for individual tasks, or design a multi-task paradigm with separate heads. However, they might suffer from accumulative errors or deficient task coordination. Instead, we argue that a favorable framework should be devised and optimized in pursuit of the ultimate goal, i.e., planning of the self-driving car. Oriented at this, we revisit the key components within perception and prediction, and prioritize the tasks such that all these tasks contribute to planning. We introduce Unified Autonomous Driving (UniAD), a comprehensive framework up-to-date that incorporates full-stack driving tasks in one network. It is exquisitely devised to leverage advantages of each module, and provide complementary feature abstractions for agent interaction from a global perspective. Tasks are communicated with unified query interfaces to facilitate each other toward planning. We instantiate UniAD on the challenging nuScenes benchmark. With extensive ablations, the effectiveness of using such a philosophy is proven by substantially outperforming previous state-of-the-arts in all aspects. Code and models are public.", "sections": [{"heading": "Introduction", "text": "With the successful development of deep learning, autonomous driving algorithms are assembled with a series of tasks 1 , including detection, tracking, mapping in perception; and motion and occupancy forecast in prediction. As depicted in Fig. 1(a), most industry solutions deploy stan- The multi-task learning scheme shares a backbone with divided task heads. (c) The end-to-end paradigm unites modules in perception and prediction. Previous attempts either adopt a direct optimization on planning in (c.1) or devise the system with partial components in (c.2). Instead, we argue in (c.3) that a desirable system should be planning-oriented as well as properly organize preceding tasks to facilitate planning. dalone models for each task independently [68,71], as long as the resource bandwidth of the onboard chip allows. Although such a design simplifies the R&D difficulty across teams, it bares the risk of information loss across modules, error accumulation and feature misalignment due to the isolation of optimization targets [57,66,82].\nA more elegant design is to incorporate a wide span of tasks into a multi-task learning (MTL) paradigm, by plugging several task-specific heads into a shared feature extractor as shown in Fig. 1(b). This is a popular practice in many domains, including general vision [79,92,108], autonomous driving 2 [15,60,101,105], such as Transfuser [20], BEV- NMP [101] \u2713 \u2713 \u2713 NEAT [19] \u2713 \u2713 BEVerse [105] \u2713 \u2713 \u2713 (c.1) [14,16,78,97] \u2713 (c.2)\nDesign\nPnPNet \u2020 [57] \u2713 \u2713 \u2713 ViP3D \u2020 [30] \u2713 \u2713 \u2713 P3 [82] \u2713 \u2713 MP3 [11] \u2713 \u2713 \u2713 ST-P3 [38] \u2713 \u2713 \u2713 LAV [15] \u2713 \u2713 \u2713 \u2713\n(c.3) UniAD (ours) \u2713 \u2713 \u2713 \u2713 \u2713 \u2713\nTable 1. Tasks comparison and taxonomy. \"Design\" column is classified as in Fig. 1. \"Det.\" denotes 3D object detection, \"Map\" stands for online mapping, and \"Occ.\" is occupancy map prediction. \u2020: these works are not proposed directly for planning, yet they still share the spirit of joint perception and prediction. UniAD conducts five essential driving tasks to facilitate planning.\nerse [105], and industrialized products, e.g., Mobileye [68], Tesla [87], Nvidia [71], etc. In MTL, the co-training strategy across tasks could leverage feature abstraction; it could effortlessly extend to additional tasks, and save computation cost for onboard chips. However, such a scheme may cause undesirable \"negative transfer\" [23,64]. By contrast, the emergence of end-to-end autonomous driving [11,15,19,38,97] unites all nodes from perception, prediction and planning as a whole. The choice and priority of preceding tasks should be determined in favor of planning. The system should be planning-oriented, exquisitely designed with certain components involved, such that there are few accumulative error as in the standalone option or negative transfer as in the MTL scheme. Table 1 describes the task taxonomy of different framework designs.\nFollowing the end-to-end paradigm, one \"tabula-rasa\" practice is to directly predict the planned trajectory, without any explicit supervision of perception and prediction as shown in Fig. 1(c.1). Pioneering works [14,16,21,22,78,95,97,106] verified this vanilla design in the closed-loop simulation [26]. While such a direction deserves further exploration, it is inadequate in safety guarantee and interpretability, especially for highly dynamic urban scenarios. In this paper, we lean toward another perspective and ask the following question: Toward a reliable and planning-oriented autonomous driving system, how to design the pipeline in favor of planning? which preceding tasks are requisite?\nAn intuitive resolution would be to perceive surrounding objects, predict future behaviors and plan a safe maneuver explicitly, as illustrated in Fig. 1(c.2). Contemporary approaches [11,30,38,57,82] provide good insights and achieve impressive performance. However, we argue that the devil lies in the details; previous works more or less fail to consider certain components (see block (c.2) in Table 1), being reminiscent of the planning-oriented spirit. We elabo-rate on the detailed definition and terminology, the necessity of these modules in the Supplementary.\nTo this end, we introduce UniAD, a Unified Autonomous Driving algorithm framework to leverage five essential tasks toward a safe and robust system as depicted in Fig. 1(c.3) and Table 1(c.3). UniAD is designed in a planning-oriented spirit. We argue that this is not a simple stack of tasks with mere engineering effort. A key component is the querybased design to connect all nodes. Compared to the classic bounding box representation, queries benefit from a larger receptive field to soften the compounding error from upstream predictions. Moreover, queries are flexible to model and encode a variety of interactions, e.g., relations among multiple agents. To the best of our knowledge, UniAD is the first work to comprehensively investigate the joint cooperation of such a variety of tasks including perception, prediction and planning in the field of autonomous driving.\nThe contributions are summarized as follows. (a) we embrace a new outlook of autonomous driving framework following a planning-oriented philosophy, and demonstrate the necessity of effective task coordination, rather than standalone design or simple multi-task learning. (b) we present UniAD, a comprehensive end-to-end system that leverages a wide span of tasks. The key component to hit the ground running is the query design as interfaces connecting all nodes. As such, UniAD enjoys flexible intermediate representations and exchanging multi-task knowledge toward planning. (c) we instantiate UniAD on the challenging benchmark for realistic scenarios. Through extensive ablations, we verify the superiority of our method over previous state-of-the-arts in all aspects.\nWe hope this work could shed some light on the targetdriven design for the autonomous driving system, providing a starting point for coordinating various driving tasks.", "publication_ref": ["b0", "b67", "b70", "b56", "b65", "b81", "b78", "b91", "b107", "b1", "b14", "b59", "b100", "b104", "b19", "b100", "b18", "b104", "b13", "b15", "b77", "b96", "b56", "b29", "b81", "b10", "b37", "b14", "b104", "b67", "b86", "b70", "b22", "b63", "b10", "b14", "b18", "b37", "b96", "b13", "b15", "b20", "b21", "b77", "b94", "b96", "b105", "b25", "b10", "b29", "b37", "b56", "b81"], "figure_ref": ["fig_0", "fig_0", "fig_0", "fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Methodology", "text": "Overview. As illustrated in Fig. 2, UniAD comprises four transformer decoder-based perception and prediction modules and one planner in the end. Queries Q play the role of connecting the pipeline to model different interactions of entities in the driving scenario. Specifically, a sequence of multi-camera images is fed into the feature extractor, and the resulting perspective-view features are transformed into a unified bird's-eye-view (BEV) feature B by an off-theshelf BEV encoder in BEVFormer [55]. Note that UniAD is not confined to a specific BEV encoder, and one can utilize other alternatives to extract richer BEV representations with long-term temporal fusion [31,74] or multi-modality fusion [58,64]. In TrackFormer, the learnable embeddings that we refer to as track queries inquire about the agents' information from B to detect and track agents. MapFormer takes map queries as semantic abstractions of road elements (e.g., lanes and dividers) and performs panoptic seg-Figure 2. Pipeline of Unified Autonomous Driving (UniAD). It is exquisitely devised following planning-oriented philosophy. Instead of a simple stack of tasks, we investigate the effect of each module in perception and prediction, leveraging the benefits of joint optimization from preceding nodes to final planning in the driving scene. All perception and prediction modules are designed in a transformer decoder structure, with task queries as interfaces connecting each node. A simple attention-based planner is in the end to predict future waypoints of the ego-vehicle considering the knowledge extracted from preceding nodes. The map over occupancy is for visual purpose only. mentation of the map. With the above queries representing agents and maps, MotionFormer captures interactions among agents and maps and forecasts per-agent future trajectories. Since the action of each agent can significantly impact others in the scene, this module makes joint predictions for all agents considered. Meanwhile, we devise an ego-vehicle query to explicitly model the ego-vehicle and enable it to interact with other agents in such a scenecentric paradigm. OccFormer employs the BEV feature B as queries, equipped with agent-wise knowledge as keys and values, and predicts multi-step future occupancy with agent identity preserved. Finally, Planner utilizes the expressive ego-vehicle query from MotionFormer to predict the planning result, and keep itself away from occupied regions predicted by OccFormer to avoid collisions.", "publication_ref": ["b54", "b30", "b73", "b57", "b63"], "figure_ref": [], "table_ref": []}, {"heading": "Perception: Tracking and Mapping", "text": "TrackFormer. It jointly performs detection and multiobject tracking (MOT) without non-differentiable postprocessing. Inspired by [100,104], we take a similar query design. Besides the conventional detection queries utilized in object detection [8,109], additional track queries are introduced to track agents across frames. Specifically, at each time step, initialized detection queries are responsible for detecting newborn agents that are perceived for the first time, while track queries keep modeling those agents detected in previous frames. Both detection queries and track queries capture the agent abstractions by attending to BEV feature B. As the scene continuously evolves, track queries at the current frame interact with previously recorded ones in a self-attention module to aggregate temporal information, until the corresponding agents disappear completely (untracked in a certain time period). Similar to [8], Track-Former contains N layers and the final output state Q A provides knowledge of N a valid agents for downstream prediction tasks. Besides queries encoding other agents surround-ing the ego-vehicle, we introduce one particular ego-vehicle query in the query set to explicitly model the self-driving vehicle itself, which is further used in planning.\nMapFormer. We design it based on a 2D panoptic segmentation method Panoptic SegFormer [56]. We sparsely represent road elements as map queries to help downstream motion forecasting, with location and structure knowledge encoded. For driving scenarios, we set lanes, dividers and crossings as things, and the drivable area as stuff [50]. Map-Former also has N stacked layers whose output results of each layer are all supervised, while only the updated queries Q M in the last layer are forwarded to MotionFormer for agent-map interaction.", "publication_ref": ["b99", "b103", "b7", "b108", "b7", "b55", "b49"], "figure_ref": [], "table_ref": []}, {"heading": "Prediction: Motion Forecasting", "text": "Recent studies have proven the effectiveness of transformer structure on the motion task [43,44,63,69,70,84,99], inspired by which we propose MotionFormer in the end-toend setting. With highly abstract queries for dynamic agents Q A and static map Q M from TrackFormer and MapFormer respectively, MotionFormer predicts all agents' multimodal future movements, i.e., top-k possible trajectories, in a scene-centric manner. This paradigm produces multi-agent trajectories in the frame with a single forward pass, which greatly saves the computational cost of aligning the whole scene to each agent's coordinate [49]. Meanwhile, we pass the ego-vehicle query from TrackFormer through Motion-Former to engage ego-vehicle to interact with other agents, considering the future dynamics. Formally, the output motion is formulated as {x i,k \u2208 R T\u00d72 |i = 1, . . . , N a ; k = 1, . . . , K} , where i indexes the agent, k indexes the modality of trajectories and T is the length of prediction horizon.\nMotionFormer. It is composed of N layers, and each layer captures three types of interactions: agent-agent, agent-map and agent-goal point. For each motion query Q i,k (defined later, and we omit subscripts i, k in the following context for simplicity), its interactions between other agents Q A or map elements Q M could be formulated as:\nQ a/m = MHCA(MHSA(Q), Q A /Q M ),(1)\nwhere MHCA, MHSA denote multi-head cross-attention and multi-head self-attention [91] respectively. As it is also important to focus on the intended position, i.e., goal point, to refine the predicted trajectory, we devise an agent-goal point attention via deformable attention [109] as follows:\nQ g = DeformAttn(Q,x l\u22121 T , B),(2)\nwherex l\u22121 T\nis the endpoint of the predicted trajectory of previous layer. DeformAttn(q,r,x), a deformable attention module, takes in the query q, reference point r and spatial feature x. It performs sparse attention on the spatial feature around the reference point. Through this, the predicted trajectory is further refined as aware of the endpoint surroundings. All three interactions are modeled in parallel, where the generated Q a , Q m and Q g are concatenated and passed to a multi-layer perceptron (MLP), resulting query context Q ctx . Then, Q ctx is sent to the successive layer for refinement or decoded as prediction results at the last layer.\nMotion queries. The input queries for each layer of Mo-tionFormer, termed motion queries, comprise two components: the query context Q ctx produced by the preceding layer as described before, and the query position Q pos . Specifically, Q pos integrates the positional knowledge in four-folds as in Eq.  \nHere the sinusoidal position encoding PE(\u2022) followed by an MLP is utilized to encode the positional points andx 0 T is set as I s at the first layer (subscripts i, k are also omitted). The scene-level anchor represents prior movement statistics in a global view, while the agent-level anchor captures the possible intention in the local coordinate. They are both clustered by k-means algorithm on the endpoints of groundtruth trajectories, to narrow down the uncertainty of prediction. Contrary to the prior knowledge, the start point provides customized positional embedding for each agent, and the predicted endpoint serves as a dynamic anchor optimized layer-by-layer in a coarse-to-fine fashion.\nNon-linear Optimization. Different from conventional motion forecasting works which have direct access to ground truth perceptual results, i.e., agents' location and corresponding tracks, we consider the prediction uncertainty from the prior module in our end-to-end paradigm. Brutally regressing the ground-truth waypoints from an imperfect detection position or heading angle may lead to unrealistic trajectory predictions with large curvature and acceleration. To tackle this, we adopt a non-linear smoother [7] to adjust the target trajectories and make them physically feasible given an imprecise starting point predicted by the upstream module. The process is:\nx * = arg min x c(x,x),(4)\nwherex andx * denote the ground-truth and smoothed trajectory, x is generated by multiple-shooting [3], and the cost function is as follows:\nc(x,x) = \u03bb xy \u2225x,x\u2225 2 + \u03bb goal \u2225x T ,x T \u2225 2 + \u03d5\u2208\u03a6 \u03d5(x), (5\n)\nwhere \u03bb xy and \u03bb goal are hyperparameters, the kinematic function set \u03a6 has five terms including jerk, curvature, curvature rate, acceleration and lateral acceleration. The cost function regularizes the target trajectory to obey kinematic constraints. This target trajectory optimization is only conducted in training and does not affect inference.", "publication_ref": ["b42", "b43", "b62", "b68", "b69", "b83", "b98", "b48", "b90", "b108", "b6", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Prediction: Occupancy Prediction", "text": "Occupancy grid map is a discretized BEV representation where each cell holds a belief indicating whether it is occupied, and the occupancy prediction task is to discover how the grid map changes in the future. Previous approaches utilize RNN structure for temporally expanding future predictions from observed BEV features [35,38,105]. However, they rely on highly hand-crafted clustering postprocessing to generate per-agent occupancy maps, as they are mostly agent-agnostic by compressing BEV features as a whole into RNN hidden states. Due to the deficient usage of agent-wise knowledge, it is challenging for them to predict the behaviors of all agents globally, which is essential to understand how the scene evolves. To address this, we present OccFormer to incorporate both scene-level and agent-level semantics in two aspects: (1) a dense scene feature acquires agent-level features via an exquisitely designed attention module when unrolling to future horizons;\n(2) we produce instance-wise occupancy easily by a matrix multiplication between agent-level features and dense scene features without heavy post-processing.\nOccFormer is composed of T o sequential blocks where T o indicates the prediction horizon. Note that T o is typically smaller than T in the motion task, due to the high computation cost of densely represented occupancy. Each block takes as input the rich agent features G t and the state (dense feature) F t\u22121 from the previous layer, and generates F t for timestep t considering both instance-and scene-level information. To get agent feature G t with dynamics and spatial priors, we max-pool motion queries from MotionFormer in the modality dimension denoted as Q X \u2208 R Na\u00d7D , with D as the feature dimension. Then we fuse it with the upstream track query Q A and current position embedding P A via a temporal-specific MLP:\nG t = MLP t ([Q A , P A , Q X ]), t = 1, . . . , T o ,(6)\nwhere [\u2022] indicates concatenation. For the scene-level knowledge, the BEV feature B is downscaled to 1 /4 resolution for training efficiency to serve as the first block input F 0 . To further conserve training memory, each block follows a downsample-upsample manner with an attention module in between to conduct pixel-agent interaction at 1 /8 downscaled feature, denoted as F t ds . Pixel-agent interaction is designed to unify the sceneand agent-level understanding when predicting future occupancy. We take the dense feature F t ds as queries, instancelevel features as keys and values to update the dense feature over time. Detailedly, F t ds is passed through a self-attention layer to model responses between distant grids, then a crossattention layer models interactions between agent features G t and per-grid features. Moreover, to align the pixel-agent correspondence, we constrain the cross-attention by an attention mask, which restricts each pixel to only look at the agent occupying it at timestep t, inspired by [17]. The update process of the dense feature is formulated as:\nD t ds = MHCA(MHSA(F t ds ), G t , attn mask = O t m ). (7)\nThe attention mask O t m is semantically similar to occupancy, and is generated by multiplying an additional agentlevel feature and the dense feature F t ds , where we name the agent-level feature here as mask feature M t = MLP(G t ). After the interaction process in Eq. (7), D t ds is upsampled to 1 /4 size of B. We further add D t ds with block input F t\u22121 as a residual connection, and the resulting feature F t is passed to the next block.\nInstance-level occupancy. It represents the occupancy with each agent's identity preserved. It could be simply drawn via matrix multiplication, as in recent query-based segmentation works [18,52]. Formally, in order to get an occupancy prediction of original size H \u00d7 W of BEV feature B, the scene-level features F t are upsampled to F t dec \u2208 R C\u00d7H\u00d7W by a convolutional decoder, where C is the channel dimension. For the agent-level feature, we further update the coarse mask feature M t to the occupancy feature U t \u2208 R Na\u00d7C by another MLP. We empirically find that generating U t from mask feature M t instead of original agent feature G t leads to superior performance. The final instance-level occupancy of timestep t is:\nO t A = U t \u2022 F t dec .(8)", "publication_ref": ["b34", "b37", "b104", "b16", "b17", "b51"], "figure_ref": [], "table_ref": []}, {"heading": "Planning", "text": "Planning without high-definition (HD) maps or predefined routes usually requires a high-level command to indicate the direction to go [11,38]. Following this, we convert the raw navigation signals (i.e., turn left, turn right and keep forward) into three learnable embeddings, named command embeddings. As the ego-vehicle query from Mo-tionFormer already expresses its multimodal intentions, we equip it with command embeddings to form a \"plan query\". We attend plan query to BEV features B to make it aware of surroundings, and then decode it to future waypoints\u03c4 .\nTo further avoid collisions, we optimize\u03c4 based on Newton's method in inference only by the following:\n\u03c4 * = arg min \u03c4 f (\u03c4,\u03c4 ,\u00d4),(9)\nwhere\u03c4 is the original planning prediction, \u03c4 * denotes the optimized planning, which is selected from multipleshooting [3] trajectories \u03c4 as to minimize cost function f (\u2022).\nO is a classical binary occupancy map merged from the instance-wise occupancy prediction from OccFormer. The cost function f (\u2022) is calculated by:\nf (\u03c4,\u03c4 ,\u00d4) = \u03bb coord \u2225\u03c4,\u03c4 \u2225 2 + \u03bb obs t D(\u03c4 t ,\u00d4 t ), (10\n)\nD(\u03c4 t ,\u00d4 t ) = (x,y)\u2208S 1 \u03c3 \u221a 2\u03c0 exp(\u2212 \u2225\u03c4 t \u2212 (x, y)\u2225 2 2 2\u03c3 2 ). (11\n)\nHere \u03bb coord , \u03bb obs , and \u03c3 are hyperparameters, and t indexes a timestep of future horizons. The l 2 cost pulls the trajectory toward the original predicted one, while the collision term D pushes it away from occupied grids, considering surrounding positions confined to S = {(x, y)|\u2225(x, y)\u2212\u03c4 t \u2225 2 < d,\u00d4 t x,y = 1}.", "publication_ref": ["b10", "b37", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Learning", "text": "UniAD is trained in two stages. We first jointly train perception parts, i.e., the tracking and mapping modules, for a few epochs (6 in our experiments), and then train the model end-to-end for 20 epochs with all perception, prediction and planning modules. The two-stage training is found more stable empirically. We refer the audience to the Supplementary for details of each loss.\nShared matching. Since UniAD involves instance-wise modeling, pairing predictions to the ground truth set is required in perception and prediction tasks. Similar to DETR [8,56], the bipartite matching algorithm is adopted in the tracking and online mapping stage. As for tracking, candidates from detection queries are paired with newborn ground truth objects, and predictions from track queries inherit the assignment from previous frames. The matching results in the tracking module are reused in motion and occupancy nodes to consistently model agents from historical tracks to future motions in the end-to-end framework. Table 2. Detailed ablations on the effectiveness of each task. We can conclude that two perception sub-tasks greatly help motion forecasting, and prediction performance also benefits from unifying the two prediction modules. With all prior representations, our goalplanning boosts significantly to ensure safety. UniAD outperforms naive MTL solution by a large margin for prediction and planning tasks, and it also owns the superiority that no substantial perceptual performance drop occurs. Only main metrics are shown for brevity. \"avg.L2\" and \"avg.Col\" are the average values across the planning horizon. * : ID-0 is the MTL scheme with separate heads for each task.", "publication_ref": ["b7", "b55"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We conduct experiments on the challenging nuScenes dataset [6]. In this section, we validate the effectiveness of our design in three aspects: joint results revealing the advantage of task coordination and its effect on planning, modular results of each task compared with previous methods, and ablations on the design space for specific modules. Due to space limit, the full suite of protocols, some ablations and visualizations are provided in the Supplementary.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "Joint Results", "text": "We conduct extensive ablations as shown in Table 2 to prove the effectiveness and necessity of preceding tasks in the end-to-end pipeline. Each row of this table shows the model performance when incorporating task modules listed in the second Modules column. The first row (ID-0) serves as a vanilla multi-task baseline with separate task heads for comparison. The best result of each metric is marked in bold, and the runner-up result is underlined in each column.\nRoadmap toward safe planning. As prediction is closer to planning compared to perception, we first investigate the two types of prediction tasks in our framework, i.e., motion forecasting and occupancy prediction. In Exp.10-12, only when the two tasks are introduced simultaneously (Exp.12), both metrics of the planning L2 and collision rate achieve the best results, compared to naive endto-end planning without any intermediate tasks (Exp.10, Fig. 1(c.1)). Thus we conclude that both these two prediction tasks are required for a safe planning objective. Taking a step back, in Exp.7-9, we show the cooperative effect of two types of prediction. The performance of both tasks get improved when they are closely integrated (Exp.9, -3.5% minADE, -5.8% minFDE, -1.3 MR(%), +2.4 IoUf.(%), +2.4 VPQ-f.(%)), which demonstrates the necessity to include both agent and scene representations. Meanwhile, in order to realize a superior motion forecasting per-formance, we explore how perception modules could contribute in Exp.4-6. Notably, incorporating both tracking and mapping nodes brings remarkable improvement to forecasting results (-9.7% minADE, -12.9% minFDE, -2.3 MR(%)). We also present Exp. ", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Modular Results", "text": "Following the sequential order of perception-predictionplanning, we report the performance of each task module in comparison to prior state-of-the-arts on the nuScenes validation set. Note that UniAD jointly performs all these tasks with a single trained network. The main metric for each task is marked with gray background in tables.\nPerception results. As for multi-object tracking in Table 3, UniAD yields a significant improvement of +6.5 and +14.2 AMOTA(%) compared to MUTR3D [104] and ViP3D [30] respectively. Moreover, UniAD achieves the lowest ID switch score, showing its temporal consistency for each tracklet. For online mapping in Table 4, UniAD performs well on segmenting lanes (+7.4 IoU(%) compared to BEVFormer), which is crucial for downstream agentroad interaction in the motion module. As our tracking module follows an end-to-end paradigm, it is still inferior to tracking-by-detection methods with complex associations such as Immortal Tracker [93], and our mapping results trail previous perception-oriented methods on specific classes. We argue that UniAD is to benefit final planning with perceived information rather than optimizing perception with full model capacity.    [57] and ViP3D [30] respectively. In terms of occupancy prediction reported in Table 6, UniAD gets notable advances in nearby areas, yielding +4.0 and +2.0 on IoU-near(%) compared to FIERY [35] and BEVerse [105] with heavy augmentations, respectively.\nPlanning results. Benefiting from rich spatial-temporal information in both the ego-vehicle query and occupancy, UniAD reduces planning L2 error and collision rate by 51.2% and 56.3% compared to ST-P3 [38], in terms of the average value for the planning horizon. Moreover, it notably outperforms several LiDAR-based counterparts, which is often deemed challenging for perception tasks.", "publication_ref": ["b103", "b29", "b92", "b56", "b29", "b34", "b104", "b37"], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Qualitative Results", "text": "Fig. 3 visualizes the results of all tasks for one complex scene. The ego vehicle drives with notice to the potential   movement of a front vehicle and lane. In the Supplementary, we show more visualizations of challenging scenarios and one promising case for the planning-oriented design, that inaccurate results occur in prior modules while the later tasks could still recover, e.g., the planned trajectory remains reasonable though objects have a large heading angle deviation or fail to be detected in tracking results. Besides, we analyze that failure cases of UniAD are mainly under some long-tail scenarios such as large trucks and trailers, shown in the Supplementary as well.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ablation Study", "text": "Effect of designs in MotionFormer. Table 8 shows that all of our proposed components described in Sec.   Effect of designs in Planner. We provide ablations on the proposed designs in planner in Table 10, i.e., attending BEV features, training with the collision loss and the optimization strategy with occupancy. Similar to previous research [37,38], a lower collision rate is preferred for safety over naive trajectory mimicking (L2 metric), and is reduced with all parts applied in UniAD.", "publication_ref": ["b36", "b37"], "figure_ref": [], "table_ref": ["tab_6", "tab_8"]}, {"heading": "Conclusion and Future Work", "text": "We discuss the system-level design for the autonomous driving algorithm framework. A planning-oriented pipeline is proposed toward the ultimate pursuit for planning, namely UniAD. We provide detailed analyses on the necessity of each module within perception and prediction. To unify tasks, a query-based design is proposed to connect all nodes in UniAD, benefiting from richer representations for agent interaction in the environment. Extensive experiments verify the proposed method in all aspects. Limitations and future work. Coordinating such a comprehensive system with multiple tasks is non-trivial and needs extensive computational power, especially trained with temporal history. How to devise and curate the system for a lightweight deployment deserves future exploration. Moreover, whether or not to incorporate more tasks such as depth estimation, behavior prediction, and how to embed them into the system, are worthy future directions as well. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Task Definition", "text": "Detection and tracking. Detection and tracking are two crucial perception tasks for autonomous driving, and we focus on representing them in the 3D space to facilitate downstream usage. 3D Detection is responsible for locating surrounding objects (coordinates, length, width, height, etc.) at each time stamp; tracking aims at finding the correspondences between different objects across time stamps and associating them temporally (i.e., assigning a consistent track ID for each agent). In the paper, we use multi-object tracking in some cases to denote the detection and tracking process. The final output is a series of associated 3D boxes in each frame, and their corresponding features Q A are forwarded to the motion module. Additionally, note that we have one special query named ego-vehicle query for downstream tasks, which would not be included in the predictionground truth matching process and it regresses the location of ego-vehicle accordingly.\nOnline mapping. Map intuitively embodies the geometric and semantic information of the environment, and online mapping is to segment meaningful road elements with onboard sensor data (multi-view images in our case) as a substitute for offline annotated high-definition (HD) maps.\nIn UniAD, we model the online map into four categories: lanes, drivable area, dividers and pedestrian crossings, and we segment them in bird's-eye-view (BEV). Similar to Q A , the map queries Q M would be further utilized in the motion forecasting module to model the agent-map interaction.\nMotion forecasting. Bridging perception and planning, prediction plays an important role in the whole autonomous driving system to ensure final safety. Typically, motion forecasting is an independently developed module that predicts agents' future trajectories with detected bounding boxes and HD maps. And the bounding boxes are ground truth annotations in most current motion datasets [27], which is not realistic in onboard scenarios. While in this paper, the motion forecasting module takes previously encoded sparse queries (i.e., Q A and Q M ) and dense BEV features B as inputs, and forecasts K plausible trajectories in future T timesteps for each agent. Besides, to be compatible with our end-to-end and scene-centric scenarios, we predict trajectories as offset according to each agent's current position. The agent features before the last decoding MLPs, which have encoded both the historical and future information will be sent to the occupancy module for scene-level future understanding. For the ego-vehicle query, it predicts future ego-motion as well (actually providing a coarse planning estimation), and the feature is employed by the planner to generate the ultimate goal.\nOccupancy prediction. Occupancy grid map is a discretized BEV representation where each cell holds a belief indicating whether it is occupied, and the occupancy prediction task is designed to discover how the grid map changes in the future for T o timesteps with multiple agent dynamics. Complementary to motion forecasting which is conditioned on sparse agents, occupancy prediction is densely represented in the whole-scene level. To investigate how the scene evolves with sparse agent knowledge, our proposed occupancy module takes as inputs both the observed BEV feature B and agent features G t . After the multi-step agentscene interaction (detailedly described in Appendix E), the instance-level probability map\u00d4 t A \u2208 R Na\u00d7H\u00d7W is generated via matrix multiplication between occupancy feature and dense scene feature. To form whole-scene occupancy with agent identity preserved\u00d4 t \u2208 R H\u00d7W which is used for occupancy evaluation and downstream planning, we simply merge the instance-level probability at each timestep using pixel-wise argmax as in [8].\nPlanning. As an ultimate goal, the planning module takes all upstream results into consideration. Traditional planning methods in the industry often are rule-based, formulated by \"if-else\" state machines conditioned on various scenarios which are described with prior detection and prediction results. In our learning-based model, we take the upstream ego-vehicle query, and the dense BEV feature B as input, and predict one trajectory\u03c4 for total T p timesteps. Then, the trajectory\u03c4 is optimized with the upstream predicted future occupancy\u00d4 to avoid collision and ensure final safety.", "publication_ref": ["b26", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "B. The Necessity of Each Task", "text": "In terms of perception, tracking in the loop as does in PnPNet [57] and ViP3D [30] is proven to complement spatial-temporal features and provide history tracks for occluded agents, refraining from catastrophic decisions for downstream planning. With the aid of HD maps [30,57,82,101] and motion forecasting, planning becomes more accurate toward higher-level intelligence. However, such information is expensive to construct and prone to be outdated, raising the demand for online mapping without HD maps. As for prediction, motion forecasting [10,29,41,42,107] generates long-term future behaviors and preserves agent identity in form of sparse waypoint outputs. Nonetheless, there exists the challenge to integrate non-differentiable box representation into subsequent planning module [30,57]. Some recent literature investigates another type of prediction task named occupancy [88] prediction to assist endto-end planning, in form of cost maps. However, the lack of agent identity and dynamics in occupancy makes it impractical to model social interactions for safe planning. The large computational consumption of modeling multi-step dense features also leads to a much shorter temporal horizon compared to motion forecasting. Therefore, to benefit from the two complementary types of prediction tasks for safe planning, we incorporate both agent-centric motion and whole-scene occupancy in UniAD.", "publication_ref": ["b56", "b29", "b29", "b56", "b81", "b100", "b9", "b28", "b40", "b41", "b106", "b29", "b56", "b87"], "figure_ref": [], "table_ref": []}, {"heading": "C. Related Work", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1. Joint perception and prediction", "text": "Joint learning of perception and prediction is proposed to avoid the cascading error in traditional modularindependence pipelines. Similar to the motion forecasting task alone, it usually has two types of output representations: agent-level bounding boxes and scene-level occupancy grid maps. Pioneering work FaF [66] predicts boxes in the future and aggregates past information to produce tracklets. IntentNet [10] extends it to reason about intentions and [25,28] further predict future states in a refinement fashion. Some exploit detection first and utilize agent features in the second prediction stage [9,53,75]. Noticing that history information is ignored, PnPNet [57] enriches it by estimating tracking association scores to avert the non-differentiable optimization process as adopted by the tracking-by-detection paradigm [54,64,85,98]. Yet, all these methods rely on non-maximum suppression (NMS) in detection which still leads to information loss. ViP3D [30] which is closely related to our work, employs agent queries in [104] to forecast, taking HD map as another input. We follow the philosophy of [30,104] in agent track queries, but also develop non-linear optimization on target trajectories to alleviate the potential inaccurate perception problem. Moreover, we introduce an ego-vehicle query for better capturing the ego behaviors in the dynamic environment, and incorporate online mapping to prevent the localization risk or high construction cost with HD map.\nThe alternative representation, namely the occupancy grid map, discretizes the BEV map into grid cells which holds a belief indicating if it is occupied. Wu et al. [96] estimate a dense motion field, while it could not capture multimodal behaviors. Fishing Net [33] also predicts deterministic future BEV semantic segmentation with multiple sensors. To address this, P3 [82] proposes non-parametric distribution of future semantic occupancy and FIERY [35] devises the first paradigm for multi-view cameras. A few methods improve the performance of FIERY with more sophisticated uncertainty modeling [1,38,105]. Notably, this representation could easily extend to motion planning for collision avoidance [11,38,82], while it loses the agent identity characteristic and takes a heavy burden to computation which may constrain the prediction horizon. In contrast, we leverage agent-level information for occupancy prediction and ensure accurate and safe planning by unifying these two modes.", "publication_ref": ["b65", "b9", "b24", "b27", "b8", "b52", "b74", "b56", "b53", "b63", "b84", "b97", "b29", "b103", "b29", "b103", "b95", "b32", "b81", "b34", "b0", "b37", "b104", "b10", "b37", "b81"], "figure_ref": [], "table_ref": []}, {"heading": "C.2. Joint prediction and planning", "text": "PRECOG [81] proposes a recurrent model that conditions forecasting on the goal position of the ego vehicle, while PiP [86] generates agents' motion considering complete presumed planning trajectories. However, producing a rough future trajectory is still challenging in the real world, toward which [62] presents a deep structured model to derive both prediction and planning from the same set of learnable costs. [39,40] couple the prediction model with classic optimization methods. Meanwhile, some motion forecasting methods implicitly include the planning task by producing their future trajectories simultaneously [12,45,70]. Similarly, we encode possible behaviors of the ego vehicle in the scene-centric motion forecasting module, but the interpretable occupancy map is utilized to further optimize the plan to stay safe.", "publication_ref": ["b80", "b85", "b61", "b38", "b39", "b11", "b44", "b69"], "figure_ref": [], "table_ref": []}, {"heading": "C.3. End-to-end motion planning", "text": "End-to-end motion planning has been an active research domain since Pomerleau [77] uses a single neural network that directly predicts control signals. Subsequent studies make great advances especially in closed-loop simulation with deeper networks [4], multi-modal inputs [2,21,78], multi-task learning [20,97], reinforcement learning [13,14,46,59,89] and distillation from certain privilege knowledge [16,103,106]. However, for such methods of directly generating control outputs from sensor data, the transfer from the synthetic environment to realistic application remains a problem considering their robustness and safety assurance [22,38]. Thus researchers aim at explicitly designing the intermediate representations of the network to prompt safety, where predicting how the scene evolves attracts broad interest. Some works [19,34,83] jointly decode planning and BEV semantic predictions to enhance interpretability, while PLOP [5] adopts a polynomial formulation to provide smooth planning results for both ego vehicle and neighbors. Cui et al. [24] introduce a contingency planner with diverse sets of future predictions and LAV [15] trains the planner with all vehicles' trajectories to provide richer training data. NMP [101] and its variant [94] estimate a cost volume to select the plan with minimal cost besides deterministic future perception. Though they risk producing inconsistent results between two modules, the cost map design is intuitive to recover the final plan in complex scenarios. Inspired by [101], most recent works [11,37,38,82,102] propose models that construct costs with both learned occupancy prediction and hand-crafted penalties. However, their performances heavily rely on the tailored cost based on human experience and the distribution from where trajectories are sampled [47]. Contrary to these approaches, we leverage the ego-motion information without sophisticated cost design and present the first attempt that incorporates the tracking module along with two genres of prediction rep-resentations simultaneously in an end-to-end model.", "publication_ref": ["b76", "b3", "b1", "b20", "b77", "b19", "b96", "b12", "b13", "b45", "b58", "b88", "b15", "b102", "b105", "b21", "b37", "b18", "b33", "b82", "b4", "b23", "b14", "b100", "b93", "b100", "b10", "b36", "b37", "b81", "b101", "b46"], "figure_ref": [], "table_ref": []}, {"heading": "D. Notations", "text": "We provide a lookup table of notations and their shapes mentioned in this paper in Table 11 for reference.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_11"]}, {"heading": "E. Implementation Details E.1. Detection and Tracking", "text": "We inherit most of the detection designs from BEV-Former [55] which takes a BEV encoder to transform image features into BEV feature B and adopts a Deformable DETR head [109] to perform detection on B. To further conduct end-to-end tracking without heavy post association, we introduce another group of queries named track queries as in MOTR [100] which continuously tracks previously observed instances according to its assigned track ID. We introduce the tracking process in detail below.\nTraining stage: At the beginning (i.e., first frame) of each training sequence, all queries are considered detection queries and predict all newborn objects, which is actually the same as BEVFormer. Detection queries are matched to the ground truth by the Hungarian algorithm [8]. They will be stored and updated via the query interaction module (QIM) for the next timestamp serving as track queries following MOTR [100]. In the next timestamp, track queries will be directly matched with a part of ground-truth objects according to the corresponding track ID, and detection queries will be matched with the remaining ground-truth objects (newborn objects). To stabilize training, we adopt the 3D IoU metric to filter the matched queries. Only those predictions having the 3D IoU with ground-truth boxes larger than a certain threshold (0.5 in practice) will be stored and updated.\nInference stage: Different from the training stage, each frame of a sequence is sent to the network sequentially, meaning that track queries could exist for a longer horizon than the training time. Another difference emerging in the inference stage is about query updating, that we use classification scores to filter the queries (0.4 for detection queries and 0.35 for track queries in practice) instead of the 3D IoU metric since the ground truth is not available. Besides, to avoid the interruption of tracklets caused by short-time occlusion, we use a lifecycle mechanism for the tracklets in the inference stage. Specifically, for each track query, it will be considered to disappear completely and be removed only when its corresponding classification score is smaller than 0.35 for a continuous period (2s in practice).", "publication_ref": ["b54", "b108", "b99", "b7", "b99"], "figure_ref": [], "table_ref": []}, {"heading": "E.2. Online Mapping", "text": "Following [56], we decompose the map query set into thing queries and stuff queries. The thing queries model instance-wise map elements (i.e., lanes, boundaries, and    pedestrian crossings) and are matched with ground truth via bipartite matching, while the stuff query is only in charge of semantic elements (i.e., drivable area) and is processed with a class-fixed assignment. We set the total number of thing queries to 300 and only 1 stuff query for the drivable area. Also, we stack 6 location decoder layers and 4 mask decoder layers (we follow the structure of those layers as in [56]). We empirically choose thing queries after the location decoder as our map queries Q M for downstream tasks.", "publication_ref": ["b55", "b55"], "figure_ref": [], "table_ref": []}, {"heading": "E.3. Motion Forecasting", "text": "To better illustrate the details, we provide a diagram as shown in Fig. 4. Our MotionFormer takes I a T , I s T ,x 0 , x l\u22121 T \u2208 R K\u00d72 to embed query position, and takes Q l\u22121 ctx as query context. Specifically, the anchors are clustered among training data of all agents by the k-means algorithm, and we set K = 6 which is compatible with our output modalities. To embed the scene-level prior, the anchor I a T is rotated and translated into the global coordinate frame according to each agent's current location and heading angle, which is denoted as I s T , as shown in Eq. (12),\nI s i,T = R i I a T + T i ,(12)\nwhere i is the index of the agent, and it is omitted later for brevity. To facilitate the coarse-to-fine paradigm, we also adopt the goal point predicted from the previous layerx l\u22121 T . In the meantime, the agent's current position is broadcast across the modality, denoted asx 0 . Then, MLPs and sinusoidal positional embeddings are applied for each of the prior positional knowledge and we summarize them as the query position Q pos \u2208 R K\u00d7D , which is of the same shape as the query context Q ctx . Q pos and Q ctx together build up our motion query. We set D to 256 throughout MotionFormer.\nAs shown in Fig. 4, our MotionFormer consists of three major transformer blocks, i.e., agent-agent, agent-map and agent-goal interaction modules. The agent-agent, agentmap interaction modules are built with standard transformer decoder layers, which are composed of a multi-head selfattention (MHSA) layer and a multi-head cross-attention (MHCA) layer, a feed-forward network (FFN) and several residual and normalization layers in between [8]. Apart from the agent queries Q A and map queries Q M , we also add the positional embeddings to those queries with sinusoidal positional embedding followed by MLP layers. The agent-goal interaction module is built upon deformable cross-attention module [109], where the goal point from the previously predicted trajectory (R ix l\u22121 i,T + T i ) is adopted as the reference point, as shown in Fig. 5. Specifically, we set the number of sampled points to 4 per trajectory, and 6 trajectories per agent as we mention above. The output features of each interaction module are concatenated and projected with MLP layers to dimension D = 256. Then, we use Gaussian Mixture Model to build each agent's trajectories, wherex l \u2208 R K\u00d7T \u00d75 . We set the prediction time horizon T to 12 (6 seconds) in UniAD. Note that we only take the first two of the last dimension (i.e., x and y) as final output trajectories. Besides, the scores of each modality are also predicted (score(x l ) \u2208 R K ). We stack the overall modules for N times, and N is set to 3 in practice.", "publication_ref": ["b7", "b108"], "figure_ref": ["fig_6", "fig_6", "fig_7"], "table_ref": []}, {"heading": "E.4. Occupancy Prediction", "text": "Given the BEV feature from upstream modules, we first downsample it by /4 with convolutional layers for efficient multi-step prediction, then pass it to our proposed Occ-Former. OccFormer is composed of T o sequential blocks shown in Fig. 6, where T o = 5 is the temporal horizon (including current and future frames) and each block is responsible for generating occupancy of one specific frame.\nDifferent from prior works which are short of agent-level knowledge, our proposed method incorporates both dense scene features and sparse agent features when unrolling the future representations. The dense scene feature is from the output of the last block (or the observed feature for current frame) and it's further downscaled (/8) by a convolution layer to reduce computation for pixel-agent interaction. The sparse agent feature is derived from the concatenation of track query Q A , agent positions P A , and motion query Q X , and it is then passed to a temporal-specific MLP for temporal sensitivity. We conduct pixel-level selfattention to model the long-term dependency required in some rapidly changing scenes, then perform scene-agent incorporation by attending each pixel of the scene to corresponding agents. To enhance the location alignment between agents and pixels, we restrict the cross-attention with an attention mask which is generated by a matrix multiplication between mask feature and downscaled scene feature, where the mask feature is produced by encoding agent feature with an MLP. We then upsample the attended dense feature to the same resolution as input F t\u22121 (/4) and add it with F t\u22121 as a residual connection for stability. The resulting feature F t is both sent to the next block and a convolutional decoder for predicting occupancy at the original BEV resolution (/1). We reuse the mask feature and pass it to another MLP to form occupancy feature, and the instance-level occupancy is therefore generated by a matrix multiplication between occupancy feature and decoded dense feature F t dec (/1). Note that the MLP layer for mask feature, the MLP layer for occupancy feature, and the convolutional decoder are shared across all T o blocks while other components are independent in each block. Dimensions of all dense features and agent features are 256 in OccFormer.", "publication_ref": [], "figure_ref": ["fig_8"], "table_ref": []}, {"heading": "E.5. Planning", "text": "As shown in Fig. 7, our planner takes the ego-vehicle query generated from the tracking and motion forecasting module, which is symbolized with the blue triangle and yellow rectangle respectively. These two queries, along with the command embedding, are encoded with MLP layers followed by a max-pooling layer across the modality dimension, where the most salient modal features are selected and aggregated. The BEV feature interaction module is built with standard transformer decoder layers, and it is stacked for N layers, where we set N = 3 here. Specifically, it cross-attends the dense BEV feature with the aggregated plan query. More qualitative results can be found in Appendix F. 5 showing the effectiveness of this module. To embed location information, we fuse the planquery with learned position embedding and the BEV feature with sinusoidal positional embedding. We then regress the planning trajectory with MLP layers, which is denoted as\u03c4 \u2208 R Tp\u00d72 . Here we set T p = 6 (3 seconds). Finally, where To is the temporal horizon (including current and future frames) and each block is responsible for generating occupancy of one specific frame. We incorporate both dense scene features and sparse agent features, which are encoded from upstream track query QA, agent position PA and motion query QX , to inject agent-level knowledge into future scene representations. We form instance-level occupancy\u00d4 t A via a matrix multiplication between agent-level feature and decoded dense feature at the end of each block.\nwe devise the collision optimizer for obstacle avoidance, which takes the predicted occupancy\u00d4 and trajectory\u03c4 as input as Eq. (10) in the main paper. We set d = 5, \u03c3 = 1.0, \u03bb coord = 1.0, \u03bb obs = 5.0.", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "E.6. Training Details", "text": "Joint learning. UniAD is trained in two stages which we find more stable. In stage one, we pre-train perception tasks including tracking and online mapping to stabilize perception predictions. Specifically, we load corresponding pretrained BEVFormer [55] weights to UniAD for fast convergence including image backbone, FPN, BEV encoder and detection decoder except for object query embeddings (due to the additional ego-vehicle query). We stop the gradient back-propagation in the image backbone to reduce memory cost and train UniAD for 6 epochs with tracking and online mapping losses as follows:\nL 1 = L track + L map . (13\n)\nFigure 7. Planner. Q ego A and Q ego ctx are the ego-vehicle query from the tracking and motion forecasting modules, respectively. Along with the high-level command, they are encoded with MLP layers followed by a max-pooling layer across the modality dimension, where the most salient modal features are selected and aggregated. The BEV feature interaction module is built with standard transformer decoder layers, and it is stacked for N layers.\nIn stage two, we keep the image backbone frozen as well, and additionally freeze BEV encoder, which is used for view transformation from image view to BEV, to further reduce memory consumption with more downstream modules. UniAD now is trained with all task losses including tracking, mapping, motion forecasting, occupancy prediction, and planning for 20 epochs (for various ablation studies in main paper, it's trained for 8 epochs for efficiency):\nL 2 = L track + L map + L motion + L occ + L plan . (14\n)\nDetailed losses and hyperparameters within each term of L 1 and L 2 are described below individually. The length of each training sequence (at each step) for tracking and BEV feature aggregation [55] in both stages is 5 (3 in ablation studies for efficiency).\nDetection&tracking loss. Following BEVFormer [55], the Hungarian loss is adopted for each paired result, which is a linear combination of a Focal loss [61] for class labels and an l 1 for 3D boxes localization. In terms of the matching strategy, candidates from newborn queries are paired with ground truth objects through bipartite matching, and predictions from track queries inherit the assigned ground truth index from previous frames. Specifically, L track = \u03bb focal L focal + \u03bb l1 L l1 , where \u03bb focal = 2 and \u03bb l1 = 0.25.\nOnline mapping loss. As in [56], this includes thing losses for lanes, dividers, and contours, also a stuff loss for the drivable area, where Focal loss is responsible for classification, L1 loss is responsible for thing bounding boxes, Dice loss and GIoU loss [80] account for segmentation. Detailedly, L map = \u03bb focal L focal +\u03bb l1 L l1 +\u03bb giou L giou +\u03bb dice L dice , with \u03bb focal = \u03bb giou = \u03bb dice = 2 and \u03bb l1 = 0.25.\nMotion forecasting loss. Like most of the prior methods, we model the multimodal trajectories as gaussian mixtures, and use the multi-path loss [12,90], which includes a classification score loss L cls and a negative log-likelihood loss term L nll , and \u03bb denotes the corresponding weight: L motion = \u03bb cls L cls + \u03bb reg L nll , where \u03bb cls = \u03bb reg = 0.5. To ensure the temporal smoothness of trajectories, we predict agents' speed at each timestep first and accumulate it across time to obtain their final trajectories [41].\nOccupancy prediction loss. The output of instance-level occupancy prediction is a binary segmentation of each agent, therefore we adopt binary cross-entropy and Dice loss [67] as the occupancy loss. Formally, L occ = \u03bb bce L bce + \u03bb dice L dice , with \u03bb bce = 5 and \u03bb dice = 1 here. Additionally, since the attention mask in the pixel-agent interaction module could be seen as a coarse prediction, we employ an auxiliary occupancy loss with the same form to supervise it.\nPlanning loss. Safety is the most crucial factor in planning. Therefore, apart from the naive imitation l 2 loss, we employ another collision loss which keeps the planned trajectory away from obstacles as follows:\nL col (\u03c4 , \u03b4) = i,t IoU(box(\u03c4 t , w + \u03b4, l + \u03b4), b i,t )), (15\n)\nL plan = \u03bb imi |\u03c4 ,\u03c4 | 2 + \u03bb col (\u03c9,\u03b4) \u03c9L col (\u03c4 , \u03b4),(16)\nwhere \u03bb imi = 1, \u03bb col = 2.5, (\u03c9, \u03b4) is a weight-value pair considering additional safety distance, box(\u03c4 t , w + \u03b4, l + \u03b4) represents the ego bounding box with an increased size at timestamp t to keep a larger safe distance, and b i,t indicates each agent forecasted in the scene. In practice, we set (\u03c9, \u03b4) to (1.0, 0.0), (0.4, 0.5), (0.1, 1.0).", "publication_ref": ["b54", "b54", "b54", "b60", "b55", "b79", "b11", "b89", "b40", "b66"], "figure_ref": [], "table_ref": []}, {"heading": "F. Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F.1. Protocols", "text": "We follow most of the basic training settings as in BEV-Former [55] for both two stages with a batch size of 1, a learning rate of 2\u00d710 \u22124 , learning rate multiplier of the backbone 0.1 and AdamW optimizer [65] with a weight decay of 1\u00d710 \u22122 . The default size of BEV size is 200\u00d7200, covering BEV ranges of [-51.2m, 51.2m] for both X and Y axis with the interval as 0.512m. More hyperparameters related to feature dimensions are shown in Table 11. Experiments are conducted with 16 NVIDIA Tesla A100 GPUs.", "publication_ref": ["b54", "b64"], "figure_ref": [], "table_ref": ["tab_11"]}, {"heading": "F.2. Metrics", "text": "Multi-object tracking. Following the standard evaluation protocols, we use AMOTA (Average Multi-object Tracking Accuracy), AMOTP (Average Multi-object Tracking Precision), Recall, and IDS (Identity Switches) to evaluate the 3D tracking performance of UniAD on nuScenes dataset. AMOTA and AMOTP are computed by integrating MOTA (Multi-object Tracking Accuracy) and MOTP (Multi-object Tracking Precision) values over all recalls:\nAMOTA = 1 n \u2212 1 r\u2208{ 1 n\u22121 , 2 n\u22121 ,...,1} MOTA r ,(17)\nMOTA r = max(0, 1 \u2212 FP r + FN r + IDS r \u2212 (1 \u2212 r)GT rGT ),(18)\nwhere FP r , FN r , and IDS r represent the number of false positives, false negatives and identity switches computed at the corresponding recall r, respectively. GT stands for the number of ground truth objects in this frame. AMOTP can be defined as:\nAMOTP = 1 n \u2212 1 r\u2208{ 1 n\u22121 , 2 n\u22121 ,...,1} i,t d i,t TP r ,(19)\nwhere d i,t denotes the position error (in x and y axis) of matched track i at time stamp t, and TP r is the number of true positives at the corresponding recall r.\nOnline mapping. We have four categories for the online mapping task, i.e., lanes, boundaries, pedestrian crossings and drivable area. We calculate the intersection-over-union (IoU) metric for each class between the network outputs and ground truth maps.\nMotion forecasting. On one hand, following the standard motion prediction protocols, we adopt conventional metrics, including minADE (minimum Average Displacement Error), minFDE (minimum Final Displacement Error) and MR (Miss Rate). Similar to the prior works [57,66,75], these metrics are only calculated within matched TPs, and we set the matching threshold to 1.0m in all of our experiments. As for the MR, we set the miss FDE threshold to 2.0m. On the other hand, we also employ recently proposed end-to-end metrics, i.e., EPA (End-to-end Prediction Accuracy) [30] and minFDE-AP [75]. For EPA, we use the same setting as in ViP3D [30] for a fair comparison. For minFDE-AP, we do not separate ground truth into multiple bins (static, linear, and non-linearly moving sub-categories) for simplicity. Specifically, only when an object's perception location and its min-FDE are within the distance threshold (1.0m and 2.0m respectively), it would be counted as a TP for the AP (average precision) calculation. Similarly to the prior works, we merge the car, truck, construction vehicle, bus, trailer, motorcycle, and bicycle as the vehicle category, and all the motion forecasting metrics provided in the experiments are evaluated on the vehicle category.\nOccupancy prediction. We evaluate the quality of predicted occupancy in both whole-scene level and instancelevel following [35,105]. Specifically, The IoU measures the whole-scene categorical segmentations which is instance-agnostic, while the Video Panoptic Quality (VPQ) [48] takes into account each instance's presence and consistency over time. The VPQ metric is calculated as follows:\nVPQ = H t=0 (pt,qt)\u2208TPt IoU(p t , q t ) |TP t | + 1 2 |FP t | + 1 2 |FN t | ,(20)\nwhere H is the future horizon and we set H = 4 (which leads to T o = 5 including the current timestamp) as in [35,105], covering 2.0s consecutive data at 2Hz. TP t , FP t , and FN t are the set of true positives, false positives, and false negatives at timestamp t respectively. Both two metrics are evaluated under two different BEV ranges, near (\"-n.\") for 30m\u00d730m and far (\"-f.\") for 100m\u00d7100m around the ego vehicle. We evaluate the results of the current step (t = 0) and the future 4 steps together on both metrics.\nPlanning. We adopt the same metrics as in ST-P3 [38], i.e., L2 error and collision rate at various timestamps.", "publication_ref": ["b56", "b65", "b74", "b29", "b74", "b29", "b34", "b104", "b47", "b34", "b104", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "F.3. Model complexity and Computational cost", "text": "We measure the complexity of UniAD and runtime on an Nvidia Tesla A100 GPU, as depicted in Table 13. Though the decoder part of tasks brings a certain amount of parameters, the computational complexity mainly comes from the encoder part, compared to the original BEVFormer detector (ID. 1). We also provide a comparison with the recent BEVerse [105]. UniAD owns more tasks, achieves superior performance, and has lower FLOPs -indicating affordable budget to additional computation cost.", "publication_ref": ["b104"], "figure_ref": [], "table_ref": []}, {"heading": "F.4. Model scale", "text": "We provide three variations of UniAD under different model scales as shown in  Table 13. Computational complexity and runtime with different modules incorporated. ID.1 is similar to original BEVFormer [55], and ID. 0 (BEVerse-Tiny) [105] is an MTL framework.\nbackbones for image-view feature extraction are ResNet-50 [32], ResNet-101 and VoVNet 2-99 [51] for UniAD-S, UniAD-B and UniAD-L respectively. Since the model scale (image encoder) mainly influences the BEV feature quality, we could observe that the perceptual scores improve with a larger backbone, which further could lead to better prediction and planning performance.", "publication_ref": ["b54", "b104", "b31", "b50"], "figure_ref": [], "table_ref": []}, {"heading": "F.5. Qualitative results", "text": "Attention mask visualization. To investigate the internal mechanism and show its explainability, we visualize the attention mask of the cross-attention module in the planner.\nAs shown in Fig. 8, the predicted tracking bounding boxes, planned trajectory, and the ground truth HD Map are rendered for reference, and the attention mask is overlayered on top. From left to right, we show two consecutive frames in a time sequence but with different navigation commands. We can observe that the planned trajectory varies largely according to the command. Also, much attention is paid to the goal lane as well as the critical agents that are yielding to our ego vehicle.\nVisualization of different scenarios. We provide visualizations for more scenarios, including cruising around the urban areas (Fig. 9), critical cases (Fig. 10), and obstacle avoidance scenarios (Fig. 11). One promising evidence for our planning-oriented design is shown in Fig. 12, where inaccurate results occur in prior modules while the later tasks could still recover. Similarly, we show results for all tasks in surround-view images, BEV, as well as the attention mask from the planner. A demo video 3 is also provided for reference.\nFigure 8. Effectiveness of navigation command and attention mask visualization. Here we demonstrate how attention is paid in accordance with the navigation command. We render the attention mask from the BEV interaction module in the planning module, the predicted tracking bounding boxes as well as the planned trajectory. The navigation command is printed on the bottom left, and the HD Map is rendered only for reference. From left to right, we show two consecutive frames in a time sequence but with different navigation commands. We can observe that the planned trajectory varies largely according to the command. Also, much attention is paid to the goal lane as well as the critical agents that are yielding to our ego vehicle. Figure 9. Visualization for cruising around the urban areas. UniAD can generate high-quality interpretable perceptual and prediction results, and make a safe maneuver. The first three columns show six camera views, and the last two columns are the predicted results and the attention mask from the planning module respectively. Each agent is illustrated with a unique color. Only top-1 and top-3 trajectories from motion forecasting are selected for visualization on images-view and BEV respectively. Here we demonstrate two critical cases. The first scenario (top) shows that the ego vehicle is yielding to two pedestrians crossing the street, and the second scenario (down) shows that the ego vehicle is yielding to a fast-moving car and waiting to go straight without protection near an intersection. We can observe that much attention is paid to the most critical agents, i.e., pedestrians and fast-moving vehicles, as well as the intended goal location.\nFigure 11. Obstacles avoidance visualization. In these two scenarios, the ego vehicle is changing lanes attentively to avoid the obstacle vehicle. From the attention mask, we can observe that our method focuses on the obstacles as well as the road in the front and back.\nFigure 12. Visualization for planning recovering from perception failures. We show an interesting case where inaccurate results occur in prior modules while the later tasks could still recover. The top row and the down row represent two consecutive frames from the same scenario. The vehicle in the red circle is moving from a far distance toward the intersection at a high speed. It is observed that the tracking module misses it at first, then captures it at the latter frame. The blue circles show a stationary car yielding to the traffic, and it is missed in both frames. Interestingly, both vehicles show strong reactions to the attention masks of the planner, even though they are missed in the prior modules. It means that our planner still pays attention to those critical though missed agents, which is intractable in previous fragmented and non-unified driving systems, and demonstrates the robustness of UniAD. Here we present a long-tail scenario, where a large trailer with a white container occupies the entire road. We can observe that our tracking module fails to detect the accurate size of the front trailer and heading angles of vehicles beside the road. Figure 14. Failure cases 2. In this case, the planner is over-cautious about the incoming vehicle in the narrow street. The dark environment is one critical type of long-tail scenarios in autonomous driving. Applying smaller collision loss weight and more regulation regarding the boundary might mitigate the problem.", "publication_ref": ["b2"], "figure_ref": ["fig_0", "fig_0", "fig_0", "fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "", "text": "Acknowledgements. This work is partially supported by National Key R&D Program of China (2022ZD0160100), and in part by Shanghai Committee of Science and Technology (21DZ1100100), and NSFC (62206172).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "StretchBEV: Stretching future instance prediction spatially and temporally", "journal": "", "year": "", "authors": "Fatma Adil Kaan Akan;  G\u00fcney"}, {"ref_id": "b1", "title": "Learning to drive by imitating the best and synthesizing the worst", "journal": "", "year": "2018", "authors": "Mayank Bansal; Alex Krizhevsky; Abhijit Ogale;  Chauffeurnet"}, {"ref_id": "b2", "title": "A multiple shooting algorithm for direct solution of optimal control problems", "journal": "", "year": "1984", "authors": "Georg Hans; Karl-Josef Bock;  Plitt"}, {"ref_id": "b3", "title": "End to end learning for self-driving cars", "journal": "", "year": "2016", "authors": "Mariusz Bojarski; Davide Del Testa; Daniel Dworakowski; Bernhard Firner; Beat Flepp; Prasoon Goyal; D Lawrence; Mathew Jackel; Urs Monfort; Jiakai Muller; Xin Zhang; Jake Zhang; Zieba Zhao;  Karol"}, {"ref_id": "b4", "title": "PLOP: Probabilistic polynomial objects trajectory planning for autonomous driving", "journal": "", "year": "2020", "authors": "Thibault Buhet; \u00c9milie Wirbel; Xavier Perrotton"}, {"ref_id": "b5", "title": "nuscenes: A multimodal dataset for autonomous driving", "journal": "", "year": "2020", "authors": "Holger Caesar; Varun Bankiti; Alex H Lang; Sourabh Vora; Venice Erin Liong; Qiang Xu; Anush Krishnan; Yu Pan; Giancarlo Baldan; Oscar Beijbom"}, {"ref_id": "b6", "title": "Oscar Beijbom, and Sammy Omari. nuplan: A closed-loop mlbased planning benchmark for autonomous vehicles", "journal": "", "year": "2021", "authors": "Holger Caesar; Juraj Kabzan; Whye Kit Kok Seang Tan; Eric Fong; Alex Wolff; Luke Lang;  Fletcher"}, {"ref_id": "b7", "title": "End-to-end object detection with transformers", "journal": "", "year": "2020", "authors": "Nicolas Carion; Francisco Massa; Gabriel Synnaeve; Nicolas Usunier; Alexander Kirillov; Sergey Zagoruyko"}, {"ref_id": "b8", "title": "Renjie Liao, and Raquel Urtasun. Spagnn: Spatially-aware graph neural networks for relational behavior forecasting from sensor data", "journal": "", "year": "", "authors": "Sergio Casas; Cole Gulino"}, {"ref_id": "b9", "title": "Intentnet: Learning to predict intention from raw sensor data", "journal": "", "year": "2018", "authors": "Sergio Casas; Wenjie Luo; Raquel Urtasun"}, {"ref_id": "b10", "title": "Mp3: A unified model to map, perceive, predict and plan", "journal": "", "year": "2005", "authors": "Sergio Casas; Abbas Sadat; Raquel Urtasun"}, {"ref_id": "b11", "title": "Multipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction", "journal": "", "year": "2020", "authors": "Yuning Chai; Benjamin Sapp; Mayank Bansal; Dragomir Anguelov"}, {"ref_id": "b12", "title": "GRI: General reinforced imitation and its application to vision-based autonomous driving", "journal": "", "year": "2021", "authors": "Raphael Chekroun; Marin Toromanoff; Sascha Hornauer; Fabien Moutarde"}, {"ref_id": "b13", "title": "Learning to drive from a world on rails", "journal": "", "year": "", "authors": "Dian Chen; Vladlen Koltun; Philipp Kr\u00e4henb\u00fchl"}, {"ref_id": "b14", "title": "Learning from all vehicles", "journal": "", "year": "", "authors": "Dian Chen; Philipp Kr\u00e4henb\u00fchl"}, {"ref_id": "b15", "title": "Learning by cheating", "journal": "", "year": "", "authors": "Dian Chen; Brady Zhou; Vladlen Koltun; Philipp Kr\u00e4henb\u00fchl"}, {"ref_id": "b16", "title": "Masked-attention mask transformer for universal image segmentation", "journal": "", "year": "", "authors": "Bowen Cheng; Ishan Misra; Alexander G Schwing; Alexander Kirillov; Rohit Girdhar"}, {"ref_id": "b17", "title": "Perpixel classification is not all you need for semantic segmentation", "journal": "", "year": "2021", "authors": "Bowen Cheng; Alex Schwing; Alexander Kirillov"}, {"ref_id": "b18", "title": "NEAT: Neural attention fields for end-to-end autonomous driving", "journal": "", "year": "", "authors": "Kashyap Chitta; Aditya Prakash; Andreas Geiger"}, {"ref_id": "b19", "title": "Transfuser: Imitation with transformer-based sensor fusion for autonomous driving", "journal": "IEEE TPAMI", "year": "2022", "authors": "Kashyap Chitta; Aditya Prakash; Bernhard Jaeger; Zehao Yu; Katrin Renz; Andreas Geiger"}, {"ref_id": "b20", "title": "End-to-end driving via conditional imitation learning", "journal": "", "year": "2018", "authors": "Felipe Codevilla; Matthias M\u00fcller; Antonio L\u00f3pez; Vladlen Koltun; Alexey Dosovitskiy"}, {"ref_id": "b21", "title": "Exploring the limitations of behavior cloning for autonomous driving", "journal": "", "year": "2019", "authors": "Felipe Codevilla; Eder Santana; M Antonio; Adrien L\u00f3pez;  Gaidon"}, {"ref_id": "b22", "title": "Multi-task learning with deep neural networks: A survey", "journal": "", "year": "2020", "authors": "Michael Crawshaw"}, {"ref_id": "b23", "title": "Lookout: Diverse multi-future prediction and planning for self-driving", "journal": "", "year": "2021", "authors": "Alexander Cui; Sergio Casas; Abbas Sadat; Renjie Liao; Raquel Urtasun"}, {"ref_id": "b24", "title": "Multixnet: Multiclass multistage multimodal motion prediction", "journal": "", "year": "", "authors": "Nemanja Djuric; Henggang Cui; Zhaoen Su; Shangxuan Wu; Huahua Wang; Fang-Chieh Chou; Luisa San Martin; Song Feng; Rui Hu; Yang Xu; Alyssa Dayan; Sidney Zhang; Brian C Becker; Gregory P Meyer; Carlos Vallespi-Gonzalez; Carl K Wellington"}, {"ref_id": "b25", "title": "CARLA: An open urban driving simulator", "journal": "", "year": "2017", "authors": "Alexey Dosovitskiy; German Ros; Felipe Codevilla; Antonio Lopez; Vladlen Koltun"}, {"ref_id": "b26", "title": "Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset", "journal": "Alexander Mc-Cauley", "year": "2021", "authors": "Scott Ettinger; Shuyang Cheng; Benjamin Caine; Chenxi Liu; Hang Zhao; Sabeek Pradhan; Yuning Chai; Ben Sapp; Yin Charles R Qi; Zoey Zhou; Aur\u00e9lien Yang; Pei Chouard; Jiquan Sun; Vijay Ngiam;  Vasudevan"}, {"ref_id": "b27", "title": "Multi-view fusion of sensor data for improved perception and prediction in autonomous driving", "journal": "", "year": "", "authors": "Sudeep Fadadu; Shreyash Pandey; Darshan Hegde; Yi Shi; Fang-Chieh Chou; Nemanja Djuric; Carlos Vallespi-Gonzalez"}, {"ref_id": "b28", "title": "Vectornet: Encoding hd maps and agent dynamics from vectorized representation", "journal": "", "year": "2020", "authors": "Jiyang Gao; Chen Sun; Hang Zhao; Yi Shen; Dragomir Anguelov; Congcong Li; Cordelia Schmid"}, {"ref_id": "b29", "title": "ViP3D: End-toend visual trajectory prediction via 3d agent queries", "journal": "", "year": "2023", "authors": "Junru Gu; Chenxu Hu; Tianyuan Zhang; Xuanyao Chen; Yilun Wang; Yue Wang; Hang Zhao"}, {"ref_id": "b30", "title": "Exploring recurrent long-term temporal fusion for multi-view 3d perception", "journal": "", "year": "2023", "authors": "Chunrui Han; Jianjian Sun; Zheng Ge; Jinrong Yang; Runpei Dong; Hongyu Zhou; Weixin Mao; Yuang Peng; Xiangyu Zhang"}, {"ref_id": "b31", "title": "Deep residual learning for image recognition", "journal": "", "year": "1921", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b32", "title": "Fishing net: Future inference of semantic heatmaps in grids", "journal": "", "year": "2020", "authors": "Noureldin Hendy; Cooper Sloan; Feng Tian; Pengfei Duan; Nick Charchut; Yuesong Xie; Chuang Wang; James Philbin"}, {"ref_id": "b33", "title": "Model-based imitation learning for urban driving", "journal": "", "year": "2022", "authors": "Anthony Hu; Gianluca Corrado; Nicolas Griffiths; Zak Murez; Corina Gurau; Hudson Yeo; Alex Kendall; Roberto Cipolla; Jamie Shotton"}, {"ref_id": "b34", "title": "FIERY: Future instance prediction in bird'seye view from surround monocular cameras", "journal": "", "year": "2021", "authors": "Anthony Hu; Zak Murez; Nikhil Mohan; Sof\u00eda Dudas; Jeffrey Hawke; Vijay Badrinarayanan; Roberto Cipolla; Alex Kendall"}, {"ref_id": "b35", "title": "Monocular quasi-dense 3d object tracking", "journal": "IEEE TPAMI", "year": "2022", "authors": "Hou-Ning Hu; Yung-Hsu Yang; Tobias Fischer; Trevor Darrell; Fisher Yu; Min Sun"}, {"ref_id": "b36", "title": "Safe local motion planning with selfsupervised freespace forecasting", "journal": "", "year": "2008", "authors": "Peiyun Hu; Aaron Huang; John Dolan; David Held; Deva Ramanan"}, {"ref_id": "b37", "title": "ST-P3: End-to-end visionbased autonomous driving via spatial-temporal feature learning", "journal": "", "year": "2022", "authors": "Shengchao Hu; Li Chen; Penghao Wu; Hongyang Li; Junchi Yan; Dacheng Tao"}, {"ref_id": "b38", "title": "Differentiable integrated motion prediction and planning with learnable cost function for autonomous driving", "journal": "", "year": "2022", "authors": "Zhiyu Huang; Haochen Liu; Jingda Wu; Chen Lv"}, {"ref_id": "b39", "title": "MATS: An interpretable trajectory forecasting representation for planning and control", "journal": "", "year": "2021", "authors": "Boris Ivanovic; Amine Elhafsi; Guy Rosman; Adrien Gaidon; Marco Pavone"}, {"ref_id": "b40", "title": "Towards capturing the temporal dynamics for trajectory prediction: a coarse-to-fine approach", "journal": "", "year": "2022", "authors": "Xiaosong Jia; Li Chen; Penghao Wu; Jia Zeng; Junchi Yan; Hongyang Li; Yu Qiao"}, {"ref_id": "b41", "title": "Ide-net: Interactive driving event and pattern extraction from human data", "journal": "", "year": "", "authors": "Xiaosong Jia; Liting Sun; Masayoshi Tomizuka; Wei Zhan"}, {"ref_id": "b42", "title": "Multi-agent trajectory prediction by combining egocentric and allocentric views", "journal": "", "year": "", "authors": "Xiaosong Jia; Liting Sun; Hang Zhao; Masayoshi Tomizuka; Wei Zhan"}, {"ref_id": "b43", "title": "HDGT: Heterogeneous driving graph transformer for multi-agent trajectory prediction via scene encoding", "journal": "", "year": "2022", "authors": "Xiaosong Jia; Penghao Wu; Li Chen; Hongyang Li; Yu Liu; Junchi Yan"}, {"ref_id": "b44", "title": "Predictionnet: Real-time joint probabilistic traffic prediction for planning, control, and simulation", "journal": "", "year": "", "authors": "Alexey Kamenev; Lirui Wang; Ollin Boer Bohan; Ishwar Kulkarni; Bilal Kartal; Artem Molchanov; Stan Birchfield; David Nist\u00e9r; Nikolai Smolyanskiy"}, {"ref_id": "b45", "title": "Learning to drive in a day", "journal": "", "year": "2019", "authors": "Alex Kendall; Jeffrey Hawke; David Janz; Przemyslaw Mazur; Daniele Reda; John-Mark Allen; Vinh-Dieu Lam; Alex Bewley; Amar Shah"}, {"ref_id": "b46", "title": "Differentiable raycasting for self-supervised occupancy forecasting", "journal": "", "year": "", "authors": "Tarasha Khurana; Peiyun Hu; Achal Dave; Jason Ziglar; David Held; Deva Ramanan"}, {"ref_id": "b47", "title": "Joon-Young Lee, and In So Kweon. Video panoptic segmentation", "journal": "", "year": "2020", "authors": "Dahun Kim; Sanghyun Woo"}, {"ref_id": "b48", "title": "Stopnet: Scalable trajectory and occupancy prediction for urban autonomous driving", "journal": "", "year": "2022", "authors": "Jinkyu Kim; Reza Mahjourian; Scott Ettinger; Mayank Bansal; Brandyn White; Ben Sapp; Dragomir Anguelov"}, {"ref_id": "b49", "title": "Panoptic segmentation", "journal": "", "year": "2019", "authors": "Alexander Kirillov; Kaiming He; Ross Girshick; Carsten Rother; Piotr Doll\u00e1r"}, {"ref_id": "b50", "title": "An energy and gpu-computation efficient backbone network for real-time object detection", "journal": "", "year": "2019", "authors": "Youngwan Lee; Joong-Won Hwang; Sangrok Lee; Yuseok Bae; Jongyoul Park"}, {"ref_id": "b51", "title": "Mask dino: Towards a unified transformer-based framework for object detection and segmentation", "journal": "", "year": "2023", "authors": "Feng Li; Hao Zhang; Shilong Liu; Lei Zhang; Lionel M Ni; Heung-Yeung Shum"}, {"ref_id": "b52", "title": "End-to-end contextual perception and prediction with interaction transformer", "journal": "", "year": "", "authors": "Bin Lingyun Luke Li; Ming Yang; Wenyuan Liang; Mengye Zeng; Sean Ren; Raquel Segal;  Urtasun"}, {"ref_id": "b53", "title": "Unifying voxel-based representation with transformer for 3d object detection", "journal": "", "year": "", "authors": "Yanwei Li; Yilun Chen; Xiaojuan Qi; Zeming Li; Jian Sun; Jiaya Jia"}, {"ref_id": "b54", "title": "BEVFormer: Learning bird's-eye-view representation from multi-camera images via spatiotemporal transformers", "journal": "", "year": "2022", "authors": "Zhiqi Li; Wenhai Wang; Hongyang Li; Enze Xie; Chonghao Sima; Tong Lu; Qiao Yu; Jifeng Dai"}, {"ref_id": "b55", "title": "Panoptic segformer: Delving deeper into panoptic segmentation with transformers", "journal": "", "year": "2022", "authors": "Zhiqi Li; Wenhai Wang; Enze Xie; Zhiding Yu; Anima Anandkumar; M Jose; Ping Alvarez; Tong Luo;  Lu"}, {"ref_id": "b56", "title": "Pnpnet: End-to-end perception and prediction with tracking in the loop", "journal": "", "year": "2020", "authors": "Ming Liang; Bin Yang; Wenyuan Zeng; Yun Chen; Rui Hu; Sergio Casas; Raquel Urtasun"}, {"ref_id": "b57", "title": "BEVFusion: A simple and robust lidar-camera fusion framework", "journal": "", "year": "", "authors": "Tingting Liang; Hongwei Xie; Kaicheng Yu; Zhongyu Xia; Zhiwei Lin; Yongtao Wang; Tao Tang; Bing Wang; Zhi Tang"}, {"ref_id": "b58", "title": "Cirl: Controllable imitative reinforcement learning for vision-based self-driving", "journal": "", "year": "2018", "authors": "Xiaodan Liang; Tairui Wang; Luona Yang; Eric Xing"}, {"ref_id": "b59", "title": "Effective adaptation in multi-task co-training for unified autonomous driving", "journal": "", "year": "", "authors": "Xiwen Liang; Yangxin Wu; Jianhua Han; Hang Xu; Chunjing Xu; Xiaodan Liang"}, {"ref_id": "b60", "title": "Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection", "journal": "", "year": "2017", "authors": "Tsung-Yi Lin; Priya Goyal; Ross Girshick"}, {"ref_id": "b61", "title": "Deep structured reactive planning", "journal": "", "year": "", "authors": "Jerry Liu; Wenyuan Zeng; Raquel Urtasun; Ersin Yumer"}, {"ref_id": "b62", "title": "Multimodal motion prediction with stacked transformers", "journal": "", "year": "", "authors": "Yicheng Liu; Jinghuai Zhang; Liangji Fang; Qinhong Jiang; Bolei Zhou"}, {"ref_id": "b63", "title": "BEVFusion: Multi-task multi-sensor fusion with unified bird's-eye view representation", "journal": "", "year": "", "authors": "Zhijian Liu; Haotian Tang; Alexander Amini; Xingyu Yang; Huizi Mao; Daniela Rus; Song Han"}, {"ref_id": "b64", "title": "Decoupled weight decay regularization", "journal": "", "year": "2018", "authors": "Ilya Loshchilov; Frank Hutter"}, {"ref_id": "b65", "title": "Fast and furious: Real time end-to-end 3d detection, tracking and motion forecasting with a single convolutional net", "journal": "", "year": "2018", "authors": "Wenjie Luo; Bin Yang; Raquel Urtasun"}, {"ref_id": "b66", "title": "V-net: Fully convolutional neural networks for volumetric medical image segmentation", "journal": "", "year": "2016", "authors": "Fausto Milletari; Nassir Navab; Seyed-Ahmad Ahmadi"}, {"ref_id": "b67", "title": "Mobileye under the hood", "journal": "", "year": "2002", "authors": " Mobileye"}, {"ref_id": "b68", "title": "Wayformer: Motion forecasting via simple & efficient attention networks", "journal": "", "year": "2022", "authors": "Nigamaa Nayakanti; Rami Al-Rfou; Aurick Zhou; Kratarth Goel; S Khaled; Benjamin Refaat;  Sapp"}, {"ref_id": "b69", "title": "Scene transformer: A unified multi-task model for behavior prediction and planning", "journal": "", "year": "2022", "authors": "Jiquan Ngiam; Benjamin Caine; Vijay Vasudevan; Zhengdong Zhang; Hao-Tien Lewis Chiang; Jeffrey Ling; Rebecca Roelofs; Alex Bewley; Chenxi Liu; Ashish Venugopal; David Weiss; Ben Sapp; Zhifeng Chen; Jonathon Shlens"}, {"ref_id": "b70", "title": "NVIDIA DRIVE End-to-End Solutions for Autonomous Vehicles", "journal": "", "year": "", "authors": " Nvidia"}, {"ref_id": "b71", "title": "Cross-view semantic segmentation for sensing surroundings", "journal": "IEEE RA-L", "year": "2020", "authors": "Jiankai Bowen Pan;  Sun;  Ho Yin Tiga; Alex Leung; Bolei Andonian;  Zhou"}, {"ref_id": "b72", "title": "Is pseudo-lidar needed for monocular 3d object detection", "journal": "", "year": "2021", "authors": "Dennis Park; Rares Ambrus; Vitor Guizilini; Jie Li; Adrien Gaidon"}, {"ref_id": "b73", "title": "Time will tell: New outlooks and a baseline for temporal multiview 3d object detection", "journal": "", "year": "", "authors": "Jinhyung Park; Chenfeng Xu; Shijia Yang; Kurt Keutzer; Kris Kitani; Masayoshi Tomizuka; Wei Zhan"}, {"ref_id": "b74", "title": "Forecasting from lidar via future object detection", "journal": "", "year": "2022", "authors": "Neehar Peri; Jonathon Luiten; Mengtian Li; Aljo\u0161a O\u0161ep; Laura Leal-Taix\u00e9; Deva Ramanan"}, {"ref_id": "b75", "title": "Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d", "journal": "", "year": "2020", "authors": "Jonah Philion; Sanja Fidler"}, {"ref_id": "b76", "title": "Alvinn: An autonomous land vehicle in a neural network", "journal": "", "year": "1988", "authors": "A Dean;  Pomerleau"}, {"ref_id": "b77", "title": "Multi-modal fusion transformer for end-to-end autonomous driving", "journal": "", "year": "", "authors": "Aditya Prakash; Kashyap Chitta; Andreas Geiger"}, {"ref_id": "b78", "title": "Jost Tobias Springenberg", "journal": "", "year": "", "authors": "Scott Reed; Konrad Zolna; Emilio Parisotto; Sergio Gomez Colmenarejo; Alexander Novikov; Gabriel Barth-Maron; Mai Gimenez; Yury Sulsky; Jackie Kay"}, {"ref_id": "b79", "title": "Generalized intersection over union: A metric and a loss for bounding box regression", "journal": "", "year": "2019", "authors": "Hamid Rezatofighi; Nathan Tsoi; Junyoung Gwak; Amir Sadeghian; Ian Reid; Silvio Savarese"}, {"ref_id": "b80", "title": "PRECOG: Prediction conditioned on goals in visual multi-agent settings", "journal": "", "year": "2019", "authors": "Nicholas Rhinehart; Rowan Mcallister; Kris Kitani; Sergey Levine"}, {"ref_id": "b81", "title": "Pranaab Dhawan, and Raquel Urtasun. Perceive, predict, and plan: Safe motion planning through interpretable semantic representations", "journal": "", "year": "2020", "authors": "Abbas Sadat; Sergio Casas; Mengye Ren; Xinyu Wu"}, {"ref_id": "b82", "title": "Safety-enhanced autonomous driving using interpretable sensor fusion transformer", "journal": "", "year": "", "authors": "Hao Shao; Letian Wang; Ruobing Chen; Hongsheng Li; Yu Liu"}, {"ref_id": "b83", "title": "Motion transformer with global intention localization and local movement refinement", "journal": "", "year": "", "authors": "Shaoshuai Shi; Li Jiang; Dengxin Dai; Bernt Schiele"}, {"ref_id": "b84", "title": "Srcn3d: Sparse r-cnn 3d surround-view camera object detection and tracking for autonomous driving", "journal": "", "year": "2022", "authors": "Yining Shi; Jingyan Shen; Yifan Sun; Yunlong Wang; Jiaxin Li; Shiqi Sun; Kun Jiang; Diange Yang"}, {"ref_id": "b85", "title": "Pip: Planninginformed trajectory prediction for autonomous driving", "journal": "", "year": "2020", "authors": "Haoran Song; Wenchao Ding; Yuxuan Chen; Shaojie Shen; Michael Yu Wang; Qifeng Chen"}, {"ref_id": "b86", "title": "", "journal": "", "year": "", "authors": " Tesla; A I Tesla;  Day"}, {"ref_id": "b87", "title": "Integrating grid-based and topological maps for mobile robot navigation", "journal": "", "year": "1996", "authors": "Sebastian Thrun; Arno B\u00fccken"}, {"ref_id": "b88", "title": "End-to-end model-free reinforcement learning for urban driving using implicit affordances", "journal": "", "year": "2020", "authors": "Marin Toromanoff; Emilie Wirbel; Fabien Moutarde"}, {"ref_id": "b89", "title": "Multipath++: Efficient information fusion and trajectory aggregation for behavior prediction", "journal": "", "year": "2021", "authors": "Balakrishnan Varadarajan; Ahmed Hefny; Avikalp Srivastava; S Khaled; Nigamaa Refaat; Andre Nayakanti; Kan Cornman; Bertrand Chen; Chi Pang Douillard; Dragomir Lam; Benjamin Anguelov;  Sapp"}, {"ref_id": "b90", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b91", "title": "Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework", "journal": "", "year": "", "authors": "Peng Wang; An Yang; Rui Men; Junyang Lin; Shuai Bai; Zhikang Li; Jianxin Ma; Chang Zhou; Jingren Zhou; Hongxia Yang"}, {"ref_id": "b92", "title": "Immortal tracker: Tracklet never dies", "journal": "", "year": "2021", "authors": "Qitai Wang; Yuntao Chen; Ziqi Pang; Naiyan Wang; Zhaoxiang Zhang"}, {"ref_id": "b93", "title": "Bin Yang, and Raquel Urtasun. Perceive, attend, and drive: Learning spatial attention for safe self-driving", "journal": "", "year": "", "authors": "Bob Wei; Mengye Ren; Wenyuan Zeng; Ming Liang"}, {"ref_id": "b94", "title": "Policy pre-training for autonomous driving via self-supervised geometric modeling", "journal": "", "year": "", "authors": "Penghao Wu; Li Chen; Hongyang Li; Xiaosong Jia; Junchi Yan; Yu Qiao"}, {"ref_id": "b95", "title": "Motionnet: Joint perception and motion prediction for autonomous driving based on bird's eye view maps", "journal": "", "year": "2020", "authors": "Pengxiang Wu; Siheng Chen; Dimitris N Metaxas"}, {"ref_id": "b96", "title": "Trajectory-guided control prediction for end-to-end autonomous driving: A simple yet strong baseline", "journal": "", "year": "2022", "authors": "Penghao Wu; Xiaosong Jia; Li Chen; Junchi Yan; Hongyang Li; Yu Qiao"}, {"ref_id": "b97", "title": "Quality matters: Embracing quality clues for robust 3d multi-object tracking", "journal": "", "year": "2022", "authors": "Jinrong Yang; En Yu; Zeming Li; Xiaoping Li; Wenbing Tao"}, {"ref_id": "b98", "title": "Agentformer: Agent-aware transformers for sociotemporal multi-agent forecasting", "journal": "", "year": "", "authors": "Ye Yuan; Xinshuo Weng; Yanglan Ou; Kris M Kitani"}, {"ref_id": "b99", "title": "Motr: End-to-end multiple-object tracking with transformer", "journal": "", "year": "2021", "authors": "Fangao Zeng; Bin Dong; Tiancai Wang; Xiangyu Zhang; Yichen Wei"}, {"ref_id": "b100", "title": "End-to-end interpretable neural motion planner", "journal": "", "year": "2019", "authors": "Wenyuan Zeng; Wenjie Luo; Simon Suo; Abbas Sadat; Bin Yang; Sergio Casas; Raquel Urtasun"}, {"ref_id": "b101", "title": "Dsdnet: Deep structured self-driving network", "journal": "", "year": "2020", "authors": "Wenyuan Zeng; Shenlong Wang; Renjie Liao; Yun Chen; Bin Yang; Raquel Urtasun"}, {"ref_id": "b102", "title": "Learning by watching", "journal": "", "year": "2021", "authors": "Jimuyang Zhang; Eshed Ohn-Bar"}, {"ref_id": "b103", "title": "MUTR3D: A Multi-camera Tracking Framework via 3D-to-2D Queries", "journal": "", "year": "2006", "authors": "Tianyuan Zhang; Xuanyao Chen; Yue Wang; Yilun Wang; Hang Zhao"}, {"ref_id": "b104", "title": "BEVerse: Unified perception and prediction in birds-eye-view for vision-centric autonomous driving", "journal": "", "year": "2022", "authors": "Yunpeng Zhang; Zheng Zhu; Wenzhao Zheng; Junjie Huang; Guan Huang; Jie Zhou; Jiwen Lu"}, {"ref_id": "b105", "title": "End-to-end urban driving by imitating a reinforcement learning coach", "journal": "", "year": "2021", "authors": "Zhejun Zhang; Alexander Liniger; Dengxin Dai; Fisher Yu; Luc Van Gool"}, {"ref_id": "b106", "title": "TNT: Target-driven trajectory prediction", "journal": "", "year": "2020", "authors": "Hang Zhao; Jiyang Gao; Tian Lan; Chen Sun; Benjamin Sapp; Balakrishnan Varadarajan; Yue Shen; Yi Shen; Yuning Chai; Cordelia Schmid; Congcong Li; Dragomir Anguelov"}, {"ref_id": "b107", "title": "Uniperceiver-moe: Learning sparse generalist models with conditional moes", "journal": "", "year": "", "authors": "Jinguo Zhu; Xizhou Zhu; Wenhai Wang; Xiaohua Wang; Hongsheng Li; Xiaogang Wang; Jifeng Dai"}, {"ref_id": "b108", "title": "Deformable detr: Deformable transformers for end-to-end object detection", "journal": "", "year": "2004", "authors": "Xizhou Zhu; Weijie Su; Lewei Lu; Bin Li; Xiaogang Wang; Jifeng Dai"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. Comparison on the various designs of autonomous driving framework. (a) Most industrial solutions deploy separate models for different tasks. (b)The multi-task learning scheme shares a backbone with divided task heads. (c) The end-to-end paradigm unites modules in perception and prediction. Previous attempts either adopt a direct optimization on planning in (c.1) or devise the system with partial components in (c.2). Instead, we argue in (c.3) that a desirable system should be planning-oriented as well as properly organize preceding tasks to facilitate planning.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "(3): (1) the position of scene-level anchors I s ; (2) the position of agent-level anchors I a ; (3) current location of the agent i and (4) the predicted goal point.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Qpos = MLP(PE(I s )) + MLP(PE(I a )) + MLP(PE(x 0 )) + MLP(PE(x l\u22121 T )).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "1-3, which indicate training perception sub-tasks together leads to comparable results to a single task. Additionally, compared with naive multi-task learning (Exp.0, Fig. 1(b)), Exp.12 outperforms it by a significant margin in all essential metrics (-15.2% minADE, -17.0% minFDE, -3.2 MR(%)), +4.9 IoU-f.(%)., +5.9 VPQf.(%), -0.15m avg.L2, -0.51 avg.Col.(%)), showing the superiority of our planning-oriented design.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "indicating that it is essential to do motion forecasting in the scene-centric manner. The agent-goal point interaction enhances the motion query with the planning-oriented visual feature, and surrounding agents can further benefit from considering the ego vehicle's intention. Moreover, the non-linear optimization strategy improves the performance (-5.0% minADE, -8.4% minFDE, -1.0 MR(%), +0.7 minFDE-mAP(%)) by taking perceptual uncertainty into account in the end-to-end scenario.Effect of designs in OccFormer.As illustrated in Table 9, attending each pixel to all agents without locality constraints (Exp.2) results in slightly worse performance compared to an attention-free baseline (Exp.1). The occupancy-guided attention mask resolves the problem and brings in gain, especially for nearby areas (Exp.3, +1.0 IoU-n.(%), +1.4 VPQ-n.(%)). Additionally, reusing the mask feature M t instead of the agent feature to acquire the occupancy feature further enhances performance.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 4 .4Figure 4. MotionFormer.It consists of N stacked agent-agent, agent-map, and agent-goal interaction transformers. The agentagent, and agent-map interaction modules are built with standard transformer decoder layers. The agent-goal interaction module is constructed upon the deformable cross-attention module[109]. I s T : the end point of scene-level anchor, I a T : the end point of clustered agent-level anchor,x0: the agent's current position,x l\u22121 T : the predicted goal point from the previous layer, Q l\u22121 ctx : query context from the previous layer.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 5 .5Figure5. Illustration of agent-goal interaction Module. The BEV visual feature is sampled near each agent's goal points with deformable cross-attention.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 6 .6Figure 6. OccFormer. It is composed of To sequential blockswhere To is the temporal horizon (including current and future frames) and each block is responsible for generating occupancy of one specific frame. We incorporate both dense scene features and sparse agent features, which are encoded from upstream track query QA, agent position PA and motion query QX , to inject agent-level knowledge into future scene representations. We form instance-level occupancy\u00d4 t A via a matrix multiplication between agent-level feature and decoded dense feature at the end of each block.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 10 .10Figure10. Critical case visualization. Here we demonstrate two critical cases. The first scenario (top) shows that the ego vehicle is yielding to two pedestrians crossing the street, and the second scenario (down) shows that the ego vehicle is yielding to a fast-moving car and waiting to go straight without protection near an intersection. We can observe that much attention is paid to the most critical agents, i.e., pedestrians and fast-moving vehicles, as well as the intended goal location.", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 13 .13Figure13. Failure cases 1. Here we present a long-tail scenario, where a large trailer with a white container occupies the entire road. We can observe that our tracking module fails to detect the accurate size of the front trailer and heading angles of vehicles beside the road.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Motion Occ. Plan AMOTA\u2191 AMOTP\u2193 IDS\u2193 IoU-lane\u2191 IoU-road\u2191 minADE\u2193 minFDE\u2193 MR\u2193 IoU-n.\u2191 IoU-f.\u2191 VPQ-n.\u2191 VPQ-f.\u2191 avg.L2\u2193 avg.Col.\u2193 0", "figure_data": "IDTrack Map \u2713 \u2713Modules \u2713\u2713\u27130.356Tracking 1.328893Mapping 0.302 0.6750.858Motion Forecasting 1.2700.18655.9Occupancy Prediction 34.6 47.826.4Planning 1.154 0.9411\u27130.3481.333791-----------2\u2713---0.3050.674---------3\u2713\u27130.3551.3367850.3010.671---------4\u2713-----0.8151.2240.182------5\u2713\u27130.3601.350919--0.7511.1090.162------6\u2713\u2713\u27130.3541.3398200.3030.6720.736(-9.7%) 1.066(-12.9%) 0.158------7\u2713--------60.537.052.429.8--8\u2713\u27130.3601.322809-----62.138.452.232.1--9\u2713\u2713\u2713\u27130.3591.35910570.3040.6750.710(-3.5%) 1.005(-5.8%) 0.14662.339.453.132.2--10\u2713-----------1.1310.77311\u2713\u2713\u2713\u27130.3661.3378890.3030.6720.7411.0770.157----1.0140.71712\u2713\u2713\u2713\u2713\u27130.3581.3346410.3020.6720.7281.0540.15462.339.552.832.31.0040.430"}, {"figure_label": "34", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Multi-object tracking. UniAD outperforms previous end-to-end MOT techniques (with image inputs only) on all metrics. \u2020: Tracking-by-detection method with post-association, reimplemented with BEVFormer for a fair comparison. Online mapping. UniAD achieves competitive performance against state-of-the-art perception-oriented methods, with comprehensive road semantics. We report segmentation IoU (%).", "figure_data": "MethodLanes\u2191 Drivable\u2191 Divider\u2191 Crossing\u2191VPN [72]18.076.0--LSS [76]18.373.9--BEVFormer [55]23.977.5--BEVerse  \u2020 [105]--30.617.2UniAD31.369.125.713.8MethodminADE(m)\u2193 minFDE(m)\u2193 MR\u2193 EPA\u2191PnPNet  \u2020 [57]1.151.950.226 0.222ViP3D [30]2.052.840.246 0.226Constant Pos.5.8010.270.347-Constant Vel.2.134.010.318-UniAD0.711.020.151 0.456"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Motion forecasting. UniAD remarkably outperforms previous vision-based end-to-end methods. We also report two settings of modeling vehicles with constant positions or velocities as comparisons. \u2020: Reimplemented with BEVFormer.Prediction results. Motion forecasting results are shown in Table5, where UniAD remarkably outperforms previous vision-based end-to-end methods. It reduces prediction errors by 38.3% and 65.4% on minADE compared to PnPNet-vision", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Occupancy prediction. UniAD gets significant improvement in nearby areas, which are more critical for planning. \"n.\" and \"f.\" indicates near (30\u00d730m) and far (50\u00d750m) evaluation ranges respectively. \u2020: Trained with heavy augmentations.", "figure_data": "MethodIoU-n.\u2191 IoU-f.\u2191 VPQ-n.\u2191 VPQ-f.\u2191FIERY [35]59.436.750.229.9StretchBEV [1]55.537.146.029.0ST-P3 [38]-38.9-32.1BEVerse  \u2020 [105]61.440.954.336.1UniAD63.440.254.733.5Method1sL2(m)\u2193 2s 3sAvg.1sCol. Rate(%)\u2193 2s 3sAvg.NMP  \u2020 [101]--2.31---1.92-SA-NMP  \u2020 [101]--2.05---1.59-FF  \u2020 [37]0.55 1.20 2.54 1.43 0.06 0.17 1.07 0.43EO  \u2020 [47]0.67 1.36 2.78 1.60 0.04 0.09 0.88 0.33ST-P3 [38]1.33 2.11 2.90 2.11 0.23 0.62 1.27 0.71UniAD0.48 0.96 1.65 1.03 0.05 0.17 0.71 0.31"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Planning. UniAD achieves the lowest L2 error and collision rate in all time intervals and even outperforms LiDAR-based methods ( \u2020) in most cases, verifying the safety of our system.", "figure_data": "IDScene-l. Anch.Goal Inter.Ego Q NLO. minADE\u2193 minFDE\u2193 MR\u2193minFDE -mAP  *  \u219110.8441.3360.1770.2462\u27130.7681.1590.1640.2673\u2713\u27130.7551.1300.1680.2644\u2713\u2713\u27130.7471.0960.1560.2665\u2713\u2713\u2713\u27130.7101.0040.1460.273"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Ablation for designs in the motion forecasting module. All components contribute to the ultimate performance. \"Scenel. Anch.\" denotes rotated scene-level anchors. \"Goal Inter.\" means the agent-goal point interaction. \"Ego Q\" represents the egovehicle query and \"NLO.\" is the non-linear optimization strategy.", "figure_data": ""}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Visualization results. We show results for all tasks in surround-view images and BEV. Predictions from motion and occupancy modules are consistent, and the ego vehicle is yielding to the front black car in this case. Each agent is illustrated with a unique color. Only top-1 and top-3 trajectories from motion forecasting are selected for visualization on image-view and BEV respectively.", "figure_data": "CAM_FRONT_LEFTCAM_FRONTCAM_FRONT_RIGHTBEV (TOP VIEW)OccupancyPredictionPlanTrackMapMotionForecastCAM_BACK_LEFTCAM_BACKCAM_BACK_RIGHTFigure 3. ID Cross. Attn.Attn. MaskMask Feat.IoU-n.\u2191 IoU-f.\u2191 VPQ-n.\u2191 VPQ-f.\u2191161.239.751.531.82\u271361.339.451.031.83\u2713\u271362.339.752.432.54\u2713\u2713\u271362.639.553.232.8IDBEV Col. Att. Loss Optim. Occ.1sL2\u2193 2s3s1sCol. Rate\u2193 2s3s10.44 0.99 1.71 0.56 0.88 1.642\u27130.44 1.04 1.81 0.35 0.71 1.583\u2713\u27130.44 1.02 1.76 0.30 0.51 1.394\u2713\u2713\u27130.54 1.09 1.81 0.13 0.42 1.05"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "C.1. Joint perception and prediction . . . . . . . . 14 C.2. Joint prediction and planning . . . . . . . . . 15 C.3. End-to-end motion planning . . . . . . . . . 15 Detection and Tracking . . . . . . . . . . . . 15 E.2. Online Mapping . . . . . . . . . . . . . . . 15 E.3. Motion Forecasting . . . . . . . . . . . . . . 17", "figure_data": "D. Notations15E. Implementation Details15E.1."}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Lookup table of notations and hyperparameters in the paper. The superscript t in certain notations denotes the t th block of OccFormer, and is omitted in descriptions for simplicity.", "figure_data": ""}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "The chosen image IoU-lane\u2191 IoU-road\u2191 minADE\u2193 minFDE\u2193 MR\u2193 EPA\u2191 IoU-n.\u2191 IoU-f.\u2191 VPQ-n.\u2191 VPQ-f.\u2191 avg.L2\u2193 avg.Col.\u2193", "figure_data": "Methods AMOTA\u2191 AMOTP\u2193 IDS\u2193 UniAD-S Tracking Encoder R50 0.241 1.488 958Mapping 0.315 0.6890.788Motion Forecasting 1.126 0.156 0.38159.4Occupancy Prediction 35.6 49.228.9Planning 1.04 0.32UniAD-BR1010.3591.3209060.3130.6910.7081.0250.151 0.45663.440.254.733.51.030.31UniAD-L V2-99  *0.4091.25915830.3230.7090.7231.0670.158 0.50864.142.655.836.91.030.29"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Comparisons between three variations of UniAD. * : pre-trained with extra depth data[73].", "figure_data": "IDDet. Track Map Motion Occ. Plan #Params FLOPs FPS0 [105]\u2713\u2713\u2713102.5M 1921G-1\u271365.9M1324G 4.22\u2713\u271368.2M1326G 2.73\u2713\u2713\u271395.8M1520G 2.24\u2713\u2713\u2713\u2713108.6M 1535G 2.15\u2713\u2713\u2713\u2713\u2713122.5M 1701G 2.06\u2713\u2713\u2713\u2713\u2713\u2713125.0M 1709G 1.8"}], "formulas": [{"formula_id": "formula_0", "formula_text": "(c.3) UniAD (ours) \u2713 \u2713 \u2713 \u2713 \u2713 \u2713", "formula_coordinates": [2.0, 62.89, 189.86, 204.64, 6.68]}, {"formula_id": "formula_1", "formula_text": "Q a/m = MHCA(MHSA(Q), Q A /Q M ),(1)", "formula_coordinates": [4.0, 92.44, 133.01, 193.92, 9.96]}, {"formula_id": "formula_2", "formula_text": "Q g = DeformAttn(Q,x l\u22121 T , B),(2)", "formula_coordinates": [4.0, 98.15, 222.58, 188.21, 13.31]}, {"formula_id": "formula_3", "formula_text": "wherex l\u22121 T", "formula_coordinates": [4.0, 50.11, 245.63, 47.02, 13.31]}, {"formula_id": "formula_5", "formula_text": "x * = arg min x c(x,x),(4)", "formula_coordinates": [4.0, 382.39, 203.24, 162.72, 16.21]}, {"formula_id": "formula_6", "formula_text": "c(x,x) = \u03bb xy \u2225x,x\u2225 2 + \u03bb goal \u2225x T ,x T \u2225 2 + \u03d5\u2208\u03a6 \u03d5(x), (5", "formula_coordinates": [4.0, 315.04, 277.21, 226.2, 20.14]}, {"formula_id": "formula_7", "formula_text": ")", "formula_coordinates": [4.0, 541.24, 277.53, 3.87, 8.64]}, {"formula_id": "formula_8", "formula_text": "G t = MLP t ([Q A , P A , Q X ]), t = 1, . . . , T o ,(6)", "formula_coordinates": [5.0, 79.54, 162.67, 206.82, 11.72]}, {"formula_id": "formula_9", "formula_text": "D t ds = MHCA(MHSA(F t ds ), G t , attn mask = O t m ). (7)", "formula_coordinates": [5.0, 64.62, 420.44, 221.75, 12.85]}, {"formula_id": "formula_10", "formula_text": "O t A = U t \u2022 F t dec .(8)", "formula_coordinates": [5.0, 135.58, 702.12, 150.78, 12.85]}, {"formula_id": "formula_11", "formula_text": "\u03c4 * = arg min \u03c4 f (\u03c4,\u03c4 ,\u00d4),(9)", "formula_coordinates": [5.0, 376.55, 236.42, 168.57, 16.21]}, {"formula_id": "formula_12", "formula_text": "f (\u03c4,\u03c4 ,\u00d4) = \u03bb coord \u2225\u03c4,\u03c4 \u2225 2 + \u03bb obs t D(\u03c4 t ,\u00d4 t ), (10", "formula_coordinates": [5.0, 321.88, 336.87, 219.09, 21.69]}, {"formula_id": "formula_13", "formula_text": ")", "formula_coordinates": [5.0, 540.96, 339.26, 4.15, 8.64]}, {"formula_id": "formula_14", "formula_text": "D(\u03c4 t ,\u00d4 t ) = (x,y)\u2208S 1 \u03c3 \u221a 2\u03c0 exp(\u2212 \u2225\u03c4 t \u2212 (x, y)\u2225 2 2 2\u03c3 2 ). (11", "formula_coordinates": [5.0, 317.32, 361.81, 223.64, 28.84]}, {"formula_id": "formula_15", "formula_text": ")", "formula_coordinates": [5.0, 540.96, 370.44, 4.15, 8.64]}, {"formula_id": "formula_16", "formula_text": "I s i,T = R i I a T + T i ,(12)", "formula_coordinates": [17.0, 389.62, 153.7, 155.5, 12.69]}, {"formula_id": "formula_17", "formula_text": "L 1 = L track + L map . (13", "formula_coordinates": [18.0, 387.0, 704.2, 153.97, 9.81]}, {"formula_id": "formula_18", "formula_text": ")", "formula_coordinates": [18.0, 540.96, 704.51, 4.15, 8.64]}, {"formula_id": "formula_19", "formula_text": "L 2 = L track + L map + L motion + L occ + L plan . (14", "formula_coordinates": [19.0, 70.04, 532.88, 212.17, 9.81]}, {"formula_id": "formula_20", "formula_text": ")", "formula_coordinates": [19.0, 282.21, 533.19, 4.15, 8.64]}, {"formula_id": "formula_21", "formula_text": "L col (\u03c4 , \u03b4) = i,t IoU(box(\u03c4 t , w + \u03b4, l + \u03b4), b i,t )), (15", "formula_coordinates": [19.0, 318.41, 510.79, 222.56, 19.91]}, {"formula_id": "formula_22", "formula_text": ")", "formula_coordinates": [19.0, 540.96, 511.11, 4.15, 8.64]}, {"formula_id": "formula_23", "formula_text": "L plan = \u03bb imi |\u03c4 ,\u03c4 | 2 + \u03bb col (\u03c9,\u03b4) \u03c9L col (\u03c4 , \u03b4),(16)", "formula_coordinates": [19.0, 342.37, 544.97, 202.74, 20.53]}, {"formula_id": "formula_24", "formula_text": "AMOTA = 1 n \u2212 1 r\u2208{ 1 n\u22121 , 2 n\u22121 ,...,1} MOTA r ,(17)", "formula_coordinates": [20.0, 70.31, 298.19, 216.05, 29.94]}, {"formula_id": "formula_25", "formula_text": "MOTA r = max(0, 1 \u2212 FP r + FN r + IDS r \u2212 (1 \u2212 r)GT rGT ),(18)", "formula_coordinates": [20.0, 50.11, 351.78, 236.25, 30.46]}, {"formula_id": "formula_26", "formula_text": "AMOTP = 1 n \u2212 1 r\u2208{ 1 n\u22121 , 2 n\u22121 ,...,1} i,t d i,t TP r ,(19)", "formula_coordinates": [20.0, 68.31, 451.7, 218.05, 31.43]}, {"formula_id": "formula_27", "formula_text": "VPQ = H t=0 (pt,qt)\u2208TPt IoU(p t , q t ) |TP t | + 1 2 |FP t | + 1 2 |FN t | ,(20)", "formula_coordinates": [20.0, 349.96, 354.56, 195.15, 30.2]}], "doi": ""}