{"title": "BoltzRank: Learning to Maximize Expected Ranking Gain", "authors": "Maksims N Volkovs; Richard S Zemel", "pub_date": "", "abstract": "Ranking a set of retrieved documents according to their relevance to a query is a popular problem in information retrieval. Methods that learn ranking functions are difficult to optimize, as ranking performance is typically judged by metrics that are not smooth. In this paper we propose a new listwise approach to learning to rank. Our method creates a conditional probability distribution over rankings assigned to documents for a given query, which permits gradient ascent optimization of the expected value of some performance measure. The rank probabilities take the form of a Boltzmann distribution, based on an energy function that depends on a scoring function composed of individual and pairwise potentials. Including pairwise potentials is a novel contribution, allowing the model to encode regularities in the relative scores of documents; existing models assign scores at test time based only on individual documents, with no pairwise constraints between documents. Experimental results on the LETOR3.0 data set show that our method out-performs existing learning approaches to ranking.", "sections": [{"heading": "Introduction", "text": "Ranking in general, and in particular web document ranking, has received a lot of attention recently primarily due to its direct application in search engines. In a document retrieval domain, the problem of learning to rank can be described as follows. During training, the system is given queries and, for each query, a retrieved set of documents and their relevance levels. The goal is then to construct a ranking function Appearing in Proceedings of the 26 th International Conference on Machine Learning, Montreal, Canada, 2009. Copyright 2009 by the author(s)/owner(s).\nwhich, when presented with a new query, would accurately rank the corresponding retrieved documents.\nSeveral metrics are used in information retrieval (IR) to evaluate the performance of a ranking function. Two standard metrics are Normalized Discounted Cumulative Gain (NDCG) (Jarvelin et al., 2000) and Mean Average Precision (MAP) (Baeza-Yates at al., 1999). NDCG offers some advantages, e.g., a truncation level parameter that reflects how many documents are shown to the user, that make it especially wellsuited to document retrieval. We will thus concentrate on NDCG throughout this paper, but will also demonstrate that MAP or any other IR evaluation metric can also be successfully optimized using our approach.\nMost current ranking algorithms that have been proposed to solve this problem can be divided into three main categories: individual, pairwise, and listwise. The individual methods such as PRank (Crammer et al., 2001) do not use any relative information between documents, instead attempting to directly create a scoring function, scores of which are then used to rank the documents. On the other hand, the pairwise methods, including RankNet (Burges et al., 2005), its extension LambdaRank , and RankBoost (Freund et al., 2003), concentrate on minimizing the relative pairwise misclassification error in document rankings. Finally, listwise methods such as AdaRank , SoftRank (Taylor et al., 2008), ListNet (Cao et al., 2007) and C-CRF (Quin at al., 2008) use lists of ranked documents as \"instances\" during training, and learn a ranking model by minimizing some listwise loss function. The approach presented in this paper also falls into the listwise category. We chose to work in the listwise domain because it is the most natural way to model the ranking problem, as it is the only approach that allows direct incorporation of IR evaluation metrics, which are always functions of ranked lists and not individual documents or their pairs. Existing ranking methods share two main disadvantages that significantly affect their performance.\nFirst, current methods are do not directly incorporate NDCG into the learning process. A surrogate function such as pairwise misclassification loss or a bound on NDCG is typically introduced, and optimizing this function leads to an indirect optimization of NDCG. The main problem with this approach is that by optimizing a different function a significant amount of effort of the system can be wasted in areas that have little or no effect on NDCG (Burges, 2006). Second, none of the current methods consider higher-order interactions between documents at test time. Even the pairwise methods, which learn based on relative information in pairs of documents, produce a scoring function that operates only on single documents for test queries. As a result, all the relative information that is used to learn the parameters of the scoring function is either disregarded or converted into a function based on individual documents. Therefore, potentially highly useful information in feature correlations between documents is not fully exploited.\nIn this paper we develop a new, flexible, ranking model that aims to solve both of the above mentioned problems. We refer to it as BoltzRank. BoltzRank utilizes a scoring function composed of individual and pairwise potentials to define a conditional probability distribution, in the form of a Boltzmann distribution, over all permutations of documents retrieved for a given query. We also formulate our approach based on a general loss function, which allows BoltzRank to directly include any IR performance measure into the learning process.\nThe rest of the paper is organized as follows. Section 2 provides a detailed description of the problem, notation and IR metrics. Section 3 introduces the BoltzRank method. Experimental results are presented in Section 4, and the final section contains conclusions and future work.", "publication_ref": ["b6", "b0", "b5", "b3", "b7", "b10", "b4", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "General Framework", "text": "Here we formalize the problem of learning to rank in the document retrieval domain. At training time we are given a set of n queries Q = {q 1 , ..., q n }, and for each query q i we are also given a set of documents D i = {d i1 , ...., d imi }, and their associated relevance levels L i = {l i1 , ..., l imi }, where m i denotes the number of retrieved documents for query q i . Each document d ij is represented as a feature vector in \u211c p , and the corresponding relevance level l ij \u2208 \u211c (typically an integer) indicates how relevant that document is to the query q i . Given query q i , our model produces a ranking of documents D i through a scoring function f (q i , D i ), which outputs a set of scores S i = {s i1 , ...., s imi }, s ij \u2208 \u211c; we denote R i = {r i1 , ..., r imi }, r ij \u2208 {1, ..., m i } as a set of ranks given to D i when documents in D i are ranked according to S i . In this representation r ij is the position of the document d ij in the ranked order, where the document with the highest score is assigned a rank of 1 and the document with the lowest score a rank of m i . The goal of learning then is to create a scoring function f (we omit the arguments q, D to reduce notational clutter) such that, given a set of documents D with relevance levels L retrieved for a new query q, the permutation R resulting from scores S assigned by f has maximal agreement with L.\nNDCG and MAP are typically used to evaluate the agreement between the ranking produced by S and the relevance levels. For a given ranking R, and relevance levels L, NDCG is defined as:\nN DCG(R, L)@T = N q T j=1 2 rel(j) \u2212 1 log(1 + j)(1)\nwhere rel(j) is the relevance level of the document with rank j, and N q is a normalizing constant that ensures that a perfect ordering has NDCG value of 1. The normalizing constant allows an NDCG measure averaged over multiple queries with different numbers of documents to be meaningful. Furthermore, T is a truncation constant and is generally set to a small value to emphasize the utmost importance of getting the top ranked documents correct.\nMAP only allows binary (relevant/not relevant) document assignments, and is defined in terms of average precision (AP):\nAP (R, L) = m j=1 P @j * rel(j) m j=1 rel(j) (2)\nwhere m is the number of documents; and P @j is the precision at j:\nP @j = j i=1 rel(i) j(3)\nMAP is then computed by averaging AP over all queries.\nBoth NDCG and MAP include summations over sorted lists of documents, and therefore are not smooth, and can not be optimized by any direct gradient-based method. In this paper we present an expectation-based method which allows direct optimization of such non-smooth evaluation metrics frequently used in information retrieval. To further emphasize that any IR evaluation metric can be optimized with our approach, we will utilize a general error scoring function Si = {si1, ..., sim i } scores given by f to Di Ri = {ri1, ..., rim i } ranks obtained by sorting Si rel(j)\nrelevance of document at rank j G(Ri, Li)\nIR performance measure function G(R, L) to represent the target performance measure. Table 1 summarizes all the notation introduced above.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Our Method: BoltzRank", "text": "In this section we describe in detail the idea behind our approach together with learning and inference methods. To simplify notation, for the remainder of this section, we drop the query index i, and work with general query q and document set D = {d 1 , ..., d m }.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Distribution Over Permutations", "text": "The main idea that motivates the BoltzRank approach is the observation that if we define a probability distribution over document permutations, and consider the expectation of the target performance measure under this distribution, then it should be possible to propagate the derivatives and update the parameters that govern the scoring function to maximize this expectation. Thus we begin this section by defining a flexible probability distribution over permutations. More formally, given a set of scores S (f ) = {s 1 , ..., s m } assigned to D by f and corresponding ranking R = {r 1 , ..., r m }, we define the conditional energy of R given S as an average over unique document pairs in R:\nE(R|S) = 2 m * (m \u2212 1) rj >r k g q (r j \u2212 r k )(s j \u2212 s k ) (4)\nwhere g q is any sign preserving function, e.g.,\ng q (x) = \u03b1 q x (5)\nwhere \u03b1 q is a query-dependent positive constant. When r j >> r k (k beats j), E(R|S) gets a large negative contribution if s j << s k and a large positive one if s j >> s k . E(R|S) thus represents the lack of compatibility between the relative document orderings given by R and those given by S (f ) , with more positive energy indicating less compatibility. The scoring function f plays a very important role in this model and is described in detail in the following section.\nUsing the energy function we can now define the conditional Boltzmann distribution over document permutations by exponentiating and normalizing:\nP (R|S) = 1 Z(S)\nexp(\u2212E(R|S)) ( 6)\nZ(S) = R exp(\u2212E(R|S))(7)\nNote that we can not compute P (R|S) or Z(S) exactly since both contain sums over exponentially many document permutations. In practice, however, we will show that efficient approximations of these quantities allow inference and learning in the model.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Properties of the Model", "text": "A fundamental problem faced by all the aforementioned methods is that the IR metric depends on ranks, which are non-smooth functions of the scores, that is, they depend on a rank vector obtained by sorting the scores, a difficult operation to differentiate. A key idea in BoltzRank is that if we treat the scores produced by the model as random variables, then we can update parameters with respect to the expectation of a rank-dependent objective, utilizing sufficient statistics based on the score distribution. Other methods have taken a similar approach. For example, Soft-Rank (Taylor et al., 2008) used a ranking form of a binomial distribution to approximate the distribution of ranks obtained by sorting scores drawn from a score distribution. BoltzRank differs from these methods in terms of how it approximates the score distribution.\nBoltzRank directly estimates the probability of a particular rank vector R, based on the compatibility of each pairwise relationship in that ranking with the respective scores, combined in a simple manner in the energy E(R|S). For comparison, SoftRank instead focuses on estimating the probability that a given document has a particular rank for a query. It obtains this by first directly estimating \u03c0 ij , the probability that document i out-ranks document j, for every other document i = j. It then uses these in two different ways (described below) to compute the probability p j (r), that document j has rank r.\nHere we compare these approaches. We begin with a simple generative model to generate rank vectors from a score vector. A distribution over score vectors is formed by placing independent Gaussians centered on each document's score s j , with a common standard deviation \u03c3 s . The distribution P (R|S) over rank vectors is obtained by drawing i.i.d. samples from this score distribution and then sorting to obtain a rank vector. One way of understanding the resulting distribution over rank vectors is in terms of pairwise contests between two documents. Under this generative model, the probability that document i is ranked above document j, is the integral of the difference of two Gaussian random variables, which is a Gaussian centered on the difference in the documents' scores:\n\u03c0 ij = P (S i > S j ) = \u221e 0 N (s|s i \u2212 s j , 2\u03c3 2 s )ds (8)\nSoftRank uses this pairwise contest probability in two ways. The first is a recursive computation for the distribution of ranks of each document j, which uses the Rank-Binomial distribution, a ranking form of a binomial distribution, to estimate the probability of the various ranks under which j beats all but r of the other documents. The second is a less expensive normal approximation to the ranks, which approximates the expected rank of document j as m i=1,i =j \u03c0 ij and variance equal to m i=1,i =j \u03c0 ij (1 \u2212 \u03c0 ij ), analogous to the normal approximation to m samples of a true binomial distribution. Both of these methods specify the joint probability of a specific rank vector as the product of each document having that particular rank. In our comparison of the methods, we assume that all the methods have the correct score vector, and that SoftRank has access to the true \u03c3 s in the generator.\nWe consider a simple case, in which there are three documents to be ranked for a single query; we randomly select the scores to be between 0 and 2, and the standard deviation in the generator is 0.2. Figure 3.2 shows an example of a distribution across rankings for a random score vector, and the distributions estimated by both of SoftRank's approximations, the Normal and the Rank-Binomial, and our Boltzmann distribution.\nWe then evaluate the quality of these three approximations, by comparing each to the true empirical distribution, using the Kullback-Leibler (KL) divergence, averaged over 500 sampled score vectors, as a distance metric. In Figure 2, we see that BoltzRank provides a more accurate approximation to the true distribution than either of SoftRank's approximations.\nWe can gain some insight into this result by analyzing the approximation to the pairwise probabilities in BoltzRank. If we use a simple version of the g q () function, i.e., g q (x) = k * sign(x) we can analytically determine \u03c0 ij : 2 3) (1 3 2) (2 1 3) (2 3 1) (3 1 2) (3 2 1)  0   0 This provides a reasonable approximation to the true \u03c0 ij , as with a proper setting for k, this logistic function closely matches the cumulative for the normal distribution specified in Equation 8.\n\u03c0 ij = P (S i > S j ) = 1 1 + exp(\u2212k * (s i \u2212 s j )) (1", "publication_ref": ["b10"], "figure_ref": ["fig_1"], "table_ref": ["tab_0"]}, {"heading": "Scoring Function", "text": "The scoring function f consists of two potentials: individual potential \u03c6 and pairwise potential \u03d5. \u03c6 operates on single documents and assigns absolute scores to them without considering any relative information. \u03d5 takes as input pairs of documents and predicts the relative difference in scores of the two documents in each pair. The final score for any given document d j is then computed in the following way:\nf (d j |q, D) = \u03c6(d j ) + k,k =j \u03d5(d j , d k ) (9)\nThe pairwise potential \u03d5 allows f to effectively enforce the learned second order relative constraints during inference which, to the best of our knowledge, has not been explored in existing ranking models. Enforcing these constraints comes at the cost of increased inference time of O(m) for any given document. If \u03d5 is dropped from the model then f reduces to the standard scoring function, which allows for extremely efficient inference. In our experiments we demonstrate that adding \u03d5 significantly improves the accuracy of the ranker. But even with \u03d5 excluded, our model still achieves competitive performance. The system thus offers a trade-off between inference speed and ranking accuracy.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning and Inference", "text": "Our model produces a distribution over rank vectors based on the estimated score vector. Several loss functions could be minimized. We could perform a form of maximum likelihood learning, as is frequently performed on Boltzmann models, to match the model's estimate of the rank distribution, P (R|S), to the true rank distribution, P (R|L), as specified by the target relevance levels L.\nHere we consider an alternative loss function, in order to incorporate the relevant IR evaluation metric to be used at test time. Our objective is to produce a probability distribution over rankings, as given in Equation 7, that assigns high probability to rankings that maximize the target performance measure G. One direct way to achieve this is by maximizing the expected performance, which leads to a new learning objective:\nG|S P = R P (R|S)G(R, L)(10)\nwhere P (R|S) is given in Equation 7. The sum in Equation 10 is intractable. Therefore instead of optimizing G|S P directly, we will optimize its Monte-Carlo estimate:\nG|S (Rq) P = R\u2208Rq P (Rq) (R|S)G(R, L)(11)\nwhere R q is the rank sample set, and P (Rq) is the approximate rank probability, obtained by normalizing over this sample:\nP (Rq) (R|S) = exp(\u2212E(R|S)) R \u2032 \u2208Rq exp(\u2212E(R \u2032 |S))(12)\nThere are a number of ways to get a representative rank sample set, including various sampling techniques. We exploit our knowledge of the relevance level set L to form an informative set, and we avoid re-sampling for computational reasons, which are of crucial importance in large learning-to-rank data sets. Ideally, we want R q to contain samples that have a full range of values of the target performance measure, since this would make it most informative during learning. Given that most IR evaluation metrics place the most weight on the top ranked documents, it is not hard to design a procedure that outputs samples with this property for each query. We give an example of such a procedure in Section 4 below. It is important to note here that there is a total of m! possible ranking assignments for a document set of m documents. Thus in order to make learning feasible we can include only a small subset of the rankings in R q . Our experiments however show that even with a small subset of rankings our model is able to successfully learn and generalize to new queries.\nOnce the samples are computed for every query, we then define our target gain as summed over all queries:\nG T OT AL = n i=1 G|S qi (Rq i ) P (13\n)\nNote that Equation 13 allows us to incorporate and optimize any IR performance measure. The derivatives of this function with respect to f can easily be computed, and straightforward gradient ascent can then be used to update f . The learning entails using f to get the scores for all documents and back propagating the derivatives of the target gain to learn f . This leads to the algorithm summarized in Algorithm 1. During inference we simply use Equation 9to compute the scores for new documents and then use these scores to rank the documents.\nOptimizing NDCG at training time may not optimize test NDCG (Taylor et al., 2008); one can gain some intuition into this by considering training with G based on NDCG@1, which only aims to get the top-ranked document correct, and no learning occurs for the rest of the documents. We thus adopt an approach motivated by our probabilistic model: we combine our target performance measure with a form of maximum likelihood. We aim to minimize the KL divergence between the true rank distribution, P (R|L) and the model's predicted distribution P (R|S). This reduces to minimizing the cross entropy between the rank sample distribution under the model and the target rank sample distribution given by the relevance levels: Rq) (R|L) log(P (Rq) (R|S)) ( 14)\nC (Rq ) = \u2212 R\u2208Rq P(\nwhere P (Rq) is given in Equation 12. The combined Algorithm 1 BoltzRank Algorithm Input: {(q 1 , D 1 , L 1 ), ..., (q n , D n , L n )} Parameters: learning rate \u03b7, tolerance \u01eb initialize scoring function: f for i = 1 to n do compute samples: R qi end for repeat for i = 1 to n do compute scores:\nS qi = f (q i , D i ) calculate query gain: O qi = \u03bb G|S qi (Rq i ) P \u2212 (1 \u2212 \u03bb)C (Rq i\n) compute gradients:\u2207f = \u2202O qi /\u2202f update scoring function: f = f + \u03b7\u2207f end for calculate total gain: O until change in total gain is below \u01eb Output: learned scoring function f learning objective to maximize then becomes:\nO = \u03bb G T OT AL \u2212 (1 \u2212 \u03bb)C T OT AL(15)\nwhere C T OT AL sums C (Rq ) over all queries. The cross entropy term provides additional learning information by showing the model what the \"target\" sample probabilities should be. We tune \u03bb via cross-validation.\nIf only individual potentials are employed, the complexity of our learning algorithm is O(knm 2 max ), where k = |R q |, is the number of query samples used to approximate the expectation, n is the number of queries, and m max is the maximum number of documents per query. This complexity remains the same if pairwise potential is included, but as was discussed earlier, inference complexity increases from O(nm max ) to O(nm 2 max ). Thus if a reasonably small number of samples is used our learning algorithm will not be significantly slower than any pairwise algorithm.", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section we describe the experiments that we carried out on LETOR3.0 OHSUMED and TD2004 ) data sets. We chose these sets because they are publicly available, include several baseline results, and provide evaluation tools to ensure accurate comparison between methods. Note that the recentlyreleased LETOR3.0 extends LETOR2.0 by including significantly more features and providing three additional datasets.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data Collections", "text": "OHSUMED is a data set of medical publication abstracts. There are 106 queries, and a total of 16,140 query document pairs for which the relevance levels are provided. The relevance judgments have three possible levels: 2 = definitely relevant, 1 = possibly relevant, and 0 = not relevant.\nTD2004 is a Topic Distillation data set from TREC2004 web search track. There are 75 queries and each query is associated with 1,000 documents. Each query document pair is given binary relevant/not relevant relevance judgment. The data set is heavily dominated by uninformative irrelevant documents with only 1,116 relevant documents out of the total of 75,000. To speed up the learning we subsampled the training data by randomly removing 80 percent of the irrelevant documents for each query; the validation and test sets were unchanged. Note that subsampling of TREC data sets is a standard procedure used for example, by (Taylor at al., 2008).\nBoth data sets come with five precomputed folds, with 60/20/20 percent splits for training, validation, and test sets. All the models were trained using the training set, fine tuned using the validation set, and tested on the test set. The results shown for each model are the averages of the results for the five folds.", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "Parameter Selection", "text": "In all our experiments we chose to use one hidden layer neural networks for \u03c6 and \u03d5 potentials. In both networks we employed scaled tanh activation and output functions. In this representation the input to \u03d5 was a concatenation of features of the two documents in each pair. To investigate the usefulness of \u03d5, we trained two versions of BoltzRank. The first, denoted as BoltzRank1, utilized a scoring function consisting of only the \u03c6 potential which, through cross validation, was selected to have 5 hidden units. The second, denoted as BoltzRank2, combined both \u03c6 and \u03d5 potentials with 3 and 5 hidden units respectively. Again, the number of hidden units was found to be optimal through cross validation. 1\nIn order to train both methods, we used the same rank sample set R q , formed as follows. First, for each query in the training set, we computed a set of 100 document rank permutations. Furthermore, we ensure that each permutation set included a full range of target performance measure values by carefully selecting which permutations to include into the set. Most IR eval-  uation metrics place a lot of importance on the top ranked documents. Thus, if for example NDCG is used, then swapping documents with relevance levels 2 and 1 will less significantly affect NDCG than swapping the documents with relevance levels 2 and 0. Using this, our procedure for computing the rank sample set for NDCG was as follows. 35% of the sample were obtained by only swapping subsets of documents with relevance levels 2 and 1, 20% were obtained by swapping subsets of documents with relevance levels 1 and 0, and the rest were obtained by swapping subsets of documents with relevance levels 2 and 0 and randomly permuting the entire set. Once these permutations were computed we then used the relevance levels to sort the documents (randomly resolving ties) and stored the resulting ranks. NDCG@5 values for an example rank sample set resulting from this procedure are shown in Figure 3(a). We see that the first 55% of the sample has very high NDCG, with values mostly above 0.5, and the remainder contains samples with low NDCG values, mostly below 0.4. Such diversity of NDCG values and sample structures should provide a lot of useful information for learning to the system. Note that this procedure can be tailored readily to any set of relevance levels. In general, throughout our experiments we found that as long as the samples were balanced and contained a wide range of target performance measure the actual procedure used to compute them had little effect on the results.\nWe experimented with various settings for \u03bb in Equation 15 including 0 and 1 and found that \u03bb = 0.9 performed best. Figure 3(b) shows the behavior of NDCG for different values of \u03bb. From this figure we see that as \u03bb decreases the NDCG increases at first and then drops off. The initial increase in NDCG supports the argument of (Taylor et al., 2008) that NDCG places too much emphasis on top-ranked documents, making learning difficult. On the other hand the significant decrease in NDCG when \u03bb approaches 0 suggests that NDCG contains crucially important ranking information and thus should not be discarded.\nWe also experimented with different g functions including linear and logistic. We found that as long as g is sign preserving and bounded its exact form had little effect on the results. Thus for all experiments we used a simple linear form: g q (x) = 2x/(m q \u2212 1). For each fold we conducted ten random restarts each time retaining the model that gave best validation results. Finally, we experimented with various sample sizes ranging from 20 to 1000, and found no significant improvement by increasing the sample size be-yond 100.", "publication_ref": ["b10"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Results", "text": "Table 2 shows test set NDCG results for the OHSUMED and TD2004 data sets. We compared BoltzRank to four baseline methods: AdaRank.NDCG and AdaRank.MAP , ListNet (Cao et al., 2007), and FRank (Tsai et al.,2007) 2 ; these methods are considered the state-of-the-art in the pairwise and listwise categories described above. From the OHSUMED data set results we see that both BoltzRank methods significantly outperform other methods, especially for small truncation levels. Furthermore, BoltzRank2 consistently outperforms BoltzRank1 on all truncation levels except four. This indicates that higher order document interactions do contain helpful information for ranking. The results for the TD2004 data set further support this conclusion, as BoltzRank1 performs well while BoltzRank2 beats all the baseline methods for all truncation levels except for one.\nExperimental results using MAP as the performance metric are also shown in Table 2. We see that the results are similar to those of NDCG, as BoltzRank2 outperforms the other methods on both data sets, and BoltzRank1 is only outperformed by one baseline method on one of the data sets.", "publication_ref": ["b4", "b4"], "figure_ref": [], "table_ref": ["tab_1", "tab_1"]}, {"heading": "Conclusion", "text": "In this paper, we have proposed a new energy-based, listwise approach to learning to rank. In this approach potentials that depend on individual documents and pairs of documents are combined to define a probability distribution over possible rank assignments to a set of documents retrieved for a given query. Pairwise potentials allow us to explore document interactions beyond individual scoring functions; these have not been previously explored at test time in this domain. We are able to optimize non-continuous listwise loss functions, based on the expectation of the objective under the probability distribution given by our model. Future work includes studying the relationship between the rank samples we use to approximate the expectation, and the local optimum of the target performance measures that the algorithm finds. An important issue is how well our approximated ranking distribution will scale if there are more relevance levels. Finally, we are exploring applications of this approach to ranking in collaborative filtering.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Modern Information Retrieval", "journal": "Addison Wesley", "year": "1999", "authors": "R Baeza-Yates; B Ribeiro-Neto"}, {"ref_id": "b1", "title": "Ranking As Function Approximation", "journal": "Springer", "year": "2006", "authors": "C Burges"}, {"ref_id": "b2", "title": "Learning to rank with nonsmooth cost functions", "journal": "", "year": "2006", "authors": "C Burges; R Rango; Q V Le"}, {"ref_id": "b3", "title": "Learning to rank using gradient descent", "journal": "", "year": "2005", "authors": "C Burges; T Shaked; E Renshaw; A Lazier; M Deeds; N Hamilton; G Hullender"}, {"ref_id": "b4", "title": "Learning to rank: From pairwise approach to listwise approach", "journal": "", "year": "2007", "authors": "Z Cao; T Qin; T Y Liu; M F Tsai; H Li"}, {"ref_id": "b5", "title": "Pranking with ranking", "journal": "", "year": "2001", "authors": "K Crammer; Y Singer"}, {"ref_id": "b6", "title": "IR evaluation methods for retrieving highly relevant documents", "journal": "Proceedings of the Special Interest Group on Information Retrieval", "year": "2000", "authors": "K Jarvelin; J Kekalainen"}, {"ref_id": "b7", "title": "An efficient boosting algorithm for combining preferences", "journal": "Journal of Machine Learning Research", "year": "2003", "authors": "Y Freund; R D Iyer; R E Schapire; Y Singer"}, {"ref_id": "b8", "title": "Global Ranking Using Conditional Random Fields", "journal": "", "year": "2008", "authors": "T Qin; T Y Liu; X D Zhang; D S Wang; H Li"}, {"ref_id": "b9", "title": "Letor: Benchmark dataset for research on learning to rank for information retrieval", "journal": "Proceedings of the Special Interest Group on Information Retrieval", "year": "2007", "authors": "Y T Liu; T Qin; J Xu; Y W Xiong; H Li"}, {"ref_id": "b10", "title": "SoftRank: Optimizing non-smooth rank metrics", "journal": "", "year": "2008", "authors": "M Taylor; J Guiver; S Robertson; T Minka"}, {"ref_id": "b11", "title": "", "journal": "", "year": "", "authors": "M F Tsai; T Y Liu; T Qin; H H Chen; W "}, {"ref_id": "b12", "title": "FRank: A ranking method with delity loss", "journal": "Proceedings of the Special Interest Group on Information Retrieval", "year": "2007", "authors": " Ma"}, {"ref_id": "b13", "title": "AdaRank: A boosting algorithm for information retrieval", "journal": "Proceedings of the Special Interest Group on Information Retrieval", "year": "2007", "authors": "J Xu; H Li"}], "figures": [{"figure_label": "12", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .Figure 2 .12Figure1. The distributions across rank vectors for a particular score vector. Note that the Normal distribution indeed provides a good approximation to the computationally intensive Rank-Binomial distribution, but neither of these provide as good a fit to the true distribution as that given by our Boltzmann distribution.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 .3Figure3. (a). NDCG@5 values for a rank sample set of 100 samples. The first sample is the target score vector, with NDCG value of 1. Every other point is a rank vector obtained by swapping the scores (here, relevance levels of 0, 1, or 2) and randomly resolving ties. (b). NDCG vs. \u03bb for the OHSUMED data set.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "A Summary Of NotationVariable Description Q = {q1, ..., qn} input queries mi number of documents for qi Di = {di1, ...., dim i } documents for qi Li = {li1, ..., lim i }", "figure_data": "relevance levels for qif (qi, Di)"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Test NDCG and MAP results, the results for the OHSUMED dat set are in the first half of the table and the results for the TD2004 dat set in the second half.", "figure_data": "MethodNDCG@1 NDCG@2 NDCG@3 NDCG@4 NDCG@5 MAPBoltzRank155.4353.0351.7750.2648.7645.22BoltzRank256.8154.0851.8350.2349.1046.04AdaRank.NDCG 53.3049.2247.9046.8846.7344.98AdaRank.MAP53.8847.8946.8247.2146.1344.87FRank53.0050.0848.1246.9445.8844.39ListNet53.2648.1047.3245.6144.3244.57BoltzRank145.3340.0037.7736.1635.9122.36BoltzRank247.6741.3339.0237.5736.3523.90AdaRank.NDCG 42.6738.0036.8835.2435.1419.36AdaRank.MAP41.3339.3337.5736.8336.0221.89FRank49.3340.6738.7535.8136.2923.88ListNet36.0034.6735.7334.6933.2522.31"}], "formulas": [{"formula_id": "formula_0", "formula_text": "N DCG(R, L)@T = N q T j=1 2 rel(j) \u2212 1 log(1 + j)(1)", "formula_coordinates": [2.0, 342.96, 284.83, 198.52, 30.64]}, {"formula_id": "formula_1", "formula_text": "AP (R, L) = m j=1 P @j * rel(j) m j=1 rel(j) (2)", "formula_coordinates": [2.0, 355.92, 477.55, 185.56, 29.55]}, {"formula_id": "formula_2", "formula_text": "P @j = j i=1 rel(i) j(3)", "formula_coordinates": [2.0, 387.36, 544.15, 154.12, 30.99]}, {"formula_id": "formula_3", "formula_text": "E(R|S) = 2 m * (m \u2212 1) rj >r k g q (r j \u2212 r k )(s j \u2212 s k ) (4)", "formula_coordinates": [3.0, 60.72, 561.93, 228.76, 27.81]}, {"formula_id": "formula_4", "formula_text": "g q (x) = \u03b1 q x (5)", "formula_coordinates": [3.0, 146.28, 616.89, 143.2, 10.33]}, {"formula_id": "formula_5", "formula_text": "P (R|S) = 1 Z(S)", "formula_coordinates": [3.0, 349.56, 139.17, 83.43, 23.52]}, {"formula_id": "formula_6", "formula_text": "Z(S) = R exp(\u2212E(R|S))(7)", "formula_coordinates": [3.0, 360.24, 169.65, 181.24, 20.9]}, {"formula_id": "formula_7", "formula_text": "\u03c0 ij = P (S i > S j ) = \u221e 0 N (s|s i \u2212 s j , 2\u03c3 2 s )ds (8)", "formula_coordinates": [4.0, 70.32, 170.7, 219.15, 26.88]}, {"formula_id": "formula_8", "formula_text": "\u03c0 ij = P (S i > S j ) = 1 1 + exp(\u2212k * (s i \u2212 s j )) (1", "formula_coordinates": [4.0, 77.52, 176.0, 286.88, 544.1]}, {"formula_id": "formula_9", "formula_text": "f (d j |q, D) = \u03c6(d j ) + k,k =j \u03d5(d j , d k ) (9)", "formula_coordinates": [4.0, 348.0, 698.0, 193.48, 21.46]}, {"formula_id": "formula_10", "formula_text": "G|S P = R P (R|S)G(R, L)(10)", "formula_coordinates": [5.0, 113.64, 476.48, 175.83, 21.02]}, {"formula_id": "formula_11", "formula_text": "G|S (Rq) P = R\u2208Rq P (Rq) (R|S)G(R, L)(11)", "formula_coordinates": [5.0, 96.72, 561.29, 192.75, 23.93]}, {"formula_id": "formula_12", "formula_text": "P (Rq) (R|S) = exp(\u2212E(R|S)) R \u2032 \u2208Rq exp(\u2212E(R \u2032 |S))(12)", "formula_coordinates": [5.0, 79.56, 641.97, 209.91, 25.52]}, {"formula_id": "formula_13", "formula_text": "G T OT AL = n i=1 G|S qi (Rq i ) P (13", "formula_coordinates": [5.0, 366.6, 319.03, 170.43, 30.63]}, {"formula_id": "formula_14", "formula_text": ")", "formula_coordinates": [5.0, 537.03, 329.01, 4.45, 9.96]}, {"formula_id": "formula_15", "formula_text": "C (Rq ) = \u2212 R\u2208Rq P(", "formula_coordinates": [5.0, 319.92, 674.09, 82.17, 22.61]}, {"formula_id": "formula_16", "formula_text": "S qi = f (q i , D i ) calculate query gain: O qi = \u03bb G|S qi (Rq i ) P \u2212 (1 \u2212 \u03bb)C (Rq i", "formula_coordinates": [6.0, 85.32, 178.77, 164.66, 37.93]}, {"formula_id": "formula_17", "formula_text": "O = \u03bb G T OT AL \u2212 (1 \u2212 \u03bb)C T OT AL(15)", "formula_coordinates": [6.0, 96.24, 357.2, 193.24, 12.14]}], "doi": ""}