{"title": "Confocal Stereo", "authors": "Samuel W Hasinoff; Kiriakos N Kutulakos", "pub_date": "", "abstract": "We present confocal stereo, a new method for computing 3D shape by controlling the focus and aperture of a lens. The method is specifically designed for reconstructing scenes with high geometric complexity or fine-scale texture. To achieve this, we introduce the confocal constancy property, which states that as the lens aperture varies, the pixel intensity of a visible in-focus scene point will vary in a sceneindependent way, that can be predicted by prior radiometric lens calibration. The only requirement is that incoming radiance within the cone subtended by the largest aperture is nearly constant. First, we develop a detailed lens model that factors out the distortions in high resolution SLR cameras (12MP or more) with large-aperture lenses (e.g., f1.2). This allows us to assemble an A\u00d7F aperture-focus image (AFI) for each pixel, that collects the undistorted measurements over all A apertures and F focus settings. In the AFI representation, confocal constancy reduces to color comparisons within regions of the AFI, and leads to focus metrics that can be evaluated separately for each pixel. We propose two such metrics and present initial reconstruction results for complex scenes.", "sections": [{"heading": "Introduction", "text": "Recent years have seen many advances in the problem of reconstructing complex 3D scenes from multiple photographs [1][2][3]. Despite this progress, however, there are many common scenes for which obtaining detailed 3D models is beyond the state of the art. One such class includes scenes that contain very high levels of geometric detail, such as hair, fur, feathers, miniature flowers, etc. These scenes are difficult to reconstruct for a number of reasons-they create complex 3D arrangements not directly representable as a single surface; their images contain fine detail beyond the resolution of common video cameras; and they create complex self-occlusion relationships. As a result, many approaches either sidestep the reconstruction problem [2], require a strong prior model for the scene [4], or rely on techniques that approximate shape at a coarse level.\nDespite these difficulties, the high-resolution sensors in today's digital cameras open the possibility of imaging complex scenes at a very high level of detail. With resolutions surpassing 12Mpixels, even individual strands of hair may be one or more pixels wide (Fig. 1a,b). In this paper, we explore the possibility of reconstructing such scenes with a new method called confocal stereo, which wide aperture (f1. aims to compute depth maps at sensor resolution. The method is designed to exploit the capabilities of high-end digital SLR cameras and requires no special equipment besides the camera and a laptop. The only key requirement is the ability to actively control both the aperture and focus setting of the lens.\nAt the heart of our approach is a property we call confocal constancy, which states that as the lens aperture varies, the pixel intensity of a visible in-focus scene point will vary in a scene-independent way, that can be predicted by prior radiometric lens calibration. To exploit confocal constancy for reconstruction, we develop a detailed lens model that factors out the geometric and radiometric distortions observable in high resolution SLR cameras with large-aperture lenses (e.g., f1.2). This allows us to assemble an A \u00d7 F aperture-focus image (AFI) for each pixel, that collects the undistorted measurements over all A apertures and F focus settings (Fig 1c). In the AFI representation, confocal constancy reduces to color comparisons within regions of the AFI and leads to focus metrics that can be evaluated separately for each pixel.\nOur work is closely related to depth-from-focus methods [5][6][7][8], with the important difference that rather than defining our focus criterion over a spatial window, we consider pixels individually and manipulate a second, independent camera parameter (i.e., aperture). To our knowledge, aperture control has been considered only in the context of depth-from-defocus methods [9][10][11][12], but these methods also rely on spatial windows and, hence, are unsuitable for reconstructing scenes at the resolutions we consider. Our work is also related to recent approaches employing finite or synthetic apertures for image-based rendering [13] and for 3D reconstruction [14,15]. Unlike these methods, our approach requires only a single camera, and requires no special illumination or scene model.\nOur work has five main contributions. First, unlike existing depth-from-focus or depth-from-defocus methods, our confocal constancy formulation shows that we can assess focus without modeling a pixel's spatial neighborhood or the blurring properties of a lens. Second, we show that depth-from-focus computations can be reduced to a pixel-matching problem, in the spirit of traditional stereo techniques. Third, we develop a method for the precise geometric and radiometric alignment of images taken at multiple focus and aperture settings, particularly suited for the case where the standard thin-lens model breaks down. Fourth, we introduce the aperture-focus-image representation as a basic tool for focus-and defocus-based 3D reconstruction. Fifth, we show that together, confocal constancy and accurate image alignment lead to a reconstruction algorithm that can compute depth maps at resolutions not attainable with existing techniques.", "publication_ref": ["b0", "b1", "b2", "b1", "b3", "b4", "b5", "b6", "b7", "b8", "b9", "b10", "b11", "b12", "b13", "b14"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Confocal Constancy", "text": "Consider a camera whose lens contains multiple elements and has a range of known focus and aperture settings. We assume that no information is available about the internal components of this lens (e.g., the spacing of its elements). We therefore model the lens as a \"black box\" that redirects incoming light toward a fixed sensor plane, with the following idealized properties:\n-Negligible absorption: light that enters the lens in a given direction is either blocked from exiting or is transmitted with no absorption. -Perfect focus: for every 3D point in front of the lens there is a unique focus setting that causes rays through the point to converge to a single pixel on the sensor plane. -Aperture-focus independence: the aperture setting controls only which rays are blocked from entering the lens; it does not affect the way that light is redirected.\nThese properties are well approximated by lenses used in professional photography applications, and we use such a lens to collect images of a 3D scene for A aperture settings, {\u03b1 1 , . . . , \u03b1 A }, and F focal settings, {f 1 , . . . , f F }. This acquisition produces a 4D set of pixel data, I \u03b1f (x, y), where I \u03b1f is the image captured with aperture \u03b1 and focal setting f . Suppose that a 3D point p on an opaque surface is in perfect focus in image I \u03b1f and suppose that it projects to pixel (x, y). In this case, the light reaching the pixel is restricted to a cone from p determined by the aperture setting (Fig. 2). For a sensor with a linear response, the intensity I \u03b1f (x, y) at the pixel is proportional to the integral of outgoing radiance over the cone, i.e.,\nI \u03b1f (x, y) = \u03ba \u03c9\u2208Cxy(\u03b1,f ) L(p, \u03c9) d\u03c9 ,(1)\nwhere \u03c9 measures solid angle, L(p, \u03c9) is the radiance for rays passing through p, \u03ba is a constant that depends only on the sensor's response function [16], and C xy (\u03b1, f ) is the cone of rays that reach (x, y). In practice, the apertures on a real lens correspond to a nested sequence of cones,\nC xy (\u03b1 1 , f ) \u2282 . . . \u2282 C xy (\u03b1 A , f ),\nleading to a monotonically-increasing intensity at the pixel. Fig. 2. Generic lens model. (a) At the ideal focus setting of pixel (x, y), the lens collects outgoing radiance from a scene point p and directs it toward the pixel. The 3D position of point p is uniquely determined by pixel (x, y) and its ideal focus setting. The shaded cone of rays, Cxy(\u03b1, f ), determines the radiance reaching the pixel. This cone is a subset of the cone subtended by p and the front aperture because some rays may be blocked by internal components of the lens, or by its back aperture. (b) For non-ideal focus settings, the lens integrates outgoing radiance from a region of the scene.\nIf the outgoing radiance at the in-focus point p remains constant within the cone of the largest aperture, and if this cone does not intersect the scene elsewhere, the relation between intensity and aperture becomes especially simple. In particular, the integral of Eq. (1) disappears and the intensity for aperture \u03b1 is proportional to the solid angle subtended by the associated cone, i.e.,\nI \u03b1f (x, y) = \u03ba C xy (\u03b1, f ) L(p) ,(2)\nwhere C xy (\u03b1, f ) = Cxy(\u03b1,f ) d\u03c9. As a result, the ratio of intensities at an in-focus point for two different apertures becomes independent of the scene:\nConfocal Constancy Property\nI \u03b1f (x, y) I \u03b11f (x, y) = C xy (\u03b1, f ) C xy (\u03b1 1 , f ) def = E xy (\u03b1, f ) .(3)\nIntuitively, the constant of proportionality, E xy (\u03b1, f ), describes the relative amount of light received from an in-focus scene point for a given aperture. This constant, which we call the relative exitance of the lens, depends on lens internal design (front and back apertures, internal elements, etc.) and varies in general with aperture, focus setting, and pixel on the sensor plane. Confocal constancy is an important property for evaluating focus for four reasons. First, it holds for a very general lens model that covers the lenses commonly used with high-quality SLR cameras. Second, it requires no assumptions about the appearance of out-of-focus points. Third, it holds for scenes with general reflectance properties, provided that radiance is nearly constant over the cone subtended by the largest aperture. 1 Fourth, and most important, it can be evaluated at pixel resolution because it imposes no requirements on the spatial layout (i.e., depths) of points in the neighborhood of p.", "publication_ref": ["b15", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "The Confocal Stereo Procedure", "text": "Confocal constancy allows us to decide whether or not the point projecting to a pixel (x, y) is in focus by comparing the intensities I \u03b1f (x, y) for different values of aperture \u03b1 and focus f . This leads to the following reconstruction procedure:\n1. (Relative exitance estimation) Compute the relative exitance of the lens for the A apertures and F focus settings (Sect. 4). 2. (Image acquisition) For each of the F focus settings, capture an image of the scene for each of the A apertures. 3. (Image alignment) Warp the captured images to ensure that a scene point projects to the same pixel in all images (Sect. 5). 4. (AFI construction) Build an A \u00d7 F aperture-focus image for each pixel, that collects the pixel's measurements across all apertures and focus settings. 5. (Confocal constancy evaluation) For each pixel, process its AFI to find the focus setting that best satisfies the confocal constancy property (Sect. 6).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Relative Exitance Estimation", "text": "In order to use confocal constancy for reconstruction, we must be able to predict how changing the lens aperture affects the appearance of scene points that are in focus. Our approach is motivated by three basic observations. First, the apertures on real lenses are non-circular and the f-stop values describing them only approximate their true area (Fig. 3a,b). Second, when the aperture diameter is a relatively large fraction of the camera-to-object distance, the solid angles subtended by different 3D points in the workspace can differ significantly. 2 Third, vignetting and off-axis illumination effects cause additional variations in the light gathered from different in-focus points [17] (Fig. 3b).\nTo deal with these issues, we explicitly compute the relative exitance of the lens, E xy (\u03b1, f ), for all apertures \u03b1 and for a sparse set of focal settings f . This can be thought of as a radiometric lens calibration step that must be performed just once for each lens. In practice, this allows us to predict aperture-induced intensity changes to within the sensor's noise level (i.e., within 1-2 gray levels).\nTo compute relative exitance for a focus setting f , we place a diffuse white plane at the in-focus position and capture one image for each aperture, \u03b1 1 , . . . , \u03b1 A . We then apply Eq. (3) to each pixel (x, y) to recover E xy (\u03b1 i , f ). To obtain E xy (\u03b1 i , f ) for focus settings that span the entire workspace, we repeat the process for multiple values of f and use interpolation to compute the in-between values. Since Eq. (3) assumes that pixel intensity is a linear function of radiance, we linearize the images using the inverse of the sensor response function [16].", "publication_ref": ["b1", "b16", "b15"], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "High-Resolution Image Alignment", "text": "The intensity comparisons needed to evaluate confocal constancy are only possible if we can locate the projection of the same 3D point in multiple images taken with different settings. The main difficulty is that real lenses map in-focus 3D points onto the image plane in a non-linear fashion that cannot be predicted by ordinary perspective projection. To enable cross-image comparisons, we develop an alignment procedure that reverses these non-linearities and warps the input images to make them consistent with a reference image.\nSince our emphasis is on reconstructing scenes at the maximum possible spatial resolution, we aim to model real lenses with enough precision to ensure sub-pixel alignment accuracy. This task is especially challenging because at resolutions of 12MP or more, we begin to approach the optical and mechanical limits of the camera. In this domain, the commonly-used thin lens (i.e., magnification) model [6-8, 15, 18] is insufficient to account for observed distortions.\nDeterministic second-order radial distortion model To model geometric distortions caused by the lens optics, we use a model with F + 5 parameters for a lens with F focal settings. The model expresses deviations from an image with reference focus setting f 1 as an additive image warp consisting of two terms-a pure magnification term m f that is specific to focus setting f , and a quadratic distortion term that amplifies the magnification: 4) where k 0 , k 1 , k 2 are the quadratic distortion parameters, (x c , y c ) is the estimated image center, and r = (x, y) \u2212 (x c , y c ) is the radial displacement. Note that when the quadratic distortion parameters are zero, the model reduces to pure magnification. Also note that the quadratic distortion term depends linearly on the focus setting as well. Empirically, we have found that the model of Eq. ( 4) is necessary to obtain sub-pixel registration at high resolutions.\nw D f (x, y) = m f + m f (f \u2212 f 1 )(k 0 + k 1 r + k 2 r 2 ) \u2212 1 \u2022 (x, y) \u2212 (x c , y c ) , (\nNon-deterministic first-order distortion model We were surprised to find that significant misalignments can occur even when the camera is controlled remotely without any change in settings, and is mounted securely on an optical table (Fig. 3g). While these motions are clearly stochastic, we also observed a reproducible, aperture-dependent misalignment of about the same magnitude (Fig. 3c-f). In order to achieve sub-pixel alignment, we approximate these motions by a global 2D translation, estimated independently for every image:\nw ND \u03b1f (x, y) = t \u03b1f .(5)\nOffline geometric lens calibration We recover the full distortion model of Eqs. (4-5) in a single optimization step, using images of a calibration pattern taken over all F focus settings at the narrowest aperture, \u03b1 1 . This optimization simultaneously estimates the F + 5 parameters of the deterministic model and the 2F parameters of the non-deterministic model. To do this, we solve a nonlinear least squares problem that minimizes the squared reprojection error over a set of features detected on the calibration pattern:\nE(x c , y c , m, k, T) = (x,y) f ||w D f (x, y) + w ND \u03b11f (x, y) \u2212 \u2206 \u03b11f (x, y) || 2 , (6\n)\nwhere m and k are the vectors of magnification and quadratic parameters, respectively; T collects non-deterministic translations; and \u2206 \u03b11f (x, y) is the displacement between a feature location at focus setting f and its location at the reference focus setting, f 1 . To increase robustness, we fit the model iteratively, removing features whose reprojection error is more than 3.0 times the median.\nOnline alignment While the deterministic warp parameters need only be computed once for a given lens, we cannot apply the non-deterministic translations computed during calibration to a different sequence. Thus, for a new capture we identify (potentially different) features in the scene and redo the optimization of Eq. ( 6), with all parameters except T fixed to the values computed offline.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "Confocal Constancy Evaluation", "text": "Together, image alignment and relative exitance estimation allow us to establish a pixel-wise geometric and radiometric correspondence across all input images, i.e., for all aperture and focus settings. Given a pixel (x, y), we use this correspondence to assemble an A \u00d7 F aperture-focus image, describing the pixel's intensity variations as a function of aperture and focus (Fig. 4a):\nAperture-Focus Image (AFI)\nAFI xy (\u03b1, f ) = 1 E xy (\u03b1, f )\u00ce \u03b1f (x, y) ,(7)\nwhere\u00ce \u03b1f denotes the images after geometric image alignment. AFIs are a rich source of information about whether or not a pixel is in focus at a particular focus setting f . We make this intuition concrete by developing two functionals that measure how well a pixel's AFI conforms to the confocal constancy property at f . Since we analyze the AFI of each pixel (x, y) separately, we drop subscripts and use AFI (\u03b1, f ) to denote its AFI.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Direct Evaluation of Confocal Constancy", "text": "Confocal constancy tells us that when a pixel is in focus, its relative intensities across aperture should match the variation predicted by the relative exitance of the lens. Since Eq. ( 7) already corrects for these variations, confocal constancy at f implies constant intensity within column f of the AFI (Fig. 4b). Hence, to find the ideal focus setting we can simply find the column with minimum variance:\nf * = arg min f Var { AFI (1, f ), . . . , AFI (A, f ) } . (8\n)\nThe reason why the variance is higher at non-ideal focus settings is that defocused pixels integrate regions of the scene surrounding the true surface point (Fig. 2b), which generally contain \"texture\" in the form of varying geometric structure or surface albedo. Hence, for confocal constancy to be discriminative as a focus measure, such texture must be present in the scene.\nEvaluation by AFI Model-Fitting A disadvantage of the previous method is that most of the AFI is ignored when testing a given focus hypothesis f , since only one column participates in the calculation of Eq. (8). In reality, the 3D location of a scene point determines both the column of the AFI where confocal constancy holds as well as the degree of blur that occurs in the AFI's remaining, \"out-of-focus\" regions. 3 By taking these regions into account, we can create a focus detector with more resistance to noise and higher discriminative power.\nIn order to take into account both in-and out-of-focus regions of a pixel's AFI, we develop an idealized, parametric AFI model that generalizes confocal constancy. This model is controlled by a single parameter-the focus hypothesis f -and is fit directly to a pixel's AFI measurements. The ideal focus setting is chosen to be the hypothesis that maximizes agreement with these measurements.\nOur AFI model is based on two key observations. First, the AFI can be decomposed into a set of F disjoint equi-blur regions that are completely determined by the focus hypothesis f (Fig. 4c). Second, under mild assumptions on scene radiance, the intensity within each equi-blur region will be constant when f is the correct hypothesis. These observations suggest that we can model the AFI as a set of F constant-intensity regions whose spatial layout is determined by the focus hypothesis f . Fitting this model to a pixel's AFI leads to a focus criterion that minimizes intensity variance in every equi-blur region (Fig. 4d):\nI \u03b1f (x, y)\u00ce \u03b1f (x, y) 1 Exy(\u03b1,f )\u00ce \u03b1f (x, y) f = 3 f = 21 f = 39 (a) (b) (c) (d)\nf * = arg min f F i=1 w f i Var AFI (\u03b1, \u03c6) | (\u03b1, \u03c6) \u2208 R f i ,(9)\nwhere R f i is the i-th equi-blur region for hypothesis f , and w f i weighs the contribution of region R f i (w f i = area(R f i ) in our experiments). To implement Eq. ( 9) we must compute the equi-blur regions for a given focus hypothesis f . Suppose that the hypothesis f is correct, and suppose that the current aperture and focus of the lens are \u03b1 and f , respectively, i.e., a scene point p is in perfect focus for this setting. Now consider \"defocusing\" the lens by changing its focus to f \u2032 (Fig. 5a). We represent the blur associated with the pair (\u03b1, f \u2032 ) by a circular disc centered on point p and parallel to the sensor plane. From similar triangles, the radius of this disc is equal to\nb \u03b1f \u2032 = \u03bb 2\u03b1 |dist(f ) \u2212 dist(f \u2032 )| dist(f \u2032 ) ,(10)\nwhere \u03bb is the focal length of the lens and dist(\u2022) converts focus settings to distances from the front aperture.\nscene dist(f ) dist(f ) dist(f \u2032 ) dist(f \u2032\u2032 ) blur radius b \u03b1f \u2032 p p focus f focus f focus f \u2032 focus f \u2032\u2032 \u03bb/\u03b1 \u03bb/\u03b1 \u2032 aperture aperture sensor sensor plane plane (a) (b)\nFig. 5. (a) Quantifying the blur due to aperture \u03b1 at a non-ideal focus setting f \u2032 . The aperture's diameter can be expressed in terms of its f-stop value \u03b1 and the focal length \u03bb. (b) A second aperture-focus combination with the same blur radius. In our AFI model, (\u03b1, f \u2032 ) and (\u03b1 \u2032 , f \u2032\u2032 ) belong to the same equi-blur region.\nGiven a focus hypothesis f , Eq. (10) assigns a \"blur radius\" to each point (\u03b1, f \u2032 ) in the AFI and induces a set of nested, wedge-shaped curves of equal blur radius (Figs. 4c and 5b). We quantize the possible blur radii into F bins associated with the widest-aperture settings, i.e., (\u03b1 A , f 1 ), . . . , (\u03b1 A , f F ), which partitions the AFI into F equi-blur regions, one per bin.\nWhile Eq. (10) fully specifies our parametric AFI model, it is important to note that this model is approximate. We have implicitly assumed that once relative exitance and geometric distortion have been factored out (Sects. 4-5), defocusing is well-approximated by the thin-lens model [17]. Moreover, the intensity at two equi-blur positions in an AFI will be constant only if two conditions hold: (i) outgoing radiance remains constant within the cone of the largest aperture for all scene points contributing intensity to the pixel (i.e., the shaded region of the scene in Fig. 2b), and (ii) depth variations within this region do not significantly affect the defocus integral. In practice, we have found that this model matches the observed pixel variations quite well (Fig. 4d).", "publication_ref": ["b2", "b16"], "figure_ref": ["fig_3", "fig_3", "fig_3", "fig_3", "fig_3"], "table_ref": []}, {"heading": "Experimental Results", "text": "To test our approach, we used a Canon EOS-1Ds digital SLR camera with a wide-aperture, fixed focal length lens (Canon EF85mm 1.2L). The lens aperture was under computer control and its focal setting was adjusted manually using a printed ruler on the body of the lens. We operated the camera at its highest resolution, capturing 4604 \u00d7 2704-pixel images in RAW 12-bit mode. Each image was demosaiced using Canon software and linearized using the algorithm in [16]. We used A = 13 apertures ranging from f1.2 to f16, and F = 61 focal settings spanning a workspace that was 17cm in depth and 1.2m away from the camera. Successive focal settings therefore corresponded to a depth difference of approximately 2.8mm. We mounted the camera on an optical table in order to allow precise ground-truth measurements and to minimize external vibrations.\nTo enable the construction of aperture-focus images, we first computed the relative exitance of the lens (Sect. 4) and then performed offline geometric calibration (Sect. 5). Our geometric distortion model was able to align the calibration images with an accuracy of approximately 0.15 pixels, estimated from \"hair\" dataset \"plastic\" dataset \"box\" dataset Fig. 6. Top: Behavior of focus criteria for a specific pixel (highlighted square) in three test datasets. The dotted graph is for 3\u00d73 variance (DFF), dashed is for direct confocal constancy (Eq. ( 8)) and the solid graph is for AFI model-fitting (Eq. ( 9)). While all three criteria often have corresponding local minima near the ideal focus setting, AFI model-fitting varies much more smoothly and exhibits no spurious local minima in these examples. For the middle example, which considers the same pixel shown in Fig. 1, the global minimum for variance is at an incorrect focus setting. This is because the pixel lies on a strand of hair only 1-2 pixels wide, beyond the resolving power of variance calculations. Bottom: AFI model fitting error and inlier fraction as a function of A (\"box\" dataset, inlier threshold = 11mm). centroids of dot features (Fig. 3e). The accuracy of online alignment was about 0.5 pixels, i.e., higher than during offline calibration but well below one pixel. This penalty is expected since far fewer features are used for online alignment.\nQuantitative evaluation: \"Box\" dataset To quantify reconstruction accuracy, we used a tilted planar scene consisting of a box wrapped in newsprint (Fig. 6). The plane of the box was measured with a FaroArm Gold 3D touch probe whose single-point accuracy was \u00b10.05mm in the camera's workspace. To relate probe coordinates to coordinates in the camera's reference frame we used the Matlab Camera Calibration Toolbox along with further correspondences between image features and 3D coordinates measured by the probe. A similar procedure was used to estimate the mapping between focal settings and the depth of in-focus points, i.e., the dist(\u2022) function in Eq. (10).\nWe computed a depth map of the scene for three focus criteria: direct confocal constancy (Eq. ( 8)), AFI model-fitting (Eq. ( 9)), and a depth-from-focus (DFF) method, applied to the widest-aperture images, that chooses the focus setting with the highest variance in a 3 \u00d7 3 window centered at each pixel. The planar shape of the scene and its detailed texture can be thought of as a best-case scenario for such window-based approaches. The plane's footprint contained 2.8 million pixels, yielding an equal number of 3D measurements. As Table 1 shows, all three methods performed quite well, with accuracies of 0.37-0.45% of the object-to-camera distance. This performance is on par with previous quantitative studies (e.g., [12]) although few results with real images have been reported in the passive depth-from-focus literature. Significantly, AFI model-fitting slightly outperforms spatial variance (DFF) in both accuracy and number of outliers even though its focus computations are performed entirely at the pixel level and, hence, are of much higher resolution. Qualitatively, this behavior is confirmed by considering all three criteria for specific pixels (Fig. 6, top).\nAs a final experiment with this dataset, we investigated how AFI model fitting degrades when a reduced number of apertures is used (i.e., for AFIs of size A \u2032 \u00d7 F with A \u2032 < A). Our results suggest that reducing the apertures to five or six causes little reduction in reconstruction quality (Fig. 6, bottom).\n\"Hair\" dataset Our second test scene was a wig with a messy hairstyle, approximately 25cm tall, surrounded by several artificial plants (Figs. 1 and 6). 4 Reconstruction results for this scene (Fig. 7) show that our confocal constancy criteria lead to very detailed depth maps, at the resolution of individual strands of hair, despite the scene's complex geometry and despite the fact that depths can vary greatly within small image neighborhoods (e.g., toward the silhouette of the hair). By comparison, the 3\u00d73 variance operator produces uniformly-lower resolution results, and generates smooth \"halos\" around narrow geometric structures like individual strands of hair. In many cases, these \"halos\" are larger than the width of the spatial operator, as blurring causes distant points to influence the results.\nIn low-texture regions, such as the cloth flower petals and leaves, fitting a model to the entire AFI allows us to exploit defocused texture from nearby scene points. Window-based methods like variance, however, generally yield even better results in such regions, because they propagate focus information from nearby texture more directly. Like all focus measures, those based on confocal constancy are uninformative in extremely untextured regions, i.e., when the AFI is constant. Such pixels may be detected using a \"confidence\" measure (e.g., assessing the steepness of the minimum) or by processing the AFI further. 4 For additional results, see http://www.cs.toronto.edu/\u223chasinoff/confocal. ", "publication_ref": ["b15", "b11", "b3", "b3"], "figure_ref": ["fig_0", "fig_2", "fig_0"], "table_ref": ["tab_0"]}, {"heading": "Concluding Remarks", "text": "The extreme locality of shape computations derived from aperture-focus images is both a key advantage and a major limitation of the current approach. While we have shown that processing a pixel's AFI leads to highly detailed reconstructions, this locality does not yet provide the means to handle large untextured regions or to reason about global scene geometry and occlusion [15,18,19]. To handle low texture, we are exploring the possibility of analyzing AFIs at multiple levels of detail and for multiple pixels simultaneously. We are also investigating a spacesweep approach to analyze occlusions, analogous to voxel-based stereo.", "publication_ref": ["b14", "b17", "b18"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "High-quality video view interpolation using a layered representation", "journal": "SIGGRAPH", "year": "2004", "authors": "C L Zitnick; S B Kang; M Uyttendaele; S Winder; R Szeliski"}, {"ref_id": "b1", "title": "Image-based rendering using imagebased priors", "journal": "IJCV", "year": "2005", "authors": "A Fitzgibbon; Y Wexler; A Zisserman"}, {"ref_id": "b2", "title": "Example-based photometric stereo: Shape reconstruction with general, varying BRDFs", "journal": "PAMI", "year": "2005", "authors": "A Hertzmann; S M Seitz"}, {"ref_id": "b3", "title": "Modeling hair from multiple views", "journal": "SIGGRAPH", "year": "2005", "authors": "Y Wei; E Ofek; L Quan; H Y Shum"}, {"ref_id": "b4", "title": "Pyramid based depth from focus", "journal": "CVPR", "year": "1988", "authors": "T Darrell; K Wohn"}, {"ref_id": "b5", "title": "Real-time focus range sensor", "journal": "PAMI", "year": "1996", "authors": "S Nayar; M Watanabe; M Noguchi"}, {"ref_id": "b6", "title": "Learning shape from defocus", "journal": "ECCV", "year": "2002", "authors": "P Favaro; S Soatto"}, {"ref_id": "b7", "title": "3D shape from anisotropic diffusion", "journal": "CVPR", "year": "2003", "authors": "P Favaro; S Osher; S Soatto; L A Vese"}, {"ref_id": "b8", "title": "A new sense for depth of field", "journal": "PAMI", "year": "1987", "authors": "A P Pentland"}, {"ref_id": "b9", "title": "Depth from defocus: A spatial domain approach", "journal": "IJCV", "year": "1994", "authors": "M Subbarao; G Surya"}, {"ref_id": "b10", "title": "Range estimation by optical differentiation", "journal": "A", "year": "1998", "authors": "H Farid; E P Simoncelli"}, {"ref_id": "b11", "title": "Rational filters for passive depth from defocus", "journal": "IJCV", "year": "1998", "authors": "M Watanabe; S K Nayar"}, {"ref_id": "b12", "title": "Dynamically reparameterized light fields. In: SIGGRAPH", "journal": "", "year": "2000", "authors": "A Isaksen; L Mcmillan; S J Gortler"}, {"ref_id": "b13", "title": "Synthetic aperture confocal imaging", "journal": "SIGGRAPH", "year": "2004", "authors": "M Levoy; B Chen; V Vaish; M Horowitz; I Mcdowall; M T Bolas"}, {"ref_id": "b14", "title": "Seeing beyond occlusions (and other marvels of a finite lens aperture)", "journal": "CVPR", "year": "2003", "authors": "P Favaro; S Soatto"}, {"ref_id": "b15", "title": "Recovering high dynamic range radiance maps from photographs", "journal": "", "year": "1997", "authors": "P Debevec; J Malik"}, {"ref_id": "b16", "title": "Modern Optical Engineering", "journal": "McGraw-Hill", "year": "2000", "authors": "W J Smith"}, {"ref_id": "b17", "title": "Seeing behind the scene: Analysis of photometric properties of occluding edges by the reversed projection blurring model", "journal": "PAMI", "year": "1998", "authors": "N Asada; H Fujiwara; T Matsuyama"}, {"ref_id": "b18", "title": "Depth from defocus vs. stereo: How different really are they?", "journal": "IJCV", "year": "2000", "authors": "Y Y Schechner; N Kiryati"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig. 1 .1Fig. 1. (a) Wide-aperture image of a complex scene. (b) Left: Successive close-ups of a region in (a), showing a single in-focus strand of hair. Right: Narrow-aperture image of the same region, with everything in focus. Confocal constancy tells us that the intensity of in-focus pixels (e.g., on the strand) changes predictably between these two views. (c) The aperture-focus image (AFI) of a pixel near the middle of the strand. A column of the AFI collects the intensities of that pixel as the aperture varies with focus fixed.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Fig. 3 .3Fig. 3. (a) Images of an SLR lens showing variation in aperture shape with corresponding images of a diffuse plane. (b) Top: comparison of relative exitances for the central pixel indicated in (a), as measured using Eq. (3) (solid graph), and as approximated using the f-stop values (dotted) according to Exy(\u03b1, f ) = \u03b1 2 1 /\u03b1 2 [16]. Bottom: comparison of the central pixel (solid) with the corner pixel (dotted) indicated in (a). The agreement is good for narrow apertures (i.e., high f-stop values), but for wider apertures, spatially-varying effects are significant. (c-g) To evaluate non-deterministic lens distortions, we computed centroids of dot features for images of a static calibration pattern. (c-f) Successive close-ups of a centroid's trajectory for three cycles (red, green, blue) of the 23 aperture settings. In (c-d) the trajectories are magnified by a factor of 100. As shown in (f), the trajectory, while stochastic, correlates with aperture setting. (g) Trajectory for the centroid of (e) over 50 images with the same lens settings.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Fig. 4 .4Fig. 4. (a) The A\u00d7F measurements for the pixel shown in Fig. 1. Left: prior to image alignment. Middle: after image alignment. Right: after accounting for relative exitance (Eq. (7)). Note that the AFI's smooth structure is discernible only after both corrections. (b) Direct evaluation of confocal constancy for three focus hypotheses. (c) Boundaries of the equi-blur regions, superimposed over the AFI (for readability, only a third are shown). (d) Results of AFI model fitting, with constant intensity in each equi-blur region, from the mean of the corresponding region in the AFI. Observe that for f = 39 the model is in good agreement with the measured AFI ((a), rightmost).", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Ground-truth accuracy results. The inlier threshold was set to 11mm. All distances were measured relative to the ground-truth plane. median ABS inlier RMS % RMS % dist. dist. (mm) dist. (mm) inliers to camera Fig.7. Center: Depth map for the \"hair\" dataset using AFI model fitting. Top: Several distinctive foreground strands of hair are resolved in the AFI-based depth map. Direct evaluation of confocal constancy is also sharp but much noisier, making structure difficult to discern. By contrast, 3 \u00d7 3 variance (DFF) exhibits thick \"halo\" artifacts and fails to detect most of the foreground strands (see also Fig.6, top). Bottom right: DFF yields smoother and more accurate depths for the low-texture leaves. Bottom left: Unlike DFF, AFI model fitting resolves structure amid significant depth discontinuities.", "figure_data": "confocal constancy evaluation3.184.61660.454AFI model fitting2.133.78840.3733 \u00d7 3 spatial variance (DFF)2.163.79800.374"}], "formulas": [{"formula_id": "formula_0", "formula_text": "I \u03b1f (x, y) = \u03ba \u03c9\u2208Cxy(\u03b1,f ) L(p, \u03c9) d\u03c9 ,(1)", "formula_coordinates": [3.0, 224.52, 529.71, 256.12, 18.33]}, {"formula_id": "formula_1", "formula_text": "C xy (\u03b1 1 , f ) \u2282 . . . \u2282 C xy (\u03b1 A , f ),", "formula_coordinates": [3.0, 345.36, 593.55, 135.25, 10.65]}, {"formula_id": "formula_2", "formula_text": "I \u03b1f (x, y) = \u03ba C xy (\u03b1, f ) L(p) ,(2)", "formula_coordinates": [4.0, 232.92, 326.55, 247.72, 10.33]}, {"formula_id": "formula_3", "formula_text": "I \u03b1f (x, y) I \u03b11f (x, y) = C xy (\u03b1, f ) C xy (\u03b1 1 , f ) def = E xy (\u03b1, f ) .(3)", "formula_coordinates": [4.0, 212.64, 398.91, 251.08, 24.21]}, {"formula_id": "formula_4", "formula_text": "w D f (x, y) = m f + m f (f \u2212 f 1 )(k 0 + k 1 r + k 2 r 2 ) \u2212 1 \u2022 (x, y) \u2212 (x c , y c ) , (", "formula_coordinates": [7.0, 142.2, 108.71, 329.93, 13.13]}, {"formula_id": "formula_5", "formula_text": "w ND \u03b1f (x, y) = t \u03b1f .(5)", "formula_coordinates": [7.0, 268.68, 296.63, 211.96, 13.25]}, {"formula_id": "formula_6", "formula_text": "E(x c , y c , m, k, T) = (x,y) f ||w D f (x, y) + w ND \u03b11f (x, y) \u2212 \u2206 \u03b11f (x, y) || 2 , (6", "formula_coordinates": [7.0, 144.72, 408.35, 331.66, 13.57]}, {"formula_id": "formula_7", "formula_text": ")", "formula_coordinates": [7.0, 476.38, 409.71, 4.25, 9.96]}, {"formula_id": "formula_8", "formula_text": "AFI xy (\u03b1, f ) = 1 E xy (\u03b1, f )\u00ce \u03b1f (x, y) ,(7)", "formula_coordinates": [8.0, 227.28, 126.03, 236.44, 24.02]}, {"formula_id": "formula_9", "formula_text": "f * = arg min f Var { AFI (1, f ), . . . , AFI (A, f ) } . (8", "formula_coordinates": [8.0, 199.56, 316.16, 276.82, 17.0]}, {"formula_id": "formula_10", "formula_text": ")", "formula_coordinates": [8.0, 476.38, 317.67, 4.25, 9.96]}, {"formula_id": "formula_11", "formula_text": "I \u03b1f (x, y)\u00ce \u03b1f (x, y) 1 Exy(\u03b1,f )\u00ce \u03b1f (x, y) f = 3 f = 21 f = 39 (a) (b) (c) (d)", "formula_coordinates": [9.0, 153.15, 63.51, 295.51, 140.32]}, {"formula_id": "formula_12", "formula_text": "f * = arg min f F i=1 w f i Var AFI (\u03b1, \u03c6) | (\u03b1, \u03c6) \u2208 R f i ,(9)", "formula_coordinates": [9.0, 179.64, 405.01, 301.0, 30.63]}, {"formula_id": "formula_13", "formula_text": "b \u03b1f \u2032 = \u03bb 2\u03b1 |dist(f ) \u2212 dist(f \u2032 )| dist(f \u2032 ) ,(10)", "formula_coordinates": [9.0, 239.52, 560.24, 241.12, 24.55]}, {"formula_id": "formula_14", "formula_text": "scene dist(f ) dist(f ) dist(f \u2032 ) dist(f \u2032\u2032 ) blur radius b \u03b1f \u2032 p p focus f focus f focus f \u2032 focus f \u2032\u2032 \u03bb/\u03b1 \u03bb/\u03b1 \u2032 aperture aperture sensor sensor plane plane (a) (b)", "formula_coordinates": [10.0, 141.67, 62.39, 334.2, 76.14]}], "doi": ""}