{"title": "Generating Semantic Annotations for Frequent Patterns with Context Analysis", "authors": "Qiaozhu Mei; Dong Xin; Hong Cheng; Jiawei Han; Chengxiang Zhai", "pub_date": "", "abstract": "As a fundamental data mining task, frequent pattern mining has widespread applications in many different domains. Research in frequent pattern mining has so far mostly focused on developing efficient algorithms to discover various kinds of frequent patterns, but little attention has been paid to the important next step -interpreting the discovered frequent patterns. Although some recent work has studied the compression and summarization of frequent patterns, the proposed techniques can only annotate a frequent pattern with non-semantical information (e.g. support), which provides only limited help for a user to understand the patterns. In this paper, we propose the novel problem of generating semantic annotations for frequent patterns. The goal is to annotate a frequent pattern with in-depth, concise, and structured information that can better indicate the hidden meanings of the pattern. We propose a general approach to generate such an annotation for a frequent pattern by constructing its context model, selecting informative context indicators, and extracting representative transactions and semantically similar patterns. This general approach has potentially many applications such as generating a dictionarylike description for a pattern, finding synonym patterns, discovering semantic relations, and summarizing semantic classes of a set of frequent patterns. Experiments on different datasets show that our approach is effective in generating semantic pattern annotations.", "sections": [{"heading": "INTRODUCTION", "text": "With its broad applications such as association rule mining [2], correlation analysis [4], classification [6], and clustering [19], discovering frequent patterns from large databases has been a central research topic in data mining for years.\nVarious techniques have been developed for mining frequent item sets [2,8], sequential patterns [3], graph patterns [22], etc. These techniques can usually output a large, complete set of frequent patterns efficiently, and provide basic statistic information such as support for each pattern. However, the excessive volume of the output pattern set and the lack of context information has made it difficult to interpret and explore the patterns. In most cases, a user only wants to explore a small set of most interesting patterns, and before exploring them, to have a rough idea about their hidden meanings or why they are interesting. This is analogous to literature reviewing. Before deciding whether to read through a paper, a reader often wants to first look at a short summary of the main ideas of the paper. Similarly, it is also highly desirable to have such a summary for a frequent pattern to explain or indicate the potential meanings of the pattern and to help a user decide whether and how to explore the pattern. Therefore, a new major challenge in frequent pattern mining has been raised by researchers, which is how to present and interpret the patterns discovered, in order to support the exploration and analysis of individual patterns. To meet this challenge and facilitate pattern interpretation, we need to annotate each frequent pattern with semantically enriched, in-depth descriptions of the pattern and its associated context.\nResearchers have employed concepts like closed frequent pattern [15], and maximum frequent pattern [16] to shrink the size of output patterns and provide more information beyond \"support\". Recently, novel methods have been proposed either to mine a compressed set of frequent patterns [20] or to summarize a large set of patterns with the most representative ones [21]. Both of them employ extra information of frequent patterns beyond the simple information of support, which is either transaction coverage [20] or pattern profiles [21]. These methods can successfully reduce the number of output patterns and present only the most interesting ones to the user. However, the information that these methods use to annotate a frequent patten is restricted to the morphological information or simple statistics (e.g. support, transaction coverage, profile); from such an annotation, users could not infer the semantics, or hidden meanings of the pattern, thus still have to look through all the data transactions in which a pattern occurs in order to figure out whether it is worth exploring.\nIn this paper, we study the problem of automatically generating semantic annotations for frequent patterns, by which we mean to extract and provide concise and in-depth information for a frequent pattern, which indicates the semantics, or hidden meanings, of the pattern.\nWhat is an appropriate semantic annotation for a frequent pattern? Generally, the hidden meaning of a pattern can be inferred from the patterns with similar meanings, the data objects co-occurring with it, and the transactions in which the pattern appears. In principle, we expect such an annotation to be compact, well structured, and indicative of the meanings of the pattern. This criterion is analogous to dictionary entries, which annotate each term with structured semantic information. Example 1: An example of a dictionary entry 1 Dictionary Term: \"pattern\" [\"paet@n], noun, ... definitions:\n1) a form or model proposed for imitation 2) a natural or chance configuration 3) ... example sentences:\n1)... a dressmaker's pattern... 2)... the pattern of events ... synonym or thesaurus: model, archetype, design, exemplar, motif, etc\nIn Example 1, we see that in a typical dictionary entry, the annotation for a term is structured as follows. First, some basic non-semantic information is presented. Second, a group of definitions are given, which suggests the semantics of the term, followed by several example sentences, which show the usage of this term in context. Besides, a set of synonyms, thesaurus or semantically similar terms are presented, which have similar definitions with this term. Analogically, if we can extract similar types of semantic information for a frequent pattern and provide such structured annotations to a user, it will be very helpful for him/her to interpret the meanings of the pattern and further explore it. Given a frequent pattern, it is trivial to generate non-semantic information such as basic statistics and morphological information, so the main challenge is to generate the semantic descriptions of a pattern, which is the goal of our work. First, we should ideally provide precise semantic definitions for a pattern like those in a dictionary. Unfortunately, this is not practical without expertise of the domain. Thus we opt to look for information that can indicate the semantics of a frequent pattern, which presumably can help a user infer the precise semantics. Our idea is inspired from natural language processing, where the semantics of a word can be inferred from its context, and words sharing similar contexts tend to be semantically similar [13]. Specifically, by defining and analyzing the context of a pattern, we can find strong context indicators and use them to represent the meanings of a pattern. Second, we also want to extract the data transactions that best represent the meanings of the pattern, which is analogical to the example sentences. Finally, semantically similar patterns (SSPs) of the given pattern, i.e., patterns with similar contexts as the original pattern, can be extracted and presented. This is similar to the synonyms or thesauri of a term in dictionary. Therefore, an example of semantic pattern annotation (SPA) can be shown as follows: Example 2: An example of annotating a frequent pattern Pattern: \"frequent pattern\" sequential pattern; support = 0.1%; closed context indicators: \"mining\", \"constraint\", \"Apriori\", \"FP-growth\" \"rakesh agrawal\", \"jiawei han\", ... example transactions:\n1)mining frequent patterns without candidate... 2)... mining closed frequent graph patterns semantically similar patterns:\n\"frequent sequential pattern\", \"graph pattern\" \"maximum pattern\", \"frequent close pattern\", ... The term \"frequent pattern\" in this example is itself a frequent itemset, or a frequent sequential pattern in text. This dictionary-like annotation provides semantic information related to \"frequent pattern\", consisting of its strongest context indicators, the most representative data transactions, and the most semantically similar patterns. Despite its importance, to the best of our knowledge, the semantic annotation of frequent patterns has not been well addressed in existing work. In this work, we define the novel problem of generating semantic annotations for frequent patterns. We propose a general approach to automatically generate structured annotations as shown in Example 2, by: 1) formally defining and modeling the context of a pattern; 2) weighting context indicators based on their strength to indicate pattern semantics; and 3) ranking transactions and semantically similar patterns based on context similarity analysis. Empirical experiments on three different datasets show that our algorithm is effective for generating semantic pattern annotations and can be applied to various real world tasks.\nThe semantic annotations generated by our algorithm have potentially many other applications, such as ranking patterns, categorizing and clustering patterns with semantics, and summarizing databases. Applications of the proposed pattern context model and semantical analysis method are also not limited to pattern annotation; other example applications include pattern compression, transaction clustering, pattern relations discovery, and pattern synonym discovery.\nThe rest of the paper is organized as follows. In Section 2, we formally define the problem of semantic pattern annotation and a series of its associated problems. In Section 3, we introduce how the pattern context is modeled and instantiated. Pattern semantic analysis and annotation generation is presented in Section 4. We discuss our experiments and results in Section 5, the related work in Section 6, and our conclusions in Section 7, respectively.", "publication_ref": ["b1", "b3", "b5", "b18", "b1", "b7", "b2", "b21", "b14", "b15", "b19", "b20", "b19", "b20", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "PROBLEM FORMULATION", "text": "In this section, we formally define the problem of semantic pattern annotation (SPA).\nLet D = {t1, t2, ..., tn} be a database containing a set of transactions ti, which can be itemsets, sequences, or graphs, etc. Let p\u03b1 be a pattern (e.g., an itemset, a subsequence, or a subgraph) in D and PD = {p1, ..., p l } be the set of all such patterns. We denote the set of transactions in which p\u03b1 appears as D\u03b1 = {ti|p\u03b1 \u2208 ti, ti \u2208 D}.\nDefinition 1 (Frequent Pattern):\nA pattern p\u03b1 is fre- quent in a dataset D, if |D\u03b1| |D| \u2265 \u03c3,\nwhere \u03c3 is a user-specified threshold and |D\u03b1| |D| is called the support of p\u03b1, usually denoted as s(\u03b1).\nDefinition 2 (Context Unit): Given a dataset D and the set of frequent patterns PD, a context unit is a basic object in D which carries semantic information and co-occurs with at least one p\u03b1 \u2208 PD in at least one transaction ti \u2208 D.\nThe set of all such context units satisfying this definition is denoted as UD.\nWith this general definition, a context unit can be an item, a pattern, or a transaction in practice, depending on the specific task and data.\nDefinition 3 (Pattern Context): Given a dataset D and a frequent pattern p\u03b1 \u2208 PD, the context of p\u03b1, denoted as c(\u03b1), is represented by a selected set of context units U\u03b1 \u2286 UD such that every u \u2208 U\u03b1 co-occurs with p\u03b1. Each selected context unit u is also called a context indicator of p\u03b1, associated with a strength weight w(u, \u03b1), which measures how well it indicates the semantics of p\u03b1.\nThe following is an example of the context for an itemset pattern in a small dataset with only two transactions. The possible context units for this dataset are single items, itemsets and transactions, and the context indicators of the itemset pattern are selected from the context units appearing with it in the same transactions. diaper, baby carriage, {milk, baby lotion}, t1, ... With the definitions above, we now define the concept of semantic annotation for a frequent pattern and the related 3 subproblems.\nDefinition 4 (Semantic Annotation): Let p\u03b1 be a frequent pattern in a dataset D, U\u03b1 be the set of context indicators of p\u03b1, and P be a set of patterns in D. A semantic annotation of p\u03b1 consists of: 1) a set of context indicators of p\u03b1, I\u03b1 \u2286 U\u03b1, s.t. \u2200u \u2208 I\u03b1 and \u2200u \u2208 U\u03b1 \u2212 I\u03b1, w(u , \u03b1) \u2264 w(u, \u03b1); 2) a set of transactions T\u03b1 \u2286 D\u03b1, s.t.\u2200t \u2208 T\u03b1 and \u2200t \u2208 D\u03b1 \u2212 T\u03b1, t is more similar to c(\u03b1) than t under some similarity measure; and 3) a set of patterns P \u2286 P s.t. \u2200p \u2208 P and \u2200p \u2208 P \u2212 P , c(p) is closer to c(\u03b1) than c(p ).\nDefinition 5 (Context Modeling): Given a dataset D and a set of possible context units U , the problem of Context Modeling is to select a subset of context units U , define a strength measure w(\u2022, \u03b1) for context indicators, and construct a model of c(\u03b1) for each given pattern p\u03b1.\nDefinition 6 (Transaction Extraction): Given a dataset D, the problem of Transaction Extraction is to define a similarity measure sim(\u2022, c(\u2022)) between a transaction and a pattern context, and to extract a set of k transactions T\u03b1 \u2286 D\u03b1 for frequent pattern p\u03b1, s.t.\u2200t \u2208 T\u03b1 and \u2200t \u2208 D\u03b1 \u2212 T\u03b1, sim(t , c(\u03b1)) \u2264 sim(t, c(\u03b1)).\nDefinition 7 (Semantically Similar Pattern (SSP) Extraction): Given a dataset D and a set of candidate patterns Pc, the problem of Semantically Similar Pattern (SSP) Extraction is to define a similarity measure sim(c(\u2022), c(\u2022)) between the contexts of two patterns, and to extract a set of k patterns P \u2286 Pc for any frequent pattern p\u03b1, s.t. \u2200p \u2208 P and \u2200p \u2208 Pc \u2212 P , sim(c(p ), c(\u03b1)) \u2264 sim(c(p), c(\u03b1)), where c(\u03b1) is the context of p\u03b1.\nWith the definitions above, we may define the task of Semantic Pattern Annotation (SPA) as to: 1) select context units and design a strength weight for each unit to model the contexts of frequent patterns; 2) design similarity measures for the contexts of two patterns, and for a transaction and a pattern context; 3) for a given frequent pattern, extract the most significant context indicators, representative transactions and semantically similar patterns to construct a structured annotation.\nThis problem is challenging in various aspects. First, we do not have prior knowledge on how to model the context of a pattern or select context units when the complete set of possible context units is huge. Second, it is not immediately clear how to analyze pattern semantics, thus the design of the strength weighting function and similarity measure is nontrivial. Finally, since no training data is available, the annotation must be generated in a completely unsupervised way. These challenges, however, also indicate a great advantage of the SPA techniques we will propose -they do not depend on any domain knowledge about the dataset or the patterns.\nIn the following two sections, we present our approaches for modeling the context of a frequent pattern and annotating patterns through semantic context analysis.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MODELING PATTERN CONTEXTS", "text": "In this section, we discuss how to model pattern contexts through selecting appropriate context units and defining appropriate strength weights. Given a dataset D and a set of frequent patterns PD, our goal is to select a set of context units which carry semantic information and can discriminate the meanings of the frequent patterns. The discriminating power of each context unit will be captured by its strength weights.\nVector Space Model (VSM) [17] is commonly used in natural language processing and information retrieval to model the content of a text. For example, in information retrieval, a document and a query are both represented as term vectors, where each term is a basic concept (i.e., word, phrase), and each element of the vector corresponds to a term weight reflecting the importance of the term. The similarity between documents and queries can thus be measured by the distance between the two vectors in the vector space. For the purpose of semantic modeling, we represent a transaction and the context of a frequent pattern both as vectors of context units. We select VSM because it makes no assumption on the vector dimensions and gives the most flexibility to the selection of dimensions and weights. Formally, the context of a frequent pattern is modeled as follows.\nContext Modeling: Given a dataset D, a selected set of context units {u1, ..., um}, we represent the context c(\u03b1) of a frequent pattern p\u03b1 as a vector w1, w2, ..., wm , where wi = w(ui, \u03b1) and w(\u2022, \u03b1) is a weighting function. A transaction t is represented as a vector v1, v2, ..., vm , where vi = 1 iff ui \u2208 t, otherwise vi = 0.\nThe two key issues in a VSM are to select the vector dimensions and to assign weights for each dimension [17]. Specifically, the effectiveness of context modeling is highly dependent on how to select context units and design the strength weights. Actually, due to the generality of VSM, the proposed vector-space pattern context model is quite general and covers different strategies for context unit selection and weighing functions. In the following subsections, we first discuss the generality of the context model, and then discuss specific solutions for the two issues respectively.", "publication_ref": ["b16", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "The Generality of Context Modeling", "text": "Some existing work has explored non-morphological information of frequent patterns with some concepts related to the \"pattern context\" defined above. We now show that the notion of \"pattern context\" is more general and can cover those concepts as special cases.\nIn [21], Yan et al. introduced the profile of an itemset for summarizing itemset patterns, which is represented as a Bernoulli Distribution Vector. In fact, this \"profile\" of a frequent itemset \u03b1 can be written as a vector w(o1, \u03b1), w(o2, \u03b1), ..., w(o d , \u03b1) over all the single items {oi} in D. Here\nw(oi, \u03b1) = t j \u2208D\u03b1 t i j |D\u03b1|\n, where t i j = 1 if oi \u2208 tj and 0 otherwise. This shows that this \"profile\" is actually a special instance of the context model as we defined, where single items are selected as context units.\nXin and others proposed a distance measure for mining compressed frequent-pattern sets, which is computed based on the transaction coverage of two patterns [20]. Interestingly, the \"transaction coverage\" is also a specific instance of \"pattern context\". Given a frequent pattern p\u03b1, the transaction coverage of p\u03b1 can be written as a vector w(t1, \u03b1), w(t2, \u03b1), ..., w(t k , \u03b1) over all the transactions {ti} in D, where each transaction is selected as a context unit, and w(ti, \u03b1) = 1 if p\u03b1 \u2208 ti and 0 otherwise.\nCovering the concepts in existing work as specific instances, the pattern context model we proposed is general and has quite a few benefits. First, it does not assume pattern types. The pattern profile proposed in [21] assumes that both transactions and patterns are itemsets, thus does not work for other patterns such as sequential patterns and graph patterns. Second, the pattern context modeling allows different granularity of context units and different weighting strategies. In many cases, single items are not informative in terms of carrying semantic information (e.g., single nucleotides in DNA sequences), and the semantic information carried by a full transaction is too complex and noisy (e.g., a text document). The context modeling we introduced bridges this gap by allowing various granularity of semantic units, and allows the user to explore the pattern semantics at the level that corresponds to their beliefs. Furthermore, this model is adaptive to different strength weighting strategies for context units, where the user's prior knowledge about the dataset and patterns can be easily plugged in.", "publication_ref": ["b20", "b19", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Context Unit Selection", "text": "With the general definition presented in Section 2, the selection of context units is quite flexible. In principle, any object in the database that carries semantic information or serves to discriminate patterns semantically can be a context unit, thus context units can be single items, transactions, patterns, or any group of items/patterns, depending on the characteristics of the task and data.\nWithout losing generality, in our work we assume a pattern is the minimal units which carries semantic information in a dataset, and thus select the context units as patterns. All kinds of units can be considered as patterns with a specific granularity. For example, in a sequence database, every single item can be viewed as a sequential pattern of length 1, and every transaction can be viewed as a sequential pattern which is identical to the transactional sequence. The choosing of patterns as context units is task dependent, and can usually be optimized with prior knowledge about the task and the data. For example, we can use words as context units in a text database, and in a graph database, we prefer subgraph patterns to be context units, since single items (i.e., vertices and edges) are noninformative.\nThis general strategy gives much freedom to select context units. However, selecting patterns of various granularity may cause the redundancy of context because these patterns are highly redundant. As discussed in previous sections, we expect the context units not only to carry semantic information but also to be as discriminative as possible to indicate the meanings of a pattern. However, when various granularity of patterns are selected as context units, some units will become less discriminative, and more severely, some becomes redundant. For example, when the pattern \"mining subgraph\" is added as a context unit, the discriminating power of other units like \"mining frequent subgraph\" and \"subgraph\" would be weakened. This is because the transactions containing the pattern \"mining subgraph\" always contain \"subgraph\", and likely also contain \"mining frequent subgraph\", which means that these patterns are highly dependent and not discriminative to indicate the semantics of the frequent patterns co-occurring with them. This redundancy also brings a lot of unnecessary dimensions into the context vector space where the dimensionality is already very high. This redundancy in dimensions will affect both the efficiency and accuracy of distance computation between two vectors, which is essential for SPA. In our work, we examine different techniques to remove the redundancy of context units without losing the semantic discriminating power.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Redundancy Removal: Existing Techniques", "text": "One may first think of using existing techniques such as pattern summarization and dimension reduction to remove the redundancy of context units.\nWhile the context units can be any patterns in principle, we are practically not interested in those with very low frequency in the databases. Therefore, the context units we initially include are frequent patterns. There exist methods for summarizing frequent patterns with k representative patterns [21], but they only work for itemset patterns and are not general enough for our purpose. Some techniques such as LSI [5] have been developed to reduce the dimensionality in high dimensional spaces, especially for text data. However, these techniques aim to mitigate the sparseness of data vectors by reducing the dimensionality, and are not tuned for removing the \"redundant\" dimensions. This is because all these dimensionality reduction techniques consider that each dimension is \"important\" and the information it carries will always be preserved, or propagated into the new space. This is, however, different from our goal of redundancy removal. For example, if d1 and d2 correspond to the patterns \"AB\" and \"ABC\" respectively, and if we consider d2 to be redundant w.r.t d1, we do not expect the information of d2 to be preserved after the removal of d2.", "publication_ref": ["b20", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Redundancy Removal: Closed Frequent Pattern", "text": "Since neither the pattern summarization nor the dimensionality reduction technique is directly applicable to our problem, we examine alternative strategies. Noticing that the redundancy of context units is likely to be caused by the inclusion of both a frequent pattern and its sub patterns, we explore closed frequent patterns [15] and maximum frequent patterns [16] to solve this problem.\nA maximal frequent pattern is a frequent pattern which does not have a frequent super-pattern. It is easy to show that maximum frequent pattern is not appropriate for this problem since it may lose important discriminative units. For example, the frequent pattern \"data cube\", although not a maximum frequent pattern, indicates different semantics from the frequent pattern \"prediction data cube\", and thus should not be removed.\nDefinition 8 (Closed Frequent Pattern): A frequent pattern p\u03b1 is closed if and only if there exists no superpattern p \u03b2 of p\u03b1, s.t. D\u03b1 = D \u03b2 .\nWe assume that a context unit is not redundant only if it is a closed pattern. This assumption is reasonable because \u2200p\u03b1 \u2208 PD, if p\u03b1 is not closed, there is always another frequent pattern p \u03b2 \u2208 PD, where p\u03b1 \u2286 p \u03b2 and \u2200ti \u2208 D, we have p\u03b1 \u2208 ti \u21d4 p \u03b2 \u2208 ti. This indicates that we can use p \u03b2 as a representative of p\u03b1 and p \u03b2 without losing any semantic discriminating power. Therefore, in our work we use closed frequent patterns as our initial set of context units. The algorithms for mining different kinds of closed frequent patterns can be found in [15,23].", "publication_ref": ["b14", "b15", "b14", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Redundancy Removal: Microclustering", "text": "However, as stated in [21], a small disturbance within the transactions may result in hundreds of subpatterns that could have different supports, which cannot be pruned by closed frequent pattern mining. Those subpatterns are usually with supports only slightly different from that of the master pattern. Therefore, their discriminating power for the semantics of the frequent patterns is very weak when their master patterns are also included as a context unit. We present clustering methods to further remove redundancy from the closed frequent patterns.\nMicroclustering is usually employed as a preprocessing step to group data points from presumably the same cluster to reduce the number of data points. In our work, we first introduce a distance measure between two frequent patterns and then introduce two microclutering algorithms to further group the close frequent patterns.\nDefinition 9 (Jaccard Distance): Let p\u03b1 and p \u03b2 as two frequent patterns. The Jaccard Distance between p\u03b1 and p \u03b2 is computed as:\nD(p\u03b1, p \u03b2 ) = 1 \u2212 |D\u03b1 \u2229 D \u03b2 | |D\u03b1 \u222a D \u03b2 |\nJaccard Distance [10] is commonly applied to cluster data based on their co-occurrence in transactions. Our need is to group the patterns that tend to appear in the same transactions, which is well captured by Jaccard Distance. Jaccard Distance has also been applied to pattern clustering in [20].\nWith Jaccard Distance, we expect to extract clusters such that the distances between inner-cluster units are bounded. We present two microclustering algorithms as follows:\nIn the Hierarchical Microclustering method presented as Algorithm 1, we iteratively group two clusters of patterns with the smallest distance, where the distance between two Algorithm 1 Hierarchical Microclustering Input: Transaction dataset D, A set of n closed frequent patterns, P = {p1, ..., pn} Threshold of distance, \u03b3 Output: A set of patterns, P = {p 1 , ..., p k } add p\u03b1 into P , where \u03b1 = argmini(di); 15: return clusters are defined as the Jaccard distance between the farthest patterns in the two clusters. The algorithm terminates when the minimal distance between clusters becomes larger than a user-specified threshold \u03b3. The second algorithm, which we call One-Pass Microclustering, iteratively assigns a closed frequent pattern p\u03b1 to its nearest cluster if the distance is below \u03b3, where the distance between p\u03b1 and a cluster C is defined as the Jaccard distance between p\u03b1 and its farthest pattern in C. Both algorithms give us a set of microclusters of closed frequent patterns. They both guarantee that the distance between any pair of patterns in the same cluster is below \u03b3. Only the medoid of each cluster is selected as a context unit. By varying \u03b3, a user can select context units with various levels of discriminating power of pattern semantics. It is clear that Algorithm 2 only passes the pattern set once and thus is more efficient than the hierarchical algorithm, at the expense that the quality of clusters depends on the order of patterns. The performance of these two methods are compared in Section 5.", "publication_ref": ["b20", "b9", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Strength Weighting for Context Units", "text": "Once the context units are selected, the remaining task is to assign a weight to each dimension of the context model, which represents how strong the context unit corresponding to this dimension indicates the meaning of a given pattern. Intuitively, the strongest context indicators for a pattern p\u03b1 should be those units that frequently co-occur with p\u03b1 but infrequently co-occur with others.\nPractically, many types of weighting functions can be used to measure the strength of a context indicator. For example, we can assign the weight for a context indicator u for p\u03b1 as the number of transactions with both u and p\u03b1. However, in principle, a good weighting function is expected to satisfy several constraints:\nGiven a set of context indicator U and a frequent pattern p\u03b1, a strength weighting function w(\u2022, p\u03b1) is good if \u2200ui \u2208 U 1. w(ui, p\u03b1) \u2264 w(p\u03b1, p\u03b1): the best semantic indicator of p\u03b1 is itself;\n2. w(ui, p\u03b1) = w(p\u03b1, ui): two patterns are equally strong to indicate the meanings of each other;\n3. w(ui, p\u03b1) = 0 if the appearance of ui and p\u03b1 is independent: ui cannot indicate the semantics of p\u03b1.\nAn obvious choice is co-occurrences, which however, may not be a good measure. One one hand, it does not satisfy constraints 3. On the other hand, we want to penalize the context units that are globally common patterns in the collection. Which means, although they may co-occur many times with p\u03b1, it may still not be a good context indicator for p\u03b1 because it also co-occurs frequently with others. In general, the context units that are strongly correlated to p\u03b1 should be weighted higher. In our work, we introduce a more principled measure.\nMutual Information (MI) is widely used to measure the mutual independency of two random variables in information theory, which intuitively measures how much information a random variable tells about the other. The definition of mutual information is given as Definition 10: (Mutual Information). Given two frequent patterns p\u03b1 and p \u03b2 , let X = {0, 1} and Y = {0, 1} be two random variables for the appearance of p\u03b1 and p \u03b2 respectively. Mutual information I(X; Y ) is computed as:\nI(X; Y ) = x\u2208X y\u2208Y P (x, y)log P (x, y) P (x)P (y)\nwhere\nP (x = 1, y = 1) = |D\u03b1\u2229D \u03b2 | |D| , P (x = 0, y = 1) = |D \u03b2 |\u2212|D\u03b1\u2229D \u03b2 | |D| , P (x = 1, y = 0) = |D\u03b1|\u2212|D\u03b1\u2229D \u03b2 | |D|\n, and\nP (x = 0, y = 0) = |D|\u2212|D\u03b1\u222aD \u03b2 | |D| .\nIn our experiments, we use standard Laplace smoothing to avoid zero probability.\nIt can be easily proved that Mutual Information satisfies all the three constraints and favors the strongly correlated units. In our work, we use mutual information to model the indicative strength of the context units selected.\nGiven a set of patterns as candidate context units, we apply closeness testing and microclustering to remove redundant units from this initial set. We then use mutual information as the weighting function for each indicator selected. Given a frequent pattern, we apply semantic analysis with its context model and generate annotations for this pattern, as discussed in the following section.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SEMANTIC ANALYSIS AND PATTERN ANNOTATION", "text": "Let U = {u1, u2, ..., u k } be a selected set of k context units and w(\u2022, p\u03b1) be the unit weighting function w.r.t. any frequent pattern p\u03b1, i.e. I(\u2022; p\u03b1). The context model, or context vector c(\u03b1) for p\u03b1 is w(u1, p\u03b1), w(u2, p\u03b1), ..., w(u k , p\u03b1) .\nAs introduced in Section 1, we make the assumption that the frequent patterns are semantically similar if their contexts are similar to each other. In our work, we analyze the semantics of frequent patterns by comparing their context models. Formally, Definition 11 (Semantical Similarity): Let p\u03b1, p \u03b2 , p \u03b4 be three frequent patterns in P and c(\u03b1), c(\u03b2), c(\u03b4) \u2208 V k be their context models. Let sim(c(\u2022), c(\u2022)) : V k \u00d7V k \u2212\u2192 R + be a similarity function of two context vectors. If sim(c(\u03b1), c(\u03b2)) > sim(c(\u03b1), c(\u03b4)), we say that p \u03b2 is semantically more similar to p\u03b1 than p \u03b4 w.r.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "t. sim(c(\u2022), c(\u2022)).", "text": "Cosine is widely used to compute the similarity between two vectors, and is well explored in information retrieval to measure the relevance between a document and a query if both are represented with a vector space model [17]. In our work, we use cosine similarity of two context vectors to measure the semantic similarity of two corresponding frequent patterns. Formally, the cosine similarity of two context vectors is computed as\nsim(c(\u03b1), c(\u03b2)) = k i=1 ai * bi k i=1 a 2 i * k i=1 b 2 i\nwhere c(\u03b1) = a1, a2, ..., a k and c(\u03b2) = b1, b2, ..., b k . With the context model and the semantical similarity measure, we now discuss how to generate semantic annotations for frequent patterns.", "publication_ref": ["b16"], "figure_ref": [], "table_ref": []}, {"heading": "Extracting Strongest Context Indicators", "text": "Let p\u03b1 be a frequent pattern and c(\u03b1) be its context model, which is defined in this work as a context vector w1, w2, ..., w k over a set of context units U = {u1, u2, ..., u k }. As defined in Section 2, wi is a weight for ui which tells how well ui indicates the semantics of p\u03b1. Therefore, the goal of extracting strongest context indicators is to extract a subset of k context units U\u03b1 \u2286 U such that \u2200ui \u2208 U\u03b1 and \u2200uj \u2208 U \u2212 U\u03b1, we have wi \u2265 wj.\nWith a strength weighting function w(\u2022, p\u03b1), e.g., mutual information as introduced in Section 3, we compute wi = w(ui, p\u03b1), rank ui \u2208 U with wi in descending order and select the top k ui's.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Extracting Representative Transactions", "text": "Let p\u03b1 be a frequent pattern, c(\u03b1) be its context model, and D = {t1, ...t l } be a set of transactions, our goal is to select kt transactions T\u03b1 \u2286 D with a similarity function s(\u2022, p\u03b1), s.t. \u2200t \u2208 T\u03b1 and \u2200t \u2208 D \u2212 T\u03b1, s(t, p\u03b1) \u2265 s(t , p\u03b1).\nTo achieve this, we first represent a transaction as a vector in the same vector space as the context model of the frequent pattern p\u03b1, i.e., over {u1, u2, ..., u k }. Then, we use the cosine similarity presented in Section 3 to compute the similarity between a transaction t and the context of p\u03b1. The rest is again a ranking problem. Formally, let c(t) = w 1 , w 2 , ..., w k where w i = 1 if ui \u2208 t and w i = 0 otherwise. We compute sim(c(t), c(\u03b1)) for each t \u2208 T\u03b1, rank them in descending order and select the top kt t's.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Extracting Semantically Similar Patterns", "text": "Let p\u03b1 be a frequent pattern, c(\u03b1) be its context model, and Pc = {p1, ..., pc} be a set of frequent patterns which are believed to be good candidates for annotating the semantics of p\u03b1, i.e., as synonyms, thesauri, or more generally as SSPs. Our goal is to extract a subset of kc patterns P c \u2286 Pc whose contexts are most similar to p\u03b1. Formally, let {c(p1), ..., c(pc)} be the context vectors for {p1, ..., pc}. We compute sim(c(pi), c(\u03b1)) for each pi \u2208 Pc, rank them in descending order, and select the top kc pi's.\nNote that the candidate SSP set for annotation is quite flexible. It can be the whole set of frequent patterns in D, or a user-specified set of patterns based on his prior knowledge. It can be a set of homogenous patterns with p\u03b1, or a set of heterogenous patterns. For example, it can be a set of patterns or terminology from the domain that a user is familiar with, and is used to annotate patterns from an unfamiliar domain. This brings great flexibility to apply the general SPA techniques to different tasks. By exploring different types of candidate SSPs, we can find quite a few interesting applications of semantic pattern annotation, which are discussed in Section 5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTS AND RESULTS", "text": "In this section, we present experiment results on three different datasets to show the effectiveness of the semantic pattern annotation technique for various real-world tasks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "DBLP Dataset", "text": "The first dataset we use is a subset of the DBLP dataset 2 . It contains papers from the proceedings of 12 major conferences in Database and Data Mining. Each transaction consists of two parts, the authors and the title of the corresponding paper. We consider two types of patterns: (1) frequent co-authorship, each of which is a frequent itemset of authors and (2) frequent title terms, each of which is a frequent sequential pattern of the title words. The goal of experiments on this dataset is to show the effectiveness of the SPA to generate a dictionary-like annotation for frequent patterns. Our experiments are designed as follows:\n1) Given a set of authors/co-authors, annotate each of them with their strongest context indicators, the most representative titles from their publications, and the co-authors or title patterns which are most semantically similar to them. Note that the most representative titles do not necessarily mean their most influential work, but rather the titles which best distinguish their work from others' work.\n2) Given a set of title terms (sequential patterns), annotate each of them with their strongest context indicators, the most representative titles, the most similar terms, and the most representative author/co-authors. Note again that the most representative author/co-authors are not necessarily the most well-known ones, but rather the authors who are most strongly correlated to the topics (terms).\nIn both experiments, we use the tools FP-Close [7] and CloSpan [23] to generate closed frequent itemsets of coauthors and closed sequential patterns of title terms respectively. The title words are stemmed by Krovertz stemmer [12], which converts the morphological variations of each English word to its root form. We set the minimum support for frequent itemset as 10 and sequential patterns as 4, 2 http://www.informatik.uni-trier.de/\u223cley/db/ which outputs 9926 closed sequential patterns. We use the One-Pass microclustering algorithm discussed in Section 3 to remove redundancy from those sequential patterns and get a smaller set of 3443 patterns, with \u03b3 = 0.9 (the average Jaccard distance between these patterns is > 0.95).  Table 1 shows the medoids and cluster members of three microclusters generated by the One-Pass microclustering algorithm discussed in Section 3, all of which begin with the term \"mine\". We see that different variations of the same concept are grouped into the same cluster, although all of them are closed patterns. This successfully reduces the pattern redundancy. It is interesting to see that the pattern \"data mine\" and \"mine data\" are assigned to different clusters, which cannot be achieved by the existing pattern summarization techniques such as [21]. The results generated by hierarchical microclustering are similar.\nIn Table 2, we selectively show the results of semantic pattern annotations. We see that the SPA system can automatically generate dictionary-like annotations for different kinds of frequent patterns. For frequent itemsets like coauthorship or single authors, the strongest context indicators are usually their other co-authors and discriminative title terms that appear in their work. The semantically similar patterns extracted also reflect the authors and terms related to their work. However, these SSPs may not even co-occur with the given pattern in a paper. For example, the pattern \"jiayong wang\", \"jiong yang&philip s yu&wei wang\" actually do not co-occur with the pattern \"xifeng yan&jiawei han\", but are extracted because their contexts are similar. For a single author, whose context is usually more diverse, the SSPs are more likely to be title terms instead of authors.\nWe also present the annotations generated for title terms, which are frequent sequential patterns. Their strongest context indicators are usually the authors who tend to write them in the titles of their papers, or the terms that tend to co-appear with them. Their SSPs usually provide interesting concepts or descriptive terms which are close to their meanings, e.g. \"information retrieval \u2192 information filter\", \"xquery \u2192 complex language, function query language\".\nIn both scenarios, the representative transactions extracted give us the titles of papers that well capture the meaning of the given patterns. We only show the title words in Table 2 for each transaction.\nThese experiments show that the SPA can generate dictionary like annotations for frequent patterns effectively. In the following two experiments, we quantitatively evaluate the performance of SPA, by applying it to two interesting tasks.", "publication_ref": ["b6", "b22", "b11", "b20"], "figure_ref": [], "table_ref": ["tab_3", "tab_5", "tab_5"]}, {"heading": "Matching Motifs and GO Terms", "text": "A challenging and promising research topic in computational biology is to predict the functions for newly discovered protein motifs, which are conserved amino acid sequence patterns characterizing the function of proteins. To solve this problem, researchers have studied how to match Gene  Note: \"I\" means context indicators; \"T\" means representative transactions; \"S\" means semantically similar patterns. We exclude 12 most frequent and non-informative English words from the collection when extracting frequent patterns.\nOntology(GO) terms with motifs [18]. Usually, each protein sequence, which contains a number of motifs, is assigned a set of GO terms that annotate its functions. The goal of the problem is to automatically match each individual motif with GO terms which best represent its functions. In this experiment, we formalize the problem as: Given a set of transactions D (protein sequences with motifs tagged and GO terms assigned), a set P of frequent patterns in D to be annotated (motifs), and a set of candidate patterns Pc with explicit semantics (GO terms), our goal is for \u2200p\u03b1 \u2208 P , find P c \u2286 Pc which best indicate the semantics of p\u03b1.\nWe used the same data set and judgments (i.e., gold standard) as used in [18]. The data has 12181 sequences, 1097 motifs, and 3761 GO terms. We also use the same performance measure as in [18] (i.e., a variant of Mean reciprocal rank (MRR) [11], notated as MRR in the following sections for convenience) to evaluate the effectiveness of the SPA technique on the Motif -GO term matching problem.\nLet G = {g1, g2, ..., gc} be a set of GO terms. Given a motif pattern p\u03b1, G = {g 1 , g 2 , ..., g k } \u2286 G is a set of \"correct\" GO terms for p\u03b1 in our judgement data. We rank G with the SPA system and pick the top ranked terms, where G is treated as either context units or semantically similar patterns to p\u03b1. This will give us a rank for each gi \u2208 G, say r(gi). MRR (w.r.t. p\u03b1) is then computed as\nM RR\u03b1 = 1 k k i=1 1 r(g i )\nwhere r(g i ) is the i th correct GO term for p\u03b1. If g i is not in the top ranked list, we set 1/r(g i ) = 0. We take the average over all the motifs, M RR = 1/m P\u03b1\u2208P M RR\u03b1 to measure the overall performance, where m is the number of motifs in our judgement file. Clearly, 0 \u2264 M RR \u2264 1. A higher M RR value indicates a higher precision, and the top-ranked GO terms have the highest influence on M RR, which is intuitively desirable.\nIf we are ranking the full candidate GO set for annotation, a \"lazy\" system may either just give them the same rank or rank them randomly. It is easy to show that the expected M RR score for these two cases are the same, which is\nE[M RR] = 1 |G| |G| i=1 1 r(gi)\nwhere |G| is the number of GO terms in G. E[M RR] drops monotonously when |G| increases, which indicates the larger the candidate set is, the more difficult is the ranking task. We use this value as the baseline to compare our results.\nWe employ all the motifs and GO terms as context units. Since these patterns are not overlapping with each other, we do not use microclustering to preprocess the context units. We compare the ranking of GO terms either as context indicators or as SSPs. We also compare the use of Mutual Information and co-occurrence as strength weight for context units. These strategies are compared in Table 3  We see that SPA is quite effective in matching motifs with GO terms, consistently outperforming the baseline. Ranking GO terms as context units achieves better results than ranking them as SSPs, which is reasonable because a GO term usually describes only one aspect of a motif's function and is shared by a number of motifs, thus its context is likely quite different from that of a motif. Interestingly, we notice that although Mutual Information is a better measure for the strength weight in principle, in this specific problem, using MI as strength weight for context units is not as good as using simple co-occurrence. This may be because there are hardly many Go terms that are globally very common in this dataset, and therefore MI over penalizes the frequent patterns. A detailed discussion on why co-occurrence measure outperforms MI on Motif-GO matching problem is given in [18].", "publication_ref": ["b17", "b17", "b17", "b10", "b17"], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Matching Gene Synonyms", "text": "As discussed in Section 4.3, the algorithm for extracting semantically similar patterns aims at finding patterns whose meaning is very close to the pattern to be annotated. Ideally, they would be synonyms, or thesauri of the given pattern. These patterns may not ever co-occur with the given pattern but tend to have similar contexts, thus cannot be extracted as strong context indicators. We do another experiment to test the performance of SPA on extracting SSPs.\nIn biomedical literature, it is common that different terms or aliases are used in different studies to denote the same gene, which are known as gene synonyms (see e.g., Table 4). These synonyms generally do not appear together but are \"replaceable\" with each other. Detecting them can help many literature mining tasks. In this experiment, we test the application of SPA to matching gene synonyms.  We construct the synonym list for 100 fly genes, which are randomly selected from the data provided by BioCre-AtIvE Task 1B 3 . Ling et al. collected 22092 abstracts from MEDLINE 4 which contain the keyword \"Drosophila\" [14]. We extract the sentences from those abstracts which contain at least one synonym in the synonym list. Only the synonyms with support \u2265 3 are kept, which gives us a small set of 41 synonyms. We then mix those synonyms which belong to different genes and use the algorithm of extracting SSPs to recover the matching of synonyms. Specifically, given a synonym from the mixed list, we rank all synonyms with the SSP extraction algorithm. The performance of the system is evaluated by comparing the ranked list with the correct synonyms for the same gene. We also use MRR as the evaluation measure. The results are shown as follows.    From Table 5, we see that the SPA algorithm is also effective for matching gene synonyms, which significantly outperforms the random baseline. When using closed sequential patterns as context units, we always achieve better results than using single words (items) as context units, where a higher minimum support (minsup) usually yields better results. When closed sequential patterns are used, further microclustering indeed improves the performance of the system. However, when the minsup is higher, this improvement is decaying. This is reasonable because when the minsup is higher, there is less redundancy among the output closed patterns. Using hierarchical microclustering is slightly better than using the one-pass algorithm, but not always.\nFinally, we discuss the performance of microclustering in removing redundant context units. The effectiveness and efficiency are shown in Figure 1. Both microclustering methods improve the precision (MRR score) when more redundant patterns are grouped into clusters. However, when \u03b3 is set too large, the precision decreases. This indicates that we may have over penalized the redundancy and lost useful context units. A good \u03b3 for this task is around 0.8.\nAlthough the cluster quality may not be optimized, the performance of one-pass microclustering is comparable to hierarchical microclustering on this task. While in principle, the hierarchical clustering is not efficient, the early termination by using a small \u03b3 saves a lot of time. The one-pass algorithm is more efficient than the hierarchical clustering, and is not affected by \u03b3. The overhead that both algorithms suffer is the computation of Jaccard distances for all pairs of patterns, i.e., O(n 2 ) where n is the number of patterns. However, this computation can be coupled in frequent pattern mining, as discussed in [20].", "publication_ref": ["b13", "b19"], "figure_ref": ["fig_1"], "table_ref": ["tab_9", "tab_11"]}, {"heading": "RELATED WORK", "text": "To the best of our knowledge, the problem of semantic pattern annotation has not been well studied in existing work.\nMost frequent pattern mining work [2,8,3,22] focuses on discovering frequent patterns efficiently from the database, and does not address the problem of pattern postprocessing. To solve the problem of high redundancy in patterns discovered, closed frequent pattern [15], maximum frequent pattern [16] and top-k closed pattern [9] are proposed to shrink the size of output patterns while keeping the important ones. However, none of this work provides additional information other than simple statistics to help users interpret the frequent patterns. The context information for a pattern tends to be ignored.\nRecently, researchers develop new techniques to approximate, summarize a frequent pattern set [1,21], or mine compressed frequent pattern sets [20]. Although they explored some kind of context information, none of the work can provide in-depth semantic annotations for frequent patterns as we do in our work. The context model proposed in our work covers both the pattern profile in [21] and transaction coverage in [20] as special cases.\nContext and semantic analysis are quite common in natural language and text processing (see e.g., [17,5,13]). Most work, however, deals with non-redundant word-based contexts, which are quite different from pattern contexts.\nIn specific domains, people have explored the context of specific data patterns to solve specific problems [18,14]. Although not optimally tuned, the general techniques proposed in our work can be well applied to those tasks.", "publication_ref": ["b1", "b7", "b2", "b21", "b14", "b15", "b8", "b0", "b20", "b19", "b20", "b19", "b16", "b4", "b12", "b17", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSIONS", "text": "Existing frequent pattern mining work usually generates a huge amount of frequent patterns without providing enough information to interpret the meanings of the patterns. Some recent work introduced postprocessing techniques to summarize and compress the pattern set, which shrinks the size of the output set of frequent patterns but does not provide semantic information for patterns.\nWe propose the novel problem of semantic pattern annotation (SPA) -generating semantic annotations for frequent patterns. A semantic annotation consists of a set of strongest context indicators, a set of representative transactions, and a set of semantically similar patterns (SSPs) to a given frequent pattern. We define a general vector-space context for a frequent pattern. We propose algorithms to exploit context modeling and semantic analysis to generate semantic annotations automatically. The context modeling and semantic analysis method we presented is quite general and can deal with any types of frequent patterns with context information. The method can be coupled with any frequent pattern mining techniques as a postprocessing step to facilitate interpretation of the discovered patterns.\nWe evaluated our approach on three different dataset and tasks. The results show that our methods can generate semantic pattern annotations effectively. As shown in our experiments, our method can be potentially applied to many interesting real world tasks through selecting different context units and focusing on candidate patterns for SSPs.\nAlthough the proposed SPA framework is quite general, in this paper, we only studied some specific instantiation of the framework based on mutual information weighting and cosine similarity measure. A major goal for future research is to fully develop the potential of the proposed framework by studying alternative instantiations. For example, we may explore other options for context unit weighting and semantic similarity measurement, the two key components in our framework.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ACKNOWLEDGMENTS", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "We thank Tao Tao and Xu Ling for providing the datasets of Motif-GO matching and gene synonym matching, respectively. This work was in part supported by the National Science Foundation under award numbers 0425852.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Approximating a collection of frequent sets", "journal": "", "year": "2004", "authors": "F Afrati; A Gionis; H Mannila"}, {"ref_id": "b1", "title": "Mining association rules between sets of items in large databases", "journal": "", "year": "1993", "authors": "R Agrawal; T Imieliski; A Swami"}, {"ref_id": "b2", "title": "Mining sequential patterns", "journal": "", "year": "1995", "authors": "R Agrawal; R Srikant"}, {"ref_id": "b3", "title": "Beyond market baskets: generalizing association rules to correlations", "journal": "", "year": "1997", "authors": "S Brin; R Motwani; C Silverstein"}, {"ref_id": "b4", "title": "Indexing by latent semantic analysis", "journal": "Journal of the American Society of Information Science", "year": "1990", "authors": "S C Deerwester; S T Dumais; T K Landauer; G W Furnas; R A Harshman"}, {"ref_id": "b5", "title": "Frequent sub-structure-based approaches for classifying chemical compounds", "journal": "", "year": "2003", "authors": "M Deshpande; M Kuramochi; G Karypis"}, {"ref_id": "b6", "title": "Efficiently using prefix-trees in mining frequent itemsets", "journal": "", "year": "2003", "authors": "G Grahne; J Zhu"}, {"ref_id": "b7", "title": "Mining frequent patterns without candidate generation: A frequent-pattern tree approach", "journal": "Data Min. Knowl. Discov", "year": "2004", "authors": "J Han; J Pei; Y Yin; R Mao"}, {"ref_id": "b8", "title": "Mining top-k frequent closed patterns without minimum support", "journal": "", "year": "2002", "authors": "J Han; J Wang; Y Lu; P Tzvetkov"}, {"ref_id": "b9", "title": "Nouvelles recherches sur la distribution florale", "journal": "Bull. Soc. Vaudoise Sci. Nat", "year": "1908", "authors": "P Jaccard"}, {"ref_id": "b10", "title": "The TREC-5 confusion track: Comparing retrieval methods for scanned text", "journal": "Information Retrieval", "year": "2000", "authors": "P Kantor; E Voorhees"}, {"ref_id": "b11", "title": "Viewing morphology as an inference process", "journal": "", "year": "1993", "authors": "R Krovetz"}, {"ref_id": "b12", "title": "Induction of semantic classes from natural language text", "journal": "", "year": "2001", "authors": "D Lin; P Pantel"}, {"ref_id": "b13", "title": "Automatically generating gene summaries from biomedical literature", "journal": "", "year": "2006", "authors": "X Ling; J Jiang; X He; Q Mei; C Zhai; B Schatz"}, {"ref_id": "b14", "title": "Discovering frequent closed itemsets for association rules", "journal": "", "year": "1999", "authors": "N Pasquier; Y Bastide; R Taouil; L Lakhal"}, {"ref_id": "b15", "title": "Efficiently mining long patterns from databases", "journal": "", "year": "1998", "authors": "J ; Roberto J Bayardo"}, {"ref_id": "b16", "title": "A vector space model for automatic indexing", "journal": "Commun. ACM", "year": "1975", "authors": "G Salton; A Wong; C S Yang"}, {"ref_id": "b17", "title": "A study of statistical methods for function prediction of protein motifs", "journal": "Applied Bioinformatics", "year": "2004", "authors": "T Tao; C Zhai; X Lu; H Fang"}, {"ref_id": "b18", "title": "Clustering transactions using large items", "journal": "", "year": "1999", "authors": "K Wang; C Xu; B Liu"}, {"ref_id": "b19", "title": "Mining compressed frequent-pattern sets", "journal": "", "year": "2005", "authors": "D Xin; J Han; X Yan; H Cheng"}, {"ref_id": "b20", "title": "Summarizing itemset patterns: a profile-based approach", "journal": "", "year": "2005", "authors": "X Yan; H Cheng; J Han; D Xin"}, {"ref_id": "b21", "title": "gspan: Graph-based substructure pattern mining", "journal": "", "year": "2002", "authors": "X Yan; J Han"}, {"ref_id": "b22", "title": "Clospan: Mining closed sequential patterns in large datasets", "journal": "", "year": "2003", "authors": "X Yan; J Han; R Afshar"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Effect of microclustering algorithms HIER: hierarchical microclustering; ONEP: one-pass microclustering; minsup = 0.3% Avg. \u03b3 = 0.96;", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "1: initialize n clusters Ci, each as a closed frequent pattern; 2: compute the Jaccard Distance dij among {p1, ..., pn}; 3: set the current minimal distance d = min(dij); Output: A set of patterns, P = {p 1 , ..., p k }", "figure_data": "4: while (d < \u03b3)5:select dst where (s, t) = argmini,jdij;6:merge clusters Cs and Ct into a new cluster Cu;7:foreach Cv = Cu8:compute duv = max(d \u03b1\u03b2 ) where p\u03b1 \u2208 Cu, p \u03b2 \u2208 Cv;9: foreach Cu;10:foreach p\u03b1 \u2208 Cu;11:computed\u03b1 = avg(d \u03b1\u03b2 ) where p \u03b2 \u2208 Cu;12:add p\u03b1 into P , where \u03b1 = argmini(di);13: returnAlgorithm 2 One-pass MicroclusteringInput: Transaction dataset D,1: initialize 0 clusters;2: compute the Jaccard Distance dij among {p1, ..., pn};3: foreach (p\u03b1 \u2208 P)4:foreach cluster Cu5:d\u03b1,u = max(d \u03b1\u03b2 ) where p \u03b2 \u2208 Cu;6:v = argminu(d\u03b1,u);7:if (d\u03b1,v < \u03b3)8:assign p\u03b1 to Cv9:else10:initialize a new cluster C = {p\u03b1}11: foreach Cu;12:foreach p\u03b1 \u2208 Cu;13:computed\u03b1 = avg(d"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Effectiveness of Microclustering", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Annotations Generated for Frequent Patterns in DBLP Dataset", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Examples of gene synonym patterns", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "M RR of SPA on gene synonym matching", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "A pattern p\u03b1 is fre- quent in a dataset D, if |D\u03b1| |D| \u2265 \u03c3,", "formula_coordinates": [2.0, 316.81, 660.62, 239.11, 23.88]}, {"formula_id": "formula_1", "formula_text": "w(oi, \u03b1) = t j \u2208D\u03b1 t i j |D\u03b1|", "formula_coordinates": [4.0, 53.8, 227.08, 84.82, 18.35]}, {"formula_id": "formula_2", "formula_text": "D(p\u03b1, p \u03b2 ) = 1 \u2212 |D\u03b1 \u2229 D \u03b2 | |D\u03b1 \u222a D \u03b2 |", "formula_coordinates": [5.0, 118.33, 576.23, 108.85, 21.77]}, {"formula_id": "formula_3", "formula_text": "I(X; Y ) = x\u2208X y\u2208Y P (x, y)log P (x, y) P (x)P (y)", "formula_coordinates": [6.0, 93.72, 520.67, 158.07, 25.14]}, {"formula_id": "formula_4", "formula_text": "P (x = 1, y = 1) = |D\u03b1\u2229D \u03b2 | |D| , P (x = 0, y = 1) = |D \u03b2 |\u2212|D\u03b1\u2229D \u03b2 | |D| , P (x = 1, y = 0) = |D\u03b1|\u2212|D\u03b1\u2229D \u03b2 | |D|", "formula_coordinates": [6.0, 54.99, 553.07, 237.89, 29.69]}, {"formula_id": "formula_5", "formula_text": "P (x = 0, y = 0) = |D|\u2212|D\u03b1\u222aD \u03b2 | |D| .", "formula_coordinates": [6.0, 53.8, 570.9, 239.1, 27.05]}, {"formula_id": "formula_6", "formula_text": "sim(c(\u03b1), c(\u03b2)) = k i=1 ai * bi k i=1 a 2 i * k i=1 b 2 i", "formula_coordinates": [6.0, 352.87, 330.41, 165.3, 30.38]}, {"formula_id": "formula_7", "formula_text": "M RR\u03b1 = 1 k k i=1 1 r(g i )", "formula_coordinates": [8.0, 129.93, 491.23, 85.64, 27.87]}, {"formula_id": "formula_8", "formula_text": "E[M RR] = 1 |G| |G| i=1 1 r(gi)", "formula_coordinates": [8.0, 122.71, 661.66, 100.09, 28.57]}], "doi": ""}