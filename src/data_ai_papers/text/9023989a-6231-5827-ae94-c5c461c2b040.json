{"title": "A Continuized View on Nesterov Acceleration for Stochastic Gradient Descent and Randomized Gossip", "authors": "Mathieu Even; Rapha\u00ebl Berthier; Francis Bach; Nicolas Flammarion; Pierre Gaillard; Hadrien Hendrikx; Laurent Massouli\u00e9; Adrien Taylor", "pub_date": "", "abstract": "We introduce the \"continuized\" Nesterov acceleration, a close variant of Nesterov acceleration whose variables are indexed by a continuous time parameter. The two variables continuously mix following a linear ordinary differential equation and take gradient steps at random times. This continuized variant benefits from the best of the continuous and the discrete frameworks: as a continuous process, one can use differential calculus to analyze convergence and obtain analytical expressions for the parameters; and a discretization of the continuized process can be computed exactly with convergence rates similar to those of Nesterov original acceleration. We show that the discretization has the same structure as Nesterov acceleration, but with random parameters. We provide continuized Nesterov acceleration under deterministic as well as stochastic gradients, with either additive or multiplicative noise. Finally, using our continuized framework and expressing the gossip averaging problem as the stochastic minimization of a certain energy function, we provide the first rigorous acceleration of asynchronous gossip algorithms.", "sections": [{"heading": "Introduction", "text": "In the last decades, the emergence of numerous applications in statistics, machine learning and signal processing has led to a renewed interest in first-order optimization methods [10]. They enjoy a low iteration cost necessary to the analysis of large datasets. The performance of first-order methods was largely improved thanks to acceleration techniques (see the review by d'Aspremont et al. [14] and the many references therein), starting with the seminal work of Nesterov [42].\nLet f : R d \u2192 R be a convex and differentiable function, minimized at x * \u2208 R d . We assume throughout the paper that f is L-smooth, i.e.,\n\u2200x, y \u2208 R d , f (y) f (x) + \u2207f (x), y \u2212 x + L 2 y \u2212 x 2 . (1\n)\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2106.07644v2 [math.OC] 27 Oct 2021\nIn addition, we sometimes assume that f is \u00b5-strongly convex for some \u00b5 > 0, i.e.,\n\u2200x, y \u2208 R d , f (y) f (x) + \u2207f (x), y \u2212 x + \u00b5 2 y \u2212 x 2 .(2)\nFor the problem of minimizing f , gradient descent is well-known to achieve a rate f (x k ) \u2212 f (x * ) = O(k \u22121 ) in the smooth case, and a rate f (x k ) \u2212 f (x * ) = O((1 \u2212 \u00b5/L) k ) in the smooth and strongly convex case. In both cases, Nesterov introduced an alternative method with essentially the same iteration cost, while achieving faster rates: it converges with rate O(k \u22122 ) in the smooth convex case and with rate O((1 \u2212 \u00b5/L) k ) in the smooth and strongly convex case [43]. These rates are then optimal among all black-box first-order methods that access gradients and linearly combine them [43,41].\nNesterov acceleration relies on several sequences of iterates-two or three, depending on the formulation-and on a clever blend of gradient steps and mixing steps between the sequences. Different interpretations and motivations underlying the precise structure of accelerated schemes were approached in many works, including [12,24,3,32,2]. A large number of these works studied continuous time equivalents of Nesterov acceleration, obtained by taking the limit when stepsizes vanish, or from a variational framework. The continuous time index t of the limit allowed to use differential calculus to study the convergence of these equivalents. Examples of studies relying on continuous time interpretation include [50,33,54,53,9,18,48,49,4,5,57,40].\nContinuized Nesterov acceleration. In this paper, we propose another continuous time equivalent to Nesterov acceleration, which we refer to as the continuized Nesterov acceleration, which avoids vanishing stepsizes. It is built by considering two sequences x t , z t \u2208 R d , t \u2208 R 0 , that continuously mix following a linear ordinary differential equation (ODE), and that take gradient steps at random times T 1 , T 2 , T 3 , . . . . Thus, in this modeling, mixing and gradient steps alternate randomly.\nThanks to the continuous index t and some stochastic calculus, one can differentiate averaged quantities (expectations) with respect to t. In particular, this leads to simple analytical expressions for the optimal parameters as functions of t, while the optimal parameters of Nesterov accelerations are defined by recurrence relations that are complicated to solve.\nThe discretizationx k = x T k ,z k = z T k , k \u2208 N, of the continuized process can be computed directly and exactly: the result is a recursion of the same form as Nesterov iteration, but with randomized parameters, and performs similarly to Nesterov original deterministic version both in theory and in simulations.\nThe continuized framework can be adapted to various settings and extensions of Nesterov acceleration.\nIn what follows, we study how the continuized acceleration behaves in the presence of additive and multiplicative noise in the gradients. In the multiplicative noise setting, our acceleration satisfies a convergence rate similar to that of [30] and depends on the statistical condition number of the problem at hand. The two acceleration schemes are not directly comparable as we work in a continuized setting and only deal with pure multiplicative noise. Our analysis is nevertheless much simpler, as it closely mimics that of Nesterov acceleration.\nApplication to accelerated gossip algorithms. The continuized modeling is natural in asynchronous parallel computing where gradient steps arrive at random times. More importantly, there are situations where the continuized version of Nesterov acceleration can be practically implemented while the original acceleration can not. In distributed settings, for instance, the total number k of gradient steps taken in the network is typically not known to each particular node; an advantage of the continuized acceleration is that it requires to know only the time t and not k.\nGossip algorithms typically feature such asynchronous and distributed behaviors [11]. In gossip problems, nodes of a network aim at computing the global average of all their values by communicating only locally (with their neighbors), and without centralized coordination. In this set-up, pairs of adjacent nodes communicate at random times, asynchronously, and in parallel, so that the total number of past communications in the network at a given time is unknown to all nodes. In this paper, we formulate the gossip problem as a stochastic optimization problem. Thanks to the continuized formalism, we naturally obtain accelerated gossip algorithms that can be implemented in an asynchronous and distributed fashion.\nSynchronous gossip algorithms rely on all nodes to communicate simultaneously [19]. Accelerating synchronous gossip algorithms have been studied in previous works, including SSDA [47], Chebyshev acceleration [39], Jacobi-Polynomial acceleration [7]. To that day, acceleration in the asynchronous setting has also been studied in a few works (see for instance geographic gossip [20], shift registers [37], ESDAC [25], and randomized Kaczmarz methods [38]). However, no algorithm in an asynchronous framework has been rigorously proven to achieve an accelerated rate for general graphs [21]. Other acceleration schemes [25,38] relied on additional synchronizations between nodes, such as the knowledge of a global iteration counter. This departs from purely asynchronous operations, hence causing practical limitation. Our accelerated randomized gossip algorithm (Section 6) recovers the same accelerated rates, and only requires the knowledge of a common continuous-time t \u2208 R 0 .\nIn this context, the continuized acceleration should be seen as a close approximation to Nesterov acceleration, that features both an insightful and convenient expression as a continuous time process and a direct implementation as a discrete iteration. We thus hope to contribute to the understanding of Nesterov acceleration. In practice, the continuized framework is relevant for handling asynchrony in decentralized optimization, where agents of a network can not share a global iteration counter, preventing accelerated decentralized and asynchronous methods.\nNotations. The index k always denotes a non-negative integer, while indices t, s always denote non-negative reals.\nStructure of the paper. In Section 2, we recall standard results on gradient descent and Nesterov acceleration. In Section 3, we introduce a continuized variant of Nesterov acceleration. In Section 4, we show that discretizing the continuized acceleration yields an iterative method similar to that of Nesterov but with random parameters. In Section 5, we study continuized Nesterov acceleration under pure-multiplicative noise. We finally present accelerated asynchronous algorithms for the gossip problem in Section 6, as well as for decentralized optimization in Section 7.", "publication_ref": ["b9", "b13", "b41", "b42", "b42", "b40", "b11", "b23", "b2", "b31", "b1", "b49", "b32", "b53", "b52", "b8", "b17", "b47", "b48", "b3", "b4", "b56", "b39", "b29", "b10", "b18", "b46", "b38", "b6", "b19", "b36", "b24", "b37", "b20", "b24", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "Reminders on Nesterov acceleration", "text": "For the sake of comparison, let us first recall the classical Nesterov acceleration. To improve the convergence rate of gradient descent, Nesterov introduced iterations of three sequences, parametrized by \u03c4 k , \u03c4 k , \u03b3 k , \u03b3 k , k 0, of the form\ny k = x k + \u03c4 k (z k \u2212 x k ) ,(3)\nx k+1 = y k \u2212 \u03b3 k \u2207f (y k ) ,(4)\nz k+1 = z k + \u03c4 k (y k \u2212 z k ) \u2212 \u03b3 k \u2207f (y k ) . (5\n)\nDepending on whether the function f is known to be (1) convex, or (2) strongly convex with a known strong convexity parameter, Nesterov provided a set of parameter choices for achieving acceleration. Theorem 1 (Convergence of accelerated gradient descent). Nesterov accelerated scheme satisfies:\n1. Choose the parameters \u03c4 k = 1 \u2212 A k A k+1 , \u03c4 k = 0, \u03b3 k = 1 L , \u03b3 k = A k+1 \u2212A k L\n, k 0, where the sequence A k , k 0, is defined by the recurrence relation\nA 0 = 0 , A k+1 = A k + 1 2 (1 + 4A k + 1) . Then f (x k ) \u2212 f (x * ) 2L x 0 \u2212 x * 2 k 2 .\n2. Assume further that f is \u00b5-strongly convex, \u00b5 > 0. Choose the constant parameters  [43,Theorem 2.1.15] for instance). In both cases, this results in a significant speedup in practice, see Figure 1.\n\u03c4 k \u2261 \u221a \u00b5/L 1+ \u221a \u00b5/L , \u03c4 k \u2261 \u00b5 L , \u03b3 k \u2261 1 L , \u03b3 k \u2261 1 \u221a \u00b5L , k 0. Then f (x k ) \u2212 f (x * ) f (x 0 ) \u2212 f (x * ) + \u00b5 2 z 0 \u2212 x * 2 1 \u2212 \u00b5 L k .\nFrom a high-level perspective, Nesterov acceleration iterates over several variables, alternating between gradient steps (always with respect to the gradient at y k ) and mixing steps, where the running value of a variable is replaced by a linear combination of the other variables. However, the precise way gradient and mixing steps are coupled is rather mysterious, and the success of the proof of Theorem 1 relies heavily on the detailed structure of the iterations. In the next section, we try to gain perspective on this structure by developing a continuized version of the acceleration.", "publication_ref": ["b42"], "figure_ref": [], "table_ref": []}, {"heading": "Continuized version of Nesterov acceleration", "text": "This paper uses several mathematical notions related to random processes. The following sections expose the results from heuristic considerations of those notions, rigorously defined in Appendix C.\nWe argue that the accelerated iteration becomes more natural when considering two variables x t , z t indexed by a continuous time t 0, that are continuously mixing and that take gradient steps at random times. More precisely, let T 1 , T 2 , T 3 , . . . 0 be random times such that T 1 , T 2 \u2212 T 1 , T 3 \u2212 T 2 , . . . are independent identically distributed (i.i.d.), of law exponential with rate 1 (any constant rate would do, we choose 1 to make the comparison with discrete time k straightforward). By convention, we choose that our stochastic processes t \u2192 x t , t \u2192 z t are c\u00e0dl\u00e0g almost surely, i.e., right continuous with well-defined left-limits x t\u2212 , z t\u2212 (Definition 6 in Appendix C). Our dynamics are parametrized by functions \u03b3 t , \u03b3 t , \u03c4 t , \u03c4 t , t 0. At random times T 1 , T 2 , . . . , our sequences take gradient steps\nx T k = x T k \u2212 \u2212 \u03b3 T k \u2207f (x T k \u2212 ) ,(6)\nz T k = z T k \u2212 \u2212 \u03b3 T k \u2207f (x T k \u2212 ) .\n(7) Because of the memoryless property of the exponential distribution, in a infinitesimal time interval [t, t + dt], the variables take gradients steps with probability dt, independently of the past. Between these random times, the variables mix through a linear, translation-invariant, ordinary differential equation (ODE)\ndx t = \u03b7 t (z t \u2212 x t )dt ,(8)\ndz t = \u03b7 t (x t \u2212 z t )dt .\n(9) Following the notation of stochastic calculus, we can write the process more compactly in terms of the Poisson point measure dN (t) = k 1 \u03b4 T k (dt), which has intensity the Lebesgue measure dt,\ndx t = \u03b7 t (z t \u2212 x t )dt \u2212 \u03b3 t \u2207f (x t )dN (t) ,(10)\ndz t = \u03b7 t (x t \u2212 z t )dt \u2212 \u03b3 t \u2207f (x t )dN (t) .(11)\nBefore giving convergence guarantees for such processes, let us digress quickly on why we can expect an iteration of this form to be mathematically appealing.\nFirst, from a Markov chain indexed by a discrete time index k, one can associate the so-called continuized Markov chain, indexed by a continuous time t, that makes transition with the same Markov kernel, but at random times, with independent exponential time intervals [1]. Following this terminology, we refer to our acceleration (10)- (11) as the continuized acceleration. The continuized Markov chain is appreciated for its continuous time parameter t, while keeping many properties of the original Markov chain; similarly the continuized acceleration is arguably simpler to analyze, while performing similarly to Nesterov acceleration.\nSecond, it can also be compared with coordinate gradient descent methods, that are easier to analyze when coordinates are selected randomly rather than in an ordered way [55]. Similarly, the continuized acceleration is simpler to analyze because the gradient steps (6)- (7) and the mixing steps (8)-( 9) alternate randomly, due to the randomness of T k , k 0.\nIn analogy with Theorem 1, we give choices of parameters that lead to accelerated convergence rates, in the convex case (1) and in the strongly convex case (2). Convergence is analyzed as a function of t. As dN (t) is a Poisson point process with rate 1, t is the expected number of gradient steps done by the algorithm. Thus t is analoguous to k in Theorem 1. In the theorem below, E denotes the expectation with respect to the Poisson point process dN (t), the only source of randomness.\nTheorem 2 (Convergence of continuized Nesterov acceleration). The continuized Nesterov acceleration satisfies the following two points.", "publication_ref": ["b0", "b10", "b54", "b6", "b0", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Choose the parameters", "text": "\u03b7 t = 2 t , \u03b7 t = 0, \u03b3 t = 1 L , \u03b3 t = t 2L . Then Ef (x t ) \u2212 f (x * ) 2L z 0 \u2212 x * 2 t 2 .\n2. Assume further that f is \u00b5-strongly convex, \u00b5 > 0. Choose the constant parameters\n\u03b7 t = \u03b7 t \u2261 \u00b5 L , \u03b3 t \u2261 1 L , \u03b3 t \u2261 1 \u221a \u00b5L . Then Ef (x t ) \u2212 f (x * ) f (x 0 ) \u2212 f (x * ) + \u00b5 2 z 0 \u2212 x * 2 exp \u2212 \u00b5 L t .\nWe give an elementary sketch of proof in Appendix D.1 and a complete proof in Appendix D.2. Many authors have proposed continuous-time versions of Nesterov acceleration using differential calculus, see the numerous references in the introduction. For instance, in Su et al. [50], an ODE is obtained from Nesterov acceleration by taking the joint asymptotic where the stepsizes vanish and the number of iterates is rescaled. The resulting ODE must be discretized to be implemented; choosing the right discretization is not straightforward as it introduces stability and approximation errors that must be controlled [57,49,46].\nOn the contrary, our continuous time process ( 10)-( 11) does not correspond to a limit where the stepsizes vanish. However, in Appendix F, we check that the random continuized acceleration has the same deterministic ODE scaling limit as Nesterov acceleration. This sanity check emphasizes that the continuized acceleration is fundamentally different from previous continuous-time equivalents. Remark 1. A similar Markovian structure can be obtained in a discrete setting by flipping i.i.d. coins to trigger gradient steps. By denoting p > 0 the probability to trigger a gradient step when flipping a coin, (i) p = 1 gives the classical setting, and (ii) p \u2192 0 while renormalizing time gives our continuized framework. In fact, this setting with updates triggered randomly is an interpolation between the classical and continuized frameworks, and consists in replacing exponential random variables by geometric random variables of parameter p for the waiting-time between updates. We thus believe the convergence guarantees described here and in the following can be adapted for this discrete scheme.", "publication_ref": ["b49", "b56", "b48", "b45"], "figure_ref": [], "table_ref": []}, {"heading": "Discrete implementation of the continuized acceleration with random parameters", "text": "In this section, we show that the continuized acceleration can be implemented exactly as a discrete algorithm. This contrasts with the discretization of ODEs that introduces discretization errors; here, we compute exactl\u1ef9\nx k := x T k ,\u1ef9 k := x T k+1 \u2212 ,z k := z T k ,\nwith the convention that T 0 = 0. The three sequencesx k ,\u1ef9 k ,z k , k 0, satisfy a recurrence relation of the same structure as Nesterov acceleration, but with random weights. The resulting randomized discrete algorithm satisfies performance guarantees similar to those of Nesterov acceleration. Theorem 3 (Discrete version of continuized acceleration). For any stochastic process of the form (10)-(11), we have\u1ef9\nk =x k + \u03c4 k (z k \u2212x k ) ,(12)\nx k+1 =\u1ef9 k \u2212\u03b3 k \u2207f (\u1ef9 k ) ,(13)\nz k+1 =z k + \u03c4 k (\u1ef9 k \u2212z k ) \u2212\u03b3 k \u2207f (\u1ef9 k ) ,(14)\nfor some random parameters \u03c4 k , \u03c4 k ,\u03b3 k ,\u03b3 k (that are functions of T k , T k+1 , \u03b7 t , \u03b7 t , \u03b3 t , \u03b3 t ).\n1. For the parameters of Theorem 2.(1),\n\u03c4 k = 1 \u2212 T k T k+1 2 , \u03c4 k = 0,\u03b3 k = 1 L , and\u03b3 k = T k 2L . Then E T 2 k (f (x k ) \u2212 f (x * )) 2L z 0 \u2212 x * 2 .\n2. For the parameters of Theorem 2.(2\n), \u03c4 k = 1 2 1 \u2212 exp \u22122 \u00b5 L (T k+1 \u2212 T k ) , \u03c4 k = tanh \u00b5 L (T k+1 \u2212 T k ) ,\u03b3 k = 1 L , and\u03b3 k = 1 \u221a \u00b5L . Then E exp \u00b5 L T k (f (x k ) \u2212 f (x * )) f (x 0 ) \u2212 f (x * ) + \u00b5 2 z 0 \u2212 x * 2 .\nThe law of T k is well known: it is the sum of k i.i.d. random variables of law exponential with rate 1; this is called an Erlang or Gamma distribution with shape parameter k and rate 1. One can use well-known properties of this law, such as its concentration around its expectation ET k = k, to derive corollaries of the bounds above. The performance guarantees are proved in Appendix D.2, and the formula for the discretization is studied in E. In Appendix A.1, we provide simulations confirming that this discrete random algorithm has a performance similar to Nesterov's original acceleration.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Continuized Nesterov acceleration of stochastic gradient descent", "text": "We now investigate the design of continuized accelerations of stochastic gradient descent. We assume that we do not have direct access to the gradient \u2207f (x) but to a random estimate \u2207f (x, \u03be), where \u03be \u2208 \u039e is random of law P. In the continuized framework, the randomness of the stochastic gradient and its time mix in a particularly convenient way. For similar reasons, Latz studied stochastic gradient descent as a gradient flow on a random function that is regenerated at a Poisson rate [35]. However, this approach has the same shortcomings as the other approaches based on gradient flows: the subsequent discretization introduces non-trivial errors. We avoid this problem here.\nWe keep the algorithms of the same form, replacing gradients by stochastic gradients. Let \u03be 1 , \u03be 2 , . . . be i.i.d. random variables of law P. We take stochastic gradient steps at the random times T 1 , T 2 , . . . ,\nx T k = x T k \u2212 \u2212 \u03b3 T k \u2207f (x T k \u2212 , \u03be k ) , z T k = z T k \u2212 \u2212 \u03b3 T k \u2207f (x T k \u2212 , \u03be k ) .\nBetween these random times, the variables mix through the same ODE\ndx t = \u03b7 t (z t \u2212 x t )dt , dz t = \u03b7 t (x t \u2212 z t )dt .\nThis can be written more compactly in terms of the Poisson point measure dN (t, \u03be) =\nk 1 \u03b4 (T k ,\u03be k ) (dt, d\u03be) on R 0 \u00d7 \u039e, which has intensity dt \u2297 P, dx t = \u03b7 t (z t \u2212 x t )dt \u2212 \u03b3 t \u039e \u2207f (x t , \u03be)dN (t, \u03be) ,(15)\ndz t = \u03b7 t (x t \u2212 z t )dt \u2212 \u03b3 t \u039e \u2207f (x t , \u03be)dN (t, \u03be) .(16)\nHere, the discussion depends on the properties satisfied by the stochastic gradients \u2207f (x, \u03be). In Appendix B, we study the so-called additive noise case. We show that the continuized acceleration satisfies perturbed convergence rates with the same choices of parameters as in Theorem 2. We thus show some robustness of the above acceleration to additive noise. Instead, in this section, we focus on the so-called pure multiplicative noise case, as it is crucial for the study of asynchronous gossip that follows. In this setting, parameters need to be chosen differently for our proof technique to work. A continuized acceleration is still possible, depending on the statistical condition number.\nWe now focus on functions f is of the following form, typical to least-squares supervised learning:\n\u2200x \u2208 R d , f (x) = E (a,b)\u223cP 1 2 (b \u2212 x, a ) 2 ,(17)\nwhere \u03be = (a, b) \u2208 R d \u00d7 R is random of law P. We assume that our stochastic first order oracle is the gradient of one realization of the expectation, namely,\n\u2207f (x, \u03be) = \u2212(b \u2212 x, a )a , \u03be = (a, b) .\nWe investigate noiseless-or purely multiplicative-stochastic gradients, in the sense that almost surely, for \u03be = (a, b)\n\u223c P: b = x * , a , so that \u2207f (x * , \u03be) = 0 .(18)\nNoiseless stochastic gradients are relevant in several situations, such as coordinate gradient descent with randomly sampled coordinates [51,44,55] (where \u2207f (x, \u03be) = m \u2207f (x), e i e i with i uniformly random in {1, . . . , d}), over-parameterized regime for least squares regression [52], function interpolation and gossip algorithms [8].\nFor a symmetric non-negative matrix A and a vector x, we denote x\n2 A = x Ax. Let H = E[aa ]\nbe the Hessian of f . Let R 2 be the smallest positive real number such that:\nE a 2 aa R 2 H .(19)\nFurther, similarly to Jain et al. [30], we define the statistical condition number of the problem as the smallest\u03ba > 0 such that:\nE a 2 H \u22121 aa \u03baH .(20)\nTheorem 4 (Continuized acceleration with pure multiplicative noise). Assume that ( 18), ( 19) and (20) hold true. Then the continuized acceleration satisfies the following.", "publication_ref": ["b34", "b50", "b43", "b54", "b51", "b7", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Choose the parameters", "text": "\u03b7 t = 2 t , \u03b7 t = 0, \u03b3 t = 1 R 2 , \u03b3 t = t 2R 2\u03ba . Then 1 2 E x t \u2212 x * 2 R 2\u03ba z 0 \u2212 x * 2 H \u22121 t 2 .\n2. Assume further that f is \u00b5-strongly convex, i.e., all eigenvalues of H are greater or equal to \u00b5, where \u00b5 > 0. The condition number of f is then defined as \u03ba = R 2 /\u00b5. For the\nparameters \u03b7 t = \u03b7 t = 1 \u221a \u03ba\u03ba , \u03b3 t = 1 R 2 and \u03b3 t = 1 R 2\n\u03ba \u03ba , we have:\n1 2 E x t \u2212 x * 2 1 2 x 0 \u2212 x * 2 + \u00b5 2 z 0 \u2212 x * 2 H \u22121 exp \u2212 t \u221a \u03ba\u03ba .\nIn the strongly convex case, the benefits of this acceleration are similar to those of Jain et al. [30] with classical discrete iterates: while stochastic gradient descent with stepsize 1/R 2 is easily shown to achieve an exponential rate of convergence 1/\u03ba, the acceleration enjoys a rate of convergence of 1/ \u221a \u03ba\u03ba. Note that from the definitions,\u03ba \u03ba, thus the acceleration performs as least as well as the naive algorithm. However, depending on the distribution of a, the improvement might either be significant or null. We refer the reader to the rich discussion in Jain et al. [30] which provides insights on the interpretation of\u03ba and on the possibility to accelerate. Below, we provide a complementary perspective on the statistical condition number in the context of gossip algorithms, where it can be interpreted in terms of effective resistances of graphs.\nAlbeit more restrictive in terms of assumptions, our analysis is much simpler than that of Jain et al. [30], as it relies on a standard Lyapunov function, similar to that of the continuized acceleration (Theorem 2). In Appendix G, we use the same analysis framework to prove convergence of accelerated coordinate descent, which is another noiseless stochastic method.", "publication_ref": ["b29", "b29", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Accelerating Randomized Gossip", "text": "The continuized framework allows designing accelerated decentralized algorithms requiring synchronized clocks, but no synchronization of the communications. In this section, we illustrate this statement in the simple case of gossip algorithms; the more general case of decentralized optimization is discussed in the next section.\nLet G = (V, E) a connected graph representing a communication network of agents. Each agent v \u2208 V is assigned a real number x 0 (v) \u2208 R. The goal of the averaging (or gossip) problem is to design an iterative procedure allowing each agent of the network to know the averagex = 1 m v\u2208V x 0 (v) using only local communications, i.e., communications between adjacent agents in the network.\nWe formalize the communication model of randomized gossip [11]. Time t is indexed continuously in R 0 . We generate a Poisson point measure dN (t, e) = k 1 \u03b4 (T k ,{v k ,w k }) with intensity measure dt\u2297P, where dt is the Lebesgue measure on R 0 and P = (P {v,w} ) {v,w}\u2208E is a probability measure on the set E of edges. For k 0, T k is a time at which edge {v k , w k } is activated: adjacent nodes v k and w k can communicate and perform a pairwise update. The Poisson point measure assumption implies that edges are activated independently of one another and from the past: the activation times of edge {v, w} form a Poisson point process of intensity P {v,w} .\nTo solve the gossip problem, Boyd et al. [11] proposed the following naive strategy: each agent v \u2208 V keeps a local estimate x t (v) of the average and, upon activation of edge {v k , w k } at time T k \u2208 R 0 , the activated nodes v k , w k average their current estimates\nx T k (v k ), x T k (w k ) \u2190\u2212 x T k \u2212 (v k ) + x T k \u2212 (w k ) 2 .\nIn this section, we accelerate this naive procedure. Our strategy is to apply Section 5 as follows.\nConsider the energy function\nf (x) = {v,w}\u2208E P {v,w} 2 (x(v) \u2212 x(w)) 2 , x = (x(v)) v\u2208V . (21\n)\nThis function is convex, smooth, and writes in the form ( 17):\nf (x) = E {v,w}\u223cP 1 2 x, a {v,w} 2 , (22\n)\nwhere a {v,w} = e v \u2212 e w and (e v ) v\u2208V forms the canonical basis of R V . As in Section 5, a stochastic gradient of f is obtained by taking the gradient of one realization of the expectation, namely:\n\u2207f (x, {v, w}) = x, a {v,w} a {v,w} = \uf8f1 \uf8f2 \uf8f3 x(v) \u2212 x(w) at coordinate v, x(w) \u2212 x(v) at coordinate w, 0 at all other coordinates. (23\n)\nAs a consequence, a stochastic gradient step with stepsize 1/2 corresponds to a local averaging alongside edge {v, w}, where {v, w} \u223c P. More generally, the randomized gossip algorithm as described by Boyd et al. [11] is the stochastic gradient descent:\ndx t = \u2212 1 2 R 0 \u00d7E \u2207f (x t , {v, w})dN (t, {v, w}) . (24\n)\nUsing Section 5, we can accelerate this algorithm if we know the strong convexity parameter of f and the constants R 2 and\u03ba as defined in ( 19) and ( 20) respectively. These constants can be intepreted as graph-related quantities here. Definition 1 (Graph-related quantities). The Laplacian matrix L \u2208 R V\u00d7V of graph G with weights (P {v,w} ) {v,w}\u2208E on the edges is the matrix with entries L v,w = \u2212P {v,w} if {v, w} \u2208 E, L v,v = w\u223cv P {v,w} , and L v,w = 0 if {v, w} / \u2208 E. We denote \u00b5 gossip the second smallest eigenvalue of L, corresponding to its smallest positive eigenvalue. For {v, w} \u2208 E, let R eff (v, w) = (e(v) \u2212 e(w)) L \u22121 (e(v) \u2212 e(w)) be the effective resistance of edge {v, w}, and R max = max {v,w}\u2208E R eff (v, w) be the maximal resistance in the graph.\nThe function f is quadratic with Hessian L, and strongly convex with parameter \u00b5 gossip on the hyperplane F = {x \u2208 R V : v\u2208V x(v) =x}; hence we use the (perhaps abusive) notation \u00b5 gossip throughout. Moreover, the conditions ( 19) and ( 20) are satisfied with R 2 = 2,\u03ba = R max . These parameters being given, the accelerated stochastic gradient descent updates ( 15)-( 16) can be instantiated as follows. Each agent v \u2208 V keeps two local estimates x t (v), z t (v) ofx, initialized at x 0 (v). Upon activation of edge {v k , w k } at time T k ,\nx T k (v k ) = x T k (w k ) = x T k \u2212 (v k ) + x T k \u2212 (w k ) 2 , z T k (v k ) = z T k \u2212 (v k ) + 1 2\u00b5 gossip R max (x T k \u2212 (w k ) \u2212 x T k \u2212 (v k )) , z T k (w k ) = z T k \u2212 (w k ) + 1 2\u00b5 gossip R max (x T k \u2212 (v k ) \u2212 x T k \u2212 (w k )) .\nBetween these updates, x t (v) and z t (v) locally mix at all nodes v \u2208 V, according to the coupled ODE:\ndx t (v) = 2\u00b5 gossip R max (z t (v) \u2212 x t (v))dt, dz t (v) = 2\u00b5 gossip R max (x t (v) \u2212 z t (v))dt.\nThis algorithm is asynchronous in the sense that it does not require global synchronous operations: the mixing of local variables does not require any synchronization since parameter t \u2208 R 0 is available at all nodes independently from the number of past updates, while a local pairwise update between adjacent nodes v and w only requires a local synchronization. Theorem 5 (Accelerated randomized gossip). Let (x t (v)) v\u2208V,t 0 be generated with accelerated randomized gossip. For any t \u2208 R 0 :\nv\u2208V 1 2 E x t (v) \u2212x 2 2 v\u2208V 1 2 x 0 (v) \u2212x 2 exp \u2212 \u00b5 gossip 2R max t .\nLet \u03b8 ARG = \u00b5gossip 2Rmax be the rate of convergence of accelerated randomized gossip, and \u03b8 RG = \u00b5 gossip be the rate of convergence of randomized gossip [11]. We have \u03b8 ARG \u03b8 RG / \u221a 2. Let us exhibit scenarios over which accelerated randomized gossip gains several orders of magnitude. Denoting P min = min {v,w}\u2208E P {v,w} , Ellens et al. [22] ensures that for {v, w} \u2208 E, P min R eff (v, w) 1, so that R max P \u22121 min . Corollary 1 (Comparison with randomized gossip). Accelerated randomized gossip achieves a rate satisfying:\n\u03b8 RG P min 2 \u03b8 ARG .\nAssume furthermore that there exist some constants c > 0 such that for all {v, w} \u2208 E, P {v,w} cP min and d v + d w 2d. Then, with C = 1/ \u221a 2cd:\nC \u03b8 RG |V| \u03b8 ARG .\nAssume now for simplicity that the Poisson intensities P {v,w} are all equal to 1/|E|. Denoting |V| = m, on the cyclic and the line graph, this gives us \u03b8 ARG = \u2126(1/m 2 ) while \u03b8 RG 1/m 3 . On a d-dimensional grid, we have \u03b8 ARG = \u2126(1/m 1+1/d ) and \u03b8 RG 1/m 1+2/d . However, on graphs with unbounded degrees, no improvements are observed, as illustrated in Figure 2, Appendix A.2. In the case of the complete graph, this is expected since at least \u03b8 \u22121 RG m communications are needed to compute the average. We thus recover the same rates as Dimakis et al. [20] for the graphs they study, but generalized to any network.", "publication_ref": ["b10", "b10", "b10", "b10", "b21", "b19"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Accelerating Asynchronous Decentralized Optimization", "text": "Our continuized framework for accelerating randomized gossip can be extended to the more general problem of decentralized optimization: each node v in the network G previously defined holds a function f v : R d \u2192 R, \u00b5-strongly convex and L-smooth. Nodes of the network collaborate to solve:\nmin x\u2208R d f (x) = 1 |V| v\u2208V f v (x) .(25)\nAs in gossip averaging, only local communications are allowed. Quantities related to f v can only be computed at node v. In the case of empirical risk minimization, f v represents the empirical risk related to node v's local data. Setting f v (x) = 1 2 x \u2212 x 0 (v) 2 leads to the averaging problem previously described. Similarly to Section 6, time is indexed continuously by t in R 0 , and communications are ruled by the same Poisson point measure dN (t, e) = k 1 \u03b4 (T k ,{v k ,w k }) on R 0 \u00d7 E. Yet, we no longer assume (as in Theorem 4) that the function f is quadratic. Instead, we write a dual formulation of Problem (25) and minimize it using a continuized version of accelerated coordinate descent [45] that we present in Appendix G. This leads to an accelerated decentralized algorithm to solve (25). Our algorithm mimics the behavior of accelerated randomized gossip: a node possesses two local parameters that mix continuously through a time-independent ODE. At time T k , adjacent nodes v k and w k use their local function in order to compute gradient conjugates \u2207f * v (x(v)), \u2207f * w (x(w)). Since the local functions are not simple quadratics anymore, the stochastic gradients \u2207f (x, {v, w}) from Equation ( 26) are replaced by terms proportional to:\nG(y, {v, w}) = \uf8f1 \uf8f2 \uf8f3 \u2207f * v (y(v)) \u2212 \u2207f * w (y(w)) at coordinate v, \u2212\u2207f * v (y(v)) \u2212 \u2207f * w (y(w)) at coordinate w, 0 at all other coordinates. (26\n)\nDue to lack of space, we describe the iterations more in details in Appendix H, together with a relevant choice of parameters. The crucial point is that, similarly to the gossip averaging case, we do not require nodes to be aware of a global iteration counter. Yet, we still obtain the same convergence rate as [25], as provided by the following theorem. The same approach can be used to \"continuize\" other accelerated randomized gossip algorithms for decentralized optimization, such as ADFS [26]. Theorem 6 (Accelerated asynchronous decentralized optimization). For (x t (v)) v\u2208V = (\u2207f * v (z t (v))) v\u2208V generated by the accelerated coordinate descent on the dual of Problem (25):\nv\u2208V 1 2 E x t (v) \u2212 x * 2 C v\u2208V 1 2 x 0 (v) \u2212 x * 2 exp \u2212 \u03b8 ARG \u221a \u03ba t ,\nwhere \u03ba = \u00b5/L is an upper bound on the condition number of f , C is a constant that depends on the graph and \u03ba, and \u03b8 ARG is the rate of convergence of accelerated randomized gossip on the graph G as defined in Theorem 5 but with graph resistances are defined in a different way (see Theorem 10).", "publication_ref": ["b24", "b44", "b24", "b24", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this work, we introduced a continuized version of Nesterov's accelerated gradients. In a nutshell, the method has two sequences of iterates which take gradient steps at random times. In between gradient steps, the two sequences mix following a simple ordinary differential equation, whose parameters are picked to ensure good convergence properties of the method.\nAs compared to other continuous time models of Nesterov acceleration, a key feature of this approach is that the method can be implemented without any approximation, as the differential equation governing the mixing procedure has a simple analytical solution. A discretization of the continuized method corresponds to an accelerated gradient method with random parameters.\nContinuization strategies were introduced in the context of Markov chains [1]. Here, they allow using acceleration mechanisms in asynchronous distributed optimization, where usually agents are not aware of the total number of iterations taken so far. This is showcased in the context of asynchronous gossip algorithms. \nf(x k ) f(x * )\nFigure 1: Comparison between gradient descent, Nesterov acceleration, and the continuized version of Nesterov acceleration, on a convex function (left plots) and a strongly convex function (right plots). For the continuized acceleration, which is randomized, the results shown in the above plots correspond to a single run. In the plots below, the thick line represents the average performance over N = 1000 runs of the continuized acceleration, while the thin lines represent the 5% and 95% quantiles.\nIn Figure 1, we compare this continuized Nesterov acceleration ( 12)-( 14) with the classical Nesterov acceleration (3)-( 5) and gradient descent. In the strongly convex case (right), we run the algorithms with the parameters of Theorem 1.(2) and 3.(2) on the function\nf (x 1 , x 2 , x 3 ) = \u00b5 2 (x 1 \u2212 1) 2 + 3\u00b5 2 (x 2 \u2212 1) 2 + L 2 (x 3 \u2212 1) 2 ,\nwith \u00b5 = 10 \u22122 and L = 1. In the convex case, we run the algorithms with the parameters of Theorem 1.(1) and 3.(1) on the function\nf (x 1 , . . . , x 100 ) = 1 2 100 i=1 1 i 2 x i \u2212 1 i 2 ,\nwhich has negligible strong convexity parameter. All iterations were initialized from x 0 = z 0 = 0. ", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Simulation of Accelerated Randomized Gossip", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Robustness of the continuized Nesterov acceleration to additive noise", "text": "In this section, we study the continuized acceleration ( 15)-( 16) under stochastic gradients. We assume that our gradient estimates are unbiased, i.e.,\n\u2200x \u2208 R d , E \u03be \u2207f (x, \u03be) = \u2207f (x) ,(27)\nand has a uniformly bounded variance, i.e., there exists \u03c3 2 0 such that\n\u2200x \u2208 R d , E \u03be \u2207f (x, \u03be) \u2212 \u2207f (x) 2 \u03c3 2 . (28\n)\nThese assumptions typically hold in the additive noise model, where \u2207f (x, \u03be) = \u2207f (x) + \u03be, and \u03be \u2208 R d satisfies E\u03be = 0, E \u03be 2 \u03c3 2 . By an abuse of terminology, we say that our stochastic gradients have \"additive noise\" when ( 27) and ( 28) hold.\nWe should emphasize that similar studies of Nesterov acceleration under additive noise has been done [34,27,56,17,13,6]. Theorem 7 (Continuized acceleration with additive noise). Assume that the stochastic gradients are unbiased (27) and have a variance uniformly bounded by \u03c3 2 (28). Then the continuized acceleration (15)-( 16) satisfies the following.\n1. For the parameters of Theorem 2.(1),\nEf (x t ) \u2212 f (x * ) 2L z 0 \u2212 x * 2 t 2 + \u03c3 2 t 3L .\n2. Assume further that f is \u00b5-strongly convex, \u00b5 > 0. For the parameters of Theorem 2.(2),\nEf (x t ) \u2212 f (x * ) f (x 0 ) \u2212 f (x * ) + \u00b5 2 z 0 \u2212 x * 2 exp \u2212 \u00b5 L t + \u03c3 2 1 \u221a \u00b5L .\nThis theorem is proved in Appendix D.3.\nIn the above bounds, L is a parameter of the algorithm, that can be taken greater than the best known smoothness constant of the function f . Increasing L reduces the stepsizes of the algorithm and performs some variance reduction. If the bound \u03c3 2 on the variance is known, one can choose L optimizing the above bounds in order to obtain algorithms that adapt to additive noise.\nIn Figure 3, we run the same simulations as in Figure 1, with two differences: (1) we add isotropic Gaussian noise on the gradients, with covariance 10 \u22124 Id, and (2) we initialized algorithms at the optimum, i.e., x 0 = z 0 = x * . Initializing at the optimum enables to isolate the effect of the additive noise only. These simulations confirm Theorem 7: the noise term is (sub-)linearly increasing in the convex case and constant in the strongly convex case.\nNote that similarly to Theorem 3, one could obtain convergence bounds for the discrete implementation under the presence of additive noise.", "publication_ref": ["b33", "b26", "b55", "b16", "b12", "b5", "b26"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "C Stochastic calculus toolbox", "text": "In this appendix, we give a short introduction to the mathematical tools that we use in this paper. For more details, the reader can consult the more rigorous monographs of Jacod and Shiryaev [29], Ikeda and Watanabe [28], Le Gall [36].", "publication_ref": ["b28", "b27", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "C.1 Poisson point measures", "text": "We fix P a probability law on some space \u039e. Definition 2. A (homogenous) Poisson point measure on R 0 \u00d7 \u039e, with intensity \u03bd(dt, d\u03be) = dt \u2297 dP(\u03be), is a random measure N on R 0 \u00d7 \u039e such that\n\u2022 For any disjoint measurable subsets A and B of R 0 \u00d7\u039e, N (A) and N (B) are independent.\n\u2022 For any measurable subset A of R 0 \u00d7 \u039e, N (A) is a Poisson random variable with parameter \u03bd(A). (If \u03bd(A) = \u221e, N (A) is equal to \u221e almost surely.) Proposition 1. Let N be a Poisson point measure on R 0 \u00d7 \u039e with intensity dt \u2297 dP(\u03be).\nThere exists a decomposition dN (t, \u03be) = k 1 \u03b4 (T k ,\u03be k ) (dt, d\u03be) on R 0 \u00d7 \u039e where 0 < T 1 < T 2 < T 3 < . . . and \u03be 1 , \u03be 2 , \u03be 3 , \u2022 \u2022 \u2022 \u2208 \u039e satisfy: All algorithms are started from the optimum x * . The results shown in the above plots correspond to a single run. In the plots below, the thick line represents the average performance over N = 100 runs of each algorithm, while the thin lines represent the 5% and 95% quantiles.\n\u2022 T 1 , T 2 \u2212 T 1 , T 3 \u2212 T 2 , . . . are i.i.\n\u2022 \u03be 1 , \u03be 2 , \u03be 3 , . . . are i.i.d. of law P and independent of the T 1 , T 2 , T 3 , . . . . Definition 3. Let N be a Poisson point measure on R 0 \u00d7 \u039e with intensity dt \u2297 dP(\u03be). The filtration F t , t 0, generated by N is defined by the formula\nF t = \u03c3 (N ([0, s] \u00d7 A) , s t, A \u2282 \u039e measurable) .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.2 Martingales and supermartingales", "text": "Let (\u2126, F, P) be a probability space and F t , t 0, a filtration on this probability space.\nDefinition 4. A random process x t \u2208 R d , t 0, is adapted if for all t 0, x t is F t -measurable. An adapted process x t \u2208 R, t 0 is a martingale (resp. supermartingale) if for all 0 s t, E[x t | F s ] = x s (resp. E[x t | F s ] x s ). Definition 5. A random variable T \u2208 [0, \u221e] is a stopping time if for all t 0, {T t} \u2208 F t .\nDefinition 6. A function x t , t 0, is said to be c\u00e0dl\u00e0g if it is right continuous and for every t > 0, the limit x t\u2212 := lim s\u2192t,s<t x s exists and is finite.\nTheorem 8 (Martingale stopping theorem). Let x t , t 0, be a martingale (resp. supermartingale) with c\u00e0dl\u00e0g trajectories and uniformly integrable. Let T be a stopping time. Then EX T = X 0 (resp. EX T X 0 ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.3 Stochastic ordinary differential equation with Poisson jumps", "text": "The continuized processes are the composition of an ordinary differential equation and stochastic Poisson jumps. It is thus a piecewise-deterministic Markov process [15,16], a special case of stochastic models that do not include any diffusion term. The stochastic calculus of this class of processes is particularly intuitive: there is no Ito correction term as with diffusive processes.\nWe fix P a probability law on some space \u039e, N a Poisson point measure on R 0 \u00d7 \u039e with intensity dt \u2297 dP(\u03be), and denote F t , t 0, the filtration generated by N . Definition 7. Let b : R d \u2192 R d and G : R d \u00d7 \u039e \u2192 R d be two functions. An random process x t \u2208 R d , t 0, is said to be a solution of the equation\ndx t = b(x t )dt + \u039e G(x t , \u03be)dN (t, \u03be)\nif it is adapted, c\u00e0dl\u00e0g, and for all t 0,\nx t = x 0 + t 0 b(x s )ds + [0,t]\u00d7\u039e G(x s\u2212 , \u03be)dN (s, \u03be) .\nIf we consider the decomposition dN (t, \u03be) = k 1 \u03b4 (T k ,\u03be k ) (dt, d\u03be) given by Proposition 1, then\n[0,t]\u00d7\u039e G(x s\u2212 , \u03be)dN (s, \u03be) = k 1 1 {T k t} G(x T k \u2212 , \u03be k ) .\nHere, we consider only autonomous equations as b and G are a function of x t , but not of t. However, there is no loss of generality, one can study time-dependent systems by studying the equation in the variable (t, x t ). This trick is used in Appendix D. Moreover, we have the decomposition\n[0,t]\u00d7\u039e (\u03d5(x s\u2212 + G(x s\u2212 , \u03be)) \u2212 \u03d5(x s\u2212 )) dN (s, \u03be) = t 0 \u039e (\u03d5(x s + G(x s , \u03be)) \u2212 \u03d5(x s )) dtdP(\u03be) + M t , where M t = [0,t]\u00d7\u039e (\u03d5(x s\u2212 + G(x s\u2212 , \u03be)) \u2212 \u03d5(x s\u2212 )) (dN (s, \u03be) \u2212 dtdP(\u03be)) is a martingale.\nThis proposition is an elementary calculus of variations formula: to compute the value of the observable \u03d5(x t ), one must sum the effects of the continuous part and of the Poisson jumps. Moreover, the integral with respect to the Poisson measure N becomes a martingale if the same integral with respect to its intensity measure dt \u2297 dP(\u03be) is removed.", "publication_ref": ["b14", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "D Analysis of the continuized Nesterov acceleration", "text": "To encompass the proofs in the convex and in the strongly convex cases in a unified way, we assume f is \u00b5-strongly convex, \u00b5 0. If \u00b5 > 0, this corresponds to assuming the \u00b5-strong convexity in the usual sense; if \u00b5 = 0, it means that we only assume the function to be convex. In other words, the proofs in the convex case can be obtained by taking \u00b5 = 0 below.\nIn this section, F t , t 0, is the filtration associated to the Poisson point measure N .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.1 Sketch of proof for Theorem 2", "text": "A complete and rigorous proof is given in Appendix D.2. Here, we only provide the heuristic of the main lines of the proof.\nThe proof is similar to the one of Nesterov acceleration: we prove that for some choices of parameters \u03b7 t , \u03b7 t , \u03b3 t , \u03b3 t , t 0, and for some functions A t , B t , t 0,\n\u03c6 t = A t (f (x t ) \u2212 f (x * )) + B t 2 z t \u2212 x * 2\nis a supermartingale. In particular, this implies that E\u03c6 t is a Lyapunov function, i.e., a non-increasing function of t.\nTo prove that \u03c6 t is a supermartingale, it is sufficient to prove that for all infinitesimal time intervals [t, t + dt], E t \u03c6 t+dt \u03c6 t , where E t denotes the conditional expectation knowing all the past of the Poisson process up to time t. Thus we would like to compute the first order variation of E t \u03c6 t+dt . This implies computing the first order variation of E t f (x t+dt ).\nFrom ( 10), we see that f (x t ) evolves for two reasons between t and t + dt:\n\u2022 x t follows the linear ODE (8), which results in the infinitesimal variation f (x t ) \u2192 f (x t ) + \u03b7 t \u2207f (x t ), z t \u2212 x t dt, and \u2022 with probability dt, x t takes a gradient step, which results in a macroscopic variation\nf (x t ) \u2192 f (x t \u2212 \u03b3 t \u2207f (x t )).\nCombining both variations, we obtain that\nE t f (x t+dt ) \u2248 f (x t ) + \u03b7 t \u2207f (x t ), z t \u2212 x t dt + dt (f (x t \u2212 \u03b3 t \u2207f (x t )) \u2212 f (x t ))\n, where the dt in the second term corresponds to the probability that a gradient step happens; note that the latter event is independent of the past up to time t.\nA similar computation can be done for E t z t \u2212 x * 2 . Putting things together, we obtain\nE t \u03c6 t+dt \u2212 \u03c6 t \u2248 dt dA t dt (f (x t ) \u2212 f (x * )) + A t \u03b7 t \u2207f (x t ), z t \u2212 x t \u2212 A t (f (x t \u2212 \u03b3 t \u2207f (x t )) \u2212 f (x t )) + dB t dt 1 2 z t \u2212 x * 2 + B t \u03b7 t z t \u2212 x * , x t \u2212 z t + B t 2 z t \u2212 \u03b3 t \u2207f (x t ) \u2212 x * 2 \u2212 z t \u2212 x * 2 .\nUsing convexity and strong convexity inequalities, and a few computations, we obtain the following upper bound:\nE t \u03c6 t+dt \u2212 \u03c6 t dt dA t dt \u2212 A t \u03b7 t \u2207f (x t ), x t \u2212 x * + dB t dt \u2212 B t \u03b7 t 1 2 z t \u2212 x * 2 + (A t \u03b7 t \u2212 B t \u03b3 t ) \u2207f (x t ), z t \u2212 x * + B t \u03b7 t \u2212 dA t dt \u00b5 1 2 x t \u2212 x * 2 + B t \u03b3 2 t \u2212 A t \u03b3 t (2 \u2212 L\u03b3 t ) 1 2 \u2207f (x t ) 2 .\nWe want this infinitesimal variation to be non-positive. Here, we choose the parameters so that \u03b3 t = 1/L, and all prefactors in the above expression are zero. This gives some constraints on the choices of parameters. We show that only one degree of freedom is left: the choice of the function A t , that must satisfy the ODE\nd 2 dt 2 A t = \u00b5 4L A t ,\nbut whose initialization remains free. Once the initialization of the function A t is chosen, this determines the full function A t and, through the constraints, all parameters of the algorithm. As \u03c6 t is a supermartingale (by design), a bound on the performance of the algorithm is given by\nEf (x t ) \u2212 f (x * ) E\u03c6 t A t \u03c6 0 A t .\nThe results presented in Theorem 2 correspond to one special choice of initialization for the function A t .\nIn this sketch of proof, our derivation of the infinitesimal variation is intuitive and elementary; however it can be made more rigorous and concise-albeit more technical-using classical results from stochastic calculus, namely Proposition 2. This is our approach in Appendix D.2.\nD.2 Noiseless case: proofs of Theorem 2 and of the bounds of Theorem 3\nIn this section, we analyze the convergence of the continuized iteration ( 10)-( 11), that we recall for the reader's convenience:\ndx t = \u03b7 t (z t \u2212 x t )dt \u2212 \u03b3 t \u2207f (x t )dN (t) , dz t = \u03b7 t (x t \u2212 z t )dt \u2212 \u03b3 t \u2207f (x t )dN (t) .\nThe choices of parameters \u03b7 t , \u03b7 t , \u03b3 t , \u03b3 t , t 0, and the corresponding convergence bounds follow naturally from the analysis. We seek sufficient conditions under which the function\n\u03c6 t = A t (f (x t ) \u2212 f * ) + B t 2 z t \u2212 x * 2 is a supermartingale.\nThe processx t = (t, x t , z t ) satisfies the equation\ndx t = b(x t )dt + G(x t )dN (t) , b(x t ) = 1 \u03b7 t (z t \u2212 x t ) \u03b7 t (x t \u2212 z t ) , G(x t ) = 0 \u2212\u03b3 t \u2207f (x t ) \u2212\u03b3 t \u2207f (x t ) .\nWe thus apply Proposition 2 to \u03c6 t = \u03d5(x t ) = \u03d5(t, x t , z t ) where\n\u03d5(t, x, z) = A t (f (x) \u2212 f (x * )) + B t 2 z \u2212 x * 2 ,\nwe obtain:\n\u03c6 t = \u03c6 0 + t 0 \u2207\u03d5(x s ), b(x s ) ds + t 0 (\u03d5(x s + G(x s )) \u2212 \u03d5(x s )) ds + M t ,\nwhere M t is a martingale. Thus, to show that \u03d5 t is a supermartingale, it is sufficient to show that the map t \u2192 \nI t := \u2207\u03d5(x t ), b(x t ) + \u03d5(x t + G(x t )) \u2212 \u03d5(x t ) 0 . We now compute \u2207\u03d5(x t ), b(x t ) = \u2202 t \u03d5(x t ) + \u2202 x \u03d5(x t ), \u03b7 t (z t \u2212 x t ) + \u2202 z \u03d5(x t ), \u03b7 t (x t \u2212 z t ) = dA t dt (f (x t ) \u2212 f (x * )) + dB t dt 1 2 z t \u2212 x * 2 + A t \u03b7 t \u2207f (x t ), z t \u2212 x t + B t \u03b7 t z t \u2212 x * , x t \u2212 z t .\nHere, we use that as f is \u00b5-strongly convex,\nf (x t ) \u2212 f (x * ) \u2207f (x t ), x t \u2212 x * \u2212 \u00b5 2 x t \u2212 x * 2 ,\nand the simple bound\nz t \u2212 x * , x t \u2212 z t = z t \u2212 x * , x t \u2212 x * \u2212 z t \u2212 x * 2 z t \u2212 x * x t \u2212 x * \u2212 z t \u2212 x * 2 1 2 z t \u2212 x * 2 + x t \u2212 x * 2 \u2212 z t \u2212 x * 2 = 1 2 x t \u2212 x * 2 \u2212 z t \u2212 x * 2 .\nThis gives\n\u2207\u03d5(x t ), b(x t ) dA t dt \u2212 A t \u03b7 t \u2207f (x t ), x t \u2212 x * + B t \u03b7 t \u2212 dA t dt \u00b5 1 2 x t \u2212 x * 2(29)\n+ dB t dt \u2212 B t \u03b7 t 1 2 z t \u2212 x * 2 + A t \u03b7 t \u2207f (x t ), z t \u2212 x * .(30)\nFurther,\n\u03d5(x t + G(x t )) \u2212 \u03d5(x t ) = A t (f (x t \u2212 \u03b3 t \u2207f (x t )) \u2212 f (x t )) + B t 2 (z t \u2212 x * ) \u2212 \u03b3 t \u2207f (x t ) 2 \u2212 z t \u2212 x * 2 .\nAs f is L-smooth,\nf (x t \u2212 \u03b3 t \u2207f (x t )) \u2212 f (x t ) \u2207f (x t ), \u2212\u03b3 t \u2207f (x t ) + L 2 \u03b3 t \u2207f (x t ) 2 = \u2212\u03b3 t (2 \u2212 L\u03b3 t ) 1 2 \u2207f (x t ) 2 .\nThis gives\n\u03d5(x t + G(x t )) \u2212 \u03d5(x t ) B t \u03b3 2 t \u2212 A t \u03b3 t (2 \u2212 L\u03b3 t ) 1 2 \u2207f (x t ) 2 \u2212 B t \u03b3 t \u2207f (x t ), z t \u2212 x * .(31)\nFinally, combining ( 29)-( 30) with ( 31), we obtain\nI t dA t dt \u2212 A t \u03b7 t \u2207f (x t ), x t \u2212 x * + dB t dt \u2212 B t \u03b7 t 1 2 z t \u2212 x * 2(32)\n+ (A t \u03b7 t \u2212 B t \u03b3 t ) \u2207f (x t ), z t \u2212 x * + B t \u03b7 t \u2212 dA t dt \u00b5 1 2 x t \u2212 x * 2(33)\n+ B t \u03b3 2 t \u2212 A t \u03b3 t (2 \u2212 L\u03b3 t ) 1 2 \u2207f (x t ) 2 .(34)\nRemember that I t 0 is a sufficient condition for \u03c6 t to be a supermartingale. Here, we choose the parameters \u03b7 t , \u03b7 t , \u03b3 t , \u03b3 t , t 0, so that all prefactors are 0. We start by taking \u03b3 t \u2261 1 L (other choices \u03b3 t < 2\nL could be possible but would give similar results) and we want to satisfy\ndA t dt = A t \u03b7 t , dB t dt = B t \u03b7 t A t \u03b7 t = B t \u03b3 t , B t \u03b7 t = dA t dt \u00b5 , B t \u03b3 2 t = A t L .\nTo satisfy the last equation, we choose\n\u03b3 t = A t LB t .(35)\nTo satisfy the third equation, we choose\n\u03b7 t = B t \u03b3 t A t = 2B t LA t .(36)\nTo satisfy the fourth equation, we choose\n\u03b7 t = dA t dt \u00b5 B t = A t \u03b7 t \u00b5 B t = \u00b5 A t LB t .(37)\nHaving now all parameters \u03b7 t , \u03b7 t , \u03b3 t , \u03b3 t constrained, we now have that \u03c6 t is Lyapunov if\ndA t dt = A t \u03b7 t = A t B t L , dB t dt = B t \u03b7 t = \u00b5 A t B t L .\nThis only leaves the choice of the initialization (A 0 , B 0 ) as free: both the algorithm and the Lyapunov depend on it. (Actually, only the relative value A 0 /B 0 matters.) Instead of solving the above system of two coupled non-linear ODEs, it is convenient to turn them into a single second-order linear ODE:\nd dt A t = 1 2 \u221a A t dA t dt = 1 2 B t L , d dt B t = 1 2 \u221a B t dB t dt = \u00b5 2 A t L .(38)\nThis can also be restated as\nd 2 dt 2 A t = \u00b5 4L A t , B t = 2 \u221a L d dt A t .(39)", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "D.2.1 Proof of the first part (convex case)", "text": "We now assume \u00b5 = 0, and we choose the solution such that A 0 = 0 and B 0 = 1. From ( 38 \n\u221a A t = 1 2 \u221a L , thus \u221a A t = t 2 \u221a\nL . The parameters of the algorithm are given by ( 35)-( 37): \u03b7 t = 2 t , \u03b7 t = 0, \u03b3 t = t 2L (and we had chosen \u03b3 t = 1 L ). From the fact that \u03c6 t is a supermartingale, we obtain that the associated algorithm satisfies\nEf (x t ) \u2212 f (x * ) E\u03c6 t A t \u03c6 0 A t = 2L z 0 \u2212 x * 2 t 2 .\nThis proves the first part of Theorem 2.\nFurther, one can apply martingale stopping Theorem 8 to the supermartingale \u03c6 t with the stopping time T k to obtain\nE [A T k (f (x k ) \u2212 f (x * ))] = E [A T k (f (x T k ) \u2212 f (x * ))] E\u03c6 T k \u03c6 0 = z 0 \u2212 x * 2 .\nThis proves the formula of Theorem 3.1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.2.2 Proof of the second part (strongly convex case)", "text": "We now assume \u00b5 > 0. We consider the solution of (39) that is exponential:\nA t = A 0 exp 1 2 \u00b5 L t , B t = A 0 \u221a \u00b5 exp 1 2 \u00b5 L t .\nThe parameters of the algorithm are given by ( 35)-( 37):\n\u03b7 t = \u03b7 t = \u00b5 L , \u03b3 t = 1\n\u221a \u00b5L (and we had chosen \u03b3 t = 1 L ). From the fact that \u03c6 t is a supermartingale, we obtain that the associated algorithm satisfies\nEf (x t ) \u2212 f (x * ) E\u03c6 t A t \u03c6 0 A t = A 0 (f (x 0 ) \u2212 f (x * )) + A 0 \u00b5 2 z 0 \u2212 x * 2 A t = f (x 0 ) \u2212 f (x * ) + \u00b5 2 z 0 \u2212 x * 2 exp \u2212 \u00b5 L t .\nThis proves the second part of Theorem 2. Similarly to above, one can also apply the martingale stopping theorem to prove the formula of Theorem 3.2. Remark 2. In the above derivation, in both the convex and strongly convex cases, we choose a particular solution of (39), while several solutions are possible. In the convex case, we make the choice A 0 = 0 to have a succinct bound that does not depend on f (x 0 ) \u2212 f (x * ). More importantly, in the strongly convex case, we choose the solution that satisfies the relation \u221a \u00b5 \u221a A t = \u221a B t , which implies that \u03b7 t , \u03b7 t , \u03b3 t , are constant functions of t, and \u03b7 t = \u03b7 t . These conditions help solving in closed form the continuous part of the process\ndx t = \u03b7 t (z t \u2212 x t )dt , dz t = \u03b7 t (x t \u2212 z t )dt ,\nwhich is crucial if we want to have a discrete implementation of our method (for more details, see Theorem 3 and its proof). However, in the strongly convex case, considering other solutions would be interesting, for instance to have an algorithm converging to the convex one as \u00b5 \u2192 0.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.3 With additive noise: proof of Theorem 7", "text": "The proof of this theorem is along the same lines as the proof of Theorem 2 above. Here, we only give the major differences.\nWe analyze the convergence of the continuized stochastic iteration (15)-( 16), that we recall for the reader's convenience:\ndx t = \u03b7 t (z t \u2212 x t )dt \u2212 \u03b3 t \u039e \u2207f (x t , \u03be)dN (t, \u03be) , dz t = \u03b7 t (x t \u2212 z t )dt \u2212 \u03b3 t \u039e \u2207f (x t , \u03be)dN (t, \u03be) .\nIn this setting, we loose the property that\n\u03c6 t = A t (f (x t ) \u2212 f * ) + B t 2 z t \u2212 x * 2\nis a supermartingale. However, we bound the increase of \u03c6 t .\nThe processx t = (t, x t , z t ) satisfies the equation\ndx t = b(x t )dt + \u039e G(x t , \u03be)dN (t, \u03be), b(x t ) = 1 \u03b7 t (z t \u2212 x t ) \u03b7 t (x t \u2212 z t ) , G(x t , \u03be) = 0 \u2212\u03b3 t \u2207f (x t , \u03be) \u2212\u03b3 t \u2207f (x t , \u03be) .\nWe apply Proposition 2 to \u03c6 t = \u03d5(x t ) = \u03d5(t, x t , z t ) and obtain\n\u03c6 t = \u03c6 0 + t 0 I s ds + M t ,(40)\nwhere M t is a martingale and\nI t = \u2207\u03d5(x t ), b(x t ) + E \u03be \u03d5(x t + G(x t , \u03be)) \u2212 \u03d5(x t ) .\nThe computation of the first term remains the same: the inequality ( 29)-( 30) holds. The computation of the second term becomes\nE \u03be \u03d5(x t + G(x t , \u03be)) \u2212 \u03d5(x t ) = A t (E \u03be f (x t \u2212 \u03b3 t \u2207f (x t , \u03be)) \u2212 f (x t )) + B t 2 E \u03be (z t \u2212 x * ) \u2212 \u03b3 t \u2207f (x t , \u03be) 2 \u2212 z t \u2212 x * 2 .\nAs f is L-smooth,\nf (x t \u2212 \u03b3 t \u2207f (x t , \u03be)) \u2212 f (x t ) \u2207f (x t ), \u2212\u03b3 t \u2207f (x t , \u03be) + L 2 \u03b3 t \u2207f (x t , \u03be) 2 , E \u03be f (x t \u2212 \u03b3 t \u2207f (x t , \u03be)) \u2212 f (x t ) \u2207f (x t ), \u2212\u03b3 t E \u03be \u2207f (x t , \u03be) + L 2 E \u03be \u03b3 t \u2207f (x t , \u03be) 2 .\nBy assumptions ( 27) and ( 28), the stochastic gradient \u2207f (x, \u03be) is unbiased and has a variance bounded by \u03c3 2 , which implies E \u03be \u2207f (x t , \u03be) 2 \u2207f (x t ) 2 + \u03c3 2 . Thus\nE \u03be f (x t \u2212 \u03b3 t \u2207f (x t , \u03be)) \u2212 f (x t ) \u2212\u03b3 t (2 \u2212 L\u03b3 t ) 1 2 \u2207f (x t ) 2 + \u03c3 2 L\u03b3 2 t 2 .\nSimilarly,\nE \u03be (z t \u2212 x * ) \u2212 \u03b3 t \u2207f (x t , \u03be) 2 \u2212 z t \u2212 x * 2 = \u22122\u03b3 t E \u03be \u2207f (x t , \u03be), z t \u2212 x * + \u03b3 2 t E \u03be \u2207f (x t , \u03be) 2 \u22122\u03b3 t \u2207f (x t ), z t \u2212 x * + \u03b3 2 t \u2207f (x t ) 2 + \u03c3 2 \u03b3 2 t . This gives \u03d5(x t + G(x t )) \u2212 \u03d5(x t ) B t \u03b3 2 t \u2212 A t \u03b3 t (2 \u2212 L\u03b3 t ) 1 2 \u2207f (x t ) 2 \u2212 B t \u03b3 t \u2207f (x t ), z t \u2212 x * + \u03c3 2 2 A t L\u03b3 2 t + B t \u03b3 2 t .\nCombining the bounds, we obtain\nI t dA t dt \u2212 A t \u03b7 t \u2207f (x t ), x t \u2212 x * + dB t dt \u2212 B t \u03b7 t 1 2 z t \u2212 x * 2 + (A t \u03b7 t \u2212 B t \u03b3 t ) \u2207f (x t ), z t \u2212 x * + B t \u03b7 t \u2212 dA t dt \u00b5 1 2 x t \u2212 x * 2 + B t \u03b3 2 t \u2212 A t \u03b3 t (2 \u2212 L\u03b3 t ) 1 2 \u2207f (x t ) 2 + \u03c3 2 2 A t L\u03b3 2 t + B t \u03b3 2 t ,\nwhich is an additive perturbation of the bound ( 32)- (34) in the noiseless case, with a perturbation proportional to \u03c3 2 . The choices of parameters of Theorem 2 cancel all first five prefactors, and satisfy \u03b3 t = 1 L , A t L\u03b3 2 t = B t \u03b3 2 t . We thus obtain\nI t \u03c3 2 A t L .\nThis bound controls the increase of \u03c6 t . Using the decomposition (50), we obtain\nEf (x t ) \u2212 f (x * ) E\u03c6 t A t \u03c6 0 A t + t 0 EI s ds A t A 0 (f (x 0 ) \u2212 f (x * )) + B 0 z 0 \u2212 x * 2 A t + \u03c3 2 L t 0 A s ds A t . D.\n3.1 Proof of the first part (convex case)\nIn this case, A t = t 2 2L and B 0 = 1. Thus\nt 0 A s ds = 1 2L t 3 3 . Thus Ef (x t ) \u2212 f (x * ) 2L z 0 \u2212 x * 2 t 2 + \u03c3 2 t 3L .\nD.3.2 Proof of the second part (strongly convex case)\nIn this case, A t = A 0 exp \u00b5 L t and B 0 = A 0 \u00b5 2 . Thus\nt 0 A s ds A 0 \u00b5 L \u22121 exp \u00b5 L t = L \u00b5 A t . Thus Ef (x t ) \u2212 f (x * ) f (x 0 ) \u2212 f (x * ) + \u00b5 2 z 0 \u2212 x * 2 exp \u2212 \u00b5 L t + \u03c3 2 1 \u221a \u00b5L .\nD.4 With Pure Multiplicative Noise: Proof of Theorem 4\nThe proof of this theorem mimics the proof of Theorem 2, with a slightly different Lyapunov function.\nWe recall that in Section 5, the function f is of the form:\n\u2200x \u2208 R d , f (x) = E 1 2 ( a, x \u2212 b) 2 ,\nwhere \u03be = (a, b) \u2208 R d \u00d7 R is of law P. Thanks to the noiseless assumption, for H = E aa , we also have:\n\u2200x \u2208 R d , f (x) = 1 2 x \u2212 x * 2\nH . The Lyapunov function studied in the proof of Theorem 2 would then write as, for t \u2208 R 0 :\n\u03c6 t = A t 2 x t \u2212 x * 2 H + B t 2 z t \u2212 x * 2 .\nAn acceleration of stochastic gradient descent using this Lyapunov function has been done by Vaswani et al. [52]. In order to have an analysis similar to Nesterov acceleration, the authors make a strong growth condition, which is too strong for many stochastic gradient problems and for our application to gossip algorithms. Instead, our analysis requires a bounded statistical condition number\u03ba, and performs a shift in terms of dependency over H:\nx \u2212 x * 2 H becomes x \u2212 x * 2 , and z t \u2212 x * 2 becomes z t \u2212 x * 2 H \u22121 .\nThe new Lyapunov function writes:\n\u03c6 t = A t 2 x t \u2212 x * 2 + B t 2 z t \u2212 x * 2 H \u22121 .\nAs in Theorem 2, the proof consists in proving that for carefully chosen parameters, \u03c6 t is a supermatingale. The processx t = (t, x t , z t ) satisfies the equation\ndx t = b(x t )dt + \u039e G(x t , \u03be)dN (t, \u03be), b(x t ) = 1 \u03b7 t (z t \u2212 x t ) \u03b7 t (x t \u2212 z t ) , G(x t , \u03be) = 0 \u2212\u03b3 t \u2207f (x t , \u03be) \u2212\u03b3 t \u2207f (x t , \u03be) .\nWe apply Proposition 2 to \u03c6 t = \u03d5(x t ) = \u03d5(t, x t , z t ) and obtain: \n\u03c6 t = \u03c6 0 + t 0 I s ds + M t ,\ndx t = \u03b7 t (z t \u2212 x t )dt = \u00b5 L (z t \u2212 x t )dt , dz t = \u03b7 t (x t \u2212 z t )dt = \u00b5 L (x t \u2212 z t )dt ,\ncan also be integrated in closed form: for t t 0 ,\nx t = x t0 + z t0 2 + x t0 \u2212 z t0 2 exp \u22122 \u00b5 L (t \u2212 t 0 ) = x t0 + 1 2 1 \u2212 exp \u22122 \u00b5 L (t \u2212 t 0 ) (z t0 \u2212 x t0 ) , z t = x t0 + z t0 2 + z t0 \u2212 x t0 2 exp \u22122 \u00b5 L (t \u2212 t 0 ) = z t0 + 1 2 1 \u2212 exp \u22122 \u00b5 L (t \u2212 t 0 ) (x t0 \u2212 z t0 ) .\nIn particular, taking t 0 = T k , t = T k+1 \u2212, we obtain \u03c4\nk = \u03c4 k = 1 2 1 \u2212 exp \u22122 \u00b5 L (T k+1 \u2212 T k ) and thus \u03c4 k = \u03c4 k 1\u2212\u03c4 k = tanh \u00b5 L (T k+1 \u2212 T k ) . Fi- nally,\u03b3 k = \u03b3 T k = 1 L and\u03b3 k = \u03b3 T k = 1 \u221a \u00b5L .\nF Heuristic ODE scaling limit of the continuized acceleration", "publication_ref": ["b33", "b51"], "figure_ref": [], "table_ref": []}, {"heading": "F.1 Convex case", "text": "With the choices of parameters of Theorem 2.(1), the continuized acceleration is\ndx t = 2 t (z t \u2212 x t )dt \u2212 1 L \u2207f (x t )dN (t) , dz t = \u2212 t 2L \u2207f (x t )dN (t) .\nThe ODE scaling limit is obtained by taking the limit L \u2192 \u221e (so that the stepsize 1/L vanishes) and rescaling the time s = t/ \u221a L. Some law of large number argument heuristically gives us that, as L \u2192 \u221e, dN (t) = dN ( \u221a Ls) \u2248 \u221a Lds. Thus in the limit, we obtain\ndx s = 2 \u221a Ls (z s \u2212 x s ) \u221a Lds \u2212 1 L \u2207f (x s ) \u221a Lds , dz s = \u2212 \u221a Ls 2L \u2207f (x s ) \u221a Lds .\nThe second term of the first equation becomes negligible in the limit. Thus the equations simplify to This is the same limiting ODE as the one found by Su et al. [50] for Nesterov acceleration.\ndx s ds = 2 s (z s \u2212 x s ) , dz s ds = \u2212 s 2 \u2207f (x s ) .", "publication_ref": ["b49"], "figure_ref": [], "table_ref": []}, {"heading": "F.2 Strongly-convex case", "text": "With the choices of parameters of Theorem 2.(2), the continuized acceleration is\ndx t = \u00b5 L (z t \u2212 x t )dt \u2212 1 L \u2207f (x t )dN (t) , dz t = \u00b5 L (x t \u2212 z t )dt \u2212 1 \u221a \u00b5L \u2207f (x t )dN (t) .\nAgain, we take joint scaling L \u2192 \u221e, s = t/ \u221a L, with the approximation dN (t) \u2248 \u221a Lds. We obtain\ndx s = \u00b5 L (z s \u2212 x s ) \u221a Lds \u2212 1 L \u2207f (x s ) \u221a Lds , dz s = \u00b5 L (x s \u2212 z s ) \u221a Lds \u2212 1 \u221a \u00b5L \u2207f (x s ) \u221a Lds .\nAs before, the second term of the first equation becomes negligible in the limit. Thus the equations simplify to\ndx s ds = \u221a \u00b5(z s \u2212 x s ) ,(44)\ndz s ds = \u221a \u00b5(x s \u2212 z s ) \u2212 1 \u221a \u00b5 \u2207f (x s ) .(45)\nFrom ( 44), we have z s = x s + 1 \u221a \u00b5 dxs ds , and by substitution in (45), we obtain\nd 2 x s ds 2 + 2 \u221a \u00b5 dx s ds + \u2207f (x s ) = 0 .\nThis is the so-called \"low-resolution\" ODE for Nesterov acceleration of Shi et al. [48].", "publication_ref": ["b44", "b47"], "figure_ref": [], "table_ref": []}, {"heading": "G Continuized Accelerated Coordinate Descent with arbitrary sampling", "text": "In this section, we focus on the following problem:\nmin x\u2208R d f (x), (46\n)\nwhere f is of the form f : x \u2192 g(Rx) for some function g and projector R \u2208 R d\u00d7d (such that R 2 = R). We further assume that f is smooth with respect to some matrix M \u2208 R d\u00d7d and \u00b5-strongly convex with respect to R, i.e.:\n\u00b5 2 x \u2212 y 2 R f (x) \u2212 f (y) \u2212 \u2207f (x) (x \u2212 y) 1 2 x \u2212 y 2 M .\nNote that \u00b5 can be equal to zero, but convergence will be slower in this case. We analyze the convergence of the following continuized coordinate descent iteration:\ndx t = \u03b7 t (z t \u2212 x t )dt \u2212 \u03b3 t \u039e R \u03be\u03be P \u03be \u2207f (x t , \u03be)dN (t, \u03be) , dz t = \u03b7 t (x t \u2212 z t )dt \u2212 \u03b3 t \u039e \u2207f (x t , \u03be)dN (t, \u03be) ,(47)\nwhere\n\u2207f (x t , \u03be) = 1 P \u03be \u2207 \u03be f (x t ),(48)\nwith the coordinate gradient \u2207 \u03be f (x t ) = e \u03be e \u03be \u2207f (x t ), with e \u03be \u2208 R d the unit vector associated with coordinate \u03be \u2208 {1, . . . , d} and P \u03be and dN are defined as in Section 6. Note that these iterations are slightly different from the previous stochastic gradient iteration since the stochastic gradient is not the same for x t and z t (same direction but different magnitudes). The following theorem is a continuized version of Hendrikx et al. [25], which is itself largely based on Nesterov and Stich [45].\nTheorem 9 (Continuized acceleration of coordinate descent). Assume that the stochastic gradients are of the coordinate descent form (48). Besides, choose parameter L such that:\nL max \u03be\u2208\u039e M \u03be\u03be R \u03be\u03be P 2 \u03be .(49)\nThen the continuized acceleration (60) satisfies the following:\n1. For \u03b7 t = 2 t , \u03b7 t = 0, \u03b3 t = 1 L , \u03b3 t = t 2L , Ef (x t ) \u2212 f (x * ) 2L z 0 \u2212 x * 2 R t 2 .\n2. Assume further that \u00b5 > 0 and choose the constant parameters \u03b7\nt = \u03b7 t \u2261 \u00b5 L , \u03b3 t \u2261 1 L , \u03b3 t \u2261 1 \u221a \u00b5L . Then , Ef (x t ) \u2212 f (x * ) f (x 0 ) \u2212 f (x * ) + \u00b5 2 z 0 \u2212 x * 2 R exp \u2212 \u00b5 L t .\nProof. Similarly to the proof in Appendix D.3, the proof of this theorem is along the same lines as the proof of Theorem 2, and we only highlight the major differences. The processx t = (t, x t , z t ) satisfies the equation\ndx t = b(x t )dt + \u039e G(x t , \u03be)dN (t, \u03be), b(x t ) = 1 \u03b7 t (z t \u2212 x t ) \u03b7 t (x t \u2212 z t ) , G(x t , \u03be) = \uf8eb \uf8ed 0 \u2212\u03b3 t R \u03be\u03be P \u03be \u2207f (x t , \u03be) \u2212\u03b3 t \u2207f (x t , \u03be) \uf8f6 \uf8f8 .\nWe also consider a slightly different Lyapunov function \u03c6 t that takes into account the projector R:\n\u03c6 t = A t (f (x t ) \u2212 f * ) + B t 2 z t \u2212 x * 2 R\nThis change of norm is essential to take into account the fact that f is not strongly convex with respect to the euclidean norm, but only with respect to \u2022 R . We apply Proposition 2 to \u03c6 t = \u03d5(x t ) = \u03d5(t, x t , z t ) and obtain\n\u03c6 t = \u03c6 0 + t 0 I s ds + M t ,(50)\nwhere M t is a martingale and\nI t = \u2207\u03d5(x t ), b(x t ) + E \u03be \u03d5(x t + G(x t , \u03be)) \u2212 \u03d5(x t ) .\nThe computation of the first term remains the same: the inequality ( 29)-( 30) holds. The computation of the second term becomes\nE \u03be \u03d5(x t + G(x t , \u03be)) \u2212 \u03d5(x t ) = A t E \u03be f x t \u2212 \u03b3 t R \u03be\u03be P \u03be \u2207f (x t , \u03be) \u2212 f (x t ) + B t 2 E \u03be (z t \u2212 x * ) \u2212 \u03b3 t \u2207f (x t , \u03be) 2 R \u2212 z t \u2212 x * 2 R .\nAs f is M -smooth,\nf x t \u2212 \u03b3 t R \u03be\u03be P \u03be \u2207f (x t , \u03be) \u2212 f (x t ) \u2207f (x t ), \u2212\u03b3 t R \u03be\u03be P \u03be \u2207f (x t , \u03be) + 1 2 \u03b3 t R \u03be\u03be P \u03be \u2207f (x t , \u03be) 2 M .\nIn the additive case, the variance is bounded by \u03c3 2 . In this case, we have that:\nR \u03be\u03be P \u03be \u2207f (x t , \u03be) 2 M = M \u03be\u03be R \u03be\u03be P 2 \u03be \u2207f (x t , \u03be) 2 R L \u2207f (x t , \u03be) 2 R ,(51)\nand similarly:\n\u2207f (x t ), \u2212\u03b3 t R \u03be\u03be P \u03be \u2207f (x t , \u03be) = \u2212\u03b3 t R \u03be\u03be P 2 \u03be \u2207 \u03be f (x t ) 2 = \u03b3 t \u2207f (x t , \u03be) 2 R .(52)\nThus:\nE \u03be f x t \u2212 \u03b3 t R \u03be\u03be P \u03be \u2207f (x t , \u03be) \u2212 f (x t ) \u03b3 t (1 \u2212 \u03b3 t L)E \u03be \u2207f (x t , \u03be) 2 R .\nSimilarly, thanks to the unbiasedness of \u2207f (x t , \u03be),\nE \u03be (z t \u2212 x * ) \u2212 \u03b3 t \u2207f (x t , \u03be) 2 R \u2212 z t \u2212 x * 2 R = \u22122\u03b3 t E \u03be R\u2207f (x t , \u03be), z t \u2212 x * + \u03b3 2 t E \u03be \u2207f (x t , \u03be) 2 R \u22122\u03b3 t \u2207f (x t ), z t \u2212 x * + \u03b3 2 t E \u03be \u2207f (x t , \u03be) 2 R . This gives \u03d5(x t + G(x t )) \u2212 \u03d5(x t ) \u2212B t \u03b3 t \u2207f (x t ), z t \u2212 x * + B t \u03b3 2 t \u2212 A t \u03b3 t (2 \u2212 L\u03b3 t ) 1 2 E \u03be \u2207f (x t , \u03be) 2 R .\nCombining the bounds, we obtain\nI t dA t dt \u2212 A t \u03b7 t \u2207f (x t ), x t \u2212 x * + dB t dt \u2212 B t \u03b7 t 1 2 z t \u2212 x * 2 R + (A t \u03b7 t \u2212 B t \u03b3 t ) \u2207f (x t ), z t \u2212 x * + B t \u03b7 t \u2212 dA t dt \u00b5 1 2 x t \u2212 x * 2 R + B t \u03b3 2 t \u2212 A t \u03b3 t (2 \u2212 L\u03b3 t ) 1 2 E \u03be \u2207f (x t , \u03be) 2 R .\nWe see that we obtain a result that is very similar to that of the deterministic case. The choices of parameters of Theorem 9 cancel all first five prefactors, and satisfy \u03b3 t = 1 L , A t L\u03b3 2 t = B t \u03b3 2 t . We thus obtain I t 0 and so \u03c6 t is a supermartingale, and the rest follows as in Appendix D.2.", "publication_ref": ["b24", "b44", "b47"], "figure_ref": [], "table_ref": []}, {"heading": "H Accelerated Decentralized Optimization with Randomized Gossip", "text": "Communications.\nWe now consider the setting of decentralized optimization considered in Section 7. More specifically, recall that we wish to solve:\nmin x\u2208R d f (x) = 1 |V| v\u2208V f v (x) ,(53)\nwhere the function f v is privately held by node v \u2208 V. To solve this problem, a classical approach is to use a dual formulation [47,25]. We first rewrite Problem (53) as:\nmin X\u2208R |V|\u00d7d , Xu=Xv \u2200{u,v}\u2208E F (X) = 1 |V| v\u2208V f v (X v ) ,(54)\nwhere X v \u2208 R d corresponds to the local parameter of node v, and the equality constraints ensures equivalence between (53) and (54). The constraints are linear and can be expressed in matrix form as:\nA X = 0,(55)\nwith A \u2208 R E\u00d7V such that ker(A ) = Span(1, ..., 1) the constant vector. The natural choice for matrix A is to choose a square root of the Laplacian matrix of graph G. For (e v ) v\u2208V and (e {v,w} ) {v,w}\u2208E the canonical bases of R V and R E , A is thus that for any {v, w} \u2208 E: Ae {v,w} = P {v,w} (e v \u2212 e w ).\nMatrix A then satisfies AA = L the Laplacian matrix of graph G with weights P {v,w} . Indeed, if W {v,w} = P {v,w} (e v \u2212 e w )(e v \u2212 e w ) corresponds to the gossip matrix for edge {v, w}, A is such that:\nAA = {v,w}\u2208E W {v,w} = L.\nThen, introducing Lagrange multipliers \u03bb, we obtain through Lagrangian duality that Problem ( 53) is equivalent to: max \u03bb\u2208R E\u00d7d \u2212F * (A\u03bb),\nwith F * the convex conjugate of F . Following the approach of Hendrikx et al. [25], we then apply Accelerated Coordinate Descent to this dual problem. Yet, we use the continuized version of Theorem 9, which allows us to remove the global iterations counter on which previous approaches rely. We see that Problem (57) has exactly the right form to apply Theorem 9, leading to the following dual iterations: \nd\u03bb (y) t = \u03b7 t (\u03bb(\nwhere P = A \u2020 A with A \u2020 is the pseudo-inverse of A, R {v,w} = e {v,w} A \u2020 Ae {v,w} . Now, we multiply these iterations by A on the left (which is standard), and we rewrite them with the following iterates:\ny t = A\u03bb (y) t , z t = A\u03bb (z) t .(59)\nNote that y t , z t \u2208 R |V|\u00d7d , and are thus variables associated with nodes of the graph. \nwhere we recall that W {v,w} = P {v,w} (e v \u2212 e w )(e v \u2212 e w ) corresponds to the gossip matrix for edge {v, w}. Besides, the dual gradients \u2207F * (y t ) are such that e v \u2207F * (y t ) = \u2207f * v (e v y t ), and so each component can be computed locally at node v.\nIn summary, the distributed decentralized algorithm writes as follows. Upon activation of edge {v k , w k } at time T k , G {v k ,w k } (T k ) = \u03c9 {v k ,w k } \u2207f * ((y\nT \u2212 k ) v k ) \u2212 \u2207f * ((y T \u2212 k ) w k ) y T k (v k ) = y T \u2212 k (v k ) \u2212 \u03b3 t R {v k ,w k } P 2 {v k ,w k } G {v k ,w k } (T k ) , y T k (w k ) = y T \u2212 k (w k ) + \u03b3 t R {v k ,w k } P 2 {v k ,w k } G {v k ,w k } (T k ) , z T k (v k ) = z T \u2212 k (v k ) \u2212 \u03b3 t G {v k ,w k } (T k ) , z T k (w k ) = z T \u2212 k (w k ) + \u03b3 t G {v k ,w k } (T k ) .(61)\nBetween these updates, y t (v) and z t (v) locally mix at all nodes v \u2208 V, according to the coupled ODE:\ndy t (v) = \u03b7 t (z t (v) \u2212 y t (v))dt, dz t (v) = \u03b7 t (y t (v) \u2212 z t (v))dt.\nThis algorithm can be implemented with local computations and pairwise communications only, since an update along edge {v, w} only involves the parameters and functions of nodes v and w. In order to fully describe this algorithm, we need to specify the various parameters. We do so, with the corresponding rate of convergence, in the following theorem. Theorem 10 (Asynchronous Accelerated Decentralized Optimization). Assume that each f v is \u00b5-strongly-convex with \u00b5 > 0 and L-smooth. Let L dual = 1 \u00b5 max {v,w} R {v,w} P {v,w} , where we recall that R {v,w} = (A \u2020 A) {v,w},{v,w} . Then, let \u03b8 ARG = \u00b5 gossip / max {v,w} R {v,w} P {v,w} where \u00b5 gossip is the smallest non-zero eigenvalue of the Laplacian of the graph G, and \u03ba = L/\u00b5 is a bound on the condition number of f . We choose the constant parameters \u03b7 t = \u03b7 t \u2261 \u03b8 ARG \u221a \u03ba , \u03b3 t \u2261 1 L dual , \u03b3 t \u2261 L \u00b5gossipL dual . The iterates produced by the algorithm described in (61) verify: Note that \u03b8 ARG is slightly different from \u03b8 ARG . Yet, following Hendrikx et al. [25], an equivalent of Corollary 1 can be obtained for \u03b8 ARG . To obtain Theorem 6, we simply choose \u03bb (y) 0 = \u03bb (z) 0 and bound the dual function suboptimality by the distance to optim using the smoothness and strong convexity of F * .\nE\nWe stress the fact that the accelerated algorithm described in this section, as well as accelerated randomized gossip in Section 6, are decentralized and asynchronous: operations are local and do not require any global synchronization, provided that a continuous time clock can be shared. This is possible only thanks to the continuized framework. However, there are some limitations: even if these algorithms are the first to achieve these rates without any global synchronization, computations and communications are here assumed to happen instantly, or to take a negligible time. Handling communication and computation physical capacity constraints such as delays or node/edge overloads in our algorithms as in [23] combined with accelerated schemes is left for future works.\nProof. First note that the Hessian of the dual objective writes for some \u03bb \u2208 R |E|\u00d7d :\nA \u2207 2 F * (A\u03bb)A 1 L A A,(62)\nsince F * is L \u22121 strongly-convex when F is L-smooth [31]. Thus, the dual objective is \u00b5 gossip /L strongly convex on the orthogonal of the kernel of A. Similarly, the smoothness of the dual objective in direction {v, w} is equal to: \nM\nThen, the result follows directly from applying Theorem ( 9), together with the smoothness of the dual gradients, since:\nE v\u2208V 1 2 \u2207f * v (z t (v)) \u2212 x 2 E 1 2\u00b5 A\u03bb (z) t \u2212 A\u03bb 2 \u03bb max (AA ) 2\u00b5 E \u03bb (z) t \u2212 \u03bb 2 R . (65\n)\nNote that the primal parameter that we are interested in is x t = \u2207f * (z t ), and not y t or z t which are dual parameters.", "publication_ref": ["b46", "b24", "b52", "b53", "b24", "b24", "b22", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgements: The authors thank Sam Power for pointing out the class of piecewise deterministic Markov processes and related references, and an anonymous reviewer for suggesting Remark 1. This work was funded in part by the French government under management of Agence Nationale de la Recherche as part of the \"Investissements d'avenir\" program, reference ANR-19-P3IA-0001(PRAIRIE 3IA Institute). We also acknowledge support from the European Research Council (grant SEQUOIA 724063), from the DGA, and from the MSR-INRIA joint centre.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "where M t is a martingale and I t = \u2207\u03d5(x t ), b(x t ) + E \u03be \u03d5(x t + G(x t , \u03be)) \u2212 \u03d5(x t ) .\nSince the Lyapunov function is not the same, we need to explicit here each term. The first term writes:\nMimicking the proof of Theorem 2, we write 1 2\nThen, expanding and taking expectation over \u03be of the first term:\nwhere we used the definition of R 2 in Equation ( 19):\nThe second term writes:\nwhere we used the definition of\u03ba in Equation ( 20):\nCombining these inequalities gives the following upper-bound on I t :\nSince I t 0 is still a sufficient condition for \u03c6 t to be a supermartingale, we choose parameters such that all prefactors are equal to 0. We first take \u03b3 t = 1 R 2 , and we want to satisfy:\nA t \u03baR 2 . To satisfy that last equality, we choose:\nThe rest of the proof then follows just as in the proof of Theorem D.2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E Proof of Theorem 3", "text": "between T k and T k+1 \u2212, we obtain that there exists \u03c4 k , \u03c4 k , such that\nFrom the first equation, we havex k = 1 1\u2212\u03c4 k (\u1ef9 k \u2212 \u03c4 kzk ), which gives by substitution in the second equation,\nwhere \u03c4 k = \u03c4 k 1\u2212\u03c4 k . Further, from ( 6)- (7), we obtain the equations\nThe stated equation ( 12)-( 14) are the combination of ( 41), ( 42) and (43).\n1. The parameters of Theorem 2.(1) are \u03b7 t = 2 t , \u03b7 t = 0, \u03b3 t = 1 L and \u03b3 t = t 2L . In this case, the ODE\ncan be integrated in closed form: for t t 0 ,\nIn particular, taking t 0 = T k , t = T k+1 \u2212, we obtain \u03c4 k = 1 \u2212 T k T k+1", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "2", "text": ", \u03c4 k = 0 and thus", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Reversible markov chains and random walks on graphs", "journal": "", "year": "2002", "authors": "David Aldous; James Allen Fill"}, {"ref_id": "b1", "title": "Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent", "journal": "", "year": "2017", "authors": "Zeyuan Allen; - Zhu; Lorenzo Orecchia"}, {"ref_id": "b2", "title": "On lower and upper bounds in smooth and strongly convex optimization", "journal": "Journal of Machine Learning Research", "year": "2016", "authors": "Yossi Arjevani; Shai Shalev-Shwartz; Ohad Shamir"}, {"ref_id": "b3", "title": "Fast convergence of inertial dynamics and algorithms with asymptotic vanishing viscosity", "journal": "Mathematical Programming", "year": "2018", "authors": "Hedy Attouch; Zaki Chbani; Juan Peypouquet; Patrick Redont"}, {"ref_id": "b4", "title": "Rate of convergence of the Nesterov accelerated gradient method in the subcritical case \u03b1 3. ESAIM: Control, Optimisation and Calculus of Variations", "journal": "", "year": "2019", "authors": "Hedy Attouch; Zaki Chbani; Hassan Riahi"}, {"ref_id": "b5", "title": "Robust accelerated gradient methods for smooth strongly convex functions", "journal": "SIAM Journal on Optimization", "year": "2020", "authors": "Alireza Necdet Serhat Aybat; Mert Fallah; Asuman Gurbuzbalaban;  Ozdaglar"}, {"ref_id": "b6", "title": "Accelerated gossip in networks of given dimension using jacobi polynomial iterations", "journal": "SIAM Journal on Mathematics of Data Science", "year": "2020", "authors": "Rapha\u00ebl Berthier; Francis Bach; Pierre Gaillard"}, {"ref_id": "b7", "title": "Tight nonparametric convergence rates for stochastic gradient descent under the noiseless linear model", "journal": "", "year": "2020", "authors": "Rapha\u00ebl Berthier; Francis Bach; Pierre Gaillard"}, {"ref_id": "b8", "title": "", "journal": "", "year": "2018", "authors": "Michael Betancourt; Michael Jordan; Ashia Wilson"}, {"ref_id": "b9", "title": "Optimization methods for large-scale machine learning", "journal": "SIAM Review", "year": "2018", "authors": "L\u00e9on Bottou; E Frank; Jorge Curtis;  Nocedal"}, {"ref_id": "b10", "title": "Randomized gossip algorithms", "journal": "IEEE Transactions on Information Theory", "year": "2006", "authors": "Stephen Boyd; Arpita Ghosh; Balaji Prabhakar; Devavrat Shah"}, {"ref_id": "b11", "title": "A geometric alternative to Nesterov's accelerated gradient descent", "journal": "", "year": "2015", "authors": "S\u00e9bastien Bubeck; Yin Tat Lee; Mohit Singh"}, {"ref_id": "b12", "title": "On acceleration with noisecorrupted gradients", "journal": "PMLR", "year": "2018", "authors": "Michael Cohen; Jelena Diakonikolas; Lorenzo Orecchia"}, {"ref_id": "b13", "title": "Acceleration methods", "journal": "", "year": "2021", "authors": "Damien Alexandre D'aspremont; Adrien Scieur;  Taylor"}, {"ref_id": "b14", "title": "Piecewise-deterministic markov processes: a general class of non-diffusion stochastic models", "journal": "Journal of the Royal Statistical Society: Series B (Methodological)", "year": "1984", "authors": "H A Mark;  Davis"}, {"ref_id": "b15", "title": "Markov models & optimization. Routledge", "journal": "", "year": "2018", "authors": "H A Mark;  Davis"}, {"ref_id": "b16", "title": "Stochastic first order methods in smooth convex optimization", "journal": "", "year": "2011", "authors": "Olivier Devolder"}, {"ref_id": "b17", "title": "The approximate duality gap technique: A unified theory of first-order methods", "journal": "SIAM Journal on Optimization", "year": "2019", "authors": "Jelena Diakonikolas; Lorenzo Orecchia"}, {"ref_id": "b18", "title": "Gossip algorithms for distributed signal processing", "journal": "Proceedings of the IEEE", "year": "2010", "authors": "A G Dimakis; S Kar; J M F Moura; M G Rabbat; A Scaglione"}, {"ref_id": "b19", "title": "Geographic gossip: Efficient averaging for sensor networks", "journal": "IEEE Transactions on Signal Processing", "year": "2008", "authors": "D G Alexandros; Anand D Dimakis; Martin J Sarwate;  Wainwright"}, {"ref_id": "b20", "title": "Geographic gossip: Efficient averaging for sensor networks", "journal": "IEEE Transactions on Signal Processing", "year": "2008", "authors": "D G Alexandros;  Dimakis; D Anand; Martin J Sarwate;  Wainwright"}, {"ref_id": "b21", "title": "Effective graph resistance", "journal": "Linear Algebra and its Applications", "year": "2011", "authors": "W Ellens; F M Spieksma; P Van Mieghem; A Jamakovic; R E Kooij"}, {"ref_id": "b22", "title": "Decentralized optimization with heterogeneous delays: a continuous-time approach", "journal": "", "year": "2021", "authors": "Hadrien Mathieu Even; Laurent Hendrikx;  Massouli\u00e9"}, {"ref_id": "b23", "title": "From averaging to acceleration, there is only a step-size", "journal": "PMLR", "year": "2015", "authors": "Nicolas Flammarion; Francis Bach"}, {"ref_id": "b24", "title": "Accelerated decentralized optimization with local updates for smooth and strongly convex objectives", "journal": "", "year": "2018", "authors": "Hadrien Hendrikx; Francis Bach; Laurent Massouli\u00e9"}, {"ref_id": "b25", "title": "An accelerated decentralized stochastic proximal algorithm for finite sums", "journal": "", "year": "2019", "authors": "Hadrien Hendrikx; Francis Bach; Laurent Massouli\u00e9"}, {"ref_id": "b26", "title": "Accelerated gradient methods for stochastic optimization and online learning", "journal": "", "year": "2009", "authors": "Chonghai Hu; Weike Pan; James Kwok"}, {"ref_id": "b27", "title": "Stochastic differential equations and diffusion processes", "journal": "Elsevier", "year": "2014", "authors": "Nobuyuki Ikeda; Shinzo Watanabe"}, {"ref_id": "b28", "title": "Limit theorems for stochastic processes", "journal": "Springer Science & Business Media", "year": "2013", "authors": "Jean Jacod; Albert Shiryaev"}, {"ref_id": "b29", "title": "Accelerating stochastic gradient descent for least squares regression", "journal": "", "year": "2018", "authors": "Prateek Jain; M Sham; Rahul Kakade; Praneeth Kidambi; Aaron Netrapalli;  Sidford"}, {"ref_id": "b30", "title": "On the duality of strong convexity and strong smoothness: Learning applications and matrix regularization", "journal": "", "year": "2009", "authors": "Shai Sham Kakade; Ambuj Shalev-Shwartz;  Tewari"}, {"ref_id": "b31", "title": "Optimized first-order methods for smooth convex minimization", "journal": "", "year": "2016", "authors": "Donghwan Kim; Jeffrey A Fessler"}, {"ref_id": "b32", "title": "Accelerated mirror descent in continuous and discrete time", "journal": "Advances in Neural Information Processing Systems", "year": "2015", "authors": "Walid Krichene; Alexandre Bayen; Peter Bartlett"}, {"ref_id": "b33", "title": "An optimal method for stochastic composite optimization", "journal": "Math. Program", "year": "2012", "authors": "Guanghui Lan"}, {"ref_id": "b34", "title": "Analysis of stochastic gradient descent in continuous time", "journal": "Statistics and Computing", "year": "2021", "authors": "Jonas Latz"}, {"ref_id": "b35", "title": "Brownian Motion, Martingales, and Stochastic Calculus", "journal": "Springer", "year": "2016", "authors": "Jean-Fran\u00e7ois Le Gall"}, {"ref_id": "b36", "title": "Analysis of accelerated gossip algorithms", "journal": "Automatica", "year": "2013", "authors": "Ji Liu; D O Brian; Ming Anderson; A Cao;  Stephen Morse"}, {"ref_id": "b37", "title": "Provably accelerated randomized gossip algorithms", "journal": "IEEE", "year": "2019", "authors": "Nicolas Loizou; Michael Rabbat; Peter Richt\u00e1rik"}, {"ref_id": "b38", "title": "Chebyshev polynomials in distributed consensus applications", "journal": "IEEE Transactions on Signal Processing", "year": "", "authors": "Eduardo Montijano; Juan Montijano; C Sagues"}, {"ref_id": "b39", "title": "A dynamical systems perspective on Nesterov acceleration", "journal": "PMLR", "year": "2019", "authors": "Michael Muehlebach; Michael Jordan"}, {"ref_id": "b40", "title": "Problem Complexity and Method Efficiency in Optimization", "journal": "Wiley-Interscience", "year": "1983", "authors": "Arkadij Semenovi\u010d Nemirovskij; David Borisovich Yudin"}, {"ref_id": "b41", "title": "A method of solving a convex programming problem with convergence rate O(1/k 2 )", "journal": "Dokl. Akad. Nauk SSSR", "year": "1983", "authors": "Yurii Nesterov"}, {"ref_id": "b42", "title": "Introductory Lectures on Convex Optimization: A Basic Course", "journal": "Springer Science & Business Media", "year": "2003", "authors": "Yurii Nesterov"}, {"ref_id": "b43", "title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "journal": "SIAM Journal on Optimization", "year": "2012", "authors": "Yurii Nesterov"}, {"ref_id": "b44", "title": "Efficiency of the accelerated coordinate descent method on structured optimization problems", "journal": "SIAM Journal on Optimization", "year": "2017", "authors": "Yurii Nesterov; Sebastian U Stich"}, {"ref_id": "b45", "title": "The connections between Lyapunov functions for some optimization algorithms and differential equations", "journal": "", "year": "2020", "authors": "Jes\u00fas Mar\u00eda Sanz-Serna ; Konstantinos Zygalakis"}, {"ref_id": "b46", "title": "Optimal algorithms for smooth and strongly convex distributed optimization in networks", "journal": "", "year": "2017", "authors": "Kevin Scaman; Francis Bach; S\u00e9bastien Bubeck; Yin Tat Lee; Laurent Massouli\u00e9"}, {"ref_id": "b47", "title": "Understanding the acceleration phenomenon via high-resolution differential equations", "journal": "", "year": "2018", "authors": "Bin Shi; Simon Du; Michael Jordan; Weijie Su"}, {"ref_id": "b48", "title": "Acceleration via symplectic discretization of high-resolution differential equations", "journal": "", "year": "2019", "authors": "Bin Shi; Simon Du; Weijie Su; Michael Jordan"}, {"ref_id": "b49", "title": "A differential equation for modeling Nesterov's accelerated gradient method: theory and insights", "journal": "", "year": "2014", "authors": "Weijie Su; Stephen Boyd; Emmanuel Candes"}, {"ref_id": "b50", "title": "A coordinate gradient descent method for nonsmooth separable minimization", "journal": "Mathematical Programming", "year": "2009", "authors": "Paul Tseng; Sangwoon Yun"}, {"ref_id": "b51", "title": "Fast and faster convergence of sgd for overparameterized models and an accelerated perceptron", "journal": "PMLR", "year": "2019", "authors": "Sharan Vaswani; Francis Bach; Mark Schmidt"}, {"ref_id": "b52", "title": "A variational perspective on accelerated methods in optimization", "journal": "Proceedings of the National Academy of Sciences", "year": "2016", "authors": "Andre Wibisono; C Ashia; Michael I Jordan Wilson"}, {"ref_id": "b53", "title": "A Lyapunov analysis of momentum methods in optimization", "journal": "", "year": "2016", "authors": "Ashia Wilson; Benjamin Recht; Michael I Jordan "}, {"ref_id": "b54", "title": "Coordinate descent algorithms", "journal": "Math. Program", "year": "2015", "authors": "Stephen Wright"}, {"ref_id": "b55", "title": "Dual averaging methods for regularized stochastic learning and online optimization", "journal": "J. Mach. Learn. Res", "year": "2010", "authors": "Lin Xiao"}, {"ref_id": "b56", "title": "Direct Runge-Kutta discretization achieves acceleration", "journal": "", "year": "2018", "authors": "Jingzhao Zhang; Aryan Mokhtari; Suvrit Sra; Ali Jadbabaie"}], "figures": [{"figure_label": "a2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "( a )Figure 2 :a2Figure 2: Comparison between randomized gossip[11] and accelerated randomized gossip from Section 6, on 3 different graphs: line with 30 nodes, 2D-Grid with 225 nodes and complete graph with 30 nodes. The probability P on the set of edges that determines at every activation which edge is activated is uniform in all cases. Parameters of the algorithm are taken as in Theorem 5. In all simulations, initialization was taken with a vector x 0 such that x 0 (v) = 0 at all nodes, except one where x 0 (v) = 1. Figures on the left represent one run of the algorithms. Figures on the right represent the average performance (thick line) for N = 1000 runes with the same settings, and the 5% and 95% quantiles (thin lines). As expected, we observe acceleration one the line and the grid, but no such phenomenon on the complete graph.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Effect of additive noise on gradient descent, Nesterov acceleration, and the continuized version of Nesterov acceleration, on a convex function (left) and a strongly convex function (right).All algorithms are started from the optimum x * . The results shown in the above plots correspond to a single run. In the plots below, the thick line represents the average performance over N = 100 runs of each algorithm, while the thin lines represent the 5% and 95% quantiles.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Proposition 2 .2Let x t \u2208 R d be a solution of dx t = b(x t )dt + \u039e G(x t , \u03be)dN (t,\u03be) and \u03d5 : R d \u2192 R be a smooth function. Then \u03d5(x t ) = \u03d5(x 0 ) + t 0 \u2207\u03d5(x s ), b(x s ) ds + [0,t]\u00d7\u039e (\u03d5(x s\u2212 + G(x s\u2212 , \u03be)) \u2212 \u03d5(x s\u2212 )) dN (s, \u03be) .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "t0\u2207\u03d5(x s ), b(x s ) ds + t 0 (\u03d5(x s + G(x s )) \u2212 \u03d5(x s ))) ds is non-increasing almost surely, i.e.,", "figure_data": ""}, {"figure_label": "22", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "2 d 2 x s ds 2 ,22and thus d 2 x s ds 2 + 3 s dx s ds + \u2207f (x s ) = 0 .", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "\u03bb 2 A2\u2020 A , with \u03bb a solution to the dual problem.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "2. The parameters of Theorem 2.(2) are \u03b7 t = \u03b7 t \u2261 \u00b5 L , \u03b3 t \u2261 1 L and \u03b3 t \u2261 1 \u221a \u00b5L . In this case, the ODE", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "{v,w} e {v,w} A \u2207F * (A\u03bb P {v,w} e {v,w} e {v,w} A \u2207F * (A\u03bb", "figure_data": "z) t \u2212 \u03bb (y) t )dt \u2212 \u03b3 tR 0 \u00d7ER {v,w} {v,w} P 2(y) t )dN (t, {v, w}) ,d\u03bb(z) t= \u03b7 t (\u03bb(y) t \u2212 \u03bb (z) t )dt \u2212 \u03b3 t1(y) t )dN (t, {v, w}) ,R 0 \u00d7E"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "dy t = \u03b7 t (z t \u2212 y t )dt \u2212 \u03b3 t (y t )dN (t, {v, w}) , dz t = \u03b7 t (y t \u2212 z t )dt \u2212 \u03b3 t R 0 \u00d7E 1 P {v,w}W {v,w} \u2207F", "figure_data": "R {v,w}R 0 \u00d7EP 2 {v,w}"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "{v,w}{v,w} = e {v,w} A \u2207 2 F * (A\u03bb)Ae {v,w} 1 \u00b5 e {v,w} A Ae {v,w} = P {v,w} 2\u00b5 .", "figure_data": "(63)Thus, we have that:L dual = max {v,w}M {v,w}{v,w} R {v,w} P 2 {v,w}=1 \u00b5max {v,w}R {v,w} P {v,w}."}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u2200x, y \u2208 R d , f (y) f (x) + \u2207f (x), y \u2212 x + L 2 y \u2212 x 2 . (1", "formula_coordinates": [1.0, 176.32, 690.63, 323.81, 22.31]}, {"formula_id": "formula_1", "formula_text": ")", "formula_coordinates": [1.0, 500.13, 697.69, 3.87, 8.64]}, {"formula_id": "formula_2", "formula_text": "\u2200x, y \u2208 R d , f (y) f (x) + \u2207f (x), y \u2212 x + \u00b5 2 y \u2212 x 2 .(2)", "formula_coordinates": [2.0, 176.71, 84.89, 327.29, 22.31]}, {"formula_id": "formula_3", "formula_text": "y k = x k + \u03c4 k (z k \u2212 x k ) ,(3)", "formula_coordinates": [3.0, 225.21, 424.13, 278.79, 9.65]}, {"formula_id": "formula_4", "formula_text": "x k+1 = y k \u2212 \u03b3 k \u2207f (y k ) ,(4)", "formula_coordinates": [3.0, 225.21, 438.03, 278.79, 9.65]}, {"formula_id": "formula_5", "formula_text": "z k+1 = z k + \u03c4 k (y k \u2212 z k ) \u2212 \u03b3 k \u2207f (y k ) . (5", "formula_coordinates": [3.0, 225.21, 453.28, 274.91, 10.62]}, {"formula_id": "formula_6", "formula_text": ")", "formula_coordinates": [3.0, 500.13, 453.6, 3.87, 8.64]}, {"formula_id": "formula_7", "formula_text": "1. Choose the parameters \u03c4 k = 1 \u2212 A k A k+1 , \u03c4 k = 0, \u03b3 k = 1 L , \u03b3 k = A k+1 \u2212A k L", "formula_coordinates": [3.0, 131.41, 515.12, 299.14, 15.2]}, {"formula_id": "formula_8", "formula_text": "A 0 = 0 , A k+1 = A k + 1 2 (1 + 4A k + 1) . Then f (x k ) \u2212 f (x * ) 2L x 0 \u2212 x * 2 k 2 .", "formula_coordinates": [3.0, 143.31, 547.01, 299.35, 65.4]}, {"formula_id": "formula_9", "formula_text": "\u03c4 k \u2261 \u221a \u00b5/L 1+ \u221a \u00b5/L , \u03c4 k \u2261 \u00b5 L , \u03b3 k \u2261 1 L , \u03b3 k \u2261 1 \u221a \u00b5L , k 0. Then f (x k ) \u2212 f (x * ) f (x 0 ) \u2212 f (x * ) + \u00b5 2 z 0 \u2212 x * 2 1 \u2212 \u00b5 L k .", "formula_coordinates": [3.0, 143.87, 627.17, 317.97, 61.72]}, {"formula_id": "formula_10", "formula_text": "x T k = x T k \u2212 \u2212 \u03b3 T k \u2207f (x T k \u2212 ) ,(6)", "formula_coordinates": [4.0, 243.59, 337.79, 260.41, 10.32]}, {"formula_id": "formula_11", "formula_text": "z T k = z T k \u2212 \u2212 \u03b3 T k \u2207f (x T k \u2212 ) .", "formula_coordinates": [4.0, 245.71, 353.04, 122.7, 11.29]}, {"formula_id": "formula_12", "formula_text": "dx t = \u03b7 t (z t \u2212 x t )dt ,(8)", "formula_coordinates": [4.0, 262.35, 414.58, 241.65, 9.65]}, {"formula_id": "formula_13", "formula_text": "dx t = \u03b7 t (z t \u2212 x t )dt \u2212 \u03b3 t \u2207f (x t )dN (t) ,(10)", "formula_coordinates": [4.0, 223.34, 471.58, 280.66, 9.65]}, {"formula_id": "formula_14", "formula_text": "dz t = \u03b7 t (x t \u2212 z t )dt \u2212 \u03b3 t \u2207f (x t )dN (t) .(11)", "formula_coordinates": [4.0, 224.4, 486.83, 279.6, 10.62]}, {"formula_id": "formula_15", "formula_text": "\u03b7 t = 2 t , \u03b7 t = 0, \u03b3 t = 1 L , \u03b3 t = t 2L . Then Ef (x t ) \u2212 f (x * ) 2L z 0 \u2212 x * 2 t 2 .", "formula_coordinates": [5.0, 238.53, 103.96, 157.19, 42.22]}, {"formula_id": "formula_16", "formula_text": "\u03b7 t = \u03b7 t \u2261 \u00b5 L , \u03b3 t \u2261 1 L , \u03b3 t \u2261 1 \u221a \u00b5L . Then Ef (x t ) \u2212 f (x * ) f (x 0 ) \u2212 f (x * ) + \u00b5 2 z 0 \u2212 x * 2 exp \u2212 \u00b5 L t .", "formula_coordinates": [5.0, 143.87, 164.83, 323.68, 45.26]}, {"formula_id": "formula_17", "formula_text": "x k := x T k ,\u1ef9 k := x T k+1 \u2212 ,z k := z T k ,", "formula_coordinates": [5.0, 169.3, 531.05, 273.39, 10.32]}, {"formula_id": "formula_18", "formula_text": "k =x k + \u03c4 k (z k \u2212x k ) ,(12)", "formula_coordinates": [5.0, 230.1, 610.87, 513.87, 9.65]}, {"formula_id": "formula_19", "formula_text": "x k+1 =\u1ef9 k \u2212\u03b3 k \u2207f (\u1ef9 k ) ,(13)", "formula_coordinates": [5.0, 225.21, 624.76, 518.79, 9.65]}, {"formula_id": "formula_20", "formula_text": "z k+1 =z k + \u03c4 k (\u1ef9 k \u2212z k ) \u2212\u03b3 k \u2207f (\u1ef9 k ) ,(14)", "formula_coordinates": [5.0, 225.21, 640.01, 278.79, 10.62]}, {"formula_id": "formula_21", "formula_text": "\u03c4 k = 1 \u2212 T k T k+1 2 , \u03c4 k = 0,\u03b3 k = 1 L , and\u03b3 k = T k 2L . Then E T 2 k (f (x k ) \u2212 f (x * )) 2L z 0 \u2212 x * 2 .", "formula_coordinates": [5.0, 143.31, 675.62, 362.44, 48.19]}, {"formula_id": "formula_22", "formula_text": "), \u03c4 k = 1 2 1 \u2212 exp \u22122 \u00b5 L (T k+1 \u2212 T k ) , \u03c4 k = tanh \u00b5 L (T k+1 \u2212 T k ) ,\u03b3 k = 1 L , and\u03b3 k = 1 \u221a \u00b5L . Then E exp \u00b5 L T k (f (x k ) \u2212 f (x * )) f (x 0 ) \u2212 f (x * ) + \u00b5 2 z 0 \u2212 x * 2 .", "formula_coordinates": [6.0, 143.87, 72.75, 327.24, 57.82]}, {"formula_id": "formula_23", "formula_text": "x T k = x T k \u2212 \u2212 \u03b3 T k \u2207f (x T k \u2212 , \u03be k ) , z T k = z T k \u2212 \u2212 \u03b3 T k \u2207f (x T k \u2212 , \u03be k ) .", "formula_coordinates": [6.0, 236.75, 356.15, 138.51, 26.54]}, {"formula_id": "formula_24", "formula_text": "dx t = \u03b7 t (z t \u2212 x t )dt , dz t = \u03b7 t (x t \u2212 z t )dt .", "formula_coordinates": [6.0, 262.35, 402.61, 87.31, 25.87]}, {"formula_id": "formula_25", "formula_text": "k 1 \u03b4 (T k ,\u03be k ) (dt, d\u03be) on R 0 \u00d7 \u039e, which has intensity dt \u2297 P, dx t = \u03b7 t (z t \u2212 x t )dt \u2212 \u03b3 t \u039e \u2207f (x t , \u03be)dN (t, \u03be) ,(15)", "formula_coordinates": [6.0, 118.52, 444.37, 385.48, 40.8]}, {"formula_id": "formula_26", "formula_text": "dz t = \u03b7 t (x t \u2212 z t )dt \u2212 \u03b3 t \u039e \u2207f (x t , \u03be)dN (t, \u03be) .(16)", "formula_coordinates": [6.0, 207.85, 494.56, 296.15, 17.24]}, {"formula_id": "formula_27", "formula_text": "\u2200x \u2208 R d , f (x) = E (a,b)\u223cP 1 2 (b \u2212 x, a ) 2 ,(17)", "formula_coordinates": [6.0, 211.02, 614.2, 292.98, 22.31]}, {"formula_id": "formula_28", "formula_text": "\u2207f (x, \u03be) = \u2212(b \u2212 x, a )a , \u03be = (a, b) .", "formula_coordinates": [6.0, 215.58, 671.07, 180.84, 8.74]}, {"formula_id": "formula_29", "formula_text": "\u223c P: b = x * , a , so that \u2207f (x * , \u03be) = 0 .(18)", "formula_coordinates": [6.0, 193.38, 697.59, 310.62, 25.26]}, {"formula_id": "formula_30", "formula_text": "2 A = x Ax. Let H = E[aa ]", "formula_coordinates": [7.0, 385.67, 123.05, 118.33, 14.11]}, {"formula_id": "formula_31", "formula_text": "E a 2 aa R 2 H .(19)", "formula_coordinates": [7.0, 259.21, 156.82, 244.79, 12.26]}, {"formula_id": "formula_32", "formula_text": "E a 2 H \u22121 aa \u03baH .(20)", "formula_coordinates": [7.0, 256.21, 202.0, 247.79, 14.11]}, {"formula_id": "formula_33", "formula_text": "\u03b7 t = 2 t , \u03b7 t = 0, \u03b3 t = 1 R 2 , \u03b3 t = t 2R 2\u03ba . Then 1 2 E x t \u2212 x * 2 R 2\u03ba z 0 \u2212 x * 2 H \u22121 t 2 .", "formula_coordinates": [7.0, 238.53, 252.87, 170.77, 44.24]}, {"formula_id": "formula_34", "formula_text": "parameters \u03b7 t = \u03b7 t = 1 \u221a \u03ba\u03ba , \u03b3 t = 1 R 2 and \u03b3 t = 1 R 2", "formula_coordinates": [7.0, 143.87, 329.91, 201.11, 14.7]}, {"formula_id": "formula_35", "formula_text": "1 2 E x t \u2212 x * 2 1 2 x 0 \u2212 x * 2 + \u00b5 2 z 0 \u2212 x * 2 H \u22121 exp \u2212 t \u221a \u03ba\u03ba .", "formula_coordinates": [7.0, 180.65, 352.97, 287.76, 23.54]}, {"formula_id": "formula_36", "formula_text": "x T k (v k ), x T k (w k ) \u2190\u2212 x T k \u2212 (v k ) + x T k \u2212 (w k ) 2 .", "formula_coordinates": [8.0, 198.71, 153.05, 214.58, 22.31]}, {"formula_id": "formula_37", "formula_text": "f (x) = {v,w}\u2208E P {v,w} 2 (x(v) \u2212 x(w)) 2 , x = (x(v)) v\u2208V . (21", "formula_coordinates": [8.0, 180.39, 209.74, 319.46, 27.96]}, {"formula_id": "formula_38", "formula_text": ")", "formula_coordinates": [8.0, 499.85, 217.49, 4.15, 8.64]}, {"formula_id": "formula_39", "formula_text": "f (x) = E {v,w}\u223cP 1 2 x, a {v,w} 2 , (22", "formula_coordinates": [8.0, 231.45, 265.01, 268.4, 22.31]}, {"formula_id": "formula_40", "formula_text": ")", "formula_coordinates": [8.0, 499.85, 272.07, 4.15, 8.64]}, {"formula_id": "formula_41", "formula_text": "\u2207f (x, {v, w}) = x, a {v,w} a {v,w} = \uf8f1 \uf8f2 \uf8f3 x(v) \u2212 x(w) at coordinate v, x(w) \u2212 x(v) at coordinate w, 0 at all other coordinates. (23", "formula_coordinates": [8.0, 143.78, 327.26, 356.07, 36.19]}, {"formula_id": "formula_42", "formula_text": ")", "formula_coordinates": [8.0, 499.85, 341.59, 4.15, 8.64]}, {"formula_id": "formula_43", "formula_text": "dx t = \u2212 1 2 R 0 \u00d7E \u2207f (x t , {v, w})dN (t, {v, w}) . (24", "formula_coordinates": [8.0, 204.83, 413.17, 295.02, 24.76]}, {"formula_id": "formula_44", "formula_text": ")", "formula_coordinates": [8.0, 499.85, 420.23, 4.15, 8.64]}, {"formula_id": "formula_45", "formula_text": "x T k (v k ) = x T k (w k ) = x T k \u2212 (v k ) + x T k \u2212 (w k ) 2 , z T k (v k ) = z T k \u2212 (v k ) + 1 2\u00b5 gossip R max (x T k \u2212 (w k ) \u2212 x T k \u2212 (v k )) , z T k (w k ) = z T k \u2212 (w k ) + 1 2\u00b5 gossip R max (x T k \u2212 (v k ) \u2212 x T k \u2212 (w k )) .", "formula_coordinates": [8.0, 170.14, 642.82, 271.72, 76.48]}, {"formula_id": "formula_46", "formula_text": "dx t (v) = 2\u00b5 gossip R max (z t (v) \u2212 x t (v))dt, dz t (v) = 2\u00b5 gossip R max (x t (v) \u2212 z t (v))dt.", "formula_coordinates": [9.0, 225.63, 103.12, 160.74, 51.52]}, {"formula_id": "formula_47", "formula_text": "v\u2208V 1 2 E x t (v) \u2212x 2 2 v\u2208V 1 2 x 0 (v) \u2212x 2 exp \u2212 \u00b5 gossip 2R max t .", "formula_coordinates": [9.0, 156.42, 241.54, 299.16, 26.8]}, {"formula_id": "formula_48", "formula_text": "\u03b8 RG P min 2 \u03b8 ARG .", "formula_coordinates": [9.0, 272.79, 377.48, 77.57, 22.31]}, {"formula_id": "formula_49", "formula_text": "C \u03b8 RG |V| \u03b8 ARG .", "formula_coordinates": [9.0, 268.03, 440.32, 75.94, 22.31]}, {"formula_id": "formula_50", "formula_text": "min x\u2208R d f (x) = 1 |V| v\u2208V f v (x) .(25)", "formula_coordinates": [9.0, 240.82, 643.65, 263.18, 26.8]}, {"formula_id": "formula_51", "formula_text": "G(y, {v, w}) = \uf8f1 \uf8f2 \uf8f3 \u2207f * v (y(v)) \u2212 \u2207f * w (y(w)) at coordinate v, \u2212\u2207f * v (y(v)) \u2212 \u2207f * w (y(w)) at coordinate w, 0 at all other coordinates. (26", "formula_coordinates": [10.0, 158.87, 177.0, 340.98, 36.71]}, {"formula_id": "formula_52", "formula_text": ")", "formula_coordinates": [10.0, 499.85, 191.86, 4.15, 8.64]}, {"formula_id": "formula_53", "formula_text": "v\u2208V 1 2 E x t (v) \u2212 x * 2 C v\u2208V 1 2 x 0 (v) \u2212 x * 2 exp \u2212 \u03b8 ARG \u221a \u03ba t ,", "formula_coordinates": [10.0, 156.22, 303.29, 299.57, 26.8]}, {"formula_id": "formula_54", "formula_text": "f(x k ) f(x * )", "formula_coordinates": [14.0, 303.93, 311.42, 13.07, 38.8]}, {"formula_id": "formula_55", "formula_text": "f (x 1 , x 2 , x 3 ) = \u00b5 2 (x 1 \u2212 1) 2 + 3\u00b5 2 (x 2 \u2212 1) 2 + L 2 (x 3 \u2212 1) 2 ,", "formula_coordinates": [14.0, 183.67, 539.91, 244.67, 22.31]}, {"formula_id": "formula_56", "formula_text": "f (x 1 , . . . , x 100 ) = 1 2 100 i=1 1 i 2 x i \u2212 1 i 2 ,", "formula_coordinates": [14.0, 221.89, 597.76, 168.21, 30.32]}, {"formula_id": "formula_57", "formula_text": "\u2200x \u2208 R d , E \u03be \u2207f (x, \u03be) = \u2207f (x) ,(27)", "formula_coordinates": [15.0, 228.7, 711.12, 275.3, 11.72]}, {"formula_id": "formula_58", "formula_text": "\u2200x \u2208 R d , E \u03be \u2207f (x, \u03be) \u2212 \u2207f (x) 2 \u03c3 2 . (28", "formula_coordinates": [16.0, 209.3, 91.0, 290.55, 12.62]}, {"formula_id": "formula_59", "formula_text": ")", "formula_coordinates": [16.0, 499.85, 94.29, 4.15, 8.64]}, {"formula_id": "formula_60", "formula_text": "Ef (x t ) \u2212 f (x * ) 2L z 0 \u2212 x * 2 t 2 + \u03c3 2 t 3L .", "formula_coordinates": [16.0, 233.74, 233.35, 180.39, 23.89]}, {"formula_id": "formula_61", "formula_text": "Ef (x t ) \u2212 f (x * ) f (x 0 ) \u2212 f (x * ) + \u00b5 2 z 0 \u2212 x * 2 exp \u2212 \u00b5 L t + \u03c3 2 1 \u221a \u00b5L .", "formula_coordinates": [16.0, 158.07, 285.88, 331.73, 22.65]}, {"formula_id": "formula_62", "formula_text": "\u2022 T 1 , T 2 \u2212 T 1 , T 3 \u2212 T 2 , . . . are i.i.", "formula_coordinates": [16.0, 135.4, 713.2, 140.73, 9.65]}, {"formula_id": "formula_63", "formula_text": "F t = \u03c3 (N ([0, s] \u00d7 A) , s t, A \u2282 \u039e measurable) .", "formula_coordinates": [17.0, 200.22, 494.27, 211.55, 9.65]}, {"formula_id": "formula_64", "formula_text": "Definition 4. A random process x t \u2208 R d , t 0, is adapted if for all t 0, x t is F t -measurable. An adapted process x t \u2208 R, t 0 is a martingale (resp. supermartingale) if for all 0 s t, E[x t | F s ] = x s (resp. E[x t | F s ] x s ). Definition 5. A random variable T \u2208 [0, \u221e] is a stopping time if for all t 0, {T t} \u2208 F t .", "formula_coordinates": [17.0, 107.39, 554.67, 398.36, 48.33]}, {"formula_id": "formula_65", "formula_text": "dx t = b(x t )dt + \u039e G(x t , \u03be)dN (t, \u03be)", "formula_coordinates": [18.0, 229.62, 167.79, 152.75, 17.24]}, {"formula_id": "formula_66", "formula_text": "x t = x 0 + t 0 b(x s )ds + [0,t]\u00d7\u039e G(x s\u2212 , \u03be)dN (s, \u03be) .", "formula_coordinates": [18.0, 196.46, 213.29, 219.07, 26.29]}, {"formula_id": "formula_67", "formula_text": "[0,t]\u00d7\u039e G(x s\u2212 , \u03be)dN (s, \u03be) = k 1 1 {T k t} G(x T k \u2212 , \u03be k ) .", "formula_coordinates": [18.0, 197.66, 279.78, 222.21, 20.17]}, {"formula_id": "formula_68", "formula_text": "[0,t]\u00d7\u039e (\u03d5(x s\u2212 + G(x s\u2212 , \u03be)) \u2212 \u03d5(x s\u2212 )) dN (s, \u03be) = t 0 \u039e (\u03d5(x s + G(x s , \u03be)) \u2212 \u03d5(x s )) dtdP(\u03be) + M t , where M t = [0,t]\u00d7\u039e (\u03d5(x s\u2212 + G(x s\u2212 , \u03be)) \u2212 \u03d5(x s\u2212 )) (dN (s, \u03be) \u2212 dtdP(\u03be)) is a martingale.", "formula_coordinates": [18.0, 108.0, 493.24, 377.15, 69.64]}, {"formula_id": "formula_69", "formula_text": "\u03c6 t = A t (f (x t ) \u2212 f (x * )) + B t 2 z t \u2212 x * 2", "formula_coordinates": [19.0, 219.59, 148.41, 172.32, 22.31]}, {"formula_id": "formula_70", "formula_text": "f (x t ) \u2192 f (x t \u2212 \u03b3 t \u2207f (x t )).", "formula_coordinates": [19.0, 143.87, 306.9, 117.49, 9.65]}, {"formula_id": "formula_71", "formula_text": "E t f (x t+dt ) \u2248 f (x t ) + \u03b7 t \u2207f (x t ), z t \u2212 x t dt + dt (f (x t \u2212 \u03b3 t \u2207f (x t )) \u2212 f (x t ))", "formula_coordinates": [19.0, 139.05, 342.12, 327.82, 9.65]}, {"formula_id": "formula_72", "formula_text": "E t \u03c6 t+dt \u2212 \u03c6 t \u2248 dt dA t dt (f (x t ) \u2212 f (x * )) + A t \u03b7 t \u2207f (x t ), z t \u2212 x t \u2212 A t (f (x t \u2212 \u03b3 t \u2207f (x t )) \u2212 f (x t )) + dB t dt 1 2 z t \u2212 x * 2 + B t \u03b7 t z t \u2212 x * , x t \u2212 z t + B t 2 z t \u2212 \u03b3 t \u2207f (x t ) \u2212 x * 2 \u2212 z t \u2212 x * 2 .", "formula_coordinates": [19.0, 111.37, 401.13, 389.26, 74.68]}, {"formula_id": "formula_73", "formula_text": "E t \u03c6 t+dt \u2212 \u03c6 t dt dA t dt \u2212 A t \u03b7 t \u2207f (x t ), x t \u2212 x * + dB t dt \u2212 B t \u03b7 t 1 2 z t \u2212 x * 2 + (A t \u03b7 t \u2212 B t \u03b3 t ) \u2207f (x t ), z t \u2212 x * + B t \u03b7 t \u2212 dA t dt \u00b5 1 2 x t \u2212 x * 2 + B t \u03b3 2 t \u2212 A t \u03b3 t (2 \u2212 L\u03b3 t ) 1 2 \u2207f (x t ) 2 .", "formula_coordinates": [19.0, 121.86, 508.83, 367.79, 78.1]}, {"formula_id": "formula_74", "formula_text": "d 2 dt 2 A t = \u00b5 4L A t ,", "formula_coordinates": [19.0, 255.55, 635.8, 102.11, 23.89]}, {"formula_id": "formula_75", "formula_text": "Ef (x t ) \u2212 f (x * ) E\u03c6 t A t \u03c6 0 A t .", "formula_coordinates": [19.0, 241.73, 698.13, 128.53, 23.23]}, {"formula_id": "formula_76", "formula_text": "dx t = \u03b7 t (z t \u2212 x t )dt \u2212 \u03b3 t \u2207f (x t )dN (t) , dz t = \u03b7 t (x t \u2212 z t )dt \u2212 \u03b3 t \u2207f (x t )dN (t) .", "formula_coordinates": [20.0, 223.34, 194.97, 165.32, 25.87]}, {"formula_id": "formula_77", "formula_text": "\u03c6 t = A t (f (x t ) \u2212 f * ) + B t 2 z t \u2212 x * 2 is a supermartingale.", "formula_coordinates": [20.0, 108.0, 251.65, 276.65, 35.11]}, {"formula_id": "formula_78", "formula_text": "dx t = b(x t )dt + G(x t )dN (t) , b(x t ) = 1 \u03b7 t (z t \u2212 x t ) \u03b7 t (x t \u2212 z t ) , G(x t ) = 0 \u2212\u03b3 t \u2207f (x t ) \u2212\u03b3 t \u2207f (x t ) .", "formula_coordinates": [20.0, 123.16, 310.14, 365.69, 32.44]}, {"formula_id": "formula_79", "formula_text": "\u03d5(t, x, z) = A t (f (x) \u2212 f (x * )) + B t 2 z \u2212 x * 2 ,", "formula_coordinates": [20.0, 206.64, 363.65, 198.71, 22.31]}, {"formula_id": "formula_80", "formula_text": "\u03c6 t = \u03c6 0 + t 0 \u2207\u03d5(x s ), b(x s ) ds + t 0 (\u03d5(x s + G(x s )) \u2212 \u03d5(x s )) ds + M t ,", "formula_coordinates": [20.0, 150.66, 402.06, 310.68, 26.29]}, {"formula_id": "formula_81", "formula_text": "I t := \u2207\u03d5(x t ), b(x t ) + \u03d5(x t + G(x t )) \u2212 \u03d5(x t ) 0 . We now compute \u2207\u03d5(x t ), b(x t ) = \u2202 t \u03d5(x t ) + \u2202 x \u03d5(x t ), \u03b7 t (z t \u2212 x t ) + \u2202 z \u03d5(x t ), \u03b7 t (x t \u2212 z t ) = dA t dt (f (x t ) \u2212 f (x * )) + dB t dt 1 2 z t \u2212 x * 2 + A t \u03b7 t \u2207f (x t ), z t \u2212 x t + B t \u03b7 t z t \u2212 x * , x t \u2212 z t .", "formula_coordinates": [20.0, 107.53, 472.48, 371.04, 81.82]}, {"formula_id": "formula_82", "formula_text": "f (x t ) \u2212 f (x * ) \u2207f (x t ), x t \u2212 x * \u2212 \u00b5 2 x t \u2212 x * 2 ,", "formula_coordinates": [20.0, 196.78, 571.69, 218.45, 22.31]}, {"formula_id": "formula_83", "formula_text": "z t \u2212 x * , x t \u2212 z t = z t \u2212 x * , x t \u2212 x * \u2212 z t \u2212 x * 2 z t \u2212 x * x t \u2212 x * \u2212 z t \u2212 x * 2 1 2 z t \u2212 x * 2 + x t \u2212 x * 2 \u2212 z t \u2212 x * 2 = 1 2 x t \u2212 x * 2 \u2212 z t \u2212 x * 2 .", "formula_coordinates": [20.0, 111.87, 611.5, 401.17, 38.06]}, {"formula_id": "formula_84", "formula_text": "\u2207\u03d5(x t ), b(x t ) dA t dt \u2212 A t \u03b7 t \u2207f (x t ), x t \u2212 x * + B t \u03b7 t \u2212 dA t dt \u00b5 1 2 x t \u2212 x * 2(29)", "formula_coordinates": [20.0, 122.13, 669.1, 381.87, 22.31]}, {"formula_id": "formula_85", "formula_text": "+ dB t dt \u2212 B t \u03b7 t 1 2 z t \u2212 x * 2 + A t \u03b7 t \u2207f (x t ), z t \u2212 x * .(30)", "formula_coordinates": [20.0, 185.64, 696.99, 318.36, 22.31]}, {"formula_id": "formula_86", "formula_text": "\u03d5(x t + G(x t )) \u2212 \u03d5(x t ) = A t (f (x t \u2212 \u03b3 t \u2207f (x t )) \u2212 f (x t )) + B t 2 (z t \u2212 x * ) \u2212 \u03b3 t \u2207f (x t ) 2 \u2212 z t \u2212 x * 2 .", "formula_coordinates": [21.0, 145.74, 92.32, 320.53, 36.38]}, {"formula_id": "formula_87", "formula_text": "f (x t \u2212 \u03b3 t \u2207f (x t )) \u2212 f (x t ) \u2207f (x t ), \u2212\u03b3 t \u2207f (x t ) + L 2 \u03b3 t \u2207f (x t ) 2 = \u2212\u03b3 t (2 \u2212 L\u03b3 t ) 1 2 \u2207f (x t ) 2 .", "formula_coordinates": [21.0, 161.03, 150.13, 289.45, 46.29]}, {"formula_id": "formula_88", "formula_text": "\u03d5(x t + G(x t )) \u2212 \u03d5(x t ) B t \u03b3 2 t \u2212 A t \u03b3 t (2 \u2212 L\u03b3 t ) 1 2 \u2207f (x t ) 2 \u2212 B t \u03b3 t \u2207f (x t ), z t \u2212 x * .(31)", "formula_coordinates": [21.0, 115.58, 217.53, 388.42, 31.16]}, {"formula_id": "formula_89", "formula_text": "I t dA t dt \u2212 A t \u03b7 t \u2207f (x t ), x t \u2212 x * + dB t dt \u2212 B t \u03b7 t 1 2 z t \u2212 x * 2(32)", "formula_coordinates": [21.0, 150.19, 274.13, 353.81, 22.31]}, {"formula_id": "formula_90", "formula_text": "+ (A t \u03b7 t \u2212 B t \u03b3 t ) \u2207f (x t ), z t \u2212 x * + B t \u03b7 t \u2212 dA t dt \u00b5 1 2 x t \u2212 x * 2(33)", "formula_coordinates": [21.0, 180.22, 302.02, 323.78, 22.31]}, {"formula_id": "formula_91", "formula_text": "+ B t \u03b3 2 t \u2212 A t \u03b3 t (2 \u2212 L\u03b3 t ) 1 2 \u2207f (x t ) 2 .(34)", "formula_coordinates": [21.0, 180.22, 328.63, 323.78, 22.31]}, {"formula_id": "formula_92", "formula_text": "dA t dt = A t \u03b7 t , dB t dt = B t \u03b7 t A t \u03b7 t = B t \u03b3 t , B t \u03b7 t = dA t dt \u00b5 , B t \u03b3 2 t = A t L .", "formula_coordinates": [21.0, 128.28, 397.74, 356.63, 22.31]}, {"formula_id": "formula_93", "formula_text": "\u03b3 t = A t LB t .(35)", "formula_coordinates": [21.0, 277.71, 443.89, 226.29, 23.22]}, {"formula_id": "formula_94", "formula_text": "\u03b7 t = B t \u03b3 t A t = 2B t LA t .(36)", "formula_coordinates": [21.0, 260.16, 491.62, 243.84, 23.22]}, {"formula_id": "formula_95", "formula_text": "\u03b7 t = dA t dt \u00b5 B t = A t \u03b7 t \u00b5 B t = \u00b5 A t LB t .(37)", "formula_coordinates": [21.0, 231.44, 539.35, 272.56, 23.22]}, {"formula_id": "formula_96", "formula_text": "dA t dt = A t \u03b7 t = A t B t L , dB t dt = B t \u03b7 t = \u00b5 A t B t L .", "formula_coordinates": [21.0, 169.98, 588.86, 273.23, 22.31]}, {"formula_id": "formula_97", "formula_text": "d dt A t = 1 2 \u221a A t dA t dt = 1 2 B t L , d dt B t = 1 2 \u221a B t dB t dt = \u00b5 2 A t L .(38)", "formula_coordinates": [21.0, 133.02, 659.13, 370.98, 23.78]}, {"formula_id": "formula_98", "formula_text": "d 2 dt 2 A t = \u00b5 4L A t , B t = 2 \u221a L d dt A t .(39)", "formula_coordinates": [21.0, 171.27, 702.42, 332.73, 24.49]}, {"formula_id": "formula_99", "formula_text": "\u221a A t = 1 2 \u221a L , thus \u221a A t = t 2 \u221a", "formula_coordinates": [22.0, 279.27, 97.01, 124.14, 20.54]}, {"formula_id": "formula_100", "formula_text": "Ef (x t ) \u2212 f (x * ) E\u03c6 t A t \u03c6 0 A t = 2L z 0 \u2212 x * 2 t 2 .", "formula_coordinates": [22.0, 205.02, 150.01, 201.95, 24.8]}, {"formula_id": "formula_101", "formula_text": "E [A T k (f (x k ) \u2212 f (x * ))] = E [A T k (f (x T k ) \u2212 f (x * ))] E\u03c6 T k \u03c6 0 = z 0 \u2212 x * 2 .", "formula_coordinates": [22.0, 134.91, 219.35, 342.19, 12.39]}, {"formula_id": "formula_102", "formula_text": "A t = A 0 exp 1 2 \u00b5 L t , B t = A 0 \u221a \u00b5 exp 1 2 \u00b5 L t .", "formula_coordinates": [22.0, 161.48, 294.22, 299.0, 22.31]}, {"formula_id": "formula_103", "formula_text": "\u03b7 t = \u03b7 t = \u00b5 L , \u03b3 t = 1", "formula_coordinates": [22.0, 339.11, 322.25, 104.17, 14.0]}, {"formula_id": "formula_104", "formula_text": "Ef (x t ) \u2212 f (x * ) E\u03c6 t A t \u03c6 0 A t = A 0 (f (x 0 ) \u2212 f (x * )) + A 0 \u00b5 2 z 0 \u2212 x * 2 A t = f (x 0 ) \u2212 f (x * ) + \u00b5 2 z 0 \u2212 x * 2 exp \u2212 \u00b5 L t .", "formula_coordinates": [22.0, 133.18, 369.14, 345.63, 52.99]}, {"formula_id": "formula_105", "formula_text": "dx t = \u03b7 t (z t \u2212 x t )dt , dz t = \u03b7 t (x t \u2212 z t )dt ,", "formula_coordinates": [22.0, 262.35, 523.88, 87.31, 25.87]}, {"formula_id": "formula_106", "formula_text": "dx t = \u03b7 t (z t \u2212 x t )dt \u2212 \u03b3 t \u039e \u2207f (x t , \u03be)dN (t, \u03be) , dz t = \u03b7 t (x t \u2212 z t )dt \u2212 \u03b3 t \u039e \u2207f (x t , \u03be)dN (t, \u03be) .", "formula_coordinates": [22.0, 206.79, 677.5, 198.42, 43.86]}, {"formula_id": "formula_107", "formula_text": "\u03c6 t = A t (f (x t ) \u2212 f * ) + B t 2 z t \u2212 x * 2", "formula_coordinates": [23.0, 226.85, 89.93, 157.81, 22.31]}, {"formula_id": "formula_108", "formula_text": "dx t = b(x t )dt + \u039e G(x t , \u03be)dN (t, \u03be), b(x t ) = 1 \u03b7 t (z t \u2212 x t ) \u03b7 t (x t \u2212 z t ) , G(x t , \u03be) = 0 \u2212\u03b3 t \u2207f (x t , \u03be) \u2212\u03b3 t \u2207f (x t , \u03be) .", "formula_coordinates": [23.0, 108.0, 148.54, 400.25, 32.44]}, {"formula_id": "formula_109", "formula_text": "\u03c6 t = \u03c6 0 + t 0 I s ds + M t ,(40)", "formula_coordinates": [23.0, 251.47, 201.54, 252.53, 26.29]}, {"formula_id": "formula_110", "formula_text": "I t = \u2207\u03d5(x t ), b(x t ) + E \u03be \u03d5(x t + G(x t , \u03be)) \u2212 \u03d5(x t ) .", "formula_coordinates": [23.0, 196.37, 248.0, 219.27, 9.65]}, {"formula_id": "formula_111", "formula_text": "E \u03be \u03d5(x t + G(x t , \u03be)) \u2212 \u03d5(x t ) = A t (E \u03be f (x t \u2212 \u03b3 t \u2207f (x t , \u03be)) \u2212 f (x t )) + B t 2 E \u03be (z t \u2212 x * ) \u2212 \u03b3 t \u2207f (x t , \u03be) 2 \u2212 z t \u2212 x * 2 .", "formula_coordinates": [23.0, 125.49, 290.34, 361.02, 36.38]}, {"formula_id": "formula_112", "formula_text": "f (x t \u2212 \u03b3 t \u2207f (x t , \u03be)) \u2212 f (x t ) \u2207f (x t ), \u2212\u03b3 t \u2207f (x t , \u03be) + L 2 \u03b3 t \u2207f (x t , \u03be) 2 , E \u03be f (x t \u2212 \u03b3 t \u2207f (x t , \u03be)) \u2212 f (x t ) \u2207f (x t ), \u2212\u03b3 t E \u03be \u2207f (x t , \u03be) + L 2 E \u03be \u03b3 t \u2207f (x t , \u03be) 2 .", "formula_coordinates": [23.0, 128.44, 345.26, 355.12, 46.68]}, {"formula_id": "formula_113", "formula_text": "E \u03be f (x t \u2212 \u03b3 t \u2207f (x t , \u03be)) \u2212 f (x t ) \u2212\u03b3 t (2 \u2212 L\u03b3 t ) 1 2 \u2207f (x t ) 2 + \u03c3 2 L\u03b3 2 t 2 .", "formula_coordinates": [23.0, 154.44, 422.57, 303.12, 23.89]}, {"formula_id": "formula_114", "formula_text": "E \u03be (z t \u2212 x * ) \u2212 \u03b3 t \u2207f (x t , \u03be) 2 \u2212 z t \u2212 x * 2 = \u22122\u03b3 t E \u03be \u2207f (x t , \u03be), z t \u2212 x * + \u03b3 2 t E \u03be \u2207f (x t , \u03be) 2 \u22122\u03b3 t \u2207f (x t ), z t \u2212 x * + \u03b3 2 t \u2207f (x t ) 2 + \u03c3 2 \u03b3 2 t . This gives \u03d5(x t + G(x t )) \u2212 \u03d5(x t ) B t \u03b3 2 t \u2212 A t \u03b3 t (2 \u2212 L\u03b3 t ) 1 2 \u2207f (x t ) 2 \u2212 B t \u03b3 t \u2207f (x t ), z t \u2212 x * + \u03c3 2 2 A t L\u03b3 2 t + B t \u03b3 2 t .", "formula_coordinates": [23.0, 107.69, 463.92, 401.51, 96.02]}, {"formula_id": "formula_115", "formula_text": "I t dA t dt \u2212 A t \u03b7 t \u2207f (x t ), x t \u2212 x * + dB t dt \u2212 B t \u03b7 t 1 2 z t \u2212 x * 2 + (A t \u03b7 t \u2212 B t \u03b3 t ) \u2207f (x t ), z t \u2212 x * + B t \u03b7 t \u2212 dA t dt \u00b5 1 2 x t \u2212 x * 2 + B t \u03b3 2 t \u2212 A t \u03b3 t (2 \u2212 L\u03b3 t ) 1 2 \u2207f (x t ) 2 + \u03c3 2 2 A t L\u03b3 2 t + B t \u03b3 2 t ,", "formula_coordinates": [23.0, 150.19, 579.43, 311.12, 78.51]}, {"formula_id": "formula_116", "formula_text": "I t \u03c3 2 A t L .", "formula_coordinates": [23.0, 281.26, 699.62, 49.49, 22.31]}, {"formula_id": "formula_117", "formula_text": "Ef (x t ) \u2212 f (x * ) E\u03c6 t A t \u03c6 0 A t + t 0 EI s ds A t A 0 (f (x 0 ) \u2212 f (x * )) + B 0 z 0 \u2212 x * 2 A t + \u03c3 2 L t 0 A s ds A t . D.", "formula_coordinates": [24.0, 108.0, 91.17, 346.45, 77.62]}, {"formula_id": "formula_118", "formula_text": "t 0 A s ds = 1 2L t 3 3 . Thus Ef (x t ) \u2212 f (x * ) 2L z 0 \u2212 x * 2 t 2 + \u03c3 2 t 3L .", "formula_coordinates": [24.0, 215.81, 177.26, 180.39, 45.01]}, {"formula_id": "formula_119", "formula_text": "t 0 A s ds A 0 \u00b5 L \u22121 exp \u00b5 L t = L \u00b5 A t . Thus Ef (x t ) \u2212 f (x * ) f (x 0 ) \u2212 f (x * ) + \u00b5 2 z 0 \u2212 x * 2 exp \u2212 \u00b5 L t + \u03c3 2 1 \u221a \u00b5L .", "formula_coordinates": [24.0, 119.16, 250.67, 384.84, 66.01]}, {"formula_id": "formula_120", "formula_text": "\u2200x \u2208 R d , f (x) = E 1 2 ( a, x \u2212 b) 2 ,", "formula_coordinates": [24.0, 229.44, 385.1, 153.11, 22.31]}, {"formula_id": "formula_121", "formula_text": "\u2200x \u2208 R d , f (x) = 1 2 x \u2212 x * 2", "formula_coordinates": [24.0, 241.92, 435.72, 121.8, 22.31]}, {"formula_id": "formula_122", "formula_text": "\u03c6 t = A t 2 x t \u2212 x * 2 H + B t 2 z t \u2212 x * 2 .", "formula_coordinates": [24.0, 226.65, 478.02, 158.69, 22.31]}, {"formula_id": "formula_123", "formula_text": "x \u2212 x * 2 H becomes x \u2212 x * 2 , and z t \u2212 x * 2 becomes z t \u2212 x * 2 H \u22121 .", "formula_coordinates": [24.0, 108.0, 548.29, 395.7, 27.6]}, {"formula_id": "formula_124", "formula_text": "\u03c6 t = A t 2 x t \u2212 x * 2 + B t 2 z t \u2212 x * 2 H \u22121 .", "formula_coordinates": [24.0, 222.01, 582.06, 167.98, 22.31]}, {"formula_id": "formula_125", "formula_text": "dx t = b(x t )dt + \u039e G(x t , \u03be)dN (t, \u03be), b(x t ) = 1 \u03b7 t (z t \u2212 x t ) \u03b7 t (x t \u2212 z t ) , G(x t , \u03be) = 0 \u2212\u03b3 t \u2207f (x t , \u03be) \u2212\u03b3 t \u2207f (x t , \u03be) .", "formula_coordinates": [24.0, 108.0, 638.63, 400.25, 32.44]}, {"formula_id": "formula_126", "formula_text": "\u03c6 t = \u03c6 0 + t 0 I s ds + M t ,", "formula_coordinates": [24.0, 251.47, 695.06, 109.06, 26.29]}, {"formula_id": "formula_127", "formula_text": "dx t = \u03b7 t (z t \u2212 x t )dt = \u00b5 L (z t \u2212 x t )dt , dz t = \u03b7 t (x t \u2212 z t )dt = \u00b5 L (x t \u2212 z t )dt ,", "formula_coordinates": [27.0, 240.87, 103.79, 166.12, 50.61]}, {"formula_id": "formula_128", "formula_text": "x t = x t0 + z t0 2 + x t0 \u2212 z t0 2 exp \u22122 \u00b5 L (t \u2212 t 0 ) = x t0 + 1 2 1 \u2212 exp \u22122 \u00b5 L (t \u2212 t 0 ) (z t0 \u2212 x t0 ) , z t = x t0 + z t0 2 + z t0 \u2212 x t0 2 exp \u22122 \u00b5 L (t \u2212 t 0 ) = z t0 + 1 2 1 \u2212 exp \u22122 \u00b5 L (t \u2212 t 0 ) (x t0 \u2212 z t0 ) .", "formula_coordinates": [27.0, 206.56, 178.73, 234.74, 108.82]}, {"formula_id": "formula_129", "formula_text": "k = \u03c4 k = 1 2 1 \u2212 exp \u22122 \u00b5 L (T k+1 \u2212 T k ) and thus \u03c4 k = \u03c4 k 1\u2212\u03c4 k = tanh \u00b5 L (T k+1 \u2212 T k ) . Fi- nally,\u03b3 k = \u03b3 T k = 1 L and\u03b3 k = \u03b3 T k = 1 \u221a \u00b5L .", "formula_coordinates": [27.0, 143.87, 295.65, 361.79, 40.75]}, {"formula_id": "formula_130", "formula_text": "dx t = 2 t (z t \u2212 x t )dt \u2212 1 L \u2207f (x t )dN (t) , dz t = \u2212 t 2L \u2207f (x t )dN (t) .", "formula_coordinates": [27.0, 223.63, 410.55, 164.74, 46.0]}, {"formula_id": "formula_131", "formula_text": "dx s = 2 \u221a Ls (z s \u2212 x s ) \u221a Lds \u2212 1 L \u2207f (x s ) \u221a Lds , dz s = \u2212 \u221a Ls 2L \u2207f (x s ) \u221a Lds .", "formula_coordinates": [27.0, 207.01, 500.88, 197.99, 53.71]}, {"formula_id": "formula_132", "formula_text": "dx s ds = 2 s (z s \u2212 x s ) , dz s ds = \u2212 s 2 \u2207f (x s ) .", "formula_coordinates": [27.0, 265.86, 575.11, 81.47, 46.79]}, {"formula_id": "formula_133", "formula_text": "dx t = \u00b5 L (z t \u2212 x t )dt \u2212 1 L \u2207f (x t )dN (t) , dz t = \u00b5 L (x t \u2212 z t )dt \u2212 1 \u221a \u00b5L \u2207f (x t )dN (t) .", "formula_coordinates": [28.0, 210.6, 113.42, 190.81, 50.94]}, {"formula_id": "formula_134", "formula_text": "dx s = \u00b5 L (z s \u2212 x s ) \u221a Lds \u2212 1 L \u2207f (x s ) \u221a Lds , dz s = \u00b5 L (x s \u2212 z s ) \u221a Lds \u2212 1 \u221a \u00b5L \u2207f (x s ) \u221a Lds .", "formula_coordinates": [28.0, 201.36, 190.88, 209.29, 53.12]}, {"formula_id": "formula_135", "formula_text": "dx s ds = \u221a \u00b5(z s \u2212 x s ) ,(44)", "formula_coordinates": [28.0, 232.45, 279.08, 271.55, 22.31]}, {"formula_id": "formula_136", "formula_text": "dz s ds = \u221a \u00b5(x s \u2212 z s ) \u2212 1 \u221a \u00b5 \u2207f (x s ) .(45)", "formula_coordinates": [28.0, 233.51, 303.55, 270.49, 22.31]}, {"formula_id": "formula_137", "formula_text": "d 2 x s ds 2 + 2 \u221a \u00b5 dx s ds + \u2207f (x s ) = 0 .", "formula_coordinates": [28.0, 237.94, 356.64, 137.32, 23.89]}, {"formula_id": "formula_138", "formula_text": "min x\u2208R d f (x), (46", "formula_coordinates": [28.0, 284.54, 454.76, 215.31, 15.27]}, {"formula_id": "formula_139", "formula_text": ")", "formula_coordinates": [28.0, 499.85, 455.08, 4.15, 8.64]}, {"formula_id": "formula_140", "formula_text": "\u00b5 2 x \u2212 y 2 R f (x) \u2212 f (y) \u2212 \u2207f (x) (x \u2212 y) 1 2 x \u2212 y 2 M .", "formula_coordinates": [28.0, 179.9, 518.26, 253.4, 22.31]}, {"formula_id": "formula_141", "formula_text": "dx t = \u03b7 t (z t \u2212 x t )dt \u2212 \u03b3 t \u039e R \u03be\u03be P \u03be \u2207f (x t , \u03be)dN (t, \u03be) , dz t = \u03b7 t (x t \u2212 z t )dt \u2212 \u03b3 t \u039e \u2207f (x t , \u03be)dN (t, \u03be) ,(47)", "formula_coordinates": [28.0, 197.7, 573.44, 306.3, 51.2]}, {"formula_id": "formula_142", "formula_text": "\u2207f (x t , \u03be) = 1 P \u03be \u2207 \u03be f (x t ),(48)", "formula_coordinates": [28.0, 253.13, 638.25, 250.87, 23.22]}, {"formula_id": "formula_143", "formula_text": "L max \u03be\u2208\u039e M \u03be\u03be R \u03be\u03be P 2 \u03be .(49)", "formula_coordinates": [29.0, 265.62, 102.0, 238.38, 24.74]}, {"formula_id": "formula_144", "formula_text": "1. For \u03b7 t = 2 t , \u03b7 t = 0, \u03b3 t = 1 L , \u03b3 t = t 2L , Ef (x t ) \u2212 f (x * ) 2L z 0 \u2212 x * 2 R t 2 .", "formula_coordinates": [29.0, 131.41, 151.91, 265.32, 42.82]}, {"formula_id": "formula_145", "formula_text": "t = \u03b7 t \u2261 \u00b5 L , \u03b3 t \u2261 1 L , \u03b3 t \u2261 1 \u221a \u00b5L . Then , Ef (x t ) \u2212 f (x * ) f (x 0 ) \u2212 f (x * ) + \u00b5 2 z 0 \u2212 x * 2 R exp \u2212 \u00b5 L t .", "formula_coordinates": [29.0, 143.87, 204.99, 361.88, 58.8]}, {"formula_id": "formula_146", "formula_text": "dx t = b(x t )dt + \u039e G(x t , \u03be)dN (t, \u03be), b(x t ) = 1 \u03b7 t (z t \u2212 x t ) \u03b7 t (x t \u2212 z t ) , G(x t , \u03be) = \uf8eb \uf8ed 0 \u2212\u03b3 t R \u03be\u03be P \u03be \u2207f (x t , \u03be) \u2212\u03b3 t \u2207f (x t , \u03be) \uf8f6 \uf8f8 .", "formula_coordinates": [29.0, 108.0, 316.32, 417.63, 37.05]}, {"formula_id": "formula_147", "formula_text": "\u03c6 t = A t (f (x t ) \u2212 f * ) + B t 2 z t \u2212 x * 2 R", "formula_coordinates": [29.0, 225.81, 376.08, 159.84, 22.31]}, {"formula_id": "formula_148", "formula_text": "\u03c6 t = \u03c6 0 + t 0 I s ds + M t ,(50)", "formula_coordinates": [29.0, 251.47, 441.39, 252.53, 26.29]}, {"formula_id": "formula_149", "formula_text": "I t = \u2207\u03d5(x t ), b(x t ) + E \u03be \u03d5(x t + G(x t , \u03be)) \u2212 \u03d5(x t ) .", "formula_coordinates": [29.0, 196.37, 490.42, 219.27, 9.65]}, {"formula_id": "formula_150", "formula_text": "E \u03be \u03d5(x t + G(x t , \u03be)) \u2212 \u03d5(x t ) = A t E \u03be f x t \u2212 \u03b3 t R \u03be\u03be P \u03be \u2207f (x t , \u03be) \u2212 f (x t ) + B t 2 E \u03be (z t \u2212 x * ) \u2212 \u03b3 t \u2207f (x t , \u03be) 2 R \u2212 z t \u2212 x * 2 R .", "formula_coordinates": [29.0, 123.42, 533.22, 365.15, 49.53]}, {"formula_id": "formula_151", "formula_text": "f x t \u2212 \u03b3 t R \u03be\u03be P \u03be \u2207f (x t , \u03be) \u2212 f (x t ) \u2207f (x t ), \u2212\u03b3 t R \u03be\u03be P \u03be \u2207f (x t , \u03be) + 1 2 \u03b3 t R \u03be\u03be P \u03be \u2207f (x t , \u03be) 2 M .", "formula_coordinates": [29.0, 112.1, 604.74, 387.8, 23.23]}, {"formula_id": "formula_152", "formula_text": "R \u03be\u03be P \u03be \u2207f (x t , \u03be) 2 M = M \u03be\u03be R \u03be\u03be P 2 \u03be \u2207f (x t , \u03be) 2 R L \u2207f (x t , \u03be) 2 R ,(51)", "formula_coordinates": [29.0, 180.42, 652.41, 323.58, 24.74]}, {"formula_id": "formula_153", "formula_text": "\u2207f (x t ), \u2212\u03b3 t R \u03be\u03be P \u03be \u2207f (x t , \u03be) = \u2212\u03b3 t R \u03be\u03be P 2 \u03be \u2207 \u03be f (x t ) 2 = \u03b3 t \u2207f (x t , \u03be) 2 R .(52)", "formula_coordinates": [29.0, 160.9, 700.24, 343.1, 24.74]}, {"formula_id": "formula_154", "formula_text": "E \u03be f x t \u2212 \u03b3 t R \u03be\u03be P \u03be \u2207f (x t , \u03be) \u2212 f (x t ) \u03b3 t (1 \u2212 \u03b3 t L)E \u03be \u2207f (x t , \u03be) 2 R .", "formula_coordinates": [30.0, 160.15, 89.64, 291.71, 23.22]}, {"formula_id": "formula_155", "formula_text": "E \u03be (z t \u2212 x * ) \u2212 \u03b3 t \u2207f (x t , \u03be) 2 R \u2212 z t \u2212 x * 2 R = \u22122\u03b3 t E \u03be R\u2207f (x t , \u03be), z t \u2212 x * + \u03b3 2 t E \u03be \u2207f (x t , \u03be) 2 R \u22122\u03b3 t \u2207f (x t ), z t \u2212 x * + \u03b3 2 t E \u03be \u2207f (x t , \u03be) 2 R . This gives \u03d5(x t + G(x t )) \u2212 \u03d5(x t ) \u2212B t \u03b3 t \u2207f (x t ), z t \u2212 x * + B t \u03b3 2 t \u2212 A t \u03b3 t (2 \u2212 L\u03b3 t ) 1 2 E \u03be \u2207f (x t , \u03be) 2 R .", "formula_coordinates": [30.0, 107.69, 135.91, 375.91, 102.97]}, {"formula_id": "formula_156", "formula_text": "I t dA t dt \u2212 A t \u03b7 t \u2207f (x t ), x t \u2212 x * + dB t dt \u2212 B t \u03b7 t 1 2 z t \u2212 x * 2 R + (A t \u03b7 t \u2212 B t \u03b3 t ) \u2207f (x t ), z t \u2212 x * + B t \u03b7 t \u2212 dA t dt \u00b5 1 2 x t \u2212 x * 2 R + B t \u03b3 2 t \u2212 A t \u03b3 t (2 \u2212 L\u03b3 t ) 1 2 E \u03be \u2207f (x t , \u03be) 2 R .", "formula_coordinates": [30.0, 149.16, 260.1, 313.15, 76.82]}, {"formula_id": "formula_157", "formula_text": "min x\u2208R d f (x) = 1 |V| v\u2208V f v (x) ,(53)", "formula_coordinates": [30.0, 240.82, 456.79, 263.18, 26.8]}, {"formula_id": "formula_158", "formula_text": "min X\u2208R |V|\u00d7d , Xu=Xv \u2200{u,v}\u2208E F (X) = 1 |V| v\u2208V f v (X v ) ,(54)", "formula_coordinates": [30.0, 190.99, 520.61, 313.01, 26.8]}, {"formula_id": "formula_159", "formula_text": "A X = 0,(55)", "formula_coordinates": [30.0, 283.87, 588.64, 220.13, 8.96]}, {"formula_id": "formula_162", "formula_text": "d\u03bb (y) t = \u03b7 t (\u03bb(", "formula_coordinates": [31.0, 119.08, 179.08, 57.23, 13.74]}, {"formula_id": "formula_164", "formula_text": "y t = A\u03bb (y) t , z t = A\u03bb (z) t .(59)", "formula_coordinates": [31.0, 246.69, 280.08, 257.31, 13.74]}, {"formula_id": "formula_166", "formula_text": "T \u2212 k ) v k ) \u2212 \u2207f * ((y T \u2212 k ) w k ) y T k (v k ) = y T \u2212 k (v k ) \u2212 \u03b3 t R {v k ,w k } P 2 {v k ,w k } G {v k ,w k } (T k ) , y T k (w k ) = y T \u2212 k (w k ) + \u03b3 t R {v k ,w k } P 2 {v k ,w k } G {v k ,w k } (T k ) , z T k (v k ) = z T \u2212 k (v k ) \u2212 \u03b3 t G {v k ,w k } (T k ) , z T k (w k ) = z T \u2212 k (w k ) + \u03b3 t G {v k ,w k } (T k ) .(61)", "formula_coordinates": [31.0, 180.88, 452.23, 323.12, 131.64]}, {"formula_id": "formula_167", "formula_text": "E", "formula_coordinates": [32.0, 190.82, 155.68, 6.64, 7.11]}, {"formula_id": "formula_168", "formula_text": "A \u2207 2 F * (A\u03bb)A 1 L A A,(62)", "formula_coordinates": [32.0, 248.63, 390.67, 255.37, 22.31]}, {"formula_id": "formula_169", "formula_text": "M", "formula_coordinates": [32.0, 142.97, 466.84, 9.66, 8.74]}, {"formula_id": "formula_171", "formula_text": "E v\u2208V 1 2 \u2207f * v (z t (v)) \u2212 x 2 E 1 2\u00b5 A\u03bb (z) t \u2212 A\u03bb 2 \u03bb max (AA ) 2\u00b5 E \u03bb (z) t \u2212 \u03bb 2 R . (65", "formula_coordinates": [32.0, 122.21, 570.17, 377.64, 26.8]}, {"formula_id": "formula_172", "formula_text": ")", "formula_coordinates": [32.0, 499.85, 577.23, 4.15, 8.64]}], "doi": "10.1109/TSP.2007.908946"}