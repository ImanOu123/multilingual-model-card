{"title": "CONDITIONAL ANTIBODY DESIGN AS 3D EQUIVARI-ANT GRAPH TRANSLATION", "authors": "Xiangzhe Kong; Wenbing Huang; Yang Liu", "pub_date": "2023-03-30", "abstract": "Antibody design is valuable for therapeutic usage and biological research. Existing deep-learning-based methods encounter several key issues: 1) incomplete context for Complementarity-Determining Regions (CDRs) generation; 2) incapability of capturing the entire 3D geometry of the input structure; 3) inefficient prediction of the CDR sequences in an autoregressive manner. In this paper, we propose Multi-channel Equivariant Attention Network (MEAN) to co-design 1D sequences and 3D structures of CDRs. To be specific, MEAN formulates antibody design as a conditional graph translation problem by importing extra components including the target antigen and the light chain of the antibody. Then, MEAN resorts to E(3)-equivariant message passing along with a proposed attention mechanism to better capture the geometrical correlation between different components. Finally, it outputs both the 1D sequences and 3D structure via a multi-round progressive full-shot scheme, which enjoys more efficiency and precision against previous autoregressive approaches. Our method significantly surpasses state-of-theart models in sequence and structure modeling, antigen-binding CDR design, and binding affinity optimization. Specifically, the relative improvement to baselines is about 23% in antigen-binding CDR design and 34% for affinity optimization.", "sections": [{"heading": "INTRODUCTION", "text": "Antibodies are Y-shaped proteins used by our immune system to capture specific pathogens. They show great potential in therapeutic usage and biological research for their strong specificity: each type of antibody usually binds to a unique kind of protein that is called antigen (Basu et al., 2019). The binding areas are mainly located at the so-called Complementarity-Determining Regions (CDRs) in antibodies (Kuroda et al., 2012). Therefore, the critical problem of antibody design is to identify CDRs that bind to a given antigen with desirable properties like high affinity and colloidal stability (Tiller & Tessier, 2015). There have been unremitting efforts made for antibody design by using deep generative models (Saka et al., 2021;Jin et al., 2021). Traditional methods focus on modeling only the 1D CDR sequences, while a recent work (Jin et al., 2021) proposes to co-design the 1D sequences and 3D structures via Graph Neural Network (GNN).\nDespite the fruitful progress, existing approaches are still weak in modeling the spatial interaction between antibodies and antigens. For one thing, the context information is insufficiently considered. The works (Liu et al., 2020;Jin et al., 2021) only characterize the relation between CDRs and the backbone context of the same antibody chain, without the involvement of the target antigen and other antibody chains, which could lack complete clues to reflect certain important properties for antibody design, such as binding affinity. For another, they are still incapable of capturing the entire 3D geometry of the input structures. One vital property of the 3D Biology is that each structure (molecular, protein, etc) should be independent to the observation view, exhibiting E(3)-equivariance 1 . To fulfill this constraint, the method by Jin et al. (2021) pre-processes the 3D coordinates as certain invariant features before feeding them to the model. However, such pre-procession will lose the message of direction in the feature and hidden spaces, making it less effective in characterizing the spatial proximity between different residues in antibodies/antigens. Further, current generative models (Saka et al., 2021;Jin et al., 2021) predict the amino acids one by one; such autoregressive fashion suffers from low efficiency and accumulated errors during inference.\nTo address the above issues, this paper formulates antibody design as E(3)-equivariant graph translation, which is equipped with the following contributions: 1. New Task. We consider conditional generation, where the input contains not only the heavy-chain context of CDRs but also the information of the antigen and the light chain. 2. Novel Model. We put forward an end-to-end Multichannel Equivariant Attention Network (MEAN) that outputs both 1D sequence and 3D structure of CDRs. MEAN operates directly in the space of 3D coordinates with E(3)-equivariance (other than previously-used E(3)-invariant models), such that it maintains the full geometry of residues. By alternating between an internal context encoder and an external attentive encoder, MEAN leverages 3D message passing along with equivariant attention mechanism to capture long-range and spatially-complicated interactions between different components of the input complex. 3. Efficient Prediction. Upon MEAN, we propose to progressively generate CDRs over multiple rounds, where each round updates both the sequences and structures in a full-shot manner. This progressive fullshot decoding strategy is less prone to accumulated errors and more efficient during the inference stage, compared with traditional autoregressive models (Gebauer et al., 2019;Jin et al., 2021). We validate the efficacy of our model on three challenging tasks: sequence and structure modeling, antigen-binding CDR design and binding affinity optimization. Compared to previous methods that neglect the context of light chain and antigen (Saka et al., 2021;Akbar et al., 2022;Jin et al., 2021), our model achieves significant improvement on modeling the 1D/3D joint distribution, and makes a great stride forward on recovering or optimizing the CDRs that bind to the target antigen.", "publication_ref": ["b5", "b27", "b42", "b37", "b21", "b21", "b31", "b21", "b21", "b37", "b21", "b12", "b21", "b37", "b1", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORK", "text": "Antibody Design Early approaches optimize antibodies with hand-crafted energy functions (Li et al., 2014;Lapidoth et al., 2015;Adolf-Bryfogle et al., 2018), which rely on costly simulations and have intrinsic defects because the inter-chain interactions are complicated in nature and cannot be fully captured by simple force fields or statistical functions (Graves et al., 2020). Hence, attention has been paid to applying deep generative models for 1D sequence prediction (Alley et al., 2019;Liu et al., 2020;Saka et al., 2021;Akbar et al., 2022). Recently, to further involve the 3D structure, Jin et al. (2021) proposes to design the sequences and structures of CDRs simultaneously. It represents antibodies with E(3)-invariant features upon the distance matrix and generates the sequences autoregressively. Its follow-up work (Jin et al., 2022) further involves the epitope, but only considers CDR-H3 without all other components in the antibody. Different from the above learning-based works, our method considers a more complete context by importing the 1D/3D information of the antigen and the light chain. More importantly, we develop an E(3)-equivariant model that is skilled in representing the geometry of and the interactions between 3D structures. Note that antibody design assumes both the CDR sequences and structures are unknown, whereas general protein design predicts sequences based on structures (Ingraham et al., 2019;Karimi et al., 2020;Cao et al., 2021).", "publication_ref": ["b30", "b28", "b0", "b13", "b3", "b31", "b37", "b1", "b21", "b22", "b19", "b25", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Equivariant Graph Neural Networks", "text": "The growing availability of 3D structural data in various fields (Jumper et al., 2021) leads to the emergence of geometrically equivariant graph neural networks (Klicpera et al., 2020;Liu et al., 2021;Puny et al., 2021;. In this paper, we exploit the scalarization-based E(n)-equivariant GNNs (Satorras et al., 2021) as the building block of our MEAN. Specifically, we adopt the multichannel extension by  that naturally complies with the multichannel representation of a residue. Furthermore, we have developed a novel equivariant attention mechanism in MEAN to better capture antibody-antigen interactions.\ndomains, and a variable domain (V H /V L ) that has three Complementarity Determining Regions (CDRs). Antigen-binding sites occur on the variable domain where the interacting regions are mostly CDRs, especially CDR-H3. The remainder of the variable domain other than CDRs is structurally well conserved and often called framework region (Kuroda et al., 2012;Jin et al., 2021). Therefore, previous works usually formalize the antibody design problem as finding CDRs that fit into the given framework region (Shin et al., 2021;Akbar et al., 2022). As suggested by (Fischman & Ofran, 2018;Jin et al., 2021), we focus on generating CDRs in heavy chains since they contribute the most to antigen-binding affinity and are the most challenging ones to characterize. Nevertheless, in contrast to previous studies, we additionally incorporate the antigen and the light chain into the context in the form of antibody-antigen complexes, to better control the binding specificity of generated antibodies. The structure of an antibody which is symmetric and Y-shaped, and we focus on the three versatile CDRs on the variable domain of the heavy chain. (C) Schematic graph construction for the antigenantibody complex, with global nodes, internal context edges E in and external interaction edges E ex .\nWe represent each antibody-antigen complex as a graph of three spatially aggregated components,\ndenoted as G = (V := {V H , V L , V A }, E := {E in , E ex }).\nHere, the components V H , V L , V A correspond to the nodes (i.e. the residues) of the heavy chain, the light chain and the antigen, respectively; E in and E ex separately contain internal edges within each component and external edges across components. To be specific, each node in V, i.e., v i = (h i , Z i ) is represented as a trainable feature embedding vector h i = s ai \u2208 R da according to its amino acid type a i and a matrix of coordinates Z i \u2208 R 3\u00d7m consisting of m backbone atoms. In our case, we set m = 4 by choosing 4 backbone atoms {N, C \u03b1 , C, O}, where C \u03b1 denotes the alpha carbon of the residue and others refer to the atoms composing the peptide bond (Figure 1). We denote the residues in CDRs to be generated as V C = {v c1 , v c2 , ..., v c n(c) }, which is a subset of V H . Since the information of each v ci is unknown in the first place, we initialized its input feature with a mask vector and the coordinates according to the even distribution between the residue right before CDRs (namely, v c1\u22121 ) and the one right after CDRs (namely, v c n(c) +1 ). In our main experiments ( \u00a7 4), we select the 48 residues of the antigen closest to the antibody in terms of the C \u03b1 distance as the epitope like Jin et al. (2021). Instead of such hand-crafted residue selection, in Appendix L, we also incorporate the full antigen and let our method determine the epitope automatically, where the efficacy of our method is still exhibited.", "publication_ref": ["b23", "b26", "b32", "b33", "b38", "b27", "b21", "b40", "b1", "b10", "b21", "b21"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Edge construction", "text": "We now detail how to construct the edges. For the internal edges, E in is defined as the edges connecting each pair of nodes within the same component if the spatial distance in terms of C \u03b1 is below a cutoff distance c 1 . Note that adjacent residues in a chain are spatially close and we always include the edge between adjacent residues in E in . In addition, we assign distinct edge types by setting e ij = 1 for those adjacency residues and e ij = 0 for others, to incorporate the 1D position information. For the external edges E ex , they are derived if the nodes from two different components have a distance less than a cutoff c 2 (c 2 > c 1 ). It is indeed necessary to separate internal and external interactions because their distance scales are very different. The external connections actually represent the interface between different chains, which dominates the binding affinity (Chakrabarti & Janin, 2002), and they are formed mainly through inter-molecular forces instead of chemical bonds that form the internal connections within chains (Yan et al., 2008). Note that all edges are constructed without the information of ground-truth CDR positions.", "publication_ref": ["b7", "b45"], "figure_ref": [], "table_ref": []}, {"heading": "Global nodes", "text": "The shape of CDR loops is closely related to the conformation of the framework region (Baran et al., 2017). Therefore, to make the generated CDRs aware of the entire context of the chain they are in, we additionally insert a global node into each component, by connecting it to all other nodes in the component. Besides, the global nodes of different components are linked to In each iteration, we alternate the internal context encoding and external interaction encoding over L layers, and then update the input features and coordinates of CDRs for the next iteration with the predicted values.\neach other, and all edges induced by the global nodes are included in E in . The coordinates of a global node are given by the mean of all coordinates of the variable domain of the corresponding chains.\nTask formulation Given the 3D antibody-antigen complex graph G = (V, E), we seek a 3D equivariant translator f to generate the amino acid type and 3D conformation for each residue in CDRs V C . Distinct from 1D sequence translation in conventional antibody design, our task requires to output 3D information, and more importantly, we emphasize equivariance to reflect the symmetry of our 3D world-the output of f will translate/rotate/reflect in the same way as its input. We now present how to design f in what follows.", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "MEAN: MULTI-CHANNEL EQUIVARIANT ATTENTION NETWORK", "text": "To derive an effective translator, it is crucial to capture the 3D interactions of the residues in different chains. The message passing mechanism in E(3)-equivariant GNNs (Satorras et al., 2021; will fulfil this purpose. Particularly, we develop the Multi-channel Equivariant Attention Network (MEAN) to characterize the geometry and topology of the input antibody-antigen complex. Each layer of MEAN alternates between the two modules: internal context encoder and external interaction encoder, which is motivated by the biological insight that the external interactions between antibodies and antigens are different from those internal interactions within each heavy/light chain. After several layers of the message passing, the node representations and coordinates are transformed into the predictions by an output module. Notably, all modules are E(3)-equivariant.\nInternal context encoder Similar to GMN , we extend EGNN (Satorras et al., 2021) from one single input vector to multichannel coordinates, since each residue is naturally represented by multiple backbone atoms. Suppose in layer l the node features are {h (l)\ni |i = 1, 2, ..., n} and the coordinates are {Z (l) i |i = 1, 2, ..., n}. We denote the relative coordinates between node i and j as Z\n(l) ij = Z (l) i \u2212 Z (l) j .\nThen, the information of each node is updated in the following form.\nm ij = \u03c6 m (h (l) i , h (l) j , (Z (l) ij ) Z (l) ij (Z (l) ij ) Z (l) ij F , e ij ),(1)\nh (l+0.5) i = \u03c6 h (h (l) i , j\u2208N (i|Ein) m ij ),(2)\nZ (l+0.5) i = Z (l) i + 1 |N (i|E in )| j\u2208N (i|Ein) Z (l) ij \u03c6 Z (m ij ),(3)\nwhere N (i|E in ) denotes the neighbors of node i regarding the internal connections E in , \u2022 F returns the Frobenius norm, and \u03c6 m , \u03c6 x , \u03c6 h , \u03c6 Z are all Multi-Layer Perceptons (MLPs) (Gardner & Dorling, 1998). Basically, m ij gathers the E(3)-invariant messages from all neighbors; then it is used to update h i via \u03c6 h and Z i via \u03c6 Z that is additionally left multiplied with Z (l) ij to keep the direction information. As E in also contains the connections between global nodes in different components, the encoder here actually involves inter-component message passing, although in a global sense. We use the superscript (l + 0.5) to indicate the features and coordinates that will be further updated by the external attentive encoder in this layer.\nExternal attentive encoder This module exploits the graph attention mechanism (Veli\u010dkovi\u0107 et al., 2017) to better describe the correlation between the residues of different components, but different from Veli\u010dkovi\u0107 et al. (2017), we design a novel E(3)-equivariant graph attention scheme based on the multichannel scalarization in the above internal context encoder. Formally, we have:\n\u03b1 ij = exp(q i k ij ) j\u2208N (i|Eex) exp(q i k ij ) ,(4)\nh (l+1) i = h (l+0.5) i + j\u2208N (i|Eex) \u03b1 ij v ij ,(5)\nZ (l+1) i = Z (l+0.5) i + j\u2208N (i|Eex) \u03b1 ij Z (l+0.5) ij \u03c6 Z (v ij ),(6)\nwhere, N (i|E ex ) denotes the neighbors of node i defined by the external interactions E ex ; q i , k ij , and v ij are the query, key, and value vectors, respectively, and \u03b1 ij is the attention weight from node j to i. Specifically, q i = \u03c6 q (h\n(l+0.5) i ), k ij = \u03c6 k ( (Z (l+0.5) ij ) Z (l+0.5) ij (Z (l+0.5) ij ) Z (l+0.5) ij F , h (l+0.5) j\n), and v ij =\n\u03c6 v ( (Z (l+0.5) ij ) Z (l+0.5) ij (Z (l+0.5) ij ) Z (l+0.5) ij F , h (l+0.5) j\n) are all E(3)-invariant, where the functions \u03c6 q , \u03c6 k , \u03c6 v are MLPs.\nOutput module After L layers of the alternations between the last two modules, we further conduct Eq. (1-3) to output the hidden featureh i and coordinatesZ i . To predict the probability of each amino acid type, we apply a SoftMax onh i : p i = Softmax(h i ), where p i \u2208 R na is the predicted distribution over all amino acid categories.\nA desirable property of MEAN is that it is E(3)-equivariant. We summarize it as a formal theorem below, with the proof deferred to Appendix E.\nTheorem 1. We denote the translation process by MEAN as\n{(p i ,Z i )} i\u2208V C = f ({h (0) i , Z(0)\ni } i\u2208V ), then f is E(3)-equivariant. In other words, for each transformation g \u2208 E(3), we have {(p i , g \u2022 Z i )} i\u2208V C = f ({h (0) i , g \u2022 Z (0) i } i\u2208V )\n, where the group action \u2022 is instantiated as g \u2022 Z := OZ for orthogonal transformation O \u2208 R 3\u00d73 and g \u2022 Z := Z + t for translation transformation t \u2208 R 3 .", "publication_ref": ["b38", "b11", "b43", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "PROGRESSIVE FULL-SHOT DECODING", "text": "Traditional methods (such as RefineGNN (Jin et al., 2021)) unravel the CDR sequence in an autoregressive way: generating one amino acid at one time. While such strategy is able to reduce the generation complexity, it inevitably incurs expensive computing and memory overhead, and will hinder the training owing to the vanishing gradient for long CDR sequences. It also acumulates errors during the inference stage. Here, thanks to the rich expressivity of MEAN, we progressively generate the CDRs over T iterations (T is much smaller than the length of the CDR sequences), and in each iteration, we predict the amino acid type and 3D coordinates of all the nodes in V C at once. We call our scheme as full-shot decoding to distinguish it from previous autoregressive approaches.\nTo be specific, given the CDRs' amino acid distribution and conformation {p\n(t) i ,Z (t) i } i\u2208V C from iteration t, we first update the embeddings of all nodes: h i = na j=1 p (t) i (j)s j , \u2200i \u2208 V C , where p (t)\ni (j) returns the probability for the j-th class and s j is the corresponding learnable embedding as defined before. Such weighted strategy leads to less accumulated error during inference compared to the maximum selection counterpart. We then replace the CDRs with the new values {h i ,Z (t) i } i\u2208V C , and denote the new graph as G (t+1) . The edges are also constructed dynamically according to the new graph. We update the next iteration as {p\n(t+1) i ,Z (t+1) i } i\u2208V C = MEAN(G (t+1) ).\nFor sequence prediction, we exert supervision for each node at each iteration:\nL seq = 1 T t 1 |V C | i\u2208V C ce (p (t) i ,p i ),(7)\nwhere ce denotes the cross entropy between the predicted distribution p (t) i and the true onep i . For structure prediction, we only exert supervision on the output iteration. Since there are usually noises in the coordination data, we adopt the Huber loss (Huber, 1992) other than the common MSE loss to avoid numerical instability (further explanation can be found in Appendix F):\nL struct = 1 |V C | i\u2208V C huber (Z (T ) i ,\u1e90 i ),(8)\nwhere\u1e90 i is the label. One benefit of the structure loss is that it conducts directly in the coordinate space, and it is still E(3)-invariant as our model is E(3)-equivariant. This is far more efficient than the loss function used in RefineGNN (Jin et al., 2021). To ensure invariance, RefineGNN should calculate over pairwise distance and angle other than coordinates, which is tedious but necessary since it can only perceive the input of node and edge features after certain invariant transformations. Finally, we balance the above two losses with \u03bb to form L = L seq + \u03bbL struct .", "publication_ref": ["b21", "b18", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTS", "text": "We assess our model on the three challenging tasks: 1. The generative task on the Structural Antibody Database (Dunbar et al., 2014) Akbar et al. (2022) to encode the context of the heavy chain and another LSTM to decode the CDRs. We implement the cross attention between the encoder and the decoder, but utilize the sequence information only. Built upon LSTM, we further test C-LSTM to consider the entire context of the antibody-antigen complex, where each component is separated by a special token. RefineGNN (Jin et al., 2021) is related to our method as it also considers the 3D geometry for antibody generation, but distinct from our method it is only E(3)-invariant and autoregressively generate the amino acid type of each residue. Since its original version only models the heavy chain, we extend it by accommodating the whole antibody-antigen complex, which is denoted as C-RefineGNN; concretely, each component is identified with a special token in the sequence and a dummy node in the structure. As denoted before, we term our model as MEAN. We train each model for 20 epochs and select the checkpoint with the lowest loss on the validation set for testing. We use the Adam optimizer with the learning rate 0.001. For MEAN, we run 3 iterations for the progressive full-shot decoding. More details are provided in Appendix D. For LSTM and RefineGNN, we borrow their default settings and source codes for fair comparisons.", "publication_ref": ["b8", "b1", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "SEQUENCE AND STRUCTURE MODELING", "text": "For quantitative evaluation, we employ Amino Acid Recovery (AAR), defined as the overlapping rate between the predicted 1D sequences and ground truths, and Root Mean Square Deviation (RMSD) regarding the 3D predicted structure of CDRs. Thanks to its inherent equivariance, our model can directly calculate RMSD of coordinates, unlike other baselines that resort to the Kabsch technique Kabsch (1976) to align the predicted and true coordinates prior to the RMSD computation. Our model requires each input complex to be complete (consisting of heavy chains, light chains, and antigens). Hence, we choose 3,127 complexes from the Structural Antibody Database (Dunbar et al., 2014, SAbDab) and remove other illegal datapoints that lack light chain or antigen. All selected complexes are renumbered under the IMGT scheme (Lefranc et al., 2003). As suggested by Jin et al. (2021), we split the dataset into training, validation, and test sets according to the clustering of CDRs to maintain the generalization test. In detail, for each type of CDRs, we first cluster the sequences via MMseqs2 (Steinegger & S\u00f6ding, 2017) that assigns the antibodies with CDR sequence identity above 40% to the same cluster, where the BLOSUM62 substitution matrix (Henikoff & Henikoff, 1992) is adopted to calculate the sequence identity. The total numbers of clusters for CDR-H1, CDR-H2, and CDR-H3 are 765, 1093, and 1659, respectively. Then we split all clusters into training, validation, and test sets with a ratio of 8:1:1. We conduct 10-fold cross validation to obtain reliable results. Further details are provided in Appendix A.\nResults Table 1 (Top) demonstrates that our MEAN significantly surpasses all other methods in terms of both the 1D sequence and 3D structure modeling, which verifies the effectiveness of MEAN Results As shown in Table 2, MEAN outperforms all baselines by a large margin in terms of both AAR and TM-score. Particularly on TM-score, the value by MEAN approaches above 0.99, implying that the designed structure is almost the same as the original one. To better show this, we visualize an example in Figure 3, where the generated fragment by MEAN almost overlaps with the ground truth, while the result of RefineGNN exhibits an apparent bias.", "publication_ref": ["b24", "b29", "b21", "b41", "b16"], "figure_ref": ["fig_2"], "table_ref": ["tab_1", "tab_3"]}, {"heading": "AFFINITY OPTIMIZATION", "text": "It is crucial to optimize various properties like binding affinity of antibodies for therapeutic purposes. This can be formulated as a search problem over the intrinsic space of generative models. In our case, we jointly optimize the sequence and structure of CDR-H3 to improve the binding affinity of   Results As shown in Table 2 (middle), our MEAN models achieve obvious progress towards discovering antibodies with better binding affinity. This further validates the advantage of explicitly modeling the interface with MEAN. Moreover, we provide the predicted \u2206\u2206G of mutating the CDRs to random sequences, denoted as Random, for better interpretation of the results. We provide further interpretation in Appendix C. We also provide a visualization example in Figure 3 (B), which indicates our MEAN does produce a novel CDR-H3 sequence/structure, with improved affinity.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": ["tab_3"]}, {"heading": "CDR-H3 DESIGN WITH DOCKED TEMPLATE", "text": "We further provide a possible pipeline to utilize our model in scenarios when the binding complex is unknown. Specifically, for antigens from RAbD (Adolf-Bryfogle et al., 2018), we aim at generating binding antibodies with high affinity. To this end, we first select an antibody from the database and remove its CDR-H3, then we use HDOCK (Yan et al., 2017) to dock it to the target antigen to obtain a template of antibody-antigen complex. With this template, we employ our model to generate antigen-binding CDR-H3s in the same way as \u00a7 4.2. To alleviate the risk of docking inaccuracy, we compose 10 such templates for each antigen and retain the highest scoring one in the subsequent generation. We first refine the generated structure with OpenMM (Eastman et al., 2017) and Rosetta (Alford et al., 2017), and then use the energy functions in Rosetta to measure the binding affinity. The comparison of the affinity distribution between the generated antibodies by our method, those by C-RefineGNN, and the original ones in RAbD is shown in Figure 4 (B). Obviously, the antibodies designed by our MEAN exhibit higher predicted binding affinity. We also present a tightly binding example in Figure 4 ", "publication_ref": ["b0", "b46", "b9"], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "ANALYSIS", "text": "We test if each proposed technique is necessary in MEAN. Table 3 shows that the removal of either the global nodes or the attention mechanism induces performance detriment. This is reasonable since the global nodes transmit information within and between components globally, and the attentive module concentrates on the local information around the interface of different components. In practice, the attentive module also provides interpretability over the significance of pairwise residue interactions, as illustrated in Appendix I. In addition, it is observed that only using the heavy chain weakens the performance apparently, and fails to derive feasible solution for the affinity optimization task, which empirically supports the necessity of inputting antigens and light chains in MEAN. Moreover, we implement a variant of MEAN by replacing the progressive full-shot decoding with the iterative refinement operation used in RefineGNN, whose performance is worse than MEAN. As discussed before, our full-shot decoding is much more efficient than the iterative refinement process, since the number of iterations in MEAN is 3 which is much smaller than that of the refinement-based variant. As reported in Table 2 (right), our method speeds up approximately 2 to 5 times depending on the lengths of the CDR sequences. We also analyze the complexity, MEAN injected with randomness, and the progressive decoding process in Appendix G, H, and J, respectively. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4", "tab_3"]}, {"heading": "CONCLUSION", "text": "In this paper we formulate antibody design as translation from the the entire context of antibodyantigen complex to versatile CDRs. We propose multi-channel equivariant attention network (MEAN) to identify and encode essential local and global information within and between different chains. We also propose progressive full-shot decoding strategy for more efficient and precise generation. Our model outperforms baselines by a large margin in terms of three generation task including distribution learning on 1D sequences and 3D structures, antigen-binding CDR-H3 design, and affinity optimization. Our work presents insights for modeling antibody-antigen interactions in further research. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A DETAILS OF SEQUENCE AND STRUCTURE MODELING", "text": "10-fold dataset splits We provide the number of clusters and antibodies in each fold of our 10-fold cross validation. When selecting fold i for testing, we use fold i \u2212 1 for validation (for fold 1 as the test set, we use fold 10 for validation) and the union of other folds for training.  (Jin et al., 2021) and the models are denoted as LSTM * , RefineGNN * , and MEAN * . Here we provide the sizes of training/validation/test set as well as their subsets of complete complexes in Table 5. We use the subsets to train MEAN * , which is only about 52% of the full sets, and all three models are evaluated on the subset of the test set for fair comparison. For RefineGNN * , we directly use the official checkpoints provided by Jin et al. (2021) for evaluation. ", "publication_ref": ["b21", "b21"], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "B ITERATIVE TARGET AUGMENTATION ALGORITHM FOR AFFINITY OPTIMIZATION", "text": "Notably, the original algorithm is designed for discrete properties, while in \u00a7 4.3 the affinity is continuous. Therefore, we adapt ITA for compatibility with our affinity scorer as follows. The core is that we maintain a list of high-quality candidates for each antibody to be optimized during the ITA process. In each iteration, we produce C candidates for each antibody and sort them together with the candidates in the high-quality list according to the scores. Then we retain the top-k candidates while dropping all others. The above process goes through all the candidates in the current list before entering the next iteration. It is expected that the distribution of the generated antibodies will move towards the higher-affinity landscape. In particular, we run 20 iterations for each pretrained generative model by setting C = 50 and k = 4.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C THE PREDICTOR USED IN AFFINITY OPTIMIZATION", "text": "According to Shan et al. (2022), the output of the \u2206\u2206G predictor has been calibrated under the unit of kcal/mol, and the correlation between the predicted \u2206\u2206G and the experimental value is 0.65 on the test set. It means if the predicted \u2206\u2206G is x, then the binding affinity is decreased by x kcal/mol under a correlation of 0.65.", "publication_ref": ["b39"], "figure_ref": [], "table_ref": []}, {"heading": "D EXPERIMENT DETAILS AND HYPERPARAMETERS", "text": "For all models incorporating antigen, we select 48 residues closest to the antibody in terms of the alpha carbon distance as the antigen information. We conduct experiments on a machine with 56 CPU cores and 10 GeForce RTX 2080 Ti GPUs. Models using iterative refinement decoding have intensive GPU memory requirements and are therefore trained with the data-parallel framework of Pytorch on 6 GPUs. Other models only need 1 GPU to train. We use Adam optimizer with lr = 0.001 and decay the learning rate by 0.95 every epoch. The batch size is set to be 16. All models are trained for 20 epochs and the checkpoint with the lowest loss on the validation set is selected for testing. The training strategy is consistent across different experiments in the paper, and the learning rate of ITA finetuning is also set to 0.001. Furthermore, we provide the hyperparameters for baselines and our MEAN in Table 6. For the RosettaAD (Alford et al., 2017) used in \u00a7 4.2, we adopt the denovo design protocol initializing with random CDRs presented in its manual 4 . For training our MEAN on the split in RefineGNN paper setting, we set alpha = 0.6, batch size to be 8, and total training epochs to be 30.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "E PROOF OF THEOREM 1", "text": "Our MEAN satisfies E(3)-equivariance as demonstrated in Theorem 1:\nTheorem 1. We denote the translation process by MEAN as\n{(p i ,Z i )} i\u2208V C = f ({h (0) i , Z(0)\ni } i\u2208V ), then f is E(3)-equivariant. In other words, for each transformation g \u2208 E(3), we have f ({h Proof. \u2200g \u2208 O(3), we have g \u2022 Z := OZ. Therefore, we can derive the following equations:\n(0) i , g \u2022 Z (0) i } i\u2208V ) = g \u2022 f ({h (0) i , Z (0) i } i\u2208V ),\nf (g \u2022 Z) = (OZ) T (OZ) |(OZ) T (OZ)| F = Z T O T OZ |Z T O T OZ| F (9) = Z T Z |Z T Z| F = f (Z),(10)\nwhich conclude the invariance of f on O(3).\nLemma 2. We denote the internal context encoder of layer l as {(h 3) and t \u2208 R 3 . Therefore we have the relative coordinates after transformation as g \u2022 Z\n(l+0.5) i , Z (l+0.5) i )} i\u2208V = \u03c3 in ({(h (l) i , Z (l) i )} i\u2208V ), then \u03c3 in is E(3)-equivariant. Proof. \u2200g \u2208 E(3), we have g \u2022 Z (l) i := OZ (l) i + t where O \u2208 O(\n(l) i \u2212 g \u2022 Z (l) j = OZ (l)\nij . According to Lemma 1 , during the propagations of our internal context encoder, we have E(3)-invariant messages:\nm ij = \u03c6 m (h (l) i , h (l) j , f (OZ (l) ij ), e ij ) = \u03c6 m (h (l) i , h (l) j , f (Z (l) ij ), e ij ),(11)\nThen it is easy to derive that the hidden states are E(3)-invariant and the coordinates are E(3)equivariant as follows:\nh (l+0.5) i = \u03c6 h (h (l) i , j\u2208N (i|Ein) m ij ),(12)\nOZ (l+0.5) i + t = O[Z (l) i + 1 |N (i|E in )| j\u2208N (i|Ein) Z (l) ij \u03c6 Z (m ij )] + t (13) = (OZ (l) i + t) + 1 |N (i|E in )| j\u2208N (i|Ein) (OZ (l) ij )\u03c6 Z (m ij ),(14)\nTherefore we have g \u2022 \u03c3 in ({(h\n(l) i , Z (l) i )} i\u2208V ) = \u03c3 in ({(h (l) i , g \u2022 Z (l) i )} i\u2208V ).\nLemma 3. We denote the external attentive encoder as {(h\n(l+1) i , Z (l+1) i )} i\u2208V = \u03c3 ex ({(h (l+0.5) i , Z (l+0.5) i )} i\u2208V ), then \u03c3 ex is E(3)-equivariant.\nProof. Similar to the proof of Lemma 2, we first derive that the query, key and value vectors are E(3)-invariant:\nq i = \u03c6 q (h (l+0.5) i ),(15)\nk ij = \u03c6 k (f (OZ (l+0.5) ij ), h (l+0.5) j ) = \u03c6 k (f (Z (l+0.5) ij ), h (l+0.5) j ),(16)\nv ij = \u03c6 v (f (OZ (l+0.5) ij ), h (l+0.5) j ) = \u03c6 v (f (Z (l+0.5) ij ), h (l+0.5) j ),(17)\nwhich directly lead to the E(3)-invariance of attention weights \u03b1 ij = exp(q i kij ) j\u2208N (i|Eex ) exp(q i kij ) . Again it is easy to derive that the hidden states are E(3)-invariant and the coordinates are E(3)equivariant as follows:\nh (l+1) i = h (l+0.5) i + j\u2208N (i|Eex) \u03b1 ij v ij ,(18)\nOZ (l+1) i + t = O(Z (l+0.5) i + j\u2208N (i|Eex) \u03b1 ij Z (l+0.5) ij v ij ) + t (19) = (OZ (l+0.5) i + t) + j\u2208N (i|Eex) \u03b1 ij OZ (l+0.5) ij v ij .(20)\nTherefore we have g \u2022 \u03c3 ex ({(h\n(l+0.5) i , Z (l+0.5) i )} i\u2208V ) = \u03c3 ex ({(h (l+0.5) i , g \u2022 Z (l+0.5) i\n)} i\u2208V ).  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "I ATTENTION VISUALIZATION", "text": "In the external attentive encoder, we apply the attention mechanism to evaluate the weights between residues in different components. It will be interesting to visualize what patterns these attentions will discover. For this purpose, we extract the attention weights between the antibody and the antigen from the last layer, and check if they can reflect the binding energy calculated by Rosetta (Alford et al., 2017). In detail, for each residue in CDR-H3, we first identify the residue in the antigen that contributes the most to its binding energy. Then we calculate the rank of the identified residue according to the attention weights yielded by MEAN. We obtain the relative rank by normalizing it with the total number of antigen residues in the interface. If the attention weights are meaningful, then the resultant rank distribution will be bias towards small numbers; otherwise, they are distributed evenly between 0 and 1. Excitingly, Figure 5 (B) displays that we arrive at the former case, indicating the close correlation between our attention weights and the binding energy calculated by Rosetta. Figure 5 (A) also visualizes an example of attention weights and the corresponding energy map, which shows that their distributions are similar. ", "publication_ref": [], "figure_ref": ["fig_5", "fig_5"], "table_ref": []}, {"heading": "A B", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "J HOW GRAPH CHANGES DURING PROGRESSIVE DECODING", "text": "We depict the variations of the density distribution of PPL and RMSD across different rounds in the progressive full-shot decoding in Figure 6. Between different rounds, the distribution of PPL remains similar, which is expected since we exert supervision with the ground truth in all the rounds. It is beyond expectation that even if we only supervise the predicted coordination of the last round, the distribution of RMSD shifts rapidly towards the optimal direction.\nWe additionally calculate the recovery rate of edges in the ground truth graph on the test set, in terms of internal edges within each component and external edges across different components. For the internal edges, the recovery rate is 89% in the beginning and 95% in the end; for the external edges, the recovery rate is 11% in the beginning and 84% in the end. The results above indicate that the linear initialization is able to recover a large part of the internal edges but only a very small percentage of external edges. By our model, we can predict a major part of both internal and external edges, suggesting the validity of our design.", "publication_ref": [], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "K LOCAL GEOMETRY", "text": "Since both RMSD and TM-score reflect the correctness of global geometry, we further provide RMSD of bond lengths and angles to validate the local geometry for Section 4.2. The bond lengths are measured in angstroms and the angles consist of the three conventional dihedral angles of the backbone structure, namely \u03c6, \u03c8, \u03c9 (Ramachandran et al., 1963). The RMSD of angles is implemented as the average of RMSD on their cosine values. The results shown in Table 9 indicate that our model still achieves much better performance regarding the local geometry. We find that Re-fineGNN achieves relatively low performance on modeling local geometry, which might be because that its indirect loss on various invariant features cannot ensure atoms in the backbone are equally supervised. For example, the precision of the coordinates of the carboxy carbon is relatively low with a high RMSD of 10.8, which results in the high error in bond lengths of local geometry. ", "publication_ref": ["b34"], "figure_ref": [], "table_ref": ["tab_11"]}, {"heading": "L FULL ANTIGEN OR ONLY EPITOPE", "text": "In this paper, we use the 48 residues closest to the antibody to represent the antigen information.\nFrom the perspective of biology, these residues compose the epitope (i.e. the binding position of antibodies) which is usually located by a biological expert or detected via certain computational methods (Haste Andersen et al., 2006) beforehand, and they usually provide enough information for designing the antigen-binding antibodies. Theoretically, only the residues close to the antibody will affect the message passing between the antigen and the antibody because we construct edges based on a cutoff of C \u03b1 distance, therefore the performance should be similar regardless of using the full antigen or only the epitope. Practically, we also explore the influence of exclusion of other residues in the antigen on our model. Specifically, we incorporate full antigen in the experiment of \u00a7 4.2 and present the results in Table 10. As expected, incorporating full antigen results in similar performance to the epitope-only strategy with slight change in AAR and structure modeling. Note that the efficacy of our pipeline for affinity optimization is influenced by the generalizability of the predictor, hence how to choose a desirable predictor is vital. As proof of concept, we currently apply the predictor in Shan et al. (2022) for its easy implementation and fast computation. Since our pipeline is general, it is possible to replace the predictor with other variants, such as wetlab validation, which can return real affinity but is time-consuming. One can also combine the advantages of the learning-based predictor and wetlab validation to improve the generalizability while keeping efficiency, by, for example, choosing only top-k samples and using the wetlab feedback to rectify the predictor, creating a so-called closed loop between \"dry computation\" and \"wet experiment\" akin to the pipeline used in Shan et al. (2022). As increasing attention has been paid to this domain, we believe more and more robust and efficient predictors will emerge in the future.\nWhile the pipeline for affinity optimization only addresses a narrow need in the field of antibody discovery, we also discuss the potential pipeline for the 'real-world' question here (i.e. generate a binidng antibody given an arbitrary antigen). The 'real-world' question might be decomposed into several components: epitope identification, antibody structure prediction, docking, CDR design, and affinity prediction. Each component itself is challenging and currently a promising topic in the community. In \u00a7 4.4, we actually combined the last three components, where we used HDock for global docking to form the initial complex, our MEAN for CDR design, and Rosetta for affinity computation. If we further add the components for epitope identification and antibody structure prediction (such as Igfold (Ruffolo et al., 2021;)), we are able to set up an end2end pipeline for antibody discovery: it can output a desirable antibody (1D sequence and 3D structure) for any given antigen target. However, setting up such an end2end pipeline is challenging, as the accumulated errors from the former components will easily make the latter fail. A potential solution is making all components learnable, and tuning them as a whole.\nLastly, our proposed model and the proof-of-concept experiments we implemented will provide valuable clues for future exploration to derive enhanced techniques. With the efforts of all researchers in the field, we have reason to believe that this ultimate problem will be solved, perhaps step by step.", "publication_ref": ["b15", "b39", "b39"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "ACKNOWLEDGMENTS", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "REPRODUCIBILITY", "text": "The codes for our MEAN are available at https://github.com/THUNLP-MT/MEAN.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "With the above lemmas, we are ready to present the full proof of Theorem 1 as follows:\nProof. For each layer in MEAN, according to Lemma 2 and 3, we have:\nSince p i is obtained by applying softmax on the hidden representations from the output module, which shares the same formula as \u03c3 in , andZ i is the same as the coordination from the output module, it is easy to derive:\nTherefore we have:\nwhich concludes Theorem 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F HUBER LOSS", "text": "We use Huber loss (Huber, 1992) for the modeling of coordinations, which is defined as follows:\nIt reads that if the L1 norm of |x \u2212 y| is smaller than \u03b4, it is MSE loss, otherwise it is L1 loss. At the beginning of the training, the deviation of the predicted structure and ground truth is large and the L1 term makes the loss less sensitive to outliers than MSE loss. When the training is almost done, the deviation is small and the MSE loss provides smoothness near 0. In practice, we find that directly using MSE loss occasionally causes NaN at the beginning of the training while Huber loss leads to a more stable training procedure. We set \u03b4 = 1 in our experiments.", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "G COMPLEXITY ANALYSIS", "text": "We provide the complexity analysis here. Suppose the numbers of nodes in the antigen, the light chain, and the heavy chain are N A , N L , and N H , respectively. The message passing in Eq. (1-6) for each node involves K neighbors at maximum, and it is performed over L rounds at each iteration.\nThen, the complexity of our algorithm over T iterations becomes O(2LKT\nwhere the number 2 refers to the joint computation of the internal and external encoders. Similarly, the complexity of RefineGNN with only heavy chain is O(LKT N H ), where T denotes the number of iterations, namely, the length of the CDR region. We would like to specify these two points:\n1. Even if our algorithm contains more nodes than RefineGNN ((\nthe extra computation overhead does not remarkably count, since the message passing for each node can be parallelly computed in current deep learning platforms (e.g. Pytorch). 2. We apply the full-shot decoding other than the autoregressive mechanism used in Re-fineGNN, hence T (T = 3 in our experiments) is much smaller than T (usually larger than 10), implying more efficiency of our method.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "H MODELING WITH RANDOMNESS", "text": "The current model is deterministic given the same inputs, but in some scenarios, the diversity of samples is required. To tackle this, we inject randomness into our model by adding standard Gaussian noises to the initialized coordinates. We denote this model as rand-MEAN and evaluate it on the tasks of sequence-structure modeling and antigen-binding CDR-H3 design. The results in Table 7 and Table 8 suggest injecting randomness into MEAN has acceptable impacts on the performance.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A general framework for computational antibody design", "journal": "PLoS computational biology", "year": "2018", "authors": "Jared Adolf-Bryfogle; Oleks Kalyuzhniy; Michael Kubitz; D Brian; Xiaozhen Weitzner; Yumiko Hu;  Adachi; R William; Roland L Dunbrack Schief;  Rosettaantibodydesign"}, {"ref_id": "b1", "title": "In silico proof of principle of machine learning-based antibody design at unconstrained scale", "journal": "Taylor & Francis", "year": "2022", "authors": "Rahmad Akbar; Philippe A Robert; R C\u00e9dric; Michael Weber; Robert Widrich; Milena Frank; Lonneke Pavlovi\u0107; Maria Scheffer; Igor Chernigovskaya; Andrei Snapkov;  Slabodkin"}, {"ref_id": "b2", "title": "The rosetta all-atom energy function for macromolecular modeling and design", "journal": "Journal of chemical theory and computation", "year": "2017", "authors": "F Rebecca; Andrew Alford;  Leaver-Fay; R Jeliazko;  Jeliazkov; J Matthew;  O'meara; P Frank; Hahnbeom Dimaio;  Park; V Maxim;  Shapovalov;  Douglas Renfrew; K Vikram; Kalli Mulligan;  Kappel"}, {"ref_id": "b3", "title": "Unified rational protein engineering with sequence-based deep representation learning", "journal": "Nature methods", "year": "2019", "authors": "C Ethan; Grigory Alley; Surojit Khimulya; Mohammed Biswas; George M Alquraishi;  Church"}, {"ref_id": "b4", "title": "Principles for computational design of binding antibodies", "journal": "Proceedings of the National Academy of Sciences", "year": "2017", "authors": "Dror Baran; Gabriele Pszolla; Gideon D Lapidoth; Christoffer Norn; Orly Dym; Tamar Unger; Shira Albeck; D Michael;  Tyka;  Fleishman"}, {"ref_id": "b5", "title": "Why recombinant antibodies-benefits and applications", "journal": "Current opinion in biotechnology", "year": "2019", "authors": "Koli Basu; Evan M Green; Yifan Cheng; Charles S Craik"}, {"ref_id": "b6", "title": "Fold2seq: A joint sequence (1d)-fold (3d) embedding-based generative model for protein design", "journal": "PMLR", "year": "2021", "authors": "Yue Cao; Payel Das; Vijil Chenthamarakshan; Pin-Yu Chen; Igor Melnyk; Yang Shen"}, {"ref_id": "b7", "title": "Dissecting protein-protein recognition sites", "journal": "Proteins: Structure, Function, and Bioinformatics", "year": "2002", "authors": "Pinak Chakrabarti; Joel Janin"}, {"ref_id": "b8", "title": "Sabdab: the structural antibody database", "journal": "Nucleic acids research", "year": "2014", "authors": "James Dunbar; Konrad Krawczyk; Jinwoo Leem; Terry Baker; Angelika Fuchs; Guy Georges; Jiye Shi; Charlotte M Deane"}, {"ref_id": "b9", "title": "Openmm 7: Rapid development of high performance algorithms for molecular dynamics", "journal": "PLoS computational biology", "year": "2017", "authors": "Peter Eastman; Jason Swails; D John;  Chodera; Yutong Robert T Mcgibbon;  Zhao; A Kyle; Lee-Ping Beauchamp; Andrew C Wang;  Simmonett; P Matthew; Chaya D Harrigan;  Stern"}, {"ref_id": "b10", "title": "Computational design of antibodies. Current opinion in structural biology", "journal": "", "year": "2018", "authors": "Sharon Fischman; Yanay Ofran"}, {"ref_id": "b11", "title": "Artificial neural networks (the multilayer perceptron)-a review of applications in the atmospheric sciences", "journal": "Atmospheric environment", "year": "1998", "authors": "W Matt; S R Gardner;  Dorling"}, {"ref_id": "b12", "title": "Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules", "journal": "Advances in Neural Information Processing Systems", "year": "2019", "authors": "Niklas Gebauer; Michael Gastegger; Kristof Sch\u00fctt"}, {"ref_id": "b13", "title": "A review of deep learning methods for antibodies", "journal": "Antibodies", "year": "2020", "authors": "Jordan Graves; Jacob Byerly; Eduardo Priego; Naren Makkapati; Vince Parish; Brenda Medellin; Monica Berrondo"}, {"ref_id": "b14", "title": "Geometrically equivariant graph neural networks: A survey", "journal": "", "year": "2022", "authors": "Jiaqi Han; Yu Rong; Tingyang Xu; Wenbing Huang"}, {"ref_id": "b15", "title": "Prediction of residues in discontinuous b-cell epitopes using protein 3d structures", "journal": "Protein Science", "year": "2006", "authors": "Morten Pernille Haste Andersen; Ole Nielsen;  Lund"}, {"ref_id": "b16", "title": "Amino acid substitution matrices from protein blocks. Proceedings of the National Academy of Sciences", "journal": "", "year": "1992", "authors": "Steven Henikoff; G Jorja;  Henikoff"}, {"ref_id": "b17", "title": "Equivariant graph mechanics networks with constraints", "journal": "", "year": "2022", "authors": "Wenbing Huang; Jiaqi Han; Yu Rong; Tingyang Xu; Fuchun Sun; Junzhou Huang"}, {"ref_id": "b18", "title": "Robust estimation of a location parameter", "journal": "Springer", "year": "1992", "authors": "J Peter;  Huber"}, {"ref_id": "b19", "title": "Generative models for graphbased protein design", "journal": "Advances in Neural Information Processing Systems", "year": "2019", "authors": "John Ingraham; Vikas Garg; Regina Barzilay; Tommi Jaakkola"}, {"ref_id": "b20", "title": "Skempi 2.0: an updated benchmark of changes in protein-protein binding energy, kinetics and thermodynamics upon mutation", "journal": "Bioinformatics", "year": "2019", "authors": "Justina Jankauskait\u0117; Brian Jim\u00e9nez-Garc\u00eda; Justas Dapk\u016bnas; Juan Fern\u00e1ndez-Recio; Iain H Moal"}, {"ref_id": "b21", "title": "Iterative refinement graph neural network for antibody sequence-structure co-design", "journal": "", "year": "2021", "authors": "Wengong Jin; Jeremy Wohlwend; Regina Barzilay; Tommi Jaakkola"}, {"ref_id": "b22", "title": "Antibody-antigen docking and design via hierarchical structure refinement", "journal": "PMLR", "year": "2022", "authors": "Wengong Jin; Regina Barzilay; Tommi Jaakkola"}, {"ref_id": "b23", "title": "Highly accurate protein structure prediction with alphafold", "journal": "Nature", "year": "2021", "authors": "John Jumper; Richard Evans; Alexander Pritzel; Tim Green; Michael Figurnov; Olaf Ronneberger; Kathryn Tunyasuvunakool; Russ Bates; Anna Augustin\u017e\u00eddek;  Potapenko"}, {"ref_id": "b24", "title": "A solution for the best rotation to relate two sets of vectors. Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography", "journal": "", "year": "1976", "authors": "Wolfgang Kabsch"}, {"ref_id": "b25", "title": "De novo protein design for novel folds using guided conditional wasserstein generative adversarial networks", "journal": "", "year": "2020", "authors": "Mostafa Karimi; Shaowen Zhu; Yue Cao; Yang Shen"}, {"ref_id": "b26", "title": "Directional message passing for molecular graphs", "journal": "", "year": "2020", "authors": "Johannes Klicpera; Janek Gro\u00df; Stephan G\u00fcnnemann"}, {"ref_id": "b27", "title": "Computer-aided antibody design", "journal": "Protein engineering", "year": "2012", "authors": "Daisuke Kuroda; Hiroki Shirai; P Matthew; Haruki Jacobson;  Nakamura"}, {"ref_id": "b28", "title": "Abdesign: A n algorithm for combinatorial backbone design guided by natural conformations and sequences", "journal": "Proteins: Structure, Function, and Bioinformatics", "year": "2015", "authors": "Dror Gideon D Lapidoth; Gabriele M Baran; Christoffer Pszolla; Assaf Norn;  Alon; D Michael;  Tyka;  Fleishman"}, {"ref_id": "b29", "title": "Imgt unique numbering for immunoglobulin and t cell receptor variable domains and ig superfamily v-like domains", "journal": "", "year": "2003", "authors": "Marie-Paule Lefranc; Christelle Pommi\u00e9; Manuel Ruiz; V\u00e9ronique Giudicelli; Elodie Foulquier; Lisa Truong; Val\u00e9rie Thouvenin-Contet; G\u00e9rard Lefranc"}, {"ref_id": "b30", "title": "Optmaven-a new framework for the de novo design of antibody variable region models targeting specific antigen epitopes", "journal": "PloS one", "year": "2014", "authors": "Tong Li; J Robert; Costas D Pantazes;  Maranas"}, {"ref_id": "b31", "title": "Antibody complementarity determining region design using high-capacity machine learning", "journal": "Bioinformatics", "year": "2020", "authors": "Ge Liu; Haoyang Zeng; Jonas Mueller; Brandon Carter; Ziheng Wang; Jonas Schilz; Geraldine Horny; E Michael; Stefan Birnbaum; David K Ewert;  Gifford"}, {"ref_id": "b32", "title": "Spherical message passing for 3d molecular graphs", "journal": "", "year": "2021", "authors": "Yi Liu; Limei Wang; Meng Liu; Yuchao Lin; Xuan Zhang; Bora Oztekin; Shuiwang Ji"}, {"ref_id": "b33", "title": "Frame averaging for invariant and equivariant network design", "journal": "", "year": "2021", "authors": "Omri Puny; Matan Atzmon; Heli Ben-Hamu; J Edward; Ishan Smith; Aditya Misra; Yaron Grover;  Lipman"}, {"ref_id": "b34", "title": "Stereochemistry of polypeptide chain configurations", "journal": "Journal of Molecular Biology", "year": "1963", "authors": " Gn Ramachandran; V Ramakrishnan;  Sasisekharan"}, {"ref_id": "b35", "title": "Deciphering antibody affinity maturation with language models and weakly supervised learning", "journal": "", "year": "2021", "authors": "A Jeffrey; Jeffrey J Ruffolo; Jeremias Gray;  Sulam"}, {"ref_id": "b36", "title": "Fast, accurate antibody structure prediction from deep learning on massive set of natural antibodies", "journal": "bioRxiv", "year": "2022", "authors": "A Jeffrey; Lee-Shin Ruffolo; Sai Chu; Jeffrey J Pooja Mahajan;  Gray"}, {"ref_id": "b37", "title": "Antibody design using lstm based deep generative model from phage display library for affinity maturation", "journal": "Scientific reports", "year": "2021", "authors": "Koichiro Saka; Taro Kakuzaki; Shoichi Metsugi; Daiki Kashiwagi; Kenji Yoshida; Manabu Wada; Hiroyuki Tsunoda; Reiji Teramoto"}, {"ref_id": "b38", "title": "E (n) equivariant graph neural networks", "journal": "PMLR", "year": "2021", "authors": "Emiel V\u0131\u0107tor Garcia Satorras; Max Hoogeboom;  Welling"}, {"ref_id": "b39", "title": "Deep learning guided optimization of human antibody against sars-cov-2 variants with broad neutralization", "journal": "Proceedings of the National Academy of Sciences", "year": "2022", "authors": "Sisi Shan; Shitong Luo; Ziqing Yang; Junxian Hong; Yufeng Su; Fan Ding; Lili Fu; Chenyu Li; Peng Chen; Jianzhu Ma"}, {"ref_id": "b40", "title": "Protein design and variant prediction using autoregressive generative models", "journal": "Nature communications", "year": "2021", "authors": "Jung-Eun Shin; Adam J Riesselman; Aaron W Kollasch; Conor Mcmahon; Elana Simon; Chris Sander; Aashish Manglik; C Andrew; Debora S Kruse;  Marks"}, {"ref_id": "b41", "title": "Mmseqs2 enables sensitive protein sequence searching for the analysis of massive data sets", "journal": "Nature biotechnology", "year": "2017", "authors": "Martin Steinegger; Johannes S\u00f6ding"}, {"ref_id": "b42", "title": "Advances in antibody design", "journal": "Annual review of biomedical engineering", "year": "2015", "authors": "E Kathryn;  Tiller; M Peter;  Tessier"}, {"ref_id": "b43", "title": "Graph attention networks", "journal": "", "year": "2017", "authors": "Petar Veli\u010dkovi\u0107; Guillem Cucurull; Arantxa Casanova; Adriana Romero; Pietro Lio; Yoshua Bengio"}, {"ref_id": "b44", "title": "How significant is a protein structure similarity with tm-score= 0.5?", "journal": "Bioinformatics", "year": "2010", "authors": "Jinrui Xu; Yang Zhang"}, {"ref_id": "b45", "title": "Characterization of protein-protein interfaces. The protein journal", "journal": "", "year": "2008", "authors": "Changhui Yan; Feihong Wu; L Robert; Drena Jernigan; Vasant Dobbs;  Honavar"}, {"ref_id": "b46", "title": "Hdock: a web server for protein-protein and protein-dna/rna docking based on a hybrid strategy", "journal": "Nucleic acids research", "year": "2017", "authors": "Yumeng Yan; Di Zhang; Pei Zhou; Botong Li; Sheng-You Huang"}, {"ref_id": "b47", "title": "ARERDYRLDY MEAN: AREGDGYFDY PDB=1a2y", "journal": "", "year": "", "authors": ""}, {"ref_id": "b48", "title": "", "journal": "", "year": "", "authors": " Cdr-H3"}, {"ref_id": "b49", "title": "ARDSGYAMDY MEAN: AREYYYYFDY PDB=1uj3", "journal": "", "year": "", "authors": ""}, {"ref_id": "b50", "title": "", "journal": "", "year": "", "authors": " Cdr-H3"}, {"ref_id": "b51", "title": "ARSEDWFAY MEAN: ARDSDYFDY PDB=5d93", "journal": "", "year": "", "authors": ""}, {"ref_id": "b52", "title": "CDR-H3 Ground truth: AGNYYGMDY MEAN: AREYYGFDY PDB=1fe8", "journal": "", "year": "", "authors": ""}, {"ref_id": "b53", "title": "CDR-H3 Ground truth: ARGSGFRWVMDY MEAN: ARDGGYGSAMDY PDB=3l95", "journal": "", "year": "", "authors": ""}, {"ref_id": "b54", "title": "", "journal": "", "year": "", "authors": " Cdr-H3"}, {"ref_id": "b55", "title": "ARGLRF MEAN: ARGFDY PDB=2vxt", "journal": "", "year": "", "authors": ""}, {"ref_id": "b56", "title": "CDR-H3 Ground truth: TRHDGTNFDY MEAN: AREGSDYMDY PDB=3s35", "journal": "", "year": "", "authors": ""}, {"ref_id": "b57", "title": "", "journal": "", "year": "", "authors": " Cdr-H3"}, {"ref_id": "b58", "title": "ARGDYYGSNSLDY MEAN: AREGDYGYYYFDY PDB=4cmh", "journal": "", "year": "", "authors": ""}, {"ref_id": "b59", "title": "", "journal": "", "year": "", "authors": " Cdr-H3"}, {"ref_id": "b60", "title": "ARDLRTGPFDY MEAN: ARDDDYGAFDY PDB=4g6j", "journal": "", "year": "", "authors": ""}, {"ref_id": "b61", "title": "More examples from antigen-binding CDR-H3 design by our MEAN", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: (A) The structure of a residue, where the backbone atoms we use are N, C \u03b1 , C, O. (B)The structure of an antibody which is symmetric and Y-shaped, and we focus on the three versatile CDRs on the variable domain of the heavy chain. (C) Schematic graph construction for the antigenantibody complex, with global nodes, internal context edges E in and external interaction edges E ex .", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure2: The overview of MEAN and the progressive full-shot decoding. In each iteration, we alternate the internal context encoding and external interaction encoding over L layers, and then update the input features and coordinates of CDRs for the next iteration with the predicted values.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: (A) The structure of two antigen-binding CDR-H3s (PDB: 1ic7) designed by MEAN (left, RMSD=0.49) and RefineGNN (right, RMSD=3.04). The sequences are also provided in the annotations. (B) The antibody-antigen complex structure after affinity optimization (PDB: 2vis, \u2206\u2206G = -5.13), with the original and optimized sequences displayed in the annotations.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: (A) CDR-H3 designed by MEAN based on a docked template targeting the antigen from PDB 5b8c (Affinity=-59.6). (B) The affinity distribution of the antibodies.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "where the group action \u2022 is instantiated as g \u2022 Z := OZ for orthogonal transformation O \u2208 R 3\u00d73 and g \u2022 Z := Z + t for translation transformation t \u2208 R 3 .In the following, we denote f (Z) = Z T Z/|Z T Z| F and the orthogonal group as O(3) = {O \u2208 R 3\u00d73 |O T O = OO T = I d }.Prior to the proof, we first present the necessary lemmas below: Lemma 1. The function f : R 3\u00d7m \u2192 R m\u00d7m is invariant on O(3). Namely, \u2200g \u2208 O(3), we have f (g \u2022 Z) = f (Z), where g \u2022 Z := OZ.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure 5: (A) The left is the attention weights from the residues in CDR-H3 to those in antigen (PDB: 4ydk). The right is the relative energy contribution of each pair of residues calculated by Rosetta. (B) The density maps of the relative ranks for the most contributing residue pairs.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 :6Figure 6: The density of PPL and RMSD of different rounds in progressive full-shot decoding. The area under each curve integrates to 1.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Top: 10-fold cross validation mean (standard deviation) for 1D sequence and 3D structure modeling on SAbDab ( \u00a74.1). Bottom: evaluations under the setting ofJin et al. (2021), denoted with a superscript * . 40\u00b15.56% 3.22\u00b10.29 37.06\u00b13.09% 3.64\u00b10.40 21.13\u00b11.59% 6.00\u00b10.55 C-RefineGNN 33.19\u00b12.99% 3.25\u00b10.40 33.53\u00b13.23% 3.69\u00b10.56 18.88\u00b11.37% 6.22\u00b10.59 MEAN 58.29\u00b17.27% 0.98\u00b10.16 47.15\u00b13.09% 0.95\u00b10.05 36.38\u00b13.08% 2.21\u00b10.16 LSTM", "figure_data": "ModelCDR-H1 AARRMSDCDR-H2 AARRMSDCDR-H3 AARRMSDLSTM40.98\u00b15.20%-28.50\u00b11.55%-15.69\u00b10.91%-C-LSTM40.93\u00b15.41%-29.24\u00b11.08%-15.48\u00b11.17%-RefineGNN39.28.02%-24.39%-18.92%-RefineGNN  *30.07%0.9727.70%0.7327.60%2.12MEAN  *62.78%0.9452.04%0.8939.87%2.20"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ": Left: Amino acid recovery (AAR), TM-score and RMSD for CDR-H3 design on RAbDbenchmark ( \u00a7 4.2). Middle: Average affinity change after optimization textsection 4.3). Right:Average CDR lengths and speedups by our full-shot decoding compared to the iterative-refinementdecoding.ModelAARTM-score RMSDModel\u2206\u2206GlengthRosettaAD LSTM C-LSTM RefineGNN C-RefineGNN 28.90% 22.50% 22.36% 22.18% 29.79% MEAN 36.77%0.9435 --0.8303 0.8317 0.98125.52 --7.55 7.21 1.81Random LSTM C-LSTM RefineGNN C-RefineGNN -3.79 +1.52 -1.48 -1.83 -3.98 MEAN -5.33CDR-H1 CDR-H2 CDR-H3 speedup (train / infer) 7.9 7.6 14.1 CDR-H1 2.1x / 1.5x CDR-H2 2.1x / 1.6x CDR-H3 4.1x / 2.8x"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Ablations of MEAN.", "figure_data": "ModelSAbDab (CDR-H3) AAR RMSDAARRAbD TM-score RMSDSKEMPI \u2206\u2206Gw/o global node33.89\u00b16.81% 2.62\u00b11.0335.07%0.97981.81-3.68w/o attention35.89\u00b13.44% 2.24\u00b10.3136.56%0.97451.95failedheavy chain only33.62\u00b16.20% 2.23\u00b10.1735.82%0.97281.97failediterative refinement 25.85\u00b11.88% 3.61\u00b10.7535.19%0.96293.20-5.19MEAN36.38\u00b13.08% 2.21\u00b10.1636.77%0.98121.81-5.33"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Kevin Yang, Wengong Jin, Kyle Swanson, Regina Barzilay, and Tommi Jaakkola. Improving molecular design by stochastic iterative target augmentation. In International Conference on Machine Learning, pp. 10716-10726. PMLR, 2020. Yang Zhang and Jeffrey Skolnick. Scoring function for automated assessment of protein structure template quality. Proteins: Structure, Function, and Bioinformatics, 57(4):702-710, 2004.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Statistics of each fold of 10-fold cross validation.RefineGNN paper settingIn Table1, we also compare LSTM, RefineGNN, and MEAN on the same split used in the paper of RefineGNN", "figure_data": "CDR-H1CDR-H2CDR-H3cluster antibodycluster antibodycluster antibodyfold 177299110237166317fold 277260110343166301fold 377363110288166348fold 477286109315166271fold 577195109419166279fold 676241109217166345fold 776427109266166306fold 876326109321166320fold 976210109374166309fold 1076520109347165331Total7653,1271,0933,12716593,127"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Number of antibodies in the training/validation/test set of Jin et al. (2021) as well as their subsets of complete complexes.", "figure_data": "CDR-H1CDR-H2CDR-H3fullsubsetfullsubsetfullsubsettraining4,050 2,1313,876 2,0353,896 2,051validation359170483241403227test326184376209437207Total4,735 2,4854,735 2,4854,735 3,127"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "", "figure_data": ": Hyperparameters for each model.hyperparameter value descriptionsharedvocab size25There are 21 categories of amino acids in eukaryote, plus the4 special tokens for heavy chain, light chain, antigen and mask.dropout0.1Dropout rate.(C-)LSTMhidden size256Size of the hidden states in LSTM.n layers2Number of layers of LSTM.(C-)RefineGNNhidden size256Size of the hidden states in the message passing network (MPN).k neighbors9Number of neighbors to include in k-nearest neighbor (KNN) graph.block size4Number of residues in a block.n layers4Number of layers in MPN.num rbf16Number of RBF kernels to consider.MEANembed size64Size of trainable embeddings for each type of amino acids.hidden size128Size of hidden states in MEAN.n layers3Number of layers in MEAN.alpha0.8The weight to balance sequence loss and structure loss.n iter3Number of iterations for progressive full-shot decoding."}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "10-fold cross validation mean (standard deviation) for 1D sequence and 3D structure modeling on SAbDab ( \u00a74.1) of rand-MEAN. 29\u00b17.27% 0.98\u00b10.16 47.15\u00b13.09% 0.95\u00b10.05 36.38\u00b13.08% 2.21\u00b10.16 rand-MEAN 56.50\u00b16.44% 0.98\u00b10.12 44.01\u00b12.72% 1.18\u00b10.20 35.68\u00b12.29% 2.36\u00b10.19", "figure_data": "ModelCDR-H1 AARRMSDCDR-H2 AARRMSDCDR-H3 AARRMSDMEAN58."}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Amino acid recovery (AAR), TM-score and RMSD for CDR-H3 design on RAbD benchmark of rand-MEAN.", "figure_data": "ModelAARTM-score RMSDMEAN36.77%0.98121.81rand-MEAN 37.30%0.97941.81"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "RMSD of bond lengths and dihedral angles of the backbone structure.", "figure_data": "ModelBond lengths Angles(\u03c6, \u03c8, \u03c9)RefineGNN10.560.456C-RefineGNN9.490.423MEAN0.370.302"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Results of antigen-binding CDR-H3 design given different forms of antigen information.", "figure_data": "AntigenAARTM-score RMSDepitope-only 36.77%0.98121.81full37.92%0.97982.00"}], "formulas": [{"formula_id": "formula_0", "formula_text": "denoted as G = (V := {V H , V L , V A }, E := {E in , E ex }).", "formula_coordinates": [3.0, 108.0, 365.71, 225.64, 10.15]}, {"formula_id": "formula_1", "formula_text": "(l) ij = Z (l) i \u2212 Z (l) j .", "formula_coordinates": [4.0, 150.53, 529.46, 73.28, 14.07]}, {"formula_id": "formula_2", "formula_text": "m ij = \u03c6 m (h (l) i , h (l) j , (Z (l) ij ) Z (l) ij (Z (l) ij ) Z (l) ij F , e ij ),(1)", "formula_coordinates": [4.0, 208.75, 553.17, 295.25, 31.4]}, {"formula_id": "formula_3", "formula_text": "h (l+0.5) i = \u03c6 h (h (l) i , j\u2208N (i|Ein) m ij ),(2)", "formula_coordinates": [4.0, 193.06, 589.08, 310.94, 17.27]}, {"formula_id": "formula_4", "formula_text": "Z (l+0.5) i = Z (l) i + 1 |N (i|E in )| j\u2208N (i|Ein) Z (l) ij \u03c6 Z (m ij ),(3)", "formula_coordinates": [4.0, 190.41, 610.34, 313.59, 23.38]}, {"formula_id": "formula_5", "formula_text": "\u03b1 ij = exp(q i k ij ) j\u2208N (i|Eex) exp(q i k ij ) ,(4)", "formula_coordinates": [5.0, 194.22, 133.75, 309.78, 25.06]}, {"formula_id": "formula_6", "formula_text": "h (l+1) i = h (l+0.5) i + j\u2208N (i|Eex) \u03b1 ij v ij ,(5)", "formula_coordinates": [5.0, 194.22, 163.72, 309.78, 17.27]}, {"formula_id": "formula_7", "formula_text": "Z (l+1) i = Z (l+0.5) i + j\u2208N (i|Eex) \u03b1 ij Z (l+0.5) ij \u03c6 Z (v ij ),(6)", "formula_coordinates": [5.0, 194.22, 185.88, 309.78, 17.27]}, {"formula_id": "formula_8", "formula_text": "(l+0.5) i ), k ij = \u03c6 k ( (Z (l+0.5) ij ) Z (l+0.5) ij (Z (l+0.5) ij ) Z (l+0.5) ij F , h (l+0.5) j", "formula_coordinates": [5.0, 254.29, 231.41, 199.01, 22.09]}, {"formula_id": "formula_9", "formula_text": "\u03c6 v ( (Z (l+0.5) ij ) Z (l+0.5) ij (Z (l+0.5) ij ) Z (l+0.5) ij F , h (l+0.5) j", "formula_coordinates": [5.0, 108.0, 254.85, 136.24, 22.09]}, {"formula_id": "formula_10", "formula_text": "{(p i ,Z i )} i\u2208V C = f ({h (0) i , Z(0)", "formula_coordinates": [5.0, 349.34, 367.26, 128.53, 14.07]}, {"formula_id": "formula_11", "formula_text": "i } i\u2208V ), then f is E(3)-equivariant. In other words, for each transformation g \u2208 E(3), we have {(p i , g \u2022 Z i )} i\u2208V C = f ({h (0) i , g \u2022 Z (0) i } i\u2208V )", "formula_coordinates": [5.0, 108.0, 370.4, 778.82, 35.78]}, {"formula_id": "formula_12", "formula_text": "(t) i ,Z (t) i } i\u2208V C from iteration t, we first update the embeddings of all nodes: h i = na j=1 p (t) i (j)s j , \u2200i \u2208 V C , where p (t)", "formula_coordinates": [5.0, 108.0, 544.36, 396.0, 41.8]}, {"formula_id": "formula_13", "formula_text": "(t+1) i ,Z (t+1) i } i\u2208V C = MEAN(G (t+1) ).", "formula_coordinates": [5.0, 290.32, 624.98, 157.67, 14.07]}, {"formula_id": "formula_14", "formula_text": "L seq = 1 T t 1 |V C | i\u2208V C ce (p (t) i ,p i ),(7)", "formula_coordinates": [5.0, 230.04, 658.87, 273.96, 27.42]}, {"formula_id": "formula_15", "formula_text": "L struct = 1 |V C | i\u2208V C huber (Z (T ) i ,\u1e90 i ),(8)", "formula_coordinates": [6.0, 228.25, 99.88, 275.75, 27.42]}, {"formula_id": "formula_16", "formula_text": "{(p i ,Z i )} i\u2208V C = f ({h (0) i , Z(0)", "formula_coordinates": [15.0, 349.34, 620.13, 128.53, 14.07]}, {"formula_id": "formula_17", "formula_text": "(0) i , g \u2022 Z (0) i } i\u2208V ) = g \u2022 f ({h (0) i , Z (0) i } i\u2208V ),", "formula_coordinates": [15.0, 108.0, 634.31, 396.0, 28.25]}, {"formula_id": "formula_18", "formula_text": "f (g \u2022 Z) = (OZ) T (OZ) |(OZ) T (OZ)| F = Z T O T OZ |Z T O T OZ| F (9) = Z T Z |Z T Z| F = f (Z),(10)", "formula_coordinates": [16.0, 208.32, 134.6, 295.68, 53.23]}, {"formula_id": "formula_19", "formula_text": "(l+0.5) i , Z (l+0.5) i )} i\u2208V = \u03c3 in ({(h (l) i , Z (l) i )} i\u2208V ), then \u03c3 in is E(3)-equivariant. Proof. \u2200g \u2208 E(3), we have g \u2022 Z (l) i := OZ (l) i + t where O \u2208 O(", "formula_coordinates": [16.0, 108.0, 211.31, 396.0, 55.15]}, {"formula_id": "formula_20", "formula_text": "(l) i \u2212 g \u2022 Z (l) j = OZ (l)", "formula_coordinates": [16.0, 314.99, 266.56, 90.43, 14.07]}, {"formula_id": "formula_21", "formula_text": "m ij = \u03c6 m (h (l) i , h (l) j , f (OZ (l) ij ), e ij ) = \u03c6 m (h (l) i , h (l) j , f (Z (l) ij ), e ij ),(11)", "formula_coordinates": [16.0, 168.71, 311.71, 335.29, 14.07]}, {"formula_id": "formula_22", "formula_text": "h (l+0.5) i = \u03c6 h (h (l) i , j\u2208N (i|Ein) m ij ),(12)", "formula_coordinates": [16.0, 181.82, 364.16, 322.18, 17.27]}, {"formula_id": "formula_23", "formula_text": "OZ (l+0.5) i + t = O[Z (l) i + 1 |N (i|E in )| j\u2208N (i|Ein) Z (l) ij \u03c6 Z (m ij )] + t (13) = (OZ (l) i + t) + 1 |N (i|E in )| j\u2208N (i|Ein) (OZ (l) ij )\u03c6 Z (m ij ),(14)", "formula_coordinates": [16.0, 154.2, 385.43, 349.8, 49.85]}, {"formula_id": "formula_24", "formula_text": "(l) i , Z (l) i )} i\u2208V ) = \u03c3 in ({(h (l) i , g \u2022 Z (l) i )} i\u2208V ).", "formula_coordinates": [16.0, 227.21, 440.88, 174.98, 14.07]}, {"formula_id": "formula_25", "formula_text": "(l+1) i , Z (l+1) i )} i\u2208V = \u03c3 ex ({(h (l+0.5) i , Z (l+0.5) i )} i\u2208V ), then \u03c3 ex is E(3)-equivariant.", "formula_coordinates": [16.0, 108.0, 461.8, 396.0, 28.25]}, {"formula_id": "formula_26", "formula_text": "q i = \u03c6 q (h (l+0.5) i ),(15)", "formula_coordinates": [16.0, 180.29, 543.16, 323.71, 14.07]}, {"formula_id": "formula_27", "formula_text": "k ij = \u03c6 k (f (OZ (l+0.5) ij ), h (l+0.5) j ) = \u03c6 k (f (Z (l+0.5) ij ), h (l+0.5) j ),(16)", "formula_coordinates": [16.0, 175.85, 560.84, 328.15, 14.07]}, {"formula_id": "formula_28", "formula_text": "v ij = \u03c6 v (f (OZ (l+0.5) ij ), h (l+0.5) j ) = \u03c6 v (f (Z (l+0.5) ij ), h (l+0.5) j ),(17)", "formula_coordinates": [16.0, 176.2, 579.36, 327.8, 14.07]}, {"formula_id": "formula_29", "formula_text": "h (l+1) i = h (l+0.5) i + j\u2208N (i|Eex) \u03b1 ij v ij ,(18)", "formula_coordinates": [16.0, 198.72, 651.82, 305.28, 17.27]}, {"formula_id": "formula_30", "formula_text": "OZ (l+1) i + t = O(Z (l+0.5) i + j\u2208N (i|Eex) \u03b1 ij Z (l+0.5) ij v ij ) + t (19) = (OZ (l+0.5) i + t) + j\u2208N (i|Eex) \u03b1 ij OZ (l+0.5) ij v ij .(20)", "formula_coordinates": [16.0, 171.11, 673.98, 332.89, 39.44]}, {"formula_id": "formula_31", "formula_text": "(l+0.5) i , Z (l+0.5) i )} i\u2208V ) = \u03c3 ex ({(h (l+0.5) i , g \u2022 Z (l+0.5) i", "formula_coordinates": [16.0, 228.27, 719.92, 211.73, 14.07]}], "doi": ""}