{"title": "NEURAL COLLAPSE UNDER MSE LOSS: PROXIMITY TO AND DYNAMICS ON THE CENTRAL PATH", "authors": "X Y Han; Vardan Papyan; David L Donoho", "pub_date": "2022-05-10", "abstract": "The recently discovered Neural Collapse (NC) phenomenon occurs pervasively in today's deep net training paradigm of driving cross-entropy (CE) loss towards zero. During NC, last-layer features collapse to their class-means, both classifiers and class-means collapse to the same Simplex Equiangular Tight Frame, and classifier behavior collapses to the nearest-class-mean decision rule. Recent works demonstrated that deep nets trained with mean squared error (MSE) loss perform comparably to those trained with CE. As a preliminary, we empirically establish that NC emerges in such MSE-trained deep nets as well through experiments on three canonical networks and five benchmark datasets. We provide, in a Google Colab notebook, PyTorch code for reproducing MSE-NC and CE-NC: here. The analytically-tractable MSE loss offers more mathematical opportunities than the hard-to-analyze CE loss, inspiring us to leverage MSE loss towards the theoretical investigation of NC. We develop three main contributions: (I) We show a new decomposition of the MSE loss into (A) terms directly interpretable through the lens of NC and which assume the last-layer classifier is exactly the least-squares classifier; and (B) a term capturing the deviation from this least-squares classifier. (II) We exhibit experiments on canonical datasets and networks demonstrating that term-(B) is negligible during training. This motivates us to introduce a new theoretical construct: the central path, where the linear classifier stays MSE-optimal for feature activations throughout the dynamics. (III) By studying renormalized gradient flow along the central path, we derive exact dynamics that predict NC.", "sections": [{"heading": "INTRODUCTION", "text": "Modern deep learning includes paradigmatic procedures that are commonly adopted, but not entirely understood. Some examples are multi-layered architectures, stochastic gradient descent, batch normalization, cross-entropy (CE) loss, and training past zero error towards zero loss. Analyzing the properties of these practices is an important research task.\nIn this work, we theoretically investigate behavior of last-layer features in classification deep nets. In particular, we consider the training of deep neural networks on datasets containing images from C different classes with N examples in each class. After passing the i-th example in the c-th class through all layers except the last-layer of the network, the network outputs some last-layer features h i,c \u2208 R P . The last-layer of the network-which, for each class c, possesses a classifier w c \u2208 R P and bias b c \u2208 R-then predicts a label for the example using the rule arg max c ( w c , h i,c + b c ). The network's performance is evaluated by calculating the error defined by Error = Ave where Ave is the operator that averages over its subscript indices.\nPrior works such as ; Belkin et al. (2019) have shown that overparameterized classifiers (such as deep nets) can \"memorize\" their training set without harming performance on unseen test data. Moreover, works such as Soudry et al. (2018) have further shown that continuing to train networks past memorization can still lead to performance improvements 1,2 . Papyan, Han, and Donoho (2020) recently examined this setting, referring to the phase during which one trains past zero-error towards zero-CE-loss as the Terminal Phase of Training (TPT). During TPT, they exposed a phenomenon called Neural Collapse (NC).", "publication_ref": ["b6", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "NEURAL COLLAPSE", "text": "NC is defined relative to the feature global mean,\n\u00b5 G = Ave i,c h i,c\n, the feature class-means, \u00b5 c = Ave i h i,c , c = 1, . . . , C, the feature within-class covariance,\n\u03a3 W = Ave i,c (h i,c \u2212 \u00b5 c )(h i,c \u2212 \u00b5 c ) ,(1)\nand the feature between-class covariance, \n\u03a3 B = Ave c (\u00b5 c \u2212 \u00b5 G )(\u00b5 c \u2212 \u00b5 G ) .\n\u00b5 c \u2212 \u00b5 G , \u00b5 c \u2212 \u00b5 G \u00b5 c \u2212 \u00b5 G 2 \u00b5 c \u2212 \u00b5 G 2 \u2192 1, c = c \u22121 C\u22121 , c = c \u00b5 c \u2212 \u00b5 G 2 \u2212 \u00b5 c \u2212 \u00b5 G 2 \u2192 0 \u2200c = c (NC3) Convergence to self-duality:\nThe (NC2) property captures convergence to a simple geometric structure called an Equiangular Tight Frame (ETF). An ETF is a collection of vectors {v c } C c=1 having equal lengths and equal, maximally separated pair-wise angles. In classification deep nets, last-layer features are of higher dimension than the number of classes i.e. P >C. In this setting, the maximal angles are given by\nv c , v c v c 2 v c 2 = 1, for c = c \u2212 1 C\u22121 , for c = c\n, and the ETF is called a Simplex ETF 4 . Observe that as C increases, the ETF approaches a (partial) orthogonal matrix. Thus, when C is large, this translates to the intuitive notion that classifiers and class-means tend with training to (near) orthogonality.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "DEEP NET CLASSIFICATION WITH MSE LOSS", "text": "While classification deep nets are typically trained with CE loss, Demirkaya et al. (2020) and Hui & Belkin (2020) recently reported deep nets trained with squared error (MSE) loss,\nL(W , b, H) = 1 2 Ave i,c W h i,c + b \u2212 y i,c 2 2 + \u03bb 2 ( W 2 F + b 2 2 ) (2) = 1 2CN W H + b1 CN \u2212 Y 2 F + \u03bb 2 ( W 2 F + b 2 2 ),\nachieve comparable test performance as those trained with CE loss. Above, H \u2208 R P \u00d7CN and Y \u2208 R C\u00d7CN are matrices resulting from stacking 5 together the feature vectors h i,c and one-hot vectors y i,c as their respective columns; W \u2208 R C\u00d7P is the matrix resulting from the stacking of classifiers w c as rows; b \u2208 R C is the vector resulting from concatenating the scalars {b c } C c=1 ; and 1 CN is the length-CN vector of ones. Table 1 in Appendix A shows supplementary measurements affirming the findings of Hui & Belkin (2020); Demirkaya et al. (2020), i.e. the measurements affirm that MSE-trained networks indeed achieve accuracies on testing data comparable to those of CE-trained networks (cf. the analogous Table 1 in Papyan, Han, and Donoho (2020)). The analytically-tractable MSE loss offers more mathematical opportunities than the hard-to-analyze CE loss and inspires this paper's main theoretical contributions explicitly characterizing MSE-NC.", "publication_ref": ["b9", "b22", "b22", "b9", "b31"], "figure_ref": [], "table_ref": ["tab_2", "tab_2"]}, {"heading": "CONTRIBUTIONS", "text": "Our main contributions are as follows:\n\u2022 We propose a new decomposition of the MSE loss: L = L NC1 + L NC2/3 + L \u22a5 LS . The terms L NC1 and L NC2/3 possess interpretations attributable to NC phenomena and assume the classifier W is exactly the least-squares optimal classifier W LS (relative to the given H); and L \u22a5 LS captures the deviation of W from W LS . (Section 2) \u2022 We provide empirical measurements of our decomposition (Figure 2 \u2022 We study the gradient flow of those renormalized features along the central path and derive exact, closed-form dynamics that imply NC. The dynamics are explicit in terms of the captures the subtlety that the size of the within-class \"noise\" (captured by \u03a3W ) should be viewed relative to the size of the class-means (captured by \u03a3B). 4 Traditional research on ETFs (cf. Strohmer & Heath Jr (2003); Fickus & Mixon (2015)) examines the P \u2264C setting where C ETF-vectors span their ambient R P space. However, in the P >C setting of classification deep nets, the vectors can not span R P . Following the precedent of Papyan, Han, and Donoho (2020), we interpret ETFs as equal-length and maximally-equiangular vectors that are not necessarily spanning.\n5 Assume the stacking is performed in i-then-c order: the first column is (i, c) = (1, 1), the second is (i, c) = (2, 1),..., the N -th is (i, c) = (N, 1), the (N + 1)-st is (i, c) = (1, 2), and so on. This matters for formalizing Equation 11 in Corollary 2.\nsingular value decomposition of the renormalized feature class-means at initialization. (Section 3.3) Additionally, we complement this paper with new, extensive measurements on five benchmark datasets-in particular, the MNIST, FashionMNIST, CIFAR10, SVHN, and STL10 datasets 6 (Deng et al., 2009;Krizhevsky & Hinton, 2009;LeCun et al., 2010;Xiao et al., 2017)-and three canonical deep nets-in particular, the VGG, ResNet, and DenseNet networks (He et al., 2016;Huang et al., 2017;Simonyan & Zisserman, 2014)-that verify the empirical reality of MSE-NC i.e. they show (NC1)-(NC4) indeed occur for networks trained with MSE loss. These experiments establish that theoretical modeling of MSE-NC is empirically well-motivated. They are lengthy-together spanning four pages with seven figures and a table-so we collectively defer them to Appendix A.", "publication_ref": ["b41", "b15", "b10", "b24", "b26", "b46", "b19", "b21", "b39"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "DECOMPOSITION OF MSE LOSS", "text": "Inspired by the community's interest in the role of the MSE loss in deep net training (Demirkaya et al., 2020;Hui & Belkin, 2020;Mixon et al., 2020;Poggio & Liao, 2020a;b), we derive a new decomposition of the MSE loss that gives insights into the NC phenomenon. First, absorb the bias vector into the weight matrix-by defining the extended weight matrix W = [W , b] \u2208 R C\u00d7(P +1) and the extended feature vector h i,c = [h i,c ; 1] \u2208 R P +1 -so that Equation 2 can be rewritten as\nL( W , H) = 1 2 Ave i,c W h i,c \u2212 y i,c 2 2 + \u03bb 2 W 2 F .(3)\nUsing h i,c , define the entities \u00b5 c , \u00b5 G , H, and \u03a3 W analogously to those in Section 1.1. We further define the extended total covariance and extended class-means matrices, respectively, as\n\u03a3 T = Ave i,c ( h i,c \u2212 \u00b5 G )( h i,c \u2212 \u00b5 G ) \u2208 R (P +1)\u00d7(P +1) M = [ \u00b5 1 , . . . , \u00b5 C ] \u2208 R (P +1)\u00d7C .\nNext, we reformulate, with weight decay incorporated, a classic result of Webb & Lowe (1990):\nProposition 1 (Webb & Lowe (1990) with Weight Decay). For fixed extended features H, the optimal classifier minimizing the MSE loss L( W , H) is\nW LS = 1 C M ( \u03a3 T + \u00b5 G \u00b5 G + \u03bbI) \u22121 ,\nwhere I is the identity matrix. Note that W LS depends on H only.\nNote that we can interpret W LS as a function of H, leading us to identify the following decomposition of L where one term depends on H only.\nTheorem 1. (Decomposition of MSE Loss; Proof in Appendix B) The MSE loss, L( W , H), can be decomposed into two terms, L( W , H) = L LS ( H) + L \u22a5 LS ( W , H),where\nL LS ( H) = 1 2 Ave i,c W LS h i,c \u2212 y i,c 2 2 + \u03bb 2 W LS 2 F ,and\nL \u22a5 LS ( W , H) = 1 2 tr ( W \u2212 W LS ) \u03a3 T + \u00b5 G \u00b5 G + \u03bbI ( W \u2212 W LS ) .\nIn the above, L LS ( H) is independent of W . Intuitively, it is the MSE-performance of the optimal classifiers W LS (rather than the \"real classifiers\" W ) on input H.\nThe component L \u22a5 LS ( W , H) is non-negative 7 and is zero only when W = W LS . Therefore, L \u22a5 LS ( H) quantifies the distance of W from W LS . In short, the least-squares component, L LS ( H), captures the behavior of the network when the classifier possesses optimal, least squares behavior. Meanwhile, the deviation component, L \u22a5 LS ( W , H), captures the divergence from that behavior.\nWe can further decompose L LS ( H) into two terms, one capturing activation collapse (NC1) and the other capturing convergence to Simplex ETF of both features and classifiers ((NC2) and (NC3)). Theorem 2. (Decomposition of Least-Squares Component; Proof in Appendix C) The leastsquares component, L LS ( H), of the MSE decomposition in Theorem 1 can be further decomposed into\nL LS ( H) = L NC1 ( H) + L NC2/3 ( H),where\nL NC1 ( H) = 1 2 tr W LS \u03a3 W + \u03bbI W LS , L NC2/3 ( H) = 1 2C W LS M \u2212 I 2 F .\nInspection of these terms is revealing. First, observe that L NC2/3 is a function of the class-means and MSE-optimal classifiers. Minimizing L NC2/3 will push the (unextended) class-means and classifiers towards the same Simpex ETF matrix 8 i.e. (NC2)-(NC3). Next, note that the within-class variation is independent of the means. Thus, despite the fact that classifiers are converging towards some (potentially large) ETF matrix, we can always reduce L NC1 by pushing \u03a3 W towards zero, which corresponds to (NC1) . This motivates us to formulate the central path: W H on the central path\nP = ( W LS ( H), H) | H \u2208 R (P +1)\u00d7CN , (4\nd dt X = \u2212\u03a0 T X X (\u2207 X L LS (X)) ,(5)\n7\nThe middle term is the sum of positive-semidefinite matrices-one of which is positive-definite. 8 Appendices C.1-C.2 elaborates upon this interpretation of LNC2/3 in more detail. 9 Both the unconstrained features model (Mixon et al., 2020) and the (1-)layer-peeled model (Fang et al., 2021) advocate for the mathematical analysis of deep nets through variants of gradient flow directly on the last-layer features and classifiers. The layer-peeled model is so-named because it proposes to analyze deep nets by \"peeling away\" one layer at a time. The unconstrained features model is so-name because features are not constrained to be the output of a deep net forward pass but are rather free variables that can be optimized directly: This captures the nearly unlimited flexibility afforded to feature engineering transformations in modern deep nets by the many nonlinear, overparameterized layers.\n10 Intuitively, one should interpret t=0 as the time-of-training when feature-classifier pairs effectively enter the central path. \nL( W , H) = L NC1 ( H) + L NC2/3 ( H) + L \u22a5 LS ( W , H) from Section 2. Starting from an early epoch in training, L \u22a5 LS ( W , H) becomes negligible compared to the dominant term, L NC1 ( H), implying L \u22a5 LS ( W , H) L LS ( H)=L NC1 ( H)+L NC2/3 ( H), i.e\n. the features and classifiers are effectively on the central path during TPT. Note that L NC2/3 ( H) diminishes the fastest among all the terms: Intuitively, this shows that the network primarily focuses on distributing the feature class-means into a \"uniform\" Simplex ETF configuration (NC1)-(NC2) early on and, from there, compresses the activations towards their class-means, i.e. (NC1) , as much as possible. Further experimental details are in Appendix A. Outlier behavior is discussed in Appendix A.7.\nwhere the operator \u03a0 T X X projects the gradient onto the tangent space of the manifold 11 , X , of all identity-covariance features. For \u03a3 \u2212 1 2 W H to be well-defined, we assume that \u03a3 W remains full-rank 12 during training. We call Equation 5 the continually renormalized gradient flow and will motivate it in more detail in later subsections. We now state our main theorem: Theorem 3. (NC Under Continually Renormalized Gradient Flow) Consider the continually renormalized gradient flow (Equation 5) in which the dynamics are restricted to the central path, and the features are renormalized to identity within-class covariance. Assume the features are initialized with zero global mean. Then, Neural Collapse emerges on the renormalized features with explicit dynamics of the flow given in Proposition 2 as well as Corollaries 1 and 2 later in this paper.\nThree assumptions standout which we discuss below: is effectively performing classification with no bias term (i.e. using only a linear classifier W ) on zero-mean data. To see this, define the globally-centered features, 11 X is a variant of the well-known generalized Stiefel manifold (Absil et al., 2009, Chapter 3.4). 12 Given the definition in Equation 1, the positive-definiteness of \u03a3W is unsurprising in deep net contexts because CN P i.e. the number of examples is larger than the width of the last-layer. Note this does not contradict (NC1). (NC1) is characterized by \u03a3W approaching the zero-matrix with training (relative to \u03a3B). In practice (for example, in our supplementary Appendix A experiments), we observed that the zero-matrix is never actually achieved during the training dynamics and \u03a3W indeed remains full-rank. globally-centered means, and total-covariance matrix in unextended coordinates:\n\u010e H = H \u2212 \u00b5 G 1 CN ; \u010e M = [\u00b5 1 \u2212 \u00b5 G , . . . , \u00b5 C \u2212 \u00b5 G ]; \u03a3 T = 1 CN \u010e H \u010e H .\nProposition 1 then reduces to the following forms for the unextended weights and biases:\nW LS = [W LS , b LS ] = C \u22121 \u010e M \u03a3 \u22121 T , C \u22121 1 C \u2212 C \u22121 \u010e M \u03a3 \u22121 T \u00b5 G .(6)\nFor an arbitrary activation-target pair (h, y), the prediction error obeys\nW LS h + b LS \u2212 y = C \u22121 \u010e M \u03a3 \u22121 T h + C \u22121 1 C \u2212 C \u22121 \u010e M \u03a3 \u22121 T \u00b5 G \u2212 y = W LS (h \u2212 \u00b5 G ) \u2212 (y \u2212 C \u22121 1 C ).\nThe last line exhibits two terms in parentheses, both traceable to the action of the bias b LS . The term h \u2212 \u00b5 G demonstrates that the bias induces a global-mean subtraction on h, while the term y \u2212 C \u22121 1 demonstrates that the bias also induces a global-mean subtraction on the one-hot targets.\n(A3) Renormalization to identity covariance: Renormalization is ubiquitous in the empirical deep learning literature through works such batch normalization and its variants (Ioffe & Szegedy, 2015;Salimans & Kingma, 2016;Wu & He, 2018;Ulyanov et al., 2016;Ba et al., 2016;Krizhevsky et al., 2012). It is also ideologically precedented in the theoretical ML literature through works such as Douglas et al. (1998); Banburski et al. (2019); Poggio & Liao (2020a;b). Within this paper, the covariance-renormalization is inspired by an invariance property of the loss as well as the intuitive concept of the signal-to-noise ratio from mathematical statistics (discussed later in Sections 3.1-3.2).\nIn the subsequent sections, we motivate and introduce the theoretical constructs necessary for proving Theorem 3 and for presenting the explicit dynamics of the flow.", "publication_ref": ["b9", "b22", "b30", "b33", "b30", "b14", "b23", "b38", "b45", "b43", "b2", "b25", "b3", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "INVARIANCE PROPERTY", "text": "By Assumption (A2), classification on the central path is equivalent to classification on globallycentered features using only the linear classifier W LS . Then, using Equation 6, we can equivalently represent the central path (Equation 4) as follows: Definition 1 (Zero Global Mean Central Path).\ns P = W LS , \u010e H W LS = C \u22121 \u010e M \u03a3 \u22121 T .\nAn invariance property holds on the central path. Let A denote a symmetric, full-rank matrix. Then, the explicit form in Equation 6 for W LS implies\nW LS (A \u010e H)A \u010e H = C \u22121 A \u010e M A \u010e H A \u010e H \u22121 A \u010e H = C \u22121 \u010e M A A \u22121 \u010e H \u010e H \u22121 A \u22121 A \u010e H = C \u22121 \u010e M \u010e H \u010e H \u22121 \u010e H = C \u22121 \u010e M \u03a3 \u22121 T \u010e H = W LS \u010e H \u010e H,(7)\nwhere the notation W LS (\u2022) makes explicit that W LS is a function of a set of input features. Thus, the actual predictions made by the least squares classifier are invariant to choice of the coordinates in which we express \u010e H, i.e. all features A \u010e H are equivalently performing. Among those coordinate systems, we will prefer the one in which the \"noise\" is \"whitened\" or \"sphered.\" Recall that we assume \u03a3 W ( \u010e H)-where the notation \u03a3 W (\u2022) expresses the within-class covariance as a function of the features-is positive-definite. Consider the coordinate transformation\n\u010e H \u2192A \u010e H with A = \u03a3 \u2212 1 2 W ( \u010e H).\nIn these coordinates, the features are \"renormalized\" to spherical covariance,\n\u03a3 W (A \u010e H) = A\u03a3 W ( \u010e H)A = I, so we will call A \u010e H the renormalized features. The class-means of A \u010e H are \u010e M (A \u010e H) = A \u010e M ( \u010e H) = \u03a3 \u2212 1 2 W ( \u010e H) \u010e M ( \u010e H),\nwhere the notation \u010e M (\u2022) expresses that the means are a function of features. Thus, Equation 5 describes continuous training dynamics where we continually \"renormalize\" features to their equivalently-performing representer.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SIGNAL-TO-NOISE RATIO MATRIX ON THE CENTRAL PATH", "text": "The \u03a3 \u2212 1 2 W \u010e M term evokes the canonical idea of a signal-to-noise ratio from statistics. To see this, note that the classifier in Equation 6 can be rewritten as\nW LS = C \u22121 \u010e M C \u22121 \u010e M \u010e M + \u03a3 W \u22121 .\nThe combined presence of the terms \u010e M \u010e M and \u03a3 W in the denominator, along with \u010e M in the numerator, signals to us the presence of the informative signal-to-noise (SNR) ratio matrix:\nSNR \u2261 \u03a3 \u2212 1 2 W \u010e M . (8\n)\nWhy this terminology? The globally-centered class-means represent the overall \"signal\" indicating that the class-means are separated from each other: if they were non-separated-e.g. all equal-\u010e M would be zero. However, a classifier decision must keep in view noise-captured by \u03a3 W -which might confuse the classifier. So, one wishes the signal be large compared to the noise. It may help to consider the mental model-which is not at all necessary to the correctness of our analysis-under which the activations are normally distributed with h i,c \u223c N (\u00b5 c , \u03a3 W ). Under that model, a linear classifier will indeed get confused if the norm \u00b5 c \u2212 \u00b5 G 2 is \"small compared\" to \u03a3 W . Replacing this somewhat vague statement by a discussion of quantitative properties of the SNR matrix is well understood by mathematical statisticians to be decisive for understanding classification performance in the normal case. Additionally, observe that\nSNR = \u010e M (\u03a3 \u2212 1 2 W \u010e H).\nThus, connecting back to Section 3.1, the SNR matrix is simply the class-means matrix of the renormalized features.\nThe SNR can be further understood through its singular value decomposition (SVD). Definition 2 (SVD of SNR Matrix). Denote the SVD of the SNR matrix (Equation 8) as follows:\nSNR = U \u2126V = C\u22121 j=1 \u03c9 j u j v j .\nHere, {\u03c9 c } C\u22121 c=1 are the non-zero singular values of SNR; \u2126 = diag {\u03c9 c } C\u22121 c=1 , 0 \u2208 R C\u00d7C ; and the left and right singular-vectors, U \u2208 R P \u00d7C and V \u2208 R C\u00d7C , are partial orthogonal and orthogonal matrices, respectively, with columns {u j } C j=1 and {v j } C j=1 . The SNR matrix is rank C \u2212 1 since \u03a3 W is assumed full-rank and \u010e M is rank C \u2212 1 due to global-mean subtraction.\nThe non-zero singular values, {\u03c9 j } C\u22121 j=1 , are decisive for understanding the separation performance of the least-squares linear classifier. Good performance demands that the singular values be large. Works such as Fukunaga (1972, Chapter 10) and Hastie & Tibshirani (1996) show that the magnitude of each singular value of the SNR matrix is the size of the class separation in the direction of the corresponding singular vector. In this sense, the smallest singular value captures the smallest separation between classes. Consequently, driving-larger the smallest non-zero singular value makes the task of linear separation more immune to (spherical) noise and to Euclidean-norm-constrained adversarial noise. As we will see in the next section, examining the dynamics of these singular-values when the features undergo gradient flow in renormalized coordinates leads to precise characterizations of Neural Collapse 13 .", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "DYNAMICS OF SNR", "text": "For any matrix Z, we use Z t in this section to denote the state of that matrix at time t. Similarly, we denote with {\u03c9 j (t)} C\u22121 j=1 the state of the non-zero SNR singular values at t. We now present Proposition 2 and Corollaries 1-2 that provide the explicit dynamics referenced by Theorem 3. Proposition 2 (Dynamics of Singular Values of SNR Matrix; Proof in Appendix D.4). Continually renormalized gradient flow on the central path (Equation 5) induces the following closed-form dynamics on the SNR singular values (Definition 2): c 1 log(\u03c9 j (t)) + c 2 \u03c9 2 j (t) + c 3 \u03c9 4 j (t) = a j + t, t \u2265 0, for all j = 1, . . . , C \u2212 1.\nc 1 , c 2 , and c 3 are positive constants independent of j, and a j is a constant depending on \u03c9 j (0).\nCorollary 1 (Properties of SNR Singular Values; Proof in Appendix D.5). SNR singular values (Definition 2) following the Equation 9dynamics satisfy the following limiting behaviors:\n1. lim t\u2192\u221e \u03c9 j (t) = \u221e and lim t\u2192\u221e \u03c9j (t)\n4 \u221a t/c3\n= 1, for all j = 1, . . . , C \u2212 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "lim t\u2192\u221e", "text": "maxj \u03c9j (t) minj \u03c9j (t) = 1.\nThe first fact shows that all non-zero singular values of the SNR matrix, \u03a3\n\u2212 1 2 W \u010e\nM , grow to infinity at an asymptotic rate of 4 t/c 3 . Intuitively, this shows that the \"signal\" becomes infinitely large compared to the \"noise\", i.e. (NC1). The second fact shows that the singular values of \u03a3 \u2212 1 2 W \u010e M (which has zero-mean columns) approach equality-implying that the limiting matrix is a Simplex ETF (Lemma 7, Appendix D.6) i.e. (NC2). Finally, Papyan, Han, and Donoho (2020, Theorem 1) show that (NC1-2) imply (NC3-4) on the central path. Corollary 2 formalizes these intuitions.\nCorollary 2 (Neural Collapse Under MSE Loss; Proof in Appendix D.6). Under continually renormalized gradient flow (Equation 5), the SNR matrix (Equation 8) converges to\nlim t\u2192\u221e 1 \u03c9 max (t) SNR t = U 0 V 0 ,(10)\nwhere U 0 \u2208 R P \u00d7(C\u22121) and V 0 \u2208 R C\u00d7(C\u22121) are the left and right singular vectors of the SNR matrix (Definition 2) at t=0 corresponding to the non-zero singular values; and \u03c9 max (t) is the largest singular value at time t. Furthermore, Corollary 1 implies the occurrence of (NC1)-(NC4) i.e. renormalized gradient flow on the central path leads to Neural Collapse.\nMoreover, denoting the Kronecker product with \u2297, the renormalized features matrix converges to We provide a detailed survey of all above-mentioned papers in Appendix E.\nlim t\u2192\u221e 1 \u03c9 max (t) \u03a3 \u2212 1 2 W,t \u010e H t = ( U 0 V 0 ) \u2297 1 N . (11", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSION", "text": "In this paper, after verifying that NC occurs when prototypical classification deep nets are trained with MSE loss on canonical datasets, we then derive and measure a novel decomposition of the MSE loss.\nWe observed that the last-layer classifier tends to the least-squares classifier-motivating us to define the central path on which the classifier behaves exactly as optimal least-squares classifier. On the central path, we showed invariance properties that inspired us to examine the renormalized features and their corresponding continually renormalized gradient flow. This flow induces closed-form dynamics that imply the occurrence of Neural Collapse.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "REPRODUCIBILITY STATEMENT", "text": "This paper is reproducible. Experimental details about all empirical results described in this paper are provided in Appendix A. Additionally, we provide PyTorch (Paszke et al., 2019) Hui & Belkin (2020) showed that the additional scaling-heuristic does need to be applied to the loss before comparable test-performance can be achieved. In informal, exploratory experiments not reported here, we were able to reproduce their results on datasets with more than ten classes. But, we feel these scaling-heuristics merit further scientific investigation of their own and are beyond the scope of this article 14 , so we do not include datasets that require scaling-heuristic tuning.", "publication_ref": ["b32", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "A.2 NETWORKS", "text": "We train VGG, ResNet, and DenseNet models with the same depth-selection procedures and architecture-specification choices as in Papyan, Han, and Donoho (2020). In the MSE setting, the final chosen depths were as follows:\nDATASET VGG RESNET DENSENET MNIST VGG11 RESNET18 DENSENET40 FASHIONMNIST VGG11 RESNET18 DENSENET250 SVHN VGG13 RESNET34 DENSENET40 CIFAR10 VGG11 RESNET50 DENSENET100 STL10 VGG11 RESNET18 DENSENET201 A.3 OPTIMIZATION METHODOLOGY\nThe optimization algorithm, parameters, and hyperparameter tuning are the same as in Papyan, Han, and Donoho (2020).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.4 COMPUTATIONAL RESOURCES", "text": "The experiments were run on Stanford University's Sherlock computing cluster. Each dataset-network combination was trained on a single GPU attached to a CPU with at least 32GB of RAM-the specific types of the CPUs/GPUs vary according to whichever was first assigned to us by the HPC cluster scheduler.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.5 FORMATTING", "text": "The coloring and formatting of the plots are the same as in Papyan, Han, and Donoho (2020).\nA.6 EXPERIMENTAL RESULTS    Results demonstrate that last-layer features and classifiers approach maximal-equiangularity. Results demonstrate that last-layer features and classifiers approach self-duality.      STL10 stands apart from the other canonical datasets for multiple reasons 15 and is a \"less-typical\" benchmark compared to more \"compulsory\" datasets like CIFAR10. In this particular case, we hypothesize that its outlier behavior might be caused by the size of the STL10 images (96 \u00d7 96 compared to the 32 \u00d7 32, after padding, of the other datasets)-leading to a higher-dimensional problem, which, in turn, induces a harder non-convex optimization problem, which might make SGD less likely to converge to a useful optimum or might make the convergence much slower and harder to observe under a fixed computational budget.\nIt was previously noted in Hui & Belkin (2020) and Demirkaya et al. (2020) that more challenging classification problems-in those projects, problems with more classes to be labeled-may require modifications to the MSE loss. We decided that such modifications are unnecessary in the ten class problems The experiments in this paper as well as those in Papyan, Han, and Donoho (2020) were conducted under the canonical setting where deep nets were trained using SGD with weight-decay and batch normalization-leading one to wonder about the roles that these particular ingredients play on the NC phenomena. Additionally, most observables focus on the train data-raising the question of how the NC phenomena behave on test data as well as their relationship to generalization. We find these questions intriguing, but feel each deserves careful experimentation outside the scope of this paper.\nNonetheless, we note here that several existing papers have already begun insightful investigations in these directions. For example, weight-decay, weight-normalization (as a proxy for batchnormalization), and SGD play key roles in the analyses of Banburski et al. (2019); Poggio & Liao (2020a;b) that, along other things, lead to the prediction of NC in homogeneous deep nets. Banburski et al. (2021) also explores the connection between NC and margin distributions with generalization.\nAnother example is Zhu et al. (2021) in which the authors conduct several experiments on ResNets trained on CIFAR10. On their models, the authors examine NC-related properties relative to the train data, test data, and randomly labeled data; they also conducted ablation studies varying control parameters (weight-decay, width, etc.) and the optimization algorithm (SGD, ADAM, L-BFGS).\nBased on their results, Zhu et al. (2021) propose thought-provoking conjectures on the role of each of these components (see Section E.7 for a brief survey of their findings).\nAs a preliminary exploration, we include Figure 12 showing the variability collapse (NC1) behavior on test data for the networks trained with MSE loss (used in this paper) as well as those trained with CE loss (released 16 by Papyan, Han, and Donoho (2020)). From Figure 12, we see that variability collapse occurs much slower on the test data than on the train data. Although not shown here, the other NC phenomena behave similarly.", "publication_ref": ["b22", "b9", "b3", "b33", "b48", "b48"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "B THEOREM 1", "text": "Theorem 1. (Decomposition of MSE Loss) The MSE loss, L( W , H), can be decomposed into two terms, L( W , H) = L LS ( H) + L \u22a5 LS ( W , H), where\nL LS ( H) = 1 2 Ave i,c W LS h i,c \u2212 y i,c 2 2 + \u03bb 2 W LS 2 F ,and\nL \u22a5 LS ( W , H) = 1 2 tr ( W \u2212 W LS ) \u03a3 T + \u00b5 G \u00b5 G + \u03bbI ( W \u2212 W LS ) .\nProof. Recall the MSE loss:\nL( W , H) = 1 2 Ave i,c W h i,c \u2212 y i,c 2 2 + \u03bb 2 W 2 F . (12\n)\nConsider a specific extended activations matrix H; the least-squares solution W minimizing L( W , H) with H kept fixed must obey the following first-order optimality condition:\nAve i,c ( W LS h i,c \u2212 y i,c ) h i,c + \u03bb W LS = 0. (13\n)\nFrom Proposition 1 in the main manuscript, the solution to the above is given by\nW LS = C \u22121 M ( \u03a3 T + \u00b5 G \u00b5 G + \u03bbI) \u22121 .\nThe loss can be rewritten as\n1 2 Ave i,c W LS h i,c \u2212 y i,c + ( W \u2212 W LS ) h i,c 2 2 + \u03bb 2 W 2 F .\nCombining the above with Equation 13gives, after rearranging:\n1 2 Ave i,c W LS h i,c \u2212 y i,c 2 2 + 1 2 Ave i,c ( W \u2212 W LS ) h i,c 2 2 \u2212 \u03bb tr W LS ( W \u2212 W LS ) + \u03bb 2 W 2 F , which is equivalent to 1 2 Ave i,c W LS h i,c \u2212 y i,c 2 2 + 1 2 Ave i,c ( W \u2212 W LS ) h i,c 2 2 + \u03bb 2 W LS 2 F + \u03bb 2 W LS \u2212 W 2 F .\nUsing the above, the loss can indeed be decomposed as\nL( W , H) = L LS ( H) + L \u22a5 LS ( W , H), where L LS ( H) \u2261 1 2 Ave i,c W LS h i,c \u2212 y i,c 2 2 + \u03bb 2 W LS 2 F(14)\nand\nL \u22a5 LS ( W , H) \u2261 1 2 Ave i,c ( W \u2212 W LS ) h i,c 2 2 + \u03bb 2 W LS \u2212 W 2 F = 1 2 tr ( W \u2212 W LS ) \u03a3 T + \u00b5 G \u00b5 G + \u03bbI ( W \u2212 W LS ) .\nThis completes the proof.\nC THEOREM 2 \nL NC1 ( H) = 1 2 tr W LS \u03a3 W + \u03bbI W LS , L NC2/3 ( H) = 1 2C W LS M \u2212 I 2 F .\nProof. Under Euclidean distance, perturbations to the extended activations matrix H which affect only the class-means or global-mean are orthogonal to perturbations which affect only the within-class covariance. Using this, L LS ( H) can be further decomposed via the Pythagorean theorem:\nL LS ( H) = 1 2 Ave i,c W LS ( h i,c \u2212 \u00b5 c ) 2 2 + \u03bb 2 W LS 2 F + 1 2 Ave c W LS \u00b5 c \u2212 y i,c 2 2 .\nThe first and second terms above merge into L NC1 ( H), while the third term becomes L NC2/3 ( H):\nL NC1 ( H) = 1 2 tr W LS \u03a3 W + \u03bbI W LS (15\n)\nL NC2/3 ( H) = 1 2C W LS M \u2212 I 2 F .\nThis completes the proof.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 INTUITIONS FOR THEOREM 2 IN UNEXTENDED COORDINATES", "text": "For intuition, consider when \u03bb = 0, i.e. the no weight-decay case. Proposition 1 in the main text gives\nW LS = C \u22121 \u010e M \u03a3 \u22121 T ,(16)\nwhere \u010e M \u2208 R P \u00d7C is the matrix with columns \u00b5 c \u2212 \u00b5 G . Returning to the unextended coordinates,\nL NC1 ( \u010e H) simplifies to L NC1 ( \u010e H) = 1 2 tr W LS \u03a3 W W LS ,(17)\nwhere\n\u010e H \u2208 R P \u00d7CN has columns h i,c \u2212 \u00b5 G . The term L NC2/3 ( \u010e H) also simplifies instructively to L NC2/3 ( \u010e H) = 1 2C W LS \u010e M \u2212 \u03a6 2 F ,(18)\nwhere \u03a6 \u2208 R C\u00d7C is the standard Simplex ETF:\n\u03a6 = I \u2212 1 C 11 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.2 ADDITIONAL INTUITIONS FOR THEOREM 2 TERMS", "text": "The expressions in Equation 15and Equation 17 further evoke the intuition that\nL NC1 ( \u010e H\n) is a variance term that goes to zero only under activation collapse (NC1) .\nMore specifically, we see that, with the class-means held constant, the only way to have \nL NC1 ( \u010e H) \u2192 0 is for \u03a3 W \u2192 0. Similarly,", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D THEOREM 3 D.1 IMPLICATIONS OF INVARIANCE", "text": "In this section, we will discuss in more detail how the invariance observed Section 3.1 leads the the continually renormalized gradient flow (Equation 5). In particular, we have seen in Section 3.1 of the main text that, on the central path, both the predictions W LS ( \u010e H) \u010e H and the MSE loss Figure 13: Fiber bundle. Any full-rank features matrix, \u010e H, has a representative element, X, on X . For any X \u2208 X , a fiber is the set S \u00d7 X i.e. {AX : A \u2208 S} (the Minkowski set product), where S is the set of symmetric positive-definite matrices. Features on the same fiber generate the same class predictions and the same MSE loss. The optimization described in Sections D.1-D.3 moves fiber-to-fiber.\nL(W LS ( \u010e H), \u010e M ( \u010e H)) are invariant under the transformation \u010e H \u2192 A \u010e H when A is a invertible matrix. Since our interest mainly lies with A = \u03a3 \u2212 1 2\nW , we will restrict the intuitive discussion in this section to positive-definite A.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Now, consider the set H of features \u010e", "text": "H with with non-singular within-class covariances. We view H as a fiber bundle 17 , where-on each fiber-live (apparently) different activations, \u010e H, that (in fact) generate the exact same class predictions and the exact same MSE loss (see Figure 13). The base space-we will denote it X -can be taken to be the collection of X \u2208 H where \u03a3 W (X\n) = I. Every \u010e H \u2208 H is representable as \u010e H = AX where A = \u03a3 \u2212 1 2\nW ( \u010e H) and \u03a3 W (X) = I. The activation matrices X in the base space might be variously called \"pre-whitened,\" \"normalized,\" \"standardized,\" or \"sphered.\" We shall call X a normalized features manifold 18 .\nInvoking invariance, we can make the following observations about our optimization task:\n1. If, at a specific t, H t happens to be a normalized set of activations (i.e. H t \u2208 X is currently located in the base space X ), there is no performance benefit to leaving the base space. There is also no performance benefit to moving along a fiber; only by moving from fiber-to-fiber can we improve performance.\n2. In some sense, we waste time except when jumping from fiber-to-fiber; we might prefer to stay in the base space all the time by forcing the dynamics to jump from fiber-to-fiber.\nOn the other hand, we started with the viewpoint of studying gradient flow of the original H. So, we consider the following natural model for the application of gradient descent in X:\n1. For a given initial feature activations, \u010e H 0 \u2208 H, we renormalize-obtaining a starting point X 0 \u2208 X ; here\nX 0 = \u03a3 \u2212 1 2 W ( \u010e H 0 ) \u010e H 0 .\nClass predictions and MSE loss performance for the MSE-optimal classifier do not change from this renormalization.\n2. We compute the usual gradient of MSE loss, at X 0 , obtaining a step \u2206X 0 = \u2212\u03b7\u2207 X L LS (X 0 ), where \u03b7 is a step size.\n3. We take a step, going to \u010e H 1 = X 0 + \u2206X 0 . 4. \u010e H 1 will not necessarily be normalized. We map back along whatever fiber we have landed upon, obtaining a corresponding point in the base space X , call this X 1 . Here,\nX 1 = \u03a3 \u2212 1 2 W ( \u010e H 1 ) \u010e H 1 .\nClass predictions and MSE loss performance do not change from this renormalization. 5. Repeat steps 2, 3, 4 at X 1 , obtaining thereby X 2 ; and so on. This process might be described as gradient descent with continual renormalization:\n1. Renormalize the initial features activation matrix; 2. Compute the ordinary gradient of MSE loss and take the gradient descent step; 3. Renormalize again after each such step; and 4. Repeat steps 2 and 3.\nWhile the continual renormalization process differs from usual gradient flow, it is both intuitively understandable and sensible. See discussion of assumption (A3) in Section 3", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "D.2 ALIGNED SNR COORDINATES", "text": "Analysis of continually renormalized gradient flow can be simplified, without loss of generality, by applying a change of basis in both row and column space: Definition 3 (Transformation into Aligned SNR Coordinates). Consider the SVD decomposition of the SNR from Definition 2 outputting left singular vectors U \u2208 R P \u00d7C and right singular vectors V \u2208 R C\u00d7C . For any matrix Z \u2208 R P \u00d7C , define the transformation Z \u2192 U ZV as the transformation into aligned-SNR coordinates.\nThe aligned-SNR coordinates is so-named because it diagonalizes i.e. aligns the SNR matrix: Definition 4 (Aligned SNR Matrix). Consider the SVD decomposition of the SNR from Definition 2. Combined with Equation 8, observe that\n\u2126 = diag {\u03c9 c } C\u22121 c=1 , 0 = U \u03a3 \u2212 1 2 W \u010e M V We call \u2126 the aligned SNR matrix.\nThe following facts about the aligned SNR matrix becomes useful later in our derivations: Observation 1 (SVD of Aligned SNR Matrix). The SVD of the aligned SNR matrix itself is simply\n\u2126 = C\u22121 j=1\n\u03c9 j e j e j , with the canonical basis {e j } C j=1 as its singular vectors.\nAs discussed in Section 3.2, the {\u03c9 c } C\u22121 c=1 that comprise \u2126 are decisive for understanding separation performance. In fact, when \u03bb=0, the MSE loss can be entirely characterized by the SNR singular values on the central path: Lemma 1 (Spectral Representation of MSE Loss). When \u03bb = 0, MSE loss obeys\nL(W LS ( \u010e H), \u010e H) = L LS (W LS ( \u010e H), \u010e H) = 1 2 C\u22121 j=1 1 \u03c9 2 j + C = L {\u03c9 j } C\u22121 j=1 ,\non the central path, and its decomposition obeys\nL NC1 ( \u010e H) = 1 2 C\u22121 j=1 \u03c9 2 j (C + \u03c9 2 j ) 2 = L NC1 {\u03c9 j } C\u22121 j=1 L NC2/3 ( \u010e H) = 1 2 C\u22121 j=1 1 C \u03c9 2 j \u03c9 2 j + C \u2212 1 2 = L NC2/3 {\u03c9 j } C\u22121 j=1 .\nProof. Using Equation 16, Equation 17, and Equation 18, the loss on the central path equals\nL(W LS , \u010e M ) = 1 2C C \u22121 \u010e M \u03a3 \u22121 T \u010e M \u2212 \u03a6 2 F + 1 2 tr C \u22121 \u010e M \u03a3 \u22121 T \u03a3 W \u03a3 \u22121 T \u010e M C \u22121 . (19) Observe \u010e M \u03a3 \u22121 T \u010e\nM and \u03a6 are simultaneously diagonalizable since they share the same [1, . . . , 1] null space and \u03a6 has C \u2212 1 equal eigenvalues. Also, observe that\n\u03a3 T = \u03a3 W + 1 C \u010e M \u010e M T .\nLet \u2126 be submatrix of the aligned-SNR matrix \u2126 corresponding to the non-zero singular values i.e. \u2126 = diag {\u03c9 j } C\u22121 c=1 . After taking simultaneous-diagonalizations and canceling partial orthogonal matrices, Equation 19 can be written solely in terms of \u2126:\nL( \u2126) = 1 2C \u2126 \u2126 \u2126 + CI C\u22121 \u22121 \u2126 \u2212 I C\u22121 2 F + 1 2 tr \u2126 ( \u2126 \u2126 + CI C\u22121 ) \u22122 \u2126 .\nThis can be further simplified into\nL({\u03c9 j } C\u22121 c=1 }) = 1 2 C\u22121 j=1 1 C \u03c9 2 j \u03c9 2 j + C \u2212 1 2 + \u03c9 2 j (C + \u03c9 2 j ) 2 = 1 2 C\u22121 j=1 1 \u03c9 2 j + C . (20\n)\nThis completes the proof for Lemma 1.\nThus, the aligned SNR matrix, U \u03a3\n\u2212 1 2 W \u010e M V\n, maximally simplifies analysis by reducing the vanilla SNR matrix to a diagonal matrix (Definition 4) whose entries determine the MSE loss (Lemma 1).\nWe introduce the alignment of features into their own notion of aligned SNR coordinates: Definition 5 (Renormalization to aligned SNR Coordinates). Consider \u010e H \u2208 R P \u00d7CN with corresponding SNR left and right singular vectors U and V (Definition 2). Then, define the SNR-aligned renormalization of the features as\nX = U (\u03a3 W ) \u2212 1 2 \u010e H(V \u2297 I N ) = U \u010e HC \u010e H \u2212 1 2 \u010e H(V \u2297 I N ) \u2208 R C\u00d7N C ,\nwhere \u2297 is the Kronecker product and C is a class-centering matrix defined as\nC = 1 CN I CN \u2212 1 N Y Y .(21)\nAs discussed in Section 3.1, the transformation \u010e\nH \u2192 \u03a3 \u2212 1 2 W \u010e\nH does not change the predictions of the least squares classifier. Hence, it does not change the MSE loss. It is easy to check that this still holds for X i.e. invariance is preserved by the change into SNR-aligned coordinates.\nIt is also easy to check that the sphericity of the within-class covariance is preserved as well: Observation 2 (Sphericity of Within-Class Covariance of SNR Coordinates).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "XCX = I C", "text": "Finally, routine applications of Definitions and canceling out of (partial) orthogonal matrix multiplications in aligned SNR coordinates lead to the following simplified representation for \u2126 and its differential: Observation 3 (Relation Between SNR Coordinates and SNR Matrix). The aligned SNR matrix, \u2126 (Definition 4) can be expressed as a function of the features in aligned SNR-coordinates X (Definition 5) as simply\n\u2126(X) = 1 N XY .\nMoreover, let d\u2126 ij (\u2022) : T X X \u2192 R be the differential 1-form 19 associated with the ij-th entry of \u2126. Define d\u2126 (\u2022) : T X X \u2192 R C\u00d7C such that, for some Z \u2208 T X X , each entry matrix of the matrix d\u2126 (Z) is d\u2126 i,j (Z). Then, it follows that\nd\u2126 (Z) = 1 N ZY , \u2200Z \u2208 T X X .\nFrom a geometric perspective, the transformation in Definition 5 maps the features, H, to X belonging to a Normalized Features Manifold: Definition 6 (Normalized Features Manifold).\nX = {X \u2208 R C\u00d7CN | XCX = I C }\nTo understand the dynamics of the singular values of \u2126-which is connected to the dynamics of the features through Lemma 1-we will analyze the gradient flow on this manifold. To this end, we first identify its tangent space at a particular X as well as the projection operator onto that tangent space. Proposition 3 (Tangent Space of Normalized Features Manifold).\nT X X = {Z \u2208 R C\u00d7CN | XCZ + ZCX = 0}\nProposition 4 (Projection Onto Tangent Space of Normalized Features Manifold).\n\u03a0 T X X (Z) = Z \u2212 1 2 (XCZ + ZCX )X D.3 CONTINUALLY RENORMALIZED GRADIENT FLOW\nIn fact, gradient flow on the normalized features manifold (Definition 6) is the continuous analogue of the intuitive, discrete algorithm in Subsection D.1. In particular, the intuitive algorithm consists of a gradient step, \u010e H 1 = X 0 + \u2206X 0 followed by a mapping to the base space of the fiber bundle,\nX 1 = \u03a3( \u010e H 1 ) \u2212 1 2 \u010e H 1 .\nFor small \u2206X 0 , these two steps are in fact equivalent (up to a negligible term) to taking a T X X -projected gradient step on the manifold X as proven below: Lemma 2. Assuming X 0 \u2208 X , a renormalized gradient step,\nX 1 = \u03a3 \u2212 1 2 W ( \u010e H 1 ) \u010e H 1 = \u03a3 \u2212 1 2 W (X 0 + \u2206X 0 ) \u2022 (X 0 + \u2206X 0 ),\nis equivalent, up to an O \u2206X 0 2 term, to a T X X -projected gradient step, i.e.\nX 1 =X 0 + \u03a0 T X X (\u2206X 0 ) + O \u2206X 0 2 . Proof. Notice that X 1 =\u03a3 \u2212 1 2 W ( \u010e H 1 ) \u010e H 1 = (X 0 + \u2206X 0 )C(X 0 + \u2206X 0 ) \u2212 1 2 (X 0 + \u2206X 0 ) = I + \u2206X 0 CX 0 + X 0 C\u2206X 0 + \u2206X 0 C\u2206X T 0 \u2212 1 2 (X 0 + \u2206X 0 ),\nwhere in the last step we used our assumption that X 0 \u2208 X , i.e., X 0 CX 0 = I. By Taylor's Theorem,\n(I + A) \u2212 1 2 = I \u2212 1 2 A + O( A 2 ).\nTherefore, we get\nX 1 = I \u2212 1 2 \u2206X 0 CX 0 + X 0 C\u2206X 0 + \u2206X 0 C\u2206X T 0 + O \u2206X 0 2 (X 0 + \u2206X 0 ) =X 0 + \u2206X 0 \u2212 1 2 \u2206X 0 CX 0 \u2212 X 0 C\u2206X 0 X 0 + O \u2206X 0 2 =X 0 + \u03a0 T X X (\u2206X 0 ) + O \u2206X 0 2 ,\nwhere the last step follows from Proposition 4.\nAs we transition from a discrete gradient descent to a continuous gradient flow, the step size \u03b7 \u2192 0, and the residual O \u2206X 0 2 in the above lemma becomes negligible, since \u2206X 0 = \u2212\u03b7\u2207 X L LS (X 0 ). This motivates us to study, in the next section, the continually renormalized gradient flow (as defined in Equation 5of the main text) of X on X :\nd dt X = \u2212\u03a0 T X X (\u2207 X L LS (X)) .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.4 PROOF OF PROPOSITION 2 (GRADIENT FLOW IN ALIGNED SNR COORDINATES)", "text": "To prove Proposition 2, we first set up some auxiliary notation and lemmas. Denote the j-th row (transposed) of X and Y , respectively, by\nx j = X e j(22)\nand y j = Y e j .\nRecall that-when differentiating SVDs-the derivative of the j-th singular value, \u03c9 j , only depends on the j-th singular subspace (see Equation 17of Townsend (2016)). As a consequence, we will see in the following lemma that, for any differential dX, the corresponding d\u03c9 j only depends on dx j . Lemma 3 (Derivative of Singular Values With Respect to SNR Coordinates). Given the differential 1-form d\u03c9 j (\u2022) : T X X \u2192 R, the following holds for all 19 dX \u2208 R C\u00d7CN :\nd\u03c9 j (\u03a0 T X X (dX)) = 1 N y j \u2212 \u03c9 j x j C dx j , \u2200j = 1, . . . , C.\nwhere (consistent with Definition 2) we adopt the convention that \u03c9 C =0 is the C-th singular value of the SNR matrix.\nProof. Without loss of generality, assume that X is represented in aligned SNR coordinates (Section D.2). Using Observation 1 and the differential of the singular value decomposition (see Equation 17of Townsend (2016)):\nd\u03c9 j (\u03a0 T X X (dX)) = e j d\u2126 (\u03a0 T X X (dX)) e j ,(23)\nwhere d\u2126 (\u2022) is defined as in Observation 3. Next, Observation 3 implies\nd\u2126 (\u03a0 T X X (dX)) = 1 N \u03a0 T X X (dX) Y .(24)\nUsing the projection onto the tangent space, given in Proposition 4 above,\n\u03a0 T X X (dX) = dX \u2212 1 2 (dX CX + XC dX )X.(25)\nCombining Equation 23, Equation 24, and Equation 25, we obtain:\nd\u03c9 j (\u03a0 T X X (dX)) =e j d\u2126 (\u03a0 T X X (dX)) e j (26) = 1 N e j \u03a0 T X X (dX) Y e j = 1 N e j dX \u2212 1 2 dX CX + XC dX X Y e j .\n19 Notation: For real-valued functions f , we use df (\u2022) : T X X \u2192 R to denote the associated differential 1-form i.e. the function outputting the directional derivative of f in the direction of the argument; For matrices Z \u2208 R m\u00d7n , we use dZ \u2208 R m\u00d7n -without succeeding parenthesis-to denote the matrix differential (cf. Townsend (2016)). These notions are equivalent: df (\u2022) can be represented as a vector df in the dual-space of T X X ; When Z is a function of some vector v, dZ can be represented as a collection of 1-forms by defining dZij (\u2022) = dZ ij dv , \u2022 for each entry Zij.\nMoreover, using Observation 1 and 3, we can simplify these expressions into 1 N e j dX Y e j = 1 N y j dx j 1 N e j dX CX XY e j =\u03c9 j e j dX CX e j = \u03c9 j x j C dx j\n1 N e j XC dX XY e j =\u03c9 j e j XC dX e j = \u03c9 j x j C dx j .\nFinally, substituting the above three expressions into Equation 26, we get:\nd\u03c9 j (\u03a0 T X X (dX)) = 1 N y j \u2212 \u03c9 j x j C dx j ,\nwhich proves the claim.\nUsing Lemma 3, we can now obtain the dynamics of x j under gradient flow: Lemma 4. Under continually renormalized gradient flow (Equation 5),\nd dt x j = \u03c9 j (C + \u03c9 2 j ) 2 1 N y j \u2212 \u03c9 j Cx j , \u2200j = 1, . . . , C,\nwhere (consistent with Definition 2) we adopt the convention that \u03c9 C =0 is the C-th singular value of the SNR matrix.\nProof. Recall x j \u2208 R CN is the j-th row of X \u2208 R C\u00d7CN , which we assume to be in aligned SNR coordinates without loss of generality (Section D.2). Applying the flow definition in Equation 5to the i-th element of x j gives\nd dt x ji = \u2212e j \u03a0 T X X dL LS dX e i = \u2212 \u03a0 T X X dL LS dX , e j e i F = \u2212 dL LS dX , \u03a0 T X X e j e i F ,\nwhere dLLS dX \u2208 R C\u00d7CN is the standard derivative within the ambient R C\u00d7CN -space, e i \u2208 R CN and e j \u2208 R C are the canonical basis vectors, and \u2022, \u2022 F is the Frobenius matrix inner-product. Next, observe that the differential 1-form dL LS (\u2022) : T X X \u2192 R satisfies the following:\ndL LS (Z) = dL LS dX , Z F , \u2200Z \u2208 T X X .\nTaking Z = \u03a0 T X X e j e i then leads to\nd dt x ji = \u2212 dL LS \u03a0 T X X e j e i = \u2212 C\u22121 k=1 dL LS d\u03c9 k d\u03c9 k \u03a0 T X X e j e i = \u2212 dL LS d\u03c9 j d\u03c9 j \u03a0 T X X e j e i\nwhere the second step follows by the chain rule, and the last step follows from Lemma 3 in that d\u03c9 k \u03a0 T X X e j e i = 0 if k = j. Moreover, Lemma 3 also gives d\u03c9 j \u03a0 T X X e j e i = 1 N y j \u2212 \u03c9 j x j C e i .\nIt then follows that\nd dt x j = \u2212 dL LS d\u03c9 j 1 N y j \u2212 \u03c9 j Cx j = \u03c9 j (C + \u03c9 2 j ) 2 1 N y j \u2212 \u03c9 j Cx j ,\nwhere the last step follows from differentiating the expression in Lemma 1.\nThe gradient flows of x j induce the dynamics of \u03c9 j , which are described below.\nLemma 5 (Induced Dynamics on SVD of SNR). Under continually renormalized gradient flow (Equation 5), the dynamics for the j-th non-zero SNR singular value (Definition 2) is\nd dt \u03c9 j = 1 N \u03c9 j (C + \u03c9 2 j ) 2 , \u2200j = 1, . . . , C \u2212 1.(27)\nProof. Without loss of generality, assume X is represented in aligned SNR coordinates (Section D.2). Observation 1, the differential of the singular value decomposition (see Equation 17of Townsend ( 2016)), and Observation 3 collectively imply that for any 20 dX \u2208 T X X , d\u03c9 j (dX) = e j d\u2126 (dX) e j = 1 N e j dX Y e j = 1 N y j dx j .\nRecall that the differential of \u03c9 j only depends on the differential of x j (see Lemma 3). Then, we can apply the chain rule:\nd dt \u03c9 j = d\u03c9 j dx j dx j dt = 1 N \u03c9 j (C + \u03c9 2 j ) 2 y j 1 N y j \u2212 \u03c9 j Cx j ,(29)\nwhere the last equality results substituting-in Lemma 4 and Equation 28. Since Y Y = N I,\n1 N 2 y j y j = 1 N .\nApplying the same relation and Equation 21:\nx j Cy j = e j XCY e j = 1 CN e j X I \u2212 1 N Y Y Y e j = 0.\nCombining all the above equations, we obtain\nd dt \u03c9 j = 1 N \u03c9 j (C + \u03c9 2 j ) 2 .\nThe closed-form given in Proposition 2 now directly follows.\nProposition 2 (Dynamics of Singular Values of SNR Matrix). Continually renormalized gradient flow on the central path (Equation 5) induces the following closed-form dynamics on the SNR singular values (Definition 2): c 1 log(\u03c9 j (t)) + c 2 \u03c9 2 j (t) + c 3 \u03c9 4 j (t) = a j + t, t \u2265 0, for all j = 1, . . . , C \u2212 1.\nc 1 , c 2 , and c 3 are positive constants independent of j, and a j is a constant depending on \u03c9 j (0).\nProof. Follows from symbolically solving the ODE in Lemma 5 with routine methods. The constants are c 1 = C 2 N , c 2 = CN , and c 3 = N 4 .\n1. lim t\u2192\u221e \u03c9 j (t) = \u221e and lim t\u2192\u221e \u03c9j (t)\n4 \u221a t/c3\n= 1, for all j = 1, . . . , C \u2212 1.\n2. lim t\u2192\u221e maxj \u03c9j (t) minj \u03c9j (t) = 1.\nProof. As t tends to infinity, the right-hand side of Equation 9diverges to infinity and therefore so does the left-hand side (LHS). As the LHS approaches infinity, the logarithmic terms become negligible compared to the dominant quartic term, implying \u03c9 4 j (t) \u2192 \u221e. Since \u03c9 j (t) are singular values, they must be non-negative-implying \u03c9 j (t) \u2192 \u221e. Based on the same argument, observe that lim t\u2192\u221e \u03c9j (t)\n4 t c 3\n= 1 for all j. Since the constant c 3 is independent of j, it follows that lim t\u2192\u221e maxj \u03c9j (t) minj \u03c9j (t) = 1.", "publication_ref": ["b42", "b42", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "D.6 PROOF OF COROLLARY 2", "text": "Lemma 6. Under continually renormalized gradient flow (Equation 5), the left and right singular vectors of the SNR matrix remain constant i.e. they are independent of t.\nProof. Without loss of generality, assume X is in represented in aligned SNR coordinates (Section D.2). Recall that, in this coordinate system, the corresponding SNR matrix \u2126 = N \u22121 XY is diagonal, and the left and right singular vectors are simply partial identity matrices.\nTo show that the singular values of the SNR remain constant, it then suffices to show that d\u2126 dt = dX dt Y is a diagonal matrix as well 21 . Lemma 4 gives\nd dt X = 1 N D 1 Y \u2212 D 2 XC\nwhere D 1 and D 2 are C\u00d7C diagonal matrices with the diagonal entries\n\u03c9j (C+\u03c9 2 j ) 2 C c=1and\n\u03c9 2 j (C+\u03c9 2 j ) 2 C c=1\n, respectively. Then,\nd dt \u2126 = d dt X Y = 1 N D 1 Y Y \u2212 D 2 XCY = D 1 ,\nwhere, in the last equality, we used the easy-to-check identities that Y Y =N I C and CY =0. In more detail, the first identity follows from the fact that Y is the one-hot label vectors stacked as columns; the second identity follows from the facts that C is the class-centering matrix, and the one-hot label is the same for examples from the same class. Thus, we have shown d\u2126 dt is diagonal, which concludes our proof.\nLemma 7. A matrix E \u2208 R P \u00d7C is a Simplex ETF if and only if 1. E has exactly C\u22121 non-zero singular values, which are all equal.\n2. E has a rank-1 nullspace spanned by the ones vector, i.e., E1 C =0. In other words, E has zero-mean columns.\nProof. First, recall from Papyan, Han, and Donoho (2020, Definition 1) that a P \u00d7C matrix E is called a Simplex ETF if it satisfies\nE E = \u03b1 C C \u2212 1 I \u2212 1 C \u2212 1 1 C 1 C\nfor some scaling \u03b1>0. We now prove the equivalence.\nSimplex ETF implies 1-2: Consider the SVD decomposition E=U E S E V E , where U E is a P \u00d7(C\u22121) partial orthogonal matrix satisfying U E U E =I, S E is the (C\u22121)\u00d7(C\u22121) diagonal matrix of singular values, and V E is a (C\u22121)\u00d7C partial orthogonal matrix satisfying V E V E =I. Since E is a Simplex ETF, by definition,\nE E = \u03b1 C C \u2212 1 I \u2212 1 C \u2212 1 1 C 1 C ,\nand according to the SVD decomposition\nE E = V E S E U E U E S E V E = V E S 2 E V E . Therefore, V E S 2 E V E = \u03b1 C C \u2212 1 I \u2212 1 C \u2212 1 1 C 1 C .\nNotice that the right-hand-side has C\u22121 equal singular values. This implies S E -and, thus, E as well-has C\u22121 equal singular values. In other words, S E is a (C\u22121)\u00d7(C\u22121) diagonal matrix equal to diag(s, . . . , s) for some scalar s>0.\nNext, notice that on the one hand\nE E1 C = \u03b1 C C \u2212 1 I \u2212 1 C \u2212 1 1 C 1 C 1 C = 0.\nOn the other hand,\nE E1 C = V E S E U E U E S E V E 1 C = V E S 2 E V E 1 C = s 2 V E V E 1 C . Combining the above, we get V E V E 1 C =0. Since V E V E is a C\u00d7C matrix of rank C\u22121, we deduce that the rank-1 nullspace of V E is spanned by the C-dimensional ones vector, i.e. V E 1 C =0. Consequently, E has zero-mean columns since E1 C =U E S E V E 1 C =0.\n1-2 implies Simplex ETF: Assume E has exactly C\u22121 non-zero, equal singular values as well as a rank-1 nullspace spanned by the ones-vector, i.e., E1 C =0. Then, E=U E S E V E where \u2022 U E is a P \u00d7(C\u22121) partial orthogonal matrix satisfying U E U E =I;\n\u2022 S E = diag(s, ..., s) is a (C\u22121)\u00d7(C\u22121) diagonal matrix for some scalar s>0; and\n\u2022 V E is a (C\u22121)\u00d7C partial orthogonal matrix satisfying V E V E =I and also satisfying V E 1 C =0.\nTherefore,\nE E = V E S E U E U E S E V E = V E S 2 E V E = s 2 V E V E . Using our assumptions on V E , V E V E + 1 C 1 C 1 C = I,\nwhere we divide each of the ones-vectors by \u221a C to create a unit vector. Thus, we conclude\nE E = s 2 I \u2212 1 C 1 C 1 C = \u03b1 C C \u2212 1 I \u2212 1 C \u2212 1 1 C 1 C , where \u03b1 = s 2 C\u22121 C\n. Hence, by definition, E is a Simplex ETF. Corollary 2 (Neural Collapse Under MSE Loss). Under continually renormalized gradient flow (Equation 5), the SNR matrix (Equation 8) converges to\nlim t\u2192\u221e 1 \u03c9 max (t) SNR t = U 0 V 0 ,(10)\nwhere U 0 \u2208 R P \u00d7(C\u22121) and V 0 \u2208 R C\u00d7(C\u22121) are the left and right singular vectors of the SNR matrix (Definition 2) at t=0 corresponding to the non-zero singular values; and \u03c9 max (t) is the where, under gradient flow, (W , H) evolve according to a nonlinear ordinary differential equation (ODE). They followed a two-step strategy for studying Neural Collapse. First, they linearized the ODE-claiming nonlinear terms are negligible for models initialized near the origin-and proved the simplified ODE converges to a subspace of (W , H) satisfying (NC1) and (NC3) . Second, they proved that gradient flow, restricted to that subspace, converges to (NC2) .\nThe assumption of small weights and classifiers leading to the linearized ODE is not aligned with today's paradigm. Specifically, the most commonly used He initialization (He et al., 2015) \nCrossEntropy(W H) s.t. W 2 \u2264 1, h i,c 2 \u2264 1,\nwhere they adopt a more technical, spectral norm constraint on W to specify their model.\nBuilding on the results of Chizat & Bach (2018;, E & Wojtowytsch also construct a simple counter-example showing that Neural Collapse need not occur in two-layer, infinite-width networkswhich have been the focus of intense recent study in the theoretical deep learning community (Mei et al., 2018;Rotskoff & Vanden-Eijnden, 2018;Arora et al., 2019 E & Wojtowytsch (2020), this work studies the optimization starting from the second-to-last layer features rather than the last-layer features. In particular, the authors use a strong-duality argument to show that NC emerges in the optimal solution of an equivalent proxy-model to the following optimization:\nmin H L\u22121 ,W L\u22121 ,W L ,\u03b3,\u03b1 L W L (BN \u03b3,\u03b1 (W L\u22121 H L\u22121 )) + , Y + \u03bb 2 \u03b3 2 2 + \u03b1 2 2 + W L 2 F ,\nwhere L(\u2022) is a general convex loss, H L\u22121 are the second-to-last layer activations, W L\u22121 are the second-to-last layer weights, W L are the network classifiers, Y are the training targets, \u03bb is a weight-decay parameter, BN \u03b3,\u03b1 (\u2022) is a batch-norm operator parameterized by \u03b1 and \u03b3, and (\u2022) + is a ReLU. The incorporation of batch-normalization and weight-decay ensures the existence of bounded, well-defined optimal solutions-serving a similar role to that of weight-normalization and weight decay in Poggio & Liao (2020a;b) as well as the norm constraints in the other aforementioned related works.\nSince strong-duality only characterizes properties of the converged optimal solution of an optimization model, Ergen & Pilanci (2020) does not provide insights into the dynamics with which that solution is achieved which training.\nE.6 FANG, HE, LONG, AND SU (2021)\nIn Fang et al. (2021), the authors introduce the (N -)layer-peeled model in which one considers only the direct optimization of the N -th-to-last layer features of a deep net along with the weights that come after the N -th-to-last layer. The motivating philosophy is that, after raw inputs are passed through some initial number of layers, the overparameterization of those layers would allow us to effectively model the N -th-to-last layer features as freely-moving in some subset of Euclidean space. In this terminology, the concurrent works of Mixon et al. (2020); Lu & Steinerberger (2020); E & Wojtowytsch (2020) on the unconstrained (last-layer) features model could be considered instances of a 1-layer-peeled model; while the model of Ergen & Pilanci (2020) could be considered as a 2-layer-peeled model. This perspective is attractive because it gives a name and organization to a common modeling philosophy behind the above-described body of independent works. For comparison, the work of Poggio & Liao (2020a;b) is a non-example of layer-peeled modeling as it considers optimization on the weights of homogeneous deep nets and not the input features. Fang et al. (2021) then analyzes a convex relaxation of the 1-layer-peeled model-with norm constraints on the weights and features-into a semidefinite program. Not only do the authors show that this model exhibits Neural Collapse in the canonical setting of balanced examples-per-class, but they also analyze the behavior of this model under imbalanced classes. While, in the imbalanced case, one would intuitively expect the Simplex ETF to \"skew\" to have bigger angles around overrepresented classes and smaller angles around under-represented ones; Fang et al. (2021) identifies the surprising phenomenon-named minority collapse-in their model where, when the imbalances pass a certain threshold, the last-layer features and classifiers of the under-represented classes collapse to be exactly the same. However, their work does not provide closed-form dynamics or rates at which collapse-in either the balanced or imbalance case-occurs. E.7 ZHU, DING, ZHOU, LI, YOU, SULAM, AND QU (2021)\nIn Zhu et al. (2021), the authors examine the following unconstrained features model:\nmin W ,H,b CrossEntropy (W H + b, Y ) + \u03bb W 2 W 2 F + \u03bb H 2 H 2 F + \u03bb b 2 b 2 F ,\nwhere (W , b) are the classifier weights and biases, H are the last layer features, and (\u03bb W , \u03bb H , \u03bb b ) are weight-decay parameters. On this model, the authors not only prove that all minima exhibit Neural Collapse but also that all local minima are global minima. In comparison to our current paper, Zhu et al. (2021) focus on characterizing the landscape of the loss and, thus, do not explore the dynamics and rate at which such minimizers of the loss are achieved. Zhu et al. (2021) also make notable empirical contributions by conducting a series of experiments on the MNIST and CIFAR10 datasets trained on MLPs, ResNet18, and ResNet50. Their measurements give evidence for the following novel NC-related phenomena in deep classification networks:\n1. NC is algorithm independent: NC emerges in realistic classification deep net training regardless of whether the algorithm is SGD, ADAM, or L-BFGS.\n2. NC occurs on random labels: NC emerges even when the one-hot target vectors are completely shuffled.\n3. Width improves NC: Increasing network width expedites NC when training with random labels.\nThe authors of Zhu et al. (2021) moreover conducted ablation experiments suggesting that the following substitutions can be made to deep neural net architectures without affecting performance:\n1. Weight-decay substitution: Replacing (A) weight-decay on the norm of all network parameters with (B) weight-decay just on the norm of the last-layer features and classifiers. ", "publication_ref": ["b18", "b7", "b29", "b37", "b1", "b12", "b33", "b13", "b14", "b28", "b13", "b33", "b14", "b14", "b48", "b48", "b48", "b48"], "figure_ref": [], "table_ref": []}, {"heading": "ACKNOWLEDGEMENTS", "text": "Some of the computing for this project was performed on the Sherlock cluster. We would like to thank Stanford University and the Stanford Research Computing Center for providing computational resources and support that contributed to these research results. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "largest singular value at time t. Furthermore, Corollary 1 implies the occurrence of (NC1)-(NC4) i.e. renormalized gradient flow on the central path leads to Neural Collapse. Moreover, denoting the Kronecker product with \u2297, the renormalized features matrix converges to\nProof. Derivation of Equation 10: Corollary 1 proves the singular values j = 1, . . . , C \u2212 1 of SNR t diverge to infinity and that their ratio tends to one. By Lemma 6, the renormalized gradient flow will not change the singular vectors of the SNR matrix, and-combined with the fact that the singular values converge to equality (second fact of Corollary 1)-we get the limit in Equation 10.\nDerivation of (NC1): Let s j (\u2022) denote the j-th singular value of its argument. Then, observe that\n(Trace is sum of singular values) \u03bb j (\u03a3 \u2020 B,t \u03a3 W,t ) = \u03bb j (\u03a3 0.5 W,t \u03a3 \u2020 B,t \u03a3 0.5 W,t ) \u2265 0, where \u03bb j (\u2022) denotes the j-th eigenvalue of its argument, and the last inequality follows from the positive-semidefiniteness of \u03a3 0.5 W,t \u03a3 \u2020 B,t \u03a3 0.5 W,t . Since the trace is the sum of eigenvalues, the only way for the trace of \u03a3 \u2020 B,t \u03a3 W,t to tend to zero is if all eigenvalues also tend to zero. All eigenvalues tending to zero implies the matrix itself tends to zero, i.e. lim t\u2192\u221e \u03a3 \u2020 B,t \u03a3 W,t = 0, which is the definition of (NC1) in Section 1.1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Derivation of (NC2)-(NC4):", "text": "Recall that the SNR matrix has zero-mean columns and rank C\u22121 (see Definition 2). This, combined with the fact that the singular values converge to equality (second fact of Corollary 1) imply, by Lemma 7, that the renormalized class-means converge to a Simplex ETF i.e. (NC2) .\nFrom Theorem 1 of Papyan, Han, and Donoho (2020), we then know that (NC3) and (NC4) follow from (NC1) and (NC2) on the central path.\nDerivation of Equation 11: Combining the limit in Equation 10 with (NC1) proves the limit in Equation 11.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E RELATED WORKS EXAMINING NEURAL COLLAPSE", "text": "In this section, we discuss the contributions and limitations of seven recent works that propose and analyze theoretical abstractions of Neural Collapse. These works are only available in preprint, and may not yet be peer-reviewed. Thus, they might ultimately appear with very different claims or results. Additionally, works such as Poggio & Liao (2020a;b); Ergen & Pilanci (2020) also analyze behaviors other than NC; we will only discuss the parts relevant to Neural Collapse here.", "publication_ref": ["b33", "b13"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Optimization algorithms on matrix manifolds", "journal": "Princeton University Press", "year": "2009", "authors": "P-A Absil; Robert Mahony; Rodolphe Sepulchre"}, {"ref_id": "b1", "title": "Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks", "journal": "PMLR", "year": "2019", "authors": "Sanjeev Arora; Simon Du; Wei Hu; Zhiyuan Li; Ruosong Wang"}, {"ref_id": "b2", "title": "", "journal": "", "year": "2016", "authors": "Jimmy Lei Ba; Jamie Ryan Kiros; Geoffrey E Hinton"}, {"ref_id": "b3", "title": "Theory iii: Dynamics and generalization in deep networks", "journal": "", "year": "2019", "authors": "Andrzej Banburski; Qianli Liao; Brando Miranda; Lorenzo Rosasco; Fernanda De ; La Torre; Jack Hidary; Tomaso Poggio"}, {"ref_id": "b4", "title": "Crossvalidation stability of deep networks", "journal": "", "year": "", "authors": "Andrzej Banburski; Fernanda De ; La Torre; Nishka Plant; Ishana Shastri; Tomaso Poggio"}, {"ref_id": "b5", "title": "Boosting the margin: A new explanation for the effectiveness of voting methods. The annals of statistics", "journal": "", "year": "1998", "authors": "Peter Bartlett; Yoav Freund; Wee Sun Lee; Robert E Schapire"}, {"ref_id": "b6", "title": "Reconciling modern machine-learning practice and the classical bias-variance trade-off", "journal": "Proceedings of the National Academy", "year": "2019", "authors": "Mikhail Belkin; Daniel Hsu; Siyuan Ma; Soumik Mandal"}, {"ref_id": "b7", "title": "On the global convergence of gradient descent for over-parameterized models using optimal transport", "journal": "", "year": "2018", "authors": "Lenaic Chizat; Francis Bach"}, {"ref_id": "b8", "title": "Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss", "journal": "", "year": "2020", "authors": "Lenaic Chizat; Francis Bach"}, {"ref_id": "b9", "title": "Exploring the role of loss functions in multiclass classification", "journal": "IEEE", "year": "2020", "authors": "Ahmet Demirkaya; Jiasi Chen; Samet Oymak"}, {"ref_id": "b10", "title": "Imagenet: A large-scale hierarchical image database", "journal": "Ieee", "year": "2009", "authors": "Jia Deng; Wei Dong; Richard Socher; Li-Jia Li; Kai Li; Li Fei-Fei"}, {"ref_id": "b11", "title": "A self-stabilized minor subspace rule", "journal": "IEEE Signal Processing Letters", "year": "1998", "authors": "C Scott;  Douglas; Shun-Ichi Sy Kung;  Amari"}, {"ref_id": "b12", "title": "On the emergence of tetrahedral symmetry in the final and penultimate layers of neural network classifiers", "journal": "", "year": "2020", "authors": "E Weinan; Stephan Wojtowytsch"}, {"ref_id": "b13", "title": "Revealing the structure of deep neural networks via convex duality", "journal": "", "year": "2020", "authors": "Tolga Ergen; Mert Pilanci"}, {"ref_id": "b14", "title": "Layer-peeled model: Toward understanding well-trained deep neural networks", "journal": "", "year": "2021", "authors": "Cong Fang; Hangfeng He; Qi Long; Weijie J Su"}, {"ref_id": "b15", "title": "Tables of the existence of equiangular tight frames", "journal": "", "year": "2015", "authors": "Matthew Fickus; Dustin G Mixon"}, {"ref_id": "b16", "title": "Introduction to Statistical Pattern Recognition", "journal": "Elsevier", "year": "1972", "authors": "Keinosuke Fukunaga"}, {"ref_id": "b17", "title": "Discriminant analysis by gaussian mixtures", "journal": "Journal of the Royal Statistical Society: Series B (Methodological)", "year": "1996", "authors": "Trevor Hastie; Robert Tibshirani"}, {"ref_id": "b18", "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "journal": "", "year": "2015", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b19", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b20", "title": "Matrix analysis", "journal": "Cambridge university press", "year": "2012", "authors": "A Roger; Charles R Johnson Horn"}, {"ref_id": "b21", "title": "Densely connected convolutional networks", "journal": "", "year": "2017", "authors": "Gao Huang; Zhuang Liu; Laurens Van Der Maaten; Kilian Q Weinberger"}, {"ref_id": "b22", "title": "Evaluation of neural architectures trained with square loss vs crossentropy in classification tasks", "journal": "", "year": "2020", "authors": "Like Hui; Mikhail Belkin"}, {"ref_id": "b23", "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "journal": "PMLR", "year": "2015", "authors": "Sergey Ioffe; Christian Szegedy"}, {"ref_id": "b24", "title": "Learning multiple layers of features from tiny images", "journal": "Citeseer", "year": "2009", "authors": "Alex Krizhevsky; Geoffrey Hinton"}, {"ref_id": "b25", "title": "Imagenet classification with deep convolutional neural networks", "journal": "", "year": "2012", "authors": "Alex Krizhevsky; Ilya Sutskever; Geoffrey E Hinton"}, {"ref_id": "b26", "title": "MNIST handwritten digit database", "journal": "", "year": "2010", "authors": "Yann Lecun; Corinna Cortes; C J Burges"}, {"ref_id": "b27", "title": "Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks", "journal": "PMLR", "year": "2020", "authors": "Mingchen Li; Mahdi Soltanolkotabi; Samet Oymak"}, {"ref_id": "b28", "title": "Neural collapse with cross-entropy loss", "journal": "", "year": "2020", "authors": "Jianfeng Lu; Stefan Steinerberger"}, {"ref_id": "b29", "title": "A mean field view of the landscape of twolayer neural networks", "journal": "Proceedings of the National Academy of Sciences", "year": "2018", "authors": "Song Mei; Andrea Montanari; Phan-Minh Nguyen"}, {"ref_id": "b30", "title": "Neural collapse with unconstrained features", "journal": "", "year": "2020", "authors": "G Dustin; Hans Mixon; Jianzong Parshall;  Pi"}, {"ref_id": "b31", "title": "Prevalence of Neural Collapse during the terminal phase of deep learning training", "journal": "Proceedings of the National Academy of Sciences", "year": "2020", "authors": "X Y Vardan Papyan; David L Han;  Donoho"}, {"ref_id": "b32", "title": "Pytorch: An imperative style, high-performance deep learning library", "journal": "", "year": "2019", "authors": "Adam Paszke; Sam Gross; Francisco Massa; Adam Lerer; James Bradbury; Gregory Chanan; Trevor Killeen; Zeming Lin; Natalia Gimelshein; Luca Antiga"}, {"ref_id": "b33", "title": "Explicit regularization and implicit bias in deep network classifiers trained with the square loss", "journal": "", "year": "2020", "authors": "Tomaso Poggio; Qianli Liao"}, {"ref_id": "b34", "title": "Implicit dynamic regularization in deep networks", "journal": "", "year": "2020", "authors": "Tomaso Poggio; Qianli Liao"}, {"ref_id": "b35", "title": "Early stopping-but when?", "journal": "Springer", "year": "1998", "authors": "Lutz Prechelt"}, {"ref_id": "b36", "title": "Overfitting in adversarially robust deep learning", "journal": "PMLR", "year": "2020", "authors": "Leslie Rice; Eric Wong; Zico Kolter"}, {"ref_id": "b37", "title": "Parameters as interacting particles: long time convergence and asymptotic error scaling of neural networks", "journal": "", "year": "2018", "authors": "M Grant; Eric Rotskoff;  Vanden-Eijnden"}, {"ref_id": "b38", "title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "journal": "", "year": "2016", "authors": "Tim Salimans; P Durk;  Kingma"}, {"ref_id": "b39", "title": "Very deep convolutional networks for large-scale image recognition", "journal": "", "year": "2014", "authors": "Karen Simonyan; Andrew Zisserman"}, {"ref_id": "b40", "title": "Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data", "journal": "The Journal of Machine Learning Research", "year": "2018", "authors": "Daniel Soudry; Elad Hoffer"}, {"ref_id": "b41", "title": "Grassmannian frames with applications to coding and communication", "journal": "Applied and computational harmonic analysis", "year": "2003", "authors": "Thomas Strohmer; Robert W Heath Jr"}, {"ref_id": "b42", "title": "Differentiating the singular value decomposition", "journal": "", "year": "2016", "authors": "James Townsend"}, {"ref_id": "b43", "title": "Instance normalization: The missing ingredient for fast stylization", "journal": "", "year": "2016", "authors": "Dmitry Ulyanov; Andrea Vedaldi; Victor Lempitsky"}, {"ref_id": "b44", "title": "The optimised internal representation of multilayer classifier networks performs nonlinear discriminant analysis", "journal": "Neural Networks", "year": "1990", "authors": "R Andrew; David Webb;  Lowe"}, {"ref_id": "b45", "title": "Group normalization", "journal": "", "year": "2018", "authors": "Yuxin Wu; Kaiming He"}, {"ref_id": "b46", "title": "Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms", "journal": "", "year": "2017", "authors": "Han Xiao; Kashif Rasul; Roland Vollgraf"}, {"ref_id": "b47", "title": "Understanding deep learning requires rethinking generalization", "journal": "", "year": "2016", "authors": "Chiyuan Zhang; Samy Bengio; Moritz Hardt; Benjamin Recht; Oriol Vinyals"}, {"ref_id": "b48", "title": "A geometric analysis of neural collapse with unconstrained features", "journal": "", "year": "2021", "authors": "Zhihui Zhu; Tianyu Ding; Jinxin Zhou; Xiao Li; Chong You; Jeremias Sulam; Qing Qu"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "(w c , h i,c + b c )}, while the weights, biases, and other parameters of the network (that determine the behavior of the layers before the last layer) in the network are updated by minimizing the CE loss defined by CE = \u2212 Ave i,c log exp{ w c , h i,c + b c } C c =1 exp{ w c , h i,c + b c } ,", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Portrait of Neural Collapse. Top figure depicts the last-layer features, classmeans, and classifiers with which NC is defined-as well as the Simplex ETF to which they all converge with training. Bottom figure shows the deviations of features from their corresponding class-means. Reproduced and modified from Figure 1 of Papyan, Han, and Donoho (2020).", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "2 W2) on realistic datasetnetwork combinations showing that L \u22a5 LS becomes negligible during training-leading us to define the central path where L \u22a5 LS =zero. (Section 2) \u2022 We reveal a key invariance property on the central path. The invariance motivates the examination of a representative set of features, X=\u03a3 \u2212 1 H, that we call renormalized features. (Sections 3.1-3.2)", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 22Figure 2 measures the empirical values of the above decomposition terms on five canonical datasets and three prototypical networks. It shows that L \u22a5 LS ( W , H) becomes negligible compared to L LS ( H) when training canonical deep nets on benchmark datasets. In other words, L( W , H)\u2248L LS ( H) starting from an early epoch in training and persists into TPT.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 2 :2Figure 2: Decomposition of MSE loss: Each array column shows a benchmark image classification dataset while each row shows a canonical deep net architecture trained with MSE loss. The red vertical line indicates the epoch at which zero training error was achieved. In each array cell, we plot terms of the MSE loss decompositionL( W , H) = L NC1 ( H) + L NC2/3 ( H) + L \u22a5 LS ( W , H) from Section 2. Starting from an early epoch in training, L \u22a5 LS ( W , H) becomes negligible compared to the dominant term, L NC1 ( H), implying L \u22a5 LS ( W , H) L LS ( H)=L NC1 ( H)+L NC2/3 ( H), i.e. the features and classifiers are effectively on the central path during TPT. Note that L NC2/3 ( H) diminishes the fastest among all the terms: Intuitively, this shows that the network primarily focuses on distributing the feature class-means into a \"uniform\" Simplex ETF configuration (NC1)-(NC2) early on and, from there, compresses the activations towards their class-means, i.e. (NC1) , as much as possible. Further experimental details are in Appendix A. Outlier behavior is discussed in Appendix A.7.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "(A1) Restriction to central path: Figure 2 shows that-in practice, during TPT-L \u22a5 LS ( W , H) is observed to be negligible compared to the dominant term, L LS ( H). This indicates ( W , H) is close to the central path (Equation 4) where L( W , H) = L LS ( H). One the central path, one is effectively training with L LS as the loss. (A2) Zero global mean: Proximity to the central path implies that the last-layer biases become close to b LS . Having biases b = b LS", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": ")4 RELATED WORKSSince the publication of Papyan, Han, and Donoho (2020), several works(Mixon et al., 2020;Lu & Steinerberger, 2020;E & Wojtowytsch, 2020;Poggio & Liao, 2020a;b;Fang et al., 2021;Ergen & Pilanci, 2020;Zhu et al., 2021) proposed mathematical frameworks ratifying Neural Collapse. Among them, Mixon et al. (2020) and Poggio & Liao (2020a;b) also examine MSE-NC. Our contributions are distinct from the MSE-NC analyses of Mixon et al. (2020) and Poggio & Liao (2020a;b): The work of Mixon et al. (2020) relies on a linearization of the unconstrained features model ODE while Poggio & Liao (2020a;b)-in a special section co-authored with Andrzej Banburski-examine homogeneous, weight-normalized networks. In contrast, this paper considers the dynamics of the renormalized gradient flow on the central path (motivated by the discussions in Sections 2-3). Neither Mixon et al. (2020) nor Poggio & Liao (2020a;b) provide exact, closed-form dynamics as we do. Outside the MSE setting, Lu & Steinerberger (2020); E & Wojtowytsch (2020); Ergen & Pilanci (2020); Fang et al. (2021); Zhu et al. (2021) examine the emergence of the NC properties under variants of the unconstrained features/layer-peeled model trained with CE loss. These works focus on characterizing the loss landscape or the global minima without describing the dynamics.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 3 :3Figure 3: Plots analogous to Figure 2 in Papyan et al. (2020), but on networks trained with MSE Loss.Results demonstrate that last-layer features and classifiers approach equinormness.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 4 :4Figure 4: Plots analogous to Figure 3 in Papyan et al. (2020), but on networks trained with MSE Loss. Results demonstrate that last-layer features and classifiers approach equiangularity.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 5 :5Figure 5: Plots analogous to Figure 4 in Papyan et al. (2020), but on networks trained with MSE Loss. Results demonstrate that last-layer features and classifiers approach maximal-equiangularity.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 6 :6Figure 6: Plots analogous to Figure 5 in Papyan et al. (2020), but on networks trained with MSE Loss. Results demonstrate that last-layer features and classifiers approach self-duality.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 7 :7Figure 7: Plots analogous to Figure 6 in Papyan et al. (2020), but on networks trained with MSE Loss. Results demonstrate that last-layer features undergo variability collapse.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 8 :8Figure 8: Plots analogous to Figure 7 in Papyan et al. (2020), but on networks trained with MSE Loss. Results demonstrate that classifier decisions converge to those of the nearest class center decision rule.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Figure 9 :9Figure 9: Plots analogous to Figure 8 in Papyan et al. (2020), but on networks trained with MSE Loss. Results demonstrate that networks become more robust when trained beyond 0-error. The median improvement in the robustness measure in the last epoch over the first epoch achieving zero training error is 0.1762; the mean improvement is 0.9278.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Figure 10 :10Figure 10: Activation collapse under MSE loss vs. CE loss: Comparison of activation collapse observed in this paper for networks trained under MSE loss (Figure 7) with that observed in Papyan et al. (2020) under CE loss. Networks trained with MSE loss tend to achieve faster activation collapse than those trained with CE.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "Figure 11 :11Figure 11: Adversarial robustness under MSE loss vs. CE loss: Comparison of adversarial robustness observed in this paper for networks trained under MSE loss (Figure 9) with that observed in Papyan et al. (2020) under CE loss. Robustness tends to be better-sometimes several magnitudes betterwhen the networks are trained with MSE loss than with CE loss.", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "Figure 12 :12Figure12: Activation collapse on test data for both losses: Activation collapse observed on test data for models trained with MSE loss (from this current paper) and CE loss (posted by Papyan, Han, and Donoho (2020) on Stanford Data Repository). On the test data, activation collapse still visibly occurs on multiple dataset-network combinations: Albeit the rate of collapse is much slower on the test data compared to that on the train data, and the plotted measure (described in Figure6of Papyan, Han, and Donoho (2020)) at the last epoch is larger than that on the train data. Also interesting is that the value at the last epoch is roughly monotonic with the difficulty of the dataset. See discussion in Section A.8.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_17", "figure_caption": "examined in this paper's experiments. But perhaps the outlier nature of the STL10-ResNet and STL10-DenseNet combinations signal that the MSE loss modifications proposed by Hui & Belkin (2020) and Demirkaya et al. (2020) ought to have been used. A.8 OPEN QUESTIONS: WEIGHT-DECAY, BATCH NORMALIZATION, SGD, GENERALIZATION, TEST DATA", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_18", "figure_caption": "Theorem 2 .2(Decomposition of Least-Squares Component) The least-squares component, L LS ( H), of the MSE decomposition in Theorem 1 can be further decomposed into L LS ( H) = L NC1 ( H) + L NC2/3 ( H), where", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "code for reproducing NC under both MSE and CE-as well as the Figure2MSE decomposition-in the following Google Colaboratory notebook: here. Experimental measurements used to generated the plots in this paper have been deposited in the Stanford Digital Repository, here. All datasets and networks used in our experiments originate from the PyTorch Model Zoo with pre-processing details described in Appendix A. Formal statements and proofs of all our theoretical results are provided in Appendices B-D.APPENDIXA MSE Neural Collapse experiments A.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Optimization methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Computational resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Formatting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.6 Experimental results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.7 Exceptions: STL10-ResNet and STL10-DenseNet . . . . . . . . . . . . . . . . . . Intuitions for Theorem 2 in unextended coordinates . . . . . . . . . . . . . . . . . C.2 Additional intuitions for Theorem 2 terms . . . . . . . . . . . . . . . . . . . . . . Implications of invariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Aligned SNR coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Continually renormalized gradient flow . . . . . . . . . . . . . . . . . . . . . . . D.4 Proof of Proposition 2 (gradient flow in aligned SNR coordinates) . . . . . . . . . D.5 Proof of Corollary 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.6 Proof of Corollary 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Mixon, Parshall, and Pi (2020) . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Lu and Steinerberger (2020) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 E and Wojtowytsch (2020) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 Poggio & Liao (2020a;b) (with Banburski) . . . . . . . . . . . . . . . . . . . . . . E.5 Ergen & Pilanci (2020) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.6 Fang, He, Long, and Su (2021) . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.7 Zhu, Ding, Zhou, Li, You, Sulam, and Qu (2021) . . . . . . . . . . . . . . . . . .A MSE NEURAL COLLAPSE EXPERIMENTSThe experiments in this section examine the properties of Neural Collapse (NC) on deep nets trained using MSE loss. The direct MSE-analogues to the cross-entropy (CE) loss table and figures in Papyan, Han, and Donoho (2020) are in Table1 and Figures 3-9 here. Furthermore, Figures 10-11 compares the MSE-NC experiments observed in this paper with the CE-NC behaviors in Papyan, Han, and Donoho (2020). Experimental descriptions and discussions are within the captions. Subsections A.1-A.5 describe formatting and experimental details. Then, Subsection A.6 collectively presents all experimental figures for this section. Subsection A.7 addresses outlier behaviors observed in the plots. Finally, Subsection A.8 discusses open questions inspired by NC.", "figure_data": "A.8 Open Questions: Weight-decay, batch normalization, SGD, generalization, test data B Theorem 1 C Theorem 2 C.1 D Theorem 3 D.1 E Related works examining Neural Collapse We consider the MNIST, FashionMNIST, CIFAR10, SVHN, and STL10 datasets with pre-processing E.1 A.1 DATASETS the same as in Papyan, Han, and Donoho (2020)."}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Table comparing test-accuracy at moment 0-error is achieved vs. at the end of training. Analogous to Table 1 of Papyan, Han, and Donoho (2020). The median improvement is 0.962 percentage points; the mean is 1.833 percentage points.", "figure_data": "DATASETNETACC. 0-ERROR ACC. FINALVGG99.2399.59MNISTRESNET99.1699.70DENSENET99.6299.70VGG92.7692.95FASHIONRESNET93.5593.76DENSENET90.5692.95VGG89.3593.91SVHNRESNET85.1892.65DENSENET95.6195.23VGG83.1384.54CIFAR10RESNET75.4376.39DENSENET91.7791.78VGG60.4367.24STL10RESNET59.3560.56DENSENET58.7460.41"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "by examining Equation 18, we see that L NC2/3 ( \u010e H) quantifies deviations of W LS \u010e M -i.e. the matrix of class-mean predictions-from the standard Simplex ETF. Under (NC2) and (NC3) , both W LS and \u010e M tend to jointly aligned ETF's, possibly in an alternate pose; so W LS \u2192 \u03a6U T and \u010e M \u2192 U \u03a6 for a partial orthogonal matrix U satisfying U TU = I C . Since \u03a6 2 = \u03a6, (NC2) and (NC3) together demand W LS \u010e M \u2192 \u03a6.Thus, L NC2/3 ( \u010e H) can be interpreted as a semi-metric reflecting the distance from achieving (NC2) and (NC3) .", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Published as a conference paper at ICLR 2022 E.1 MIXON, PARSHALL, AND PI (2020) Mixon et al. (2020) considered the unconstrained features model in Equation 2 (without weight decay)", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "CE(W , M ) s.t. w c 2 = \u00b5 c 2 = 1. Since under linear separability the CE loss can be driven arbitrarily close to zero, just by re-scaling the norms of W and M , the authors further imposed a norm constraint on w c and \u00b5 c . Lu & Steinerberger observe that the global minimizer of this optimization problem is only achieved once W and M are the same Simplex ETF. This derivation is suggestive, but it does not identify closed-form dynamics which would get gradient flow to such a global minimizer, nor does it address the rate of convergence to Neural Collapse. Additionally, the constraint on \u00b5 c possesses no immediate or direct analogy to standard deep net training-where procedures often control the norm of the weights W , but not features H-nor class-means M .", "figure_data": "is designed:(i) to create weights with non-negligible magnitude; and (ii) to preserve the magnitude of features, asthey propagate throughout the layers of the network, exactly so that last-layer features would havenon-negligible magnitude. Moreover, the analysis of Mixon et al. essentially assumes that (NC1) and(NC3) occur much sooner than (NC2) . However, from the experiments in both Papyan, Han, andDonoho (2020) and this paper, there is no empirical evidence that (NC2) happens slower than (NC1)and (NC3) in practice.E.2 LU AND STEINERBERGER (2020)While the MSE loss provides a mathematically natural setting for analysis, the modern paradigm inmulti-class classification with deep learning involves training with CE loss, which is more challengingto analyze than MSE.Lu & Steinerberger (2020) studied the (one-example-per-class) unconstrained 22 features model withCE loss:min W ,ME.3 E AND WOJTOWYTSCH (2020)E & Wojtowytsch (2020) also consider the unconstrained features model 22 with CE loss,min W ,H"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "). Thus, E & Wojtowytsch's counterexample suggests the alternative perspective that, despite the expressiveness of infinite-width, two-layer networks, such abstractions do not capture key aspects of trained deep nets.As withLu & Steinerberger (2020), standard deep net training does not possess any direct analogies for constraining the norm of features (as opposed to weights)-nor are there any paradigmatic regularizations that correspond to controlling the spectral norm on W . Moreover, the work does not characterize any closed-form dynamics or the rate of convergence to Neural Collapse.E.4 POGGIO & LIAO (2020A;B) (WITH BANBURSKI)Distinguished from the simplified unconstrained features models in the previously mentioned works is the theoretical analysis ofPoggio & Liao (2020a;b)  (in a special section, co-authored with Andrzej Banburski).The authors study deep homogeneous classification networks, with weight normalization layers, trained with stochastic gradient descent and weight decay. This is much closer to today's training paradigm, but the setting still differs from the one in which Neural Collapse has been empirically observed in Papyan, Han, and Donoho (2020) and in Section A of this paper. In particular, they replace batch normalization with weight normalization and consider deep homogeneous networks; homogeneous networks can not have bias vectors nor skip connections, which are present both in ResNet and DenseNet. Moreover, the work gives explicit descriptions of neither the dynamics nor the rate of convergence to Neural Collapse.E.5 ERGEN & PILANCI (2020)While the above-described works tend to focus on either the used-in-practice CE loss or the theoretically-insightful MSE loss,Ergen & Pilanci (2020) observed that these are both instances of the general class of convex loss functions and, thus, one could derive insights from the classical convex analysis literature. Moreover, compared toMixon et al. (2020);Lu & Steinerberger (2020);", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "2. Classifier substitution: Replacing (A) the last-layer classifiers that are trained with SGD with (B) Simplex ETF classifiers that are fixed throughout training. While the authors only demonstrated these new behaviors on limited network-dataset combinations, these experiments indeed inspire interesting conjectures about the generalization behavior of deep nets as well as potential architecture design improvements; Zhu et al. (2021) discuss many of these conjectures and related open-questions in detail.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u00b5 G = Ave i,c h i,c", "formula_coordinates": [2.0, 174.88, 252.28, 60.97, 14.46]}, {"formula_id": "formula_1", "formula_text": "\u03a3 W = Ave i,c (h i,c \u2212 \u00b5 c )(h i,c \u2212 \u00b5 c ) ,(1)", "formula_coordinates": [2.0, 131.57, 333.5, 174.43, 14.46]}, {"formula_id": "formula_2", "formula_text": "\u03a3 B = Ave c (\u00b5 c \u2212 \u00b5 G )(\u00b5 c \u2212 \u00b5 G ) .", "formula_coordinates": [2.0, 134.82, 374.78, 144.37, 14.16]}, {"formula_id": "formula_3", "formula_text": "\u00b5 c \u2212 \u00b5 G , \u00b5 c \u2212 \u00b5 G \u00b5 c \u2212 \u00b5 G 2 \u00b5 c \u2212 \u00b5 G 2 \u2192 1, c = c \u22121 C\u22121 , c = c \u00b5 c \u2212 \u00b5 G 2 \u2212 \u00b5 c \u2212 \u00b5 G 2 \u2192 0 \u2200c = c (NC3) Convergence to self-duality:", "formula_coordinates": [2.0, 107.67, 509.79, 189.74, 56.02]}, {"formula_id": "formula_4", "formula_text": "v c , v c v c 2 v c 2 = 1, for c = c \u2212 1 C\u22121 , for c = c", "formula_coordinates": [3.0, 228.29, 135.46, 148.2, 23.43]}, {"formula_id": "formula_5", "formula_text": "L(W , b, H) = 1 2 Ave i,c W h i,c + b \u2212 y i,c 2 2 + \u03bb 2 ( W 2 F + b 2 2 ) (2) = 1 2CN W H + b1 CN \u2212 Y 2 F + \u03bb 2 ( W 2 F + b 2 2 ),", "formula_coordinates": [3.0, 167.67, 262.94, 336.33, 48.58]}, {"formula_id": "formula_6", "formula_text": "L( W , H) = 1 2 Ave i,c W h i,c \u2212 y i,c 2 2 + \u03bb 2 W 2 F .(3)", "formula_coordinates": [4.0, 203.34, 313.5, 300.66, 22.31]}, {"formula_id": "formula_7", "formula_text": "\u03a3 T = Ave i,c ( h i,c \u2212 \u00b5 G )( h i,c \u2212 \u00b5 G ) \u2208 R (P +1)\u00d7(P +1) M = [ \u00b5 1 , . . . , \u00b5 C ] \u2208 R (P +1)\u00d7C .", "formula_coordinates": [4.0, 195.75, 380.74, 219.99, 34.35]}, {"formula_id": "formula_8", "formula_text": "W LS = 1 C M ( \u03a3 T + \u00b5 G \u00b5 G + \u03bbI) \u22121 ,", "formula_coordinates": [4.0, 226.27, 470.89, 159.46, 22.31]}, {"formula_id": "formula_9", "formula_text": "Theorem 1. (Decomposition of MSE Loss; Proof in Appendix B) The MSE loss, L( W , H), can be decomposed into two terms, L( W , H) = L LS ( H) + L \u22a5 LS ( W , H),where", "formula_coordinates": [4.0, 107.67, 556.06, 396.33, 24.43]}, {"formula_id": "formula_10", "formula_text": "L LS ( H) = 1 2 Ave i,c W LS h i,c \u2212 y i,c 2 2 + \u03bb 2 W LS 2 F ,and", "formula_coordinates": [4.0, 108.0, 587.48, 301.04, 38.99]}, {"formula_id": "formula_11", "formula_text": "L \u22a5 LS ( W , H) = 1 2 tr ( W \u2212 W LS ) \u03a3 T + \u00b5 G \u00b5 G + \u03bbI ( W \u2212 W LS ) .", "formula_coordinates": [4.0, 158.1, 625.54, 295.79, 22.31]}, {"formula_id": "formula_12", "formula_text": "L LS ( H) = L NC1 ( H) + L NC2/3 ( H),where", "formula_coordinates": [5.0, 125.99, 193.68, 170.24, 9.68]}, {"formula_id": "formula_13", "formula_text": "L NC1 ( H) = 1 2 tr W LS \u03a3 W + \u03bbI W LS , L NC2/3 ( H) = 1 2C W LS M \u2212 I 2 F .", "formula_coordinates": [5.0, 218.41, 206.81, 175.18, 46.5]}, {"formula_id": "formula_14", "formula_text": "P = ( W LS ( H), H) | H \u2208 R (P +1)\u00d7CN , (4", "formula_coordinates": [5.0, 216.89, 402.28, 283.24, 11.88]}, {"formula_id": "formula_15", "formula_text": "d dt X = \u2212\u03a0 T X X (\u2207 X L LS (X)) ,(5)", "formula_coordinates": [5.0, 240.63, 592.04, 263.37, 22.31]}, {"formula_id": "formula_16", "formula_text": "L( W , H) = L NC1 ( H) + L NC2/3 ( H) + L \u22a5 LS ( W , H) from Section 2. Starting from an early epoch in training, L \u22a5 LS ( W , H) becomes negligible compared to the dominant term, L NC1 ( H), implying L \u22a5 LS ( W , H) L LS ( H)=L NC1 ( H)+L NC2/3 ( H), i.e", "formula_coordinates": [6.0, 108.0, 324.97, 397.75, 40.17]}, {"formula_id": "formula_17", "formula_text": "\u010e H = H \u2212 \u00b5 G 1 CN ; \u010e M = [\u00b5 1 \u2212 \u00b5 G , . . . , \u00b5 C \u2212 \u00b5 G ]; \u03a3 T = 1 CN \u010e H \u010e H .", "formula_coordinates": [7.0, 147.24, 99.63, 317.51, 22.31]}, {"formula_id": "formula_18", "formula_text": "W LS = [W LS , b LS ] = C \u22121 \u010e M \u03a3 \u22121 T , C \u22121 1 C \u2212 C \u22121 \u010e M \u03a3 \u22121 T \u00b5 G .(6)", "formula_coordinates": [7.0, 162.49, 141.99, 341.51, 13.31]}, {"formula_id": "formula_19", "formula_text": "W LS h + b LS \u2212 y = C \u22121 \u010e M \u03a3 \u22121 T h + C \u22121 1 C \u2212 C \u22121 \u010e M \u03a3 \u22121 T \u00b5 G \u2212 y = W LS (h \u2212 \u00b5 G ) \u2212 (y \u2212 C \u22121 1 C ).", "formula_coordinates": [7.0, 160.75, 176.47, 290.14, 27.91]}, {"formula_id": "formula_20", "formula_text": "s P = W LS , \u010e H W LS = C \u22121 \u010e M \u03a3 \u22121 T .", "formula_coordinates": [7.0, 217.16, 441.4, 177.68, 13.31]}, {"formula_id": "formula_21", "formula_text": "W LS (A \u010e H)A \u010e H = C \u22121 A \u010e M A \u010e H A \u010e H \u22121 A \u010e H = C \u22121 \u010e M A A \u22121 \u010e H \u010e H \u22121 A \u22121 A \u010e H = C \u22121 \u010e M \u010e H \u010e H \u22121 \u010e H = C \u22121 \u010e M \u03a3 \u22121 T \u010e H = W LS \u010e H \u010e H,(7)", "formula_coordinates": [7.0, 143.15, 494.64, 360.85, 61.41]}, {"formula_id": "formula_22", "formula_text": "\u010e H \u2192A \u010e H with A = \u03a3 \u2212 1 2 W ( \u010e H).", "formula_coordinates": [7.0, 380.61, 615.9, 125.14, 16.18]}, {"formula_id": "formula_23", "formula_text": "\u03a3 W (A \u010e H) = A\u03a3 W ( \u010e H)A = I, so we will call A \u010e H the renormalized features. The class-means of A \u010e H are \u010e M (A \u010e H) = A \u010e M ( \u010e H) = \u03a3 \u2212 1 2 W ( \u010e H) \u010e M ( \u010e H),", "formula_coordinates": [7.0, 108.0, 647.92, 302.63, 46.91]}, {"formula_id": "formula_24", "formula_text": "W LS = C \u22121 \u010e M C \u22121 \u010e M \u010e M + \u03a3 W \u22121 .", "formula_coordinates": [8.0, 218.53, 132.15, 174.93, 14.26]}, {"formula_id": "formula_25", "formula_text": "SNR \u2261 \u03a3 \u2212 1 2 W \u010e M . (8", "formula_coordinates": [8.0, 271.99, 184.49, 228.14, 16.18]}, {"formula_id": "formula_26", "formula_text": ")", "formula_coordinates": [8.0, 500.13, 189.9, 3.87, 8.64]}, {"formula_id": "formula_27", "formula_text": "SNR = \u010e M (\u03a3 \u2212 1 2 W \u010e H).", "formula_coordinates": [8.0, 302.82, 302.11, 90.89, 16.18]}, {"formula_id": "formula_28", "formula_text": "SNR = U \u2126V = C\u22121 j=1 \u03c9 j u j v j .", "formula_coordinates": [8.0, 238.37, 363.85, 135.27, 30.32]}, {"formula_id": "formula_30", "formula_text": "4 \u221a t/c3", "formula_coordinates": [9.0, 302.47, 116.6, 22.34, 14.89]}, {"formula_id": "formula_31", "formula_text": "maxj \u03c9j (t) minj \u03c9j (t) = 1.", "formula_coordinates": [9.0, 179.95, 142.01, 59.99, 14.61]}, {"formula_id": "formula_32", "formula_text": "\u2212 1 2 W \u010e", "formula_coordinates": [9.0, 411.12, 166.44, 24.7, 16.18]}, {"formula_id": "formula_33", "formula_text": "lim t\u2192\u221e 1 \u03c9 max (t) SNR t = U 0 V 0 ,(10)", "formula_coordinates": [9.0, 243.97, 280.32, 260.03, 23.22]}, {"formula_id": "formula_34", "formula_text": "lim t\u2192\u221e 1 \u03c9 max (t) \u03a3 \u2212 1 2 W,t \u010e H t = ( U 0 V 0 ) \u2297 1 N . (11", "formula_coordinates": [9.0, 222.58, 372.5, 277.27, 23.23]}, {"formula_id": "formula_35", "formula_text": "DATASET VGG RESNET DENSENET MNIST VGG11 RESNET18 DENSENET40 FASHIONMNIST VGG11 RESNET18 DENSENET250 SVHN VGG13 RESNET34 DENSENET40 CIFAR10 VGG11 RESNET50 DENSENET100 STL10 VGG11 RESNET18 DENSENET201 A.3 OPTIMIZATION METHODOLOGY", "formula_coordinates": [14.0, 108.25, 489.42, 312.0, 104.62]}, {"formula_id": "formula_36", "formula_text": "L LS ( H) = 1 2 Ave i,c W LS h i,c \u2212 y i,c 2 2 + \u03bb 2 W LS 2 F ,and", "formula_coordinates": [22.0, 108.0, 137.21, 301.04, 36.57]}, {"formula_id": "formula_37", "formula_text": "L \u22a5 LS ( W , H) = 1 2 tr ( W \u2212 W LS ) \u03a3 T + \u00b5 G \u00b5 G + \u03bbI ( W \u2212 W LS ) .", "formula_coordinates": [22.0, 158.1, 172.58, 295.79, 22.31]}, {"formula_id": "formula_38", "formula_text": "L( W , H) = 1 2 Ave i,c W h i,c \u2212 y i,c 2 2 + \u03bb 2 W 2 F . (12", "formula_coordinates": [22.0, 203.34, 221.31, 296.51, 22.31]}, {"formula_id": "formula_39", "formula_text": ")", "formula_coordinates": [22.0, 499.85, 228.36, 4.15, 8.64]}, {"formula_id": "formula_40", "formula_text": "Ave i,c ( W LS h i,c \u2212 y i,c ) h i,c + \u03bb W LS = 0. (13", "formula_coordinates": [22.0, 227.3, 283.78, 272.56, 14.46]}, {"formula_id": "formula_41", "formula_text": ")", "formula_coordinates": [22.0, 499.85, 284.12, 4.15, 8.64]}, {"formula_id": "formula_42", "formula_text": "W LS = C \u22121 M ( \u03a3 T + \u00b5 G \u00b5 G + \u03bbI) \u22121 .", "formula_coordinates": [22.0, 221.73, 320.72, 168.54, 12.69]}, {"formula_id": "formula_43", "formula_text": "1 2 Ave i,c W LS h i,c \u2212 y i,c + ( W \u2212 W LS ) h i,c 2 2 + \u03bb 2 W 2 F .", "formula_coordinates": [22.0, 190.08, 351.21, 233.03, 22.31]}, {"formula_id": "formula_44", "formula_text": "1 2 Ave i,c W LS h i,c \u2212 y i,c 2 2 + 1 2 Ave i,c ( W \u2212 W LS ) h i,c 2 2 \u2212 \u03bb tr W LS ( W \u2212 W LS ) + \u03bb 2 W 2 F , which is equivalent to 1 2 Ave i,c W LS h i,c \u2212 y i,c 2 2 + 1 2 Ave i,c ( W \u2212 W LS ) h i,c 2 2 + \u03bb 2 W LS 2 F + \u03bb 2 W LS \u2212 W 2 F .", "formula_coordinates": [22.0, 107.64, 393.84, 394.56, 64.94]}, {"formula_id": "formula_45", "formula_text": "L( W , H) = L LS ( H) + L \u22a5 LS ( W , H), where L LS ( H) \u2261 1 2 Ave i,c W LS h i,c \u2212 y i,c 2 2 + \u03bb 2 W LS 2 F(14)", "formula_coordinates": [22.0, 107.64, 486.11, 396.36, 48.08]}, {"formula_id": "formula_46", "formula_text": "L \u22a5 LS ( W , H) \u2261 1 2 Ave i,c ( W \u2212 W LS ) h i,c 2 2 + \u03bb 2 W LS \u2212 W 2 F = 1 2 tr ( W \u2212 W LS ) \u03a3 T + \u00b5 G \u00b5 G + \u03bbI ( W \u2212 W LS ) .", "formula_coordinates": [22.0, 157.3, 550.36, 297.41, 48.08]}, {"formula_id": "formula_47", "formula_text": "L NC1 ( H) = 1 2 tr W LS \u03a3 W + \u03bbI W LS , L NC2/3 ( H) = 1 2C W LS M \u2212 I 2 F .", "formula_coordinates": [22.0, 218.41, 690.84, 175.18, 45.93]}, {"formula_id": "formula_48", "formula_text": "L LS ( H) = 1 2 Ave i,c W LS ( h i,c \u2212 \u00b5 c ) 2 2 + \u03bb 2 W LS 2 F + 1 2 Ave c W LS \u00b5 c \u2212 y i,c 2 2 .", "formula_coordinates": [23.0, 145.93, 126.32, 320.14, 22.31]}, {"formula_id": "formula_49", "formula_text": "L NC1 ( H) = 1 2 tr W LS \u03a3 W + \u03bbI W LS (15", "formula_coordinates": [23.0, 221.04, 176.02, 278.81, 22.31]}, {"formula_id": "formula_50", "formula_text": ")", "formula_coordinates": [23.0, 499.85, 183.08, 4.15, 8.64]}, {"formula_id": "formula_51", "formula_text": "L NC2/3 ( H) = 1 2C W LS M \u2212 I 2 F .", "formula_coordinates": [23.0, 215.61, 200.0, 140.89, 22.31]}, {"formula_id": "formula_52", "formula_text": "W LS = C \u22121 \u010e M \u03a3 \u22121 T ,(16)", "formula_coordinates": [23.0, 260.52, 294.36, 243.48, 13.31]}, {"formula_id": "formula_53", "formula_text": "L NC1 ( \u010e H) simplifies to L NC1 ( \u010e H) = 1 2 tr W LS \u03a3 W W LS ,(17)", "formula_coordinates": [23.0, 108.0, 323.68, 396.0, 32.25]}, {"formula_id": "formula_54", "formula_text": "\u010e H \u2208 R P \u00d7CN has columns h i,c \u2212 \u00b5 G . The term L NC2/3 ( \u010e H) also simplifies instructively to L NC2/3 ( \u010e H) = 1 2C W LS \u010e M \u2212 \u03a6 2 F ,(18)", "formula_coordinates": [23.0, 134.47, 358.87, 369.53, 40.86]}, {"formula_id": "formula_55", "formula_text": "\u03a6 = I \u2212 1 C 11 .", "formula_coordinates": [23.0, 270.25, 423.35, 71.49, 22.31]}, {"formula_id": "formula_56", "formula_text": "L NC1 ( \u010e H", "formula_coordinates": [23.0, 143.87, 499.93, 35.34, 9.84]}, {"formula_id": "formula_57", "formula_text": "L NC1 ( \u010e H) \u2192 0 is for \u03a3 W \u2192 0. Similarly,", "formula_coordinates": [23.0, 108.0, 531.71, 396.0, 20.64]}, {"formula_id": "formula_58", "formula_text": "L(W LS ( \u010e H), \u010e M ( \u010e H)) are invariant under the transformation \u010e H \u2192 A \u010e H when A is a invertible matrix. Since our interest mainly lies with A = \u03a3 \u2212 1 2", "formula_coordinates": [24.0, 108.0, 348.63, 396.0, 24.19]}, {"formula_id": "formula_59", "formula_text": ") = I. Every \u010e H \u2208 H is representable as \u010e H = AX where A = \u03a3 \u2212 1 2", "formula_coordinates": [24.0, 108.0, 424.61, 396.35, 24.19]}, {"formula_id": "formula_60", "formula_text": "X 0 = \u03a3 \u2212 1 2 W ( \u010e H 0 ) \u010e H 0 .", "formula_coordinates": [24.0, 203.69, 626.3, 86.76, 16.18]}, {"formula_id": "formula_61", "formula_text": "X 1 = \u03a3 \u2212 1 2 W ( \u010e H 1 ) \u010e H 1 .", "formula_coordinates": [25.0, 280.43, 134.51, 87.0, 16.18]}, {"formula_id": "formula_62", "formula_text": "\u2126 = diag {\u03c9 c } C\u22121 c=1 , 0 = U \u03a3 \u2212 1 2 W \u010e M V We call \u2126 the aligned SNR matrix.", "formula_coordinates": [25.0, 107.17, 440.08, 280.78, 28.31]}, {"formula_id": "formula_63", "formula_text": "\u2126 = C\u22121 j=1", "formula_coordinates": [25.0, 268.63, 506.7, 37.96, 30.32]}, {"formula_id": "formula_64", "formula_text": "L(W LS ( \u010e H), \u010e H) = L LS (W LS ( \u010e H), \u010e H) = 1 2 C\u22121 j=1 1 \u03c9 2 j + C = L {\u03c9 j } C\u22121 j=1 ,", "formula_coordinates": [25.0, 156.81, 616.1, 298.39, 30.32]}, {"formula_id": "formula_65", "formula_text": "L NC1 ( \u010e H) = 1 2 C\u22121 j=1 \u03c9 2 j (C + \u03c9 2 j ) 2 = L NC1 {\u03c9 j } C\u22121 j=1 L NC2/3 ( \u010e H) = 1 2 C\u22121 j=1 1 C \u03c9 2 j \u03c9 2 j + C \u2212 1 2 = L NC2/3 {\u03c9 j } C\u22121 j=1 .", "formula_coordinates": [25.0, 178.19, 665.96, 255.61, 67.88]}, {"formula_id": "formula_66", "formula_text": "L(W LS , \u010e M ) = 1 2C C \u22121 \u010e M \u03a3 \u22121 T \u010e M \u2212 \u03a6 2 F + 1 2 tr C \u22121 \u010e M \u03a3 \u22121 T \u03a3 W \u03a3 \u22121 T \u010e M C \u22121 . (19) Observe \u010e M \u03a3 \u22121 T \u010e", "formula_coordinates": [26.0, 108.0, 100.28, 396.0, 41.19]}, {"formula_id": "formula_67", "formula_text": "\u03a3 T = \u03a3 W + 1 C \u010e M \u010e M T .", "formula_coordinates": [26.0, 255.18, 156.59, 101.65, 22.31]}, {"formula_id": "formula_68", "formula_text": "L( \u2126) = 1 2C \u2126 \u2126 \u2126 + CI C\u22121 \u22121 \u2126 \u2212 I C\u22121 2 F + 1 2 tr \u2126 ( \u2126 \u2126 + CI C\u22121 ) \u22122 \u2126 .", "formula_coordinates": [26.0, 120.3, 235.05, 371.4, 28.06]}, {"formula_id": "formula_69", "formula_text": "L({\u03c9 j } C\u22121 c=1 }) = 1 2 C\u22121 j=1 1 C \u03c9 2 j \u03c9 2 j + C \u2212 1 2 + \u03c9 2 j (C + \u03c9 2 j ) 2 = 1 2 C\u22121 j=1 1 \u03c9 2 j + C . (20", "formula_coordinates": [26.0, 188.5, 283.76, 311.36, 69.15]}, {"formula_id": "formula_70", "formula_text": ")", "formula_coordinates": [26.0, 499.85, 333.32, 4.15, 8.64]}, {"formula_id": "formula_71", "formula_text": "\u2212 1 2 W \u010e M V", "formula_coordinates": [26.0, 254.76, 389.12, 31.77, 16.18]}, {"formula_id": "formula_72", "formula_text": "X = U (\u03a3 W ) \u2212 1 2 \u010e H(V \u2297 I N ) = U \u010e HC \u010e H \u2212 1 2 \u010e H(V \u2297 I N ) \u2208 R C\u00d7N C ,", "formula_coordinates": [26.0, 143.81, 473.79, 324.38, 15.41]}, {"formula_id": "formula_73", "formula_text": "C = 1 CN I CN \u2212 1 N Y Y .(21)", "formula_coordinates": [26.0, 241.02, 512.61, 262.98, 22.31]}, {"formula_id": "formula_74", "formula_text": "H \u2192 \u03a3 \u2212 1 2 W \u010e", "formula_coordinates": [26.0, 295.04, 548.1, 57.83, 16.18]}, {"formula_id": "formula_75", "formula_text": "\u2126(X) = 1 N XY .", "formula_coordinates": [26.0, 266.02, 714.46, 79.97, 22.31]}, {"formula_id": "formula_76", "formula_text": "d\u2126 (Z) = 1 N ZY , \u2200Z \u2208 T X X .", "formula_coordinates": [27.0, 232.61, 123.74, 146.78, 22.31]}, {"formula_id": "formula_77", "formula_text": "X = {X \u2208 R C\u00d7CN | XCX = I C }", "formula_coordinates": [27.0, 225.31, 195.76, 161.37, 11.72]}, {"formula_id": "formula_78", "formula_text": "T X X = {Z \u2208 R C\u00d7CN | XCZ + ZCX = 0}", "formula_coordinates": [27.0, 205.75, 271.39, 200.5, 11.72]}, {"formula_id": "formula_79", "formula_text": "\u03a0 T X X (Z) = Z \u2212 1 2 (XCZ + ZCX )X D.3 CONTINUALLY RENORMALIZED GRADIENT FLOW", "formula_coordinates": [27.0, 108.25, 304.39, 287.4, 43.97]}, {"formula_id": "formula_80", "formula_text": "X 1 = \u03a3( \u010e H 1 ) \u2212 1 2 \u010e H 1 .", "formula_coordinates": [27.0, 108.0, 391.63, 86.67, 12.53]}, {"formula_id": "formula_81", "formula_text": "X 1 = \u03a3 \u2212 1 2 W ( \u010e H 1 ) \u010e H 1 = \u03a3 \u2212 1 2 W (X 0 + \u2206X 0 ) \u2022 (X 0 + \u2206X 0 ),", "formula_coordinates": [27.0, 185.83, 434.65, 240.34, 16.18]}, {"formula_id": "formula_82", "formula_text": "X 1 =X 0 + \u03a0 T X X (\u2206X 0 ) + O \u2206X 0 2 . Proof. Notice that X 1 =\u03a3 \u2212 1 2 W ( \u010e H 1 ) \u010e H 1 = (X 0 + \u2206X 0 )C(X 0 + \u2206X 0 ) \u2212 1 2 (X 0 + \u2206X 0 ) = I + \u2206X 0 CX 0 + X 0 C\u2206X 0 + \u2206X 0 C\u2206X T 0 \u2212 1 2 (X 0 + \u2206X 0 ),", "formula_coordinates": [27.0, 108.0, 481.98, 349.19, 100.12]}, {"formula_id": "formula_83", "formula_text": "(I + A) \u2212 1 2 = I \u2212 1 2 A + O( A 2 ).", "formula_coordinates": [27.0, 232.01, 607.53, 147.98, 22.31]}, {"formula_id": "formula_84", "formula_text": "X 1 = I \u2212 1 2 \u2206X 0 CX 0 + X 0 C\u2206X 0 + \u2206X 0 C\u2206X T 0 + O \u2206X 0 2 (X 0 + \u2206X 0 ) =X 0 + \u2206X 0 \u2212 1 2 \u2206X 0 CX 0 \u2212 X 0 C\u2206X 0 X 0 + O \u2206X 0 2 =X 0 + \u03a0 T X X (\u2206X 0 ) + O \u2206X 0 2 ,", "formula_coordinates": [27.0, 115.47, 647.94, 381.06, 65.91]}, {"formula_id": "formula_85", "formula_text": "d dt X = \u2212\u03a0 T X X (\u2207 X L LS (X)) .", "formula_coordinates": [28.0, 240.63, 151.44, 131.93, 22.31]}, {"formula_id": "formula_86", "formula_text": "x j = X e j(22)", "formula_coordinates": [28.0, 280.65, 239.11, 223.35, 9.68]}, {"formula_id": "formula_87", "formula_text": "d\u03c9 j (\u03a0 T X X (dX)) = 1 N y j \u2212 \u03c9 j x j C dx j , \u2200j = 1, . . . , C.", "formula_coordinates": [28.0, 173.19, 353.19, 265.63, 22.31]}, {"formula_id": "formula_88", "formula_text": "d\u03c9 j (\u03a0 T X X (dX)) = e j d\u2126 (\u03a0 T X X (dX)) e j ,(23)", "formula_coordinates": [28.0, 213.38, 462.56, 290.62, 10.65]}, {"formula_id": "formula_89", "formula_text": "d\u2126 (\u03a0 T X X (dX)) = 1 N \u03a0 T X X (dX) Y .(24)", "formula_coordinates": [28.0, 224.12, 500.14, 279.88, 22.31]}, {"formula_id": "formula_90", "formula_text": "\u03a0 T X X (dX) = dX \u2212 1 2 (dX CX + XC dX )X.(25)", "formula_coordinates": [28.0, 197.83, 546.39, 306.17, 22.31]}, {"formula_id": "formula_91", "formula_text": "d\u03c9 j (\u03a0 T X X (dX)) =e j d\u2126 (\u03a0 T X X (dX)) e j (26) = 1 N e j \u03a0 T X X (dX) Y e j = 1 N e j dX \u2212 1 2 dX CX + XC dX X Y e j .", "formula_coordinates": [28.0, 149.1, 595.15, 354.9, 61.83]}, {"formula_id": "formula_92", "formula_text": "d\u03c9 j (\u03a0 T X X (dX)) = 1 N y j \u2212 \u03c9 j x j C dx j ,", "formula_coordinates": [29.0, 209.33, 195.56, 193.35, 22.31]}, {"formula_id": "formula_93", "formula_text": "d dt x j = \u03c9 j (C + \u03c9 2 j ) 2 1 N y j \u2212 \u03c9 j Cx j , \u2200j = 1, . . . , C,", "formula_coordinates": [29.0, 188.2, 282.97, 236.79, 24.51]}, {"formula_id": "formula_94", "formula_text": "d dt x ji = \u2212e j \u03a0 T X X dL LS dX e i = \u2212 \u03a0 T X X dL LS dX , e j e i F = \u2212 dL LS dX , \u03a0 T X X e j e i F ,", "formula_coordinates": [29.0, 225.86, 390.7, 159.9, 81.15]}, {"formula_id": "formula_95", "formula_text": "dL LS (Z) = dL LS dX , Z F , \u2200Z \u2208 T X X .", "formula_coordinates": [29.0, 217.18, 530.01, 177.63, 24.86]}, {"formula_id": "formula_96", "formula_text": "d dt x ji = \u2212 dL LS \u03a0 T X X e j e i = \u2212 C\u22121 k=1 dL LS d\u03c9 k d\u03c9 k \u03a0 T X X e j e i = \u2212 dL LS d\u03c9 j d\u03c9 j \u03a0 T X X e j e i", "formula_coordinates": [29.0, 220.39, 584.46, 159.38, 82.88]}, {"formula_id": "formula_97", "formula_text": "d dt x j = \u2212 dL LS d\u03c9 j 1 N y j \u2212 \u03c9 j Cx j = \u03c9 j (C + \u03c9 2 j ) 2 1 N y j \u2212 \u03c9 j Cx j ,", "formula_coordinates": [30.0, 225.73, 101.57, 161.74, 52.62]}, {"formula_id": "formula_98", "formula_text": "d dt \u03c9 j = 1 N \u03c9 j (C + \u03c9 2 j ) 2 , \u2200j = 1, . . . , C \u2212 1.(27)", "formula_coordinates": [30.0, 214.65, 233.33, 289.35, 24.51]}, {"formula_id": "formula_100", "formula_text": "d dt \u03c9 j = d\u03c9 j dx j dx j dt = 1 N \u03c9 j (C + \u03c9 2 j ) 2 y j 1 N y j \u2212 \u03c9 j Cx j ,(29)", "formula_coordinates": [30.0, 186.97, 372.24, 317.03, 24.51]}, {"formula_id": "formula_101", "formula_text": "1 N 2 y j y j = 1 N .", "formula_coordinates": [30.0, 273.93, 424.48, 65.34, 22.31]}, {"formula_id": "formula_102", "formula_text": "x j Cy j = e j XCY e j = 1 CN e j X I \u2212 1 N Y Y Y e j = 0.", "formula_coordinates": [30.0, 167.48, 472.16, 277.04, 22.31]}, {"formula_id": "formula_103", "formula_text": "d dt \u03c9 j = 1 N \u03c9 j (C + \u03c9 2 j ) 2 .", "formula_coordinates": [30.0, 259.64, 521.7, 93.91, 24.51]}, {"formula_id": "formula_105", "formula_text": "4 \u221a t/c3", "formula_coordinates": [31.0, 302.47, 137.26, 22.34, 14.89]}, {"formula_id": "formula_106", "formula_text": "4 t c 3", "formula_coordinates": [31.0, 198.46, 246.68, 14.51, 10.94]}, {"formula_id": "formula_107", "formula_text": "d dt X = 1 N D 1 Y \u2212 D 2 XC", "formula_coordinates": [31.0, 249.89, 408.87, 112.73, 22.31]}, {"formula_id": "formula_108", "formula_text": "\u03c9j (C+\u03c9 2 j ) 2 C c=1and", "formula_coordinates": [31.0, 431.5, 433.66, 72.5, 22.57]}, {"formula_id": "formula_109", "formula_text": "\u03c9 2 j (C+\u03c9 2 j ) 2 C c=1", "formula_coordinates": [31.0, 115.84, 455.88, 53.08, 23.38]}, {"formula_id": "formula_110", "formula_text": "d dt \u2126 = d dt X Y = 1 N D 1 Y Y \u2212 D 2 XCY = D 1 ,", "formula_coordinates": [31.0, 186.26, 483.22, 240.67, 22.31]}, {"formula_id": "formula_111", "formula_text": "E E = \u03b1 C C \u2212 1 I \u2212 1 C \u2212 1 1 C 1 C", "formula_coordinates": [31.0, 226.81, 671.52, 150.04, 22.31]}, {"formula_id": "formula_112", "formula_text": "E E = \u03b1 C C \u2212 1 I \u2212 1 C \u2212 1 1 C 1 C ,", "formula_coordinates": [32.0, 224.6, 155.26, 162.81, 22.31]}, {"formula_id": "formula_113", "formula_text": "E E = V E S E U E U E S E V E = V E S 2 E V E . Therefore, V E S 2 E V E = \u03b1 C C \u2212 1 I \u2212 1 C \u2212 1 1 C 1 C .", "formula_coordinates": [32.0, 107.69, 200.57, 291.44, 51.14]}, {"formula_id": "formula_114", "formula_text": "E E1 C = \u03b1 C C \u2212 1 I \u2212 1 C \u2212 1 1 C 1 C 1 C = 0.", "formula_coordinates": [32.0, 202.3, 311.29, 207.4, 22.31]}, {"formula_id": "formula_115", "formula_text": "E E1 C = V E S E U E U E S E V E 1 C = V E S 2 E V E 1 C = s 2 V E V E 1 C . Combining the above, we get V E V E 1 C =0. Since V E V E is a C\u00d7C matrix of rank C\u22121, we deduce that the rank-1 nullspace of V E is spanned by the C-dimensional ones vector, i.e. V E 1 C =0. Consequently, E has zero-mean columns since E1 C =U E S E V E 1 C =0.", "formula_coordinates": [32.0, 108.0, 355.45, 397.74, 54.05]}, {"formula_id": "formula_116", "formula_text": "E E = V E S E U E U E S E V E = V E S 2 E V E = s 2 V E V E . Using our assumptions on V E , V E V E + 1 C 1 C 1 C = I,", "formula_coordinates": [32.0, 108.0, 534.9, 317.31, 48.11]}, {"formula_id": "formula_117", "formula_text": "E E = s 2 I \u2212 1 C 1 C 1 C = \u03b1 C C \u2212 1 I \u2212 1 C \u2212 1 1 C 1 C , where \u03b1 = s 2 C\u22121 C", "formula_coordinates": [32.0, 107.64, 604.22, 326.07, 43.14]}, {"formula_id": "formula_118", "formula_text": "lim t\u2192\u221e 1 \u03c9 max (t) SNR t = U 0 V 0 ,(10)", "formula_coordinates": [32.0, 243.97, 679.64, 260.03, 23.23]}, {"formula_id": "formula_119", "formula_text": "CrossEntropy(W H) s.t. W 2 \u2264 1, h i,c 2 \u2264 1,", "formula_coordinates": [34.0, 211.67, 533.87, 220.54, 9.68]}, {"formula_id": "formula_120", "formula_text": "min H L\u22121 ,W L\u22121 ,W L ,\u03b3,\u03b1 L W L (BN \u03b3,\u03b1 (W L\u22121 H L\u22121 )) + , Y + \u03bb 2 \u03b3 2 2 + \u03b1 2 2 + W L 2 F ,", "formula_coordinates": [35.0, 119.67, 354.79, 372.67, 22.31]}, {"formula_id": "formula_121", "formula_text": "min W ,H,b CrossEntropy (W H + b, Y ) + \u03bb W 2 W 2 F + \u03bb H 2 H 2 F + \u03bb b 2 b 2 F ,", "formula_coordinates": [36.0, 146.66, 180.47, 318.68, 22.31]}], "doi": ""}