{"title": "Dual-Mandate Patrols: Multi-Armed Bandits for Green Security", "authors": "Lily Xu; Elizabeth Bondi; Fei Fang; Andrew Perrault; Kai Wang; Milind Tambe", "pub_date": "", "abstract": "Conservation efforts in green security domains to protect wildlife and forests are constrained by the limited availability of defenders (i.e., patrollers), who must patrol vast areas to protect from attackers (e.g., poachers or illegal loggers). Defenders must choose how much time to spend in each region of the protected area, balancing exploration of infrequently visited regions and exploitation of known hotspots. We formulate the problem as a stochastic multi-armed bandit, where each action represents a patrol strategy, enabling us to guarantee the rate of convergence of the patrolling policy. However, a naive bandit approach would compromise short-term performance for long-term optimality, resulting in animals poached and forests destroyed. To speed up performance, we leverage smoothness in the reward function and decomposability of actions. We show a synergy between Lipschitzcontinuity and decomposition as each aids the convergence of the other. In doing so, we bridge the gap between combinatorial and Lipschitz bandits, presenting a no-regret approach that tightens existing guarantees while optimizing for short-term performance. We demonstrate that our algorithm, LIZARD, improves performance on real-world poaching data from Cambodia.", "sections": [{"heading": "Introduction", "text": "Green security efforts to protect wildlife, forests, and fisheries require defenders (patrollers) to conduct patrols across protected areas to guard against attacks (illegal activities) (Lober 1992). For example, to prevent poaching, rangers conduct patrols to remove snares laid out to trap animals (Fig. 1). Green security games have been proposed to apply Stackelberg security games, a game-theoretic model of the repeated interaction between a defender and an attacker, to domains such as the prevention of illegal logging, poaching, or overfishing (Fang, Stone, and Tambe 2015). A growing body of work develops scalable algorithms for Stackelberg games (Basilico, Gatti, and Amigoni 2009;Blum, Haghtalab, and Procaccia 2014). Subsequent work has focused more on applicability to the real-world, employing machine learning to predict attack hotspots then using game-theoretic planning to design patrols (Nguyen et al. 2016;Gholami et al. 2018).\nCopyright \u00a9 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. However, many protected areas lack adequate and unbiased past patrol data, disabling us from learning a reasonable adversary model in the first place (Moreto and Lemieux 2015). As one of many examples, Bajo Madidi in the Bolivian Amazon was newly designated as a national park in 2019 (Berton 2020). The park is plagued with illegal logging, and patrollers do not have historical data from which to make predictions. The defenders do not want to spend patrol effort solely on information gathering; they must simultaneously maximize detection of attacks. As green security efforts get deployed on an ever-larger scale in hundreds of protected areas around the world (Xu et al. 2020), addressing this information-gathering challenge becomes crucial.\nMotivated by these practical needs, we focus on conducting dual-mandate patrols 1 , with the goal of simultaneously detecting illegal activities and collecting valuable data to improve our predictive model, achieving higher longterm reward. The key challenge with dual-mandate patrols is the exploration-exploitation tradeoff: whether to follow the best patrol strategy indicated by historical data or conduct new patrols to get a better understanding of the attackers. Some recent work proposes using multi-armed bandits to formulate the problem (Xu, Tran-Thanh, and Jennings 2016;Gholami et al. 2019). Despite their advances, we show that these approaches require unrealistically long time horizons to achieve good performance. In the real world, these initial losses are less tolerable and can lead to stakeholders aban-doning such patrol-assistance systems. As we are designing this system for future deployment, it is critical to account for these practical constraints.\nIn this paper, we address real-world characteristics of green security domains to design dual-mandate patrols, prioritizing strong performance in the short term as well as long term. Concretely, we introduce LIZARD, a bandit algorithm that accounts for (i) decomposability of the reward function, (ii) smoothness of the decomposed reward function across features, (iii) monotonicity of rewards as patrollers exert more effort, and (iv) availability of historical data. LIZARD leverages both decomposability and Lipschitz continuity simultaneously, bridging the gap between combinatorial and Lipschitz bandits. We prove that LIZARD achieves no-regret when adaptively discretizing the metric space, generalizing results from both Chen et al. (2016) and Kleinberg, Slivkins, and Upfal (2019). Specifically, we improve upon the regret bound of Lipschitz bandits for non-trivial cases with more than one dimension and extend combinatorial bandits to continuous spaces. Additionally, we show that LIZARD dramatically outperforms existing algorithms on real poaching data from Cambodia.", "publication_ref": ["b20", "b9", "b1", "b3", "b24", "b10", "b23", "b2", "b29", "b28", "b11", "b6", "b17"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Background", "text": "Green security domains have been extensively modeled as green security games (Haskell et al. 2014;Fang et al. 2016;Mc Carthy et al. 2016;Kamra et al. 2018). In these games, resource-constrained defenders protect large areas (e.g., forests, savannas, wetlands) from adversaries who repeatedly attack them. Most work has focused on learning attacker behavior models from historical data (Nguyen et al. 2016;Gholami et al. 2018) and using these models to plan patrols with limited lookahead to prevent future attacks (Fang, Stone, and Tambe 2015;Xu et al. 2017).\nDespite successful deployment in some conservation areas (Xu et al. 2020), researchers have recognized that it is not always possible to have abundant historical data when first deploying a green security algorithm. Fig. 2 demonstrates the shortcomings of a pure exploitation approach. Using a simulator built from real-world poaching data, we observe a large shortfall in reward unless an unrealistically large amount of historical data is collected. Naively exploiting historical data compromises reward unless a very large amount of data is collected. The gap between the orange and dashed green lines reveals the opportunity cost (regret) of acting solely based off historical data.\nSome recent work has begun to consider the need to also explore. Xu, Tran-Thanh, and Jennings (2016) follow an online learning paradigm, modeling the defender actions as pulling a set of arms (targets to patrol) at each timestep against an attacker who adversarially sets the rewards at each target. The model is then solved using FPL-UE, a variant of Follow the Perturbed Leader (FPL) algorithm (Kalai and Vempala 2005). Gholami et al. (2019) propose a hybrid model, MINION, that employs the FPL algorithm to choose between two meta arms representing FPL-UE and a supervised learning model based on historical data. However, both papers ignore domain structure such as feature similarity be-0 500 1,000 1,500 2,000 0.6 0.8 1 Num. timesteps historical data Avg. reward true top 10 targets empirical top 10 targets Figure 2: Naively protecting 10 out of 100 potential poaching targets based on our predictions would require 6 years of data to accurately protect the most important 10 targets. tween targets and do not incorporate existing historical data into the online learners.\nThere is a plethora of work on multi-armed bandits (MABs) (Bubeck and Cesa-Bianchi 2012). For brevity, we focus on confidence bound-based algorithms; other common methods include epsilon-greedy and Thompson sampling (Lattimore and Szepesv\u00e1ri 2018). For stochastic MAB with finite arms, the seminal UCB1 algorithm (Auer, Cesa-Bianchi, and Fischer 2002) always chooses the arm with the highest upper confidence bound and achieves no-regret, i.e., the expected reward is sublinearly close to always pulling the best arm in hindsight. For continuous arms, we must impose assumptions on the payoff function for tractability. Lipschitz continuity bounds the rate of change of a function \u00b5, requiring that |\u00b5(x) \u2212 \u00b5(y)| \u2264 L \u2022 D(x, y) for two points x, y and distance function D. We refer to L as the Lipschitz constant. Kleinberg, Slivkins, and Upfal (2019) propose the zooming algorithm for Lipschitz MAB, which extends UCB1 with an adaptive refinement step to place more arms in high-reward areas of the continuous space. A parallel line of work applies Lipschitz-continuity assumptions to bandits to generate tighter confidence bounds (Bubeck et al. 2011;Magureanu, Combes, and Proutiere 2014). Chen et al. (2016) extend UCB1 to the combinatorial case with the CUCB algorithm, which tracks individual payoffs separately and uses an oracle to select the best combination. However, CUCB does not extend to continuous arms. Our proposed algorithm, LIZARD, outperforms both the zooming and CUCB algorithms by explicitly handling arms that are both Lipschitz-continuous and combinatorial.\nWildlife decline due to poaching demands fast action, so we cannot afford the infinite-time horizon usually considered by bandit algorithms. The case of short-horizon MABs is not well-studied. Some papers of this type (Gray, Zhu, and Onta\u00f1\u00f3n 2020) focus on adjusting exploration patterns rather than not address the types of challenges induced by smoothness and combinatorial rewards, as are present in the domain we consider.", "publication_ref": ["b13", "b8", "b22", "b15", "b24", "b10", "b9", "b27", "b29", "b28", "b14", "b11", "b4", "b19", "b0", "b17", "b5", "b21", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Problem Description", "text": "The protected area for which we must plan patrols is discretized into N targets, each associated with a feature vector \u20d7 y i \u2208 R K which is static across the time horizon T . In the poaching prevention domain, for example, the K features include geospatial characteristics such as distance to river, forest cover, animal density, and slope. We consider each timestep to be a short period of patrolling, e.g., a day, and a time horizon that reflects our foreseeable horizon for patrol planning, e.g., 1.5 years, corresponding to 500 timesteps.\nIn each round, the defender determines an effort vector \u20d7 \u03b2 = (\u03b2 1 , . . . , \u03b2 N ) which specifies the amount of effort to spend on each target. The defender has a budget B for total effort, i.e., i \u03b2 i \u2264 B. In practice, \u03b2 i may represent the number of hours spent on foot patrolling in target i. If the defender has two teams of patrollers, each of which can patrol for 5 hours a day, then B = 10. The planned patrols have to be conveyed clearly to the human patrollers on the ground to then be executed (Plumptre et al. 2014). Thus, an arbitrary value of \u03b2 i may be impractical. For example, we may ask the patrollers to patrol in an area for 30 minutes, but not 21.3634 minutes. Therefore, we enforce discretized patrol effort levels, requiring \u03b2 i \u2208 \u03a8 = {\u03c8 1 , . . . , \u03c8 J } for J levels of effort. Some targets may be attacked. The reward of a patrol corresponds to the total number of targets where attacks were detected (Critchlow et al. 2015). Let the expected reward of a patrol vector \u20d7 \u03b2 be \u00b5( \u20d7 \u03b2). Our objective is to specify an effort vector \u20d7 \u03b2 (t) for each timestep t in an online fashion to minimize regret with respect to the optimal effort vector \u20d7 \u03b2 \u22c6 against a stochastic adversary, where regret is defined as t) ). In practice, the likelihood of a defender detecting an attack is dependent on the amount of patrol effort exerted. In previous work on poaching prevention, a target can represent a large region, e.g., 1 \u00d7 1 km area, where snares are well-hidden. Thus, spending more time means the human patrollers can check the whole region more thoroughly and are more likely to detect snares. We represent the defender's expected reward at target i as a function \u00b5 i (\u03b2 i ) \u2208 [0, 1]. We define random variables X (t) i as the observed reward (attack or no attack) from target i at time t. Then X (t) i follows a Bernoulli distribution with mean \u00b5 i (\u03b2\nT \u00b5( \u20d7 \u03b2 \u22c6 ) \u2212 T t=1 \u00b5( \u20d7 \u03b2(\n(t) i ) with effort \u03b2 (t) i .", "publication_ref": ["b25", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Domain Characteristics", "text": "We leverage the following four characteristics pervasive throughout green security domains to direct our approach.\nDecomposability The overall expected reward for the defender is decomposable across targets and additive. For executing a patrol with effort \u20d7 \u03b2 across all targets, the expected composite reward is a function \u00b5( \u20d7\n\u03b2) = N i=1 \u00b5 i (\u03b2 i ).\nLipschitz-continuity As discussed, the expected reward to the defender at target i is given by the function \u00b5 i (\u03b2 i ), which is dependent on effort \u03b2 i . Furthermore, the expected reward depends on the features \u20d7 y i of that target, that is,\n\u00b5 i (\u03b2 i ) = \u00b5(\u20d7 y i , \u03b2 i ) for all i.\nPast work to predict poaching patterns using machine learning models, which rely on assumptions of Lipschitz continuity, have been shown to perform well in real-world field experiments (Xu et al. 2020). Thus, we assume that the reward function \u00b5(\u2022, \u2022) is Lipschitz-continuous in feature space as well as across effort levels. That is, two distinct targets in the protected area with identical features will have identical reward functions, and two targets a and b with features \u20d7 y a , \u20d7 y b and effort \u03b2 a , \u03b2 b have rewards that differ by no more than\n| \u00b5(\u20d7 y a , \u03b2 a ) \u2212 \u00b5(\u20d7 y b , \u03b2 b )| \u2264 L \u2022 D((\u20d7 y a , \u03b2 a ), (\u20d7 y b , \u03b2 b )) (1)\nfor some Lipschitz constant L and distance function D, such as Euclidean distance. Hence, the composite reward \u00b5( \u20d7 \u03b2) is also Lipschitz-continuous.\nFor simplicity of notation, we assume that the same Lipschitz constant applies over the entire space. In practice, we could achieve tighter bounds by separately estimating the Lipschitz constant for each dimension.\nMonotonicity The more effort spent on a target, the higher the expected reward as our likelihood of finding a snare increases. That is, we assume \u00b5(\u03b2 i ) is monotonically nondecreasing in \u03b2 i . Additionally, we assume that zero effort corresponds with zero reward (\u00b5 i (0) = 0), as defenders cannot prevent attacks on targets they do not visit.\nHistorical data Finally, many conservation areas have data from past patrols, which we use to warm start the online learning algorithm. To improve our short-term performance, we are careful in how we integrate historical data, as described in Sec. 4.3.", "publication_ref": ["b29"], "figure_ref": [], "table_ref": []}, {"heading": "Domain Challenges", "text": "There are additional challenges in green security domains that prevent us from directly applying existing algorithms. No-regret guarantees describe the performance at infinite time horizons, but, practically, we require strong performance within short time frames. Losses in initial rounds are less tolerable, as the consequences of ineffective patrols are deforestation and loss of wildlife. In addition, conservation managers may lose faith in using an online learning approach. In response, we provide an algorithm with infinite horizon guarantees and also empirically show strong performance in the short term.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "LIZARD Online Learning Algorithm", "text": "We present our algorithm, LIZARD (LIpschitZ Arms with Reward Decomposability), for online learning in green security domains.\nStandard bandit algorithms suffer from the curse of dimensionality: the set of arms would be \u03a8 N , which has size J N . Thus, we cast the problem as a combinatorial bandit (Chen et al. 2016). At each iteration, we choose a patrol strategy \u20d7 \u03b2 that satisfies the budget constraint and observe the patrol outcome of each target i under the chosen effort \u03b2 i . An arm is one effort level \u03b2 i on a specific target i; a super arm is \u20d7 \u03b2, a collection of N arms. By tracking decomposed rewards, we only need to track observations from N J arms.\nWe now maintain exponentially fewer samples, but the number of arms is still prohibitively large. With, say, N = Algorithm 1: LIZARD 1 Inputs: Number of targets N , time horizon T , budget B, discretization levels \u03a8, target features \u20d7 y i 2 n(i, \u03c8 j ) = 0, reward(i, \u03c8 j ) = 0 \u2200i \u2208 [N ], j \u2208 [J] 3 for t = 1, 2, . . . , T do 4 Compute UCB t using Eq. 4 5Solve P(UCB t , B, N, T ) to select super arm \u20d7 \u03b2\n6\nObserve rewards\nX (t) 1 , X (t) 2 , . . . , X (t) n 7 for i = 1, 2, . . . , N do 8 reward(i, \u03b2 i ) = reward(i, \u03b2 i ) + X (t) i 9 n(i, \u03b2 i ) = n(i, \u03b2 i ) + 1 reward \u00b5i(\u03b2i) 0 0.2 0.4 0 0.5 1 A B (a) effort \u03b2i 0 0.2 0.4 C A B (b) 0 0.2 0.4 C A B (c)\nFigure 3: The Lipschitz assumption enables us to prune confidence bounds. We show the impact of each SELFUCBs on the UCBs of other arms in effort space of target i. The solid brackets represent the SELFUCBs. The dashed lines represent the bounds imposed by each arm on the rest of the space. The shaded green region covers the potential value of the reward function at different levels of effort. We visualize the additive effect of (a) Lipschitz-continuity, (b) zero effort yields zero reward, and (c) monotonicity. Note that these plots demonstrate UCBs for one target and that Lipschitz continuity also applies across targets based on feature similarity.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Upper Confidence Bounds with Similarity", "text": "We take an upper confidence bound (UCB) approach where the rewards are tracked separately for different arms. We show that we can incorporate Lipschitz-continuity of the reward functions into the UCB of each arm to achieve tighter confidence bounds. Let\u03bc t (i, j) = reward t (i, \u03c8 j )/n t (i, \u03c8 j ) be the average reward of target i at effort \u03c8 j given cumulative empirical reward reward t (i, \u03c8 j ) over n t (i, \u03c8 j ) arm pulls by timestep t.\nLet r t (i, j) be the confidence radius at time t defined as rt(i, j) = 3 log(t) 2nt (i, \u03c8j) .\n(2)\nWe distinguish between UCB and a term we call SELFUCB. The SELFUCB of an arm (i, j) representing target i with effort level j at time t is the UCB of an arm based only on its own observations, given by SELFUCB t (i, j) =\u03bc t (i, j) + r t (i, j) .\n(\n)3\nThis definition of SELFUCB corresponds with the standard interpretation of confidence bounds from UCB1 (Auer, Cesa-Bianchi, and Fischer 2002). The UCB of an arm is then computed by taking the minimum of the bounds of all SELFUCBs as applied to the arm. These bounds are determined by adding the distance between arm (i, j) and all other arms (u, v) to the SELFUCB:\nUCB t (i, j) = min u\u2208[N ] v\u2208[J] {SELFUCB t (u, v) + L \u2022 dist} (4) dist = max{0, \u03c8 v \u2212 \u03c8 j } + D(\u20d7 y i , \u20d7 y u )\nwhich exploits Lipschitz continuity between the arms. See Fig. 3 for a visualization. The distance between two arms depends on the similarity of their features and effort. The first term of dist considers similarity of effort level (Fig. 3a); the second considers feature similarity between targets according to distance function D. We define UCB t (i, 0) = 0 for all i \u2208 [N ] due to the assumption that zero effort yields zero reward (Fig. 3b). To address the monotonically nondecreasing reward across effort space, we constrain the first term of dist to be nonnegative (Fig. 3c).", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Super Arm Selection", "text": "With the computed UCBs, the selection of super arms (patrol strategies) can be framed as a knapsack optimization problem. We aim to maximize the value of our sack (sum of the UCBs) subject to a budget constraint (total effort). To solve this problem, we use the following integer linear program P.\nmax z i\u2208[N ] j\u2208[J] z i,j \u2022 UCB t (i, j) (P) s.t. z i,j \u2208 {0, 1} \u2200i \u2208 [N ], j \u2208 [J] j\u2208[J] z i,j = 1 \u2200i \u2208 [N ] i\u2208[N ] j\u2208[J] z i,j \u03c8 j \u2264 B\nThere is one auxiliary variable z i,j , constrained to be binary, for each level of patrol effort for each target. Setting z i,j = 1 means we exert \u03c8 j effort on target i. The second constraint sets \u03b2 i by requiring that we pull one arm per target (which may be the zero effort arm \u03b2 i = 0). The third constraint mandates that we stay within budget. This integer program has N J variables and N + 1 constraints. Pseudocode for the LIZARD algorithm is given in Algorithm 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Incorporating Historical Data", "text": "We incorporate historical data to further improve bounds without compromising the regret guarantee (see Sec. 5.3). Shivaswamy and Joachims (2012) show that, in the infinite horizon case, a logarithmic amount of historical data can reduce regret from logarithmic to constant. However, care must be taken to consider short-term effects. Standard methods that include historical arm pulls as if they occurred online can result in poor short-term performance, as in the following example: Example 1. Let A and B be two arms with noise-free rewards \u00b5(A) = 1 and \u00b5(B) = 0. Suppose that in the historical data, A is pulled H \u226b 1 times and B has never been pulled. If those historical arm pulls are counted in the calculation of confidence radii, then we must pull the bad arm B first s \u2265 O(H/ log H) times until the UCB of arm B is smaller than the UCB of arm A:\n3 log s 2s = 0 + r s (B) \u2264 1 + r s (A) = 1 + 3 log s 2H yielding linear regret O(s) = O H log H in the first s rounds.\nIntuitively, the more times the optimal arm is pulled in the historical data, the greater the short-term regret. Because we expect the historical data to oversample good arms (as patrollers prefer visiting areas that are frequently attacked), we design LIZARD to avoid this problem by ignoring the number of historical arm pulls when computing the confidence radii. Specifically, LIZARD initializes the observed rewards reward t (\u2022, \u2022) and the number of pulls n t (\u2022, \u2022) with the historical observations, but, when computing the confidence radii r t (i, j), LIZARD only considers the number of pulls from online play.", "publication_ref": ["b26"], "figure_ref": [], "table_ref": []}, {"heading": "Regret Analysis", "text": "We provide a regret bound for Algorithm 1 with fixed discretization (Sec. 5.1), which is useful in practice but cannot achieve theoretical no-regret due to the discretization factor. Thus, we then offer Algorithm 2 with adaptive discretization to achieve no-regret (Sec. 5.2), showing that there is no barrier to achieving no regret in practice other than the need for fixed discretization in operationalizing our algorithm in the field. Our regret bound improves upon that of the zooming algorithm of Kleinberg, Slivkins, and Upfal (2019) for all dimensions d > 1. In our problem formulation, each dimension of the continuous action space represents a target, so d = N . In fact, the regret bound for the zooming algorithm is a provable lower bound; we are able to improve this lower bound through decomposition (Sec. 5.3). Furthermore, we extend the line of research on combinatorial bandits, generalizing the CUCB algorithm from Chen et al. (2016) to continuous spaces.\nFull proofs are included in the appendix.", "publication_ref": ["b17", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Fixed Discretization", "text": "Theorem 2. Given the minimum discretization gap \u2206, number of arms N , Lipschitz constant L, and time horizon T , the regret bound of Algorithm 1 with SELFUCB is\nReg \u2206 (T ) \u2264 O N L\u2206T + N 3 \u2206 \u22121 T log T + N 2 L\u2206 \u22121 . (5)\nProof sketch. The regret in Theorem 2 comes from (i) discretization regret in the first term of Eq. 5 and (ii) suboptimal arm selections in the last two terms. The discretization regret is due to inaccurate approximation caused by discretization, where the error can be bounded by rounding the optimal arm selection and fractional effort levels to their closest discretized levels. The suboptimal arm selections are due to insufficient samples across all discretized subarms and have sublinear regret in terms of T .\nUnder a finite horizon, it is sufficient to pick a discretization level based on the time horizon. For example, given a finite horizon T , we can pick a discretization that is optimal relative to T . We want to use a finer discretization to minimize the discretization regret, while at the same time minimizing the number of effort levels to explore. Thus, we trade off between the selection regret and discretization regret: we want them to be of the same order as the regret bound in Theorem 2, i.e., O( N\n3 \u2206 \u22121 T log T ) = O(N L\u2206T ), or equivalently \u2206 = (log T /T ) 1 3 N 1 3 L \u2212 2 3\n. This result matches the intuition that we should use a finer discretization when either (i) the total timesteps is larger, (ii) the number of targets is smaller, or (iii) the Lipschitz constant is larger (i.e., function values change more drastically).\nWith Theorem 2 we observe that the barrier to achieving no-regret is the discretization error which is linear in all terms, which brings us to adaptive discretization.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Adaptive Discretization", "text": "To achieve no-regret with an infinite time horizon, we need adaptive discretization. Adaptive discretization is less practical on the ground, but would be useful in other bandit settings where we could more precisely spend our budget such as influence maximization and facility location. As shown in Algorithm 2, we begin with a coarse patrol strategy, beginning with binary decisions on whether or not to visit each target, then gradually progress to a finer discretization.\nTheorem 3. Given the number of arms N , Lipschitz constant L, and time horizon T , the regret bound of Algorithm 2 with SELFUCB is be given by\nReg(T ) \u2264 O L 4 3 N T 2 3 (log T ) 1 3 .(6)\nProof sketch. The regret bound in Theorem 2 is not sublinear due to the additional discretization error. An intuitive way to alleviate this error is to adaptively reduce the discretization gap. We run each discretization gap \u2206 for T \u2206 = N L 2 \u2206 3 log N L 2 \u2206 3 time steps to make the discretization error and the selection error of the same order. We then start over with a finer discretization \u2206/2 to make the discretization error smaller. After summing the regret from all different phrases of discretization, we achieve sublinear regret as shown in Eq. 6. \n\u03a8 = {0, 1} , gap \u2206 = 1 3 T k = N 2 3k L 2 log N 2 3k L 2 \u2200k \u2208 N \u222a {0} 4 n(i, \u03c8 j ) = 0, reward(i, \u03c8 j ) = 0 \u2200i \u2208 [N ], j \u2208 [J] 5 for t = 1, 2, . . . , T do 6 if t > k\u22121 j=0 T j then 7 Set \u2206 = 2 \u2212k and \u03a8 = {0, \u2206, ..., 1}8\nCompute UCB t using Eq. 4 9Solve P(UCB t , B, N, T ) to select super arm \u20d7 \u03b2 10\nObserve rewards\nX (t) 1 , X (t) 2 , . . . , X (t) n 11 for i = 1, 2, . . . , N do 12 reward(i, \u03b2 i ) = reward(i, \u03b2 i ) + X (t) i 13 n(i, \u03b2 i ) = n(i, \u03b2 i ) + 1\nUnder reasonable problem settings, T dominates all other variables, so the regret in Theorem 3 is effectively of order O(T 2 3 (log T ) ), which approaches\u00d5(T ) as d approaches infinity (Kleinberg, Slivkins, and Upfal 2008). More generally, our regret bound improves upon that of the zooming algorithm for any d > 1.\nTheorem 3 signifies that LIZARD can successfully decouple the N -dimensional metric space into individual sub-dimensions while maintaining the smaller regret order, showcasing the power of decomposibility.", "publication_ref": ["b16"], "figure_ref": [], "table_ref": []}, {"heading": "Tightening Confidence Bounds", "text": "We have so far offered regret bounds to account for decomposability and Lipschitz-continuity across effort space. We now guarantee that the regret bounds continue to hold with Lipschitz-continuity in feature space, monotonicity, and historical information. We first look at how prior knowledge affects the regret bound in the combinatorial bandit setting: Theorem 4. Consider a combinatorial multi-arm bandit problem. If the bounded smoothness function given is f (x) = \u03b3x \u03c9 for some \u03b3 > 0, \u03c9 \u2208 (0, 1] and the Lipschitz upper confidence bound is applied to all m base arms, the cumulative regret at time T is bounded by\nReg(T ) \u2264 2\u03b3 2\u2212\u03c9 (6m log T ) \u03c9 2 \u2022 T 1\u2212 \u03c9 2 + \u03c0 2 3 + 1 mR max\nwhere R max is the maximum regret achievable.\nTheorem 4 matches the regret of the CUCB algorithm (Chen et al. 2016), generalizing combinatorial bandits to continuous spaces. Theorem 4 also allows us to generalize Theorems 2 and 3 to our setting with a tighter UCB from Lipschitz-continuity, which yields Theorem 5:  Theorem 5. Given the minimum discretization gap \u2206, number of targets N , Lipschitz constant L, and time horizon T , the regret bound of Algorithm 1 with UCB is\nReg \u2206 (T ) \u2264 O N L\u2206T + N 3 \u2206 \u22121 T log T + N 2 L\u2206 \u22121 (7)\nand the regret bound of Algorithm 2 with UCB is\nReg(T ) \u2264 O L 4 3 N T 2 3 (log T ) 1 3 . (8\n)\nFinally, when historical data is used, we can treat all the historical arm pulls as previous arm pulls with regret bounded by the maximum regret R max . 2 This yields a regret bound with a time-independent constant and is thus still sublinear, achieving no-regret. Taken together, these capture all the properties of the LIZARD algorithm, so we can state: Corollary 6. The regret bounds of Algorithm 1 and Algorithm 2 still hold with the inclusion of decomposability, Lipschitz-continuity, monotonicity, and historical data.\nTheorem 5 highlights the interplay between Lipschitzcontinuity and decomposition. The zooming algorithm achieves the provable lower bound on regret\u00d5(T d+1 d+2 ) (Kleinberg 2004). The regret that we achieve improves upon that lower bound for all d > 1, which is only possible due to the addition of decomposition.", "publication_ref": ["b6", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We conduct experiments using both synthetic data and real poaching data. Our results validate that the addition of decomposition and Lipschitz-continuity not only improves our theoretical guarantees but also leads to stronger empirical performance. We show that LIZARD (Algorithm 1) learns effectively within practical time horizons.\nWe consider a patrol planning problem with N = 25 or 100 targets (each a 1 sq. km grid cell), representing the region reachable from a single patrol post, and time horizon T = 500 representing a year and a half of patrols. The budget is the number of teams, e.g., B = 5 corresponds to 5 teams of rangers. We use 50 timesteps of historical data, approximately two months of patrol, as we focus on achieving strong performance in parks with limited historical patrols.  Real-world data We leverage patrol data from Srepok Wildlife Sanctuary in Cambodia to learn the reward function, as dependent on features and effort (Xu et al. 2020). We train a machine learning classifier to predict whether rangers will detect poaching activity at each target for a given effort level. We then generate piecewise-linear approximations of the reward functions, shown in Fig. 4, which naturally intersects the origin and is nondecreasing with increased effort.\nSynthetic data To produce synthetic data, we generate piecewise-linear functions that mimic the behavior of realworld reward functions. We define feature similarity as the maximum difference between the reward functions across all effort levels. We generate historical data with a biased distribution over the targets, corresponding to the realistic scenario where rangers spend more effort on targets that are easily accessible, such as those closest to a patrol post.\nAlgorithms We compare to three baselines: CUCB (Chen et al. 2016), zooming (Kleinberg, Slivkins, and Upfal 2019), and MINION (Gholami et al. 2018). Zooming is an online learning algorithm that ignores decomposability, whereas CUCB uses decomposition but ignores similarity between arms. We use exploit history as a naive baseline, which greedily exploits historical data with a static strategy. We compute the optimal strategy exactly by solving a mixedinteger program over the true piecewise-linear reward functions, subject to the budget constraint. Experiments were run on a macOS machine with 2.4 GHz quad-core i5 processors and 16 GB memory. We take the mean of 30 trials.\nFig. 5 shows performance on both real-world and synthetic data, evaluated as the reward achieved at timestep t, where the reward of historical exploit is 0 and of optimal is 1. The performance of LIZARD significantly surpasses that of the baselines throughout. On the real-world data, LIZARD provides a particular advantage over CUCB in the early rounds. Table 1 shows the performance of each algorithm at T = 200 and 500, with varying values of N and B. LIZARD beats the baselines in every scenario.\nWe return to the four characteristics of green security domains (Sec. 3.1) and show that integrating each feature improves LIZARD. See Fig. 6 for results from our ablation study. Decomposability: CUCB, a naive decomposed bandits algorithm, greatly exceeds the non-decomposed zooming algorithm throughout most of Table 1, demonstrating the value of decomposition. Lipschitz-continuity: Fig. 6a reveals the value of information gained from knowing the exact value of the Lipschitz constant in each dimension (L i exact). We do not assume perfect knowledge of the Lipschitz constants L i , instead selecting an approximate value L i = 1 and using that same estimate across all dimensions. As shown, significantly overestimating L i with L i = 2 hinders performance; this setting is closer to that of CUCB, with no Lipschitz continuity. Monotonicity: Fig. 6b shows that the monotonicity assumption adds a significant boost to performance. Historical data: Fig. 6c demonstrates the value of adding historical information in early rounds. By timestep 40, the benefit of historical data is negligible.\nWe present LIZARD, an integrated algorithm for online learning in green security domains that leverages the advantages of decomposition and Lipschitz-continuity. With this approach, we transcend the proven lower regret bound of the zooming algorithm for Lipschitz bandits and extend combinatorial bandits to continuous spaces, showcasing their combined benefit. These results validate our approach of treating real-world conditions not as constraints but rather as useful features that lead to faster convergence. On top of achieving theoretical no-regret, we also demonstrate improved shortterm performance empirically, increasing the usefulness of this approach in practice-particularly in high-stakes environments where we cannot compromise short-term reward.\nThe next step is to move toward deployment. Researchers have demonstrated success deploying patrol strategies in the field to prevent poaching (Xu et al. 2020). Our LIZARD algorithm could be directly implemented as-is by integrating historical data, estimating the Lipschitz constant, and setting the budget as the number of hours available per day.", "publication_ref": ["b29", "b6", "b10", "b29"], "figure_ref": ["fig_3", "fig_4", "fig_5", "fig_5", "fig_5", "fig_5"], "table_ref": ["tab_1", "tab_1"]}, {"heading": "Ethics and Broader Impact", "text": "This research has been conducted in close collaboration with domain experts to design our approach. Their insights have shaped the trajectory of our project. For example, our initial idea was to plan information-gathering patrols, taking an active learning approach to gather data where the predictive model was most uncertain. However, conservation experts pointed out in subsequent discussions that patrollers could not afford to spend time purely gathering data; they must prioritize preventing illegal poaching, logging, and fishing in the short-term. These priorities guided our project, particularly our focus on minimizing short-term regret as well as long-term regret.\nOur work is intended to serve as an assistive technology, helping rangers identify potentially snare-laden regions they otherwise might have missed, given that these parks can be up to thousands of square kilometers and rangers can only patrol a small number of regions at each timestep. We do not intend this work to replace the critical role that rangers play in conservation management; no AI tool could substitute for the complex skills and domain insights that rangers and park managers have to plan and conduct patrols.\nA concern that might be raised could be that poachers who get access to this algorithm could predict where rangers might go and therefore place their snares more strategically. However, this algorithm would only be of use with access to the patrol data, which poachers would not have.\nA Proof of Theorem 2 Theorem 2. Given the minimum discretization gap \u2206, number of arms N , Lipschitz constant L, and time horizon T , the regret bound of Algorithm 1 with SELFUCB is\nReg \u2206 (T ) \u2264 O N L\u2206T + N 3 \u2206 \u22121 T log T + N 2 L\u2206 \u22121 . (5)\nProof. We first analyze the discretization regret. We then bound the arm selection regret by treating the problem as a combinatorial multi-armed bandit problem and apply a theorem given by Chen et al. (2016). Combining these two regret terms gives us the overall regret bound for the fixed discretization algorithm.\nDiscretization Regret Using discretization levels \u03a8 with a minimum gap \u2206, let OPT \u03a8 be the true optimum value and \u20d7 \u03b2 \u22c6 \u03a8 be the optimal solution when the rewards of all discretized points are fully known. Let the true optimum and the optimal solution without discretization be OPT and \u20d7 \u03b2 \u22c6 . Then we have\nOPT = \u00b5( \u20d7 \u03b2 \u22c6 ) = N i=1 \u00b5i(\u03b2 \u22c6 i ) \u2264 N i=1 \u00b5i(\u230a\u03b2 \u22c6 i \u230b \u03a8 ) + L|\u03b2 \u22c6 i \u2212 \u230a\u03b2 \u22c6 i \u230b \u03a8 | \u2264 N i=1 \u00b5i(\u230a\u03b2 \u22c6 i \u230b \u03a8 ) + L\u2206 = N i=1 \u00b5i(\u230a\u03b2 \u22c6 i \u230b \u03a8 ) + N i=1 L\u2206 \u2264 OPT\u03a8 + N L\u2206 .(9)\nThis yields OPT \u2212 OPT \u03a8 \u2264 N L\u2206, which is the discretization regret incurred by the given discretization.\nSelection Regret Given a discretization \u03a8, we treat each target with a specified patrol effort as a base arm. If we assume the smallest discretization gap is \u2206, we can bound the total number of base arms by N/\u2206. We denote a feasible patrol coverage as a super arm: a set of base arms that must satisfy the budget constraints and the restriction of selecting exactly one effort level per target. The set of all feasible super arms implicitly forms a feasibility constraint, which fits into the context of the combinatorial multi-armed bandit problem (Chen et al. 2016). In addition, our Lipschitz reward function also satisfies the bounded smoothness condition with\n|\u00b5( \u20d7 \u03b2) \u2212 \u00b5 \u2032 ( \u20d7 \u03b2)| = i \u00b5i(\u03b2i) \u2212 \u00b5 \u2032 i (\u03b2i) \u2264 N max i,\u03b2 i |\u00b5i(\u03b2i) \u2212 \u00b5 \u2032 i (\u03b2i)| = N \u2225\u00b5 \u2212 \u00b5 \u2032 \u2225\u221e = f (\u039b) (10)\nwhere f (\u039b) = N \u039b, \u039b = \u2225\u00b5 \u2212 \u00b5 \u2032 \u2225 \u221e is a linear function of \u039b. So we can apply the regret bound of combinatorial multiarmed bandit with polynomial bounded smoothness function f (x) = \u03b3x \u03c9 (Theorem 2 given by Chen et al.)\nReg(T ) \u2264 2\u03b3 2 \u2212 \u03c9 (6m log T ) \u03c9 2 \u2022 T 1\u2212 \u03c9 2 + \u03c0 2 3 + 1 mRmax\nwhere \u03c9 = 1, \u03b3 = N , m is the number of base arms, and R max is the maximum regret that can be achieved. In our domain, the number of base arms is bounded by m \u2264 N/\u2206. The maximum regret is bounded by the maximum reward achievable, which can be bounded by monotonicity and Lipschitz-continuity:\nRmax \u2264 \u00b5( \u20d7 \u03b2 \u22c6 ) = i \u00b5i(\u03b2 \u22c6 i ) \u2264 N L .\nBy substituting \u03c9 = 1, \u03b3 = N , and using upper bounds for m and R max , we get a valid regret bound of Algorithm 1 with fixed discretization gap \u2206:\nReg discrete \u2206 (T ) \u2264 2N 6N T log T \u2206 + \u03c0 2 3 + 1 N 2 L \u2206 . (11\n)\nThis regret is defined with respect to the optimum over all feasible discrete selections. Therefore, we can combine Inequalities 9 and 11 to derive the regret with respect to the true optimum\nReg \u2206 (T ) \u2264 2N 6N T log T \u2206 + \u03c0 2 3 + 1 N 2 L \u2206 + N L\u2206T = O N 3 T log T \u2206 + N 2 L \u2206 + N L\u2206T (12)\nwhere the gap in Inequality 9 is for each iteration, so must be multiplied by the number of timesteps T .", "publication_ref": ["b6", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "B Proof of Theorem 3", "text": "Theorem 3. Given the number of arms N , Lipschitz constant L, and time horizon T , the regret bound of Algorithm 2 with SELFUCB is be given by\nReg(T ) \u2264 O L 4 3 N T 2 3 (log T ) 1 3 .(6)\nProof. We want to set 2N 6N T log T /\u2206 and N L\u2206T in Eq. 12 to be of the same order. This is equivalent to solving\nO N N T log T /\u2206 = O(N L\u2206T ) \u21d0\u21d2 O T log T = O N L 2 \u2206 3 . Applying the fact O (T / log T ) = c =\u21d2 T = O(c log c) for any constant c yields T\u2206 = O N L 2 \u2206 3 log N L 2 \u2206 3\nwhich is the optimal stopping condition to let the two terms of the regret bound in Eq. 12 be of the same order. We set\nT\u2206 = N L 2 \u2206 3 log N L 2 \u2206 3 .\nSubstitute this into Eq. 12 with t \u2264 T \u2206 = N L 2 \u2206 3 log N \nL\u2206 2 + \u03c0 2 3 + 1 N 2 L \u2206 .\nDue to the discretization regret term, the bound given by Eq. 12 is not a sublinear function in T , so is not no-regret.\nTo achieve no-regret, we need to adaptively adjust the discretization levels. Therefore, as described in Algorithm 2, we gradually reduce the discretization gap \u2206 i = 2 \u2212i depending on the number of timesteps elapsed. For total timesteps T , let k \u2208 N \u222a {0} such that\nk\u22121 i=0 T \u2206i \u2264 T \u2264 k i=0 T \u2206i .\nThen we can bound the regret at time T by the regret at time k i=0 T \u2206i , which yields\nReg(T ) \u2264 Reg k i=0 T\u2206 i \u2264 k i=0 Reg \u2206 i (T\u2206 i ) \u2264 k i=0 \uf8eb \uf8ed (4 \u221a 3 + 1) N 2 log N L 2 \u2206 3 i L\u2206 2 i + \u03c0 2 3 + 1 N 2 L \u2206i \uf8f6 \uf8f8 \u2264 k i=0 (4 \u221a 3 + 1) N 2 L 2 2i log N L 2 2 3i + \u03c0 2 3 + 1 N 2 L2 i \u2264 (4 \u221a 3 + 1) N 2 L 2 2k+2 log N L 2 2 3k+3 + \u03c0 2 3 + 1 N 2 L2 k+1 .\nSince we know\nT = O k i=0 T\u2206 i =\u21d2 T = O k i=0 N L 2 \u2206 3 i log N L 2 \u2206 3 i =\u21d2 T = O k i=0 N L 2 2 3i log N L 2 2 3i =\u21d2 T = O N L 2 2 3k+3 log N L 2 2 3k+3 ,\nwe therefore have T = O( N L 2 2 3k log ( N L 2 2 3k )), which yields 2 3k = O( L 2 T N log T ). Replacing all 2 k with O(( L 2 T N log T )\n1\n3 ) provides a regret bound dependent on T and N only:\nReg(T ) \u2264 O N 2 L 2 2k log N L 2 2 3k + N 2 L2 k \u2264 O N 2 L L 2 T N log T 2 3 log T log T + N 2 L L 2 T N log T 1 3 \u2264 O L 1 3 N T 2 3 (log T ) 1 3 + L 5 3 N 5 3 T 1 3 (log T ) \u2212 1 3 . (13\n)\nWe only care about the regret order in terms of the dominating variable T , so the second term is dominated by the first term. Therefore, Reg(T\n) \u2264 O L 1 3 N T 2 3 (log T )1 3\n.\nTheorem 4. Consider a combinatorial multi-arm bandit problem. If the bounded smoothness function given is f (x) = \u03b3x \u03c9 for some \u03b3 > 0, \u03c9 \u2208 (0, 1] and the Lipschitz upper confidence bound is applied to all m base arms, the cumulative regret at time T is bounded by Reg(T ) \u2264 2\u03b3 2\u2212\u03c9 (6m log T )\n\u03c9 2 \u2022 T 1\u2212 \u03c9 2 + \u03c0 2 3 + 1 mR max\nwhere R max is the maximum regret achievable.\nTo prove Theorem 4, we first describe a weaker version in Lemma 7. Lemma 7 shows that if all arms are sufficiently sampled, the probability that we hit a bad super arm is small. Lemma 7 attributes all the bad super arm selections with error greater than R min to a maximum regret R max , which overestimates the overall regret and can be tightened with a more careful analysis (omitted here due to space). Lemma 7. Given the prior knowledge of Lipschitzcontinuity, monotonicity, and the definition of UCB in Sec. 4, the regret bound of CUCB algorithm (Chen et al. 2016) holds with Reg(T ) \u2264 6m log T (f \u22121 (R min )) 2 + \u03c0 2 3 m 2 + m R max . ( 14)\nProof. The main proof relies on whether the tighter UCB defined in Sec. 4 works for the combinatorial multi-armed bandit problem. Specifically, we must confirm that the proof of CUCB given by Chen et al. (2016) works with a different confidence bound definition. Define T i,t as the number of pulls of arm i up to time t. We assume there are m total base arms. Let \u00b5 = [\u00b5 1 , \u00b5 2 , . . . , \u00b5 m ] be the vector of true expectations of all base arms. Let X i,t be the revelation of arm i from a pull at time t andX i,s = (", "publication_ref": ["b6", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "s j=1 X i,j )/s be the average reward of the first s pulls of arm i. Therefore, we writeX i,Ti,t to represent the average reward of arm i at time t as defined in Sec. 4. Let S t be the super arm pulled in round t. We say round t is bad if S t is not the optimal arm.\nDefine event E t = {\u2200i \u2208 [m], |X i,Ti,t\u22121 \u2212 \u00b5 i | \u2264 r \u2032 t (i)}, where recall that r t (i) = 3 log t(2T i,t\u22121 ) \u22121 is the standard confidence radius, and r \u2032 t (i) = UCB t (i) \u2212X i,Ti,t\u22121 \u2264 SELFUCB t (i) \u2212X i,Ti,t\u22121 = r t (i) is the tighter confidence bound we introduce in Sec. 4. We want to bound the probability that all the sampled averages are close to the true mean value \u00b5 i with distance at most the confidence radius r \u2032 t (i) \u2200i. At time t, the probability that the sampled meanX i,Ti,t\u22121 of arm i lies within the ball with center \u00b5 i and radius r t (i) is bounded by 1\u22122t \u22123 (double-sided Hoeffding bound), which characterizes the probability of having one single SELFUCB bound hold for arm i. Moreover, at a fixed time t, if we have all m SELFUCB bounds hold, then all the m UCB bounds will automatically hold because each bound introduced by the Lipschitz continuity from any arm is a valid bound and thus UCB bound, the minimum of all the valid bounds, must also hold. Therefore, using the notation of SELFUCB radius r \u2032 t (i), we have\nwhere the second inequality is due to the fact that\nunder the Lipschitz condition, and the last inequality is due to the union bound across all m arms and t time steps. Equivalently, we have \u00acprob{\u00acE t } \u2264 2mt \u22122 .\nAccording to the definition r \u2032\nand UCB t (i) \u2265 \u00b5 i \u2200i. Let \u039b := 3 log t 2lt and \u039b \u2032 t := max i\u2208St r \u2032 t (i) \u2264 max i\u2208St r t (i) := \u039b t . Then we have\nLet UCB t = [UCB t (1), UCB t (2), . . . , UCB t (m)] be a vector of UCBs of all subarms. If the above two events on the left-hand side both hold {E t , S t is bad, \u2200i \u2208 S t , T i,t\u22121 > l t } at time t, if we denote reward \u00b5 (S) = i\u2208S \u00b5 i , we have\nwhere the first inequality is due to the monotonicity of function f and \u039b > \u039b \u2032 t ; the second is due to the definition of bounded smoothness function f in Eq. 10 and the condition that\nthe third is due to the optimal selection of S t with respect to UCB t ; the fourth is due to the optimality of opt UCBt , where S \u22c6 \u00b5 is the true optimal solution with respect to the real \u00b5; the fifth is due to the upper confidence bound between UCB t \u2265 \u00b5 when E t is true; and the sixth is again due to the optimality of S \u22c6 \u00b5 . By substituting l t = 6 log t (f \u22121 (Rmin)) 2 into \u039b = 3 log t 2lt , we have f (2\u039b) = R min . Eq. 15 contradicts the definition of minimum regret R min , the smallest non-zero regret (since S t is bad thus not optimal but it is also closer to the optimal with distance smaller than R min ). That is:\nLastly, we bound the total number of bad arm pulls by the probability that E t did not happen and a time-dependent term as shown by Chen et al. (2016) (proof omitted for space):\nWe then bound the cumulative regret by attributing the maximum regret to each of the bad arm pulls, which yields:\nProof of Theorem 4. As shown by Chen et al. (2016), the first term in Eq. 14 can be further broken down by having a finer regret attribution. Now we assign a maximum regret to all the bad super arm pulls, while we can in fact more carefully analyze the cumulative regret by counting the number of bad arm pulls resulting in a certain amount of regret. The only part different between ours and Chen et al. (2016) is the last term in Eq. 14, which is not affected by the additional analysis. So we conclude that the same argument still applies, where when the bounded smoothness function is of the form f (x) = \u03b3x \u03c9 , we can get a similar result:\nFinally, we are ready to prove Theorem 5. Theorem 5. Given the minimum discretization gap \u2206, number of targets N , Lipschitz constant L, and time horizon T , the regret bound of Algorithm 1 with UCB is\nand the regret bound of Algorithm 2 with UCB is\nProof. Eqs. 7 and 8 can be proved by the same argument of the proof of Theorem 2 and 3. As shown in Eq. 13, we get:", "publication_ref": ["b6", "b6", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "C Experiment details", "text": "The features associated with each target in Srepok Wildlife Sanctuary are distance to roads, major roads, rivers, streams, waterholes, park boundary, international boundary, patrol posts, and villages; forest cover; elevation and slope; conservation zone; and animal density of banteng, elephant, muntjac, pig. For LIZARD, CUCB, and zooming, we speed up the confidence radius from Eq. 2 as r t (i, j) = \u03f5 2nt(i,j) with \u03f5 = 0.1 to address the limited time horizon, which we empirically found to improve performance (no choice of \u03f5 affects the order of the algorithm's performance).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Finite-time analysis of the multiarmed bandit problem", "journal": "Machine Learning", "year": "2002", "authors": "P Auer; N Cesa-Bianchi; P Fischer"}, {"ref_id": "b1", "title": "Leaderfollower strategies for robotic patrolling in environments with arbitrary topologies", "journal": "", "year": "2009", "authors": "N Basilico; N Gatti; F Amigoni"}, {"ref_id": "b2", "title": "Rare trees are disappearing as 'wood pirates' log Bolivian national parks", "journal": "", "year": "2020", "authors": "E F Berton"}, {"ref_id": "b3", "title": "Learning optimal commitment to overcome insecurity", "journal": "", "year": "2014", "authors": "A Blum; N Haghtalab; A D Procaccia"}, {"ref_id": "b4", "title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "journal": "Foundations and Trends in Machine Learning", "year": "2012", "authors": "S Bubeck; N Cesa-Bianchi"}, {"ref_id": "b5", "title": "X-armed bandits", "journal": "Journal of Machine Learning Research", "year": "2011-05", "authors": "S Bubeck; R Munos; G Stoltz; C Szepesv\u00e1ri"}, {"ref_id": "b6", "title": "Combinatorial multi-armed bandit and its extension to probabilistically triggered arms", "journal": "Journal of Machine Learning Research", "year": "2016", "authors": "W Chen; Y Wang; Y Yuan; Q Wang"}, {"ref_id": "b7", "title": "Spatiotemporal trends of illegal activities from rangercollected data in a Ugandan national park", "journal": "Conservation Biology", "year": "2015", "authors": "R Critchlow; A J Plumptre; M Driciru; A Rwetsiba; E J Stokes; C Tumwesigye; F Wanyama; C Beale"}, {"ref_id": "b8", "title": "Deploying PAWS: Field Optimization of the Protection Assistant for Wildlife Security", "journal": "", "year": "2016", "authors": "F Fang; T H Nguyen; R Pickles; W Y Lam; G R Clements; B An; A Singh; M Tambe; A Lemieux"}, {"ref_id": "b9", "title": "When security games go green: Designing defender strategies to prevent poaching and illegal fishing", "journal": "", "year": "2015", "authors": "F Fang; P Stone; M Tambe"}, {"ref_id": "b10", "title": "Adversary models account for imperfect crime data: Forecasting and planning against real-world poachers", "journal": "", "year": "2018", "authors": "S Gholami; S Mc Carthy; B Dilkina; A Plumptre; M Tambe; M Driciru; F Wanyama; A Rwetsiba; M Nsubaga; J Mabonga"}, {"ref_id": "b11", "title": "Don't Put All Your Strategies in One Basket: Playing Green Security Games with Imperfect Prior Knowledge", "journal": "", "year": "2019", "authors": "S Gholami; A Yadav; L Tran-Thanh; B Dilkina; M Tambe"}, {"ref_id": "b12", "title": "Regression Oracles and Exploration Strategies for Short-Horizon Multi-Armed Bandits", "journal": "IEEE", "year": "2020", "authors": "R C Gray; J Zhu; S Onta\u00f1\u00f3n"}, {"ref_id": "b13", "title": "Robust Protection of Fisheries with COmPASS", "journal": "", "year": "2014", "authors": "W Haskell; D Kar; F Fang; M Tambe; S Cheung; E Denicola"}, {"ref_id": "b14", "title": "Efficient algorithms for online decision problems", "journal": "J. of Computer and System Sciences", "year": "2005", "authors": "A Kalai; S Vempala"}, {"ref_id": "b15", "title": "Policy learning for continuous space security games using neural networks", "journal": "", "year": "2018", "authors": "N Kamra; U Gupta; F Fang; Y Liu; M Tambe"}, {"ref_id": "b16", "title": "Multi-armed bandits in metric spaces", "journal": "ACM", "year": "2008", "authors": "R Kleinberg; A Slivkins; E Upfal"}, {"ref_id": "b17", "title": "Bandits and experts in metric spaces", "journal": "J. of the ACM", "year": "2019", "authors": "R Kleinberg; A Slivkins; E Upfal"}, {"ref_id": "b18", "title": "Nearly tight bounds for the continuum-armed bandit problem", "journal": "", "year": "2004", "authors": "R D Kleinberg"}, {"ref_id": "b19", "title": "Bandit algorithms", "journal": "", "year": "2018", "authors": "T Lattimore; C Szepesv\u00e1ri"}, {"ref_id": "b20", "title": "Using forest guards to protect a biological reserve in Costa Rica: one step towards linking parks to people", "journal": "Journal of Environmental Planning and Management", "year": "1992", "authors": "D J Lober"}, {"ref_id": "b21", "title": "Lipschitz bandits: Regret lower bounds and optimal algorithms", "journal": "COLT", "year": "2014", "authors": "S Magureanu; R Combes; A Proutiere"}, {"ref_id": "b22", "title": "Preventing illegal logging: Simultaneous optimization of resource teams and tactics for security", "journal": "", "year": "2016", "authors": "S M Mc Carthy; M Tambe; C Kiekintveld; M L Gore; A Killion"}, {"ref_id": "b23", "title": "Poaching in Uganda: Perspectives of law enforcement rangers", "journal": "Deviant Behavior", "year": "2015", "authors": "W D Moreto; A M Lemieux"}, {"ref_id": "b24", "title": "CAPTURE: A New Predictive Anti-Poaching Tool for Wildlife Protection", "journal": "", "year": "2016", "authors": "T H Nguyen; A Sinha; S Gholami; A Plumptre; L Joppa; M Tambe; M Driciru; F Wanyama; A Rwetsiba; R Critchlow"}, {"ref_id": "b25", "title": "Efficiently targeting resources to deter illegal activities in protected areas", "journal": "Journal of Applied Ecology", "year": "2014", "authors": "A J Plumptre; R A Fuller; A Rwetsiba; F Wanyama; D Kujirakwinja; M Driciru; G Nangendo; J E Watson; H P Possingham"}, {"ref_id": "b26", "title": "Multi-armed bandit problems with history", "journal": "", "year": "2012", "authors": "P Shivaswamy; T Joachims"}, {"ref_id": "b27", "title": "Optimal patrol planning for green security games with black-box attackers", "journal": "Springer", "year": "2017", "authors": "H Xu; B Ford; F Fang; B Dilkina; A Plumptre; M Tambe; M Driciru; F Wanyama; A Rwetsiba; M Nsubaga"}, {"ref_id": "b28", "title": "Playing repeated security games with no prior knowledge", "journal": "", "year": "2016", "authors": "H Xu; L Tran-Thanh; N R Jennings"}, {"ref_id": "b29", "title": "Stay Ahead of Poachers: Illegal Wildlife Poaching Prediction and Patrol Planning Under Uncertainty with Field Test Evaluations", "journal": "", "year": "2020", "authors": "L Xu; S Gholami; S M Carthy; B Dilkina; A Plumptre; M Tambe; R Singh; M Nsubuga; J Mabonga; M Driciru"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Rangers searching for snares (right) near a waterhole (left) in Srepok Wildlife Sanctuary in Cambodia. The waterhole is frequented by deer, pig, and bison, which are targeted by poachers.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Our regret bound matches the bound of the zooming algorithm with covering dimension d = 1. Our setting is instead N -dimensional, falling into a space with covering dimension d = N . The regret bound for a metric space with covering dimension d is O(T d+1 d+2 (log T ) 1 d+2", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Patrol effort \u03b2i Avg reward \u00b5i(\u03b2i)", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Map of Srepok with a 5 \u00d7 5 km region highlighted and the real-world reward functions of the corresponding 25 targets.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Performance, measured in terms of percentage of reward achieved between OPTIMAL \u2212 EXPLOIT, over time. Shaded region shows standard error. Setting shown is N = 25, B = 1. LIZARD (green) performs best.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 6 :6Figure 6: Impact of (a) different Lipschitz constants L i , (b) monotonicity assumption, (c) varying amount of historical data. The green line in each plot depicts the setting used by LIZARD. Run on synthetic data with N = 25, B = 1.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Adaptively Discretized LIZARD 1 Inputs: Number of targets N , time horizon T , budget B, target features \u20d7 y i , Lipschitz constant L", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Performance across multiple problem settings, throughout which LIZARD achieves closest-to-optimal performance.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "T \u00b5( \u20d7 \u03b2 \u22c6 ) \u2212 T t=1 \u00b5( \u20d7 \u03b2(", "formula_coordinates": [3.0, 54.0, 328.53, 88.76, 14.11]}, {"formula_id": "formula_1", "formula_text": "(t) i ) with effort \u03b2 (t) i .", "formula_coordinates": [3.0, 205.47, 458.97, 79.58, 14.07]}, {"formula_id": "formula_2", "formula_text": "\u03b2) = N i=1 \u00b5 i (\u03b2 i ).", "formula_coordinates": [3.0, 188.87, 575.28, 77.41, 14.11]}, {"formula_id": "formula_3", "formula_text": "\u00b5 i (\u03b2 i ) = \u00b5(\u20d7 y i , \u03b2 i ) for all i.", "formula_coordinates": [3.0, 54.0, 651.36, 119.39, 9.65]}, {"formula_id": "formula_4", "formula_text": "| \u00b5(\u20d7 y a , \u03b2 a ) \u2212 \u00b5(\u20d7 y b , \u03b2 b )| \u2264 L \u2022 D((\u20d7 y a , \u03b2 a ), (\u20d7 y b , \u03b2 b )) (1)", "formula_coordinates": [3.0, 331.35, 119.8, 226.65, 9.65]}, {"formula_id": "formula_5", "formula_text": "6", "formula_coordinates": [4.0, 55.89, 148.87, 3.49, 6.27]}, {"formula_id": "formula_6", "formula_text": "X (t) 1 , X (t) 2 , . . . , X (t) n 7 for i = 1, 2, . . . , N do 8 reward(i, \u03b2 i ) = reward(i, \u03b2 i ) + X (t) i 9 n(i, \u03b2 i ) = n(i, \u03b2 i ) + 1 reward \u00b5i(\u03b2i) 0 0.2 0.4 0 0.5 1 A B (a) effort \u03b2i 0 0.2 0.4 C A B (b) 0 0.2 0.4 C A B (c)", "formula_coordinates": [4.0, 55.89, 143.74, 232.8, 168.34]}, {"formula_id": "formula_7", "formula_text": ")3", "formula_coordinates": [4.0, 550.26, 168.74, 7.74, 8.64]}, {"formula_id": "formula_8", "formula_text": "UCB t (i, j) = min u\u2208[N ] v\u2208[J] {SELFUCB t (u, v) + L \u2022 dist} (4) dist = max{0, \u03c8 v \u2212 \u03c8 j } + D(\u20d7 y i , \u20d7 y u )", "formula_coordinates": [4.0, 333.03, 275.35, 224.97, 38.93]}, {"formula_id": "formula_9", "formula_text": "max z i\u2208[N ] j\u2208[J] z i,j \u2022 UCB t (i, j) (P) s.t. z i,j \u2208 {0, 1} \u2200i \u2208 [N ], j \u2208 [J] j\u2208[J] z i,j = 1 \u2200i \u2208 [N ] i\u2208[N ] j\u2208[J] z i,j \u03c8 j \u2264 B", "formula_coordinates": [4.0, 334.81, 534.07, 223.2, 73.41]}, {"formula_id": "formula_10", "formula_text": "3 log s 2s = 0 + r s (B) \u2264 1 + r s (A) = 1 + 3 log s 2H yielding linear regret O(s) = O H log H in the first s rounds.", "formula_coordinates": [5.0, 54.0, 256.97, 238.5, 55.25]}, {"formula_id": "formula_11", "formula_text": "Reg \u2206 (T ) \u2264 O N L\u2206T + N 3 \u2206 \u22121 T log T + N 2 L\u2206 \u22121 . (5)", "formula_coordinates": [5.0, 330.19, 78.31, 227.81, 10.27]}, {"formula_id": "formula_12", "formula_text": "3 \u2206 \u22121 T log T ) = O(N L\u2206T ), or equivalently \u2206 = (log T /T ) 1 3 N 1 3 L \u2212 2 3", "formula_coordinates": [5.0, 319.5, 302.06, 238.5, 23.1]}, {"formula_id": "formula_13", "formula_text": "Reg(T ) \u2264 O L 4 3 N T 2 3 (log T ) 1 3 .(6)", "formula_coordinates": [5.0, 367.21, 569.31, 190.79, 12.33]}, {"formula_id": "formula_14", "formula_text": "\u03a8 = {0, 1} , gap \u2206 = 1 3 T k = N 2 3k L 2 log N 2 3k L 2 \u2200k \u2208 N \u222a {0} 4 n(i, \u03c8 j ) = 0, reward(i, \u03c8 j ) = 0 \u2200i \u2208 [N ], j \u2208 [J] 5 for t = 1, 2, . . . , T do 6 if t > k\u22121 j=0 T j then 7 Set \u2206 = 2 \u2212k and \u03a8 = {0, \u2206, ..., 1}8", "formula_coordinates": [6.0, 55.49, 96.37, 215.97, 86.73]}, {"formula_id": "formula_15", "formula_text": "X (t) 1 , X (t) 2 , . . . , X (t) n 11 for i = 1, 2, . . . , N do 12 reward(i, \u03b2 i ) = reward(i, \u03b2 i ) + X (t) i 13 n(i, \u03b2 i ) = n(i, \u03b2 i ) + 1", "formula_coordinates": [6.0, 52.41, 200.42, 206.83, 48.4]}, {"formula_id": "formula_16", "formula_text": "Reg(T ) \u2264 2\u03b3 2\u2212\u03c9 (6m log T ) \u03c9 2 \u2022 T 1\u2212 \u03c9 2 + \u03c0 2 3 + 1 mR max", "formula_coordinates": [6.0, 59.96, 609.42, 223.64, 14.84]}, {"formula_id": "formula_17", "formula_text": "Reg \u2206 (T ) \u2264 O N L\u2206T + N 3 \u2206 \u22121 T log T + N 2 L\u2206 \u22121 (7)", "formula_coordinates": [6.0, 325.61, 237.38, 232.39, 10.88]}, {"formula_id": "formula_18", "formula_text": "Reg(T ) \u2264 O L 4 3 N T 2 3 (log T ) 1 3 . (8", "formula_coordinates": [6.0, 367.21, 272.41, 186.92, 12.33]}, {"formula_id": "formula_19", "formula_text": ")", "formula_coordinates": [6.0, 554.13, 276.1, 3.87, 8.64]}, {"formula_id": "formula_20", "formula_text": "Reg \u2206 (T ) \u2264 O N L\u2206T + N 3 \u2206 \u22121 T log T + N 2 L\u2206 \u22121 . (5)", "formula_coordinates": [10.0, 64.69, 110.86, 227.81, 10.27]}, {"formula_id": "formula_21", "formula_text": "OPT = \u00b5( \u20d7 \u03b2 \u22c6 ) = N i=1 \u00b5i(\u03b2 \u22c6 i ) \u2264 N i=1 \u00b5i(\u230a\u03b2 \u22c6 i \u230b \u03a8 ) + L|\u03b2 \u22c6 i \u2212 \u230a\u03b2 \u22c6 i \u230b \u03a8 | \u2264 N i=1 \u00b5i(\u230a\u03b2 \u22c6 i \u230b \u03a8 ) + L\u2206 = N i=1 \u00b5i(\u230a\u03b2 \u22c6 i \u230b \u03a8 ) + N i=1 L\u2206 \u2264 OPT\u03a8 + N L\u2206 .(9)", "formula_coordinates": [10.0, 92.84, 279.35, 199.66, 134.54]}, {"formula_id": "formula_22", "formula_text": "|\u00b5( \u20d7 \u03b2) \u2212 \u00b5 \u2032 ( \u20d7 \u03b2)| = i \u00b5i(\u03b2i) \u2212 \u00b5 \u2032 i (\u03b2i) \u2264 N max i,\u03b2 i |\u00b5i(\u03b2i) \u2212 \u00b5 \u2032 i (\u03b2i)| = N \u2225\u00b5 \u2212 \u00b5 \u2032 \u2225\u221e = f (\u039b) (10)", "formula_coordinates": [10.0, 66.7, 587.43, 225.8, 39.36]}, {"formula_id": "formula_23", "formula_text": "Reg(T ) \u2264 2\u03b3 2 \u2212 \u03c9 (6m log T ) \u03c9 2 \u2022 T 1\u2212 \u03c9 2 + \u03c0 2 3 + 1 mRmax", "formula_coordinates": [10.0, 61.15, 679.79, 223.71, 21.51]}, {"formula_id": "formula_24", "formula_text": "Rmax \u2264 \u00b5( \u20d7 \u03b2 \u22c6 ) = i \u00b5i(\u03b2 \u22c6 i ) \u2264 N L .", "formula_coordinates": [10.0, 370.03, 122.48, 137.44, 20.04]}, {"formula_id": "formula_25", "formula_text": "Reg discrete \u2206 (T ) \u2264 2N 6N T log T \u2206 + \u03c0 2 3 + 1 N 2 L \u2206 . (11", "formula_coordinates": [10.0, 328.47, 183.77, 225.79, 21.51]}, {"formula_id": "formula_26", "formula_text": ")", "formula_coordinates": [10.0, 554.27, 191.62, 3.73, 7.77]}, {"formula_id": "formula_27", "formula_text": "Reg \u2206 (T ) \u2264 2N 6N T log T \u2206 + \u03c0 2 3 + 1 N 2 L \u2206 + N L\u2206T = O N 3 T log T \u2206 + N 2 L \u2206 + N L\u2206T (12)", "formula_coordinates": [10.0, 324.55, 257.34, 233.45, 49.7]}, {"formula_id": "formula_28", "formula_text": "Reg(T ) \u2264 O L 4 3 N T 2 3 (log T ) 1 3 .(6)", "formula_coordinates": [10.0, 367.21, 400.65, 190.79, 12.33]}, {"formula_id": "formula_29", "formula_text": "O N N T log T /\u2206 = O(N L\u2206T ) \u21d0\u21d2 O T log T = O N L 2 \u2206 3 . Applying the fact O (T / log T ) = c =\u21d2 T = O(c log c) for any constant c yields T\u2206 = O N L 2 \u2206 3 log N L 2 \u2206 3", "formula_coordinates": [10.0, 319.5, 454.17, 238.5, 89.45]}, {"formula_id": "formula_30", "formula_text": "T\u2206 = N L 2 \u2206 3 log N L 2 \u2206 3 .", "formula_coordinates": [10.0, 392.15, 574.47, 93.2, 19.74]}, {"formula_id": "formula_31", "formula_text": "L\u2206 2 + \u03c0 2 3 + 1 N 2 L \u2206 .", "formula_coordinates": [10.0, 402.76, 679.79, 114.9, 21.51]}, {"formula_id": "formula_32", "formula_text": "k\u22121 i=0 T \u2206i \u2264 T \u2264 k i=0 T \u2206i .", "formula_coordinates": [11.0, 64.52, 132.29, 115.25, 14.11]}, {"formula_id": "formula_33", "formula_text": "Reg(T ) \u2264 Reg k i=0 T\u2206 i \u2264 k i=0 Reg \u2206 i (T\u2206 i ) \u2264 k i=0 \uf8eb \uf8ed (4 \u221a 3 + 1) N 2 log N L 2 \u2206 3 i L\u2206 2 i + \u03c0 2 3 + 1 N 2 L \u2206i \uf8f6 \uf8f8 \u2264 k i=0 (4 \u221a 3 + 1) N 2 L 2 2i log N L 2 2 3i + \u03c0 2 3 + 1 N 2 L2 i \u2264 (4 \u221a 3 + 1) N 2 L 2 2k+2 log N L 2 2 3k+3 + \u03c0 2 3 + 1 N 2 L2 k+1 .", "formula_coordinates": [11.0, 56.56, 167.7, 246.4, 119.93]}, {"formula_id": "formula_34", "formula_text": "T = O k i=0 T\u2206 i =\u21d2 T = O k i=0 N L 2 \u2206 3 i log N L 2 \u2206 3 i =\u21d2 T = O k i=0 N L 2 2 3i log N L 2 2 3i =\u21d2 T = O N L 2 2 3k+3 log N L 2 2 3k+3 ,", "formula_coordinates": [11.0, 91.56, 311.81, 163.38, 114.93]}, {"formula_id": "formula_35", "formula_text": "Reg(T ) \u2264 O N 2 L 2 2k log N L 2 2 3k + N 2 L2 k \u2264 O N 2 L L 2 T N log T 2 3 log T log T + N 2 L L 2 T N log T 1 3 \u2264 O L 1 3 N T 2 3 (log T ) 1 3 + L 5 3 N 5 3 T 1 3 (log T ) \u2212 1 3 . (13", "formula_coordinates": [11.0, 54.0, 480.7, 234.77, 70.19]}, {"formula_id": "formula_36", "formula_text": ")", "formula_coordinates": [11.0, 288.77, 543.12, 3.73, 7.77]}, {"formula_id": "formula_37", "formula_text": ") \u2264 O L 1 3 N T 2 3 (log T )1 3", "formula_coordinates": [11.0, 148.48, 585.21, 102.35, 11.61]}, {"formula_id": "formula_38", "formula_text": "\u03c9 2 \u2022 T 1\u2212 \u03c9 2 + \u03c0 2 3 + 1 mR max", "formula_coordinates": [11.0, 165.33, 670.94, 118.27, 14.84]}], "doi": ""}