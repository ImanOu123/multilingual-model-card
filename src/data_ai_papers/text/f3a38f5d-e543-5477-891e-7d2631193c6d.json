{"title": "On Learning Sets of Symmetric Elements", "authors": "Haggai Maron; Or Litany; Gal Chechik; Ethan Fetaya", "pub_date": "", "abstract": "Learning from unordered sets is a fundamental learning setup, recently attracting increasing attention. Research in this area has focused on the case where elements of the set are represented by feature vectors, and far less emphasis has been given to the common case where set elements themselves adhere to their own symmetries. That case is relevant to numerous applications, from deblurring image bursts to multi-view 3D shape recognition and reconstruction. In this paper, we present a principled approach to learning sets of general symmetric elements. We first characterize the space of linear layers that are equivariant both to element reordering and to the inherent symmetries of elements, like translation in the case of images. We further show that networks that are composed of these layers, called Deep Sets for Symmetric elements layers (DSS), are universal approximators of both invariant and equivariant functions, and that these networks are strictly more expressive than Siamese networks. DSS layers are also straightforward to implement. Finally, we show that they improve over existing setlearning architectures in a series of experiments with images, graphs and point-clouds.", "sections": [{"heading": "Introduction", "text": "Learning with data that consists of unordered sets of elements is an important problem with numerous applications, from classification and segmentation of 3D data (Zaheer et al., 2017;Qi et al., 2017;Su et al., 2015;Kalogerakis et al., 2017) to image deblurring (Aittala & Durand, 2018). In this setting, each data point consists of a set of elements, and the task is independent of element order. This independence induces a symmetry structure, which can be used to design deep models with improved efficiency and gen-1 NVIDIA Research 2 Stanford University 3 Bar Ilan University. Correspondence to: Haggai Maron <hmaron@nvidia.com>.\nProceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the author(s). eralization. Indeed, models that respect set symmetries, e.g. (Zaheer et al., 2017;Qi et al., 2017), have become the leading approach for solving such tasks. However, in many cases, the elements of the set themselves adhere to certain symmetries, as happens when learning with sets of images, sets of point-clouds and sets of graphs. It is still unknown what is the best way to utilize these additional symmetries.\nA common approach to handle per-element symmetries, is based on processing elements individually. First, one processes each set-element independently into a feature vector using a Siamese architecture (Bromley et al., 1994), and only then fuses information across all feature vectors. When following this process, the interaction between the elements of the set only occurs after each element has already been processed, possibly omitting low-level details. Indeed, it has been recently shown that for learning sets of images (Aittala & Durand, 2018;Sridhar et al., 2019;Liu et al., 2019), significant gain can be achieved with intermediate information-sharing layers.\nIn this paper, we present a principled approach to learning sets of symmetric elements. First, we describe the symmetry group of these sets, and then fully characterize the space of linear layers that are equivariant to this group. Notably, this characterization implies that information between set elements should be shared in all layers. For example, Figure 1 illustrates a DSS layer for sets of images. DSS layers provide a unified framework that generalizes several previously-described architectures for a variety of data types. In particular, it directly generalizes DeepSets (Zaheer et al., 2017). Moreover, other recent works can also be viewed as special cases of our approach (Hartford et al., 2018;Aittala & Durand, 2018;Sridhar et al., 2019).\nA potential concern with equivariant architectures is that restricting layers to be equivariant to some group of symmetries may reduce the expressive power of the model (Maron et al., 2019c;Morris et al., 2018;Xu et al., 2019). We eliminate this potential limitation by proving two universalapproximation theorems for invariant and equivariant DSS networks. Simply put, these theorems state that if invariant (equivariant) networks for the elements of interest are universal, then the corresponding invariant (equivariant) DSS networks on sets of such elements are also universal. One important corollary of these results is that DSS networks arXiv:2002.08599v4 [cs.LG] 29 Nov 2020 Figure 1. (a) A DSS layer for a set of images is composed of Siamese layer (blue) and an aggregation module (orange). The Siamese part is a convolutional layer (L1) that is applied to each element independently. In the aggregation module, the sum of all images is processed by a different convolutional layer (L2) and is added to the output of the Siamese part. (b) An example of a simple DSS-based invariant network.\nare strictly more expressive than Siamese networks.\nTo summarize, this paper has three main contributions: (1) We characterize the space of linear equivariant layers for sets of elements with symmetries. (2) We prove two universal approximation theorems for networks that are composed of DSS layers. (3) We demonstrate the empirical benefits of the DSS layers in a series of tasks, from classification through matching to selection, applied to diverse data from images to graphs and 3D point-clouds. These experiments show consistent improvement over previous approaches.", "publication_ref": ["b62", "b43", "b51", "b24", "b0", "b62", "b43", "b3", "b0", "b50", "b32", "b62", "b21", "b0", "b50", "b38", "b39", "b60"], "figure_ref": [], "table_ref": []}, {"heading": "Previous work", "text": "Learning with sets. Several studies designed network architectures for set-structured input. Vinyals et al. (2015) suggested to extend the sequence-to-sequence framework of Sutskever et al. (2014) to handle sets. The prominent works of Ravanbakhsh et al. (2016); Edwards & Storkey (2016); Zaheer et al. (2017); Qi et al. (2017) proposed to use standard feed-forward neural networks whose layers are constrained to be equivariant to permutations. These models, when combined with a set-pooling layer, were also shown to be universal approximators of continuous permutationinvariant functions. Wagstaff et al. (2019) provided a theoretical study on the limitations of representing functions on sets with such networks. In another related work, Murphy et al. (2018) suggested to model permutation-invariant functions as an average of permutation-sensitive functions.\nThe specific case of learning sets of images was explored in several studies. Su et al. (2015); Kalogerakis et al. (2017) targeted classification and segmentation of 3D models by processing images rendered from several view points. These methods use a Siamese convolutional neural network to process the images, followed by view-pooling layer. Esteves et al. (2019) recently considered the same setup and suggested to perform convolutions on a subgroup of the rotation group, which enables joint processing of all views. Sridhar et al. (2019) tackled 3D shape reconstruction from multiple view points and suggest using several equivariant meanremoval layers in which the mean of all images is subtracted from each image in the set. Aittala & Durand (2018) targeted image burst deblurring and denoising, and suggested to use set-pooling layers after convolutional blocks in which for each pixel, the maximum over all images is concatenated to all images. Liu et al. (2019) proposed to use an attentionbased information sharing block for face recognition tasks. In Gordon et al. (2020) the authors modify neural processes by adding a translation equivariance assumption, treating the inputs as a set of translation equivariant objects.\nEquivariance in deep learning. The prototypical example for equivariance in learning is probably visual object recognition, where the prevailing Convolutional Neural Networks (CNNs) are constructed from convolution layers which are equivariant to image translations. In the past few years, researchers have used invariance and equivariance considerations to devise deep learning architectures for other types of data. In addition to set-structured data discussed above, researchers suggested equivariant models for interaction between sets (Hartford et al., 2018), graphs Maron et al., 2019b;a;Chen et al., 2019;Albooyeh et al., 2019) and relational databases (Graham & Ravanbakhsh, 2019). Another successful line of work took into account other image symmetries such as reflections and rotations (Dieleman et al., 2016;Cohen & Welling, 2016a;Worrall et al., 2017;Cheng et al., 2018), spherical symmetries (Cohen et al., 2018;Esteves et al., 2017), or 3D symmetries (Weiler et al., 2018;Winkels & Cohen, 2018;Worrall & Brostow, 2018;Thomas et al., 2018;Weiler et al., 2018). From a theoretical point of view, several papers studied the properties of equivariant layers (Ravanbakhsh et al., 2017;Cohen et al., 2019a) and characterized the expressive power of models that use such layers (Yarotsky, 2018;Maron et al., 2019c;Keriven & Peyr\u00e9, 2019;Maehara & NT, 2019;Segol & Lipman, 2019).", "publication_ref": ["b53", "b52", "b44", "b14", "b62", "b43", "b54", "b40", "b51", "b24", "b16", "b50", "b0", "b32", "b19", "b21", "b37", "b4", "b1", "b20", "b13", "b6", "b59", "b5", "b8", "b15", "b55", "b56", "b58", "b53", "b55", "b45", "b9", "b61", "b38", "b25", "b34", "b47"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Notation and basic definitions", "text": "Let x \u2208 R represent an input that adheres to a group of symmetries G \u2264 S , the symmetric group on elements. G captures those transformations that our task-of-interest is invariant (or equivariant) to. The action of G on R is defined by (g \u2022 x) i = x g \u22121 (i) . For example, when inputs are images of size h\u00d7w, we have = hw and G can be a group that applies cyclic translations, or left-right reflections to an image.\nA function is called G-equivariant if f (g \u2022 x) = g \u2022 f (x) for all g \u2208 G. Similarly, a function f is called G-invariant if f (g \u2022 x) = f (x) for all g \u2208 G.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G-invariant networks", "text": "G-equivariant networks are a popular way to model Gequivariant functions. These networks are composed of several linear G-equivariant layers, interleaved with activation functions like ReLU, and have the following form:\nf = L k \u2022 \u03c3 \u2022 L k\u22121 \u2022 \u2022 \u2022 \u2022 \u03c3 \u2022 L 1 ,(1)\nWhere L i : R \u00d7di \u2192 R \u00d7di+1 are linear G-equivariant layers, d i are the feature dimensions and \u03c3 is a point-wise activation function. It is straightforward to show that this architecture results in a G-equivariant function. G-invariant networks are defined by adding an invariant layer on top of a G-equivariant function followed by a multilayer Perceptron (MLP), and have the form:\ng = m \u2022 \u03c3 \u2022 h \u2022 \u03c3 \u2022 f ,(2)\nwhere h :\nR \u00d7d k+1 \u2192 R d k+2 is a linear G-invariant layer and m : R d k+2 \u2192 R d k+3 is an MLP.\nIt can be readily shown that this architecture results in a G-invariant function.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Characterizing equivariant layers", "text": "The main building block of G-invariant/equivariant networks are linear G-invariant/equivariant layers. To implement these networks, one has to characterize the space of linear G-invariant/equivariant layers, namely, L i , h in Equations (1-2). For example, it is well known that for images with the group G of circular 2D translations, the space of linear G-equivariant layers is simply the space of all 2D convolutions operators (Puschel & Moura, 2008). Unfortunately, such elegant characterizations are not available for most permutation groups.\nCharacterizing linear G-equivariant layers can be reduced to the task of solving a set of linear equations in the following way: We are looking for a linear operator L : R \u2192 R that commutes with all the elements in G, namely:\nL(g \u2022 x) = g \u2022 L(x), x \u2208 R , g \u2208 G.(3)\nNote that L can be realized as a \u00d7 matrix (which will be denoted in the same way), and as in Maron et al. (2019b), Equation 3 is equivalent to the following linear system:\ng \u2022 L = L, g \u2208 G,(4)\nwhere g acts on both dimensions of L. The solution space of Equation 4 characterizes the space of all G-equivariant linear layers, or equivalently, defines a parameter sharing scheme on the layer parameters for the group G (Wood & Shawe-Taylor, 1996;Ravanbakhsh et al., 2017). We will denote the dimension of this space as E(G). We note that in many important cases (e.g., (Zaheer et al., 2017;Hartford et al., 2018;Maron et al., 2019b;Albooyeh et al., 2019)) |G| is exponential in so it is not possible to solve the linear system naively, and one has to resort to other strategies.", "publication_ref": ["b42", "b37", "b57", "b45", "b62", "b21", "b37", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Deep Sets", "text": "Since the current paper generalizes DeepSets (Zaheer et al., 2017), we summarize their main results for completeness.\nLet {x 1 , . . . x n } \u2282 R be a set, which we represent in arbitrary order as a vector x \u2208 R n . DeepSets characterized all S n -equivariant layers, namely, all matrices L \u2208 R n\u00d7n such that g \u2022 L(x) = L(g \u2022 x) for any permutation g \u2208 S n and have shown that these operators have the following structure: L = \u03bbI n + \u03b211 T . When considering sets with higher dimensional features, i.e., x i \u2208 R d and X \u2208 R n\u00d7d , this characterization takes the form:\nL(X) i = L 1 (x i ) + L 2 \uf8eb \uf8ed n j =i x j \uf8f6 \uf8f8 ,(5)\nwhere \nL 1 , L 2 : R d \u2192 R d are", "publication_ref": ["b62"], "figure_ref": [], "table_ref": []}, {"heading": "DSS layers", "text": "Our main goal is to design deep models for sets of elements with non-trivial per-element symmetries. In this section, we first formulate the symmetry group G of such sets. The deep models we advocate are composed of linear G-equivariant layers (DSS layers), therefore, our next step is to find a simple and practical characterization of the space of these layers.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sets with symmetric elements", "text": "Let {x 1 , . . . x n } \u2282 R d be a set of elements with symmetry group H \u2264 S d . We wish to characterize the space of linear maps L : R n\u00d7d \u2192 R n\u00d7d that are equivariant to both the natural symmetries of the elements, represented by the elements of the group H, as well as to the order of the n elements, represented by S n .\nIn our setup, H operates on all elements x i in the same way. More formally, the symmetry group is defined by G = S n \u00d7 H, where S n is the symmetric group on n elements. This group operates on X \u2208 R n\u00d7d by applying the permutation q \u2208 S n to the first dimension and the same element h \u2208 H to the second dimension, namely ((q, h) \u2022 X) ij = X q \u22121 (i)h \u22121 (j) . Figure 2 illustrates this setup.  (Aittala & Durand, 2018).\nOne can also consider another setup, where the members of H that are applied to each element of the set may differ. Section C of the supplementary material formulates this setup and characterizes the corresponding equivariant layers in the common case where H acts transitively on {1, . . . , d}.\nWhile this setup can be used to model several interesting learning scenarios, it turns out that the corresponding equivariant networks are practically reduced to Siamese networks that were suggested in previous works.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Characterization of equivariant layers", "text": "This subsection provides a practical characterization of linear G-equivariant layers for G = S n \u00d7 H. Our result generalizes DeepSets (equation 5) whose layers are tailored for H = {I d }, by replacing the linear operators L 1 , L 2 with linear H-equivariant operators. This result is summarized in the following theorem:\nTheorem 1. Any linear G\u2212equivariant layer L : R n\u00d7d \u2192 R n\u00d7d is of the form L(X) i = L H 1 (x i ) + L H 2 \uf8eb \uf8ed n j =i x j \uf8f6 \uf8f8 ,\nwhere\nL H 1 , L H 2 are linear H-equivariant functions\nNote that this is equivalent to the following formulations L(X\n) i = L H 1 (x i ) + L H 2 ( n j=1 x j ) = L H 1 (x i ) + n j=1 L H 2 (x j\n) due to linearity, and we will use them interchangeably throughout the paper. Figure 1 illustrates Theorem 1 for sets of images. In this case, applying a DSS layer amounts to: (i) Applying the same convolutional layer L 1 to all images in the set (blue); (ii) Applying another convolutional layer L 2 to the sum of all images (orange); and (iii) summing the outputs of these two layers. We discuss this theorem in the context of other widely-used data types such as point-clouds and graphs in section F of the Supplementary material.\nWe begin the proof by stating a useful lemma, that provides a formula for the dimension of the space of linear G-equivariant maps:\nLemma 1. Let G \u2264 S , then the dimension of the space of G-equivariant linear functions L : R \u2192 R is E(G) = 1 |G| g\u2208G tr(P (g)) 2 ,\nwhere P (g) is the permutation matrix that corresponds to the permutation g.\nThe proof is given in the supplementary material. Given this lemma we can now prove Theorem 1:\nProof of Theorem 1. We wish to prove that all linear Gequivariant layers L : R n\u00d7k \u2192 R n\u00d7k are of the form\nL(X) i = L H 1 (x i ) + L H 2 ( n j =i x j ).\nClearly, layers of this form are linear and equivariant. Moreover, the dimension of the space of these operators is exactly 2E(H) since we need to account for two linearly independent H-equivariant operators. The linear independence follows from the fact that their support in the matrix representation of L is disjoint. On the other hand, using Lemma 1 we have:\nE(G) = 1 |G| g\u2208G tr(P (g)) 2 = = 1 |H| 1 n! q\u2208Sn h\u2208H tr(P (q) \u2297 P (h)) 2 = 1 |H| 1 n! q\u2208Sn h\u2208H tr(P (q)) 2 tr(P (h)) 2 = 1 |H| h\u2208H tr(P (h)) 2 \u2022 \uf8eb \uf8ed 1 n! q\u2208Sn tr(P (q)) 2 \uf8f6 \uf8f8 = E(H)E(S n ) = 2E(H).\nHere we used the fact that the trace is multiplicative with respect to the Kronecker product as well as the fact that E(S n ) = 2 (see (Zaheer et al., 2017) or Appendix 2 in (Maron et al., 2019b) for a generalization of this result).\nTo conclude, we have a linear subspace\nL | L(X) i = L H 1 (x i ) + L H 2 ( n j =i x j )\n, which is a subspace of the space of all linear G-equivariant operators, but has the same dimension, which implies that both spaces are equal.\nRelation to (Aittala & Durand, 2018;Sridhar et al., 2019). In the specific case of a set of images and translation equivariance, L H i are convolutions. In this setting, (Aittala & Durand, 2018;Sridhar et al., 2019) have previously proposed using set-aggregation layers after convolutional blocks. The main differences between these studies and the current paper are: (1) Our work applies to all types of symmetric elements and not just images; (2) We derive these layers from first principles; (3) We provide a theoretical analysis (Section 5); (4) We apply an aggregation step at each layer instead of only after convolutional blocks.\nGeneralizations. Section A of the supplementary material generalizes Theorem 1 to equivariant linear layers with multiple features. It also generalizes to several additional types of equivariant layers:\nL : R n\u00d7d \u2192 R, L : R n\u00d7d \u2192 R n and L : R n\u00d7d \u2192 R d .\nProduct of arbitrary permutation groups. See Section B of the supplementary material for further discussion and characterization of the space of equivariant maps for a product of arbitrary permutation groups.", "publication_ref": ["b62", "b37", "b0", "b50", "b0", "b50"], "figure_ref": [], "table_ref": []}, {"heading": "A universal approximation theorem", "text": "When restricting a network to be invariant (equivariant) to some group action, one may worry that these restrictions could reduce the network expressive power (see Maron et al. (2019c) or Xu et al. (2019) for concrete examples). We now show that networks that are constructed from DSS layers do not suffer from loss of expressivity. Specifically, we show that for any group H that induces a universal H-invariant (equivariant) network, its corresponding G-invariant (equivariant) network has high expressive power: we prove universal approximation for invariant and equivariant functions defined on any compact set with zero intersection with a specific low-dimensional set E \u2282 R n\u00d7d . The precise definition of E is given in the supplementary.\nWe first state a lemma, which we later use for proving our universal-approximation theorems. The lemma shows that one can uniquely encode orbits of a group H in an invariant way by using a polynomial function. The full proof is given in Section D of the supplementary material.\nLemma 2. Let H \u2264 S d then there exists a polynomial function u : R d \u2192 R l , for some l \u2208 N, for which u(x) = u(y) if and only if x = h \u2022 y for some h \u2208 H.\nProof idea. This lemma is a generalization of Proposition 1 in (Maron et al., 2019a) and we follow their proof. The main idea is that for any such group H there exists a finite set of invariant polynomials whose values on R d uniquely define each orbit of H in R d .", "publication_ref": ["b38", "b60", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "Invariant functions", "text": "We are now ready to state and prove our first universal approximation theorem. As before, the full proof can be found in the supplementary material (Section D).\nTheorem 2. Let K \u2282 R n\u00d7d be a compact domain such that K = \u222a g\u2208G gK and\nK \u2229 E = \u2205. G-invariant networks are universal approximators (in \u2022 \u221e sense) of continuous G-invariant functions on K if H-invariant networks are universal 1 .\nProof idea. Let f : K \u2192 R be a continuous G-invariant function we wish to approximate. The idea of the proof is as follows: (1) We compute an S n -invariant set descriptor n j=1 x j and concatenate it to all elements as an extra feature channel. Later on, we compute an H-invariant representation for each set elements separately, thus lose information regarding their relative position. This shared descriptor will allow us to synchronize these individual invariant representations into a global G-invariant representation. The excluded set E is the set of points where this descriptor is degenerate and does not allow us to correctly synchronize the separate elements. (2) we encode each element x i and the extra descriptor 3) we encode the resulting set of descriptors with a unique S n -invariant polynomial set descriptor u Sn {u H (x i , n j=1 x j )} i\u2208[n] \u2208 R l Sn (4) we map the unique set descriptor u Sn {u H (x i , n j=1 x j )} i\u2208 [n] to the appropriate value defined by f (5) we use the classic universal approximation theorem (Cybenko, 1989;Hornik et al., 1989) and our assumption on the universality of H-invariant networks to conclude that there exists a G-invariant network that can approximate each one of the previous stages to arbitrary precision on K.\nn j=1 x j with a unique H-invariant polynomial descriptor u H (x i , n j=1 x j ) \u2208 R l H (\nRelation to Siamese networks. If we omit step (1) in the proof of Theorem 2, i.e., calculating the sum of all the inputs and concatenating it to each element, the proof implies that simple Siamese architectures that apply an Hinvariant network to each element in the set followed by a sum aggregation and finally an MLP, are universal with respect to the wreath-product symmetries (see discussion at section C of the supplementary material), which is a strictly larger group than G (assuming H is not trivial). Since Ginvariant networks can simulate any such Siamese network, Theorem 2 implies that G-invariant networks are strictly more expressive then Siamese networks. This is because G-invariant networks can distinguish different G-orbits that fall into the same wreath-product orbits . In section 6, we compare the Siamese architecture to our DSS networks and show that DSS-based architectures perform better in practice on several tasks.\nRelation to (Maron et al., 2019c). The authors proved that for any permutation group G, G-invariant networks have a universal approximation property, if the networks are allowed to use high-order tensors as intermediate representations (i.e., X \u2208 R d l for 2 \u2264 l \u2264 n 2 ), which are computationally prohibitive. We provide a complementary result by proving that if first-order 2 H-invariant networks are universal, so are first-order G-invariant networks.", "publication_ref": ["b11", "b22", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "Equivariant functions", "text": "Three possible types of equivariant functions can be considered. First, functions of the form f : R n\u00d7d \u2192 R n . For example, such a function can model a selection task in which we are given a set {x 1 , . . . , x n } and we wish to select a specific element from that set. Second, functions of the form f : R n\u00d7d \u2192 R d . An example for this type of functions would be an image-deblurring task in which we are given several noisy measurements of the same scene and we wish to generate a single high quality image (e.g., (Aittala & Durand, 2018)). Finally, functions of the form f : R n\u00d7d \u2192 R n\u00d7d . This type of functions can be used to model tasks such as image co-segmentation where the input consists of several images and the task is to predict a joint segmentation map.\nIn this subsection we will prove a similar universality result for the third type of G-equivariant functions that were mentioned above, namely f : R n\u00d7d \u2192 R n\u00d7d . We note that the equivariance of the first and second types can be easily deduced from this case. One can transform, for example, an R n\u00d7d \u2192 R d G-equivariant function into a R n\u00d7d \u2192 R n\u00d7d function by repeating the R d vector n times and use our general approximation theorem on this function. We can get back a R n\u00d7d \u2192 R d function by averaging over the first dimension.\nTheorem 3. Let K \u2282 R n\u00d7d be a compact domain such that K = \u222a g\u2208G gK and K \u2229 E = \u2205. G-equivariant net- works are universal approximators (in \u2022 \u221e sense) of continuous R n\u00d7d \u2192 R n\u00d7d G-equivariant functions on K if H-equivariant networks are universal.\nProof idea. The proof follows a similar line to the universality proof in (Segol & Lipman, 2019): First, we use the fact that equivariant polynomials are dense in the space of continuous equivariant functions. This enables us to assume that the function we wish to approximate is a G-equivariant polynomial. Next we show that for every output element, 2 First-order networks use only first-order tensors.\nthe mapping R n\u00d7d \u2192 R d can be written as a sum of Hequivariant base polynomials with invariant coefficients. The base polynomials can be approximated by our assumption on H and the invariant mappings can be approximated by leveraging a slight modification of theorem 2. Finally we show how we can combine all the parts and approximate the full function with a G-equivariant network.\nThe full proof is given in Section D of the supplementary material.", "publication_ref": ["b0", "b47"], "figure_ref": [], "table_ref": []}, {"heading": "Examples", "text": "We can use Theorems (2-3) to show that DSS-based networks are universal in two important cases. For tabular data, which was considered by Hartford et al. (2018), the symmetries are G = S n \u00d7 S d . From the universality of S n -invariant and equivariant networks (Zaheer et al., 2017;Segol & Lipman, 2019) we get that G-invariant (equivariant) networks are universal as well 3 . For sets of images, when H is the group of circular translations, it was shown in Yarotsky ( 2018) that H-invariant/equivariant networks are universal 4 , which implies universality of our DSS models.", "publication_ref": ["b21", "b62", "b47"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section we investigate the effectiveness of DSS layers in practice, by comparing them to previously suggested architectures and different aggregation schemes. We use the experiments to answer two basic questions: (1) Early or late aggregation? Can early aggregation architectures like DSS and its variants improve learning compared to Late aggregation architectures, which fuse the set information at the end of the data processing pipeline? and (2) How to aggregate? What is the preferred early aggregation scheme?\nTasks. We evaluated DSS in a series of six experiments spanning a wide range of tasks: from classification (R n\u00d7d \u2192 R), through selection (R n\u00d7d \u2192 R n ) and burst image deblurring (R n\u00d7d \u2192 R d ) to general equivariant tasks (R n\u00d7d \u2192 R n\u00d7d ). The experiments also demonstrate the applicability of DSS to a range of data types, including point-clouds, images and graphs. Figure 3 illustrates the various types of tasks evaluated. A detailed description of all tasks, architectures and datasets is given in the supplementary material (Section E).\nCompeting methods. We compare DSS to four other models: (1) MLP; (2) DeepSets (DS) (Zaheer et al., 2017); Figure 3. We consider all possible types of invariant and equivariant learning tasks in our settings: classification (R n\u00d7d \u2192 R), selection (R n\u00d7d \u2192 R n ), merging (R n\u00d7d \u2192 R d ) and general equivariant tasks (R n\u00d7d \u2192 R n\u00d7d ).\n(3) Siamese network; (4) Siamese network followed by DeepSets (Siamese+DS).\nWe also compare several variants of our DSS layers:\n(1) DSS(sum): our basic DSS layer from Theorem 1 (2) DSS(max): DSS with max-aggregation instead of sum-aggregation (3) DSS(Aittala): DSS with the aggregation proposed in (Aittala & Durand, 2018), namely,\nL(x) i \u2192 [L H (x i ), max n j=1 L H (x j )]\nwhere [] denotes feature concatenation and L H is a linear H-equivariant layer (4) DSS(Sridhar): DSS layers with the aggregation proposed in (Sridhar et al., 2019) \n,i.e., L(x) i \u2192 L H (x i ) \u2212 1 n n j=1 L H (x j ).\nEvaluation protocol. For a fair comparison, for each particular task, all models have roughly the same number of parameters. In all experiments, we report the mean and standard deviation over 5 random initializations. Experiments were conducted using NVIDIA DGX with V100 GPUs.", "publication_ref": ["b62", "b0", "b50"], "figure_ref": [], "table_ref": []}, {"heading": "Classification with multiple measurements", "text": "To illustrate the benefits of DSS, we first evaluate it in a signal-classification task using a synthetic dataset that we generated. Each sample consists of a set of n = 25 noisy measurements of the same 1D periodic signal sampled at 100 time-steps (see Figure 3). The clean signals are sampled uniformly from three signal types -sine, saw-tooth and square waves -with varying amplitude, DC component, phase-shift and frequency. The task is to predict the signal type given the set of noisy measurements. Figure 4 depicts the classification accuracy as a function of varying training set sizes, showing that DSS(sum) outperforms all other methods. Notably, DSS(sum) layers achieve signifi- cantly higher accuracy then the DeepSets architecture which takes into account the set structure but not within-element symmetry. DSS(sum) also outperforms the the Siamese and Siamese+DS architectures, which do not employ early aggregation. DSS(Sridhar) fails, presumably because it employs a mean removal aggregation scheme which is not appropriate for this task (removes the signal and leaves the noise).", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Selection tasks", "text": "We next test DSS layers on selection tasks. In these tasks, we are given a set and wish to choose one element of the set that obeys a predefined property. Formally, each task is modelled as a G-equivariant function f : R n\u00d7d \u2192 R n , where the output vector represents the probability of selecting each element. The architecture comprises of three convolutional blocks employing Siamese or DSS variants, followed by a DeepSets block. We note that the Siamese+DS model was suggested for similar selection tasks in (Zaheer et al., 2017).\nFrame selection in images and shapes. The first selection task is to find a particular frame within an unordered set of frames extracted from a video/shape sequence. For videos, we used the UCF101 dataset (Soomro et al., 2012). Each set contains n = 8 frames that were generated by randomly drawing a video, a starting position and frame ordering. The task is to select the \"first\" frame, namely, the one that appeared earliest in the video. Table 1    handle multiple data types. Specifically, we showcase how DSS operates on point-clouds and graphs. Given a short sequences of 3D human shapes preforming various activities, the task is to identify which frame was the center frame in the original non-shuffled sequence. These human shapes are represented as point-clouds in the first experiment and as graphs (point-clouds + connectivity) in the second. To generate the data, we cropped 7-frame-long sequences from the Dynamic Faust dataset (Bogo et al., 2017) in which the shapes are given as triangular meshes. To generate pointclouds, we simply use the mesh vertices. To generate graphs, we use the graph defined by the triangular mesh 5 . See Figure 5 for an illustration of this task.\nResults are summarized in Table 1, comparing DSS variants to a late-aggregation baseline (Siamese +DS) and to random choice. We further compared to a simple yet strong baseline. Using the mapping between points across shapes, we computed the mean of each point, and searched for the shape that was closest to that mean in L 1 sense. Frames in the sequence are 80msec apart, which limits the deviations around the mean, making it a strong baseline. Indeed, it achieved an accuracy of 34.47, which outperforms both late aggregation, DSS(max) and DSS(Aitalla). In contrast, sum-based early aggregation methods reach significantly higher accuracy. Interestingly, using a graph representation provided a small improvement over point-clouds for almost all methods .\nHighest quality image selection. Given a set of n = 20 degraded images of the same scene, the task is to select the highest-quality image. We generate data for this task from the Places dataset (Zhou et al., 2017), by adding noise and Gaussian blur to each image. The target image is defined to be the image that is the most similar in L 1 norm sense to the original image (see Figure 3 for an illustration). Notably, DSS consistently improves over Siamese+DS with a margin of 1% to 3%. See Table 2.", "publication_ref": ["b62", "b49", "b2", "b63"], "figure_ref": ["fig_1"], "table_ref": ["tab_3", "tab_3", "tab_4"]}, {"heading": "Color-channel matching", "text": "To illustrate the limitation of late-aggregation, we designed a very simple image-to-image task that highlights why early aggregation can be critical: learning to combine color channels into full images. Here, each sample consists of six images, generated from two randomly selected color images, by separating each image into three color channels.\nIn each mono-chromatic image two channels were set to zero, yielding a d = 64 \u00d7 64 \u00d7 3 image. The task is to predict the fully colored image (i.e., imputing the missing color channels) for each of the set element. This can be formulated as a R n\u00d7d \u2192 R n\u00d7d G-equivariant task. See Figure 3 for an example.\nWe use a U-net architecture (Ronneberger et al., 2015), where convolutions and deconvolutions are replaced with Siamese layers or DSS variants. A DeepSets block is placed between the encoder and the decoder. Table 3 shows that layers with early aggregation significantly outperform DS+Siamese. For context, we add the error value of a trivial predictor which imputes the zeroed color channels by replicating the input color channel, resulting in a gray-scale image. This experiment was conducted on two datasets:  CelebA (Liu et al., 2018), and Places (Zhou et al., 2017).", "publication_ref": ["b46", "b33", "b63"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "Burst image deblurring", "text": "Finally, we test DSS layers in a task of deblurring image bursts as in (Aittala & Durand, 2018). In this task, we are given a set of n = 5 blurred and noisy images of the same scene and aim to generate a single high quality image. This can be formulated as a R n\u00d7d \u2192 R d G-equivariant task. See results in Table 3, where we also added the mean absolute error of a trivial predictor that outputs the median pixel of the images in the burst at each pixel. More details can be found in the supplementary material.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "Summary of experiments", "text": "The above experiments demonstrate that applying early aggregation using DSS layers improves learning in various tasks and data types, compared with earlier architectures like Siamese+DS. This improvement might be attributed to the provably higher expressive power of DSS networks. More specifically, the basic DSS layer, DSS(sum), performs well on all tasks, and DSS(Aittala) has also yielded strong results. DSS(Sridhar) performs well on some tasks but fails on others. See Section G of the supplementary materials for additional experiments on a multi-view reconstruction task.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we have presented a principled approach for designing deep networks for sets of elements with symmetries: We have characterized the space of equivariant maps for such sets, analyzed its expressive power, exemplified its benefits over standard set learning approaches over a variety of tasks and data types and have shown that our approach generalizes several successful previous works.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Supplementary material", "text": "A. Generalizations of equivariant layer characterization A.1. Equivariant layers for multiple features\nThe following generalization to sets of elements with multiple features can be proved in a similar way to the section 3.1 in (Maron et al., 2019b).\nTheorem 4. Any linear G\u2212equivariant layer L : R n\u00d7d\u00d7f \u2192 R n\u00d7d\u00d7f is of the form\nL(X) i = L 1 (x i ) + L 2 ( n j =i x j ),\nwhere L i , i = 1, 2 are linear H-equivariant functions. The dimension of the space of these layers is 2E(H)f f .", "publication_ref": ["b37"], "figure_ref": [], "table_ref": []}, {"heading": "A.2. General equivariant and invariant layers", "text": "In the main text we characterized all G-invariant functions of the form L : R n\u00d7d \u2192 R n\u00d7d . Here, we characterize all other possibilities of equivariant and invariant functions. The proof is identical to the proof of Theorem 1 in the main paper.\nTheorem 5. 1. Any linear G\u2212equivariant layer L : R n\u00d7d \u2192 R n is of the form L(X) i = L H 1 (x i ) + L H 2 ( n j =i x j ), where L H i , i = 1, 2 are linear H-invariant functions. 2. Any linear G\u2212equivariant layer L : R n\u00d7d \u2192 R d is of the form L(X) = L H ( n j=1 x j )\n, where L H is linear H-equivariant function.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Any linear G-invariant layer", "text": "L : R n\u00d7d \u2192 R is of the form L(X) = L H ( n j=1 x j )\n, where L H is linear H-invariant function.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B. Products of arbitrary permutation groups", "text": "Here, we show that Theorem 1 can be generalized to products of arbitrary permutation groups. Our first step is noting that the second part of the proof of Theorem 1 can be easily modified to show that E(H 1 \u00d7 H 2 ) = E(H 1 ) \u2022 E(H 2 ) for any permutation groups H 1 , H 2 . Indeed, Theorem 1 is a special case of the following theorem, which characterizes the space of linear equivariant maps for arbitrary products of permutation groups.\nTheorem 6. Let H 1 \u2264 S n , H 2 \u2264 S d and {L j i } E(Hj ) i=1 , j = 1, 2 are bases for the spaces of linear H j -equivariant maps. Let G = H 1 \u00d7 H 2 act on R n\u00d7d by multiplication, (h 1 , h 2 ) \u2022 X := h 1 Xh T 2 .\nThen, a basis for the space of linear G\u2212equivariant layers L : R n\u00d7d \u2192 R n\u00d7d is given by\nT i1,i2 = L 1 i1 \u2297 L 2 i2 , i 1 = 1, . . . , E(H1), i 2 = 1, . . . , E(H 2 )\nProof. {T i1,i2 } is G-equivariant and linearly independent as a tensor product of linearly independent sets. Moreover, its size is exactly E(H 1 ) \u2022 E(H 2 ) so it must span the whole space of G-equivariant layers.\nThe basis mentioned in Theorem 6 can be implemented using the Kronecker product identity:\nT i1,i2 (X) = L 1 i1 XL 2 T i2(6)\nIn other words, these operators can be implemented by applying L 1 i1 to the columns of X and L 2 i2 to the rows of X. To verify that Theorem 6 is indeed a generalization of Theorem 1, consider H 1 = S n . A basis for S n -invariant layers is given by L 1 1 = I n , L 1 2 = 1 n 1 T n . From Theorem 6 it follows that a basis for the space of G-invariant linear layers is given by I n \u2297 L 2 i and 1 n 1 T n \u2297 L 2 i which, by Equation 6, gives the basis from Theorem 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C. Equivariant layers for order dependent action", "text": "As mentioned in the main text, we can consider a different learning setup, where tasks are equivariant to applying different elements of H to different elements in the set. In this section, we formulate this setup and prove that when H acts transitively on {1, . . . , d}, for example, in the case of images and sets, the corresponding equivariant layers are Siamese H-equivariant layers with an additional global summation term. In this setup, G = {(h 1 , . . . , h n , \u03c3)} is the semi-direct product \u2295 n i=1 H S n (also called restricted wreath product) and the action of G on R n\u00d7d is defined as\n((h 1 , . . . , h n , \u03c3) \u2022 X) ij = X \u03c3 \u22121 (i),h \u22121 i (j)\n. We can now characterize the set of G-equivariant layers for this setup. Theorem 7. If H acts transitively on {1, . . . , d} then any linear G-equivariant layer L is of the form:\nL(X) i = L 1 (x i ) + \u03b2 \uf8eb \uf8ed n j=1 d k=1 x jk \uf8f6 \uf8f8 .\nL 1 is an H-equivariant layer and \u03b2 \u2208 R. The dimension of the space of linear G-equivariant maps is E(H) + 1.\nProof. We want to characterize the space of G-equivariant maps. According to equation 4, we need to find the null space of the following fixed point equation g \u2022 L = L, g \u2208 G. As shown in (Wood & Shawe-Taylor, 1996;Ravanbakhsh et al., 2017), this is equivalent to revealing the parameter-sharing scheme that is induced by G, which we will define next. Let L \u2208 R nd\u00d7nd represent a linear G-equivariant map, where we think of the input X \u2208 R n\u00d7d as a row-stack x \u2208 R nd . The works mentioned above assert that L st = L kl if and only if there exists an element g \u2208 G such that g(s) = l, g(t) = k. Namely, the indices (s, t) and (k, l) share a parameter if and only if they belong to the same orbit of G when acting on {1, . . . , nd} 2 .\nWe now find this parameter-sharing scheme for G. For readability, we use two indices (i, j) to represent an index in s \u2208 {1, . . . , nd}. Given two such indices (s, t) = (i s , j s , i t , j t ) we wish to find their orbit under the action of G. We split this question into two cases and treat them one by one: (1) We first consider the case where i s = j s . In this case, the orbit of (i s , j s , i t , j t ) consists of all indices (l, k) = (i l , j l , i k , j k ) such that i l = i k which, in turn, implies that all the elements of L that are not on the d \u00d7 d block diagonal share their parameter.\n(2) In the case where i s = i t , applying the group action shows that all the d \u00d7 d diagonal blocks represent the same H-equivariant function. Proof. As discussed in Section 3, L can be realized as a \u00d7 matrix, and the problem of finding all linear G-equivariant functions L can be reduced to solving the following fixed-point equation: g \u2022 L = L. Recall that we are interested in the dimension of the space of linear G-equivariant layers, or equivalently, the dimension of the null space of the fixed-point equation. One way to obtain it, is by applying the trace function to the projection operator onto this null-space. See (Fulton & Harris, 2013), section 2.2 for a derivation. In our case, this projection is given by \u03c6 = 1 |G| g\u2208G P (g) \u2297 P (g) which implies:\nE(G) = tr(\u03c6) = 1 |G| g\u2208G tr(P (g) \u2297 P (g)) = 1 |G| g\u2208G tr(P (g)) 2 ,\nwhere \u2297 is a Kronecker product, P (g) \u2297 P (g) is the matrix representation of the action of G on R \u00d7 and we use the fact that the trace is multiplicative with respect to the Kronecker product.", "publication_ref": ["b57", "b45", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "D.2. Proof of Theorem 2", "text": "Proof of lemma 2. This lemma is a generalization of Proposition 1 in (Maron et al., 2019a) and we follow their proof idea. By Noether's theorem (see, e.g., (Yarotsky, 2018;Maron et al., 2019c)), there is a finite set of invariant polynomials\np i : R d \u2192 R l i=1\nthat generate the ring of invariant polynomials, that is, any invariant polynomial p(x) can be written as p(x) = q( p i (x)} l i=1 where q : R l \u2192 R is some general polynomial. We define u(x) = (p i (x)) l i=1 . On one hand, assume that y = g \u2022 x then by the invariance of the polynomials p i we get u(y) = u(g \u2022 x) = u(x). On the other hand, if u(x) = u(y) assume towards contradiction that g \u2022 y = x for all g \u2208 G, then the orbits G \u2022 x, G \u2022 y are disjoint. As both sets are finite, there is a continuous function f :\nR d \u2192 R such that f | G\u2022x \u2264 \u22122 and f | G\u2022y \u2265 2.\nUsing the Stone-Weierstrass theorem (Simmons, 1963) we can get a polynomial p with the property p| G\n\u2022x \u2264 \u22121 and p| G\u2022y \u2265 1. Define p = 1 |G| g\u2208G p(g \u2022 x)\nthen p is a G-invariant polynomial and using the discussion above we can write p(x) = q( p i (x)} l i=1 for some polynomial q. This, in turn, implies the following contradiction:\n1 \u2264 p(y) = q(u(y)) = q(u(x)) = p(x) \u2264 \u22121\nBefore we can continue, we need to define the low-dimensional set E from the theorem statement. There are several possible definitions for E. Here we define it in a relatively simple way.\nE = \u222a d j>k=1 { n i=1 x ik = n i=1 x ij }.\nUsing this definition, E is finite union of linear sub spaces of codimension 1. The definition makes sure that for every X \u2208 R n\u00d7d \\ E, the vector n i=1 x i \u2208 R d has exactly d different numbers. See discussion after the proof for an alternative definition of E with higher codimension. We note that it is possible to add random noise to break ties and move data points away from E.\nProof of Theorem 2. As previously mentioned, our first task is to concatenate to each element the sum of all elements. Next, we encode each element\nx i := [x i , n j=1\nx j ] with a unique H-invariant polynomial descriptor u H that exists according to lemma 3. We define the following map U H : R n\u00d7d\u00d72 \u2192 R n\u00d7d\u00d7l H by applying u H in the following way:\nU H (X) i,j,: = u H ( x i ), j = 1, . . . , d\nIn other words, U H encodes each x i using u H and repeats this encoding d times on the second dimension of the output tensor. Note that since u H is H-invariant then each component of U H that is applied to a specific element x i is H-equivariant. Let Y \u2208 R n\u00d7d\u00d7l H denote the output of U H and y i = u H (x i ) the unique H-invariant descriptors. Our second step is to map this set of unique descriptors to a unique set descriptor. By using Lemma 2, there exists a function u Sn : R n\u00d7l H \u2192 R l Sn that computes this encoding. Moreover, in the specific case of S n , u Sn can be chosen to be in the following form: u Sn (y 1 , . . . , y n ) = n i=1 p(y i ) where p : R l H \u2192 R l Sn is a multivariate polynomial (see section 4 in (Maron et al., 2019a) for more details). We define:\nU Sn (Y ) = 1 d n i=1 d j=1 p(Y i,j,: )\nand note that U Sn (Y ) = u Sn (y 1 , . . . , y n ) is exactly the unique S n -invariant set descriptor, and that U Sn is a G-invariant function as it is composed of applying feature-wise polynomials and summation. Moreover, according to Lemma 4 the mapping u: X \u2192 U Sn (Y ) is an injective mapping from G orbits on K to R l Sn and we can think about it as a function that assigns a unique descriptor to each G-orbit.\nUp until now, we have mapped our set of elements X to a unique set descriptor U Sn (U H (X)). Our next step is to map each such set descriptor to the value f (X). Intuitively, we would have liked to apply the function\nr = f \u2022 (U Sn \u2022 U H ) \u22121\nto the output of U Sn \u2022 U H but unfortunately, U Sn \u2022 U H is not injective so an inverse function is not well defined. Because f and u = U Sn \u2022 U H are invariant to the action of G = S n \u00d7 H there exists unique continuous mapsf ,\u0169 from the quotient space R n\u00d7d /G such that f =f \u2022 \u03c0 and u =\u0169 \u2022 \u03c0 where \u03c0 is the projection map to the quotient space. From the fact that our domain K \u2282 R n\u00d7d is compact we get thatK = \u03c0(K) is compact and\u0169 is bijective betweenK and its image. We can now write f = (f \u2022\u0169 \u22121 )\u2022\u0169\u2022\u03c0 and define r =f \u2022\u0169 \u22121 , which is continuous from lemma 5\nIn the last stage of the proof, we use the universal approximation properties of MLPs (Cybenko, 1989;Hornik et al., 1989) in order to approximate the three functions mentioned above, i.e., U Sn , U H , r, using a G-invariant network.\nFirst, calculating the set descriptor n j=1 x j can be calculated with a single G-equivariant layer. Next, we turn to U H which is defined as an element-wise application of the H-invariant function u H . We note that U applies a continuous H-invariant function element-wise which can be approximated by an H-invariant network according to our assumption. Furthermore, an element-wise application of an H-invariant network is a G-equivariant network which implies that there is a G-equivariant network N H : R n\u00d72d \u2192 R n\u00d7d\u00d7l H that uniformly approximates it.\nNext, we would like to approximate U Sn . From the universality of MLPs there exists MLP an M 1 : R l H \u2192 R l Sn and such that M 1 approximates p, which implies that\nn i=1 M 1 (y i ) approximates u Sn ({y i } n i=1\n). We define the next equivariant layers to apply M 1 to the feature dimension of Y . We then apply a scaled G-invariant summation function in order to get\n1 d n i=1 d j=1 M 1 (y i ) as output.\nOur last function to approximate is r and since it is a continuous function defined on a compact domain we can approximate it with an MLP M 2 .\nTo summarize, we have written our function of interest f as a composition of three functions U H , U Sn , r, and constructed a networks that uniformly approximates each one of these functions, which, by using the uniform continuity of the functions, gives us a uniform approximation of their composition.\nLemma 3. Let H \u2264 S d act on R d\u00d7k by applying the same element h \u2208 H to each channel, then there exists a polynomial function u : R d\u00d7k \u2192 R l , for some l \u2208 N, for which u(x) = u(y) if and only if x = h \u2022 y for some h \u2208 H.\nProof. We look at an isomorphic copy of H as a subgroup of S d\u2022k . From Lemma 2 there exists such a polynomial descriptor, which is clearly H-invariant.\nLemma 4. Let K \u2282 R n\u00d7d such that K \u2229 E = \u2205 then the polynomial mapping u: X \u2192 U Sn (Y ) is G-invariant and injective as function from R n\u00d7d /G.\nProof. The map is clearly invariant as a composition of invariant and equivariant functions, so we need show that this encoding is unique modulo G for X \u2208 R n\u00d7d . To see this we first note that since the set encoding in unique up to permutation we can reconstruct z \u03c3(1) , ..., z \u03c3 (n) for some \u03c3 \u2208 S n . Then for each z i we can reconstrcut [x i , y] up to a permutation h i \u2208 H, i.e. we can recover\n[h \u03c3(1) x \u03c3(1) , h \u03c3(1) y], ..., [h \u03c3(n) x \u03c3(n) , h \u03c3(n) y]\nWe know that the concatenated y part of the input is identical, and contains only unique numbers from our assumption K \u2229 E = \u2205 so we can permute each one such that they all agree with the first element h \u03c3(1) y. Since all elements of y are unique, the single permutation for the i'th element that does this is h \u03c3(1) h \u22121 \u03c3(i) to get 1) y] which is the same as our input up to (\u03c3, h \u03c3( 1) ) \u2208 G.\n[h \u03c3(1) x \u03c3(1) , h \u03c3(1) y], [h \u03c3(1) x \u03c3(2) , h \u03c3(1) y], ..., [h \u03c3(1) x \u03c3(n) , h \u03c3(\nLemma 5. Let K \u2282 R m be a compact domain and f : K \u2192 R be a continuous function such that f = h \u2022 g. If g is continuous, then h is continuous on g(K).\nProof. Assume that this is incorrect, then there is a sequence y i = g(x i ) such that y i \u2192 y 0 but h(y i ) h(y 0 ). Without loss of generality, assume that x i \u2192 x 0 \u2208 K (otherwise choose a converging sub sequence). We have\nf (x i ) = h(g(x i )) = h(y i ) h(y 0 ) = h(g(x 0 )) = f (x 0 )\nwhich is a contradiction to the continuity of f .\nAlternative choices of E. We can make E smaller by using power-sum set descriptors in addition to the sum we used in the proof above, namely\nx i := [x i , n i=1 x i , n i=1 x 2 i , . . . , n i=1 x n i ]\nwhere for x \u2208 R d , x k is the element-wise power operation. In this case, it is enough for us to have a single l \u2208 {1, . . . , n} for which\nn i=1 x k i has d different numbers. Here, we can define E = \u2229 n l=1 \u222a d j>k=1 { n i=1 x l ik = n i=1 x l ij }.\nThis is an intersection of n sets of codimension 1.", "publication_ref": ["b36", "b61", "b38", "b48", "b36", "b11", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "D.3. Proof of Theorem 3", "text": "For the equivariance proof, we need to use Theorem 2 on subsets of size n \u2212 1 of the inputs. For that reason, we define E to make sure that all sums of such subsets have different numbers. We define:\nE = \u222a n l=1 \u222a d j>k=1 { n i =l x ik = n i =l x ij } .\nAs in the proof of Theorem 2, we can define E using the power-sum polynomials and get a smaller set.\nProof of Theorem 3. We first note that G-equivariant polynomials are dense in the space of continuous G-equivariant functions over a compact domain. For proof, see Lemma 4 in (Segol & Lipman, 2019): while the statement in the paper is about S n equivariant polynomial, the proof trivially extends for every finite group. We therefore start by approximating P : R n\u00d7d \u2192 R n\u00d7d , an equivariant polynomial map of degree at most m. We look at P 1 = R n\u00d7d \u2192 R d the first element in the output of P . If P is S n \u00d7 H equivariant then by lemma 6 P 1 is S n\u22121 invariant when S n\u22121 operates on the last n \u2212 1 rows and H equivariant. If we fix x 2 , ..., x n then P 1 (x 1 , ..., x n ) is a H-equivariant polynomial in x 1 . The space of H-equivariant polynomials of bounded degree is a finite dimensional linear space and therefore has a basis q 1 , ..., q T . We can therefore write P 1 (x 1 , ..., x n ) = \u03b1 k (x 2 , ..., x n ) \u2022 q k (x 1 ) where \u03b1 k : R n\u22121\u00d7d \u2192 R are the coefficients. Because P 1 is S n\u22121 -invariant, H-equivariant and q k are a basis it is easy to see that \u03b1 k must be S n\u22121 \u00d7 H -invariant: If \u03c3 is a permutation on the last n \u2212 1 elements then P 1 (x 1 , ..., x n ) = P 1 (x 1 , x \u03c3(2) ..., x \u03c3(n) ) since P 1 is invariant to \u03c3. We then have \u03b1 k (x 2 , ..., x n ) \u2022 q k (x 1 ) = \u03b1 k (x \u03c3(2) , ..., x \u03c3(n) ) \u2022 q k (x 1 ) and because q k form a basis this means that for each k, \u03b1 k (x 2 , ..., x n ) is equal to \u03b1 k (x \u03c3(2) , ..., x \u03c3(n) ) proving S n\u22121 invariance. The same idea shows H-invariance.\nNext, we note that since P is G-equivariant we have [P (x 1 , ..,\nx n )] i = k \u03b1 k (x 1 , ...x i\u22121 , x i+1 , x n ) \u2022 q k (x i\n) by applying a permutation that only switches 1 and i. We will now show how this can be approximated using our G-equivariant network. This is the key for the proof as we break down the equivariant function to invariant functions that we can already approximate (up to the fact that they are S n\u22121 -invariant and not S n invariant), and H-equivariant functions that can be approximated by the assumption on H. The fact that \u03b1 k are invariant to permutations of the other n \u2212 1 elements and not the whole set is not an issue, as that can be implemented easily in our framework as our basic layers separates the sum over other elements with the operation over the current one (see theorem 1) which is exactly the operation needed.\nWe will use function names from the proof of theorem 2 when applicable for clarity. The first step of approximating P will be U H : R n\u00d7d \u2192 R n\u00d7d\u00d7l H +1 that maps each element to a unique H-invariant descriptor (same as in the proof of Theorem 2) plus the original information on a separate channel. The second mapping is U Sn\u22121 : R n\u00d7d\u00d7l H +1 \u2192 R n\u00d7d\u00d7l S n\u22121 +1 that computes an S n\u22121 \u00d7 H-invariant representation of the other n \u2212 1 inputs at each point plus the original input. Next, we need to compute the equivariant polynomial base elements and invariant coefficients. The coefficients are a continuous mapping r : R l S n\u22121 \u2192 R T (proof of continuity is the same as in the proof of Theorem 2) and can be approximated by an MLP, the equivariant polynomials q k : R d \u2192 R d are H-equivariant and continuous and can be approximated by an H-equivariant network that is applied to each element independently. The last operation is the multiplication and summation over basis elements, which can be approximated by an MLP on the channel dimension. This breaks down the computation of P into parts that each can be approximated by an equivariant neural network and therefore so can P . Since all polynomials are dense in the space of equivariant functions this shows that each equivariant function can be approximated by an equivariant neural network.\nLemma 6. If f : R n\u00d7d \u2192 R n\u00d7d is S n \u00d7 H equivariant, then f 1 : R n\u00d7d \u2192 R d the first element of f is S n\u22121 invariant and H equivariant. We assume S n\u22121 acts by permuting the last n-1 elements.\nProof. The proof is simple, we can think of f 1 as \u03c0 1 \u2022 f , ,i.e., f followed by the projection map on the first element. Since the permutations in S n\u22121 leave the first element in place, \u03c0 1 is invariant to them and so is f 1 as composition of equivariant and invariant. It is also clear that \u03c0 1 is H equivariant making f 1 H equivariant.", "publication_ref": ["b47"], "figure_ref": [], "table_ref": []}, {"heading": "E. Implementation details", "text": "All experiments (unless stated otherwise) were conducted using the PyTorch framework (Paszke et al., 2017), trained with the Adam optimizer (Kingma & Ba, 2014) on NVIDIA V100 GPU. We performed hyper-parameter search for all methods to choose a learning rate in {10 \u22121 , 10 \u22122 , . . . , 10 \u2212 7}. All model architectures use batch normalization (Ioffe & Szegedy, 2015) after each linear layer.\nDatasets. The following datasets were used:(1) Places (Zhou et al., 2017), an image dataset with natural scenes such as beach, parking lot or soccer field; (2) UCF101 (Soomro et al., 2012), an action recognition dataset for realistic action videos;\n(3) Celeba (Liu et al., 2018), a large scale image dataset that contains celebrity faces; (4) Dynamic Faust (Bogo et al., 2017), triangular meshes of real people performing different activities; (5) ImageNet (Deng et al., 2009)  From each clean signal, we generate a set of size 25 by replicating the signal and adding independent noise to each copy. The noise is sampled from an i.i.d. Gaussian distribution with zero mean and 3a standard deviation where a is the amplitude of the signal.", "publication_ref": ["b41", "b26", "b23", "b63", "b49", "b33", "b2", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Network and training.", "text": "For training we used batch size of 64 and ran for 200 epochs with validation-based early stopping. Training took between 15 minutes for MLP to 5 hours for DSS(Aitalla). For all layer types, we used three layers followed by a fully connected layer with the following number of features: MLP (840,420,420),Siamese (220,220,110),DSS (160,160,80), DSS (max) (160,160,80),Siamese+DS(2 Siamese layers + a single DS layer) (200,200,100), DS (1000, 1000, 500), DS (max) (1000, 1000, 500), Aittala (Aittala & Durand, 2018) (160, 160, 80), Sridhar (Sridhar et al., 2019) (220, 220, 110). In models that use convolution, we used strided-convolution with stride 2. For all models, we have used sum-pooling on the set and spatial dimensions before the fully connected layer.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.2. Image selection", "text": "Data preparation. The data for the video frame ordering experiment was taken from the UFC101 dataset (Soomro et al., 2012). For the highest quality image selection task we used the Places dataset (Zhou et al., 2017). For the places dataset, we first selected 25 classes that have the largest number of images. We then generated the train and validation sets from the standard train split, and used the standard validation split as test. In both cases, we used 20,000 training examples and 2,000 validation and test examples. The set sizes are n = 8 for the frame ordering experiment and n = 20 for the image quality assessment experiment. Train Image size was reduced to 80 \u00d7 80 and we used random cropping to 64 \u00d7 64 as well as random flipping as data augmentation. For the image quality task we also used random rotations. For the highest image quality task, we sampled a base blur \u03c3 \u223c U [0, 1] for each image, and another example specific \u03c3 \u223c U [0, 1] and used \u03c3 + \u03c3 as the Gaussian width for blurring; i.i.d Gaussian noise was added to the result for the Gaussian noise case and i.i.d random pixels where zeroed-out in the Occlusion noise case.", "publication_ref": ["b49", "b63"], "figure_ref": [], "table_ref": []}, {"heading": "Network and training.", "text": "For training we used batch size of 16 for 200 epochs with validation-based early stopping. training time was 6.5 (3.75) hours for DS and 4.5 (3.25) hours for DSS for the image quality assessment task (video frame ordering task). The network architecture is based on the image anomaly detection network suggested by (Zaheer et al., 2017) and is composed of convolutional part followed by a DeepSets block. The convolutional part consists of three blocks, each of which consists of the following number of features (32,32,64),(64,64,128),(128,128,256) for DSS(sum) and DSS(max),(90,90,100),(100,100,100) (110,110,128) for DSS(Aittala) and (50,50,100),(100,100,180), (200,200,256) for DS+Siamese and DSS (Sridhar). All DeepSets blocks have three layers with features(256,128,1).", "publication_ref": ["b62"], "figure_ref": [], "table_ref": []}, {"heading": "E.3. Shape selection", "text": "Data preparation. The data for the shape selection task was taken from Dynamic Fuast (Bogo et al., 2017). This dataset contains 3D videos of 10 human subjects (male and female) performing 15 activities (e.g. jumping jacks, punching, etc.) The data are represented as triangular meshes of the same topology. Importantly, all the shapes are in one-to-one correspondence. For the graph modality we directly use the mesh, see sec. F for more details. For the point-cloud modality we simply use the mesh vertices. We generated sets of 7 frames by randomly cropping sequences and shuffling their order. Note that, since the scans were captured at 60fps, the motion between few consecutive frames is approximately rigid and at constant velocity. To make the problem more challenging we chose to skip every k frames. To choose k we ran the following simple experiment. For each value of k we computed the mean shape of the set by averaging the point coordinates of all the set elements. We then searched for the shape in the set that was closest to the mean shape and evaluated the accuracy on the validation set. The results were 80. 95, 43.75, 32.91, 27.73 for skip sizes of 1, 3, 5, 10 respectively. We ended up choosing a skip size of 5. For testing we used a held out set. We chose a very challenging split where both the subjects and the activities are not seen at train time.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "Network and training.", "text": "We repeated all experiments with 3 different seeds, trained for 100 epochs using Adam optimizer on an NVIDIA TitanX with validation-based early stopping. For the point-cloud modality, all methods were ran using a batch-size of 16. Training times were roughly 2 hours. The network architecture is based on a PointNet module (Qi et al., 2017) with 1D convolutions of dimensions (64, 256, 128) followed by a DeepSets block of dimensiones (128,128,128,1). For the graph experiment we used batch-sizes of 8 for the architecture of Aittala, and 12 for all the other architectures. Training took about 20 hours. The architecture is based on a pytorch-geometric (Fey & Lenssen, 2019) implementation of Graph Convolutional Networks (GCN) (Kipf & Welling, 2016) with the adjacency matrix:\u00c2 = A + 2I. Dimensions of the graph layers were the same as described above for PointNet. We note that CGN, and in general message passing networks on graphs are not universal approximators.", "publication_ref": ["b43", "b17", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "E.4. Color matching", "text": "Data preparation. We used the Places (Zhou et al., 2017) and the CelebA (Liu et al., 2018) datasets. For the places dataset, we first selected 25 classes that have the largest number of images. We generated the train and validation sets from the standard train split, and used the standard validation split as test. For the CelebA dataset, we used the standard splits. In both cases we generate 30,000 train examples and 3,000 examples for validation and test with resolution 64 \u00d7 64 Network and training. We used U-net like networks. All architecture are composed of an encoder followed by a DeepSets block and a decoder. The encoder and decoder are composed of convolution blocks (2X(conv,batchnorm,relu)) according to the DSS variant/Siamese+DS architecture with each folowed by a max pooling layer with stride=2 for the first encoding layers and stride=8 for the last encoding layer. The DeepSets block is composed of three DeepSets layers with the same number of features as in its input. each decoding block applies similar convolution blocks, upsamples the signal and concatenates the appropriate features from the encoding phase. We use the following number of features: (50,100,150,200) for DSS(sum) and DSS(max), (64,128,200,300) for DSS (Sridhar) and Siamese+DS and (75,100,150,160) for DSS (Aittala). Training was done with batch size = 32 for 50 epochs starting with initial learning rate 0.001 and learning rate decay of 0.4 every 10 epochs, and with validation-based early stopping. We use the L 1 loss.", "publication_ref": ["b63", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "E.5. Burst image deblurring", "text": "Data generation. We follow the protocol in (Aittala & Durand, 2018). We generate blurred images by randomizing a (non-centered) blur kernel and noise. We use a loss that penalizes the deviations of the output image and its gradients from the original image. We also added the mean absolute error of the trivial predictor that outputs the median pixel of the images in the burst at each pixel (the mean predictor produced worse results). we used training set of size 100,000 and test/validation sets of size 10,000, randomly chosen from the ImageNet dataset (Deng et al., 2009). We down-sample images to 128 \u00d7 128 for efficiency. Training was done with batch size=32, learning rate of 0.003 and a decay rate of 0.95 every epoch for 35 epochs and validation-based early stopping.\nNetwork and training. we have used the same network architecture as in the color channel matching experiment followed by a set max pooling layer and two additional 2D convolutions. We have used the following number of features: (48,50,100,150,200) for DSS(sum) and DSS(max), (48,64,128,200,300) for DSS (Sridhar) and Siamese+DSS and (75,100,110,125,125) for DSS (Aittala).", "publication_ref": ["b0", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "F. Examples", "text": "In this subsection, we discuss how our general results can be used in three specific scenarios: learning sets of images, sets of sets, and sets of graphs.\nLearning sets of images. In this case, we write d = h \u2022 w for h, w \u2208 N, that is, each x i is a vector in R hw , and we H to be the group of 2D circular translations. According to Theorem 1, a general linear equivariant layer for this setup can be written as L(x i ) = L 1 (x i ) + L 2 j =i x j . In other words, the layer consists of two different convolutional layers, where the first layer L 1 is applied to each image independently and the second layer L 2 is applied to the sum of all images. This layer is easy to implement and we make an extensive use of it in the experiment section (section 6). We note that certain temporal or periodic signals can be handled in a similar fashion. In this case x i \u2208 R k and is the group of 1D circular translations.\nLearning sets of sets. Another useful application of our theory is for learning sets of consistently-ordered sets. See Section 6 for an example on sets of point-clouds. Here, we set d = m \u00d7 k where each item x i is an m \u00d7 k matrix representing a set of k-dimensional points. The group H in this case is S m which acts by permuting rows. We note that equivariance to this type of group action was first considered by (Hartford et al., 2018) for learning interactions between sets. In their paper, Hartford et al. (2018) also characterized the maximal linear equivariant basis (a special case of Theorem 1) which (as they nicely show) can be easily implemented by simple summation operations.\nLearning sets of graphs. Our layers can also be used to learn sets of graphs for tasks such as graph anomaly detection and graph classification. See Section 6 for an example on graphs that represent 3D shapes. In this case, d = k 2 and each item x i is a k \u00d7 k tensor (possibly with another feature dimension) representing the interactions between the k vertices in the graph (e.g., an adjacency of affinity matrix). The group H in this case is S k , acting on x i by permuting its rows and columns.\nRevisiting Deep Sets As our final example we note that both the characterization of equivariant layers and the universal approximation results in (Zaheer et al., 2017) are special cases of our theoretical results (Theorems 1, 2) where we set H = I d , that is, the symmetry group of the elements x i is trivial.", "publication_ref": ["b21", "b21", "b62"], "figure_ref": [], "table_ref": []}, {"heading": "G. Multi-view reconstruction", "text": "We tested DSS on a multi-view reconstruction task. Here, the input is a set of images of a 3D object and the task is to predict its 3D structure. We closely follow Sridhar et al. (2019), that pose this task as a learning problem in which a network is trained to \"lift\" image pixels in each view to their 3D normalized coordinate space (NOCS). NOCS is unique in that it canonicalizes shape pose and scale and thus makes the view-aggregation as simple as a union operation. In addition to predicting the NOCS representation of each foreground pixel, the network also predicts the coordinates of the occluded part of the object as if the camera was an x-ray.\nThe architecture proposed by (Sridhar et al., 2019) advocates a mean-subtraction aggregation scheme. After each convolutional block, the mean of all set elements is subtracted. This aggregation scheme can be seen as a specific case of DSS, in which the sum of all elements is further processed by a different convolution layer and only then added to the elements. This raises up an interesting question of whether a simple modification to the architecture of (Sridhar et al., 2019), in the form of changing the aggregation step to apply a convolution block to the sum of all set elements, can improve performance.\nFollowing the same experimental settings prescribed by the authors, we tested the modified architecture (named Sridhar+DSS) in the case of a fixed-sized input of 3 views per model on three different classes of 3d objects as in (Sridhar et al., 2019): cars, Airplanes and chairs. The results are summarized in table 4. As can be seen, our proposed modification gives a significant boost in performance on 2 out of 3 object classes. While we lack a good explanation for why the performance on chairs is decreased, this result suggests that it is worth to further explore the potential benefit of DSS for this task. We leave the full exploration of this specific task to future work.  4. Reconstruction error for the Multi-view 3D object reconstruction task. We compare the performance reported in (Sridhar et al., 2019) and our suggested modification (Sridhar+DSS). Reported errors are 2-way Chamfer distance between the ground truth shape and its reconstruction, multiplied by 100.", "publication_ref": ["b50", "b50", "b50", "b50", "b50"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "This research was supported by an Israel science foundation grant 737/18. We thank Srinath Sridhar and Davis Rempe for useful discussions.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Burst image deblurring using permutation invariant convolutional neural networks", "journal": "", "year": "2018", "authors": "M Aittala; F Durand"}, {"ref_id": "b1", "title": "Incidence networks for geometric deep learning", "journal": "", "year": "2019", "authors": "M Albooyeh; D Bertolini; S Ravanbakhsh"}, {"ref_id": "b2", "title": "Dynamic FAUST: Registering human bodies in motion", "journal": "", "year": "2017-07", "authors": "F Bogo; J Romero; G Pons-Moll; M J Black"}, {"ref_id": "b3", "title": "Signature verification using a\" siamese\" time delay neural network", "journal": "", "year": "1994", "authors": "J Bromley; I Guyon; Y Lecun; E S\u00e4ckinger; R Shah"}, {"ref_id": "b4", "title": "On the equivalence between graph isomorphism testing and function approximation with gnns", "journal": "", "year": "2019", "authors": "Z Chen; S Villar; L Chen; J Bruna"}, {"ref_id": "b5", "title": "Rotdcf: Decomposition of convolutional filters for rotation-equivariant deep networks", "journal": "", "year": "2018", "authors": "X Cheng; Q Qiu; R Calderbank; G Sapiro"}, {"ref_id": "b6", "title": "Group equivariant convolutional networks", "journal": "", "year": "2016", "authors": "T Cohen; M Welling"}, {"ref_id": "b7", "title": "", "journal": "", "year": "1990", "authors": "T S Cohen; M Welling;  Steerable Cnns"}, {"ref_id": "b8", "title": "", "journal": "", "year": "2018", "authors": "T S Cohen; M Geiger; J K\u00f6hler; M Welling"}, {"ref_id": "b9", "title": "A general theory of equivariant cnns on homogeneous spaces", "journal": "", "year": "2019", "authors": "T S Cohen; M Geiger; M Weiler"}, {"ref_id": "b10", "title": "Gauge equivariant convolutional networks and the icosahedral cnn", "journal": "", "year": "2019", "authors": "T S Cohen; M Weiler; B Kicanaoglu; M Welling"}, {"ref_id": "b11", "title": "Approximation by superpositions of a sigmoidal function", "journal": "", "year": "1989", "authors": "G Cybenko"}, {"ref_id": "b12", "title": "Imagenet: A large-scale hierarchical image database", "journal": "Ieee", "year": "2009", "authors": "J Deng; W Dong; R Socher; L.-J Li; K Li; L Fei-Fei"}, {"ref_id": "b13", "title": "Exploiting cyclic symmetry in convolutional neural networks", "journal": "", "year": "2016", "authors": "S Dieleman; J De Fauw; K Kavukcuoglu"}, {"ref_id": "b14", "title": "Towards a neural statistician", "journal": "", "year": "2016", "authors": "H Edwards; A Storkey"}, {"ref_id": "b15", "title": "and Daniilidis, K. 3d object classification and retrieval with spherical cnns", "journal": "", "year": "2017", "authors": "C Esteves; C Allen-Blanchette; A Makadia"}, {"ref_id": "b16", "title": "Equivariant multi-view networks", "journal": "", "year": "2019", "authors": "C Esteves; Y Xu; C Allen-Blanchette; K Daniilidis"}, {"ref_id": "b17", "title": "Fast graph representation learning with pytorch geometric", "journal": "", "year": "2019", "authors": "M Fey; J E Lenssen"}, {"ref_id": "b18", "title": "Representation theory: a first course", "journal": "Springer Science & Business Media", "year": "2013", "authors": "W Fulton; J Harris"}, {"ref_id": "b19", "title": "Convolutional conditional neural processes. In International Conference on Learning Representations", "journal": "", "year": "2020", "authors": "J Gordon; W P Bruinsma; A Y K Foong; J Requeima; Y Dubois; R E Turner"}, {"ref_id": "b20", "title": "Deep models for relational databases", "journal": "", "year": "2019", "authors": "D Graham; S Ravanbakhsh"}, {"ref_id": "b21", "title": "Deep models of interactions across sets", "journal": "", "year": "2018", "authors": "J S Hartford; D R Graham; K Leyton-Brown; S Ravanbakhsh"}, {"ref_id": "b22", "title": "Multilayer feedforward networks are universal approximators", "journal": "", "year": "1989", "authors": "K Hornik; M Stinchcombe; H White"}, {"ref_id": "b23", "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "journal": "", "year": "2015", "authors": "S Ioffe; C Szegedy"}, {"ref_id": "b24", "title": "3d shape segmentation with projective convolutional networks", "journal": "", "year": "2017", "authors": "E Kalogerakis; M Averkiou; S Maji; S Chaudhuri"}, {"ref_id": "b25", "title": "Universal invariant and equivariant graph neural networks. CoRR, abs", "journal": "", "year": "1905", "authors": "N Keriven; G Peyr\u00e9"}, {"ref_id": "b26", "title": "A method for stochastic optimization", "journal": "", "year": "2014", "authors": "D P Kingma; J Ba;  Adam"}, {"ref_id": "b27", "title": "Semi-supervised classification with graph convolutional networks", "journal": "", "year": "2016", "authors": "T N Kipf; M Welling"}, {"ref_id": "b28", "title": "covariant hierarchical neural network architecture for learning atomic potentials", "journal": "", "year": "2018", "authors": "R. N-Body Kondor;  Networks"}, {"ref_id": "b29", "title": "On the generalization of equivariance and convolution in neural networks to the action of compact groups", "journal": "", "year": "2018", "authors": "R Kondor; S Trivedi"}, {"ref_id": "b30", "title": "Covariant compositional networks for learning graphs", "journal": "", "year": "2018", "authors": "R Kondor; H T Son; H Pan; B Anderson; S Trivedi"}, {"ref_id": "b31", "title": "Deep functional maps: Structured prediction for dense shape correspondence", "journal": "", "year": "2017", "authors": "O Litany; T Remez; E Rodol\u00e0; A Bronstein; M Bronstein"}, {"ref_id": "b32", "title": "Permutation-invariant feature restructuring for correlation-aware image set-based recognition", "journal": "", "year": "2019", "authors": "X Liu; Z Guo; S Li; L Kong; P Jia; J You; B Kumar"}, {"ref_id": "b33", "title": "Large-scale celebfaces attributes (celeba) dataset", "journal": "", "year": "2018-08", "authors": "Z Liu; P Luo; X Wang; X Tang"}, {"ref_id": "b34", "title": "A simple proof of the universality of invariant/equivariant graph neural networks", "journal": "", "year": "2019", "authors": "T Maehara; H Nt"}, {"ref_id": "b35", "title": "probably) concave graph matching", "journal": "", "year": "2018", "authors": "H Maron; Y Lipman"}, {"ref_id": "b36", "title": "Provably powerful graph networks", "journal": "", "year": "2019", "authors": "H Maron; H Ben-Hamu; H Serviansky; Y Lipman"}, {"ref_id": "b37", "title": "Invariant and equivariant graph networks", "journal": "", "year": "2019", "authors": "H Maron; H Ben-Hamu; N Shamir; Y Lipman"}, {"ref_id": "b38", "title": "On the universality of invariant networks", "journal": "", "year": "2019", "authors": "H Maron; E Fetaya; N Segol; Y Lipman"}, {"ref_id": "b39", "title": "Higher-order graph neural networks", "journal": "", "year": "2018", "authors": "C Morris; M Ritzert; M Fey; W L Hamilton; J E Lenssen; G Rattan; M Grohe;  Weisfeiler"}, {"ref_id": "b40", "title": "Janossy pooling: Learning deep permutationinvariant functions for variable-size inputs", "journal": "", "year": "2018", "authors": "R L Murphy; B Srinivasan; V Rao; B Ribeiro"}, {"ref_id": "b41", "title": "Automatic differentiation in pytorch", "journal": "", "year": "2017", "authors": "A Paszke; S Gross; S Chintala; G Chanan; E Yang; Z Devito; Z Lin; A Desmaison; L Antiga; A Lerer"}, {"ref_id": "b42", "title": "Algebraic signal processing theory: Foundation and 1-d time", "journal": "IEEE Transactions on Signal Processing", "year": "2008", "authors": "M Puschel; J M Moura"}, {"ref_id": "b43", "title": "Deep learning on point sets for 3d classification and segmentation", "journal": "", "year": "2017", "authors": "C R Qi; H Su; K Mo; L J Guibas;  Pointnet"}, {"ref_id": "b44", "title": "Deep learning with sets and point clouds", "journal": "", "year": "2016", "authors": "S Ravanbakhsh; J Schneider; Poczos ; B "}, {"ref_id": "b45", "title": "Equivariance through parameter-sharing", "journal": "", "year": "2017", "authors": "S Ravanbakhsh; J Schneider; Poczos ; B "}, {"ref_id": "b46", "title": "U-net: Convolutional networks for biomedical image segmentation", "journal": "Springer", "year": "2015", "authors": "O Ronneberger; P Fischer; T Brox"}, {"ref_id": "b47", "title": "On universal equivariant set networks", "journal": "", "year": "2019", "authors": "N Segol; Y Lipman"}, {"ref_id": "b48", "title": "Introduction to topology and modern analysis", "journal": "", "year": "1963", "authors": "G F Simmons"}, {"ref_id": "b49", "title": "A dataset of 101 human actions classes from videos in the wild", "journal": "", "year": "2012", "authors": "K Soomro; A R Zamir; M Shah;  Ucf101"}, {"ref_id": "b50", "title": "Multiview aggregation for learning category-specific shape reconstruction", "journal": "", "year": "2019", "authors": "S Sridhar; D Rempe; J Valentin; S Bouaziz; L J Guibas"}, {"ref_id": "b51", "title": "Multi-view convolutional neural networks for 3d shape recognition", "journal": "", "year": "2015", "authors": "H Su; S Maji; E Kalogerakis; E Learned-Miller"}, {"ref_id": "b52", "title": "Sequence to sequence learning with neural networks", "journal": "", "year": "2014", "authors": "I Sutskever; O Vinyals; Le ; Q "}, {"ref_id": "b53", "title": "Order matters: Sequence to sequence for sets", "journal": "", "year": "2015", "authors": "N Thomas; T Smidt; S Kearnes; L Yang; L Li; K Kohlhoff; P Riley; O Vinyals; S Bengio; M Kudlur"}, {"ref_id": "b54", "title": "On the limitations of representing functions on sets", "journal": "", "year": "2019", "authors": "E Wagstaff; F B Fuchs; M Engelcke; I Posner; Osborne ; M "}, {"ref_id": "b55", "title": "Learning Rotationally Equivariant Features in Volumetric Data", "journal": "", "year": "2018", "authors": "M Weiler; M Geiger; M Welling; W Boomsma; T Cohen;  Steerable Cnns"}, {"ref_id": "b56", "title": "S. 3d g-cnns for pulmonary nodule detection", "journal": "", "year": "2018", "authors": "M Winkels; T Cohen"}, {"ref_id": "b57", "title": "Representation theory and invariant neural networks", "journal": "Discrete applied mathematics", "year": "1996", "authors": "J Wood; J Shawe-Taylor"}, {"ref_id": "b58", "title": "Cubenet: Equivariance to 3d rotation and translation", "journal": "", "year": "2018", "authors": "D Worrall; G Brostow"}, {"ref_id": "b59", "title": "Harmonic networks: Deep translation and rotation equivariance", "journal": "", "year": "2017", "authors": "D E Worrall; S J Garbin; D Turmukhambetov; G J Brostow"}, {"ref_id": "b60", "title": "How powerful are graph neural networks?", "journal": "", "year": "2019", "authors": "K Xu; W Hu; J Leskovec; S Jegelka"}, {"ref_id": "b61", "title": "Universal approximations of invariant maps by neural networks", "journal": "", "year": "2018", "authors": "D Yarotsky"}, {"ref_id": "b62", "title": "Deep sets", "journal": "", "year": "2017", "authors": "M Zaheer; S Kottur; S Ravanbakhsh; B Poczos; R R Salakhutdinov; A J Smola"}, {"ref_id": "b63", "title": "Places: A 10 million image database for scene recognition", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2017", "authors": "B Zhou; A Lapedriza; A Khosla; A Oliva; A Torralba"}], "figures": [{"figure_label": "4", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 4 .4Figure 4. Comparison of set learning methods on the signal classification task. Shaded area represents standard deviation.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 5 .5Figure 5. Shape-selection task on human shape sequences. Shapes are represented as graphs or as point-clouds. The task is to select the central frame (red). Numbers indicate frame order.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 6 .6Figure 6. parameter sharing schemes for (a) G = Sn \u00d7 H and (b) G = \u2295 n i=1 H Sn, where d = 4, n = 5 and H = C4 the cyclic group of four elements. Each color represents a parameter.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 66Figure 6 illustrates parameter sharing schemes for G-equivariant layers for (a) G = S n \u00d7 H and (b) G = H n S n , where d = 4, n = 5 and H = C 4 the cyclic group of four elements. Here, each color represents a parameter. Note that all off-diagonal elements in (b) are represented by the same parameter in contrast to (a). The invariant universality proof of Theorem 2 applies in this case as well.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "large image classification dataset. E.1. Signal classification experiment Data preparation. We generated 30,000 training examples and 3,000 test and validation examples. The type of the signal was uniformly sampled from the three possible types (sine, rectangular and saw-tooth). Frequency and amplitude were uniformly sampled from [1, 10], horizontal shift was uniformly sampled from [0, 2\u03c0], vertical shift was sampled from [\u22125, 5].", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "details the accuracy of all compared methods in this task, showing that DSS(sum) and DSS(Aittala) outperform Siamese+DS and DSS(Sridhar) by a large margin.In a second selection task, we demonstrate that DSS can", "figure_data": "DatasetData typeLate Aggregation Siamese+DSDSS (sum)Early Aggregation DSS (max) DSS (Sridhar)DSS (Aittala)Random choiceUCF101Images36.41% \u00b1 1.4376.6% \u00b1 1.5176.39% \u00b1 1.0160.15% \u00b1 0.7677.96% \u00b1 1.6912.5%Dynamic FaustPoint-clouds22.26% \u00b1 0.6442.45% \u00b1 1.3228.71% \u00b1 0.6454.26% \u00b1 1.6626.43% \u00b1 3.9214.28%Dynamic FaustGraphs26.53% \u00b1 1.9944.24% \u00b1 1.2830.54% \u00b1 1.2753.16% \u00b1 1.4726.66% \u00b1 4.2514.28%"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Frame selection tasks for images, point-clouds and graphs. Numbers represent average classification accuracy.", "figure_data": "Noise type and strengthLate Aggregation Siamese+DSDSS (sum)Early Aggregation DSS (max) DSS (Sridahr)DSS (Aittala)Random choiceGaussian \u03c3 = 1077.2% \u00b1 0.3778.48% \u00b1 0.4877.99% \u00b1 1.176.8% \u00b1 0.2578.34% \u00b1 0.495%Gaussian \u03c3 = 3065.89% \u00b1 0.6668.35% \u00b1 0.5567.85% \u00b1 0.4061.52% \u00b1 0.5466.89% \u00b1 0.585%Gaussian \u03c3 = 5059.24% \u00b1 0.5162.6% \u00b1 0.4561.59% \u00b1 1.0055.25% \u00b1 0.4062.02% \u00b1 1.035%Occlusion 10%82.15% \u00b1 0.4583.13%\u00b1 1.0083.27 \u00b1 0.5183.21% \u00b1 0.33883.19% \u00b1 0.675%Occlusion 30%77.47% \u00b1 0.3778% \u00b1 0.8978.69% \u00b1 0.3278.71% \u00b1 0.2678.27% \u00b1 0.675%Occlusion 50%76.2% \u00b1 0.8277.29% \u00b1 0.4076.64% \u00b1 0.4577.04% \u00b1 0.7577.03% \u00b1 0.585%"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Highest-quality image selection. Values indicate the mean accuracy.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Color-channel matching and burst deblurring tasks. Values indicate mean absolute error per pixel over the test set where the pixel values are in [0, 255]. TP stands for the trivial grey-scale predictor.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "A function is called G-equivariant if f (g \u2022 x) = g \u2022 f (x) for all g \u2208 G. Similarly, a function f is called G-invariant if f (g \u2022 x) = f (x) for all g \u2208 G.", "formula_coordinates": [3.0, 55.44, 106.08, 234.0, 32.87]}, {"formula_id": "formula_1", "formula_text": "f = L k \u2022 \u03c3 \u2022 L k\u22121 \u2022 \u2022 \u2022 \u2022 \u03c3 \u2022 L 1 ,(1)", "formula_coordinates": [3.0, 107.52, 229.14, 181.92, 9.65]}, {"formula_id": "formula_2", "formula_text": "g = m \u2022 \u03c3 \u2022 h \u2022 \u03c3 \u2022 f ,(2)", "formula_coordinates": [3.0, 125.94, 339.86, 163.5, 8.96]}, {"formula_id": "formula_3", "formula_text": "R \u00d7d k+1 \u2192 R d k+2 is a linear G-invariant layer and m : R d k+2 \u2192 R d k+3 is an MLP.", "formula_coordinates": [3.0, 55.44, 363.75, 234.17, 22.83]}, {"formula_id": "formula_4", "formula_text": "L(g \u2022 x) = g \u2022 L(x), x \u2208 R , g \u2208 G.(3)", "formula_coordinates": [3.0, 97.96, 613.91, 191.48, 9.3]}, {"formula_id": "formula_5", "formula_text": "g \u2022 L = L, g \u2208 G,(4)", "formula_coordinates": [3.0, 135.46, 676.81, 153.98, 8.96]}, {"formula_id": "formula_6", "formula_text": "L(X) i = L 1 (x i ) + L 2 \uf8eb \uf8ed n j =i x j \uf8f6 \uf8f8 ,(5)", "formula_coordinates": [3.0, 353.91, 323.07, 187.53, 33.76]}, {"formula_id": "formula_7", "formula_text": "L 1 , L 2 : R d \u2192 R d are", "formula_coordinates": [3.0, 334.56, 369.88, 90.71, 11.23]}, {"formula_id": "formula_8", "formula_text": "Theorem 1. Any linear G\u2212equivariant layer L : R n\u00d7d \u2192 R n\u00d7d is of the form L(X) i = L H 1 (x i ) + L H 2 \uf8eb \uf8ed n j =i x j \uf8f6 \uf8f8 ,", "formula_coordinates": [4.0, 55.11, 517.28, 234.33, 67.85]}, {"formula_id": "formula_9", "formula_text": "L H 1 , L H 2 are linear H-equivariant functions", "formula_coordinates": [4.0, 81.91, 598.94, 174.65, 12.2]}, {"formula_id": "formula_10", "formula_text": ") i = L H 1 (x i ) + L H 2 ( n j=1 x j ) = L H 1 (x i ) + n j=1 L H 2 (x j", "formula_coordinates": [4.0, 65.96, 631.79, 225.41, 27.84]}, {"formula_id": "formula_11", "formula_text": "Lemma 1. Let G \u2264 S , then the dimension of the space of G-equivariant linear functions L : R \u2192 R is E(G) = 1 |G| g\u2208G tr(P (g)) 2 ,", "formula_coordinates": [4.0, 307.44, 165.2, 234.0, 61.07]}, {"formula_id": "formula_12", "formula_text": "L(X) i = L H 1 (x i ) + L H 2 ( n j =i x j ).", "formula_coordinates": [4.0, 307.44, 343.89, 143.7, 14.11]}, {"formula_id": "formula_13", "formula_text": "E(G) = 1 |G| g\u2208G tr(P (g)) 2 = = 1 |H| 1 n! q\u2208Sn h\u2208H tr(P (q) \u2297 P (h)) 2 = 1 |H| 1 n! q\u2208Sn h\u2208H tr(P (q)) 2 tr(P (h)) 2 = 1 |H| h\u2208H tr(P (h)) 2 \u2022 \uf8eb \uf8ed 1 n! q\u2208Sn tr(P (q)) 2 \uf8f6 \uf8f8 = E(H)E(S n ) = 2E(H).", "formula_coordinates": [4.0, 311.27, 440.93, 226.35, 145.66]}, {"formula_id": "formula_14", "formula_text": "L | L(X) i = L H 1 (x i ) + L H 2 ( n j =i x j )", "formula_coordinates": [4.0, 314.08, 667.1, 155.81, 14.11]}, {"formula_id": "formula_15", "formula_text": "L : R n\u00d7d \u2192 R, L : R n\u00d7d \u2192 R n and L : R n\u00d7d \u2192 R d .", "formula_coordinates": [5.0, 55.44, 251.67, 235.25, 22.83]}, {"formula_id": "formula_16", "formula_text": "K \u2229 E = \u2205. G-invariant networks are universal approximators (in \u2022 \u221e sense) of continuous G-invariant functions on K if H-invariant networks are universal 1 .", "formula_coordinates": [5.0, 307.44, 141.34, 234.0, 44.59]}, {"formula_id": "formula_17", "formula_text": "n j=1 x j with a unique H-invariant polynomial descriptor u H (x i , n j=1 x j ) \u2208 R l H (", "formula_coordinates": [5.0, 307.44, 346.89, 234.0, 39.42]}, {"formula_id": "formula_18", "formula_text": "Theorem 3. Let K \u2282 R n\u00d7d be a compact domain such that K = \u222a g\u2208G gK and K \u2229 E = \u2205. G-equivariant net- works are universal approximators (in \u2022 \u221e sense) of continuous R n\u00d7d \u2192 R n\u00d7d G-equivariant functions on K if H-equivariant networks are universal.", "formula_coordinates": [6.0, 55.11, 554.93, 235.98, 58.13]}, {"formula_id": "formula_19", "formula_text": "L(x) i \u2192 [L H (x i ), max n j=1 L H (x j )]", "formula_coordinates": [7.0, 55.44, 404.54, 147.49, 12.32]}, {"formula_id": "formula_20", "formula_text": ",i.e., L(x) i \u2192 L H (x i ) \u2212 1 n n j=1 L H (x j ).", "formula_coordinates": [7.0, 56.64, 441.78, 232.8, 25.12]}, {"formula_id": "formula_21", "formula_text": "L(X) i = L 1 (x i ) + L 2 ( n j =i x j ),", "formula_coordinates": [12.0, 234.41, 243.75, 128.06, 30.55]}, {"formula_id": "formula_22", "formula_text": "Theorem 5. 1. Any linear G\u2212equivariant layer L : R n\u00d7d \u2192 R n is of the form L(X) i = L H 1 (x i ) + L H 2 ( n j =i x j ), where L H i , i = 1, 2 are linear H-invariant functions. 2. Any linear G\u2212equivariant layer L : R n\u00d7d \u2192 R d is of the form L(X) = L H ( n j=1 x j )", "formula_coordinates": [12.0, 55.44, 354.78, 487.74, 47.51]}, {"formula_id": "formula_23", "formula_text": "L : R n\u00d7d \u2192 R is of the form L(X) = L H ( n j=1 x j )", "formula_coordinates": [12.0, 193.58, 419.8, 213.77, 14.11]}, {"formula_id": "formula_24", "formula_text": "Theorem 6. Let H 1 \u2264 S n , H 2 \u2264 S d and {L j i } E(Hj ) i=1 , j = 1, 2 are bases for the spaces of linear H j -equivariant maps. Let G = H 1 \u00d7 H 2 act on R n\u00d7d by multiplication, (h 1 , h 2 ) \u2022 X := h 1 Xh T 2 .", "formula_coordinates": [12.0, 55.11, 550.62, 486.32, 25.94]}, {"formula_id": "formula_25", "formula_text": "T i1,i2 = L 1 i1 \u2297 L 2 i2 , i 1 = 1, . . . , E(H1), i 2 = 1, . . . , E(H 2 )", "formula_coordinates": [12.0, 172.63, 597.1, 251.63, 12.69]}, {"formula_id": "formula_26", "formula_text": "T i1,i2 (X) = L 1 i1 XL 2 T i2(6)", "formula_coordinates": [12.0, 252.47, 686.98, 288.97, 14.34]}, {"formula_id": "formula_27", "formula_text": "((h 1 , . . . , h n , \u03c3) \u2022 X) ij = X \u03c3 \u22121 (i),h \u22121 i (j)", "formula_coordinates": [13.0, 54.28, 378.81, 163.71, 13.16]}, {"formula_id": "formula_28", "formula_text": "L(X) i = L 1 (x i ) + \u03b2 \uf8eb \uf8ed n j=1 d k=1 x jk \uf8f6 \uf8f8 .", "formula_coordinates": [13.0, 220.18, 429.03, 156.53, 33.76]}, {"formula_id": "formula_29", "formula_text": "E(G) = tr(\u03c6) = 1 |G| g\u2208G tr(P (g) \u2297 P (g)) = 1 |G| g\u2208G tr(P (g)) 2 ,", "formula_coordinates": [14.0, 212.48, 189.88, 171.92, 58.21]}, {"formula_id": "formula_30", "formula_text": "p i : R d \u2192 R l i=1", "formula_coordinates": [14.0, 60.01, 340.74, 68.64, 16.11]}, {"formula_id": "formula_31", "formula_text": "R d \u2192 R such that f | G\u2022x \u2264 \u22122 and f | G\u2022y \u2265 2.", "formula_coordinates": [14.0, 194.7, 393.83, 191.33, 11.23]}, {"formula_id": "formula_32", "formula_text": "\u2022x \u2264 \u22121 and p| G\u2022y \u2265 1. Define p = 1 |G| g\u2208G p(g \u2022 x)", "formula_coordinates": [14.0, 320.88, 405.48, 221.72, 13.47]}, {"formula_id": "formula_33", "formula_text": "1 \u2264 p(y) = q(u(y)) = q(u(x)) = p(x) \u2264 \u22121", "formula_coordinates": [14.0, 206.67, 457.18, 183.53, 8.74]}, {"formula_id": "formula_34", "formula_text": "E = \u222a d j>k=1 { n i=1 x ik = n i=1 x ij }.", "formula_coordinates": [14.0, 302.35, 518.56, 154.0, 14.11]}, {"formula_id": "formula_35", "formula_text": "x i := [x i , n j=1", "formula_coordinates": [14.0, 154.55, 597.21, 65.57, 14.11]}, {"formula_id": "formula_36", "formula_text": "U H (X) i,j,: = u H ( x i ), j = 1, . . . , d", "formula_coordinates": [14.0, 222.1, 637.21, 152.67, 9.65]}, {"formula_id": "formula_37", "formula_text": "U Sn (Y ) = 1 d n i=1 d j=1 p(Y i,j,: )", "formula_coordinates": [15.0, 238.66, 93.13, 119.57, 30.32]}, {"formula_id": "formula_38", "formula_text": "r = f \u2022 (U Sn \u2022 U H ) \u22121", "formula_coordinates": [15.0, 449.51, 202.57, 92.23, 11.23]}, {"formula_id": "formula_39", "formula_text": "n i=1 M 1 (y i ) approximates u Sn ({y i } n i=1", "formula_coordinates": [15.0, 248.41, 398.43, 156.72, 14.11]}, {"formula_id": "formula_40", "formula_text": "1 d n i=1 d j=1 M 1 (y i ) as output.", "formula_coordinates": [15.0, 56.64, 423.39, 131.21, 14.56]}, {"formula_id": "formula_41", "formula_text": "Lemma 4. Let K \u2282 R n\u00d7d such that K \u2229 E = \u2205 then the polynomial mapping u: X \u2192 U Sn (Y ) is G-invariant and injective as function from R n\u00d7d /G.", "formula_coordinates": [15.0, 55.44, 571.38, 486.0, 22.82]}, {"formula_id": "formula_42", "formula_text": "[h \u03c3(1) x \u03c3(1) , h \u03c3(1) y], ..., [h \u03c3(n) x \u03c3(n) , h \u03c3(n) y]", "formula_coordinates": [15.0, 209.36, 658.63, 178.15, 9.96]}, {"formula_id": "formula_43", "formula_text": "[h \u03c3(1) x \u03c3(1) , h \u03c3(1) y], [h \u03c3(1) x \u03c3(2) , h \u03c3(1) y], ..., [h \u03c3(1) x \u03c3(n) , h \u03c3(", "formula_coordinates": [16.0, 55.44, 70.22, 243.7, 9.96]}, {"formula_id": "formula_44", "formula_text": "f (x i ) = h(g(x i )) = h(y i ) h(y 0 ) = h(g(x 0 )) = f (x 0 )", "formula_coordinates": [16.0, 182.64, 192.54, 231.6, 9.65]}, {"formula_id": "formula_45", "formula_text": "x i := [x i , n i=1 x i , n i=1 x 2 i , . . . , n i=1 x n i ]", "formula_coordinates": [16.0, 166.37, 254.11, 177.64, 14.11]}, {"formula_id": "formula_46", "formula_text": "n i=1 x k i has d different numbers. Here, we can define E = \u2229 n l=1 \u222a d j>k=1 { n i=1 x l ik = n i=1 x l ij }.", "formula_coordinates": [16.0, 55.08, 266.06, 487.6, 26.07]}, {"formula_id": "formula_47", "formula_text": "E = \u222a n l=1 \u222a d j>k=1 { n i =l x ik = n i =l x ij } .", "formula_coordinates": [16.0, 364.26, 333.76, 178.92, 14.11]}, {"formula_id": "formula_48", "formula_text": "x n )] i = k \u03b1 k (x 1 , ...x i\u22121 , x i+1 , x n ) \u2022 q k (x i", "formula_coordinates": [16.0, 301.28, 522.95, 180.2, 11.14]}], "doi": ""}