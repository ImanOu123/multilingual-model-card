{"title": "MULTIINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning", "authors": "Zhiyang Xu; Ying Shen; Lifu Huang", "pub_date": "", "abstract": "Instruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks. However, it has yet to be explored for vision and multimodal tasks. In this work, we introduce MUL-TIINSTRUCT, the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks in a unified seq-toseq format covering 10 broad categories. The tasks are derived from 21 existing open-source datasets and each task is equipped with 5 expertwritten instructions. We take OFA (Wang et al., 2022a) as the base pre-trained model for multimodal instruction tuning, and to further improve its zero-shot performance, we explore multiple transfer learning strategies to leverage the large-scale NATURAL INSTRUCTIONS dataset (Mishra et al., 2022). Experimental results demonstrate strong zero-shot performance on various unseen multimodal tasks and the benefit of transfer learning from a text-only instruction dataset. We also design a new evaluation metric -Sensitivity, to evaluate how sensitive the model is to the variety of instructions. Our results indicate that fine-tuning the model on a diverse set of tasks and instructions leads to a reduced sensitivity to variations in instructions for each task 1 .", "sections": [{"heading": "Introduction", "text": "With the advances in large-scale pre-trained language models (PLMs), recent studies have explored various efficient learning paradigms (Brown et al., 2020;Liu et al., 2021;Wei et al., 2021;Xie et al., 2021) to generalize PLMs to new tasks without task-specific tuning. Among these, instruction tuning (Wei et al., 2021) has achieved significant success in zero-shot learning on natural language processing tasks. By fine-tuning a PLM on tasks described through instructions, instruction tuning allows the model to learn to understand and follow the instructions to perform predictions on unseen tasks. Recent advancement in multimodal pretraining Alayrac et al., 2022;Bao et al., 2022;Wang et al., 2022c) has shown the potential of jointly interpreting text and images in a shared semantic space, which further leads us to ask: can the instruction tuning be leveraged to improve the generalizability of Vision-Language pretrained models on multi-modal and vision tasks?\nIn this work, we propose MULTIINSTRUCT, the first benchmark dataset for multimodal instruction tuning with 62 diverse tasks from 10 broad categories, including Visual Question Answering (Goyal et al., 2017;Suhr et al., 2017), Commonsense Reasoning (Zellers et al., 2019;Xie et al., 2019), Visual Relationship Understanding (Krishna et al., 2017) and so on. We equipped each task with 5 instructions that are written by two experts in natural language processing. As shown in Figure 1, we formulate all the tasks into a unified sequence-to-sequence format in which the input text, images, instructions, and bounding boxes are represented in the same token space.\nWe use OFA  2 , a unified model that is pre-trained on a diverse set of multimodal and unimodal tasks in a single Transformerbased sequence-to-sequence framework, as the base pre-trained multimodal language model, and fine-tune it on MULTIINSTRUCT. To utilize NATU-RAL INSTRUCTIONS (Mishra et al., 2022), a largescale text-only instruction tuning dataset, we further explore two transfer learning strategies, in-Grounded Caption", "publication_ref": ["b4", "b25", "b47", "b2", "b3", "b42", "b10", "b37", "b51", "b46", "b17", "b29"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Text Localization Referring Expression Selection", "text": "Output: blue and white tennis racquet", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Input:", "text": "Generate a caption for <bin_198> <bin_32> <bin_400> <bin_193>.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Input:", "text": "Select the region that contains the text \"den\". Options: <bin_206> <bin_119> <bin_448> <bin_181> ||||<bin_357> <bin_518> <bin_456> <bin_574> ||||<bin_229> <bin_604> <bin_304> <bin_654>", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Input:", "text": "Select the region of the object described by \"A blue train in the front.\". Options: <bin_242> <bin_180> <bin_736> <bin_475> |||| <bin_88> <bin_291> <bin_203> <bin_473>|||| <bin_193> <bin_339> <bin_247> <bin_442>\nOutput: <bin_229> <bin_604> <bin_304> <bin_654>\nOutput: <bin_242> <bin_180> <bin_736> <bin_475>\nQuestion-Image Matching", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Output:", "text": "the question is irrelevant to the image", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Input:", "text": "Given the content of image, do you have enough information to answer \"Is it a sunny day?\"? Options: \"the question is relevant to the image\" or \"the question is irrelevant to the image\" cluding Mixed Instruction Tuning and Sequential Instruction Tuning. Experimental results demonstrate strong zero-shot performance on various unseen multimodal tasks with instruction tuning and the potential of further improving it by leveraging large-scale text-only instruction datasets.\nAs suggested by previous studies (Webson and Pavlick, 2022;Liu et al., 2022b), PLMs are highly sensitive toward the wording and length of instructions. Thus, we propose a new metric -Sensitivity, which measures how sensitive the model is toward the variety of instructions for the same task. Experimental results demonstrate that (1) instruction tuning significantly reduces the sensitivity of OFA to the varying wording of instructions. The more tuning tasks and instructions for each task are introduced, the lower sensitivity tends to be achieved, and (2) transferring from a larger text-only instruction dataset can also significantly reduces the sensitivity of OFA.", "publication_ref": ["b44", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Multimodal Pretraining Multimodal pretraining (Tan and Bansal, 2019;Cho et al., 2021;Singh et al., 2022;Alayrac et al., 2022;Li et al., 2022b,a) has significantly advanced the vision-language tasks. Several recent studies (Cho et al., 2021;Wang et al., 2022a,c;Lu et al., 2022) also started to build a unified pre-training framework to handle a diverse set of cross-modal and unimodal tasks. Among them, VL-T5 (Cho et al., 2021) tackles vision-and-language tasks with a unified text-generation objective conditioned on multimodal inputs, while OFA  further extends it to image generation tasks by using a unified vocabulary for all text and visual tokens. BEIT-3 (Wang et al., 2022c) utilizes a novel shared Multiway Transformer network with a shared self-attention module to align different modalities and provide deep fusion. Building on the success of multimodal pretraining, our work focuses on improving the generalization and zeroshot performance on various unseen multimodal tasks through instruction tuning.\nEfficient Language Model Tuning To improve the generalizability and adaptivity of large-scale pre-trained language models, various efficient language model tuning strategies have been proposed recently. Prompt tuning (Liu et al., 2021;Li and Liang, 2021;Sanh et al., 2022) aims to learn a task-specific prompt by reformulating the downstream tasks to the format that the model was initially trained on and has shown competitive performance across various natural language processing applications. As a special form of prompt tuning, in-context learning (Xie et al., 2021;Min et al., 2021) takes one or a few examples as the prompt to demonstrate the task. Instruction tuning (Wei et al., 2021) is another simple yet effective strategy to improve the generalizability of large language models. NATURAL IN-STRUCTIONS (Mishra et al., 2022) is a meta-dataset containing diverse tasks with human-authored definitions, things to avoid, and demonstrations. It has shown effectiveness in improving the generalizability of language models even when the size is relatively small (e.g., BART_base) (Mishra et al., 2022;Wang et al., 2022d). InstructDial (Gupta et al., 2022) applies instruction tuning to the dialogue domain and shows significant zero-shot performance on unseen dialogue tasks. While these studies have been successful in text-only domains, it has not yet been extensively explored for vision or multimodal tasks.", "publication_ref": ["b38", "b6", "b34", "b2", "b6", "b26", "b6", "b25", "b20", "b32", "b47", "b28", "b29", "b29", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "MULTIINSTRUCT", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Multimodal Task and Data Collection", "text": "The MULTIINSTRUCT dataset is designed to cover a wide range of multimodal tasks that require reasoning among regions, images, and text. These tasks are meant to teach machine learning models to perform various tasks such as object recognition, visual relationship understanding, text-image grounding, and so on by following instructions so that they can perform zero-shot prediction on unseen tasks. To build MULTIINSTRUCT, we first collect 34 tasks from the existing studies in visual and multimodal learning, covering Visual Question Answering (Goyal et al., 2017;Krishna et al., 2017;Zhu et al., 2016;Hudson and Manning, 2019;Singh et al., 2019;Marino et al., 2019), Commonsense Reasoning (Suhr et al., 2017;Liu et al., 2022a;Zellers et al., 2019;Xie et al., 2019), Region Understanding (Krishna et al., 2017), Image Understanding (Kafle and Kanan, 2017;Chiu et al., 2020), Grounded Generation (Krishna et al., 2017;Yu et al., 2016;Lin et al., 2014), Image-Text Matching (Lin et al., 2014;Goyal et al., 2017), Grounded Matching (Krishna et al., 2017;Veit et al., 2016;Yu et al., 2016), Visual Relationship (Krishna et al., 2017;Pham et al., 2021), Temporal Ordering tasks that are created from WikiHow 3 , and Miscellaneous (Yao et al., 2022;Kiela et al., 2020;Das et al., 2017;Lin et al., 2014;Veit et al., 2016;Alam et al., 2022). Each of the 34 tasks can be found with one or multiple open-source datasets, which are incorporated into MULTIINSTRUCT. Details of each task and their corresponding datasets are shown in Tables 7 to 9 in Appendix.\nFor each of these tasks, we further examine the possibility of deriving new tasks based on the input and output of the original task to augment the task repository. For example, Visual Grounding requires the model to generate a caption for a given region in the image. We derive two additional tasks from it: Grounded Caption Selection, which is a simpler task that requires the model to select the 3 https://www.wikihow.com. corresponding caption from multiple candidates for the given region, and Visual Grounding Selection, which requires the model to select the corresponding region from the provided candidate regions based on a given caption. Compared with Visual Grounding, these two new tasks require different skills based on distinct input and output information. In this way, we further derived 28 new tasks from the 34 existing tasks. We divide all 62 tasks into 10 broad categories as shown in Figure 2.\nFor the existing tasks, we use their available open-source datasets to create instances (i.e., input and output pairs) while for each new task, we create its instances by extracting the necessary information from instances of existing tasks or reformulating them. Each new task is created with 5,000 to 5M instances. We split the 62 tasks into training and evaluation based on the following criteria: (1) we take the tasks that are similar to the pre-training tasks of OFA  for training; and\n(2) we select the challenging multimodal tasks that do not overlap with the training tasks for evaluation. Table 5 and Table 6 in Appendix A show the detailed statistics for the training and evaluation tasks in MULTIINSTRUCT and Tables 7 to 9 show their corresponding datasets.", "publication_ref": ["b10", "b17", "b13", "b35", "b27", "b37", "b23", "b51", "b46", "b17", "b15", "b5", "b17", "b50", "b22", "b22", "b10", "b17", "b39", "b50", "b17", "b31", "b48", "b7", "b22", "b39", "b1"], "figure_ref": ["fig_2"], "table_ref": ["tab_15", "tab_12", "tab_13", "tab_15"]}, {"heading": "Task Instruction Creation", "text": "We first provide a definition for \"instruction\" used in MULTIINSTRUCT. An instruction is defined with a template that describes how the task should be performed and contains an arbitrary number of placeholders, including <TEXT>, <REGION> and <OPTION>, for the input information from the original task. For example, in the instruction of the Grounded Captioning task, \"Generate a caption for <REGION>\", <REGION> is the placeholder for region-specific information. Note that the placeholder <OPTION> is only used in classification tasks and for some tasks, the input may also include an image that is not included in the instruction and will be fed as a separate input to the model. To produce high-quality instructions that accurately convey the intended tasks, we employ an iterative annotation process involving two expert annotators who have a thorough understanding of the task and the dataset.\nStep 1: each annotator first writes 2-3 instructions for each task by giving them the specific goals of  this task, the format of input data, and 10 example instances randomly sampled from the dataset. The information about the dataset is obtained from the dataset's README file or the publication that introduced the dataset. For newly derived tasks, we provide annotators with task descriptions along with 10 constructed example instances.\nStep 2: to guarantee the quality of the instructions and that they effectively convey the intended tasks, we have each annotator review the instructions created by their peers, checking if they can clearly understand and identify the intended task by just reading the instruction. If any issues are identified, the reviewing annotator provides suggestions and works with the original annotator to revise the instructions.\nStep 3: to ensure the consistency and avoid conflicts or repetition among instructions from different annotators, we have both annotators review the sets of instructions together, identifying any discrepancies or inconsistencies. If any are found, the annotators collaborate to resolve them and create a final set of instructions that accurately and clearly describe the task. In this way, each task will be created with 5 high-quality instructions.\nStep 4: we repeat steps 1-3 to create 5 instructions for each of the training and evaluation tasks. Finally, both annotators review each task and its instructions and filter out the task that is not repre-sentative or overlaps with other tasks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Multimodal Instruction Formatting", "text": "To unify the processing of various input/output data types, we follow the method from OFA , which involves representing images, text, and bounding box coordinates as tokens in a unified vocabulary. Specifically, we apply bytepair encoding (BPE) (Sennrich et al., 2016) to encode the text input. For the target image, we apply VQ-GAN (Esser et al., 2021) to generate discrete image tokens through image quantization.\nTo represent regions or bounding boxes of an image, we discretize the four corner coordinates into location tokens such as \"<bin_242> <bin_180> <bin_736> <bin_475>\" where each location token \"<bin_NUM>\" represents a quantized coordinate obtained by dividing the image into 1,000 bins. This approach allows us to convert different types of input into a unified vocabulary. All tasks in MULTIINSTRUCT can then be formulated as natural language sequence-to-sequence generation problems, where the input includes: (1) an image (if there is no input image, a black picture is used as the input); and (2) an instruction where the placeholders such as <TEXT>, <REGION> or <OPTION> are filled with specific information of each input instance. Notably, for the <OPTION> of the instructions for classification tasks, we intro-duce two special tokens for this field: \"[Options]\" to mark the beginning of the option field and \"||||\" to delimit the given options. We concatenate all the options with \"||||\" in the option field and the model will directly generate one option from them. Figure 1 provides several examples of the formulated input and illustrates how the original data input is combined with the instruction in the MULTIINSTRUCT.", "publication_ref": ["b33", "b8"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Problem Setup and Models", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Problem Setup", "text": "We follow the same instruction tuning setting as the previous study (Wei et al., 2021) and mainly evaluate the zero-shot learning capabilities of the finetuned large language models. Specifically, given a pre-trained multimodal language model M , we aim to finetune it on a collection of instruction tasks T . Each task t \u2208 T is associated with a number of training instances\nD t = {(I t , x t j , y t j ) \u2208 I t \u00d7 X t \u00d7 Y t } N j=1\n, where x t j denotes the input text, image, region, and options if provided, y t j denotes the output of each instance, and I t represents the set of five task instructions written by experts. The input information from x t j will be used to fill in the placeholders in the instruction.\nWe use OFA  as the pretrained multimodal model due to its unified architecture and flexible input-output modalities. We finetune it on our MULTIINSTRUCT dataset to demonstrate the effectiveness of instruction tuning. Specifically, we use the transformer-based encoder of OFA to encode the instruction along with all necessary information and an optional image, and predict the output with the transformer-based decoder. Given that the training dataset contains many tasks, we mix all the training instances from these tasks and randomly shuffle them. For each instance, we also randomly sample an instruction template for each batch-based training. Note that, though some of the training tasks in MULTIINSTRUCT are similar to the pre-training tasks of OFA 4 , we ensure that the evaluation tasks in MULTIINSTRUCT do not overlap with either the pre-training tasks in OFA nor the training tasks in MULTIINSTRUCT. 4 ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Transfer Learning from NATURAL INSTRUCTIONS", "text": "We notice that the scale of NATURAL INSTRUC-TIONS (Mishra et al., 2022) is significantly larger than MULTIINSTRUCT, indicating the potential of transferring the instruction learning capability from the larger set of natural language tasks to multimodal tasks. We take 832 English tasks in NAT-URAL INSTRUCTIONS and explore several simple transfer-learning strategies:\nMixed Instruction Tuning (OFA MixedInstruct )\nWe combine the instances of NATURAL INSTRUC-TIONS and MULTIINSTRUCT and randomly shuffle them before finetuning OFA with instructions.\nNote that, each task in NATURAL INSTRUCTIONS is just associated with one instruction while for each instance from MULTIINSTRUCT, we always randomly sample one instruction from the five instructions for each instance of training.", "publication_ref": ["b29"], "figure_ref": [], "table_ref": []}, {"heading": "Sequential Instruction Tuning (OFA SeqInstruct )", "text": "Inspired by the Pre-Finetuning approach discussed in Aghajanyan et al. (2021), we propose a twostage sequential instruction tuning strategy where we first fine-tune OFA on the NATURAL INSTRUC-TIONS dataset to encourage the model to follow instructions to perform language-only tasks, and then further fine-tune it on MULTIINSTRUCT to adapt the instruction learning capability to multimodal tasks. To maximize the effectiveness of the NATURAL INSTRUCTIONS dataset, we use all instances in English-language tasks to tune the model in the first training stage.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "Evaluation Metrics We report the accuracy for classification tasks and ROUGE-L (Lin, 2004) for all generation tasks. For the region classification task, we compute the Intersection over Union (IoU) between the generated region and all regions in the options, select the option with the highest IoU as the prediction, and compute accuracy based on this prediction. If the predicted region has no intersection with any of the regions in the options, we treat this prediction as incorrect. For classification tasks where the answer is not a single-word binary classification, we also report ROUGE-L scores following Mishra et al. (2022), which treats all tasks as text generation problems. For each task, we conduct five experiments by evaluating the model using one of the five instructions in each experiment. We re-port the mean and maximum performance and the standard deviation of the performance across all five experiments. We also compute the aggregated performance for each model based on the mean of the model's performance on all multimodal and NLP unseen tasks. We use Rouge-L as the evaluation metric for most tasks and accuracy for tasks that only have accuracy as a metric.\nIn addition, as instruction tuning mainly relies on the instructions to guide the model to perform prediction on various unseen multimodal tasks, we further propose to evaluate how sensitive the model is to the variety of human-written instructions in the same task, which has not been discussed in previous instruction tuning studies but is necessary to understand the effectiveness of instruction tuning. We thus further design a new metric as follows: Sensitivity refers to the model's capability of consistently producing the same results, regardless of slight variations in the wording of instructions, as long as the intended task remains the same. Specifically, for each task t \u2208 T , given its associated instances with task instructions: D t = {(I t , x t j , y t j ) \u2208 I t \u00d7 X t \u00d7 Y t } N j=1 , we formally define sensitivity as:\nE t\u2208T \u03c3 i\u2208I t E (x,y)\u2208D t [L(f \u03b8 (i, x), y)] \u00b5 i\u2208I t E (x,y)\u2208D t [L(f \u03b8 (i, x), y)]\nwhere L denotes the evaluation metric such as accuracy or ROUGE-L, f \u03b8 (\u2022) represents the multimodal instruction-tuned model. 6 Results and Discussion", "publication_ref": ["b21", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Effectiveness of Instruction Tuning on MULTIINSTRUCT", "text": "We evaluate the zero-shot performance of various approaches on all the unseen evaluation tasks, as shown in Table 1 and 2    the potential benefits of the much larger text-only instruction datasets to multimodal instruction tuning.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Impact of Increasing Multimodal Instruction Task Clusters", "text": "To evaluate the impact of the number of tasks clusters for instruction tuning, we start with the task groups shown in Figure 2   (5) Region (Region Understanding), together with (6) NLP, a collection of NLP tasks from NATU-RAL INSTRUCTIONS. We measure the change in both the aggregated performance and sensitivity of OFA MixedInstruct as we gradually add the task clusters for training.\nAs we increase the number of task clusters, we observe an improvement in both the mean and maximum aggregated performance and a decrease in sensitivity, as shown in Figure 3. Note that low sensitivity indicates that the model can produce consistent results despite variations in the wording of instructions. These results suggest that increasing the number of task clusters improves the model's performance on unseen tasks and leads to more consistent outputs. The results also support the effectiveness of our proposed MULTIINSTRUCT dataset.", "publication_ref": [], "figure_ref": ["fig_2", "fig_3"], "table_ref": []}, {"heading": "Effect of Diverse Instructions on Instruction Tuning", "text": "We hypothesize that using a diverse set of instructions for each task during multimodal instruction tuning can improve the model's zero-shot performance on unseen tasks and reduce its sensitivity to variation in the instructions. To test this hypothesis, we train an OFA model on MULTIINSTRUCT with a single fixed instruction template per task and compare its performance with OFA finetuned on 5 different instructions. As shown in Table 3, OFA finetuned on 5 instructions achieves much higher aggregated performance on all evaluation tasks and shows lower sensitivity. These results demonstrate the effectiveness of increasing the diversity of instructions and suggest that future work could explore crowd-sourcing or automatic generation strategies to create even more diverse instructions for instruction tuning.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Effect of Fine-tuning Strategies on Model Sensitivity", "text": "In Section 6.3 and 6.4, we have shown that the more tasks and instructions used for instruction tuning, the lower sensitivity the model will achieve toward the variations in instructions for each task.\nWe further investigate the impact of fine-tuning and transfer learning strategies on model sensitivity. Figure 4 shows the averaged sensitivity of each model across all multimodal unseen tasks. The original OFA exhibits significantly higher sensitivity to variations in instructions compared to models fine-tuned on instruction datasets, indicating that multimodal instruction tuning significantly improves the model's capability on interpreting instructions, even with varying wordings. In addition, by transferring the large-scale NATURAL INSTRUC-TIONS dataset to MULTIINSTRUCT, sensitivity is also reduced by a large margin, highlighting the benefit of fine-tuning the model on a larger instruction dataset, regardless of different formats and modalities.\n7 Zero-Shot Performance on NLP Tasks  to OFA SeqInstruct . Based on the results in Tables 1, 2 and 4, we conclude that OFA MixedInstruct is able to achieve overall best aggregated performance on all multimodal and NLP tasks and shows much lower sensitivity towards variations in the wording of instructions, making it the most promising approach.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We present a new large-scale multi-modal instruction tuning benchmark dataset -MULTIINSTRUCT, which covers a wide variety of vision and multimodal tasks while each task is associated with multiple expert-written instructions. By finetuning OFA , a recently state-ofthe-art multimodal pre-trained language model, on MULTIINSTRUCT with instruction tuning, its zeroshot performance on various unseen multimodal tasks is significantly improved. We also explore several transfer learning techniques to leverage the much larger text-only NATURAL INSTRUCTIONS dataset and demonstrate its benefit. Moreover, we design a new evaluation metric Sensitivity to assess the model's sensitivity towards the variations in the wording of instructions. Results show that the model becomes less sensitive to these variations after being fine-tuned on a variety of tasks and instructions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Limitations of Data Collection Our proposed dataset only targets English language tasks. Future work should explore multimodal instruction tuning in a more diverse language setting and augment our MULTIINSTRUCT with multi-multilingual tasks.\nIn addition, our current dataset mainly focuses on vision-language tasks. Datasets from more diverse modalities should be considered such as audio (Panayotov et al., 2015;Gemmeke et al., 2017;You et al., 2022) and video (Soomro et al., 2012;Ionescu et al., 2014). While we have built a novel multimodal instruction dataset containing 62 tasks, the number of tasks and associated instructions remains limited. To address this, future research could consider utilizing crowd-sourcing or automatic generation and augmentation techniques to increase the variety of instructions available.", "publication_ref": ["b30", "b9", "b49", "b36", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Limitations of Experiments and Evaluation", "text": "Our work is the first to explore instruction tuning on multimodal tasks and shows improved performance compared to baseline methods. However, there is still room for improvement, specifically in utilizing text-only instruction datasets. Future research could explore alternative architectures and stronger vision-language pre-trained models, or develop additional training loss functions to better utilize these unimodal instruction datasets. Additionally, we only used OFA as the baseline model as it was the largest open-source multimodal pretrained model available when we conducted this research. As more and stronger multimodal pretrained models being publicly available, it would be interesting to conduct a thorough comparison between models with different sizes. Finally, we take the first step to define sensitivity as a metric to evaluate the robustness of the models on understanding and following human-written instructions, which can be a potential standard metric for all the following instruction-tuning studies. However, it's only based on the variation of model performance across different instructions for the same task. In the future, we will consider more broad factors, e.g., the model's capability to understand different instructions for different tasks (Inter-task sensitivity), to further improve the sensitivity metric for instruction tuning. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Tasks Defined in MULTIINSTRUCT", "text": "Table 5 shows the distribution of input and output modalities for both training and evaluation tasks in MULTIINSTRUCT, and Table 6 shows the detailed statistics for all the training and evaluation tasks separately. Tables 7 to 9 provide a comprehensive list of the 62 tasks included in MULTIINSTRUCT, along with one example of instruction for each task.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_12", "tab_13", "tab_15"]}, {"heading": "Input modality Output Modality # of Training # of Testing Image Text Region", "text": "Image Text Region   (Singh et al., 2019) requires models to read and reason about the text in an image to answer questions based on them.\n\u2713 \u2713 1 0 \u2713 \u2713 \u2713 14 5 \u2713 \u2713 \u2713 9 1 \u2713 \u2713 \u2713 2 0 \u2713 \u2713 \u2713 3 1 \u2713 \u2713 \u2713 \u2713 9 0 \u2713 \u2713 \u2713 \u2713 1 0\nGrounded VQA (Zhu et al., 2016) requires models to answer the questions about an image, with the answers being specific visual regions within the image.\nCommonsense VQA (Zellers et al., 2019) requires the model to answer a multiple-choice question that requires commonsense reasoning about an image. Both the question and answers are presented in a combination of natural language and references to specific image regions within the image.\nVisual Entailment (Xie et al., 2019) requires the model to determine whether the image semantically entails the text.\nNatural Language for Visual Reasoning (NLVR) (Suhr et al., 2017) requires the model to answer a question that requires visual and set-theoretic reasoning on a synthetic image.\nVisual Text Extraction is a new task derived from Hateful Memes (Kiela et al., 2020) dataset. This task requires the model to extract the text that appears in the image.\nVisual Dialogue (Das et al., 2017) requires the model to answer a question given an image and a dialogue history.\nDisaster Type Classification (Alam et al., 2022) requires the model to determine the disaster type based on the image.", "publication_ref": ["b35", "b51", "b46", "b37", "b7", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "B.2 NLP Evaluation Tasks", "text": "Below are the task names of the 20 NLP tasks that we used to test the zero-shot performance of all the methods. The 20 NLP tasks are from the default test split of the NATURAL INSTRUCTIONS dataset.\nDuring testing, we leverage the 'Definition' of the task as an instruction and prepend it with each input.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.3 Approaches for Comparison", "text": "OFA  denotes the original pre-trained OFA model without any fine-tuning.\nHere, we use OFA-large 8 which contains 472M parameters and was trained on 8 tasks shown in Table 10. As reported in , OFA has demonstrated certain zero-shot capability on unseen multimodal tasks.\nOFA TaskName is finetuned on MULTIINSTRUCT but it does not use the instructions we created for the tasks. Instead, we prepend the task name to each input and use a semicolon to separate the task name and the input. For a fair comparison, we still keep the two special tokens \"[Options]\" and \"||||\" for the option field.\nOFA MultiInstruct only fine-tunes OFA on our newly introduced MULTIINSTRUCT dataset with instruction tuning.\nOFA NaturalInstruct only fine-tunes OFA on the large-scale NATURAL INSTRUCTIONS dataset (Mishra et al., 2022;Wang et al., 2022d) with instruction tuning. To ensure a fair comparison, we evaluate this baseline on instruction templates that removed all specific tokens, including \"[Options]\" and \"||||\", since the model being tested has not been exposed to these specific tokens during instruction-tuning. We want to ensure that the evaluation is not biased in favor of models that have seen these tokens during training.\nOFA MixedInstruct fine-tunes OFA on the mix of the large-scale NATURAL INSTRUCTIONS (Mishra et al., 2022;Wang et al., 2022d) and MULTIIN-STRUCT dataset with instruction tuning.\nOFA SeqInstruct sequentially fine-tunes OFA on the large-scale NATURAL INSTRUCTIONS (Mishra et al., 2022;Wang et al., 2022d) and MULTIIN-STRUCT dataset with instruction tuning.", "publication_ref": ["b29", "b29", "b29"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "B.4 Training Details", "text": "We set the maximum length of input tokens to 1024 and the maximum target length to 512. For image preprocessing, we strictly follow the process in the OFA. Please refer to the original paper for more details. We train the models on 8 Nvidia A100 GPUs with a batch size 8 per GPU, a learning rate of 1e-05, and float16 enabled for 3 epochs for all the setups and datasets. We run all the experiments once.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Attention Analysis", "text": "In Section 6.1, we have demonstrated that finetuning OFA with NATURAL INSTRUCTIONS alone results in a decline in its zero-shot performance. In this section, we examine one possible reason for this decline by examining if fine-tuning the model on a text-only instruction dataset causes it to give less attention to image inputs.\nTo understand this, we conduct an analysis of the self-attention layers within the OFA encoder. The OFA encoder comprises 12 selfattention layers, each with 16 attention heads. We denote the input to self-attention layer l as h (l) = [x For each self-attention layer, we first compute the attention given to the image states in relation to text states for each attention head. Specifically, for each text state as the query, we sum its attention scores on image states (i.e. the attention scores where the text state is the query and image states are the keys). We then compute the text-to-image attention across all text states. Finally, we average the text-to-image across all attention heads. This results in a text-to-image attention score for each self-attention layer.\nFigure 5 illustrates the results of text-to-image attention scores on three unseen multimodal tasks: Text VQA, Visual Entailment, and Visual Text Extraction. The results on all three unseen tasks show that, in all self-attention layers of the OFA encoder, OFA NaturalInstruct has significantly lower text-to-image attention scores compared to other models. This decrease is particularly pronounced in the first two self-attention layers. This suggests that fine-tuning the model on a text-only instruction dataset leads to a reduction in the attention paid to image inputs, which may explain the decline in zero-shot performance.      and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\nWe don't have paid participants involved D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\nThe data is publically available D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nThe data is collected from publicly available benchmark datasets and there is no potential ethical issue.\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? Not applicable. Left blank.", "publication_ref": [], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "Acknowledgments", "text": "This research is based upon work supported by the U.S. DARPA KMASS Program # HR001121S0034. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\nNo response.\nB4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? No response.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? No response.\nB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\nNo response.\nC Did you run computational experiments? section 5, 6, 7, appendix B C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? appendix B.4", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Muppet: Massive multi-task representations with pre-finetuning", "journal": "", "year": "2021", "authors": "Armen Aghajanyan; Anchit Gupta; Akshat Shrivastava; Xilun Chen; Luke Zettlemoyer; Sonal Gupta"}, {"ref_id": "b1", "title": "Medic: a multi-task learning dataset for disaster image classification", "journal": "", "year": "2022", "authors": "Firoj Alam; Tanvirul Alam; Md Hasan; Abul Hasnat; Muhammad Imran; Ferda Ofli"}, {"ref_id": "b2", "title": "Flamingo: a visual language model for few-shot learning", "journal": "", "year": "2022", "authors": "Jeff Jean-Baptiste Alayrac; Pauline Donahue; Antoine Luc; Iain Miech; Yana Barr; Karel Hasson; Arthur Lenc; Katie Mensch; Malcolm Millican;  Reynolds"}, {"ref_id": "b3", "title": "Beit: Bert pre-training of image transformers", "journal": "", "year": "2022", "authors": "Hangbo Bao; Li Dong; Songhao Piao; Furu Wei"}, {"ref_id": "b4", "title": "Language models are few-shot learners", "journal": "", "year": "2020", "authors": "Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell"}, {"ref_id": "b5", "title": "Assessing image quality issues for real-world problems", "journal": "", "year": "2020", "authors": "Tai-Yin Chiu; Yinan Zhao; Danna Gurari"}, {"ref_id": "b6", "title": "Unifying vision-and-language tasks via text generation", "journal": "PMLR", "year": "2021", "authors": "Jaemin Cho; Jie Lei; Hao Tan; Mohit Bansal"}, {"ref_id": "b7", "title": "Visual dialog", "journal": "", "year": "2017", "authors": "Abhishek Das; Satwik Kottur; Khushi Gupta; Avi Singh; Deshraj Yadav; M F Jos\u00e9; Devi Moura; Dhruv Parikh;  Batra"}, {"ref_id": "b8", "title": "Taming transformers for high-resolution image synthesis", "journal": "", "year": "2021", "authors": "Patrick Esser; Robin Rombach; Bjorn Ommer"}, {"ref_id": "b9", "title": "Audio set: An ontology and human-labeled dataset for audio events", "journal": "IEEE", "year": "2017-03-05", "authors": "F Jort;  Gemmeke; P W Daniel; Dylan Ellis; Aren Freedman; Wade Jansen; R Lawrence; Manoj Moore; Marvin Plakal;  Ritter"}, {"ref_id": "b10", "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering", "journal": "", "year": "2017", "authors": "Yash Goyal; Tejas Khot; Douglas Summers-Stay; Dhruv Batra; Devi Parikh"}, {"ref_id": "b11", "title": "Improving zero and few-shot generalization in dialogue through instruction tuning", "journal": "", "year": "2022", "authors": "Prakhar Gupta; Cathy Jiao; Yi-Ting Yeh; Shikib Mehri; Maxine Eskenazi; Jeffrey P Bigham"}, {"ref_id": "b12", "title": "Ptr: Prompt tuning with rules for text classification", "journal": "AI Open", "year": "2022", "authors": "Xu Han; Weilin Zhao; Ning Ding; Zhiyuan Liu; Maosong Sun"}, {"ref_id": "b13", "title": "Gqa: A new dataset for real-world visual reasoning and compositional question answering", "journal": "", "year": "2019", "authors": "A Drew; Christopher D Hudson;  Manning"}, {"ref_id": "b14", "title": "Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments", "journal": "IEEE Trans. Pattern Anal. Mach. Intell", "year": "2014", "authors": "Catalin Ionescu; Dragos Papava; Vlad Olaru; Cristian Sminchisescu"}, {"ref_id": "b15", "title": "An analysis of visual question answering algorithms", "journal": "", "year": "2017", "authors": "Kushal Kafle; Christopher Kanan"}, {"ref_id": "b16", "title": "Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. 2020. The hateful memes challenge: Detecting hate speech in multimodal memes", "journal": "Advances in Neural Information Processing Systems", "year": "", "authors": "Douwe Kiela; Hamed Firooz; Aravind Mohan; Vedanuj Goswami"}, {"ref_id": "b17", "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "journal": "International journal of computer vision", "year": "2017", "authors": "Ranjay Krishna; Yuke Zhu; Oliver Groth; Justin Johnson; Kenji Hata; Joshua Kravitz; Stephanie Chen; Yannis Kalantidis; Li-Jia Li; David A Shamma"}, {"ref_id": "b18", "title": "Uni-perceiver v2: A generalist model for large-scale vision and vision-language tasks", "journal": "CoRR", "year": "2022", "authors": "Hao Li; Jinguo Zhu; Xiaohu Jiang; Xizhou Zhu; Hongsheng Li; Chun Yuan; Xiaohua Wang; Yu Qiao; Xiaogang Wang; Wenhai Wang; Jifeng Dai"}, {"ref_id": "b19", "title": "Lavender: Unifying video-language understanding as masked language modeling", "journal": "", "year": "2022", "authors": "Linjie Li; Zhe Gan; Kevin Lin; Chung-Ching Lin; Zicheng Liu; Ce Liu; Lijuan Wang"}, {"ref_id": "b20", "title": "Prefix-tuning: Optimizing continuous prompts for generation", "journal": "Association for Computational Linguistics", "year": "2021-08-01", "authors": "Lisa Xiang; Percy Li;  Liang"}, {"ref_id": "b21", "title": "Rouge: A package for automatic evaluation of summaries", "journal": "", "year": "2004", "authors": "Chin-Yew Lin"}, {"ref_id": "b22", "title": "Microsoft coco: Common objects in context", "journal": "Springer", "year": "2014", "authors": "Tsung-Yi Lin; Michael Maire; Serge Belongie; James Hays; Pietro Perona; Deva Ramanan; Piotr Doll\u00e1r; C Lawrence Zitnick"}, {"ref_id": "b23", "title": "Visual spatial reasoning", "journal": "", "year": "2022", "authors": "Fangyu Liu; Guy Emerson; Nigel Collier"}, {"ref_id": "b24", "title": "Few-shot parameter-efficient finetuning is better and cheaper than in-context learning", "journal": "", "year": "", "authors": "Haokun Liu; Derek Tam; Mohammed Muqeeth; Jay Mohta; Tenghao Huang"}, {"ref_id": "b25", "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing", "journal": "", "year": "2021", "authors": "Pengfei Liu; Weizhe Yuan; Jinlan Fu; Zhengbao Jiang; Hiroaki Hayashi; Graham Neubig"}, {"ref_id": "b26", "title": "Unifiedio: A unified model for vision, language, and multimodal tasks", "journal": "", "year": "2022", "authors": "Jiasen Lu; Christopher Clark; Rowan Zellers; Roozbeh Mottaghi; Aniruddha Kembhavi"}, {"ref_id": "b27", "title": "Ok-vqa: A visual question answering benchmark requiring external knowledge", "journal": "", "year": "2019", "authors": "Kenneth Marino; Mohammad Rastegari; Ali Farhadi; Roozbeh Mottaghi"}, {"ref_id": "b28", "title": "Metaicl: Learning to learn in context", "journal": "", "year": "2021", "authors": "Sewon Min; Mike Lewis; Luke Zettlemoyer; Hannaneh Hajishirzi"}, {"ref_id": "b29", "title": "Cross-task generalization via natural language crowdsourcing instructions", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Swaroop Mishra; Daniel Khashabi; Chitta Baral; Hannaneh Hajishirzi"}, {"ref_id": "b30", "title": "Librispeech: An ASR corpus based on public domain audio books", "journal": "IEEE", "year": "2015-04-19", "authors": "Vassil Panayotov; Guoguo Chen; Daniel Povey; Sanjeev Khudanpur"}, {"ref_id": "b31", "title": "Learning to predict visual attributes in the wild", "journal": "", "year": "2021", "authors": "Khoi Pham; Kushal Kafle; Zhe Lin; Zhihong Ding; Scott Cohen; Quan Tran; Abhinav Shrivastava"}, {"ref_id": "b32", "title": "Multitask prompted training enables zero-shot task generalization", "journal": "", "year": "2022", "authors": "Victor Sanh; Albert Webson; Colin Raffel; Stephen H Bach; Lintang Sutawika; Zaid Alyafeai; Antoine Chaffin; Arnaud Stiegler; Arun Raja; Manan Dey; Canwen Bari; Urmish Xu; Shanya Thakker; Eliza Sharma Sharma; Taewoon Szczechla; Gunjan Kim;  Chhablani; V Nihal; Debajyoti Nayak; Jonathan Datta; Mike Chang;  Tian-Jian; Han Jiang; Matteo Wang; Sheng Manica;  Shen"}, {"ref_id": "b33", "title": "Neural machine translation of rare words with subword units", "journal": "Long Papers", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"ref_id": "b34", "title": "Flava: A foundational language and vision alignment model", "journal": "", "year": "2022", "authors": "Amanpreet Singh; Ronghang Hu; Vedanuj Goswami; Guillaume Couairon; Wojciech Galuba; Marcus Rohrbach; Douwe Kiela"}, {"ref_id": "b35", "title": "Towards vqa models that can read", "journal": "", "year": "2019", "authors": "Amanpreet Singh; Vivek Natarajan; Meet Shah; Yu Jiang; Xinlei Chen; Dhruv Batra; Devi Parikh; Marcus Rohrbach"}, {"ref_id": "b36", "title": "UCF101: A dataset of 101 human actions classes from videos in the wild", "journal": "CoRR", "year": "2012", "authors": "Khurram Soomro; Mubarak Amir Roshan Zamir;  Shah"}, {"ref_id": "b37", "title": "A corpus of natural language for visual reasoning", "journal": "Short Papers", "year": "2017", "authors": "Alane Suhr; Mike Lewis; James Yeh; Yoav Artzi"}, {"ref_id": "b38", "title": "Lxmert: Learning cross-modality encoder representations from transformers", "journal": "", "year": "2019", "authors": "Hao Tan; Mohit Bansal"}, {"ref_id": "b39", "title": "Coco-text: Dataset and benchmark for text detection and recognition in natural images", "journal": "", "year": "2016", "authors": "Andreas Veit; Tomas Matera; Lukas Neumann; Jiri Matas; Serge Belongie"}, {"ref_id": "b40", "title": "Unifying architectures, tasks, and modalities through a simple sequenceto-sequence learning framework", "journal": "", "year": "2022", "authors": "Peng Wang; An Yang; Rui Men; Junyang Lin; Shuai Bai; Zhikang Li; Jianxin Ma; Chang Zhou; Jingren Zhou; Hongxia Yang"}, {"ref_id": "b41", "title": "The art of prompting: Event detection based on type specific prompts", "journal": "", "year": "2022", "authors": "Sijia Wang; Mo Yu; Lifu Huang"}, {"ref_id": "b42", "title": "Image as a foreign language: Beit pretraining for all vision and visionlanguage tasks", "journal": "", "year": "2022", "authors": "Wenhui Wang; Hangbo Bao; Li Dong; Johan Bjorck; Zhiliang Peng; Qiang Liu; Kriti Aggarwal; Saksham Owais Khan Mohammed; Subhojit Singhal; Furu Som;  Wei"}, {"ref_id": "b43", "title": "Kuntal Kumar Pal", "journal": "", "year": "", "authors": "Yizhong Wang; Swaroop Mishra; Pegah Alipoormolabashi; Yeganeh Kordi; Amirreza Mirzaei; Anjana Arunkumar; Arjun Ashok; Arut Selvan Dhanasekaran; Atharva Naik; David Stap; Eshaan Pathak; Giannis Karamanolakis; Gary Haizhi; Ishan Lai; Ishani Purohit; Jacob Mondal; Kirby Anderson; Krima Kuznia; Maitreya Doshi;  Patel"}, {"ref_id": "b44", "title": "Do promptbased models really understand the meaning of their prompts?", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Albert Webson; Ellie Pavlick"}, {"ref_id": "b45", "title": "Finetuned language models are zero-shot learners", "journal": "CoRR", "year": "", "authors": "Jason Wei; Maarten Bosma; Y Vincent; Kelvin Zhao; Adams Wei Guu; Brian Yu; Nan Lester;  Du"}, {"ref_id": "b46", "title": "Visual entailment: A novel task for fine-grained image understanding", "journal": "", "year": "2019", "authors": "Ning Xie; Farley Lai; Derek Doran; Asim Kadav"}, {"ref_id": "b47", "title": "An explanation of in-context learning as implicit bayesian inference", "journal": "CoRR", "year": "2021", "authors": "Sang Michael Xie; Aditi Raghunathan; Percy Liang; Tengyu Ma"}, {"ref_id": "b48", "title": "End-to-end multimodal fact-checking and explanation generation: A challenging dataset and models", "journal": "", "year": "2022", "authors": "Aditya Barry Menglong Yao; Lichao Shah; Jin-Hee Sun; Lifu Cho;  Huang"}, {"ref_id": "b49", "title": "End-to-end spoken conversational question answering: Task, dataset and model", "journal": "", "year": "2022-07-10", "authors": "Chenyu You; Nuo Chen; Fenglin Liu; Shen Ge; Xian Wu; Yuexian Zou"}, {"ref_id": "b50", "title": "Modeling context in referring expressions", "journal": "Springer", "year": "2016", "authors": "Licheng Yu; Patrick Poirson; Shan Yang; Alexander C Berg; Tamara L Berg"}, {"ref_id": "b51", "title": "From recognition to cognition: Visual commonsense reasoning", "journal": "", "year": "2019", "authors": "Rowan Zellers; Yonatan Bisk; Ali Farhadi; Yejin Choi"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Example Instances from MULTIINSTRUCT for Four Tasks.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 provides several instruction examples for the tasks included in MULTIINSTRUCT.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Task Groups Included in MULTIINSTRUCT. The yellow boxes represent tasks used for evaluation, while the white boxes indicate tasks used for training.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Model Performance as the Number of Multimodal Instruction Task Clusters Increases. The number in the parenthesis of each cluster denotes the number of tasks.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "where L is the length of sequence. The input h(0)  I+T ] to the first selfattention layer is actually the concatenation of image embeddings and text embeddings, where I, T is the length of image and text embeddings respectively. For ease of understanding and simplicity, we have altered the naming conventions and refer to x l p , p = [1, ..., I] as image states and x l p , p = [I + 1, ..., I + T ] as text states.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 5 :5Figure 5: Text-to-Image Attention of OFA Encoder.", "figure_data": ""}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "The standard deviation and mean of the model's performance across all instructions are denoted by \u03c3 i\u2208I t [\u2022] and \u00b5 i\u2208I t [\u2022], respectively.Das et al., 2017), and Disaster Type Classification(Alam et al., 2022). These tasks belong to three task groups: Commonsense Reasoning, VQA, and Miscellaneous as shown in Figure2. Tasks in the Commonsense Reasoning group have no overlap with any training task groups. Tasks in Miscellaneous do not share similarities with other tasks in the group. Although Text VQA and Grounded VQA belong to the VQA task group, they require additional skills such as extracting text from images or generating regions, making them fundamentally different from other tasks in VQA. In addition to multimodal tasks, we also evaluate the model on 20 NLP tasks collected from the test split of NATURAL INSTRUCTIONS. for Comparison We denote the OFA finetuned on MULTIINSTRUCT as OFA MultiInstruct , and compare it with the original pre-trained OFA 5 , OFA TaskName which is fine-tuned on MULTIINSTRUCT but uses the task name instead of instruction to guide the model to make predictions, and several approaches that leverage the large-scale NATURAL INSTRUCTIONS dataset, including OFA NaturalInstruct which only fine-tunes OFA on NATURAL INSTRUCTIONS with instruction tuning, OFA MixedInstruct and OFA SeqInstruct that are specified in Section 4.2. More details regarding the evaluation datasets, baseline approaches and training details can be found in Appendix B.", "figure_data": "Evaluation datasets We evaluate the models on nine unseen multimodal tasks: Text VQA (Singh et al., 2019), Grounded VQA (Zhu et al., 2016), Commonsense VQA (Zellers et al., 2019), Visual Entailment (Xie et al., 2019), Visual Spatial Rea-soning (Liu et al., 2022a), Natural Language for Visual Reasoning (NLVR) (Suhr et al., 2017), Vi-sual Text Extraction (Kiela et al., 2020), Visual Dialogue (Approaches"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": ". Our results indicate that OFA MultiInstruct significantly improves the model's zero-short performance over the original pre-trained OFA model across all unseen tasks and metrics, demonstrating the effectiveness of multimodal instruction tuning on MULTIINSTRUCT. As seen in Table2, OFA achieves extremely low (nearly zero) zero-shot performance on the Grounded VQA task, which requires the model to generate region-specific tokens in order to answer the question. By examining the generated results, we find that OFA, without instruction tuning, failed to follow the instruction and produce results that contain region tokens. However, by fine-tuning OFA on MULTIINSTRUCT, the model is able to better interpret and follow the instructions to properly generate the expected output. Addition-50.60 \u00b1 1.12 33.01 31.17 \u00b1 1.59 55.96 55.06 \u00b10.76 55.81 53.90 \u00b11.38 56.97 56.18 \u00b1 0.95", "figure_data": "Commonsense VQA RougeL ACC Avg \u00b1 Std Max Avg \u00b1 Std 17.93 14.97 \u00b1 4.30 0.73 Max 0.40 \u00b10.29 48.99 -29.01 -52.01 Transfer Learning from NATURAL INSTRUCTIONS Model OFA OFA TaskName OFA MultiInstruct OFA NaturalInstruct 27.15 14.99 \u00b1 9.12 7.35 2.04 \u00b1 3.01 OFA MixedInstruct 50.40 49.34 \u00b1 1.04 31.31 30.27 \u00b1 0.94 OFA SeqInstruct 50.93 50.07 \u00b1 1.07 32.28 31.23 \u00b1 1.09Visual Entailment ACC Max Avg\u00b1 Std 49.99 41.86 \u00b1 10.99 55.70 -33.28 14.86 \u00b1 16.68 54.63 53.74 \u00b1 0.97 53.66 52.98 \u00b1 0.56Visual Spatial Reasoning ACC Max Avg\u00b1 Std 54.99 35.29 \u00b1 22.21 53.76 -51.44 36.44 \u00b1 20.72 55.13 52.61 \u00b1 1.64 54.86 53.11 \u00b1 1.45NLVR ACC Avg\u00b1 Std 56.06 52.10 \u00b1 3.35 Max 55.35 -56.06 35.98 \u00b1 21.64 56.67 55.96 \u00b1 0.48 57.58 56.63 \u00b1 0.66"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Zero-shot Performance on Multimodal Commonsense Reasoning. The best performance is in bold.", "figure_data": "Text VQA RougeL Avg\u00b1 Std 15.21 9.30 \u00b1 5.42 Max 23.80 -27.22 26.46 \u00b1 0.83 Transfer Learning from NATURAL INSTRUCTIONS Grounded VQA Acc Model Max Avg\u00b1 Std OFA 0.02 0.00 \u00b1 0.01 0.00 -OFA TaskName OFA MultiInstruct 64.32 47.22 \u00b1 23.08 OFA NaturalInstruct 5.59 5.40 \u00b1 0.24 0.00 0.00 \u00b1 0.00 OFA MixedInstruct 24.15 23.67 \u00b1 0.47 63.79 54.99 \u00b1 18.16 OFA SeqInstruct 27.03 26.67 \u00b1 0.47 64.19 54.46 \u00b1 15.96Visual Text Extraction RougeL Max Avg\u00b1 Std 36.31 17.62 \u00b1 16.82 36.30 -74.35 62.43 \u00b111.56 5.65 1.24 \u00b1 2.48 62.43 46.56 \u00b1 14.92 71.63 60.62 \u00b1 12.31Visual Dialogue RougeL Max Avg \u00b1 Std 45.46 28.71 \u00b1 9.81 25.18 -46.38 32.91 \u00b17.59 30.94 27.91 \u00b1 2.16 46.08 38.02 \u00b1 5.25 46.17 35.10 \u00b1 6.92Disaster Type Classification ACC Max Avg \u00b1 Std 14.30 9.64 \u00b1 4.34 62.65 -64.88 56.00 \u00b112.96 56.64 38.21 \u00b1 15.35 68.31 64.31 \u00b1 2.39 64.46 57.89 \u00b1 9.51"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "One potential reason for this decline in performance is that during fine-tuning on the text-only dataset, the model learns to focus more on text tokens and attend less to image tokens. To verify this assumption, we compare the attention of text tokens on image tokens between OFA NaturalInstruct and other methods and observe that text tokens attend much less to image tokens after fine-tuning on the NATURAL INSTRUCTIONS dataset. The detailed explanations and analysis can be found in Appendix C.Another observation is that although our transfer learning methods do not lead to significant performance gains over OFA MixedInstruct , both OFA SeqInstruct and OFA MixedInstruct achieve lower standard deviation on 6 out of 9 unseen multimodal tasks compared with OFA MultiInstruct , demonstrating", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Effect of Different Number of Instructions. Performance of OFA MultiInstruct finetuned on different numbers of instructions.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Zero-shot Performance on NLP tasks. The performance is reported in Rouge-L and the best performance is in bold.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Distribution of input and output modalities for all the tasks in MULTIINSTRUCT.", "figure_data": "Train Eval"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Detailed statistics in MULTIINSTRUCT.", "figure_data": "B More Details for Experimental SetupB.1 Multimodal Evaluation DatasetsText VQA"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Answer the question <QUESTION> based on the content of the given image.", "figure_data": "CategoryTask NameDatasetDescriptionExistOpen-Domain VQAVQAv2 Genome et al., 2017) et al., 2017), Visual (Goyal (Krishna\u2713VQAVQAVisual7w (Zhu et al., 2016)Answer a visual question <QUESTION> by selecting an answer from given options. <OPTION>\u2713Compositional VQAGQA (Hudson and Manning, 2019)Answer a compositional question based on the content of the given image. Question: <QUESTION>\u2713Outside Knowl-edge VQAOK-VQA et al., 2019)(MarinoBased on your knowledge, <QUESTION>?\u2713Grounded Cap-tioningVisual Genome (Kr-ishna et al., 2017)Given the region <REGION> in the image, generate a caption for that region.\u2713Visual Ground-ingVisual Genome (Kr-ishna et al., 2017)Given a caption <TEXT> for some region in the image, identify the region and generate its bounding box.\u2713Grounded GenerationGrounded Object Identifi-cation Object Ground-ingMSCOCO (Lin et al., 2014) MSCOCO (Lin et al., 2014)Identify the type of an object in <REGION>. What are the regions containing the object [TEXT]?\u2713 \u00d7Referring Expression GroundingRefCOCO (Yu et al., 2016)Locate a region in an image based on the referring expression [TEXT].\u2713Referring Expression GenerationRefCOCO (Yu et al., 2016)Generate the referring expression for an object in region <REGION>.\u2713Text Localiza-tionCOCO-Text (Veit et al., 2016)Select a region from options that contain the text <TEXT> in the image. <OPTION>\u2713Most-Overlapping tion Region Selec-Visual Genome (Kr-ishna et al., 2017)Given the region <REGION>, decide which region in the options overlaps most with given region. <OPTION>\u00d7Region UnderstandingNon-Overlapping Region Selec-tion Least-Overlapping tion Region Selec-Visual Genome (Kr-ishna et al., 2017) Visual Genome (Kr-ishna et al., 2017)Which option does not share common area with <REGION>? <OPTION> \"Which option has the least shared area with <REGION>?<OPTION>\u00d7 \u00d7Overlapping Re-gion SelectionVisual Genome (Kr-ishna et al., 2017)Which region from options that has common area with <REGION>? <OPTION>\u00d7Region Overlap-ping DetectionVisual Genome (Kr-ishna et al., 2017)Does <REGION1> share common area with <REGION2>? <OPTION>\u00d7Region AreaVisual Genome (Kr-ishna et al., 2017)Compute the area of <REGION>.\u00d7Region-Caption MatchingVisual Genome (Kr-ishna et al., 2017)Decide if the caption matches the given region <REGION> in the image.\u00d7Grounded Cap-tion SelectionVisual Genome (Kr-ishna et al., 2017)Given a region <REGION> in the image, select a caption from given options for that region. <OPTION>\u00d7Visual Ground-ing SelectionVisual Genome (Kr-ishna et al., 2017)Given a caption <TEXT> for some region in the image, select the region from the options. <OPTION>\u00d7Grounded MatchingReferring Expression SelectionRefCOCO (Yu et al., 2016)Select a region from options based on the referring expression <TEXT>. <OPTION>\u00d7Object-Region MatchingMSCOCO (Lin et al., 2014)Does region <REGION> contain the object <TEXT>?\u00d7Object-Region SelectionMSCOCO (Lin et al., 2014)Select the region containing the given object <TEXT>. <OPTION>\u00d7Object Match-ingMSCOCO (Lin et al., 2014)Do objects in region <REGION1> and region <REGION2> have the same type?\u00d7Missing Object SelectionMSCOCO (Lin et al., 2014)Select an object from options that does not appear in any of the given regions <REGION>. <OPTION>\u00d7Region-Text MatchingCOCO-Text (Veit et al., 2016)Does region <REGION> contain the text <TEXT>?\u00d711459"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Detailed Group of Training Tasks Included in MULTIINSTRUCT. The complete list of 53 multi-modal tasks, along with examples of the instructions for each task. The existing tasks are indicated with \u2713, while the newly derived tasks are indicated using \u00d7. <QUESTION> based on the color of an object.<OPTION> \u2713 This task asks you to identify if an object appears in the image. <QUESTION><OPTION> \u2713 In this task you are asked a question about the type of an object in the image. <QUESTION><OPTION> \u2713 Given the subject in region <REGION>, where is the object in the image that has relationship <TEXT> with the subject?\u00d7 Given the object in region <REGION>, where is the subject in the image that has relationship <TEXT> with the object?\u00d7 Decide if the claim can be supported by the given image and the context. For task <TASK>, given the history steps and the current step with its corresponding image, what is the next step for this task? <HISTORY> <TASK>, given the current step <STEP>, decide if the content of the image is the next or previous step.\u00d7 <TASK>, given the current step specified by the image, decide if the step <STEP> is the next or previous step.", "figure_data": "Task NameDatasetDescriptionExistColor Recogni-tion Answer the question: Object Detec-TDIUC (Kafle and Kanan, 2017) tion TDIUC (Kafle and Kanan, 2017)Object Recogni-tionTDIUC (Kafle and Kanan, 2017)Image UnderstandingScene Recogni-tion CountingTDIUC (Kafle and Kanan, 2017) TDIUC (Kafle and Kanan, 2017)Look at the environment in the image and answer the question accordingly. <QUESTION><OPTION> Question: <QUESTION> Please answer the question by counting the object mentioned in the question. <OPTION>\u2713 \u2713Sentiment Un-derstandingTDIUC (Kafle and Kanan, 2017)Question: <QUESTION><OPTION> Please answer the question by interpret-ing the sentiment in the image.\u2713Position Reason-ingTDIUC (Kafle and Kanan, 2017)In this task, you need to analyze the position of objects in an image and answer the following question. <QUESTION><OPTION>\u2713Utility Affor-danceTDIUC (Kafle and Kanan, 2017)Please take a look at the picture and answer the following question by thinking about what each object in the picture can be used for.\u2713<QUESTION><OPTION>Sport standingUnder-TDIUC (Kafle and Kanan, 2017)There are some sports taking place in the image.<QUESTION><OPTION>\u2713Image QualityIQA (Chiu et al., 2020)Select a reason from the options to explain why the image quality is bad. <OPTION>\u2713Object Relation-shipVisual Genome (Kr-ishna et al., 2017)What is the relationship between the subject in region <REGION1> and object in region <REGION2>?\u2713Visual Object IdentificationVisual Genome (Kr-ishna et al., 2017)Given the subject in region <REGION>, what is the object that has a rela-tionship <TEXT> with that subject?\u00d7Visual RelationshipVisual Subject IdentificationVisual Genome (Kr-ishna et al., 2017)Given the object in region <REGION>, what is the subject that has a rela-tionship <TEXT> with that object?\u00d7Visual Object LocalizationVisual Genome (Kr-ishna et al., 2017)Visual Subject LocalizationVisual Genome (Kr-ishna et al., 2017)Grounded Im-age Attribute IdentificationVAW (Pham et al., 2021)Decide which option is the attribute of the object in the region <REGION>. <OPTION>\u2713Image-Text MatchingImage-Text Matching Question-Image MatchingMSCOCO (Lin et al., 2014) VQAv2 (Goyal et al., 2017)Decide if the text matches the image. Decide if the image contains an answer to the question <QUESTION>.\u00d7 \u00d7Image-Text Se-lectionMSCOCO (Lin et al., 2014)Select the text that best matches the image. <OPTION>\u00d7Multimodal Fac-tual CheckingMOCHEG (Yao et al., 2022)\u2713MiscellaneousText LegibilityCOCO-Text (Veit et al., 2016)Decide if the text in the given region is legible.\u2713Text Type Clas-sificationCOCO-Text (Veit et al., 2016)Read the text in the given region and determine the type of text from options.\u2713Image Caption-ingMSCOCO (Lin et al., 2014)Generate a sentence to describe the content of the image.\u2713Wikihow Next Step GenerationWikiHow 7\u00d7Temporal OrderingWikihow Next Step Selection Wikihow Text-Image Temporal Ordering For the task Wikihow Image-WikiHow For task <TASK>, select the immediate next step to the step specified by the image. WikiHow Ordering Text Temporal WikiHow For the task \u00d7 \u00d7"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "(Continued) Detailed Group of Training Tasks Included in MULTIINSTRUCT. The complete list of 53 multi-modal tasks, along with examples of the instructions for each task. The existing tasks are indicated with \u2713, while the newly derived tasks are indicated using \u00d7. <TEXT> from the content of image? Select your answer from the options. <OPTION> \u2713", "figure_data": "11461"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "Detailed Group of Evaluation Tasks Included in MULTIINSTRUCT. The complete list of 9 multi-modal tasks, along with examples of the instructions for each task. The existing tasks are indicated with \u2713, while the newly derived tasks are indicated using \u00d7.C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? appendix B.4 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? section 5 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? section 3 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)", "figure_data": "etc.)?section 5D Did you use human annotators (e.g., crowdworkers) or research with human participants?section 3"}], "formulas": [{"formula_id": "formula_0", "formula_text": "D t = {(I t , x t j , y t j ) \u2208 I t \u00d7 X t \u00d7 Y t } N j=1", "formula_coordinates": [5.0, 70.86, 370.68, 218.28, 35.42]}, {"formula_id": "formula_1", "formula_text": "Mixed Instruction Tuning (OFA MixedInstruct )", "formula_coordinates": [5.0, 306.14, 219.08, 219.01, 14.92]}, {"formula_id": "formula_2", "formula_text": "E t\u2208T \u03c3 i\u2208I t E (x,y)\u2208D t [L(f \u03b8 (i, x), y)] \u00b5 i\u2208I t E (x,y)\u2208D t [L(f \u03b8 (i, x), y)]", "formula_coordinates": [6.0, 89.89, 427.52, 168.1, 29.18]}, {"formula_id": "formula_3", "formula_text": "\u2713 \u2713 1 0 \u2713 \u2713 \u2713 14 5 \u2713 \u2713 \u2713 9 1 \u2713 \u2713 \u2713 2 0 \u2713 \u2713 \u2713 3 1 \u2713 \u2713 \u2713 \u2713 9 0 \u2713 \u2713 \u2713 \u2713 1 0", "formula_coordinates": [12.0, 316.59, 447.83, 187.83, 48.55]}], "doi": "10.18653/v1/2021.emnlp-main.468"}