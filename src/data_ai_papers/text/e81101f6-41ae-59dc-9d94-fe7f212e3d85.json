{"title": "Improving Pretraining Techniques for Code-Switched NLP", "authors": "Richeek Das; Sahasra Ranjan; Shreya Pathak; Preethi Jyothi", "pub_date": "", "abstract": "Pretrained models are a mainstay in modern NLP applications. Pretraining requires access to large volumes of unlabeled text. While monolingual text is readily available for many of the world's languages, access to large quantities of code-switched text (i.e., text with tokens of multiple languages interspersed within a sentence) is much more scarce. Given this resource constraint, the question of how pretraining using limited amounts of code-switched text could be altered to improve performance for code-switched NLP becomes important to tackle. In this paper, we explore different masked language modeling (MLM) pretraining techniques for code-switched text that are cognizant of language boundaries prior to masking. The language identity of the tokens can either come from human annotators, trained language classifiers, or simple relative frequencybased estimates. We also present an MLM variant by introducing a residual connection from an earlier layer in the pretrained model that uniformly boosts performance on downstream tasks. Experiments on two downstream tasks, Question Answering (QA) and Sentiment Analysis (SA), involving four code-switched language pairs (Hindi-English, Spanish-English, Tamil-English, Malayalam-English) yield relative improvements of up to 5.8 and 2.7 F1 scores on QA (Hindi-English) and SA (Tamil-English), respectively, compared to standard pretraining techniques. To understand our task improvements better, we use a series of probes to study what additional information is encoded by our pretraining techniques and also introduce an auxiliary loss function that explicitly models language identification to further aid the residual MLM variants.", "sections": [{"heading": "Introduction", "text": "Multilingual speakers commonly switch between languages within the confines of a conversation or a sentence. This linguistic process is known as codeswitching or code-mixing. Building computational models for code-switched inputs is very important in order to cater to multilingual speakers across the world (Zhang et al., 2021).\nMultilingual pretrained models such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) appear to be a natural choice to handle code-switched inputs. However, prior work demonstrated that representations directly extracted from pretrained multilingual models are not very effective for code-switched tasks (Winata et al., 2019). Pretraining multilingual models using code-switched text as an intermediate task, prior to task-specific finetuning, was found to improve performance on various downstream code-switched tasks (Khanuja et al., 2020a;Prasad et al., 2021a). Such an intermediate pretraining step relies on access to unlabeled code-switched text, which is not easily available in large quantities for different language pairs. This prompts the question of how pretraining could be made more effective for code-switching within the constraints of limited amounts of code-switched text. 1 In this work, we propose new pretraining techniques for code-switched text by focusing on two fronts: a) modified pretraining objectives that explicitly incorporate information about codeswitching (detailed in Section 2.1) and b) architectural changes that make pretraining with codeswitched text more effective (detailed in Section 2.2).\nPretraining objectives. The predominant objective function used during pretraining is masked language modeling (MLM) that aims to reconstruct randomly masked tokens in a sentence. We will henceforth refer to this standard MLM objective as STDMLM. Instead of randomly masking tokens, we propose masking the tokens straddling language boundaries in a code-switched sentence; language boundaries in a sentence are characterized by two words of different languages. We refer to this objective as SWITCHMLM. A limitation of this technique is that it requires language identification (LID) of the tokens in a code-switched sentence. LID tags are not easily obtained, especially when dealing with transliterated (Romanized) forms of tokens in other languages. We propose a surrogate for SWITCHMLM called FREQMLM that infers LID tags using relative counts from large monolingual corpora in the component languages.\nArchitectural changes. Inspired by prior work that showed how different layers of models like mBERT specifically encode lexical, syntactic and semantic information (Rogers et al., 2020), we introduce a regularized residual connection from an intermediate layer that feeds as input into the MLM head during pretraining. We hypothesize that creating a direct connection from a lower layer would allow for more language information to be encoded within the learned representations. To more explicitly encourage LID information to be encoded, we also introduce an auxiliary LID-based loss using representations from the intermediate layer where the residual connection is drawn. We empirically verify that our proposed architectural changes lead to representations that are more language-aware by using a set of probing techniques that measure the switching accuracy in a code-switched sentence.\nWith our proposed MLM variants, we achieve consistent performance improvements on two natural language understanding tasks, factoidbased Question Answering (QA) in Hindi-English and Sentiment Analysis (SA) in four different language pairs, Hindi-English, Spanish-English, Tamil-English and Malayalam-English. Sections 3 and 4 elaborate on datasets, experimental setup and our main results, along with accompanying analyses including probing experiments.\nOur code and relevant datasets are available at the following link: https://github.com/ csalt-research/code-switched-mlm.", "publication_ref": ["b48", "b14", "b44", "b19", "b30", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MLM Pretraining Objectives", "text": "In the Standard MLM objective (Devlin et al., 2019) that we refer to as STDMLM, a fixed percentage (typically 15%) of tokens in a given sentence are marked using the [MASK] token and the objective is to predict the [MASK] tokens via an output softmax over the vocabulary. Consider an input sentence X = x 1 , . . . , x n with n tokens, a predetermined masking fraction f and an n-dimensional bit vector S = {0, 1} n that indicates whether or not a token is allowed to be replaced with [MASK]. A masking function M takes X, f and S as its inputs and produces a new token sequence X mlm as its output\nX mlm = M(X, S, f )\nwhere X mlm denotes the input sentence X with f % of the maskable tokens (as deemed by S) randomly replaced with [MASK].\nFor STDMLM, S = {1} n which means that any of the tokens in the sentence are allowed to be masked. In our proposed MLM techniques, we modify S to selectively choose a set of maskable tokens.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "SWITCHMLM", "text": "SWITCHMLM is informed by the transitions between languages in a code-switched sentence. Consider the following Hindi-English code-switched sentence and its corresponding LID tags:\nLaptop mere bag me rakha hai EN HI EN HI HI HI For SWITCHMLM, we are only interested in potentially masking those words that surround language transitions. S is determined using information about the underlying LID tags for all tokens. In the example above, these words would be \"Laptop\", \"mere\", \"bag\" and \"me\". Consequently, S for this example would be\nS = [1, 1, 1, 1, 0, 0].\nLID information is not readily available for many language pairs. Next, in FREQMLM, we extract proxy LID tags using counts derived from monolingual corpora for the two component languages.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "FREQMLM", "text": "For a given language pair, one requires access to LID-tagged text or an existing LID tagger to implement SWITCHMLM. LID tags are hard to infer especially when dealing with transliterated or Romanized word forms. To get around this dependency, we try to assign LID tags to the tokens only based on relative frequencies obtained from monolingual corpora in the component languages. S = F(X, C en , C lg ) = {0, 1} n where F assigns 1 to those tokens that straddle language boundaries and LIDs are determined for each token based on their relative frequencies in a monolingual corpus of the embedded language (that we fix as English) C en and a monolingual corpus of the matrix language C lg .\nFor a given token x, we define nll_en and nll_lg as negative log-likelihoods of the relative frequencies of x appearing in C en and C lg , respectively. nll values are set to -1 if the word does not appear in the corpus or if the word has a very small count and yields very high nll values (greater than a fixed threshold that we arbitrarily set to ln 10). The subroutine to assign LIDs is defined as follows:\ndef Assign_LID ( nll_en , nll_lg ) :\nif nll_en == -1 and nll_lg == -1: return OTHER elif nll_en != -1 and nll_lg == -1:\nreturn EN elif nll_en == -1 and nll_lg != -1: return LG elif nll_lg + ln (10) < nll_en : return LG elif nll_en + ln (10) < nll_lg : return EN elif nll_lg <= nll_en : return AMB -LG elif nll_en < nll_lg : return AMB -EN else : return OTHER Here, AMB-LG, AMB-EN refer to ambiguous tokens that have reasonable counts but are not sufficiently large enough to be confidently marked as either EN or LG tokens. Setting AMB-EN to EN and AMB-LG to LG yielded the best results and we use this mapping in all our FREQMLM experiments. (Additional experiments with other FREQMLM variants by treating the ambiguous tokens separately are described in Appendix C.2.)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Architectural Modifications", "text": "In Section 2.1, we presented new MLM objectives that mask tokens around language transitions (or switch-points) in a code-switched sentence. The main intuition behind masking around switchpoints was to coerce the model to encode information about possible switch-point positions in a sentence. (Later, in Section 4.2, we empirically verify this claim using a probing classifier with representations from a SWITCHMLM model compared to an STDMLM model.) We suggest two architectural changes that could potentially help further exploit switch-point information in the code-switched text. Prior studies have carried out detailed investigations of how BERT works and what kind of information is encoded within representations in each of its layers (Jawahar et al., 2019;Liu et al., 2019;Rogers et al., 2020). These studies have found that lower layers encode information that is most taskinvariant, final layers are the most task-specific and the middle layers are most amenable to transfer. This suggests that language information could be encoded in any of the lower or middle layers. To act as a direct conduit to this potential source of language information during pretraining, we introduce a simple residual connection from an intermediate layer that is added to the output of the last Transformer layer in mBERT. We refer to this modified mBERT as RESBERT. We also apply dropout to the residual connection which acts as a regularizer and is important for performance improvements.", "publication_ref": ["b18", "b22", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "Residual Connection with Dropout", "text": "We derive consistent performance improvements in downstream tasks with RESBERT when the residual connections are drawn from a lower layer for SWITCHMLM. With STDMLM, we see significant improvements when residual connections are drawn from the later layers. (We elaborate on this further using probing experiments.)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Auxiliary LID Loss", "text": "With RESBERT, we add a residual connection to a lower or middle layer with the hope of gaining more direct access to information about potential switch-point transitions. We can further encourage this intermediate layer to encode language information by imposing an auxiliary LID-based loss. Figure 1 shows how token representations of an intermediate layer, from which a residual connection is drawn, feed as input into a multi-layer perceptron MLP to predict the LID tags of each token. To ensure that this LID-based loss does not destroy other useful information that is already present in the layer embeddings, we also add an L2 regularization for representations from all the layers to avoid large departures from the original embeddings. Given a sentence x 1 , . . . , x n , we have a corresponding sequence of bits y 1 , . . . , y n where y i = 1 represents that x i lies at a language boundary. Then the new loss L aux can be defined as:\nL aux = \u03b1 n i=1 \u2212 log MLP(x i )+\u03b2 L j=1 ||W j \u2212W j || 2\nwhere MLP(x i ) is the probability with which MLP labels x i as y i ,W j refers to the original embedding matrix corresponding to layer j, W j refers to the new embedding matrix and \u03b1, \u03b2 are scaling hyperparameters for the LID prediction and L2-regularization loss terms, respectively.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Experimental Setup", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Datasets", "text": "We aggregate real code-switched text from multiple sources, described in Appendix B, to create pretraining corpora for Hindi-English, Spanish-English, Tamil-English and Malayalam-English consisting of 185K, 66K, 118K and 34K sentences, respectively. We also extract code-switched data from a very large, recent Hindi-English corpus L3CUBE (Nayak and Joshi, 2022) consisting of 52.9M sentences scraped from Twitter. More details about L3CUBE are in Appendix B.\nFor FREQMLM described in Section 2.1.2, we require a monolingual corpus for English and one for each of the component languages in the four code-switched language pairs. Large monolingual corpora will provide coverage over a wider vocabulary and consequently lead to improved LID predictions for words in code-switched sentences. We use counts computed from the following monolingual corpora to implement FREQMLM.\nEnglish. We use OPUS-100 (Zhang et al., 2020), which is a large English-centric translation dataset consisting of 55 million sentence pairs and comprising diverse corpora including movie subtitles, GNOME documentation and the Bible.\nSpanish. We use a large Spanish corpus released by (Ca\u00f1ete et al., 2020) that contains 26.5 million sentences accumulated from 15 unlabeled Spanish text datasets spanning Wikipedia articles and European parliament notes.\nHindi, Tamil and Malayalam. The Dakshina corpus (Roark et al., 2020) is a collection of text in both Latin and native scripts for 12 South Asian languages including Hindi, Tamil and Malayalam. Samanantar (Ramesh et al., 2022) is a large publicly-available parallel corpus for Indic languages. We combined Dakshina and Samanatar 2 datasets to obtain roughly 10M, 5.9M and 5.2M sentences for Hindi, Malayalam and Tamil respectively. We used this combined corpus to perform NLL-based LID assignment in FREQMLM.\nThe Malayalam monolingual corpus is quite noisy with many English words appearing in the text. To implement FREQMLM for ML-EN, we use an alternate monolingual source called Aksharantar (Madhani et al., 2022). It is a large publiclyavailable transliteration vocabulary-based dataset for 21 Indic languages with 4.1M words specifically in Malayalam. We further removed common English words 3 from Aksharantar's Malayalam vocabulary to improve the LID assignment for FRE-QMLM. We used this dataset with an alternate LID assignment technique that only checks if a word exists, without accumulating any counts. (This is described further in Section 4.1.)", "publication_ref": ["b26", "b47", "b6", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "SA and QA Tasks", "text": "We use the GLUECOS benchmark (Khanuja et al., 2020a) to evaluate our models for Sentiment Analysis (SA) and Question Answering (QA). GLUECOS provides an SA task dataset for Hindi-English and Spanish-English. The Spanish-English SA dataset (Vilares et al., 2016)  STDMLM + RESBERT 66.8 9 \u00b1 3.0 64.6 9 \u00b1 1.7 64.4 9 \u00b1 2.0 77 5 \u00b1 0.3 68.4 9 \u00b1 0.0 76.6 9 \u00b1 0.2 63.1 9 \u00b1 1.1 SW/FREQMLM + RESBERT 68.8 2 \u00b1 3.1 68.9 2 \u00b1 3.0 68.1 2 \u00b1 3.0 77.4 2 \u00b1 0.3 68.9 2 \u00b1 0.4 77.1 2 \u00b1 0.2 63.7 2 \u00b1 1.8 SW/FREQMLM + RESBERT + L aux 68 2 \u00b1 3.0 68. 9 2 \u00b1 3.2  69.8 2 \u00b1 3.0 77.6 2 \u00b1 0.2 69.1 2 \u00b1 0.4 77.2 2 \u00b1 0.4 63.7   4 Results and Analysis", "publication_ref": ["b19", "b42"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Main Results", "text": "Table 1 shows our main results using all our proposed MLM techniques applied to the downstream tasks QA and SA. We use F1-scores as an evaluation metric for both QA and SA. For QA, we report the average scores from the top 8-performing (out of 10) seeds, and for SA, we report average F1-scores from the top 10-performing seeds (out of 12). We observed that the F1 scores were notably poorer for one seed, likely due to the small test-sets for QA (54 examples) and SA (211 for Spanish-English). To safeguard against such outlier seeds, we report average scores from the top-K runs. We show results for two multilingual pretrained models, mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020). 4 Improvements with MLM pretraining objectives. From Table 1, we note that STDMLM is always better than the baseline model (sans pretraining). Among the three MLM pretraining objectives, SWITCHMLM consistently outperforms both STDMLM and FREQMLM across both tasks. We observe statistical significance at p < 0.05 (with p-values of 0.01 and lower for some language pairs) using the Wilcoxon Signed Rank test when comparing F1 scores across multiple seeds using SWITCHMLM compared to STDMLM on both QA and SA tasks.\nAs expected, FREQMLM acts as a surrogate to SWITCHMLM trailing behind it in perfor-mance while outperforming STDMLM. Since Tamil-English and Malayalam-English pretraining corpora were not LID-tagged, we do not show SWITCHMLM numbers for these two language pairs and only report FREQMLM-based scores. For QA, we observe that FREQMLM hurts XLM-R while significantly helps mBERT in performance compared to STDMLM. We hypothesize that this is largely caused by QA having a very small train set (of size 295), in conjunction with XLM-R being five times larger than mBERT and the noise inherent in LID tags from FREQMLM (compared to SWITCHMLM). We note here that using FRE-QMLM with XLM-R for SA does not exhibit this trend since Hindi-English SA has a larger train set with 15K sentences.\nConsiderations specific to FREQMLM. The influence of SWITCHMLM and FREQMLM on downstream tasks depends both on (1) the amount of code-switched pretraining text and (2) the LID tagging accuracy. Malayalam-English (ML-EN) is an interesting case where STDMLM does not yield significant improvements over the baseline. This could be attributed to the small amount of real code-switched text in the ML-EN pretraining corpus (34K). Furthermore, we observe that FRE-QMLM fails to surpass STDMLM. This could be due to the presence of many noisy English words in the Malayalam monolingual corpus. To tackle this, we devise an alternative to the NLL LID-tagging approach that we call X-HIT. X-HIT only considers vocabularies of English and the matrix language, and checks if a given word appears in the vocabulary of English or the matrix language to mark its LID. Unlike NLL which is count-based, X-HIT only checks for the existence of a word in a vocabulary. This approach is particularly useful for language pairs where the monolingual corpus is small and unreliable. Appendix C.1 provides more insights about when to choose X-HIT over NLL.\nWe report a comparison between the NLL and X-HIT LID-tagging approaches for ML-EN sentences in Table 2. Since X-HIT uses a clean dictionary instead of a noisy monolingual corpus for LID assignment, we see improved performance with X-HIT compared to NLL. However, given the small pretraining corpus for ML-EN, FREQMLM still underperforms compared to STDMLM.\nTo assess how much noise can be tolerated in the LID tags derived via NLL, Table 3 shows the label distribution across true and predicted labels using  the NLL LID-tagging approach for Hindi-English.\nWe observe that while a majority of HI and EN tokens are correctly labeled as being HI and EN tags, respectively, a fairly sizable fraction of tags totaling 18% and 17% for HI and EN, respectively, are wrongly predicted. This shows that FREQMLM performs reasonably well even in the presence of noise in the predicted LID tags.   Results on Alternate Pretraining Corpus. To assess the difference in performance when using pretraining corpora of varying quality, we extract roughly the same number of Hindi-English sentences from L3CUBE (185K) as is present in the Hindi-English pretraining corpus we used for Table 1. Roughly 45K of these 185K sentences have human-annotated LID tags. For the remaining sentences, we use the GLUECOS LID tagger (Khanuja et al., 2020a).\nTable 4 shows the max and mean F1-scores for HI-EN SA for all our MLM variants. These numbers exhibit the same trends observed in Table 1. Also, since the L3CUBE dataset is much cleaner than the 185K dataset we used previously for Hindi-English, we see a notable performance gain in Table 4 for HI-EN compared to the numbers in Table 1. Nayak and Joshi (2022) further provide an mBERT model HINGMBERT pretrained on the entire L3CUBE dataset of 52.93M sentences. This model outperforms all the mBERT pretrained models, confirming that a very large amount of pretrain-ing text, if available, yields superior performance.", "publication_ref": ["b14", "b19", "b26"], "figure_ref": [], "table_ref": ["tab_2", "tab_2", "tab_4", "tab_6", "tab_7", "tab_2"]}, {"heading": "Probing Experiments", "text": "We use probing classifiers to test our claim that the amount of switch-point information encoded in the neural representations from specific layers has increased with our proposed pretraining variants compared to STDMLM. Alain and Bengio (2016) first introduced the idea of using linear classifier probes for features at every model layer, and Kim et al. (2019) further developed new probing tasks to explore the effects of various pretraining objectives in sentence encoders.\nLinear Probing. We first adopt a standard linear probe to check for the amount of switch-point information encoded in neural representations of different model layers. For a sentence x 1 , . . . , x n , consider a sequence of bits y 1 , . . . , y n referring to switch-points where y i = 1 indicates that x i is at a language boundary. The linear probe is a simple feedforward network that takes layer-wise representations as its input and is trained to predict switch-points via a binary cross-entropy loss. We train the linear probe for around 5000 iterations.\nConditional Probing. Linear probing cannot detect when representations are more predictive of switch-point information in comparison to a baseline. Hewitt et al. (2021) offer a simple extension of the theory of usable information to propose conditional probing. We adopt this method for our task and define performance in terms of predicting the switch-point sequence as:\nPerf(f [B(X), \u03d5(X)]) \u2212 Perf(f ([B, 0]))\nwhere X is the input sequence of tokens, B is the STDMLM pretrained model, \u03d5 is the model trained with one of our new pretraining techniques, f is a linear probe, [\u2022, \u2022] denotes concatenation of embeddings and Perf is any standard performance metric. We set Perf to be a soft Hamming Distance between the predicted switch-point sequence and the ground-truth bit sequence. To train f , we follow the same procedure outlined in Section 4.2, except we use concatenated representations from two models as its input instead of a single representation.", "publication_ref": ["b1", "b21", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Probing Results", "text": "Figure 2 shows four salient plots using linear probing and conditional probing. In Figure 2a, we observe that the concatenated representations from models trained with STDMLM and SWITCHMLM carry more switch-point information than using STDMLM alone. This offers an explanation for the task-specific performance improvements we observe with SWITCHMLM. With greater amounts of switch-point information, SWITCHMLM models arguably tackle the code-switched downstream NLU tasks better.\nFrom Figure 2c, we observe that the intermediate layer (9) from which the residual connection is drawn carries a lot more switch-point information than the final layer in STDMLM. In contrast, from Figure 2d, we find this is not true for SWITCHMLM models, where there is a very small difference between switch-point information encoded by an intermediate and final layer. This might explain to some extent why we see larger improvements using a residual connection with STDMLM compared to SWITCHMLM (as discussed in Section 4.1).\nFigure 2b shows that adding a residual connection from layer 9 of an STDMLM-trained model, that is presumably rich in switch-point information, provides a boost to switch-point prediction accuracy compared to using STDMLM model alone.\nWe note here that the probing experiments in this section offer a post-hoc analysis of the effectiveness of introducing a skip connection during pretraining. We do not actively use probing to choose the best layer to add a skip connection.", "publication_ref": [], "figure_ref": ["fig_1", "fig_1", "fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "Related Work", "text": "While not related to code-switching, there has been prior work on alternatives or modifications to pretraining objectives like MLM. Yamaguchi et al. (2021) is one of the first works to identify the lack of linguistically intuitive pretraining objectives. They propose new pretraining objectives which perform similarly to MLM given a similar pretrain duration. In contrast, Clark et al. (2020) sticks to the standard MLM objective, but questions whether masking only 15% of tokens in a sequence is sufficient to learn meaningful representations. Wettig et al. (2022) maintains that higher masking up to even 80% can preserve model performance on downstream tasks. All of the aforementioned methods are static and do not exploit a partially trained model to devise better masking strategies on the fly. and content in different training stages. Ours is the first work to offer both MLM modifications and architectural changes aimed specifically at codeswitched pretraining.\nPrior work on improving code-switched NLP has focused on generative models of code-switched text to use as augmentation (Gautam et al., 2021;Tarunesh et al., 2021a), merging real and synthetic code-switched text for pretraining (Khanuja et al., 2020b;Santy et al., 2021b), intermediate task pretraining including MLM-style objectives (Prasad et al., 2021b). However, no prior work has provided an in-depth investigation into how pretraining using code-switched text can be altered to encode information about language transitions within a code-switched sentence. We show that switch-point information is more accurately preserved in models pretrained with our proposed techniques and this eventually leads to improved performance on code-switched downstream tasks.", "publication_ref": ["b45", "b12", "b43", "b15", "b40", "b20", "b36", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "Pretraining multilingual models with codeswitched text prior to finetuning on task-specific data has been found to be very effective for code-switched NLP tasks. In this work, we focus on developing new pretraining techniques that are more language-aware and make effective use of limited amounts of real code-switched text to derive performance improvements on two downstream tasks across multiple language pairs. We design new pretraining objectives for code-switched text and suggest new architectural modifications that further boost performance with the new objectives in place. In future work, we will investigate how to make effective use of pretraining with synthetically generated code-switched text.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Our current FREQMLM techniques tend to fail on LID predictions when the linguistic differences between languages are small. For example, English and Spanish are quite close: (1) they are written in the same script, (2) English and Spanish share a lot of common vocabulary. This can confound FREQMLM.\nThe strategy to select the best layer for drawing residual connections in RESBERT is quite tedious. For a 12-layer mBERT, we train 10 RESBERT models with residual connections from some intermediate layer x \u2208 {1, \u2022 \u2022 \u2022 , 10} and choose the best layer based on validation performance. This is quite computationally prohibitive. We are considering parameterizing the layer choice using gating functions so that it can be learned without having to resort to a tedious grid search.\nIf the embedded language in a code-switched sentence has a very low occurrence, we will have very few switch-points. This might reduce the number of maskable tokens to a point where even masking all the maskable tokens will not satisfy the overall 15% masking requirement. However, we never faced this issue. In our experiments, we compensate by masking around 25%-35% of the maskable tokens (calculated based on the switch-points in the dataset).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Training details", "text": "We employed the mBERT and XLM-R models for our experiments. The mBERT model has 178 million parameters and 12 transformer layers, while the XLM-R model has 278 million parameters and 24 transformer layers. AdamW optimizer (Loshchilov and Hutter, 2019) and a linear scheduler were used in all our experiments, which were conducted on a single NVIDIA A100 Tensor Core GPU.\nFor the pretraining step, we utilized a batch size of 4, a gradient accumulation step of 20, and 4 epochs for the mBERT base model. For the XLM-R base model, we set the batch size to 8 and the gradient accumulation step to 4. For the Sentiment Analysis task, we used a batch size of 8, a learning rate of 5e-5, and a gradient accumulation step of 1 for the mBERT base model. Meanwhile, we set the batch size to 32 and the learning rate to 5e-6 for the XLM-R base model. For the downstream task of Question Answering, we used the same hyperparameters for both mBERT and XLM-R: a batch size of 4 and a gradient accumulation step of 10. Results were reported for multiple epochs, as stated in Section 4.1. All the aforementioned hyperparameters were kept consistent for all language pairs. In the auxiliary LID loss-based experiments mentioned in Section 3.3, we did not perform a search for the best hyperparameters. Instead, we set \u03b1 to 5e-2 and \u03b2 to 5e-4, where \u03b1 and \u03b2 are defined in Section 2.2.2.", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "B Pretraining Dataset", "text": "We use the ALL-CS (Tarunesh et al., 2021b) corpus, which consists of 25K Hindi-English LID-tagged code-switched sentences. We combine this corpus with code-switched text data from prior work ; Swami et al. (2018); Chandu et al. (2018b); Patwa et al. (2020); Bhat et al. (2017); Patro et al. (2017) resulting in a total of 185K LID-tagged Hindi-English code-switched sentences.\nFor Spanish-English code-switched text data, we pooled data from prior work Patwa et al. (2020); Solorio et al. (2014);AlGhamdi et al. (2016); Aguilar et al. (2018); Vilares et al. (2016) to get a total of 66K  We pooled 118K Tamil-English code-switched sentences from Chakravarthi et al. (2020bChakravarthi et al. ( , 2021; Banerjee et al. (2018); Mandl et al. (2021) and 34K Malayalam-English code-switched sentences from Chakravarthi et al. (2020aChakravarthi et al. ( , 2021; Mandl et al. (2021). These datasets do not have ground-truth LID tags and high-quality LID tagger for TA-EN and ML-EN are not available. Hence, we do not perform SWITCHMLM experiments for these language pairs. We will refer to the combined datasets for Hindi-English, Spanish-English, Malayalam-English, and Tamil-English code-switched sentences as HI-EN COMBINED-CS , ES-HI COMBINED-CS , ML-HI COMBINED-CS , and TA-EN COMBINED-CS respectively.\nNayak and Joshi (2022) released the L3Cube-HingCorpus and HingLID Hindi-English codeswitched datasets. L3Cube-HingCorpus is a codeswitched Hindi-English dataset consisting of 52.93M sentences scraped from Twitter. L3Cube-HingLID is a Hindi-English code-switched language identification dataset which consists of 31756, 6420, and 6279 train, test, and validation samples, respectively. We extracted roughly 140k sentences from L3Cube-HingCorpus with a similar average sentence length as the HI-EN COMBINED-CS dataset, assigned LID tags using the GLUECOS LID tagger (Khanuja et al., 2020a), and combined it with the 45k sentences of L3Cube-HingLID to get around 185K sentences in total. We use this L3CUBE -185k dataset in Section 4.1 to examine the effects of varying quality of pretraining corpora.", "publication_ref": ["b41", "b39", "b11", "b29", "b4", "b28", "b29", "b38", "b2", "b0", "b42", "b8", "b25", "b3", "b25", "b7", "b25", "b25", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "C FREQMLM C.1 X-HIT LID assignment", "text": "The Malayalam-English code-switched dataset (ML-EN COMBINED-CS ) has fairly poor Roman transliterations of Malayalam words. This makes it difficult for the NLL approach to assign the correct LID to these words since it is based on the likelihood scores of the word in the monolingual dataset. Especially for rare Malayalam words in the sentence, the NLL approach fails to assign the correct LID and instead ends up assigning a high number of \"OTHER\" tags.\nThe X-HIT approach described in Section 4.1 addresses this issue. X-HIT first checks the occurrence of the word in Malayalam vocabulary, then checks if it is an English word. Since we have a high-quality English monolingual dataset, we can be confident that the words that are left out are rare or poorly transliterated Malayalam words, and hence are tagged ML.\nAs an illustration, we compare the LID tags assigned to the example Malayalam-English code-switched sentence Maduraraja trailer erangiyapo veendum kaanan vannavar undel evide likiko in Table 5 using NLL and X-HIT, with the latter being more accurate.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_9"]}, {"heading": "C.2 Masking strategies for ambiguous tokens", "text": "In the NLL approach of FREQMLM described in Section 2.1.2, we assign ambiguous (AMB) LID tokens to words when it is difficult to differentiate between nll scores with confidence. To make use of AMB tokens, we introduce a probabilistic masking approach that classifies the words based on their ambiguity at the switch-points.\n\u2022 Type 0: If none of the words at the switch-point are marked ambiguous, mask them with prob. p 0\n\u2022 Type 1: If one of the words at the switch-point is marked ambiguous, mask it with prob. p 1\n\u2022 Type 2: If both the words are marked ambiguous, mask them with prob. p 2\nWe try out different masking probabilities, which sum up to p = 0.15. Say we mask tokens of the words of Type 0, 1, and 2 in the ratio r 0 , r 1 , r 2 and the counts of these words in the dataset are n 0 , n 1 , n 2 respectively, then the masking probabilities p 0 , p 1 , p 2 are determined by the following equation:\np 0 n 0 + p 1 n 1 + p 2 n 2 = p(n 0 + n 1 + n 2 )\nIt is easy to see that the probabilities should be in the same proportion as our chosen masking ratios, i.e., p 0 : p 1 : p 2 :: r 0 : r 1 : r 2 . We report the results we obtained for this experiment in Table 6.   ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_11"]}, {"heading": "D RESBERT results", "text": "Table 7 presents our results for STDMLM and SWITCHMLM for RESBERT on all layers x \u2208 {1, \u2022 \u2022 \u2022 , 10} with a dropout rate of p = 0.5. The trend of results achieved with RESBERT clearly depends on the type of masking strategy used. In the case of STDMLM + RESBERT, we see a gradual improvement in test performance as we go down the residually connected layers, eventually peaking at layer 10. On the other hand, we do not see a clear trend in the case of SWITCHMLM + RESBERT. In both cases, we select the best layer to add a residual connection based on its performance on the SA validation set. We do a similar set of experiments for the TA-EN language pair to choose the best layer, which turns out to be layer 5 for STDMLM and layer 9 for SWITCHMLM pretraining. For the language pairs ES-EN, HI-EN (L3CUBE ), and ML-EN, we do not search for the best layer for RESBERT. As a general rule of thumb, we use layer 2 for SWITCHMLM and layer 9 for STDMLM pretraining of RESBERT for these language pairs.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_12"]}, {"heading": "ACL 2023 Responsible NLP Checklist", "text": "A For every submission:\nA1. Did you describe the limitations of your work?\nWe discussed the limitations of work in section 7 of the paper.\nA2. Did you discuss any potential risks of your work?\nOur work does not have any immediate risks as it is related to improving pretraining techniques for code-switched NLU.\nA3. Do the abstract and introduction summarize the paper's main claims?\nAbstraction and Introduction in Section 1 of the paper summarize the main paper's claim.\nA4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB Did you use or create scientific artifacts?\nYes, we use multiple datasets that we described in Section 3.1. Apart from the dataset, we use pretrained mBERT and XLMR models described in Section 1. In section 3, we cite the GLUECoS benchmark to test and evaluate our approach and the Indic-trans tool to transliterate the native Indic language sentences in the dataset.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B1. Did you cite the creators of artifacts you used?", "text": "We cite the pretrained models in section 1, the GLUECos benchmark, the Indic-trans tool, and the datasets in section 3 of the paper.\nB2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo, we used open-source code, models and datasets for all our experiments. Our new code will be made publicly available under the permissive MIT license.\nB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Yes, the usage of the existing artifacts mentioned above was consistent with their intended use. We use the mBERT and XLMR pretrained models as the base model, the dataset mentioned to train and test our approach, GLUECoS as the fine-tuning testing benchmark, and Indic-trans for transliteration of the native Indic language sentences. B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? We used publicly available code-switched datasets containing content scraped from social media. We hope that the dataset creators have taken steps to check the data for offensive content.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? No, we did not create any artifacts. B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. Yes, we report these relevant statistics for the dataset that we use in section 3.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "The last author would like to gratefully acknowledge a faculty grant from Google Research India supporting research on models for code-switching. The authors are thankful to the anonymous reviewers for constructive suggestions that helped improve the submission.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Gustavo Aguilar; Fahad Alghamdi; Victor Soto; Thamar Solorio"}, {"ref_id": "b1", "title": "Understanding intermediate layers using linear classifier probes", "journal": "", "year": "2016", "authors": "Guillaume Alain; Yoshua Bengio"}, {"ref_id": "b2", "title": "Part of speech tagging for code switched data", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Fahad Alghamdi; Giovanni Molina; Mona Diab; Thamar Solorio; Abdelati Hawwari; Victor Soto; Julia Hirschberg"}, {"ref_id": "b3", "title": "A dataset for building code-mixed goal oriented conversation systems", "journal": "", "year": "2018", "authors": "Suman Banerjee; Nikita Moghe; Siddhartha Arora; Mitesh M Khapra"}, {"ref_id": "b4", "title": "Joining hands: Exploiting monolingual treebanks for parsing of code-mixing data", "journal": "", "year": "2017", "authors": "Irshad Bhat; A Riyaz; Manish Bhat; Dipti Shrivastava;  Sharma"}, {"ref_id": "b5", "title": "Iiit-h system submission for fire2014 shared task on transliterated search", "journal": "ACM", "year": "2015", "authors": "Ahmad Irshad; Vandan Bhat; Aniruddha Mujadia; Riyaz Ahmad Tammewar; Manish Bhat;  Shrivastava"}, {"ref_id": "b6", "title": "Spanish pre-trained bert model and evaluation data", "journal": "", "year": "2020", "authors": "Jos\u00e9 Ca\u00f1ete; Gabriel Chaperon; Rodrigo Fuentes; Jou-Hui Ho; Hojin Kang; Jorge P\u00e9rez"}, {"ref_id": "b7", "title": "A sentiment analysis dataset for codemixed Malayalam-English", "journal": "", "year": "2020", "authors": "Navya Bharathi Raja Chakravarthi; Shardul Jose; Elizabeth Suryawanshi; John Philip Sherly;  Mc-Crae"}, {"ref_id": "b8", "title": "Corpus creation for sentiment analysis in code-mixed Tamil-English text", "journal": "", "year": "2020", "authors": "Vigneshwaran Bharathi Raja Chakravarthi; Ruba Muralidaran; John Philip Priyadharshini;  Mccrae"}, {"ref_id": "b9", "title": "2021. Findings of the shared task on offensive language identification in Tamil, Malayalam, and Kannada", "journal": "Association for Computational Linguistics", "year": "", "authors": "Ruba Bharathi Raja Chakravarthi; Navya Priyadharshini; Anand Jose; M Kumar; Thomas Mandl; Prasanna Kumar Kumaresan; Rahul Ponnusamy; R L Hariharan; John P Mccrae; Elizabeth Sherly"}, {"ref_id": "b10", "title": "Code-mixed question answering challenge: Crowdsourcing data and techniques", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Khyathi Chandu; Ekaterina Loginova; Vishal Gupta; Josef Van Genabith; G\u00fcnter Neumann; Manoj Chinnakotla; Eric Nyberg; Alan W Black"}, {"ref_id": "b11", "title": "Language informed modeling of code-switched text", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Khyathi Chandu; Thomas Manzini; Sumeet Singh; Alan W Black"}, {"ref_id": "b12", "title": "Electra: Pre-training text encoders as discriminators rather than generators", "journal": "", "year": "2020", "authors": "Kevin Clark; Minh-Thang Luong; Quoc V Le; Christopher D Manning"}, {"ref_id": "b13", "title": "Unsupervised cross-lingual representation learning at scale", "journal": "", "year": "2020", "authors": "Alexis Conneau; Kartikay Khandelwal; Naman Goyal; Vishrav Chaudhary; Guillaume Wenzek; Francisco Guzm\u00e1n; Edouard Grave; Myle Ott; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b14", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b15", "title": "Comet: Towards code-mixed translation using parallel monolingual sentences", "journal": "", "year": "2021", "authors": "Devansh Gautam; Prashant Kodali; Kshitij Gupta; Anmol Goel; Manish Shrivastava; Ponnurangam Kumaraguru"}, {"ref_id": "b16", "title": "Training data augmentation for code-mixed translation", "journal": "", "year": "2021", "authors": "Abhirut Gupta; Aditya Vavre; Sunita Sarawagi"}, {"ref_id": "b17", "title": "Conditional probing: measuring usable information beyond a baseline", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "John Hewitt; Kawin Ethayarajh; Percy Liang; Christopher Manning"}, {"ref_id": "b18", "title": "What does BERT learn about the structure of language", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Ganesh Jawahar; Beno\u00eet Sagot; Djam\u00e9 Seddah"}, {"ref_id": "b19", "title": "GLUECoS: An evaluation benchmark for code-switched NLP", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Simran Khanuja; Sandipan Dandapat; Anirudh Srinivasan; Sunayana Sitaram; Monojit Choudhury"}, {"ref_id": "b20", "title": "Gluecos: An evaluation benchmark for codeswitched nlp", "journal": "", "year": "2020", "authors": "Simran Khanuja; Sandipan Dandapat; Anirudh Srinivasan; Sunayana Sitaram; Monojit Choudhury"}, {"ref_id": "b21", "title": "Probing what different NLP tasks teach machines about function word comprehension", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Najoung Kim; Roma Patel; Adam Poliak; Patrick Xia; Alex Wang; Tom Mccoy; Ian Tenney; Alexis Ross; Tal Linzen; Benjamin Van Durme; Samuel R Bowman; Ellie Pavlick"}, {"ref_id": "b22", "title": "Linguistic knowledge and transferability of contextual representations", "journal": "", "year": "2019", "authors": "F Nelson; Matt Liu; Yonatan Gardner;  Belinkov; E Matthew; Noah A Peters;  Smith"}, {"ref_id": "b23", "title": "Decoupled weight decay regularization", "journal": "", "year": "2019", "authors": "Ilya Loshchilov; Frank Hutter"}, {"ref_id": "b24", "title": "Anoop Kunchukuttan, Pratyush Kumar, and Mitesh Shantadevi Khapra. 2022. Aksharantar: Towards building open transliteration tools for the next billion users", "journal": "", "year": "", "authors": "Yash Madhani; Sushane Parthan; Priyanka Bedekar; Ruchi Khapra"}, {"ref_id": "b25", "title": "Overview of the hasoc track at fire 2020: Hate speech and offensive language identification in tamil, malayalam, hindi, english and german", "journal": "Association for Computing Machinery", "year": "2021", "authors": "Thomas Mandl; Sandip Modha; Anand Kumar; M ; Bharathi Raja Chakravarthi"}, {"ref_id": "b26", "title": "L3Cube-HingCorpus and HingBERT: A code mixed Hindi-English dataset and BERT language models", "journal": "", "year": "2022", "authors": "Ravindra Nayak; Raviraj Joshi"}, {"ref_id": "b27", "title": "Sentiment analysis of code-mixed indian languages: An overview of sail c ode \u2212 mixedsharedtask@icon", "journal": "", "year": "2017", "authors": "Dipankar Braja Gopal Patra; Amitava Das;  Das"}, {"ref_id": "b28", "title": "All that is English may be Hindi: Enhancing language identification through automatic ranking of the likeliness of word borrowing in social media", "journal": "", "year": "2017", "authors": "Jasabanta Patro; Bidisha Samanta; Saurabh Singh; Abhipsa Basu; Prithwish Mukherjee; Monojit Choudhury; Animesh Mukherjee"}, {"ref_id": "b29", "title": "SemEval-2020 task 9: Overview of sentiment analysis of code-mixed tweets", "journal": "", "year": "2020", "authors": "Parth Patwa; Gustavo Aguilar; Sudipta Kar; Suraj Pandey; Pykl Srinivas; Bj\u00f6rn Gamb\u00e4ck; Tanmoy Chakraborty"}, {"ref_id": "b30", "title": "The effectiveness of intermediate-task training for code-switched natural language understanding", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Archiki Prasad; Mohammad Ali Rehan; Shreya Pathak; Preethi Jyothi"}, {"ref_id": "b31", "title": "", "journal": "", "year": "2021", "authors": "Archiki Prasad; Mohammad Ali Rehan; Shreya Pathak; Preethi Jyothi"}, {"ref_id": "b32", "title": "Samanantar: The largest publicly available parallel corpora collection for 11 indic languages", "journal": "", "year": "", "authors": "Gowtham Ramesh; Sumanth Doddapaneni; Aravinth Bheemaraj; Mayank Jobanputra; A K Raghavan; Ajitesh Sharma; Sujit Sahoo; Harshita Diddee; J Mahalakshmi; Divyanshu Kakwani; Navneet Kumar; Aswin Pradeep; Srihari Nagaraj; Kumar Deepak; Vivek Raghavan"}, {"ref_id": "b33", "title": "Processing South Asian languages written in the Latin script: the Dakshina dataset", "journal": "", "year": "2020", "authors": "Brian Roark; Lawrence Wolf-Sonkin; Christo Kirov; Sabrina J Mielke; Cibu Johny"}, {"ref_id": "b34", "title": "A primer in BERTology: What we know about how BERT works", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "Anna Rogers; Olga Kovaleva; Anna Rumshisky"}, {"ref_id": "b35", "title": "BERTologiCoMix: How does codemixing interact with multilingual BERT?", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Sebastin Santy; Anirudh Srinivasan; Monojit Choudhury"}, {"ref_id": "b36", "title": "Bertologicomix: How does codemixing interact with multilingual bert?", "journal": "", "year": "2021", "authors": "Sebastin Santy; Anirudh Srinivasan; Monojit Choudhury"}, {"ref_id": "b37", "title": "A Twitter corpus for Hindi-English code mixed POS tagging", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Kushagra Singh; Indira Sen; Ponnurangam Kumaraguru"}, {"ref_id": "b38", "title": "Overview for the first shared task on language identification in code-switched data", "journal": "", "year": "2014", "authors": "Thamar Solorio; Elizabeth Blair; Suraj Maharjan; Steven Bethard; Mona Diab; Mahmoud Ghoneim; Abdelati Hawwari; Fahad Alghamdi; Julia Hirschberg; Alison Chang; Pascale Fung"}, {"ref_id": "b39", "title": "A corpus of english-hindi code-mixed tweets for sarcasm detection", "journal": "", "year": "2018", "authors": "Sahil Swami; Ankush Khandelwal; Vinay Singh; Manish Syed Sarfaraz Akhtar;  Shrivastava"}, {"ref_id": "b40", "title": "From machine translation to code-switching: Generating high-quality code-switched text", "journal": "Long Papers", "year": "2021", "authors": "Ishan Tarunesh; Syamantak Kumar; Preethi Jyothi"}, {"ref_id": "b41", "title": "From machine translation to code-switching: Generating high-quality code-switched text", "journal": "Long Papers", "year": "2021", "authors": "Ishan Tarunesh; Syamantak Kumar; Preethi Jyothi"}, {"ref_id": "b42", "title": "EN-ES-CS: An English-Spanish code-switching Twitter corpus for multilingual sentiment analysis", "journal": "", "year": "2016", "authors": "David Vilares; Miguel A Alonso; Carlos G\u00f3mez-Rodr\u00edguez"}, {"ref_id": "b43", "title": "", "journal": "", "year": "2022", "authors": "Alexander Wettig; Tianyu Gao; Zexuan Zhong; Danqi Chen"}, {"ref_id": "b44", "title": "Code-switched language models using neural based synthetic data from parallel sentences", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Andrea Genta Indra Winata; Chien-Sheng Madotto; Pascale Wu;  Fung"}, {"ref_id": "b45", "title": "Frustratingly simple pretraining alternatives to masked language modeling", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Atsuki Yamaguchi; George Chrysostomou; Katerina Margatina; Nikolaos Aletras"}, {"ref_id": "b46", "title": "Learning better masking for better language model pre-training", "journal": "", "year": "2022", "authors": "Dongjie Yang; Zhuosheng Zhang; Hai Zhao"}, {"ref_id": "b47", "title": "Improving massively multilingual neural machine translation and zero-shot translation", "journal": "", "year": "2020", "authors": "Biao Zhang; Philip Williams; Ivan Titov; Rico Sennrich"}, {"ref_id": "b48", "title": "Language-agnostic and languageaware multilingual natural language understanding for large-scale intelligent voice assistant application", "journal": "IEEE", "year": "2021", "authors": "Jonathan Daniel Yue Zhang; Yao Hueser; Sarah Li;  Campbell"}, {"ref_id": "b49", "title": "C Did you run computational experiments? Yes, we ran computational experiments to improve the pretraining approach for Code-Switched NLU. The description, setup, and results are described in sections 2, 3, and 4", "journal": "", "year": "", "authors": ""}, {"ref_id": "b50", "title": "GPU hours), and computing infrastructure used? Yes, we reported all these details in Appendix section A", "journal": "", "year": "", "authors": ""}, {"ref_id": "b51", "title": "Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? Yes, we reported all these details in Appendix section A", "journal": "", "year": "", "authors": ""}, {"ref_id": "b52", "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? We report the average F1 scores for our major experiments over multiple seeds, which we mentioned in the result section 4. We report max, average, and standard deviation for various other experiments in section 4 over multiple seeds. Probing tasks described in sections", "journal": "", "year": "", "authors": ""}, {"ref_id": "b53", "title": "We used multiple existing packages, viz. GLUECoS, HuggingFace Transformers, and Indic-Trans. We report the parameter settings and models in Appendix section A", "journal": "", "year": "", "authors": " Nltk;  Spacy;  Rouge"}, {"ref_id": "b54", "title": "crowdworkers) or research with human participants? Left blank", "journal": "", "year": "", "authors": ""}, {"ref_id": "b55", "title": "Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators", "journal": "", "year": "", "authors": " D1"}, {"ref_id": "b56", "title": "crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic", "journal": "", "year": "", "authors": ""}, {"ref_id": "b57", "title": "Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?", "journal": "", "year": "", "authors": " D3"}, {"ref_id": "b58", "title": "Was the data collection protocol approved (or determined exempt) by an ethics review board? No response", "journal": "", "year": "", "authors": " D4"}, {"ref_id": "b59", "title": "Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response", "journal": "", "year": "", "authors": " D5"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Modified mBERT with Residual Connection (RESBERT) and Auxiliary LID Loss (L aux ).", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Probing results comparing the amount of language boundary information encoded in different layers of different models. Note: (1) If not mentioned, assume final layer representations (2) RESBERT 9,STDMLM represents the mBERT model having a residual connection from layer 9 and trained with STDMLM pretraining.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "QA and SA scores for primary models and language pairs. Note: For RESBERT results, the subscript near the F1 scores represents the layer from which the residual connection is drawn for that particular model. The best layer to add a residual connection was determined by validation performance on the downstream NLU tasks. Since we do not have a development set for QA, we choose the same layer as chosen by SA validation for the QA task. The training process and hyperparameter details can be found in Appendix A.", "figure_data": "and 211 examples in the training, developmentand test sets, respectively. The Hindi-EnglishSA dataset (Patra et al., 2018) consists of 15K,1.5K and 3K code-switched tweets in the train-ing, development and test sets, respectively. TheTamil-English (Chakravarthi et al., 2020a) andMalayalam-English (Chakravarthi et al., 2020b)SA datasets are extracted from YouTube commentscomprising 9.6K/1K/2.7K and 3.9K/436/1.1K ex-amples in the train/dev/test sets, respectively. TheQuestion Answering Hindi-English factoid-baseddataset (Chandu et al., 2018a) from GLUECOS con-sists of 295 training and 54 test question-answer-context triples. Because of the unavailability of thedev set, we report QA results on a fixed number oftraining epochs i.e., 20, 30, and 40 epochs.3.3 RESBERT and Auxiliary Loss: Implementation detailsWe modified the mBERT architectures for the threemain tasks of masked language modeling, questionanswering (QA), and sequence classification byincorporating residual connections as outlined inSection 2.2.1. The MLM objective was used duringpretraining with residual connections drawn fromlayers x \u2208 {1, \u2022 \u2022 \u2022 , 10} and a dropout rate of p = 0.5."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "ModelF1 (max) F1 (avg) Std. Dev.", "figure_data": "Baseline (mBERT)77.2976.420.42STDMLM77.3976.670.48FREQMLM (NLL)76.6176.200.43FREQMLM (X-HIT)77.2976.460.43"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Comparison of various FREQMLM approaches for the Malayalam-English SA task.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "While the benefits are not as clear as with STDMLM, even SWITCHMLM marginally benefits from a residual connection on examining QA and SA results. Since LID tags are not available for TA-EN and ML-EN, we use FREQMLM pretraining with residual connections. Given access to LID tags, both HI-EN and ES-EN use SWITCHMLM pretraining with residual connections. SW/FRE-QMLM in Table 1 refers to either SWITCHMLM or FREQMLM pretraining depending on the language pair.We observe an interesting trend as we change the layer x \u2208 {1, \u2022 \u2022 \u2022 , 10} from which the residual connection is drawn, depending on the MLM objective. When RESBERT is used in conjunction with STDMLM, we see a gradual performance", "figure_data": "ModelF1 (max) F1 (avg) Std. Dev.STDMLM69.0168.180.56SWITCHMLM70.7169.191.06FREQMLM69.4168.810.58STDMLM + RESBERT 9 SWMLM + RESBERT 2 SWMLM + RESBERT 2 + L aux HINGMBERT69.48 69.76 69.66 72.3668.99 69.23 69.29 71.420.60 0.64 0.25 0.70"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": ": HI-EN SA task scores with L3CUBE -185k pretraining corpus. The subscript with RESBERT rep-resents the residual connection layer for that particular setting.gain as we go deeper down the layers. Whereas wefind a slightly fluctuating response in the case ofSWITCHMLM-here, it peaks at some early layer.The complete trend is elaborated in Appendix D.The residual connections undoubtedly help. We seean overall jump in performance from STDMLM toRESBERT + STDMLM and from SWITCHMLMto RESBERT + SWITCHMLM.The auxiliary loss over switch-points describedin Section 2.2.2 aims to help encode switch-pointinformation more explicitly. As with RESBERT,we use the auxiliary loss with SWITCHMLM pre-training for HI-EN and ES-EN, and with FRE-QMLM pretraining for TA-EN and ML-EN. Asshown in Table 1, SW/FREQMLM + RESBERT+ L aux yields our best model for code-switched mBERT consistently across all SA tasks."}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "LID assignment comparison for NLL and X-HIT sentences. These sentences have ground-truth LID tags associated with them.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "FREQMLM QA scores (fine-tuned on 40 epochs) for experiments incorporating AMB tokens", "figure_data": "Test ResultsVal ResultsMethod Max Avg Stdev Avg StdevSTDMLM + RESBERTlayer 1 68.2 67.7 layer 2 68.5 67.9 layer 3 69.3 68.2 layer 4 68.8 68.2 layer 5 69.6 68.7 layer 6 68.9 68.3 layer 7 69.5 68.3 layer 8 69.5 68.5 layer 9 68.4 68.4 layer 10 69.4 68.80.4 0.8 1 0.6 0.7 0.5 1.1 0.7 0 0.463.3 63.6 63.6 63.6 63.3 63.6 63.9 63.8 64.1 640.3 0.3 0.5 0.4 0.5 0.2 0.1 0.2 0.3 0.2SWITCHMLM + RESBERTlayer 1 68.8 layer 2 69.4 68.9 68 layer 3 69 68.4 layer 4 68.6 68.1 layer 5 68.6 68.2 layer 6 68.5 67.8 layer 7 69.9 68.1 layer 8 68.9 68.2 layer 9 69.5 68.6 layer 10 68.8 680.6 0.5 0.4 0.4 0.3 0.5 1.3 0.8 0.7 0.663.2 63.8 63.4 63.7 63.8 63.6 63.6 63.6 62.9 63.70.4 0.5 0.3 0.6 0.4 0.4 0.5 0.2 0.1 0.2"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "RESBERT results for COMBINED-CS (HI-EN language pair). We choose the best layer to draw a residual connection based on the results achieved on the Validation set of the SA Task.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "X mlm = M(X, S, f )", "formula_coordinates": [2.0, 369.03, 282.54, 92.5, 18.93]}, {"formula_id": "formula_1", "formula_text": "S = [1, 1, 1, 1, 0, 0].", "formula_coordinates": [2.0, 407.08, 604.38, 87.62, 10.91]}, {"formula_id": "formula_2", "formula_text": "L aux = \u03b1 n i=1 \u2212 log MLP(x i )+\u03b2 L j=1 ||W j \u2212W j || 2", "formula_coordinates": [4.0, 70.86, 359.75, 223.57, 34.69]}, {"formula_id": "formula_3", "formula_text": "Perf(f [B(X), \u03d5(X)]) \u2212 Perf(f ([B, 0]))", "formula_coordinates": [7.0, 327.11, 534.55, 176.35, 20.55]}, {"formula_id": "formula_4", "formula_text": "p 0 n 0 + p 1 n 1 + p 2 n 2 = p(n 0 + n 1 + n 2 )", "formula_coordinates": [13.0, 337.79, 590.89, 165.9, 10.71]}], "doi": "10.48550/ARXIV.1610.01644"}