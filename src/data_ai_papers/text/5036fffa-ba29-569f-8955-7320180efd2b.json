{"title": "Archipelago: Nonparametric Bayesian Semi-Supervised Learning", "authors": "Ryan Prescott Adams", "pub_date": "", "abstract": "Semi-supervised learning (SSL), is classification where additional unlabeled data can be used to improve accuracy. Generative approaches are appealing in this situation, as a model of the data's probability density can assist in identifying clusters. Nonparametric Bayesian methods, while ideal in theory due to their principled motivations, have been difficult to apply to SSL in practice. We present a nonparametric Bayesian method that uses Gaussian processes for the generative model, avoiding many of the problems associated with Dirichlet process mixture models. Our model is fully generative and we take advantage of recent advances in Markov chain Monte Carlo algorithms to provide a practical inference method. Our method compares favorably to competing approaches on synthetic and real-world multi-class data.", "sections": [{"heading": "Introduction", "text": "Semi-supervised learning (SSL) algorithms solve the problem of classification under the circumstance that only a subset of the training data are labeled. In contrast to the purely-supervised setting, semi-supervised learning assumes that the probability density of the data is important to discovering the decision boundary. Semi-supervised learning is motivated by the situation where copious training data are available, but hand-labeling the data is expensive.\nFrom a Bayesian perspective, the natural way to perform semi-supervised learning is with a generative model of the data. One explicitly models the densities that gave rise to the observations, integrating out the class assignments for unlabeled data. We can then integrate out any parameters of the density model and arrive at predictive classifications of unseen data.\nWe would like to perform this modeling while making the fewest possible assumptions about the densities in question. Nonparametric Bayesian methods are appealing in this regard, as they incorporate an infinite number of parameters into the model. By using a model with an infinite number of parameters, we do not have to perform difficult computation to determine the appropriate dimensionality.\nUnfortunately, many nonparametric Bayesian density models are ill-suited for the semi-supervised setting. Specifically, the main assumption in SSL is that data of the same class will cluster together; the labeled data provide the appropriate class and we use the unlabeled data to determine the cluster boundary. The most common nonparametric Bayesian density model, the Dirichlet Process Mixture Model (DPMM) (Escobar & West, 1995), suffers from the problem that the mixture components are located independently. There is no tendency for mixture models to form contiguous densities. If we take the natural approach of using a DPMM to flexibly model each class density in a semisupervised learning problem, the mixtures will take unlabeled data away from other clusters.\nDue to these difficulties with specifying a flexible density model, discriminative methods are more common than generative models for semi-supervised learning. The Bayesian discriminative model of Lawrence and Jordan (2005) use a Gaussian process (GP) to construct a nonparametric model for binary SSL. Similarly, Chu et al. (2007) and Sindhwani et al. (2007) specify nonparametric Bayesian discriminative models with GPs that exploit graph-based side information.\nIn this paper, we present a fully-Bayesian generative approach to semi-supervised learning that uses Gaussian processes to specify the prior on class densities. We call the model Archipelago as it performs Bayesian clustering with infinite-dimensional density models, but it prefers contiguous densities. These clusters can form irregular shapes, like islands in a chain.", "publication_ref": ["b6", "b8", "b5", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "The Archipelago Model", "text": "We consider a model on a D-dimensional real space, R D , with K possible labels. Let x be a point in R D . We define K scalar functions (one for each class) {g k (x)} K k=1 , where g k (\u2022) : R D \u2192 R. Conditioned on a point x, these functions are used with a softmax function to construct a categorical distribution for l \u2208 1, 2, . . . , K, the class label of x:\np(l | x, {g k (x)} K k=1 ) = exp{g l (x)} \u039b(x) (1)\nwhere we use \u039b(x) = K k=1 exp{g k (x)} for notational convenience. We combine this discriminative classifier with a density model for x that is also constructed from the g k (x):\np(x | {g k (x)} K k=1 ) = 1 Z \u039b(x) \u03c0(x) 1 + \u039b(x)(2)\nwhere \u03c0(x) is a base density that will be explained shortly. The constant Z is the normalization given by\nZ = R D dx \u039b(x) \u03c0(x) 1 + \u039b(x) .(3)\nWe combine Eqs. 1 and 2 to construct a joint likelihood for the location and label together:\np(x, l | {g k (x)} K k=1 ) = 1 Z exp{g l (x)} \u03c0(x) 1 + \u039b(x) .(4)\nFor the semi-supervised learning problem, this construction is appealing because Eq. 2 can be viewed as marginalizing out the class label in Eq. 4. This is in contrast to typical Bayesian generative methods for SSL which would construct p(x, l) from p(x | l)p(l), which does not necessarily allow the class label to be easily marginalized out. We have a set of K coupled densities which are each intractable individually, but whose sum is tractable.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Gaussian Process Prior on g k (x)", "text": "To make the Archipelago model nonparametric, we use K independent Gaussian process priors on the functions {g k (x)} K k=1 . The GP allows one to specify general beliefs about functions, without choosing a finite set of basis functions. The idea of the GP is that the joint distribution over a discrete set of function values is a multivariate Gaussian determined by the corresponding inputs. The covariance matrix and mean vector that parameterize this Gaussian distribution arise from a positive definite covariance function C(\u2022, \u2022) : R D \u00d7 R D \u2192 R and a mean function m(\u2022) : R D \u2192 R. Typically, the covariance function is chosen so that points near to each other in the input space have strongly correlating outputs. We will assume that the the kth GP covariance function has hyperparameter \u03b8 k . We will take the mean functions to be zero. See Rasmussen and Williams (2006) for a comprehensive treatment of Gaussian processes.\n(d) (c) (b) (a) \u03c0(x) g 2 (x) g 3 (x) {x} g 1 (x) {r}", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "The Generative Procedure", "text": "Coupling the likelihood of Eq. 4 with GP priors on the {g k (x)} K k=1 enables us to define a fully-generative process for arriving at labeled data from a set of K random coupled densities. In other words, we can define a process that is equivalent to drawing a set of Algorithm 1 Generate Q labeled data from the prior in Section 2 Inputs: Base density \u03c0(x), GP hyperparameters {\u03b8 k } K k=1 , number of samples to generate Q Outputs:\nQ data/label pairs Q = {x q , l q } Q q=1 for k \u2190 1 . . . K do X k \u2190 \u2205, G k \u2190 \u2205 \u22b2 Initialize conditioning sets. end for Q \u2190 \u2205 \u22b2 Initialize return value. while |Q| < Q do \u22b2 Until Q values accepted. x \u223c \u03c0(x)\n\u22b2 Draw a proposal from the base density.\nfor k \u2190 1 . . . K do g k (x) \u223c GP(g |x, X k , G k , \u03b8 k ) \u22b2 Draw the function from the GP. \u039b(x) \u2190 \u039b(x) + exp{g k (x)} X k \u2190 X k \u222ax, G k \u2190 G k \u222a g k (x)\n\u22b2 Update conditioning sets. end for r \u223c U(0, 1)\n\u22b2 Draw a uniform variate on (0, 1).\nfor k \u2190 1 . . . K do \u03c1 k \u2190 \u03c1 k\u22121 + exp{g k (x)} 1+\u039b(x)\n\u22b2 Calculate the next partition. if r < \u03c1 k then Q \u2190 Q \u222a {x, k} \u22b2 Accept this proposal and store. break end if end for end while data from the density in Eq. 2 with a random \u039b(x) arising from the GP priors on {g k (x)} K k=1 , and then labeling that data according to Eq. 1.\nTo generate a labeled datum, we first draw a proposalx from the base density \u03c0(x). We then draw a function value from each of the GPs at the pointx. We denote these function draws as {g k (x)} K k=1 . We use the function values to partition the unit interval [0, 1] into K + 1 non-overlapping segments with boundaries 0, \u03c1 1 , \u03c1 2 , . . . , \u03c1 K , 1 where\n\u03c1 k = \u03c1 k\u22121 + exp{g 1 (x)} 1 + \u039b(x)(5)\nand \u03c1 0 = 0. We draw a uniform random variate r from (0, 1) and either assign it label k if \u03c1 k\u22121 < r < \u03c1 k , or reject the point if r \u2265 \u03c1 K . If we reject thex, we make another proposal and continue until we accept.\nWhen making these new proposals, we sample conditionally from the GP, incorporating the function draws from previous proposals. We call the set of information already known about function g k (x) the conditioning set, with inputs X k and outputs G k . We can continue this rejection/acceptance process, generating as many data points as we wish, \"discovering\" the set of functions {g k (x)} K k=1 as we iterate. This procedure is given in Algorithm 1 and shown graphically in Figure 1. This generative procedure is infinitely exchangeable and the data are exact in the sense that they are not biased by the starting state of a finite Markov chain. Infinite exchangeability means that the probability of a set of data is the same under any ordering. It is also not necessary to determine the {g k (x)} K k=1 over any more than a finite number of points in R D , nor is it necessary to determine the normalization constant Z in order to generate these data.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Inference", "text": "We have so far described a nonparametric generative model for labeled data. We now use this model to perform semi-supervised learning. The Archipelago inference task to find the predictive distribution over class labels of an unlabeled datum x \u22c6 , given N labeled data {x n , l n } N n=1 and P unlabeled data {x p } P p=1 , integrating out the latent function {g k (x)} K k=1 . Treating the function g k (x) as an infinite vector g k , our objective is to integrate out these K vectors: 6) where the posterior distribution on {g k } K k=1 arises from Eq.s 2 and 4 and is proportional to\np(l \u22c6 | x \u22c6 , {x n , l n } N n=1 , {x p } P p=1 | {\u03b8 k } K k=1 ) = dg 1 \u2022 \u2022 \u2022 dg K p(l \u22c6 | x \u22c6 , {g k } K k=1 ) \u00d7 p({g k } K k=1 | {x n , l n } N n=1 , {x p } P p=1 , {\u03b8 k } K k=1 ) (\np({g k } K k=1 , {x n , l n } N n=1 , {x p } P p=1 | {\u03b8 k } K k=1 ) = Z \u2212N \u2212P K k=1 GP(g k | {x n } N n=1 , {x p } P p=1 , \u03b8 k ) \u00d7 N n=1 exp{g ln (x n )} \u03c0(x n ) 1 + \u039b(x n ) P p=1 \u039b(x p ) \u03c0(x p ) 1 + \u039b(x p ) .(7)\nGiven a set of samples of {g k } K k=1 , we can es-timate the distribution in Eq. 6 via a sum, but acquiring samples from the posterior on the g k is difficult for two reasons: 1) Each g k is an infinite-dimensional object and we cannot store it na\u00efvely in memory, and 2) the posterior distribution\np({g k } K k=1 | {x n , l n } N n=1 , {x p } P p=1 , {\u03b8 k } K k=1\n) is doublyintractable due to the presence of Z in Eq. 7.\nIn Bayesian inference, it is common to have an unnormalized posterior distribution, where the unknown constant is intractable, but not dependent on the parameters; Markov chain Monte Carlo (MCMC) methods are well-suited for this situation. Doublyintractable posterior distributions (Murray et al., 2006) are those in which the likelihood function also has an intractable constant that does depend on the model parameters. This situation arises most commonly in undirected graphical models, such as Ising models (where Z is called the partition function), but it is also found in density models and doubly-stochastic processes, e.g. the Log Gaussian Cox Process. In such cases, even MCMC -the \"sledgehammer\" of Bayesian inference -is difficult or impossible to apply.\nFortunately, recent advances in MCMC methods have made it possible to perform inference in doublyintractable models under special circumstances. If it is possible to generate \"fantasy data\" exactly from a particular setting of the parameters, then it is possible to perform inference using the method of M\u00f8ller et al. (2006), Exchange Sampling (Murray et al., 2006), or modeling of the latent history of the generative procedure (Beskos et al., 2006). In general, these methods were developed with the undirected graphical model in mind, where it is sometimes possible to generate exact data via Coupling From The Past. In the case of Archipelago, we follow the approach of Adams et al. (2009) in modeling the latent history of the generative process. This approach not only resolves the problem of double intractability, but it also results in a Markov chain with an infinite-dimensional state that requires only a finite amount of memory.\nGiven a set of data on which we place the prior of Section 2, we are asserting that these data were generated via the procedure of Section 2.2. If we knew the history of the generative process that resulted in our observed data, then we would know everything necessary to make a prediction at a new location x \u22c6 . We perform posterior inference on this latent history by introducing a set of latent variables: 1) the number of latent rejections M , 2) the locations of the latent rejections {x m } M m=1 , and 3) the values of the functions at the rejections, at the labeled data and at the unlabeled data. We can run the generative algorithm without calculating Z and without knowing the functions g k (x) everywhere, and via the latent variable model we are able to inherit these properties for inference.\nIntegrating out the classes of the unlabeled data, as in Eq. 2, the joint distribution over the function values, the number and location of the latent rejections and the fixed data is\np({g k } K k=1 , M, {x m } M m=1 , {x p } P p=1 , {x n , l n } N n=1 ) = K k=1 GP({g k (x n )} N n=1 , {g k (x p )} P p=1 , {g k (x m )} M m=1 ) \u00d7 N n=1 exp{g ln (x n )} \u03c0(x n ) 1 + \u039b(x n ) P p=1 \u039b(x p ) \u03c0(x p ) 1 + \u039b(x p ) \u00d7 M m=1 \u03c0(x m ) 1 + \u039b(x m ) . (8)\nThis joint distribution is implicitly conditioned on the GP hyperparameters {\u03b8 k } K k=1 . We have abused notation slightly by not explicitly conditioning the GP on the input locations.\nThe joint distribution in Eq. 8 is proportional to the posterior distribution over the latent state, and we sample from it with three kinds of Markov transitions: updating the number of latent rejections, updating the rejection locations, and updating the function values.", "publication_ref": ["b10", "b9", "b10", "b4", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Sampling the Number of Rejections", "text": "We take advantage of the infinite exchangeability of the generative process to construct a Metropolis-Hastings move that can add or remove rejections from the latent history. Our model for inference is that the data came about as the result of Algorithm 1, stopping when there were N +P data accepted. The unlabeled data were generated by the P labels being discarded, and the joint distribution in Eq. 8 integrates out the unknown label. Due to infinite exchangeability, we can propose a reordering of the latent history at any time and this move will always be accepted. Thus to insert a new latent rejection, we propose moving a rejection that occurred after the N +P th acceptance to be moved to sometime before it. This idea is shown graphically in Figure 2. To delete a rejection, we make the opposite type of transition: select one of the current rejections at random and propose moving it to after the N +P th acceptance.\nIn practice, it is not necessary to actually maintain an ordering. We an additional rejection, we first propose a new location for it,x, by drawing it from \u03c0(x). We then draw each of the K function values at that location, {g k (x)} K k=1 , conditioning on all of the current function values. Finally, we accept or reject with Metropolis-Hastings acceptance ratio\na ins = (1 \u2212 b(M + 1, N + P )) (M + N + P ) b(M, N + P ) (M + 1) (1 + \u039b(x)) .(9)\nIf we are proposing a deletion, we choose an index m uniformly from among the M rejections and remove it with Metropolis-Hastings acceptance ratio\na del = b(M \u2212 1, N + P ) M (1 + \u039b(x m )) (1 \u2212 b(M, N + P )) (M + N + P \u2212 1) .(10)\nIn practice we have often found it reasonable to simply set b(M, N + P ) = 1 2 . More sophisticated proposals may yield improvements in mixing, but we have not thoroughly explored this topic. Typically we make several of these transitions (\u2248 10) for each of the transitions in Sections 3.2 and 3.3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sampling the Rejection Locations", "text": "Conditioned on the current function values and the number of rejections M , we also want to update the locations of the rejections. We iterate over each of the M rejections and make a Metropolis-Hastings proposal with two stages. First, we draw a new locationx m from a proposal density q(x m \u2190 x m ). Second, we draw a new set of function values at this location, {g k (x m )} K k=1 , from the GP, conditioning on all of the current function values and locations. We then accept or reject the proposal according to the acceptance ratio\na m = q(x m \u2190x m ) \u03c0(x m ) (1 + \u039b(x m )) q(x m \u2190 x) \u03c0(x m ) (1 + \u039b(x m )) .(11)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sampling the Function Values", "text": "Sampling the function is the most critical aspect of the model: it is the function values that we depend on to make predictions. We iterate over each of the K functions and use Hamiltonian Monte Carlo (HMC) (Neal, 1997) for efficient sampling. HMC augments the Markov chain with random \"momenta\" and then simulates Hamiltonian dynamics in fictional time.\nIn this way, Metropolis-Hastings proposals are guided using gradient information and random walk behavior is avoided. We perform gradient calculations in the \"whitened\" space resulting from transforming the function values with the inverse Cholesky decomposition of the covariance matrix. This is a more natural metric space in which to find gradients. For the kth function, the log conditional posterior distribution is\nln p(g k | M, {x m } M m=1 , {x n , l n } N n=1 , {x p } P p=1 , \u03b8 k ) = \u2212 1 2 g T k \u03a3 \u22121 g k + g T k 1 ln=k \u2212 N n=1\nln(1 + \u039b(x n )) ( 12)\n+ P p=1 ln \u039b(x p ) 1 + \u039b(x p ) \u2212 M m=1 ln(1 + \u039b(x m )) + const\nwhere 1 ln=k is a vector whose components are 1 where l n = k, and 0 otherwise. \u03a3 is the covariance matrix resulting from application of the covariance kernel to the concatenation of labeled, unlabeled and rejected data locations. Eq. 12 comes from: 1) a GP prior term, 2) a labeled data acceptance term, 3) an unlabeled data acceptance term, and 4) a rejection term.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Sampling the Hyperparameters", "text": "An appealing feature of Bayesian inference methods is the ability to perform hierarchical inference. Here, we sample the hyperparameters, {\u03b8 k } K k=1 , governing the GP covariance function. Conditioned on the number and locations of the rejections, and the function values, we do hyperparameter inference via Hamiltonian Monte Carlo as described by Neal (1997).\nWe can also introduce hyperparameters \u03c6 into the base density \u03c0(x). For example, if \u03c0(x) is a Gaussian distribution, \u03c6 might be the mean and covariance parameters. Conditioning on the number and locations of the rejections, inference of \u03c6 is a standard MCMC parametric density estimation problem that can be solved with Gibbs sampling or Metropolis-Hastings.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Making Predictions", "text": "With samples from the posterior distribution over the function values, we can now approximate the integral in Eq. 6. At each MCMC step after burn-in, we sample the function values {g k (x \u22c6 )} K k=1 from the GP, conditioned on the current state. From these we can approximate the predictive distribution via a mixture of softmax functions. We might also be interested in the class-conditional predictive distributions. These K distributions are the ones that arise on data space, conditioning on membership in class k, but integrating out the latent function and hyperparameters. While not available in closed form, it is straightforward to generate predictive fantasies. At each MCMC step after burn-in, run the generative process forward from the current state until a datum is accepted that is a member of class k.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Empirical Results", "text": "We compared the Archipelago model and inference method to three other multi-class Gaussian process classification approaches: the standard softmax GP classifier (Williams & Barber, 1998), the probit classifier of Girolami and Rogers (2006), and the multiclass extension (Rogers & Girolami, 2007) of the Null Category Noise Model of Lawrence and Jordan (2005). We compared these models on three two-dimensional toy problems and two real-world datasets. Unlabeled data were ignored in the softmax and probit models.", "publication_ref": ["b19", "b7", "b15", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Toy Pinwheel Data", "text": "The toy problems were noisy pinwheels with three, four and five classes, shown in Figures 3(a), 3(d), and 3(g), respectively. There were 50 data points in each set and the algorithms were run with 1, 2, 4, and 8 labeled data in each class. 300 held-out data for each class were used to evaluate the predictive distributions by error rate and perplexity. Inference for each method was performed using MCMC, and each used an isotropic squared-exponential covariance function. For each method we also sampled from the length-scale and amplitude hyperparameters using Hamiltonian Monte Carlo (HMC) (Neal, 1997). The Archipelago base density was a bivariate Gaussian. Figure 3 shows the predictive modes for the Archipelago and softmax models, as well as the entropy of the Archipelago predictive distribution as a function of space. Numerical results are in Table 1.", "publication_ref": ["b11"], "figure_ref": ["fig_2", "fig_2"], "table_ref": ["tab_0"]}, {"heading": "Wine Data", "text": "The wine data are thirteen-dimensional, with three classes from Aeberhard et al. (1992) via Asuncion and Newman (2007). As in Rogers and Girolami (2007), we translated and scaled the inputs to have zero mean and unit variance. We used an ARD covariance function (Rasmussen & Williams, 2006) and HMC for in- ference of length scales and amplitude. We divided the data into two sets of 89 for training and testing, with the classes divided approximately evenly. We ran four sets of experiments with each method, as with the toy data, with 1, 2, 4, and 8 labeled data in each class. We used a multivariate Gaussian for the Archipelago base density. The results are given in Table 1.\n(a) Toy3 Arch (b) Toy3 SmGP (c) Arch Entropy (d) Toy4 Arch (e) Toy4 SmGP (f) Arch Entropy (g) Toy5 Arch (h) Toy5 SmGP (i) Arch Entropy", "publication_ref": ["b2", "b3", "b15", "b14"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Oil Pipe Data", "text": "The oil pipe data are included with software for Williams and Barber (1998). The task is to classify the type of flow in a pipe (laminar, hydrogenous, or amorphous), using a set of fourteen gamma ray readings. We split the 400 data into 134 training and 266 testing. We used an ARD covariance function and followed the same procedures as in Section 4.2. The results are given in Table 1.", "publication_ref": ["b19"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Discussion", "text": "Archipelago combines ideas both from Gaussian process density modeling and GP classification. When using a kernel such as the squared-exponential, it ex-presses the idea that similar data should have similar probabilities and similar class assignments. In contrast, an SSL approach with a mixture model for each class must maintain both a class and mixture component assignment for each datum, and the notion of smoothness does not extend beyond the parametric form of the components.\nThe most relevant Bayesian model to Archipelago is the discriminative Null Category Noise Model (NCNM) (Lawrence & Jordan, 2005) and the multiclass extension of Rogers and Girolami (2007). The NCNM enforces the idea of \"margin\" in a GP classifier by requiring a null-class transition between any two \"real\" classes. The latent rejections play a similar role in Archipelago by allowing regions with mass under \u03c0(x) to have no data. However, Archipelago models data in the null class explicitly as part of inference, to let Archipelago model arbitrarily complex densities. In contrast to Archipelago, NCNM requires a priori setting of the null-category width and labeling probability. In almost all of our tests, Archipelago had lower test classification error than the NCNM. In the regime with few labels, Archipelago outperformed the other methods on test error.\nIn supervised classification and regression with GPs (and in the NCNM), it is sufficient to know only the values of the functions at the observed data. In density modeling, however, the areas where there is little density are as important as those with significant density. Unfortunately, data are only likely to be observed in areas of notable density, so we must have some way to \"peg down\" the functions in the low-density areas.\nThe explicitly-modeled latent rejections, although a part of the generative model, fulfill this need in a way that would be difficult to motivate in a purely discriminative setting.", "publication_ref": ["b8", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Computational Considerations", "text": "Gaussian processes have relatively high computational costs, and Archipelago inherits them. With a na\u00efve implementation, the time complexity per MCMC step is O(K(N + P + M )) 3 and the space complexity is O(K(N\n+ P + M ) 2 ).\nThere is a large literature on sparse approximations to GPs, e.g. Qui\u00f1onero-Candela and Rasmussen ( 2005), but we have not explored this topic. Mixing of Markov chains is an additional concern, but we have not found this to be a problem in practice due to our use of Hamiltonian Monte Carlo. Figure 4 shows an autocorrelation plot of the number of latent rejections during an MCMC run on the Toy5 data.\nAs the computational complexity grows rapidly with the number of latent rejections, minimizing these rejections is paramount. The number of latent rejections relates directly to the mismatch between the base density \u03c0(x) and the true data density. Hyperparameter inference as described in Section 3.4 can lower the computational burden by adapting \u03c0(x) to the data, but it is important to choose an appropriate parametric family. In high dimensions, this choice is difficult to make, and in the absence of significant domain knowledge we expect that Archipelago will not work well in high dimensions.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Future Directions and Model Variations", "text": "One advantage of Gaussian processes for density modeling is that GPs can be applied to alternate domains. Positive definite kernels exist for spaces such as graphs and permutation groups, and Archipelago can be applied directly to SSL problems in these domains.\nIn this paper we have assumed that the K GPs are independent. It may be useful to introduce dependency between the latent functions using, for example, the method of Teh et al. (2005). We have also assumed that the number of classes K is known and that an example from each class has been observed. It would be interesting to consider a model where the number of true classes can exceed the number of observed classes.\nIn this case, it may be useful to allow for infinite K.", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "Summary of Contributions", "text": "We presented a nonparametric Bayesian generative approach to the semi-supervised learning problem. It works for any number of classes and does not require any finite-dimensional approximation for inference. It improves over mixture-based Bayesian approaches to SSL while still modeling complex density functions.\nIn empirical tests, our model compares favorably with Bayesian discriminative approaches, particularly when little labeled data are available.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "Ryan Adams thanks the Gates Cambridge Trust and the Canadian Institute for Advanced Research.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "", "journal": "", "year": "2009", "authors": "R P Adams; I Murray; D J C Mackay"}, {"ref_id": "b1", "title": "The Gaussian process density sampler", "journal": "", "year": "", "authors": ""}, {"ref_id": "b2", "title": "Comparison of classifiers in high dimensional settings", "journal": "", "year": "1992", "authors": "S Aeberhard; D Coomans; O De Vel"}, {"ref_id": "b3", "title": "UCI machine learning repository", "journal": "", "year": "2007", "authors": "A Asuncion; D Newman"}, {"ref_id": "b4", "title": "Exact and computationally efficient likelihood-based estimation for discretely observed diffusion processes", "journal": "Journal of the Royal Statistical Society, Series B", "year": "2006", "authors": "A Beskos; O Papaspiliopoulos; G O Roberts; P Fearnhead"}, {"ref_id": "b5", "title": "Relational learning with Gaussian processes", "journal": "Advances in Neural Information Processing Systems", "year": "2007", "authors": "W Chu; V Sindhwani; Z Ghahramani; S S Keerthi"}, {"ref_id": "b6", "title": "Bayesian density estimation and inference using mixtures", "journal": "Journal of the American Statistical Association", "year": "1995", "authors": "M D Escobar; M West"}, {"ref_id": "b7", "title": "Variational Bayesian multinomial probit regression with Gaussian process priors", "journal": "Neural Computation", "year": "2006", "authors": "M Girolami; S Rogers"}, {"ref_id": "b8", "title": "Semisupervised learning via Gaussian processes", "journal": "", "year": "2005", "authors": "N D Lawrence; M I Jordan"}, {"ref_id": "b9", "title": "An efficient Markov chain Monte Carlo method for distributions with intractable normalising constants", "journal": "Biometrika", "year": "2006", "authors": "J M\u00f8ller; A N Pettitt; R Reeves; K K Berthelsen"}, {"ref_id": "b10", "title": "MCMC for doubly-intractable distributions", "journal": "Uncertainty in Artificial Intelligence", "year": "2006", "authors": "I Murray; Z Ghahramani; D Mackay"}, {"ref_id": "b11", "title": "", "journal": "", "year": "1997", "authors": "R M Neal"}, {"ref_id": "b12", "title": "Monte Carlo implementation of Gaussian process models for Bayesian regression and classification", "journal": "", "year": "", "authors": ""}, {"ref_id": "b13", "title": "A unifying view of sparse approximate Gaussian process regression", "journal": "Journal of Machine Learning Research", "year": "2005", "authors": "J Qui\u00f1onero-Candela; C E Rasmussen"}, {"ref_id": "b14", "title": "Gaussian processes for machine learning", "journal": "MIT Press", "year": "2006", "authors": "C E Rasmussen; C K I Williams"}, {"ref_id": "b15", "title": "Multi-class semisupervised learning with the \u01eb-truncated multinomial probit Gaussian process", "journal": "", "year": "2007", "authors": "S Rogers; M Girolami"}, {"ref_id": "b16", "title": "", "journal": "", "year": "2007", "authors": "V Sindhwani; W Chu; S S Keerthi"}, {"ref_id": "b17", "title": "Semi-supervised Gaussian process classifiers. International Joint Conference on Artificial Intelligence", "journal": "", "year": "", "authors": ""}, {"ref_id": "b18", "title": "Semiparametric latent factor models. International Conference on", "journal": "Artificial Intelligence and Statistics", "year": "2005", "authors": "Y W Teh; M Seeger; M I Jordan"}, {"ref_id": "b19", "title": "Bayesian classification with Gaussian processes", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "1998", "authors": "C K I Williams; D Barber"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. The generative process for Archipelago in one dimension. (a) Proposals are drawn from the base density \u03c0(x). (b) Next, each of the K = 3 functions is sampled from the Gaussian process at the proposal locations. (c) The functions are used to partition the unit interval into K + 1 slices at each proposal location and uniform variates r are drawn in the vertical coordinate. (d) If r falls in the topmost partition, the proposal is rejected. Otherwise, it is accepted and assigned the class label based on the partition in which r falls.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2. a) An initial state of the latent history, with three possible classes (K = 3, shown as , , and ), and some latent rejections (M = 6). b) Propose a new rejection after the last acceptance by running the procedure forward. c) Propose moving this new rejection into a random location in the history. d) If accepted, we now have a history with an additional latent rejection (M = 7).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 .3Figure 3. The results of applying Archipelago to \"pinwheel\" data with one labeled datum per class (marked with an 'x'). The top row has three classes, the middle row has four classes and the bottom has five. The figures on the left show the modes of the class assignments from Archipelago. The middle column shows the class assignment modes using a softmax GP. The right column shows the entropies of the predictive distribution produced by Archipelago.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 .4Figure 4. Autocorrelation plot of the number of rejections in Toy5 with one labeled datum during MCMC simulation.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Results from four methods applied to test data: Archipelago, the softmax GP (SmGP), the probit GP (PrGP) and the Null Category Noise Model (NCNM).", "figure_data": "Arch SmGP PrGP NCNM1err 0.167 0.347 0.290 perp 1.461 2.588 1.631 14.120 0.612Toy32 4err 0.111 0.312 0.279 perp 1.565 2.769 1.350 err 0.042 0.118 0.142 perp 1.092 1.310 1.041 1.002 0.462 4.801 0.1148err 0.037 0.056 0.137 perp 1.070 1.065 1.043 1.000 0.0701err 0.120 0.305 0.416 perp 2.478 3.436 1.239 16.160 0.554Toy42 4err 0.092 0.188 0.253 perp 2.471 2.861 7.896 err 0.032 0.119 0.077 perp 1.469 1.505 1.343 1.113 0.429 8.257 0.0568err 0.025 0.061 0.088 0.0533 perp 1.135 1.076 1.086 1.0091err 0.183 0.192 0.515 perp 3.766 4.519 3.533 294.758 0.482Toy52 4err 0.124 0.181 0.178 perp 2.811 2.008 1.476 11.669 0.365 err 0.081 0.159 0.113 0.093 perp 2.345 1.730 1.375 1.2108err 0.051 0.095 0.073 0.051 perp 2.105 1.290 1.201 1.0931err 0.141 0.260 0.303 perp 1.928 2.790 1.769 31.832 0.393Wine2 4err 0.135 0.292 0.213 perp 2.654 2.760 2.851 err 0.090 0.146 0.101 perp 1.198 2.440 1.387 1.153 0.393 2.799 0.1018err 0.0899 0.067 0.067 0.067 perp 1.084 1.250 1.224 1.0451err perp 2.997 3.024 5.631 3.56K 0.538 0.568 0.432 0.650Oil Pipe2 4err 0.297 0.331 0.308 perp 2.451 2.576 3.499 1.024 0.365 err 0.211 0.215 0.271 0.176 perp 1.658 1.864 2.558 1.3018err perp 1.241 1.137 1.232 1.002 0.068 0.053 0.068 0.038"}], "formulas": [{"formula_id": "formula_0", "formula_text": "p(l | x, {g k (x)} K k=1 ) = exp{g l (x)} \u039b(x) (1)", "formula_coordinates": [2.0, 99.98, 279.35, 189.46, 23.53]}, {"formula_id": "formula_1", "formula_text": "p(x | {g k (x)} K k=1 ) = 1 Z \u039b(x) \u03c0(x) 1 + \u039b(x)(2)", "formula_coordinates": [2.0, 98.68, 363.17, 190.76, 23.54]}, {"formula_id": "formula_2", "formula_text": "Z = R D dx \u039b(x) \u03c0(x) 1 + \u039b(x) .(3)", "formula_coordinates": [2.0, 120.14, 426.05, 169.3, 25.03]}, {"formula_id": "formula_3", "formula_text": "p(x, l | {g k (x)} K k=1 ) = 1 Z exp{g l (x)} \u03c0(x) 1 + \u039b(x) .(4)", "formula_coordinates": [2.0, 74.09, 488.37, 215.35, 23.53]}, {"formula_id": "formula_4", "formula_text": "(d) (c) (b) (a) \u03c0(x) g 2 (x) g 3 (x) {x} g 1 (x) {r}", "formula_coordinates": [2.0, 329.04, 75.48, 202.8, 263.58]}, {"formula_id": "formula_5", "formula_text": "Q data/label pairs Q = {x q , l q } Q q=1 for k \u2190 1 . . . K do X k \u2190 \u2205, G k \u2190 \u2205 \u22b2 Initialize conditioning sets. end for Q \u2190 \u2205 \u22b2 Initialize return value. while |Q| < Q do \u22b2 Until Q values accepted. x \u223c \u03c0(x)", "formula_coordinates": [3.0, 65.4, 95.15, 857.48, 73.7]}, {"formula_id": "formula_6", "formula_text": "for k \u2190 1 . . . K do g k (x) \u223c GP(g |x, X k , G k , \u03b8 k ) \u22b2 Draw the function from the GP. \u039b(x) \u2190 \u039b(x) + exp{g k (x)} X k \u2190 X k \u222ax, G k \u2190 G k \u222a g k (x)", "formula_coordinates": [3.0, 79.23, 169.65, 462.2, 39.09]}, {"formula_id": "formula_7", "formula_text": "for k \u2190 1 . . . K do \u03c1 k \u2190 \u03c1 k\u22121 + exp{g k (x)} 1+\u039b(x)", "formula_coordinates": [3.0, 79.23, 229.43, 105.92, 21.42]}, {"formula_id": "formula_8", "formula_text": "\u03c1 k = \u03c1 k\u22121 + exp{g 1 (x)} 1 + \u039b(x)(5)", "formula_coordinates": [3.0, 119.01, 460.6, 170.43, 23.54]}, {"formula_id": "formula_9", "formula_text": "p(l \u22c6 | x \u22c6 , {x n , l n } N n=1 , {x p } P p=1 | {\u03b8 k } K k=1 ) = dg 1 \u2022 \u2022 \u2022 dg K p(l \u22c6 | x \u22c6 , {g k } K k=1 ) \u00d7 p({g k } K k=1 | {x n , l n } N n=1 , {x p } P p=1 , {\u03b8 k } K k=1 ) (", "formula_coordinates": [3.0, 317.4, 521.23, 215.55, 55.64]}, {"formula_id": "formula_10", "formula_text": "p({g k } K k=1 , {x n , l n } N n=1 , {x p } P p=1 | {\u03b8 k } K k=1 ) = Z \u2212N \u2212P K k=1 GP(g k | {x n } N n=1 , {x p } P p=1 , \u03b8 k ) \u00d7 N n=1 exp{g ln (x n )} \u03c0(x n ) 1 + \u039b(x n ) P p=1 \u039b(x p ) \u03c0(x p ) 1 + \u039b(x p ) .(7)", "formula_coordinates": [3.0, 317.4, 613.75, 224.03, 83.21]}, {"formula_id": "formula_11", "formula_text": "p({g k } K k=1 | {x n , l n } N n=1 , {x p } P p=1 , {\u03b8 k } K k=1", "formula_coordinates": [4.0, 55.44, 128.28, 179.63, 12.69]}, {"formula_id": "formula_12", "formula_text": "p({g k } K k=1 , M, {x m } M m=1 , {x p } P p=1 , {x n , l n } N n=1 ) = K k=1 GP({g k (x n )} N n=1 , {g k (x p )} P p=1 , {g k (x m )} M m=1 ) \u00d7 N n=1 exp{g ln (x n )} \u03c0(x n ) 1 + \u039b(x n ) P p=1 \u039b(x p ) \u03c0(x p ) 1 + \u039b(x p ) \u00d7 M m=1 \u03c0(x m ) 1 + \u039b(x m ) . (8)", "formula_coordinates": [4.0, 317.4, 167.19, 224.03, 119.39]}, {"formula_id": "formula_13", "formula_text": "a ins = (1 \u2212 b(M + 1, N + P )) (M + N + P ) b(M, N + P ) (M + 1) (1 + \u039b(x)) .(9)", "formula_coordinates": [5.0, 69.35, 322.66, 220.08, 23.76]}, {"formula_id": "formula_14", "formula_text": "a del = b(M \u2212 1, N + P ) M (1 + \u039b(x m )) (1 \u2212 b(M, N + P )) (M + N + P \u2212 1) .(10)", "formula_coordinates": [5.0, 66.92, 408.35, 222.51, 23.75]}, {"formula_id": "formula_15", "formula_text": "a m = q(x m \u2190x m ) \u03c0(x m ) (1 + \u039b(x m )) q(x m \u2190 x) \u03c0(x m ) (1 + \u039b(x m )) .(11)", "formula_coordinates": [5.0, 76.21, 691.3, 213.22, 24.11]}, {"formula_id": "formula_16", "formula_text": "ln p(g k | M, {x m } M m=1 , {x n , l n } N n=1 , {x p } P p=1 , \u03b8 k ) = \u2212 1 2 g T k \u03a3 \u22121 g k + g T k 1 ln=k \u2212 N n=1", "formula_coordinates": [5.0, 319.0, 277.14, 215.44, 48.02]}, {"formula_id": "formula_17", "formula_text": "+ P p=1 ln \u039b(x p ) 1 + \u039b(x p ) \u2212 M m=1 ln(1 + \u039b(x m )) + const", "formula_coordinates": [5.0, 320.08, 323.03, 208.73, 30.35]}, {"formula_id": "formula_18", "formula_text": "(a) Toy3 Arch (b) Toy3 SmGP (c) Arch Entropy (d) Toy4 Arch (e) Toy4 SmGP (f) Arch Entropy (g) Toy5 Arch (h) Toy5 SmGP (i) Arch Entropy", "formula_coordinates": [6.0, 323.15, 142.46, 207.63, 189.64]}, {"formula_id": "formula_19", "formula_text": "+ P + M ) 2 ).", "formula_coordinates": [7.0, 104.03, 590.43, 58.62, 11.07]}], "doi": ""}