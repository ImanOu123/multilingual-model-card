{"title": "Distilling Script Knowledge from Large Language Models for Constrained Language Planning", "authors": "Siyu Yuan; Jiangjie Chen; Ziquan Fu; Xuyang Ge; Soham Shah; Charles Robert Jankowski; Yanghua Xiao \u2660 \u00b6; Deqing Yang", "pub_date": "", "abstract": "In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., \"make a cake\"), but leaves more specific goals with multi-facet constraints understudied (e.g., \"make a cake for diabetics\"). In this paper, we define the task of constrained language planning for the first time. We propose an overgenerate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, CoScript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, CoScript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability. 1   ", "sections": [{"heading": "Introduction", "text": "To accomplish everyday goals, humans usually plan their actions in accordance with step-by-step instructions. Such instructions are discovered as goal-oriented scripts Abelson, 1975, 2013), involving a set of prototypical event sequences to achieve goals. For the example in Figure 1, to achieve the goal (make a cake), one usually has to follow certain steps of instructions, e.g., gather ingredients, preheat the oven, etc. The planning for such step-by-step scripts chains up reasoning toward complex goals (Abelson, 1976;. Therefore, the automation of planning envisions more intelligent and reasonable AI 1) Gather your ingredients.\n2) Preheat oven to 350 \u00b0F. Grease an 8-inch (20 cm) cake pan with butter or margarine. 3) In a medium bowl, combine flour, baking powder and salt. 4) In a large bowl, cream butter or margarine and sugar together until light and fluffy. Beat in eggs, one at a time. 5) Add vanilla extract and almond extract. 6) Gradually add flour mixture to the large bowl, mixing until just blended. 7) Pour batter into prepared cake pan. 8) Bake cake for 30 minutes. 9) Remove cake from oven and let cool before serving.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Script for the Goal: Make a Cake for Diabetics", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sugar for diabetics?! InstructGPT", "text": "(text-davinci-002)\nSteps Figure 1: A list of steps InstructGPT generates to plan for the goal \"make a cake for diabetics\". InstructGPT mistakenly adds sugar to the cake, which is unfit for diabetic patients. This example shows that InstructGPT sometimes cannot effectively and faithfully script for a specific goal with fine-grained constraints.\nsystems in various domains, such as executable robotic systems (Kovalchuk et al., 2021; and reasoning systems for problemsolving .\nRecent studies have identified that language models (LMs) can be used to plan scripts (Sancheti and Rudinger, 2022). Previous work  has shown that large language models (LLMs), such as GPT-3 (Brown et al., 2020) InstructGPT (Ouyang et al., 2022) and PaLM (Chowdhery et al., 2022), can effectively decompose goals into procedural steps in a zero-/few-shot manner. To train specialized models, researchers have proposed datasets for the automatic understanding and generation of script knowledge (Schank and Abelson, 1975;Regneri et al., 2010;Wanzare et al., 2016;Lyu et al., 2021;Sak-aguchi et al., 2021). However, previous work mainly focuses on planning for the abstract goals of stereotypical activities (Abelson, 1976). Planning for goals with specific constraints (e.g., for diabetics) still remains under-studied.\nIn this paper, we define the problem of constrained language planning, which imposes different constraints on the goals of planning. An abstract goal, for example, make a cake, can be inherited by different real-life specific goals with multi-faceted constraints. A cake can be made for 1) different ingredients (e.g., chocolate or vanilla); 2) various tools (e.g., with a microwave or an oven); or 3) different purposes (e.g., for a wedding or a birthday party). A good planner should write scripts that are reasonable and faithful to constraints. However, LLMs sometimes do not plan faithfully toward the constraints. As showcased in Figure 1, InstructGPT suggests adding sugar to the cake for diabetic patients. Also, due to a shortage of datasets for constrained language planning, the ability of smaller but specialized models to plan with specific constraints has been underexplored.\nIn this paper, we aim to evaluate and improve the constrained language planning ability of LLMs, while distilling a dataset from LLMs to train specialized models. Our empirical study finds that LLMs tend to plan fluently but unfaithfully to the constraints. Thus, we employ an over-generatethen-filter approach (Wiegreffe et al., 2022) to satisfy the quality of the generated scripts to constraints. The main idea is to select high-quality ones from multiple generated scripts. Then, we use LLMs (e.g., InstructGPT) with this approach to generate a dataset for constrained language planning, which inherits the idea of symbolic knowledge distillation from models (West et al., 2022). We thus arrive at a Constrained Script dataset, i.e., CoScript, which consists of 55,000 high-quality scripts with specific goals and steps. Experiments show that, when trained on CoScript, smaller models such as T5 (Raffel et al., 2020) can achieve good performance, even surpassing that of LLMs.\nOur contributions are summarized as follows: 1) To our knowledge, we are the first to establish the constrained language planning problem, which advances language planning toward more specific goals. 2) We evaluate the few-shot constrained language planning ability of LLMs and develop an over-generate-then-filter method for LLMs, resulting in a 26% increase in accuracy. 3) Based on our method, we use LLMs to generate a highquality script dataset (CoScript) for constrained language planning. By leveraging the CoScript, we endow specialized and smaller models with constrained language planning ability, which achieves comparable performance to that of LLMs.", "publication_ref": ["b24", "b45", "b6", "b47", "b42", "b53", "b31", "b0", "b56", "b55", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Language Planning Language planning aims to decompose a goal into sequences of steps (Kaplan and Baldauf, 1997), which is widely used in robotics (Kaiser et al., 2014;Paxton et al., 2019;Berg et al., 2022) and procedural text generation (Goldfarb-Tarrant et al., 2020;Hu et al., 2022). Early studies approach language planning with syntactic parsing for the context (Koller and Stone, 2007;Garoufi and Koller, 2010). Recently, researchers have investigated the planning capability of language models in various domains (Olmo et al., 2021;Valmeekam et al., 2022). However, they mainly focus on generating scripts for stereotypical activities toward abstract goals. For example,  proposes to plan for the general-typed tasks for embodied agents, while Yang et al. (2021) edits actions for abstract goals to video retrieval. In contrast, we explore planning for specific goals (e.g., \"make a cake for diabetics\").  has benchmarked LLMs for planning with included/excluded objects, but they merely study this problem in a limited scope (only dozens of cases) without further in-depth analysis.\nScripts A structure describing a sequence of events in a particular scenario is script (Schank and Abelson, 1975), consisting of two types: 1) Narrative script: a narrative chain of events describing a particular scenario derived from narrative texts such as recipes (Fang et al., 2022) or stories (Tandon et al., 2020); 2) Goal-oriented script (Regneri et al., 2010;Wanzare et al., 2016): an appropriate sequence of steps as instructions to achieve a goal. In this work, the steps for achieving a given goal in language planning can be categorized into the second class. Many datasets for goal-oriented scripts have been proposed to improve the language planning ability of LMs (Sakaguchi et al., 2021;Lyu et al., 2021). However, they mainly consist of abstract goals with prototypical instructions and thus are not built to train LMs for planning with more specific goals.  1: Three types of constraints and their definitions that are used to prompt for new instances of specific goals. In the examples (Ex.), upon the abstract goal, we give two instances for each type of constraint by combining the goal with constraints into specific goals. The constraint within each example is highlighted .\nIn-Context Learning With the great success of LLMs (Brown et al., 2020;Ouyang et al., 2022;Chowdhery et al., 2022), in-context learning (Brown et al., 2020;Min et al., 2022) has established its great task-solving potentials with a textual task instruction and a few examples. Moreover, when being used for dataset construction, the data samples that LLMs generate can sometimes outperform crowd-sourced human-authored data in factuality and fluency (Lu et al., 2022a;Min et al., 2022). This shows a promising alternative to costly large-scale crowd-sourcing to construct datasets using LLMs (Wiegreffe et al., 2022;Liu et al., 2022a;West et al., 2022). Inspired by these studies, in our work, we adopt the in-context learning for LLMs not only for better language planning, but also as a reliable crowd-worker to scale up the planning data into a reusable dataset to train smaller models.", "publication_ref": ["b19", "b18", "b39", "b2", "b15", "b16", "b22", "b14", "b35", "b51", "b58", "b47", "b9", "b42", "b53", "b31", "b6", "b32", "b29", "b32", "b56", "b27", "b55"], "figure_ref": [], "table_ref": []}, {"heading": "Definitions", "text": "Before diving into technical details, we first clarify some important terms used in the paper.\nScripts A goal-oriented script is a list of steps (S = {s 1 , s 2 , \u2022 \u2022 \u2022 , s |S| }) that fulfill a certain goal (G) (e.g., \"make a cake\") (Suddendorf and Corballis, 2007;Schank and Abelson, 2013). The language planning task is defined as M : G \u2192 S, where M is the planning model. Goals Different from previous studies that focus mostly on abstract goals with prototypical scripts,  we define a taxonomic structure of goals by extending the derivatives of abstract goals. We define a specific goal that inherits from an abstract one but with new information as a constraint to limit the scope. An abstract goal, denoted as G a , refers to stereotypical activities, e.g., \"make a cake\". A specific goal, denoted as G c , is derived from the corresponding G a with various constraints, e.g., \"make a chocolate cake\".", "publication_ref": ["b49", "b48"], "figure_ref": [], "table_ref": []}, {"heading": "Constraints and Constrained Language Planning", "text": "To enrich the semantics of specific goals, we define three types of constraints, i.e., modifier, method and intent, as shown in Table 1. They express different angles of extending an abstract goal and can be further instantiated and concreted. Constrained language planning denotes generating a constraint-faithful script S : S = M(G c ) toward specific goals (G c ) with various constraints (C).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Constrained Language Planning with LLMs", "text": "In this section, we evaluate and enhance the constraint language planning ability of LLMs. The overall workflow is illustrated in Figure 2. We first extend the specific goals G c from the abstract ones G a using a human-in-the-loop acquisition approach with LLMs ( \u00a7 4.2, Step 1), and propose an over-generate-then-filter framework to obtain scripts ( \u00a7 4.3,. Then, we reveal that LLMs (e.g., GPT-3 (Brown et al., 2020), In-structGPT (Ouyang et al., 2022)) are prone to be unfaithful to the constraints in G c , and our approach can alleviate this problem ( \u00a7 4.4). We use text-davinci-002 as the default InstructGPT variant, which has \u2265175B parameters. 2 ", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "In-Context Learning for LLMs", "text": "We deploy LLMs for constrained language planning via in-context learning (Brown et al., 2020;Ouyang et al., 2022). Given a task input (X), we first write a task prompt (T ) describing the task, and then provide several examples (E\n= {E i } |E| i=1\n, where E i = (X i , Y i ) are used for few-shot learning). An LLM generates output (Y ) by completing the prompt (Y = M(T, E, X)). The whole process does not require any gradient update, allowing LLMs to generate new specific goals and scripts without massive training data.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data Source for Examples", "text": "We adopt wikiHow (Koupaee and Wang, 2018), a data source of instructional articles on various topics, as the initial dataset for providing examples. The articles are titled as \"how to ...?\", describing abstract goals, and consist of steps to achieve them. We use the titles (G a ) and steps (S) as examples.", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "Acquisition of Specific Goals", "text": "Since no dataset of specific goals exists to support our study, we have to acquire these goals first. As elaborated in Table 1, we extend the abstract goals with multi-faceted constraints for human-in-theloop data acquisition using InstructGPT.\nFirst, we manually prepare a pool of examples that derive specific goals from an abstract one with constraints. 3 Each example is attached to a constraint type (i.e., modifier, method or intent), and contains more than one constraint and specific goal so that InstructGPT is prompted to generate multiple G c for one G a . Next, given an abstract goal from wikiHow, we enumerate each constraint type to ensure data diversity. Then, we sample several examples of the constraint type from the pool. Finally, we input the task prompt, examples and the G a into InstructGPT for the completion of G c .\nAn example in Table 2 (I) shows InstructGPT generates constraints \"chocolate\" and \"vanilla\" for G a (\"make a cake\") given the constraint type modifier and some examples, and completes the specific goals (\"make a chocolate cake\" and \"make a vanilla cake\").", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Acquisition of Scripts", "text": "After getting specific goals with constraints, we can test the ability of LLMs to fulfill them.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Planning with InstructGPT", "text": "We first write a task prompt T . Given the G c , we back-trace its G a and extract the verbs (\"make\") and nouns (\"cake\") from G a . Then we use the verbs and nouns as keywords to retrieve two similar goals as examples E from the wikiHow dataset. Finally, the task prompt T , examples E and G c with constraint C are fed into InstructGPT. As shown in Table 2 (II), we adopt the scripts, i.e., \"make a cake\" and \"make a cupcake\" to prompt InstructGPT to generate a script for \"make a chocolate cake\".\nOver-Generation and Filtering Using the above-mentioned approach, generated scripts by InstructGPT are reasonable and fluent. However, they sometimes are not faithful to the constraints under closer examination ( \u00a7 4.4). Previous studies have shown that the output quality of LLMs falls in high variance (Wiegreffe et al., 2022), leading to bad performance. Thus, we adopt the idea of over-generate-then-filter to improve generation quality, which is shown to be effective in previous work (Wiegreffe et al., 2022;Liu et al., 2022a). We over-generate K sampled from InstructGPT. 4 Next, a filter model is developed to select the faithful scripts. Due to the diverse expressions of language, we rely not on rules and patterns (i.e., constraint words must appear in the script), but on the semantic similarity between goals and scripts for filtering. For example, \"decorating the cake with candles\" could be a faithful step to make a cake \"for a birthday party\". Motivated by this, we first collect a set of goals, consisting of the target goal (G + c ) as a positive sample and others ({G \u2212 c }) generated from the same abstract goal (G a ) as negative samples. In the previous case, the negatives include \"make a cake in the microwave\" and \"make a cake for a wedding\". We convert scripts and goals into InstructGPT embeddings (text-embedding-ada-002) and calculate cosine similarity as similarity scores to measure semantic similarity. Additionally, we reward the script that explicitly contains the keywords of the target constraint. We only keep the script if G + c scores the highest in the goal set.", "publication_ref": ["b56", "b56", "b27"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Evaluation", "text": "We randomly collect 100 abstract goals (e.g., \"make a cake\") from wikiHow and conduct manual evaluations on the generated specific goals and their scripts. We compare our methods with instruction tuning methods, T0 (Sanh et al., 2022) and Flan-T5 (Chung et al., 2022), vanilla GPT-3 (Ouyang et al., 2022) with different sizes, Codex (Chen et al., 2021) and InstructGPT (Ouyang et al., 2022) with different sizes. We also add \"Let's think step by step\" before each answer for script generation, which is a simple but effective trick to improve zero-shot reasoning for LLMs (Kojima et al., 2022). For a retrieval baseline, we directly use the goals to search and retrieve the most relevant scripts from Table 3: Accuracy (%) of generated scripts for different constraint types by manual evaluation. f sim denotes the choice for similarity function during filtering, i.e., replacing InstructGPT embedding with that of Sim-CSE (Gao et al., 2021) and Sentence-BERT (Reimers and Gurevych, 2019). f sim = None denotes we only reserve the scripts that contain constraint words.\nthe wikiHow website 5 as results.\nAre specific goals generated by LLMs of high quality? We ask InstructGPT to generate 300 (3\u00d7) specific goals for 3 constraint types based on the 100 abstract goals from wikiHow. For evaluation, we recruit annotators on Amazon Mechanical Turk to check whether these goals are correct. Each case is examined by three annotators, who reach an inter-rater agreement at Fleiss's \u03ba = 0.86 (Fleiss et al., 1981). InstructGPT achieves 98.00% accuracy, indicating that LLMs can derive specific goals of rather high quality.\nCan LLMs write scripts for specific goals? To answer this question, we first let InstructGPT generate scripts for the 100 abstract goals from wikiHow and ask three annotators to check the correctness of the scripts (with Fleiss's \u03ba = 0.79).\nThe correctness is decided by both the fulfillment of the goal and the completeness of the semantics. InstructGPT achieves 97.00% accuracy, proving that LLMs can plan for abstract goals very well. However, it is not the case for specific goals. We sample 100 specific goals from 300 generated ones (mentioned above) and evaluate the scripts generated from baselines and our method. Table 3 reports the overall accuracy of the results. Notably, ours reduces to virtually one dot in the graphic because it does not have many errors (0-1%). SE and FE denote semantic completeness and faithfulness error.\nWe find that: 1) Overall, all baselines achieve unsatisfactory results on planning for specific goals, with InstructGPT outperforming others. Especially, the scripts with intent-type constraints have the worst accuracy, and adding \"let's think step-by-step\" does not help much; 2) The retrieval from wikiHow does not lead to the desired script; 3) With our method, InstructGPT can generate scripts of higher quality by a large margin; 4) Replacing the similarity function with embeddings from other pretrained models results in performance drops.\nWhat types of errors do LLMs usually make in this task? To respond to the motivations of our methods, we conduct detailed analyses to investigate why LLMs fail. We evaluate the model planning performance in two aspects: 1) Semantic completeness (SE): whether the steps in the script are missing, repeated or in the wrong order; 2) Faithfulness to the constraints (FE): whether the script is faithful to the constraints and the steps are coherent (related) within the script. We define six types of errors upon the two, i.e., i) SE: missing, repeated step(s) and wrong order and ii) FE: no constraint, unrelated step(s) or incoherent step(s). 6 Annotators are asked to review 100 scripts generated by InstructGPT and mark the error types. 7 Results in Figure 3 show that: 1) The semantic completeness in generated scripts is acceptable, but the faithfulness to the constraints can not be guaran-  teed; 2) Our method greatly improves the planning quality both in semantic completeness and faithfulness.\nWhat kinds of goals do InstructGPT typically fail? By far, we already know that LLMs fail at specific goals, especially for intent-type constraints.\nWe dig into more fine-grained topic categories of constraints defined in wikiHow. The heat map in Figure 4 shows that the planning performance of InstructGPTs varies considerably for goals of different categories, and the planning accuracy for each category improves greatly with our method.", "publication_ref": ["b46", "b13", "b21", "b13", "b43", "b11", "b75"], "figure_ref": ["fig_1", "fig_3"], "table_ref": []}, {"heading": "Script Distillation from LLMs", "text": "Since LLMs are costly to deploy, it is essential to enable language planning ability for smaller, specialized models. Creating datasets is an inevitable step to this end. However, previous datasets do not enable planning for specific goals (Sakaguchi et al., 2021;Lyu et al., 2021), and manual dataset annotation is expensive and highly demanding. Thus, we follow the idea of symbolic knowledge distillation (West et al., 2022) to distill constrained language planning datasets from LLMs.", "publication_ref": ["b31", "b55"], "figure_ref": [], "table_ref": []}, {"heading": "CoScript: A Dataset for Constrained Language Planning", "text": "We now apply our method for building a first-of-itskind Constrained Script dataset of language planning, named as CoScript.  for a large-scale dataset. We collect 14,945 article titles as seed abstract goals and retrieve 34,260 similar goals with scripts from wikiHow as examples to prompt InstructGPT (175B) for data generation. Following \u00a7 4, dataset construction process consists of three steps, as in Figure 2: 1) We first enumerate constraint types with examples for InstructGPT and obtain specific goals (after de-duplication) based on the seed abstract goals. 2) Then, InstructGPT over-generates K scripts for the specific goals and 3) our filter framework selects the faithful scripts as the final data. 8 In total, we generate 55,000 specific goals with corresponding scripts. We randomly choose 2,000 data as the validation set and 3,000 data as the test set. To ensure the quality of the validation and test set, we ask crowd-sourced workers to find and revise the incorrect samples. By collecting the annotation data for error identification of these 5,000 samples, we estimate to achieve 97.80% accuracy for specific goals and 94.98% for constrained script generation, consistent with the results in Table 3.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Dataset Analysis", "text": "Script Diversity Analysis As shown in Table 4, despite the larger scale of wikiHow, CoScript has more specific goals than wikiHow and thus is valuable for the constrained language planning task. Besides, previous studies (Fu et al., 2021;Narayan et al., 2022) find that the texts generated by LMs may be too repetitive and less diverse. For this concern, we compare our CoScript with a recent goal-oriented script dataset proScript (Sakaguchi et al., 2021) created by crowd-sourcing. As reported in Table 4, 1) CoScript is much larger than proScript, with more scripts and a higher number of steps per script; 2) CoScript exhibits high lexical diversity, with more unique words than human-written proScript.\n8 Details about hyper-parameters and costs can be found in Appendix C.1. Constraint Analysis Figure 5 shows the constraint distribution of CoScript. We compute the proportions of constraint types with their representative categories obtained from Probase (Wu et al., 2012), and the initial words of constraint instances. We find CoScript shows high heterogeneity and pluralism in the generated specific goals. Interestingly, InstructGPT tends to start with the word \"if \" or \"when\" for hypothetical constraints (e.g., \"if someone is lactose intolerant\" for \"make a cake\"), suggesting the potential for future research on counterfactual reasoning in language planning. We also analyze the domain distribution of CoScript in the Appendix C.2", "publication_ref": ["b12", "b34", "b57"], "figure_ref": ["fig_4"], "table_ref": ["tab_5", "tab_5"]}, {"heading": "Constrained Language Planning with Specialized Models", "text": "With CoScript, we can train smaller but specialized models for constrained language planning.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental setup", "text": "Baselines We use GPT-2 (causal LM) (Radford et al., 2019) and T5 (encoder-decoder LM) (Raffel et al., 2020) as baselines. Given goals, the models are trained to generate a list of steps S for planning. Moreover, we adopt the idea of retrievalaugmented text generation (Lewis et al., 2020) and add retrieved examples in the input to improve generation quality.\nMetrics We use BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004) and BERTScore (Zhang et al., 2020) as automatic metrics to measure semantic completeness. We also train a binary classification model to decide whether the generated texts are faithful to the constraints. Specifically, we   collect 50,000 data from CoScript as positive samples, and shuffle the goals and scripts to construct 50,000 negative ones. Then, we fine-tune a DeBERTa (v3 large) model (Khashabi et al., 2020) for classification, achieving 91.53% accuracy on the test set.\nTraining Data To gain a fine-grained perspective on planning toward specific goals, we train LMs on both wikiHow (D wi tr ) and CoScript (D co tr ), and test them on CoScript test set (D co te ). Both datasets share similar scripts, but the goals in wikiHow are mostly abstract ones. For wikiHow, we also randomly collect 50,000 goals with scripts as D wi tr .", "publication_ref": ["b40", "b41", "b25", "b38", "b26", "b59", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "The comparison for models trained on wikiHow and CoScript are shown in Table 5. In general, LMs trained on CoScript outperform that on wikiHow. T5 outperforms GPT-2 in faithfulness, possibly due to its encoder-decoder framework being better at handling input information. However, GPT-2 outperforms T5 on other text generation metrics for scripts. This could be because CoScript   is distilled from InstructGPT, leading to a biased data distribution that favors decoder-only causal language models, e.g., the GPT family.\nBased on Table 5, we find that augmenting models with retrieved examples can improve semantic completeness. However, the constraint faithfulness could be undermined as models tend to mimic the retrieved examples. To further understand the role of retrieval augmentation, we conduct a manual evaluation that based on 100 random samples generated by T5 (3B) with and without retrieval augmentation. We discover that 57% of T5's results are correct, and the number goes up to 70% with retrieval augmentation. Thus, although we observe a slight drop in faithfulness score (93.00 \u2192 92.53 from Table 5), retrieval augmentation still brings much improvement over the base model.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7", "tab_7", "tab_7"]}, {"heading": "Faithfulness of Constraints of Different Types", "text": "Will LLMs' planning preferences for constraint types pass to the specialized models? We find the results in Table 6 are consistent with that of LLMs (Table 3). Specialized models are also the worst at specific goals with intent-typed constraints.\nCoScript vs. wikiHow We mix two datasets together with a hyper-parameter \u03b1 to control the proportion of two datasets, where the new training set D tr = \u03b1D co tr + (1 \u2212 \u03b1)D wi tr . By altering \u03b1 (constant data size), the faithfulness curves in Figure 6 shows that adding more data from CoScript consistently improves model performance in constraint faithfulness. Thus, training on CoScript contributes to more faithful planners. Specialized Models vs. LLMs We further finetune a T5 (3B) on CoScript and wikiHow to generate scripts for the specific goals in \u00a7 4.4, which are held out from the training set. Table 7 shows that T5 fine-tuned on CoScript with retrieval augmentation can generate scripts of higher quality than most LLMs in Table 3, indicating that smaller models can surpass larger models when properly trained on suitable datasets.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": ["tab_8", "tab_10"]}, {"heading": "Conclusion", "text": "In this paper, we define planning toward specific goals with constraints. We propose a better prompting method for LLMs, and distill a novel dataset from LLMs (CoScript) to improve the constrained language planning ability of specialized models. Experiments show that our method improves the planning quality of LLMs for specific goals, and smaller models trained on CoScript even outperform LLMs. We hope the CoScript dataset will be a valuable resource to advance the research on language planning with more complex and diverse goals and constraints.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "The proposed method for improving LLMs is a post-hoc re-ranking approach, and we do not improve LLMs themselves due to the difficulty of fine-tuning LLMs. Besides, we improve the ability of constrained language planning for smaller models from the perspective of building task-related datasets, but do not consider investigating the model itself, other than adopting retrieval augmentation. In addition, because automatic metrics for generated text are limited, the automatic evaluation of this paper may result in an overestimation or underestimation of the mentioned methods, though we attempt to mitigate this by incorporating a moderate amount of human evaluation. Despite the advanced planning capabilities of newer language models, our work remains significantly valuable to the knowledge distillation of these LLMs into smaller and more cost-effective models.\nWe also discover several limitations of the proposed CoScript datasets. First, the specific goal explored in this work only inherits from an abstract one with one extra constraint. However, in real-life situations, complex planning may involve multiple constraints, which we do not investigate in this work. Another limitation of CoScript is that our dataset is generated from InstructGPT, and thus the data distributions may be biased to favor causal language models. This is a common issue with machine-generated datasets, which we address by manually curating CoScript's validation and test sets. Furthermore, there are still some incorrect samples (about 5%) in the training data without manual correction due to the limits of budget and time. Last but not least, we only consider whether the script can be executed at the human level. The script execution for robots Lu et al., 2022b) is unstudied in our work, and there still exist huge gaps in transferring complex human language to one that is understandable and executable by robots.", "publication_ref": ["b30"], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "Use of Human Annotations We protect the privacy rights of crowd-sourced workers and pay them above the local minimum wage. We use Amazon Mechanical Turk (AMT) and require 300 annotators to be located in the U.S. as a proxy for English competency. We pay at a rate of $6/hour for 20 samples. We acknowledge that constructing datasets from large language models may suffer from toxic language and cause severe risks for social society (Ousidhoum et al., 2021;Baldini et al., 2022). Therefore, we ask the annotators to discard the offensive and harmful data when reviewing the CoScript. However, there may still be prejudicial data in our final dataset that goes unnoticed.\nwikiHow Source The content available on wiki-How is shared under a Creative Commons License (CC-BY-NC-SA) 9 , which permits others to share, copy, distribute, and adapt the content for noncommercial purposes. In our research, we use wikiHow as an initial dataset for providing examples to construct our dataset. Our dataset is released on GitHub and is only used to advance academic research on language planning with more complex and diverse goals and constraints. Therefore, we emphasize that our usage aligns with the requirements under the license.\nCovered Domains in CoScript CoScript is derived from wikiHow and encompasses 19 daily life goal categories (as illustrated in Figure 8). These categories cover a wide range of practical topics of everyday life. However, as shown in Figure 8, we emphasize that sensitive and high-risk domains, including medical, legal, and high-stakes financial advice, are excluded from the dataset to minimize potential risks related to inaccurate or misleading information. We encourage researchers and developers to leverage this dataset to build models that accurately understand and respond to user queries on various non-sensitive, non-critical topics.\nFactuality, Toxicity and Biases We recognize that the factuality of generated content is crucial, especially in high-stakes scenarios. Therefore, annotators are asked to verify the consistency between generated scripts and goals with constraints for validation and test sets. They also assess and revise the content to minimize hallucinations, factual errors, and any inappropriate or misleading information.\nPrevious work found that LLMs may generate toxic contents (Cao et al., 2022;Liu et al., 2022b). We highlight that our dataset is not intended for safety-critical applications or as a substitute for expert advice in such domains. Annotators are specifically instructed to discard offensive and harmful data during the review of the validation and test sets in CoScript. However, despite these precautions, there may still be some prejudicial data that goes unnoticed in our final dataset.", "publication_ref": ["b36", "b1", "b4", "b28"], "figure_ref": ["fig_7", "fig_7"], "table_ref": []}, {"heading": "B Implementation Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1 Handpicked Examples for Specific Goal Generation", "text": "We follow the instructions proposed by  to better construct the three examples for specific goal generation. As shown in Table 13, we turn long descriptions into bulleted lists in the task prompt for better generation. In addition, in each example, we list two specific goals with constraints, which can prompt InstructGPT to generate multiple specific goals for the given abstract goals. In our experiment, we conduct the specific goals generation with different numbers of examples and report the accuracy and the total number of generated specific goals for 100 abstract goals. The results in  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.2 Prompts Format", "text": "To explore the prompt formats on script generation, we test 100 samples mentioned in \u00a7 4.4 without using the task prompt or replacing the original words with other words. The results in Table 9 show that: 1) task prompt can help InstructGPT to better understand the task and thus can improve the model performance on script generation; 2) adopting popular words to write the prompts can better improve the effect of prompts.  ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_14"]}, {"heading": "B.3 Over-generation Hyper-parameters", "text": "To evaluate the LLMs on planning for specific goals, we randomly sample 100 specific goals and Human annotators are asked to evaluate K generations for each specific goal on 100 samples. The legends indicate the number of constraint-faithful scripts in K over-generation. We report the proportion of generated scripts faithful to the constraints.\nthen generate scripts from the baseline and our method. Figure 7 reports the faithfulness to constraints. We find that: 1) the output quality of In-structGPT falls in high variance, and the script may be unfaithful to the constraints; 2) over-generation can amplify the likelihood of constraint satisfaction, and K = 2 is sufficient.", "publication_ref": [], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "B.4 Error Types", "text": "As shown in Figure 10, we evaluate the model planning performance on two aspects, i.e., Semantic completeness and Faithfulness to the constraints ( \u00a7 4.4), and define six types of errors.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.5 Case Study", "text": "Table 14 lists three examples by InstructGPT (175B) and our approach. The first and second examples show that the scripts generated by Instruct-GPT may fail in unfaithfulness to the constraints.\nThe third examples demonstrate that although the scripts generated by InstructGPT can be faithful to the constraints, they may suffer from other error types. In contrast, the over-generating-and-filtering method can amplify the likelihood of high-quality generations and thus can make LLMs better planners.\nC CoScript Details", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "C.1 Generation Hyper-parameters", "text": "We queried the text-davinci-002 model through the OpenAI API on June 25 to July 5, 2022.\nCoScript is generated under a specified license that is compatible with the conditions under Ope-nAI API. In total, the generation for CoScript costs about $5,000. The hyper-parameters for script generation are shown in ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Repeat steps", "text": "Steps that are repeated in the script.\nMissing steps Important steps that are missing.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Faithfulness to Constraints", "text": "No constraint Script is unfaithful to the constraint", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Incoherent steps", "text": "Steps that are related to the goal, but are not coherent within the script.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Unrelated steps", "text": "Steps that are not related to the goal.   ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.2 Domain Examination", "text": "As shown in Figure 8, CoScript is derived from wikiHow and encompasses 19 daily life goal categories. These categories cover a wide range of practical topics of everyday life, excluding sensitive and high-stakes topics like medical and legal matters.\nIn addition, we adopt ChatGPT to assess each specific goal in CoScript to further mitigate risks associated with sensitive domains. The instruction is shown in Table 12. Upon examination, ChatGPT identifies that 0.24% of specific goals (132) within the CoScript involve a sensitive domain, such as \"relieve head pain with medication\" in the Health domain. We manually remove these data and sub-  stitute them with new specific goals (e.g., relieve head pain with meditation) to ensure the safety of our dataset. We encourage researchers and developers to leverage this dataset to build models that accurately understand and respond to user queries on non-sensitive, non-critical topics.", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": ["tab_2"]}, {"heading": "C.3 Qualitative Generations", "text": "We randomly select qualitative generations from the CoScript. Table 15, Table 16 and Table 17 show some specific goal generations under different types of constraints. Table 18 shows some scripts generated based on our proposed pipeline.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7", "tab_8", "tab_10", "tab_11"]}, {"heading": "D Crowd-sourcing Details", "text": "Interface Details We conduct human evaluations on Amazon Mechanical Turk. Screenshots of the instructions and annotation interface are shown in Figure 9 and 10. etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgement", "text": "We thank the anonymous reviewers for their valuable comments, and Wei Shi and Shuang Li from Fudan University for their useful suggestions for the manuscript. This work is supported by the Chinese NSF Major Research Plan (No.92270121), Shanghai Science and Technology Innovation Action Plan (No.21511100401) and the Science and Technology Commission of Shanghai Municipality Grant (No. 22511105902).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Author Contributions", "text": "Siyu Yuan Lead the project, develop the original method and original code, lead the curation of the dataset, contribute to the original experiments, and contribute to the original manuscript.\nJiangjie Chen Conceptualization of the original idea, supervision of the research activity planning and execution, contribution to the original manuscript and figures, and acquisition of financial support for the project.\nZiquan Fu Contribute to the original idea, provide financial support for the project, revise the manuscript, and contribute to data curation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Xuyang Ge Contribute to the experiments and data curation.", "text": "Soham Shah Provide financial support for the project and revise the manuscript.\nCharles Robert Jankowski Provide financial support for the project and revise the manuscript.\nYanghua Xiao Provide financial support for the project and revise the manuscript.\nDeqing Yang Provide financial support for the project, revise the manuscript, and oversee the research activity execution.\nStep 1: Specific Goal Evaluation We first assess the specific goals generated by InstructGPT with three types of constraints. We ask the turkers to check the specific goal whether inherits the abstract goal and contains a constraint.\nStep 2: Script Evaluation In the second step, we show a script of the specific goal with actionable steps. We then ask two questions:\n1. Does the script meet the constraint in the specific goal? (Yes, No, or Not sure). In our preliminary experiments, we found that the semantic completeness in the scripts generated based on InstructGPT (175B) is acceptable, but faithfulness to the constraints can not be guaranteed. This question assesses this tricky error;\n2. Are the steps in the script correct in achieving the specific goal? (Yes, No, or Not sure). This question is to assess whether the script can indeed accomplish the given goal. Although we have checked the constraint in the first question, there are still other error types (as shown in Figure 10). Then, we ask the turkers to review the generated scripts. If the scripts cannot achieve the given goal, they must point out the wrong steps and select the error types.   Thanks for participating in this HIT! Please spend some time reading this instruction and the example section to better understand our HIT!\nIn this hit, you need to complete 20 data labeling tasks. In each task, you will be presented a general goal about everyday activities (such as \"make a cake\" ) and a specific goal which inherits the general goal but is more specific and has a reasonable CONSTRAINT (such as \"make a chocolate cake\" ). You will answer 3 questions for each task.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Question 1", "text": "In Question 1, you need to assess whether the specific goal is reasonable. For example, making a chocolate cake is a reasonable constraint of making a cake, whereas making a lego cake is not reasonable.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Question 2", "text": "Then, you will read a script of the specific goal with actionable steps (in the cake' s example, the script is the steps towards making a cake). Question 2 is to check whether the script MEETS THE CONSTRAINT. If the specific goal is making a chocolate cake and the script does not mention chocolate, then it does not meet the constraint.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Question 3", "text": "In Question 3, you will assess whether the script can indeed accomplish the given goal. If the script can not accomplish the given goal, you need to point out the wrong steps and SELECT THE ERROR TYPES. A script for making chocolate cake might mention chocolate, but if its making instructions are wrong, then you need to reflect it in Question 3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Notes:", "text": "\u2022 A general goal involves STEREOTYPICAL ACTIVITIES such as \"make a cake\", while a specific goal can be multi-facet WITH A REASONABLE CONSTRAINT.\no For example, a cake can be made for different purposes (for a wedding or a birthday party), with various tools (with a microwave or an oven) or with different ingredients (chocolate or vanilla). \u2022 If you think the specific goal is not reasonable, choose NO in Question 1, but still proceed with Question 2 and 3 pretending that it is reasonable. For example, making a LEGO cake is not reasonable, but you can still assess whether the corresponding script meets the constraint or not. Remember you can always choose \"I am not sure\" . \u2022 You SHOULD NOT ignore grammar and spelling mistakes in the script.\n\u2022 You can SEARCH GOOGLE to help you judge whether the script can achieve the goal, especially if you are not sure about the script.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Example of Error Types", "text": "Specific Goal: Make A Vanilla Cake Script:\n1. Gather your ingredients. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Wrong Script Explanation", "text": "Steps that are in the wrong order Steps that are repeated in the script Steps that are related to the goal, but are not coherent within the script\nSteps that are not related to the goal Important steps that are missing D I certify that I have read and understand all the instructions.  and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? Ethics Statement D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? Appendix D D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nWe do not have any human subjects research and use filtered model-generated data as datasets.\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? Ethics Statement", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Script processing in attitude formation and decision making", "journal": "", "year": "1976", "authors": " Robert P Abelson"}, {"ref_id": "b1", "title": "Your fairness may vary: Pretrained language model fairness in toxic text classification", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Ioana Baldini; Dennis Wei; Moninder Karthikeyan Natesan Ramamurthy; Mikhail Singh;  Yurochkin"}, {"ref_id": "b2", "title": "Using language to generate state abstractions for long-range planning in outdoor environments", "journal": "", "year": "2022", "authors": "Matthew Berg; George Konidaris; Stefanie Tellex"}, {"ref_id": "b3", "title": "Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners", "journal": "Curran Associates, Inc", "year": "", "authors": "Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel Ziegler; Jeffrey Wu; Clemens Winter; Chris Hesse; Mark Chen; Eric Sigler; Mateusz Litwin"}, {"ref_id": "b4", "title": "Hallucinated but factual! inspecting the factuality of hallucinations in abstractive summarization", "journal": "Long Papers", "year": "2022", "authors": "Meng Cao; Yue Dong; Jackie Cheung"}, {"ref_id": "b5", "title": "", "journal": "", "year": "", "authors": "Mark Chen; Jerry Tworek; Heewoo Jun; Qiming Yuan; Henrique Ponde De Oliveira Pinto; Jared Kaplan; Harri Edwards; Yuri Burda; Nicholas Joseph; Greg Brockman"}, {"ref_id": "b6", "title": "Palm: Scaling language modeling with pathways", "journal": "", "year": "2022", "authors": "Aakanksha Chowdhery; Sharan Narang; Jacob Devlin; Maarten Bosma; Gaurav Mishra; Adam Roberts; Paul Barham;  Hyung Won; Charles Chung; Sebastian Sutton;  Gehrmann"}, {"ref_id": "b7", "title": "Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models", "journal": "", "year": "", "authors": " Hyung Won; Le Chung; Shayne Hou; Barret Longpre; Yi Zoph; William Tay; Eric Fedus; Xuezhi Li;  Wang"}, {"ref_id": "b8", "title": "Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks", "journal": "", "year": "2022", "authors": "M Katherine; Catherine Collins; Jiahai Wong; Megan Feng; Joshua B Wei;  Tenenbaum"}, {"ref_id": "b9", "title": "What does it take to bake a cake? the RecipeRef corpus and anaphora resolution in procedural text", "journal": "", "year": "2022", "authors": "Biaoyan Fang; Timothy Baldwin; Karin Verspoor"}, {"ref_id": "b10", "title": "Association for Computational Linguistics", "journal": "", "year": "", "authors": "Ireland Dublin"}, {"ref_id": "b11", "title": "The measurement of interrater agreement. Statistical methods for rates and proportions", "journal": "", "year": "1981", "authors": "L Joseph; Bruce Fleiss; Myunghee Levin;  Cho Paik"}, {"ref_id": "b12", "title": "A theoretical analysis of the repetition problem in text generation", "journal": "", "year": "2021", "authors": "Zihao Fu; Wai Lam; Anthony Man-Cho So; Bei Shi"}, {"ref_id": "b13", "title": "SimCSE: Simple contrastive learning of sentence embeddings", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Tianyu Gao; Xingcheng Yao; Danqi Chen"}, {"ref_id": "b14", "title": "Automated planning for situated natural language generation", "journal": "", "year": "2010", "authors": "Konstantina Garoufi; Alexander Koller"}, {"ref_id": "b15", "title": "Content planning for neural story generation with aristotelian rescoring", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Seraphina Goldfarb-Tarrant; Tuhin Chakrabarty; Ralph Weischedel; Nanyun Peng"}, {"ref_id": "b16", "title": "PLANET: Dynamic content planning in autoregressive transformers for long-form text generation", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Zhe Hu; Jiachen Hou Pong Chan; Xinyan Liu; Hua Xiao; Lifu Wu;  Huang"}, {"ref_id": "b17", "title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents", "journal": "PMLR", "year": "2022-07", "authors": "Wenlong Huang; Pieter Abbeel; Deepak Pathak; Igor Mordatch"}, {"ref_id": "b18", "title": "Extracting common sense knowledge from text for robot planning", "journal": "", "year": "2014", "authors": "Peter Kaiser; Mike Lewis; P A Ronald; Tamim Petrick; Mark Asfour;  Steedman"}, {"ref_id": "b19", "title": "Language planning from practice to theory", "journal": "Multilingual Matters", "year": "1997", "authors": "B Robert; Richard B Kaplan;  Baldauf"}, {"ref_id": "b20", "title": "UNIFIEDQA: Crossing format boundaries with a single QA system", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Daniel Khashabi; Sewon Min; Tushar Khot; Ashish Sabharwal; Oyvind Tafjord; Peter Clark; Hannaneh Hajishirzi"}, {"ref_id": "b21", "title": "Large language models are zero-shot reasoners", "journal": "", "year": "2022", "authors": "Takeshi Kojima; Shane Shixiang; Machel Gu; Yutaka Reid; Yusuke Matsuo;  Iwasawa"}, {"ref_id": "b22", "title": "Sentence generation as a planning problem", "journal": "Association for Computational Linguistics", "year": "2007", "authors": "Alexander Koller; Matthew Stone"}, {"ref_id": "b23", "title": "Wikihow: A large scale text summarization dataset", "journal": "", "year": "2018", "authors": "Mahnaz Koupaee; William Yang Wang"}, {"ref_id": "b24", "title": "Verifying plans and scripts for robotics tasks using performance level profiles", "journal": "", "year": "2021", "authors": "Alexander Kovalchuk; Shashank Shekhar; Ronen I Brafman"}, {"ref_id": "b25", "title": "Retrieval-augmented generation for knowledgeintensive nlp tasks", "journal": "Curran Associates, Inc", "year": "2020", "authors": "Patrick Lewis; Ethan Perez; Aleksandra Piktus; Fabio Petroni; Vladimir Karpukhin; Naman Goyal; Heinrich K\u00fcttler; Mike Lewis; Wen-Tau Yih; Tim Rockt\u00e4schel; Sebastian Riedel; Douwe Kiela"}, {"ref_id": "b26", "title": "ROUGE: A package for automatic evaluation of summaries", "journal": "Association for Computational Linguistics", "year": "2004", "authors": "Chin-Yew Lin"}, {"ref_id": "b27", "title": "WANLI: Worker and AI collaboration for natural language inference dataset creation", "journal": "", "year": "2022", "authors": "Alisa Liu; Swabha Swayamdipta; Noah A Smith; Yejin Choi"}, {"ref_id": "b28", "title": "A token-level reference-free hallucination detection benchmark for free-form text generation", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Tianyu Liu; Yizhe Zhang; Chris Brockett; Yi Mao; Zhifang Sui; Weizhu Chen; Bill Dolan"}, {"ref_id": "b29", "title": "Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity", "journal": "Long Papers", "year": "2022", "authors": "Yao Lu; Max Bartolo; Alastair Moore; Sebastian Riedel; Pontus Stenetorp"}, {"ref_id": "b30", "title": "Neuro-symbolic causal language planning with commonsense prompting", "journal": "", "year": "2022", "authors": "Yujie Lu; Weixi Feng; Wanrong Zhu; Wenda Xu; Xin Eric Wang; Miguel Eckstein; William Yang Wang"}, {"ref_id": "b31", "title": "Goal-oriented script construction", "journal": "UK. Association for Computational Linguistics", "year": "2021", "authors": "Qing Lyu; Li Zhang; Chris Callison-Burch"}, {"ref_id": "b32", "title": "Rethinking the role of demonstrations: What makes in-context learning work? ArXiv preprint", "journal": "", "year": "2022", "authors": "Sewon Min; Xinxi Lyu; Ari Holtzman; Mikel Artetxe; Mike Lewis; Hannaneh Hajishirzi; Luke Zettlemoyer"}, {"ref_id": "b33", "title": "Reframing instructional prompts to GPTk's language", "journal": "", "year": "2022", "authors": "Swaroop Mishra; Daniel Khashabi; Chitta Baral; Yejin Choi; Hannaneh Hajishirzi"}, {"ref_id": "b34", "title": "A well-composed text is half done! composition sampling for diverse conditional generation", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Shashi Narayan; Gon\u00e7alo Sim\u00f5es; Yao Zhao; Joshua Maynez; Dipanjan Das; Michael Collins; Mirella Lapata"}, {"ref_id": "b35", "title": "Gpt3-to-plan: Extracting plans from text using gpt-3. FinPlan", "journal": "", "year": "2021", "authors": "Alberto Olmo; Sarath Sreedharan; Subbarao Kambhampati"}, {"ref_id": "b36", "title": "Probing toxic content in large pre-trained language models", "journal": "Long Papers", "year": "2021", "authors": "Nedjma Ousidhoum; Xinran Zhao; Tianqing Fang; Yangqiu Song; Dit-Yan Yeung"}, {"ref_id": "b37", "title": "Training language models to follow instructions with human feedback", "journal": "", "year": "", "authors": "Long Ouyang; Jeff Wu; Xu Jiang; Diogo Almeida; L Carroll; Pamela Wainwright; Chong Mishkin; Sandhini Zhang; Katarina Agarwal;  Slama"}, {"ref_id": "b38", "title": "Bleu: a method for automatic evaluation of machine translation", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"ref_id": "b39", "title": "Prospection: Interpretable plans from language by predicting the future", "journal": "", "year": "2019", "authors": "Chris Paxton; Yonatan Bisk; Jesse Thomason"}, {"ref_id": "b40", "title": "Language models are unsupervised multitask learners", "journal": "OpenAI blog", "year": "2019", "authors": "Alec Radford; Jeffrey Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"ref_id": "b41", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "J. Mach. Learn. Res", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; J Peter;  Liu"}, {"ref_id": "b42", "title": "Learning script knowledge with web experiments", "journal": "", "year": "2010", "authors": "Michaela Regneri; Alexander Koller; Manfred Pinkal"}, {"ref_id": "b43", "title": "Sentence-BERT: Sentence embeddings using Siamese BERTnetworks", "journal": "", "year": "2019", "authors": "Nils Reimers; Iryna Gurevych"}, {"ref_id": "b44", "title": "2021. proScript: Partially ordered scripts generation", "journal": "", "year": "", "authors": "Keisuke Sakaguchi; Chandra Bhagavatula; Niket Ronan Le Bras; Peter Tandon; Yejin Clark;  Choi"}, {"ref_id": "b45", "title": "What do large language models learn about scripts?", "journal": "", "year": "2022", "authors": "Abhilasha Sancheti; Rachel Rudinger"}, {"ref_id": "b46", "title": "Multitask prompted training enables zero-shot task generalization", "journal": "", "year": "2022", "authors": "Victor Sanh; Albert Webson; Colin Raffel; Stephen Bach; Lintang Sutawika; Zaid Alyafeai; Antoine Chaffin; Arnaud Stiegler; Arun Raja; Manan Dey; Canwen Bari; Urmish Xu; Shanya Thakker; Eliza Sharma Sharma; Taewoon Szczechla; Gunjan Kim; Nihal Chhablani; Debajyoti Nayak; Jonathan Datta; Mike Chang;  Tian-Jian; Han Jiang; Matteo Wang; Sheng Manica;  Shen"}, {"ref_id": "b47", "title": "Scripts, plans, and knowledge", "journal": "Morgan Kaufmann Publishers Inc", "year": "1975", "authors": "C Roger; Robert P Schank;  Abelson"}, {"ref_id": "b48", "title": "Scripts, plans, goals, and understanding: An inquiry into human knowledge structures", "journal": "Psychology Press", "year": "2013", "authors": "C Roger;  Schank;  Robert P Abelson"}, {"ref_id": "b49", "title": "The evolution of foresight: What is mental time travel, and is it unique to humans?", "journal": "Behavioral and brain sciences", "year": "2007", "authors": "Thomas Suddendorf; C Michael;  Corballis"}, {"ref_id": "b50", "title": "A dataset for tracking entities in open domain procedural text", "journal": "", "year": "2020", "authors": "Niket Tandon; Keisuke Sakaguchi; Bhavana Dalvi; Dheeraj Rajagopal; Peter Clark; Michal Guerquin; Kyle Richardson; Eduard Hovy"}, {"ref_id": "b51", "title": "Large language models still can't plan (a benchmark for LLMs on planning and reasoning about change)", "journal": "", "year": "2022", "authors": "Karthik Valmeekam; Alberto Olmo; Sarath Sreedharan; Subbarao Kambhampati"}, {"ref_id": "b52", "title": "Self-consistency improves chain of thought reasoning in language models", "journal": "", "year": "2022", "authors": "Xuezhi Wang; Jason Wei; Dale Schuurmans; Quoc Le; Ed Chi; Denny Zhou"}, {"ref_id": "b53", "title": "A crowdsourced database of event sequence descriptions for the acquisition of high-quality script knowledge", "journal": "", "year": "2016", "authors": "D A Lilian; Alessandra Wanzare; Stefan Zarcone; Manfred Thater;  Pinkal"}, {"ref_id": "b54", "title": "Chain of thought prompting elicits reasoning in large language models", "journal": "", "year": "2022", "authors": "Jason Wei; Xuezhi Wang; Dale Schuurmans; Maarten Bosma; Fei Xia; Ed H Chi; V Quoc; Denny Le;  Zhou"}, {"ref_id": "b55", "title": "Symbolic knowledge distillation: from general language models to commonsense models", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Peter West; Chandra Bhagavatula; Jack Hessel; Jena Hwang; Liwei Jiang; Ximing Ronan Le Bras; Sean Lu; Yejin Welleck;  Choi"}, {"ref_id": "b56", "title": "Reframing human-AI collaboration for generating free-text explanations", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Sarah Wiegreffe; Jack Hessel; Swabha Swayamdipta; Mark Riedl; Yejin Choi"}, {"ref_id": "b57", "title": "Probase: A probabilistic taxonomy for text understanding", "journal": "Association for Computing Machinery", "year": "2012", "authors": "Wentao Wu; Hongsong Li; Haixun Wang; Kenny Q Zhu"}, {"ref_id": "b58", "title": "Induce, edit, retrieve: Language grounded multimodal schema for instructional video retrieval", "journal": "", "year": "2021", "authors": "Yue Yang; Joongwon Kim; Artemis Panagopoulou; Mark Yatskar; Chris Callison-Burch"}, {"ref_id": "b59", "title": "Bertscore: Evaluating text generation with BERT", "journal": "", "year": "2020-04-26", "authors": "Tianyi Zhang; Varsha Kishore; Felix Wu; Kilian Q Weinberger; Yoav Artzi"}, {"ref_id": "b60", "title": "Pick the right fabric", "journal": "", "year": "", "authors": ""}, {"ref_id": "b61", "title": "Choose the right length", "journal": "", "year": "", "authors": ""}, {"ref_id": "b62", "title": "Find the right fit", "journal": "", "year": "", "authors": ""}, {"ref_id": "b63", "title": "Accessorize appropriately", "journal": "", "year": "", "authors": ""}, {"ref_id": "b64", "title": "Wear the right shoes", "journal": "", "year": "", "authors": ""}, {"ref_id": "b65", "title": "Find a pencil skirt that fits you well. 2. Pair your pencil skirt with a fitted top", "journal": "", "year": "", "authors": ""}, {"ref_id": "b66", "title": "Choose the right shoes to complete your look", "journal": "", "year": "", "authors": ""}, {"ref_id": "b67", "title": "Accessorize as needed", "journal": "", "year": "", "authors": ""}, {"ref_id": "b68", "title": "Specific Goal: Clean your skin with soap and water Script", "journal": "", "year": "", "authors": ""}, {"ref_id": "b69", "title": "Rub the cleanser over your skin in gentle, circular motions", "journal": "", "year": "", "authors": ""}, {"ref_id": "b70", "title": "Rinse the cleanser off your skin with lukewarm water", "journal": "", "year": "", "authors": ""}, {"ref_id": "b71", "title": "Pat your skin dry with a clean, soft cloth. Script (Ours): 1. Wet your skin with warm water", "journal": "", "year": "", "authors": ""}, {"ref_id": "b72", "title": "Apply a dollop of mild soap to your hands", "journal": "", "year": "", "authors": ""}, {"ref_id": "b73", "title": "Gently massage the lather into your skin in a circular motion", "journal": "", "year": "", "authors": ""}, {"ref_id": "b74", "title": "Rinse the soap off your skin with warm water", "journal": "", "year": "", "authors": ""}, {"ref_id": "b75", "title": "Pat your skin dry with a clean towel", "journal": "", "year": "", "authors": ""}, {"ref_id": "b76", "title": "Specific Goal: Make stewed fruit with a crockpot Script", "journal": "", "year": "", "authors": ""}, {"ref_id": "b77", "title": "Cook on low for four hours", "journal": "", "year": "", "authors": ""}, {"ref_id": "b78", "title": "", "journal": "", "year": "", "authors": " Script"}, {"ref_id": "b79", "title": "Cook the fruit on low heat in the crockpot", "journal": "", "year": "", "authors": ""}, {"ref_id": "b80", "title": "Divide the fruit evenly into four bowls", "journal": "", "year": "", "authors": ""}, {"ref_id": "b81", "title": "Make your crumble", "journal": "", "year": "", "authors": ""}, {"ref_id": "b82", "title": "Sprinkle the crumble over your fruit and make it", "journal": "", "year": "", "authors": ""}, {"ref_id": "b83", "title": "The generation comparison between InstructGPT (175B) with our method. We highlight the constraints in the given examples", "journal": "Table", "year": "", "authors": ""}, {"ref_id": "b84", "title": "Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? No response", "journal": "", "year": "", "authors": ""}, {"ref_id": "b85", "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run", "journal": "", "year": "", "authors": ""}, {"ref_id": "b86", "title": "for preprocessing, for normalization, or for evaluation", "journal": "", "year": "", "authors": ""}, {"ref_id": "b87", "title": "crowdworkers) or research with human participants?", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: The workflow of using InstructGPT to generate specific goals (Step 1) and planning for the goals with the over-generate-then-filter framework (Step 2-3).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure3: Errors of the generated scripts by human evaluation. The axis of the radar chart is in log-scale. Notably, ours reduces to virtually one dot in the graphic because it does not have many errors (0-1%). SE and FE denote semantic completeness and faithfulness error.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: The heat-map depicts the human-evaluated script accuracy of different methods in different topic categories for specific goals.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Statistics of constraint types in CoScript dataset, with representative topic categories or the first words for each constraint type.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 6 :6Figure 6: The faithfulness curves when altering the proportions of CoScript (\u03b1) and wikiHow (1 \u2212 \u03b1) in a fixed-size training set.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 7 :7Figure 7: Statistics of the constraints evaluation results.Human annotators are asked to evaluate K generations for each specific goal on 100 samples. The legends indicate the number of constraint-faithful scripts in K over-generation. We report the proportion of generated scripts faithful to the constraints.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 8 :8Figure 8: The category distribution of CoScript. The categories are derived from wikiHow", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "you describe the limitations of your work? Limitations A2. Did you discuss any potential risks of your work? Ethics Statement A3. Do the abstract and introduction summarize the paper's main claims? Abstract and Section 1 A4. Have you used AI writing assistants when working on this paper? Check grammar for the whole paper. B Did you use or create scientific artifacts? Section 5.1 B1. Did you cite the creators of artifacts you used? Section 4.1 B2. Did you discuss the license or terms for use and / or distribution of any artifacts? Section 4.1 and Section 5.1 and Appendix C.1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Section 4.1 and Section 5.1 and Appendix C.1 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Ethics Statement B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Section 5 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Section 5 C5Did you run computational experiments? Section 4.4 and Section 6 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? Section 4.4 and Section 6.1 and Appendix C.1", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": "Abstract Goal: Make a cakeFiltered Scripts 1. Gather your ingredients ... ... 4. Add the cocoa powder Script 3 1 2 2 Candidate Scripts k \u2026 similarity score Specific Goals: G1(+modifier): Make a chocolate cake G2(+method): Make a cake Output: Specific goals with corresponding plans Step 1 Generate specific goals with InstructGPT via in-context learning Step 2 Over-generate candidate scripts with InstructGPT via in-context learning"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ": Examples of prompt for InstructGPT for spe-cific goal generation and script generation via in-context learning. Generated texts are highlighted."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Statistics of CoScript and previous script datasets proScript and wikiHow, w.r.t. data size, number of unique tokens (# UT), the average number of specific goals for each abstract ones (Avg Gc #), and the average number of steps in scripts (Avg S #).", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Overall script generation performance for models trained on different training sets. Note that the test set is the same for all models.", "figure_data": "ModelModifier Method Intent AllT5 (large) +retrieval91.54 87.3992.57 85.8690.21 91.81 84.44 86.03GPT-2 (large) +retrieval78.78 77.3378.77 78.2869.48 76.73 70.97 76.30"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Faithfulness scores of specialized models for each constraint type on the test set of CoScript.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "", "figure_data": "show that three examples are good in oursettings.# Examples Accuracy # Total2 3 495.16% 96.99% 95.44%545 537 539"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "The specific goals generation performance of InstructGPT with different numbers of examples.", "figure_data": ""}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "", "figure_data": ": Accuracy (%) of different prompt formats by manual evaluation. We replace (r.) the words in Table 2 with other words for comparison."}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "If both generations are faithful, we randomly select one into the dataset.", "figure_data": "AspectsError TypesExplanationExample: Make a vanilla cake Constraint: VanillaSemantic CompletenessWrong orderSteps that are in the wrong order.Correct Script: 1. Gather your ingredients. 2. Preheat the oven to 325\u00b0F and grease and flour a cake pan. 3. Cream the butter and suger. 4. Add the eggs and vanilla. 5. Stir in the cake flour. 6. Pour the batter into the pan. 7. Bake the cake for 1 hour 15 minutes.Generated Script: 1. Preheat the oven to 325\u00b0F and grease and flour a cake pan. 2. Gather your ingredients. 3. Buy your ingredients. 4. Cream the butter and salt. 5. Stir in the cake flour. 6. Have a shower. 7. Pour the batter into the pan. 8. Bake the cake for 1 hour 15 minutes."}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "The error types and their explanation with examples.", "figure_data": "Hyper-parameter AssignmentTop-p Temperature Max tokens Presence penalty Frequency penalty K1.0 1.0 512 0.0 0.0 2"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "Hyper-parameters for script generation from InstructGPT.", "figure_data": "Home and GardenSports and FitnessComputers and ElectronicsFood and Entertaining TravelPersonal Care and StyleEducation and Communications Philosophy and ReligionFinance and BusinessYouthFamily LifeWork WorldHolidays and TraditionsPets and Animals RelationshipsArts and Entertainment HealthCars & Other VehiclesHobbies and Crafts"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "The instruction for ChatGPT to identify sensitive and high-risk domains. Generated texts by ChatGPT are highlighted.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "= {E i } |E| i=1", "formula_coordinates": [4.0, 239.7, 599.47, 47.56, 22.43]}], "doi": "10.18653/v1/2022.findings-acl.176"}