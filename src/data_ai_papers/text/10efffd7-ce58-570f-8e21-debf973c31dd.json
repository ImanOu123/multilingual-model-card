{"title": "Design Challenges and Misconceptions in Neural Sequence Labeling", "authors": "Jie Yang; Shuailong Liang; Yue Zhang", "pub_date": "", "abstract": "We investigate the design challenges of constructing effective and efficient neural sequence labeling systems, by reproducing twelve neural sequence labeling models, which include most of the state-of-the-art structures, and conduct a systematic model comparison on three benchmarks (i.e. NER, Chunking, and POS tagging). Misconceptions and inconsistent conclusions in existing literature are examined and clarified under statistical experiments. In the comparison and analysis process, we reach several practical conclusions which can be useful to practitioners.", "sections": [{"heading": "Introduction", "text": "Sequence labeling models have been used for fundamental NLP tasks such as POS tagging, chunking and named entity recognition (NER). Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009;Passos et al., 2014;Luo et al., 2015) with handcrafted features and task-specific resources. With advances in deep learning, neural models have given state-of-the-art results on many sequence labeling tasks (Ling et al., 2015;Lample et al., 2016;Ma and Hovy, 2016). Words and characters are encoded in distributed representations (Mikolov et al., 2013) and sentence-level features are learned automatically during end-to-end training. Many existing state-of-the-art neural sequence labeling models utilize word-level Long Short-Term Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels Lample et al., 2016;Ma and Hovy, 2016;Peters et al., 2017). As an alternative, Convolution Neural Network (CNN) (LeCun et al., 1989) has also been used for its ability of parallel computing, leading to an efficient training and decoding process.\nDespite them being dominant in the research literature, reproducing published results for neural models can be challenging, even if the codes are available open source. For example, Reimers and Gurevych (2017b) conduct a large number of experiments using the code of Ma and Hovy (2016), but cannot obtain comparable results as reported in the paper. Liu et al. (2018) report lower average F-scores on NER when reproducing the structure of Lample et al. (2016), and on POS tagging when reproducing Ma and Hovy (2016). Most literature compares results with others by citing the scores directly Lample et al., 2016) without re-implementing them under the same setting, resulting in less persuasiveness on the advantage of their models. In addition, conclusions from different reports can be contradictory. For example, most work observes that stochastic gradient descent (SGD) gives best performance on NER task (Chiu and Nichols, 2016;Lample et al., 2016;Ma and Hovy, 2016), while Reimers and Gurevych (2017b) report that SGD is the worst optimizer on the same datasets.\nThe comparison between different deep neural models is challenging due to sensitivity on experimental settings. We list six inconsistent configurations in literature, which lead to difficulties for fair comparison.\n\u2022 Dataset. Most work reports sequence labeling results on both CoNLL 2003 English NER (Tjong Kim Sang and De Meulder, 2003) and PTB POS (Marcus et al., 1993) datasets (Collobert et al., 2011;Ma and Hovy, 2016). Ling et al. (2015) give results only on POS dataset, while some papers (Chiu and Nichols, 2016;Lample et al., 2016;Strubell et al., 2017) report results on the NER dataset only. dos Santos et al. (2015) conducts experiments on NER for Portuguese and Spanish.\nMost work uses the development set to select hyperparameters (Lample et al., 2016;Ma and Hovy, 2016), while others add development set into training set (Chiu and Nichols, 2016;Peters et al., 2017). Reimers and Gurevych (2017b) use a smaller dataset (13862 vs 14987 sentences). Different from Ma and Hovy (2016) and Liu et al. (2018),  choose a different data split on the POS dataset. Liu et al. (2018) and Hashimoto et al. (2017) use different development sets for chunking.\n\u2022 Preprocessing. A typical data preprocessing step is to normize digit characters (Chiu and Nichols, 2016;Lample et al., 2016;Yang et al., 2016;Strubell et al., 2017). Reimers and Gurevych (2017b) use fine-grained representations for less frequent words. Ma and Hovy (2016) do not use preprocessing.\n\u2022 Features. Strubell et al. (2017) and Chiu and Nichols (2016) apply word spelling features and  further integrate context features. Collobert et al. (2011) and  use neural features to represent external gazetteer information. Besides, Lample et al. (2016) and Ma and Hovy (2016) use end-to-end structure without handcrafted features.\n\u2022 Hyperparameters including learning rate, dropout rate (Srivastava et al., 2014), number of layers, hidden size etc. can strongly affect the model performance. Chiu and Nichols (2016) search for the hyperparameters for each task and show that the system performance is sensitive to the choice of hyperparameters. However, existing models use different parameter settings, which affects the fair comparison.\n\u2022 Evaluation. Some literature reports results using mean and standard deviation under different random seeds (Chiu and Nichols, 2016;Peters et al., 2017;Liu et al., 2018). Others report the best result among different trials (Ma and Hovy, 2016), which cannot be compared directly.\n\u2022 Hardware environment can also affect system accuracy. Liu et al. (2018) observes that the system gives better accuracy on NER task when trained using GPU as compared to using CPU. Besides, the running speeds are highly affected by the hardware environment.\nTo address the above concerns, we systematically analyze neural sequence labeling models on three benchmarks: CoNLL 2003 NER (Tjong Kim Sang and De Meulder, 2003), CoNLL 2000 chunking (Tjong Kim Sang and Buchholz, 2000) and PTB POS tagging (Marcus et al., 1993). Table 1 shows a summary of the models we investigate, which can be categorized under three settings: (i) character sequence representations ; (ii) word sequence representations; (iii) inference layer. Although various combinations of these three settings have been proposed in the literature, others have not been examined. We compare all models in Table 1, which includes most state-of-the-art methods. To make fair comparisons, we build a unified framework 1 to reproduce the twelve neural sequence labeling architectures in Table 1. Experiments show that our framework gives comparable or better results on reproducing existing works, showing the practicability and reliability of our analysis for practitioners. The detailed comparison and analysis show that (i) Character information provides a significant improvement on accuracy; (ii) Word-based LSTM models outperforms CNN models in most cases; (iii) CRF can improve model accuracy on NER and chunking but does not on POS tagging. Our framework is based on PyTorch with batched implementation, which is highly efficient, facilitating quick configurations for new tasks.  Hammerton (2003) was the first to exploit LSTM for sequence labeling.  built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM (Lample et al., 2016;Liu et al., 2018), GRU (Yang et al., 2016), and CNN (Chiu and Nichols, 2016;Ma and Hovy, 2016) features. Yang et al. (2017a) proposed a neural reranking model to improve NER models. These models achieve state-of-the-art results in the literature.\nReimers and Gurevych (2017b) compared several word-based LSTM models for several sequence labeling tasks, reporting the score distributions over multiple runs rather than single value. They investigated the influence of various hyperparameters and configurations. Our work is similar in comparing different neural architectures under unified settings, but differs in four main aspects: 1) Their experiments are based on a BiLSTM with handcrafted word features, while our experiments are based on end-to-end neural models without human knowledge. 2) Their system gives relatively low performances on standard benchmarks 2 , while ours can give comparable or better results with state-of-the-art models, rendering our observations more informative for practitioners. 3) Our findings are more consistent with most previous work on configurations such as usefulness of character information (Lample et al., 2016;Ma and Hovy, 2016), optimizer (Chiu and Nichols, 2016;Lample et al., 2016;Ma and Hovy, 2016) and tag scheme (Ratinov and Roth, 2009;Dai et al., 2015). In contrast, many results of Reimers and Gurevych (2017b) contradict existing reports. 4) We conduct a wider range of comparison for word sequence representations, including all combinations of character CNN/LSTM and word CNN/LSTM structures, while Reimers and Gurevych (2017b) studied the word LSTM models only.", "publication_ref": ["b22", "b19", "b15", "b13", "b11", "b16", "b18", "b11", "b16", "b21", "b12", "b25", "b16", "b14", "b11", "b16", "b11", "b0", "b11", "b16", "b25", "b31", "b17", "b1", "b16", "b13", "b0", "b11", "b28", "b3", "b11", "b16", "b0", "b21", "b25", "b16", "b14", "b14", "b7", "b0", "b11", "b33", "b28", "b25", "b16", "b28", "b0", "b1", "b11", "b16", "b27", "b0", "b0", "b21", "b14", "b16", "b14", "b31", "b30", "b17", "b6", "b11", "b14", "b33", "b0", "b16", "b34", "b11", "b16", "b0", "b11", "b16", "b22", "b2", "b25"], "figure_ref": [], "table_ref": ["tab_0", "tab_0", "tab_0"]}, {"heading": "Neural Sequence Labeling Models", "text": "Our neural sequence labeling framework contains three layers, i.e., a character sequence representation layer, a word sequence representation layer and an inference layer, as shown in Figure 1. Character information has been proven to be critical for sequence labeling tasks (Chiu and Nichols, 2016;Lample et al., 2016;Ma and Hovy, 2016), with LSTM and CNN being used to model character sequence information (\"Char Rep.\"). Similarly, on the word level, LSTM or CNN structures can be leveraged to capture long-term information or local features (\"Word Rep.\"), respectively. Subsequently, the inference layer assigns labels to each word using the hidden states of word sequence representations.  ", "publication_ref": ["b0", "b11", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Character Sequence Representations", "text": "Character features such as prefix, suffix and capitalization can be represented with embeddings through a feature-based lookup table (Collobert et al., 2011;Strubell et al., 2017), or neural networks without human-defined features (Lample et al., 2016;Ma and Hovy, 2016). In this work, we focus on neural character sequence representations without hand-engineered features.\nCharacter CNN. Using a CNN structure to encode character sequences was firstly proposed by Santos and Zadrozny (2014), and followed by many subsequent investigations (dos Santos et al., 2015;Chiu and Nichols, 2016;Ma and Hovy, 2016). In our experiments, we take the same structure as Ma and Hovy (2016), using one layer CNN structure with max-pooling to capture character-level representations. Figure 2(a) shows the CNN structure on representing word \"Mexico\".\nCharacter LSTM. Shown as Figure 2(b), in order to model the global character sequence information of a word \"Mexico\", we utilize a bidirectional LSTM on the character sequence of each word and concatenate the left-to-right final state F LST M and the right-to-left final state B LST M as character sequence representations. Liu et al. (2018) applied one bidirectional LSTM for the character sequence over a sentence rather than each word individually. We examined both structures and found that they give comparable accuracies on sequence labeling tasks. We choose Lample et al. (2016)'s structure as its character LSTMs can be calculated in parallel, making the system more efficient.", "publication_ref": ["b1", "b28", "b11", "b16", "b3", "b0", "b16", "b16", "b14", "b11"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Word Sequence Representations", "text": "Similar to character sequences in words, we can model word sequence information through LSTM or CNN structures. LSTM has been widely used in sequence labeling (Lample et al., 2016;Ma and Hovy, 2016;Chiu and Nichols, 2016;Liu et al., 2018). CNN can be much faster than LSTM due to the fact that convolution calculation can be parallel on the input sequence (Collobert et al., 2011;dos Santos et al., 2015;Strubell et al., 2017).\nWord CNN. Figure 3(a) shows the multi-layer CNN on word sequence, where words are represented by embeddings. If a character sequence representation layer is used, then word embeddings and character sequence representations are concatenated for word representations. For each CNN layer, a window of size 3 slides along the sequence, extracting local features on the word inputs and a ReLU function (Glorot et al., 2011) is followed. We follow Strubell et al. (2017) by using 4 CNN layers. Batch normalization (Ioffe and Szegedy, 2015) and dropout (Srivastava et al., 2014) are applied following each CNN layer.\nWord LSTM. Shown in Figure 3(b), word representations are fed into a forward LSTM and backward LSTM, respectively. The forward LSTM captures the word sequence information from left to right, while the backward LSTM extracts information in a reversed direction. The hidden states of the forward and backward LSTMs are concatenated at each word to give global information of the whole sequence.", "publication_ref": ["b11", "b16", "b0", "b14", "b1", "b3", "b28", "b5", "b28", "b9", "b27"], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "Inference Layer", "text": "The inference layer takes the extracted word sequence representations as features and assigns labels to the word sequence. Independent local decoding with a linear layer mapping word sequence representations  to label vocabulary and performing softmax can be quite effective (Ling et al., 2015), while for tasks that with strong output label dependency, such as NER, CRF is a more appropriate choice. In this work, we examine both softmax and CRF as inference layer on three sequence labeling tasks.", "publication_ref": ["b13"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We investigate the main influencing factors to system accuracy, including the character sequence representations, word sequence representations, inference algorithm, pretrained embeddings, tag scheme, running environment and optimizer; analyzing system performances from the perspective of decoding speed and accuracies on in-vocabulary (IV) and out-of-vocabulary (OOV) entities/chunks/words.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Settings", "text": "Data. The NER dataset has been standardly split in Tjong Kim Sang and De Meulder (2003) (Toutanova et al., 2003;Santos and Zadrozny, 2014;Ma and Hovy, 2016;Liu et al., 2018), we adopt the standard splits by using sections 0-18 as training set, sections 19-21 as development set and sections 22-24 as test set. No preprocessing is performed on either dataset except for normalizing digits. The dataset statistics are listed in Table 2.\nHyperparameters. Table 3 shows the hyperparameters used in our experiments, which mostly follow Ma and Hovy (2016), including the learning rate \u03b7 = 0.015 for word LSTM models. For word CNN based models, a large \u03b7 leads to convergence problem. We take \u03b7 = 0.005 with more epochs (200) instead. GloVe 100-dimension (Pennington et al., 2014) is used to initialize word embeddings and character embeddings are randomly initialized. We use mini-batch stochastic gradient descent (SGD) with a decayed learning rate to update parameters. For NER and chunking, we the BIOES tag scheme.\nEvaluation. Standard precision (P), recall (R) and F1-score (F) are used as the evaluation metrics for NER and chunking; token accuracy is used to evaluate the performance of POS tagger. Development datasets are used to select the optimal model among all epochs, and we report scores of the selected model on the test dataset. To reduce the volatility of the system, we conduct each experiment 5 times under different random seeds, and report the max, mean, and standard deviation for each model.", "publication_ref": ["b31", "b32", "b26", "b16", "b14", "b16", "b20"], "figure_ref": [], "table_ref": ["tab_5", "tab_6"]}, {"heading": "Results", "text": "Tables 4, 5 and 6 show the results of the twelve models on NER, chunking and POS datasets, respectively. Existing work has also been listed in the tables for comparison. To simplify the description, we use \"CLSTM\" and \"CCNN\" to represent character LSTM and character CNN encoder, respectively. Similarly, \"WLSTM\" and \"WCNN\" represent word LSTM and word CNN structure, respectively.   As shown in Table 4, most NER work focuses on WLSTM+CRF structures with different character sequence representations. We re-implement the structure of several reports (Chiu and Nichols, 2016;Ma and Hovy, 2016;Peters et al., 2017), which take the CCNN+WLSTM+CRF architecture. Our reproduced models give slightly better performances. The results of Lample et al. (2016) can be reproduced by our CLSTM+WLSTM+CRF. In most cases, our \"Nochar\" based models underperform their corresponding prototypes Strubell et al., 2017), which utilize the hand-crafted features.\nTable 5 shows the results of the chunking task. Peters et al. (2017) give the best reported single model results (95.00\u00b10.08), and our CLSTM+WLSTM+CRF gives a comparable performance (94.93\u00b10.05). We re-implement Zhai et al. (2017)'s model in our Nochar+WLSTM but cannot reproduce their results, this may because that they use grid search for hyperparameter selection. Our Nochar+WCNN+CRF can give comparable results with Collobert et al. (2011), even ours does not include character information.\nThe results of the POS tagging task is shown in Table 6. The results of Lample et al. (2016), Ma and Hovy (2016) and Yang et al. (2017b) can be reproduced by our CLSTM+WLSTM+CRF and CCNN+WLSTM+CRF models. Our WLSTM based models give better results than WLSTM+CRF based models, this is consistent with the fact that Ling et al. (2015) take CLSTM+WLSTM without CRF layer but achieve the best POS accuracy. Santos and Zadrozny (2014) build a pure CNN structure on both character and word level, which can be reproduced by our CCNN+WCNN models.\nBased on above observations, most results in the literature are reproducible. Our implementations can achieve the comparable or better results with state-of-the-art models. We do not fine-tune any hyperparameter to fit the specific task. Results on Table 4, 5 and 6 are all under the same hyperparameters, which demonstrates the generalization ability of our framework.", "publication_ref": ["b0", "b16", "b21", "b11", "b28", "b21", "b37", "b1", "b11", "b16", "b35", "b13"], "figure_ref": [], "table_ref": ["tab_8", "tab_9", "tab_10", "tab_8"]}, {"heading": "Network settings", "text": "Character LSTM vs Character CNN. Unlike the observations of Reimers and Gurevych (2017b), in our experiments, character information can significantly (p < 0.01) 3 improve sequence labeling models (by comparing the row of Nochar with CLSTM or CCNN on Table 4, 5 and 6), while the difference between CLSTM and CCNN is not significant. In most cases, CLSTM and CCNN can give comparable results under different frameworks and different tasks. CCNN gives the best NER result under the WL-STM+CRF framework, while CLSTM gets better NER results in all other configurations. For chunking and POS tagging, CLSTM consistently outperforms CCNN under all settings, while the difference is statistically insignificant (p > 0.2). We conclude that the difference between CLSTM and CCNN is small, which is consistent with the observation of Reimers and Gurevych (2017b).\nWord LSTM vs Word CNN. By comparing the performances of WLSTM+CRF, WLSTM with WCNN+CRF, WCNN on the three benchmarks, we conclude that word-based LSTM models are significantly (p < 0.01) better than word-based CNN models in most cases. It demonstrates that global word context information is necessary for sequence labeling.\nSoftmax vs CRF. Models with CRF inference layer can consistently outperform the models with softmax layer under all configurations on NER and chunking tasks, proving the effectiveness of label dependency information. While for POS tagging, the local softmax based models give slightly better accuracies while the difference is insignificant (p > 0.2).     ", "publication_ref": ["b25", "b25"], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "External factors", "text": "In addition to model structures, external factors such as pretrained embeddings, tag scheme, and optimizer can significantly influence system performance. We investigate a set of external factors on the NER dataset with the two best models: CLSTM+WLSTM+CRF and CCNN+WLSTM+CRF. Pretrained embedding. Figure 4(a) shows the F1-scores of the two best models on the NER test set with two different pretrained embeddings, as well as the random initialization. Compared with the random initialization, models using pretrained embeddings give significant improvements (p < 0.01). The GloVe 100-dimension embeddings give higher F1-scores than SENNA (Collobert et al., 2011) on both models, which is consistent with the observation of Ma and Hovy (2016).\nTag scheme. We examine two different tag schemes: BIO and BIOES (Ratinov and Roth, 2009). The results are shown in Figure 4(b). In our experiments, models using BIOES are significantly (p < 0.05) better than BIO. Our observation is consistent with most literature (Ratinov and Roth, 2009;Dai et al., 2015). Reimers and Gurevych (2017b) report that the difference between the schemes is insignificant.\nRunning environment. Liu et al. (2018) observe that neural sequence labeling models can give better results on GPU rather than CPU. We conduct repeated experiments on both GPU and CPU environments. The results are shown in Figure 4(b). Models run on CPU give a lower mean F1-score than models run on GPU, while the difference is insignificant (p > 0.2).\nOptimizer. We compare different optimizers including SGD, Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012, RMSProp (Tieleman and Hinton, 2012) and Adam (Kingma and Ba, 2014). The results are shown in Figure 5 5 . In contrast to Reimers and Gurevych (2017b), who reported that SGD is the worst optimizer, our results show that SGD outperforms all other optimizers significantly (p < 0.01), with a slower convergence process during training. Our observation is consistent with most literature (Chiu and Nichols, 2016;Lample et al., 2016;Ma and Hovy, 2016).", "publication_ref": ["b1", "b16", "b22", "b22", "b2", "b25", "b14", "b4", "b29", "b25", "b0", "b11", "b16"], "figure_ref": ["fig_3", "fig_3", "fig_3"], "table_ref": []}, {"heading": "Analysis", "text": "Decoding speed. We test the decoding speeds of the twelve models on the NER dataset using a Nvidia GTX 1080 GPU. Figure 6 shows the decoding times on 10000 NER sentences. The CRF inference layer severely limits the decoding speed due to the left-to-right inference process, which disables the parallel decoding. Character LSTM significantly slows down the system. Compared with models without character information, adding character CNN representations does not affect the decoding speed too much but can give significant accuracy improvements (shown in Section 4.3). Due to the support of parallel computing, word-based CNN models are consistently faster than word-based LSTM models, with close accuracies, leading to large utilization potential in practice.  OOV error. We conduct error analysis on in-vocabulary and out-of-vocabulary words with the CRF based models 6 . Following Ma and Hovy (2016), words in the test set are divided into four subsets: in-vocabulary words, out-of-training-vocabulary words (OOTV), out-of-embedding-vocabulary words (OOEV) and out-of-both-vocabulary words (OOBV). For NER and chunking, we consider entities or chunks rather than words. The OOV entities and chunks are categorized following Ma and Hovy (2016).\nTable 7 shows the performances of different OOV splits on three benchmarks. The top three rows list the performances of word-based LSTM CRF models, followed by the word-based CNN CRF models. The results of OOEV in NER keep 100% because of there exist only 8 OOEV entities and all are recognized correctly. It is obvious that character LSTM or CNN representations improve OOTV and OOBV the most on both WLSTM+CRF and WCNN+CRF models across all three datasets, proving that the main contribution of neural character sequence representations is to disambiguate the OOV words. Models with character LSTM representations give the best IV scores across all configurations, which may be because character LSTM can be well trained on IV data, bringing the useful global character sequence information. On the OOVs, character LSTM and CNN gives comparable results.", "publication_ref": ["b16", "b16"], "figure_ref": [], "table_ref": ["tab_12"]}, {"heading": "Conclusion", "text": "We built a unified neural sequence labeling framework to reproduce and compare recent state-of-theart models with different configurations. We explored three neural model design decisions: character sequence representations, word sequence representations, and inference layer. Experiments show that character information helps to improve model performances, especially on disambiguating OOV words. Character-level LSTM and CNN structures give comparable improvements, with the latter being more efficient. In most cases, models with word-level LSTM encoders outperform those with CNN, at the expense of longer decoding time. We observed that the CRF inference algorithm is effective on NER and chunking tasks, but does not have the advantage on POS tagging. With controlled experiments on the NER dataset, we showed that BIOES tags are better than BIO. Besides, pretrained GloVe 100d embedding and SGD optimizer give significantly better performances compared to their competitors.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We thank the anonymous reviewers for their useful comments. Yue Zhang is the corresponding author.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Named entity recognition with bidirectional LSTM-CNNs. TACL", "journal": "", "year": "2016", "authors": "Jason Chiu; Eric Nichols"}, {"ref_id": "b1", "title": "Natural language processing (almost) from scratch", "journal": "Journal of Machine Learning Research", "year": "2011-08", "authors": "Ronan Collobert; Jason Weston; L\u00e9on Bottou; Michael Karlen; Koray Kavukcuoglu; Pavel Kuksa"}, {"ref_id": "b2", "title": "Enhancing of chemical compound and drug name recognition using representative tag scheme and fine-grained tokenization", "journal": "Journal of cheminformatics", "year": "2015", "authors": "Hong-Jie Dai; Po-Ting Lai; Yung-Chun Chang; Richard Tzong-Han Tsai"}, {"ref_id": "b3", "title": "Boosting named entity recognition with neural character embeddings", "journal": "", "year": "2015", "authors": "Santos C\u0131cero Dos; Victor Guimaraes; Rio Niter\u00f3i; Janeiro De"}, {"ref_id": "b4", "title": "Adaptive subgradient methods for online learning and stochastic optimization", "journal": "Journal of Machine Learning Research", "year": "2011-07", "authors": "John Duchi; Elad Hazan; Yoram Singer"}, {"ref_id": "b5", "title": "Deep sparse rectifier neural networks", "journal": "", "year": "2011", "authors": "Xavier Glorot; Antoine Bordes; Yoshua Bengio"}, {"ref_id": "b6", "title": "Named entity recognition with long short-term memory", "journal": "Association for Computational Linguistics", "year": "2003", "authors": "James Hammerton"}, {"ref_id": "b7", "title": "A joint many-task model: Growing a neural network for multiple nlp tasks", "journal": "", "year": "2017", "authors": "Kazuma Hashimoto; Yoshimasa Tsuruoka; Richard Socher"}, {"ref_id": "b8", "title": "Bidirectional LSTM-CRF models for sequence tagging", "journal": "", "year": "2015", "authors": "Zhiheng Huang; Wei Xu; Kai Yu"}, {"ref_id": "b9", "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "journal": "", "year": "2015", "authors": "Sergey Ioffe; Christian Szegedy"}, {"ref_id": "b10", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b11", "title": "Neural architectures for named entity recognition", "journal": "", "year": "2016", "authors": "Guillaume Lample; Miguel Ballesteros; Sandeep Subramanian; Kazuya Kawakami; Chris Dyer"}, {"ref_id": "b12", "title": "Backpropagation applied to handwritten zip code recognition", "journal": "Neural computation", "year": "1989", "authors": "Yann Lecun; Bernhard Boser; S John; Donnie Denker; Richard E Henderson; Wayne Howard; Lawrence D Hubbard;  Jackel"}, {"ref_id": "b13", "title": "Finding function in form: Compositional character models for open vocabulary word representation", "journal": "", "year": "2015", "authors": "Wang Ling; Chris Dyer; Alan W Black; Isabel Trancoso; Ramon Fermandez; Silvio Amir; Luis Marujo; Tiago Luis"}, {"ref_id": "b14", "title": "Empower sequence labeling with task-aware neural language model", "journal": "AAAI", "year": "2018", "authors": "Liyuan Liu; Jingbo Shang; Frank Xu; Xiang Ren; Huan Gui; Jian Peng; Jiawei Han"}, {"ref_id": "b15", "title": "Joint entity recognition and disambiguation", "journal": "", "year": "2015", "authors": "Gang Luo; Xiaojiang Huang"}, {"ref_id": "b16", "title": "End-to-end sequence labeling via Bi-directional LSTM-CNNs-CRF", "journal": "", "year": "2016", "authors": "Xuezhe Ma; Eduard Hovy"}, {"ref_id": "b17", "title": "Building a large annotated corpus of english: The penn treebank", "journal": "Computational linguistics", "year": "1993", "authors": "P Mitchell; Mary Ann Marcus; Beatrice Marcinkiewicz;  Santorini"}, {"ref_id": "b18", "title": "Distributed representations of words and phrases and their compositionality", "journal": "", "year": "2013", "authors": "Tomas Mikolov; Ilya Sutskever; Kai Chen; Greg S Corrado; Jeff Dean"}, {"ref_id": "b19", "title": "Lexicon infused phrase embeddings for named entity resolution", "journal": "", "year": "2014", "authors": "Alexandre Passos; Vineet Kumar; Andrew Mccallum"}, {"ref_id": "b20", "title": "Glove: Global vectors for word representation", "journal": "", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher Manning"}, {"ref_id": "b21", "title": "Semi-supervised sequence tagging with bidirectional language models", "journal": "", "year": "2017", "authors": "Matthew Peters; Waleed Ammar; Chandra Bhagavatula; Russell Power"}, {"ref_id": "b22", "title": "Design challenges and misconceptions in named entity recognition", "journal": "", "year": "2009", "authors": "Lev Ratinov; Dan Roth"}, {"ref_id": "b23", "title": "Semi-supervised multitask learning for sequence labeling", "journal": "", "year": "2017", "authors": "Marek Rei"}, {"ref_id": "b24", "title": "Optimal hyperparameters for deep lstm-networks for sequence labeling tasks", "journal": "", "year": "2017", "authors": "Nils Reimers; Iryna Gurevych"}, {"ref_id": "b25", "title": "Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging", "journal": "", "year": "2017", "authors": "Nils Reimers; Iryna Gurevych"}, {"ref_id": "b26", "title": "Learning character-level representations for part-of-speech tagging", "journal": "", "year": "2014", "authors": "D Cicero; Bianca Santos;  Zadrozny"}, {"ref_id": "b27", "title": "Dropout: a simple way to prevent neural networks from overfitting", "journal": "Journal of Machine Learning Research", "year": "2014", "authors": "Nitish Srivastava; Geoffrey E Hinton; Alex Krizhevsky; Ilya Sutskever; Ruslan Salakhutdinov"}, {"ref_id": "b28", "title": "Fast and accurate entity recognition with iterated dilated convolutions", "journal": "", "year": "2017", "authors": "Emma Strubell; Patrick Verga; David Belanger; Andrew Mccallum"}, {"ref_id": "b29", "title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning", "journal": "", "year": "2012", "authors": "Tijmen Tieleman; Geoffrey Hinton"}, {"ref_id": "b30", "title": "Introduction to the conll-2000 shared task: Chunking", "journal": "Association for Computational Linguistics", "year": "2000", "authors": "Erik F Tjong Kim Sang; Sabine Buchholz"}, {"ref_id": "b31", "title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition", "journal": "", "year": "2003", "authors": "Erik F Tjong Kim Sang; Fien De Meulder"}, {"ref_id": "b32", "title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "journal": "Association for Computational Linguistics", "year": "2003", "authors": "Kristina Toutanova; Dan Klein; D Christopher; Yoram Manning;  Singer"}, {"ref_id": "b33", "title": "Multi-task cross-lingual sequence tagging from scratch", "journal": "", "year": "2016", "authors": "Zhilin Yang; Ruslan Salakhutdinov; William Cohen"}, {"ref_id": "b34", "title": "Neural reranking for named entity recognition", "journal": "", "year": "2017", "authors": "Jie Yang; Yue Zhang; Fei Dong"}, {"ref_id": "b35", "title": "Transfer learning for sequence tagging with hierarchical recurrent networks", "journal": "", "year": "2017", "authors": "Zhilin Yang; Ruslan Salakhutdinov; William W Cohen"}, {"ref_id": "b36", "title": "Adadelta: an adaptive learning rate method", "journal": "", "year": "2012", "authors": "D Matthew;  Zeiler"}, {"ref_id": "b37", "title": "Neural models for sequence chunking", "journal": "", "year": "2017", "authors": "Feifei Zhai; Saloni Potdar; Bing Xiang; Bowen Zhou"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Neural character sequence representations.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Neural word sequence representations.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Performance comparison on the CoNLL 2003 NER task.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Neural sequence labeling models in literatures. * represents using handcrafted neural features.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Statistics of datesets.", "figure_data": "ParameterValue ParameterValuechar emb size30word emb size100char hidden50word hidden200CNN window3word CNN layer4batch size10dropout rate0.5L 2 regularization \u03bb 1e-8learning rate decay 0.05word LSTM \u03b70.015 word CNN \u03b70.005word LSTM epochs 100word CNN epochs 200"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Hyperparameters.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Results for NER.4    ", "figure_data": "Results (F1-score)WLSTM+CRFchunking WLSTM WCNN+CRF WCNNNocharLiterature Max Ours Mean\u00b1std 94.37\u00b10.11 94.46 (H-15)* 94.4994.13 (Z-17) 95.02 (H-17)* 93.79 93.75\u00b10.0494.32 (C-11)* -94.23 94.12 94.11\u00b10.08 94.08\u00b10.06CLSTMLiterature Max Ours Mean\u00b1std 94.93\u00b10.05 93.15 (R-17) 94.66 (Y-17) \u2021 95.00-94.33 94.28\u00b10.04-94.76 94.66\u00b10.01-94.55 94.48\u00b10.07Literature95.00\u00b10.08 (P-17) ---CCNNOursMax Mean\u00b1std 94.86\u00b10.14 95.0694.24 94.19\u00b10.0494.77 94.66\u00b10.1394.51 94.47\u00b10.03"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Results for chunking.4    ", "figure_data": "Results (Accuracy)WLSTM+CRFPOS WLSTMWCNN+CRF WCNNNochar97.55 (H-15)* 97.20 Mean\u00b1std 97.19\u00b10.01 Literature Max Ours96.93 (M-16) 97.29 (C-11)* 96.13 (S-14) 97.45 (H-17)* 97.23 96.99 97.07 97.20\u00b10.02 96.95\u00b10.04 97.01\u00b10.04CLSTMLiterature Max Ours Mean\u00b1std 97.47\u00b10.02 97.35\u00b10.09 (L-16) \u2020 97.78 (L-15) 97.55 (Y-17) \u2021 97.49 97.51 97.48\u00b10.02-97.38 97.33\u00b10.03-97.38 97.33\u00b10.04Literature97.55 (M-16)97.33 (M-16) -97.32 (S-14)CCNNOursMax Mean\u00b1std 97.43\u00b10.02 97.4697.51 97.44\u00b10.0497.33 97.29\u00b10.0397.33 97.30\u00b10.02"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Results for POS tagging.4   ", "figure_data": "90 9291.08 91.1190.54 90.71CLSTM+WLSTM+CRF CCNN+WLSTM+CRF91.291.08 91.1191.00 91.0390.91 CLSTM+WLSTM+CRF CCNN+WLSTM+CRFF1-score86 88F1-score90.8 91.090.828483.37 83.3690.6GloVeSENNARandomGPU/BIOESCPUBIO(a) Pretrained embeddings.(b) Tag scheme and running environment."}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Results for OOV analysis.", "figure_data": ""}], "formulas": [], "doi": ""}