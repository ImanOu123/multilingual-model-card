{"title": "Evolutionary Spectral Clustering by Incorporating Temporal Smoothness", "authors": "Yun Chi; Xiaodan Song; Dengyong Zhou; Koji Hino; Belle L Tseng", "pub_date": "", "abstract": "Evolutionary clustering is an emerging research area essential to important applications such as clustering dynamic Web and blog contents and clustering data streams. In evolutionary clustering, a good clustering result should fit the current data well, while simultaneously not deviate too dramatically from the recent history. To fulfill this dual purpose, a measure of temporal smoothness is integrated in the overall measure of clustering quality. In this paper, we propose two frameworks that incorporate temporal smoothness in evolutionary spectral clustering. For both frameworks, we start with intuitions gained from the well-known k-means clustering problem, and then propose and solve corresponding cost functions for the evolutionary spectral clustering problems. Our solutions to the evolutionary spectral clustering problems provide more stable and consistent clustering results that are less sensitive to short-term noises while at the same time are adaptive to long-term cluster drifts. Furthermore, we demonstrate that our methods provide the optimal solutions to the relaxed versions of the corresponding evolutionary k-means clustering problems. Performance experiments over a number of real and synthetic data sets illustrate our evolutionary spectral clustering methods provide more robust clustering results that are not sensitive to noise and can adapt to data drifts.", "sections": [{"heading": "INTRODUCTION", "text": "In many clustering applications, the characteristics of the objects to be clustered change over time. Very often, such characteristic change contains both long-term trend due to concept drift and short-term variation due to noise. For example, in the blogosphere where blog sites are to be clustered (e.g., for community detection), the overall interests of a blogger and the blogger's friendship network may drift slowly over time and simultaneously, short-term variation may be triggered by external events. As another example, in an ubiquitous computing environment, moving objects equipped with GPS sensors and wireless connections are to be clustered (e.g., for traffic jam prediction or for animal migration analysis). The coordinate of a moving object may follow a certain route in the long-term but its estimated coordinate at a given time may vary due to limitations on bandwidth and sensor accuracy.\nThese application scenarios, where the objects to be clustered evolve with time, raise new challenges to traditional clustering algorithms. On one hand, the current clusters should depend mainly on the current data features -aggregating all historic data features makes little sense in nonstationary scenarios. On the other hand, the current clusters should not deviate too dramatically from the most recent history. This is because in most dynamic applications, we do not expect data to change too quickly and as a consequence, we expect certain levels of temporal smoothness between clusters in successive timesteps. We illustrate this point by using the following example. Assume we want to partition 5 blogs into 2 clusters. Figure 1 shows the relationship among the 5 blogs at time t-1 and time t, where each node represents a blog and the numbers on the edges represent the similarities (e.g., the number of links) between blogs. Obviously, the blogs at time t-1 should be clustered by Cut I. The clusters at time t are not so clear. Both Cut II and Cut III partition the blogs equally well. However, according to the principle of temporal smoothness, Cut III should be preferred because it is more consistent with recent history (time t-1 ). Similar ideas have long been used in time series analysis [5] where moving averages are often used to smooth out short-term fluctuations. Because similar short-term variances also exist in clustering applications, either due to data noises or due to non-robust behaviors  In this paper, we propose two evolutionary spectral clustering algorithms in which the clustering cost functions contain terms that regularize temporal smoothness. Evolutionary clustering was first formulated by Chakrabarti et al. [3] where they proposed heuristic solutions to evolutionary hierarchical clustering problems and evolutionary k-means clustering problems. In this paper, we focus on evolutionary spectral clustering algorithms under a more rigorous framework. Spectral clustering algorithms have solid theory foundation [6] and have shown very good performances. They have been successfully applied to many areas such as document clustering [22,15], imagine segmentation [19,21], and Web/blog clustering [9,18]. Spectral clustering algorithms can be considered as solving certain graph partition problems, where different graph-based measures are to be optimized. Based on this observation, we define the cost functions in our evolutionary spectral clustering algorithms by using the graph-based measures and derive corresponding (relaxed) optimal solutions. At the same time, it has been shown that these graph partition problems have close connections to different variation of the k-means clustering problems. Through these connections, we demonstrate that our evolutionary spectral clustering algorithms provide solutions to the corresponding evolutionary k-means clustering problems as special cases.\nIn summary, our main contributions in this paper can be summarized as the following:\n1. We propose two frameworks for evolutionary spectral clustering in which the temporal smoothness is incorporated into the overall clustering quality. To the best of our knowledge, our frameworks are the first evolutionary versions of the spectral clustering algorithms.\n2. We derive optimal solutions to the relaxed versions of the proposed evolutionary spectral clustering frameworks. Because the unrelaxed versions are shown to be NP-hard, our solutions provide both the practical ways of obtaining the final clusters and the upper bounds on the performance of the algorithms.\n3. We also introduce extensions to our algorithms to handle the case where the number of clusters changes with time and the case where new data points are inserted and old ones are removed over time.", "publication_ref": ["b4", "b2", "b5", "b21", "b14", "b18", "b20", "b8", "b17"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Related Work", "text": "As stated in [3], evolutionary clustering is a fairly new topic formulated in 2006. However, it has close relationships with other research areas such as clustering data streams, incremental clustering, and constrained clustering.\nIn clustering data streams, large amount of data that arrive at high rate make it impractical to store all the data in memory or to scan them multiple times. Under such a new data model, many researchers have investigated issues such as how to efficiently cluster massive data set by using limited memory and by one-pass scanning of data [12], and how to cluster evolving data streams under multiple resolutions so that a user can query any historic time period with guaranteed accuracy [1]. Clustering data stream is related to our work in that data in data streams evolve with time. However, instead of the scalability and one-pass-access issues, we focus on how to obtain clusters that evolve smoothly over time, an issue that has not been studied in the above works.\nIncremental clustering algorithms are also related to our work. There exists a large research literature on incremental clustering algorithms, whose main task is to efficiently apply dynamic updates to the cluster centers [13], medoids [12], or hierarchical trees [4] when new data points arrive. However, in most of these studies, newly arrived data points have no direct relationship with existing data points, other than that they probably share similar statistical characteristics. In comparison, our study mainly focuses on the case when the similarity among existing data points varies with time, although we can also handle insertion and removal of data points over time. In [16], an algorithm is proposed to cluster moving objects based on a novel concept of microclustering. In [18], an incremental spectral clustering algorithm is proposed to handles similarity changes among objects that evolve with time. However, the focus of both [16] and [18] is to improve computation efficiency at the cost of lower cluster quality.\nThere is also a large body of work on constrained clustering. In these studies, either hard constraints such as cannot links and must links [20] or soft constraints such as prior preferences [15] are incorporated in the clustering task. In comparison, in our work the constraints are not given a priori. Instead, we set our goal to optimize a cost function that incorporates temporal smoothness. As a consequence, some soft constraints are automatically implied when historic data and clusters are connected with current ones.\nOur work is especially inspired by the work by Chakrabarti et al. [3], in which they propose an evolutionary hierarchical clustering algorithm and an evolutionary k-means clustering algorithm. We mainly discuss the latter because of its connection to spectral clustering. Chakrabarti et al. proposed to measure the temporal smoothness by a distance between the clusters at time t and those at time t-1. Their cluster distance is defined by (1) pairing each centroid at t to its nearest peer at t-1 and (2) summing the distances between all pairs of centroids. We believe that such a distance has two weak points. First, the pairing procedure is based on heuristics and it could be unstable (a small perturbation on the centroids may change the pairing dramatically). Second, because it ignores the fact that the same data points are to be clustered in both t and t-1, this distance may be sensitive to the movement of data points such as shifts and rotations (e.g., consider a fleet of vehicles that move together while the relative distances among them remain the same).", "publication_ref": ["b2", "b11", "b0", "b12", "b11", "b3", "b15", "b17", "b15", "b17", "b19", "b14", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "NOTATIONS AND BACKGROUND", "text": "First, a word about notation. Capital letters, such as W and Z, represent matrices. Lower case letters in vector forms, such as vi and \u00b5 l , represent column vectors. Scripted letters, such as V and Vp, represent sets. For easy presentation, for a given variable, such as W and vi, we attach a subscript t, i.e., Wt and vi,t, to represent the value of the variable at time t. And we use Tr(W ) to represent the trace of W where Tr(W ) = i W (i, i). In addition, for a matrix X \u2208 R n\u00d7k , we use span(X) to represent the subspace spanned by the columns of X. For vector norms we use the Euclidian norm and for matrix norms we use the Frobenius norm, i.e., W 2 = i,j W (i, j) 2 = Tr(W T W ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The clustering problem", "text": "We state the clustering problem in the following way. For a set V of n nodes, a clustering result is a partition {V1, . . . , V k } of the nodes in V such that V = \u222a k l=1 V l and Vp \u2229 Vq = \u2205 for 1 \u2264 p, q \u2264 k, p = q. A partition (clustering result) can be equivalently represented as an n-by-k matrix Z whose elements are in {0, 1} where Z(i, j) = 1 if only if node i belongs to cluster j. Obviously, Z \u2022 1 k = 1n, where 1 k and 1n are k-dimensional and n-dimensional vectors of all ones. In addition, we can see that the columns of Z are orthogonal. Furthermore, we normalize Z in the following way: we divide the l-th column of Z by |V l | to getZ, where |V l | is the size of V l . Note that the columns ofZ are orthonormal, i.e.,Z TZ = I k .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "K-means clustering", "text": "The k-means clustering problem is one of the most widelystudied clustering problems. Assume the i-th node in V can be represented by an m-dimensional feature vector vi \u2208 R m , then the k-means clustering problem is to find a partition {V1, . . . , V k } that minimizes the following measure\nKM = k l=1 i\u2208V l vi \u2212 \u00b5 l 2 (1)\nwhere \u00b5 l is the centroid (mean) of the l-th cluster, i.e., \u00b5 l = j\u2208V l vj /|V l |. A well-known algorithm to the k-means clustering problem is the so called k-means algorithm in which after initially randomly picking k centroids, the following procedure is repeated until convergence: all the data points are assigned to the clusters whose centroids are nearest to them, and then the cluster centroids are updated by taking the average of the data points assigned to them.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Spectral clustering", "text": "The basic idea of spectral clustering is to cluster based on the eigenvectors of a (possibly normalized) similarity matrix W defined on the set of nodes in V. Very often W is positive semi-definite. Commonly used similarities include the inner product of the feature vectors, W (i, j) = v T i vj , the diagonally-scaled Gaussian similarity, W (i, j) = exp(\u2212( vi \u2212 vj ) T diag( \u03b3)( vi \u2212 vj )), and the affinity matrices of graphs.\nSpectral clustering algorithms usually solve graph partitioning problems where different graph-based measures are to be optimized. Two popular measures are to maximize the average association and to minimize the normalized cut [19]. For two subsets, Vp and Vq, of the node set V (where Vp and Vq do not have to be disjoint), we first define the association between Vp and Vq as assoc(Vp, Vq) = i\u2208Vp,j\u2208Vq W (i, j) Then we can write the k-way average association as\nAA = k l=1 assoc(V l , V l ) |V l | (2)\nand the k-way normalized cut as\nNC = k l=1 assoc(V l , V\\V l ) assoc(V l , V)(3)\nwhere V\\V l is the complement of V. For consistency, we further define the negated average association as\nNA = Tr(W ) \u2212 AA = Tr(W ) \u2212 k l=1 assoc(V l , V l ) |V l |(4)\nwhere, as will be shown later, NA is always non-negative if W is positive semi-definite. In the remaining of the paper, instead of maximizing AA, we equivalently aim to minimize NA, and as a result, all the three objective functions, KM , NA and NC are to be minimized. Finding the optimal partition Z for either the negated average association or the normalized cut is NP-hard [19]. Therefore, in spectral clustering algorithms, usually a relaxed version of the optimization problem is solved by (1) computing eigenvectors X of some variations of the similarity matrix W , (2) projecting all data points to span(X), and (3) applying the k-means algorithm to the projected data points to obtain the clustering result. While it may seem nonintuitive to apply spectral analysis and then again use the k-means algorithm, it has been shown that such procedures have many advantages such as they work well in the cases when the data points are not linearly separable [17]. The focus of our paper is in step (1). For steps (2) and (3) we follow the standard procedures in traditional spectral clustering and thus will not give more details on them.", "publication_ref": ["b18", "b18", "b16", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "EVOLUTIONARY SPECTRAL CLUSTERING-TWO FRAMEWORKS", "text": "In this section we propose two frameworks for evolutionary spectral clustering. We first describe the basic idea.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Basic Idea", "text": "We define a general cost function to measure the quality of a clustering result on evolving data points. The function contains two costs. The first cost, snapshot cost (CS), only measures the snapshot quality of the current clustering result with respect to the current data features, where a higher snapshot cost means worse snapshot quality. The second cost, temporal cost (CT ), measures the temporal smoothness in terms of the goodness-of-fit of the current clustering result with respect to either historic data features or historic clustering results, where a higher temporal cost means worse temporal smoothness. The overall cost function 1 is defined as a linear combination of these two costs:\nCost = \u03b1 \u2022 CS + \u03b2 \u2022 CT (5)\nwhere 0 \u2264 \u03b1 \u2264 1 is a parameter assigned by the user and together with \u03b2(= 1 \u2212 \u03b1), they reflect the user's emphasis on the snapshot cost and temporal cost, respectively.\nIn both frameworks that we propose, for a current partition (clustering result), the snapshot cost CS is measured by the clustering quality when the partition is applied to the current data. The two frameworks are different in how the temporal cost CT is defined. In the first framework, which we name PCQ for preserving cluster quality, the current partition is applied to historic data and the resulting cluster quality determines the temporal cost. In the second framework, which we name PCM for preserving cluster membership, the current partition is directly compared with the historic partition and the resulting difference determines the temporal cost.\nIn the discussion of both frameworks, we first use the kmeans clustering problem, Equation ( 1), as a motivational example and then formulate the corresponding evolutionary spectral clustering problems (both NA and NC). We also provide the optimal solutions to the relaxed versions of the evolutionary spectral clustering problems and show how they relate back to the evolutionary k-means clustering problem. In addition, in this section, we focus on a special case where the number of clusters does not change with time and neither does the number of nodes to be clustered. We will discuss the more general cases in the next section.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Preserving Cluster Quality (PCQ)", "text": "In the first framework, PCQ, the temporal cost is expressed as how well the current partition clusters historic data. We illustrate this through an example. Assume that two partitions, Zt and Z \u2032 t , cluster the current data at time t equally well. However, to cluster historic data at time t-1, the clustering performance using partition Zt is better than using partition Z \u2032 t . In such a case, Zt is preferred over Z \u2032 t because Zt is more consistent with historic data. We formalize this idea for the k-means clustering problem using the following overall cost function\nCostKM = \u03b1 \u2022 CSKM + \u03b2 \u2022 CTKM (6) = \u03b1 \u2022 KM t Zt + \u03b2 \u2022 KMt\u22121 Zt = \u03b1 \u2022 k l=1 i\u2208V l,t vi,t \u2212 \u00b5 l,t 2 + \u03b2 \u2022 k l=1 i\u2208V l,t vi,t\u22121 \u2212 \u00b5 l,t\u22121 2\nwhere Zt means \"evaluated by the partition Zt, where Zt is computed at time t\" and \u00b5 l,t\u22121 = j\u2208V l,t vj,t\u22121/|V l,t | .\nNote that in the formula of CTKM , the inner summation is over all data points in V l,t , the clusters at time t. That is, although the feature values used in the summation are those at time t-1 (i.e., vi,t\u22121's), the partition used is that at time t (i.e., Zt). As a result, this cost CTKM = KMt\u22121 Zt penalizes those clustering results (at t) that do not fit well with recent historic data (at t-1 ) and therefore promotes temporal smoothness of clusters.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Negated Average Association", "text": "We now formulate the PCQ framework for evolutionary spectral clustering. We start with the case of negated aver-age association. Following the idea of Equation ( 6), at time t, for a given partition Zt, a natural definition of the overall cost is\nCostNA = \u03b1 \u2022 CSNA + \u03b2 \u2022 CTNA (7) = \u03b1 \u2022 NAt Zt + \u03b2 \u2022 NAt\u22121 Zt\nThe above cost function is almost identical to Equation ( 6), except that the cluster quality is measured by the negated average association NA rather than the k-means KM .\nNext, we derive a solution to minimizing CostNA. First, it can be easily shown that the negated average association defined in Equation ( 4) can be equivalently written as\nNA = Tr(W ) \u2212 Tr(Z T WZ)(8)\nTherefore 2 we write the overall cost (7) as\nCostNA = \u03b1 \u2022 [Tr(Wt) \u2212 Tr(Z T t WtZt)](9)\n+ \u03b2 \u2022 [Tr(Wt\u22121) \u2212 Tr(Z T t Wt\u22121Zt)] = Tr(\u03b1Wt + \u03b2Wt\u22121) \u2212 Tr Z T t (\u03b1Wt + \u03b2Wt\u22121)Zt\nNotice that the first term Tr(\u03b1Wt + \u03b2Wt\u22121) is a constant independent of the clustering partitions and as a result, minimizing CostNA is equivalent to maximizing the trace Tr[Z T t (\u03b1Wt + \u03b2Wt\u22121)Zt], subject toZt being a normalized indicator matrix (cf Sec 2.1). Because maximizing the average association is an NP-hard problem, finding the so-lutionZt that minimizes CostNA is also NP-hard. So following most spectral clustering algorithms, we relaxZt to Xt \u2208 R n\u00d7k with X T t Xt = I k . It is well-known [11] that one solution to this relaxed optimization problem is the matrix Xt whose columns are the k eigenvectors associated with the top-k eigenvalues of matrix \u03b1Wt + \u03b2Wt\u22121. Therefore, after computing the solution Xt we can project the data points into span(Xt) and then apply k-means to obtain a solution to the evolutionary spectral clustering problem under the measure of negated average association. In addition, the value Tr(\u03b1Wt + \u03b2Wt\u22121) \u2212 Tr X T t (\u03b1Wt + \u03b2Wt\u22121)Xt provides a lower bound on the performance of the evolutionary clustering problem.\nMoreover, Zha et al. [22] have shown a close connection between the k-means clustering problem and spectral clustering algorithms -they proved that if we put the mdimensional feature vectors of the n data points in V into an m-by-n matrix A = ( v1, . . . , vn), then\nKM = Tr(A T A) \u2212 Tr(Z T A T AZ)(10)\nComparing Equations ( 10) and ( 8), we can see that the kmeans clustering problem is a special case of the negated average association spectral clustering problem, where the similarity matrix W is defined by the inner product A T A. As a consequence, our solution to the NA evolutionary spectral clustering problem can also be applied to solve the k-means evolutionary clustering problem in the PCQ framework, i.e., under the cost function defined in Equation (6).", "publication_ref": ["b6", "b10", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Normalized Cut", "text": "For the normalized cut, we extend the idea of Equation ( 6) similarly. By replacing the KM Equation ( 6) with NC, we define the overall cost for evolutionary normalized cut to be\nCostNC = \u03b1 \u2022 CSNC + \u03b2 \u2022 CTNC (11) = \u03b1 \u2022 NCt Zt + \u03b2 \u2022 NCt\u22121 Zt\nShi et al. [19] have proved that computing the optimal solution to minimize the normalized cut is NP-hard. As a result, finding an indicator matrix Zt that minimizes CostNC is also NP-hard. We now provide an optimal solution to a relaxed version of the problem. Bach et al. [2] proved that for a given partition Z, the normalized cut can be equivalently written as\nNC = k \u2212 Tr Y T D \u2212 1 2 W D \u2212 1 2 Y (12\n)\nwhere D is a diagonal matrix with D(i, i) = n j=1 W (i, j) and Y is any matrix in R n\u00d7k that satisfies two conditions: (a) the columns of D \u22121/2 Y are piecewise constant with respect to Z and (b) Y T Y = I k . We remove the constraint (a) to get a relaxed version for the optimization problem\nCostNC \u2248 \u03b1 \u2022 k \u2212 \u03b1 \u2022 Tr X T t D \u2212 1 2 t WtD \u2212 1 2 t Xt (13) + \u03b2 \u2022 k \u2212 \u03b2 \u2022 Tr X T t D \u2212 1 2 t\u22121 Wt\u22121D \u2212 1 2 t\u22121 Xt = k \u2212 Tr X T t \u03b1D \u2212 1 2 t WtD \u2212 1 2 t + \u03b2D \u2212 1 2 t\u22121 Wt\u22121D \u2212 1 2 t\u22121 Xt\nfor some Xt \u2208 R n\u00d7k such that X T t Xt = I k . Again we have a trace maximization problem and a solution is the matrix Xt whose columns are the k eigenvectors associated with the top-k eigenvalues of matrix \u03b1D\n\u2212 1 2 t WtD \u2212 1 2 t + \u03b2D \u2212 1 2 t\u22121 Wt\u22121D \u2212 1 2 t\u22121 .\nAnd again, after obtaining Xt, we can further project data points into span(Xt) and then apply the k-means algorithm to obtain the final clusters.\nMoreover, Dhillon et al. [8] have proved that the normalized cut approach can be used to minimize the cost function of a weighted kernel k-means problem. As a consequence, our evolutionary spectral clustering algorithm can also be applied to solve the evolutionary version of the weighted kernel k-means clustering problem.", "publication_ref": ["b18", "b1", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Discussion on the PCQ Framework", "text": "The PCQ evolutionary clustering framework provides a data clustering technique similar to the moving average framework in time series analysis, in which the short-term fluctuation is expected to be smoothed out. The solutions to the PCQ framework turn out to be very intuitive -the historic similarity matrix is scaled and combined with current similarity matrix and the new combined similarity matrix is fed to traditional spectral clustering algorithms.\nNotice that one assumption we have used in the above derivation is that the temporal cost is determined by data at time t-1 only. However, the PCQ framework can be easily extended to cover longer historic data by including similarity matrices W 's at older time, probably with different weights (e.g., scaled by an exponentially decaying factor to emphasize more recent history).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Preserving Cluster Membership (PCM)", "text": "The second framework of evolutionary spectral clustering, PCM, is different from the first framework, PCQ, in how the temporal cost is measured. In this second framework, the temporal cost is expressed as the difference between the current partition and the historic partition. We again illustrate this by an example. Assume that two partitions, Zt and Z \u2032 t , cluster current data at time t equally well. However, when compared to the historic partition Zt\u22121, Zt is much more similar to Zt\u22121 than Z \u2032 t is. In such a case, Zt is preferred over Z \u2032 t because Zt is more consistent with historic partition. We first formalize this idea for the evolutionary k-means problem. For convenience of discussion, we write the current partition as Zt = {V1,t, . . . , V k,t } and the historic partition as Zt\u22121 = {V1,t\u22121, . . . , V k,t\u22121 }. Now we want to define a measure for the difference between Zt and Zt\u22121. Comparing two partitions has long been studied in the literatures of classification and clustering. Here we use the traditional chi-square statistics [14] to represent the distance between two partitions\n\u03c7 2 (Zt, Zt\u22121) = n k i=1 k j=1 |Vij | 2 |Vi,t| \u2022 |Vj,t\u22121| \u2212 1\nwhere |Vij | is the number of nodes that are both in Vi,t (at time t) and in Vj,t\u22121 (at time t-1 ). Actually, in the above definition, the number of clusters k does not have to be the same at time t and t-1, and we will come back to this point in the next section. By ignoring the constant shift of -1 and the constant scaling n, we define the temporal cost for the k-means clustering problem as\nCTKM = \u2212 k i=1 k j=1 |Vij | 2 |Vi,t| \u2022 |Vj,t\u22121| (14)\nwhere the negative sign is because we want to minimize CTKM . The overall cost can be written as\nCostKM = \u03b1 \u2022 CSKM + \u03b2 \u2022 CTKM (15) = \u03b1 \u2022 k l=1 i\u2208V l,t vi,t \u2212 \u00b5 l,t 2 \u2212 \u03b2 \u2022 k i=1 k j=1 |Vij | 2 |Vi,t| \u2022 |Vj,t\u22121|", "publication_ref": ["b13"], "figure_ref": [], "table_ref": []}, {"heading": "Negated Average Association", "text": "Recall that in the case of negated average association, we want to maximize NA = Tr(Z T WZ) whereZ is further relaxed to continuous-valued X, whose columns are the k eigenvectors associated with the top-k eigenvalues of W . So in the PCM framework, we shall define a distance dist(Xt, Xt\u22121) between Xt, a set of eigenvectors at time t, and Xt\u22121, a set of eigenvectors at time t-1. However, there is a subtle point -for a solution X \u2208 R n\u00d7k that maximizes Tr(X T W X), any X \u2032 = XQ is also a solution, where Q \u2208 R k\u00d7k is an arbitrary orthogonal matrix. This is because Tr(X T W X) = Tr(X T W XQQ T ) = Tr((XQ) T W XQ) = Tr(X \u2032T W X \u2032 ). Therefore we want a distance dist(Xt, Xt\u22121) that is invariant with respect to the rotation Q. One such solution, according to [11], is the norm of the difference between two projection matrices, i.e.,", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "dist(Xt, Xt", "text": "\u22121) = 1 2 XtX T t \u2212 Xt\u22121X T t\u22121 2 (16)\nwhich essentially measures the distance between span(Xt) and span(Xt\u22121). Furthermore in Equation ( 16), the number of columns in Xt does not have to be the same as that in Xt\u22121 and we will discuss this in the next section. By using this distance to quantify the temporal cost, we derive the total cost for the negated average association as\nCostNA = \u03b1 \u2022 CSNA + \u03b2 \u2022 CTNA (17) =\u03b1 \u2022 Tr(Wt) \u2212 Tr(X T t WtXt) + \u03b2 2 \u2022 XtX T t \u2212 Xt\u22121X T t\u22121 2 =\u03b1 \u2022 Tr(Wt) \u2212 Tr(X T t WtXt) + \u03b2 2 Tr XtX T t \u2212 Xt\u22121X T t\u22121 T XtX T t \u2212 Xt\u22121X T t\u22121 =\u03b1 \u2022 Tr(Wt) \u2212 Tr(X T t WtXt) + \u03b2 2 Tr(XtX T t XtX T t \u2212 2XtX T t Xt\u22121X T t\u22121 + Xt\u22121X T t\u22121 Xt\u22121X T t\u22121 ) =\u03b1 \u2022 Tr(Wt) \u2212 Tr(X T t WtXt) + \u03b2k \u2212 \u03b2Tr X T t Xt\u22121X T t\u22121 Xt =\u03b1 \u2022 Tr(Wt) + \u03b2 \u2022 k \u2212 Tr X T t (\u03b1Wt + \u03b2Xt\u22121X T t\u22121 )Xt\nTherefore, an optimal solution that minimizes CostNA is the matrix Xt whose columns are the k eigenvectors associated with the top-k eigenvalues of the matrix \u03b1Wt + \u03b2Xt\u22121X T t\u22121 . After getting Xt, the following steps are the same as before.\nFurthermore, it can be shown that the un-relaxed version of the distance defined in Equation ( 16) for spectral clustering is equal to that defined in Equation ( 15) for k-means clustering by a constant shift. That is, it can be shown (cf. Bach et al. [2]) that\n1 2 Z tZ T t \u2212Zt\u22121Z T t\u22121 2 = k \u2212 k i=1 k j=1 |Vij | 2 |Vi,t| \u2022 |Vj,t\u22121|(18)\nAs a result, the evolutionary spectral clustering based on negated average association in the PCM framework provides a relaxed solution to the evolutionary k-means clustering problem defined in the PCM framework, i.e., Equation (15).", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Normalized Cut", "text": "It is straightforward to extend the PCM framework from the negated average association to normalized cut as\nCostNC = \u03b1 \u2022 CSNC + \u03b2 \u2022 CTNC (19) = \u03b1 \u2022 k \u2212 \u03b1 \u2022 Tr X T t D \u2212 1 2 t WtD \u2212 1 2 t Xt + \u03b2 2 \u2022 XtX T t \u2212 Xt\u22121X T t\u22121 2 = k \u2212 Tr X T t \u03b1D \u2212 1 2 t WtD \u2212 1 2 t + \u03b2Xt\u22121X T t\u22121 Xt\nTherefore, an optimal solution that minimizes CostNC is the matrix Xt whose columns are the k eigenvectors associated with the top-k eigenvalues of the matrix \u03b1D\n\u2212 1 2 t WtD \u2212 1 2 t + \u03b2Xt\u22121X T t\u22121 .\nAfter obtaining Xt, the subsequent steps are the same as before.\nIt is worth mentioning that in the PCM framework, CostNC has an advantage over CostNA in terms of the ease of selecting an appropriate \u03b1. In CostNA, the two terms CSNA and CTNA are of different scales -CSNA measures a sum of variances and CTNA measures some probability distribution. Consequently, this difference needs to be considered when choosing \u03b1. In contrast, for CostNC , because the CSNC is\nnormalized, both D \u2212 1 2 t WtD \u2212 1 2 t\nand Xt\u22121X T t\u22121 have the same 2-norms scale, for both matrices have \u03bbmax = 1. Therefore, the two terms CSNC and CTNC are comparable and \u03b1 can be selected in a straightforward way.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discussion on the PCM Framework", "text": "In the PCM evolutionary clustering framework, all historic data are taken into consideration (with different weights) -Xt partly depends on Xt\u22121, which in turn partly depends on Xt\u22122 and so on. Let us look at two extreme cases. When \u03b1 approaches 1, the temporal cost will become unimportant and as a result, the clusters are computed at each time window independent of other time windows. On the other hand, when \u03b1 approaches 0, the eigenvectors in all time windows are required to be identical. Then the problem becomes a special case of the higher-order singular value decomposition problem [7], in which singular vectors are computed for the three modes (the rows of W , the columns of W , and the timeline) of a data tensor W where W is constructed by concatenating Wt's along the timeline.\nIn addition, if the similarity matrix Wt is positive semidefinite, then \u03b1D\n\u2212 1 2 t WtD \u2212 1 2 t + \u03b2Xt\u22121X T t\u22121 is also positive semi-definite because both D \u2212 1 2 t WtD \u2212 1 2 t\nand Xt\u22121X T t\u22121 are positive semi-definite.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Comparing Frameworks PCQ and PCM", "text": "Now we compare the PCQ and PCM frameworks. For simplicity of discussion, we only consider time slots t and t-1 and ignore older history.\nIn terms of the temporal cost, PCQ aims to maximize Tr(X T t Wt\u22121Xt) while for PCM, Tr(X T t Xt\u22121X T t\u22121 Xt) is to be maximized. However, these two are closely connected. By applying the eigen-decomposition on Wt\u22121, we have\nX T t Wt\u22121Xt = X T t (Xt\u22121, X \u22a5 t\u22121 )\u039bt\u22121(Xt\u22121, X \u22a5 t\u22121 ) T Xt\nwhere \u039bt\u22121 is a diagonal matrix whose diagonal elements are the eigenvalues of Wt\u22121 ordered by decreasing magnitude, and Xt\u22121 and X \u22a5 t\u22121 are the eigenvectors associated with the first k and the residual n \u2212 k eigenvectors of Wt\u22121, respectively. It can be easily verified that both Tr(X T t Wt\u22121Xt) and Tr(X T t Xt\u22121X T t\u22121 Xt) are maximized when Xt = Xt\u22121 (or more rigorously, when span(Xt) = span(Xt\u22121)). The differences between PCQ and PCM are (a) if the eigenvectors associated with the smaller eigenvalues (other than the top k) are considered and (b) the level of penalty when Xt deviates from Xt\u22121. For PCQ, all the eigenvectors are considered and their deviations between time t and t-1 are penalized according to the corresponding eigenvalues. For PCM, rather than all eigenvectors, only the first k eigenvectors are considered and they are treated equally. In other words, in the PCM framework, other than the historic cluster membership, all details about historic data are ignored.\nAlthough by keeping only historic cluster membership, PCM introduces more information loss, there may be benefits in other aspects. For example, the CT part in the PCM framework does not necessarily have to be temporal costit can represent any prior knowledge about cluster membership. For example, we can cluster blogs purely based on interlinks. However, other information such as the content of the blogs and the demographic data about the bloggers may provide valuable prior knowledge about cluster membership that can be incorporated into the clustering. The PCM framework can handle such information fusion easily.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "GENERALIZATION", "text": "There are two assumptions in the PCQ and the PCM framework proposed in the last section. First, we assumed that the number of clusters remains the same over all time. Second, we assumed that the same set of nodes is to be clustered in all timesteps. Both assumptions are too restrictive in many applications. In this section, we extend our frameworks to handle the issues of variation in cluster numbers and insertion/removal of nodes over time.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Variation in Cluster Numbers", "text": "In our discussions so far, we have assumed that the number of clusters k does not change with time. However, keeping a fixed k over all time windows is a very strong restriction. Determining the number of clusters is an important research problem in clustering and there are many effective methods for selecting appropriate cluster numbers (e.g., by thresholding the gaps between consecutive eigenvalues).\nHere we assume that the number of cluster k at time t has been determined by one of these methods and we investigate what will happen if the cluster number k at time t is different from the cluster number k \u2032 at time t-1.\nIt turns out that both the PCQ and the PCM frameworks can handle variations in cluster number already. In the PCQ framework, the temporal cost is expressed by historic data themselves, not by historic clusters and therefore the computation at time t is independent of the cluster number k \u2032 at time t-1. In the PCM framework, as we have mentioned, the partition distance (Equation 14) and the subspace distance (Equation 16) can both be used without change when the two partitions have different numbers of clusters. As a result, both of our PCQ and PCM frameworks can handle variations in the cluster numbers.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Insertion and Removal of Nodes", "text": "Another assumption that we have been using is that the number of nodes in V does not change with time. However, in many applications the data points to be clustered may vary with time. In the blog example, very often there are old bloggers who stop blogging and new bloggers who just start. Here we propose some heuristic solutions to handle this issue.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Node Insertion and Removal in PCQ", "text": "For the PCQ framework, the key is \u03b1Wt + \u03b2Wt\u22121. When old nodes are removed, we can simply remove the corresponding rows and columns from Wt\u22121 to getWt\u22121 (assum-ingWt\u22121 is n1 \u00d7 n1). However, when new nodes are inserted at time t, we need to add entries toWt\u22121 and to extended it to\u0174t\u22121, which has the same dimension as Wt (assuming Wt is n2 \u00d7 n2). Without lost of generality, we assume that the first n1 rows and columns of Wt correspond to those nodes inWt\u22121. We propose to achieve this by definin\u011d\nWt\u22121= W t\u22121 Et\u22121 E T t\u22121 Ft\u22121 for Et\u22121 = 1 n 1W t\u22121 1n 1 1 T n 2 \u2212n 1 Ft\u22121 = 1 n 2 1 1 T n 1W t\u22121 1n 1 1n 2 \u2212n 1 1 T n 2 \u2212n 1\nSuch a heuristic has the following good properties, whose proofs are skipped due to the space limitation.\nProperty 1. (1)\u0174t\u22121 is positive semi-definite if Wt\u22121 is. (2)\nIn\u0174t\u22121, for each existing node v old , each newly inserted node vnew looks like an average node in that the similarity between vnew and v old is the same as the average similarity between any existing node and v old . (3) In\u0174t\u22121, the similarity between any pair of newly inserted nodes is the same as the average similarity among all pairs of existing nodes.\nWe can see that these properties are appealing when no prior knowledge is given about the newly inserted nodes.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Node Insertion and Removal in PCM", "text": "For the PCM framework, when old nodes are removed, we remove the corresponding rows from Xt\u22121 to getXt\u22121 (assumingXt\u22121 is n1 \u00d7 k). When new nodes are inserted at time t, we extendXt\u22121 toXt\u22121, which has the same dimension as Xt (assuming Xt is n2 \u00d7 k) as follow\u015d\nXt\u22121 = X t\u22121 Gt\u22121 for Gt\u22121 = 1 n1 1n 2 \u2212n 1 1 T n 1X t\u22121(20)\nThat is, we insert new rows as the row average ofXt\u22121.\nAfter obtainingXt\u22121, we replace the term \u03b2Xt\u22121X T t\u22121 with \u03b2Xt\u22121(X T t\u22121Xt\u22121 ) \u22121X T t\u22121 in Equations ( 17) and (19). Such a heuristic has the following good property, whose proof is skipped due to the space limit.\nProperty 2. Equation ( 20) corresponds to for each newly inserted nodes, assigning to it a prior clustering membership that is approximately proportional to the size of the clusters at time t-1.", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTAL STUDIES", "text": "In this section, we report experimental studies based on both synthetic data sets and a real blog data set.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Synthetic Data", "text": "First, we use several experiments on synthetic data sets to illustrate the good properties of our algorithms.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "NA-based Evolutionary Spectral Clustering", "text": "In this subsection, we show three experimental studies based on synthetic data. In the first experiment, we demonstrate a stationary case where data variation is due to a zero-mean noise. In the second experiment, we show a nonstationary case where there are concept drifts. In the third experiment, we show a case where there is a large difference between the PCQ and PCM frameworks.\nBy using the k-means algorithm, we design two baselines. The first baseline, which we call ACC, accumulates all historic data before the current timestep t and applies the kmeans algorithm on the aggregated data. The second baseline, which we call IND, independently applies the k-means algorithm on the data in only timestep t and ignore all historic data before t.\nFor our algorithms, we use the NA-based PCQ and PCM algorithms, because of the equivalence between the NAbased spectral clustering problem and the k-means clustering problem (Equation ( 10)). We choose to use W = A T A in the NA-based evolutionary spectral clustering and compare its results with that of the k-means baseline algorithms. For a fair comparison, we use the KM defined for the k-means clustering problem (i.e., Equation ( 1)) as the measure for performance, where a smaller KM value is better.\nThe data points to be clustered are generated in the following way. 800 two-dimensional data points are initially positioned as described in Figure 2(a) at timestep 1. As can be seen, there are roughly four clusters (the data were actually generated by using four Gaussian distributions centered in the four quadrants). Then in timesteps 2 to 10, we perturb the initial positions of the data points by adding different noises according to the experimental setup. Unless stated otherwise, all experiments are repeated 50 times with different random seeds and the average performances are reported.  In the first experiment, for timesteps 2 through 10, we add an i.i.d. Gaussian noise following N (0, 0.5) to the initial positions of the data points. We use this data to simulation a stationary situation where the concept is relatively stable but there exist short-term noises. In Figures 3(a) and 3(b), we report the snapshot cost CSKM and the temporal cost CTKM for the two baselines and for our algorithms (with \u03b1 = 0.9 for both PCQ and PCM) from timesteps 1 to 10. For both costs, a lower value is better. As can be seen from the figure, the ACC baseline has low temporal smoothness but very high snapshot cost, whereas the IND baseline has the low snapshot cost but very high temporal cost. In comparison, our two algorithms show low temporal cost at the price of a little increase in snapshot cost. The overall cost \u03b1 \u2022 CSKM + \u03b2 \u2022 CTKM is given in Figure 3(c). As can be seen, the ACC baseline has the worst overall performance and our algorithms improve a little over the IND baseline. In addition, Figure 3(d) shows the degree of cluster change over time as defined in Equation (18). We can see that as  The performance for the stationary synthetic data set, which shows that PCQ and PCM result in low temporal cost at a price of a small increase in snapshot cost expected, the cluster membership change using our frameworks is less dramatic than that of the IND baseline, which takes no historic information into account.\nNext, for the same data set, we let \u03b1 increase from 0.2 to 1 with a step of 0.1. Figure 4 shows the average snapshot cost and the temporal cost over all 10 timesteps under different settings of \u03b1. As we expected, when \u03b1 increases, to emphasize more on the snapshot cost, we get better snapshot quality at the price of worse temporal smoothness. This result demonstrates that our frameworks are able to control the tradeoff between the snapshot quality and the temporal smoothness. In the second experiment, we simulate a non-stationary situation. At timesteps 2 through 10, before adding random noises, we first rotate all data points by a small random angle (with zero mean and a variance of \u03c0/4). Figure 2(b) shows the positions of data points in a typical timestep. Figure 5 gives the performance of the four algorithms. As can be seen, while the performance of our algorithms and the IND baseline has little change, the performance of the ACC baseline becomes very poor. This result shows that if an aggregation approach is used, we should not aggregate the data features in a non-stationary scenario -instead, we should aggregate the similarities among data points.  In the third experiment, we show a case where the PCQ and PCM frameworks behave differently. We first generate data points using the procedure described in the first experiment (the stationary scenario), except that this time we generate 60 timesteps for a better view. This time, instead of 4 clusters, we let the algorithms partition the data into 2 clusters. From Figure 2(a) we can see that there are obviously two possible partitions, a horizonal cut or a vertical cut at the center, that will give similar performance where the performance difference will mainly be due to short-term noises. Figure 6 shows the degree of cluster membership change over the 60 timesteps in one run (for obvious reasons, no averaging is taken in this experiment). As can be seen, the cluster memberships of the two baselines jump around, which shows that they are not robust to noise in this case. Also can be seen, the cluster membership of the PCM algorithm varies much lesser than that of the PCQ algorithm. The reason for this difference is that switching the partition from the horizontal cut to the vertical cut will introduce much higher penalty to PCM than to PCQ -PCM is directly penalized by the change of eigenvectors, which corresponds to the change of cluster membership; for PCQ, the penalty is indirectly acquired from historic data, not historic cluster membership.  ", "publication_ref": ["b17"], "figure_ref": ["fig_3", "fig_5", "fig_5", "fig_5", "fig_6", "fig_3", "fig_8", "fig_3", "fig_10"], "table_ref": []}, {"heading": "NC-based Evolutionary Spectral Clustering", "text": "It is difficult to compare the NC-based evolutionary spectral clustering with the k-means clustering algorithm. Instead, in this experiment, we use a toy example in the 2dimensional Euclidean space with only 4 timesteps (as shown in Figure 7) to compare the non-evolutionary version (upper panels, with \u03b1 = 1) and the evolutionary version (lower panels, with \u03b1 = 0.9) of the NC-based evolutionary spectral clustering algorithms. Figure 7 gives the clustering results with the correct cluster numbers provided to the algorithm. As can be seen, for the non-evolutionary version, at timestep 2, the two letters \"D\"'s are confused because they move too near to each other. At timestep 4, due to the change of cluster number, part of the newly introduced letter \"0\" is confused with the second \"D\". Neither happens to the evolutionary version, in which the temporal smoothness is taken into account.  As a conclusion, these experiments based on synthetic data sets demonstrate that compared to traditional clustering methods, our evolutionary spectral clustering algorithms can provide clustering results that are more stable and consistent, less sensitive to short-term noise, and adaptive to long-term trends.", "publication_ref": [], "figure_ref": ["fig_12", "fig_12"], "table_ref": []}, {"heading": "Real Blog Data", "text": "The real blog data was collected by an NEC in-house blog crawler. Due to space limit, we will not describe how the data was collected and refer interested readers to [18] for details. This NEC blog data set contains 148,681 entry-toentry links among 407 blogs crawled during 63 consecutive weeks, between July 10th in 2005 and September 23rd in 2006. By looking at the contents of the blogs, we discovered two main groups: a set of 314 blogs with technology focus and a set of 93 blogs with politics focus. Figure 8 shows the blog graph for this NEC data set, where the nodes are blogs (with different labels depending on their group member) and the edges are interlinks among blogs (obtained by aggregating all entry-to-entry links). One application of clustering blogs is to discover communities. Since we already have the ground truth of the two communities based on content analysis, we start by running the clustering algorithms with k = 2. The data is prepared in this way: each week corresponds to a timestep; all the entry-to-entry links in a week are used to construct an affinity matrix for the blogs of that week (i.e., those blogs that are relevant to at least one entry-to-entry link in that week); and the affinity matrix is used as the similarity matrix W in the clustering algorithms. For baselines, we again use ACC and IND, except that this time the normalized cut algorithm is used. For our algorithms, we use the NC-based PCQ and PCM. Figures 9(a),(b), and (c) give the CSNC, CTNC, and CostNC for the two baseline algorithms and the PCM algorithm (to make the figures readable, we did not plot the results for PCQ, which are similar to those of PCM, The performance on the NEC data, which shows that evolutionary spectral clustering clearly outperforms non-evolutionary ones as shown in Table 1). In Figure 9(d), we show the error between the cluster results and the ground truth obtained from content analysis, where the error is the distance between partitions defined in Equation (18). As can be seen from these figures, the evolutionary spectral clustering has the best performance in all four measures. The high snapshot cost of IND was surprising to us. We believe this could be due to the non-robustness of the normalized cut package (which we obtained from the homepage of the first author of [19]). In addition, note that CTNC is usually smaller than CSNC because CTNC is computed over those nodes that are active in both t and t-1 and such nodes are usually less than those that are active at t. This is also one of the reasons for the high variation of the curves. In addition, we run the algorithms under different cluster numbers and report the performance in Table 1, where the best results among the same category are in bold face. Our evolutionary clustering algorithms always give more stable and consistent cluster results than the baselines where the historic data is totally ignored or totally aggregated.", "publication_ref": ["b17", "b17", "b18"], "figure_ref": ["fig_13", "fig_14", "fig_14"], "table_ref": ["tab_0", "tab_0"]}, {"heading": "CONCLUSION", "text": "There are new challenges when traditional clustering techniques are applied to new data types, such as streaming data and Web/blog data, where the relationship among data evolves with time. On one hand, because of long-term concept drifts, a naive approach based on aggregation will not give satisfactory cluster results. On the other hand, shortterm variations occur very often due to noise. Preferably the cluster results should not change dramatically over short time and should exhibit temporal smoothness. In this paper, we propose two frameworks to incorporate temporal smoothness in evolutionary spectral clustering. In both frameworks, a cost function is defined where in addition to the traditional cluster quality cost, a second cost is introduced to regularize the temporal smoothness. We then derive the (relaxed) optimal solutions for solving the cost functions. The solutions turn out to have very intuitive interpretation and have forms analogous to traditional techniques used in time series analysis. Experimental studies demonstrate that these new frameworks provide cluster results that are both stable and consistent in the short-term and adaptive in the long run.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ACKNOWLEDGMENTS", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "We thank Shenghuo Zhu, Wei Xu, and Kai Yu for the inspiring discussions, and thank Junichi Tatemura for helping us prepare the data sets.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A framework for clustering evolving data streams", "journal": "", "year": "2003", "authors": "C C Aggarwal; J Han; J Wang; P S Yu"}, {"ref_id": "b1", "title": "Learning spectral clustering, with application to speech separation", "journal": "Journal of Machine Learning Research", "year": "2006", "authors": "F R Bach; M I Jordan"}, {"ref_id": "b2", "title": "Evolutionary clustering", "journal": "", "year": "2006", "authors": "D Chakrabarti; R Kumar; A Tomkins"}, {"ref_id": "b3", "title": "Incremental clustering and dynamic information retrieval", "journal": "", "year": "1997", "authors": "M Charikar; C Chekuri; T Feder; R Motwani"}, {"ref_id": "b4", "title": "The Analysis of Time Series: An Introduction", "journal": "Chapman & Hall/CRC", "year": "", "authors": "C Chatfield"}, {"ref_id": "b5", "title": "Spectral Graph Theory", "journal": "American Mathematical Society", "year": "1997", "authors": "F R K Chung"}, {"ref_id": "b6", "title": "A multilinear singular value decomposition", "journal": "SIAM J. on Matrix Analysis and Applications", "year": "2000", "authors": "L De Lathauwer; B De Moor; J Vandewalle"}, {"ref_id": "b7", "title": "Kernel k-means: spectral clustering and normalized cuts", "journal": "", "year": "2004", "authors": "I S Dhillon; Y Guan; B Kulis"}, {"ref_id": "b8", "title": "K-means clustering via principal component analysis", "journal": "", "year": "2004", "authors": "C Ding; X He"}, {"ref_id": "b9", "title": "On a theorem of weyl concerning eigenvalues of linear transformations", "journal": "", "year": "1949", "authors": "K Fan"}, {"ref_id": "b10", "title": "Matrix Computations", "journal": "Johns Hopkins University Press", "year": "1996", "authors": "G Golub; C V Loan"}, {"ref_id": "b11", "title": "Clustering data streams", "journal": "", "year": "2000", "authors": "S Guha; N Mishra; R Motwani; L O'callaghan"}, {"ref_id": "b12", "title": "Genic: A single pass generalized incremental algorithm for clustering", "journal": "", "year": "2004", "authors": "C Gupta; R Grossman"}, {"ref_id": "b13", "title": "Comparing partitions", "journal": "Journal of Classification", "year": "1985", "authors": "L J Hubert; P Arabie"}, {"ref_id": "b14", "title": "Document clustering with prior knowledge", "journal": "", "year": "2006", "authors": "X Ji; W Xu"}, {"ref_id": "b15", "title": "Clustering moving objects", "journal": "", "year": "2004", "authors": "Y Li; J Han; J Yang"}, {"ref_id": "b16", "title": "On spectral clustering: Analysis and an algorithm", "journal": "", "year": "2001", "authors": "A Ng; M Jordan; Y Weiss"}, {"ref_id": "b17", "title": "Incremental spectral clustering with application to monitoring of evolving blog communities", "journal": "", "year": "2007", "authors": "H Ning; W Xu; Y Chi; Y Gong; T Huang"}, {"ref_id": "b18", "title": "Normalized cuts and image segmentation", "journal": "IEEE Trans. on Pattern Analysis and Machine Intelligence", "year": "2000", "authors": "J Shi; J Malik"}, {"ref_id": "b19", "title": "Constrained K-means clustering with background knowledge", "journal": "", "year": "2001", "authors": "K Wagstaff; C Cardie; S Rogers; S Schroedl"}, {"ref_id": "b20", "title": "Segmentation using eigenvectors: A unifying view", "journal": "", "year": "1999", "authors": "Y Weiss"}, {"ref_id": "b21", "title": "Spectral relaxation for k-means clustering", "journal": "", "year": "2001", "authors": "H Zha; X He; C H Q Ding; M Gu; H D Simon"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: An evolutionary clustering scenario", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: (a) The initial data point positions and (b) A typical position in the non-stationary case", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 3 :3Figure 3: The performance for the stationary synthetic data set, which shows that PCQ and PCM result in low temporal cost at a price of a small increase in snapshot cost", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 4 :4Figure 4: The tradeoff between snapshot cost and temporal cost, which can be controlled by \u03b1", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 5 :5Figure 5: Performance for a non-stationary synthetic data set, which shows that aggregating data features does not work", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 6 :6Figure 6: A case where PCM is more robust vs PCQ", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 7 :7Figure 7: A toy example that demonstrates our evolutionary spectral clustering is more robust and can handle changes of cluster number", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Figure 8 :8Figure 8: The blog graph for the NEC data set", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Figure 9 :9Figure 9: The performance on the NEC data, which shows that evolutionary spectral clustering clearly outperforms non-evolutionary ones", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Performance under Different Cluster Numbers for the Blog Data Set", "figure_data": "ACC IND NC PCQ NC PCMCS0.760.790.680.46k=2CT0.590.200.100.06Total Cost0.740.730.630.42CS1.221.531.121.07k=3CT0.980.220.240.02Total Cost1.211.431.060.98CS1.712.051.701.71k=4CT1.400.180.390.03Total Cost1.691.891.591.57"}], "formulas": [{"formula_id": "formula_0", "formula_text": "KM = k l=1 i\u2208V l vi \u2212 \u00b5 l 2 (1)", "formula_coordinates": [3.0, 122.64, 442.64, 170.27, 28.61]}, {"formula_id": "formula_1", "formula_text": "AA = k l=1 assoc(V l , V l ) |V l | (2)", "formula_coordinates": [3.0, 390.36, 97.4, 165.59, 27.82]}, {"formula_id": "formula_2", "formula_text": "NC = k l=1 assoc(V l , V\\V l ) assoc(V l , V)(3)", "formula_coordinates": [3.0, 383.88, 151.16, 172.07, 27.7]}, {"formula_id": "formula_3", "formula_text": "NA = Tr(W ) \u2212 AA = Tr(W ) \u2212 k l=1 assoc(V l , V l ) |V l |(4)", "formula_coordinates": [3.0, 333.48, 215.84, 222.47, 27.7]}, {"formula_id": "formula_4", "formula_text": "Cost = \u03b1 \u2022 CS + \u03b2 \u2022 CT (5)", "formula_coordinates": [3.0, 390.36, 681.88, 165.59, 9.16]}, {"formula_id": "formula_5", "formula_text": "CostKM = \u03b1 \u2022 CSKM + \u03b2 \u2022 CTKM (6) = \u03b1 \u2022 KM t Zt + \u03b2 \u2022 KMt\u22121 Zt = \u03b1 \u2022 k l=1 i\u2208V l,t vi,t \u2212 \u00b5 l,t 2 + \u03b2 \u2022 k l=1 i\u2208V l,t vi,t\u22121 \u2212 \u00b5 l,t\u22121 2", "formula_coordinates": [4.0, 100.08, 463.36, 192.83, 93.69]}, {"formula_id": "formula_6", "formula_text": "CostNA = \u03b1 \u2022 CSNA + \u03b2 \u2022 CTNA (7) = \u03b1 \u2022 NAt Zt + \u03b2 \u2022 NAt\u22121 Zt", "formula_coordinates": [4.0, 363.24, 94.12, 192.71, 26.78]}, {"formula_id": "formula_7", "formula_text": "NA = Tr(W ) \u2212 Tr(Z T WZ)(8)", "formula_coordinates": [4.0, 381.24, 194.96, 174.71, 11.04]}, {"formula_id": "formula_8", "formula_text": "CostNA = \u03b1 \u2022 [Tr(Wt) \u2212 Tr(Z T t WtZt)](9)", "formula_coordinates": [4.0, 331.68, 228.68, 224.27, 11.86]}, {"formula_id": "formula_9", "formula_text": "+ \u03b2 \u2022 [Tr(Wt\u22121) \u2212 Tr(Z T t Wt\u22121Zt)] = Tr(\u03b1Wt + \u03b2Wt\u22121) \u2212 Tr Z T t (\u03b1Wt + \u03b2Wt\u22121)Zt", "formula_coordinates": [4.0, 341.4, 244.16, 194.85, 29.38]}, {"formula_id": "formula_10", "formula_text": "KM = Tr(A T A) \u2212 Tr(Z T A T AZ)(10)", "formula_coordinates": [4.0, 369.96, 539.12, 185.87, 11.05]}, {"formula_id": "formula_11", "formula_text": "CostNC = \u03b1 \u2022 CSNC + \u03b2 \u2022 CTNC (11) = \u03b1 \u2022 NCt Zt + \u03b2 \u2022 NCt\u22121 Zt", "formula_coordinates": [5.0, 100.44, 109.96, 192.35, 26.78]}, {"formula_id": "formula_12", "formula_text": "NC = k \u2212 Tr Y T D \u2212 1 2 W D \u2212 1 2 Y (12", "formula_coordinates": [5.0, 99.96, 224.8, 188.75, 12.28]}, {"formula_id": "formula_13", "formula_text": ")", "formula_coordinates": [5.0, 288.71, 228.12, 4.08, 8.97]}, {"formula_id": "formula_14", "formula_text": "CostNC \u2248 \u03b1 \u2022 k \u2212 \u03b1 \u2022 Tr X T t D \u2212 1 2 t WtD \u2212 1 2 t Xt (13) + \u03b2 \u2022 k \u2212 \u03b2 \u2022 Tr X T t D \u2212 1 2 t\u22121 Wt\u22121D \u2212 1 2 t\u22121 Xt = k \u2212 Tr X T t \u03b1D \u2212 1 2 t WtD \u2212 1 2 t + \u03b2D \u2212 1 2 t\u22121 Wt\u22121D \u2212 1 2 t\u22121 Xt", "formula_coordinates": [5.0, 55.8, 315.16, 236.99, 66.13]}, {"formula_id": "formula_15", "formula_text": "\u2212 1 2 t WtD \u2212 1 2 t + \u03b2D \u2212 1 2 t\u22121 Wt\u22121D \u2212 1 2 t\u22121 .", "formula_coordinates": [5.0, 53.76, 427.36, 239.12, 29.41]}, {"formula_id": "formula_16", "formula_text": "\u03c7 2 (Zt, Zt\u22121) = n k i=1 k j=1 |Vij | 2 |Vi,t| \u2022 |Vj,t\u22121| \u2212 1", "formula_coordinates": [5.0, 343.2, 273.8, 178.97, 27.58]}, {"formula_id": "formula_17", "formula_text": "CTKM = \u2212 k i=1 k j=1 |Vij | 2 |Vi,t| \u2022 |Vj,t\u22121| (14)", "formula_coordinates": [5.0, 370.32, 386.72, 185.51, 27.58]}, {"formula_id": "formula_18", "formula_text": "CostKM = \u03b1 \u2022 CSKM + \u03b2 \u2022 CTKM (15) = \u03b1 \u2022 k l=1 i\u2208V l,t vi,t \u2212 \u00b5 l,t 2 \u2212 \u03b2 \u2022 k i=1 k j=1 |Vij | 2 |Vi,t| \u2022 |Vj,t\u22121|", "formula_coordinates": [5.0, 321.36, 446.92, 234.47, 43.29]}, {"formula_id": "formula_19", "formula_text": "\u22121) = 1 2 XtX T t \u2212 Xt\u22121X T t\u22121 2 (16)", "formula_coordinates": [5.0, 396.18, 675.12, 159.65, 20.85]}, {"formula_id": "formula_20", "formula_text": "CostNA = \u03b1 \u2022 CSNA + \u03b2 \u2022 CTNA (17) =\u03b1 \u2022 Tr(Wt) \u2212 Tr(X T t WtXt) + \u03b2 2 \u2022 XtX T t \u2212 Xt\u22121X T t\u22121 2 =\u03b1 \u2022 Tr(Wt) \u2212 Tr(X T t WtXt) + \u03b2 2 Tr XtX T t \u2212 Xt\u22121X T t\u22121 T XtX T t \u2212 Xt\u22121X T t\u22121 =\u03b1 \u2022 Tr(Wt) \u2212 Tr(X T t WtXt) + \u03b2 2 Tr(XtX T t XtX T t \u2212 2XtX T t Xt\u22121X T t\u22121 + Xt\u22121X T t\u22121 Xt\u22121X T t\u22121 ) =\u03b1 \u2022 Tr(Wt) \u2212 Tr(X T t WtXt) + \u03b2k \u2212 \u03b2Tr X T t Xt\u22121X T t\u22121 Xt =\u03b1 \u2022 Tr(Wt) + \u03b2 \u2022 k \u2212 Tr X T t (\u03b1Wt + \u03b2Xt\u22121X T t\u22121 )Xt", "formula_coordinates": [6.0, 53.76, 104.08, 256.67, 154.1]}, {"formula_id": "formula_21", "formula_text": "1 2 Z tZ T t \u2212Zt\u22121Z T t\u22121 2 = k \u2212 k i=1 k j=1 |Vij | 2 |Vi,t| \u2022 |Vj,t\u22121|(18)", "formula_coordinates": [6.0, 62.76, 366.56, 230.03, 27.58]}, {"formula_id": "formula_22", "formula_text": "CostNC = \u03b1 \u2022 CSNC + \u03b2 \u2022 CTNC (19) = \u03b1 \u2022 k \u2212 \u03b1 \u2022 Tr X T t D \u2212 1 2 t WtD \u2212 1 2 t Xt + \u03b2 2 \u2022 XtX T t \u2212 Xt\u22121X T t\u22121 2 = k \u2212 Tr X T t \u03b1D \u2212 1 2 t WtD \u2212 1 2 t + \u03b2Xt\u22121X T t\u22121 Xt", "formula_coordinates": [6.0, 66.48, 489.04, 226.31, 78.26]}, {"formula_id": "formula_23", "formula_text": "\u2212 1 2 t WtD \u2212 1 2 t + \u03b2Xt\u22121X T t\u22121 .", "formula_coordinates": [6.0, 53.76, 597.88, 239.12, 24.85]}, {"formula_id": "formula_24", "formula_text": "normalized, both D \u2212 1 2 t WtD \u2212 1 2 t", "formula_coordinates": [6.0, 53.76, 706.0, 120.03, 14.89]}, {"formula_id": "formula_25", "formula_text": "\u2212 1 2 t WtD \u2212 1 2 t + \u03b2Xt\u22121X T t\u22121 is also positive semi-definite because both D \u2212 1 2 t WtD \u2212 1 2 t", "formula_coordinates": [6.0, 316.8, 266.68, 239.08, 29.53]}, {"formula_id": "formula_26", "formula_text": "X T t Wt\u22121Xt = X T t (Xt\u22121, X \u22a5 t\u22121 )\u039bt\u22121(Xt\u22121, X \u22a5 t\u22121 ) T Xt", "formula_coordinates": [6.0, 329.88, 408.08, 212.37, 11.86]}, {"formula_id": "formula_27", "formula_text": "Wt\u22121= W t\u22121 Et\u22121 E T t\u22121 Ft\u22121 for Et\u22121 = 1 n 1W t\u22121 1n 1 1 T n 2 \u2212n 1 Ft\u22121 = 1 n 2 1 1 T n 1W t\u22121 1n 1 1n 2 \u2212n 1 1 T n 2 \u2212n 1", "formula_coordinates": [7.0, 49.2, 625.28, 253.95, 28.41]}, {"formula_id": "formula_28", "formula_text": "Property 1. (1)\u0174t\u22121 is positive semi-definite if Wt\u22121 is. (2)", "formula_coordinates": [7.0, 53.76, 688.44, 238.73, 20.6]}, {"formula_id": "formula_29", "formula_text": "Xt\u22121 = X t\u22121 Gt\u22121 for Gt\u22121 = 1 n1 1n 2 \u2212n 1 1 T n 1X t\u22121(20)", "formula_coordinates": [7.0, 335.88, 216.24, 219.95, 22.29]}], "doi": ""}