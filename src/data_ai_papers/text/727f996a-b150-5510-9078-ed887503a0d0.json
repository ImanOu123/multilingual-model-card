{"title": "Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker", "authors": "Melanie Sclar; Sachin Kumar; Peter West; Alane Suhr; Yejin Choi; Yulia Tsvetkov", "pub_date": "", "abstract": "Theory of Mind (ToM)-the ability to reason about the mental states of other people-is a key element of our social intelligence. Yet, despite their ever more impressive performance, large-scale neural language models still lack basic theory of mind capabilities out-of-the-box. We posit that simply scaling up models will not imbue them with theory of mind due to the inherently symbolic and implicit nature of the phenomenon, and instead investigate an alternative: can we design a decoding-time algorithm that enhances theory of mind of off-the-shelf neural language models without explicit supervision? We present SYMBOLICTOM, a plug-andplay approach to reason about the belief states of multiple characters in reading comprehension tasks via explicit symbolic representation. More concretely, our approach tracks each entity's beliefs, their estimation of other entities' beliefs, and higher-order levels of reasoning, all through graphical representations, allowing for more precise and interpretable reasoning than previous approaches. Empirical results on the well-known ToMi benchmark (Le et al., 2019) demonstrate that SYMBOLICTOM dramatically enhances off-the-shelf neural networks' theory of mind in a zero-shot setting while showing robust out-of-distribution performance compared to supervised baselines. Our work also reveals spurious patterns in existing theory of mind benchmarks, emphasizing the importance of out-of-distribution evaluation and methods that do not overfit a particular dataset.", "sections": [{"heading": "Introduction", "text": "Reasoning about other people's intentions, desires, thoughts, and beliefs is a cornerstone of human social intelligence. Children naturally develop an understanding of every individual's unique mental state and how it might impact their actions (Frith et al., 2003). Known as Theory of Mind (ToM) (Premack and Woodruff, 1978), this ability is crucial for efficient and effective communication.\nAlice and Bob are in a room with a basket and a box. Alice puts some celery in the basket and leaves the room. Bob then moves the celery into the box. Figure 1: A simple story requiring theory of mind. Note that Alice's belief of the celery's location differs from reality (i.e. Alice holds a false belief ). Readers must reason that Alice will look for the celery where she left it, and that Bob will make that same assumption. Questions shown require different depths of mental state modeling.", "publication_ref": ["b13", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Where will Bob search for the celery? (*) Where does Bob think that Alice will look for the celery when she returns? (**)", "text": "Alice\nCognitive and literary studies have extensively argued theory of mind's key role in understanding stories, in order to explain and predict each character's actions (Zunshine, 2006;Carney et al., 2014;Leverage et al., 2010;van Duijn et al., 2015, inter alia). As exemplified in Figure 1, readers need to model Bob's mental state (called first-order ToM), as well as Bob's estimation of Alice's mental state (second-order ToM) to answer questions.\nDespite recent progress in language understanding abilities, large language models have been shown to lack theory of mind skills (Sap et al., 2022). Existing efforts to enable them have primarily relied on supervised methods (e.g., Grant et al., 2017;Nematzadeh et al., 2018;Arodi and Cheung, 2021). However, current reading comprehension datasets for theory of mind reasoning are simplistic and lack diversity, leading to brittle downstream models which, as we show, fail in the presence of even slight out-of-distribution perturbations.\nWe introduce SYMBOLICTOM, an inferencetime method that improves large language models' theory of mind capabilities by augmenting them with an explicit symbolic graphical representation of each character's beliefs. Unlike prior efforts, our approach does not require training and instead divides the problem into simpler subtasks, leveraging off-the-shelf models to solve them, and carefully consolidating their results. This makes SYMBOLIC-TOM significantly more robust than existing models trained specifically for theory of mind behavior.\nWhile beliefs about the world state differ among people, most existing work on encoding belief states do not model this behavior relying on singular graphs (Jansen, 2022;Jacqmin et al., 2022). SYMBOLICTOM, instead, utilizes a set of graphs, each representing what the character p 1 thinks that p 2 believes that [...] p m assumes to be the current state of the world, where m is the maximum reasoning depth as determined by the user. This explicit, recursive mental state representation enables the model to answer questions from the perspective of each character. SYMBOLICTOM's process of selecting and querying a particular character's graph grounds it in cognitive science research arguing theory of mind as an essential mechanism of selective attention (Leslie et al., 2004). Our approach also instills desirable inductive biases, such as object permanence-for example, object locations (represented by edges in the graphs) are assumed to be constant until the method can infer a change. Although existing NLP datasets only test up to second-order reasoning (i.e., m \u2264 2), SYMBOLICTOM is designed to work at any depth.\nSYMBOLICTOM dramatically improves the performance of large language models in theory of mind reading comprehension tasks. For example, GPT-3-Davinci's (Brown et al., 2020) accuracy on the ToMi benchmark (Le et al., 2019) increases by 38 absolute points using SYMBOLICTOM (yielding 92% accuracy averaging across question types). Furthermore, we extend the ToMi test sets with diverse story structures and sentence paraphrases and demonstrate that our approach is significantly more robust than supervised approaches.", "publication_ref": ["b50", "b8", "b22", "b32", "b15", "b24", "b3", "b18", "b17", "b21", "b7", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Motivation and Background", "text": "Although large-scale language models have recently shown improvements in some classic theory of mind examples, they are still far from reliably showing theory of mind capabilities (Sap et al., 2022;Yu et al., 2022;Ullman, 2023;Shapira et al., 2023). While the training data for these models includes human-written stories which require theory of mind reasoning, this information is largely implicit and hence difficult for models to learn. ChatGPT and GPT3-Davinci's incorrect answers to Figure 1's question #2 are shown below. 1 ChatGPT (gpt-3.5-turbo): Based on the information provided, Bob would likely think that Alice will look for the celery in the box when she returns. Since Bob moved the celery from the basket to the box, he would assume that Alice would expect to find it in its new location. GPT3 (text-davinci-003): Bob will likely think that Alice will look for the celery in the box, since that is where he moved it.\nNatural stories which make theory of mind explicit are scarce, necessitating automatically generated, template-based datasets like ToM-bAbI (Nematzadeh et al., 2018) and ToMi (Le et al., 2019). However, templated narratives cover limited types of interactions, and include only simplistic discourse and sentence structures. On the other hand, relying on human-generated data, e.g., in situated dialogue (Bara et al., 2021), leads to barriers in dataset size due to high annotation costs. Moreover, another source of data-text-based games with multiple characters-also faces limitations; in particular, modeling mental states is required mainly to infer intents  and to maintain a consistent style of each character (Qiu et al., 2022). Rather, in this work, we aim to study and evaluate differences in knowledge and beliefs among multiple characters, traditional cognitive aspects of theory of mind.\nTo the best of our knowledge, the only available datasets for measuring theory of mind in reading comprehension tasks are ToM-bAbI and ToMi. Because of their templated nature, supervised training on them is prone to overfitting to spurious artifacts in the data. While ToMi was developed to counter this behavior in ToM-bAbI by introducing noise in the form of flexible sentence ordering and distractor sentences and characters, we show it still faces the same pitfalls. Due to theory of mind's inherently implicit nature and limited naturally available data, in this work, we argue against supervision as a way forward and instead call for unsupervised, or inference-time approaches that combine modern neural models and traditional symbolic algorithms.", "publication_ref": ["b32", "b30", "b40", "b24", "b20", "b4", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "Methods", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SYMBOLICTOM: Algorithm Overview", "text": "Our goal is to automatically answer reading comprehension questions given a story involving multiple characters, without requiring any supervised training or fine-tuning on this task. We first introduce key notation, then provide a high-level overview of SYMBOLICTOM (Algorithm 1). Notation We use the term k-th order theory of mind to refer to an estimate of what a character p 1 thinks that p 2 thinks that [...] p k thinks about the world state. We denote this belief by B p 1 ,...,p k . We let k \u2264 m, where m is a maximum reasoning depth. This is a user-specified limit, denoting the maximum recursion that the reader is assumed to be capable of performing. For instance, in Figure 1, questions #1 and #2 measure 1st-and 2nd-order theory of mind respectively; B Bob refers to Bob's beliefs about the current world state, and B Bob,Alice represents Bob's estimation of Alice's beliefs about the world state. In this work, B p 1 ,...,p k only represents beliefs about the current world state, without additional modeling of other characters' mental states, such as their opinions.\nA benefit of this notation is that any belief state can be represented as an m-th order one.\nWe assume that what p k thinks that p k thinks is equivalent to what p k thinks, and by induction, B p 1 ...p k \u2261 B p 1 ,...,p k ,p k ,...,p k , where the last p k is repeated m \u2212 k times. We adopt this notation going forward, denoting all states as m-th order. As a conceptual note, the set of belief states {B p 1 ...p k ,q k+1 ...qm | \u2200q k+1 , . . . , q m } represents the mental state from the perspective of p 1 , . . . , p k , using m \u2212 k order of theory of mind.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Local and Global Context", "text": "We represent each B p 1 ...p k as a graph (a simplified version is depicted in Figure 1) where each node represents an entity (e.g. a character, object, room, container) and each edge connects two nodes with a stated relationship in the story. We construct the graphs by iterating through a story one sentence at a time, and adding both nodes and edges to the graph (BELIEFTRACKINGSTRUCTURE; described in \u00a73.2 and Algorithm 2). Each edge is also paired with the sentence from the story from which it was constructed. We refer to the set of all belief state graphs as the local contexts. We also maintain a global context graph, denoted by G, which contains the true world state. G has an identical structure to B p 1 ...p k . See A.1 for a detailed definition of G.\nQuestion Answering After parsing a story and constructing the complete set of belief-tracking structures, we can use these structures to answer questions by querying the appropriate graph and considering it as the real-world state. For example, if the question is \"Where will Bob think that Alice will look for the celery?\", we retrieve B Bob, Alice , but if instead the question were \"Where will Bob look for the celery?\", we would retrieve B Bob . In both cases, we would ask \"Where is the celery?\" on the retrieved graph. Figure 2 shows an example of the full pipeline.\nGiven a question, we identify the relevant characters p 1 , . . . , p k mentioned in order heuristically, and rephrase the question to ask directly about the world state (PROCESSQUESTION; owing to the questions' templatic nature in our evaluation data, this approach rephrases all questions correctly). 2 We then retrieve the corresponding graph; i.e., B p 1 ,...,p k , of which we can simply ask the question \"Where is the celery?\". To obtain the answer, we first reconstruct a subset S \u2032 of sentences in the original story, consisting of those represented by the retrieved graph (SENTENCESREPRESENTEDBYGRAPH). We then use a large language model L to answer the simplified question zero-shot given S \u2032 , using as input the sentences in S \u2032 in the same order as they appeared in the original text, and preserving phrasing. We optionally further filter S \u2032 based on the entities mentioned in the question (FILTERBASEDONQUESTION). An ablation study showed this last step can often be skipped (see Appendix C.1). ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Computing the Belief Graphs", "text": "B p 1 ...p k\nAssuming each story is told chronologically, SYM-BOLICTOM processes each sentence s sequentially in two stages (Algorithm 2). First, it extracts all actions in s and updates the global context G from an omniscient point of view while identifying the characters (W) who witnessed actions and world state changes described in the sentence. Second, for each witness w \u2208 W, it propagates this new information to update w's local contexts; i.e., we only update B p 1 ,...,pm with, for 1 \u2264 i \u2264 m, each p i \u2208 W, and leave the rest unchanged.\nAs an example, when processing the last sentence in Figure 3, we update Bob and Charles's state (B Bob and B Charles ) and the perception of G 2. Propagate new information to local contexts if and only if all people involved are witnesses. Algorithm 2 Belief Tracking\n\u2718 BAlice,Alice \u2718 BAlice,Bob \u2718 BAlice,Charles = \u2205 \u2718 BBob,Alice \u2713 BBob,Bob = BBob \u2713 BBob,Charles \u2718 BCharles,Alice \u2713 BCharles,Bob \u2713 BCharles,Charles = BCharles = \u2205 = what X thinks that Y thinks is the current world state BX,Y =\nfunction BELIEFTRACKINGSTRUCTURE(sentences) for s \u2208 sentences do G, W \u2190 GLOBALCONTEXTUPDATE(G, s) for all [p1, . . . , pm] \u2208 W m do Bp 1 ...pm \u2190 LOCALCONTEXTUPDATE(Bp 1 .\n..pm ,G,s) end for end for end function others' respective state (B Bob,Charles , B Charles, Bob ), but we need not update Alice's state, or Bob and Charles's perception of Alice's mental state, because she did not witness the actions described.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Detecting Witnesses, Updating Graphs, and Propagating Knowledge", "text": "Starting with an empty graph, for each new sentence s, we update the global context G by combining off-the-shelf models in four steps (Algorithm 3; GLOBALCONTEXTUPDATE). First, we detect the existing edges E in G that contradict s. This is implemented as detecting Natural Language Inference (NLI) contradictions, considering s as the premise, and every edge in G as a hypothesis. Second, we augment G with new edges and nodes, by first deriving a natural language representation r of the state resulting from the actions described in s, and then extract new nodes and edges from r as OpenIE triples (Stanovsky et al., 2018). For example, for \"Bob then moves the celery to the box\", the resulting state r would be the sentence \"The celery is in the box\". To obtain r from s, we prompt a language model such as GPT3 (see Appendix A.2 for details). After obtaining r, we use the corresponding triple (e.g., (celery, box, is in)) to add new nodes and edges to G if not already present (e.g., the nodes \"celery\" and \"box\", and a directed edge connecting them labeled by \"is in\"). Importantly, we only add edges that represent positive relations between nodes; i.e., there will not be an edge representing \"The celery is not in the box\". Third, we detect the witnesses W of the actions described in s. Since each character will be a node in G, we identify W as all the characters that are in the same connected component as the newly added edges. Finally, we remove all edges E that are no longer valid in G as identified by the NLI contradictions. This step is done last to ensure all witnesses are found before their edges are deleted.\nAlgorithm 3 World State Beliefs Graphs Update\nfunction GLOBALCONTEXTUPDATE(G, s) E \u2190 DETECTCONTRADICTINGEDGES(G, s) G \u2190 G \u222a TRIPLES(RESULTINGSTATE(s)) W \u2190 FINDWITNESSES(G) G \u2190 G \\ E return G, W end function function LOCALCONTEXTUPDATE(C, G, s) E \u2190 DETECTCONTRADICTINGEDGES(G, s) C \u2190 C \u222a TRIPLES(RESULTINGSTATE(s)) C \u2190 PROPAGATEKNOWLEDGE(G, C, s) C \u2190 C \\ E return C end function\nThe local contexts (B p 1 ,...,p k ) are updated similarly (LOCALCONTEXTUPDATE in Algorithm 3), except for an additional step of knowledge propagation. While performing an action, a character may implicitly gain information not described in the text. For example, when entering a room, a character may gain knowledge of the people and visible objects in the room. This knowledge (already present in G, which tracks the omniscient world state) needs to be propagated to each B p 1 ,...,p k with each p i \u2208 W. As G represents the true world state, we simplify the problem: if a character p i is in a specific connected component D of G, then it possesses all knowledge encoded in D. To model implicit knowledge gain, we add all edges in D to B p 1 ,...,p k . As D represents the latest global context information, we remove from the local context edges that are in B p 1 ,...,p k but not in D (representing outdated beliefs about the world state).", "publication_ref": ["b36"], "figure_ref": [], "table_ref": []}, {"heading": "Notes on Memory Efficiency", "text": "Memory requirements grow exponentially with m, the maximum order of theory of mind considered. However, m in practice is small, as humans find tasks increasingly challenging as m increases. For example, psychological tests for m = 3 are aimed at teenagers and adults (Valle et al., 2015). All experiments in this work are done with m = 2, the maximum order of theory of mind reasoning that current datasets evaluate. If memory were a concern, one could process the questions first for memory efficiency, and compute only the graphs B p 1 ,...,p k required for target queries.", "publication_ref": ["b41"], "figure_ref": [], "table_ref": []}, {"heading": "Fundamental Issues in Existing ToM Datasets", "text": "Construction of ToMi As introduced in \u00a72, the sole large-scale theory of mind dataset for reading comprehension tasks is ToMi (Le et al., 2019). Barring its added distractor characters and sentences, ToMi strictly mimics the Sally-Anne test, a widely adopted evaluation for assessing children's social cognitive ability to reason about others' mental states (Wimmer and Perner, 1983;Baron-Cohen et al., 1985). Stories are structured are as follows: characters A and B are in a room, and A moves an object from an opaque container to another; B may or may not leave the room before A moves the object. B will know the object's new location if and only if they were in the room at the time it was moved. Four types of ToM questions are posed: first-order or second-order, probing a character about either a true or a false belief (i.e, belief that matches reality or not). ToMi also includes questions probing about reality (or zeroth-order ToM, Sclar et al., 2022) and memory. ToMi has six types of sentences (i.e. six primitives) with set phrasing. These include someone (a) entering or (b) exiting a room; the location of (c) an object or (d) a person; (e) someone moving an object; and (f) someone's opinion about an object (distractors). Primitives are combined into stories with a finite list of possible orderings. Despite the limited types of primitives, correctly answering questions requires high-order levels of reasoning.\nTemplated stories are filled with randomly sampled objects, locations, containers, and rooms from a set list. ToMi implicitly assumes that questions about the story do not depend on these decisions, only on the underlying story template. Yet, in a small-scale human study, we find physical com-1. Oliver entered the front yard. 2. Ethan entered the front yard. 3. Liam entered the kitchen. 4. objectA is in the basket. 5. Ethan exited the front yard. 6. Ethan entered the kitchen. 7. Oliver moved objectA to the containerX. 8. Where does Ethan think objectA is?\nToMi Gold Label: basket Table 1: Interpretation of ambiguities in ToMi can be affected by commonsense. In the above template, the correct label is that Ethan thinks objectA is in the basket, as this is where he last saw it. Setting objectA to hat and containerX to box results in 80% human accuracy. However, setting these to apple and pantry, accuracy drops to 20%. Physical commonsense suggests the pantry is likely in the kitchen, changing the answer to pantry, but regardless of the identity of objectA or containerX, the correct label in ToMi is basket. monsense leads human answers to change, and disagree with ToMi's labels depending on the noun. Table 1 presents an example where the object and container have a large effect on human responses. 3 Resolving Unintentional Ambiguities ToMi's story construction process often leaves object locations ambiguous, which forces humans to (incorrectly) rely on their physical commonsense. For example, the location of the basket in line 4 of Table 1 is ambiguous. This ambiguity is at times resolved at a later step in the story (Arodi and Cheung, 2021), but it is not true for all cases, and these resolutions were not expressly intended by ToMi's original design. This complicates the task beyond theory of mind. For example, in Table 1, the reader must conclude from \"Oliver is in front yard\", \"Oliver moved the objectA (...)\", and \"The objectA is in basket\" that the basket is in the front yard, and hence that Ethan saw it there. This requires 3-hop reasoning, and knowing ahead of time that, in ToMi, characters do not change rooms unless explicitly stated.\nTo solve these unintentional ambiguities and additional 3-hop reasoning requirements, and instead solely measure theory of mind reasoning skills, we automatically add a sentence that disambiguates the location of each container immediately after each primitive (c) or (e) (e.g., adding \"The basket is in the front yard\" as line 5 in Table 1). Finally, as reported in Arodi and Cheung (2021); Sap et al. (2022), ToMi contains some mislabeled secondorder questions, which we also correct.", "publication_ref": ["b20", "b46", "b5", "b33", "b3", "b3", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We experiment with several base LMs, and evaluate each of them both out-of-the-box via zeroshot prompting, and by applying SYMBOLICTOM to ToMi stories to produce answers. We evaluate Macaw-3B (Tafjord and Clark, 2021), GPT3-{Curie,Davinci} (Brown et al., 2020), Flan-T5-{XL,XXL} (Chung et al., 2022), LLaMA-{7B, 13B} (Touvron et al., 2023), GPT3.5 (OpenAI, 2022), and GPT4 (OpenAI, 2023). We use WANLI  for identifying NLI contradictions, and the AllenNLP library (Gardner et al., 2018) for OpenIE. We additionally refine each subject and object in extracted triples to remove any stopwords that may be accidentally included by OpenIE.\nWe first evaluate SYMBOLICTOM's performance as a plug-and-play method for different base LMs on ToMi ( \u00a75.1). We then test whether performance gains are robust to ToMi story structure modifications ( \u00a75.2). Finally, we explore SYMBOL-ICTOM's robustness to linguistic diversity ( \u00a75.3).", "publication_ref": ["b7", "b26", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Supervised Models", "text": "For comparison, we train two supervised models: Textual Time Travel (TTT) (Arodi and Cheung, 2021), and a fine-tuned GPT3-Curie. TTT is a modification of EntNet (Henaff et al., 2017) designed for theory of mind tasks; GPT3-Curie is finetuned on 6000 ToMi examples for one epoch. GPT3-Curie achieves near-perfect performance when finetuned on ToMi (98.5% accuracy when averaging all questions; Table 5). Interestingly, GPT3-Curie achieves a higher accuracy than the theory of mind-motivated TTT (accuracy 92.3%). We explore model robustness in \u00a75.2.", "publication_ref": ["b3", "b16"], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "In-Domain Evaluation", "text": "We evaluate all base LMs comparing their performance out-of-the-box, versus when adding SYM-BOLICTOM. Figure 4 shows results by question type, showing dramatic improvements for all theory of mind questions: +62 points in accuracy for first-order false-belief questions for Flan-T5-XL, +78 points in accuracy for second-order false-belief questions for GPT3.5, among other improvements. In addition, we observe all models maintain nearperfect performance with and without SYMBOL-ICTOM in memory questions. Supervised models  5.\nshow high accuracy for all question types.\nWe only see significant decreases in performance for reality questions in Flan-T5 models. This can be partially attributed to the questions' phrasing: questions are posed as \"Where is the celery really?\". Removing really results in 96% accuracy for Flan-T5-XL. Flan-T5-XXL empirically shows a bias towards providing a room rather than container as an answer when only one container is mentioned, which is often the case for SYMBOLICTOMfiltered stories. Rooms are invalid answers in ToMi. An ablation on the final filter function of Algorithm 1 suggests that keeping more containers in the final story reduces this bias and still yields significant improvements for false-belief questions across all models (see \u00a7C.1). Besides reality questions, Flan-T5-XXL with SYMBOLICTOM achieves results comparable to the supervised TTT.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": ["tab_7"]}, {"heading": "Story Structure Robustness Test Sets", "text": "We create three test sets by modifying ToMi's stories structures without adding new types of actions or linguistic diversity. Table 2: Precision using SYMBOLICTOM on all questions from 100 stories for each of the modified test sets D i . Supervised models were trained on ToMi; all others do not require training. Parenthesis reflect differences between using and not using SYMBOLICTOM: bold reflects higher overall performance, and green reflects the highest net improvements when using SYMBOLICTOM.\nDouble Room False Belief Story (D 1 ) Two false belief substories involving the same two characters p 1 , p 2 are concatenated to yield a longer, more complex story. Each substory has different objects being moved, across different containers. The system is probed using all four combinations of secondorder theory of mind questions involving the two characters and locations. Questions are evenly split between the first and second substory.\nThree Active Characters Story (D 2 ) Three characters p 1 , p 2 , p 3 are in the same room, where an object o 1 and three containers c 1 , c 2 , c 3 are available. The story is as follows: p 2 leaves before p 1 moves o 1 from c 1 to c 2 , but p 3 witnesses the move.\nThen, p 1 leaves the room. Later, p 3 moves the object to container c 3 without any witnesses. The system is probed using all combinations of secondorder theory of mind questions.\nMultiple Object Movements Across Four Containers (D 3 ) Two characters p 1 , p 2 are in a room, with a single object, and four containers c 1 , . . . , c 4 . p 1 moves the object from c 1 to c 2 and right before leaving the room, p 2 enters. p 2 then moves the object to c 3 , and then c 4 . We probe with all first and second-order theory of mind questions.\nResults Supervised models significantly overfit to ToMi's original story structures (Table 2). In contrast, all models had high accuracy when equipped with SYMBOLICTOM, especially larger models, such as GPT3.5, LLaMA-{7B,13B}, among others. D 2 may also be used to test third-order ToM reasoning, asking questions such as \"Where does p 1 think that p 2 thinks that p 1 will search for the o 1 ?\". Third-order ToM is a reasoning depth currently untested by available NLP benchmarks. SYMBOL-ICTOM consistently enhances the performance of off-the-shelf LLMs and outperforms supervised methods in the third-order ToM setting. See details in Appendix C.2. This experiment showcases how extensions of ToMi may be used to test higherorder reasoning. This is the first approach towards testing third-order ToM in LLMs; a benchmark to comprehensively test such order of reasoning exceeds the scope of this paper.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Paraphrasing Robustness Evaluation", "text": "We assess the robustness of all models when utilizing various wordings for each sentence. We reword all templates using GPT3-Davinci, utilizing different choices of objects, rooms, and names, and manually excluded incorrect paraphrases. The resulting dataset-ParaphrasedToMi-exhibits much greater complexity, as these rewordings can express actions in a less straightforward way. All paraphrases are shown in Appendix B.1.\nFigure 5 demonstrates significant performance decreases for supervised models transferring to ParaphrasedToMi. TTT's average accuracy drops 54 points from ToMi, with losses across all question types. Finetuned GPT3 exhibits significant losses in false-belief questions (-40 average accuracy) but is robust for other question types.\nMethods without supervision also suffer significant losses, but SYMBOLICTOM still results in  large improvements for theory of mind questions. Models equipped with SYMBOLICTOM perform significantly better than the supervised TTT model across all theory of mind questions. Paraphrased-ToMi is significantly more difficult for SYMBOLIC-TOM since it triggers more errors in edge removal (due to errors in NLI classification), as well as errors in edge insertion (due to errors in the resulting state's triple extraction). Although computing RESULTINGSTATE by prompting the base LMs was successful with original phrasings (as defined in \u00a73.2.1), we observed differences in robustness when prompting with paraphrases. We found implementing RESULTINGSTATE with GPT3 reliable, and thus we use it for all models. Results using other models are included in \u00a7C.3: false-belief performance is even better for models like LLaMA, GPT3.5, or GPT4.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Related Work", "text": "Existing Approaches Classical reasoning tasks require achieving some goal, e.g., proving a statement, given a set of facts and universally valid rules (e.g., . A common approach is to decompose the target reasoning task into subtasks, for example by using off-the-shelf LMs (Creswell et al., 2023;Kazemi et al., 2022;Nye et al., 2021). We use a similar technique in SYMBOLICTOM, breaking the higher-level reasoning task into graph reasoning subtasks. Nonetheless, these approaches cannot be simply ported to our domain: stories' facts (i.e. the world state) change over time and are not universally accessible to all characters, and commonsense rules and assumptions like object permanence must made explicit. SYMBOLICTOM's design addresses these challenges by maintaining and updating graphs about facts and beliefs as a story progresses.\nIn scenarios where world state changes over time, such as in text-based games, existing approaches maintain and update structured world representations as the world state changes (Ammanabrolu and Riedl, 2021;Adhikari et al., 2020). However, while these approaches could potentially be applied in our scenario to update G, they would not address the problems of multiple-belief representation or knowledge propagation to witnesses' graphs, with some approaches even being explicitly impossible for modeling second-order ToM (Qiu et al., 2022).\nToM beyond NLP Theory of mind is also crucial in multi-agent reinforcement learning (Rabinowitz et al., 2018), including in bidirectional symbolic-communication Sclar et al., 2022), unidirectional natural-language settings (Zhu et al., 2021); and recently, by combining reinforcement learning, planning, and language, to create a human-level Diplomacy player (, FAIR). It has also received increased attention in humancomputer interaction  and explainable AI (Akula et al., 2022).\nPsychologists divide theory of mind into two types of reasoning: affective (emotions, desires) and cognitive (beliefs, knowledge) (Shamay-Tsoory et al., 2010), with the former developing earlier in children (Wellman, 2014). Our work focuses on the latter, but the principle of multiple belief representation could also be applied to affective theory of mind reasoning. Existing work has shown that humans are proficient at second-order or higher false-belief reasoning, also referred to as advanced ToM (Bia\u0142ecka-Pikul et al., 2017), with evidence that we can perform even third-and fourthorder reasoning (Valle et al., 2015;Osterhaus et al., 2016). While, to best of our knowledge, no dataset requires beyond second-order ToM, SYMBOLIC-TOM explicitly models the recursive reasoning that supports queries of any reasoning order.", "publication_ref": ["b10", "b19", "b25", "b2", "b0", "b30", "b31", "b33", "b49", "b1", "b45", "b6", "b41", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions", "text": "Theory of mind is an essential social intelligence ability. Developing agents with theory of mind is requisite for a wide range of applications, including reading comprehension, tutoring, dialogue, personalization, and negotiation. For example, in reading comprehension settings (and broadly for natural language understanding), having a multi-level understanding of texts is crucial for providing meaningful and contextualized answers: stories often rely on theory of mind reasoning to create conflict (e.g., in murder mysteries, drama, and romances, as in the final acts of Romeo and Juliet).\nWe present SYMBOLICTOM, a plug-and-play method to enable theory of mind reasoning in language models via explicit symbolic representations in the form of nested belief states. SYMBOLIC-TOM requires no training or fine-tuning, a key aspect for a domain with scarce supervised data and limited success in learning from massive unlabeled text alone. With experiments on reading comprehension tasks, our approach demonstrates dramatic improvement in the accuracy of base language models, especially for false-belief scenarios.\nWe also show that, in contrast to supervised methods, SYMBOLICTOM is highly robust to story perturbations and out-of-domain inputs where supervised methods suffer significant degradations (as in, e.g., Yu et al., 2022). 5 Our results show the promise of augmenting neural language models with symbolic knowledge for improving their social reasoning skills. We leave to future work to investigate similar approaches for other types of social intelligence; as well as develop new datasets that cover a more diverse set of interactions.", "publication_ref": ["b30"], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "SYMBOLICTOM assumes stories are written chronologically, which may not hold for some human-written stories. This may be alleviated using time-stamping models like Faghihi and Kordjamshidi (2021). Furthermore, since we use off-theshelf models (WANLI  and Ope-nIE (Stanovsky et al., 2018)) to create and update the graphs, the presented approach may propagate errors as revealed in the linguistic diversity experiments. However, these issues can be largely alle-viated by using more sophisticated models, even the LLMs like GPT3 themselves. We do not experiment with them due to budgetary restrictions.\nCurrently, all NLP datasets available for theory of mind reasoning describe Sally-Anne tests. In these datasets, the concept of large distances is absent, meaning that anyone specified to be in a location is assumed to be a witness of the actions that occur there. This assumption can be violated in realistic settings. For example, \"Anne is in the USA\" does not imply she is a witness to every action happening in the USA. In future work, this approach can be improved by refining the witnesses detection algorithm to incorporate physical commonsense reasoning. We could also refine the witness detection algorithm by sampling paths between the inserted edge and each node referring to a person, to query an LM directly on that substory by asking if the person witnessed the action. To be able to test both of these ideas, we would need to obtain new theory of mind datasets with significantly more types of interactions and physical commonsense in the stories.", "publication_ref": ["b11", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "Theory of mind research at its core deals with reasoning about the mental states of others. In this work, we focus on reading comprehension, a task which can similarly be exposed to ethical concerns: for example, when a model makes erroneous predictions about the mental states of characters in the description, when it is misused to reason about private situations, and when it makes predictions which reinforce social biases. This issue can be exacerbated if the characters are actual people. In this work, however, we experiment with simple, prototypical character references from a public dataset, and not with actual people. This decision is intentional. Furthermore, we focus on reasoning about physical objects and observers' knowledge about their location in space, which is less prone to ethical concerns. This data can nonetheless lead to biased decisions, such as imbalanced decisions correlated with social attributes like gender (often correlated with names). Future work in this area may include scenarios with more realistic human-agent interaction, such as dialogue tasks, where parties involved may not have the same incentive structure. These scenarios will need to be handled with special care as they could lead to agents learning to deceive humans by exploiting a predicted (lack of) knowledge.\nThe state-of-the-art in machine theory of mind is still far from these capabilities, but we believe it is important to consider these risks when designing experiments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Additional Details on SYMBOLICTOM", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Detailed Description of Information", "text": "Contained in Global Context G\nIn the main paper, we define G as a graph containing the true world state (as opposed to beliefs about the current world state). This means that G will represent where people and objects are truly located, regardless of beliefs. G will in general contain only the observable true world state. Thus, information passed verbally would not be stored in the global context (e.g. someone speaking in a room is not observable after they finished talking), and would instead be stored in the local contexts of the people that heard the speech. Since verbal interactions are not tested by available datasets, this distinction is not relevant in ToMi.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Prompts for Resulting State Extraction", "text": "For GPT3-Curie we 2-shot prompt with the following prompt (both for original and linguistic diversity experiments):\nJohn quit his job. The resulting state after this action is that John no longer has a job.\\n\\nJohn signed a contract. The resulting state after this action is that the contract is signed.\\n\\n<sentence>.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The resulting state after this action is that", "text": "We find that GPT3-Davinci, Flan-T5-XL, GPT3.5, and GPT4 are able to zero-shot answer to this subtask just by describing the instruction, but smaller models benefit from few-shot. We were unable to query Macaw for this task, so we instead rely on GPT3-Curie, a model of comparable size. Zero-shot instruction is as follows:\n<sentence>. What is the resulting state after this action? Do not add new information. The resulting state after this action is that\nWe observe that GPT3 is significantly more robust to paraphrases than Flan-T5: Flan-T5 models are poor at detecting the resulting state for florid paraphrases, although the original phrasings are a straightforward task for Flan-T5.\nLarger models like GPT3.5 and GPT4 are able to perform the task well zero-shot, similarly to GPT3; LLaMA models require fewer demonstrations than Flan-T5. We ran all main experiments implementing Resulting State Extraction with GPT3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.3 Solving PROCESSQUESTION using GPT3", "text": "Our explorations suggest that GPT3 (Curie and GPT3-Davinci text-davinci-002-the version used in all our experiments) can successfully extract entities and rephrase the question. See Figure 6 for an example prompt.     Performance is shown for each question type, dots in upper triangle imply performance improvements. Full results table may be found in Table 6. question type. Regardless of the final filter application, GPT4+SYMBOLICTOM significantly outperforms out-of-the-box GPT4 in all four ToM question types and maintains performance on Reality and Memory questions. For Flan-T5-XL, Flan-T5-XL+SYMBOLICTOM outperforms Flan-T5-XL significantly in all four ToM question types (e.g. +76 and +36 points in accuracy for first and secondorder false belief questions), and shows slight declines for Reality and Memory questions-in line with findings on the full algorithm, but with less stark declines, suggesting that having more entities may help reduce bias towards answering rooms instead of containers. See Table 6 for the full table of accuracy differences.\nRegardless of the final filtering application, SYMBOLICTOM shows improvements in theory of mind questions for all models. We only find the filter application to be relevant to beat the base model in theory of mind questions for Flan-T5-XXL.", "publication_ref": [], "figure_ref": ["fig_6"], "table_ref": ["tab_8", "tab_8"]}, {"heading": "C.2 Third-Order Theory of Mind Evaluation", "text": "We ask all third-order theory of mind questions for each D 2 story, such as \"Where does p 1 think that p 2 thinks that p 1 will search for the o 1 ?\". Questions involving p 2 will have a final answer c 1 , since everyone saw p 2 leaving. We ask all six possible 76 Table 4: Precision using SYMBOLICTOM on all questions from 100 stories for each of the modified test sets D i . Supervised models were trained on ToMi; all others do not require training. Parenthesis reflect differences between using and not using SYMBOLICTOM: bold reflects higher overall performance, and green reflects the highest net improvements when using SYMBOLICTOM.\nquestions involving p 2 . We also ask the two thirdorder theory of mind questions that do not involve p 2 nor repeats the same person twice consecutively (\"Where does p 1 think that p 3 thinks that p 1 will search for the o 1 ?\" and \"Where does p 3 think that p 1 thinks that p 3 will search for the o 1 ?\"), totaling eight questions per D 2 story. Table 4 shows results for all models using k = 2 representations (same depth as in the main paper). Using SYMBOLICTOM significantly outperforms the supervised baselines and yields dramatic improvements with respect to using the LLMs offthe-shelf. We hypothesize that although the task theoretically requires k = 3, the second-order theory of mind representation already helps models avoid attending to parts of the story that are inaccessible to relevant characters.    D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? Annotators are only used to contribute to a small comment and are not used in evaluating our method.\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? Left blank.\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? Left blank.\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nLeft blank.\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? Not necessary for the small-scale experiment ran.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We thank Lucille Njoo and Tianxing He for the valuable discussions, and Akshatha Arodi for the support in running the Textual Time Travel code base. S.K. gratefully acknowledges support from Google Ph.D. Fellowship. We also thank OpenAI for providing academic access to their language model API. This material is based upon work partly funded by the DARPA CMO under Contract No. HR001120C0124, by DARPA MCS program through NIWC Pacific (N66001-19-2-4031), by NSF DMS-2134012, by NSF CAREER Grant No. IIS2142739, and an Alfred P. Sloan Foundation Fellowship. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily state or reflect those of the United States Government or any agency thereof.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "person1 exited the room1. person3 entered the room1. person3 moved the object1 to the container1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.2.4 Four Containers with Multiple Movements", "text": "person1 is in the room1. The object1 is in the container1. The container1 is in the room1. person1 moved the object1 to the container2. The container2 is in the room1. person2 entered the room1. person1 exited the room1. person2 moved the object1 to the container3. The container3 is in the room1. person2 moved the object1 to the container4. The container4 is in the room1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Expanded Results", "text": "Experimental Note: All zero-shot GPT3 (text-curie-001 and text-davinci-002) experiments were performed between November 2022 and January 2023. GPT3.5 (gpt-3.5-turbo) and GPT4 (gpt-4) were added in May 2023.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 Ablating FILTERBASEDONQUESTION from SYMBOLICTOM", "text": "FILTERBASEDONQUESTION definition This function filters the story S \u2032 to obtain an even shorter subset of the original story S \u2032\u2032 by only keeping edges where at least one of the endpoints represents an entity mentioned in the question.\nLast step of Algorithm 1 is applying FIL-TERBASEDONQUESTION, which yields an even shorter story to feed language models. We evaluate the effect this final filter has on the final performances reported by SYMBOLICTOM.\nFILTERBASEDONQUESTION has a positive effect on Macaw-3B, GPT3, Flan-T5-XXL, and LLaMA-7B (+7, +3.5, +12.8, and +15 points in average accuracy gain across all question types), and a mild negative one on Flan-T5-XL, and GPT4 (-5.3, and -4 points of accuracy on average). See Table 7 for all differences between executing SYM-BOLICTOM using this final filtering or not.   8: Results for ParaphrasedToMi when using the same model for implementing the RESULTINGSTATE function as in the final question-answering task (except using Davinci for Macaw, who did not show reliable enough few shot-prompting). Dots in upper triangle imply performance with SYMBOLICTOM is higher than using the base model out-of-the-box. Horizontal lines reflect supervised models' performance (higher is better). RESULTINGSTATE(s) refers to the state of the world after s has been performed. For example, if \"Oliver moved the apple to the box\", then the resulting state is that \"The apple is in the box\". If \"Oliver exited the bedroom\", the resulting state would be that \"Oliver is no longer in the bedroom\". These are the relationships that we may insert in a context graph-actions are instantaneous and do not reflect an observable state.\nIn this section, we explore using the same LLM for implementing RESULTINGSTATE as well as the final inference. In the main text, we use Davinci for all non-GPT3-based models.\nWe find GPT3 to be among the most reliable to answer the resulting state of a given action in a zero-shot (Davinci) or two-shot (Curie) manner. Similarly, GPT3.5 and GPT4 perform well zeroshot: for experiments, we use GPT3.5 zero-shot and GPT4 two-shot to improve the resulting phrasing stability.\nAdditional exploration shows that although Flan-T5 models perform worse zero-shot than GPT models, they are capable of performing this task with more careful prompting. Figure 8 shows the results after nine-shot prompting Flan-T5-XL and elevenshot prompting Flan-T5-XXL. Our explorations show that LLaMA models require fewer demonstrations than the Flan-T5 models to compute the resulting state: we observe highly reliable results when using six-shot prompting for LLaMA-7B, and seven-shot prompting for LLaMA-13B. Accuracy using LLaMA was even higher than when using GPT3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.4 Detailed Result Tables", "text": "All results in the appendix show accuracy as a ratio (between 0 and 1). For simplicity of reading, in the main text, they are referred to in percentages (values 0 to 100, higher is better). Figures 5, 6, and 7 show performances when applying the final filtering function, when not applying it, and the difference in performance between the two, respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ACL 2023 Responsible NLP Checklist", "text": "A For every submission:", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A1. Did you describe the limitations of your work?", "text": "Section \"Limitations\" after Conclusions but before the references, as required by ACL 2023 guidelines.\nA2. Did you discuss any potential risks of your work?\nSection \"Ethics Statement\" after Conclusions but before the references, as required by ACL 2023 guidelines.\nA3. Do the abstract and introduction summarize the paper's main claims?\nAbstract + Section 1 A4. Have you used AI writing assistants when working on this paper? GPT3-Davinci, for brainstorming paraphrases of sentences in Section 1 and Section 2. We later edited these paraphrases, but GPT3-Davinci gave interesting suggestions. B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nArtifact is an NLP research dataset.\nB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Section 4 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? The dataset is artificially generated. B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Section 4 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Section 5", "text": "The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Learning dynamic belief graphs to generalize on text-based games", "journal": "Advances in Neural Information Processing Systems", "year": "2020", "authors": "Ashutosh Adhikari; Xingdi Yuan; Marc-Alexandre C\u00f4t\u00e9; Mikul\u00e1\u0161 Zelinka; Marc-Antoine Rondeau; Romain Laroche; Pascal Poupart; Jian Tang; Adam Trischler; Will Hamilton"}, {"ref_id": "b1", "title": "Cx-tom: Counterfactual explanations with theory-of-mind for enhancing human trust in image recognition models. iScience", "journal": "", "year": "2022", "authors": "R Arjun; Keze Akula; Changsong Wang; Sari Liu; Hongjing Saba-Sadiya; Sinisa Lu; Joyce Todorovic; Song-Chun Chai;  Zhu"}, {"ref_id": "b2", "title": "Learning knowledge graph-based world models of textual environments", "journal": "", "year": "2021", "authors": "Prithviraj Ammanabrolu; Mark Riedl"}, {"ref_id": "b3", "title": "Textual time travel: A temporally informed approach to theory of mind", "journal": "", "year": "2021", "authors": "Akshatha Arodi; Jackie Chi Kit Cheung"}, {"ref_id": "b4", "title": "Mindcraft: Theory of mind modeling for situated dialogue in collaborative tasks", "journal": "", "year": "2021", "authors": "Cristian-Paul Bara; Ch-Wang Sky; Joyce Chai"}, {"ref_id": "b5", "title": "Does the autistic child have a \"theory of mind", "journal": "Cognition", "year": "1985", "authors": "Simon Baron-Cohen; Alan M Leslie; Uta Frith"}, {"ref_id": "b6", "title": "Advanced theory of mind in adolescence: Do age, gender and friendship style play a role", "journal": "Journal of Adolescence", "year": "2017", "authors": "Marta Bia\u0142ecka-Pikul; Anna Ko\u0142odziejczyk; Sandra Bosacki"}, {"ref_id": "b7", "title": "Language models are few-shot learners", "journal": "", "year": "2020", "authors": "Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell"}, {"ref_id": "b8", "title": "Inference or enaction? the impact of genre on the narrative processing of other minds", "journal": "PloS one", "year": "2014", "authors": "James Carney; Rafael Wlodarski; Robin Dunbar"}, {"ref_id": "b9", "title": "Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models", "journal": "", "year": "", "authors": " Hyung Won; Le Chung; Shayne Hou; Barret Longpre; Yi Zoph; William Tay; Eric Fedus; Xuezhi Li;  Wang"}, {"ref_id": "b10", "title": "Selection-inference: Exploiting large language models for interpretable logical reasoning", "journal": "", "year": "2023", "authors": "Antonia Creswell; Murray Shanahan; Irina Higgins"}, {"ref_id": "b11", "title": "Time-stamped language model: Teaching language models to understand the flow of events", "journal": "", "year": "2021", "authors": "Parisa Hossein Rajaby Faghihi;  Kordjamshidi"}, {"ref_id": "b12", "title": "Hugh Zhang, and Markus Zijlstra. 2022. Human-level play in the game of <i>diplomacy</i> by combining language models with strategic reasoning", "journal": "Science", "year": "", "authors": "Sasha Miller; Adithya Mitts; Stephen Renduchintala; Dirk Roller; Weiyan Rowe; Joe Shi; Alexander Spisak; David Wei;  Wu"}, {"ref_id": "b13", "title": "Development and neurophysiology of mentalizing", "journal": "", "year": "1431", "authors": "C D Frith; D M Wolpert; Uta Frith; Christopher D Frith"}, {"ref_id": "b14", "title": "AllenNLP: A deep semantic natural language processing platform", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Matt Gardner; Joel Grus; Mark Neumann; Oyvind Tafjord; Pradeep Dasigi; Nelson F Liu; Matthew Peters; Michael Schmitz; Luke Zettlemoyer"}, {"ref_id": "b15", "title": "How can memory-augmented neural networks pass a false-belief task", "journal": "Cognitive Science", "year": "2017", "authors": "Erin Grant; Aida Nematzadeh; Thomas L Griffiths"}, {"ref_id": "b16", "title": "Tracking the world state with recurrent entity networks", "journal": "", "year": "2017", "authors": "Mikael Henaff; Jason Weston; Arthur Szlam; Antoine Bordes; Yann Lecun"}, {"ref_id": "b17", "title": "do you follow me?\": A survey of recent approaches in dialogue state tracking", "journal": "", "year": "2022", "authors": "L\u00e9o Jacqmin; Lina M Rojas Barahona; Benoit Favre"}, {"ref_id": "b18", "title": "A systematic survey of text worlds as embodied natural language environments", "journal": "", "year": "2022", "authors": "Peter Jansen"}, {"ref_id": "b19", "title": "Lambada: Backward chaining for automated reasoning in natural language", "journal": "", "year": "2022", "authors": "Najoung Seyed Mehran Kazemi; Deepti Kim; Xin Bhatia; Deepak Xu;  Ramachandran"}, {"ref_id": "b20", "title": "Revisiting the evaluation of theory of mind through question answering", "journal": "", "year": "2019", "authors": "Matthew Le; Y-Lan Boureau; Maximilian Nickel"}, {"ref_id": "b21", "title": "Core mechanisms in 'theory of mind'", "journal": "Trends in cognitive sciences", "year": "2004", "authors": "Alan M Leslie; Ori Friedman; Tim P German"}, {"ref_id": "b22", "title": "Theory of mind and literature", "journal": "Purdue University Press", "year": "2010", "authors": "Paula Leverage; Howard Mancing; Richard Schweickert"}, {"ref_id": "b23", "title": "WANLI: Worker and ai collaboration for natural language inference dataset creation", "journal": "", "year": "2022", "authors": "Alisa Liu; Swabha Swayamdipta; Noah A Smith; Yejin Choi"}, {"ref_id": "b24", "title": "Evaluating theory of mind in question answering", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Aida Nematzadeh; Kaylee Burns; Erin Grant; Alison Gopnik; Tom Griffiths"}, {"ref_id": "b25", "title": "Improving coherence and consistency in neural sequence models with dualsystem, neuro-symbolic reasoning", "journal": "Advances in Neural Information Processing Systems", "year": "2021", "authors": "Maxwell Nye; Michael Tessler; Josh Tenenbaum; M Brenden;  Lake"}, {"ref_id": "b26", "title": "ChatGPT: Optimizing language models for dialogue", "journal": "", "year": "2022", "authors": " Openai"}, {"ref_id": "b27", "title": "OpenAI. 2023. GPT-4 technical report", "journal": "", "year": "", "authors": ""}, {"ref_id": "b28", "title": "Scaling of advanced theory-of-mind tasks", "journal": "Child development", "year": "2016", "authors": "Christopher Osterhaus; Susanne Koerber; Beate Sodian"}, {"ref_id": "b29", "title": "Does the chimpanzee have a theory of mind?", "journal": "Behavioral and brain sciences", "year": "1978", "authors": "David Premack; Guy Woodruff"}, {"ref_id": "b30", "title": "Towards socially intelligent agents with mental state transition and human value", "journal": "", "year": "2022", "authors": "Liang Qiu; Yizhou Zhao; Yuan Liang; Pan Lu; Weiyan Shi; Zhou Yu; Song-Chun Zhu"}, {"ref_id": "b31", "title": "Machine theory of mind", "journal": "PMLR", "year": "2018", "authors": "Neil Rabinowitz; Frank Perbet; Francis Song; Chiyuan Zhang; Ali Eslami; Matthew Botvinick"}, {"ref_id": "b32", "title": "Neural theory-of-mind? on the limits of social intelligence in large lms", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Maarten Sap; Ronan Lebras; Daniel Fried; Yejin Choi"}, {"ref_id": "b33", "title": "Symmetric machine theory of mind", "journal": "PMLR", "year": "2022", "authors": "Melanie Sclar; Graham Neubig; Yonatan Bisk"}, {"ref_id": "b34", "title": "The role of the orbitofrontal cortex in affective theory of mind deficits in criminal offenders with psychopathic tendencies", "journal": "Cortex", "year": "2010", "authors": "Hagai Simone G Shamay-Tsoory;  Harari"}, {"ref_id": "b35", "title": "Maarten Sap, and Vered Shwartz. 2023. Clever hans or neural theory of mind? stress testing social reasoning in large language models", "journal": "", "year": "", "authors": "Natalie Shapira; Mosh Levy; Xuhui Seyed Hossein Alavi; Yejin Zhou; Yoav Choi;  Goldberg"}, {"ref_id": "b36", "title": "Supervised open information extraction", "journal": "Long Papers", "year": "2018", "authors": "Gabriel Stanovsky; Julian Michael; Luke Zettlemoyer; Ido Dagan"}, {"ref_id": "b37", "title": "General-purpose question-answering with macaw", "journal": "", "year": "2021", "authors": "Oyvind Tafjord; Peter Clark"}, {"ref_id": "b38", "title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Oyvind Tafjord; Bhavana Dalvi; Peter Clark"}, {"ref_id": "b39", "title": "Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models", "journal": "", "year": "", "authors": "Hugo Touvron; Thibaut Lavril; Gautier Izacard; Xavier Martinet; Marie-Anne Lachaux; Timoth\u00e9e Lacroix; Naman Baptiste Rozi\u00e8re; Eric Goyal;  Hambro"}, {"ref_id": "b40", "title": "Large language models fail on trivial alterations to theory-of-mind tasks", "journal": "", "year": "2023", "authors": "Tomer Ullman"}, {"ref_id": "b41", "title": "Theory of mind development in adolescence and early adulthood: The growing complexity of recursive thinking ability. Europe's journal of psychology", "journal": "", "year": "2015", "authors": "Annalisa Valle; Davide Massaro; Ilaria Castelli; Antonella Marchetti"}, {"ref_id": "b42", "title": "When narrative takes over: The representation of embedded mindstates in shakespeare's othello. Language and Literature", "journal": "", "year": "2015", "authors": "Ineke Max J Van Duijn; Arie Sluiter;  Verhagen"}, {"ref_id": "b43", "title": "Towards mutual theory of mind in human-ai interaction: How language reflects what students perceive about a virtual teaching assistant", "journal": "", "year": "2021", "authors": "Qiaosi Wang; Koustuv Saha; Eric Gregori; David Joyner; Ashok Goel"}, {"ref_id": "b44", "title": "Tom2c: Target-oriented multi-agent communication and cooperation with theory of mind", "journal": "", "year": "2022", "authors": "Yuanfei Wang; Jing Xu; Yizhou Wang"}, {"ref_id": "b45", "title": "Making minds: How theory of mind develops", "journal": "Oxford University Press", "year": "2014", "authors": " Henry M Wellman"}, {"ref_id": "b46", "title": "Beliefs about beliefs: Representation and constraining function of wrong beliefs in young children's understanding of deception", "journal": "Cognition", "year": "1983", "authors": "Heinz Wimmer; Josef Perner"}, {"ref_id": "b47", "title": "Mona Diab, and Asli Celikyilmaz. 2022. Alert: Adapting language models to reasoning tasks", "journal": "", "year": "", "authors": "Ping Yu; Tianlu Wang; Olga Golovneva; Badr Alkhamissy; Gargi Ghosh"}, {"ref_id": "b48", "title": "An ai dungeon master's guide: Learning to converse and guide with intents and theory-of-mind in dungeons and dragons", "journal": "", "year": "2022", "authors": "Pei Zhou; Andrew Zhu; Jennifer Hu; Jay Pujara; Xiang Ren; Chris Callison-Burch; Yejin Choi; Prithviraj Ammanabrolu"}, {"ref_id": "b49", "title": "Few-shot language coordination by modeling theory of mind", "journal": "PMLR", "year": "2021", "authors": "Hao Zhu; Graham Neubig; Yonatan Bisk"}, {"ref_id": "b50", "title": "Why we read fiction: Theory of mind and the novel", "journal": "Ohio State University Press", "year": "2006", "authors": "Lisa Zunshine"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Alice and Bob are in a room with a basket and a box. Alice puts some celery in the basket and leaves the room. Bob then moves the celery into the box. Charles immediately enters the room. Charles puts the celery in a chest.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: High-level depiction of the belief update procedure for m = 2. B p1,...,p k denotes a graph, and the graph updating procedure is detailed in the main text.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure4: Accuracy for each ToMi question type and base model (higher is better). Dots in the upper triangle have higher performance with SYMBOLICTOM than the base model out-of-the-box. Horizontal lines give supervised models' performance. Full results in Table5.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure5: Results for ParaphrasedToMi when prompting GPT3 as implementation of RESULTINGSTATE (Davinci for all except for Curie). Dots in the upper triangle imply performance with SYMBOLICTOM is higher than using the base model out-of-the-box. Horizontal lines reflect supervised models' performance (higher is better).", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 :6Figure6: GPT3 shows one-shot generalization abilities from first-order to second-order questions.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 7 :7Figure7: Precision using SYMBOLICTOM on ToMi, for several language models without the final filter function. Performance is shown for each question type, dots in upper triangle imply performance improvements. Full results table may be found in Table6.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Algorithm 1 SYMBOLICTOM B \u2190 BELIEFTRACKINGSTRUCTURE(sentences) p1,. . ., p k , question \u2032 \u2190 PROCESSQUESTION(question) S \u2032 \u2190 SENTENCESREPRESENTEDBYGRAPH(Bp 1 ,...,p k ) S \u2032\u2032 \u2190 FILTERBASEDONQUESTION(S \u2032 , question) return S \u2032\u2032 , question \u2032", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ": Number of paraphrases per original sentence template. Paraphrases were obtained from prompting GPT3-Davinci (text-davinci-002)."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": ".50] 0.79 [0.33] 0.86 [0.34] 0.84 [0.17] 0.10 [0.14] 0.95 [0.91] GPT3-Curie 0.77 [0.42] 0.82 [0.35] 0.73 [0.26] 0.89 [0.26] 0.61 [0.69] 0.99 [0.86] GPT3-Davinci 0.96 [0.75] 0.96 [0.25] 0.93 [0.14] 0.90 [0.26] 0.77 [0.86] 0.98 [0.98]", "figure_data": "1st TB1st FB2nd TB2nd FBRealityMemoryMacaw-3B 0.86 [0Flan-T5-XL 0.98 [0.97] 0.80 [0.18] 0.98 [0.68] 0.78 [0.56] 0.73 [0.97] 1.00 [1.00]Flan-T5-XXL0.98 [0.84] 0.95 [0.67] 1.00 [0.76] 0.90 [0.39] 0.13 [0.63] 1.00 [1.00]LLaMA-7B0.82 [0.32] 0.95 [0.66] 0.66 [0.31] 0.72 [0.41] 0.87 [0.37] 1.00 [0.83]LLaMA-13B0.82 [0.60] 0.86 [0.67] 0.70 [0.53] 0.62 [0.77] 0.87 [0.48] 1.00 [0.90]GPT3.50.97 [0.76] 0.95 [0.66] 0.99 [0.02] 0.87 [0.09] 0.98 [1.00] 0.99 [0.80]GPT40.98 [0.83] 0.94 [0.73] 0.98 [0.36] 0.89 [0.64] 0.94 [1.00] 1.00 [1.00]Finetuned GPT30.950.990.971.001.001.00TTT-learned0.841.000.820.881.001.00"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Performance per model and question using SYMBOLICTOM, with out-of-the-box performance shown in brackets (100 samples per question type). Bottom rows represent supervised baselines.", "figure_data": "1st TB1st FB2nd TB2nd FBRealityMemoryMacaw-3B0.54 [0.50] 0.86 [0.33] 0.56 [0.34] 0.88 [0.17] 0.16 [0.14] 0.98 [0.91]GPT3-Curie0.66 [0.42] 0.79 [0.35] 0.69 [0.26] 0.87 [0.26] 0.65 [0.69] 0.94 [0.86]GPT3-Davinci 0.94 [0.75] 0.88 [0.25] 0.90 [0.14] 0.83 [0.26] 0.83 [0.86] 0.90 [0.98]Flan-T5-XL1.00 [0.97] 0.94 [0.18] 1.00 [0.68] 0.92 [0.56] 0.88 [0.97] 0.85 [1.00]Flan-T5-XXL0.74 [0.84] 0.69 [0.67] 0.68 [0.76] 0.64 [0.39] 0.44 [0.63] 1.00 [1.00]LLaMA-7B0.48 [0.32] 0.95 [0.66] 0.38 [0.31] 0.98 [0.41] 0.48 [0.37] 0.84 [0.83]LLaMA-13B0.75 [0.60] 0.96 [0.67] 0.70 [0.53] 0.96 [0.77] 0.57 [0.48] 0.89 [0.90]GPT3.50.99 [0.76] 1.00 [0.66] 1.00 [0.02] 0.98 [0.09] 0.98 [1.00] 0.90 [0.80]GPT40.99 [0.83] 1.00 [0.73] 1.00 [0.36] 0.98 [0.64] 1.00 [1.00] 1.00 [1.00]Finetuned GPT30.950.990.971.001.001.00TTT-learned0.841.000.820.881.001.00"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Performance per model and question using SYMBOLICTOM without FILTERBASEDONQUESTION, with out-of-the-box performance shown in brackets (100 samples per question type). Bottom rows represent supervised baselines.", "figure_data": "1st TB 1st FB 2nd TB 2nd FB Reality MemoryMacaw-3B0.32-0.070.30-0.04-0.06-0.03GPT3-Curie0.110.030.040.02-0.040.05GPT3-Davinci0.020.080.030.07-0.060.08Flan-T5-XL-0.02-0.14-0.02-0.14-0.150.15Flan-T5-XXL0.240.260.320.26-0.310.00LLaMA-7B0.340.000.28-0.260.390.16LLaMA-13B0.07-0.100.00-0.340.300.11GPT3.5-0.02-0.05-0.01-0.110.000.09GPT4-0.01-0.06-0.02-0.09-0.060.00"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Differences between accuracy of base models using SYMBOLICTOM with the final FILTERBASEDONQUES-TION filter, and without using the final filter. As shown in Table5 and 6, both versions are still far superior to not using SYMBOLICTOM.C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? Model does not require training, it is inference-time only.C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? Model does not require training, it is inference-time only.C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? Section 5C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,", "figure_data": "C Did you run computational experiments?Section 5etc.)?Section 5D Did you use human annotators (e.g., crowdworkers) or research with human participants?Section 4"}], "formulas": [{"formula_id": "formula_0", "formula_text": "B p 1 ...p k", "formula_coordinates": [4.0, 235.42, 527.26, 31.96, 12.72]}, {"formula_id": "formula_1", "formula_text": "\u2718 BAlice,Alice \u2718 BAlice,Bob \u2718 BAlice,Charles = \u2205 \u2718 BBob,Alice \u2713 BBob,Bob = BBob \u2713 BBob,Charles \u2718 BCharles,Alice \u2713 BCharles,Bob \u2713 BCharles,Charles = BCharles = \u2205 = what X thinks that Y thinks is the current world state BX,Y =", "formula_coordinates": [4.0, 313.23, 218.35, 204.05, 49.02]}, {"formula_id": "formula_2", "formula_text": "function BELIEFTRACKINGSTRUCTURE(sentences) for s \u2208 sentences do G, W \u2190 GLOBALCONTEXTUPDATE(G, s) for all [p1, . . . , pm] \u2208 W m do Bp 1 ...pm \u2190 LOCALCONTEXTUPDATE(Bp 1 .", "formula_coordinates": [4.0, 315.11, 361.5, 194.9, 57.05]}, {"formula_id": "formula_3", "formula_text": "function GLOBALCONTEXTUPDATE(G, s) E \u2190 DETECTCONTRADICTINGEDGES(G, s) G \u2190 G \u222a TRIPLES(RESULTINGSTATE(s)) W \u2190 FINDWITNESSES(G) G \u2190 G \\ E return G, W end function function LOCALCONTEXTUPDATE(C, G, s) E \u2190 DETECTCONTRADICTINGEDGES(G, s) C \u2190 C \u222a TRIPLES(RESULTINGSTATE(s)) C \u2190 PROPAGATEKNOWLEDGE(G, C, s) C \u2190 C \\ E return C end function", "formula_coordinates": [5.0, 79.82, 347.25, 172.28, 151.15]}], "doi": "10.1016/j.isci.2021.103581"}