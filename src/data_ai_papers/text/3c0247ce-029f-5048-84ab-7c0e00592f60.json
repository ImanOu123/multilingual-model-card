{"title": "MASTERING THE GAME OF NO-PRESS DIPLOMACY VIA HUMAN-REGULARIZED REINFORCEMENT LEARNING AND PLANNING", "authors": "Anton Bakhtin; Meta Ai; David J Wu; Adam Lerer; Jonathan Gray; Meta Ai Athul; Paul Jacob; Gabriele Mit;  Farina; Alexander H Miller; Noam Brown", "pub_date": "2022-10-11", "abstract": "No-press Diplomacy is a complex strategy game involving both cooperation and competition that has served as a benchmark for multi-agent AI research. While self-play reinforcement learning has resulted in numerous successes in purely adversarial games like chess, Go, and poker, self-play alone is insufficient for achieving optimal performance in domains involving cooperation with humans. We address this shortcoming by first introducing a planning algorithm we call DiL-piKL that regularizes a reward-maximizing policy toward a human imitationlearned policy. We prove that this is a no-regret learning algorithm under a modified utility function. We then show that DiL-piKL can be extended into a self-play reinforcement learning algorithm we call RL-DiL-piKL that provides a model of human play while simultaneously training an agent that responds well to this human model. We used RL-DiL-piKL to train an agent we name Diplodocus. In a 200-game no-press Diplomacy tournament involving 62 human participants spanning skill levels from beginner to expert, two Diplodocus agents both achieved a higher average score than all other participants who played more than two games, and ranked first and third according to an Elo ratings model. * Equal first author contribution. 1 Dota 2 is a two-team zero-sum game, but the presence of full information sharing between teammates makes it equivalent to 2p0s. Beyond 2p0s settings, self-play algorithms have also proven successful in highly adversarial games like six-player poker Brown & Sandholm (2019).", "sections": [{"heading": "INTRODUCTION", "text": "In two-player zero-sum (2p0s) settings, principled self-play algorithms converge to a minimax equilibrium, which in a balanced game ensures that a player will not lose in expectation regardless of the opponent's strategy (Neumann, 1928). This fact has allowed self-play, even without human data, to achieve remarkable success in 2p0s games like chess (Silver et al., 2018), Go (Silver et al., 2017), poker (Bowling et al., 2015;, and Dota 2 (Berner et al., 2019). 1 In principle, any finite 2p0s game can be solved via self-play given sufficient compute and memory. However, in games involving cooperation, self-play alone no longer guarantees good performance when playing with humans, even with infinite compute and memory. This is because in complex domains there may be arbitrarily many conventions and expectations for how to cooperate, of which humans may use only a small subset (Lerer & Peysakhovich, 2019). The clearest example of this is language. A self-play agent trained from scratch without human data in a cooperative game involving free-form communication channels would almost certainly not converge to using English as the medium of communication. Obviously, such an agent would perform poorly when paired with a human English speaker. Indeed, prior work has shown that na\u00efve extensions of self-play from scratch without human data perform poorly when playing with humans or human-like agents even in dialogue-free domains that involve cooperation rather than just competition, such as the benchmark games no-press Diplomacy (Bakhtin et al., 2021) and Hanabi (Siu et al., 2021;.", "publication_ref": ["b25", "b29", "b4", "b2", "b20", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "BACKGROUND AND PRIOR WORK", "text": "Diplomacy is a benchmark 7-player mixed cooperative/competitive game featuring simultaneous moves and a heavy emphasis on negotiation and coordination. In the no-press variant of the game, there is no cheap talk communication. Instead, players only implicitly communicate through moves.\nIn the game, seven players compete for majority control of 34 \"supply centers\" (SCs) on a map. On each turn, players simultaneously choose actions consisting of an order for each of their units to hold, move, support or convoy another unit. If no player controls a majority of SCs and all remaining players agree to a draw or a turn limit is reached then the game ends in a draw. In this case, we use a common scoring system in which the score of player i is C 2 i / i C 2 i , where C i is the number of SCs player i owns. A more detailed description of the rules is provided in Appendix B.\nMost recent successes in no-press Diplomacy use deep learning to imitate human behavior given a corpus of human games. The first Diplomacy agent to leverage deep imitation learning was Paquette et al. (2019). Subsequent work on no-press Diplomacy have mostly relied on a similar architecture with some modeling improvements (Gray et al., 2020;Anthony et al., 2020;Bakhtin et al., 2021). Gray et al. (2020) proposed an agent that plays an improved policy via one-ply search. It uses policy and value functions trained on human data to to conduct search using regret minimization.\nSeveral works explored applying self-play to compute improved policies. Paquette et al. (2019) applied an actor-critic approach and found that while the agent plays stronger in populations of other self-play agents, it plays worse against a population of human-imitation agents. Anthony et al. (2020) used a self-play approach based on a modification of fictitious play in order to reduce drift from human conventions. The resulting policy is stronger than pure imitation learning in both 1vs6 and 6vs1 settings but weaker than agents that use search. Most recently, Bakhtin et al. (2021) combined one-ply search based on equilibrium computation with value iteration to produce an agent called DORA. DORA achieved superhuman performance in a 2p0s version of Diplomacy without human data, but in the full 7-player game plays poorly with agents other than itself. Jacob et al. (2022) showed that regularizing inference-time search techniques can produce agents that are not only strong but can also model human behaviour well. In the domain of no-press Diplomacy, they show that regularizing hedge (an equilibrium-finding algorithm) with a KL-divergence penalty towards a human imitation learning policy can match or exceed the human action prediction accuracy of imitation learning while being substantially stronger. KL-regularization toward human behavioral policies has previously been proposed in various forms in single-and multi-agent RL algorithms (Nair et al., 2018;Siegel et al., 2020;Nair et al., 2020), and was notably employed in AlphaStar (Vinyals et al., 2019), but this has typically been used to improve sample efficiency and aid exploration rather than to better model and coordinate with human play.\nAn alternative line of research has attempted to build human-compatible agents without relying on human data (Hu et al., 2020;Strouse et al., 2021). These techniques have shown some success in simplified settings but have not been shown to be competitive with humans in large-scale collaborative environments.", "publication_ref": ["b26", "b13", "b0", "b1", "b13", "b26", "b0", "b1", "b19", "b23", "b28", "b24", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "MARKOV GAMES", "text": "In this work, we focus on multiplayer Markov games (Shapley, 1953).\nDefinition. An n-player Markov game \u2206 is a tuple S, A 1 , . . . , A n , r 1 , . . . , r n , p where S is the state space, A i is the action space of player i (i = 1, . . . , n), r i : S \u00d7 A 1 \u00d7 \u2022 \u2022 \u2022 \u00d7 A n \u2192 R is the reward function for player i, f : S \u00d7 A 1 \u00d7 \u2022 \u2022 \u2022 \u00d7 A n \u2192 S is the transition function.\nThe goal of each player i, is to choose a policy \u03c0 i (s) : S \u2192 \u2206A i that maximizes the expected reward for that player, given the policies of all other players. In case of n = 1, a Markov game reduces to a Markov Decision Process (MDP) where an agent interacts with a fixed environment.\nAt each state s, each player i simultaneously chooses an action a i from a set of actions A i . We denote the actions of all players other than i as a \u2212i . Players may also choose a probability distribution over actions, where the probability of action a i is denoted \u03c0 i (s, a i ) or \u03c3 i (a i ) and the vector of probabilities is denoted \u03c0 i (s) or \u03c0 i .", "publication_ref": ["b27"], "figure_ref": [], "table_ref": []}, {"heading": "HEDGE", "text": "Hedge Littlestone & Warmuth (1994); Freund & Schapire (1997) is an iterative algorithm that converges to an equilibrium. We use variants of hedge for planning by using them to compute an equilibrium policy on each turn of the game and then playing that policy.\nAssume that after player i chooses an action a i and all other players choose actions a \u2212i , player i receives a reward of u i (a i , a \u2212i ), where u i will come from our RL-trained value function. We denote the average reward in hindsight for action a i up to iteration t as Q t (a i ) = 1\nOn each iteration t of hedge, the policy \u03c0 t i (a i ) is set according to \u03c0 t i (a i ) \u221d exp Q t\u22121 (a i )/\u03ba t\u22121 where \u03ba t is a temperature parameter. 2 It is proven that if \u03ba t is set to 1 \u221a t then as t \u2192 \u221e the average policy over all iterations converges to a coarse correlated equilibrium, though in practice it often comes close to a Nash equilibrium as well. In all experiments we set \u03ba t = 3St 10 \u221a t on iteration t, where S t is the observed standard deviation of the player's utility up to iteration t, based on a heuristic from . A simpler choice is to set \u03ba t = 0, which makes the algorithm equivalent to fictitious play (Brown, 1951).\nRegret matching (RM) (Blackwell et al., 1956;Hart & Mas-Colell, 2000) is an alternative equilibrium-finding algorithm that has similar theoretical guarantees to hedge and was used in previously work on Diplomacy Gray et al. (2020); Bakhtin et al. (2021). We do not use this algorithm but we do evaluate baseline agents that use RM.", "publication_ref": ["b21", "b12", "b5", "b3", "b14", "b13", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "DORA: SELF-PLAY LEARNING IN MARKOV GAMES", "text": "Our approach draws significantly from DORA (Bakhtin et al., 2021), which we describe in more detail here. In this approach, the authors run an algorithm that is similar to past model-based reinforcement-learning methods such as AlphaZero (Silver et al., 2018), except in place of Monte Carlo tree search, which is unsound in simultaneous-action games such as Diplomacy or other imperfect information games, it instead uses an equilibrium-finding algorithm such as hedge or RM to iteratively approximate a Nash equilibrium for the current state (i.e., one-step lookahead search). A deep neural net trained to predict the policy is used to sample plausible actions for all players to reduce the large action space in Diplomacy down to a tractable subset for the equilibrium-finding procedure, and a deep neural net trained to predict state values is used to evaluate the results of joint actions sampled by this procedure. Beginning with a policy and value network randomly initialized from scratch, a large number of self-play games are played and the resulting equilibrium policies and the improved 1-step value estimates computed on every turn from equilibrium-finding are added to a replay buffer used for subsequently improving the policy and value. Additionally, a double-oracle (McMahan et al., 2003) method was used to allow the policy to explore and discover additional actions, and the same equilibrium-finding procedure was also used at test time.\nFor the core update step, Bakhtin et al. (2021) propose Deep Nash Value Iteration (DNVI), a value iteration procedure similar to Nash Q-Learning (Hu & Wellman, 2003), which is a generalization of Q-learning (Watkins, 1989) from MDPs to Stochastic games. The idea of Nash-Q is to compute equilibrium policies \u03c3 in a subgame where the actions correspond to the possible actions in a current state and the payoffs are defined using the current approximation of the value function. Bakhtin et al. (2021) propose an equivalent update that uses a state value function V (s) instead of a state-action value function Q(s, a):\nV (s) \u2190 (1 \u2212 \u03b1)V (s) + \u03b1(r + \u03b3 a \u03c3(a )V (f (s, a ))) (1\n)\nwhere \u03b1 is the learning rate, \u03c3(\u2022) is the probability of joint action in equilibrium, a is joint action, and f is the transition function. For 2p0s games and certain other game classes, this algorithm converges to a Nash equilibrium in the original stochastic game under the assumption that an exploration policy is used such that each state is visited infinitely often .\nThe tabular approach of Nash-Q does not scale to large games such as Diplomacy. DNVI replaces the explicit value function table and update rule in 1 with a value function parameterized by a neural network, V (s; \u03b8 v ) and uses gradient descent to update it using the following loss:\nValueLoss(\u03b8 v ) = 1 2 V (s; \u03b8 v ) \u2212 r(s) \u2212 \u03b3 a \u03c3(a )V f (s, a );\u03b8 v 2 (2)\nThe summation used in 2 is not feasible in games with large action spaces as the number of joint actions grow exponentially with the number of players. Bakhtin et al. (2021) address this issue by considering only a subset of actions at each step. An auxiliary function, a policy proposal network \u03c0 i (s, a i ; \u03b8 \u03c0 ), models the probability that an action a i of player i is in the support of the equilibrium \u03c3. Only the top-k sampled actions from this distribution are considered when solving for the equilibrium policy \u03c3 and computing the above value loss. Once the equilibrium is computed, the equilibrium policy is also used to further train the policy proposal network using cross entropy loss:\nPolicyLoss(\u03b8 \u03c0 ) = \u2212 i ai\u2208Ai \u03c3 i (a) log \u03c0 i (s, a i ; \u03b8 \u03c0 ).\n(3) Bakhtin et al. (2021) report that the resulting agent DORA does very well when playing with other copies of itself. However, DORA performs poorly in games with 6 human human-like agents.", "publication_ref": ["b1", "b22", "b1", "b17", "b1", "b1", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "PIKL: MODELING HUMANS WITH IMITATION-ANCHORED PLANNING", "text": "Behavioral cloning (BC) is the standard approach for modeling human behaviors given data. Behavioral cloning learns a policy that maximizes the likelihood of the human data by gradient descent on a cross-entropy loss. However, as observed and discussed in Jacob et al. (2022), BC often falls short of accurately modeling or matching human-level performance, with BC models underperforming the human players they are trained to imitate in games such as Chess, Go, and Diplomacy. Intuitively, it might seem that initializing self-play with an imitation-learned policy would result in an agent that is both strong and human-like. Indeed, Bakhtin et al. (2021) showed improved performance against human-like agents when initializing the DORA training procedure from a human imitation policy and value, rather than starting from scratch. However, we show in subsection 5.3 that such an approach still results in policies that deviate from human-compatible equilibria. Jacob et al. (2022) found that an effective solution was to perform search with a regularization penalty proportional to the KL divergance from a human imitation policy. This algorithm is referred to as piKL. The form of piKL we focus on in this paper is a variant of hedge called piKL-hedge, in which each player i seeks to maximize expected reward, while at the same time playing \"close\" to a fixed anchor policy \u03c4 i . The two goals can be reconciled by defining a composite utility function that adds a penalty based on the \"distance\" between the player policy and their anchor policy, with coefficient \u03bb i \u2208 [0, \u221e) scaling the penalty.\nFor each player i, we define i's utility as a function of the agent policy \u03c0 i \u2208 \u2206(A i ) given policies \u03c0 \u2212i of all other agents:\u0169\ni,\u03bbi (\u03c0 i , \u03c0 \u2212i ) := u i (\u03c0 i , \u03c0 \u2212i ) \u2212 \u03bb i D KL (\u03c0 i \u03c4 i )(4)\nAlgorithm 1: DIL-PIKL (for Player i) Data: \u2022 A i set of actions for Player i;\n\u2022 u i reward function for Player i;\n\u2022 \u039b i a set of \u03bb values to consider for Player i;\n\u2022 \u03b2 i a belief distribution over \u03bb values for Player i.\n1 function INITIALIZE() 2 t \u2190 0 3 for each action a i \u2208 A i do 4 Q 0 i (a i ) \u2190 0 5 function PLAY() 6 t \u2190 t + 1 7 sample \u03bb \u223c \u03b2 i 8 let \u03c0 t i,\u03bb\nbe the policy such that\n\u03c0 t i,\u03bb (a i ) \u221d exp Q t\u22121 (a i ) + \u03bb log \u03c4 i (a i ) \u03ba t\u22121 + \u03bb 9 sample an action a t i \u223c \u03c0 t i,\u03bb\n10 play a t i \u2208 A i and observe actions a t \u2212i played by the opponents\n11 for each a i \u2208 A i do 12 Q t (a i ) \u2190 t\u22121 t Q t\u22121 (a i ) + 1 t u i (a i , a t \u2212i )\nFigure 1: DiL-piKL algorithm. Lines with highlights show the main differences between this algorithm and piKL-Hedge algorithm proposed in Jacob et al. (2022).\nFigure 2: \u03bbpop represents the commonknowledge belief about the \u03bb parameter or distribution used by all players. \u03bbagent represents the \u03bb value actually used by the agent to determine its policy. By having \u03bbagent differ from \u03bbpop, DiL-piKL interpolates between an equilibrium under the utility function ui, behavioral cloning and best response to behavioral cloning policies. piKL assumed a common \u03bb, which moved it along one axis of the space. Our agent models and coordinates with high-\u03bb players while playing a lower \u03bb itself.\nThis results in a modification of hedge such that on each iteration t, \u03c0 t i (a i ) is set according to\n\u03c0 t i (a i ) \u221d exp Q t\u22121 (a i ) + \u03bb log \u03c4 i (a i ) \u03ba t\u22121 + \u03bb (5)\nWhen \u03bb is large, the utility function is dominated by the KL-divergence term \u03bb i D KL (\u03c0 i \u03c4 i ), and so the agent will naturally tend to play a policy \u03c0 i close to the anchor policy \u03c4 i . When \u03bb i is small, the dominating term is the rewards u i (\u03c0 i , a t \u2212i ) and so the agent will tend to maximize reward without as closely matching the anchor policy \u03c4 i .", "publication_ref": ["b19", "b1", "b19", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "DISTRIBUTIONAL LAMBDA PIKL (DIL-PIKL)", "text": "piKL trades off between the strength of the agent and the closeness to the anchor policy using a single fixed \u03bb parameter. In practice, we find that sampling \u03bb from a probability distribution each iteration produces better performance. In this section, we introduce distributional lambda piKL (DiL-piKL), which replaces the single \u03bb parameter in piKL with a probability distribution \u03b2 over \u03bb values. On each iteration, each player i samples a \u03bb value from \u03b2 i and then chooses a policy based on Equation 5 using that sampled \u03bb. Figure 1 highlights the difference between piKL and DiL-piKL.\nOne interpretation of DiL-piKL is that each choice of \u03bb is an agent type, where agent types with high \u03bb choose policies closer to \u03c4 while agent types with low \u03bb choose policies that are more \"optimal\" and less constrained to a common-knowledge anchor policy. A priori, each player is randomly sampled from this population of agent types, and the distribution \u03b2 i represents the common-knowledge uncertainty about which of the agent types player i may be. Another interpretation is that piKL assumed an exponential relation between action EV and likelihood, whereas DiL-piKL results in a fatter-tailed distribution that may more robustly model different playing styles or game situations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "COORDINATING WITH PIKL POLICIES", "text": "While piKL and DiL-piKL are intended to model human behavior, an optimal policy in cooperative environments should be closer to a best response to this distribution. Selecting different \u03bb values for the common-knowledge population versus the policy the agent actually plays allows us to interpolate between BC, best response to BC, and equilibrium policies (Figure 2). In practice, our agent samples from \u03b2 i during equilibrium computation but ultimately plays a low \u03bb policy, modeling the fact that other players are unaware of our agent's true type.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "THEORETICAL PROPERTIES OF DIL-PIKL", "text": "DiL-piKL can be understood as a sampled form of follow-the-regularized-leader (FTRL). Specifically, one can think of Algorithm 1 as an instantiation of FTRL over the Bayesian game induced by the set \u039b i = supp \u03b2 i of types \u03bb i and the regularized utilities\u0169 i,\u03bbi of each player i. In the appendix we show that when a player i learns using DiL-piKL, the distributions \u03c0 t i,\u03bb for any type \u03bb i \u2208 \u039b i are no-regret with respect to the regularized utilities\u0169 i,\u03bbi defined in (4). Formally:\nTheorem 1 (abridged). Let W be a bound on the maximum absolute value of any payoff in the game, and Q i := 1 ni a\u2208Ai log \u03c4 i (a). Then, for any player i, type \u03bb i \u2208 \u039b i , and number of iterations T , the regret cumulated can be upper bounded as\nmax \u03c0\u2208\u2206(Ai) T t=1\u0169 i,\u03bbi (\u03c0, a t \u2212i ) \u2212\u0169 i,\u03bbi (\u03c0 t i,\u03bbi , a t \u2212i ) \u2264 W 2 4 min 2 log T \u03bb i , T \u03b7 + log n i \u03b7 + \u03c1 i,\u03bbi ,\nwhere the game constant \u03c1 i,\u03bbi is defined as \u03c1 i,\u03bbi := \u03bb i (log\nn i + Q i ).\nThe traditional analysis of FTRL is not applicable to DiL-piKL because the utility functions, as well as their gradients, can be unbounded due to the nonsmoothness of the regularization term \u2212\u03bb i D KL (\u03c0 \u03c4 i ) that appears in the regularized utility function\u0169 i,\u03bbi , and therefore a more sophisticated analysis needs to be carried out. Furthermore, even in the special case of a single type (i.e., a singleton set \u039b i ), where DiL-piKL coincides with piKL, the above guarantee significantly refines the analysis of piKL in two ways. First, it holds no matter the choice of stepsize \u03b7 > 0, thus implying a O(log T /(T \u03bb i )) regret bound without assumptions on \u03b7 other than \u03b7 = \u2126(1). Second, in the cases in which \u03bb i is tiny, by choosing \u03b7 = \u0398(1/ \u221a T ) we recover a sublinear guarantee (of order \u221a T ) on the regret.\nIn 2p0s games, the logarithmic regret of Theorem 1 immediately implies that the average polic\u0233\n\u03c0 T i,\u03bbi := 1 T T t=1 \u03c0 t i,\u03bbi of each player i is a C log T T\n-approximate Bayes-Nash equilibrium strategy. In fact, a strong guarantee on the last-iterate convergence of the algorithm can be obtained too: Theorem 2 (abridged; Last-iterate convergence of piKL in 2p0s games). When both players in a 2p0s game learn using DiL-piKL for T iterations, their policies converge almost surely to the unique Bayes-Nash equilibrium (\u03c0 * i,\u03bbi ) of the regularized game defined by utilities\u0169 i,\u03bbi (4).\nThe last-iterate guarantee stated in Theorem 2 crucially relies on the strong convexity of the regularized utilities, and conceptually belongs with related efforts in showing last-iterate convergence of online learning methods. However, a key difficulty that sets apart Theorem 2 is the fact that the learning agents observe sampled actions from the opponents, which makes the proof of the result (as well as the obtained convergence rate) different from prior approaches.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "DESCRIPTION OF DIPLODOCUS", "text": "By replacing the equilibrium-finding algorithm used in DORA with DiL-piKL, we hypothesize that we can learn a strong and human-compatible policy as well as a value function that can accurately evaluate game states, assuming strong and human-like continuation policies. We call this self-play algorithm RL-DiL-piKL. We use RL-DiL-piKL to train value and policy proposal networks and use DiL-piKL during test-time search.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "TRAINING", "text": "Our training algorithm closely follows that of DORA, described in Section 2.3. The loss functions used are identical to DORA and the training procedure is largely the same, except in place of RM to compute the equilibrium policy \u03c3 on each turn of a game during self-play, we use DiL-piKL with a \u03bb distribution and human imitation anchor policy \u03c4 that is fixed for all of training. See Appendix H for a detailed description of differences between DORA and RL-DiL-piKL.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "TEST-TIME SEARCH", "text": "Following Bakhtin et al. (2021), at evaluation time we perform 1-ply lookahead where on each turn we sample up to 30 of the most likely actions for each player from the RL policy proposal network. However, rather than using RM to compute the equilibrium \u03c3, we apply DiL-piKL.\nAs also mentioned previously in Section 3, while our agent samples \u03bb i from the probability distribution \u03b2 i when computing the DiL-piKL equilibrium, the agent chooses its own action to actually play using a fixed low \u03bb. For all experiments, including all ablations, the agent uses the same BC anchor policy. For DiL-piKL experiments for each player i we set \u03b2 i to be uniform over {10 \u22124 , 10 \u22123 , 10 \u22122 , 10 \u22121 } and play according to \u03bb = 10 \u22124 , except for the first turn of the game. On the first turn we instead sample from {10 \u22122 , 10 \u22121.5 , 10 \u22121 , 10 \u22120.5 } and play according to \u03bb = 10 \u22122 , so that the agent plays more diverse openings, which more closely resemble those that humans play.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTS", "text": "We first compare the performance of two variants of Diplodocus in a population of prior agents and other baseline agents. We then report results of Diplodocus playing in a tournament with humans.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTAL SETUP", "text": "In order to measure the ability of agents to play well against a diverse set of opponents, we play many games between AI agents where each of the seven players are sampled randomly from a population of baselines (listed in Appendix D) or the agent to be tested. We report scores for each of the following algorithms against the baseline population:\nDiplodocus-Low and Diplodocus-High are the proposed agents that use RL-DiL-piKL during training with 2 player types {10 \u22124 , 10 \u22121 } and {10 \u22122 , 10 \u22121 }, respectively. DORA is an agent that is trained via self-play and uses RM as the search algorithm during training and test-time. Both the policy and the value function are randomly initialized at the start of training. DNVI is similar to DORA, but the policy proposal and value networks are initialized from human BC pretraining. DNVI-NPU is similar to DNVI, but during training only the RL value network is updated. The policy proposal network is still trained but never fed back to self-play workers, to limit self-play drift from human conventions. The final RL policy proposal network is only used at the end, at test time (along with the RL value network).\nBRBot is an approximate best response to the BC policy. It was trained the same as Diplodocus, except that during training the agent plays one distinguished player each game with \u03bb = 0 while all other players use \u03bb \u2248 \u221e. SearchBot, a one-step lookahead equilibrium search agent from (Gray et al., 2020), evaluated using their published model. HedgeBot is an agent similar to SearchBot (Gray et al., 2020) but using our latest architecture and using hedge rather than RM as the equilibrium-finding algorithm. FPPI-2 and SL are two agents from (Anthony et al., 2020), evaluated using their published model.\nAfter computing these population scores, as a final evaluation we organized a tournament where we evaluated four agents for 50 games each in a population of online human participants. We evaluated two baseline agents, BRBot and DORA, and two of our new agents, Diplodocus-Low and Diplodocus-High.\nIn order to limit the duration of games to only a few hours, these games used a time limit of 5 minutes per turn and a stochastic game-end rule where at the beginning of each game year between", "publication_ref": ["b13", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Agent", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Score against population", "text": "Diplodocus-Low 29% \u00b1 1% Diplodocus-High 28% \u00b1 1% DNVI-NPU (retrained) (Bakhtin et al., 2021) 20% \u00b1 1% BRBot 18% \u00b1 1% DNVI (retrained) (Bakhtin et al., 2021) 15% \u00b1 1% HedgeBot (retrained) (Jacob et al., 2022) 14% \u00b1 1% DORA (retrained) (Bakhtin et al., 2021) 13% \u00b1 1% FPPI-2 (Anthony et al., 2020) 9% \u00b1 1% SearchBot (Gray et al., 2020) 7% \u00b1 1% SL (Anthony et al., 2020) 6% \u00b1 1%\nTable 1: Performance of different agents in a population of various agents. Agents above the line were trained using identical neural network architectures. Agents below the line were evaluated using the models and the parameters provided by the authors. The \u00b1 shows one standard error.\n1909 and 1912 the game ends immediately with 20% chance per year, increasing in 1913 to a 40% chance. Players were not told which turn the game would end on for a specific game, but were told the distribution it was sampled from. Our agents were also trained based on this distribution. 3 Players were recruited from Diplomacy mailing lists and from webdiplomacy.net. In order to mitigate the risk of cheating by collusion, players were paid hourly rather than based on in-game performance. Each game had exactly one agent and six humans. The players were informed that there was an AI agent in each game, but did not know which player was the bot in each particular game. In total 62 human participants played 200 games with 44 human participants playing more than two games and 39 human participants playing at least 5 games.", "publication_ref": ["b1", "b1", "b19", "b1", "b13", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTAL RESULTS", "text": "We first report results for our agents in the fixed population described in Appendix D. The results, shown in Table 1, show Diplodocus-Low and Diplodocus-High perform the best by a wide margin.\nWe next report results for the human tournament in Table 2. For each listed player, we report their average score, Elo rating, and rank within the tournament based on Elo among players who played at least 5 games. Elo ratings were computed using a standard generalization of BayesElo (Coulom, 2005) to multiple players (Hunter, 2004) (see Appendix I for details). This gives similar rankings as average score, but also attempts to correct for both the average strength of the opponents, since some games may have stronger or weaker opposition, as well as for which of the seven European powers a player was assigned in each game, since some starting positions in Diplomacy are advantaged over others. To regularize the model, a weak Bayesian prior was applied such that each player's rating was normally distributed around 0 with a standard deviation of around 350 Elo.\nThe results show that Diplodocus-High performed best among all the humans by both Elo and average score. Diplodocus-Low followed closely behind, ranking second according to average score and third by Elo. BRBot performed relatively well, but ended ranked below that of both DiL-piKL agents and several humans. DORA performed relatively poorly.\nTwo participants achieved a higher average score than the Diplodocus agents, a player averaging 35% but who only played two games, and a player with a score of 29% who played only one game.\nWe note that given the large statistical error margins, the results in Table 2 do not conclusively demonstrate that Diplodocus outperforms the best human players, nor do they alone demonstrate an unambiguous separation between Diplodocus and BRBot. However, the results do indicate that Diplodocus performs at least at the level of expert players in this population of players with diverse skill levels. Additionally, the superior performance of both Diplodocus agents compared to BRBot is consistent with the results from the agent population experiments in Table 1.", "publication_ref": ["b9", "b18"], "figure_ref": [], "table_ref": ["tab_1", "tab_1"]}, {"heading": "Rank", "text": "Elo Avg Score # Games  participants who played at least 5 games. The \u00b1 shows one standard error. In addition to the tournament, we asked three expert human players to evaluate the strength of the agents in the tournament games based on the quality of their actions. Games were presented to these experts with anonymized labels so that the experts were not aware of which agent was which in each game when judging that agent's strategy. All the experts picked a Diplodocus agent as the strongest agent, though they disagreed about whether Diplodocus-High or Diplodocus-Low was best. Additionally, all experts indicated one of the Diplodocus agents as the one they would most like to cooperate with in a game. We provide detailed responses in Appendix C. As a proxy for agent strength, we measure the average score of an agent vs 6 copies of HedgeBot.\n\u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 DORA 32 -20 13% \u00b1 3% 50 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 Human 43 -187 1% \u00b1 1% 7", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "RL TRAINING COMPARISON", "text": "As a proxy for modeling humans, we compute prediction accuracy of human moves on a validation dataset of roughly 630 games held out from training of the human BC model, i.e., how often the most probable action under the policy corresponds to the one chosen by a human. Similar to Bakhtin et al. (2021), we found that agents without biasing techniques (DORA and DNVI) diverge from human play as training progress. By contrast, Diplodocus-High achieves significant improvement in score while keeping the human prediction accuracy high.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "DISCUSSION", "text": "In this work we describe RL-DiL-piKL and use it to train an agent for no-press Diplomacy that placed first in a human tournament. We ascribe Diplodocus's success in Diplomacy to two ideas.\nFirst, DiL-piKL models a population of player types with different amounts of regularization to a human policy while ultimately playing a strong (low-\u03bb) policy itself. This improves upon simply playing a best response to a BC policy by accounting for the fact that humans are less likely to play highly suboptimal actions and by reducing overfitting of the best response to the BC policy. Second, incorporating DiL-piKL in self-play allows us to learn an accurate value function in a diversity of situations that arise from strong and human-like players. Furthermore, this value assumes a human continuation policy that makes fewer blunders than the BC policy, allowing us to correctly estimate the values of positions that require accurate play (such as stalemate lines).\nIn conclusion, combining human imitation, planning, and RL presents a promising avenue for building agents for complex cooperative and mixed-motive environments. Further work could explore regularized search policies that condition on more complex human behavior, including dialogue. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B DESCRIPTION OF DIPLOMACY", "text": "The rules of no-press Diplomacy are complex; a full description is provided by Paquette et al. (2019).\nNo-press Diplomacy is a seven-player zero-sum board game in which a map of Europe is divided into 75 provinces. 34 of these provinces contain supply centers (SCs), and the goal of the game is for a player to control a majority (18) of the SCs. Each players begins the game controlling three or four SCs and an equal number of units.\nThe game consists of three types of phases: movement phases in which each player assigns an order to each unit they control, retreat phases in which defeated units retreat to a neighboring province, and adjustment phases in which new units are built or existing units are destroyed.\nDuring a movement phase, a player assigns an order to each unit they control. A unit's order may be to hold (defend its province), move to a neighboring province, convoy a unit over water, or support a neighboring unit's hold or move order. Support may be provided to units of any player. We refer to a tuple of orders, one order for each of a player's units, as an action. That is, each player chooses one action each turn. There are an average of 26 valid orders for each unit (Paquette et al., 2019), so the game's branching factor is massive and on some turns enumerating all actions is intractable.\nImportantly, all actions occur simultaneously. In live games, players write down their orders and then reveal them at the same time. This makes Diplomacy an imperfect-information game in which an optimal policy may need to be stochastic in order to prevent predictability.\nDiplomacy is designed in such a way that cooperation with other players is almost essential in order to achieve victory, even though only one player can ultimately win.\nA game may end in a draw on any turn if all remaining players agree. Draws are a common outcome among experienced players because players will often coordinate to prevent any individual from reaching 18 centers. The two most common scoring systems for draws are draw-size scoring (DSS), in which all surviving players equally split a win, and sum-of-squares scoring (SoS), in which player i receives a score of\nC 2 i j\u2208N C 2 j\n, where C i is the number of SCs that player i controls (Fogel, 2020). Throughout this paper we use SoS scoring except in anonymous games against humans where the human host chooses a scoring system.", "publication_ref": ["b26", "b26", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "C EXPERT EVALUATION OF THE AGENTS", "text": "The anonymous format of the tournament aimed at reducing possible biases of players towards the agent, e.g., trying to collectively eliminate the agents as targeting the agent is a simple way to break the symmetry. At the same time a significant property of Diplomacy is knowing the play styles of different players and using this knowledge to make decision of whom to trust and whom to chose as an ally. To evaluates this aspect of the game play we asked for qualitative feedback from three Diplomacy experts. Each player was given 7 games (one per power) from each of the 4 different agents that played in the tournament. The games evaluated by each expert were disjoint from the games evaluated by the other experts. The games were anonymized such that the experts were not able to tell which agent played in the game based on the username or from the date. We asked a few questions about the game play of each agent independently and then asked the experts to choose the best agent for strength and human-like behavior. The experts referred to the agents as Agent1, ..., Agent4, but we de-anonymized the agents in the answers below.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 OVERALL WHAT IS THE STRONGEST AGENT?", "text": "Expert 1 I think Diplodocus-Low was the strongest, then BRBot closely followed by Diplodocus-High. DORA is a distant third.\nExpert 2 Diplodocus-High.\nExpert 3 Diplodocus-Low. This feels stronger than a human in certain ways while still being very human-like.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "WHAT IS THE MOST HUMAN-LIKE/BOT-LIKE AGENT?", "text": "Expert 1 Most human-like is Diplodocus-High. A boring human, but a human nonetheless. Diplodocus-Low is not far behind, then BRBot and DORA both of which are very non-human albeit in very different ways.\nExpert 2 Diplodocus-High.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Expert 3 Diplodocus-Low. WHAT IS THE AGENT YOU'D LIKE TO COOPERATE WITH?", "text": "Expert 1 This is the most interesting question. I think Diplodocus-Low, because I like how it plays -we'd \"vibe\" -but also because I think it is quite predictable in what motivates it to change alliances. That's potentially exploitable, even with the strong tactics it has. I'd least like to work with Diplodocus-High as it seems to be very much in it for itself. I suspect it would be quite unpleasant to play against as it is tactically excellent and seems hard to work with. I'd love to be on a board with DORA, as I'd expect my chances to solo to go up dramatically! It would be a very fun game so long as you weren't on the receiving end of some of its play.\nExpert 2 Diplodocus-High.\nExpert 3 Diplodocus-Low. Diplodocus-High is also strong, but seems much less interesting to play with, because of the way it commits to alliances without taking into account who is actually willing to work with it. This limits what a human can do to change their situation quite a lot and would be fairly frustrating in the position of a neighbour being attacked by it.\nBRBot and DORA feel too weak to be particularly interesting.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.2 DORA HOW WOULD YOU EVALUATE THE OVERALL STRENGTH OF THE AGENT?", "text": "Expert 1 Not great. There's a lot to criticize here -from bad opening play (Russia = bonkers), to poor defense (Turkey) and just generally bad tactics and strategy compared to the other agents (France attacking Italy when Italy is their only ally was an egregious example of this).\nExpert 2 Very weak. Seemed to invite its own demise with the way it routinely picked fights in theaters it had no business being in and failing to cooperate with neighbors Expert 3 Poor. It is bad at working with players, and it makes easily avoidable blunders even when working alone.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "HOW WOULD YOU EVALUATE THE ABILITY OF THE AGENT TO COOPERATE?", "text": "Expert 1 It seems to make efforts, but it also seems to misjudge what humans are likely to do. There's indicative support orders and they're pretty good, but it also doesn't seem to understand or account for vindictiveness over always playing best. The Turkey game where it repeatedly seems to expect Russia to not attack is an example of this.\nExpert 2 Poor. Seemed to pick fights without seeing or soliciting support necessary to win, failed to support potential allies in useful ways to take advantage of their position.\nExpert 3 Middling to Poor. It very occasionally enters good supports but it often enters bad ones, and has a habit of attacking too many people at once (and not considering that attacking those people will turn them against it). It has a habit of annoying many players and doing badly as a result.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.3 BRBOT HOW WOULD YOU EVALUATE THE OVERALL STRENGTH OF THE AGENT?", "text": "Expert 1 The agent has solid, at least human level tactics and clearly sees opportunities to advance and acts accordingly. Sometimes this is to the detriment of the strategic position, but the balance is fair given the gunboat nature of the games. Overall, the bot feels naturally to be in the \"better than average human\" range rather than super-human, but the results indicate that it performs at a higher level than the \"feeling\" it gives. It has a major opportunity for improvement, discussed in the next point.\nExpert 2 Overall, seemed fairly weak and seemed to be able to succeed most frequently when benefiting from severe mistakes from neighboring agents. That being said it was able to exploit those mistakes somewhat decently in some cases and at least grow to some degree off of it. Expert 3 Middling. It is tactically strong when not having to work with other players and when it has a considerable number of units, but is quite weak when attempting to cooperate with other players. Its defensive strength varies quite significantly too, possibly also based on unit countwhen it had relatively few units it missed very obvious defensive tactics.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "HOW WOULD YOU EVALUATE THE ABILITY OF THE AGENT TO COOPERATE?", "text": "Expert 1 The bot is hyperactively trying to coordinate and signal to the other players that it wants to work with them. Sometimes this is in the form of ridiculous orders that probably indicate desperation more than a mutually beneficial alliance, and this backfires as you may expect. At its best it makes exceptional signaling moves (RUSSIA game 4 : War -Mos in Fall 1901 is exceptional) but at worst it is embarrassingly bad and leads to it getting attacked (TURKEY game 5 : supporting convoys from Gre -Smy or supporting other powers moving to Armenia). The other weakness is that it tends to make moves like these facing all other powers -this is not optimal as indicating to all other powers that you want to work with them is equivalent to not indicating anything at all -if anything it seems a little duplicitous. This is especially true when the bot is still inclined to stab when the opportunity presents itself, which means the signaling is superficial and unlikely to work repeatedly. Overall, the orders show the ability to cooperate, signal, and work together, but the hyperactivity of the bot is limiting the effectiveness of the tools to achieve the best results.\nExpert 2 Poor. Random support orders seemed to be thrown without an overarching strategy behind them. Moves didn't seem to suggest long term thoughts of collaboration. Expert 3 Poor. When attempting to work with another player, it almost always gives them the upper hand, and even issues supports that suggest it is okay with that player taking its SCs when it should not be. It sometimes matches supports to human moves, but does not seem to do this very often. The nonsensical supports are much more common.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.4 DIPLODOCUS-HIGH HOW WOULD YOU EVALUATE THE OVERALL STRENGTH OF THE AGENT?", "text": "Expert 1 The tactics are unadventurous and sometimes seem below human standards (for example, the train of army units in the Italy game; the whole Turkey game) but conversely they also have a longer view of the game (see also: Italy game -the trained bounces don't matter strategically). There's less nonsense too; if I were to sum the bot up in two words it would be \"practical\" and \"boring\".\nExpert 2 Seemed to be strong. Wrote generally good tactical orders, showed generally good strategic sense. Showed discipline and a willingness to let allies survive in weak positions while having units that could theoretically stab for dots with ease remaining right next to that weak ally. There were some highly questionable moments as both Italy and France early on in 1901 strategy which seemed to heavily harm their ability to get out of the box.\nThe Austrian game was particularly impressive in terms of its ability to handle odd scenarios and achieve the solo despite receiving pressure on multiple occasions on multiple fronts. Expert 3 Generally strong. It is good at signalling and forming alliances, is tactically strong when in its favoured alliance, and is especially strong when ahead. Its main weakness seems to be an inability to adapt -if its favoured alliance is declined, it will often keep trying to 'pitch' that same alliance instead of working towards alternatives.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "HOW WOULD YOU EVALUATE THE ABILITY OF THE AGENT TO COOPERATE?", "text": "Expert 1 Low. It doesn't put much effort into this. The French game, for example, the bot just seems to accept it is being attacked and fight through it. It's so boring and tactical and shows little care for cooperation. Many great gunboat players do this but it will not hold up in press games. What it does seem to do is capitalize on other player's mistakes -see the Austrian game where it sneaks into Scandinavia and optimizes to get to 18 (there can't be a lot of training data for that!).\nExpert 2 Very strong ability to cooperate as seen in the Turkish game, but in other games seemed to try and pick fights against the entire world in ways that were ultimately self-defeating.\nExpert 3 Good. It can work well with human players, matching supports and even using signalling supports in ways humans would. It frequently attempts to side with a player who is attacking it, though, so it seems to have a problem with identifying which player to work with.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.5 DIPLODOCUS-LOW HOW WOULD YOU EVALUATE THE OVERALL STRENGTH OF THE AGENT?", "text": "Expert 1 Exceptional. Very strong tactics and a clear directionality to what it does -it seems to understand what the strategic value of a position is and it acts with efficiency to achieve the strategic goals. It has great results (time drawn out of a few wins!) but also fights back from \"losing\" positions extremely well which makes it quantifiably strong, but it also just plays a beautiful and effective game. Very strong indeed. It does sometimes act too aggressively for tournament play (Austria is the example where this came home to roost) -the high risk/reward plays are generally but not always correct in single games, but for tournament play it goes for broke a bit too much (This is outside the scope of the agent I suspect, as it is playing to scoring system not tournament scoring system). Against human players who may not see the longer term impact of their play, it results in games like this one. . . which is ugly both for Austria and for everyone else except Turkey.\nExpert 2 Very weak. Seemed to abandon its own position in many cases to pursue questionable adventures. Sometimes they worked out but generally they failed, resulting in things like a Germany under siege holding Edi while they as England are off in Portugal and are holding onto their home centers only because FG were under siege by the south.\nExpert 3 Very strong. It can signal alliances very well and generally chooses the correct allies, seems strong tactically even on defence, and makes some plays you would not expect from a human player but which are outright stronger than a human player would make.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "HOW WOULD YOU EVALUATE THE ABILITY OF THE AGENT TO COOPERATE?", "text": "Expert 1 Pretty good. It sends signaling moves and makes efforts to support other players quite a lot (see in particular Russia). I particularly like the skills being shown to work together tactically and try and support other units -this is both effective and quite human. This is my favorite bot by some distance when it comes to cooperating with the other players. There is a weakness in that it does seem to reassess alliances every turn, which means sometimes the excellent work indicating and supporting is undone without getting the chance to realize the gains (Examples with Russia and Italy).\nExpert 2 Poor. Didn't seem to give meaningful support orders when they would have helped and gave plenty of meaningless signaling supports and some questionable ones like supporting the English into SKA in F1901 as Germany among other oddities Expert 3 Good. It signals alliances in very human ways, through clear signalling builds, accurate support moves where it makes sense, and support holds otherwise. It also seems to match supports with its allies well.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D POPULATION BASED EVALUATION", "text": "In general-sum games like Diplomacy, winrate in head-to-head matches against a previous version of an agent may not be as informative because of nontransitivity between agents. For example, exploitative agents such as best-response-to-BC may do particularly well against BC or other pure imitation-learning agents, and less well against all other agents. Additionally, Bakhtin et al. (2021) found that a pair of independently and equally-well-trained RL agents may each appear very weak in a population composed of the other due to converging to incompatible equilibria. Many agents also varied significantly in how well they performed against other search-based agents.\nTherefore, we resort to playing against a population of previously training agents as was done in Jacob et al. (2022), intended to measure more broadly how well an agent does on average against a wider suite of various human-like agents.\nMore precisely, we define a fixed set of baseline agents as a population. To determine an agent's average population score, we add that agent into the population and then play games where in each game, all 7 players are uniformly randomly sampled from the population with replacement, keeping only games where the agent to be tested was sampled at least once. Note that unlike Jacob et al. (2022), we run a separate population test for each new agent to be tested, rather than combining all agents to be tested within a single population.\nFor the experiments in Table 1 and subsection 5.1 we used the following 8 baseline agents:\n\u2022 An single-turn BR agent that assumes everyone else plays the BC policy.\n\u2022 An agent doing RM search with BC policy and value functions. We use 2 copies of this agent trained on different subsets of data. \u2022 DiL-piKL agent with BC policy and value functions. We use 4 different versions of this data with different training data and model architecture. \u2022 DiL-piKL agent where the policy and value functions are trained with self-play with Reinforced-PiKL with high lambda (\u03bb = 3 \u00d7 10 \u22122 ).\nFor the experiments in this paper we used 1000 games for each such population test.", "publication_ref": ["b1", "b19", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "E THEORETICAL PROPERTIES OF DIL-PIKL", "text": "In this section we study the last-iterate convergence of DiL-piKL, establishing that in two-player zero-sum games DiL-piKL converges to the (unique) Bayes-Nash equilibrium of the regularized Bayesian game. As a corollary (in the case in which each player has exactly one type), we conclude that piKL converges to the Nash equilibrium of the regularized game in two-player zero-sum games.\nWe start from a technical result. In all that follows, we will always let u t i be a shorthand for the vector (u i (a, a t \u2212i )) a\u2208Ai . Lemma 1. Fix any player i, \u03bb i \u2208 \u039b i , and t \u2265 1. For all \u03c0, \u03c0 \u2208 \u2206(A i ), the iterates \u03c0 t i,\u03bbi and \u03c0 t+1 i,\u03bbi defined in Line 8 of Algorithm 1 satisfy\n\u03b7 \u03b7\u03bb i t + 1 \u2212u t i + \u03bb i \u2207\u03c6(\u03c0 t i,\u03bbi ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ) + \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) \u2212 \u2207\u03c6(\u03c0 t i,\u03bbi ), \u03c0 \u2212 \u03c0 = 0.\nProof. If t = 1, then the results follows from direct inspection: \u03c0 1 i,\u03bbi is the uniform policy (and so \u2207\u03c6(\u03c0 1 i,\u03bbi ), \u03c0 \u2212\u03c0 = 0 for any \u03c0, \u03c0 \u2208 \u2206(A i ), and so the statement reduces to the first-order optimality conditions for the problem \u03c0 2 i,\u03bbi = arg max \u03c0\u2208\u2206(Ai) {\u2212\u03c6(\u03c0)/\u03b7 + u 1 i , \u03c0 \u2212\u03bb i D KL (\u03c0 \u03c4 i )}. So, we now focus on the case t \u2265 2. The iterates \u03c0 t+1 i,\u03bbi and \u03c0 t i,\u03bbi produced by DiL-piKL are respectively the solutions to the optimization problem\n\u03c0 t+1 i,\u03bbi = arg max \u03c0\u2208\u2206(Ai) \u2212 \u03c6(\u03c0) \u03b7t + \u016a t i , \u03c0 \u2212 \u03bb i D KL (\u03c0 \u03c4 i ) , \u03c0 t i,\u03bbi = arg max\nTaking the difference between the equalities, we find\n\u2212\u016a t i +\u016a t\u22121 i + \u03bb i + 1 \u03b7t \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) \u2212 \u03bb i + 1 \u03b7(t \u2212 1) \u2207\u03c6(\u03c0 t i,\u03bbi ), \u03c0 \u2212 \u03c0 = 0\nWe now use the fact that\u016a\nt i \u2212\u016a t\u22121 i = \u2212 1 t \u2212 1\u016a t i + 1 t \u2212 1 u t i . to further write 1 t \u2212 1 \u2212u t i +\u016a t i + \u03bb i + 1 \u03b7t \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) \u2212 \u03bb i + 1 \u03b7(t \u2212 1) \u2207\u03c6(\u03c0 t i,\u03bbi ), \u03c0 \u2212 \u03c0 = 0 (7)\nFrom Equation ( 6) we find\n\u016a t i , \u03c0 \u2212 \u03c0 = \u03bb i \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ) + 1 \u03b7t \u2207\u03c6(\u03c0 t+1 i,\u03bbi\n), \u03c0 \u2212 \u03c0 and so, plugging back the previous relationship in Equation ( 7) we can write, for all \u03c0, \u03c0 \u2208 \u2206(A i ),\n0 = 1 t \u2212 1 \u2212u t i + \u03bb i \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ) + 1 \u03b7t \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) + \u03bb i + 1 \u03b7t \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) \u2212 \u03bb i + 1 \u03b7(t \u2212 1) \u2207\u03c6(\u03c0 t i,\u03bbi ), \u03c0 \u2212 \u03c0 = 1 t \u2212 1 \u2212u t i + \u03bb i \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ) + \u03bb i + 1 \u03b7(t \u2212 1) \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) \u2212 \u03bb i + 1 \u03b7(t \u2212 1) \u2207\u03c6(\u03c0 t i,\u03bbi ), \u03c0 \u2212 \u03c0 = 1 t \u2212 1 \u2212u t i + \u03bb i \u2207\u03c6(\u03c0 t i,\u03bbi ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ) + \u03b7\u03bb i t + 1 \u03b7(t \u2212 1) \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) \u2212 \u03b7\u03bb i t + 1 \u03b7(t \u2212 1) \u2207\u03c6(\u03c0 t i,\u03bbi ), \u03c0 \u2212 \u03c0 .\nDividing by (\u03b7\u03bb i t + 1)/(\u03b7(t \u2212 1)) yields the statement.\nCorollary 1. Fix any player i, \u03bb i \u2208 \u039b i , and t \u2265 1. For all \u03c0 \u2208 \u2206(A i ), the iterates \u03c0 t i,\u03bbi and \u03c0 t+1 i,\u03bbi defined in Line 8 of Algorithm 1 satisfy\n\u2212u t i + \u03bb i \u2207\u03c6(\u03c0 t i,\u03bbi ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ), \u03c0 \u2212 \u03c0 t+1 i,\u03bbi = \u03bb i t + 1 \u03b7 D KL (\u03c0 \u03c0 t+1 i,\u03bbi ) \u2212 D KL (\u03c0 \u03c0 t i,\u03bbi ) + D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) .\nProof. Since Lemma 1 holds for all \u03c0, \u03c0 \u2208 \u2206(A i ), we can in particular set \u03c0 = \u03c0 t+1 i,\u03bbi , and obtain\n\u03b7 \u03b7\u03bb i t + 1 \u2212u t i + \u03bb i \u2207\u03c6(\u03c0 t i,\u03bbi ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ), \u03c0 \u2212 \u03c0 t+1 i,\u03bbi + \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) \u2212 \u2207\u03c6(\u03c0 t i,\u03bbi ), \u03c0 \u2212 \u03c0 t+1 i,\u03bbi = 0. (8) Using the three-point identity \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) \u2212 \u2207\u03c6(\u03c0 t i,\u03bbi ), \u03c0 \u2212 \u03c0 t+1 i,\u03bbi = D KL (\u03c0 \u03c0 t i,\u03bbi ) \u2212 D KL (\u03c0 \u03c0 t+1 i,\u03bbi ) \u2212 D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) in Equation (8) yields D KL (\u03c0 \u03c0 t+1 i,\u03bbi ) = D KL (\u03c0 \u03c0 t i,\u03bbi ) \u2212 D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) + \u03b7 \u03b7\u03bb i t + 1 \u2212u t i + \u03bb i \u2207\u03c6(\u03c0 t i,\u03bbi ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ), \u03c0 \u2212 \u03c0 t+1 i,\u03bbi .\nMultiplying by \u03bb i t + 1/\u03b7 yields the statement.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.1 REGRET ANALYSIS", "text": "Let\u0169 t i,\u03bbi be the regularized utility of agent type \u03bb i \u2208 \u039b \u0129\nu t i,\u03bbi : \u2206(A i ) \u03c0 \u2192 u t i , \u03c0 \u2212 \u03bb i D KL (\u03c0 \u03c4 i ).\nObservation 1. We note the following:\n\u2022 For any i \u2208 {1, 2} and \u03bb i \u2208 \u039b i , the function\u0169 t i,\u03bbi satisfies\nu t i,\u03bbi (\u03c0) =\u0169 t i,\u03bbi (\u03c0 ) + \u2207\u0169 t i,\u03bbi (\u03c0 ), \u03c0 \u2212 \u03c0 \u2212 \u03bb i D KL (\u03c0 \u03c0 ) \u2200 \u03c0, \u03c0 \u2208 \u2206(A i ).\n\u2022 Furthermore,\n\u2212\u2207\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ) = \u2212u t i + \u03bb i \u2207\u03c6(\u03c0 t ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ).\nUsing Corollary 1 we have the following Lemma 2. For any player i and type \u03bb i \u2208 \u039b i ,\nu t i,\u03bbi (\u03c0) \u2212\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ) \u2264 u t i 2 \u221e 4\u03bb i t + 4/\u03b7 + \u03bb i D KL (\u03c0 t i,\u03bbi \u03c4 i ) \u2212 D KL (\u03c0 t+1 i,\u03bbi \u03c4 i ) \u2212 \u03bb i t + 1 \u03b7 D KL (\u03c0 \u03c0 t+1 i,\u03bbi ) + \u03bb i (t \u2212 1) + 1 \u03b7 D KL (\u03c0 \u03c0 t i,\u03bbi ).\nProof. From Lemma 1,\n0 = \u03bb i t + 1 \u03b7 \u2212 D KL (\u03c0 \u03c0 t+1 i,\u03bbi ) + D KL (\u03c0 \u03c0 t i,\u03bbi ) \u2212 D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) + \u2212\u2207\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ), \u03c0 \u2212 \u03c0 t+1 i,\u03bbi = \u03bb i t + 1 \u03b7 \u2212 D KL (\u03c0 \u03c0 t+1 i,\u03bbi ) + D KL (\u03c0 \u03c0 t i,\u03bbi ) \u2212 D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) + \u2207\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ), \u03c0 t i,\u03bbi + \u03c0 t+1 i,\u03bbi + \u2212\u2207\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ), \u03c0 \u2212 \u03c0 t i,\u03bbi = \u03bb i t + 1 \u03b7 \u2212 D KL (\u03c0 \u03c0 t+1 i,\u03bbi ) + D KL (\u03c0 \u03c0 t i,\u03bbi ) \u2212 D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) + \u2212\u2207\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ), \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi \u2212\u0169 t i,\u03bbi (\u03c0) +\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ) \u2212 \u03bb i D KL (\u03c0 \u03c0 t i,\u03bbi ).\nRearranging, we find\nu t i,\u03bbi (\u03c0) \u2212\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ) = \u2212 \u03bb i t + 1 \u03b7 D KL (\u03c0 \u03c0 t+1 i,\u03bbi ) + \u03bb i (t \u2212 1) + 1 \u03b7 D KL (\u03c0 \u03c0 t i,\u03bbi ) \u2212 \u03bb i t + 1 \u03b7 D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) + \u2212\u2207\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ), \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi(9)\n.\n(10)\nWe now upper bound the term in (9) using convexity of the function \u03c0 \u2192 D KL (\u03c0 \u03c4 i ), as follows:\n\u2212\u2207\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ), \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi = \u2212u t i , \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi + \u03bb i \u2207\u03c6(\u03c0 t i,\u03bbi ) \u2212 \u2207\u03c6(\u03c4 i ), \u03c0 t+1 i,\u03bbi \u2212 \u03c0 t i,\u03bbi \u2264 \u2212u t i , \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi + \u03bb i D KL (\u03c0 t+1 i,\u03bbi \u03c4 i ) \u2212 D KL (\u03c0 t i,\u03bbi \u03c4 i ) .\nSubstituting the above bound into (10) yields\nu t i,\u03bbi (\u03c0) \u2212\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ) \u2264 \u2212u t i , \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi \u2212 \u03bb i t + 1 \u03b7 D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) + \u03bb i D KL (\u03c0 t i,\u03bbi \u03c4 i ) \u2212 D KL (\u03c0 t+1 i,\u03bbi \u03c4 i ) \u2212 \u03bb i t + 1 \u03b7 D KL (\u03c0 \u03c0 t+1 i,\u03bbi ) + \u03bb i (t \u2212 1) + 1 \u03b7 D KL (\u03c0 \u03c0 t i,\u03bbi ) \u2264 u t i 2 \u221e 4\u03bb i t + 4/\u03b7 + \u03bb i t + 1 \u03b7 \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi 2 1 \u2212 \u03bb i t + 1 \u03b7 D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) + \u03bb i D KL (\u03c0 t i,\u03bbi \u03c4 i ) \u2212 D KL (\u03c0 t+1 i,\u03bbi \u03c4 i ) \u2212 \u03bb i t + 1 \u03b7 D KL (\u03c0 \u03c0 t+1 i,\u03bbi ) + \u03bb i (t \u2212 1) + 1 \u03b7 D KL (\u03c0 \u03c0 t i,\u03bbi ),\nwhere the second inequality follows from Young's inequality. Finally, by using the strong convexity of the KL divergence between points \u03c0 t i,\u03bbi and \u03c0 t+1 i,\u03bbi , that is,\nD KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) \u2265 \u03c0 t+1 i,\u03bbi \u2212 \u03c0 t i,\u03bbi2\n1 , yields the statement.\nNoting that the right-hand side of Lemma 2 is telescopic, we immediately have the following. Theorem 3. For any player i and type \u03bb i \u2208 \u039b i , and policy \u03c0 \u2208 \u2206(A i ), the following regret bound holds at all times T :\nT t=1\u0169 t i,\u03bbi (\u03c0) \u2212\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ) \u2264 W 2 4 min 2 log T \u03bb i , T \u03b7 + log n i \u03b7 + \u03bb i (log n i + Q i ).\nProof. From Lemma 2 we have that\nT t=1\u0169 t i,\u03bbi (\u03c0) \u2212\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ) \u2264 W 2 4 T t=1 1 \u03bb i t + 1/\u03b7 + \u03bb i D KL (\u03c0 1 i,\u03bbi \u03c4 i ) + D KL (\u03c0 \u03c0 1 i,\u03bbi ) \u03b7 \u2264 W 2 4 T t=1 min 1 \u03bb i t , \u03b7 + \u03bb i (log n i + Q i ) + log n i \u03b7 \u2264 W 2 4 min 2 log T \u03bb i , \u03b7T + \u03bb i (log n i + Q i ) + log n i \u03b7 ,\nwhere the second inequality follows from the fact that \u03bb i t + 1/\u03b7 \u2265 max{\u03bb i t, 1/\u03b7} and the fact that \u03c0 1 i,\u03bbi is the uniform strategy.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.2 LAST-ITERATE CONVERGENCE IN TWO-PLAYER ZERO-SUM GAMES", "text": "In two-player game with payoff matrix A for Player 1, a Bayes-Nash equilibrium to the regularized game is a collection of policies (\u03c0 * i,\u03bbi ) such that for any supported type \u03bb i of Player i \u2208 {1, 2}, the policy \u03c0 * i,\u03bbi is a best response to the average policy of the opponent. In symbols,\n\u03c0 * 1,\u03bb1 \u2208 arg max \u03c0\u2208\u2206(A1) A E \u03bb2\u223c\u03b22 \u03c0 * 2,\u03bb2 , \u03c0 + \u03bb 1 D KL (\u03c0 \u03c4 1 ) \u2200 \u03bb 1 \u2208 \u039b 1 , \u03c0 * 2,\u03bb2 \u2208 arg max \u03c0\u2208\u2206(A2) \u2212A E \u03bb1\u223c\u03b21 \u03c0 * 1,\u03bb1 , \u03c0 + \u03bb 2 D KL (\u03c0 \u03c4 2 ) \u2200 \u03bb 2 \u2208 \u039b 2 .\nDenoting\u03c0 * 1 := E \u03bb1\u223c\u03b21 \u03c0 * 1,\u03bb1 ,\u03c0 * 2 := E \u03bb2\u223c\u03b22 \u03c0 * 2,\u03bb2 , the first-order optimality conditions for the best response problems above are\nA\u03c0 * 2 + \u03bb 1 \u2207\u03c6(\u03c0 * 1,\u03bb1 ) \u2212 \u03bb 1 \u2207\u03c6(\u03c4 1 ), \u03c0 * 1,\u03bb1 \u2212 \u03c0 1,\u03bb1 \u2265 0 \u2200 \u03c0 1,\u03bb1 \u2208 \u2206(A 1 ), \u2212A \u03c0 * 1 + \u03bb 2 \u2207\u03c6(\u03c0 * 2,\u03bb2 ) \u2212 \u03bb 2 \u2207\u03c6(\u03c4 2 ), \u03c0 * 2,\u03bb2 \u2212 \u03c0 2,\u03bb2 \u2265 0 \u2200 \u03c0 2,\u03bb2 \u2208 \u2206(A 2 ).\nWe also mention the following standard lemma. Lemma 3. Let (\u03c0 * i,\u03bbi ) i\u2208{1,2},\u03bb1\u2208\u039bi be the unique Bayes-Nash equilibrium of the regularized game. Let policies \u03c0 i,\u03bbi be arbitrary, and let:\n\u2022\u03c0 1 := E \u03bb1\u223c\u03b21 \u03c0 1,\u03bb1 ,\u03c0 2 := E \u03bb2\u223c\u03b22 \u03c0 2,\u03bb2 ; \u2022 \u03b1 := E \u03bb1\u223c\u03b21 \u2212A\u03c0 2 + \u03bb 1 \u2207\u03c6(\u03c0 1,\u03bb1 ) \u2212 \u03bb 1 \u2207\u03c6(\u03c4 1 ), \u03c0 * 1,\u03bb1 \u2212 \u03c0 1,\u03bb1 ; \u2022 \u03b2 := E \u03bb2\u223c\u03b22 A \u03c0 1 + \u03bb 2 \u2207\u03c6(\u03c0 2,\u03bb2 ) \u2212 \u03bb 2 \u2207\u03c6(\u03c4 2 ), \u03c0 * 2,\u03bb2 \u2212 \u03c0 2,\u03bb2 .\nThen,\n\u03b1 + \u03b2 \u2264 \u2212 i\u2208{1,2} E \u03bbi\u223c\u03b2i \u03bb i D KL (\u03c0 i,\u03bbi \u03c0 * i,\u03bbi ) + \u03bb i D KL (\u03c0 * i,\u03bbi \u03c0 i,\u03bbi ) .\nThe following potential function will be key in the analysis:\n\u03a8 t := i\u2208{1,2} E \u03bbi\u223c\u03b2i \u03bb i (t \u2212 1) + 1 \u03b7 D KL (\u03c0 * i,\u03bbi \u03c0 t i,\u03bbi ) + \u03bb i D KL (\u03c0 t i,\u03bbi \u03c4 i ) , t \u2208 {1, 2, . . . }.\nProposition 1. At all times t \u2208 {1, 2, . . . }, let\n\u03c0 t \u2212i := E \u03bb\u2212i\u223c\u03b2\u2212i \u03c0 t \u2212i,\u03bb\u2212i .\nThe potential \u03a8 t satisfies the inequality\n\u03a8 t+1 \u2264 \u03a8 t + i\u2208{1,2} E \u03bbi\u223c\u03b2i u t i 2 \u221e 4\u03bb i t + 4/\u03b7 + A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi.\nProof. By multiplying both sides of Corollary 1 for the choice \u03c0 = \u03c0 * i,\u03bbi , taking expectations over \u03bb i \u223c \u03b2 i , and summing over the player i \u2208 {1, 2}, we find\ni\u2208{1,2} E \u03bbi\u223c\u03b2i \u03bb i t + 1 \u03b7 D KL (\u03c0 * i,\u03bbi \u03c0 t+1 i,\u03bbi ) = i\u2208{1,2} E \u03bbi\u223c\u03b2i \u03bb i t + 1 \u03b7 D KL (\u03c0 * i,\u03bbi \u03c0 t i,\u03bbi ) \u2212 i\u2208{1,2} E \u03bbi\u223c\u03b2i \u03bb i t + 1 \u03b7 D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) + i\u2208{1,2} E \u03bbi\u223c\u03b2i \u2212u t i + \u03bb i \u2207\u03c6(\u03c0 t i,\u03bbi ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ), \u03c0 * i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi(\u2663)\n. ( 11)\nWe now proceed to analyze the last summation on the right-hand side. First,\n(\u2663) = i\u2208{1,2} E \u03bbi\u223c\u03b2i \u2212A i\u03c0 t \u2212i + \u03bb i \u2207\u03c6(\u03c0 t i,\u03bbi ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ), \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi (\u2660) + i\u2208{1,2} E \u03bbi\u223c\u03b2i \u2212u t i + \u03bb i \u2207\u03c6(\u03c0 t i,\u03bbi ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ), \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi (\u2665) + i\u2208{1,2} E \u03bbi\u223c\u03b2i A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi . (12\n)\nUsing Lemma 3 we can immediately write\n(\u2660) \u2264 i\u2208{1,2} E \u03bbi\u223c\u03b2i \u2212\u03bb i D KL (\u03c0 * i,\u03bbi \u03c0 t i,\u03bbi ) .\nBy manipulating the inner product in (\u2665), we have\n(\u2665) = i\u2208{1,2} E \u03bbi\u223c\u03b2i \u2212u t i , \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi \u2212 \u03bb i \u2207\u03c6(\u03c0 t i,\u03bbi ) \u2212 \u03c6(\u03c0 t+1 i,\u03bbi ), \u03c0 t+1 i,\u03bbi \u2212 \u03c0 t i,\u03bbi \u2264 i\u2208{1,2} E \u03bbi\u223c\u03b2i \u2212u t i , \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi + \u03bb i D KL (\u03c0 t+1 i,\u03bbi \u03c4 i ) \u2212 D KL (\u03c0 t i,\u03bbi \u03c4 i ) \u2264 i\u2208{1,2} E \u03bbi\u223c\u03b2i u t i 2 \u221e 4\u03bb i t + 4/\u03b7 + \u03bb i t + 1 \u03b7 \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi 2 1 + i\u2208{1,2} E \u03bbi\u223c\u03b2i \u03bb i D KL (\u03c0 t+1 i,\u03bbi \u03c4 i ) \u2212 D KL (\u03c0 t i,\u03bbi \u03c4 i ) ,\nwhere the last inequality follow from the fact that ab \u2264 a 2 /(4\u03c1) + \u03c1b 2 for all choices of a, b \u2265 0 and \u03c1 > 0. Substituting the individual bounds into ( 12) yields\n(\u2663) \u2264 i\u2208{1,2} E \u03bbi\u223c\u03b2i \u03bb i D KL (\u03c0 t+1 i,\u03bbi \u03c4 i ) \u2212 D KL (\u03c0 t i,\u03bbi \u03c4 i ) + i\u2208{1,2} E \u03bbi\u223c\u03b2i u t i 2 \u221e 4\u03bb i t + 4/\u03b7 + \u03bb i t + 1 \u03b7 \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi 2 1 + i\u2208{1,2} E \u03bbi\u223c\u03b2i A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi .\nFinally, plugging the above bound into (11) and rearranging terms yields\n\u03a8 t+1 \u2264 \u03a8 t + i\u2208{1,2} E \u03bbi\u223c\u03b2i \u2212 \u03bb i t + 1 \u03b7 D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) + i\u2208{1,2} E \u03bbi\u223c\u03b2i u t i 2 \u221e 4\u03bb i t + 4/\u03b7 + \u03bb i t + 1 \u03b7 \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi 2 1 + i\u2208{1,2} E \u03bbi\u223c\u03b2i A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi . \u2264 \u03a8 t + i\u2208{1,2} E \u03bbi\u223c\u03b2i \u2212 \u03bb i t + 1 \u03b7 \u03c0 t+1 i,\u03bbi \u2212 \u03c0 t i,\u03bbi 2 1 + i\u2208{1,2} E \u03bbi\u223c\u03b2i u t i 2 \u221e 4\u03bb i t + 4/\u03b7 + \u03bb i t + 1 \u03b7 \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi 2 1 + i\u2208{1,2} E \u03bbi\u223c\u03b2i A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi . \u2264 \u03a8 t + i\u2208{1,2} E \u03bbi\u223c\u03b2i u t i 2 \u221e 4\u03bb i t + 4/\u03b7 + A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi\n, as we wanted to show.\nTheorem 4. As in Proposition 1, let\u03c0\nt \u2212i := E \u03bb\u2212i\u223c\u03b2\u2212i \u03c0 t \u2212i,\u03bb\u2212i .\nLetD T KL be the notion of distance defined as\nD T KL := i\u2208{1,2} E \u03bbi\u223c\u03b2i (\u03bb i + \u03ba T \u22121 )D KL (\u03c0 * i,\u03bbi \u03c0 T i,\u03bbi ) .\nAt all times T = 2, 3, . . . ,\nD T KL \u2264 1 T \uf8eb \uf8ed \u03c1 + log n i \u03b7 + W 2 2 i\u2208{1,2} E \u03bbi\u223c\u03b2i min 2 log T \u03bb i , \u03b7T \uf8f6 \uf8f8 + 2 T T t=1 i\u2208{1,2} E \u03bbi\u223c\u03b2i A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi ,where\n\u03c1 := 2 i\u2208{1,2} E \u03bbi\u223c\u03b2i [\u03bb i ] (log n i + Q i ).\nProof. Using the bound on \u03a8 t+1 \u2212 \u03a8 t given by Proposition 1 we obtain\n\u03a8 T \u2212 \u03a8 1 = T \u22121 t=1 (\u03a8 t+1 \u2212 \u03a8 t ) \u2264 T \u22121 t=1 i\u2208{1,2} E \u03bbi\u223c\u03b2i u t i 2 \u221e 4\u03bb i t + 4/\u03b7 + A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi = 1 4 i\u2208{1,2} E \u03bbi\u223c\u03b2i T t=1 u t i 2 \u221e \u03bb i t + 1/\u03b7 + T t=1 i\u2208{1,2} E \u03bbi\u223c\u03b2i A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi \u2264 1 4 i\u2208{1,2} E \u03bbi\u223c\u03b2i T \u22121 t=1 W 2 \u03bb i t + 1/\u03b7 + T t=1 i\u2208{1,2} E \u03bbi\u223c\u03b2i A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi .\nWe can now bound\nT t=1 W 2 \u03bb i t + 1/\u03b7 \u2264 W 2 T t=1 min 1 \u03bb i t , \u03b7 \u2264 W 2 min T t=1 1 \u03bb i t , T t=1 \u03b7 \u2264 W 2 min 2 log T \u03bb i , T \u03b7 .\nOn the other hand, note that\n\u03a8 T \u2212 \u03a8 1 = \u2212\u03a8 1 + i\u2208{1,2} E \u03bbi\u223c\u03b2i \u03bb i (T \u2212 1) + 1 \u03b7 D KL (\u03c0 * i,\u03bbi \u03c0 T i,\u03bbi ) + \u03bb i D KL (\u03c0 T i,\u03bbi \u03c4 i ) \u2265 \u2212\u03a8 1 + i\u2208{1,2} (T \u2212 1) E \u03bbi\u223c\u03b2i (\u03bb i + \u03ba T \u22121 )D KL (\u03c0 * i,\u03bbi \u03c0 T i,\u03bbi ) = (T \u2212 1)D T KL \u2212 i\u2208{1,2} E \u03bbi\u223c\u03b2i D KL (\u03c0 * i,\u03bbi \u03c0 1 i,\u03bbi ) \u03b7 \u2212 \u03bb i D KL (\u03c0 1 i,\u03bbi \u03c4 i ) \u2265 (T \u2212 1)D T KL \u2212 i\u2208{1,2} E \u03bbi\u223c\u03b2i log n i \u03b7 + \u03bb i (log n i + Q i ) = (T \u2212 1)D T KL \u2212 \u03c1,\nwhere the last inequality follows from expanding the definition of the KL divergence and using the fact that \u03c0 1 i,\u03bbi is the uniform strategy. Combining the inequalities and dividing by T \u2212 1 yields\nD T KL \u2264 W 2 4 i\u2208{1,2} E \u03bbi\u223c\u03b2i min 2 log T (T \u2212 1)\u03bb i , T T \u2212 1 \u03b7 + \u03c1 T \u2212 1 + 1 T \u2212 1 T t=1 i\u2208{1,2} E \u03bbi\u223c\u03b2i A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi .\nFinally, using the fact that 2(T \u2212 1) \u2265 T yields the statement.\nTheorem 5 (Last-iterate convergence of DiL-piKL in two-player zero-sum games). Let \u03c1 be as in the statement of Theorem 4. When both players in a zero-sum game learn using DiL-piKL for T iterations, their policies converge to the unique Bayes-Nash equilibrium (\u03c0 * 1 , \u03c0 * 2 ) of the regularized game defined by utilities (4), in the following senses:\n(a) In expectation: for all i \u2208 {1, 2} and \u03bb i \u2208 \u039b i , at a rate of roughly log T /(\u03bb i T )\nE D KL (\u03c0 * i,\u03bbi \u03c0 T i,\u03bbi ) \u2264 1 \u03bb i T \uf8eb \uf8ed \u03c1 + log n i \u03b7 + W 2 2 j\u2208{1,2} E \u03bbj \u223c\u03b2j min 2 log T \u03bb j , \u03b7T \uf8f6 \uf8f8 .\n(We remark that for \u03b7 = 1/ \u221a T the convergence is never slower than 1/ \u221a T ).\n(b) With high probability, at a rate of roughly 1/ \u221a T : for any \u03b4 \u2208 (0, 1) and Player i \u2208 {1, 2},\nP \u2200 \u03bb i \u2208 \u039b i : D KL (\u03c0 * i,\u03bbi \u03c0 T i,\u03bbi ) \u2264 E D KL (\u03c0 * i,\u03bbi \u03c0 T i,\u03bbi ) + 8 \u221a 2 W \u03bb i \u221a T log |\u039b i | \u03b4 \u2265 1 \u2212 \u03b4.\nA n upper bound on E D KL (\u03c0 * i,\u03bbi \u03c0 T i,\u03bbi ) was given in the previous point.\n(c) Almost surely in the limit:\nP \u2200 \u03bb i \u2208 \u039b i : D KL (\u03c0 * i,\u03bbi \u03c0 T i,\u03bbi ) T \u2192+\u221e \u2212 \u2212\u2212\u2212\u2212 \u2192 0 = 1 \u2200i \u2208 {1, 2}.\nProof. We prove the three statements incrementally.\n(a) Let F t be the \u03c3-algebra generated by {u t\ni | t = 1, . . . , t \u2212 1, i \u2208 {1, 2}}. We let E t [ \u2022 ] := E[ \u2022 | F t ].\nSince piKL is a deterministic algorithm, \u03c0 t i,\u03bbi is F t -measurable. Hence, given that u t i is an unbiased estimator of A i\u03c0 t \u2212i we have that at all times t\nE t A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi = E t A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi = 0. (13\n)\nNote that from the definition ofD T KL given in Theorem 4\nD KL (\u03c0 * i,\u03bbi \u03c0 T i,\u03bbi ) \u2264 1 \u03bb iD T KL .(14)\nHence, taking expectations and using (13) yields the statement.\n(b) To prove high-probability convergence, we use the Azuma-Hoeffding concentration inequality. In particular, (13) shows that the stochastic process\n\uf8eb \uf8ed j\u2208{1,2} E \u03bbj \u223c\u03b2j A j\u03c0 t \u2212j \u2212 u t j , \u03c0 * j \u2212 \u03c0 t j \uf8f6 \uf8f8 t=1,2,...\nis a martingale difference sequence adapted to the filtration F t . Furthermore, note that\nj\u2208{1,2} E \u03bbj \u223c\u03b2j A j\u03c0 t \u2212j \u2212 u t j , \u03c0 * j,\u03bbj \u2212 \u03c0 t j,\u03bbj \u2264 4W\nfor all t. Hence, using the Azuma-Hoeffding inequality for martingale difference sequences we obtain that for all \u03b4 \u2208 (0, 1),\nP \uf8ee \uf8f0 T t=1 j\u2208{1,2} E \u03bbj \u223c\u03b2j A j\u03c0 t \u2212j \u2212 u t j , \u03c0 * j \u2212 \u03c0 t j \u2264 4W 2T log 1 \u03b4 \uf8f9 \uf8fb \u2265 1 \u2212 \u03b4.\nPlugging the above probability bound in the statement of Theorem 4 and using the union bound over \u03bb i \u2208 \u039b i yields the statement.\n(c) follows from (b) via a standard application of the Borel-Cantelli lemma.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F MODEL ARCHITECTURE", "text": "Our model architecture closely resembles the architecture used in past work on no-press Diplomacy (Bakhtin et al., 2021;Jacob et al., 2022;Anthony et al., 2020;Paquette et al., 2019;Gray et al., 2020). One-hot (7 players), or all zero 7 Land/coast/water One-hot 3 Supply center owner One-hot (7 players), or all zero 8 Home center One-hot (7 players), or all zero 7   Given a gamestate, to construct the input to the model, for each of the 81 possible locations and/or special coastal areas on the board that a unit can occupy, we encode the 38 feature channels described in Table 3 for that location. We also encode the previous board state in this way, as well using an encoding of the order history as described in Gray et al. (2020) provides an additional 202 channels per board location indicating the prior orders at that location.\nSeparately, we also encode per-player and global features of the gamestate into additional tensors (Table 4,Table 5). Each of these tensors (per-location, per-player, global) is passed through a linear layer with 224 output channels, and then all three are concatenated to a single (81+7+1) x 224 tensor. Thereafter, following Bakhtin et al. (2021), we apply a learnable positional bias and pass the result to a to a standard transformer encoder architecture with 10 layers, channel width 224, 8 dot-product-self-attention heads per layer, and GeLU activation.\nFinally we decode the policy via the same LSTM decoder head as Gray et al. (2020), and predict the game values of all 7 players using a value head that applies softmax attention to the encoder output, followed by a linear layer with 224 channels, GeLU activation, a linear layer with 7 channels, and a softmax. See Figure 4 for a graphical diagram of the model.", "publication_ref": ["b1", "b19", "b0", "b26", "b13", "b13", "b1", "b13"], "figure_ref": ["fig_2"], "table_ref": ["tab_4", "tab_5", "tab_6"]}, {"heading": "G HUMAN IMITATION ANCHOR POLICY TRAINING", "text": "Similar to prior work (Bakhtin et al., 2021;Jacob et al., 2022;Gray et al., 2020), to obtain a human imitation anchor policy with which to use for piKL regularization and to initialize the RL policy, we train the architecture described in Appendix F on a dataset of roughly 46000 online Diplomacy games provided by webdiplomacy.net. We train jointly on both games with full-press Diplomacy (i.e. where players were able to communicate via messages) and no-press Diplomacy and at inference time and/or during RL, condition the relevant global feature in Table 5 to indicate the model should predict for no-press Diplomacy. Also in common with the same prior work, we apply data filtering to skip training on actions where players missed the time limit and a default null action was inputted by the website, and to only train on actions played by the top half of rated players. We also adopt the method of Jacob et al. (2022) to augment the data by permuting the labels of the 7 players randomly during training, since the game's rules are fully equivariant to such permutations. See Table 6 for a list of other hyperparameters.   Bakhtin et al. (2021), described in detail in Section 2.3. The overall self-play procedure (see Figure 5), the training data recorded, loss function used on that data, and sampling methods we use are all the same. The differences are:\n\u2022 Although our model architecture is largely identical to that of past work, some minor details, including the precise encoding of input features, and the construction of the value head are different, see Appendix F for description of our architecture.\n\u2022 During RL training, we initialize the RL policy proposal and value functions from the human IL anchor policy and value function (Appendix G) instead of randomly from scratch, and during training, rather than using regret matching to compute the 1-step equilibrium \u03c3 on each turn of the game, we use DiL-piKL. The distribution of \u03bb and the human IL anchor policy remain fixed through all of training.\n\u2022 During training, the action chosen to explore in the self-play game uses a randomly chosen \u03bb from the DiL-piKL distribution. Similarly, the RL policy is trained to predict the policy of a random \u03bb. This ensures that the RL policy, when used at test time to propose actions, samples both human IL-like actions from high \u03bb, as well as more optimized actions from lower \u03bb, and that gamestates resulting from the entire range of possible \u03bb are in-distribution for the RL policy and value models.\n\u2022 Unlike DORA, double-oracle action exploration is not used during training. We found that with the additional diversity and regularization of the human anchor policy, it was unnecessary.\n\u2022 All models were also trained with the same stochastic game-end rules we used in evaluation games against human players described in Section 5.1.\nIn Diplomacy, on the 200 games of the human tournament in which we evaluated Diplodocus and other agents, the empirical fitted b i values for the 7 different starting countries in the game are displayed in Table 8.  ", "publication_ref": ["b1", "b19", "b13", "b19", "b1"], "figure_ref": [], "table_ref": ["tab_6", "tab_8", "tab_10"]}, {"heading": "", "text": "where we let the averages utility vectors b\u0113\nSince the regularizing function negative entropy \u03c6 is Legendre, the policies \u03c0 t+1 i,\u03bbi and \u03c0 t i,\u03bbi are in the relative interior of the probability simplex, and therefore the first-order optimality conditions for \u03c0 t+1 i,\u03bbi and \u03c0 t i,\u03bbi are respectively  \u2022 Some hyperparameters we use may be different than that of past work. See Appendix H.1 for a list of hyperparameters.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "H.1 HYPER-PARAMETERS FOR RL TRAINING", "text": "For the evaluation in this paper we trained Diplodocus and BRBot agents and re-trained DNVI, DNVI-NPU, and DORA agents. We provide the hyper-parameters used in table 7. We show parameters out of 3 agents from Bakhtin et al. (2021) as they are the same.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "H.2 INFERENCE TIME EFFECT OF DIL-PIKL", "text": "In Figure 6 we show that running DiL-piKL at evaluation time alone is not enough to get the demonstrated performance improvement in population scores. Using DiL-piKL on top of human imitationlearned policy/value functions does not improve the population score compared to Hedge. However, applying this search method on top of policies/values that were trained via RL with DiL-piKL results in significant improvement in the scores.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "I BAYES-ELO", "text": "BayesElo (Coulom, 2005) models each player's expected share of the total score in a 2-player game as proportional to:\nwhere r i is the Elo rating of player i, b 1 and b 2 are the advantage/disadvantage of playing first/second in Elo, s(i) \u2208 1, 2 indicates whether i played first or second, and c = 400 log 10 (e) is a fixed scaling constant that adjusts for the particular arbitrary numerical scale of ratings expected by users, in  particular that 400 points in Elo systems generally corresponds to a 10-fold increase in expected winning odds or expected average score.. It then finds joint maximum-a-posteriori values r i , b i given all observed data and an optional prior to regularize the model. In some cases, the biases b i may also be hardcoded or provided as parameters rather than inferred from the data, in our work we infer them. In our application, we use a weak Bayesian prior such that each player's rating was a-priori normally distributed around 0 with a standard deviation of around 350 Elo.\nBayesElo generalizes naturally to more than 2 players simply by allowing i and s(i) to range over {1, ..., n} rather than {1, 2}, and we straightforwardly apply this to Diplomacy. Since there are 7 players, we similarly jointly fit b 1 ,...b 7 on the data to model the asymmetric advantage/disadvantage of the 7 different starting positions. Computed Elo ratings closely reflect empirical winning percentages of players in a given population, but also take into account variability in the strength of opposition in a game, and the starting advantage/disadvantage b i . For example, if a player achieved a high average score but was abnormally lucky in drawing advantageous starting countries across all their games, then the model would likely estimate a lower rating than if they achieved the same results with more difficult starting countries.", "publication_ref": ["b9"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Learning to play no-press diplomacy with best response policy iteration", "journal": "Curran Associates, Inc", "year": "2020", "authors": "Thomas Anthony; Tom Eccles; Andrea Tacchetti; J\u00e1nos Kram\u00e1r; Ian Gemp; Thomas Hudson; Nicolas Porcel; Marc Lanctot; Julien Perolat; Richard Everett; Satinder Singh"}, {"ref_id": "b1", "title": "No-press diplomacy from scratch", "journal": "", "year": "2021", "authors": "Anton Bakhtin; David Wu; Adam Lerer; Noam Brown"}, {"ref_id": "b2", "title": "Dota 2 with large scale deep reinforcement learning", "journal": "", "year": "2019", "authors": "Christopher Berner; Greg Brockman; Brooke Chan; Vicki Cheung; Przemys\u0142aw Debiak; Christy Dennison; David Farhi; Quirin Fischer; Shariq Hashme; Chris Hesse"}, {"ref_id": "b3", "title": "An analog of the minimax theorem for vector payoffs", "journal": "Pacific Journal of Mathematics", "year": "1956", "authors": "David Blackwell"}, {"ref_id": "b4", "title": "Heads-up limit hold'em poker is solved", "journal": "Science", "year": "2015", "authors": "Michael Bowling; Neil Burch; Michael Johanson; Oskari Tammelin"}, {"ref_id": "b5", "title": "Iterative solution of games by fictitious play. Activity analysis of production and allocation", "journal": "", "year": "1951", "authors": "W George;  Brown"}, {"ref_id": "b6", "title": "Superhuman AI for heads-up no-limit poker: Libratus beats top professionals", "journal": "Science", "year": "2017", "authors": "Noam Brown; Tuomas Sandholm"}, {"ref_id": "b7", "title": "Superhuman AI for multiplayer poker", "journal": "Science", "year": "2019", "authors": "Noam Brown; Tuomas Sandholm"}, {"ref_id": "b8", "title": "Dynamic thresholding and pruning for regret minimization", "journal": "", "year": "2017", "authors": "Noam Brown; Christian Kroer; Tuomas Sandholm"}, {"ref_id": "b9", "title": "", "journal": "", "year": "2005", "authors": "R\u00e9mi Coulom;  Bayeselo"}, {"ref_id": "b10", "title": "K-level reasoning for zero-shot coordination in hanabi", "journal": "", "year": "2021", "authors": "Brandon Cui; Hengyuan Hu; Luis Pineda; Jakob Foerster"}, {"ref_id": "b11", "title": "To whom tribute is due: The next step in scoring systems", "journal": "", "year": "2020", "authors": "Brandon Fogel"}, {"ref_id": "b12", "title": "A decision-theoretic generalization of on-line learning and an application to boosting", "journal": "Journal of computer and system sciences", "year": "1997", "authors": "Yoav Freund; Robert E Schapire"}, {"ref_id": "b13", "title": "Human-level performance in nopress diplomacy via equilibrium search", "journal": "", "year": "2020", "authors": "Jonathan Gray; Adam Lerer; Anton Bakhtin; Noam Brown"}, {"ref_id": "b14", "title": "A simple adaptive procedure leading to correlated equilibrium", "journal": "Econometrica", "year": "2000", "authors": "Sergiu Hart; Andreu Mas-Colell"}, {"ref_id": "b15", "title": "other-play\" for zero-shot coordination", "journal": "PMLR", "year": "2020", "authors": "Hengyuan Hu; Adam Lerer; Alex Peysakhovich; Jakob Foerster"}, {"ref_id": "b16", "title": "Off-belief learning", "journal": "", "year": "2021", "authors": "Hengyuan Hu; Adam Lerer; Brandon Cui; Luis Pineda; David Wu; Noam Brown; Jakob Foerster"}, {"ref_id": "b17", "title": "Nash q-learning for general-sum stochastic games", "journal": "Journal of machine learning research", "year": "2003-11", "authors": "Junling Hu;  Michael P Wellman"}, {"ref_id": "b18", "title": "Mm algorithms for generalized bradley-terry models. The annals of statistics", "journal": "", "year": "2004", "authors": "David R Hunter"}, {"ref_id": "b19", "title": "Jacob Andreas, and Noam Brown. Modeling strong and human-like gameplay with kl-regularized search", "journal": "PMLR", "year": "2022", "authors": "Athul Paul Jacob; David J Wu; Gabriele Farina; Adam Lerer; Hengyuan Hu"}, {"ref_id": "b20", "title": "Learning existing social conventions via observationally augmented self-play", "journal": "ACM", "year": "2019", "authors": "Adam Lerer; Alexander Peysakhovich"}, {"ref_id": "b21", "title": "The weighted majority algorithm. Information and computation", "journal": "", "year": "1994", "authors": "Nick Littlestone; Manfred K Warmuth"}, {"ref_id": "b22", "title": "Planning in the presence of cost functions controlled by an adversary", "journal": "", "year": "2003", "authors": "Brendan Mcmahan; Geoffrey Gordon; Avrim Blum"}, {"ref_id": "b23", "title": "Overcoming exploration in reinforcement learning with demonstrations", "journal": "IEEE", "year": "2018", "authors": "Ashvin Nair; Bob Mcgrew; Marcin Andrychowicz; Wojciech Zaremba; Pieter Abbeel"}, {"ref_id": "b24", "title": "Accelerating online reinforcement learning with offline datasets", "journal": "", "year": "2020", "authors": "Ashvin Nair; Murtaza Dalal; Abhishek Gupta; Sergey Levine"}, {"ref_id": "b25", "title": "Zur theorie der gesellschaftsspiele", "journal": "Mathematische annalen", "year": "1928", "authors": " J V Neumann"}, {"ref_id": "b26", "title": "No-press diplomacy: Modeling multi-agent gameplay", "journal": "", "year": "2019", "authors": "Philip Paquette; Yuchen Lu; Seton Steven Bocco; Max Smith; O-G Satya; Jonathan K Kummerfeld; Joelle Pineau; Satinder Singh; Aaron C Courville"}, {"ref_id": "b27", "title": "Stochastic games", "journal": "", "year": "1953", "authors": "S Lloyd;  Shapley"}, {"ref_id": "b28", "title": "Keep doing what worked: Behavioral modelling priors for offline reinforcement learning", "journal": "", "year": "2020", "authors": "Y Noah; Jost Tobias Siegel; Felix Springenberg; Abbas Berkenkamp; Michael Abdolmaleki; Thomas Neunert; Roland Lampe; Nicolas Hafner; Martin Heess;  Riedmiller"}, {"ref_id": "b29", "title": "Mastering the game of go without human knowledge", "journal": "Nature", "year": "2017", "authors": "David Silver; Julian Schrittwieser; Karen Simonyan; Ioannis Antonoglou; Aja Huang; Arthur Guez; Thomas Hubert; Lucas Baker; Matthew Lai; Adrian Bolton"}], "figures": [{"figure_label": "3", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 3 :3Figure 3: Performance of different agents as a function of the number of RL training steps. Left: Scores against 6 human-like HedgeBot agents. The gray dotted line at score 1/7 \u2248 14.3% corresponds to tying HedgeBot. The error bars show one standard error. Right: Order prediction accuracy of each agent's raw RL policy on a held-out set of human games. The gray dotted line corresponds to the behavioral cloning policy. Overall: Diplodocus-High achieves a high score while also maintaining high prediction accuracy. Unregularized agents DNVI and DORA do far worse on both metrics.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 33Figure 3 compares different RL agents across the course of training. To simplify the comparison, we vary the training methods for the value and policy proposal networks, but use the same search setting at evaluation time.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Model architecture used for policy/value learning in no-press Diplomacy.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Performance of four different agents in a population of human players, ranked by Elo, among all 43", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140-1144, 2018. Ho Chit Siu, Jaime Pe\u00f1a, Edenna Chen, Yutai Zhou, Victor Lopez, Kyle Palko, Kimberlee Chang, and Ross Allen. Evaluation of human-ai teams for learned and rule-based agents in hanabi. Advances in Neural Information Processing Systems, 34:16183-16195, 2021. DJ Strouse, Kevin McKee, Matt Botvinick, Edward Hughes, and Richard Everett. Collaborating with humans without human data. Advances in Neural Information Processing Systems, 34: 14502-14515, 2021.", "figure_data": "Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Juny-oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmasterlevel in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. 1989.A AUTHOR CONTRIBUTIONS"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Per-location board state input features", "figure_data": "FeatureType Number of ChannelsNumber of builds allowed during winter Float1"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Per-player board state input features", "figure_data": "FeatureTypeChannelsSeason (spring/fall/winter)One-hot3Year (encoded as (y \u2212 1901)/10) Float1Game has dialogue?Binary1Scoring system usedOne-hot2"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Global board state input features", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Hyper-parameter values used to train the IL anchor policy on human data.", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "For each starting country, the empirical advantage/disadvantage of starting as that country measured in Elo rating equivalent units, fitted jointly with all players' Elo ratings on the 200 Diplomacy games of the tournament. The values roughly agree with common opinions among Diplomacy players, particularly that France is one of the best starting countries in no-press, while Austria and England are among the weaker starts.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "V (s) \u2190 (1 \u2212 \u03b1)V (s) + \u03b1(r + \u03b3 a \u03c3(a )V (f (s, a ))) (1", "formula_coordinates": [4.0, 190.79, 150.18, 309.34, 20.09]}, {"formula_id": "formula_1", "formula_text": ")", "formula_coordinates": [4.0, 500.13, 150.52, 3.87, 8.64]}, {"formula_id": "formula_2", "formula_text": "ValueLoss(\u03b8 v ) = 1 2 V (s; \u03b8 v ) \u2212 r(s) \u2212 \u03b3 a \u03c3(a )V f (s, a );\u03b8 v 2 (2)", "formula_coordinates": [4.0, 153.59, 268.15, 350.41, 32.99]}, {"formula_id": "formula_3", "formula_text": "PolicyLoss(\u03b8 \u03c0 ) = \u2212 i ai\u2208Ai \u03c3 i (a) log \u03c0 i (s, a i ; \u03b8 \u03c0 ).", "formula_coordinates": [4.0, 193.88, 409.93, 214.28, 20.06]}, {"formula_id": "formula_4", "formula_text": "i,\u03bbi (\u03c0 i , \u03c0 \u2212i ) := u i (\u03c0 i , \u03c0 \u2212i ) \u2212 \u03bb i D KL (\u03c0 i \u03c4 i )(4)", "formula_coordinates": [4.0, 212.2, 719.42, 291.8, 10.0]}, {"formula_id": "formula_5", "formula_text": "1 function INITIALIZE() 2 t \u2190 0 3 for each action a i \u2208 A i do 4 Q 0 i (a i ) \u2190 0 5 function PLAY() 6 t \u2190 t + 1 7 sample \u03bb \u223c \u03b2 i 8 let \u03c0 t i,\u03bb", "formula_coordinates": [5.0, 111.03, 170.99, 113.9, 88.39]}, {"formula_id": "formula_6", "formula_text": "\u03c0 t i,\u03bb (a i ) \u221d exp Q t\u22121 (a i ) + \u03bb log \u03c4 i (a i ) \u03ba t\u22121 + \u03bb 9 sample an action a t i \u223c \u03c0 t i,\u03bb", "formula_coordinates": [5.0, 111.39, 266.31, 176.6, 38.17]}, {"formula_id": "formula_7", "formula_text": "11 for each a i \u2208 A i do 12 Q t (a i ) \u2190 t\u22121 t Q t\u22121 (a i ) + 1 t u i (a i , a t \u2212i )", "formula_coordinates": [5.0, 108.7, 327.06, 172.96, 21.03]}, {"formula_id": "formula_8", "formula_text": "\u03c0 t i (a i ) \u221d exp Q t\u22121 (a i ) + \u03bb log \u03c4 i (a i ) \u03ba t\u22121 + \u03bb (5)", "formula_coordinates": [5.0, 220.7, 444.21, 283.3, 24.8]}, {"formula_id": "formula_9", "formula_text": "max \u03c0\u2208\u2206(Ai) T t=1\u0169 i,\u03bbi (\u03c0, a t \u2212i ) \u2212\u0169 i,\u03bbi (\u03c0 t i,\u03bbi , a t \u2212i ) \u2264 W 2 4 min 2 log T \u03bb i , T \u03b7 + log n i \u03b7 + \u03c1 i,\u03bbi ,", "formula_coordinates": [6.0, 114.36, 321.15, 383.28, 30.2]}, {"formula_id": "formula_10", "formula_text": "n i + Q i ).", "formula_coordinates": [6.0, 344.71, 363.74, 39.03, 9.65]}, {"formula_id": "formula_11", "formula_text": "\u03c0 T i,\u03bbi := 1 T T t=1 \u03c0 t i,\u03bbi of each player i is a C log T T", "formula_coordinates": [6.0, 108.0, 504.8, 204.31, 14.56]}, {"formula_id": "formula_12", "formula_text": "\u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 DORA 32 -20 13% \u00b1 3% 50 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 Human 43 -187 1% \u00b1 1% 7", "formula_coordinates": [9.0, 183.57, 189.95, 242.38, 41.83]}, {"formula_id": "formula_13", "formula_text": "C 2 i j\u2208N C 2 j", "formula_coordinates": [12.0, 228.54, 692.68, 25.76, 18.43]}, {"formula_id": "formula_14", "formula_text": "\u03b7 \u03b7\u03bb i t + 1 \u2212u t i + \u03bb i \u2207\u03c6(\u03c0 t i,\u03bbi ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ) + \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) \u2212 \u2207\u03c6(\u03c0 t i,\u03bbi ), \u03c0 \u2212 \u03c0 = 0.", "formula_coordinates": [17.0, 134.98, 396.2, 350.7, 23.23]}, {"formula_id": "formula_15", "formula_text": "\u03c0 t+1 i,\u03bbi = arg max \u03c0\u2208\u2206(Ai) \u2212 \u03c6(\u03c0) \u03b7t + \u016a t i , \u03c0 \u2212 \u03bb i D KL (\u03c0 \u03c4 i ) , \u03c0 t i,\u03bbi = arg max", "formula_coordinates": [17.0, 176.65, 503.25, 235.55, 47.36]}, {"formula_id": "formula_16", "formula_text": "\u2212\u016a t i +\u016a t\u22121 i + \u03bb i + 1 \u03b7t \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) \u2212 \u03bb i + 1 \u03b7(t \u2212 1) \u2207\u03c6(\u03c0 t i,\u03bbi ), \u03c0 \u2212 \u03c0 = 0", "formula_coordinates": [18.0, 137.88, 99.46, 343.72, 22.31]}, {"formula_id": "formula_17", "formula_text": "t i \u2212\u016a t\u22121 i = \u2212 1 t \u2212 1\u016a t i + 1 t \u2212 1 u t i . to further write 1 t \u2212 1 \u2212u t i +\u016a t i + \u03bb i + 1 \u03b7t \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) \u2212 \u03bb i + 1 \u03b7(t \u2212 1) \u2207\u03c6(\u03c0 t i,\u03bbi ), \u03c0 \u2212 \u03c0 = 0 (7)", "formula_coordinates": [18.0, 108.0, 144.16, 396.0, 60.51]}, {"formula_id": "formula_18", "formula_text": "\u016a t i , \u03c0 \u2212 \u03c0 = \u03bb i \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ) + 1 \u03b7t \u2207\u03c6(\u03c0 t+1 i,\u03bbi", "formula_coordinates": [18.0, 169.28, 224.38, 234.12, 22.31]}, {"formula_id": "formula_19", "formula_text": "0 = 1 t \u2212 1 \u2212u t i + \u03bb i \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ) + 1 \u03b7t \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) + \u03bb i + 1 \u03b7t \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) \u2212 \u03bb i + 1 \u03b7(t \u2212 1) \u2207\u03c6(\u03c0 t i,\u03bbi ), \u03c0 \u2212 \u03c0 = 1 t \u2212 1 \u2212u t i + \u03bb i \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ) + \u03bb i + 1 \u03b7(t \u2212 1) \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) \u2212 \u03bb i + 1 \u03b7(t \u2212 1) \u2207\u03c6(\u03c0 t i,\u03bbi ), \u03c0 \u2212 \u03c0 = 1 t \u2212 1 \u2212u t i + \u03bb i \u2207\u03c6(\u03c0 t i,\u03bbi ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ) + \u03b7\u03bb i t + 1 \u03b7(t \u2212 1) \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) \u2212 \u03b7\u03bb i t + 1 \u03b7(t \u2212 1) \u2207\u03c6(\u03c0 t i,\u03bbi ), \u03c0 \u2212 \u03c0 .", "formula_coordinates": [18.0, 108.0, 267.43, 392.95, 178.8]}, {"formula_id": "formula_20", "formula_text": "\u2212u t i + \u03bb i \u2207\u03c6(\u03c0 t i,\u03bbi ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ), \u03c0 \u2212 \u03c0 t+1 i,\u03bbi = \u03bb i t + 1 \u03b7 D KL (\u03c0 \u03c0 t+1 i,\u03bbi ) \u2212 D KL (\u03c0 \u03c0 t i,\u03bbi ) + D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) .", "formula_coordinates": [18.0, 124.85, 499.09, 368.39, 48.37]}, {"formula_id": "formula_21", "formula_text": "\u03b7 \u03b7\u03bb i t + 1 \u2212u t i + \u03bb i \u2207\u03c6(\u03c0 t i,\u03bbi ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ), \u03c0 \u2212 \u03c0 t+1 i,\u03bbi + \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) \u2212 \u2207\u03c6(\u03c0 t i,\u03bbi ), \u03c0 \u2212 \u03c0 t+1 i,\u03bbi = 0. (8) Using the three-point identity \u2207\u03c6(\u03c0 t+1 i,\u03bbi ) \u2212 \u2207\u03c6(\u03c0 t i,\u03bbi ), \u03c0 \u2212 \u03c0 t+1 i,\u03bbi = D KL (\u03c0 \u03c0 t i,\u03bbi ) \u2212 D KL (\u03c0 \u03c0 t+1 i,\u03bbi ) \u2212 D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) in Equation (8) yields D KL (\u03c0 \u03c0 t+1 i,\u03bbi ) = D KL (\u03c0 \u03c0 t i,\u03bbi ) \u2212 D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) + \u03b7 \u03b7\u03bb i t + 1 \u2212u t i + \u03bb i \u2207\u03c6(\u03c0 t i,\u03bbi ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ), \u03c0 \u2212 \u03c0 t+1 i,\u03bbi .", "formula_coordinates": [18.0, 108.0, 577.38, 396.0, 141.34]}, {"formula_id": "formula_22", "formula_text": "u t i,\u03bbi : \u2206(A i ) \u03c0 \u2192 u t i , \u03c0 \u2212 \u03bb i D KL (\u03c0 \u03c4 i ).", "formula_coordinates": [19.0, 207.94, 132.09, 196.11, 12.69]}, {"formula_id": "formula_23", "formula_text": "u t i,\u03bbi (\u03c0) =\u0169 t i,\u03bbi (\u03c0 ) + \u2207\u0169 t i,\u03bbi (\u03c0 ), \u03c0 \u2212 \u03c0 \u2212 \u03bb i D KL (\u03c0 \u03c0 ) \u2200 \u03c0, \u03c0 \u2208 \u2206(A i ).", "formula_coordinates": [19.0, 149.05, 205.4, 349.76, 12.69]}, {"formula_id": "formula_24", "formula_text": "\u2212\u2207\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ) = \u2212u t i + \u03bb i \u2207\u03c6(\u03c0 t ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ).", "formula_coordinates": [19.0, 222.29, 258.36, 203.29, 12.69]}, {"formula_id": "formula_25", "formula_text": "u t i,\u03bbi (\u03c0) \u2212\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ) \u2264 u t i 2 \u221e 4\u03bb i t + 4/\u03b7 + \u03bb i D KL (\u03c0 t i,\u03bbi \u03c4 i ) \u2212 D KL (\u03c0 t+1 i,\u03bbi \u03c4 i ) \u2212 \u03bb i t + 1 \u03b7 D KL (\u03c0 \u03c0 t+1 i,\u03bbi ) + \u03bb i (t \u2212 1) + 1 \u03b7 D KL (\u03c0 \u03c0 t i,\u03bbi ).", "formula_coordinates": [19.0, 115.22, 329.76, 381.57, 51.64]}, {"formula_id": "formula_26", "formula_text": "0 = \u03bb i t + 1 \u03b7 \u2212 D KL (\u03c0 \u03c0 t+1 i,\u03bbi ) + D KL (\u03c0 \u03c0 t i,\u03bbi ) \u2212 D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) + \u2212\u2207\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ), \u03c0 \u2212 \u03c0 t+1 i,\u03bbi = \u03bb i t + 1 \u03b7 \u2212 D KL (\u03c0 \u03c0 t+1 i,\u03bbi ) + D KL (\u03c0 \u03c0 t i,\u03bbi ) \u2212 D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) + \u2207\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ), \u03c0 t i,\u03bbi + \u03c0 t+1 i,\u03bbi + \u2212\u2207\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ), \u03c0 \u2212 \u03c0 t i,\u03bbi = \u03bb i t + 1 \u03b7 \u2212 D KL (\u03c0 \u03c0 t+1 i,\u03bbi ) + D KL (\u03c0 \u03c0 t i,\u03bbi ) \u2212 D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) + \u2212\u2207\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ), \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi \u2212\u0169 t i,\u03bbi (\u03c0) +\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ) \u2212 \u03bb i D KL (\u03c0 \u03c0 t i,\u03bbi ).", "formula_coordinates": [19.0, 108.0, 437.43, 441.52, 113.0]}, {"formula_id": "formula_27", "formula_text": "u t i,\u03bbi (\u03c0) \u2212\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ) = \u2212 \u03bb i t + 1 \u03b7 D KL (\u03c0 \u03c0 t+1 i,\u03bbi ) + \u03bb i (t \u2212 1) + 1 \u03b7 D KL (\u03c0 \u03c0 t i,\u03bbi ) \u2212 \u03bb i t + 1 \u03b7 D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) + \u2212\u2207\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ), \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi(9)", "formula_coordinates": [19.0, 108.0, 587.36, 395.18, 60.39]}, {"formula_id": "formula_28", "formula_text": "\u2212\u2207\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ), \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi = \u2212u t i , \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi + \u03bb i \u2207\u03c6(\u03c0 t i,\u03bbi ) \u2212 \u2207\u03c6(\u03c4 i ), \u03c0 t+1 i,\u03bbi \u2212 \u03c0 t i,\u03bbi \u2264 \u2212u t i , \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi + \u03bb i D KL (\u03c0 t+1 i,\u03bbi \u03c4 i ) \u2212 D KL (\u03c0 t i,\u03bbi \u03c4 i ) .", "formula_coordinates": [19.0, 111.87, 697.84, 399.76, 33.19]}, {"formula_id": "formula_29", "formula_text": "u t i,\u03bbi (\u03c0) \u2212\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ) \u2264 \u2212u t i , \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi \u2212 \u03bb i t + 1 \u03b7 D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) + \u03bb i D KL (\u03c0 t i,\u03bbi \u03c4 i ) \u2212 D KL (\u03c0 t+1 i,\u03bbi \u03c4 i ) \u2212 \u03bb i t + 1 \u03b7 D KL (\u03c0 \u03c0 t+1 i,\u03bbi ) + \u03bb i (t \u2212 1) + 1 \u03b7 D KL (\u03c0 \u03c0 t i,\u03bbi ) \u2264 u t i 2 \u221e 4\u03bb i t + 4/\u03b7 + \u03bb i t + 1 \u03b7 \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi 2 1 \u2212 \u03bb i t + 1 \u03b7 D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) + \u03bb i D KL (\u03c0 t i,\u03bbi \u03c4 i ) \u2212 D KL (\u03c0 t+1 i,\u03bbi \u03c4 i ) \u2212 \u03bb i t + 1 \u03b7 D KL (\u03c0 \u03c0 t+1 i,\u03bbi ) + \u03bb i (t \u2212 1) + 1 \u03b7 D KL (\u03c0 \u03c0 t i,\u03bbi ),", "formula_coordinates": [20.0, 108.0, 99.26, 418.13, 150.24]}, {"formula_id": "formula_30", "formula_text": "D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) \u2265 \u03c0 t+1 i,\u03bbi \u2212 \u03c0 t i,\u03bbi2", "formula_coordinates": [20.0, 225.64, 280.81, 157.46, 13.38]}, {"formula_id": "formula_31", "formula_text": "T t=1\u0169 t i,\u03bbi (\u03c0) \u2212\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ) \u2264 W 2 4 min 2 log T \u03bb i , T \u03b7 + log n i \u03b7 + \u03bb i (log n i + Q i ).", "formula_coordinates": [20.0, 137.12, 358.91, 338.4, 30.2]}, {"formula_id": "formula_32", "formula_text": "T t=1\u0169 t i,\u03bbi (\u03c0) \u2212\u0169 t i,\u03bbi (\u03c0 t i,\u03bbi ) \u2264 W 2 4 T t=1 1 \u03bb i t + 1/\u03b7 + \u03bb i D KL (\u03c0 1 i,\u03bbi \u03c4 i ) + D KL (\u03c0 \u03c0 1 i,\u03bbi ) \u03b7 \u2264 W 2 4 T t=1 min 1 \u03bb i t , \u03b7 + \u03bb i (log n i + Q i ) + log n i \u03b7 \u2264 W 2 4 min 2 log T \u03bb i , \u03b7T + \u03bb i (log n i + Q i ) + log n i \u03b7 ,", "formula_coordinates": [20.0, 116.31, 416.6, 378.84, 93.19]}, {"formula_id": "formula_33", "formula_text": "\u03c0 * 1,\u03bb1 \u2208 arg max \u03c0\u2208\u2206(A1) A E \u03bb2\u223c\u03b22 \u03c0 * 2,\u03bb2 , \u03c0 + \u03bb 1 D KL (\u03c0 \u03c4 1 ) \u2200 \u03bb 1 \u2208 \u039b 1 , \u03c0 * 2,\u03bb2 \u2208 arg max \u03c0\u2208\u2206(A2) \u2212A E \u03bb1\u223c\u03b21 \u03c0 * 1,\u03bb1 , \u03c0 + \u03bb 2 D KL (\u03c0 \u03c4 2 ) \u2200 \u03bb 2 \u2208 \u039b 2 .", "formula_coordinates": [20.0, 139.31, 611.4, 333.39, 46.07]}, {"formula_id": "formula_34", "formula_text": "A\u03c0 * 2 + \u03bb 1 \u2207\u03c6(\u03c0 * 1,\u03bb1 ) \u2212 \u03bb 1 \u2207\u03c6(\u03c4 1 ), \u03c0 * 1,\u03bb1 \u2212 \u03c0 1,\u03bb1 \u2265 0 \u2200 \u03c0 1,\u03bb1 \u2208 \u2206(A 1 ), \u2212A \u03c0 * 1 + \u03bb 2 \u2207\u03c6(\u03c0 * 2,\u03bb2 ) \u2212 \u03bb 2 \u2207\u03c6(\u03c4 2 ), \u03c0 * 2,\u03bb2 \u2212 \u03c0 2,\u03bb2 \u2265 0 \u2200 \u03c0 2,\u03bb2 \u2208 \u2206(A 2 ).", "formula_coordinates": [20.0, 141.5, 700.41, 332.88, 29.45]}, {"formula_id": "formula_35", "formula_text": "\u2022\u03c0 1 := E \u03bb1\u223c\u03b21 \u03c0 1,\u03bb1 ,\u03c0 2 := E \u03bb2\u223c\u03b22 \u03c0 2,\u03bb2 ; \u2022 \u03b1 := E \u03bb1\u223c\u03b21 \u2212A\u03c0 2 + \u03bb 1 \u2207\u03c6(\u03c0 1,\u03bb1 ) \u2212 \u03bb 1 \u2207\u03c6(\u03c4 1 ), \u03c0 * 1,\u03bb1 \u2212 \u03c0 1,\u03bb1 ; \u2022 \u03b2 := E \u03bb2\u223c\u03b22 A \u03c0 1 + \u03bb 2 \u2207\u03c6(\u03c0 2,\u03bb2 ) \u2212 \u03bb 2 \u2207\u03c6(\u03c4 2 ), \u03c0 * 2,\u03bb2 \u2212 \u03c0 2,\u03bb2 .", "formula_coordinates": [21.0, 135.4, 138.79, 283.12, 65.67]}, {"formula_id": "formula_36", "formula_text": "\u03b1 + \u03b2 \u2264 \u2212 i\u2208{1,2} E \u03bbi\u223c\u03b2i \u03bb i D KL (\u03c0 i,\u03bbi \u03c0 * i,\u03bbi ) + \u03bb i D KL (\u03c0 * i,\u03bbi \u03c0 i,\u03bbi ) .", "formula_coordinates": [21.0, 158.27, 233.92, 295.45, 22.6]}, {"formula_id": "formula_37", "formula_text": "\u03a8 t := i\u2208{1,2} E \u03bbi\u223c\u03b2i \u03bb i (t \u2212 1) + 1 \u03b7 D KL (\u03c0 * i,\u03bbi \u03c0 t i,\u03bbi ) + \u03bb i D KL (\u03c0 t i,\u03bbi \u03c4 i ) , t \u2208 {1, 2, . . . }.", "formula_coordinates": [21.0, 108.0, 289.46, 401.78, 27.27]}, {"formula_id": "formula_38", "formula_text": "\u03c0 t \u2212i := E \u03bb\u2212i\u223c\u03b2\u2212i \u03c0 t \u2212i,\u03bb\u2212i .", "formula_coordinates": [21.0, 251.44, 346.38, 109.13, 16.73]}, {"formula_id": "formula_39", "formula_text": "\u03a8 t+1 \u2264 \u03a8 t + i\u2208{1,2} E \u03bbi\u223c\u03b2i u t i 2 \u221e 4\u03bb i t + 4/\u03b7 + A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi.", "formula_coordinates": [21.0, 156.1, 390.52, 299.8, 30.9]}, {"formula_id": "formula_40", "formula_text": "i\u2208{1,2} E \u03bbi\u223c\u03b2i \u03bb i t + 1 \u03b7 D KL (\u03c0 * i,\u03bbi \u03c0 t+1 i,\u03bbi ) = i\u2208{1,2} E \u03bbi\u223c\u03b2i \u03bb i t + 1 \u03b7 D KL (\u03c0 * i,\u03bbi \u03c0 t i,\u03bbi ) \u2212 i\u2208{1,2} E \u03bbi\u223c\u03b2i \u03bb i t + 1 \u03b7 D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) + i\u2208{1,2} E \u03bbi\u223c\u03b2i \u2212u t i + \u03bb i \u2207\u03c6(\u03c0 t i,\u03bbi ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ), \u03c0 * i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi(\u2663)", "formula_coordinates": [21.0, 117.92, 467.5, 377.28, 118.67]}, {"formula_id": "formula_41", "formula_text": "(\u2663) = i\u2208{1,2} E \u03bbi\u223c\u03b2i \u2212A i\u03c0 t \u2212i + \u03bb i \u2207\u03c6(\u03c0 t i,\u03bbi ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ), \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi (\u2660) + i\u2208{1,2} E \u03bbi\u223c\u03b2i \u2212u t i + \u03bb i \u2207\u03c6(\u03c0 t i,\u03bbi ) \u2212 \u03bb i \u2207\u03c6(\u03c4 i ), \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi (\u2665) + i\u2208{1,2} E \u03bbi\u223c\u03b2i A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi . (12", "formula_coordinates": [21.0, 132.92, 613.82, 366.93, 114.65]}, {"formula_id": "formula_42", "formula_text": ")", "formula_coordinates": [21.0, 499.85, 708.27, 4.15, 8.64]}, {"formula_id": "formula_43", "formula_text": "(\u2660) \u2264 i\u2208{1,2} E \u03bbi\u223c\u03b2i \u2212\u03bb i D KL (\u03c0 * i,\u03bbi \u03c0 t i,\u03bbi ) .", "formula_coordinates": [22.0, 214.87, 103.79, 182.26, 22.6]}, {"formula_id": "formula_44", "formula_text": "(\u2665) = i\u2208{1,2} E \u03bbi\u223c\u03b2i \u2212u t i , \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi \u2212 \u03bb i \u2207\u03c6(\u03c0 t i,\u03bbi ) \u2212 \u03c6(\u03c0 t+1 i,\u03bbi ), \u03c0 t+1 i,\u03bbi \u2212 \u03c0 t i,\u03bbi \u2264 i\u2208{1,2} E \u03bbi\u223c\u03b2i \u2212u t i , \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi + \u03bb i D KL (\u03c0 t+1 i,\u03bbi \u03c4 i ) \u2212 D KL (\u03c0 t i,\u03bbi \u03c4 i ) \u2264 i\u2208{1,2} E \u03bbi\u223c\u03b2i u t i 2 \u221e 4\u03bb i t + 4/\u03b7 + \u03bb i t + 1 \u03b7 \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi 2 1 + i\u2208{1,2} E \u03bbi\u223c\u03b2i \u03bb i D KL (\u03c0 t+1 i,\u03bbi \u03c4 i ) \u2212 D KL (\u03c0 t i,\u03bbi \u03c4 i ) ,", "formula_coordinates": [22.0, 130.56, 155.73, 345.36, 120.38]}, {"formula_id": "formula_45", "formula_text": "(\u2663) \u2264 i\u2208{1,2} E \u03bbi\u223c\u03b2i \u03bb i D KL (\u03c0 t+1 i,\u03bbi \u03c4 i ) \u2212 D KL (\u03c0 t i,\u03bbi \u03c4 i ) + i\u2208{1,2} E \u03bbi\u223c\u03b2i u t i 2 \u221e 4\u03bb i t + 4/\u03b7 + \u03bb i t + 1 \u03b7 \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi 2 1 + i\u2208{1,2} E \u03bbi\u223c\u03b2i A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi .", "formula_coordinates": [22.0, 152.69, 317.6, 300.32, 88.83]}, {"formula_id": "formula_46", "formula_text": "\u03a8 t+1 \u2264 \u03a8 t + i\u2208{1,2} E \u03bbi\u223c\u03b2i \u2212 \u03bb i t + 1 \u03b7 D KL (\u03c0 t+1 i,\u03bbi \u03c0 t i,\u03bbi ) + i\u2208{1,2} E \u03bbi\u223c\u03b2i u t i 2 \u221e 4\u03bb i t + 4/\u03b7 + \u03bb i t + 1 \u03b7 \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi 2 1 + i\u2208{1,2} E \u03bbi\u223c\u03b2i A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi . \u2264 \u03a8 t + i\u2208{1,2} E \u03bbi\u223c\u03b2i \u2212 \u03bb i t + 1 \u03b7 \u03c0 t+1 i,\u03bbi \u2212 \u03c0 t i,\u03bbi 2 1 + i\u2208{1,2} E \u03bbi\u223c\u03b2i u t i 2 \u221e 4\u03bb i t + 4/\u03b7 + \u03bb i t + 1 \u03b7 \u03c0 t i,\u03bbi \u2212 \u03c0 t+1 i,\u03bbi 2 1 + i\u2208{1,2} E \u03bbi\u223c\u03b2i A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi . \u2264 \u03a8 t + i\u2208{1,2} E \u03bbi\u223c\u03b2i u t i 2 \u221e 4\u03bb i t + 4/\u03b7 + A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi", "formula_coordinates": [22.0, 149.21, 434.13, 307.28, 229.51]}, {"formula_id": "formula_47", "formula_text": "t \u2212i := E \u03bb\u2212i\u223c\u03b2\u2212i \u03c0 t \u2212i,\u03bb\u2212i .", "formula_coordinates": [22.0, 258.23, 711.66, 102.33, 16.73]}, {"formula_id": "formula_48", "formula_text": "D T KL := i\u2208{1,2} E \u03bbi\u223c\u03b2i (\u03bb i + \u03ba T \u22121 )D KL (\u03c0 * i,\u03bbi \u03c0 T i,\u03bbi ) .", "formula_coordinates": [23.0, 193.6, 103.61, 224.8, 22.6]}, {"formula_id": "formula_49", "formula_text": "D T KL \u2264 1 T \uf8eb \uf8ed \u03c1 + log n i \u03b7 + W 2 2 i\u2208{1,2} E \u03bbi\u223c\u03b2i min 2 log T \u03bb i , \u03b7T \uf8f6 \uf8f8 + 2 T T t=1 i\u2208{1,2} E \u03bbi\u223c\u03b2i A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi ,where", "formula_coordinates": [23.0, 108.0, 151.28, 385.05, 89.38]}, {"formula_id": "formula_50", "formula_text": "\u03c1 := 2 i\u2208{1,2} E \u03bbi\u223c\u03b2i [\u03bb i ] (log n i + Q i ).", "formula_coordinates": [23.0, 230.01, 249.02, 151.98, 20.87]}, {"formula_id": "formula_51", "formula_text": "\u03a8 T \u2212 \u03a8 1 = T \u22121 t=1 (\u03a8 t+1 \u2212 \u03a8 t ) \u2264 T \u22121 t=1 i\u2208{1,2} E \u03bbi\u223c\u03b2i u t i 2 \u221e 4\u03bb i t + 4/\u03b7 + A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi = 1 4 i\u2208{1,2} E \u03bbi\u223c\u03b2i T t=1 u t i 2 \u221e \u03bb i t + 1/\u03b7 + T t=1 i\u2208{1,2} E \u03bbi\u223c\u03b2i A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi \u2264 1 4 i\u2208{1,2} E \u03bbi\u223c\u03b2i T \u22121 t=1 W 2 \u03bb i t + 1/\u03b7 + T t=1 i\u2208{1,2} E \u03bbi\u223c\u03b2i A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi .", "formula_coordinates": [23.0, 111.21, 303.9, 389.59, 140.39]}, {"formula_id": "formula_52", "formula_text": "T t=1 W 2 \u03bb i t + 1/\u03b7 \u2264 W 2 T t=1 min 1 \u03bb i t , \u03b7 \u2264 W 2 min T t=1 1 \u03bb i t , T t=1 \u03b7 \u2264 W 2 min 2 log T \u03bb i , T \u03b7 .", "formula_coordinates": [23.0, 217.44, 468.54, 170.19, 92.79]}, {"formula_id": "formula_53", "formula_text": "\u03a8 T \u2212 \u03a8 1 = \u2212\u03a8 1 + i\u2208{1,2} E \u03bbi\u223c\u03b2i \u03bb i (T \u2212 1) + 1 \u03b7 D KL (\u03c0 * i,\u03bbi \u03c0 T i,\u03bbi ) + \u03bb i D KL (\u03c0 T i,\u03bbi \u03c4 i ) \u2265 \u2212\u03a8 1 + i\u2208{1,2} (T \u2212 1) E \u03bbi\u223c\u03b2i (\u03bb i + \u03ba T \u22121 )D KL (\u03c0 * i,\u03bbi \u03c0 T i,\u03bbi ) = (T \u2212 1)D T KL \u2212 i\u2208{1,2} E \u03bbi\u223c\u03b2i D KL (\u03c0 * i,\u03bbi \u03c0 1 i,\u03bbi ) \u03b7 \u2212 \u03bb i D KL (\u03c0 1 i,\u03bbi \u03c4 i ) \u2265 (T \u2212 1)D T KL \u2212 i\u2208{1,2} E \u03bbi\u223c\u03b2i log n i \u03b7 + \u03bb i (log n i + Q i ) = (T \u2212 1)D T KL \u2212 \u03c1,", "formula_coordinates": [23.0, 113.78, 585.14, 379.18, 145.25]}, {"formula_id": "formula_54", "formula_text": "D T KL \u2264 W 2 4 i\u2208{1,2} E \u03bbi\u223c\u03b2i min 2 log T (T \u2212 1)\u03bb i , T T \u2212 1 \u03b7 + \u03c1 T \u2212 1 + 1 T \u2212 1 T t=1 i\u2208{1,2} E \u03bbi\u223c\u03b2i A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi .", "formula_coordinates": [24.0, 117.46, 112.69, 377.07, 66.15]}, {"formula_id": "formula_55", "formula_text": "E D KL (\u03c0 * i,\u03bbi \u03c0 T i,\u03bbi ) \u2264 1 \u03bb i T \uf8eb \uf8ed \u03c1 + log n i \u03b7 + W 2 2 j\u2208{1,2} E \u03bbj \u223c\u03b2j min 2 log T \u03bb j , \u03b7T \uf8f6 \uf8f8 .", "formula_coordinates": [24.0, 143.87, 272.95, 363.38, 34.15]}, {"formula_id": "formula_56", "formula_text": "P \u2200 \u03bb i \u2208 \u039b i : D KL (\u03c0 * i,\u03bbi \u03c0 T i,\u03bbi ) \u2264 E D KL (\u03c0 * i,\u03bbi \u03c0 T i,\u03bbi ) + 8 \u221a 2 W \u03bb i \u221a T log |\u039b i | \u03b4 \u2265 1 \u2212 \u03b4.", "formula_coordinates": [24.0, 143.87, 348.68, 374.23, 32.77]}, {"formula_id": "formula_57", "formula_text": "P \u2200 \u03bb i \u2208 \u039b i : D KL (\u03c0 * i,\u03bbi \u03c0 T i,\u03bbi ) T \u2192+\u221e \u2212 \u2212\u2212\u2212\u2212 \u2192 0 = 1 \u2200i \u2208 {1, 2}.", "formula_coordinates": [24.0, 188.01, 431.55, 271.84, 14.23]}, {"formula_id": "formula_58", "formula_text": "i | t = 1, . . . , t \u2212 1, i \u2208 {1, 2}}. We let E t [ \u2022 ] := E[ \u2022 | F t ].", "formula_coordinates": [24.0, 143.87, 486.35, 360.13, 20.96]}, {"formula_id": "formula_59", "formula_text": "E t A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi = E t A i\u03c0 t \u2212i \u2212 u t i , \u03c0 * i,\u03bbi \u2212 \u03c0 t i,\u03bbi = 0. (13", "formula_coordinates": [24.0, 162.5, 526.56, 337.36, 12.69]}, {"formula_id": "formula_60", "formula_text": ")", "formula_coordinates": [24.0, 499.85, 528.95, 4.15, 8.64]}, {"formula_id": "formula_61", "formula_text": "D KL (\u03c0 * i,\u03bbi \u03c0 T i,\u03bbi ) \u2264 1 \u03bb iD T KL .(14)", "formula_coordinates": [24.0, 261.99, 562.79, 242.01, 23.23]}, {"formula_id": "formula_62", "formula_text": "\uf8eb \uf8ed j\u2208{1,2} E \u03bbj \u223c\u03b2j A j\u03c0 t \u2212j \u2212 u t j , \u03c0 * j \u2212 \u03c0 t j \uf8f6 \uf8f8 t=1,2,...", "formula_coordinates": [24.0, 220.14, 635.31, 207.09, 37.72]}, {"formula_id": "formula_63", "formula_text": "j\u2208{1,2} E \u03bbj \u223c\u03b2j A j\u03c0 t \u2212j \u2212 u t j , \u03c0 * j,\u03bbj \u2212 \u03c0 t j,\u03bbj \u2264 4W", "formula_coordinates": [24.0, 218.46, 705.54, 212.87, 22.6]}, {"formula_id": "formula_64", "formula_text": "P \uf8ee \uf8f0 T t=1 j\u2208{1,2} E \u03bbj \u223c\u03b2j A j\u03c0 t \u2212j \u2212 u t j , \u03c0 * j \u2212 \u03c0 t j \u2264 4W 2T log 1 \u03b4 \uf8f9 \uf8fb \u2265 1 \u2212 \u03b4.", "formula_coordinates": [25.0, 169.73, 113.69, 308.4, 34.15]}], "doi": ""}