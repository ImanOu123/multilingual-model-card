{"title": "HIGH-DIMENSIONAL LIMIT THEOREMS FOR SGD: EFFECTIVE DYNAMICS AND CRITICAL SCALING", "authors": "Ben G\u00e9rard; Reza Arous; Aukosh Gheissari;  Jagannath", "pub_date": "2023-08-17", "abstract": "We study the scaling limits of stochastic gradient descent (SGD) with constant step-size in the high-dimensional regime. We prove limit theorems for the trajectories of summary statistics (i.e., finite-dimensional functions) of SGD as the dimension goes to infinity. Our approach allows one to choose the summary statistics that are tracked, the initialization, and the step-size. It yields both ballistic (ODE) and diffusive (SDE) limits, with the limit depending dramatically on the former choices. We show a critical scaling regime for the step-size, below which the effective ballistic dynamics matches gradient flow for the population loss, but at which, a new correction term appears which changes the phase diagram. About the fixed points of this effective dynamics, the corresponding diffusive limits can be quite complex and even degenerate. We demonstrate our approach on popular examples including estimation for spiked matrix and tensor models and classification via two-layer networks for binary and XOR-type Gaussian mixture models. These examples exhibit surprising phenomena including multimodal timescales to convergence as well as convergence to sub-optimal solutions with probability bounded away from zero from random (e.g., Gaussian) initializations. At the same time, we demonstrate the benefit of overparametrization by showing that the latter probability goes to zero as the second layer width grows.", "sections": [{"heading": "Introduction", "text": "Stochastic gradient descent (SGD) is the go-to method for large-scale optimization problems in modern data science. It is often used to train complex parametric models on high-dimensional data. Since its introduction in [62], there has been a tremendous amount of work in analyzing its evolution.\nIn fixed dimensions, the asymptotic theory of SGD, and stochastic approximations more broadly, is by now classical. There have been works on path-wise limit theorems, such as functional central limit theorems and even large deviations principles [62,49,46,39,26,13,25,11]. At the core of this line of work is the idea that in the limit where the step-size, or learning rate, tends to zero, the trajectory of SGD with a fixed loss function (appropriately rescaled in time) converges to the solution of gradient flow for the population loss with the same initialization. Recently there has been considerable interest in quantifying the rate of this trajectory-wise convergence to higher order, in terms of a diffusion approximation. Namely, there are many works developing asymptotic expansions of the trajectory in the learning rate [47,41,43,2,44]. Motivated by this, there is a rich line of work bounding the time to equilibrium for the associated diffusion approximation (as well as Langevin-type modifications) under uniform ellipticity assumptions [47,59,18,77]. There is also an interesting line of work obtaining PDE limits in the \"shallow network\" regime where the dimension of the parameter space diverges but the dimension of the data remains constant: see e.g., [50,63,19,69,4].\nIn recent years, there has been considerable interest in understanding the high-dimensional setting, where one is constrained in the amount of data or the run-time of the algorithm due to the highdimensional nature of the data and the complexity of the model being trained. In these regimes, one cannot simply take the learning rate to be arbitrarily small as this would force an unlimited sample size and run-time. This is a common issue in high-dimensional statistics and the standard analytic approach is to study regimes where the sample size scales with the dimension of the problem [74,75].\nFor SGD with constant learning rate, there has been recent progress on quantifying the dimension dependence of the sample complexity for various tasks on general (pseudo or quasi-) convex objectives [14,15,68,53,33,24] and special classes of non-convex objectives [31,71,6]. There has also been important work on scaling limits as the dimension tends to infinity for the specific problems of linear regression [76,55], Online PCA [76,42], and phase retrieval [71] from random starts, and teacherstudent networks [64,65,32,73] and two-layer networks for XOR Gaussian mixtures [60] from warm starts. We also note that the study of high-dimensional regimes of gradient descent and Langevin dynamics have a history from the statistical physics perspective, e.g., in [21,22,67,48,17,45].\nWe develop a unified approach to the scaling limits of SGD in high-dimensions with constant learning rate that allows us to understand a broad range of estimation tasks. One of course cannot develop a high-dimensional scaling limit for the full trajectory of SGD as the dimension of the underlying parameter space is growing. On the other hand, in practice, one is rarely interested in the full trajectory; instead one typically tracks the trajectory of various summary statistics of the algorithm's evolution, such as the loss, the amplitude of various weights, or correlations between the classifier and the ground truth (in a supervised setting). We show in Theorem 2.3 that under mild regularity assumptions, the evolution of these summary statistics converges as the dimension grows to the solution of a system of (possibly stochastic) differential equations. These effective dynamics depend dramatically on the initializations (warm vs. random or cold), the parameter regions in which one is developing the scaling limit, and the scaling of the step-size with the dimension.\nIn practice, SGD often exhibits two types of phases in training: ballistic phases where the summary statistics macroscopically change in value, and diffusive phases, where they fluctuate microscopically. (During training, the evolution can start with either, and can even alternate multiple times between these phases.) Our approach allows us to develop scaling limits for both types of phases.\nIn ballistic phases, the effective dynamics are given by an ordinary differential equation (ODE) and the finite-dimensional intuition that the summary statistics evolve under the gradient flow for the population loss is correct provided the (constant) learning rate is sufficiently small in the dimension. When the learning rate follows a certain critical scaling-matching scalings commonly used in the high-dimensional statistics literature-an additional correction term appears. At this critical scaling, the phase portrait deviates significantly from that of the population gradient flow. Furthermore, in microscopic neighborhoods of the fixed points of this ODE, the effective dynamics become diffusive and are given by SDEs which can exhibit a wide range of (possibly degenerate) behaviors. We note that the appearance of the correction term in the ballistic phase was first observed in the setting of teacher-student networks in [64,65] and very recently investigated in detail in [73].\nAs a simple, first example of the departure of the effective dynamics in the critical step-size regime from the classical perspective, we study estimation for spiked matrix and tensor models in Section 3. In these models, the effective dynamics are exactly solvable and when the step-size scales critically with the dimension, in the ballistic phase the dynamics have additional fixed points as compared to the population gradient flow. The stability of these fixed points exhibit sharp transitions at special signal-to-noise ratios. When initialized randomly, the SGD starts in a microscopic neighborhood of an uninformative such fixed point, within which its effective dynamics become diffusive and exhibit a sharp transition between mean-reverting and mean-repellent Ornstein-Uhlenbeck (OU) processes.\nTo demonstrate our approach on more complex classification tasks typically studied using neural networks, we study a Gaussian mixture model analogue of the classical XOR problem in Section 5. (The XOR problem is arguably the canonical example of a decision boundary requiring at least two-layers to represent [51].) Here we find that the natural summary statistics are 22 dimensional, and their (ballistic) effective dynamics exhibit a rich phenomenology between some 39 connected fixed point regions of varying topological dimension. Surprisingly, we find that if we initialize the weights of the network randomly (following a Gaussian distribution), then the algorithm will converge to a classifier with macroscopic generalization error with probability 29 /32 and then follow a degenerate diffusion. On the other hand, we demonstrate the benefit of overparametrization, showing that as the width of the second layer grows, the probability of ballistically converging to a Bayes optimal classifier goes to 1; this is a mathematically rigorous example of the lottery ticket hypothesis of [30].\nBefore delving into the XOR problem, we first analyze the classification of a two component Gaussian mixture model in Section 4. This task is of course best solved using a one-layer network i.e., logistic regression, but with a two-layer network it exhibits some similar phenomenologies to the XOR problem while being more amenable to finer analysis. Here, we again find that if with random initial weights, with probability 1/2 the SGD will first converge to a classifier with macroscopic generalization error, and then follow a degenerate diffusion in a microscopic neighborhood of that set of unstable fixed points. We demonstrate this both empirically for positive signal-to-noise ratio and theoretically in the limit where the SNR tends to zero after the dimension tends to infinity.\nWhile the above are a few examples that we are able to solve in detail for both their ballistic and diffusive limits, we expect our main theorem to be applicable and lend new insights into a host of other problems including SGD for finite-rank matrix and tensor PCA, and one and two-layer neural networks applied to mixtures of k-Gaussians for fixed k \u2265 2. We leave this to future investigation. In this paper, we only consider the simplest variant of SGD, namely online SGD; we leave other variants involving batching and re-use to future works.", "publication_ref": ["b62", "b62", "b49", "b46", "b39", "b26", "b12", "b24", "b10", "b47", "b41", "b43", "b1", "b44", "b47", "b59", "b17", "b77", "b50", "b63", "b18", "b69", "b3", "b74", "b75", "b13", "b14", "b68", "b53", "b33", "b23", "b31", "b71", "b5", "b76", "b55", "b76", "b42", "b71", "b64", "b65", "b32", "b73", "b60", "b20", "b21", "b67", "b48", "b16", "b45", "b64", "b65", "b73", "b51", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "Main result", "text": "Suppose that we are given a sequence of i.i.d. data Y 1 , Y 2 , . . . taking values in Y n \u2286 R dn with law P n \u2208 M 1 (R dn ), and a loss function L n : X n \u00d7 Y n \u2192 R, where here X n \u2286 R pn is the parameter space. Consider online stochastic gradient descent with constant learning rate, \u03b4 n , which is given by\nX \u2113 = X \u2113\u22121 \u2212 \u03b4 n \u2207L n (X \u2113\u22121 , Y \u2113 ) ,\nwith possibly random initialization X 0 \u223c \u00b5 n \u2208 M 1 (X n ). Our interest is in understanding this evolution, (X \u2113 ), in the regime where both p n and d n \u2192 \u221e as n \u2192 \u221e. To this end, suppose that there is a finite collection of summary statistics of (X \u2113 ) whose evolution we are interested in. More precisely, suppose that we are given a sequence of functions u n \u2208 C 1 (R pn ; R k ) for some fixed k, where u n (x) = (u n 1 (x), ..., u n k (x)), and our goal is to understand the evolution of u n (X \u2113 ). To develop a scaling limit, we need some regularity assumptions on the relationship between how the step-size scales in relation to the loss, its gradients, and the data distribution. To this end let\nH(x, Y ) = L n (x, Y ) \u2212 \u03a6(x)\nwhere\n\u03a6(x) = E[L n (x, Y )] .\nIn the following, we suppress the dependence of H on Y and instead view H as a random function of x, denoted H(x). We let V (x) = E [\u2207H(x) \u2297 \u2207H(x)] be the covariance matrix for \u2207H at x. We make two assumptions on the triple (u n , L n , P n ) and the step size \u03b4 n . The first is an upper bound on the learning rate in terms of the regularity of the summary statistics. The second is our key assumption and asks that the summary statistic evolutions asymptotically close. These assumptions need not hold uniformly over the entire parameter space R pn , only uniformly over pre-images of compact sets under u n . We start with the regularity assumption, ensuring tightness of trajectories of the summary statistics. Definition 2.1. A triple (u n , L n , P n ) is \u03b4 n -localizable with localizing sequence (E K ) K if there is an exhaustion by compacts (E K ) K of R k , and constants C K (independent of n) such that (1) \nmax i sup x\u2208u \u22121 n (E K ) ||\u2207 2 u n i || op \u2264 C K \u2022 \u03b4 \u22121/2 n\n, and\nmax i sup x\u2208u \u22121 n (E K ) ||\u2207 3 u n i || op \u2264 C K ; (2) sup x\u2208u \u22121 n (E K ) \u2225\u2207\u03a6\u2225 \u2264 C K , and sup x\u2208u \u22121 n (E K ) E[\u2225\u2207H\u2225 8 ] \u2264 C K \u03b4 \u22124 n ; (3) max i sup x\u2208u \u22121 n (E K ) E[\u27e8\u2207H, \u2207u n i \u27e9 4 ] \u2264 C K \u03b4 \u22122 n , and max i sup x\u2208u \u22121 n (E K ) E[\u27e8\u2207 2 u n i , \u2207H \u2297 \u2207H \u2212 V \u27e9 2 ] = o(\u03b4 \u22123 n ).\nTo help the reader parse this assumption, we provide an in-depth discussion of each of these items, along with examples to have in mind in Remark 1. For now, we make the crucial observation that (1)-( 3) are all closed under decreasing the step-size \u03b4 n so for any reasonable task and family of summary statistics there will be a scaling of the step-size with n below which they will satisfy the conditions of Definition 2.1. For concreteness, summary statistics that are good to have in mind are correlations of the parameters with certain ground truth vectors, \u2113 2 norms of the parameters, and the population loss itself.\nWe now turn to our second assumption, that the limiting evolution equations for the family of summary statistics chosen close. Define the following first and second-order differential operators,\nA n = i \u2202 i \u03a6\u2202 i , and L n = 1 2 i,j V ij \u2202 i \u2202 j . (2.1)\nAlternatively written, A n = \u27e8\u2207\u03a6, \u2207\u27e9 and L n = 1 2 \u27e8V, \u2207 2 \u27e9. Definition 2.2. A family of summary statistics (u n ) are asymptotically closable for learning rate \u03b4 n if (u n , L n , P n ) are \u03b4 n -localizable with localizing sequence (E K ) K , and furthermore there exist locally Lipschitz functions h :\nR k \u2192 R k and \u03a3 : R k \u2192 R k\u00d7k , such that sup x\u2208u \u22121 n (E K ) \u2212 A n + \u03b4 n L n u n (x) \u2212 h(u n (x)) \u2192 0 , (2.2) sup x\u2208u \u22121 n (E K ) \u2225\u03b4 n J n V J T n \u2212 \u03a3(u n (x))\u2225 \u2192 0 . (2.3)\nIn this case we call h the effective drift, and \u03a3 the effective volatility.\nWe are now ready to present our main result. For a function f and measure \u00b5 we let f * \u00b5 denote the push-forward of \u00b5.\nTheorem 2.3. Let (X \u03b4n \u2113 ) \u2113 be stochastic gradient descent initialized from X 0 \u223c \u00b5 n for \u00b5 n \u2208 M 1 (R pn ) with learning rate \u03b4 n for the loss L n (\u2022, \u2022) and data distribution P n . For a family of summary statistics\nu n = (u n i ) k i=1 , let (u n (t)) t be the linear interpolation of (u n (X \u03b4n \u230at\u03b4 \u22121 n \u230b )) t .\nSuppose that u n are asymptotically closable with learning rate \u03b4 n , effective drift h, and effective volatility \u03a3, and that the pushforward of the initial data has (u n ) * \u00b5 n \u2192 \u03bd weakly for some \u03bd \u2208 M 1 (R k ). Then (u n (t)) t \u2192 (u t ) t weakly as n \u2192 \u221e, where u t solves\ndu t = h(u t )dt + \u03a3(u t )dB t .\n(2.4) initialized from \u03bd, where B t is a standard Brownian motion in R k .\nThe proof of Theorem 2.3 is provided in Section 6 and can be seen as a version of the classical martingale problem (see [70]) for high-dimensional stochastic gradient descent. We call the solution to (2.4) the effective dynamics of the summary statistics u n . The fact that h, \u03a3 are locally Lipschitz ensures that this solution is unique.\nWe end this subsection with discussion of the various scalings appearing in Definition 2.1.\nRemark 1. The kinds of summary statistics that we most frequently have in mind for application are (1) linear functions of the parameter space X n , for instance the correlation with a unit vector, or some ground truth; (2) radial statistics, like the \u2113 2 -norm of the parameters, or some subset of the parameters; and (3) rescaled versions (usually blown up by \u03b4 \u22121/2 n of these near their fixed points, as described in Section 2.2. Regarding the item (1) in Definition 2.1, for linear functions, it trivially holds; for radial statistics, the Hessian is a block identity matrix, so item (1) holds as long as \u03b4 n is O(1); therefore item (1) is most restrictive for rescalings of non-linear statistics, e.g., u(x) = \u03b4 \u2212\u03b1 n (\u2225x\u2225 2 \u2212 1) where it prevents consideration of this statistic with \u03b1 > 1/2. Turning to item (2) of Definition 2.1, we comment that the regularity assumptions made on \u03a6, L here are less restrictive than uniform Lipchitz assumptions common to the literature. In particular, we do not assume the population loss is Lipschitz everywhere, as we may have that K u \u22121 n (E K ) does not cover X n , nor does it imply uniform smoothness of H (and in turn L) as we may (and will) be taking \u03b4 n \u2192 0 with n.\nLet us lastly motivate the scalings appearing in item (3), which ensure there is some independence between H and the values of \u2207u and \u2207 2 u at x. As a testbed, suppose that \u2207H(x) is a random vector with i.i.d. entries all of order 1. If u is a rescaled linear statistic, e.g., \u03b4 \u22121/2 n \u27e8x, e 1 \u27e9 then the first bound of item ( 3) is saturated, and the second of course is trivial due to the linearity of u. The second bound is saturated by taking a rescaling of a radial statistic, e.g., \u03b4 \u22121/2 n \u2225x\u2225 2 , again assuming for maximal simplicity that \u2207H is an i.i.d. random vector with order one entries. In fact, the second part of item (3) could be dropped at the expense of more complicated diffusion coefficients in limiting SDE's: see Remark 2.\nRemark 2. While we discussed above the reasons for which the various scalings of Definition 2.1 were selected, it is interesting to ask what changes in Theorem 2.3 should certain of the assumptions of Definition 2.1 be violated. Most of the assumed bounds in the definition of localizability are used to establish tightness and ensure higher order terms in Taylor expansions vanish in the n \u2192 \u221e limit. In principle the second assumption in item (3) of Definition 2.1 could be dropped; in that case, the same quantity is still ensured to be O(\u03b4 \u22123 ) by the other localizability assumptions. Then Theorem 2.3 would still apply, but the limiting diffusion matrix would be the n \u2192 \u221e limit (assuming it exists) of\n\u03b4JV J T + \u03b4 2 E[\u27e8\u2207H, J\u27e9 \u2297 \u27e8\u2207 2 u, \u2207H \u2297 \u2207H \u2212 V \u27e9] + \u03b4 2 E[\u27e8\u2207 2 u, \u2207H \u2297 \u2207H \u2212 V \u27e9 \u2297 \u27e8\u2207H, J\u27e9] + \u03b4 3 E[\u27e8\u2207 2 u, \u2207H \u2297 \u2207H \u2212 V \u27e9 \u22972 ] ,\nas opposed to simply the limit of \u03b4JV J T .\nGenerically, the choice of summary statistics to which to apply Theorem 2.3 depends both on the quantities one is interested in, and the specifics of the task. In our examples, the choices are natural: correlations with ground truth vectors, finite numbers of final layer weights, and \u2113 2 norms of the parameters. In less structured settings, the choice of summary statistics may be more open-ended. One could start with a summary statistic of interest, like the projection in a principal subspace of an empirical matrix (a covariance or, as suggested experimentally in e.g., [66,54], a Hessian), or the population loss itself. Then from that statistic, one would determine the other statistics needed to build an asymptotically closed family per Definition 2.2.", "publication_ref": ["b0", "b70", "b2", "b66", "b54"], "figure_ref": [], "table_ref": []}, {"heading": "2.1.", "text": "Comparison to fixed dimensional perspective: critical v.s. subcritical step-sizes. Let us compare this with the classical limit theory of SGD in fixed dimension. For the sake of this discussion, suppose that not only does (2.2) hold, but each of the two terms A n u and \u03b4 n L n u (recall (2.1)) individually admit n \u2192 \u221e limits: namely that there exists f , g :\nR k \u2192 R k such that sup x\u2208u \u22121 n (E K ) \u2225A n u n (x) \u2212 f (u n (x))\u2225 \u2192 0 , (2.5) sup x\u2208u \u22121 n (E K ) \u2225\u03b4 n L n u n (x) \u2212 g(u n (x))\u2225 \u2192 0 , (2.6)\nin which case, evidently (2.2) holds with h = \u2212f + g. When (2.5) and (2.6) both hold, we call f , g and \u03a3 the population drift, the population corrector, and the diffusion matrix of u respectively. From the fixed dimensional perspective, when (2.5) holds, one predicts u to solve\ndu t = \u2212f (u t )dt ,(2.7)\nwith initial data u 0 \u223c u * \u00b5. as this is its evolution under gradient descent on the population loss \u03a6. Evidently this perspective only applies in the high-dimensional limit of Theorem 2.3 if both the population corrector g and the diffusion matrix \u03a3 are zero. We find that for any triple (u n , L n , P n ), there is a scaling of the learning rate \u03b4 n with n below which g = \u03a3 = 0, and the effective dynamics agree with the population dynamics (2.7) (we call this the sub-critical scaling regime, where the classical perspective applies), and a critical scaling regime in which g and \u03a3 may be non-zero, and the high-dimensionality induces non-trivial corrections to f . (In the case of teacher-student networks, the terms f and g can be compared to the \"learning\" and \"variance\" terms in Eq. (14a) of [73].) To see this, notice that if the triple (u n , L n , P n ) is \u03b4 n -localizable for some \u03b4 n \u2192 0, then it is also \u03b4 \u2032 n -localizable for every sequence \u03b4 \u2032 n = O(\u03b4 n ). If furthermore (2.3) and (2.5)-(2.6) hold for \u03b4 n with some f , g and \u03a3, then these limits also exists for \u03b4 \u2032 n = o(\u03b4 n ) with the same f but with g = \u03a3 = 0. As such, there can be exactly one scaling of \u03b4 n with n at which g or \u03a3 may be non-zero, and for all smaller scales of \u03b4 n , the fixed-dimensional perspective of (2.7) applies. 1 2.2. Ballistic vs. diffusive behavior of effective dynamics. In all of our examples, the diffusion matrix for the effective dynamics of the most natural choice of summary statistics is zero even in the critical scaling regime where h \u0338 = f . We call this the ballistic limit. In this case, the effective dynamics of the summary statistics is given by the ODE system\ndu t = h(u t )dt .\n(2.8)\nIn these settings, the phase portrait of the summary statistics is asymptotically that of this flow.\nNote that by construction of the scaling limit, the phase portrait of the ballistic limit only describes the evolution of summary statistics on length-scales that are order 1 and number of iterations that are order 1/\u03b4 n . If one is then interested in the evolution of u n in microscopic o(1) neighborhoods of the fixed points of the ballistic effective dynamics of (2.8), Theorem 2.3 also allows one to develop separate diffusive limits there.\nTo study diffusive regimes, one must apply Theorem 2.3 to re-centered and re-scaled summary statistics,\u0169 n (t) = \u03b4 \u2212\u03b1 n (u n (t) \u2212 u \u22c6 ) where u \u22c6 is a fixed point of (2.8). 2 To apply Theorem 2.3, \u03b1 must be chosen appropriately so that the triple (\u0169 n (t), L n , P n ) is \u03b4 nlocalizable and to pick out the next order drifts for\u0169-the first order term being zero microscopically close to u \u22c6 -and such that the initial data still converges (\u0169 n ) * \u00b5 n \u2192\u03bd.\nThis then leads to the rescaled effective dynamics of the summary statistics u n near u \u22c6 :\nd\u0169 t =h(\u0169 t )dt +\u03a3 1/2 (\u0169 t )dB t with\u0169 0 \u223c\u03bd . (2.9)\nThe rescaled effective dynamics are similar in spirit to diffusion one typically finds for the evolution of SGD near critical points in fixed dimensions. However, we note two important differences as compared to this perspective. Firstly, since this is a high-dimensional limit of general summary statistics, (2.9) applies in a neighborhood of a fixed point of the effective ODE system (2.8), rather than the population dynamics (2.7). Secondly, in many examples (indeed all the ones we study) the SDE's we get are degenerate, so that uniform ellipticity assumptions typically used to understand hitting and mixing times in these regimes do not apply. The degeneracies can take various forms, with\u03a3 sometimes being rank deficient in the entire\u0169-space, and sometimes vanishing completely as \u03a3 approaches certain distinguished points, for instance u \u22c6 . The implications of such degeneracies can be severe, as degenerate diffusions can be absorbed for arbitrarily long times by their unstable fixed points (c.f. the simple case of a 1D geometric Brownian motion).\nRemark 3 (Training at the edge of stability and critical scaling). In [20], it was empirically observed that the best training for neural networks does not occur when step-sizes are small enough for the classical gradient flow approximation to be valid. Instead, it occurs at the edge of stability where the step size is just small enough for the training to remain stable. Here, the loss fluctuates for some time before eventually converging to lower values than it would with smaller step size. This critical step size scaling is defined via the sharpness, namely the largest eigenvalue of the training loss Hessian. For a selection of recent theoretical investigations of this phenomenon see, e.g., [1,23,5,78]. While sharpness and edge of stability do not have direct analogues in the context of online SGD, a qualitatively similar phenomenon can be seen by taking the population loss as a summary statistic. The critical scaling of the learning rate with dimension discussed in Sections 2.1-2.2 constrains the step size in terms of the top eigenvalue of the Hessian of the loss. With this scaling, the population loss fluctuates near critical regions of its ballistic flow, allowing it to escape the critical region, whereas with a sub-critical learning rate the population loss stays stuck. We leave more detailed investigation of this connection to edge-of-stability phenomena for SGD to future investigation.", "publication_ref": ["b73", "b0", "b19", "b0", "b22", "b4", "b78"], "figure_ref": [], "table_ref": []}, {"heading": "Part 2. Examples", "text": "In the following sections, we demonstrate Theorem 2.3 on a range of popular examples of highdimensional statistical tasks. We begin first in Section 3 by presenting an application to a widely studied problem of high-dimensional estimation: namely, de-noising a rank one tensor that has been corrupted additively by Gaussian noise. We then turn to classification. Our aim in these examples is to demonstrate the applicability of our result to the analysis of multi-layer neural networks. To this end we analyze the training dynamics of a two-layer neural network for two canonical classification tasks, namely classification of a symmetric, binary gaussian mixture model (Section 4) and classification of a Gaussian analogue of the XOR problem of Minsky-Papert (Section 5).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Matrix and Tensor PCA", "text": "3.1. Model and background. Consider the problem of de-noising a rank one tensor that has been corrupted additively by Gaussian noise via SGD. A popular statistical model of this task is the spiked tensor model [61]. Suppose that we are given i.i.d. samples of data of the form\nY \u2113 = \u03bbv \u2297k + W \u2113\nwhere W \u2113 are i.i.d. copies of a k-tensor whose entries are i.i.d. standard Gaussians, v \u2208 R n is a unit vector, and \u03bb = \u03bb n > 0 is the signal-to-noise ratio. Our goal is to infer v.\nIn the case k = 2, this is a version of the well-known spiked matrix model of PCA [37] for which there is, by now, a substantial literature regarding the statistical thresholds. For a necessarily small selection see, e.g., [7,56,52,58,27]. For related work on online learning in this context see, e.g., [68]. Of particular interest in this direction is the well-known phase transition at \u03bb = 1 for estimation in this problem, which was determined first for Wishart ensembles in [7] and subsequently for this setting in [29,16,12]. As we will see in Section 3.3 below, we find a dynamical analogue of this transition at \u03bb = 1.\nThe case k \u2265 3 was introduced by Montanari and Richard [61] as a natural generalization of the spiked matrix models for estimation (and testing) problems where the data has multiple indices or requires higher moments. Here there has been a large literature on the statistical thresholds for estimation and testing, see, e.g., [52,57,10,40,36]. In this setting, there has also been a tremendous literature on the computational aspects of this problem as it is viewed as a important example of a model with a statistical-computational gap, namely, a setting where there is a gap between the regimes of statistical and computational tractability. See, e.g., [61,35,34,40,38,8,6].\nWe begin with these examples as their effective dynamics are particularly simple to analyze. In particular, they are are exactly solvable and only require two summary statistics, a correlation observable and a radial term. Even with this relative simplicity, we encounter a wide range of ODE and SDE limits. In particular, as mentioned above, we find dynamical phase transitions corresponding to the aforementioned thresholds in these models. For our analysis we will focus exclusively on the most interesting, critical step-size scaling which corresponds to the proportional asymptotics regime from the random matrix theory literature.\n3.2. Analysis. We take as loss the (negative) log-likelihood 3 namely,\nL(x, Y ) = ||Y \u2212 x \u2297k || 2 .\nThe pair m = m(x) := \u27e8x, v\u27e9 and r\n2 \u22a5 = r 2 \u22a5 (x) := \u2225x \u2212 mv\u2225 2 = \u2225x\u2225 2 \u2212 m 2 are such that \u03a6(x) = \u22122\u03bbm k + (r 2 \u22a5 + m 2 ) k + c\n, and the law of L only depends on them. In our normalization with \u03bb > 0 fixed, the regime \u03b4 n = o(1/n) is sub-critical and the regime \u03b4 n = \u0398(1/n) is critical. 4 We focus our presentation on the most interesting regime, namely the critical scaling regime of \u03b4 n = c \u03b4 /n for some constant c \u03b4 . Recalling the relation between number of samples and step-size, we see that this regime corresponds to the proportional asymptotics regime most studied in the random matrix theory literature where the above-mentioned transition for the top eigenvalue occurs. Note, however, that the limits in the subcritical regime are in all cases recovered by taking the c \u03b4 \u2193 0 limits of the ODE's/SDE's of the critical regime.\nFor notational simplicity, let R 2 := m 2 + r 2 \u22a5 . We consider the pair u n = (u 1 , u 2 ) = (m, r 2 \u22a5 ), for which Theorem 2.3 yields the following effective dynamics.\nProposition 3.1. Fix k \u2265 2, \u03bb > 0, c \u03b4 > 0 and let \u03b4 n = c\u03b4 /n. Then u n (t) converges as n \u2192 \u221e to the solution of the following ODE initialized from lim n\u2192\u221e (u n ) * \u00b5 n : dm = 2m(\u03bbkm k\u22122 \u2212 kR 2(k\u22121) )dt , dr 2 \u22a5 = \u22124kR 2(k\u22121) (r 2 \u22a5 \u2212 c \u03b4 )dt . (3.1)\nWe are able to identify and classify the set of fixed points of this effective dynamics. We focus on the critical step-size regime with c \u03b4 = 1 where one sees from (3.1) that r 2 \u22a5 \u2192 1, where the problem in the matrix case is most directly related to an eigenvalue problem (see Section 7 for the generic c \u03b4 dependencies). Throughout the following, we use the following notion of stability/unstability of a set of fixed points of an ODE. Definition 3.2. We call a set of fixed points U for an ODE stable if for every \u03f5 > 0, for every u \u2208 B \u03f5 (U ), the solution of the ODE with initialization u converges to some point in U as t \u2192 \u221e. Otherwise, we call U unstable. Proposition 3.3. Eq. (3.1) has isolated fixed points classified as follows. Let \u03bb c (k) be as in (7.6) and\nm \u2020 (k, \u03bb) \u2264 m \u22c6 (k, \u03bb) be as in (7.7) (if k = 2, \u03bb c = 1 and m \u2020 = m \u22c6 = \u221a \u03bb \u2212 1): (1) An unstable fixed point at (0, 0) and a fixed point at (0, 1); if k = 2, (0, 1) is stable if \u03bb < \u03bb c (2) and unstable if \u03bb > \u03bb c (2); if k > 2 (0, 1) is always stable. (2) If \u03bb > \u03bb c (k): when k = 2, two stable fixed points at (\u00b1m \u22c6 (2), 1). When k \u2265 3, two unstable\nfixed points at (\u00b1m \u2020 (k), 1) and two stable fixed points at (\u00b1m \u22c6 (k), 1).\nRemark 4. The presence of two pairs of fixed points when k \u2265 3 with non-zero correlation with v may seem surprising-indeed it indicates that even some warm starts will fail to attain good correlation with the signal when \u03bb is finite. This is an interesting consequence of the corrector in (3.1) and if one tracks the c \u03b4 dependence in the above, the fixed point m \u2020 goes to zero as c \u03b4 \u2192 0 and this barrier to recovery from warm starts vanishes as one approaches sub-critical step-sizes. \nif \u00b5 n \u223c N (0, I n /n), then (u n ) * \u00b5 n \u2192 \u03b4 (0,1) weakly. Now rescale and let\u0169 n = (m, r 2 \u22a5 ) = ( \u221a nm, r 2 \u22a5 ). Evidently,\u03bd = lim n (\u0169 n ) * \u00b5 n = N (0, 1) \u2297 \u03b4 1 .\nProposition 3.4. Fix k \u2265 2, \u03bb > 0 and \u03b4 n = 1/n. Then\u0169 n (t) converges as n \u2192 \u221e to the solution of the following SDE initialized from\u03bd:\ndm = 2m(2\u03bb1 k=2 \u2212 kr 2(k\u22121) \u22a5 )dt + 2(kr 2(k\u22121) \u22a5 ) 1 /2 dB t dr 2 \u22a5 = \u22124kr 2(k\u22121) \u22a5 (r 2 \u22a5 \u2212 1)dt . (3.2)\nWe see that r 2 \u22a5 now solves an autonomous ODE which converges exponentially to 1. When k = 2, as t tends to \u221e, the equation form behaves like\ndm = 4(\u03bb \u2212 1)mdt + 2 \u221a 2dB t .\nThis is an OU process which is mean-reverting when \u03bb < 1 and mean-repellent when \u03bb > 1. By stitching together the prelimits of these OU processes at a sequence of scales interpolating between that of\u0169 n and u n , we expect that one could show that for any \u03bb > 1, SGD reaches the stable fixed points at (\u00b1m \u22c6 (2), 1) in O(n log n) steps (with precise asymptotics, etc.), while when \u03bb < 1, the mean-reverting nature of the OU suggests it needs a much larger number of samples in order to correlate with the vector v. See Figure 1 for an overview, and Figures 2-3 for more refined numerical verification of this intuition.", "publication_ref": ["b61", "b37", "b6", "b56", "b52", "b58", "b27", "b68", "b6", "b29", "b15", "b11", "b61", "b52", "b57", "b9", "b40", "b36", "b61", "b35", "b34", "b40", "b38", "b7", "b5", "b3"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "3.4.", "text": "On the sample complexity of tensor PCA. When k \u2265 3, SGD is known to require a polynomially diverging sample complexity or \u03bb in order to solve the tensor PCA problem [6]. Accordingly, when \u03bb is kept finite in n, the expression form in (3.2) is always a mean-reverting OU-type process. Interestingly, one can also capture the (diverging) signal-to-noise threshold for SGD to recover v in tensor PCA by our methods. Indeed, for k \u2265 3 if one considers \u03bb n = \u039bn (k\u22122)/2 (matching the predicted gradient-based algorithm threshold from [8]),\u0169 n would instead converge to the solution of\ndm = 2m(k\u039b \u2212 kr 2(k\u22121) \u22a5 )dt + 2(kr 2(k\u22121) \u22a5 ) 1 /2 dB t dr 2 \u22a5 = \u22124kr 2(k\u22121) \u22a5 (r 2 \u22a5 \u2212 1)dt , (3.3)\nwhich transitions between mean-reverting and mean-repellent at \u039b c (k) = 1, as in k = 2.\nWe only considered a few specific choices of summary statistics in the above, and the strength of Theorem 2.3 derives from its general applicability. As demonstrations, let us mention a few other examples that we would expect to be of interest in the study of SGD for matrix and tensor PCA. The first example is a limiting ballistic limit theorem for the evolution of the population loss \u03a6(x). The population loss can be taken added to the family of summary statistics in our \u03b4 n -localizable triple; in the case of k-tensor PCA, this yields,\nd\u03a6 = \u2212 4k 2 m 2 \u03bb 2 m 2(k\u22122) \u2212 2\u03bbm k\u22122 R 2k\u22122 + R 4k\u22124 \u2212 4k 2 R 4(k\u22121) (r 2 \u22a5 \u2212 c \u03b4 ) dt .\n(3.4) 3.5. A finer diffusive limit theorem at a random start. The second example is a diffusive limit theorem near the fixed point (m, r 2 \u22a5 ) = (0, 1) (as opposed to (3.3) where we blew up only the m variable about the saddle set m = 0 and therefore onlym was moving diffusively). In order to do so, we consider the scaling limit of the pair (m,r \u22a5 ) = ( \u221a nm, \u221a n(r 2 \u22a5 \u2212 1)) and find the following limit:\ndm = 2k(\u03bb1 k=2m k\u22121 \u2212 1)dt + 2 \u221a kdB (1) t , dr 2 \u22a5 = \u22124kr 2 \u22a5 dt + 2 k(k \u2212 1)dB(2)\nt .\n(3.5) Interestingly, with this double rescaling, the n \u2192 \u221e limit yields a pair of OU processes that are decoupled, namely, each of their drifts are autonomous and their stochastic parts independent. This pair of independent OU processes is depicted in Figures 4-5.\n4. Two-layer networks for classifying a binary Gaussian mixture 4.1. Model and Background. As a warm-up to the XOR problem that we will consider in Section 5, we consider the problem of supervised classification of a binary Gaussian mixture model (binary GMM) which is defined as follows. Suppose that we are given i.i.d. samples of the form Y = (y, X), where y is a {0, 1}-valued Ber(1/2) random variable and, conditionally on y, we have\nX \u223c N ((2y \u2212 1)\u00b5, I/\u03bb) ,\nwhere \u00b5 \u2208 R N is a fixed unit vector, I is the identity on R N , and \u03bb > 0 is the signal-to-noise ratio.\nHere, y is called the class label and X is called the data. Our goal is to construct an estimator, y =\u0177(X), of the class label, y, which depends on the data, X, alone. Depicted is the evolution of summary statistic r 2 \u22a5 (t) for 10n steps of SGD initialized randomly. This follows a stable OU process independent of \u03bb.\nIt is classical [3] that the Bayes optimal estimator in this setting is given by\u0177 = sgn(\u00b5 \u2022 x). Furthermore, this estimator can be achieved by (a rounding of) the output of a single layer neural network trained using the binary-cross-entropy loss (4.1). This is also called logistic regression. The single-layer setting can be easily analyzed via our framework. Our focus here, however, is to demonstrate our analysis on multi-layer neural networks.\nTo that end, we consider now the same setting, except that we will estimate the class labels using a simple two-layer neural network. (Note that the Bayes' optimal estimator is still expressible by this architecture.) At first glance, this may seem an elementary setting with little to say. However as we will see, even in this simple setting surprising behaviour can occur in the high-dimensional setting which runs counter to common intuition. Furthermore, as we will see in Section 5, the phenomena occurring here also appear in richer problems such as the XOR problem. 4.2. Analysis. For the sake of concreteness, we consider classification via the following architecture (though our techniques generalize to other settings mutatis mutandis): The first layer has weights (W 1 , W 2 ) \u2208 R N \u00d7 R N and ReLu activation, g(x) = x \u2228 0; and the second layer has weights v 1 , v 2 \u2208 R and sigmoid activation, \u03c3(x) = 1/(1 + e \u2212x ). The output of the multi-layer network is then \u03c3(v \u2022 g(W X)) Our parameter space is then X n = R 2N +2 and we therefore take n = 2N + 2 when applying Theorem 2.3.\nAs we are interested in supervised classification, we take the usual binary cross-entropy loss with \u2113 2 regularization. In our setting, this reduces to optimizing\nL (v i , W i ) i\u2208{1,2} ; (y, X) = \u2212yv \u2022 g(W X) + log(1 + e v\u2022g(W X) ) + p(v, W ) , (4.1)\nwhere g is applied component wise and p(v, W ) := (\u03b1/2)(||v|| 2 + ||W || 2 ).\nIt can be shown (see Lemma 8.1) that the law of the loss at a given point, (v, W ) \u2208 X n , depends only on the 7 summary statistics,\nu n = (v 1 , v 2 , m 1 , m 2 , R \u22a5 11 , R \u22a5 12 , R \u22a5 22 ),(4.2)\nwhere\nm i = W i \u2022 \u00b5 and R \u22a5 ij = W \u22a5 i \u2022 W \u22a5 j with W \u22a5 i = W i \u2212 m i \u00b5 denoting the part of W i orthogonal to \u00b5. For a point, (v, W ) \u2208 X n , let A \u00b5 i = E[X \u2022\u00b51 W i \u2022X\u22650 (\u03c3(v\u2022g(W X)) \u2212 y)] , A \u22a5 ij = E[X \u2022W \u22a5 j 1 W i \u2022X\u22650 (\u03c3(v\u2022g(W X)) \u2212 y)] , B ij = E[1 W i \u2022X\u22650 1 W j \u2022X\u22650 (\u03c3(v\u2022g(W X)) \u2212 y) 2 ]. (4.3)\nBy similar reasoning to Lemma 8.1, it can be seen that these are functions only of u n , and we denote them as such, e.g.,\nA \u00b5 i = A \u00b5 i (u n )\n. See Section 8. The critical scaling for \u03b4 is then of order \u0398(1/n) and we obtain the following. Proposition 4.1. Let u n be as in (4.2) and fix any \u03bb > 0 and \u03b4 n = c\u03b4 /N. Then u n (t) converges to the solution of the ODE system,u t = \u2212f (u t ) + g(u t ), initialized from lim n\u2192\u221e (u n ) * \u00b5 n , with:\nf v i = m i A \u00b5 i (u) + A \u22a5 ii (u) + \u03b1v i , f m i = v i A \u00b5 i (u) + \u03b1m i , f R \u22a5 ij = v i A \u22a5 ij (u) + v j A \u22a5 ji (u) + 2\u03b1R \u22a5 ij ,\nand correctors\ng v i = g m i = 0, g R \u22a5 ij = c \u03b4 v i v j \u03bb B ij for i, j = 1, 2.", "publication_ref": ["b5", "b7", "b2"], "figure_ref": ["fig_12"], "table_ref": []}, {"heading": "Low variance asymptotics.", "text": "Due to the Gaussian integrals defining f , g, it is difficult to analyze the ODE system defined by Proposition 4.1, let alone any rescaled effective dynamics. For ease of analysis, we next send \u03bb \u2192 \u221e corresponding to a small noise regime for the Gaussian mixture. We emphasize that this limit is taken after n \u2192 \u221e and therefore is still approximately on the critical scale of \u03bb = \u0398(1) at which there is a transition in the existence of any fixed point which is a good classifier. In particular, if \u03bb = \u03bb n is any diverging sequence, then the limiting effective dynamics would exactly match that attained by now sending \u03bb \u2192 \u221e. In Figure 6, we demonstrate numerically that the following predicted fixed points from the \u03bb \u2192 \u221e limit match those arising at finite large N and \u03bb > 0. 5\nProposition 4.2. The \u03bb \u2192 \u221e limit of the ODE system of Proposition 4.1 is given b\u1e8f\nm i = v i 2 \u03c3(\u2212v \u2022 m) \u2212 \u03b1m i m 1 m 2 > 0 v i 2 \u03c3(\u2212v i m i ) \u2212 \u03b1m i else ,v i = m i 2 \u03c3(\u2212v \u2022 m) \u2212 \u03b1v i m 1 m 2 > 0 m i 2 \u03c3(\u2212v i m i ) \u2212 \u03b1v i else , and\u1e58 \u22a5 ij = \u22122\u03b1R \u22a5 ij .\nThe fixed points of this system are classified as follows. All fixed points have\nR \u22a5 ij = 0 and m i = v i for i, j = {1, 2}. In (v 1 , v 2 ), the coordinates are classified by (1) A fixed point at (v 1 , v 2 ) = (0, 0) that is stable if \u03b1 > 1 /4; (2) If \u03b1 < 1 /4\n, two unstable sets of fixed points at the quarter-circles given by\n(v 1 , v 2 ) having v 1 v 2 > 0 such that v 2 1 + v 2 2 = C \u03b1 for C \u03b1 := log(1 \u2212 2\u03b1) \u2212 log(2\u03b1). (3) If \u03b1 < 1 /4, two stable fixed points at (v 1 , v 2 ) equals ( \u221a C \u03b1 , \u2212 \u221a C \u03b1 ) and (\u2212 \u221a C \u03b1 , \u221a C \u03b1 ).\nIf \u00b5 n is e.g., given by\n(v 1 , v 2 ) \u223c N (0, I 2 ) and W 1 , W 2 \u223c N (0, I N /(\u03bbN )) then \u03bd := lim(u n ) * \u00b5 n is N (0, I 2 ) in the v 1 , v2\ncoordinates, and is in the basin of attraction of the quarter-circles of item (2) with probability 1 /2 and the basin of attraction of the stable fixed points of (3) with probability 1 /2.  4.4. Convergence to spurious solutions. Let us pause to interpret this result. The stable fixed points when \u03b1 < 1/4 are the optimal classifiers, whereas the unstable set of fixed points given by item (2) misclassify half of the data. Therefore, the above indicates that when solving the above task with randomly initialized weights, one of the following two scenarios occur, each with probability 1/2 (with respect to the initialization): the algorithm will converge to the optimal classifier in linear time or it will appear to have converged to a macroscopically sub-optimal classifier on the same timescale, see Figures 6-7 for numerical verification of this at finite N and \u03bb.\n4.5. Degeneracy of diffusive limits. It is then natural to ask about the behaviour of the SGD in the latter regime, after it converges to the sub-optimal classifiers which lie on the aforementioned quarter-circles. Proposition 4.2 rigorously justified the exchange of n \u2192 \u221e and \u03bb \u2192 \u221e limits in the ballistic phase. In the diffusive phase, one could in principle find the quarter circle of fixed points of the ODE in Proposition 4.1 and consider rescaled observables\u1e7d i ,m i corresponding to blowing up v i , m i in diffusive O(n \u22121/2 ) neighborhoods about them to get SDE limits from Theorem 2.3.\nIn order to have explicit formulae, in what follows, we consider the diffusive limits obtained when taking \u03bb = \u221e, for which we know the precise locations of these fixed points from Proposition 4.2. This also captures the limit obtained by taking any \u03bb n diverging faster than O(n 1/2 ); the numerics of Figure 8 demonstrate its qualitative consistency with the behavior in microscopic neighborhoods of fixed points at \u03bb finite. where the diffusions can be seen to not be of full rank.\nProposition 4.3. Let \u03b4 n = 1 /N, (a 1 , a 2 ) \u2208 R 2 + be such that a 2 1 + a 2 2 = C \u03b1 and let\u1e7d i = \u221a N (v i \u2212 a i ) andm i = \u221a N (m i \u2212 a i ).\nWhen \u03bb = \u221e, the SDE system obtained by applying Theorem 2.3 to\u0169 n is\nd\u1e7d i =\u03b1(m i \u2212\u1e7d i ) + a i (\u03b1 \u2212 2\u03b1 2 ) a k (\u1e7d k +m k ) +\u03a3 1 /2 dB t \u2022 e v i , dR \u22a5 ii = \u22122\u03b1R \u22a5 ii dt , dm i =\u03b1(\u1e7d i \u2212m i ) + a i (\u03b1 \u2212 2\u03b1 2 ) a k (\u1e7d k +m k ) +\u03a3 1 /2 dB t \u2022 e m i , dR \u22a5 ij = \u22122\u03b1R \u22a5 ij dt ,\nwhere\u03a3 is a matrix whose only non-zero entries are\u03a3\u1e7d i\u1e7dj =\u03a3m imj =\u03a3\u1e7d imj = \u03b1 2 a i a j .\nNotice that this diffusion matrix is rank 1, so this diffusion is non-trivial but degenerate even in the rescaled coordinates (\u1e7d i ,m i ). Moreover, the entries of\u03a3 vanish on the axes a 1 = 0 or a 2 = 0. In particular, crossing from the unstable quarter ring into the quadrants v 1 v 2 < 0 where the stable fixed points lie is impossible in the noiseless setting, and happens on a much larger timescale at finite \u03bb.\n5. Two-layer networks for the XOR Gaussian mixture 5.1. Model and Background. For our final example, consider the problem of supervised learning for an XOR-type Gaussian mixture model in R N . Suppose that we are given i.i.d. samples of the form Y = (y, X), where y is Ber(1/2) and X has the following distribution: if y = 1 then X is a 1 /2-1 /2 mixture of N (\u00b5, I/\u03bb) and N (\u2212\u00b5, I/\u03bb) and if y = 0 it is a 1 /2-1 /2 mixture of N (\u03bd, I/\u03bb) and N (\u2212\u03bd, I/\u03bb), where \u03bb > 0, and \u00b5, \u03bd are orthogonal unit vectors. Here, y is the class label and X is the data.\nThis data model is a Gaussian mixture model analogue of the (in)famous XOR problem of Minsky-Papert [51]. In particular, it is easy to see that the optimal decision boundary is not expressible by a single-layer neural network as the data is not linearly separable. That said, it is also straightforward to see that this decision boundary is realizable by simple two-layer networks. 6 We focus on this example as a demonstration of the applicability of our techniques to the analysis of the training dynamics for two-layer neural networks on natural data models. While this model is arguably the simplest model requiring a multi-layer network to solve, it nevertheless exhibits very complex phenomenology. We mention that some of these complexities were also observed in a very similar setup in [60] where ballistic limits from warm starts were derived.", "publication_ref": ["b51", "b5", "b60"], "figure_ref": ["fig_11", "fig_4", "fig_5"], "table_ref": []}, {"heading": "Analysis.", "text": "Consider the corresponding classification problem using a two-layer neural network, taking as our estimator of the class label\u0177(X) to be the natural rounding of \u03c3(v \u2022 g(W X)), where \u03c3 and g are the sigmoid and ReLU as in Section 4. We take W to be a K \u00d7 N matrix and v to be a K-vector. 2 ) values that 500 runs of SGD converge to after 100N steps from a random Gaussian initialization. The \u2212 and \u00d7 are the unstable and stable fixed points of the \u03bb = \u221e ballistic effective dynamics. This demonstrates that the fixed points of the limiting effective dynamics have the same qualitative structure at finite \u03bb as \u03bb = \u221e, and approach the \u03bb = \u221e ones as \u03bb gets large.\nTo train the network, we again consider the binary cross-entropy loss with \u2113 2 -penalty. This loss is identical to (4.1) mutatis mutandis. For the readers convenience, we recall that the loss is of the form\nL (v i , W i ) i\u2264K ; (y, X) = \u2212yv \u2022 g(W X) + log(1 + e v\u2022g(W X) ) + p(v, W ) ,\nwhere again \u03c3, g are applied component wise and again p(v, W ) := (\u03b1/2)(||v|| 2 + ||W || 2 ).\nIn Lemma 9.1 below, we show that the law of the loss at a point (v, W ) depends only on the following 4K + K 2 variables: for\n1 \u2264 i \u2264 j \u2264 K, v i , m \u00b5 i = W i \u2022 \u00b5 , m \u03bd i = W i \u2022 \u03bd , R \u22a5 ij = W \u22a5 i \u2022 W \u22a5 j (5.1)\nwhere\nW \u22a5 i = W i \u2212 m \u00b5 i \u00b5 \u2212 m \u03bd i \u03bd\nis the part perpendicular to \u00b5, \u03bd. Furthermore, this lemma shows that, if u n given by these variables, then for any fixed \u03bb > 0, the localizability criterion of Definition 2.1 holds as long as \u03b4 n = O(1/n). We can then apply Theorem 2.3 to obtain limits in both the ballistic and diffusive phases. To this end, we need to define the following auxiliary functions analogous to (4.3) above. For a point (v, W ) \u2208 R K+KN , define the quantity\nA i = E X1 W i \u2022X\u22650 \u2212 y + \u03c3(v \u2022 g(W X)) ,\nand let\nA \u00b5 i = \u00b5 \u2022 A i , A \u03bd i = \u03bd \u2022 A i , A \u22a5 ij = W \u22a5 j \u2022 A i . Furthermore, let B ij = E 1 W i \u2022X\u22650 1 W j \u2022X\u22650 \u2212 y + \u03c3(v \u2022 g(W X)) 2 .\nBy similar reasoning, it can be shown that these functions are expressible as functions of u n alone (see Section 9 below). We then find the following effective ballistic dynamics. \nf v i = m \u00b5 i A \u00b5 i (u) + m \u03bd i A \u03bd i (u) + A \u22a5 ii (u) + \u03b1v i , f m \u00b5 i = v i A \u00b5 i + \u03b1m \u00b5 i , f R \u22a5 ij = v i A \u22a5 ij (u) + v j A \u22a5 ji (u) + 2\u03b1R \u22a5 ij , f m \u03bd i = v i A \u03bd i + \u03b1m \u03bd i .\nand correctors The fraction of endpoints (SGD after 100N steps from a random Gaussian initialization) with v having two positive entries and two negative entries, and with the consequent correct signs on m \u00b5 i , m \u03bd i , corresponding to the stable fixed points of the \u03bb = \u221e dynamics; it matches the predicted 29 32 , 3 32 fractions.\ng v i = g m \u00b5 i = g m \u03bd i = 0, and g R \u22a5 ij = c \u03b4 v i v j \u03bb B ij for 1 \u2264 i \u2264 j \u2264 K.", "publication_ref": ["b29"], "figure_ref": [], "table_ref": []}, {"heading": "Low variance asymptotics.", "text": "As with the binary GMM, one can develop the large \u03bb limit of these asymptotics after n \u2192 \u221e. The effective dynamics in this regime are noticeably more tractable.\nWe defer the precise expressions of these dynamics to Proposition 9.1 below. Let us instead classify the corresponding fixed points. \n\u00b5 , I \u2212 \u00b5 , I + \u03bd , I \u2212 \u03bd )\n, is a set of fixed points that have R \u22a5 ij = 0 for all i, j, and have\n(1) m \u00b5 i = m \u03bd i = v i = 0 for i \u2208 I 0 , (2) m \u00b5 i = v i > 0 such that i\u2208I + \u00b5 v 2 i = logit(\u22124\u03b1) and m \u03bd i = 0 for all i \u2208 I + \u00b5 , (3) \u2212m \u00b5 i = v i > 0 such that i\u2208I \u2212 \u00b5 v 2 i = logit(\u22124\u03b1) and m \u03bd i = 0 for all i \u2208 I \u2212 \u00b5 ,(4)\nm \u03bd i = v i < 0 such that i\u2208I + \u03bd v 2 i = logit(\u22124\u03b1) and m \u00b5 i = 0 for all i \u2208 I + \u03bd , (5) \u2212m \u03bd i = v i < 0 such that i\u2208I \u2212 \u03bd v 2 i = logit(\u22124\u03b1\n) and m \u00b5 i = 0 for all i \u2208 I \u2212 \u03bd . In the K = 4 case, these form 39 connected sets of fixed points, and of which 4! = 24 are fixed points that are stable, corresponding to the possible permutations in which each of I + \u00b5 , I \u2212 \u00b5 , I + \u03bd , I \u2212 \u03bd are singletons.\nSimilar to the binary GMM, in Figures 9-10, we demonstrate numerically that the following predicted fixed points from the \u03bb \u2192 \u221e limit match those arising at finite large n and \u03bb > 0.\nIn the K = 4 case, we can also exactly calculate the probability that the effective dynamics in the ballistic phase converges to a stable fixed point (as opposed to an unstable one). From a Gaussian initialization \u00b5 n where v i \u223c N (0, 1) and W i \u223c N (0, I N /N ) independently, this converges to 3 /32. We refer the reader to Section 9.4 for the proof.\n5.4. Overparametrization in the XOR GMM. Since the the derivations of the ballistic limiting equations apply for general K, we can also study the probability of ballistic convergence to a stable vs. unstable fixed point as one varies K. This addresses the regime of overparametrization for the XOR GMM since K = 4 suffices to express a Bayes-optimal classifier. In this more generic setting, the probability of being in the ballistic domain of attraction of the stable fixed points (corresponding to the Bayes optimal classifiers) is\n1 2 K K\u22122 k=2 K k (1 \u2212 2 1\u2212k )(1 \u2212 2 1+k\u2212K ) , (5.2)\nwhich goes to 1 exponentially fast as K grows. This clearly demonstrates the benefits of overparametrizaiton of the landscape in a concrete two-layer network: a random initialization is more likely to to contain the \"right\" initial signature (corresponding to none of I + \u00b5 , I \u2212 \u00b5 , I + \u03bd , I \u2212 \u03bd being empty at initialization) in order to be in the basin of a Bayes optimal classifier as the width grows, and as long as the right signature is present in the nodes at initialization, the SGD will ballistically converge to a global minimizer of the population loss. This is a rigorous example of the well-known lottery ticket hypothesis of [30]. Roughly speaking, the lottery ticket hypothesis proposes that the reason for the success of overparametrized networks is that they give more attempts for a sufficiently expressive subnetwork to be initialized well, and succeed at the task on its own. 5.5. Diffusive limits at unstable fixed points. As an example of the diffusions that can arise in the rescaled effective dynamics at the unstable fixed points, let us consider the unstable fixed points in which v has the correct signature (two positive, two negative) but for each of those we are at a corresponding quarter-ring. By way of example, we can set K = 4, or equivalently focus on a fixed point where all indices beyond the first four have\nv i = m \u00b5 i = m \u03bd i = 0.\nHere, the dynamics effectively becomes a pair of 2 two-layer GMM's on quarter-rings (as in Section 4), that are anti-correlated. More precisely, let (a 1,\u00b5 , a 2,\u00b5 ) be such that a 2 1,\u00b5 +a 2 2,\u00b5 = C \u03b1 and (a 3,\u03bd , a 4,\u03bd ) such that a 2 3,\u03bd +a 2 4,\u03bd = C \u03b1 , for C \u03b1 = \u2212logit(4\u03b1). Take as fixed points about which we expand to be\nv i = m \u00b5 i = a i,\u00b5 > 0 and v i = m \u03bd i = a i,\u03bd < 0 for i = 3, 4. Namely, we let v i = \u221a N (v i \u2212 a i,\u00b5 ) i = 1, 2 \u221a N (v i \u2212 a i,\u03bd ) i = 3, 4 , m \u00b5 i = \u221a N (m \u00b5 i \u2212 a i,\u00b5 ) i = 1, 2 m \u03bd i = \u221a N (m \u03bd i \u2212 a i,\u03bd ) i = 3, 4\n.\n(Setm \u03bd i = 0 for i = 1, 2 andm \u00b5 i = 0 for i = 3, 4 in\u0169 n effectively removing those variables.) Proposition 5.3. Let \u03b4 n = 1 /N and let\u0169 n = (\u1e7d i ,m \u00b5 i ,m \u03bd i , R \u22a5 ij ). When \u03bb = \u221e, Theorem 2.3 can be applied and\u0169 n (t) converges to the solution of the SDE d\u0169(t) = \u2212h(\u0169)dt + \u03a3(\u0169)dB t wher\u1ebd\nh\u1e7d i = \u03b1(\u1e7d i \u2212m \u00b5 i ) \u2212 a i,\u00b5 (\u03b1 \u2212 4\u03b1 2 ) k=1,2 a k,\u00b5 (\u1e7d k +m \u00b5 k ) i = 1, 2 \u03b1(\u1e7d i \u2212m \u03bd i ) \u2212 a i,\u03bd (\u03b1 \u2212 4\u03b1 2 ) k=3,4 a k,\u03bd (\u1e7d k +m \u03bd k ) i = 3, 4\n, hm \u00b5 i (resp.,hm \u03bd i ) is like h\u1e7d i for i = 1, 2 (resp., i = 3, 4) with\u1e7d i andm \u00b5 i (resp.,m \u03bd i ) swapped, h R \u22a5 ij = 2\u03b1R \u22a5 ij , and\u03a3 is the constant rank-2 matrix whose non-zero entries ar\u1ebd In this section, we prove our main convergence result, namely Theorem 2.3. The drift terms can be seen from a Taylor expansion out to second order, with the role played by \u03b4 n -localizability being to justify neglecting certain negligible second order terms, as well as all higher order terms. The identification of the stochastic term is via the classical martingale problem [70] for summary statistics of stochastic gradient descent in the high-dimensional n \u2192 \u221e limit.\n\u03a3\u1e7d i\u1e7dj =\u03a3m \u00b5 im \u00b5 j =\u03a3\u1e7d im \u00b5 j = 3\u03b1 2 a i,\u00b5 a j,\u00b5 if i, j \u2208 {1, 2} , \u03a3\u1e7d i\u1e7dj =\u03a3m \u03bd im \u03bd j =\u03a3\u1e7d im \u03bd j = 3\u03b1 2 a i,\u03bd a j,\u03bd if i, j \u2208 {3, 4} , \u03a3\u1e7d i\u1e7dj =\u03a3m \u00b5 im \u03bd j =\u03a3 m \u00b5 i ,v j =\u03a3\u1e7d im \u03bd j = \u2212\u03b1 2 a i,\u00b5 a j,\u03bd if i \u2208 {1, 2}, j \u2208 {3,\nNotational remark. For ease of notation, in the following we say that f \u2272 g if there is some constant C > 0 such that f \u2264 Cg and that f \u2272 a g if there is some constant C(a) > 0 depending only on a such that f \u2264 C(a)g. We will often suppress the dependence on n in subscripts, when it is clear from context.\nProof of Theorem 2.3. Our aim is to establish u n \u2192 u weakly as random variables on C([0, \u221e)) where u solves (2.4). It is equivalent to show the same on C([0, T ]) equipped with the sup-norm for every T > 0.\nLet \u03c4 n K denote the exit time for the interpolated process u n (t) from E n K . Define its pre-image E * K,n := u \u22121 n (E n K ) and let L \u221e K,n = L \u221e (E * K,n ). For a function f , we use the shorthand f \u2113 to denote f (X \u2113 ). By Taylor's theorem, we have that for any C 3 function f and any \u2113 \u2264 \u03c4 n K /\u03b4,\nf \u2113 = f (X \u2113\u22121 \u2212 \u03b4\u2207\u03a6 \u2113\u22121 \u2212 \u03b4\u2207H \u2113 \u2113\u22121 ) = f \u2113\u22121 + \u03b4[A f \u2113 \u2212 A f \u2113\u22121 ] + \u03b4[M f \u2113 \u2212 M f \u2113\u22121 ] + O(\u03b4 3 ||\u2207 3 f || L \u221e K,n \u2022 ||\u2207L|| 3 L \u221e K,n\n) , (6.1)\nwhere A f \u2113 and M f \u2113 are defined by their increments as follows:\nA f \u2113 \u2212 A f \u2113\u22121 = \u2212 A n + \u03b4L n f \u2113\u22121 + 1 2 \u2207\u03a6 \u2297 \u2207\u03a6, \u2207 2 f \u2113\u22121 , M f \u2113 \u2212 M f \u2113\u22121 = \u2212 \u2207H \u2113 , \u2207f \u2113\u22121 + \u03b4(E f \u2113 \u2212 E f \u2113\u22121 ) , E f \u2113 \u2212 E f \u2113\u22121 = \u2207 2 f (\u2207\u03a6, \u2207H \u2113 ) \u2113\u22121 + 1 2 \u2207 2 f, \u2207H \u2113 \u2297 \u2207H \u2113 \u2212 V \u2113\u22121 , for A n = \u27e8\u2207\u03a6, \u2207\u27e9, L n = 1 2 i,j V ij \u2202 i \u2202 j and V = E[\u2207H \u2297 \u2207H] as in (2.", "publication_ref": ["b30", "b70"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "1).", "text": "Observe that A f \u2113 is previsible (with respect to the filtration generated by (Y 1 , ..., Y \u2113\u22121 )), and M f \u2113 is a martingale. We bound these for f = u j among u n = (u 1 , ..., u k ).\nRecalling Definition 2.1, since u n are \u03b4 n -localizable, the error term in (6.1) has\n\u03b4 3 sup x\u2208E * K,n E[||\u2207 3 u j || \u2022 ||\u2207L|| 3 ] \u2272 \u03b4 3 ||\u2207 3 u j || L \u221e K,n ||\u2207\u03a6|| 3 L \u221e K,n + sup E * K,n E||\u2207H|| 3 \u2272 K \u03b4 3/2 .\nSince \u03b4 n goes to 0 as n \u2192 \u221e, we may thus write u j (X \u2113 ) as\nu j (X \u2113 ) = u j (0) + \u03b4 \u2113 \u2032 \u2264\u2113 A u j \u2113 \u2032 \u2212 A u j \u2113 \u2032 \u22121 + \u03b4 \u2113 \u2032 \u2264\u2113 M u j \u2113 \u2032 \u2212 M u j \u2113 \u2032 \u22121 + o(1) ,\nwhere the last term is o(1) in L 1 uniformly for \u2113 \u2264 \u03c4 K /\u03b4. Now let us define for s \u2208 [0, T ],\na \u2032 j (s) = A u j [s/\u03b4] \u2212 A u j [s/\u03b4]\u22121 and b \u2032 j (s) = M u j [s/\u03b4] \u2212 M u j [s/\u03b4]\u22121\nIf we let\na j (s) = s 0 a \u2032 j (s \u2032 )ds \u2032 = a j (\u03b4[s/\u03b4]) + (s \u2212 \u03b4[s/\u03b4])(A u j [s/\u03b4] \u2212 A u j [s/\u03b4]\u22121 )\nand similarly b j (s) = s 0 b \u2032 j (s \u2032 )ds \u2032 , then recalling that u n (s) is the linear interpolation of (u j ([s/\u03b4])) j , we may write\nu n (s) = u n (0) + a n (s) + b n (s) + o(1)\n.\nwhere a n (s) = (a j (s)) j and b n (s) = (b j (s)) j . We now prove that the sequence (u n (s \u2227 \u03c4 n K )) is tight in C([0, T ]) with limit points which are (1/4)-Holder for each K. To this end, let us define\nv n (s) = u n (0) + a n (s) + b n (s) .\nAs the o(1) error above is uniform in t, we have that\nsup 0\u2264s\u2264\u03c4 n K ||u n (s) \u2212 v n (s)|| \u2192 0 , in L 1 .\nThus it suffices to show the claimed tightness and Holder properties of limit points for v n instead of u n . We aim to show that for all 0 \u2264 s, t \u2264 T ,\nE||v n (s \u2227 \u03c4 K ) \u2212 v n (t \u2227 \u03c4 K )|| 4 \u2272 K,T (t \u2212 s) 2 , (6.2)\nfrom which we will get that the sequence v n (s \u2227 \u03c4 K ) is uniformly 1/4-H\u00f6lder by Kolmogorov's continuity theorem. Evidently, for all s, t we have that\n\u2225v n (s) \u2212 v n (t)\u2225 \u2264 \u2225a n (s) \u2212 a n (t)\u2225 + \u2225b n (s) \u2212 b n (t)\u2225 .\nWe control these terms in turn. We will do this coordinate wise and, for readability, fix some j \u2264 k and let u = u j , a = a j , b = b j etc. For the previsible term, we have\nE|a(s \u2227 \u03c4 K ) \u2212 a(t \u2227 \u03c4 K )| 4 \u2272 E \u03b4 \u2113 (\u2212A n + \u03b4L n )u \u2113 4 + E \u03b4 2 \u2113 \u2207\u03a6 \u2297 \u2207\u03a6, \u2207 2 u \u2113 4 , (6.3)\nwhere these sums are over steps \u2113 ranging from\n[s/\u03b4] \u2227 \u03c4 K /\u03b4 to [t/\u03b4] \u2227 \u03c4 K /\u03b4. Let h = (h j ) j\u2264k be as in (2.\n2). Then the first term in (6.3) satisfies\nE \u03b4 \u2113 (\u2212A n + \u03b4L n )u \u2113 4 \u2272 K E|\u03b4 \u2113 h j (u n ) \u2113 | 4 + o((t \u2212 s) 4 ) \u2264 (t \u2212 s) 4 ||h j || 4 L \u221e (E n K ) + o(1) \u2272 K (t \u2212 s)\n4 by continuity of h j . For the second term in (6.3),\nE|\u03b4 2 \u2207\u03a6 \u2297 \u2207\u03a6, \u2207 2 u \u2113 | 4 \u2264 \u03b4 8 |((t \u2212 s)/\u03b4)| sup x\u2208E * K,n ||\u2207\u03a6(x)|| 2 sup x\u2208E * K,n ||\u2207 2 u(x)|| op 4\nwhich is \u2272 K \u03b4 2 (t \u2212 s) 4 by items (1)-(2) \u03b4 n -localizability. (Applying this bound for s = 0, t = T , the last term in a is vanishing in the limit for each K whenever \u03b4 n = o(1).) Combining the above bounds yields 4 . For the martingale term, notice that by Burkholder's inequality,\nE|a(s \u2227 \u03c4 K ) \u2212 a(t \u2227 \u03c4 K )| 4 \u2272 K (t \u2212 s)\nE|b(s \u2227 \u03c4 K ) \u2212 b(t \u2227 \u03c4 K )| 4 = E \u03b4 (M u \u2113 \u2212 M u \u2113\u22121 ) 4 \u2272 E \u03b4 2 (M u \u2113 \u2212 M u \u2113\u22121 ) 2 2 ,\nwhere the sum again runs over steps \u2113 ranging from [s/\u03b4] \u2227 \u03c4 K to [t/\u03b4] \u2227 \u03c4 K . Repeatedly using the inequality (x + y + z) 2 \u2272 x 2 + y 2 + z 2 , it suffices to bound the above quantity for each of the three terms defining the martingale difference M u \u2113 \u2212 M u \u2113\u22121 respectively. For the first term in that martingale difference, observe that\nE \u03b4 2 \u2113 \u2207H \u2113 , \u2207u 2 \u2113\u22121 2 = \u03b4 4 \u2113,\u2113 \u2032 E \u2207H \u2113 , \u2207u 2 \u2113\u22121 \u2207H \u2113 \u2032 , \u2207u 2 \u2113 \u2032 \u22121 \u2264 \u03b4 \u2113 \u03b4 2 E \u2207H \u2113 , \u2207u 4 \u2113\u22121 1/2 2 \u2272 K (t \u2212 s) 2 , (6.4)\nwhere in the second line we used Cauchy-Schwarz and in the last we used item (3) of \u03b4 n -localizability.\nFor the second term in the martingale difference,\nE \u03b4 4 \u2113 \u2207 2 u(\u2207\u03a6, \u2207H \u2113 ) \u2113\u22121 2 2 \u2264 \u03b4 6 (t \u2212 s) 2 sup x\u2208E * K,n ||\u2207 2 u(x)|| 4 op \u2022 ||\u2207\u03a6(x)|| 4 \u2022 E||\u2207H(x)|| 4 \u2272 K \u03b4 2 (t \u2212 s) 2 ,(6.5)\nby items (1)-( 2) of \u03b4 n -localizability. Finally, by the same reasoning, for the third term,\nE \u03b4 4 \u2113 \u2207 2 u, \u2207H \u2113 \u2297 \u2207H \u2113 \u2212 V 2 \u2113\u22121 2 \u2272 \u03b4 6 (t \u2212 s) 2 sup x\u2208E * K,n ||\u2207 2 u(x)|| 4 op \u2022 E ||\u2207H(x)|| 8 \u2272 K (t \u2212 s) 2 . (6.6)\nAll of the above terms are O((t \u2212 s) 2 ) since 0 \u2264 s, t \u2264 T . Thus we have the claimed (6.2), and by Kolmogorov's continuity theorem, (v n (s \u2227 \u03c4 K )) s , are uniformly 1 /4-Holder and thus the sequence is tight with 1 /4-Holder limit points. Notice furthermore that if we look at\n(v n (t \u2227 \u03c4 K ) \u2212 a n (t \u2227 \u03c4 K )) t ,\nthis sequence is also tight and the limits points are continuous martingales. Let us examine their limiting quadratic variations.\nLet v K n (t) = v n (t \u2227 \u03c4 K )\nand define a K n (t) and b K n (t) analogously. Furthermore, let v K (t), a K (t) and b K (t) be their respective limits which we have shown to exist and be 1 /4-Holder.\nWe will compute the limiting quadratic variation for b K (t). For ease of notation, let\n\u2206M u i \u2113 = M u i \u2113 \u2212 M u i \u2113\u22121 and \u2206E u i \u2113 = E u i \u2113 \u2212 E u i \u2113\u22121 . Notice first that for 1 \u2264 i, j \u2264 k, b K n,i (t)b K n,j (t) \u2212 t 0 \u03b4E \u2206M u i [s/\u03b4]\u2227\u03c4 K \u2206M u j [s/\u03b4]\u2227\u03c4 K ds ,\nis a martingale. We therefore need to consider the limit as n \u2192 \u221e of the integral above. Write\nE[\u2206M u i \u2113 \u2206M u j \u2113 ] = \u27e8\u2207u i , V \u2207u j \u27e9 + \u03b4E[\u27e8\u2207H \u2113 , \u2207u i \u27e9 \u2113\u22121 \u2206E u j \u2113 ] + \u03b4E[\u2206E u i \u2113 \u27e8\u2207H \u2113 , \u2207u j \u27e9 \u2113\u22121 ] (6.7) + \u03b4 2 E[\u2206E u i \u2113 \u2206E u j \u2113 ] .\nConsider the integrals of \u03b4 times each of these four terms separately. For the first term,\nsup t\u2264T t 0 \u03b4 \u27e8\u2207u i , V \u2207u j \u27e9 [s/\u03b4]\u2227\u03c4 K \u2212 \u03a3 ij (v K n (s))ds \u2264 T sup x\u2208E * K,n |\u03b4 \u27e8\u2207u i , V \u2207u j \u27e9 (x) \u2212 \u03a3 ij (u n (x))| ,\ngoes to zero as n \u2192 \u221e by the assumption in (2.3). We now reason that the integrals of \u03b4 times the other three terms in (6.7) all go to zero as n \u2192 \u221e. The second and third are identical: by Cauchy-Schwarz,\nsup x\u2208E * K,n |\u03b4 2 E[\u27e8\u2207H, \u2207u i \u27e9\u2206E u j \u2113 ]| \u2264 \u03b4 2 E[\u27e8\u2207H, \u2207u i \u27e9 2 ] 1/2 E[(\u2206E u i \u2113 ) 2 ] 1/2 .\nThe first expectation contributes \u03b4 \u22121/2 by the first part of item (3) of localizability. Also,\nE[(\u2206E u i \u2113 ) 2 ] 1/2 \u2272 E[\u27e8\u2207 2 u i , \u2207\u03a6 \u2297 \u2207H\u27e9 2 ] 1/2 + E[\u27e8\u2207 2 u i , \u2207H \u2297 \u2207H \u2212 V \u27e9 2 ] 1/2 . (6.8)\nThe first of these terms is at most \u03b4 \u22121 as argued in (6.5). The second is o(\u03b4 \u22123/2 ) by the second part of item (3) in the definition of localizability. As such, we are able to conclude that\nsup t\u2264T t 0 \u03b4 2 E[\u27e8\u2207H, \u2207u i \u27e9 [s/\u03b4]\u2227\u03c4 K \u2206E u j [s/\u03b4]\u2227\u03c4 K ]ds , goes to zero as n \u2192 \u221e.\nThe integral of \u03b4 times the fourth term in (6.7) is handled similarly using Cauchy-Schwarz and the bound of o(\u03b4 \u22123/2 ) on (6.8).\nAltogether, we end up with\nlim n\u2192\u221e sup i,j\u2264k sup t\u2264T t 0 \u03b4E[\u2206M u i [s/\u03b4]\u2227\u03c4 K \u2206M u j [s/\u03b4]\u2227\u03c4 K ]ds \u2212 t 0 \u03a3 ij (v K n (s))ds = 0 .\nThus, if we consider the continuous martingales given by b K (t), its angle bracket is, by definition, given by\n\u27e8b K \u27e9 t = t 0 \u03a3(v K (s))ds .\nBy Ito's formula for continuous martingales (see, e.g., [28, Theorem 5.2.9]), we have that f\n(v t ) \u2212 t 0 Lf (v s )ds is a martingale for all f \u2208 C \u221e 0 (R k ),where\nL = 1 2 k ij=1 \u03a3 ij \u2202 i \u2202 j \u2212 k i=1 h i \u2202 i .\nSince, by assumption, h, \u221a \u03a3 are locally Lipschitz-and thus Lipschitz on E K -this property uniquely characterizes the solutions to (2.4) (see, e.g., [70,Theorem 6.3.4]). Thus v K converges to the solution of (2.4) stopped at \u03c4 K . By a standard localization argument [70,, every limit point v(t) of v n (t) solves the SDE (2.4) (using here that E K is an exhaustion by compact sets of R k ). \u25a1", "publication_ref": ["b3", "b3", "b70", "b70"], "figure_ref": [], "table_ref": []}, {"heading": "Proofs for matrix and tensor PCA", "text": "In this section, we prove the results of Section 3. We will state them in the more general setting where we add a ridge penalty to the loss, so that for \u03b1 \u2265 0 fixed, the loss is given by\nL(x, Y ) = \u22122(\u27e8W, x \u2297k \u27e9 + \u03bb\u27e8x, v\u27e9 k ) + ||x|| 2k + \u03b1 2 \u2225x\u2225 2 + c(Y ) , (7.1)\nwhere c(Y ) only depends on Y . Note that H(x) = \u22122\u27e8W, x \u2297k \u27e9.\nOur first aim is to establish Proposition 3.1, showing that the summary statistics u n = (m, r 2 \u22a5 ) satisfy the conditions of Theorem 2.3 with the desired f , g and \u03a3. We begin by checking localizability for u n . In what follows, for ease of notation we will denote r 2 = r 2 \u22a5 and R 2 = m 2 + r 2 . In these coordinates,\n\u03a6(x) = \u22122\u03bbm k + (r 2 + m 2 ) k + \u03b1 2 (r 2 + m 2 ) + c \u2032 (7.2)\nLemma 7.1. The distribution of L(x, Y ) depends only on u n = (m, r 2 ). Furthermore, if \u03bb is fixed and \u03b4 n = O(1/n), then u n is \u03b4 n -localizable for E K being the centered balls of radius K in R 2 .\nProof. We check the items in Definition 2.1 one by one, beginning with item (1). Express the derivatives for u n as\n\u2207m = v , \u2207r 2 = 2(x \u2212 mv) . (7.3)\nNotice that \u2207 2 m = 0, while \u2207 2 r 2 = 2(I \u2212 vv T ), whose operator norm is simply 2, and \u2207 \u2113 u i = 0 for all \u2113 \u2265 3.\nFor item (2), differentiating (7.2), \u2207\u03a6 = \u2202 1 \u03d5\u2207m + \u2202 2 \u03d5\u2207r 2 , where\n\u2202 1 \u03d5 = \u22122\u03bbkm k\u22121 + (2kR 2k\u22122 + \u03b1)m \u2202 2 \u03d5 = kR 2k\u22122 + \u03b1 2 .\nNotice that \u27e8\u2207m, \u2207m\u27e9 = 1, \u2207m, \u2207r 2 = 0, and \u2207r 2 , \u2207r 2 = 4r 2 . Consider\n\u2225\u2207\u03a6\u2225 \u2264 |\u2202 1 \u03d5|\u2225\u2207m\u2225 + |\u2202 2 \u03d5|\u2225\u2207r 2 \u2225 ;\nthe bounding quantity is evidently a continuous function of m, r 2 and therefore as long as x is such that (m, r 2 ) \u2208 E K , it is bounded by some C(K). Next, if we consider If w = \u2207m = v then \u2225w\u2225 = 1 and if w = \u2207r 2 = 2(x \u2212 mv) then \u2225w\u2225 \u2264 C(K), so in both cases this is at most C(k, K)n 2 . Finally, \u2207 2 u is only non-zero if u = r in which case it is I \u2212 vv T . Then,\nE[\u2225\u2207H\u2225 8 ] \u2264 C k E[\u2225W (x, . . . , x, \u2022)\u2225 8 ] \u2264 E\u2225W \u2225 8 op \u2022 R 8k \u2264 C(k,\nE[\u27e8\u2207 2 r, \u2207H \u2297 \u2207H \u2212 V \u27e9 2 ] \u2264 2E[\u2225\u2207H\u2225 4 ] \u2264 C(k, K)n 2\nby the second item in the definition of localizability, and evidently the right-hand side is\nO(\u03b4 \u22122 ) if \u03b4 n = O(1/n). \u25a1\nProof of Proposition 3.1. Having checked localizability for u n , we apply Theorem 2.3. To compute f , by the above,\nf m = \u22122\u03bbkm k\u22121 + (2kR 2k\u22122 + \u03b1)m , f r 2 = 2r 2 (2kR 2k\u22122 + \u03b1) .\nWe next turn to calculating the corrector. For this, we first calculate the matrix\nV = E[\u2207H \u2297 \u2207H].\nRecalling that H = \u22122\u27e8W, x \u2297k \u27e9 where W is an i.i.d. Gaussian k-tensor, we have that\nV ij = E[\u2202 i H\u2202 j H] = 4k(k \u2212 1)x i x j R 2k\u22124 + 4kR 2k\u22122 1{i = j} . (7.4)\nIn particular, for \u03b4 = c \u03b4 /n, we have \u03b4L \u03b4 m = 0 and\n\u03b4L \u03b4 r 2 = 4c \u03b4 n k (n \u2212 1)R 2k\u22122 + (k \u2212 1)r 2 R 2k\u22124\nfrom which we obtain in the limit that n \u2192 \u221e that g m = 0 and g r 2 = 4c \u03b4 kR 2k\u22122 . Together, these yield the ODE system of (3.1),\nu 1 = 2u 1 (\u03bbku k\u22122 1 \u2212 kR 2k\u22122 \u2212 \u03b1) ,u 2 = \u2212(4u 2 \u2212 4c \u03b4 )kR 2k\u22122 \u2212 2\u03b1u 2 .\nwhich in the \u03b1 = 0 case matches Proposition 3.1. Finally, to see that \u03a3 = 0, consider \nJV J T = 4k(k \u2212 1)m 2 R 2k\u22124 + 4kR 2k\u22122 4k(k \u2212 1)m(R 2 \u2212 m)R 2k\u22124 4k(k \u2212 1)m(R 2 \u2212 m)R 2k\u22124 4k(k \u2212 1)(R 2 \u2212 m) 2 R\n\u03bbku k\u22121 1 = kR 2k\u22122 + \u03b1 u 1 , and 2c \u03b4 kR 2k\u22122 = 2kR 2k\u22122 + \u03b1 u 2 .\nIf u 1 = 0, then R 2 = u 2 and there are two possible fixed points: either u 2 = 0 or u 2 solves\nku k\u22122 2 (2c \u03b4 \u2212 2u 2 ) = \u03b1. Notice that if k = 2, this has a nontrivial solution of the form c \u03b4 \u2212 \u03b1 2 = u 2 , provided \u03b1 < \u03b1 c (2) := 2c \u03b4 , and if k > 2, this has a nontrivial solution provided \u03b1 \u2264 max x\u22650 kx k\u22122 (2c \u03b4 \u2212 2x) at c \u03b4 (k \u2212 2)x k\u22123 \u2212 (k \u2212 1)x k\u22122 = 0 i.e., c \u03b4 (k\u22122) k\u22121 = x. This gives \u03b1 < \u03b1 c (k) := 2c k\u22121 \u03b4 k(k \u2212 1) \u2212(k\u22121) (k \u2212 2) k\u22122 .\nEvidently when we take \u03b1 = 0, then its non-trivial solution is at u 2 = 1 for all k \u2265 2.\nAlternatively, if u 1 \u0338 = 0 at a fixed point, then we can simplify further and get\n\u03bbu k\u22122 1 = R 2k\u22122 + \u03b1/k ,and\nkR 2k\u22122 = (kR 2k\u22122 + \u03b1)u 2 ,\nso that at the fixed point,\nu k\u22122 1 = kR 2k\u22122 + \u03b1 \u03bbk , and u 2 = 2c \u03b4 kR 2k\u22122 2kR 2k\u22122 + \u03b1 .\nFor simplicity of calculations, set \u03b1 = 0 as is the case in Proposition 3.1. Then, we simply get u 2 = c \u03b4 . In the case of k = 2, we also find that there is a solution if and only if \u03bb > c \u03b4 , in which case R 2 = \u03bb, from which together with R 2 = u 2 1 + u 2 , we also get\nu 1 = \u00b1 \u221a \u03bb \u2212 c \u03b4 .\nIn the general case of k > 2, we find that R\n2 = c \u03b4 + \u03bb \u2212 2 k\u22122 R 4(k\u22121)\nk\u22122 . This has real solutions (all of which have R \u2265 u 2 = c \u03b4 as required) whenever \u03bb > \u03bb c (k) defined as\n\u03bb c (k) := c \u03b4 k k/2 (2k \u2212 2) k\u22121 (k \u2212 2) (k\u22122)/2 . (7.6) (Interpreting 0 0 = 1, this returns \u03bb c (2) = c \u03b4 .\n) With this \u03bb, whenever \u03bb > \u03bb c (k), the equation for R 2 has exactly two real solutions, both of which are at least c \u03b4 which we can denote by\n\u03c1 \u2020 (k, \u03bb) := inf{\u03c1 \u2265 1 : \u03bb \u2212 2 k\u22122 \u03c1 2(k\u22121) k\u22122 \u2212 \u03c1 + c \u03b4 = 0} , \u03c1 \u22c6 (k, \u03bb) := sup{\u03c1 \u2265 1 : \u03bb \u2212 2 k\u22122 \u03c1 2(k\u22121) k\u22122 \u2212 \u03c1 + c \u03b4 = 0} .\nWhen \u03bb > \u03bb c (k), \u03c1 \u2020 < \u03c1 \u22c6 and when \u03bb = \u03bb c (k), the two are equal. Given this, we can then solve for u 1 at the corresponding fixed point, and find that they occur at\nm \u2020 (k, \u03bb) = \u03c1 \u2020 \u2212 c \u03b4 ,and\nm \u22c6 (k, \u03bb) = \u221a \u03c1 \u22c6 \u2212 c \u03b4 , (7.7)\nas claimed. \u25a1 7.2. Effective dynamics for the population loss. In practice, one is interested in tracking the loss, or ideally, the generalization error. In this subsection, we add the generalization error \u03a6 to our set of summary statistics and obtain limiting equations for its evolution from (3.4). Recalling (7.2), the fact that \u03a6 is a localizable summary statistic follows from the facts that \u2225\u2207m\u2225, \u2225\u2207r 2 \u2225 \u2264 C(K), and the fact that \u03a6 is a smooth n-independent function of m, r 2 .\nFor simplicity of calculations let us stick to \u03b1 = 0.\nf \u03a6 = \u27e8\u2207\u03a6, \u2207\u03a6\u27e9 = 4\u03bb 2 k 2 m 2(k\u22121) \u2212 8\u03bbk 2 m k R 2k\u22122 + 4k 2 R 4k\u22124 m 2 + 4k 2 r 2 R 4k\u22124 = 4k 2 m 2 \u03bb 2 m 2(k\u22122) \u2212 2\u03bbm k\u22122 R 2k\u22122 + R 4k\u22124 + 4k 2 r 2 R 4k\u22124 .\nNext, consider the corrector for \u03a6. For this, notice that\n1 2 \u2207 2 \u03a6 = \u2212\u03bbk(k \u2212 1)m k\u22122 \u2207m \u22972 + kR 2k\u22122 \u2207m \u22972 + k(k \u2212 1)R 2(k\u22122) (2m\u2207m + \u2207r 2 ) \u2297 \u2207m + k(k \u2212 1)R 2(k\u22122) (2m\u2207m \u2297 \u2207r 2 + \u2207r 2 \u2297 \u2207r 2 ) + 1 2 \u2202 2 \u03d5\u2207 2 r 2 .\nRecalling V from (7.4), and taking \u03b4 = c \u03b4 /n, all the terms in ij V ij \u2202 i \u2202 j \u03a6 vanish in the limit except the contribution from the \u2207 2 r 2 , which yields g \u03a6 = lim n\u2192\u221e \u03b4L \u03b4 \u03a6 = 4c \u03b4 k 2 R 4(k\u22121) Finally, we wish to compute the volatility for the stochastic part of the evolution of \u03a6. For this, consider \u2207\u03a6V \u2207\u03a6 T and notice that all the entries of that matrix are continuous functions of u n and thus go to zero when multiplied by \u03b4 = O(1/n).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "7.3.", "text": "Diffusive limits at the equator. In this subsection, we develop the stochastic limit theorems for the rescaled observables about the axis m = 0. Here we take as variables (\u0169 1 ,\u0169 2 ) = ( \u221a nm, r 2 ). For simplicity of presentation, we take \u03b1 = 0 and c \u03b4 = 1.\nProof of Proposition 3.2. We begin by checking localizability. The change from the original variables is in the J matrix, in which now \u2207\u0169 1 = \u221a n\u2207m = \u221a nv. This does not affect items (1)-(2) of localizability; for item (3), notice that\nE[\u27e8\u2207H, \u2207m\u27e9 4 ] = n 2 E[\u27e8\u2207H, v\u27e9 4 ] \u2264 n 2 E[W 4 1,...,1 ] \u2264 Cn 2 . The second part of item (3) is unchanged since \u2207 2\u0169 1 = 0. Computing the drifts, \u27e8\u2207\u03a6, \u2207\u0169 1 \u27e9 = \u2212 2k\u03bb \u221a nm k\u22121 + 2k \u221a nR 2k\u22122 m = \u22122k\u03bbn \u2212 k\u22122 2\u0169 k\u22121 1 + 2k(r 2 + (\u0169 2 1 /n)) k\u22121\u0169 1 , \u27e8\u2207\u03a6, \u2207r 2 \u27e9 =4kr 2 R 2k\u22122 = 4kr 2 (r 2 + (\u0169 2 1 /n)) k\u22121 .\nTaking limits as n \u2192 \u221e, as long as \u03bb is fixed in n, we see that f is given by\nf\u0169 1 = \u22122k\u03bb\u0169 k\u22121 1 + 2k\u0169 k\u22121 2\u0169 1 k = 2 2k\u0169 k\u22121 2\u0169 1 k \u2265 3 , and f\u0169 2 = 4k\u0169 k 2 .\nWe turn to obtaining the correctors in these rescaled coordinates. Evidently \u03b4L\u0169 1 = 0 still by linearity of\u0169 1 . Following the calculation for the corrector, it is now given by g\u0169 2 = 4k\u0169 k\u22121 2 . Next we consider the volatility of the stochastic process one gets in the limit. Recalling JV J T from (7.5), and noticing that the rescaling J \u2192J multiplies its (1, 1)-entry by n and its off-diagonal entries by \u221a n, we find that in the new coordinates,\nJVJ T = 4k(k \u2212 1)\u0169 2 1 R 2k\u22124 + 4knR 2k\u22122 4k(k \u2212 1)\u0169 1 (R 2 \u2212 m)R 2k\u22124 4k(k \u2212 1)\u0169 1 (R 2 \u2212 m)R 2k\u22124 4k(k \u2212 1)(R 2 \u2212 m) 2 R 2k\u22124 (7.8)\nMultiplying by \u03b4 = 1/n and taking the limit as n \u2192 \u221e, the only entry of this matrix that survives is from \u03a3 11 where we get \u03a3 11 = 4k\u0169 k\u22121 2 as claimed. \u25a1\nRegarding the discussion in the k \u2265 3 case of (3.3), when \u03bb n = \u039bn (k\u22122)/2 , observe that the first term in \u27e8\u03a6, \u2207\u0169 1 \u27e9 above would not vanish and would instead converge to \u22124k\u039b\u0169 k\u22121 1 . 7.4. Diffusive limit for the radius. We now show how to rescale the radial term r 2 to obtain a diffusive limit for r 2 about r 2 = 1. (For readability, we take the case c \u03b4 = 1 though an analogous result works for general c \u03b4 .) To this end,\nconsider\u0169 n = (\u0169 1 ,\u0169 2 ) = ( \u221a nm, \u221a n(r 2 \u2212 1)\n). Now J is in terms of \u2207\u0169 1 = \u221a n\u2207m and \u2207\u0169 2 = \u221a n\u2207u 2 . Let us verify localizability for\u0169 n ; the only changes as compared to the previous subsection are those entailing\u0169 2 .\nFor item (1), \u2225\u2207 2\u0169 2 \u2225 op = O( \u221a n) and \u2207 3\u0169 2 = 0. For the first part of item ( 3),\nE[\u27e8\u2207H, \u2207\u0169 2 \u27e9 4 ] = n 2 E[\u27e8\u2207H, 2(x \u2212 mv)\u27e9 4 ] \u2272 n 2 (R 4k + m 4 )E[W 4 1,...,1 ]) \u2272 K n 2\n, where we used in the first inequality that the law of H is rotation invariant and H is a k-homogenous function. For the second part of item ( 3),\nE[\u27e8\u2207 2\u0169 2 , \u2207H \u2297 \u2207H \u2212 V \u27e9 2 ] \u2264 nVar(\u2225\u2207H\u2225 2 ) . We now express Var(\u2225\u2207H\u2225 2 ) = i Var((\u2202 i H) 2 ) + i\u0338 =j Cov((\u2202 i H) 2 , (\u2202 j H) 2 ) .\nThe \u2202 i H are Gaussian with mean zero, and by (7.4), variance 2) . Recall the following fact about Gaussians: if X, Y are Gaussians with variances \u03c3 2 and covariance t, then Cov(X 2 , Y 2 ) \u2264 Ct 2 \u03c3 4 for some universal constant C. Also, Var(X 2 ) \u2264 C\u03c3 4 . Applying this to \u2202 i H, we get\nC \u2032 k R 2(k\u22122) x 2 i +C k R 2(k\u22121) and covariance C k x i x j R 2(k\u2212\nVar(\u2225\u2207H\u2225 2 ) \u2272 K n + i,j x 2 i x 2 j \u2272 K n .\nCombined with the above, this gives a bound of n 2 = O(\u03b4 \u22122 ) on the second part of item (3). We now calculate the resulting drifts. For f , write\nA n u 1 = \u22122k\u03bb1 k=2\u0169 k\u22121 1 + 2kr 2(k\u22121)\u0169 1 = \u22122k\u03bb1 k=2\u0169 k\u22121 1 + 2k(1 + n \u22121/2\u0169 2 ) k\u22121 A n\u01692 = 4kn 1/2 r 2 (r 2 + (\u0169 2 1 /n)) k\u22121 = 4kn 1/2 (1 + n \u22121/2\u0169 2 )(1 + n \u22121/2\u0169 2 + n \u22121\u01692 1 ) k\u22121 = 4kn 1/2 + 4k 2\u0169 2 +o(1)\nWe next calculate the prelimits of the corrector. Evidently \u03b4L\u0169 1 = 0 still by linearity of\u0169 1 and\n\u03b4L\u0169 2 = \u221a n\u03b4L \u03b4 r 2 = 4 \u221a n k (n \u2212 1)R 2k\u22122 + (k \u2212 1)(1 + n \u22121/2\u0169 2 )R 2k\u22124\nCombining terms and sending n \u2192 \u221e, we obtain\nf\u0169 1 \u2212 g\u0169 1 = \u22122k\u03bb1 k=2\u0169 k\u22121 1 + 2k , and f\u0169 2 \u2212 g\u0169 2 = 4k\u0169 2 .\nIt remains to compute the volatility of the stochastic process one gets in the limit. Recalling JV J T from (7.5) and noticing that the rescaling J toJ has now multiplied all four of its entries by n, we find that in the new coordinates,\nJVJ T = 4k(k \u2212 1)\u0169 2 1 R 2k\u22124 + 4knR 2k\u22122 4k(k \u2212 1)n 1/2\u0169 1 (R 2 \u2212 m)R 2k\u22124 4k(k \u2212 1)n 1/2\u0169 1 (R 2 \u2212 m)R 2k\u22124 4k(k \u2212 1)n(R 2 \u2212 m) 2 R 2k\u22124 .(7.9)\nMultiplying by \u03b4 = 1/n and taking the limit as n \u2192 \u221e, the two entries of this matrix that survive are \u03a3 11 and \u03a3 22 , where \u03a3 11 = 4k and \u03a3 22 = 4k(k \u2212 1). All in all, we obtain (3.5).", "publication_ref": ["b1", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Proofs for the binary Gaussian mixture model", "text": "Recall the cross-entropy loss for the binary GMM with SGD from (4.1), and recall the set of summary statistics u n from (4.2).\nLemma 8.1. The distribution of L((v, W )) depends only on u n from (4.2). In particular, we have that \u03a6(x) = \u03d5(u n ) for some \u03d5. Furthermore, u n satisfy the bounds in item (1) of Definition 2.1 if E K is the ball of radius K in R 2N +2 . Proof. Let X \u00b5 \u223c N (\u00b5, I/\u03bb) and X \u2212\u00b5 \u223c N (\u2212\u00b5, I/\u03bb). Then, notice that\nL((v, W )) d = \u2212v \u2022 g(W X \u00b5 ) + log(1 + e v\u2022g(W X\u00b5) ) + p(v, W ) w. prob. 1/2 log(1 + e v\u2022g(\u2212W X\u00b5) ) + p(v, W ) w. prob. 1/2 .\nNext, notice that as a vector, (W\n1 X \u00b5 , W 2 X \u00b5 ) is distributed as (m 1 + Z 1,\u00b5 m 1 + Z 1,\u22a5 , m 2 + Z 2,\u00b5 m 2 + Z 2,\u22a5 ), where Z 1,\u00b5 , Z 2,\u00b5 are i.i.d. N (0, \u03bb \u22121 )\n, and Z 1,\u22a5 , Z 2,\u22a5 are jointly Gaussian with means zero and covariance\n\u03bb \u22121 R \u22a5 11 R \u22a5 12 R \u22a5 12 R \u22a5 22 (8.1)\nSimilarly, the distribution of W X \u2212\u00b5 also only depends on\n(m i , R \u22a5 ij ) i,j . Finally, p(v, W ) = \u03b1 2 v 2 1 + v 2 2 + m 2 1 + R \u22a5 11 + m 2 2 + R \u22a5 22\nTherefore, at any point (v, W ), the law of L((v, W )), and thus \u03a6, is simply a function of u n (v, W ).\nTo see that the summary statistics satisfy the bounds of item (1) in Definition 2.1, write\n\u2207 = (\u2202 v 1 , \u2202 v 2 , \u2207 W 1 , \u2207 W 2 ). Then J = (\u2207u \u2113 ) \u2113 = \uf8ee \uf8ef \uf8ef \uf8f0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 \u00b5 0 W \u22a5 2 2W \u22a5 1 0 0 0 0 \u00b5 W \u22a5 1 0 2W \u22a5 2 \uf8f9 \uf8fa \uf8fa \uf8fb T (8.2)\nFor the higher derivatives, evidently we only have second derivatives in the last 3 variables each of which is given by a block diagonal matrix where only one block is non-zero and is given by an identity matrix. The third derivatives of all elements of u n are zero. \u25a1\nWe can now express the loss, the population loss, and their respective derivatives and they (their laws at a fixed point) will evidently only depend on the summary statistics. One arrives at the following expressions for \u2207L by direct calculation from (4.1).\n\u2207 v i L = (W i \u2022 X)1 W i \u2022X\u22650 \u2212 y + \u03c3(v \u2022 g(W X) + \u03b1v i (8.3) \u2207 W i L = v i X1 W i \u2022X\u22650 \u2212 y + \u03c3(v \u2022 g(W X)) + \u03b1W i (8.4)\nIn what follows, for an arbitrary vector w \u2208 R N , we use the notation\nA i = E X1 W i \u2022X\u22650 \u2212 y + \u03c3(v \u2022 g(W X)(8.5)\n(Notice that if w \u2208 {\u00b5, W i , W \u22a5 i }, then A i \u2022 w is only a function of u n by the same reasoning as used in Lemma 8.1.) Then, we can also easily express\n\u2207 v i \u03a6 = W i \u2022 A i + \u03b1v i \u2207 W i \u03a6 = v i A i + \u03b1W i (8.6)\nand for H = L \u2212 \u03a6,\n\u2207 v i H = W i \u2022 X1 W i \u2022X\u22650 \u2212 y + \u03c3(v \u2022 g(W X) \u2212 A i ,(8.7)\n\u2207 W i H = v i X1 W i \u2022X\u22650 \u2212 y + \u03c3(v \u2022 g(W X) \u2212 A i .(8.8)\nFinally, the matrix V can be expressed as follows:\nV v i ,v j = E (W i \u2022 X)(W j \u2022 X)1 W i \u2022X\u22650 1 W j \u2022X\u22650 (\u2212y + \u03c3(v \u2022 g(W X))) 2 \u2212 (W i \u2022 A i )(W j \u2022 A j ) V v i ,W j = v j E (W i \u2022 X)X1 W i \u2022X\u22650 1 W j \u2022X\u22650 (\u2212y + \u03c3(v \u2022 g(W X))) 2 \u2212 v j (W i \u2022 A i )A j V W i ,W j = v i v j E X \u22972 1 W i \u2022X\u22650 1 W j \u2022X\u22650 (\u2212y + \u03c3(v \u2022 g(W X))) 2 \u2212 v i v j A i \u2297 A j . (8.9)\nLet us conclude this subsection with the following simple preliminary bounds that will be useful towards establishing the conditions of \u03b4 n -localizability from Definition 2.1, and the promised limiting equations. The proofs of these are straightforward using Gaussianity and are provided in Section 10 for completeness.\nLemma 8.2. Fix w \u2208 R n . We have E[|X \u2022 w| 8 ] \u2272 (w \u2022 \u00b5) 8 + \u2225w\u2225 8 \u03bb \u22124 and \u2225A i \u2225 \u2264 C(u n ). Lemma 8.3. For each i, for every R \u22a5\nii < \u221e and every m i > 0, we have\nlim \u03bb\u2192\u221e P W i \u2022 X \u00b5 < 0) = 0 . (8.10)\nFor every v i , R \u22a5 ij and m i \u0338 = 0 for i, j = 1, 2, we have\nlim \u03bb\u2192\u221e E \u03c3(v \u2022 g(W X \u00b5 )) \u2212 \u03c3(v \u2022 g(m)) = 0 . (8.11) Fact 8.1. Fix \u00b5 \u2208 S N \u22121 (1)\n, and let g(x) = x \u2228 0 and X \u00b5 \u223c N (\u00b5, I/\u03bb). There is a function C : R 2 \u2192 R + such that for all \u03bb > 0, \u03b8 \u2208 R, and\n(v i , W i ) \u2208 R \u00d7 R N , E[exp(\u03b8v i g(W i \u2022 X \u00b5 ))] \u2264 exp \u03b8v i m i + 1 2\u03bb \u03b8 2 v 2 i R \u22a5 ii . 8.1.\nVerifying the conditions of Theorem 2.3 for fixed \u03bb. Throughout this section we will take \u00b5 = e 1 . By rotational invariance of the problem, this is without loss of generality, and only simplifies certain expressions. The \u03b4 n -localizability can be seen by application of the moment bounds listed above. Lemma 8.4. For \u03b4 n = O(1/N ) and any fixed \u03bb, the 2-layer GMM with observables u n is \u03b4 n -localizable for E K being balls of radius K about the origin in R 7 .\nProof. The condition on u n was satisfied per Lemma 8.1. Recalling \u2207\u03a6 from (8.6), one can verify that the norm of each of the four terms in \u2207\u03a6 is individually bounded, using the Cauchy-Schwarz inequality together with the bound of Lemma 8.2 on \u2225A i \u2225.\nNext, consider bounding\nE[\u2225\u2207H\u2225 8 ] by i=1,2 E[|\u2207 v i H| 8 ]+E[\u2225\u2207 W i H\u2225 8\n], and recall the expressions for \u2207H from (8.7)- (8.8). Using the trivial bound |\u03c3(x)| \u2264 1, and the inequality (a + b) 8 \n\u2264 C(a 8 + b 8 ), for i \u2208 {1, 2}, the first term is at most C(E[|X \u2022 W i | 8 ] + \u2225W i \u2225 8 \u2225A i \u2225 8\n) which is bounded by a constant depending continuously on u n per Lemma 8.2. If we let Z be a standard Gaussian, the quantity\nE[\u2225\u2207 W i H\u2225 8 ] is controlled by C v 8 i E \u2225X1 W i \u2022X\u22650 \u03c3(\u2212v \u2022 g(W X))\u2225 8 + v 8 i \u2225A i \u2225 8 \u2264 C|v i | 8 1 + E||Z|| 8 \u03bb 4 .\nUsing the well-known bound that E[\u2225Z\u2225 8 ] \u2264 N 4 , and the fact that \u03b4 = O(1/N ), we see that this is at most C\u03b4 \u22124 . We next verify the claimed bound that\n\u03b4 2 n sup i sup x\u2208u \u22121 n (E K ) E[\u27e8\u2207H, \u2207u i \u27e9 4 ] \u2264 C(K) .(8.12)\nWhen u i is v i , this is simply a fourth moment bound on \u2207 v i H, which follows from the 8'th moment by Jensen's inequality. When u i is m i , or R \u22a5 ij , the bound follows from\nE[\u27e8\u2207 W i H, w\u27e9 4 ] \u2264 C|v i | 4 E[|X \u2022 w| 4 ] + \u2225w\u2225 4 \u2225A i \u2225 4 ,\nfor choices of w being either \u00b5 in which case \u2225w\u2225 = 1 or W \u22a5 i in which case \u2225w\u2225 = R \u22a5 ii . For each K, this is at most some constant C(K) using the two bounds of Lemma 8.2.\nFinally, consider the quantity\nE[\u27e8\u2207 2 u, \u2207H \u2297 \u2207H \u2212 V \u27e9 2 ]\n. This is only non-zero for u \u2208 {R \u22a5 ij } for which \u2207 2 u is a block-identity matrix, having operator norm at most 2 in all cases. Therefore, this quantity is at most 4E [\u2225\u2207H\u2225 4 ] which is at most N 2 by the above proved second item in the definition of localizability. This is therefore O(\u03b4 \u22122 n ) = o(\u03b4 \u22123 n ) as needed. \u25a1\nProof of Proposition 4.1. The convergence of the population drift to f from Proposition 4.1 follows by taking the inner products of \u2207L from (8.6) with the rows of J from (8.2), and noticing that A \u00b5 i from (4.3) is exactly A i \u2022 \u00b5 and A \u22a5 ij from (4.3) is exactly A i \u2022 W \u22a5 j . Next consider the convergence of the correctors to the claimed g. The variables u \u2208 {v 1 , v 2 , m 1 , m 2 } are linear so L n u = 0 and for these, g u = 0. For u = R \u22a5 ij for i, j \u2208 {1, 2}, the relevant entries in V are those corresponding to W \u22a5 i and W \u22a5 j . For ease of notation, in what follows let \u03c0 = \u03c3(v \u2022 g(W X)). For ease of calculation taking \u00b5 = e 1 , we have L n R \u22a5 ij = k\u0338 =1 V W ik ,W jk , which by (8.9), and the choice of \u03b4 n = c \u03b4 /N , is given by\n\u03b4 n L n R \u22a5 ij = c \u03b4 N k\u0338 =1 v i v j E (X \u2022 e k ) 2 1 W i \u2022X\u22650 1 W j \u2022X\u22650 (\u2212y + \u03c0) 2 \u2212 (A i \u2022 e k )(A j \u2022 e k ) = c \u03b4 N v i v j E \u2225X \u22a5 \u2225 2 1 W i \u2022X\u22650 1 W j \u2022X\u22650 (\u2212y + \u03c0) 2 \u2212 \u27e8A i \u2212 A \u00b5 i \u00b5, A j \u2212 A \u00b5 j \u00b5\u27e9 . (8.13)\nConsider the two terms separately. First, rewrite\n1 N E[\u2225X \u22a5 \u2225 2 1 W i \u2022X\u22650 1 W j \u2022X\u22650 (\u2212y + \u03c0) 2 ] as E 1 N \u2225X \u22a5 \u2225 2 \u2212 \u03bb \u22121 1 W i \u2022X\u22650 1 W j \u2022X\u22650 (\u2212y + \u03c0) 2 + \u03bb \u22121 B ij .\nOf course the second term is exactly what we want to be g u , so we will show the first term here goes to zero. By Cauchy-Schwarz, if Z \u223c N (0, I \u2212 e \u22972 1 ), the first term above is at most\n\u03bb \u22121 E[( \u2225Z\u2225 2 N \u2212 1) 2 ] 1/2 \u2264 2 \u03bb \u221a\nN , where we used the fact that for a standard Gaussian, g \u223c N (0, 1), we have E[(g 2 \u2212 1) 2 ] = 2. It remains to show the inner product term in (8.13) goes to zero as n \u2192 \u221e. For this term, rewrite\n1 N \u27e8A i \u2212 A \u00b5 i \u00b5, A j \u2212 A \u00b5 j \u00b5\u27e9 = 1 N E (X \u22a5 1 \u2022 X \u22a5 2 )1 W i \u2022X 1 \u22650 1 W j \u2022X 2 \u22650 (\u2212y + \u03c0 1 )(\u2212y + \u03c0 2 ) ,\nwhere X 1 , X 2 are i.i.d. copies of X, and \u03c0 1 , \u03c0 2 are the corresponding \u03c3(v \u2022g(W X 1 )) and \u03c3(v\n\u2022g(W X 2 )). By Cauchy-Schwarz, if Z, Z \u2032 are i.i.d. N (0, I \u2212 e \u22972 1 )\n, this is at most\n1 \u03bbN E[(Z \u2022 Z \u2032 ) 2 ] 1/2 \u2264 1 \u03bb \u221a N .\nThis term therefore also vanishes as n \u2192 \u221e, yielding the desired limit for the corrector,\ng R \u22a5 ij = c \u03b4 v i v j \u03bb E 1 W i \u2022X\u22650 1 W j \u2022X\u22650 (\u2212y + \u03c0) 2 = c \u03b4 v i v j \u03bb B ij .\nwhich we emphasize is only a function of u n . We lastly need to show that the diffusion matrix \u03a3 n goes to zero as n \u2192 \u221e when \u03b4 n = O(1/n). This is straightforward to see by considering any element of JV J T and using Cauchy-Schwarz together with the two bounds of Lemma 8.2 to bound it in absolute value by some C(K) independent of n. Then when multiplying by any \u03b4 n = o(1), this entire matrix will evidently vanish. \u25a1 8.2. The small-noise limit of the effective dynamics. One can now take a \u03bb \u2192 \u221e limit to arrive at the ODE system of Proposition 4.2.\nProof of Proposition 4.2. We begin with considering lim \u03bb\u2192\u221e A \u00b5 i : its limiting value will depend on the signs of both m 1 and m 2 . We can express A \u00b5 i from (4.3) as\nE[(X \u2022 \u00b5)1 W i \u2022X\u22650 (\u2212y + \u03c3(v \u2022 g(W X)))] = 1 2 E (X \u00b5 \u2022 \u00b5)1 W i \u2022X\u00b5\u22650 (\u22121 + \u03c3(v \u2022 g(W X \u00b5 ))) + 1 2 E (\u2212X \u00b5 \u2022 \u00b5)1 W i \u2022X\u00b5\u22640 \u03c3(v \u2022 g(\u2212W X \u00b5 )) .\nWe claim that the two terms on the right-hand side converge to \u2212\n1 2 1 m i >0 \u03c3(\u2212v \u2022 g(m)) and \u2212 1 2 1 m i <0 \u03c3(v \u2022 g(\u2212m)\n) respectively. This follows by e.g., writing the difference as\nE (X \u00b5 \u2022 \u00b5)1 W i \u2022X\u00b5\u22650 \u03c3(\u2212v \u2022 g(W X \u00b5 )) \u2212 1 m i \u22650 \u03c3(\u2212v \u2022 g(m)) (8.14) = E (X \u00b5 \u2022 \u00b5 \u2212 1)1 W i \u2022X\u00b5\u22650 \u03c3(\u2212v \u2022 g(W X \u00b5 )) + E (1 W i \u2022X\u00b5\u22650 \u2212 1 m i \u22650 )\u03c3(\u2212v \u2022 g(W X \u00b5 )) + 1 m i \u22650 E \u03c3(\u2212v \u2022 g(W X \u00b5 )) \u2212 \u03c3(\u2212v \u2022 g(m)) .\nCall these three terms I, II, and III. For I, we use the fact that E[|X \u00b5 \u2022 \u00b5 \u2212 1|] goes to zero as \u03bb \u2192 \u221e; II is evidently bounded by P(W i \u2022 X \u00b5 < 0) when m i > 0 or its symmetric counterpart when m i < 0-both vanishing as \u03bb \u2192 \u221e per (8.10) in Lemma 8.3; finally, III goes to zero as \u03bb \u2192 \u221e by (8.11) in Lemma 8.3.\nPutting the above together, we find that Next consider the limit as \u03bb \u2192 \u221e of A \u22a5 ij from (4.3), which we claim converges to 0. Write\nlim \u03bb\u2192\u221e A \u00b5 i = \u2212 1 2 1 m i >0 \u03c3(\u2212v \u2022 g(m)) \u2212 1 2 1 m i <0 \u03c3(v \u2022 g(\u2212m)) ,at\nA \u22a5 ij = \u2212 1 2 E (X \u00b5 \u2022 W \u22a5 j )1 W i \u2022X\u22650 \u03c3(\u2212v \u2022 g(W X \u00b5 )) (8.15) \u2212 1 2 E (X \u00b5 \u2022 W \u22a5 j )1 W i \u2022X\u00b5<0 \u03c3(v \u2022 g(\u2212W X \u00b5 )) .\nThese two terms are bounded similarly. The absolute value of the first of these is bounded by\n(1/2)E[|X \u00b5 \u2022 W \u22a5 j |] which is at most (1/2) R \u22a5 jj \u03bb \u22121/2 by (8.2).\nThe second is analogously bounded. These evidently go to zero as \u03bb \u2192 \u221e.\nFinally, since\n|B ij | \u2264 1, the quantity g R \u22a5 ij = c \u03b4 v i v j\n\u03bb B ij evidently goes to zero as \u03bb \u2192 \u221e. \u25a1 Remark 5. The above argument used m i \u0338 = 0 for the limit of A \u00b5 i . If one considers the cases when m i = 0, the limiting drifts still apply. For this, it suffices to show that if m i = 0, then A \u00b5 i converges to zero. Without loss of generality, suppose m 1 = 0 and consider\nA 1 \u2022 \u00b5 = E Z 1,\u00b5 1 Z 1,\u22a5 \u22650 \u03c3(\u2212v \u2022 g(Z 1,\u22a5 , m 2 Z 2,\u00b5 + Z 2,\u22a5 )) .\nThis is zero independently of \u03bb by independence of Z 1,\u00b5 from the other Gaussians in the expectation.\nEvidently, every fixed point must have R \u22a5 ij = 0. Furthermore, if we let u i = v i \u2212 m i , the\u1e45\nu i = \u2212 u i 2 \u03c3(\u2212v \u2022 m) \u2212 \u03b1u i m 1 m 2 > 0 \u2212 u i 2 \u03c3(\u2212v i m i ) \u2212 \u03b1u i else ,\nand therefore every fixed point of the ODE system must have u i = 0, which is to say v i = m i . Therefore, it suffices to characterize the fixed points in terms of (v 1 , v 2 ) as claimed. This reduces to v i \u03c3(\u2212\u2225v\u2225 2 ) = 2\u03b1v i v 1 v 2 > 0 if v 1 v 2 > 0 and v i \u03c3(\u2212v 2 i ) = 2\u03b1v i otherwise. Observe first that the point (v 1 , v 2 ) = (0, 0) is a fixed point of this system. If (v 1 , v 2 ) \u0338 = 0, then dividing out by v i , the above reduces to \u03c3(\u2212\u2225v\u2225 2 ) = 2\u03b1 if v 1 v 2 > 0 and \u03c3(\u2212v 2 i ) = 2\u03b1 otherwise. Recalling that C \u03b1 = \u2212 logit(2\u03b1) = log(1 \u2212 2\u03b1) \u2212 log(2\u03b1) we obtain the claimed set of fixed points by inverting these equations (they only have a solution if \u03b1 < 1/4).\nIn order to study the stability of the various fixed points, notice first that the ODE system of Proposition 4.2 is a gradient system for the \u03bb = \u221e population loss,\n\u03a6(v, m) = 1 2 log(1 + e \u2212v\u2022g(m) ) + log(1 + e v\u2022g(\u2212m) ) + \u03b1 2 i=1,2 (v 2 i + m 2 i + R \u22a5 ii ) .\nSince it is a gradient system, with only the specified fixed points, the stability of a fixed point can be deduced by showing it is the minimizer of \u03a6. In particular, the values of \u03a6 at its critical points are given by \u03a6 0 = log 2 at v 1 = v 2 = 0, \u03a6 + = 1 2 (log 2 + log(1 + e \u2212C\u03b1 ) + \u03b1C \u03b1 when v 1 v 2 > 0, and \u03a6 \u2212 = log(1 + e \u2212C\u03b1 ) + 2\u03b1C \u03b1 when v 1 v 2 < 0. It is a simple calculus exercise to show that the smallest of these is \u03a6 0 when \u03b1 > 1/4 and \u03a6 \u2212 when \u03b1 < 1/4.\nTo show that each of the other critical points are all unstable, one can find a direction along which the dynamical system is locally repelled from it. For instance, we will show that the ring of fixed points with v i = m i and R \u22a5 ij = 0 with v 1 v 2 \u2264 0 is unstable, by showing a repelling direction arbitrarily close to the point\nv 1 = \u2212 \u221a C \u03b1 , v 2 = 0. If v 1 = \u2212 \u221a C\n\u03b1 and v 2 = \u03f5 > 0, thenv 2 there reduces to \u03f5( \u03c3(\u2212\u03f5 2 ) 2 \u2212 \u03b1), and as long as \u03b1 < 1/4, there exists \u03f5 > 0 such that \u03c3(\u2212\u03f5 2 ) > 2\u03b1 sov 2 > 0 for all \u03f5 small enough. 8.3. Rescaled effective dynamics around unstable fixed points. In this section, we consider scaling limits of the rescaled effective dynamics in their noiseless limit, where the rescaling is about the unstable set of fixed points given by the quarter circle v 2 1 +v 2 2 = C \u03b1 per item (2) of Proposition 4.2. Let \u03b4 n = c\u03b4 /N, and fix (a 1 , a 2 ) \u2208 R 2 + with a 2 1 + a 2 2 = C \u03b1 , and let u n be the variables of (4.2) with\nv i , m i replaced by\u1e7d i = \u221a N (v i \u2212 a i ) andm i = \u221a N (m i \u2212 a i ).\nProof of Proposition 4.3. We start by considering the drift process for these rescaled variables. Notice that the rescaling induces the transformationJ multiplying J by \u221a N in its entries corresponding to v i , m i . The fact that the rescaled variables satisfy the conditions of Theorem 2.3 follows as in Lemma 8.4 with the only distinction arising in the bound on (8.12), where previously we did not use the \u03b4 2 n factor-in the new coordinates, the factor of \u221a N raised to the fourth power is cancelled out by \u03b4 2 n as long as \u03b4 n = O(1/N ). For the population drift of the new variables, if the variables\u1e7d i ,m i are in a ball of radius K in R 4 (which we take to be our E K ), the signs of m i agree, and therefore\nf\u1e7d i = \u2212 \u221a N v i 2 \u03c3(\u2212v \u2022 m) + \u03b1 \u221a N m i and fm i = \u2212 \u221a N m i 2 \u03c3(\u2212v \u2022 m) + \u03b1 \u221a N v i .\nWe wish to claim that these expressions have consistent limits when\u1e7d i ,m i are localized to E K for fixed K. notice that in m i = a i + N \u22121/2m i and v i = a i + N \u22121/2\u1e7d i , and using\na 2 j = C \u03b1 , v \u2022 m = C \u03b1 + N \u22121/2 j=1,2 a j (\u1e7d j +m j ) + O(1/n) .\nNow Taylor expanding the sigmoid function, and using the definition of C \u03b1 , we get\n\u03c3(\u2212v \u2022 m) = \u03c3(\u2212C \u03b1 ) + (v \u2022 m \u2212 C \u03b1 )\u03c3(\u2212C \u03b1 )(1 \u2212 \u03c3(\u2212C \u03b1 )) + O(n \u22121 ) = 2\u03b1 + N \u22121/2 a j j=1,2 \u1e7d j +m j (2\u03b1)(1 \u2212 2\u03b1) + O(n \u22121 ) .\nPlugging these into the earlier expressions for f\u1e7d i , we see that\nf\u1e7d i = \u2212 N 1/2 a i +m i 2 2\u03b1 + a j N 1/2 j=1,2 \u1e7d j +m j (2\u03b1)(1 \u2212 2\u03b1) + O 1 n + \u03b1(n 1/2 a i +\u1e7d i ) = \u2212\u03b1m i + \u03b1\u1e7d i \u2212 a i (\u03b1 \u2212 2\u03b1 2 )\nj=1,2 a j (\u1e7d j +m j ) + O(n \u22121/2 ) .\nTaking the limit as n \u2192 \u221e, this yields exactly the population drift claimed for the\u1e7d i variable.\nThe calculation for fm i is analogous, and the equations for R \u22a5 ij are evidently unchanged by the transformation of v i , m i to\u1e7d i ,m i . Furthermore, these variables are still linear so no corrector is introduced.\nWe now turn to computing the limiting diffusion matrix \u03a3 in the new variables\u1e7d i ,m i . We first use the following expression for the matrix V when \u03bb = \u221e, by taking the \u03bb = \u221e in (8.9):\nV v i ,v j = m i m j 4 \u2022 \u03c3(\u2212v \u2022 m) 2 m 1 m 2 > 0 \u03c3(\u2212v i m i )\u03c3(\u2212v j m j ) else ,\nwith similar expressions for V v i ,W j and V W i ,W j . Rewriting in\u1e7d andm, we see that in E K ,\nV v i ,v j = \u03b1 2 a i a j + O(n \u22121/2 ) , V v i ,W j = \u00b5(\u03b1 2 a i a j + O(n \u22121/2 )) , V W i ,W j = \u00b5 \u22972 (\u03b1 2 a i a j + O(n \u22121/2 )) .\nNow multiplying this on both sides byJ, for the\u0169 n variables, the two factors of \u221a N fromJ cancel out with the choice of \u03b4 n = 1/N , and in the n \u2192 \u221e limit, leave\u03a3 v i v j =\u03a3 m i m j =\u03a3 v i m j = \u03b1 2 a i a j as claimed. \u25a1", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "Proofs for the XOR Gaussian mixture model", "text": "Fix two orthogonal vectors \u00b5, \u03bd \u2208 R N and recall the cross-entropy loss with penalty p(v, W ) = \u03b1 2 (\u2225v\u2225 2 + \u2225W \u2225 2 ). For the XOR GMM with SGD, the cross-entropy loss is given by\nL(v, W ) = \u2212yv \u2022 g(W X) + log 1 + e v\u2022g(W X) + p(v, W ) (9.1)\nwhere if the class label y = 1, then X is a symmetric binary Gaussian mixture with means \u00b1\u00b5, and if y = 0, then X is a symmetric Gaussian mixture with means \u00b1\u03bd. This has the same form as the loss for the 2-layer binary GMM, and we will find many similarities in the below between them. Indeed, the only difference is in the distribution of X conditionally on the class label y as described, and the fact that v is now in R K and W = (W i ) i=1,...,K is now a K \u00d7 N matrix. In what follows we take n = KN + K. As such, all the formulae of (8.3)-(8.9) also hold for the XOR GMM, but with the law of (y, X) now understood differently.\nRemark 6. We could also have added a bias at each layer, however the Bayes classifier in this problem is an \"X\" centered at the origin so we can safely take the biases to be 0.\n9.1. Summary statistics and localizability. Recall the set of summary statistics u n from (5.1). The next lemma shows that u n form a good set of summary statistics.\nLemma 9.1. The distribution of L((v, W )) depends only on u n from (5.1). In particular, we have that \u03a6(x) = \u03d5(u n ) for some \u03d5. Furthermore, u n satisfy the bounds in item (1) of Definition 2.1 with an exhaustion by balls of R KN +K .\nProof. Let X w = N (w, I/\u03bb) for w \u2208 {\u00b5, \u2212\u00b5, \u03bd, \u2212\u03bd}. Notice that the law of L at a fixed point (v, W ) \u2208 R K+KN can be written as\nL((v, W )) d = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \u2212v \u2022 g(W X \u00b5 ) + log(1 + e v\u2022g(W X\u00b5) ) + p(v, W ) w. prob. 1/4 \u2212v \u2022 g(W X \u2212\u00b5 ) + log(1 + e v\u2022g(W X \u2212\u00b5 ) ) + p(v, W ) w. prob. 1/4 log(1 + e v\u2022g(W X\u03bd ) ) + p(v, W ) w. prob. 1/4 log(1 + e v\u2022g(W X \u2212\u03bd ) ) + p(v, W ) w. prob. 1/4 (9.2)\nNext, notice that as a vector\nW X \u03b9 = (m i + Z i,\u03b9 m \u03b9 i + Z i\u22a5 ) i=1,...,K for \u03b9 \u2208 {\u00b5, \u03bd} ,\nwhere Z i,\u03b9 are i.i.d. N (0, \u03bb \u22121 ) and (Z i\u22a5 ) are jointly Gaussian with covariance matrix\nCov(Z i\u22a5 , Z j\u22a5 ) = \u03bb \u22121 R \u22a5 ij .\nSimilarly, the law of W X \u2212\u03b9 depends only on (m \u03b9 i , R \u22a5 ij ). Finally,\np(v, W ) = \u03b1 2 i=1,...,K v 2 i + R \u22a5 ii .\nTherefore, at a fixed point (v, W ) the law of L(v, W ) is only a function of u n (v, W ).\nTo see that the summary statistics satisfy the bounds of item (1) in Definition 2.1, note that the non-zero entries of J = (\u2207u \u2113 ) \u2113 are as follows.\n\u2202 v i v i = 1 , \u2207 W i m \u00b5 i = \u00b5 , \u2207 W i m \u03bd i = \u03bd , \u2207 W i R \u22a5 jk = W \u22a5 j \u03b4 ij + W \u22a5 k \u03b4 ik , (9.3)\nwhere \u03b4 ij is 1 if i = j and 0 otherwise. For higher derivatives, we only have second derivatives in the R \u22a5 jk variables, each of which is given by a block diagonal matrix where only one block is non-zero and it is twice an identity matrix. Thus the operator norm of these second derivatives is 2. The third derivatives of all elements of u n are zero. \u25a1\nIn the following, let\nA i = E X1 W i \u2022X\u22650 \u2212 y + \u03c3(v \u2022 g(W X)) .\nBy the same reasoning as in Lemma 9.1, if w \u2208 {\u00b5, \u03bd, W i , W \u22a5 i }, then w \u2022 A i is only a function of u n . We then also have the conclusions of Lemma 8.2 for X distributed according to the XOR GMM by simply decomposing it into two mixtures, and we will therefore appeal to this lemma meaning its analogue for the XOR GMM. Proof of Proposition 5.1. The convergence of the population drift to f from Proposition 4.1 follows by taking the inner products of \u2207L from (8.6) with the rows of J from (9.3), and noticing that A \u00b5 i is exactly A i \u2022 \u00b5, A \u03bd i is exactly \u03bd \u2022 A i , and A \u22a5 ij is exactly A i \u2022 W \u22a5 j . We next consider the population correctors. The fact that g v i = g m \u00b5 i = g m \u03bd i = 0 follows from the fact that the Hessians of v i , m \u00b5 i , m \u03bd i are zero. For the corrector g R \u22a5 ij for 1 \u2264 i \u2264 j \u2264 K, the relevant entries of V are those corresponding to W \u22a5 i and W \u22a5 j . For ease of notation, in what follows let \u03c0 = \u03c3(v \u2022 g(W X)).\nSimilar to the calculation of (8.13),\n\u03b4 n L n R \u22a5 ij = c \u03b4 N v i v j E \u2225X \u22a5 \u2225 2 1 W i \u2022X\u22650 1 W j \u2022X\u22650 (\u03c0 \u2212 y) 2 \u2212 \u27e8A i \u2212 A \u00b5 i \u00b5 \u2212 A \u03bd i \u03bd, A j \u2212 A \u00b5 j \u00b5 \u2212 A \u03bd j \u03bd\u27e9 .\nBy the same arguments on the concentration of the norm of Gaussian vectors as used in the binary GMM case, then we deduce from this that\ng R \u22a5 ij = c \u03b4 v i v j \u03bb E 1 W i \u2022X\u22650 1 W j \u2022X\u22650 (\u2212y + \u03c0) 2 = c \u03b4 v i v j \u03bb B ij .\nFinally, let us establish that the limiting diffusion matrix is all-zero whenever \u03b4 n = o(1). This follows exactly as it did in the proof of Proposition 4.1. \u25a1 9.3. Small noise limit of the effective dynamics. The aim of this section is to establish the following small-noise \u03bb \u2192 \u221e limit of the effective dynamics ODE of Proposition 5.1. This will again be quite similar to the analogous proofs for the binary GMM in Section 8, and when these similarities are clear we will omit details. \nv i = m \u00b5 i 4 1 m \u00b5 i \u22650 \u03c3(\u2212v \u2022 g(m \u00b5 )) \u2212 1 m \u00b5 i <0 \u03c3(\u2212v \u2022 g(\u2212m \u00b5 )) \u2212 m \u03bd i 4 1 m \u03bd i \u22650 \u03c3(v \u2022 g(m \u03bd )) \u2212 1 m \u03bd i <0 \u03c3(v \u2022 g(\u2212m \u03bd )) \u2212 \u03b1v i , m \u00b5 i = v i 4 1 m \u00b5 i \u22650 \u03c3(\u2212v \u2022 g(m \u00b5 )) \u2212 1 m \u00b5 i <0 \u03c3(\u2212v \u2022 g(\u2212m \u00b5 )) \u2212 \u03b1m \u00b5 i , m \u03bd i = \u2212 v i 4 1 m \u03bd i \u22650 \u03c3(\u2212v \u2022 g(m \u03bd )) \u2212 1 m \u03bd i <0 \u03c3(\u2212v \u2022 g(\u2212m \u03bd )) \u2212 \u03b1m \u03bd i , and\u1e58 \u22a5 ij = \u22122\u03b1R \u22a5 ij for 1 \u2264 i \u2264 j \u2264 K. Proof.\nLet us begin with convergence of A \u00b5 i . We claim that it converges to\nlim \u03bb\u2192\u221e A \u00b5 i = \u2212 1 4 1 m \u00b5 i >0 \u03c3(\u2212v \u2022 g(m \u00b5 )) \u2212 1 4 1 m \u00b5 i <0 \u03c3(v \u2022 g(\u2212m)) .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "In order to see this, expand", "text": "A i = 1 4 E \u2212 X \u00b5 1 W i \u2022X\u00b5\u22650 (\u03c3(\u2212v \u2022 g(W X \u00b5 ))) \u2212 1 4 E X \u2212\u00b5 1 W i \u2022X \u2212\u00b5 \u22650 (\u03c3(\u2212v \u2022 g(W X \u2212\u00b5 ))) + 1 4 E X \u03bd 1 W i \u2022X\u03bd \u22650 (\u03c3(v \u2022 g(W X \u03bd ))) + 1 4 E X \u2212\u03bd 1 W i \u2022X \u2212\u03bd \u22650 (\u03c3(v \u2022 g(W X \u2212\u03bd ))) .\nThe point will be that when taking the inner product with \u00b5, the first two terms here contribute to the limit and the latter two vanish, while when taking the inner product with \u03bd, the first two terms vanish in the \u03bb \u2192 \u221e limit while the latter two contribute.\nConsider e.g., the first of the four terms above, and inner product with \u00b5. In this case, consider\nE (X \u00b5 \u2022 \u00b5)1 W i \u2022X\u00b5\u22650 \u03c3(\u2212v \u2022 g(W X \u00b5 )) \u2212 1 m \u00b5 i \u22650 \u03c3(\u2212v \u2022 g(m \u00b5\n)) , which is precisely the quantity that was exactly shown to go to zero as \u03bb \u2192 \u221e in (8.14). To see that the third and fourth terms above go to zero when taking their inner product with \u00b5, observe that they become\nE (X \u03bd \u2022 \u00b5)1 W i \u2022X\u03bd \u22650 \u03c3(v \u2022 g(W X \u03bd )) \u2264 E[|X \u03bd \u2022 \u00b5|] ,\nwhich by orthogonality of \u00b5 and \u03bd is at most \u03bb \u22121/2 by the reasoning of Lemma 8.2, therefore vanishing as \u03bb \u2192 \u221e. Together with its analogue for X \u2212\u03bd , this implies the claim for the convergence of A \u00b5 i , as well as its analogous limit of A \u03bd i . We next consider the limit as \u03bb \u2192 \u221e of A \u22a5 ij , which we claim goes to 0. Using the expansion of A i from earlier in this proof, we can consider A \u22a5 ij = A i \u2022 W \u22a5 j as four terms having the form of the terms in (8.15), which were there showed to go to zero as \u03bb \u2192 \u221e. Since W \u22a5 j here is orthogonal both to \u00b5 and \u03bd, the same proof applies.\nFinally, in order to see that the limit as \u03bb \u2192 \u221e of g R \u22a5 dividing one by v i and the other by m \u00b5 i to see that\n4\u03b1 v i m \u00b5 i = 4\u03b1 m \u00b5 i v i , or v 2 i = (m \u00b5 i ) 2 ,\nas claimed. The fixed points having v i < 0 are solved symmetrically.\nOur classification now reduces to understanding the possible values taken by (v 1 , ..., v K ) given their signs (when non-zero). Fix a partition (I 0 , I + \u00b5 , I \u2212 \u00b5 , I + \u03bd , I \u2212 \u03bd ) of {1, ..., K} and consider the set of fixed points having m \u00b5 i = m \u03bd i = v i = 0 for i \u2208 I 0 , m \u00b5 i = v i > 0 on I + \u00b5 and so on as designated by Proposition 9.2; by the above any fixed point is of this form. It remains to check that the values of v i on each of these sets are as described by the proposition.\nIn order to see this, fix e.g., i \u2208 I + \u00b5 . Then, m \u00b5 i = v i and m \u03bd i = 0, and so the fixed point equations reduce to\n4\u03b1v i = v i \u03c3(\u2212v \u2022 g(m \u00b5 )) , or 4\u03b1 = \u03c3 \u2212 j\u2208I + \u00b5 v 2 j ,\nsince the only coordinates where g(m \u00b5 ) will be non-zero are j \u2208 I + \u00b5 , where m \u00b5 j = v j . Inverting the sigmoid function, this implies exactly the claimed j\u2208I + \u00b5 v 2 j = logit(\u22124\u03b1). The cases of I \u2212 \u00b5 , I + \u03bd , I \u2212 \u03bd are analogous, concluding the proof. The count of the number of connected components of fixed points this forms is sensitive to K, so for concreteness let us perform it when K = 4. We first notice that the fixed point at (0, ..., 0) is disconnected from all others. Fixed points corresponding to some (I 0 , ..., I \u2212 \u03bd ) are part of the same connected component of fixed points if one goes from one to the other by moving an element of I \u03b7 \u03b9 (for some \u03b9 \u2208 {\u00b5, \u03bd} and \u03b7 \u2208 {\u00b1} to I 0 without making I \u03b7 \u03b9 empty, or by moving an element of I 0 to a non-empty I \u03b7 \u03b9 . We turn now to studying the stability of these various sets of fixed points. Observe that in the \u03bb \u2192 \u221e limit, the dynamical system of Proposition 9.1 is a gradient system for the population loss 9.5. Diffusive limit on critical submanifolds. We now consider scaling limits of the rescaled effective dynamics in their noiseless limit, where the rescaling is about the unstable set of fixed points given by the product of two quarter circles where I + \u00b5 = {1, 2} and I + \u03bd = {3, 4} (if K > 4, examine the fixed point in which all coordinates after the first four are in I 0 ). In what follows, fix (a 1,\u00b5 , a 2,\u00b5 ) \u2208 R 2 + with a 2 1,\u00b5 + a 2 2,\u00b5 = C \u03b1 , and a 2 3,\u03bd + a 2 4,\u03bd = C \u03b1 , and let u n be the variables of (4.2) with v i , m \u00b5 i , m \u03bd i replaced by\u1e7d\ni = \u221a N (v i \u2212 a i,\u00b5 ) i = 1, 2 \u2212 \u221a N (v i \u2212 a i,\u03bd ) i = 3, 4 andm \u00b5 i = \u221a N (m \u00b5 i \u2212 a i,\u00b5 ) i = 1, 2 0 i = 3, 4 ,m \u03bd i = 0 i = 1, 2 \u221a N (m \u03bd i \u2212 a i,\u03bd ) i = 3, 4\n.\nBy the choices ofm \u00b5 i = 0 andm \u03bd i = 0, we mean that we formally mean that we remove those variables from\u0169 n , and for us now E K will be the ball of radius K in the other coordinates, and the point {0} for (m \u00b5 i ) i=3,4 and (m \u03bd i ) i=1,2 .\nProof of Proposition 5.3. The fact that the rescaled variables\u0169 n satisfy the conditions of Theorem 2.3 follows as in Lemma 9.2 with the only distinction arising in the bound on (8.12), where previously we did not use the \u03b4 2 n factor, but is still satisfied using \u03b4 n = O(1/n). We next consider the population drift of the new variables\u1e7d i ,m \u00b5 i andm \u03bd i . If we take these variables to be in E K , and recall the population drifts etc. in the \u03bb = \u221e setting from Proposition 9.1, for i = 1, 2, we have f\u1e7d i is the n \u2192 \u221e limit of Plugging these in, and taking the n \u2192 \u221e limit we find that for i = 1, 2,\nf\u1e7d i = \u03b1(\u1e7d i \u2212m \u00b5 i ) \u2212 a i,\u00b5 (\u03b1 \u2212 4\u03b1 2 ) k=1,2 a k,\u00b5 (\u1e7d k +m \u00b5 k ) .\nBy a similar reasoning, for i = 3, 4, we have\nf\u1e7d i = \u03b1(\u1e7d i \u2212m \u03bd i ) \u2212 a i,\u03bd (\u03b1 \u2212 4\u03b1 2 ) k=3,4\na k,\u03bd (\u1e7d k +m \u03bd k ) .\nThe claimed equations for fm \u00b5 i when i = 1, 2 and fm \u03bd i when i = 3, 4 hold by analogous reasoning, and the equations for f R \u22a5 ij are evidently unaffected by the change of variables to\u0169 n . Regarding the population correctors, they are also unaffected (all zero) since the variables that were changed in\u0169 n are all linear.\nIt remains to compute the volatility matrix in the coordinates v i ,m \u00b5 i ,m \u03bd i . We first use the following expression for the matrix V when \u03bb = \u221e, by taking \u03bb = \u221e in (8.9). If i, j \u2208 {1, 2}, then\nV v i ,v j = 3 16 m \u00b5 i m \u00b5 j \u03c3(\u2212v \u2022 m \u00b5 ) 2 i, j \u2208 {1, 2} 3 16 m \u03bd i m \u03bd j \u03c3(v \u2022 m \u03bd ) 2 i, j \u2208 {3, 4}\nand if i \u2208 {1, 2} and j \u2208 {3, 4}, then\nV v i ,v j = \u2212 1 16 m \u00b5 i m \u03bd j \u03c3(\u2212v \u2022 m \u00b5 )\u03c3(v \u2022 m \u03bd )\nWhen considering \u03a3 v i ,v j we multiply this by N coming fromJ andJ T , but also multiply by \u03b4 = 1/N , so that taking the limit as n \u2192 \u221e, we get\n\u03a3 v i ,v j = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3\n3\u03b1 2 a i,\u00b5 a j,\u00b5 i, j \u2208 {1, 2} 3\u03b1 2 a i,\u03bd a j,\u03bd i, j \u2208 {3, 4} \u22123\u03b1 2 a i,\u00b5 a j,\u03bd i \u2208 {1, 2}, j \u2208 {3, 4}\n.\nBy a similar reasoning, if i, j \u2208 {1, 2}, then\nV v i ,W j \u2022 \u00b5 = 3 16 v j m \u00b5 i \u03c3(\u2212v \u2022 m \u00b5 ) 2 i, j \u2208 {1, 2} V v i ,W j \u2022 \u03bd = 3 16 v j m \u03bd i \u03c3(v \u2022 m \u03bd ) 2 i, j \u2208 {3, 4}\nand if i \u2208 {1, 2} and j \u2208 {3, 4}, then The quantities in the expectations are at most some universal constant times (w \u2022 \u00b5) 8 + \u03bb \u22124 (w \u2022 Z) 8 . To bound the expectation of the second term here, notice that w \u2022 Z is distributed as N (0, \u2225w\u2225 2 ) implying the desired. The bound on A i goes as follows. Evidently it suffices to let X \u00b5 = \u00b5 + \u03bb \u22121/2 Z for Z \u223c N (0, I), and prove the bound on the norm of\nV v i ,W j \u2022 \u03bd = \u2212 1 16 v j m \u00b5 i \u03c3(\u2212v \u2022 m \u00b5 )\u03c3(v\nE[X \u00b5 1 W i \u2022X\u00b5\u22650 (\u22121 + \u03c3(g(W X \u00b5 )))] = E[(\u00b5 + \u03bb \u22121/2 Z)1 W i \u2022X\u00b5\u22650 (\u22121 + \u03c3(g(W X \u00b5 )))] . Now decompose Z as Z \u00b5 \u00b5 + Z 1,\u22a5 W \u22a5 1 + Z 2,\u22a5 W \u22a5 2 + Z 3\n, where Z \u00b5 \u223c N (0, 1) is independent of (Z 1,\u22a5 , Z 2,\u22a5 ) which is distributed as N (0, A) with A given by (8.1), which is independent of Z 3 distributed as a standard Gaussian vector orthogonal to the subspace spanned by (\u00b5, W \u22a5 1 , W \u22a5 2 ). By independence of Z 3 from the indicator and the argument of the sigmoid, all those terms contribute nothing to the expectation, and therefore,\n\u2225A i \u2225 2 \u2264 w\u2208{\u00b5,W \u22a5 1 ,W \u22a5 2 } E[(X \u2022 w) 2 1 W i \u2022X\u22650 (\u2212y + \u03c3(g(W X)))] \u2264 (1 + R \u22a5 11 + R \u22a5 22 )(1 + \u03bb \u22121 ) .\nHere, we used the first inequality of the lemma. This yields the desired. \u25a1\nProof of Lemma 8.3. The proof of (8.10) is easily seen by rewriting the probability in question as P(W i \u2022 X \u00b5 < 0) = P N (0, \u03bb \u22121 ) < \u2212m i (m\n2 i + R \u22a5 ii ) \u22121/2 = e \u2212m 2 i \u03bb/2(m 2 i +R \u22a5 ii ) ,\nso that as long as m i > 0 this goes to zero as \u03bb \u2192 \u221e. We turn to (8.11). Consider\nE \u03c3(v \u2022 g(W X \u00b5 )) \u2212 \u03c3(v \u2022 g(m)\n) \u2264 E e v\u2022g(W X\u00b5) \u2212 e v\u2022g(m)\n\u2264 E e v 1 g(W 1 \u2022X\u00b5) e v 2 g(W 2 \u2022X\u00b5) \u2212 e v 1 g(m 1 ) e v 2 g(m 2 ) .\nThis in turn is bounded by E e v 2 g(W 2 X\u00b5) e v 1 g(W 1 X\u00b5) \u2212 e v 1 g(m 1 ) + e v 1 g(m 1 ) E e v 2 g(W 2 X\u00b5) \u2212 e v 2 g(m 2 ) . (10.1)\nApplying Cauchy-Schwarz to the first term, it suffices to establish the following bounds E e 2v i g(W i X\u00b5) \u2264 C , and lim\n\u03bb\u2192\u221e E e v i g(W i X\u00b5) \u2212 e v i g(m i ) 2 = 0 .\nTo demonstrate the first of these inequalities, notice that E e 2v i g(W i X\u00b5) \u2264 E e 2v i |W i X\u00b5| \u2264 C .\nuniformly over \u03bb, per Fact 8.1. For the second desired bound, expand e v i g(W i \u2022X\u00b5) \u2212 e v i g(m i ) as\ne v i (W i \u2022X\u00b5)1 W i \u2022X\u00b5\u22650 \u2212 e v i (W i \u2022X\u00b5)1 m i \u22650 + e v i (W i \u2022X\u00b5)1 m i \u22650 \u2212 e v i m i 1 m i \u22650 .\nIt suffices to show the expectation of the square of each of these goes to zero as \u03bb \u2192 \u221e. First,\nE e v i (W i \u2022X\u00b5)1 W i \u2022X\u00b5\u22650 \u2212 e v i (W i \u2022X\u00b5)1 m i \u22650 2 \u2264 (1 \u2228 e v i (W i \u2022X\u00b5) )E[1 W i \u2022X\u00b5\u22650 \u2212 1 m i \u22650 ] .\nIf m i \u0338 = 0, the expectation on the right goes to zero by (8.10). Second, E e v i (W i \u2022X\u00b5)1 m i \u22650 \u2212 e v i m i 1 m i \u22650 2 \u2264 E (e v i (W i \u2022X\u00b5) \u2212 e v i m i ) 2 1 m i \u22650 .\nWhen m i < 0, this is evidently zero; when m i > 0, if G \u03bb \u223c N (0, I/\u03bb), this is\ne 2v i m i E (e v i (W i \u2022G \u03bb ) \u2212 1) 2 .\nwhich goes to zero as O(\u03bb \u22121 ) when \u03bb \u2192 \u221e, by the explicit formula for the moment generating function of the Gaussian W i \u2022 G \u03bb , whose variance is (m\n2 i + R \u22a5 ii )\u03bb \u22121 . \u25a1", "publication_ref": ["b7"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "", "text": "Acknowledgements. The authors thank the anonymous referees for their useful comments and suggestions. The authors thank F. Krzakala, L. Zdeborova, and B. Loureiro for interesting conversations and suggestions, especially suggesting we investigate the role of overparametrization in the XOR GMM. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "\u03bb B ij is zero, which follows from the fact that |B ij | \u2264 1. \u25a1 Proposition 9.2. The fixed points of the ODE system of Proposition 9.1 are classified as follows. If \u03b1 > 1/8, then the only fixed point is at u n = 0. If 0 < \u03b1 < 1/8, then let (I 0 , I + \u00b5 , I \u2212 \u00b5 , I + \u03bd , I \u2212 \u03bd ) be any disjoint (possibly empty) subsets whose union is {1, ..., K}. Corresponding to that tuple (I 0 , I + \u00b5 , I \u2212 \u00b5 , I + \u03bd , I \u2212 \u03bd ), is a set of fixed points that have R \u22a5 ij = 0 for all i, j, and have\nm \u00b5 i = v i > 0 such that i\u2208I + \u00b5 v 2 i = logit(\u22124\u03b1) and m \u03bd i = 0 for all i \u2208\n) and m \u00b5 i = 0 for all i \u2208 I \u2212 \u03bd . In the K = 4 case, these form 39 connected sets of fixed points, and of which 4! = 24 are fixed points that are stable, corresponding to the possible permutations in which each of I + \u00b5 , I \u2212 \u00b5 , I + \u03bd , I \u2212 \u03bd are singletons.\nProof. Evidently, any fixed point must have R \u22a5 ij = 0 for all i, j. Furthermore, the point v i = m \u00b5 i = m \u03bd i = 0 for i = 1, ..., K evidently forms a fixed point of the system. Now suppose there is some fixed point with v i = 0 for some i; in that case, it must be that m \u00b5 i = 0 and m \u03bd i = 0. Therefore, we can select a subset I 0 of {1, ..., K} such that v i = m \u00b5 i = m \u03bd i = 0 for i \u2208 I 0 . For any such choice of I 0 , consider next, i / \u2208 I 0 . We first claim that if v i > 0 at a fixed point, then m \u00b5 i \u2208 {\u00b1v i } and m \u03bd i = 0, whereas if v i < 0 then m \u03bd i \u2208 {\u00b1v i } and m \u00b5 i = 0. To see this, notice that at any fixed point,\n. Since \u03c3 is non-negative, if v i > 0, the sign of the right-hand side of the first equation is the same as the sign of m \u00b5 i so it can have a non-zero solution, while the sign of the right-hand side of the second equation is the opposite of the sign of m \u03bd i , so any such fixed point must have m \u03bd i = 0. To see that m \u00b5 i = \u00b1v i at such a fixed point, now set m \u03bd i = 0 and take the fixed point equations for v i and m \u00b5 i , At a fixed point (which necessarily has v i = m i , R \u22a5 ii = 0, and is characterized by the partition of {1, ..., 4} into I + \u00b5 , I \u2212 \u00b5 , I + \u03bd , I \u2212 \u03bd , this reduces to\n\u00b5 is non-empty and 0 if it is empty, and similarly for I \u2212 \u00b5 , I + \u03bd , I \u2212 \u03bd , this turns into a simple optimization problem over the number of non-empty I + \u00b5 , I \u2212 \u00b5 , I + \u03bd , I \u2212 \u03bd . Just as in the binary GMM case, it becomes evident that when \u03b1 > 1/8, this is minimized at v i = 0 for all i (i.e., they are all empty and I 0 = {1, ..., 4}, whereas when \u03b1 < 1/8 the above is minimized when every one of I + \u00b5 , I \u2212 \u00b5 , I + \u03bd , I \u2212 \u03bd are all non-empty. This yields the global minima of \u03a6 in these coordinates, and ensures the fixed points we claimed were stable are indeed stable.\nTo show the instability of any other connected set of fixed points, the reasoning goes just as in the binary GMM case: consider a small perturbation of the specified critical region in the direction of the stable fixed points and it can be seen by examining the drifts directly, that the dynamical system has a repelling direction. \u25a1 Remark 7. When K > 4, the counting of connected components of fixed points of course changes.\nHowever, what is still clear by an identical calculation is that the sets of fixed points minimizing \u03a6 will still be (0, ..., 0) when \u03b1 > 1/8 and will be all fixed points that have all four of I + \u00b5 , ..., I \u2212 \u03bd being non-empty if \u03b1 < 1/8. Notice that when \u03b1 < 1/8 and K > 4, even the set of stable fixed points become connected to form a single stable manifold. 9.4. 3 \u204432-probability of ballistic convergence to an optimal classifier. We now reason that when K = 4 the ballistic effective dynamics of Proposition 9.1 is such that under an uninformative Gaussian initialization, the probability of being in a basin of attraction of one of the 24 stable fixed points is 3/32. Begin by noticing that if the first layer weights are initialized as W i \u223c N (0, I N /N ) independently for i = 1, ..., 4 and the second layer weights v i are independent standard Gaussians, then the projection onto the coordinate system\nThe \u03b4-functions at zero for m \u00b5 i , m \u03bd i however cause some trouble because of the indicator functions on the sign of m \u00b5 i and m \u03bd i in the equations of Proposition 9.1. In order to handle this, we can instead consider the pre-limit as a mixture (over all the possible signings\n) of initializations where m \u00b5 i \u223c \u03f5 \u00b5 i |Z| for Z being Gaussian of variance 1/N . For any such signing \u03f5, we take the limit per Proposition 9.1 to obtain the limiting ODE's with the indicators taking their values corresponding to the signings \u03f5. Thus the limit with the Gaussian initialization can be thought of as the equal mixture over the same signings \u03f5 of the various ODE's obtained from Proposition 9.1 with the various indicators taking values 0 or 1. With that in mind, can interpret the initial m \u00b5 i (0), m \u03bd i (0) as random variables that take values 0 + and 0 \u2212 with probability 1/2 each, the superscript being the signing dictating which indicator should be 1.\nUnder the flow of Proposition 9.1, if v i (0) is positive, then m \u03bd i stays fixed at zero, and if m \u00b5 i (0) = 0 \u2212 then m \u00b5 i becomes negative infinitesimally quickly, whereas if m \u00b5 i (0) = 0 + then it becomes positive infinitesimally quickly. At any rate, the sign of v i never changes to negative from such an initialization, and similarly if v i (0) is negative, the sign of v i will never change to positive. As such, in order to have a chance at being in the basin of attraction of one of the stable fixed points outlined in Proposition 9.2, it must be the case that two of (v i (0)) i have positive sign and two of them have negative sign; evidently this has probability 4 2 /2 4 = 3/8. Given that two of v i (0) are positive, and two of them are negative-say without loss of generality that i = 1, 2 are the coordinates in which it is positive, and i = 3, 4 are the coordinates in which it is negative-then the dynamical system for (v 1 , v 2 , m \u00b5 1 , m \u00b5 2 ) is exactly the ballistic limit of the two-layer GMM studied in Section 4, for which we found that the probability of converging to a good classifier is 1/2. Similarly, the dynamical system for (v 2 , v 4 , m \u03bd 3 , m \u03bd 4 ) independently gives a further probability 1/2 of converging to its good classifier. Together, these yield a probability of 3/32 of converging to one of the 4! many optimal classifiers for the XOR GMM.\nRemark 8. Generically, if K \u2265 4, by a similar reasoning to the above, in order to fall in the basin of attraction of the stable fixed points, it must be the case that the initialization has some four indices each of which initially belong to I + \u00b5 , I \u2212 \u00b5 , I + \u03bd , I \u2212 \u03bd . This is the probability that v i (0) are positive for at least two indices, and negative for at least two indices, and then among the indices at which v i (0) is positive, there is at least one index where m \u00b5 i is positive and one where it is negative, and similarly with v i (0) negative and m \u03bd i . Doing this combinatorial calculation out, we find that the probability of being in a good initialization is exactly the expression in (5.4). This is easily seen to go to 1 exponentially fast as K \u2192 \u221e since the initial choice of v i (0)'s will typically have around K/2 positive and K/2 negative coordinates, and with exponentially high probability those will have both positive and negative m \u00b5 i and m \u03bd i .", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Learning threshold neurons via the \"edge of stability", "journal": "", "year": "", "authors": "Kwangjun Ahn; Sebastien Bubeck; Sinho Chewi; Yin Tat Lee; Felipe Suarez; Yi Zhang"}, {"ref_id": "b1", "title": "Normal approximation for stochastic gradient descent via non-asymptotic rates of martingale CLT", "journal": "PMLR", "year": "2019", "authors": "Andreas Anastasiou; Krishnakumar Balasubramanian;  Murat;  Erdogdu"}, {"ref_id": "b2", "title": "An introduction to multivariate statistical analysis", "journal": "John Wiley & Sons]", "year": "1962", "authors": "Theodore Wilbur; Anderson "}, {"ref_id": "b3", "title": "A mean-field limit for certain deep neural networks", "journal": "", "year": "2019", "authors": "Dyego Ara\u00fajo; I Roberto; Daniel Oliveira;  Yukimura"}, {"ref_id": "b4", "title": "Understanding gradient descent on the edge of stability in deep learning", "journal": "", "year": "2022-07", "authors": "Sanjeev Arora; Zhiyuan Li; Abhishek Panigrahi"}, {"ref_id": "b5", "title": "Online stochastic gradient descent on non-convex losses from high-dimensional inference", "journal": "Journal of Machine Learning Research", "year": "2021", "authors": "Gerard Ben Arous; Reza Gheissari; Aukosh Jagannath"}, {"ref_id": "b6", "title": "Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices. The Annals of Probability", "journal": "", "year": "2005", "authors": "Jinho Baik; G\u00e9rard Ben Arous; Sandrine P\u00e9ch\u00e9"}, {"ref_id": "b7", "title": "Algorithmic thresholds for tensor PCA", "journal": "Annals of Probability", "year": "2020", "authors": "G\u00e9rard Ben Arous; Reza Gheissari; Aukosh Jagannath"}, {"ref_id": "b8", "title": "Bounding flows for spherical spin glass dynamics", "journal": "Communications in Mathematical Physics", "year": "2020", "authors": "G\u00e9rard Ben Arous; Reza Gheissari; Aukosh Jagannath"}, {"ref_id": "b9", "title": "The landscape of the spiked tensor model", "journal": "Comm. Pure Appl. Math", "year": "2019", "authors": "G\u00e9rard Ben Arous; Song Mei; Andrea Montanari; Mihai Nica"}, {"ref_id": "b10", "title": "Dynamics of stochastic approximation algorithms", "journal": "Springer", "year": "1999", "authors": "Michel Bena\u00efm"}, {"ref_id": "b11", "title": "The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices", "journal": "Advances in Mathematics", "year": "2011", "authors": "Florent Benaych; - Georges; Raj Rao Nadakuditi"}, {"ref_id": "b12", "title": "Adaptive algorithms and stochastic approximations", "journal": "Applications of Mathematics", "year": "1990", "authors": "Albert Benveniste; Michel M\u00e9tivier; Pierre Priouret"}, {"ref_id": "b13", "title": "On-Line Learning and Stochastic Approximations", "journal": "Cambridge University Press", "year": "1999", "authors": "L\u00e9on Bottou"}, {"ref_id": "b14", "title": "Large scale online learning", "journal": "MIT Press", "year": "2004", "authors": "L\u00e9on Bottou; Yan Le Cun"}, {"ref_id": "b15", "title": "The largest eigenvalues of finite rank deformation of large wigner matrices: convergence and nonuniversality of the fluctuations. The Annals of Probability", "journal": "", "year": "2009", "authors": "Mireille Capitaine; Catherine Donati-Martin; Delphine F\u00e9ral"}, {"ref_id": "b16", "title": "The high-dimensional asymptotics of first order methods with random data", "journal": "", "year": "2021", "authors": "Michael Celentano; Chen Cheng; Andrea Montanari"}, {"ref_id": "b17", "title": "Stochastic gradient and Langevin processes", "journal": "PMLR", "year": "2020", "authors": "Xiang Cheng; Dong Yin; Peter Bartlett; Michael Jordan"}, {"ref_id": "b18", "title": "On the global convergence of gradient descent for over-parameterized models using optimal transport", "journal": "", "year": "2018", "authors": "Lenaic Chizat; Francis Bach"}, {"ref_id": "b19", "title": "Gradient descent on neural networks typically occurs at the edge of stability", "journal": "", "year": "2021", "authors": "Jeremy Cohen; Simran Kaur; Yuanzhi Li; Zico Kolter; Ameet Talwalkar"}, {"ref_id": "b20", "title": "The spherical p-spin interaction spin-glass model", "journal": "Zeitschrift f\u00fcr Physik B Condensed Matter", "year": "1993", "authors": "A Crisanti; H-J Horner;  Sommers"}, {"ref_id": "b21", "title": "Analytical solution of the off-equilibrium dynamics of a long-range spin-glass model", "journal": "Phys. Rev. Lett", "year": "1993-07", "authors": "Leticia F Cugliandolo; Jorge Kurchan"}, {"ref_id": "b22", "title": "Self-stabilization: The implicit bias of gradient descent at the edge of stability", "journal": "", "year": "2023", "authors": "Alex Damian; Eshaan Nichani; Jason D Lee"}, {"ref_id": "b23", "title": "Bridging the gap between constant step size stochastic gradient descent and Markov chains", "journal": "Ann. Statist", "year": "", "authors": "Aymeric Dieuleveut; Alain Durmus; Francis Bach"}, {"ref_id": "b24", "title": "Algorithmes stochastiques", "journal": "Math\u00e9matiques & Applications", "year": "", "authors": "Marie Duflo"}, {"ref_id": "b25", "title": "", "journal": "", "year": "1996", "authors": " Springer-Verlag"}, {"ref_id": "b26", "title": "Stochastic approximation and large deviations: Upper bounds and w.p.1 convergence", "journal": "SIAM Journal on Control and Optimization", "year": "1989", "authors": "Paul Dupuis; J Harold;  Kushner"}, {"ref_id": "b27", "title": "Fundamental limits of detection in the spiked Wigner model", "journal": "The Annals of Statistics", "year": "2020", "authors": "Ahmed El Alaoui; Florent Krzakala; Michael Jordan"}, {"ref_id": "b28", "title": "Markov processes", "journal": "John Wiley & Sons, Inc", "year": "1986", "authors": "N Stewart; Thomas G Ethier;  Kurtz"}, {"ref_id": "b29", "title": "The largest eigenvalue of rank one deformation of large wigner matrices", "journal": "Communications in mathematical physics", "year": "2007", "authors": "Delphine F\u00e9ral; Sandrine P\u00e9ch\u00e9"}, {"ref_id": "b30", "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks", "journal": "", "year": "2019", "authors": "Jonathan Frankle; Michael Carbin"}, {"ref_id": "b31", "title": "Escaping from saddle points -online stochastic gradient for tensor decomposition", "journal": "PMLR", "year": "2015-07", "authors": "Rong Ge; Furong Huang; Chi Jin; Yang Yuan"}, {"ref_id": "b32", "title": "Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup", "journal": "", "year": "2019", "authors": "Sebastian Goldt; Madhu Advani; M Andrew; Florent Saxe; Lenka Krzakala;  Zdeborov\u00e1"}, {"ref_id": "b33", "title": "Tight analyses for non-smooth stochastic gradient descent", "journal": "PMLR", "year": "2019-06", "authors": "J A Nicholas; Christopher Harvey; Yaniv Liaw; Sikander Plan;  Randhawa"}, {"ref_id": "b34", "title": "Fast spectral algorithms from sum-ofsquares proofs: tensor decomposition and planted sparse vectors", "journal": "ACM", "year": "2016", "authors": "B Samuel; Tselil Hopkins; Jonathan Schramm; David Shi;  Steurer"}, {"ref_id": "b35", "title": "Tensor principal component analysis via sum-of-square proofs", "journal": "", "year": "2015", "authors": "B Samuel; Jonathan Hopkins; David Shi;  Steurer"}, {"ref_id": "b36", "title": "Statistical thresholds for tensor PCA", "journal": "Ann. Appl. Probab", "year": "2020", "authors": "Aukosh Jagannath; Patrick Lopatto; L\u00e9o Miolane"}, {"ref_id": "b37", "title": "On the distribution of the largest eigenvalue in principal components analysis", "journal": "Annals of statistics", "year": "2001", "authors": "M Iain;  Johnstone"}, {"ref_id": "b38", "title": "Community detection in hypergraphs, spiked tensor models, and sum-of-squares", "journal": "IEEE", "year": "2017", "authors": "Chiheon Kim; S Afonso; Michel X Bandeira;  Goemans"}, {"ref_id": "b39", "title": "Asymptotic behavior of stochastic approximation and large deviations", "journal": "IEEE transactions on automatic control", "year": "1984", "authors": "J Harold;  Kushner"}, {"ref_id": "b40", "title": "Statistical and computational phase transitions in spiked tensor estimation", "journal": "IEEE", "year": "2017", "authors": "Thibault Lesieur; L\u00e9o Miolane; Marc Lelarge; Florent Krzakala; Lenka Zdeborov\u00e1"}, {"ref_id": "b41", "title": "Diffusion approximations for online principal component estimation and global convergence", "journal": "Advances in Neural Information Processing Systems", "year": "2017", "authors": "Chris Junchi Li; Mengdi Wang; Han Liu; Tong Zhang"}, {"ref_id": "b42", "title": "Online ICA: Understanding global dynamics of nonconvex optimization via diffusion processes", "journal": "Curran Associates, Inc", "year": "2016", "authors": "Chris Junchi Li; Zhaoran Wang; Han Liu"}, {"ref_id": "b43", "title": "Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations", "journal": "The Journal of Machine Learning Research", "year": "2019", "authors": "Qianxiao Li; Cheng Tai; E Weinan"}, {"ref_id": "b44", "title": "On the validity of modeling SGD with stochastic differential equations (SDEs)", "journal": "", "year": "", "authors": "Zhiyuan Li; Sadhika Malladi; Sanjeev Arora"}, {"ref_id": "b45", "title": "High-dimensional asymptotics of Langevin dynamics in spiked matrix models", "journal": "", "year": "2022", "authors": "Tengyuan Liang; Subhabrata Sen; Pragya Sur"}, {"ref_id": "b46", "title": "Analysis of recursive stochastic algorithms", "journal": "IEEE Trans. Automatic Control, AC", "year": "1977", "authors": "Lennart Ljung"}, {"ref_id": "b47", "title": "Stochastic gradient descent as approximate Bayesian inference", "journal": "The Journal of Machine Learning Research", "year": "2017", "authors": "Stephan Mandt; D Matthew; David M Hoffman;  Blei"}, {"ref_id": "b48", "title": "Marvels and pitfalls of the Langevin algorithm in noisy high-dimensional inference", "journal": "Physical Review X", "year": "2020", "authors": "Stefano Sarao Mannelli; Giulio Biroli; Chiara Cammarota; Florent Krzakala; Pierfrancesco Urbani; Lenka Zdeborov\u00e1"}, {"ref_id": "b49", "title": "Functional and random central limit theorems for the Robbins-Munro process", "journal": "Journal of Applied Probability", "year": "1976", "authors": "D L Mcleish"}, {"ref_id": "b50", "title": "A mean field view of the landscape of two-layer neural networks", "journal": "", "year": "2018", "authors": "Song Mei; Andrea Montanari; Phan-Minh Nguyen"}, {"ref_id": "b51", "title": "Perceptrons, Reissue of the 1988 Expanded Edition with a new foreword by L\u00e9on Bottou: An Introduction to Computational Geometry", "journal": "MIT press", "year": "2017", "authors": "Marvin Minsky; A Seymour;  Papert"}, {"ref_id": "b52", "title": "On the limitation of spectral methods: From the gaussian hidden clique problem to rank-one perturbations of gaussian tensors", "journal": "", "year": "2015", "authors": "Andrea Montanari; Daniel Reichman; Ofer Zeitouni"}, {"ref_id": "b53", "title": "Stochastic gradient descent, weighted sampling, and the randomized kaczmarz algorithm", "journal": "MIT Press", "year": "2014", "authors": "Deanna Needell; Nathan Srebro; Rachel Ward"}, {"ref_id": "b54", "title": "Measurements of three-level hierarchical structure in the outliers in the spectrum of deepnet hessians", "journal": "PMLR", "year": "2019-06", "authors": " Vardan Papyan"}, {"ref_id": "b55", "title": "SGD in the large: Average-case analysis, asymptotics, and stepsize criticality", "journal": "PMLR", "year": "2021", "authors": "Courtney Paquette; Kiwon Lee; Fabian Pedregosa; Elliot Paquette"}, {"ref_id": "b56", "title": "Asymptotics of sample eigenstructure for a large dimensional spiked covariance model", "journal": "Statistica Sinica", "year": "2007", "authors": "Debashis Paul"}, {"ref_id": "b57", "title": "Statistical limits of spiked tensor models", "journal": "Ann. Inst. Henri Poincar\u00e9 Probab. Stat", "year": "2020", "authors": "Amelia Perry; Alexander S Wein; Afonso S Bandeira"}, {"ref_id": "b58", "title": "Optimality and sub-optimality of pca i: Spiked random matrix models", "journal": "The Annals of Statistics", "year": "2018", "authors": "Amelia Perry;  Alexander S Wein; S Afonso; Ankur Bandeira;  Moitra"}, {"ref_id": "b59", "title": "Non-convex learning via stochastic gradient Langevin dynamics: a nonasymptotic analysis", "journal": "Proceedings of Machine Learning Research", "year": "2017-07", "authors": "Maxim Raginsky; Alexander Rakhlin; Matus Telgarsky"}, {"ref_id": "b60", "title": "Classifying high-dimensional gaussian mixtures: Where kernel methods fail and neural networks succeed", "journal": "PMLR", "year": "2021", "authors": "Maria Refinetti; Sebastian Goldt; Florent Krzakala; Lenka Zdeborov\u00e1"}, {"ref_id": "b61", "title": "A statistical model for tensor PCA", "journal": "", "year": "2014", "authors": "Emile Richard; Andrea Montanari"}, {"ref_id": "b62", "title": "A stochastic approximation method", "journal": "Ann. Math. Statistics", "year": "1951", "authors": "Herbert Robbins; Sutton Monro"}, {"ref_id": "b63", "title": "Trainability and accuracy of neural networks: An interacting particle system approach", "journal": "", "year": "2018", "authors": "M Grant; Eric Rotskoff;  Vanden-Eijnden"}, {"ref_id": "b64", "title": "Dynamics of on-line gradient descent learning for multilayer neural networks", "journal": "", "year": "1995", "authors": "David Saad; Sara Solla"}, {"ref_id": "b65", "title": "On-line learning in soft committee machines", "journal": "Physical Review E", "year": "1995", "authors": "David Saad; Sara A Solla"}, {"ref_id": "b66", "title": "Empirical analysis of the hessian of over-parametrized neural networks", "journal": "CoRR", "year": "2017", "authors": "Levent Sagun; Utku Evci; V Ugur G\u00fcney; Yann N Dauphin; L\u00e9on Bottou"}, {"ref_id": "b67", "title": "Who is afraid of big bad minima? analysis of gradient-flow in spiked matrix-tensor models", "journal": "", "year": "2019", "authors": "Stefano Sarao Mannelli; Giulio Biroli; Chiara Cammarota; Florent Krzakala; Lenka Zdeborov\u00e1"}, {"ref_id": "b68", "title": "Convergence of stochastic gradient descent for PCA", "journal": "PMLR", "year": "2016", "authors": "Ohad Shamir"}, {"ref_id": "b69", "title": "Mean field analysis of neural networks: A central limit theorem", "journal": "", "year": "2020", "authors": "Justin Sirignano; Konstantinos Spiliopoulos"}, {"ref_id": "b70", "title": "Multidimensional diffusion processes", "journal": "Springer-Verlag", "year": "2006", "authors": "W Daniel; S R Stroock;  Srinivasa Varadhan"}, {"ref_id": "b71", "title": "Phase retrieval via randomized Kaczmarz: theoretical guarantees. Information and Inference: A", "journal": "Journal of the IMA", "year": "2018-04", "authors": "Yan Shuo Tan; Roman Vershynin"}, {"ref_id": "b72", "title": "Ordinary differential equations and dynamical systems", "journal": "American Mathematical Soc", "year": "2012", "authors": "Gerald Teschl"}, {"ref_id": "b73", "title": "Phase diagram of stochastic gradient descent in high-dimensional two-layer neural networks", "journal": "", "year": "2022", "authors": "Rodrigo Veiga; Ludovic Stephan; Bruno Loureiro; Florent Krzakala; Lenka Zdeborov\u00e1"}, {"ref_id": "b74", "title": "High-Dimensional Probability", "journal": "Cambridge University Press", "year": "2018", "authors": "Roman Vershynin"}, {"ref_id": "b75", "title": "High-dimensional statistics: A non-asymptotic viewpoint", "journal": "Cambridge University Press", "year": "2019", "authors": "J Martin;  Wainwright"}, {"ref_id": "b76", "title": "Scaling limit: Exact and tractable analysis of online learning algorithms with applications to regularized regression and PCA", "journal": "", "year": "2017", "authors": "Chuang Wang; Jonathan Mattingly; Yue Lu"}, {"ref_id": "b77", "title": "A hitting time analysis of stochastic gradient Langevin dynamics", "journal": "Proceedings of Machine Learning Research", "year": "1980", "authors": "Yuchen Zhang; Percy Liang; Moses Charikar"}, {"ref_id": "b78", "title": "Understanding edge-of-stability training dynamics with a minimalist example", "journal": "", "year": "2023", "authors": "Xingyu Zhu; Zixuan Wang; Xiang Wang; Mo Zhou; Rong Ge"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. Matrix PCA summary statistics in dim. n = 1500 run for 10n steps at \u03bb = 0.8 < \u03bb c in (a)-(b) and \u03bb = 1.2 > \u03bb c in (c)-(d). Here, \u00d7 and \u2212 mark the stable fixed points of the systems. (a) and (c) demonstrate the mean-reverting and mean-repellent OU processes that arise as diffusive limits of the m variable, and (b) and (d) depict the trajectories in (m, r 2 \u22a5 ) space.", "figure_data": ""}, {"figure_label": "82223", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "(a) \u03bb = 0. 8 ( 2 Figure 2 . 2 Figure 3 .82223Figure 2. Matrix PCA in dimension n = 2000 with various values of \u03bb near the critical \u03bb = 1. Depicted is the evolution of summary statistics (m, r 2 \u22a5 ) for 10n steps of SGD initialized randomly.", "figure_data": ""}, {"figure_label": "82425", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "(a) \u03bb = 0. 8 ( 2 Figure 4 . 2 Figure 5 .82425Figure 4. Matrix PCA in dimension n = 2000 with various values of \u03bb near the critical \u03bb = 1. Depicted is the evolution of summary statistic m(t) zoomed in about an O(n \u22121/2 ) window of m = 0 for 1.5 * n steps of SGD initialized randomly. In (a)-(b) one sees stable OU processes, and in (c)-(d) one sees unstable OU processes.", "figure_data": ""}, {"figure_label": "16", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "(a) \u03bb = 1 (Figure 6 .16Figure 6. GMM in dimension N = 500 with \u03b1 = 0.1 at various values of \u03bb. Depicted are (m 1 , m 2) values that 500 runs of SGD converge to after 100N steps from a random Gaussian initialization. The \u2212 and \u00d7 are the unstable and stable fixed points of the \u03bb = \u221e ballistic effective dynamics. The fixed points of the limiting effective dynamics have the same structure at finite \u03bb as \u03bb = \u221e, and that as \u03bb gets large quantitatively approach the \u03bb = \u221e ones.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 7 .7Figure 7. GMM in dimension N = 500 with \u03b1 = 0.1. Depicted is the fraction of endpoints (SGD after 100N steps from a random Gaussian initialization) with m 1 m 2 < 0, corresponding to the stable fixed points of the \u03bb = \u221e dynamics; it matches the predicted 1 2 \u2212 1 2 fraction.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 8 .8Figure 8. Binary GMM in dim. N = 250 with \u03bb = 100 and \u03b1 = 0.1. Diffusive limits for (a) m 1 individually, and (b)-(c) the pairs (m 1 , m 2 ) and (v 1 , v 2 ) where the diffusions can be seen to not be of full rank.", "figure_data": ""}, {"figure_label": "109", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "(a) \u03bb = 10 (Figure 9 .109Figure 9. XOR in dimension N = 500 with \u03b1 = 0.1 and K = 4. Depicted are (m \u00b5 1 , m \u00b52 ) values that 500 runs of SGD converge to after 100N steps from a random Gaussian initialization. The \u2212 and \u00d7 are the unstable and stable fixed points of the \u03bb = \u221e ballistic effective dynamics. This demonstrates that the fixed points of the limiting effective dynamics have the same qualitative structure at finite \u03bb as \u03bb = \u221e, and approach the \u03bb = \u221e ones as \u03bb gets large.", "figure_data": ""}, {"figure_label": "51", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Proposition 5 . 1 .51Let u n be as in (5.1) and fix any \u03bb > 0 and \u03b4 n = c \u03b4 /N . Then u n (t) converges to the solution of the ODE systemu t = \u2212f (u t ) + g(u t ), initialized from lim n (u n ) * \u00b5 n with", "figure_data": ""}, {"figure_label": "1010", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "(a) \u03bb = 10 (Figure 10 .1010Figure 10. XOR in dimension N = 500 with \u03b1 = 0.1 and K = 4. The fraction of endpoints (SGD after 100N steps from a random Gaussian initialization) with v having two positive entries and two negative entries, and with the consequent correct signs on m \u00b5 i , m \u03bd i , corresponding to the stable fixed points of the \u03bb = \u221e dynamics; it matches the predicted29 32 , 3 32 fractions.", "figure_data": ""}, {"figure_label": "52", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Proposition 5 . 2 .52The fixed points of the ODE system of Proposition 9.1 are classified as follows. If \u03b1 > 1/8, then the only fixed point is at u n = 0.If 0 < \u03b1 < 1/8, then let (I 0 , I + \u00b5 , I \u2212 \u00b5 , I + \u03bd , I \u2212 \u03bd ) be any disjoint (possibly empty) subsets whose union is {1, ..., K}. Corresponding to that tuple (I 0 , I +", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 11 .11Figure 11. XOR GMM in dim. N = 250 and \u03b1 = 0.1 and K = 4. (a) and (c) display the degenerate diffusive limits in the regime of Proposition 5.3 in (m \u00b5 1 , m \u00b5 2 ) coordinates at \u03bb = 100 and \u03bb = 1000. Conversely, (b) and (d) display the diffusive limits in the regime of Proposition 5.3 in (m \u00b5 1 , m \u03bd 3 ), where the limiting diffusions are independent and are of rank 2.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "6 .64} . Numerical simulations in Figure 11 confirm these degenerate diffusive limits at finite \u03bb. Part 3. Proofs Proof of Theorem 2.3", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "K)n 44where the bound on the operator norm of an i.i.d. Gaussian k-tensor can be found, e.g., in [9, Lemma 4.7]. Moving on to item (3), by the same reasoning, for every w, E[\u27e8\u2207H, w\u27e9 4 ] \u2264 16kE[|W (w, x, . . . , x)| 4 ] \u2264 C(k, K)n 2 \u2225w\u2225 .", "figure_data": ""}, {"figure_label": "475711", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "2k\u2212 4 , ( 7 . 5 ) 7 . 1 . 1 .475711which when multiplied by \u03b4 = O(1/n) evidently vanishes.\u25a1 The fixed points of Proposition 3.1. We now turn to analyzing the ODE of Proposition 3.Proof of Proposition 3.3. At the fixed points of the ODE in Proposition 3.1,", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "which point, we see that if m 1 , m 2 \u2265 0, this becomes 1 2 \u03c3(\u2212v \u2022 m), as it is if m 1 , m 2 \u2264 0. If m 1 \u2265 0 and m 2 \u2264 0, then you get lim \u03bb A \u00b5 1 = \u2212 1 2 \u03c3(\u2212v 1 m 1 ) and lim \u03bb A \u00b5 2 = \u2212 1 2 \u03c3(\u2212v 2 m 2 ) and likewise if m 1 \u2264 0 and m 2 \u2265 0.", "figure_data": ""}, {"figure_label": "9292", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "Lemma 9 . 2 . 9 . 2 .9292For \u03b4 = O(1/N ) and any fixed \u03bb, the 2-layer XOR GMM with observables u n is \u03b4 n -localizable for E K being balls of radius K about the origin in R 4K+( K 2 ) .Proof. The condition on u n was satisfied per Lemma 9.1. Recalling \u2207\u03a6 from (8.6), one can verify that the norm of each of the four terms in \u2207\u03a6 is individually bounded, using the Cauchy-Schwarz inequality together with the bound of Lemma 8.2 on \u2225A i \u2225, naturally adapted to XOR. The remaining estimates are also analogous to the proof of Lemma 8.4 with the analogue of Lemma 8.2 applied. \u25a1 Effective dynamics for the XOR GMM.", "figure_data": ""}, {"figure_label": "91", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "Proposition 9 . 1 .91In the \u03bb \u2192 \u221e limit, the ODE from Proposition 5.1 converges t\u022f", "figure_data": ""}, {"figure_label": "22", "figure_type": "figure", "figure_id": "fig_17", "figure_caption": "2 i (v 2 i22e \u2212v\u2022g(m \u00b5 ) ) + \u2022 \u2022 \u2022 + log(1 + e \u2212v\u2022g(\u2212m \u03bd ) ) + \u03b1 + (m \u00b5 i ) 2 + (m \u03bd i ) 2 + R \u22a5 ii ) .", "figure_data": ""}, {"figure_label": "22", "figure_type": "figure", "figure_id": "fig_18", "figure_caption": "2 j=1, 2 a22v \u2022 g(m \u00b5 )) \u2212 \u221a N \u03b1v iIf we then use the expansionv \u2022 g(m \u00b5 ) = C \u03b1 + N \u22121/j,\u00b5 (\u1e7d j +m \u00b5 j ) + O(1/n) from which we obtain \u03c3(\u2212v \u2022 g(m \u00b5 )) = \u03c3(\u2212C \u03b1 ) + 1 \u221a N j=1,2 a j,\u00b5 (\u1e7d j +m \u00b5 j ) (4\u03b1)(1 \u2212 4\u03b1) + O( 1 n )", "figure_data": ""}, {"figure_label": "103", "figure_type": "figure", "figure_id": "fig_19", "figure_caption": "10 . 3 .103\u2022 m \u03bd ) . Taking the limit as n \u2192 \u221e, we again recover the claimed limiting diffusion matrix, and similar calculations yield the same for \u03a3m \u00b5 i ,m \u00b5 j , \u03a3m \u03bd i ,m \u03bd j and \u03a3m \u00b5 i ,m \u03bd j , concluding the proof. \u25a1 Proofs of technical lemmas for Gaussian mixtures In this section, we establish the technical bounds on Gaussian moments in Lemmas 8.2-8.Proof of Lemma 8.2. For the first bound, let Z \u223c N (0, I) and considerE[|X \u2022 w| 8 ] = 1 2 E[(w \u2022 \u00b5 + \u03bb \u22121/2 w \u2022 Z) 8 ] + 1 2 E[(\u2212w \u2022 \u00b5 + \u03bb \u22121/2 w \u2022 Z) 8 ] .", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "X \u2113 = X \u2113\u22121 \u2212 \u03b4 n \u2207L n (X \u2113\u22121 , Y \u2113 ) ,", "formula_coordinates": [3.0, 231.19, 397.1, 149.61, 10.77]}, {"formula_id": "formula_1", "formula_text": "H(x, Y ) = L n (x, Y ) \u2212 \u03a6(x)", "formula_coordinates": [3.0, 157.02, 512.45, 130.14, 10.63]}, {"formula_id": "formula_2", "formula_text": "\u03a6(x) = E[L n (x, Y )] .", "formula_coordinates": [3.0, 358.27, 512.45, 96.71, 10.63]}, {"formula_id": "formula_3", "formula_text": "max i sup x\u2208u \u22121 n (E K ) ||\u2207 2 u n i || op \u2264 C K \u2022 \u03b4 \u22121/2 n", "formula_coordinates": [3.0, 108.22, 669.7, 198.24, 17.36]}, {"formula_id": "formula_4", "formula_text": "max i sup x\u2208u \u22121 n (E K ) ||\u2207 3 u n i || op \u2264 C K ; (2) sup x\u2208u \u22121 n (E K ) \u2225\u2207\u03a6\u2225 \u2264 C K , and sup x\u2208u \u22121 n (E K ) E[\u2225\u2207H\u2225 8 ] \u2264 C K \u03b4 \u22124 n ; (3) max i sup x\u2208u \u22121 n (E K ) E[\u27e8\u2207H, \u2207u n i \u27e9 4 ] \u2264 C K \u03b4 \u22122 n , and max i sup x\u2208u \u22121 n (E K ) E[\u27e8\u2207 2 u n i , \u2207H \u2297 \u2207H \u2212 V \u27e9 2 ] = o(\u03b4 \u22123 n ).", "formula_coordinates": [3.0, 89.38, 671.49, 414.13, 34.19]}, {"formula_id": "formula_5", "formula_text": "A n = i \u2202 i \u03a6\u2202 i , and L n = 1 2 i,j V ij \u2202 i \u2202 j . (2.1)", "formula_coordinates": [4.0, 189.5, 228.91, 351.77, 29.46]}, {"formula_id": "formula_6", "formula_text": "R k \u2192 R k and \u03a3 : R k \u2192 R k\u00d7k , such that sup x\u2208u \u22121 n (E K ) \u2212 A n + \u03b4 n L n u n (x) \u2212 h(u n (x)) \u2192 0 , (2.2) sup x\u2208u \u22121 n (E K ) \u2225\u03b4 n J n V J T n \u2212 \u03a3(u n (x))\u2225 \u2192 0 . (2.3)", "formula_coordinates": [4.0, 183.56, 310.32, 357.72, 69.39]}, {"formula_id": "formula_7", "formula_text": "u n = (u n i ) k i=1 , let (u n (t)) t be the linear interpolation of (u n (X \u03b4n \u230at\u03b4 \u22121 n \u230b )) t .", "formula_coordinates": [4.0, 72.0, 461.84, 333.91, 18.04]}, {"formula_id": "formula_8", "formula_text": "du t = h(u t )dt + \u03a3(u t )dB t .", "formula_coordinates": [4.0, 235.17, 525.42, 141.67, 10.67]}, {"formula_id": "formula_9", "formula_text": "\u03b4JV J T + \u03b4 2 E[\u27e8\u2207H, J\u27e9 \u2297 \u27e8\u2207 2 u, \u2207H \u2297 \u2207H \u2212 V \u27e9] + \u03b4 2 E[\u27e8\u2207 2 u, \u2207H \u2297 \u2207H \u2212 V \u27e9 \u2297 \u27e8\u2207H, J\u27e9] + \u03b4 3 E[\u27e8\u2207 2 u, \u2207H \u2297 \u2207H \u2212 V \u27e9 \u22972 ] ,", "formula_coordinates": [5.0, 92.04, 372.64, 427.92, 30.93]}, {"formula_id": "formula_10", "formula_text": "R k \u2192 R k such that sup x\u2208u \u22121 n (E K ) \u2225A n u n (x) \u2212 f (u n (x))\u2225 \u2192 0 , (2.5) sup x\u2208u \u22121 n (E K ) \u2225\u03b4 n L n u n (x) \u2212 g(u n (x))\u2225 \u2192 0 , (2.6)", "formula_coordinates": [5.0, 209.93, 578.36, 331.34, 67.62]}, {"formula_id": "formula_11", "formula_text": "du t = \u2212f (u t )dt ,(2.7)", "formula_coordinates": [5.0, 267.13, 696.14, 274.14, 10.7]}, {"formula_id": "formula_12", "formula_text": "du t = h(u t )dt .", "formula_coordinates": [6.0, 270.4, 307.04, 71.2, 10.67]}, {"formula_id": "formula_13", "formula_text": "d\u0169 t =h(\u0169 t )dt +\u03a3 1/2 (\u0169 t )dB t with\u0169 0 \u223c\u03bd . (2.9)", "formula_coordinates": [6.0, 195.24, 483.57, 346.03, 13.13]}, {"formula_id": "formula_14", "formula_text": "Y \u2113 = \u03bbv \u2297k + W \u2113", "formula_coordinates": [7.0, 265.94, 446.01, 79.62, 12.06]}, {"formula_id": "formula_15", "formula_text": "L(x, Y ) = ||Y \u2212 x \u2297k || 2 .", "formula_coordinates": [8.0, 250.89, 155.68, 110.21, 12.07]}, {"formula_id": "formula_16", "formula_text": "2 \u22a5 = r 2 \u22a5 (x) := \u2225x \u2212 mv\u2225 2 = \u2225x\u2225 2 \u2212 m 2 are such that \u03a6(x) = \u22122\u03bbm k + (r 2 \u22a5 + m 2 ) k + c", "formula_coordinates": [8.0, 72.0, 190.85, 402.25, 32.71]}, {"formula_id": "formula_17", "formula_text": "Proposition 3.1. Fix k \u2265 2, \u03bb > 0, c \u03b4 > 0 and let \u03b4 n = c\u03b4 /n. Then u n (t) converges as n \u2192 \u221e to the solution of the following ODE initialized from lim n\u2192\u221e (u n ) * \u00b5 n : dm = 2m(\u03bbkm k\u22122 \u2212 kR 2(k\u22121) )dt , dr 2 \u22a5 = \u22124kR 2(k\u22121) (r 2 \u22a5 \u2212 c \u03b4 )dt . (3.1)", "formula_coordinates": [8.0, 71.17, 348.52, 470.1, 46.02]}, {"formula_id": "formula_18", "formula_text": "m \u2020 (k, \u03bb) \u2264 m \u22c6 (k, \u03bb) be as in (7.7) (if k = 2, \u03bb c = 1 and m \u2020 = m \u22c6 = \u221a \u03bb \u2212 1): (1) An unstable fixed point at (0, 0) and a fixed point at (0, 1); if k = 2, (0, 1) is stable if \u03bb < \u03bb c (2) and unstable if \u03bb > \u03bb c (2); if k > 2 (0, 1) is always stable. (2) If \u03bb > \u03bb c (k): when k = 2, two stable fixed points at (\u00b1m \u22c6 (2), 1). When k \u2265 3, two unstable", "formula_coordinates": [8.0, 89.38, 527.41, 451.9, 61.78]}, {"formula_id": "formula_19", "formula_text": "if \u00b5 n \u223c N (0, I n /n), then (u n ) * \u00b5 n \u2192 \u03b4 (0,1) weakly. Now rescale and let\u0169 n = (m, r 2 \u22a5 ) = ( \u221a nm, r 2 \u22a5 ). Evidently,\u03bd = lim n (\u0169 n ) * \u00b5 n = N (0, 1) \u2297 \u03b4 1 .", "formula_coordinates": [9.0, 72.0, 282.37, 470.12, 31.88]}, {"formula_id": "formula_20", "formula_text": "dm = 2m(2\u03bb1 k=2 \u2212 kr 2(k\u22121) \u22a5 )dt + 2(kr 2(k\u22121) \u22a5 ) 1 /2 dB t dr 2 \u22a5 = \u22124kr 2(k\u22121) \u22a5 (r 2 \u22a5 \u2212 1)dt . (3.2)", "formula_coordinates": [9.0, 89.7, 354.39, 451.57, 16.26]}, {"formula_id": "formula_21", "formula_text": "dm = 4(\u03bb \u2212 1)mdt + 2 \u221a 2dB t .", "formula_coordinates": [9.0, 234.11, 402.45, 143.78, 20.19]}, {"formula_id": "formula_22", "formula_text": "dm = 2m(k\u039b \u2212 kr 2(k\u22121) \u22a5 )dt + 2(kr 2(k\u22121) \u22a5 ) 1 /2 dB t dr 2 \u22a5 = \u22124kr 2(k\u22121) \u22a5 (r 2 \u22a5 \u2212 1)dt , (3.3)", "formula_coordinates": [9.0, 98.06, 625.81, 443.21, 16.26]}, {"formula_id": "formula_23", "formula_text": "d\u03a6 = \u2212 4k 2 m 2 \u03bb 2 m 2(k\u22122) \u2212 2\u03bbm k\u22122 R 2k\u22122 + R 4k\u22124 \u2212 4k 2 R 4(k\u22121) (r 2 \u22a5 \u2212 c \u03b4 ) dt .", "formula_coordinates": [10.0, 113.13, 430.3, 385.74, 14.27]}, {"formula_id": "formula_24", "formula_text": "dm = 2k(\u03bb1 k=2m k\u22121 \u2212 1)dt + 2 \u221a kdB (1) t , dr 2 \u22a5 = \u22124kr 2 \u22a5 dt + 2 k(k \u2212 1)dB(2)", "formula_coordinates": [10.0, 94.35, 506.93, 398.19, 21.72]}, {"formula_id": "formula_25", "formula_text": "X \u223c N ((2y \u2212 1)\u00b5, I/\u03bb) ,", "formula_coordinates": [10.0, 248.16, 656.22, 115.69, 9.57]}, {"formula_id": "formula_26", "formula_text": "L (v i , W i ) i\u2208{1,2} ; (y, X) = \u2212yv \u2022 g(W X) + log(1 + e v\u2022g(W X) ) + p(v, W ) , (4.1)", "formula_coordinates": [11.0, 134.72, 677.97, 406.55, 13.71]}, {"formula_id": "formula_27", "formula_text": "u n = (v 1 , v 2 , m 1 , m 2 , R \u22a5 11 , R \u22a5 12 , R \u22a5 22 ),(4.2)", "formula_coordinates": [12.0, 221.92, 105.69, 319.36, 14.19]}, {"formula_id": "formula_28", "formula_text": "m i = W i \u2022 \u00b5 and R \u22a5 ij = W \u22a5 i \u2022 W \u22a5 j with W \u22a5 i = W i \u2212 m i \u00b5 denoting the part of W i orthogonal to \u00b5. For a point, (v, W ) \u2208 X n , let A \u00b5 i = E[X \u2022\u00b51 W i \u2022X\u22650 (\u03c3(v\u2022g(W X)) \u2212 y)] , A \u22a5 ij = E[X \u2022W \u22a5 j 1 W i \u2022X\u22650 (\u03c3(v\u2022g(W X)) \u2212 y)] , B ij = E[1 W i \u2022X\u22650 1 W j \u2022X\u22650 (\u03c3(v\u2022g(W X)) \u2212 y) 2 ]. (4.3)", "formula_coordinates": [12.0, 72.0, 127.02, 469.27, 66.84]}, {"formula_id": "formula_29", "formula_text": "A \u00b5 i = A \u00b5 i (u n )", "formula_coordinates": [12.0, 164.16, 212.5, 65.6, 15.55]}, {"formula_id": "formula_30", "formula_text": "f v i = m i A \u00b5 i (u) + A \u22a5 ii (u) + \u03b1v i , f m i = v i A \u00b5 i (u) + \u03b1m i , f R \u22a5 ij = v i A \u22a5 ij (u) + v j A \u22a5 ji (u) + 2\u03b1R \u22a5 ij ,", "formula_coordinates": [12.0, 144.33, 280.9, 323.35, 36.36]}, {"formula_id": "formula_31", "formula_text": "g v i = g m i = 0, g R \u22a5 ij = c \u03b4 v i v j \u03bb B ij for i, j = 1, 2.", "formula_coordinates": [12.0, 142.12, 323.8, 215.94, 18.15]}, {"formula_id": "formula_32", "formula_text": "m i = v i 2 \u03c3(\u2212v \u2022 m) \u2212 \u03b1m i m 1 m 2 > 0 v i 2 \u03c3(\u2212v i m i ) \u2212 \u03b1m i else ,v i = m i 2 \u03c3(\u2212v \u2022 m) \u2212 \u03b1v i m 1 m 2 > 0 m i 2 \u03c3(\u2212v i m i ) \u2212 \u03b1v i else , and\u1e58 \u22a5 ij = \u22122\u03b1R \u22a5 ij .", "formula_coordinates": [12.0, 71.17, 495.85, 440.13, 54.27]}, {"formula_id": "formula_33", "formula_text": "R \u22a5 ij = 0 and m i = v i for i, j = {1, 2}. In (v 1 , v 2 ), the coordinates are classified by (1) A fixed point at (v 1 , v 2 ) = (0, 0) that is stable if \u03b1 > 1 /4; (2) If \u03b1 < 1 /4", "formula_coordinates": [12.0, 72.0, 551.22, 383.99, 42.04]}, {"formula_id": "formula_34", "formula_text": "(v 1 , v 2 ) having v 1 v 2 > 0 such that v 2 1 + v 2 2 = C \u03b1 for C \u03b1 := log(1 \u2212 2\u03b1) \u2212 log(2\u03b1). (3) If \u03b1 < 1 /4, two stable fixed points at (v 1 , v 2 ) equals ( \u221a C \u03b1 , \u2212 \u221a C \u03b1 ) and (\u2212 \u221a C \u03b1 , \u221a C \u03b1 ).", "formula_coordinates": [12.0, 89.38, 583.62, 450.57, 36.86]}, {"formula_id": "formula_35", "formula_text": "(v 1 , v 2 ) \u223c N (0, I 2 ) and W 1 , W 2 \u223c N (0, I N /(\u03bbN )) then \u03bd := lim(u n ) * \u00b5 n is N (0, I 2 ) in the v 1 , v2", "formula_coordinates": [12.0, 72.0, 626.51, 468.0, 23.65]}, {"formula_id": "formula_36", "formula_text": "Proposition 4.3. Let \u03b4 n = 1 /N, (a 1 , a 2 ) \u2208 R 2 + be such that a 2 1 + a 2 2 = C \u03b1 and let\u1e7d i = \u221a N (v i \u2212 a i ) andm i = \u221a N (m i \u2212 a i ).", "formula_coordinates": [14.0, 71.17, 231.16, 470.11, 34.5]}, {"formula_id": "formula_37", "formula_text": "d\u1e7d i =\u03b1(m i \u2212\u1e7d i ) + a i (\u03b1 \u2212 2\u03b1 2 ) a k (\u1e7d k +m k ) +\u03a3 1 /2 dB t \u2022 e v i , dR \u22a5 ii = \u22122\u03b1R \u22a5 ii dt , dm i =\u03b1(\u1e7d i \u2212m i ) + a i (\u03b1 \u2212 2\u03b1 2 ) a k (\u1e7d k +m k ) +\u03a3 1 /2 dB t \u2022 e m i , dR \u22a5 ij = \u22122\u03b1R \u22a5 ij dt ,", "formula_coordinates": [14.0, 97.32, 273.46, 417.37, 36.01]}, {"formula_id": "formula_38", "formula_text": "L (v i , W i ) i\u2264K ; (y, X) = \u2212yv \u2022 g(W X) + log(1 + e v\u2022g(W X) ) + p(v, W ) ,", "formula_coordinates": [15.0, 140.04, 316.37, 331.91, 13.18]}, {"formula_id": "formula_39", "formula_text": "1 \u2264 i \u2264 j \u2264 K, v i , m \u00b5 i = W i \u2022 \u00b5 , m \u03bd i = W i \u2022 \u03bd , R \u22a5 ij = W \u22a5 i \u2022 W \u22a5 j (5.1)", "formula_coordinates": [15.0, 163.83, 366.0, 377.44, 33.37]}, {"formula_id": "formula_40", "formula_text": "W \u22a5 i = W i \u2212 m \u00b5 i \u00b5 \u2212 m \u03bd i \u03bd", "formula_coordinates": [15.0, 101.79, 404.95, 110.7, 15.55]}, {"formula_id": "formula_41", "formula_text": "A i = E X1 W i \u2022X\u22650 \u2212 y + \u03c3(v \u2022 g(W X)) ,", "formula_coordinates": [15.0, 203.97, 479.28, 204.07, 11.55]}, {"formula_id": "formula_42", "formula_text": "A \u00b5 i = \u00b5 \u2022 A i , A \u03bd i = \u03bd \u2022 A i , A \u22a5 ij = W \u22a5 j \u2022 A i . Furthermore, let B ij = E 1 W i \u2022X\u22650 1 W j \u2022X\u22650 \u2212 y + \u03c3(v \u2022 g(W X)) 2 .", "formula_coordinates": [15.0, 72.0, 514.44, 356.75, 54.66]}, {"formula_id": "formula_43", "formula_text": "f v i = m \u00b5 i A \u00b5 i (u) + m \u03bd i A \u03bd i (u) + A \u22a5 ii (u) + \u03b1v i , f m \u00b5 i = v i A \u00b5 i + \u03b1m \u00b5 i , f R \u22a5 ij = v i A \u22a5 ij (u) + v j A \u22a5 ji (u) + 2\u03b1R \u22a5 ij , f m \u03bd i = v i A \u03bd i + \u03b1m \u03bd i .", "formula_coordinates": [15.0, 123.52, 639.84, 364.97, 37.35]}, {"formula_id": "formula_44", "formula_text": "g v i = g m \u00b5 i = g m \u03bd i = 0, and g R \u22a5 ij = c \u03b4 v i v j \u03bb B ij for 1 \u2264 i \u2264 j \u2264 K.", "formula_coordinates": [15.0, 142.12, 682.43, 295.12, 18.15]}, {"formula_id": "formula_45", "formula_text": "\u00b5 , I \u2212 \u00b5 , I + \u03bd , I \u2212 \u03bd )", "formula_coordinates": [16.0, 280.32, 383.2, 64.42, 13.64]}, {"formula_id": "formula_46", "formula_text": "(1) m \u00b5 i = m \u03bd i = v i = 0 for i \u2208 I 0 , (2) m \u00b5 i = v i > 0 such that i\u2208I + \u00b5 v 2 i = logit(\u22124\u03b1) and m \u03bd i = 0 for all i \u2208 I + \u00b5 , (3) \u2212m \u00b5 i = v i > 0 such that i\u2208I \u2212 \u00b5 v 2 i = logit(\u22124\u03b1) and m \u03bd i = 0 for all i \u2208 I \u2212 \u00b5 ,(4)", "formula_coordinates": [16.0, 89.38, 414.3, 372.74, 58.52]}, {"formula_id": "formula_47", "formula_text": "m \u03bd i = v i < 0 such that i\u2208I + \u03bd v 2 i = logit(\u22124\u03b1) and m \u00b5 i = 0 for all i \u2208 I + \u03bd , (5) \u2212m \u03bd i = v i < 0 such that i\u2208I \u2212 \u03bd v 2 i = logit(\u22124\u03b1", "formula_coordinates": [16.0, 89.38, 459.96, 364.09, 31.8]}, {"formula_id": "formula_48", "formula_text": "1 2 K K\u22122 k=2 K k (1 \u2212 2 1\u2212k )(1 \u2212 2 1+k\u2212K ) , (5.2)", "formula_coordinates": [17.0, 216.8, 286.4, 324.47, 33.98]}, {"formula_id": "formula_49", "formula_text": "v i = m \u00b5 i = m \u03bd i = 0.", "formula_coordinates": [17.0, 305.27, 503.64, 90.21, 15.55]}, {"formula_id": "formula_50", "formula_text": "v i = m \u00b5 i = a i,\u00b5 > 0 and v i = m \u03bd i = a i,\u03bd < 0 for i = 3, 4. Namely, we let v i = \u221a N (v i \u2212 a i,\u00b5 ) i = 1, 2 \u221a N (v i \u2212 a i,\u03bd ) i = 3, 4 , m \u00b5 i = \u221a N (m \u00b5 i \u2212 a i,\u00b5 ) i = 1, 2 m \u03bd i = \u221a N (m \u03bd i \u2212 a i,\u03bd ) i = 3, 4", "formula_coordinates": [17.0, 72.0, 543.75, 523.52, 66.3]}, {"formula_id": "formula_51", "formula_text": "h\u1e7d i = \u03b1(\u1e7d i \u2212m \u00b5 i ) \u2212 a i,\u00b5 (\u03b1 \u2212 4\u03b1 2 ) k=1,2 a k,\u00b5 (\u1e7d k +m \u00b5 k ) i = 1, 2 \u03b1(\u1e7d i \u2212m \u03bd i ) \u2212 a i,\u03bd (\u03b1 \u2212 4\u03b1 2 ) k=3,4 a k,\u03bd (\u1e7d k +m \u03bd k ) i = 3, 4", "formula_coordinates": [17.0, 145.38, 676.16, 313.37, 31.27]}, {"formula_id": "formula_52", "formula_text": "\u03a3\u1e7d i\u1e7dj =\u03a3m \u00b5 im \u00b5 j =\u03a3\u1e7d im \u00b5 j = 3\u03b1 2 a i,\u00b5 a j,\u00b5 if i, j \u2208 {1, 2} , \u03a3\u1e7d i\u1e7dj =\u03a3m \u03bd im \u03bd j =\u03a3\u1e7d im \u03bd j = 3\u03b1 2 a i,\u03bd a j,\u03bd if i, j \u2208 {3, 4} , \u03a3\u1e7d i\u1e7dj =\u03a3m \u00b5 im \u03bd j =\u03a3 m \u00b5 i ,v j =\u03a3\u1e7d im \u03bd j = \u2212\u03b1 2 a i,\u00b5 a j,\u03bd if i \u2208 {1, 2}, j \u2208 {3,", "formula_coordinates": [18.0, 132.24, 114.44, 595.44, 57.02]}, {"formula_id": "formula_53", "formula_text": "f \u2113 = f (X \u2113\u22121 \u2212 \u03b4\u2207\u03a6 \u2113\u22121 \u2212 \u03b4\u2207H \u2113 \u2113\u22121 ) = f \u2113\u22121 + \u03b4[A f \u2113 \u2212 A f \u2113\u22121 ] + \u03b4[M f \u2113 \u2212 M f \u2113\u22121 ] + O(\u03b4 3 ||\u2207 3 f || L \u221e K,n \u2022 ||\u2207L|| 3 L \u221e K,n", "formula_coordinates": [18.0, 129.54, 480.73, 342.84, 36.04]}, {"formula_id": "formula_54", "formula_text": "A f \u2113 \u2212 A f \u2113\u22121 = \u2212 A n + \u03b4L n f \u2113\u22121 + 1 2 \u2207\u03a6 \u2297 \u2207\u03a6, \u2207 2 f \u2113\u22121 , M f \u2113 \u2212 M f \u2113\u22121 = \u2212 \u2207H \u2113 , \u2207f \u2113\u22121 + \u03b4(E f \u2113 \u2212 E f \u2113\u22121 ) , E f \u2113 \u2212 E f \u2113\u22121 = \u2207 2 f (\u2207\u03a6, \u2207H \u2113 ) \u2113\u22121 + 1 2 \u2207 2 f, \u2207H \u2113 \u2297 \u2207H \u2113 \u2212 V \u2113\u22121 , for A n = \u27e8\u2207\u03a6, \u2207\u27e9, L n = 1 2 i,j V ij \u2202 i \u2202 j and V = E[\u2207H \u2297 \u2207H] as in (2.", "formula_coordinates": [18.0, 72.0, 548.62, 391.4, 81.19]}, {"formula_id": "formula_55", "formula_text": "\u03b4 3 sup x\u2208E * K,n E[||\u2207 3 u j || \u2022 ||\u2207L|| 3 ] \u2272 \u03b4 3 ||\u2207 3 u j || L \u221e K,n ||\u2207\u03a6|| 3 L \u221e K,n + sup E * K,n E||\u2207H|| 3 \u2272 K \u03b4 3/2 .", "formula_coordinates": [18.0, 102.05, 687.59, 407.9, 23.35]}, {"formula_id": "formula_56", "formula_text": "u j (X \u2113 ) = u j (0) + \u03b4 \u2113 \u2032 \u2264\u2113 A u j \u2113 \u2032 \u2212 A u j \u2113 \u2032 \u22121 + \u03b4 \u2113 \u2032 \u2264\u2113 M u j \u2113 \u2032 \u2212 M u j \u2113 \u2032 \u22121 + o(1) ,", "formula_coordinates": [19.0, 143.81, 92.63, 324.39, 26.53]}, {"formula_id": "formula_57", "formula_text": "a \u2032 j (s) = A u j [s/\u03b4] \u2212 A u j [s/\u03b4]\u22121 and b \u2032 j (s) = M u j [s/\u03b4] \u2212 M u j [s/\u03b4]\u22121", "formula_coordinates": [19.0, 160.1, 145.06, 291.3, 17.09]}, {"formula_id": "formula_58", "formula_text": "a j (s) = s 0 a \u2032 j (s \u2032 )ds \u2032 = a j (\u03b4[s/\u03b4]) + (s \u2212 \u03b4[s/\u03b4])(A u j [s/\u03b4] \u2212 A u j [s/\u03b4]\u22121 )", "formula_coordinates": [19.0, 152.45, 178.77, 307.11, 28.58]}, {"formula_id": "formula_59", "formula_text": "u n (s) = u n (0) + a n (s) + b n (s) + o(1)", "formula_coordinates": [19.0, 215.46, 240.7, 176.64, 10.67]}, {"formula_id": "formula_60", "formula_text": "v n (s) = u n (0) + a n (s) + b n (s) .", "formula_coordinates": [19.0, 231.0, 303.45, 150.0, 10.67]}, {"formula_id": "formula_61", "formula_text": "sup 0\u2264s\u2264\u03c4 n K ||u n (s) \u2212 v n (s)|| \u2192 0 , in L 1 .", "formula_coordinates": [19.0, 210.96, 340.65, 190.09, 22.6]}, {"formula_id": "formula_62", "formula_text": "E||v n (s \u2227 \u03c4 K ) \u2212 v n (t \u2227 \u03c4 K )|| 4 \u2272 K,T (t \u2212 s) 2 , (6.2)", "formula_coordinates": [19.0, 202.22, 400.86, 339.05, 13.18]}, {"formula_id": "formula_63", "formula_text": "\u2225v n (s) \u2212 v n (t)\u2225 \u2264 \u2225a n (s) \u2212 a n (t)\u2225 + \u2225b n (s) \u2212 b n (t)\u2225 .", "formula_coordinates": [19.0, 177.64, 456.0, 256.72, 10.67]}, {"formula_id": "formula_64", "formula_text": "E|a(s \u2227 \u03c4 K ) \u2212 a(t \u2227 \u03c4 K )| 4 \u2272 E \u03b4 \u2113 (\u2212A n + \u03b4L n )u \u2113 4 + E \u03b4 2 \u2113 \u2207\u03a6 \u2297 \u2207\u03a6, \u2207 2 u \u2113 4 , (6.3)", "formula_coordinates": [19.0, 132.0, 518.55, 409.27, 44.92]}, {"formula_id": "formula_65", "formula_text": "[s/\u03b4] \u2227 \u03c4 K /\u03b4 to [t/\u03b4] \u2227 \u03c4 K /\u03b4. Let h = (h j ) j\u2264k be as in (2.", "formula_coordinates": [19.0, 83.96, 571.29, 349.61, 23.79]}, {"formula_id": "formula_66", "formula_text": "E \u03b4 \u2113 (\u2212A n + \u03b4L n )u \u2113 4 \u2272 K E|\u03b4 \u2113 h j (u n ) \u2113 | 4 + o((t \u2212 s) 4 ) \u2264 (t \u2212 s) 4 ||h j || 4 L \u221e (E n K ) + o(1) \u2272 K (t \u2212 s)", "formula_coordinates": [19.0, 136.1, 602.58, 335.07, 49.94]}, {"formula_id": "formula_67", "formula_text": "E|\u03b4 2 \u2207\u03a6 \u2297 \u2207\u03a6, \u2207 2 u \u2113 | 4 \u2264 \u03b4 8 |((t \u2212 s)/\u03b4)| sup x\u2208E * K,n ||\u2207\u03a6(x)|| 2 sup x\u2208E * K,n ||\u2207 2 u(x)|| op 4", "formula_coordinates": [19.0, 105.43, 678.72, 400.65, 28.23]}, {"formula_id": "formula_68", "formula_text": "E|a(s \u2227 \u03c4 K ) \u2212 a(t \u2227 \u03c4 K )| 4 \u2272 K (t \u2212 s)", "formula_coordinates": [20.0, 215.74, 113.49, 172.76, 13.18]}, {"formula_id": "formula_69", "formula_text": "E|b(s \u2227 \u03c4 K ) \u2212 b(t \u2227 \u03c4 K )| 4 = E \u03b4 (M u \u2113 \u2212 M u \u2113\u22121 ) 4 \u2272 E \u03b4 2 (M u \u2113 \u2212 M u \u2113\u22121 ) 2 2 ,", "formula_coordinates": [20.0, 99.34, 152.53, 413.31, 19.16]}, {"formula_id": "formula_70", "formula_text": "E \u03b4 2 \u2113 \u2207H \u2113 , \u2207u 2 \u2113\u22121 2 = \u03b4 4 \u2113,\u2113 \u2032 E \u2207H \u2113 , \u2207u 2 \u2113\u22121 \u2207H \u2113 \u2032 , \u2207u 2 \u2113 \u2032 \u22121 \u2264 \u03b4 \u2113 \u03b4 2 E \u2207H \u2113 , \u2207u 4 \u2113\u22121 1/2 2 \u2272 K (t \u2212 s) 2 , (6.4)", "formula_coordinates": [20.0, 123.54, 242.87, 417.73, 65.38]}, {"formula_id": "formula_71", "formula_text": "E \u03b4 4 \u2113 \u2207 2 u(\u2207\u03a6, \u2207H \u2113 ) \u2113\u22121 2 2 \u2264 \u03b4 6 (t \u2212 s) 2 sup x\u2208E * K,n ||\u2207 2 u(x)|| 4 op \u2022 ||\u2207\u03a6(x)|| 4 \u2022 E||\u2207H(x)|| 4 \u2272 K \u03b4 2 (t \u2212 s) 2 ,(6.5)", "formula_coordinates": [20.0, 100.95, 347.04, 440.33, 56.55]}, {"formula_id": "formula_72", "formula_text": "E \u03b4 4 \u2113 \u2207 2 u, \u2207H \u2113 \u2297 \u2207H \u2113 \u2212 V 2 \u2113\u22121 2 \u2272 \u03b4 6 (t \u2212 s) 2 sup x\u2208E * K,n ||\u2207 2 u(x)|| 4 op \u2022 E ||\u2207H(x)|| 8 \u2272 K (t \u2212 s) 2 . (6.6)", "formula_coordinates": [20.0, 139.49, 431.59, 401.78, 56.55]}, {"formula_id": "formula_73", "formula_text": "(v n (t \u2227 \u03c4 K ) \u2212 a n (t \u2227 \u03c4 K )) t ,", "formula_coordinates": [20.0, 414.48, 524.86, 127.03, 10.75]}, {"formula_id": "formula_74", "formula_text": "Let v K n (t) = v n (t \u2227 \u03c4 K )", "formula_coordinates": [20.0, 83.96, 561.83, 111.68, 13.65]}, {"formula_id": "formula_75", "formula_text": "\u2206M u i \u2113 = M u i \u2113 \u2212 M u i \u2113\u22121 and \u2206E u i \u2113 = E u i \u2113 \u2212 E u i \u2113\u22121 . Notice first that for 1 \u2264 i, j \u2264 k, b K n,i (t)b K n,j (t) \u2212 t 0 \u03b4E \u2206M u i [s/\u03b4]\u2227\u03c4 K \u2206M u j [s/\u03b4]\u2227\u03c4 K ds ,", "formula_coordinates": [20.0, 72.0, 587.08, 468.0, 63.9]}, {"formula_id": "formula_76", "formula_text": "E[\u2206M u i \u2113 \u2206M u j \u2113 ] = \u27e8\u2207u i , V \u2207u j \u27e9 + \u03b4E[\u27e8\u2207H \u2113 , \u2207u i \u27e9 \u2113\u22121 \u2206E u j \u2113 ] + \u03b4E[\u2206E u i \u2113 \u27e8\u2207H \u2113 , \u2207u j \u27e9 \u2113\u22121 ] (6.7) + \u03b4 2 E[\u2206E u i \u2113 \u2206E u j \u2113 ] .", "formula_coordinates": [20.0, 95.09, 673.82, 446.18, 34.91]}, {"formula_id": "formula_77", "formula_text": "sup t\u2264T t 0 \u03b4 \u27e8\u2207u i , V \u2207u j \u27e9 [s/\u03b4]\u2227\u03c4 K \u2212 \u03a3 ij (v K n (s))ds \u2264 T sup x\u2208E * K,n |\u03b4 \u27e8\u2207u i , V \u2207u j \u27e9 (x) \u2212 \u03a3 ij (u n (x))| ,", "formula_coordinates": [21.0, 88.39, 89.58, 435.22, 30.54]}, {"formula_id": "formula_78", "formula_text": "sup x\u2208E * K,n |\u03b4 2 E[\u27e8\u2207H, \u2207u i \u27e9\u2206E u j \u2113 ]| \u2264 \u03b4 2 E[\u27e8\u2207H, \u2207u i \u27e9 2 ] 1/2 E[(\u2206E u i \u2113 ) 2 ] 1/2 .", "formula_coordinates": [21.0, 149.88, 168.4, 312.25, 24.98]}, {"formula_id": "formula_79", "formula_text": "E[(\u2206E u i \u2113 ) 2 ] 1/2 \u2272 E[\u27e8\u2207 2 u i , \u2207\u03a6 \u2297 \u2207H\u27e9 2 ] 1/2 + E[\u27e8\u2207 2 u i , \u2207H \u2297 \u2207H \u2212 V \u27e9 2 ] 1/2 . (6.8)", "formula_coordinates": [21.0, 128.07, 219.28, 413.2, 15.49]}, {"formula_id": "formula_80", "formula_text": "sup t\u2264T t 0 \u03b4 2 E[\u27e8\u2207H, \u2207u i \u27e9 [s/\u03b4]\u2227\u03c4 K \u2206E u j [s/\u03b4]\u2227\u03c4 K ]ds , goes to zero as n \u2192 \u221e.", "formula_coordinates": [21.0, 72.0, 271.23, 341.81, 43.82]}, {"formula_id": "formula_81", "formula_text": "lim n\u2192\u221e sup i,j\u2264k sup t\u2264T t 0 \u03b4E[\u2206M u i [s/\u03b4]\u2227\u03c4 K \u2206M u j [s/\u03b4]\u2227\u03c4 K ]ds \u2212 t 0 \u03a3 ij (v K n (s))ds = 0 .", "formula_coordinates": [21.0, 134.22, 360.11, 343.56, 28.58]}, {"formula_id": "formula_82", "formula_text": "\u27e8b K \u27e9 t = t 0 \u03a3(v K (s))ds .", "formula_coordinates": [21.0, 246.09, 420.85, 119.83, 28.58]}, {"formula_id": "formula_83", "formula_text": "(v t ) \u2212 t 0 Lf (v s )ds is a martingale for all f \u2208 C \u221e 0 (R k ),where", "formula_coordinates": [21.0, 77.15, 452.38, 462.85, 27.09]}, {"formula_id": "formula_84", "formula_text": "L = 1 2 k ij=1 \u03a3 ij \u2202 i \u2202 j \u2212 k i=1 h i \u2202 i .", "formula_coordinates": [21.0, 237.6, 485.14, 136.79, 33.71]}, {"formula_id": "formula_85", "formula_text": "L(x, Y ) = \u22122(\u27e8W, x \u2297k \u27e9 + \u03bb\u27e8x, v\u27e9 k ) + ||x|| 2k + \u03b1 2 \u2225x\u2225 2 + c(Y ) , (7.1)", "formula_coordinates": [21.0, 161.86, 652.28, 379.41, 15.26]}, {"formula_id": "formula_86", "formula_text": "\u03a6(x) = \u22122\u03bbm k + (r 2 + m 2 ) k + \u03b1 2 (r 2 + m 2 ) + c \u2032 (7.2)", "formula_coordinates": [22.0, 192.82, 100.96, 348.45, 24.43]}, {"formula_id": "formula_87", "formula_text": "\u2207m = v , \u2207r 2 = 2(x \u2212 mv) . (7.3)", "formula_coordinates": [22.0, 228.77, 195.16, 312.5, 12.06]}, {"formula_id": "formula_88", "formula_text": "\u2202 1 \u03d5 = \u22122\u03bbkm k\u22121 + (2kR 2k\u22122 + \u03b1)m \u2202 2 \u03d5 = kR 2k\u22122 + \u03b1 2 .", "formula_coordinates": [22.0, 162.4, 261.06, 287.2, 15.26]}, {"formula_id": "formula_89", "formula_text": "\u2225\u2207\u03a6\u2225 \u2264 |\u2202 1 \u03d5|\u2225\u2207m\u2225 + |\u2202 2 \u03d5|\u2225\u2207r 2 \u2225 ;", "formula_coordinates": [22.0, 222.85, 301.53, 166.31, 13.13]}, {"formula_id": "formula_90", "formula_text": "E[\u2225\u2207H\u2225 8 ] \u2264 C k E[\u2225W (x, . . . , x, \u2022)\u2225 8 ] \u2264 E\u2225W \u2225 8 op \u2022 R 8k \u2264 C(k,", "formula_coordinates": [22.0, 149.63, 352.55, 285.35, 14.19]}, {"formula_id": "formula_91", "formula_text": "E[\u27e8\u2207 2 r, \u2207H \u2297 \u2207H \u2212 V \u27e9 2 ] \u2264 2E[\u2225\u2207H\u2225 4 ] \u2264 C(k, K)n 2", "formula_coordinates": [22.0, 178.4, 455.98, 254.71, 12.68]}, {"formula_id": "formula_92", "formula_text": "O(\u03b4 \u22122 ) if \u03b4 n = O(1/n). \u25a1", "formula_coordinates": [22.0, 72.0, 476.68, 468.0, 25.54]}, {"formula_id": "formula_93", "formula_text": "f m = \u22122\u03bbkm k\u22121 + (2kR 2k\u22122 + \u03b1)m , f r 2 = 2r 2 (2kR 2k\u22122 + \u03b1) .", "formula_coordinates": [22.0, 147.22, 541.31, 317.56, 13.86]}, {"formula_id": "formula_94", "formula_text": "V = E[\u2207H \u2297 \u2207H].", "formula_coordinates": [22.0, 451.26, 563.05, 90.85, 10.18]}, {"formula_id": "formula_95", "formula_text": "V ij = E[\u2202 i H\u2202 j H] = 4k(k \u2212 1)x i x j R 2k\u22124 + 4kR 2k\u22122 1{i = j} . (7.4)", "formula_coordinates": [22.0, 162.17, 593.22, 379.1, 13.13]}, {"formula_id": "formula_96", "formula_text": "\u03b4L \u03b4 r 2 = 4c \u03b4 n k (n \u2212 1)R 2k\u22122 + (k \u2212 1)r 2 R 2k\u22124", "formula_coordinates": [22.0, 193.26, 632.7, 218.47, 24.43]}, {"formula_id": "formula_97", "formula_text": "u 1 = 2u 1 (\u03bbku k\u22122 1 \u2212 kR 2k\u22122 \u2212 \u03b1) ,u 2 = \u2212(4u 2 \u2212 4c \u03b4 )kR 2k\u22122 \u2212 2\u03b1u 2 .", "formula_coordinates": [22.0, 122.75, 693.63, 366.49, 14.7]}, {"formula_id": "formula_98", "formula_text": "JV J T = 4k(k \u2212 1)m 2 R 2k\u22124 + 4kR 2k\u22122 4k(k \u2212 1)m(R 2 \u2212 m)R 2k\u22124 4k(k \u2212 1)m(R 2 \u2212 m)R 2k\u22124 4k(k \u2212 1)(R 2 \u2212 m) 2 R", "formula_coordinates": [23.0, 133.4, 93.29, 330.0, 24.9]}, {"formula_id": "formula_99", "formula_text": "\u03bbku k\u22121 1 = kR 2k\u22122 + \u03b1 u 1 , and 2c \u03b4 kR 2k\u22122 = 2kR 2k\u22122 + \u03b1 u 2 .", "formula_coordinates": [23.0, 134.72, 198.82, 342.57, 14.7]}, {"formula_id": "formula_100", "formula_text": "ku k\u22122 2 (2c \u03b4 \u2212 2u 2 ) = \u03b1. Notice that if k = 2, this has a nontrivial solution of the form c \u03b4 \u2212 \u03b1 2 = u 2 , provided \u03b1 < \u03b1 c (2) := 2c \u03b4 , and if k > 2, this has a nontrivial solution provided \u03b1 \u2264 max x\u22650 kx k\u22122 (2c \u03b4 \u2212 2x) at c \u03b4 (k \u2212 2)x k\u22123 \u2212 (k \u2212 1)x k\u22122 = 0 i.e., c \u03b4 (k\u22122) k\u22121 = x. This gives \u03b1 < \u03b1 c (k) := 2c k\u22121 \u03b4 k(k \u2212 1) \u2212(k\u22121) (k \u2212 2) k\u22122 .", "formula_coordinates": [23.0, 72.0, 242.76, 468.29, 89.93]}, {"formula_id": "formula_101", "formula_text": "\u03bbu k\u22122 1 = R 2k\u22122 + \u03b1/k ,and", "formula_coordinates": [23.0, 156.33, 372.97, 149.3, 14.7]}, {"formula_id": "formula_102", "formula_text": "kR 2k\u22122 = (kR 2k\u22122 + \u03b1)u 2 ,", "formula_coordinates": [23.0, 327.33, 373.06, 128.34, 13.13]}, {"formula_id": "formula_103", "formula_text": "u k\u22122 1 = kR 2k\u22122 + \u03b1 \u03bbk , and u 2 = 2c \u03b4 kR 2k\u22122 2kR 2k\u22122 + \u03b1 .", "formula_coordinates": [23.0, 178.65, 415.4, 254.7, 26.39]}, {"formula_id": "formula_104", "formula_text": "u 1 = \u00b1 \u221a \u03bb \u2212 c \u03b4 .", "formula_coordinates": [23.0, 385.13, 467.24, 75.08, 19.18]}, {"formula_id": "formula_105", "formula_text": "2 = c \u03b4 + \u03bb \u2212 2 k\u22122 R 4(k\u22121)", "formula_coordinates": [23.0, 289.64, 487.42, 103.49, 16.56]}, {"formula_id": "formula_106", "formula_text": "\u03bb c (k) := c \u03b4 k k/2 (2k \u2212 2) k\u22121 (k \u2212 2) (k\u22122)/2 . (7.6) (Interpreting 0 0 = 1, this returns \u03bb c (2) = c \u03b4 .", "formula_coordinates": [23.0, 70.73, 525.54, 470.54, 48.98]}, {"formula_id": "formula_107", "formula_text": "\u03c1 \u2020 (k, \u03bb) := inf{\u03c1 \u2265 1 : \u03bb \u2212 2 k\u22122 \u03c1 2(k\u22121) k\u22122 \u2212 \u03c1 + c \u03b4 = 0} , \u03c1 \u22c6 (k, \u03bb) := sup{\u03c1 \u2265 1 : \u03bb \u2212 2 k\u22122 \u03c1 2(k\u22121) k\u22122 \u2212 \u03c1 + c \u03b4 = 0} .", "formula_coordinates": [23.0, 184.55, 595.79, 242.9, 38.82]}, {"formula_id": "formula_108", "formula_text": "m \u2020 (k, \u03bb) = \u03c1 \u2020 \u2212 c \u03b4 ,and", "formula_coordinates": [23.0, 172.73, 679.46, 143.35, 10.84]}, {"formula_id": "formula_109", "formula_text": "m \u22c6 (k, \u03bb) = \u221a \u03c1 \u22c6 \u2212 c \u03b4 , (7.7)", "formula_coordinates": [23.0, 337.78, 671.36, 203.49, 18.94]}, {"formula_id": "formula_110", "formula_text": "f \u03a6 = \u27e8\u2207\u03a6, \u2207\u03a6\u27e9 = 4\u03bb 2 k 2 m 2(k\u22121) \u2212 8\u03bbk 2 m k R 2k\u22122 + 4k 2 R 4k\u22124 m 2 + 4k 2 r 2 R 4k\u22124 = 4k 2 m 2 \u03bb 2 m 2(k\u22122) \u2212 2\u03bbm k\u22122 R 2k\u22122 + R 4k\u22124 + 4k 2 r 2 R 4k\u22124 .", "formula_coordinates": [24.0, 120.23, 157.01, 371.05, 31.16]}, {"formula_id": "formula_111", "formula_text": "1 2 \u2207 2 \u03a6 = \u2212\u03bbk(k \u2212 1)m k\u22122 \u2207m \u22972 + kR 2k\u22122 \u2207m \u22972 + k(k \u2212 1)R 2(k\u22122) (2m\u2207m + \u2207r 2 ) \u2297 \u2207m + k(k \u2212 1)R 2(k\u22122) (2m\u2207m \u2297 \u2207r 2 + \u2207r 2 \u2297 \u2207r 2 ) + 1 2 \u2202 2 \u03d5\u2207 2 r 2 .", "formula_coordinates": [24.0, 94.8, 215.93, 423.59, 34.36]}, {"formula_id": "formula_112", "formula_text": "E[\u27e8\u2207H, \u2207m\u27e9 4 ] = n 2 E[\u27e8\u2207H, v\u27e9 4 ] \u2264 n 2 E[W 4 1,...,1 ] \u2264 Cn 2 . The second part of item (3) is unchanged since \u2207 2\u0169 1 = 0. Computing the drifts, \u27e8\u2207\u03a6, \u2207\u0169 1 \u27e9 = \u2212 2k\u03bb \u221a nm k\u22121 + 2k \u221a nR 2k\u22122 m = \u22122k\u03bbn \u2212 k\u22122 2\u0169 k\u22121 1 + 2k(r 2 + (\u0169 2 1 /n)) k\u22121\u0169 1 , \u27e8\u2207\u03a6, \u2207r 2 \u27e9 =4kr 2 R 2k\u22122 = 4kr 2 (r 2 + (\u0169 2 1 /n)) k\u22121 .", "formula_coordinates": [24.0, 71.61, 426.93, 446.98, 88.54]}, {"formula_id": "formula_113", "formula_text": "f\u0169 1 = \u22122k\u03bb\u0169 k\u22121 1 + 2k\u0169 k\u22121 2\u0169 1 k = 2 2k\u0169 k\u22121 2\u0169 1 k \u2265 3 , and f\u0169 2 = 4k\u0169 k 2 .", "formula_coordinates": [24.0, 151.37, 542.24, 309.25, 30.24]}, {"formula_id": "formula_114", "formula_text": "JVJ T = 4k(k \u2212 1)\u0169 2 1 R 2k\u22124 + 4knR 2k\u22122 4k(k \u2212 1)\u0169 1 (R 2 \u2212 m)R 2k\u22124 4k(k \u2212 1)\u0169 1 (R 2 \u2212 m)R 2k\u22124 4k(k \u2212 1)(R 2 \u2212 m) 2 R 2k\u22124 (7.8)", "formula_coordinates": [24.0, 134.43, 651.72, 406.84, 25.96]}, {"formula_id": "formula_115", "formula_text": "consider\u0169 n = (\u0169 1 ,\u0169 2 ) = ( \u221a nm, \u221a n(r 2 \u2212 1)", "formula_coordinates": [25.0, 267.35, 131.15, 204.42, 18.49]}, {"formula_id": "formula_116", "formula_text": "E[\u27e8\u2207H, \u2207\u0169 2 \u27e9 4 ] = n 2 E[\u27e8\u2207H, 2(x \u2212 mv)\u27e9 4 ] \u2272 n 2 (R 4k + m 4 )E[W 4 1,...,1 ]) \u2272 K n 2", "formula_coordinates": [25.0, 125.25, 196.65, 356.15, 14.19]}, {"formula_id": "formula_117", "formula_text": "E[\u27e8\u2207 2\u0169 2 , \u2207H \u2297 \u2207H \u2212 V \u27e9 2 ] \u2264 nVar(\u2225\u2207H\u2225 2 ) . We now express Var(\u2225\u2207H\u2225 2 ) = i Var((\u2202 i H) 2 ) + i\u0338 =j Cov((\u2202 i H) 2 , (\u2202 j H) 2 ) .", "formula_coordinates": [25.0, 71.45, 250.05, 376.21, 66.49]}, {"formula_id": "formula_118", "formula_text": "C \u2032 k R 2(k\u22122) x 2 i +C k R 2(k\u22121) and covariance C k x i x j R 2(k\u2212", "formula_coordinates": [25.0, 72.0, 326.23, 468.01, 26.98]}, {"formula_id": "formula_119", "formula_text": "Var(\u2225\u2207H\u2225 2 ) \u2272 K n + i,j x 2 i x 2 j \u2272 K n .", "formula_coordinates": [25.0, 217.84, 387.8, 176.33, 24.58]}, {"formula_id": "formula_120", "formula_text": "A n u 1 = \u22122k\u03bb1 k=2\u0169 k\u22121 1 + 2kr 2(k\u22121)\u0169 1 = \u22122k\u03bb1 k=2\u0169 k\u22121 1 + 2k(1 + n \u22121/2\u0169 2 ) k\u22121 A n\u01692 = 4kn 1/2 r 2 (r 2 + (\u0169 2 1 /n)) k\u22121 = 4kn 1/2 (1 + n \u22121/2\u0169 2 )(1 + n \u22121/2\u0169 2 + n \u22121\u01692 1 ) k\u22121 = 4kn 1/2 + 4k 2\u0169 2 +o(1)", "formula_coordinates": [25.0, 109.35, 454.21, 392.79, 51.41]}, {"formula_id": "formula_121", "formula_text": "\u03b4L\u0169 2 = \u221a n\u03b4L \u03b4 r 2 = 4 \u221a n k (n \u2212 1)R 2k\u22122 + (k \u2212 1)(1 + n \u22121/2\u0169 2 )R 2k\u22124", "formula_coordinates": [25.0, 138.51, 531.21, 327.97, 25.49]}, {"formula_id": "formula_122", "formula_text": "f\u0169 1 \u2212 g\u0169 1 = \u22122k\u03bb1 k=2\u0169 k\u22121 1 + 2k , and f\u0169 2 \u2212 g\u0169 2 = 4k\u0169 2 .", "formula_coordinates": [25.0, 154.2, 583.04, 303.6, 14.7]}, {"formula_id": "formula_123", "formula_text": "JVJ T = 4k(k \u2212 1)\u0169 2 1 R 2k\u22124 + 4knR 2k\u22122 4k(k \u2212 1)n 1/2\u0169 1 (R 2 \u2212 m)R 2k\u22124 4k(k \u2212 1)n 1/2\u0169 1 (R 2 \u2212 m)R 2k\u22124 4k(k \u2212 1)n(R 2 \u2212 m) 2 R 2k\u22124 .(7.9)", "formula_coordinates": [25.0, 119.05, 651.74, 422.22, 26.41]}, {"formula_id": "formula_124", "formula_text": "L((v, W )) d = \u2212v \u2022 g(W X \u00b5 ) + log(1 + e v\u2022g(W X\u00b5) ) + p(v, W ) w. prob. 1/2 log(1 + e v\u2022g(\u2212W X\u00b5) ) + p(v, W ) w. prob. 1/2 .", "formula_coordinates": [26.0, 125.19, 198.19, 361.62, 27.06]}, {"formula_id": "formula_125", "formula_text": "1 X \u00b5 , W 2 X \u00b5 ) is distributed as (m 1 + Z 1,\u00b5 m 1 + Z 1,\u22a5 , m 2 + Z 2,\u00b5 m 2 + Z 2,\u22a5 ), where Z 1,\u00b5 , Z 2,\u00b5 are i.i.d. N (0, \u03bb \u22121 )", "formula_coordinates": [26.0, 72.0, 238.51, 470.12, 24.26]}, {"formula_id": "formula_126", "formula_text": "\u03bb \u22121 R \u22a5 11 R \u22a5 12 R \u22a5 12 R \u22a5 22 (8.1)", "formula_coordinates": [26.0, 263.28, 281.7, 277.99, 27.25]}, {"formula_id": "formula_127", "formula_text": "(m i , R \u22a5 ij ) i,j . Finally, p(v, W ) = \u03b1 2 v 2 1 + v 2 2 + m 2 1 + R \u22a5 11 + m 2 2 + R \u22a5 22", "formula_coordinates": [26.0, 195.56, 317.68, 248.76, 44.9]}, {"formula_id": "formula_128", "formula_text": "\u2207 = (\u2202 v 1 , \u2202 v 2 , \u2207 W 1 , \u2207 W 2 ). Then J = (\u2207u \u2113 ) \u2113 = \uf8ee \uf8ef \uf8ef \uf8f0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 \u00b5 0 W \u22a5 2 2W \u22a5 1 0 0 0 0 \u00b5 W \u22a5 1 0 2W \u22a5 2 \uf8f9 \uf8fa \uf8fa \uf8fb T (8.2)", "formula_coordinates": [26.0, 70.72, 383.37, 470.55, 88.04]}, {"formula_id": "formula_129", "formula_text": "\u2207 v i L = (W i \u2022 X)1 W i \u2022X\u22650 \u2212 y + \u03c3(v \u2022 g(W X) + \u03b1v i (8.3) \u2207 W i L = v i X1 W i \u2022X\u22650 \u2212 y + \u03c3(v \u2022 g(W X)) + \u03b1W i (8.4)", "formula_coordinates": [26.0, 179.51, 576.85, 361.76, 29.47]}, {"formula_id": "formula_130", "formula_text": "A i = E X1 W i \u2022X\u22650 \u2212 y + \u03c3(v \u2022 g(W X)(8.5)", "formula_coordinates": [26.0, 208.51, 638.92, 332.76, 11.58]}, {"formula_id": "formula_131", "formula_text": "\u2207 v i \u03a6 = W i \u2022 A i + \u03b1v i \u2207 W i \u03a6 = v i A i + \u03b1W i (8.6)", "formula_coordinates": [26.0, 194.41, 696.14, 346.86, 11.58]}, {"formula_id": "formula_132", "formula_text": "\u2207 v i H = W i \u2022 X1 W i \u2022X\u22650 \u2212 y + \u03c3(v \u2022 g(W X) \u2212 A i ,(8.7)", "formula_coordinates": [27.0, 178.83, 94.64, 362.45, 11.58]}, {"formula_id": "formula_133", "formula_text": "\u2207 W i H = v i X1 W i \u2022X\u22650 \u2212 y + \u03c3(v \u2022 g(W X) \u2212 A i .(8.8)", "formula_coordinates": [27.0, 174.96, 118.51, 366.31, 11.58]}, {"formula_id": "formula_134", "formula_text": "V v i ,v j = E (W i \u2022 X)(W j \u2022 X)1 W i \u2022X\u22650 1 W j \u2022X\u22650 (\u2212y + \u03c3(v \u2022 g(W X))) 2 \u2212 (W i \u2022 A i )(W j \u2022 A j ) V v i ,W j = v j E (W i \u2022 X)X1 W i \u2022X\u22650 1 W j \u2022X\u22650 (\u2212y + \u03c3(v \u2022 g(W X))) 2 \u2212 v j (W i \u2022 A i )A j V W i ,W j = v i v j E X \u22972 1 W i \u2022X\u22650 1 W j \u2022X\u22650 (\u2212y + \u03c3(v \u2022 g(W X))) 2 \u2212 v i v j A i \u2297 A j . (8.9)", "formula_coordinates": [27.0, 91.72, 153.7, 449.55, 50.53]}, {"formula_id": "formula_135", "formula_text": "Lemma 8.2. Fix w \u2208 R n . We have E[|X \u2022 w| 8 ] \u2272 (w \u2022 \u00b5) 8 + \u2225w\u2225 8 \u03bb \u22124 and \u2225A i \u2225 \u2264 C(u n ). Lemma 8.3. For each i, for every R \u22a5", "formula_coordinates": [27.0, 72.0, 266.51, 426.03, 31.2]}, {"formula_id": "formula_136", "formula_text": "lim \u03bb\u2192\u221e P W i \u2022 X \u00b5 < 0) = 0 . (8.10)", "formula_coordinates": [27.0, 245.68, 305.94, 295.59, 16.42]}, {"formula_id": "formula_137", "formula_text": "lim \u03bb\u2192\u221e E \u03c3(v \u2022 g(W X \u00b5 )) \u2212 \u03c3(v \u2022 g(m)) = 0 . (8.11) Fact 8.1. Fix \u00b5 \u2208 S N \u22121 (1)", "formula_coordinates": [27.0, 72.0, 348.81, 469.27, 34.94]}, {"formula_id": "formula_138", "formula_text": "(v i , W i ) \u2208 R \u00d7 R N , E[exp(\u03b8v i g(W i \u2022 X \u00b5 ))] \u2264 exp \u03b8v i m i + 1 2\u03bb \u03b8 2 v 2 i R \u22a5 ii . 8.1.", "formula_coordinates": [27.0, 72.0, 385.82, 353.35, 49.23]}, {"formula_id": "formula_139", "formula_text": "E[\u2225\u2207H\u2225 8 ] by i=1,2 E[|\u2207 v i H| 8 ]+E[\u2225\u2207 W i H\u2225 8", "formula_coordinates": [27.0, 198.93, 553.02, 215.08, 14.17]}, {"formula_id": "formula_140", "formula_text": "\u2264 C(a 8 + b 8 ), for i \u2208 {1, 2}, the first term is at most C(E[|X \u2022 W i | 8 ] + \u2225W i \u2225 8 \u2225A i \u2225 8", "formula_coordinates": [27.0, 72.0, 567.89, 469.51, 25.54]}, {"formula_id": "formula_141", "formula_text": "E[\u2225\u2207 W i H\u2225 8 ] is controlled by C v 8 i E \u2225X1 W i \u2022X\u22650 \u03c3(\u2212v \u2022 g(W X))\u2225 8 + v 8 i \u2225A i \u2225 8 \u2264 C|v i | 8 1 + E||Z|| 8 \u03bb 4 .", "formula_coordinates": [27.0, 72.0, 606.74, 410.5, 43.91]}, {"formula_id": "formula_142", "formula_text": "\u03b4 2 n sup i sup x\u2208u \u22121 n (E K ) E[\u27e8\u2207H, \u2207u i \u27e9 4 ] \u2264 C(K) .(8.12)", "formula_coordinates": [27.0, 209.5, 683.65, 331.77, 23.81]}, {"formula_id": "formula_143", "formula_text": "E[\u27e8\u2207 W i H, w\u27e9 4 ] \u2264 C|v i | 4 E[|X \u2022 w| 4 ] + \u2225w\u2225 4 \u2225A i \u2225 4 ,", "formula_coordinates": [28.0, 184.69, 109.99, 242.61, 14.01]}, {"formula_id": "formula_144", "formula_text": "E[\u27e8\u2207 2 u, \u2207H \u2297 \u2207H \u2212 V \u27e9 2 ]", "formula_coordinates": [28.0, 231.99, 160.96, 126.44, 12.13]}, {"formula_id": "formula_145", "formula_text": "\u03b4 n L n R \u22a5 ij = c \u03b4 N k\u0338 =1 v i v j E (X \u2022 e k ) 2 1 W i \u2022X\u22650 1 W j \u2022X\u22650 (\u2212y + \u03c0) 2 \u2212 (A i \u2022 e k )(A j \u2022 e k ) = c \u03b4 N v i v j E \u2225X \u22a5 \u2225 2 1 W i \u2022X\u22650 1 W j \u2022X\u22650 (\u2212y + \u03c0) 2 \u2212 \u27e8A i \u2212 A \u00b5 i \u00b5, A j \u2212 A \u00b5 j \u00b5\u27e9 . (8.13)", "formula_coordinates": [28.0, 93.34, 344.44, 447.94, 57.2]}, {"formula_id": "formula_146", "formula_text": "1 N E[\u2225X \u22a5 \u2225 2 1 W i \u2022X\u22650 1 W j \u2022X\u22650 (\u2212y + \u03c0) 2 ] as E 1 N \u2225X \u22a5 \u2225 2 \u2212 \u03bb \u22121 1 W i \u2022X\u22650 1 W j \u2022X\u22650 (\u2212y + \u03c0) 2 + \u03bb \u22121 B ij .", "formula_coordinates": [28.0, 167.25, 409.67, 335.22, 39.54]}, {"formula_id": "formula_147", "formula_text": "\u03bb \u22121 E[( \u2225Z\u2225 2 N \u2212 1) 2 ] 1/2 \u2264 2 \u03bb \u221a", "formula_coordinates": [28.0, 72.0, 483.08, 125.08, 19.21]}, {"formula_id": "formula_148", "formula_text": "1 N \u27e8A i \u2212 A \u00b5 i \u00b5, A j \u2212 A \u00b5 j \u00b5\u27e9 = 1 N E (X \u22a5 1 \u2022 X \u22a5 2 )1 W i \u2022X 1 \u22650 1 W j \u2022X 2 \u22650 (\u2212y + \u03c0 1 )(\u2212y + \u03c0 2 ) ,", "formula_coordinates": [28.0, 107.22, 536.73, 398.76, 24.43]}, {"formula_id": "formula_149", "formula_text": "\u2022g(W X 2 )). By Cauchy-Schwarz, if Z, Z \u2032 are i.i.d. N (0, I \u2212 e \u22972 1 )", "formula_coordinates": [28.0, 72.0, 570.16, 470.12, 25.78]}, {"formula_id": "formula_150", "formula_text": "1 \u03bbN E[(Z \u2022 Z \u2032 ) 2 ] 1/2 \u2264 1 \u03bb \u221a N .", "formula_coordinates": [28.0, 394.68, 581.53, 119.76, 16.68]}, {"formula_id": "formula_151", "formula_text": "g R \u22a5 ij = c \u03b4 v i v j \u03bb E 1 W i \u2022X\u22650 1 W j \u2022X\u22650 (\u2212y + \u03c0) 2 = c \u03b4 v i v j \u03bb B ij .", "formula_coordinates": [28.0, 170.69, 615.79, 270.62, 24.49]}, {"formula_id": "formula_152", "formula_text": "E[(X \u2022 \u00b5)1 W i \u2022X\u22650 (\u2212y + \u03c3(v \u2022 g(W X)))] = 1 2 E (X \u00b5 \u2022 \u00b5)1 W i \u2022X\u00b5\u22650 (\u22121 + \u03c3(v \u2022 g(W X \u00b5 ))) + 1 2 E (\u2212X \u00b5 \u2022 \u00b5)1 W i \u2022X\u00b5\u22640 \u03c3(v \u2022 g(\u2212W X \u00b5 )) .", "formula_coordinates": [29.0, 97.29, 143.71, 417.42, 50.56]}, {"formula_id": "formula_153", "formula_text": "1 2 1 m i >0 \u03c3(\u2212v \u2022 g(m)) and \u2212 1 2 1 m i <0 \u03c3(v \u2022 g(\u2212m)", "formula_coordinates": [29.0, 72.0, 201.05, 468.0, 29.24]}, {"formula_id": "formula_154", "formula_text": "E (X \u00b5 \u2022 \u00b5)1 W i \u2022X\u00b5\u22650 \u03c3(\u2212v \u2022 g(W X \u00b5 )) \u2212 1 m i \u22650 \u03c3(\u2212v \u2022 g(m)) (8.14) = E (X \u00b5 \u2022 \u00b5 \u2212 1)1 W i \u2022X\u00b5\u22650 \u03c3(\u2212v \u2022 g(W X \u00b5 )) + E (1 W i \u2022X\u00b5\u22650 \u2212 1 m i \u22650 )\u03c3(\u2212v \u2022 g(W X \u00b5 )) + 1 m i \u22650 E \u03c3(\u2212v \u2022 g(W X \u00b5 )) \u2212 \u03c3(\u2212v \u2022 g(m)) .", "formula_coordinates": [29.0, 99.31, 242.97, 441.96, 83.14]}, {"formula_id": "formula_155", "formula_text": "lim \u03bb\u2192\u221e A \u00b5 i = \u2212 1 2 1 m i >0 \u03c3(\u2212v \u2022 g(m)) \u2212 1 2 1 m i <0 \u03c3(v \u2022 g(\u2212m)) ,at", "formula_coordinates": [29.0, 72.0, 409.27, 371.8, 44.02]}, {"formula_id": "formula_156", "formula_text": "A \u22a5 ij = \u2212 1 2 E (X \u00b5 \u2022 W \u22a5 j )1 W i \u2022X\u22650 \u03c3(\u2212v \u2022 g(W X \u00b5 )) (8.15) \u2212 1 2 E (X \u00b5 \u2022 W \u22a5 j )1 W i \u2022X\u00b5<0 \u03c3(v \u2022 g(\u2212W X \u00b5 )) .", "formula_coordinates": [29.0, 185.09, 504.66, 356.18, 50.56]}, {"formula_id": "formula_157", "formula_text": "(1/2)E[|X \u00b5 \u2022 W \u22a5 j |] which is at most (1/2) R \u22a5 jj \u03bb \u22121/2 by (8.2).", "formula_coordinates": [29.0, 70.72, 576.52, 294.62, 14.21]}, {"formula_id": "formula_158", "formula_text": "|B ij | \u2264 1, the quantity g R \u22a5 ij = c \u03b4 v i v j", "formula_coordinates": [29.0, 149.56, 603.32, 169.08, 18.15]}, {"formula_id": "formula_159", "formula_text": "A 1 \u2022 \u00b5 = E Z 1,\u00b5 1 Z 1,\u22a5 \u22650 \u03c3(\u2212v \u2022 g(Z 1,\u22a5 , m 2 Z 2,\u00b5 + Z 2,\u22a5 )) .", "formula_coordinates": [29.0, 173.43, 679.4, 265.15, 11.74]}, {"formula_id": "formula_160", "formula_text": "u i = \u2212 u i 2 \u03c3(\u2212v \u2022 m) \u2212 \u03b1u i m 1 m 2 > 0 \u2212 u i 2 \u03c3(\u2212v i m i ) \u2212 \u03b1u i else ,", "formula_coordinates": [30.0, 208.03, 94.33, 195.94, 30.89]}, {"formula_id": "formula_161", "formula_text": "\u03a6(v, m) = 1 2 log(1 + e \u2212v\u2022g(m) ) + log(1 + e v\u2022g(\u2212m) ) + \u03b1 2 i=1,2 (v 2 i + m 2 i + R \u22a5 ii ) .", "formula_coordinates": [30.0, 121.28, 250.43, 369.43, 29.46]}, {"formula_id": "formula_162", "formula_text": "v 1 = \u2212 \u221a C \u03b1 , v 2 = 0. If v 1 = \u2212 \u221a C", "formula_coordinates": [30.0, 215.09, 385.5, 169.08, 19.05]}, {"formula_id": "formula_163", "formula_text": "v i , m i replaced by\u1e7d i = \u221a N (v i \u2212 a i ) andm i = \u221a N (m i \u2212 a i ).", "formula_coordinates": [30.0, 72.0, 486.91, 288.05, 19.87]}, {"formula_id": "formula_164", "formula_text": "f\u1e7d i = \u2212 \u221a N v i 2 \u03c3(\u2212v \u2022 m) + \u03b1 \u221a N m i and fm i = \u2212 \u221a N m i 2 \u03c3(\u2212v \u2022 m) + \u03b1 \u221a N v i .", "formula_coordinates": [30.0, 106.11, 618.78, 399.78, 26.82]}, {"formula_id": "formula_165", "formula_text": "a 2 j = C \u03b1 , v \u2022 m = C \u03b1 + N \u22121/2 j=1,2 a j (\u1e7d j +m j ) + O(1/n) .", "formula_coordinates": [30.0, 192.12, 660.75, 304.83, 46.2]}, {"formula_id": "formula_166", "formula_text": "\u03c3(\u2212v \u2022 m) = \u03c3(\u2212C \u03b1 ) + (v \u2022 m \u2212 C \u03b1 )\u03c3(\u2212C \u03b1 )(1 \u2212 \u03c3(\u2212C \u03b1 )) + O(n \u22121 ) = 2\u03b1 + N \u22121/2 a j j=1,2 \u1e7d j +m j (2\u03b1)(1 \u2212 2\u03b1) + O(n \u22121 ) .", "formula_coordinates": [31.0, 143.12, 93.45, 325.76, 45.74]}, {"formula_id": "formula_167", "formula_text": "f\u1e7d i = \u2212 N 1/2 a i +m i 2 2\u03b1 + a j N 1/2 j=1,2 \u1e7d j +m j (2\u03b1)(1 \u2212 2\u03b1) + O 1 n + \u03b1(n 1/2 a i +\u1e7d i ) = \u2212\u03b1m i + \u03b1\u1e7d i \u2212 a i (\u03b1 \u2212 2\u03b1 2 )", "formula_coordinates": [31.0, 97.11, 170.2, 417.78, 51.38]}, {"formula_id": "formula_168", "formula_text": "V v i ,v j = m i m j 4 \u2022 \u03c3(\u2212v \u2022 m) 2 m 1 m 2 > 0 \u03c3(\u2212v i m i )\u03c3(\u2212v j m j ) else ,", "formula_coordinates": [31.0, 183.59, 330.54, 244.83, 28.13]}, {"formula_id": "formula_169", "formula_text": "V v i ,v j = \u03b1 2 a i a j + O(n \u22121/2 ) , V v i ,W j = \u00b5(\u03b1 2 a i a j + O(n \u22121/2 )) , V W i ,W j = \u00b5 \u22972 (\u03b1 2 a i a j + O(n \u22121/2 )) .", "formula_coordinates": [31.0, 153.04, 392.03, 305.93, 46.83]}, {"formula_id": "formula_170", "formula_text": "L(v, W ) = \u2212yv \u2022 g(W X) + log 1 + e v\u2022g(W X) + p(v, W ) (9.1)", "formula_coordinates": [31.0, 173.97, 564.32, 367.3, 12.06]}, {"formula_id": "formula_171", "formula_text": "L((v, W )) d = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \u2212v \u2022 g(W X \u00b5 ) + log(1 + e v\u2022g(W X\u00b5) ) + p(v, W ) w. prob. 1/4 \u2212v \u2022 g(W X \u2212\u00b5 ) + log(1 + e v\u2022g(W X \u2212\u00b5 ) ) + p(v, W ) w. prob. 1/4 log(1 + e v\u2022g(W X\u03bd ) ) + p(v, W ) w. prob. 1/4 log(1 + e v\u2022g(W X \u2212\u03bd ) ) + p(v, W ) w. prob. 1/4 (9.2)", "formula_coordinates": [32.0, 121.9, 187.31, 419.37, 58.43]}, {"formula_id": "formula_172", "formula_text": "W X \u03b9 = (m i + Z i,\u03b9 m \u03b9 i + Z i\u22a5 ) i=1,...,K for \u03b9 \u2208 {\u00b5, \u03bd} ,", "formula_coordinates": [32.0, 177.96, 273.42, 256.07, 14.19]}, {"formula_id": "formula_173", "formula_text": "Cov(Z i\u22a5 , Z j\u22a5 ) = \u03bb \u22121 R \u22a5 ij .", "formula_coordinates": [32.0, 245.77, 315.75, 120.47, 14.19]}, {"formula_id": "formula_174", "formula_text": "p(v, W ) = \u03b1 2 i=1,...,K v 2 i + R \u22a5 ii .", "formula_coordinates": [32.0, 231.23, 361.3, 149.55, 24.76]}, {"formula_id": "formula_175", "formula_text": "\u2202 v i v i = 1 , \u2207 W i m \u00b5 i = \u00b5 , \u2207 W i m \u03bd i = \u03bd , \u2207 W i R \u22a5 jk = W \u22a5 j \u03b4 ij + W \u22a5 k \u03b4 ik , (9.3)", "formula_coordinates": [32.0, 118.99, 438.87, 422.28, 15.55]}, {"formula_id": "formula_176", "formula_text": "A i = E X1 W i \u2022X\u22650 \u2212 y + \u03c3(v \u2022 g(W X)) .", "formula_coordinates": [32.0, 203.97, 543.62, 204.07, 11.55]}, {"formula_id": "formula_177", "formula_text": "\u03b4 n L n R \u22a5 ij = c \u03b4 N v i v j E \u2225X \u22a5 \u2225 2 1 W i \u2022X\u22650 1 W j \u2022X\u22650 (\u03c0 \u2212 y) 2 \u2212 \u27e8A i \u2212 A \u00b5 i \u00b5 \u2212 A \u03bd i \u03bd, A j \u2212 A \u00b5 j \u00b5 \u2212 A \u03bd j \u03bd\u27e9 .", "formula_coordinates": [33.0, 156.66, 208.55, 298.69, 43.9]}, {"formula_id": "formula_178", "formula_text": "g R \u22a5 ij = c \u03b4 v i v j \u03bb E 1 W i \u2022X\u22650 1 W j \u2022X\u22650 (\u2212y + \u03c0) 2 = c \u03b4 v i v j \u03bb B ij .", "formula_coordinates": [33.0, 170.69, 287.87, 270.62, 24.49]}, {"formula_id": "formula_179", "formula_text": "v i = m \u00b5 i 4 1 m \u00b5 i \u22650 \u03c3(\u2212v \u2022 g(m \u00b5 )) \u2212 1 m \u00b5 i <0 \u03c3(\u2212v \u2022 g(\u2212m \u00b5 )) \u2212 m \u03bd i 4 1 m \u03bd i \u22650 \u03c3(v \u2022 g(m \u03bd )) \u2212 1 m \u03bd i <0 \u03c3(v \u2022 g(\u2212m \u03bd )) \u2212 \u03b1v i , m \u00b5 i = v i 4 1 m \u00b5 i \u22650 \u03c3(\u2212v \u2022 g(m \u00b5 )) \u2212 1 m \u00b5 i <0 \u03c3(\u2212v \u2022 g(\u2212m \u00b5 )) \u2212 \u03b1m \u00b5 i , m \u03bd i = \u2212 v i 4 1 m \u03bd i \u22650 \u03c3(\u2212v \u2022 g(m \u03bd )) \u2212 1 m \u03bd i <0 \u03c3(\u2212v \u2022 g(\u2212m \u03bd )) \u2212 \u03b1m \u03bd i , and\u1e58 \u22a5 ij = \u22122\u03b1R \u22a5 ij for 1 \u2264 i \u2264 j \u2264 K. Proof.", "formula_coordinates": [33.0, 71.17, 425.36, 686.57, 141.02]}, {"formula_id": "formula_180", "formula_text": "lim \u03bb\u2192\u221e A \u00b5 i = \u2212 1 4 1 m \u00b5 i >0 \u03c3(\u2212v \u2022 g(m \u00b5 )) \u2212 1 4 1 m \u00b5 i <0 \u03c3(v \u2022 g(\u2212m)) .", "formula_coordinates": [33.0, 164.49, 573.75, 283.02, 24.43]}, {"formula_id": "formula_181", "formula_text": "A i = 1 4 E \u2212 X \u00b5 1 W i \u2022X\u00b5\u22650 (\u03c3(\u2212v \u2022 g(W X \u00b5 ))) \u2212 1 4 E X \u2212\u00b5 1 W i \u2022X \u2212\u00b5 \u22650 (\u03c3(\u2212v \u2022 g(W X \u2212\u00b5 ))) + 1 4 E X \u03bd 1 W i \u2022X\u03bd \u22650 (\u03c3(v \u2022 g(W X \u03bd ))) + 1 4 E X \u2212\u03bd 1 W i \u2022X \u2212\u03bd \u22650 (\u03c3(v \u2022 g(W X \u2212\u03bd ))) .", "formula_coordinates": [33.0, 101.45, 619.54, 404.56, 50.56]}, {"formula_id": "formula_182", "formula_text": "E (X \u00b5 \u2022 \u00b5)1 W i \u2022X\u00b5\u22650 \u03c3(\u2212v \u2022 g(W X \u00b5 )) \u2212 1 m \u00b5 i \u22650 \u03c3(\u2212v \u2022 g(m \u00b5", "formula_coordinates": [34.0, 161.35, 92.34, 275.47, 16.48]}, {"formula_id": "formula_183", "formula_text": "E (X \u03bd \u2022 \u00b5)1 W i \u2022X\u03bd \u22650 \u03c3(v \u2022 g(W X \u03bd )) \u2264 E[|X \u03bd \u2022 \u00b5|] ,", "formula_coordinates": [34.0, 189.78, 163.0, 236.08, 11.51]}, {"formula_id": "formula_184", "formula_text": "4\u03b1 v i m \u00b5 i = 4\u03b1 m \u00b5 i v i , or v 2 i = (m \u00b5 i ) 2 ,", "formula_coordinates": [35.0, 209.75, 92.0, 192.51, 30.6]}, {"formula_id": "formula_185", "formula_text": "4\u03b1v i = v i \u03c3(\u2212v \u2022 g(m \u00b5 )) , or 4\u03b1 = \u03c3 \u2212 j\u2208I + \u00b5 v 2 j ,", "formula_coordinates": [35.0, 172.27, 240.49, 267.46, 27.58]}, {"formula_id": "formula_186", "formula_text": "i = \u221a N (v i \u2212 a i,\u00b5 ) i = 1, 2 \u2212 \u221a N (v i \u2212 a i,\u03bd ) i = 3, 4 andm \u00b5 i = \u221a N (m \u00b5 i \u2212 a i,\u00b5 ) i = 1, 2 0 i = 3, 4 ,m \u03bd i = 0 i = 1, 2 \u221a N (m \u03bd i \u2212 a i,\u03bd ) i = 3, 4", "formula_coordinates": [37.0, 72.0, 156.28, 401.18, 96.2]}, {"formula_id": "formula_187", "formula_text": "f\u1e7d i = \u03b1(\u1e7d i \u2212m \u00b5 i ) \u2212 a i,\u00b5 (\u03b1 \u2212 4\u03b1 2 ) k=1,2 a k,\u00b5 (\u1e7d k +m \u00b5 k ) .", "formula_coordinates": [37.0, 179.72, 564.92, 252.56, 25.65]}, {"formula_id": "formula_188", "formula_text": "f\u1e7d i = \u03b1(\u1e7d i \u2212m \u03bd i ) \u2212 a i,\u03bd (\u03b1 \u2212 4\u03b1 2 ) k=3,4", "formula_coordinates": [37.0, 180.54, 621.94, 179.34, 24.85]}, {"formula_id": "formula_189", "formula_text": "V v i ,v j = 3 16 m \u00b5 i m \u00b5 j \u03c3(\u2212v \u2022 m \u00b5 ) 2 i, j \u2208 {1, 2} 3 16 m \u03bd i m \u03bd j \u03c3(v \u2022 m \u03bd ) 2 i, j \u2208 {3, 4}", "formula_coordinates": [38.0, 200.06, 104.34, 210.69, 31.74]}, {"formula_id": "formula_190", "formula_text": "V v i ,v j = \u2212 1 16 m \u00b5 i m \u03bd j \u03c3(\u2212v \u2022 m \u00b5 )\u03c3(v \u2022 m \u03bd )", "formula_coordinates": [38.0, 212.73, 158.31, 186.54, 24.43]}, {"formula_id": "formula_191", "formula_text": "\u03a3 v i ,v j = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3", "formula_coordinates": [38.0, 195.14, 217.33, 50.28, 42.98]}, {"formula_id": "formula_192", "formula_text": "V v i ,W j \u2022 \u00b5 = 3 16 v j m \u00b5 i \u03c3(\u2212v \u2022 m \u00b5 ) 2 i, j \u2208 {1, 2} V v i ,W j \u2022 \u03bd = 3 16 v j m \u03bd i \u03c3(v \u2022 m \u03bd ) 2 i, j \u2208 {3, 4}", "formula_coordinates": [38.0, 192.0, 286.11, 228.0, 50.56]}, {"formula_id": "formula_193", "formula_text": "V v i ,W j \u2022 \u03bd = \u2212 1 16 v j m \u00b5 i \u03c3(\u2212v \u2022 m \u00b5 )\u03c3(v", "formula_coordinates": [38.0, 203.94, 356.67, 172.0, 24.43]}, {"formula_id": "formula_194", "formula_text": "E[X \u00b5 1 W i \u2022X\u00b5\u22650 (\u22121 + \u03c3(g(W X \u00b5 )))] = E[(\u00b5 + \u03bb \u22121/2 Z)1 W i \u2022X\u00b5\u22650 (\u22121 + \u03c3(g(W X \u00b5 )))] . Now decompose Z as Z \u00b5 \u00b5 + Z 1,\u22a5 W \u22a5 1 + Z 2,\u22a5 W \u22a5 2 + Z 3", "formula_coordinates": [38.0, 72.0, 574.67, 430.17, 34.11]}, {"formula_id": "formula_195", "formula_text": "\u2225A i \u2225 2 \u2264 w\u2208{\u00b5,W \u22a5 1 ,W \u22a5 2 } E[(X \u2022 w) 2 1 W i \u2022X\u22650 (\u2212y + \u03c3(g(W X)))] \u2264 (1 + R \u22a5 11 + R \u22a5 22 )(1 + \u03bb \u22121 ) .", "formula_coordinates": [38.0, 94.29, 666.42, 423.42, 28.36]}, {"formula_id": "formula_196", "formula_text": "2 i + R \u22a5 ii ) \u22121/2 = e \u2212m 2 i \u03bb/2(m 2 i +R \u22a5 ii ) ,", "formula_coordinates": [39.0, 324.87, 94.62, 156.65, 15.64]}, {"formula_id": "formula_197", "formula_text": "E \u03c3(v \u2022 g(W X \u00b5 )) \u2212 \u03c3(v \u2022 g(m)", "formula_coordinates": [39.0, 110.98, 158.47, 144.73, 10.63]}, {"formula_id": "formula_198", "formula_text": "e v i (W i \u2022X\u00b5)1 W i \u2022X\u00b5\u22650 \u2212 e v i (W i \u2022X\u00b5)1 m i \u22650 + e v i (W i \u2022X\u00b5)1 m i \u22650 \u2212 e v i m i 1 m i \u22650 .", "formula_coordinates": [39.0, 143.7, 366.82, 329.59, 12.34]}, {"formula_id": "formula_199", "formula_text": "E e v i (W i \u2022X\u00b5)1 W i \u2022X\u00b5\u22650 \u2212 e v i (W i \u2022X\u00b5)1 m i \u22650 2 \u2264 (1 \u2228 e v i (W i \u2022X\u00b5) )E[1 W i \u2022X\u00b5\u22650 \u2212 1 m i \u22650 ] .", "formula_coordinates": [39.0, 113.46, 410.55, 385.09, 15.63]}, {"formula_id": "formula_200", "formula_text": "e 2v i m i E (e v i (W i \u2022G \u03bb ) \u2212 1) 2 .", "formula_coordinates": [39.0, 243.29, 503.93, 125.42, 12.68]}, {"formula_id": "formula_201", "formula_text": "2 i + R \u22a5 ii )\u03bb \u22121 . \u25a1", "formula_coordinates": [39.0, 333.56, 541.03, 206.44, 14.0]}], "doi": ""}