{"title": "If Beam Search is the Answer, What was the Question?", "authors": "Clara Meister; Ryan Cotterell; Tim Vieira", "pub_date": "", "abstract": "Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural language generators frequently leads to low-quality results (Stahlberg and Byrne, 2019). Rather, most state-of-the-art results on language generation tasks are attained using beam search despite its overwhelmingly high search error rate. This implies that the MAP objective alone does not express the properties we desire in text, which merits the question: if beam search is the answer, what was the question? We frame beam search as the exact solution to a different decoding objective in order to gain insights into why high probability under a model alone may not indicate adequacy. We find that beam search enforces uniform information density in text, a property motivated by cognitive science. We suggest a set of decoding objectives that explicitly enforce this property and find that exact decoding with these objectives alleviates the problems encountered when decoding poorly calibrated language generation models. Additionally, we analyze the text produced using various decoding strategies and see that, in our neural machine translation experiments, the extent to which this property is adhered to strongly correlates with BLEU.", "sections": [{"heading": "Introduction", "text": "As a simple search heuristic, beam search has been used to decode models developed by the NLP community for decades. Indeed, it is noteworthy that beam search is one of the few NLP algorithms that has stood the test of time: It has remained a cornerstone of NLP systems since the 1970s (Reddy, 1977). As such, it became the natural choice for decoding neural probabilistic text generators-whose design makes evaluating the full search space impossible (Kalchbrenner and Blunsom, 2013;Sutskever et al., 2014;Vinyals and Le, 2015;Yin et al., 2016). While there is no formal guarantee that beam search will return-Figure 1: Average std. deviation \u03c3 of surprisals (per sentence) and corpus BLEU for translations generated using exact search over the MAP objective with a greedy regularizer (Eq. (11)) with varying degrees of \u03bb. References for beam search (k = 5 and k = 100) are included. Sub-graph shows the explicit relationship between BLEU and \u03c3. \u03bb and \u03c3 axes are log-scaled.\nor even approximate-the highest-scoring candidate under a model, it has repeatedly proven its merit in practice (Serban et al., 2017;Edunov et al., 2018;Yang et al., 2019) and, thus, has largely been tolerated-even embraced-as NLP's go-to search heuristic. However, in the context of neural machine translation (NMT), a shocking empirical finding has emerged: Using beam search to decode sentences from neural text generators almost invariably leads to better text than using exact search (or beam search with a very large beam size). In fact, Stahlberg and Byrne (2019) report that exact search returns the empty string in > 50% of cases, 1 showing that the success of beam search does not stem from its ability to approximate exact decoding in practice, but rather due to a hidden inductive bias embedded in the algorithm. This inductive bias appears to be paramount for generating desirable text from neural probabilistic text generators. While several works explore this phenomenon (Murray and Chiang, 2018;Yang et al., 2018;Stahlberg and Byrne, 2019;Cohen and Beck, 2019), no one has yet hypothesized what beam search's hidden inductive bias may be. Our work fills this gap.\nWe analyze the beam search blessing by reverse engineering an objective that beam search returns the exact solution for. Specifically, we introduce a regularizer for the the standard (MAP) decoding objective for text generation models such that the exact solution to this regularized objective is equivalent to the solution found by beam search under the unmodified objective. Qualitative inspection reveals that our \"beam search regularizer\" has a clear connection to a theory in cognitive science-the uniform information density hypothesis (UID; Levy and Jaeger, 2007). The UID hypothesis states that-subject to the constraints of the grammar-humans prefer sentences that distribute information (in the sense of information theory) equally across the linguistic signal, e.g., a sentence. In other words, human-produced text, regardless of language, tends to have evenly distributed surprisal, formally defined in information theory as negative log-probability. This connection suggests beam search has an interpretation as exact decoding, but with a UID-promoting regularizer that encourages evenly distributed surprisal in generated text. This insight naturally leads to the development of several new regularizers that likewise enforce the UID property.\nEmpirically, we experiment with our novel regularizers in the decoding of NMT models. We first observe a close relationship between the standard deviation of surprisals-an operationalization of UID-and BLEU, which suggests that high-quality text does indeed exhibit the UID property. Additionally, we find that even with exact search, our regularized objective leads to performance similar to beam search on standard NMT benchmarks. Both of these observations are reflected in Fig. 1. Lastly, we see that our regularizers alleviate the text-quality degradation typically seen when decoding with larger beam sizes. We take all the above as evidence that our proposed explanation of beam search's inductive bias indeed elucidates why the algorithm performs so well as a search heuristic for language generation tasks.", "publication_ref": ["b38", "b20", "b44", "b49", "b54", "b39", "b8", "b53", "b42", "b30", "b52", "b42", "b6", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Neural Probabilistic Text Generation", "text": "Probabilistic text generators define a probability distribution p \u03b8 (y | x) over an output space of hypotheses Y (to be defined in Eq. (1)) conditioned on an input x. 2 Modern generators are typically parameterized by a deep neural network-possibly recurrent-with a set of learned weights \u03b8. In the case of text generation, the full set of possible hypotheses grows exponentially with the vocabulary size |V|. We consider the set of complete hypotheses, i.e., valid outputs, as\nY := {BOS \u2022 v \u2022 EOS | v \u2208 V * } (1)\nwhere \u2022 is string concatenation and V * is the Kleene closure of V. In words, valid hypotheses are text, e.g., sentences or phrases, padded with distinguished tokens, BOS and EOS. In this work, we consider models that are locally normalized, i.e., the model p \u03b8 is defined as the product of probability distributions:\np \u03b8 (y | x) = |y| t=1 p \u03b8 (y t | x, y <t ) (2)\nwhere each p \u03b8 (\u2022 | x, y <t ) is a distribution with support overV := V \u222a {EOS} and y <1 = y 0 := BOS. The decoding objective for text generation aims to find the most-probable hypothesis among all candidate hypotheses, i.e. we aim to solve the following optimization problem:\ny = argmax y\u2208Y log p \u03b8 (y | x)\n(3) This is commonly known as maximum a posteriori (MAP) decoding since p \u03b8 is a probability model. While there exists a wealth of literature on decoding algorithms for statistical text generation models, e.g., phrase-based machine translation models, many of these methods cannot reasonably be used with neural models. Specifically, due to the non-Markovian structure of most neural text generators, dynamic-programming algorithms for searching over the exponentially large space are not efficient in this setting. Indeed, there are formal results that solving Eq. (3) with a recurrent neural network is NP-hard (Chen et al., 2018). Therefore decoding is performed almost exclusively with heuristic methods, such as beam search.", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "Beam Search", "text": "Beam search is a form of pruned breadth-first search where the breadth is limited to k \u2208 Z + (i.e., a maximum of k hypotheses) are expanded at each time step. We express beam search as the following recursion:\nY 0 = {BOS} (4) Y t = argmax Y \u2286Bt, |Y |=k log p \u03b8 (Y | x)(5)\nwhere we define the candidate set at t > 0\nB t = y t 1 \u2022 y | y \u2208V and y t 1 \u2208 Y t 1 (6)\nFor notational convenience, we define EOS \u2022 EOS = EOS. The above algorithm terminates after a fixed number of iterations 3 n max and the set Y nmax is returned. We overload p \u03b8 (\u2022 | x) to take a set of hypotheses as an argument instead of just a single hypothesis. In this case, p \u03b8 (Y | x) := y\u2208Y p \u03b8 (y | x). 4 Using a similar schema, the argmax may also operate over a different objective, e.g., logprobabilities combined with various rewards or penaties, such as those discussed in \u00a72.2. Beam search has a long history in sequence transduction. For example, many of the decoding strategies used in statistical machine translation (SMT) systems were variants of beam search (Och et al., 1999;Koehn et al., 2003;Koehn, 2004). As language generation systems moved away from phrase-based statistical approaches and towards neural models, beam search remained the de-facto decoding algorithm (Sutskever et al., 2014;Vinyals and Le, 2015). However, it has been observed that when used as a decoding algorithm for neural text generation, beam search (for small beams) typically has a large percentage of search errors 3 If all hypotheses in Yt end in EOS for some t < nmax, then we may terminate beam search early as it is then gauranteed that Yt = Yn max . We do not consider further earlystopping methods for beam search (Huang et al., 2017;Yang et al., 2018;Meister et al., 2020) as they generally should not affect the quality of the decoded set. 4 There do exist objectives that take into account interactions between hypotheses in a set, e.g., diverse beam search (Vijayakumar et al., 2018), but we do not consider those here. (Stahlberg and Byrne, 2019). Counterintuitively, it is widely known that increasing the beam size beyond 5 can hurt model performance in terms of downstream evaluation metrics (e.g., BLEU, ROUGE); while a number of prior works have referred to this phenomenon as a curse (Koehn and Knowles, 2017;Yang et al., 2018;Cohen and Beck, 2019), it should perhaps be seen as a blessing. Beam search typically generates well-formed and coherent text from probabilistic models, whose global optimum in many cases is the empty string, when they otherwise might fail to produce text at all. As we demonstrate in \u00a74, this text also tends to be human-like. We will subsequently explore possible reasons as to why beam search leads to desirable text from models that are otherwise poorly calibrated, i.e., poor representations of the true distribution p(y | x) (Guo et al., 2017).", "publication_ref": ["b32", "b24", "b22", "b44", "b49", "b15", "b52", "b29", "b48", "b42", "b23", "b52", "b6", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Alternative Decoding Objectives", "text": "When the MAP objective (Eq. (3)) is used for decoding neural text generators, the results are generally not satisfactory. Among other problems, the generated texts are often short and defaults to highfrequency words (Cho et al., 2014;Vinyals and Le, 2015;Shen et al., 2016). Methods such as length and coverage normalization (Jean et al., 2015;Tu et al., 2016;Murray and Chiang, 2018), which augment the MAP objective with an additive term or multiplicative factor, have been adopted to alleviate these issues. For example, two such forms of length 5 and coverage normalization use the following modified MAP objective respectively during decoding to produce higher-quality output:\nlog p \u03b8 (y | x) + \u03bb|y| (7) log p \u03b8 (y | x)+\u03bb |x| i=1 log min \uf8eb \uf8ed 1, |y| j=1 \u03b1 ij \uf8f6 \uf8f8 (8)\nwhere \u03bb > 0 is the (tunable) strength of the reward and \u03b1 ij is the attention weight (Bahdanau et al., 2015) from the j th decoding step over the i th input. Eq. (7) directly rewards longer outputs  while Eq. (8) aims to reward coverage of input words in a prediction using the attention mechanism of an encoder-decoder model as an oracle (Tu et al., 2016). While such methods help obtain stateof-the-art results in neural MT Gehring et al., 2017;, we view them as a patch to the observed problems. The fact that text quality still degrades with increased beam sizes when these rewards are used (Koehn and Knowles, 2017;Ott et al., 2018a) suggests that they do not address the inherent issues with text generation systems. We subsequently hypothesize about the nature of these issues and provide a set of linguistically motivated regularizers-inspired by beam search-that appear to alleviate them.", "publication_ref": ["b5", "b49", "b40", "b19", "b46", "b30", "b0", "b46", "b10", "b23", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "Deriving Beam Search", "text": "We introduce a regularized decoding framework. The idea is simple; we seek to solve the regularized optimization problem to decode\ny = argmax y\u2208Y log p \u03b8 (y | x) \u2212 \u03bb \u2022 R(y) (9)\nfor a strategically chosen R(\u2022). Clearly, for certain R(\u2022), we recover the decoding objectives discussed in \u00a72.2. The question we ask in this work is the following: If we want to view beam search as an exact-decoding algorithm, which R(\u2022) should we choose to recover beam search?\nWe discovered an elegant answer rooted in information theory and cognitive science (the connections are discussed in-depth in \u00a74). We first define the model's time-dependent surprisals, which are an information-theoretic concept that characterizes the amount of new information expressed at time t:\nu 0 (BOS) = 0 u t (y) = \u2212 log p \u03b8 (y | x, y <t ), for t \u2265 1 (10)\nNote that minimally surprising means maximally probable. For the special case of greedy decoding, where k = 1, the following choice of regularizer recovers beam search for sufficiently large \u03bb:\nR greedy (y) = |y| t=1 u t (y t ) \u2212 min y \u2208V u t (y ) 2 (11)\nThe intuition behind Eq. ( 11) is to encourage locally optimal decisions: Every local surprise u t should be close to the minimally surprising choice. In the limiting case where locally optimal decisions are not just encouraged, but rather enforced, we recover greedy search.\nFormally, we have the following theorem:\nTheorem 3.1. The argmax of log p \u03b8 (y | x) \u2212 \u03bb \u2022 R greedy (y) is exactly computed by greedy search in the limiting case as \u03bb \u2192 \u221e.\nProof. By induction. In App. A.\nTheorem 3.1 establishes that greedy search is the limiting case of a regularizer that seeks to encourage decisions to have high-probability locally. In contrast, the optimal MAP solution will generally not have this property. This is because a globally optimal MAP decoder may require a locally suboptimal decision for the sake of being able to make a compensatory decision later that leads to global optimality. 6 We now consider the generalization of greedy search (k = 1) to full beam search (k \u2265 1). Recall that beam search returns not just a single output, but rather a set of outputs. Thus, we must consider the set-decoding objective\nY = argmax Y \u2286Y, |Y |=k log p \u03b8 (Y | x) \u2212 \u03bb \u2022 R(Y ) (12)\nwhere, as before, we have used our overloaded notation p \u03b8 (\u2022 | x) to score sets of hypotheses. Similarly to R greedy , we formulate a greedy set-regularizer to recover beam search:\nR beam (Y ) = (13\n) nmax t=1 \uf8eb \uf8ec \uf8edut(Yt) \u2212 min Y \u2286Bt, |Y |=k u t (Y ) \uf8f6 \uf8f7 \uf8f8 2\nwhere Y t = {y 1:t | y \u2208 Y } corresponds to the set of hypotheses expanded by t steps. 7 Note that we additionally overload surprisal to operate on sets, u t (Y ) = y\u2208Y u t (y). We prove an analogous theorem to Theorem 3.1 for this regularizer.\nTheorem 3.2. The argmax of log p \u03b8 (Y | x) \u2212 \u03bb \u2022 R(Y ) is computed by beam search with beam size of k = |Y | as \u03bb \u2192 \u221e.\nProof. The proof follows from the same argument as Theorem 3.1, albeit with sets instead of an individual hypothesis.\nNote that in the (predominant) case where we want to return a single candidate sentence as the output rather than an entire set-as would be generated by Eq. (12)-we can take the highest-probability sequence in the chosen set Y as our decoded output. The objective in Eq. ( 12) boils down to a subset selection problem which, given the size of Y, is a computationally prohibitive optimization problem. Nonetheless, we can use it to analyze the properties enforced on generated text by beam search.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "From Beam Search to UID", "text": "The theoretical crux of this paper hinges on a proposed relationship between beam search and the uniform information density hypothesis (Levy, 2005;Levy and Jaeger, 2007), a concept from cognitive science: Hypothesis 4.1. \"Within the bounds defined by grammar, speakers prefer utterances that distribute information uniformly across the signal (information density). Where speakers have a choice between several variants to encode their message, they prefer the variant with more uniform information density (ceteris paribus)\" (Jaeger, 2010).\nAt its core, the theory seeks to explain various aspects of human language processing in terms of information theory; it is often applied to an area of psycholinguistics known as sentence processing where the UID hypothesis is used to explain experimental data (Hale, 2001). As the UID hypothesis concerns a cognitive process (virtually) independent of the language in use, the theory should hold across languages (Jaeger and Tily, 2011).\nTo see the hypothesis in action, consider the classic case of syntactic reduction from Levy and Jaeger (2007):\n(1) How big is [ NP the family i [ RC (that) you cook for \u2212i ]]?\nIn the above example, the sentence does not require the relativizer that at the start of the relative clause (denoted by RC); it would also be syntactically correct without it. However, many would agree that the relativizer makes the text qualitatively better. The information-theoretic explanation of this perception is that without the relativizer, the first word of a relative clause conveys two pieces of information simultaneously: the onset of a relative clause and part of its internal contents. Including the relativizer spreads this information across two words, thereby distributing information across the sentence more uniformly and avoiding instances of high surprisal-which, from a psycholinguistic perspective, are displeasing. In short, the relativizer helps to ensure the UID property of the sentence. Importantly, the preference suggested by the UID hypothesis is between possible utterances (i.e., outputs) where grammaticality and information content are held constant. Any violation of these assumptions presents confounding factors when measuring, or optimizing, the information density of the generated text. In our setting, there is reason to believe that grammaticallity and information content are approximately held constant while selecting between hypothesis. First, the high-probability outputs of neural generation models tend to be grammatical (Holtzman et al., 2020). Second, because decoding is conditioned on a specific input x, the conditional probability model p \u03b8 (y | x) is able to assign high-probability to outputs y that are plausible outputs (e.g., translations) of the given x. Thus, even though the various y are not constrained to be sematically equivalent to one another, they tend to express similar information because they are at least relevant to the same x. This is why our regularized optimization problem Eq. ( 9) combines an information-density regularizer with log p \u03b8 (y | x): the term log p \u03b8 (y | x) rewards grammaticallity and content relevance, whereas the information-density regularizer encourages the human preferences posited by the UID hypothesis. The parameter \u03bb allows the preferences to be calibrated to perform well on downstream evaluation metrics, such as BLEU and ROUGE.", "publication_ref": ["b26", "b27", "b16", "b12", "b17", "b27", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "The UID Bias in Beam Search", "text": "It may not be immediately obvious how the UID hypothesis relates to beam search. After all, beam search narrows the scope of the search to only the lowest surprisal candidates at each time step, which does not clearly lead to a uniform distribution of surprisals in the final decoded sequences. The connection is best seen visually. Fig. 2 shows the time-dependent surprisals u t under the model of several candidate translations (German to English). Recall that we have u t (y) \u2208 [0, \u221e) and that the standard decoding objective explicitly minimizes the sum of surprisals, i.e., maximizes log-probability. Therefore, the only way the distribution of a solution can become distinctly nonuniform is when there are several high-surprisal decisions in the mix; we observe this in the orange and red curves. Intuitively, this corresponds to the notion of compensation discussed earlier: a globally optimal decoding scheme may select a high-surprisal step at some point in order to shorten the length of the path or to take a low-surprisal step later on. We observe an extreme example of this behavior above: Selecting the EOS character at the first step leads to a very non-uniform distribution, i.e., the degenerate distribution, which, violates our operationalization of UID described subsequently. In summary, we see that as \u03bb is decreased, the decoded sentences obey the UID property less strictly. Indeed, setting \u03bb = 0, i.e., exact inference of the MAP objective, results in the empty string.\nA number of successful sampling methods (pnucleus sampling (Holtzman et al., 2020) and topk sampling (Fan et al., 2018)) enforce the UID property in generated text by the same logic as above. Both methods eliminate many of the highsurprisal choices at any given decoding step by narrowing the set of tokens that may be chosen.", "publication_ref": ["b14", "b9"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Cognitive Motivation for Beam Search", "text": "The goal of this work is to expose a possible inductive bias of beam search. We now exhibit our primary hypothesis Hypothesis 4.2. Beam search is a cognitively motivated search heuristic for decoding language gen-eration models. The success of beam search on such tasks is, in part, due to the fact that it inherently biases the search procedure towards text that humans prefer.\nThe foundation of the argument for this hypothesis follows naturally from the previous sections: First, we demonstrated in \u00a73 that beam search is an exact decoding algorithm for a certain regularized objective-to wit, the one in Eq. (9). Qualitatively, we related the behavior of the regularizer to the UID hypothesis from cognitive science. As a final step, we next provide operationalizations of UID-in the form of regularizers within our regularized decoding framework-through which we can empirically test the validity of this hypothesis.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Generalized UID Decoding", "text": "If beam search is trying to optimize for UID, can we beat it at its own game? This section develops a battery of possible sentence-level UID measures, which can be used as regularizers in our regularized decoding framework and compared experimentally on downstream evaluation metrics.\nVariance Regularizer. We first consider the variance regularizer from Jain et al. (2018). In essence, UID concerns the distribution of information over the course (i.e., time steps) of a sentence. A natural measure for this is variance of the surprisals.\nR var (y) = 1 |y| |y| t=1 u t (y t ) \u2212 \u00b5 2 (14\n)\nwhere \u00b5 = 1 /|y| |y| t=1 u t (y t ). This regularizer, in contrast to Eq. (11), is a much more straightforward encoding of the UID: it directly operationalizes UID through variance.\nLocal Consistency. Next we consider a local consistency regularizer, also taken from Jain et al. (2018), that encourages adjacent surprisals to have similar magnitude:\nR local (y) = 1 |y| |y| t=1 u t (y t ) \u2212 u t\u22121 (y t\u22121 ) 2 (15)\nAgain, this is a straightforward encoding of the UID: if every surprisal is similar to its neighbor, it will be close to uniform. Note that both of the above regularizers are defined for all decoding steps t > 0 since we define u 0 (y 0 ) = 0, y 0 = BOS for all valid hypotheses.\nMax Regularizer. We propose a UID-inspired regularizer of our own design that exploits the nature of MAP decoding, for which the overarching goal is to find a solution with low surprisal. In this setting, one strategy is to penalize decisions that move the distribution away from 0, the lowest possible surprisal. This suggests\nR max (y) = |y| max t=1 u t (y t )(16)\nwould regularize for UID. Such a regularizer would also directly penalize extreme compensation during decoding (discussed in \u00a73). It is worth noting that this regularizer has a connection to entropy regularization, which can be seen by looking at the formula for R\u00e9nyi entropy.\nSquared Regularizer. Finally, we consider a novel squared penalty, that, again, exploits the goal of MAP decoding. If we wish to keep everything uniform, we can try to push all surprisals close to 0, but this time with a squared penalty:\nR square (y) = |y| t=1 u t (y t ) 2(17)\nExperimentally, we expect to see the following: If encouraging decoded text to exhibit UID is helpful-and our logic in constructing regularizers is sound-all the regularizers (Eq. ( 14) to ( 17)) should lead to roughly the same performance under exact decoding and beam search with large beam widths. Such results would not only validate the connection between UID and high-quality text; comparable performance of optimal beam search 8 and exact search under our regularized objective would provide explicit evidence for our declarative explanation of the inductive bias in beam search.", "publication_ref": ["b18", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We explore how encouraging uniform information density in text generated by neural probabalistic text generators affects its downstream quality. To this end, we decode NMT models using the regularized objective (Eq. ( 9)) with our UID regularizers. We perform exact decoding for a range of \u03bb and observe how text quality (quantified by BLEU (Papineni et al., 2002) using the SacreBLEU (Post, 2018) system) and the distribution of surprisal changes. We additionally evaluate our regularizers under the beam search decoding strategy to see if penalizing violations of UID alleviates the text-quality degradation typically seen with increased beam widths. Experiments are performed using models trained on the IWSLT'14 De-En (Cettolo et al., 2012) and WMT'14 En-Fr (Bojar et al., 2014) datasets. For reproducibility, we use the model provided by fairseq  for the WMT'14 task; 9 we use the data pre-processing scripts and recommended hyperparameter settings provided by fairseq for training a model on the IWSLT'14 De-En dataset. We use the Newstest'14 dataset as the test set for the WMT'14 model. All model and data information can be found on the fairseq NMT repository. 10", "publication_ref": ["b36", "b37", "b3", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Exact Decoding", "text": "To perform exact decoding of neural probabilistic text generators, we build on the decoding framework of Stahlberg et al. (2017), albeit using Dijkstra's algorithm (Dijkstra, 1959) instead of depthfirst search as we find it decreases decoding time. Note that Dijkstra's algorithm is guaranteed to find the global optimum when path cost is monotoni-Figure 3: BLEU as a function of beam width for various regularizers. We choose \u03bb for each regularizer by best performance on validation sets (see App. B). y-scales are broken to show minimum BLEU values. x-axis is log-scaled.\ncally increasing, which is the case for hypotheses under the scoring scheme used by neural probabilistic text generators (see Meister et al. (2020) for more detailed discussion). While the variance and local consistency regularizers Eq. ( 14) and ( 15) break this monotonicity property, we can still guarantee optimality by using a stopping criterion similar to the one proposed by Yang et al. (2018). Explicitly, we check if the top-scoring complete hypothesis has a greater score than the maximum possible score of any hypothesis in the queue. All scores are bounded due to the maximum-length criterion. Additionally, we lower-bound each search by the score of the empty string to decrease the memory footprint, i.e., we stop considering hypotheses whose scores (or maximum possible score in the case of Eq. ( 14) and ( 15)) drop below that of the empty string at any time step.\nFig. 1 demonstrates how the addition of the greedy UID regularizer (Eq. ( 11) ) to the regularized MAP objective (Eq. ( 9)) affects characteristics of the global optimum under the model as we vary \u03bb. Notably, increasing the strength of the regularizer appears to alleviate the text quality degradation seen with exact search, leading to results that approach the BLEU of those generated using optimal beam search. Fig. 1 also shows a strong inverse relationship between BLEU and average standard deviation (per sentence) of surprisals. We take these observations as empirical validation of Hyp. 4.2.", "publication_ref": ["b43", "b7", "b29", "b52"], "figure_ref": [], "table_ref": []}, {"heading": "Regularized Beam Search", "text": "We next look at how the regularized decoding objective affects text generated using beam search. As previously noted, text quality generally degrades with increased beam size when using the standard MAP objective; this phenomenon is demonstrated in Fig. 3. UID regularization appears to alleviate k = 5 k = 10 k = 100 k = 500  this problem. Notably, the greedy and squared regularizer aid performance for larger beam sizes more so than other regularizers, for which we still see a slight drop in performance for larger beam sizes. This drop is negligible compared to the one observed for unregularized beam search-a drop which is also frequently observed for lengthnormalized decoding (Koehn and Knowles, 2017). While intuitively, variance and local variance are the purest encodings of UID, they perform the poorest of the regularizers. Arguably, this may be due to the fact that they do not simultaneously (as the other regularizers do) penalize for high surprisal.\nWe additionally decode with a combination of the UID regularizers in tandem. We collectively tune the \u03bb value for each of the regularizers on validation sets. We report performance in Tab. 1 and see that results outperform standard and lengthnormalized, i.e. score divided by sequence length, beam search with noticeable improvements for larger beams. Search details and parameter settings may be found in App. B. Notably, combining multiple UID regularizers does not lead to as great an increase in performance as one might expect, which hints that a single method for enforcing UID is sufficient for promoting quality in generated text.", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Neural probabilistic text generators are far from perfect; prior work has shown that they often generate text that is generic (Vinyals and Le, 2015;, unnatural (Holtzman et al., 2020), and sometimes even non-existent (Stahlberg and Byrne, 2019). In the context of the degenerate behavior of these models, the beam search curse-a specific phenomenon where using a larger beam size leads to worse performance-has been analyzed by a number of authors (Koehn and Knowles, 2017;Murray and Chiang, 2018;Yang et al., 2018;Stahlberg and Byrne, 2019;Jean et al., 2015;Tu et al., 2016;Cohen and Beck, 2019). Many of these authors attribute the performance drop (as search becomes better) to an inherent bias in neural sequence models to pefer shorter sentences. Other authors have ascribed fault to the model architectures, or how they are trained (Cho et al., 2014;Sountsov and Sarawagi, 2016;Vinyals et al., 2017;Ott et al., 2018a;Kumar and Sarawagi, 2019). To remedy the problem, a large number of regularized decoding objectives and modified training techniques have been proposed. In contrast, this work analyzes the behavior of neural text generators from a different angle: We provide a plausible answer-inspired by psycholinguistic theory-as to why beam search (with small beams) leads to high-quality text, rather than another explanation of why exact search performs so badly.", "publication_ref": ["b49", "b14", "b42", "b23", "b30", "b52", "b42", "b19", "b46", "b6", "b5", "b41", "b50", "b33", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We analyze beam search as a decoding strategy for text generation models by framing it as the solution to an exact decoding problem. We hypothesize that beam search has an inductive bias which can be linked to the promotion of uniform information density (UID), a theory from cognitive science regarding even distribution of information in linguistic signals. We observe a strong relationship between variance of surprisals (an operationalization of UID) and BLEU in our experiments with NMT models. With the aim of further exploring decoding strategies for neural text generators in the context of UID, we design a set of objectives to explicitly encourage uniform information density in text generated from neural probabalistic models and find that they alleviate the quality degradation typically seen with increased beam widths.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Theory", "text": "Proof. We prove Theorem 3.2 by induction. We denote the argmax of log p \u03b8 (y | x) \u2212 \u03bb \u2022 R greedy (y) as y R and the solution found by greedy search as y greedy . We will show that y greedy t = y R t for all 0 \u2264 t \u2264 max(|y R |, |y greedy |). The theorem holds trivially for the base case of t = 0 because y 0 must be BOS for any valid hypothesis by definition of the hypothesis space (Eq. (1)). Now, by the inductive hypothesis, suppose y greedy i = y R i for all i < t. We will show that our regularized objective must choose the same word as greedy search at time-step t. In the limiting case of Eq. (11), the following function reflects the penalty to the distribution over tokens at position t:\nlim \u03bb\u2192\u221e \u03bb \u2022 u t (y t ) \u2212 min y \u2208V u t (y ) 2 =\n0 if u t (y t ) = min y \u2208V u t (y ) \u221e otherwise Since minimum surprisal implies maximum log-probability, the above function clearly returns either 0 or \u221e depending on whether the decoding choice at time-step t is greedy. Therefore the only choice that would not send the hypothesis score to \u2212\u221e is the greedy choice, which implies any feasible solution to our objective must have y R t = y greedy t . By the principle of induction, y greedy t = y R t for all 0 \u2264 t \u2264 |y R | = |y greedy |, which in turn implies y greedy = y R .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Parameters", "text": "For values in Fig. 3, we perform grid search over \u03bb \u2208 [0.2, 0.5, 0.7, 1, 2, 3,4,6,7,8,9,10] and choose the \u03bb with the best validation set performance. For combined UID regularization, we perform hyperparameter search over the 5 strength parameters, each sampled uniformly from the following values: [0, 0.2, 0.5, 0.7, 1, 2, 3,4,6,7,8,9,10]. We run 50 trials on the validation set; \u03bb = 5 and \u03bb = 2 yield the best performance for the greedy and squared regularizers, respectively with all others \u03bb set to 0.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Additional Plots", "text": "Figure 4: BLEU vs. std. deviation of surprisals for translations generated with beam search on test sets of IWSLT'14 and WMT'14. Size of point indicates beam width used (between 5 and 100). In contrast to the subgraph of Fig. 1, the x-axis is not log-scaled.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We would like to thank Ari Holtzman and Jason Eisner for useful feedback and discussion that helped improve this work.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Neural machine translation by jointly learning to align and translate", "journal": "", "year": "2015", "authors": "Dzmitry Bahdanau; Kyunghyun Cho; Yoshua Bengio"}, {"ref_id": "b1", "title": "Scheduled sampling for sequence prediction with recurrent neural networks", "journal": "", "year": "2015", "authors": "Samy Bengio; Oriol Vinyals; Navdeep Jaitly; Noam Shazeer"}, {"ref_id": "b2", "title": "Findings of the 2014 workshop on statistical machine translation", "journal": "", "year": "2014", "authors": "Ond\u0159ej Bojar; Christian Buck; Christian Federmann; Barry Haddow; Philipp Koehn; Johannes Leveling; Christof Monz; Pavel Pecina; Matt Post; Herve Saint-Amand; Radu Soricut; Lucia Specia; Ale\u0161 Tamchyna"}, {"ref_id": "b3", "title": "Wit 3 : Web inventory of transcribed and translated talks", "journal": "", "year": "2012", "authors": "Mauro Cettolo; Christian Girardi; Marcello Federico"}, {"ref_id": "b4", "title": "Recurrent neural networks as weighted language recognizers", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Yining Chen; Sorcha Gilroy; Andreas Maletti; Jonathan May; Kevin Knight"}, {"ref_id": "b5", "title": "On the properties of neural machine translation: Encoder-decoder approaches", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Kyunghyun Cho; Bart Van Merri\u00ebnboer; Dzmitry Bahdanau; Yoshua Bengio"}, {"ref_id": "b6", "title": "Empirical analysis of beam search performance degradation in neural sequence models", "journal": "", "year": "2019", "authors": "Eldan Cohen; Christopher Beck"}, {"ref_id": "b7", "title": "A note on two problems in connexion with graphs", "journal": "Numerische Mathematik", "year": "1959", "authors": "W Edsger;  Dijkstra"}, {"ref_id": "b8", "title": "Understanding back-translation at scale", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Sergey Edunov; Myle Ott; Michael Auli; David Grangier"}, {"ref_id": "b9", "title": "Hierarchical neural story generation", "journal": "", "year": "2018", "authors": "Angela Fan; Mike Lewis; Yann Dauphin"}, {"ref_id": "b10", "title": "Convolutional sequence to sequence learning", "journal": "", "year": "2017", "authors": "Jonas Gehring; Michael Auli; David Grangier; Denis Yarats; Yann N Dauphin"}, {"ref_id": "b11", "title": "On calibration of modern neural networks", "journal": "", "year": "2017", "authors": "Chuan Guo; Geoff Pleiss; Yu Sun; Kilian Q Weinberger"}, {"ref_id": "b12", "title": "A probabilistic Earley parser as a psycholinguistic model", "journal": "", "year": "2001", "authors": "John Hale"}, {"ref_id": "b13", "title": "Improved neural machine translation with smt features", "journal": "AAAI Press", "year": "2016", "authors": "Wei He; Zhongjun He; Hua Wu; Haifeng Wang"}, {"ref_id": "b14", "title": "The curious case of neural text degeneration", "journal": "", "year": "2020", "authors": "Ari Holtzman; Jan Buys; Maxwell Forbes; Yejin Choi"}, {"ref_id": "b15", "title": "When to finish? optimal beam search for neural text generation (modulo beam size)", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Liang Huang; Kai Zhao; Mingbo Ma"}, {"ref_id": "b16", "title": "Redundancy and reduction: Speakers manage syntactic information density", "journal": "Cognitive Psychology", "year": "2010", "authors": "T ; Florian Jaeger"}, {"ref_id": "b17", "title": "On language 'utility': Processing complexity and communicative efficiency", "journal": "", "year": "2011", "authors": "T ; Florian Jaeger; Harry Tily"}, {"ref_id": "b18", "title": "Uniform information density effects on syntactic choice in Hindi", "journal": "", "year": "2018", "authors": "Ayush Jain; Vishal Singh; Sidharth Ranjan; Rajakrishnan Rajkumar; Sumeet Agarwal"}, {"ref_id": "b19", "title": "Montreal neural machine translation systems for WMT'15", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "S\u00e9bastien Jean; Orhan Firat; Kyunghyun Cho; Roland Memisevic; Yoshua Bengio"}, {"ref_id": "b20", "title": "Recurrent convolutional neural networks for discourse compositionality", "journal": "Association for Computational Linguistics", "year": "2013", "authors": "Nal Kalchbrenner; Phil Blunsom"}, {"ref_id": "b21", "title": "Algorithm Design", "journal": "Addison-Wesley Longman Publishing Co., Inc", "year": "2005", "authors": "Jon Kleinberg;  Tardos"}, {"ref_id": "b22", "title": "Pharaoh: A beam search decoder for phrase-based statistical machine translation models", "journal": "Springer Berlin Heidelberg", "year": "2004", "authors": "Philipp Koehn"}, {"ref_id": "b23", "title": "Six challenges for neural machine translation", "journal": "Vancouver. Association for Computational Linguistics", "year": "2017", "authors": "Philipp Koehn; Rebecca Knowles"}, {"ref_id": "b24", "title": "Statistical phrase-based translation", "journal": "", "year": "2003", "authors": "Philipp Koehn; Franz J Och; Daniel Marcu"}, {"ref_id": "b25", "title": "Calibration of encoder decoder models for neural machine translation", "journal": "CoRR", "year": "2019", "authors": "Aviral Kumar; Sunita Sarawagi"}, {"ref_id": "b26", "title": "Probabilistic Models of Word Order and Syntactic Discontinuity", "journal": "", "year": "2005", "authors": "Roger Levy"}, {"ref_id": "b27", "title": "Speakers optimize information density through syntactic reduction", "journal": "MIT Press", "year": "2007", "authors": "P Roger; T F Levy;  Jaeger"}, {"ref_id": "b28", "title": "A diversity-promoting objective function for neural conversation models", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Jiwei Li; Michel Galley; Chris Brockett; Jianfeng Gao; Bill Dolan"}, {"ref_id": "b29", "title": "Best-first beam search. Transactions of the Association for Computational Linguistics", "journal": "", "year": "2020", "authors": "Clara Meister; Tim Vieira; Ryan Cotterell"}, {"ref_id": "b30", "title": "Correcting length bias in neural machine translation", "journal": "", "year": "2018", "authors": "Kenton Murray; David Chiang"}, {"ref_id": "b31", "title": "Facebook FAIR's WMT19 news translation task submission", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Nathan Ng; Kyra Yee; Alexei Baevski; Myle Ott; Michael Auli; Sergey Edunov"}, {"ref_id": "b32", "title": "Improved alignment models for statistical machine translation", "journal": "", "year": "1999", "authors": "Franz Josef Och; Christoph Tillmann; Hermann Ney"}, {"ref_id": "b33", "title": "Analyzing uncertainty in neural machine translation", "journal": "", "year": "2018", "authors": "Myle Ott; Michael Auli; David Grangier; Marc'aurelio Ranzato"}, {"ref_id": "b34", "title": "fairseq: A fast, extensible toolkit for sequence modeling", "journal": "", "year": "2019", "authors": "Myle Ott; Sergey Edunov; Alexei Baevski; Angela Fan; Sam Gross; Nathan Ng; David Grangier; Michael Auli"}, {"ref_id": "b35", "title": "Scaling neural machine translation", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Myle Ott; Sergey Edunov; David Grangier; Michael Auli"}, {"ref_id": "b36", "title": "BLEU: A method for automatic evaluation of machine translation", "journal": "", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"ref_id": "b37", "title": "A call for clarity in reporting BLEU scores", "journal": "", "year": "2018", "authors": "Matt Post"}, {"ref_id": "b38", "title": "Speech understanding systems: A summary of results of the five-year research effort at carnegie mellon university", "journal": "", "year": "1977", "authors": "Raj Reddy"}, {"ref_id": "b39", "title": "Multiresolution recurrent neural networks: An application to dialogue response generation", "journal": "", "year": "2017", "authors": "Iulian Serban; Tim Klinger; Gerald Tesauro; Kartik Talamadupula; Bowen Zhou; Yoshua Bengio; Aaron Courville"}, {"ref_id": "b40", "title": "Minimum risk training for neural machine translation", "journal": "Long Papers", "year": "2016", "authors": "Shiqi Shen; Yong Cheng; Zhongjun He; Wei He; Hua Wu; Maosong Sun; Yang Liu"}, {"ref_id": "b41", "title": "Length bias in encoder decoder models and a case for global conditioning", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Pavel Sountsov; Sunita Sarawagi"}, {"ref_id": "b42", "title": "On NMT Search Errors and Model Errors: Cat Got Your Tongue?", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Felix Stahlberg; Bill Byrne"}, {"ref_id": "b43", "title": "SGNMT -a flexible NMT decoding platform for quick prototyping of new models and search strategies", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Felix Stahlberg; Eva Hasler; Danielle Saunders; Bill Byrne"}, {"ref_id": "b44", "title": "Sequence to sequence learning with neural networks", "journal": "", "year": "2014", "authors": "Ilya Sutskever; Oriol Vinyals; V Quoc;  Le"}, {"ref_id": "b45", "title": "Advances in Neural Information Processing Systems", "journal": "Curran Associates, Inc", "year": "", "authors": "K Q Lawrence;  Weinberger"}, {"ref_id": "b46", "title": "Modeling coverage for neural machine translation", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Zhaopeng Tu; Zhengdong Lu; Yang Liu; Xiaohua Liu; Hang Li"}, {"ref_id": "b47", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b48", "title": "Diverse beam search for improved description of complex scenes", "journal": "AAAI Press", "year": "2018", "authors": "K Ashwin; Michael Vijayakumar; Ramprasaath R Cogswell; Qing Selvaraju; Stefan Sun; David J Lee; Dhruv Crandall;  Batra"}, {"ref_id": "b49", "title": "A neural conversational model", "journal": "", "year": "2015", "authors": "Oriol Vinyals; V Quoc;  Le"}, {"ref_id": "b50", "title": "Show and tell: Lessons learned from the 2015 mscoco image captioning challenge", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2017", "authors": "Oriol Vinyals; Alexander Toshev; Samy Bengio; Dumitru Erhan"}, {"ref_id": "b51", "title": "Google's neural machine translation system: Bridging the gap between human and machine translation", "journal": "CoRR", "year": "2016", "authors": "Yonghui Wu; Mike Schuster; Zhifeng Chen; Quoc V Le; Mohammad Norouzi; Wolfgang Macherey; Maxim Krikun; Yuan Cao; Qin Gao; Klaus Macherey; Jeff Klingner; Apurva Shah; Melvin Johnson; Xiaobing Liu; Lukasz Kaiser; Stephan Gouws; Yoshikiyo Kato; Taku Kudo; Hideto Kazawa; Keith Stevens; George Kurian; Nishant Patil; Wei Wang"}, {"ref_id": "b52", "title": "Breaking the beam search curse: A study of (re-)scoring methods and stopping criteria for neural machine translation", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Yilin Yang; Liang Huang; Mingbo Ma"}, {"ref_id": "b53", "title": "Xlnet: Generalized autoregressive pretraining for language understanding", "journal": "Curran Associates, Inc", "year": "2019", "authors": "Zhilin Yang; Zihang Dai; Yiming Yang; Jaime Carbonell; R Russ; Quoc V Salakhutdinov;  Le"}, {"ref_id": "b54", "title": "Neural generative question answering", "journal": "", "year": "2016", "authors": "Jun Yin; Xin Jiang; Zhengdong Lu; Lifeng Shang; Hang Li; Xiaoming Li"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Surprisals (according to p \u03b8 ) by time step of sequences generated with various decoding strategies. Values of \u03bb indicate the greedy regularizer was used with the corresponding \u03bb value. Note that beam search (k=5) and exact search (\u03bb = 1.0) return the same prediction in this example, and thus, are represented by the same line.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ": BLEU scores on first 1000 samples of New-stest2014 for predictions generated with various decod-ing strategies. Best scores per beam size are bolded."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "\u03bb settings used during decoding in Fig.3and reported in table Tab. 1.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "Y := {BOS \u2022 v \u2022 EOS | v \u2208 V * } (1)", "formula_coordinates": [2.0, 345.26, 321.75, 180.28, 12.3]}, {"formula_id": "formula_1", "formula_text": "p \u03b8 (y | x) = |y| t=1 p \u03b8 (y t | x, y <t ) (2)", "formula_coordinates": [2.0, 347.42, 449.8, 178.12, 34.6]}, {"formula_id": "formula_2", "formula_text": "y = argmax y\u2208Y log p \u03b8 (y | x)", "formula_coordinates": [2.0, 356.48, 587.44, 119.86, 18.41]}, {"formula_id": "formula_3", "formula_text": "Y 0 = {BOS} (4) Y t = argmax Y \u2286Bt, |Y |=k log p \u03b8 (Y | x)(5)", "formula_coordinates": [3.0, 117.69, 251.21, 172.58, 44.37]}, {"formula_id": "formula_4", "formula_text": "B t = y t 1 \u2022 y | y \u2208V and y t 1 \u2208 Y t 1 (6)", "formula_coordinates": [3.0, 83.34, 331.05, 206.93, 10.71]}, {"formula_id": "formula_5", "formula_text": "log p \u03b8 (y | x) + \u03bb|y| (7) log p \u03b8 (y | x)+\u03bb |x| i=1 log min \uf8eb \uf8ed 1, |y| j=1 \u03b1 ij \uf8f6 \uf8f8 (8)", "formula_coordinates": [3.0, 317.94, 543.45, 207.6, 52.53]}, {"formula_id": "formula_6", "formula_text": "y = argmax y\u2208Y log p \u03b8 (y | x) \u2212 \u03bb \u2022 R(y) (9)", "formula_coordinates": [4.0, 81.37, 315.11, 208.9, 18.42]}, {"formula_id": "formula_7", "formula_text": "u 0 (BOS) = 0 u t (y) = \u2212 log p \u03b8 (y | x, y <t ), for t \u2265 1 (10)", "formula_coordinates": [4.0, 77.67, 521.76, 212.6, 27.17]}, {"formula_id": "formula_8", "formula_text": "R greedy (y) = |y| t=1 u t (y t ) \u2212 min y \u2208V u t (y ) 2 (11)", "formula_coordinates": [4.0, 74.25, 627.61, 216.02, 34.6]}, {"formula_id": "formula_9", "formula_text": "Y = argmax Y \u2286Y, |Y |=k log p \u03b8 (Y | x) \u2212 \u03bb \u2022 R(Y ) (12)", "formula_coordinates": [4.0, 312.73, 351.27, 212.81, 27.02]}, {"formula_id": "formula_10", "formula_text": "R beam (Y ) = (13", "formula_coordinates": [4.0, 330.78, 461.08, 190.22, 10.77]}, {"formula_id": "formula_11", "formula_text": ") nmax t=1 \uf8eb \uf8ec \uf8edut(Yt) \u2212 min Y \u2286Bt, |Y |=k u t (Y ) \uf8f6 \uf8f7 \uf8f8 2", "formula_coordinates": [4.0, 358.64, 461.43, 166.9, 61.03]}, {"formula_id": "formula_12", "formula_text": "Theorem 3.2. The argmax of log p \u03b8 (Y | x) \u2212 \u03bb \u2022 R(Y ) is computed by beam search with beam size of k = |Y | as \u03bb \u2192 \u221e.", "formula_coordinates": [4.0, 306.92, 612.18, 218.63, 36.74]}, {"formula_id": "formula_13", "formula_text": "R var (y) = 1 |y| |y| t=1 u t (y t ) \u2212 \u00b5 2 (14", "formula_coordinates": [7.0, 96.75, 86.96, 188.98, 34.6]}, {"formula_id": "formula_14", "formula_text": ")", "formula_coordinates": [7.0, 285.72, 99.96, 4.54, 9.46]}, {"formula_id": "formula_15", "formula_text": "R local (y) = 1 |y| |y| t=1 u t (y t ) \u2212 u t\u22121 (y t\u22121 ) 2 (15)", "formula_coordinates": [7.0, 81.88, 256.98, 208.38, 46.99]}, {"formula_id": "formula_16", "formula_text": "R max (y) = |y| max t=1 u t (y t )(16)", "formula_coordinates": [7.0, 128.81, 505.0, 161.45, 21.85]}, {"formula_id": "formula_17", "formula_text": "R square (y) = |y| t=1 u t (y t ) 2(17)", "formula_coordinates": [7.0, 124.69, 698.63, 165.57, 34.6]}, {"formula_id": "formula_18", "formula_text": "lim \u03bb\u2192\u221e \u03bb \u2022 u t (y t ) \u2212 min y \u2208V u t (y ) 2 =", "formula_coordinates": [13.0, 140.1, 196.17, 165.38, 23.79]}], "doi": "10.18653/v1/N18-1205"}