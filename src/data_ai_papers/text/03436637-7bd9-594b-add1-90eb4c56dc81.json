{"title": "AlphaHoldem: High-Performance Artificial Intelligence for Heads-Up No-Limit Poker via End-to-End Reinforcement Learning", "authors": "Enmin Zhao; Renye Yan; Jinqiu Li; Kai Li; Junliang Xing", "pub_date": "", "abstract": "Heads-up no-limit Texas hold'em (HUNL) is the quintessential game with imperfect information. Representative prior works like DeepStack and Libratus heavily rely on counterfactual regret minimization (CFR) and its variants to tackle HUNL. However, the prohibitive computation cost of CFR iteration makes it difficult for subsequent researchers to learn the CFR model in HUNL and apply it in other practical applications. In this work, we present AlphaHoldem, a highperformance and lightweight HUNL AI obtained with an endto-end self-play reinforcement learning framework. The proposed framework adopts a pseudo-siamese architecture to directly learn from the input state information to the output actions by competing the learned model with its different historical versions. The main technical contributions include a novel state representation of card and betting information, a multi-task self-play training loss function, and a new model evaluation and selection metric to generate the final model. In a study involving 100,000 hands of poker, AlphaHoldem defeats Slumbot and DeepStack using only one PC with three days training. At the same time, AlphaHoldem only takes 2.9 milliseconds for each decision-making using only a single GPU, more than 1,000 times faster than DeepStack. We release the history data among among AlphaHoldem, Slumbot, and top human professionals in the author's GitHub repository to facilitate further studies in this direction.", "sections": [{"heading": "Introduction", "text": "Poker is a typical imperfect information game (IIG) that has a long history as a challenging problem for developing Artificial Intelligence (AI) that can address hidden information (Waterman 1970). Among different poker games, Heads-up no-limit Texas hold'em (HUNL) is a two-player poker game in which two cards are initially dealt face-down to each player, and additional cards are dealt with face-up in three subsequent rounds. No-limit means no restriction on the bet size, although it may be restricted by the total amount wagered in each game. Because of its explicit problem setting with large decision space (\u223c10 161 information sets) and strategic complexity, HUNL has been an excellent benchmark and challenging problem for developing AI al-gorithms for studying the two-player zero-sum games with imperfect information (Bard et al. 2013;Jackson 2013).\nRecently, with the aid of increasing computing resources, computer programs have reached the performance that exceeds expert human players in many games, e.g., Go (Silver et al. 2016), MahJong (Li et al. 2020), DOTA (Berner et al. 2019), and StarCraft (Vinyals et al. 2019). These AI systems collect a tremendous amount of replay samples either from human experts or self-play of the system to train some complex learning models. The adoption of deep neural networks significantly improves the learning ability and performance of these systems. However, the model training process often lasts for dozens of days using thousands of CPU/GPUs, making these models extremely expensive to obtain. According to the reported computing resources used in AlphaGo (Silver et al. 2016), training an AlphaGo model costs about 35 million dollars.\nAs for the HUNL AI, new algorithms are also progressing very fast under the counterfactual regret minimization (CFR) framework (Zinkevich et al. 2007). Deep-Stack (Moravcik et al. 2017) and Libratus (Brown and Sandholm 2018) are independently developed and demonstrate expert-level performance. Both DeepStack and Libratus compute an abstraction of the game and introduce subgame solving with the CFR+ (Tammelin et al. 2015) algorithm to learn HUNL AIs. Under the CFR framework, the primary computation cost comes from the CFR iteration process performed in both the model training and testing stages. To ensure high-quality prediction, this iteration process often needs to be carried out for more than 1,000 times in practice (Moravcik et al. 2017). This restriction makes the training of a high-performance CFR-based HUNL AI computationally infeasible for most research institutions and prevents the application of the CFR model into larger IIGs.\nThis work aims to develop a high-performance HUNL AI with affordable computation and storage costs for small research institutions and inspire further studies to develop more universal solutions for AI of Texas hold'em and other IIGs. To this end, we propose AlphaHoldem, a HUNL AI trained from an end-to-end reinforcement learning framework rather than the CFR framework that gives birth to most of the current HUNL AIs. We design a pseudo-Siamese architecture in this framework that directly learns from the input game information to produce the action through a single feedforward pass of a neural network. This new architecture eliminates the need for highly computationally intensive CFR iterative inference during training and testing stages. To accelerate the training process of AlphaHoldem, we develop a set of new techniques for efficient learning the AlphaHoldem framework, including game state representations, training loss functions, and model generation strategies.\nIn contrast to previous abstraction-based methods in HUNL AI design, AlphaHoldem does not perform any card information abstractions using human domain knowledge. Instead, it encodes the game information into tensors containing the current and historical poker information. This new multidimensional tensor representation permits efficient learning of the decision model using convolutional networks. As for the learning algorithms, we propose a new loss function in the actor-critic paradigm which significantly improves the model learning speed and stability. To perform better early-stopping and generate a strong HUNL model, we propose a new self-play procedure to simultaneously reduce the training cost and guarantee the model performance. This is achieved by keeping only one agent as the main training objective but maintains a pool of competing agents to play with the main agent to ensure the replay sampling diversity. The proposed new loss function also helps in selecting the competing agents in the pool.\nThe size of the whole AlphaHoldem model is less than 100MB. We finish the training of the AlphaHoldem AI in three days using only one single computing server of 8 GPUs and 64 CPU cores. During inference, AlphaHoldem takes only 2.9 \u00d7 10 \u22123 second for each decision in a NVIDIA TI-TAN V GPU. We evaluate the effectiveness of AlphaHoldem through extensive experimental analyses and comparisons. In a study involving 200,000 hands of poker, AlphaHoldem beats DeepStack and Slumbot with statistical significance by a margin of 16.91 mbb/h and 111.56 mbb/h, respectively. This work makes the following three main contributions:\n\u2022 We present a general and end-to-end self-play reinforcement learning framework to tackle the challenging HUNL problem: inference from state information directly to the final action using only a forward pass of the neural network in each decision point. \u2022 We develop a set of new techniques to speed up the learning process of the AlphaHoldem: a new game state representation without the abstraction of the card information or any human knowledge, a new policy loss function that limits the distribution of policies, and a new self-play procedure that quickly generates the best model. \u2022 We obtain a high-performance HUNL AI AlphaHoldem: it is trained in three days using a single machine and beats the current two best HUNL AI, Slumbot and Deep-Stack, with only 3ms decision time, more than 1,000 times faster than DeepStack.\nWe have released the history data among AlphaHoldem, Slumbot, and top human professionals for research purposes in the author's GitHub repository 1 to facilitate further studies in large-scale IIGs.  ", "publication_ref": ["b30", "b2", "b15", "b26", "b20", "b4", "b29", "b26", "b21", "b8", "b28", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Texas hold'em has long served as the benchmarks for developing IIG algorithms (Rubin and Watson 2011;Bard et al. 2013) CFR is a conceptually simple iterative algorithm that tries to minimize the regrets of both players so that the timeaveraged strategy approach to the Nash equilibrium. Thereafter, CFR-based methods dominate the design of Texas hold'em AI (Lanctot et al. 2009;Jackson 2013;Burch, Johanson, and Bowling 2014). After the Head-up Limit Texas hold'em is solved in 2015 (Bowling et al. 2015), much research effort has focused on No-limit Texas hold'em and recently made milestone progress. DeepStack (Moravcik et al. 2017) adopts a neural network to approximate the tabular CFR and performs recursive reasoning. Libratus (Brown and Sandholm 2018) computes a blueprint for the overall strategy and fixes potential weaknesses identified by the opponents in the blueprint strategy. They are independently developed and both have defeated professional human players in HUNL. Pluribus (Brown and Sandholm 2019b) further applies similar procedure into multiplayer no-limit Texas hold'em and report super-human performance. Despite significant progress, all the milestone Texas hold'em AIs are built upon CFR, which requires costly computation to obtain the counterfactual values and large storage to store the model. In the inference stage, the CFR iteration process also consumes much computation. Besides, these methods only solve an abstracted game employ different kinds of Texas hold'em domain knowledge. This work aims to overcome these limitations of current HUNL AIs and produce a more general solution. Some recent works also make efforts towards this direction. NFSP (Heinrich and Silver 2016) and Poker-CNN (Yakovenko et al. 2016) have approached state-of-the-art performance in limit Texas hold'em. DeepCFR (Brown et al. 2019) further improves the performance by approximates CFR's behavior in the game using deep neural networks and Discounted CFR (Brown and Sandholm 2019a). Inspired by AlphaGo (Silver et al. 2016), ReBel (Brown et al. 2020) combines search and reinforcement learning in HUNL AI. Despite superhuman performance reported, it still needs iterative learning in both the Simple value of the cards with no same value training and inference stages, consuming expensive computations. In Table 1, we compare typical HUNL AIs from different aspects. AlphaHoldem is the first AI that obtains competitive performance in HUNL solely through reinforcement learning. It is also the AI with the lowest training and testing costs without encoding any domain knowledge.\nK A Q J 1 0 Q K J 1 0 9 A A A A 1 0 4 2 7 9 Q A A A K K K A Q J 1 0 A A K 7 Q A A A Q K A A K Q Q K A Q 4 J Strong Weak (b) HUNL cards strength", "publication_ref": ["b22", "b2", "b15", "b11", "b5", "b21", "b8", "b10", "b14", "b31", "b7", "b9", "b26", "b6"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Prerequisites", "text": "Texas Hold'em Rules. Texas hold'em is a repeated game, each of which begins with two cards (hole cards) dealt face down to each player, and then five cards (community cards) dealt face up in three stages. The stages consist of a series of three cards (the flop), later an additional single card (the turn), and a final card (the river). Each player seeks the best five cards from any combination of the five community cards and two hole cards. Players have betting options to check, call, raise, or fold. Rounds of betting take place before the flop is dealt with and after each subsequent deal. The player who has the best hand and has not folded by the end of all betting rounds wins all the money bet for the hand, known as the pot. In HUNL, two players play the game with the bet size restricted only by the total amount wagered in each game. Figure 1(a) illustrates one HUNL game, and Figure 1(b) shows the cards strength. Reinforcement Learning (RL). In self-play, given a fixed opponent, the original two-player HUNL game reduces to a single-player RL problem since the opponent can be regarded as part of the environment. We consider the standard RL formalism, i.e., Markov Decision Process (MDP). An MDP consists of a set of states S = {s 0 , s 1 , s 2 , . . . , s t , . . . },  a set of actions A = {a k } K k=1 , and a reward function r : S \u00d7 A \u2192 R. After executing an action a t \u2208 A at each state s t \u2208 S, the agent will enter a new state s t+1 according to the transition probability model and get a reward r(s t+1 |s t , a t ). The objective of the agent is to maximize the cumulative rewards R = \u221e t=0 \u03b3 t r(s t+1 |s t , a t ), where \u03b3 is the discount factor to favor more recent rewards.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "AlphaHoldem Architecture", "text": "AlphaHoldem aims to remove the expensive computation of CFR iteration in both the training and testing stages of a HUNL AI. It thus pursues an end-to-end learning framework to perform efficient and effective decision-making in IIGs.\nHere end-to-end means that the framework directly accepts the game board information and outputs the actions without encoding handcrafted features as inputs or performing iterative reasoning in the decision process. AlphaHoldem adopts the RL framework to achieve this goal, and the only force to drive the model to learn is the game reward.\nIn HUNL, the game board information includes the current and historical card information and the player action information. The agent chooses from a set of betting actions to play the game and try to win more rewards. To capture the complex relationship among the game board information, the desired betting actions, and the game rewards, Alpha-Holdem designs a pseudo-Siamese architecture equipped with the RL schema to learn the underlying relationships from end to end. We illustrate the end-to-end learning architecture of AlphaHoldem in Figure 2.\nAs shown in Figure 2, the input of the architecture is the game state representations of action and card information, which are respectively sent to the top and bottom streams of the Siamese architecture. Since the action and card representations provide different kinds of information to the learning architecture, we first isolate the parameter-sharing of the Siamese architecture to enable the two ConvNets to learn adaptive feature representations, which are then fused through fully connected layers to produce the desired actions. This design is the reason why we call it pseudo-Siamese architecture. To train this deep architecture, we present a novel Trinal-Clip loss function to update the model parameters using off-policy RL algorithms. We obtain the final model through a new self-play procedure that plays the current model with a pool of its K best historical versions to sample diverse training data from the huge game state space. 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 \u2026 0 0 0 0 0 0 0 0 \u2026 0 0 0 0 Round 4 Action 6 0 0 0 0 \u2026 1 0 0 0 0 0 0 0 \u2026 0 0 0 0\n0 0 0 0 \u2026 1 0 0 0 1 0 1 1 \u2026 1 1 1 0 0 0 1 0 \u2026 0 0 0 0 0 0 0 0 \u2026 0 0 0 0 0 0 0 0 \u2026 1 0 0 0 1 0 1 1 \u2026 1 1 1 1 0 0 0 0 \u2026 1 0 0 0 0 0 0 0 \u2026 0 0 0 0 0 0 0 0 \u2026 1 0 0 0 1 0 1 1 \u2026 1 1 1 1\n0 0 0 0 \u2026 1 0 0 0 1 0 1 1 \u2026 1 1 1 1 p1 p2 sum legal fold checkcall allin", "publication_ref": [], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "Original card information Action information encoding", "text": "Example: Player 1 in the small blind plays an action `bet pot' after getting a hand `AsAc'.\nbet1/2 2pot 3/2 1 3/4 Figure 3: A state representation example when Player 1 in the small blind plays 'bet pot' after getting an hand 'AsAc'.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning Speedup Techniques", "text": "The core to the success of AlphaHoldem depends on a set of training speedup techniques that makes the learning of a superhuman HUNL AI with the current lowest computation and storage costs possible. In the following, we highlight and expatiate three new crucial techniques in speedup training the AlphaHoldem model. We believe these new techniques and underlying principles are helpful to develop general learning algorithms for more IIG AIs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Effective Game State Representation", "text": "The existence of private information and flexibility of bet size cause the HUNL AI learning extremely challenging. Previous CFR-based methods often abstract the cards and bet information into different groups and use their concatenated coding vectors as game state representation to make the iterative reasoning process feasible. The abstracted code vector loses many important game information and may not capture the complex relationship between the game information and optimal decisions. To obtain an effective and suitable feature representation for end-to-end learning from the game state directly to the desired decision, we design a new multidimensional feature representation to encode both the current and historical card and bet information.\nIn HUNL, the card information and action information exhibit different characteristics. We thus represent them as two separated three-dimension tensors and let the following network learn to fuse them (Figure 2). We design the card tensor in six channels to represent the agent's two hole cards, three flop cards, one turn card, one river card, all public cards, and all hole and public cards. Each channel is a 4 \u00d7 13 sparse binary matrix, with 1 in each position denoting the corresponding card. For the actor tensor, since there are usually at most six sequential actions in each of the four rounds, we design it in 24 channels. Each channel is a 4 \u00d7 n b sparse binary matrix, where n b is the number of betting options, and the four dimensions correspond to the first player's action, the second player's action, the sum of two players' action, and the legal actions. To understand this representation, Figure 3 illustrates one example that a player in the small blind plays an action 'bet pot' after getting a hand 'AsAc'.\nThis representation has several advantages: 1) there is no abstraction of the card information thus reserves all the game information; 2) the action representation is general and can denote different number of betting options (though n b = 9 produce satisfactory results in the experiment); 3) all the historical information is encoded to aid reasoning with hidden information; and 4) the multidimensional tensor representation is very suitable for modern deep neural architectures like ResNet (He et al. 2016) to learn effective feature hierarchies, as verified in the AlphaGo AI training.", "publication_ref": ["b12"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Effective Learning with Trinal-Clip PPO", "text": "With the multidimensional feature representation, one key factor to train the deep architecture is the learning paradigm with suitable loss functions. We adopt the actor-critic paradigm with off-policy training (Konda and Tsitsiklis 2000), which performs updating asynchronously on replayed experiences. The actor-critic paradigm trains a value function V \u03b8 (s t ) and a policy \u03c0 \u03b8 (a t |s t ), and updates them iteratively by sampling from the replay buffer.\nWe employ the popular Proximal Policy Optimization (PPO) (Schulman et al. 2017) learning algorithm to update the policies \u03c0 \u03b8 in the actor-critic framework. PPO defines the ratio function r t (\u03b8) = \u03c0 \u03b8 (at|st) \u03c0 \u03b8 \u2032 (at|st) as the ratio between the current policy \u03c0 \u03b8 and the old policy \u03c0 \u03b8 \u2032 , the advantage function\u00c2 t which describes how much better between two consecutive states s t+1 , s t , over randomly selecting an action according to \u03c0 \u03b8 , and the policy loss function L p as:\nL p (\u03b8) = Et min rt(\u03b8)\u00c2t, clip (rt(\u03b8), 1 \u2212 \u03f5, 1 + \u03f5)\u00c2t ,(1)\nwhere clip(r t (\u03b8), 1 \u2212 \u03f5, 1 + \u03f5) ensures r t lie in the interval (1 \u2212 \u03f5, 1 + \u03f5), and \u03f5 is a clip ratio hyper-parameter with typical value 0.2. The value loss L v is defined as:\nL v (\u03b8) = E t (R \u03b3 t \u2212 V \u03b8 (s t )) 2 ,(2)\nin which R \u03b3 t represents the traditional \u03b3-return. In the HUNL training process, however, the above PPO loss function is difficult to converge. We find two main reasons for this problem: 1) when \u03c0 \u03b8 (a t |s t ) \u226b \u03c0 \u03b8 old (a t |s t ) and the advantage function\u00c2 t < 0, the policy loss L p (\u03b8) will introduce a large variance; 2) due to the uncertainty of the opponent's policy distribution in HUNL (e.g., player performs bluffing), the value loss L v (\u03b8) is often too large. To speed up and stabilize the training process of AlphaHoldem, we design a Trinal-Clip PPO loss function. It introduces one more clipping hyper-parameter \u03b4 1 for the policy loss when A t < 0, and two more clipping hyper-parameters \u03b4 2 and \u03b4 3 for the value loss. The policy loss function L tcp for Trinal-Clip PPO is defined as:\nL tcp (\u03b8) = E t clip (r t (\u03b8), clip (r t (\u03b8), 1 \u2212 \u03f5, 1 + \u03f5) , \u03b4 1 )\u00c2 t ,\n(3) where \u03b4 1 > 1 + \u03f5 indicates the upper bound, and \u03f5 is the original clip in PPO. The clipped value loss function L tcv for Trinal-Clip PPO is defined as:\nL tcv (\u03b8) = E t (clip (R \u03b3 t , \u2212\u03b4 2 , \u03b4 3 ) \u2212 V \u03b8 (s t )) 2 .(4)\nIn training the HUNL AI, the hyper-parameters \u03b4 2 and \u03b4 3 represent the total number of chips the player has placed and the opponent has placed, respectively. Thus, these two hyper-parameters do not require manual tuning but are dynamically calculated according to the chips played in the replay. This constriction significantly reduces the variance of the value function, while also eliminates the influence of the polices' irrationality. Some previous works also report that clipping on PPO's policy loss achieves better results in MOBA games (Ye et al. 2020a,b) and MuJoCo (Andrychowicz et al. 2020). Our proposed Trinal-Clip PPO loss further verifies this point in training a HUNL AI. Moreover, this work finds that clipping on the value function further improves the training efficiency and stability significantly, especially for imperfectinformation games like HUNL which contains rewarding signals with high variance. The Trinal-Clip PPO loss function improves the learning effectiveness of the actor-critic framework, and we believe it is applicable for a wide range of RL applications with imperfect information.", "publication_ref": ["b17", "b25", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Efficient Model Selection and Generation", "text": "With the proposed Trinal-Clip PPO loss function, the most direct way is using the self-play algorithm (Samuel 1959) to train the HUNL agent. However, due to the private information in HUNL, simple self-play learning designed for perfect information game (Heinrich, Lanctot, and Silver 2015;Silver et al. 2016Silver et al. , 2018 often causes the agent trapped in a local minimum and defeated by agents with counter-strategies. AlphaStar (Vinyals et al. 2019) designs a population-based training (PBT) procedure to maintain multiple self-play agents and obtains promising results in the real-time strategy game StarCraft II. The PBT procedure needs a tremendous computational resource to ensure good performance.\nTo obtain a high-performance HUNL AI with both low computation cost and strong decision-making ability, we propose a new type of self-play algorithm which trains only one agent but learns strong and diverse policies. The proposed algorithm maintains a pool of competing agents from the historical versions of the main agent. Then, by com-peting among different agents, the algorithm selects the K best survivors from their ELO (Vinyals et al. 2019) scores and generates experience replays simultaneously. The main agent learns from the replays and thus can compete with different opponents, maintaining a strong decision-making ability of high-flexible policies. Since the proposed algorithm performs self-play among the main agent and its K best historical versions, we refer to it as K-Best Self-Play.\nIn Figure 4, we compare the proposed K-Best Self-Play algorithm against five existing self-play algorithms: 1) the Naive Self-Play (Samuel 1959;Silver et al. 2018), which plays with the agent itself; 2) the Best-Win Self-Play (Silver et al. 2016), which plays with the best agent in history; 3) the Delta-Uniform Self-Play (Bansal et al. 2018), which plays with the agent in the last \u03b4 timestamps; 4) the PBT Self-Play (Vinyals et al. 2019), which trains multiple agents and play with each other; and 5) NFSP (Heinrich and Silver 2016), which plays with the best responses. K-Best Self-Play inherits PBT's merit of diverse policy styles while maintains computational efficiency of single-thread agent training as in Naive, Best-Win, Delta-Uniform self-plays. It also approximates NFSP's best-response calculation strategy by exploring the policies from the K best agents. As reported in the NFSP paper (Heinrich and Silver 2016), calculating the best response to a HUNL AI from the whole policy space is currently computational prohibitive.", "publication_ref": ["b23", "b13", "b26", "b27", "b29", "b29", "b23", "b27", "b26", "b1", "b29", "b14", "b14"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Experimental Evaluations", "text": "We train the AlphaHoldem model on one computing server with 8 NVIDIA TITAN V GPUs and one AMD 2.00 GHz CPU with 64 cores. For each of the experiments conducted below, including ablations, performance, and comparisons, we use the same quantity of resources to train the model: one ordinary machine with 8 GPUs and 64 CPU cores, unless otherwise stated. AlphaHoldem has a total of 8.6 million parameters, including 1.8 million parameters in the ConvNets and 6.8 million parameters in the fully connected layers.\nAs for the experimental settings, the mini-batch size per GPU is set to 2,048; thus, the total batch size is 16,384. We use Adam (Kingma and Ba 2015) with initial learning rate 0.0003. For the Trinal-Clip PPO loss, the hyper-parameters \u03b4 1 is set to 3, and \u03b4 2 and \u03b4 3 is dynamically calculated according to the chips played by the players, which range from 0 to 2,0000. The discount factor is set to 0.999. For policy updates, we use GAE (Schulman et al. 2016) with \u03bb = 0.95 as the advantage estimator. The best performing AlphaHoldem model is trained for a total of 50,000 iterations. During one iteration, there are eight MPI threads, each of which contains 128 environments and 128 steps. Therefore, AlphaHoldem uses a total of 6.5 billion training samples (about 2.7 billion hands). The model winning performance is measured in milli-big-blinds per hand (mbb/h), a standard metric in the poker AI community, representing the average winnings measured in thousandths of the big blinds.", "publication_ref": ["b16", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Ablation Studies", "text": "To analyze the effectiveness of each component in Alpha-Holdem, we have conducted extensive ablation studies, as shown in Table 2. The results of each row are obtained by replacing one component of AlphaHoldem, and the rest remains unchanged. All models use the same number of training samples (i.e., 0.65 billion), and we use ELO scores to compare their performance. For state representation comparison, we consider three alternative methods: 1) Vectorized state representation like DeepCFR (Brown et al. 2019) (Vector). It uses vectors to represent the card information (two 52-dimensional vectors) and the action information (each betting position represented by a binary value specifying whether a bet has occurred and a float value specifying the bet size); 2) PokerCNN-based state representation (Yakovenko et al. 2016) (PokerCNN) uses 3D tensors to represent card and action information together and use a single ConvNet to learn features; 3) State representation without history information (W/O History Information) is similar to AlphaHoldem except that it does not contain history action information.\nAs shown in Table 2, state representation has a significant impact on the final performance. PokerCNN performs better than the vectorized state representation Vector, demonstrating that it is more effective to represent state information using structured tensors. AlphaHoldem outperforms Poker-CNN since it uses a pseudo-Siamese architecture to handle card and action information separately. AlphaHoldem is also better than W/O History Information since historical action information is critical to decision-making in HUNL. Alpha-Holdem obtains the best performance thanks to its effective multidimensional state representation, which encodes historical information and is suitable for ConvNets to learn effective feature hierarchies.\nFor the loss function, we evaluate the Trinal-Clip PPO loss in AlphaHoldem against two kinds of PPO losses: 1) the Original PPO loss (Schulman et al. 2017 dem) obtains the best performance. The results in Table 2 show that adding policy-clip and value-clip upon the PPO loss help improve the performance.\nTo further demonstrate the benefits of the Trinal-Clip PPO loss, we compare the learning curves of these three models in Figure 5. It demonstrates that the Triple-Clip PPO loss's learning curve is more stable than those of the Original PPO and the Dual-clip PPO. This performance improvement is mainly because AlphaHoldem's policy-clip and value-clip loss effectively limit its output to a reasonable range, thus ensuring the stability of the policy update. In addition, we find the model with a minor overall loss generally performs better after adding the value-clip loss, which is also very convenient for model selection during training. This phenomenon also demonstrates that the Trinal-Clip loss helps the model to converge to a better policy.\nFor self-play methods, we compare AlphaHoldem's K-Best Self-Play with Naive Self-Play (Samuel 1959;Silver et al. 2018), Best-Win Self-Play (Silver et al. 2016), Delta-Uniform Self-Play (Bansal et al. 2018), and PBT Self-Play (Vinyals et al. 2019). Interestingly, compared with the more sophisticated Delta-Uniform Self-Play and PBT Self-Play, Naive Self-Play and Best-Win Self-Play achieve better performance, possible because more complex self-play strategies are more data-hungry. However, the performance of Naive and Best-Win Self-Play are still behind K-Best Self-Play, since simplistic self-play methods can not overcome the notorious cyclical strategy problem in IIGs. Our K-Best Self-Play method obtains the best performance under the same amount of training data, striking a good balance between efficiency and effectiveness.", "publication_ref": ["b31", "b25", "b23", "b27", "b26", "b1", "b29"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Comparison with State-of-the-arts and Humans", "text": "Although many milestone events (e.g., DeepStack, Libratus, ReBeL, etc.)   cent years, almost all of these AIs are not publicly available, making the comparison between different AIs extremely difficult. To the best of our knowledge, Slumbot (Jackson 2013), the champion of the 2018 annual computer poker competition, is the only publicly available HUNL AI that provides comparisons through an online website 2 . Slumbot is a strong abstraction-based static agent whose entire policy is precomputed and used as a lookup table. Overall, Slumbot first uses some abstraction algorithms to create a smaller abstract HUNL game. Then it approximates the Nash equilibrium in the abstract game using a CFR algorithm and finally executes the resulting strategy in the original game. Static AIs like Slumbot suffer from the off-tree action problem, i.e., an action taken by an opponent that is not in the abstraction. A more principled approach is to solve subgames that immediately follow that off-tree action online. DeepStack and Libratus are representative online AIs based on this idea. We reimplement DeepStack following the original paper's key ideas and obtain a strong AI named Open-Stack 3 . Specifically, we spend three weeks using 120 GPUs to generate millions of samples to train the value networks. It is worth noting that the creator of Libratus, recently coauthored a paper (Zarick et al. 2020), in which they also reimplemented DeepStack. OpenStack has achieved similar results to theirs, i.e., playing with Slumbot for 100,000 games, OpenStack's gain is 103.08 mbb/h, which validates the correctness of our reimplementation.\nWe compare our AlphaHoldem with the above two strong HUNL AIs, i.e., Slumbot and OpenStack for 100,000 hands, and Table 3 shows the head-to-head comparison results. We can see from Table 3 that AlphaHoldem outperforms Slumbot by a large margin. Compared with Slumbot, Al-phaHoldem does not require domain knowledge for abstraction and achieves better performance while significantly reducing computational and storage resources. AlphaHoldem also beats OpenStack by 16.91 mbb/h. Unlike OpenStack, AlphaHoldem does not need iterative learning in both the training and inference stages. Given input state representation, it performs only one feedforward pass of the neural network to output the action directly.\nTo further verify AlphaHoldem's performance, we evaluate it against four HUNL human professionals. We invite these professional players who have participated in many continental level invitational tournaments, two of whom achieved in the top 10 of continental level tournaments. We ask each player to play against AlphaHoldem for about 2,500 hands. AlphaHoldem beats these professionals by 10.27 mbb/h in average, which supports its highperformance in beating Slumbot and DeepStack.", "publication_ref": ["b15", "b34"], "figure_ref": [], "table_ref": ["tab_7", "tab_7"]}, {"heading": "Visualization of AlphaHoldem's Learned Policy", "text": "To analyze AlphaHoldem's learned policy, we compare the action frequencies where the agent is the first player to act and has no prior state influencing it (Zarick et al. 2020) with those from human professional 4 , DeepStack, and Open-Stack. Figure 6 shows the policies on how to play the first two cards from the professional human and the three agents. AlphaHoldm's policy is very similar to those of the human professional and the two well-trained agents. These results validate that AlphaHoldem learns a reasonable policy.", "publication_ref": ["b34"], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Conclusive Remarks and Future Works", "text": "We have presented AlphaHoldem, an end-to-end reinforcement learning framework to obtain superhuman HUNL AI with the current lowest computation and storage costs and without encoding any human domain knowledge. We achieve this goal through a set of new technical contributions to speed up the training process and simultaneously guarantees the adaptability of the HUNL agent. The proposed learning framework and the speedup training techniques are extendable to the multi-player Texas hold'em and other IIGs like MahJong and Bridge. In future, we plan to expand the proposed framework on more IIG games to promote the development of more general IIG learning frameworks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "What matters for on-policy deep actor-critic methods? a large-scale study", "journal": "", "year": "2020", "authors": "M Andrychowicz; A Raichuk; P Sta\u0144czyk; M Orsini; S Girgin; R Marinier; L Hussenot; M Geist; O Pietquin; M Michalski"}, {"ref_id": "b1", "title": "Emergent complexity via multi agent competition", "journal": "", "year": "2018", "authors": "T Bansal; J Pachocki; S Sidor; I Sutskever; I Mordatch"}, {"ref_id": "b2", "title": "The annual computer poker competition", "journal": "AI Magazine", "year": "2013", "authors": "N Bard; J Hawkin; J Rubin; M Zinkevich"}, {"ref_id": "b3", "title": "Pattern classification in no-limit poker: A head-start evolutionary approach", "journal": "", "year": "2007", "authors": "B Beattie; G Nicolai; D Gerhard; R J Hilderman"}, {"ref_id": "b4", "title": "Dota 2 with large scale deep reinforcement learning", "journal": "", "year": "2019", "authors": "C Berner; G Brockman; B Chan; V Cheung; P D\u0119biak; C Dennison; D Farhi; Q Fischer; S Hashme; C Hesse"}, {"ref_id": "b5", "title": "Heads-up limit hold'em poker is solved", "journal": "Science", "year": "2015", "authors": "M Bowling; N Burch; M Johanson; O Tammelin"}, {"ref_id": "b6", "title": "Combining deep reinforcement learning and search for imperfect-information games", "journal": "", "year": "2020", "authors": "N Brown; A Bakhtin; A Lerer; Q Gong"}, {"ref_id": "b7", "title": "Deep counterfactual regret minimization", "journal": "", "year": "2019", "authors": "N Brown; A Lerer; S Gross; T Sandholm"}, {"ref_id": "b8", "title": "Superhuman AI for heads-up no-limit poker: Libratus beats top professionals", "journal": "Science", "year": "2018", "authors": "N Brown; T Sandholm"}, {"ref_id": "b9", "title": "Solving imperfectinformation games via discounted regret minimization", "journal": "", "year": "2019", "authors": "N Brown; T Sandholm"}, {"ref_id": "b10", "title": "Superhuman AI for multiplayer poker", "journal": "Science", "year": "2019", "authors": "N Brown; T Sandholm"}, {"ref_id": "b11", "title": "Solving imperfect information games using decomposition", "journal": "", "year": "2014", "authors": "N Burch; M Johanson; M Bowling"}, {"ref_id": "b12", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "K He; X Zhang; S Ren; J Sun"}, {"ref_id": "b13", "title": "Fictitious self-play in extensive-form games", "journal": "", "year": "2015", "authors": "J Heinrich; M Lanctot; D Silver"}, {"ref_id": "b14", "title": "Deep reinforcement learning from self-play in imperfect-information games", "journal": "", "year": "2016", "authors": "J Heinrich; D Silver"}, {"ref_id": "b15", "title": "Slumbot NL: Solving large games with counterfactual regret minimization using sampling and distributed processing", "journal": "", "year": "2013", "authors": "E G Jackson"}, {"ref_id": "b16", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015", "authors": "D P Kingma; J Ba"}, {"ref_id": "b17", "title": "Actor-Critic Algorithms", "journal": "", "year": "2000", "authors": "V Konda; J Tsitsiklis"}, {"ref_id": "b18", "title": "", "journal": "", "year": "", "authors": "M Lanctot; K Waugh; M Zinkevich; M Bowling"}, {"ref_id": "b19", "title": "Monte Carlo sampling for regret minimization in extensive games", "journal": "", "year": "", "authors": ""}, {"ref_id": "b20", "title": "SuphX: Mastering Mahjong with deep reinforcement learning", "journal": "", "year": "2020", "authors": "J Li; S Koyamada; Q Ye; G Liu; C Wang; R Yang; L Zhao; T Qin; T.-Y Liu; H.-W Hon"}, {"ref_id": "b21", "title": "DeepStack: Expert-level artificial intelligence in heads-up no-limit poker", "journal": "Science", "year": "2017", "authors": "M Moravcik; M Schmid; N Burch; V Lisy; D Morrill; N Bard; T Davis; K Waugh; M Johanson; M Bowling"}, {"ref_id": "b22", "title": "Computer poker: A review", "journal": "Artificial Intelligence", "year": "2011", "authors": "J Rubin; I Watson"}, {"ref_id": "b23", "title": "Some studies in machine learning using the game of checkers", "journal": "IBM Journal of Research and Development", "year": "1959", "authors": "A L Samuel"}, {"ref_id": "b24", "title": "High-dimensional continuous control using generalized advantage estimation", "journal": "", "year": "2016", "authors": "J Schulman; P Moritz; S Levine; M Jordan; P Abbeel"}, {"ref_id": "b25", "title": "", "journal": "", "year": "2017", "authors": "J Schulman; F Wolski; P Dhariwal; A Radford; O Klimov"}, {"ref_id": "b26", "title": "Mastering the game of Go with deep neural networks and tree search", "journal": "Nature", "year": "2016", "authors": "D Silver; A Huang; C J Maddison; A Guez; L Sifre; G Van Den Driessche; J Schrittwieser; I Antonoglou; V Panneershelvam; M Lanctot"}, {"ref_id": "b27", "title": "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play", "journal": "Science", "year": "2018", "authors": "D Silver; T Hubert; J Schrittwieser; I Antonoglou; M Lai; A Guez; M Lanctot; L Sifre; D Kumaran; T Graepel"}, {"ref_id": "b28", "title": "Solving heads-up limit Texas Hold'em", "journal": "", "year": "2015", "authors": "O Tammelin; N Burch; M Johanson; M Bowling"}, {"ref_id": "b29", "title": "Grandmaster level in Star-Craft II using multi-agent reinforcement learning", "journal": "Nature", "year": "2019", "authors": "O Vinyals; I Babuschkin; W M Czarnecki; M Mathieu; A Dudzik; J Chung; D H Choi; R Powell; T Ewalds; P Georgiev"}, {"ref_id": "b30", "title": "Generalization learning techniques for automating the learning of heuristics", "journal": "Artificial Intelligence", "year": "1970", "authors": "D A Waterman"}, {"ref_id": "b31", "title": "Poker-CNN: A pattern learning strategy for making draws and bets in poker games using convolutional networks", "journal": "", "year": "2016", "authors": "N Yakovenko; L Cao; C Raffel; J Fan"}, {"ref_id": "b32", "title": "Towards playing full moba games with deep reinforcement learning", "journal": "", "year": "2020", "authors": "D Ye; G Chen; W Zhang; S Chen; B Yuan; B Liu; J Chen; Z Liu; F Qiu; H Yu"}, {"ref_id": "b33", "title": "Mastering complex control in moba games with deep reinforcement learning", "journal": "", "year": "2020", "authors": "D Ye; Z Liu; M Sun; B Shi; P Zhao; H Wu; H Yu; S Yang; X Wu; Q Guo"}, {"ref_id": "b34", "title": "Unlocking the Potential of Deep Counterfactual Value Networks", "journal": "", "year": "2007", "authors": "R Zarick; B Pellegrino; N Brown; C Banister; M Johanson; M Bowling; C Piccione"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Four cards of the same value Full House Combination of three of a kind and a pair Flush Five cards of the same suit Straight Sequence of 5 cards in increasing value Three-of-a-Kind Three cards with the same value Two Pair Two times two cards with the same value One Pair Simple value of two card with the same value No Pair", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: An brief illustration of the HUNL game rules.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: End-to-end learning architecture of AlphaHoldem.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Comparison of different self-play algorithms. The proposed K-Best self-play algorithm can learn both strong and diverse decision styles with low computation cost.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Loss Curves for Original PPO, Dual-clip PPO and Trinal-Clip among the whole training process. The model with smaller overall loss (shown as blue circles) generally performs better.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 6 :6Figure6: Probabilities for not folding as the first action for each possible hand. The bottom-left half shows the policy when the suits of two private cards do not match, and the top-right half shows the policy when the suits of two private cards match. Left to right represent the policies of Professional Human, DeepStack, and AlphaHoldem, respectively.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Cost comparisons of HUNL AIs. AlphaHoldem achieves good results with less computational resources.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": "all cardsRound 1Action 2flop cardsRound 1hole cardsAction 1"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "have been achieved in HUNL AI research in re-Slumbot OpenStack Professionals AlphaHoldem 111.56 \u00b1 16.06 16.91 \u00b1 22.34 10.27 \u00b1 65.13", "figure_data": "Hands100,000100,00010,000"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Head-to-head results of AlphaHoldem against Slumbot, OpenStack, and human professionals, measured in mbb/h. We list the results against human professionals in aggregate. The \u00b1 shows 95% confidence interval.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "K A Q J 1 0 Q K J 1 0 9 A A A A 1 0 4 2 7 9 Q A A A K K K A Q J 1 0 A A K 7 Q A A A Q K A A K Q Q K A Q 4 J Strong Weak (b) HUNL cards strength", "formula_coordinates": [3.0, 107.08, 199.25, 184.17, 125.04]}, {"formula_id": "formula_1", "formula_text": "0 0 0 0 \u2026 1 0 0 0 1 0 1 1 \u2026 1 1 1 0 0 0 1 0 \u2026 0 0 0 0 0 0 0 0 \u2026 0 0 0 0 0 0 0 0 \u2026 1 0 0 0 1 0 1 1 \u2026 1 1 1 1 0 0 0 0 \u2026 1 0 0 0 0 0 0 0 \u2026 0 0 0 0 0 0 0 0 \u2026 1 0 0 0 1 0 1 1 \u2026 1 1 1 1", "formula_coordinates": [4.0, 176.13, 77.62, 92.91, 39.47]}, {"formula_id": "formula_2", "formula_text": "0 0 0 0 \u2026 1 0 0 0 1 0 1 1 \u2026 1 1 1 1 p1 p2 sum legal fold checkcall allin", "formula_coordinates": [4.0, 147.32, 126.22, 99.7, 34.57]}, {"formula_id": "formula_3", "formula_text": "L p (\u03b8) = Et min rt(\u03b8)\u00c2t, clip (rt(\u03b8), 1 \u2212 \u03f5, 1 + \u03f5)\u00c2t ,(1)", "formula_coordinates": [4.0, 328.08, 344.44, 229.92, 23.23]}, {"formula_id": "formula_4", "formula_text": "L v (\u03b8) = E t (R \u03b3 t \u2212 V \u03b8 (s t )) 2 ,(2)", "formula_coordinates": [4.0, 375.44, 408.27, 182.57, 13.9]}, {"formula_id": "formula_5", "formula_text": "L tcp (\u03b8) = E t clip (r t (\u03b8), clip (r t (\u03b8), 1 \u2212 \u03f5, 1 + \u03f5) , \u03b4 1 )\u00c2 t ,", "formula_coordinates": [4.0, 319.5, 595.77, 244.21, 11.72]}, {"formula_id": "formula_6", "formula_text": "L tcv (\u03b8) = E t (clip (R \u03b3 t , \u2212\u03b4 2 , \u03b4 3 ) \u2212 V \u03b8 (s t )) 2 .(4)", "formula_coordinates": [4.0, 336.69, 661.42, 221.31, 13.9]}], "doi": ""}