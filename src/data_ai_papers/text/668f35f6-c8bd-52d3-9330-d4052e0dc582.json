{"title": "On the Expressivity of Markov Reward", "authors": "David Abel; Will Dabney; Anna Harutyunyan; Mark K Ho; Michael L Littman; Doina Precup; Satinder Singh", "pub_date": "2022-01-18", "abstract": "Reward is the driving force for reinforcement-learning agents. This paper is dedicated to understanding the expressivity of reward as a way to capture tasks that we would want an agent to perform. We frame this study around three new abstract notions of \"task\" that might be desirable: (1) a set of acceptable behaviors, (2) a partial ordering over behaviors, or (3) a partial ordering over trajectories. Our main results prove that while reward can express many of these tasks, there exist instances of each task type that no Markov reward function can capture. We then provide a set of polynomial-time algorithms that construct a Markov reward function that allows an agent to optimize tasks of each of these three types, and correctly determine when no such reward function exists. We conclude with an empirical study that corroborates and illustrates our theoretical findings. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).", "sections": [{"heading": "Introduction", "text": "How are we to use algorithms for reinforcement learning (RL) to solve problems of relevance in the world? Reward plays a significant role as a general purpose signal: For any desired behavior, task, or other characteristic of agency, there must exist a reward signal that can incentivize an agent to learn to realize these desires. Indeed, the expressivity of reward is taken as a backdrop assumption that frames RL, sometimes called the reward hypothesis: \"...all of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward)\" [53,29,6]. In this paper, we establish first steps toward a systematic study of the reward hypothesis by examining the expressivity of reward as a signal. We proceed in three steps.\n1. An Account of \"Task\". As rewards encode tasks, goals, or desires, we first ask, \"what is a task?\". We frame our study around a thought experiment (Figure 1) involving the interactions between a designer, Alice, and a learning agent, Bob, drawing inspiration from Ackley and Littman [2], Sorg [50], and Singh et al. [46]. In this thought experiment, we draw a distinction between how Alice thinks of a task (TASKQ) and the means by which Alice incentivizes Bob to pursue this task (EXPRESSIONQ). This distinction allows us to analyze the expressivity of reward as an answer to the latter question, conditioned on how we answer the former. Concretely, we study three answers to the TASKQ in the context of finite Markov Decision Processes (MDPs): A task is either (1) a set of acceptable behaviors (policies), (2) a partial ordering over behaviors, or (3) a partial ordering over trajectories. Further detail and motivation for these task types is provided in Section 3, but broadly they can be viewed as generalizations of typical notions of task such as a choice of goal or optimal behavior. Given these three answers to the TASKQ, we then examine the expressivity of reward.\n2. Expressivity of Markov Reward. The core of our study asks whether there are tasks Alice would like to convey-as captured by the answers to the TASKQ-that admit no characterization in terms of a Markov reward function. Our emphasis on Markov reward functions, as opposed to arbitrary history-based reward functions, is motivated by several factors. First, disciplines such as computer science, psychology, biology, and economics typically rely on a notion of reward as a numerical proxy for the immediate worth of states of affairs (such as the financial cost of buying a solar panel or the fitness benefits of a phenotype). Given an appropriate way to describe states of affairs, Markov reward functions can represent immediate worth in an intuitive manner that also allows for reasoning about combinations, sequences, or re-occurrences of such states of affairs. Second, it is not clear that general history-based rewards are a reasonable target for learning as they suffer from the curse of dimensionality in the length of the history. Lastly, Markov reward functions are the standard in RL. A rigorous analysis of which tasks they can and cannot convey may provide guidance into when it is necessary to draw on alternative formulations of a problem. Given our focus on Markov rewards, we treat a reward function as accurately expressing a task just when the value function it induces in an environment adheres to the constraints of a given task.\n3. Main Results. We find that, for all three task types, there are environment-task pairs for which there is no Markov reward function that realizes the task (Theorem 4.1). In light of this finding, we design polynomial-time algorithms that can determine, for any given task and environment, whether a reward function exists in the environment that captures the task (Theorem 4.3). When such a reward function does exist, the algorithms also return it. Finally, we conduct simple experiments with these procedures to provide empirical insight into the expressivity of reward (Section 5).\nCollectively, our results demonstrate that there are tasks that cannot be expressed by Markov reward in a rigorous sense, but we can efficiently construct such reward functions when they do exist (and determine when they do not). We take these findings to shed light on the nature of reward maximization as a principle, and highlight many pathways for further investigation.", "publication_ref": ["b52", "b28", "b5", "b1", "b49", "b45"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Background", "text": "RL defines the problem facing an agent that learns to improve its behavior over time by interacting with its environment. We make the typical assumption that the RL problem is well modeled by an agent interacting with a finite Markov Decision Process (MDP), defined by the tuple (S, A, R, T, \u03b3, s 0 ). An MDP gives rise to deterministic behavioral policies, \u03c0 : S \u2192 A, and the value, V \u03c0 : S \u2192 R, and action-value, Q \u03c0 : S \u00d7 A \u2192 R, functions that measure their quality. We will refer to a Controlled Markov Process (CMP) as an MDP without a reward function, which we denote E for environment.\nWe assume that all reward functions are deterministic, and may be a function of either state, stateaction pairs, or state-action-state triples, but not history. Henceforth, we simply use \"reward function\" to refer to a deterministic Markov reward function for brevity, but note that more sophisticated settings beyond MDPs and deterministic Markov reward functions are important directions for future work.\nFor more on MDPs or RL, see the books by Puterman [41] and Sutton and Barto [54] respectively.", "publication_ref": ["b40", "b53"], "figure_ref": [], "table_ref": []}, {"heading": "Other Perspectives on Reward", "text": "We here briefly summarize relevant literature that provides distinct perspectives on reward.\nTwo Roles of Reward. As Sorg [50] identifies (Chapter 2), reward can both define the task the agent learns to solve, and define the \"bread crumbs\" that allow agents to efficiently learn to solve the task. This distinction has been raised elsewhere [2,46,47], and is similar to the extrinsic-intrinsic reward divide [45,66]. Tools such as reward design [34,51] or reward shaping [36] focus on offering more efficient learning in a variety of environments, so as to avoid issues of sparsity and long-term credit assignment. We concentrate primarily on reward's capacity to express a task, and defer learning dynamics to an (important) stage of future work.\nDiscounts, Expectations, and Rationality. Another important facet of reward is how it is used in producing behavior. The classical view offered by the Bellman equation (and the reward hypothesis) is that the quantity of interest to maximize is expected, discounted, cumulative reward. Yet it is possible to disentangle reward from the expectation [5], to attend only to ordinal [60] or maximal rewards [26], or to adopt different forms of discounting [61,11]. In this work, we take the standard view that agents will seek to maximize value for a particular discount factor \u03b3, but recognize that there are interesting directions beyond these commitments, such as inspecting the limits of reward in constrained MDPs as studied by Szepesv\u00e1ri [56]. We also note the particular importance of work by Pitis [40], who examines the relationship between classical decision theory [59] and MDPs by incorporating additional axioms that account for stochastic processes with discounting [24,35,48,49]. Drawing inspiration from Pitis [40] and Sunehag and Hutter [52], we foresee valuable pathways for future work that further makes contact between RL and various axioms of rationality.\nPreferences. In place of numerical rewards, preferences of different kinds may be used to evaluate an agent's behaviors, drawing from the literature on preference-learning [25] and ordinal dynamic programming [8,35,48]. This premise gives rise to preference-based reinforcement learning (PbRL) in which an agent interacts with a CMP and receives evaluative signals in the form of preferences over states, actions, or trajectories. This kind of feedback inspires and closely parallels the task types we propose in this work. A comprehensive survey of PbRL by Wirth et al. [64] identifies critical differences in this setup from traditional RL, categorizes recent algorithmic approaches, and highlights important open questions. Recent work focuses on analysing the sample efficiency of such methods [65,38] with close connections to learning from human feedback in real time [23,32,7].\nTeaching and Inverse RL. The inverse RL (IRL) and apprenticeship learning literature examine the problem of learning directly from behavior [37,1]. The classical problem of IRL is to identify which reward function (often up to an equivalence class) a given demonstrator is optimizing. We emphasize the relevance of two approaches: First, work by Syed et al. [55], who first illustrate the applicability of linear programming [22] to apprenticeship learning; and second, work by Amin et al. [4], who examine the repeated form of IRL. The methods of IRL have recently been expanded to include variations of cooperative IRL [14], and assistive learning [43], which offer different perspectives on how to frame interactive learning problems.\nReward Misspecification. Reward is also notoriously hard to specify. As pointed out by Littman et al. [30], \"putting a meaningful dollar figure on scuffing a wall or dropping a clean fork is challenging.\" Along these lines, Hadfield-Menell et al. [16] identify cases in which well-intentioned designers create reward functions that produce unintended behavior [39]. MacGlashan et al. [33] find that human-provided rewards tend to depend on a learning agent's entire policy, rather than just the current state. Further, work by Hadfield-Menell et al. [15] and Kumar et al. [27] suggest that there are problems with reward as a learning mechanism due to misspecification and reward tampering [10]. These problems have given rise to approaches to reward learning, in which a reward function is inferred from some evidence such as behavior or comparisons thereof [20].\nOther Notions of Task. As a final note, we highlight alternative approaches to task specification. Building on the Free Energy Principle [13,12], Hafner et al. [17] consider a variety of task types in terms of minimization of distance to a desired target distribution [3]. Alternatively, Littman et al. [30] and Li et al. [28] propose variations of linear temporal logic (LTL) as a mechanism for specifying a task to RL agents, with related literature extending LTL to the multi-task [58] and multi-agent [18] settings, or using reward machines for capturing task structure [19]. Jothimurugan et al. [21] take a similar approach and propose a task specification language for RL based on logical formulas that evaluate whether trajectories satisfy the task, similar in spirit to the logical task compositions framework developed by Tasse et al. [57]. Many of these notions of task are more general than those we consider. A natural direction for future work broadens our analysis to include these kinds of task.\n3 An Account of Reward's Expressivity: The TASKQ and EXPRESSIONQ\nConsider an onlooker, Alice, and an earnest learning agent, Bob, engaged in the interaction pictured in Figure 1. Suppose that Alice has a particular task in mind that she would like Bob to learn to solve, and that Alice constructs a reward function to incentivize Bob to pursue this task. Here, Alice is playing the role of \"all of what we mean by goals and purposes\" for Bob to pursue, with Bob playing the role of the standard reward-maximizing RL agent.\nTwo Questions About Task. To give us leverage to study the expressivity of reward, it is useful to draw a distinction between two stages of this process: 1) Alice thinks of a task that she would like Bob to learn to solve, and 2) Alice creates a reward function (and perhaps chooses \u03b3) that conveys the chosen task to Bob. We inspect these two separately, framed by the following two questions. The first we call the task-definition question (TASKQ) which asks: What is a task? The second we call the task-expression question (EXPRESSIONQ) which asks: Which learning signal can be used as a mechanism for expressing any task to Bob?\nReward Answers The EXPRESSIONQ. We suggest that it may be useful to treat reward as an answer to the EXPRESSIONQ rather than the TASKQ. On this view, reward is treated as an expressive language for incentivizing reward-maximizing agents: Alice may attempt to translate any task into a reward function that incentivizes Bob to pursue the task, no matter which environment Bob inhabits, which task Alice has chosen, or how she has represented the task to herself. Indeed, it might be the case that Alice's knowledge of the task far exceeds Bob's representational or perceptual capacity. Alice may know every detail of the environment and define the task based on this holistic vantage, while Bob must learn to solve the task through interaction alone, relying only on a restricted class of functions for modeling and decision making.\nUnder this view, we can assess the expressivity of reward as an answer to the EXPRESSIONQ conditioned on how we answer the TASKQ. For example, if the TASKQ is answered in terms of natural language descriptions of desired states of affairs, then reward may fail to convey the chosen task due to the apparent mismatch in abstraction between natural language and reward (though some work has studied such a proposal [31,62]).", "publication_ref": ["b49", "b1", "b45", "b46", "b44", "b65", "b33", "b50", "b35", "b4", "b59", "b25", "b60", "b10", "b55", "b39", "b58", "b23", "b34", "b47", "b48", "b39", "b51", "b24", "b7", "b34", "b47", "b63", "b64", "b37", "b22", "b31", "b6", "b36", "b0", "b54", "b21", "b3", "b13", "b42", "b29", "b15", "b38", "b32", "b14", "b26", "b9", "b19", "b12", "b11", "b16", "b2", "b29", "b27", "b57", "b17", "b18", "b20", "b56", "b30", "b61"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Answers to the TASKQ: What is a Task?", "text": "In RL, tasks are often associated with a choice of goal, reward function (R), reward-discount pair (R, \u03b3), or perhaps a choice of optimal policy (alongside those task types surveyed previously, such as LTL). However, it is unclear whether these constructs capture the entirety of what we mean by \"task\".\nFor example, consider the Russell and Norvig [42] grid world: A 4\u00d73 grid with one wall, one terminal fire state, and one terminal goal state (pictured with a particular reward function in Figure 4a). In such an environment, how might we think about tasks? A standard view is that the task is to reach the goal as quickly as possible. This account, however, fails to distinguish between the non-optimal behaviors, such as the costly behavior of the agent moving directly into the fire and the neutral behavior of the agent spending its existence in the start state. Indeed, characterizing a task in terms of choice of \u03c0 * or goal fails to capture these distinctions. Our view is that a suitably rich account of task should allow for the characterization of this sort of preference, offering the flexibility to scale from specifying only the desirable behavior (or outcomes) to an arbitrary ordering over behaviors (or outcomes).\nIn light of these considerations, we propose three answers to the TASKQ that can convey general preferences over behavior or outcome: 1) A set of acceptable policies, 2) A partial ordering over policies, or 3) A partial ordering over trajectories. We adopt these three as they can capture many kinds of task while also allowing a great deal of flexibility in the level of detail of the specification.\nName Notation Generalizes Constraints Induced by T\nSOAP \u03a0 G task-as-\u03c0 * equal: V \u03c0g (s 0 ) = V \u03c0 g (s 0 ) > V \u03c0 b (s 0 ), \u2200 \u03c0g,\u03c0 g \u2208\u03a0 G ,\u03c0 b \u2208\u03a0 B range: V \u03c0g (s 0 ) > V \u03c0 b (s 0 ), \u2200 \u03c0g\u2208\u03a0 G ,\u03c0 b \u2208\u03a0 B PO L \u03a0 SOAP (\u03c0 1 \u2295 \u03c0 2 ) \u2208 L \u03a0 =\u21d2 V \u03c01 (s 0 ) \u2295 V \u03c02 (s 0 ) TO L \u03c4,N task-as-goal (\u03c4 1 \u2295 \u03c4 2 ) \u2208 L \u03c4,N =\u21d2 G(\u03c4 1 ; s 0 ) \u2295 G(\u03c4 2 ; s 0 )\nTable 1: A summary of the three proposed task types. We further list the constraints that determine whether a reward function realizes each task type in an MDP, where we take \u2295 to be one of '<', '>', or '=', and G is the discounted return of the trajectory.", "publication_ref": ["b41"], "figure_ref": ["fig_12"], "table_ref": []}, {"heading": "SOAPs, POs, and TOs", "text": "(SOAP) Set Of Acceptable Policies. A classical view of the equivalence of two reward functions is based on the optimal policies they induce. For instance, Ng et al. [36] develop potential-based reward shaping by inspecting which shaped reward signals will ensure that the optimal policy is unchanged. Extrapolating, it is natural to say that for any environment E, two reward functions are equivalent if the optimal policies they induce in E are the same. In this way, a task is viewed as a choice of optimal policy. As discussed in the grid world example above, this notion of task fails to allow for the specification of the quality of other behaviors. For this reason, we generalize task-as-optimal-policy to a set of acceptable policies, defined as follows.\nDefinition 3.1. A set of acceptable policies (SOAP) is a non-empty subset of the deterministic policies, \u03a0 G \u2286 \u03a0, with \u03a0 the set of all deterministic mappings from S to A for a given E.\nWith one task type defined, it is important to address what it means for a reward function to properly realize, express, or capture a task in a given environment. We offer the following account.", "publication_ref": ["b35"], "figure_ref": [], "table_ref": []}, {"heading": "Definition 3.2.", "text": "A reward function is said to realize a task T in an environment E just when the start-state value (or return) induced by the reward function exactly adheres to the constraints of T .\nPrecise conditions for the realization of each task type are provided alongside each task definition, with a summary presented in column four of Table 1.\nFor SOAPs, we take the start-state value V \u03c0 (s 0 ) to be the mechanism by which a reward function realizes a SOAP. That is, for a given E and \u03a0 G , a reward function R is said to realize the \u03a0 G in E when the start-state value function is optimal for all good policies, and strictly higher than the start-state value of all other policies. It is clear that SOAP strictly generalizes a task in terms of a choice of optimal policy, as captured by the SOAP \u03a0 G = {\u03c0 * }.\nWe note that there are two natural ways for a reward function to realize a SOAP: First, each \u03c0 g \u2208 \u03a0 G has optimal start-state value and all other policies are sub-optimal. We call this type equal-SOAP, or just SOAP for brevity. Alternatively, we might only require that the acceptable policies are each near-optimal, but are allowed to differ in start-state value so long as they are all better than every bad policy \u03c0 b \u2208 \u03a0 B . That is, in this second kind, there exists an \u2265 0 such that every \u03c0 g \u2208 \u03a0 G is -optimal in start-state value, V * (s 0 ) \u2212 V \u03c0g (s 0 ) \u2264 , while all other policies are worse. We call this second realization condition range-SOAP. We note that the range realization generalizes the equal one: Every equal-SOAP is a range-SOAP (by letting = 0). However, there exist range-SOAPs that are expressible by Markov rewards that are not realizable as an equal-SOAP. We illustrate this fact with the following proposition. All proofs are presented in Appendix B.\nProposition 3.1. There exists a CMP, E, and choice of \u03a0 G such that \u03a0 G can be realized under the range-SOAP criterion, but cannot be realized under the equal-SOAP criterion.\nOne such CMP is pictured Figure 2b. Consider the SOAP \u03a0 G = {\u03c0 11 , \u03c0 12 , \u03c0 21 }: Under the equal-SOAP criterion, if each of these three policies are made optimal, any reward function will also make \u03c0 22 (the only bad policy) optimal as well. In contrast, for the range criterion, we can choose a reward function that assigns lower rewards to a 2 than a 1 in both states. In general, we take the equal-SOAP realization as canonical, as it is naturally subsumed by our next task type.\n(PO) Partial Ordering on Policies. Next, we suppose that Alice chooses a partial ordering on the deterministic policy space. That is, Alice might identify a some great policies, some good, and some bad policies to strictly avoid, and remain indifferent to the rest. POs strictly generalize equal SOAPs, as any such SOAP is a special choice of PO with only two equivalence classes. We offer the following definition of a PO. Definition 3.3. A policy order (PO) of the deterministic policies \u03a0 is a partial order, denoted L \u03a0 .\nAs with SOAPs, we take the start-state value V \u03c0 (s 0 ) induced by a reward function R as the mechanism by which policies are ordered. That is, given E and L \u03a0 , we say that a reward function R realizes L \u03a0 in E if and only if the resulting MDP, M = (E, R), produces a start-state value function that orders \u03a0 according to L \u03a0 .\n(TO) Partial Ordering on Trajectories. A natural generalization of goal specification enriches a notion of task to include the details of how a goal is satisfied-that is, for Alice to relay some preference over trajectory space [63], as is done in preference based RL [64]. Concretely, we suppose Alice specifies a partial ordering on length N trajectories of (s, a) pairs, defined as follows. As with PO, we say that a reward function realizes a trajectory ordering L \u03c4,N if the ordering determined by each trajectory's cumulative discounted N -step return from s 0 , denoted G(\u03c4 ; s 0 ), matches that of the given L \u03c4,N . We note that trajectory orderings can generalize goal-based tasks at the expense of a larger specification. For instance, a TO can convey the task, \"Safely reach the goal in less than thirty steps, or just get to the subgoal in less than twenty steps.\"\nRecap. We propose to assess the expressivity of reward by first answering the TASKQ in terms of SOAPs, POs, or TOs, as summarized by Table 1. We say that a task T is realized in an environment E under reward function R if the start-state value function (or return) produced by R imposes the constraints specified by T , and are interested in whether reward can always realize a given task in any choice of E. We make a number of assumptions along the way, including: (1) Reward functions are Markov and deterministic, (2) Policies of interest are deterministic, (3) The environment is a finite CMP, (4) \u03b3 is part of the environment, (5) We ignore reward's role in shaping the learning process, (6) Start-state value or return is the appropriate mechanism to determine if a reward function realizes a given task. Relaxation of these assumptions is a critical direction for future work.", "publication_ref": ["b62", "b63"], "figure_ref": [], "table_ref": []}, {"heading": "Analysis: The Expressivity of Markov Reward", "text": "With our definitions and objectives in place, we now present our main results.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Express SOAPs, POs, and TOs", "text": "We first ask whether reward can always realize a given SOAP, PO, or TO, for an arbitrary E. Our first result states that the answer is \"no\"-there are tasks that cannot be realized by any reward function. Thus, reward is incapable of capturing certain tasks. What tasks are they, precisely? Intuitively, inexpressible tasks involve policies or trajectories that must be correlated in value in an MDP. That is, if two policies are nearly identical in behavior, it is unlikely that reward can capture the PO that places them at opposite ends of the ordering. A simple example is the \"always move the same direction\" task in a grid world, with state defined as an (x, y) pair. The SOAP \u03a0 G = {\u03c0 \u2190 , \u03c0 \u2191 , \u03c0 \u2192 , \u03c0 \u2193 } conveys this task, but no Markov reward function can make these policies strictly higher in value than all others.\nExample: Inexpressible SOAPs. Observe the two CMPs pictured in Figure 2, depicting two kinds of inexpressible SOAPs. On the left, we consider the SOAP \u03a0 G = {\u03c0 21 }, containing only the policy that executes a 2 in the left state (s 0 ), and a 1 in the right (s 1 ). This SOAP is inexpressible through reward, but only because reward cannot distinguish the start-state value of \u03c0 21 and \u03c0 22 since the policies differ only in an unreachable state. This is reminiscent of Axiom 5 from Pitis [40], which s0 s1 a1 a1 a2 a2\n< l a t e x i t s h a 1 _ b a s e 6 4 = \" t p 9 e x u N 8 f E V L + K t c i U f w f r d B W 7 I = \"   \n> A A A P m H i c j V d t b 9 s 2 E H b 3 2 n l b 0 2 7 7 t H 3 h F h T o A i W Q Z D u O C w T o m r 5 s Q F / S I G m L R l l A y b T N h h I 1 i k r i c g L 2 U / Z 1 + 0 f 7 N z t S f p E o p 6 g B S 9 T d w + P D 4 / G O D F N G M + m 6 / 1 3 7 6 O N P P v 3 s 8 + t f t L / 8 6 u s b a z d v f f M y 4 7 m I y F H E G R e v Q 5 w R R h N y J K l k 5 H U q C I 5 D R l 6 F Z 3 t a / + q c i I z y 5 F B O U 3 I S 4 3 F C R z T C E k S n N 7 8 L 9 u n p Y 7 S L A h W k 9 F T 5 X h E U p z f X 3 S 3 X / F C z 4 c 0 a 6 6 3 Z b / / 0 1 g 0 W D H m U x y S R E c N Z d u y 5 q T x R W E g a M V K 0 b 6 P N z U 2 0 j 6 M z P C a Z / m g H e U b S U n A c Y z G m y a 6 7 5 d P k R I 0 J j 4 k U 0 6 K K U S Q B 8 w J L 4 q A Y y 4 n I R p m D Y G o h z C R 2 E I 4 z L T Y N O S k F 2 T Q O o c H G X N B S x s Z p R n K g y o d g J 6 U j n k g H Z X k 4 o m M H j X L G U h j L Q V H 2 R 8 4 l g R G G O J u I n I F M 0 r N 3 D g p D s B N y f i Z x C G r 9 t S S R C + a g S 7 M q N f L H I 4 F j A p O a 8 O H u I T 1 7 c 6 L i o Z E N i z Y 4 J 8 U M D C Q c a T 5 b t W k n 5 C K 9 l O R S O q a l 5 2 i 6 b G q H m v X N E E 6 G 6 J m m a 6 S g f E B I + p S C 1 F D J 2 s G Q j C B A z J c a Q n D k p F A H j + 8 X y n U G H c f 3 u o W N S X O R s j n K c 3 t O z 3 X 8 j t v A s T p w 2 3 c G f c f v 9 R r A Y W V U H 2 z 1 n D 6 A b l u o K j d v x + l 1 H a / b b 8 J Y F d Z 3 H W / g r R x 0 L A h J Z r j e t u O B S X 8 A / g M f P Z 2 u 9 k 7 F 8 m D H Q V 7 X h c e g M f E q z n N 9 w P g a 2 O / Y w C p V 3 9 t 2 k N / p w a N J V k A w z O w N A O F 5 2 q j X s H c x o X J h D / y D 5 g 8 b y A V O x g t k R 7 P b H s C j 4 9 v I q p d 2 O n o q f Q f t v N + b 2 p V g s K / R D S S r I T W / c k q 9 v o 2 U e A 7 z f Q + m 4 W n f u A 1 v p z Q 5 W 8 S O H n N b O 9 L d s X F T w h i / W H h H G + t q n o N B c 8 5 4 O s N 1 w V Z 3 o P 9 m Z z 2 h 4 4 m 0 o 6 I W c M a N s G + A b s P u h 0 + 9 z l W T 8 H W s + X 5 H B 2 g A 2 z 3 i c Q x 7 W w U k z Q o V n G M B D c p 4 o h P q 4 T S F L M 4 Q o z G V d + v 4 B I T Q I U + G k P q J V A k K h J 4 V F o J f o I A m I w m T D z S q s H p m u e 4 J z 1 N F o S x 4 x e + J D U k F h 0 g N 9 K s K 0 n s K s p P m p B O S b o d h v W s c F c f e C b x L n F r 3 7 P H j c I k I w x U A y G o 0 K u a A W D U A 4 c O F V j 2 0 l Q K D c u k K S 8 u 0 l p H R S u U z b T Z U z 2 y T b 0 r 5 G 1 v + o p S / s O U H p f w A 5 H X F E K K j O P Z P V D s I C Z T D Z Y k 4 F h y W M u I i I W L X S 6 F i h V A d x j M p B N O u S Q o n 7 e A 8 g 7 p B l B f H R R t c M Y J K A B F H d a 0 v 0 J 1 1 7 + e 7 C F w o 0 b r O A Q R G X Z Y h T e c B g e o t i F 6 6 5 6 k u t l x s q A C K c 0 w h p O E d O L r 1 P i C + n A O h Z a 0 M B T v D 2 e o I w l R w n 4 7 / b P j B T E G X X E C W j o j g W E F E E U z m x V j 5 E U Q p e K J 8 m J n M Q D N r c k K 4 I P G G m j c K d V i 2 i q p e 0 Q R 8 G u W Z 5 D G c E C w Q S c 6 p 4 I k + 1 a g l B u K z j c C N g i x Z B 2 C v b k q H b m B Z N 9 2 A q y 1 u o 6 s 4 w Z q L Q u 1 x w R n 4 e 3 o V L Q P 7 A F 4 a Z x M z f V c w K + V X E d M + U g c E D m 5 n V 5 H S k A / g p A 1 b l M w C N B k Z s T 7 d 4 H O C D j E c c R s j Q z 0 B s R l\nA i l W G S 1 W I G L g N 5 n J o E p H 5 Z t m u Y i C d v S S S 1 T y H W F 2 0 L M 9 8 D 9 n Z Y I B j W V W / P v G o a c o l j c 0 Z 9 O G v U t I y A O w v 1 x L x q G q h w K c / K 7 K n 2 K x 8 1 l C h 3 Q W 0 3 z H U j H E G O e q S f 9 b S k F R s r N a W 5 j d X 2 Z u 9 K n l r k 4 g M e w i 7 Z m 8 f a P k A z m u k L R a F u w 2 4 q f 4 E W k A T u P x C f 6 v m h V 1 i 6 D P Y H y V T c k E 9 w C r c R W z z C M W V T l a b s b V U F F 1 J Y R D P 4 g u A h X G I e g W R\nF C R z T C E k S n N 7 8 L 9 u n p Y 7 S L A h W k 9 F T 5 X h E U p z f X 3 S 3 X / F C z 4 c 0 a 6 6 3 Z b / / 0 1 g 0 W D H m U x y S R E c N Z d u y 5 q T x R W E g a M V K 0 b 6 P N z U 2 0 j 6 M z P C a Z / m g H e U b S U n A c Y z G m y a 6 7 5 d P k R I 0 J j 4 k U 0 6 K K U S Q B 8 w J L 4 q A Y y 4 n I R p m D Y G o h z C R 2 E I 4 z L T Y N O S k F 2 T Q O o c H G X N B S x s Z p R n K g y o d g J 6 U j n k g H Z X k 4 o m M H j X L G U h j L Q V H 2 R 8 4 l g R G G O J u I n I F M 0 r N 3 D g p D s B N y f i Z x C G r 9 t S S R C + a g S 7 M q N f L H I 4 F j A p O a 8 O H u I T 1 7 c 6 L i o Z E N i z Y 4 J 8 U M D C Q c a T 5 b t W k n 5 C K 9 l O R S O q a l 5 2 i 6 b G q H m v X N E E 6 G 6 J m m a 6 S g f E B I + p S C 1 F D J 2 s G Q j C B A z J c a Q n D k p F A H j + 8 X y n U G H c f 3 u o W N S X O R s j n K c 3 t O z 3 X 8 j t v A s T p w 2 3 c G f c f v 9 R r A Y W V U H 2 z 1 n D 6 A b l u o K j d v x + l 1 H a / b b 8 J Y F d Z 3 H W / g r R x 0 L A h J Z r j e t u O B S X 8 A / g M f P Z 2 u 9 k 7 F 8 m D H Q V 7 X h c e g M f E q z n N 9 w P g a 2 O / Y w C p V 3 9 t 2 k N / p w a N J V k A w z O w N A O F 5 2 q j X s H c x o X J h D / y D 5 g 8 b y A V O x g t k R 7 P b H s C j 4 9 v I q p d 2 O n o q f Q f t v N + b 2 p V g s K / R D S S r I T W / c k q 9 v o 2 U e A 7 z f Q + m 4 W n f u A 1 v p z Q 5 W 8 S O H n N b O 9 L d s X F T w h i / W H h H G + t q n o N B c 8 5 4 O s N 1 w V Z 3 o P 9 m Z z 2 h 4 4 m 0 o 6 I W c M a N s G + A b s P u h 0 + 9 z l W T 8 H W s + X 5 H B 2 g A 2 z 3 i c Q x 7 W w U k z Q o V n G M B D c p 4 o h P q 4 T S F L M 4 Q o z G V d + v 4 B I T Q I U + G k P q J V A k K h J 4 V F o J f o I A m I w m T D z S q s H p m u e 4 J z 1 N F o S x 4 x e + J D U k F h 0 g N 9 K s K 0 n s K s p P m p B O S b o d h v W s c F c f e C b x L n F r 3 7 P H j c I k I w x U A y G o 0 K u a A W D U A 4 c O F V j 2 0 l Q K D c u k K S 8 u 0 l p H R S u U z b T Z U z 2 y T b 0 r 5 G 1 v + o p S / s O U H p f w A 5 H X F E K K j O P Z P V D s I C Z T D Z Y k 4 F h y W M u I i I W L X S 6 F i h V A d x j M p B N O u S Q o n 7 e A 8 g 7 p B l B f H R R t c M Y J K A B F H d a 0 v 0 J 1 1 7 + e 7 C F w o 0 b r O A Q R G X Z Y h T e c B g e o t i F 6 6 5 6 k u t l x s q A C K c 0 w h p O E d O L r 1 P i C + n A O h Z a 0 M B T v D 2 e o I w l R w n 4 7 / b P j B T E G X X E C W j o j g W E F E E U z m x V j 5 E U Q p e K J 8 m J n M Q D N r c k K 4 I P G G m j c K d V i 2 i q p e 0 Q R 8 G u W Z 5 D G c E C w Q S c 6 p 4 I k + 1 a g l B u K z j c C N g i x Z B 2 C v b k q H b m B Z N 9 2 A q y 1 u o 6 s 4 w Z q L Q u 1 x w R n 4 e 3 o V L Q P 7 A F 4 a Z x M z f V c w K + V X E d M + U g c E D m 5 n V 5 H S k A / g p A 1 b l M w C N B k Z s T 7 d 4 H O C D j E c c R s j Q z 0 B s R l\nA i l W G S 1 W I G L g N 5 n J o E p H 5 Z t m u Y i C d v S S S 1 T y H W F 2 0 L M 9 8 D 9 n Z Y I B j W V W / P v G o a c o l j c 0 Z 9 O G v U t I y A O w v 1 x L x q G q h w K c / K 7 K n 2 K x 8 1 l C h 3 Q W 0 3 z H U j H E G O e q S f 9 b S k F R s r N a W 5 j d X 2 Z u 9 K n l r k 4 g M e w i 7 Z m 8 f a P k A z m u k L R a F u w 2 4 q f 4 E W k A T u P x C f 6 v m h V 1 i 6 D P Y H y V T c k E 9 w C r c R W z z C M W V T l a b s b V U F F 1 J Y R D P 4 g u A h X G I e g W R B s c q w x r d t 5 X C z Q h u L J W r D / d S z b 6 P N\nx k t / y + t t u S + 6 6 / e O / i p v q t d b P 7 R + a t 1 p e a 1 + 6 1 7 r 1 9 Z + 6 6 g V t d 6 1 / m 7 9 0 / p 3 7 f u 1 e 2 u P 1 3 4 r o R 9 d m 9 1 u v 2 3 V f m s H / w O 5 n Z i u < / l a t e x i t >\n\u21e7 \u2327 = { 21 } s1 s0 a2 a1 a1 a2 (b) Entailment Case\nFigure 2: Two CMPs in which there is a SOAP that is not expressible under any Markov reward function. On the left, \u03a0 G = {\u03c0 21 } is not realizable, as \u03c0 21 can not be made better than \u03c0 22 because s 1 is never reached. On the right, the XOR-like-SOAP, \u03a0 G = {\u03c0 12 , \u03c0 21 } is not realizable: To make these two policies optimal, it is entailed that \u03c0 22 and \u03c0 11 must be optimal, too. explicitly excludes preferences of this sort. On the right, we find a more interesting case: The chosen SOAP is similar to the XOR function, \u03a0 G = {\u03c0 12 , \u03c0 21 }. Here, the task requires that the agent choose each action in exactly one state. However, there cannot exist a reward function that makes only these policies optimal, as by consequence, both policies \u03c0 11 and \u03c0 22 must be optimal as well.\nNext, we show that Theorem 4.1 is not limited to a particular choice of transition function or \u03b3. Proposition 4.2. There exist choices of E \u00acT = (S, A, \u03b3, s 0 ) or E \u00ac\u03b3 = (S, A, T, s 0 ), together with a task T , such that there is no (T, R) pair that realizes T in\nE \u00acT or (R, \u03b3) in E \u00ac\u03b3 .\nThis result suggests that the scope of Theorem 4.1 is actually quite broad-even if the transition function or \u03b3 are taken as part of the reward specification, there are tasks that cannot be expressed. We suspect there are ways to give a precise characterization of all inexpressible tasks from an axiomatic perspective, which we hope to study in future work.", "publication_ref": ["b39"], "figure_ref": [], "table_ref": []}, {"heading": "Constructive Algorithms: Task to Reward", "text": "We now analyze how to determine whether an appropriate reward function can be constructed for any (E, T ) pair. We pose a general form of the reward-design problem [34,51,9] as follows. Definition 4.1. The REWARDDESIGN problem is: Given E = (S, A, T, \u03b3, s 0 ), and a T , output a reward function R alice that ensures T is realized in M = (E, R alice ).\nIndeed, for all three task types, there is an efficient algorithm for solving the reward-design problem. Theorem 4.3. The REWARDDESIGN problem can be solved in polynomial time, for any finite E, and any T , so long as reward functions with infinitely many outputs are considered.\nTherefore, for any choice of finite CMP, E, and a SOAP, PO, or TO, we can find a reward function that perfectly realizes the task in the given environment, if such a reward function exists. Each of the three algorithms are based on forming a linear program that matches the constraints of the given task type, which is why reward functions with infinitely many outputs are required. Pseudo-code for SOAP-based reward design is presented in Algorithm 1. Intuitively, the algorithms compute the discounted expected-state visitation distribution for a collection of policies; in the case of SOAP, for instance, these policies include \u03a0 G and what we call the \"fringe\", the set of policies that differ from a \u03c0 g \u2208 \u03a0 G by exactly one action. Then, we use these distributions to describe linear inequality constraints ensuring that the start-state value of the good policies are better than those of the fringe.\nAs highlighted by Theorem 4.1 there are SOAPs, POs, and TOs that are not realizable. Thus, it is important to determine how the algorithms mentioned in Theorem 4.3 will handle such cases. Our next corollary illustrates that the desirable outcome is achieved: For any E and T , the algorithms will output a reward function that realizes T in E, or output '\u22a5' when no such function exists. Corollary 4.4. For any task T and environment E, deciding whether T is expressible in E is solvable in polynomial time.\nTogether, Theorem 4.1 and Theorem 4.3 constitute our main results: There are environment-task pairs in which Markov reward cannot express the chosen task for each of SOAPs, POs, and TOs. However, there are efficient algorithms for deciding whether a task is expressible, and for constructing the realizing reward function when it exists. We will study the use of one of these algorithms in Section 5, but first attend to other aspects of reward's expressivity.", "publication_ref": ["b33", "b50", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 1 SOAP Reward Design", "text": "INPUT: E = (S, A, T, \u03b3, s 0 ), \u03a0 G . OUTPUT: R, or \u22a5.\n1: \u03a0 fringe = compute_fringe(\u03a0 G ) 2: for \u03c0 g,i \u2208 \u03a0 G do\nCompute state-visitation distributions.\n3:\n\u03c1 g,i = compute_exp_visit(\u03c0 g,i , E) 4: for \u03c0 f,i \u2208 \u03a0 fringe do 5: \u03c1 f,i = compute_exp_visit(\u03c0 f,i , E) 6: C eq = {} Make Equality Constraints. 7: for \u03c0 g,i \u2208 \u03a0 G do 8: C eq .add(\u03c1 g,0 (s 0 ) \u2022 X = \u03c1 g,i (s 0 ) \u2022 X) 9: C ineq = {}\nMake Inequality Constraints. 10: for \u03c0 f,j \u2208 \u03a0 fringe do 11:\nC ineq .add(\u03c1 f,j (s 0 ) \u2022 X + \u2264 \u03c1 g,0 (s 0 ) \u2022 X) 12: R out , out = linear_programming(obj. = max , constraints = C ineq , C eq )\nSolve LP.\n13: if out > 0 then Check if successful. return R out 14: else return \u22a5", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Other Aspects of Reward's Expressivity", "text": "We next briefly summarize other considerations about the expressivity of reward. As noted, Theorem 4.3 requires the use of a reward function that can produce infinitely many outputs. Our next result proves this requirement is strict for efficient reward design. Theorem 4.5. A variant of the REWARDDESIGN problem with finite reward outputs is NP-hard.\nWe provide further details about the precise problem studied in Appendix B. Beyond reward functions with finitely-many outputs, we are also interested in extensions of our results to multiple environments.\nWe next present a positive result indicating our algorithms can extend to the case where Alice would like to design a reward function for a single task across multiple environments. Proposition 4.6. For any SOAP, PO, or TO, given a finite set of CMPs, E = {E 1 , . . . , E n }, with shared state-action space, there exists a polynomial time algorithm that outputs one reward function that realizes the task (when possible) in all CMPs in E.\nA natural follow up question to the above result asks whether task realization is closed under a set of CMPs. Our next result answers this question in the negative. Theorem 4.7. Task realization is not closed under sets of CMPs with shared state-action space.\nThat is, there exist choices of T and\nE = {E 1 , . . . , E n } such that T is realizable in each E i \u2208 E\nindependently, but there is not a single reward function that realizes T in all E i \u2208 E simultaneously.\nIntuitively, this shows that Alice must know precisely which environment Bob will inhabit if she is to design an appropriate reward function. Otherwise, her uncertainty over E may prevent her from designing a realizing reward function. We foresee iterative extensions of our algorithms in which Alice and Bob can react to one another, drawing inspiration from repeated IRL by Amin et al. [4].", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We next conduct experiments to shed further light on the findings of our analysis. Our focus is on SOAPs, though we anticipate the insights extend to POs and TOs as well with little complication. In the first experiment, we study the fraction of SOAPs that are expressible in small CMPs as we vary aspects of the environment or task (Figure 3). In the second, we use one algorithm from Theorem 4.3  SOAP Expressivity. First, we estimate the fraction of SOAPs that are expressible in small environments. For each data point, we sample 200 random SOAPs and run Algorithm 1 described by Theorem 4.3 to determine whether each SOAP is realizable in the given CMP. We ask this question for both the equal (color) variant of SOAP realization and the range (grey) variant. We inspect SOAP expressivity as we vary six different characteristics of E or \u03a0 G : The number of actions, the number of states, the discount \u03b3, the number of good policies in each SOAP, the Shannon entropy of T at each (s, a) pair, and the \"spread\" of each SOAP. The spread approximates average edit distance among policies in \u03a0 G determined by randomly permuting actions of a reference policy by a coin weighted according to the value on the x-axis. We use the same set of CMPs for each environment up to any deviations explicitly made by the varied parameter (such as \u03b3 or entropy). Unless otherwise stated, each CMP has four states and three actions, with a fixed but randomly chosen transition function.\nResults are presented in Figure 3. We find that our theory is borne out in a number of ways. First, as Theorem 4.1 suggests, we find SOAP expressivity is strictly less than one in nearly all cases. This is evidence that inexpressible tasks are not only found in manufactured corner cases, but rather that expressivity is a spectrum. We further observe-as predicted by Proposition 3.1-clear separation between the expressivity of range-SOAP (grey) vs. equal-SOAP (color); there are many cases where we can find a reward function that makes the good policies near optimal and better than the bad, but cannot make those good policies all exactly optimal. Additionally, several trends emerge as we vary the parameter of environment or task, though we note that such trends are likely specific to the choice of CMP and may not hold in general. Perhaps the most striking trend is in Figure 3f, which shows a decrease in expressivity as the SOAPs become more spread out. This is quite sensible: A more spread out SOAP is likely to lead to more entailments of the kind discussed in Figure 2b.\nLearning with SOAP-designed Rewards. Next, we contrast the learning performance of Qlearning under a SOAP-designed reward function (visualized in Figure 4a) with that of the regular goal-based reward in the Russell and Norvig [42] grid world. In this domain, there is 0.35 slip probability such that, on a 'slip' event, the agent randomly applies one of the two orthogonal action effects. The regular goal-based reward function provides +1 when the agent enters the terminal flag cell, and \u22121 when the agent enters the terminal fire cell. The bottom left state is the start-state, and the black cell is an impassable wall.     Results are presented in Figure 4. On the right, we present a particular kind of learning curve contrasting the performance of Q-learning with the SOAP reward (blue) and regular reward (green). The y-axis measures, at the end of each episode, the average (inverse) minimum edit distance between Q-learning's greedy policy and any policy in the SOAP. Thus, when the series reaches 1.0, Q-learning's greedy policy is identical to one of the two SOAP policies. We first find that Q-learning is able to quickly learn a \u03c0 g \u2208 \u03a0 G under the designed reward function. We further observe that the typical reward does not induce a perfect match in policy-at convergence, the green curve hovers slightly below the blue, indicating that the default reward function is incentivizing different policies to be optimal. This is entirely sensible, as the two SOAP policies are extremely cautious around the fire; they choose the orthogonal (and thus, safe) action in fire-adjacent states, relying on slip probability to progress. Lastly, as expected given the amount of knowledge contained in the SOAP, the SOAP reward function allows Q-learning to rapidly identify a good policy compared to the typical reward.\nJ i w v B a U j l h E f 7 7 R 2 H R 0 M 1 Y S J k q Z z l Z Y x i E Z i X N G U W C W k 6 l c k 4 s Q j 4 x Q M 3 h B a h Y Y J i 3 U i n h S C Z h R 4 0 g o m Q v J A F k z h h G V A V I 7 A T 8 7 G I U o s k m T f m E 4 u M s y C I Y S y L + M n v m U g Z j D C i y V R m A c h S f v 6 H R T w P 7 H h C n K f U A z V + r U h k M r D I l V 7 S C v n T s a Q h g 0 l N x W j / h J + / H a p w p G W j v A n O i W k A B i\nt I 0 T M Z D o G 7 Y I E r v u Y m C 6 9 l K a F k T C N / B g f Y K 0 J 1 + i D p F E Z J d o x U B A C d h 9 + f i r q Y i v T E g d H 8 E I 8 y L S q Y m V K d e 0 0 h p t y S b P U J t Q U F p U 5 z i Z n / s J r / 0 D M T I P h q T Z a D 0 l k E T F K J H r i 2 Z G G s Y 0 X 9 v G p X M L 6 I 3 j E / R Z / C p l y 2 D c x i s 5 r 7 d o k I K J b n A / 2 q a N g V D f V h + t G 8 U d E G D N y Z q 6 f 6 V d H E p U C v R / 0 C J Y v t W t", "publication_ref": ["b41"], "figure_ref": ["fig_7", "fig_7", "fig_7", "fig_12", "fig_12"], "table_ref": []}, {"heading": "t Y X A i G I Z U T H u 2 3 d x w e n a g J E y F L 5 S w v Y x S L w L y k K b N I S N O p T M a J R c A r H j g h t A g N E x T r R j o t B M k s 9 K A R T I T k h S y Y x", "text": "A n L g K o Y g Z 2 Y j 0 W U W i T J v D G f W G S c B U E M Y\nO i m c N N 3 z B O o G U 3 Y Y 5 k 1 w x R g q A U Q c x 3 N C T u 5 s 2 H f v E X B h S j Y w B z A Y d V W H k M 5 D B u V b M l y 6 F z F W W y G 3 l A v V O e Q Q 0 v B 2 L W x 9 C E g v F 0 B o G S v", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We have here investigated the expressivity of Markov reward, framed around three new accounts of task. Our main results show that there exist choices of task and environment in which Markov reward cannot express the chosen task, but there are efficient algorithms that decide whether a task is expressible and construct a reward function that captures the task when such a function exists. We conclude with an empirical examination of our analysis, corroborating the findings of our theory. We take these to be first steps toward understanding the full scope of the reward hypothesis.\nThere are many routes forward. A key direction moves beyond the task types we study here, and relaxes our core assumptions-the environment might not be a finite CMP, Alice may not know the environment precisely, reward may be a function of history, or Alice may not know how Bob represents state. Along similar lines, a critical direction incorporates how reward impacts Bob's learning dynamics rather than start-state value. Further, we note the potential relevance to the recent reward-is-enough hypothesis proposed by Silver et al. [44]; we foresee pathways to extend our analysis to examine this newer hypothesis, too. For instance, in future work, it is important to assess whether reward is capable of inducing the right kinds of attributes of cognition, not just behavior.\n(Q3) Why restrict attention to SOAPs, POs, and TOs?\nA: First, we recognize these do not necessarily capture all of what we hope to convey to learning agents. It is an important next step in our work to enrich the analysis with more general objectives. Still, we believe that these each represent an interesting, relatively general, and concrete template for what a task might look like. They are quite flexible: SOAPs can be simple while POs and TOs can be complex. (Q4) Why restrict to the start-state value?\nA: We adopt start-state value due to its simplicity. Other considerations might be: (1) The expected value under some chosen distribution, or (2) That the constraint hold over all states (so, for SOAPs, each \u03c0 g is better than each \u03c0 b in value for all states). We note that the former case is identical to start-state value, as we can always add a start-state to any CMP where all actions lead to the desired next-state distribution in T . The latter case is slightly more complicated, so we chose not to focus on it as we prefer the simplicity of the start-state case. However, we note that many inexpressible tasks under the start-state criterion remain inexpressible under the \"all-state\" criterion (such as the XOR example from Figure 2b).", "publication_ref": ["b43", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "B Proofs", "text": "We next restate each central result, and present its proof. Proposition 3.1. There exists a CMP, E, and choice of \u03a0 G such that \u03a0 G can be realized under the range-SOAP criterion, but cannot be realized under the equal-SOAP criterion. Consider the example in Figure 5 and the SOAP \u03a0 G = {\u03c0 11 , \u03c0 21 , \u03c0 12 }. This \u03a0 G indicates that all policies are acceptable except the policy that always takes a 2 . That is, the policy subscripts denote which actions each policy takes in each state (\u03c0 12 means a 1 in s 0 , a 2 in s 1 ).\nFirst, let us note that range SOAP is realizable: The listed rewards allow for each policy in \u03a0 G to obtain RMAX/2 1\u2212\u03b3 value or better, while \u03c0 22 achieves zero. Letting \u03b5 = VMAX/2, this choice of rewards satisfies the criteria, and the \u03a0 G is \u03b5-realized in the given MDP.\nNext, note that there can exist no other choice of rewards that realize the equal SOAP. That is, such that V \u03c011 (s 0 ) = V \u03c012 (s 0 ) = V \u03c021 (s 0 ) > V \u03c022 (s 0 ). This fact is a consequence of the tie in values between the policies. Here, we see that any choice of rewards that makes V \u03c012 (s 0 ) = V \u03c021 (s 0 ) will also give \u03c0 22 that same value. Thus, the given \u03a0 G is unrealizable under equal SOAP. We proceed by proving the existence of a pair (E, T ), for each of T as a SOAP, PO, or TO. Indeed, we find that the simple XOR case is inexpressible for all three task types.", "publication_ref": [], "figure_ref": ["fig_13"], "table_ref": []}, {"heading": "SOAP.", "text": "For SOAP, we consider the CMP with two states and two actions from Figure 5, and the SOAP \u03a0 G = {\u03c0 12 , \u03c0 21 }. That is, the chosen task is for the learning agent to find a policy that chooses each action in exactly one state. Here, we find that any Markov reward function that makes a 1 optimal in the left state will, by consequence, make a 1 an optimal action no matter what is done in other states. In other words, we cannot assign rewards to (s, a) pairs so that an action's optimality depends on which optimal action is taken in the other state. Thus, all choices of Markov reward function that make {\u03c0 12 , \u03c0 21 } optimal will also make {\u03c0 11 , \u03c0 22 } optimal, too.\nPO. Since the given SOAP is a special case of a PO, we have already identified a given inexpressible PO.\nTO. For TO, for simplicity we consider the same CMP. We let N = 2, and suppose that the desired trajectory ordering is over state-action pairs, giving rise to a set of good trajectories, and a set of bad trajectories:\nL \u03c4,N := {\u03c4 G , \u03c4 B }, (B.1) \u03c4 G = {{(s 0 , a 1 ), (s 1 , a 2 )}, {(s 0 , a 2 ), (s 1 , a 1 )}} , (B.2) \u03c4 B = {{(s 0 , a 1 ), (s 1 , a 1 )}, {(s 0 , a 2 ), (s 1 , a 2 )}} . (B.\n3)\nThe same reasoning from the above cases applies: We cannot make the good trajectories strictly higher in return than the bad trajectories.\nProposition 4.2. There exist choices of E \u00acT = (S, A, \u03b3, s 0 ) or E \u00ac\u03b3 = (S, A, T, s 0 ), together with a task T , such that there is no (T, R) pair that realizes T in\nE \u00acT or (R, \u03b3) in E \u00ac\u03b3 .\nProof of Proposition 4.2.\nThe running XOR example is actually inexpressible for all choices of T , or of \u03b3. That is, there is no way to make \u03c0 12 and \u03c0 21 strictly better than both \u03c0 22 and \u03c0 11 by varying \u03b3 or T . Such examples likely exist for any choice of S and A of size greater than one.\nTheorem 4.3. The REWARDDESIGN problem can be solved in polynomial time, for any finite E, and any SOAP, PO, or TO, so long as a reward-function family with infinitely many outputs is used.\nProof of Theorem 4. We proceed by constructing a linear program (LP) whose solution is the desired reward function (or correctly outputs that there is no such reward function). Specifically, note that we want to choose a reward function so that all the policies in \u03a0 G have strictly higher start-state value than all the policies not in the set. We present the proof through five observations.\nFirst, observe that any reward function that will induce the desired ordering ensures that the optimal policy \u03c0 * is in the set \u03a0 G . This is true since \u03c0 * (under the chosen reward function) is better than all policies. So it is better than all policies not in \u03a0 G .\nSecond, note that the set \u03a0 G is well connected in the following sense. Let a step in policy space from some reference policy \u03c0 ref to be a move to any other deterministic policy that differs from \u03c0 ref in exactly one state. Then, for any pair of policies in \u03a0 G , there must be a sequence of policies in \u03a0 G , each one step apart from the next, from one to the other. This follows from the policy-improvement theorem: we can get from any policy to an optimal policy in a sequence of policies such that each policy ( 1) is one step from the previous one and (2) strictly dominates the previous one. Since any policy that strictly dominates a policy in \u03a0 G must be better than the policy in \u03a0 G , it must also be in \u03a0 G (if the problem constraint is satisfied). That means if we choose two policies in \u03a0 G , \u03c0 1 and \u03c0 2 , both can reach \u03c0 * in a sequence of single steps while staying within \u03a0 G . Since steps are symmetric, \u03a0 G is connected.\nThe connected set of policies in \u03a0 G has a \"fringe\" \u03a0 fringe -a set of policies not in \u03a0 G that are one step from a policy in \u03a0 G .\nThird, for the constraints of the problem to be satisfied, every policy \u03c0 g \u2208 \u03a0 G must be strictly better than every policy \u03c0 f \u2208 \u03a0 fringe .\nFourth, observe that |\u03a0 fringe | <= |A||\u03a0 G |, so \u03a0 fringe is polynomial sized.\nFifth, we can construct a polynomial-sized LP that expresses that every policy \u03c0 g \u2208 \u03a0 G is strictly better than every policy \u03c0 f \u2208 \u03a0 fringe . Note that the direct way to build this LP has a \"strictly better than\" comparison between each policy \u03c0 f \u2208 \u03a0 fringe and each policy \u03c0 \u2208 \u03a0 G .\nThat's at most |A||\u03a0 G | 2 inequalities.\nWe now tie the above observations together to show that the solution to this LP solves the constraints of the problem. That is, the reward function returned makes it so every policy \u03c0 g \u2208 \u03a0 G is strictly better than every policy not in \u03a0 G , and no valid reward function is excluded (so, if a solution exists, it will be found).\nThe argument that no valid reward function is excluded is simply because the set of constraints in the LP is a subset of the defining constraints of the problem. Specifically, the LP constrains the policies inside \u03a0 G to be strictly better than the ones on the fringe instead of all policies not in \u03a0 G .\nThe argument that only constraining the values on the fringe automatically constrains all the others proceeds as follows. First, with respect to the returned reward function, there is some optimal policy \u03c0 * . That policy \u03c0 * must be in \u03a0 G . To see why, let us assume it is not. That means there is a sequence of improving steps that turn a policy in \u03a0 G to \u03c0 * (currently assumed to be out of \u03a0 G ). But, any such sequence must go through the fringe, and we constrained the fringe so that all of the policies in \u03a0 G are strictly better than them. So, \u03c0 * must be in \u03a0 G .\nNext, we know that all policies in \u03a0 G must be strictly better than the policies not in \u03a0 G . To see why, consider an \"improving\" path from some policy \u03c0 b \u2208 \u03a0 G to \u03c0 * . Since \u03c0 * is in \u03a0 G , we know this path must go through some policy \u03c0 f \u2208 \u03a0 fringe . Since it's an improving path, that means \u03c0 f is better than \u03c0 b . But, every policy in \u03a0 G is strictly better than \u03c0 f , so it must also be strictly better than \u03c0 b .\nLemma B.2. The PO variant of REWARDDESIGN can be solved in polynomial time.", "publication_ref": [], "figure_ref": ["fig_13"], "table_ref": []}, {"heading": "Proof of Lemma B.2.", "text": "We proceed by constructing a procedure that calls a linear program whose answer is the reward function that induces the given L \u03a0 in M , or the procedure correctly outputs that there is no such R.\nConsider the set of policies in \u03a0, numbered \u03c0 1 , . . . , \u03c0 i , . . . , \u03c0 N . Note that the value of \u03c0 i in s 0 can be computed in terms of the expected reward under the policy's discounted expected state-action visitation distribution. That is, for each \u03c0 i , let Since the given MDP is assumed to have finite state-action space, note that \u03c1 i may be interpreted as a vector whose elements correspond to \u03c1 i (s 0 , a 0 ), \u03c1 i (s 0 , a 1 ), and so on.\nThe value of \u03c0 i under a given reward function R (which may also be interpreted as a vector) is then produced by the dot product R \u2022 \u03c1 i .\nGiven L \u03a0 , we want to find an R that ensures a set of linear constraints hold:\nR \u2022 \u03c1 0 \u2265 R \u2022 \u03c1 1 \u2265 . . . . (B.5)\nNote that the trivial reward function, R \u2205 : s \u2192 0, is a solution to the above linear program. However, we can ensure some minimal increment improvement of for non-tying policies, where\nR \u2022 \u03c1 0 \u2265 R \u2022 \u03c1 1 + \u2265 . . . . (B.6\n) This minimal increment is sufficient to separate policies with tying scores and avoids the degenerate solution of R \u2205 , so long as there are infinitely many reward outputs feasible.\nNote that the input is of size N , where N is the number of constraints imposed on the policy ordering. In the worst case, N = |L \u03a0 | \u2264 |A| |S| . If there are fewer constraints than either |A| or |S|, then N = max{|S|, |A|}. The amount of computational work required is split across two steps: 1.\u00d5(N ): Compute \u03c1 i for each policy \u03c0 1 . . . \u03c0 N .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "O(N 3", "text": "): Formulate and solve the above linear program. Thus, since the described linear program can be constructed in polynomial time outputs a reward function that induces the given L \u03a0 , we conclude the PO case.\nLemma B.3. The TO variant of REWARDDESIGN can be solved in polynomial time.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Lemma B.3.", "text": "The algorithm follows the same construction as those catered toward SOAPs and POs, but is in fact much simpler.\nWe can form linear inequality constraints on the return of two trajectories as follows. First recall that the N -step discounted return of a trajectory \u03c4 is\nG(\u03c4 ; s 0 ) = N \u22121 i=0 \u03b3 i R(s i , a i ), (B.7)\nassuming reward is a function of state and action for simplicity. Note that because reward functions are assumed to be deterministic, the quantity G(\u03c4 ; s 0 ) is not a random variable. Now, given two trajectories,\n\u03c4 i = {(s (i) 0 , a (i) 0 ), . . . , (s (i) N \u22121 , a (i) N \u22121 )}, \u03c4 j = {(s (j) 0 , a (j) 0 ), . . . , (s (j) N \u22121 , a(j)\nN \u22121 )}, (B.8) note that we can express linear inequality constraints as follows,\n\u03c4 i \u2022 R \u2212 \u03c4 j \u2022 R \u2264 , (B.9)\nwhere R is a length N state-action vector, and \u2208 Q \u22650 is a slack variable to be maximized as part of the optimization. Inequality constraints follow the same structure, only simpler. By the same reasoning that underlies the construction of the SOAP and PO based algorithms, the above set of constraints define a linear program whose solution is the realizing reward function, if it exists. For each of PO and TO, the constraints we construct define precisely the space of constraints that constitute the given task. Thus, since linear programming will find a solution for the given constraint set, we know that these two forms of the algorithm will also correctly decide when no reward function exists.\nThe SOAP case is slightly more involved, but still relatively straightforward. We note that the policy fringe, \u03a0 fringe , is a subset of \u03a0 B , since \u03a0 fringe consists of some policies not in \u03a0 G by construction. This means that the constraints produced that separate each good policy from a fringe policy in value are a subset of the true constraints (those that separate each \u03c0 g \u2208 \u03a0 G from each \u03c0 b \u2208 \u03a0 B ). Hence, since constraint relaxations of this kind have the property that they do not exclude solutions, we conclude that our proposed linear program will correctly determine when no satisfying reward function exists.\nNext, we provide further details on Theorem 4.5 that examines reward design when only finitely many reward outputs may be used. As noted in the main text, Theorem 4.3 requires that Alice is allowed to design a reward function that can produce infinitely many outputs. It is natural to wonder whether this requirement is strict. Theorem 4.5 answers this question in the affirmative, by proving that the following decision problem is hard. Definition B.1. The FINITE-PO-REWARDDECISION problem is defined as follows: Given E = (S, A, T, \u03b3, s 0 ), and a set of k policy inequalities (\u03c0 xi < \u03c0 yi ), output True iff there is a reward function R(s ) that induces the given policy inequalities.\nNote that this formulation focuses on POs, and on reward as a function of next-state. Unfortunately, we find this problem is NP-hard, showing that for reward design to be efficient, infinitely many reward outputs are needed. Theorem 4.5. The FINITE-PO-REWARDDECISION problem is NP-hard.\nProof of Theorem 4.5.\nWe assume every T (s | s, a) is expressed as a rational number. We also assume that all policies are deterministic, Markov policies, although results should extend to stochastic policies with rational probabilities as well.\nThe binary PO problem is the same, but it insists that every R(s ) \u2208 {0, 1} for all s in the returned reward function.\nObservation 1: The binary PO decision problem is in NP. We can guess an assignment of R(s, a) to either 0 or 1, then evaluate in inequality using linear equation solving as policy evaluation.\nWe show that the binary PO decision problem can be used to decide the NP-hard monotone clause 3-SAT problem with a polynomial reduction.\nA monotone clause 3-SAT problem consists of a set of n variables, and m clauses. Each clause consists of three variables and is either a positive clause or a negative clause. In a positive clause, all three variables appear as literals. In a negative clause, all three variables appear as negated literals. The problem is the same as the standard 3-SAT problem except for the restriction that we cannot mix positive and negative literals in a clause.\nWe can convert an instance of monotone clause 3-SAT to the binary PO problem as follows.\nThere is only one state where decisions are possible. It is the initial state of the MDP. Each action from this state results in an action-specific probabilistic transition to a set of terminal states, each of which is associated with a terminal reward value.\nBecause of the simple structure of this MDP, each policy corresponds to an action and vice versa. And, each terminal state corresponds to a reward and vice versa. So, each policy can be viewed as a convex combination of rewards.\nWe create two terminal states s 0 and s 1 and create one action (a 0 ) that transitions directly to s 0 and one (a 1 ) that transitions directly to s 1 . We then add a policy constraint that says the a 0 < a 1 . Because all rewards are in {0, 1}, that forces the reward for s 0 to be 0 and for s 1 to be 1. Those become our logical primitives, in a sense.\nNext, we add 2n states, one for each positive and negative literal in the 3-SAT problem. For each variable v, we add an action a v with a 50-50 transition to s v and s v , along with two constraints: a 0 < a v < a 1 . These constraints ensure that the reward assignment to s v and s v can be interpreted as an assignment to the literals where one gets a 1 and the other gets a zero.\nThere is no other way to satisfy these constraints. Now, for each positive clause c consisting of variables v 1 , v 2 , and v 3 , we create an action a c that transitions to s v1 , s v2 , and s v3 with equal probability. We add a policy constraint that a c > a 0 , forcing the assignment of rewards to the variables to correspond to a satisfying assignment for that clause. (At least one of the rewards needs to be set to 1.)\nFor the each negative clause c consisting of variables v 1 , v 2 , and v 3 , we create an action a c that transitions to s v1 , s v2 , and s v3 with equal probability. We add a policy constraint that a c < a 1 , forcing the assignment of rewards to the variables to correspond to a satisfying assignment for that clause. (At least one of the rewards needs to be set to 0.) By the way the MDP is constructed, the constraints are satisfied if and only if the rewards represent a satisfying assignments for the given monotone clause 3-SAT formula. Therefore, an efficient solution to the binary PO decision problem would provide an efficient solution to the NP-hard monotone clause 3-SAT problem.\nProposition 4.6. For any SOAP, PO, or TO, given a finite set of CMPs E = {E 1 , . . . , E n } with shared state-action space, there exists a polynomial time algorithm that outputs a single reward function that realizes the task (when possible) in each CMP in E.\nProof of Proposition 4.6.\nFrom Theorem 4.3, we know that there is an algorithm to solve the reward design problem for any task and a single environment, in polynomial time. We form the multi-environment algorithm by simply combining the constraints formed by each individual linear program. By the properties of linear programming, the resulting solution will either satisfy all of the given constraints, as desired, or will correctly identify that no such satisficing solution exists.\nTheorem 4.7. Task realization is not closed under sets of CMPs with shared state-action space. That is, there exist choices of T and E = {E 1 , . . . , E n } such that T is realizable in each E i \u2208 E independently, but there is not a single reward function that realizes T in all E i \u2208 E simultaneously.\nProof of Theorem 4.7.\nWe consider a pair of CMPs, (E X , E Y ), with the same three states and two actions. We will show that there exists choice of T such that T is realizable in E X and E Y independently, but not in both simultaneously. Our result assumes we restrict to reward functions that are only a function of state, but we suspect similar cases exist for reward functions on (s, a) pairs and (s, a, s ) triples.\nConsider the pair of CMPs in Figure 6. These two CMPs share a state-action space and start-state, but not a transition function (and, say, a \u03b3 > 0.5). Let us further suppose we are interested in the SOAP \u03a0 G = {\u03c0 112 , \u03c0 212 }, that is, the policies that take a 1 in s 1 , and a 2 in s 2 . In both CMPs, the transition function from s 0 transitions to s 1 with probability 0.5 and s 2 with probability 0.5, for both actions.\nWe first show that this \u03a0 G is realizable in both CMPs. For the CMP on the left, note that the reward function R(s 1 ) = 1, R(s 2 ) = \u22121, with \u03b3 = 0.95 will ensure V \u03c0112 (s 0 ) = V \u03c0212 (s 0 ), and that both policies are strictly better than all others. Next, note that the same is true for the example on the right where R(s 1 ) = \u22121, R(s 2 ) = 1. Thus, \u03a0 G is independently realizable in both of these CMPs.\nHowever, there cannot exist a reward function that makes \u03c0 112 and \u03c0 212 strictly better than all other policies in both CMPs-it is either better to stay in s 1 , or to stay in s 2 , but it cannot be the case that both are true simultaneously.  ", "publication_ref": [], "figure_ref": ["fig_19"], "table_ref": []}, {"heading": "C Experimental Details", "text": "Next, we provide further details about our experiments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 Expressibility Experiments", "text": "First, we provide additional information about the first experiment that explores the fraction of SOAPs that are expressible in small CMPs.\nSix Variants. In each figure, we vary one aspect of the environment or task along the x-axis. Most of these are self-explanatory (a: number of actions, b: number of states, c: \u03b3), though the plots in (d), (e) and (f) are slightly more involved. In (d), we vary the size of each sampled SOAP, corresponding to the number of good policies in the SOAP. That is, a SOAP of size one consists only of a single good policy, \u03a0 G = {\u03c0 * }. In (e), we vary the Shannon entropy of the transition function on a per state-action basis as per: H(T ) = \u2212 s \u2208S log 2 T (s | s, a). This is accomplished by interpolating between the fully deterministic transition function that only transitions to a single next-state and the uniform random transition function through a simple soft-max distribution in which one next-state is the intended transition, while each other next-state receives a small amount of probability mass depending on the given entropy. In plot (f), we vary the spread of the SOAP, which is a measure of how different the good policies in the SOAP are on average. The x-axis corresponds to an approximate edit-distance between policies, where each point on the x-axis defines the parameter of a coin that we flip to determine whether to change a chosen reference policy's action for each state. So, we first randomly sample one policy, say \u03c0 1 . Then, we construct the next policy for the SOAP as follows: For a given coin weight \u03b8, we flip a coin at each state of the CMP. If the coin lands heads (the trial is successful), then we change the action of the new policy to a fixed action chosen uniformly at random. Thus, when \u03b8 is zero, the SOAP will only contain \u03c0 1 . When \u03b8 approaches one, the SOAP will likely contain many different policies.\nEnvironment Details. In each case, unless otherwise specified, the underlying environment is a four state, three action CMP with \u03b3 = 0.95, and a transition function that is a multinomial over next-states sampled from a Dirichilet-multinomial distribution with \u03b1 parameters set to 1 |S| . When not specified, the size of each SOAP is two. We varied many aspects of these parameters and found little change in the nature of the plots, though trends will be shifted up or down in many cases. For instance, given the downward trend of Figure (3d) as the SOAP size increases, we know that the remaining plots will each be scaled downward if we were to run the same experiment for a SOAP size larger than two. We sample random SOAPs by first sampling a SOAP size randomly between 1 and |\u03a0|. Then, we sample N SOAPS of the chosen size uniformly at random (unless otherwise specified, as in the case of Figure (3f)).", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "C.2 Learning Experiments", "text": "In the grid environment, we set slip probability of 0.35 for all (non-terminal) states. When a \"slip\" event occurs, the action effect is orthogonal to the intended direction. For instance, in the bottom left cell, if the up action is executed, there is a 0.175 chance the agent will execute left (thus staying in the bottom right cell), and a 0.175 chance the agent will execute right, and a 0.65 chance the agent will move up a cell. We experiment with tabular Q-learning with -greedy exploration, with = 0.2 and learning rate \u03b1 = 0.1 and no annealing. Each episode consists of 10 steps in the environment, with 250 episodes per algorithm. We repeat the experiment 50 times and report 95% confidence intervals. The y-axis measures, at the end of each episode, the (inverse) minimum edit distance between Q-learning's greedy policy and any of the policies in the SOAP along the trajectory taken by Q-learning's greedy policy. Thus, when the series reaches 1.0, Q-learning's greedy policy is identical to one of the SOAP policies in the states that the greedy policy will reach. We observe that the gap between the blue and green curves is due to the different kinds of policies that the SOAP reward and the regular reward promote-one is not necessarily better or worse than the other, they just convey different kinds of objectives.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments and Disclosure of Funding", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Anticipated Questions", "text": "We first address questions that might arise in response to the main text.\n(Q1) What does it mean for Bob to *solve* one of these tasks? That is, if Alice chooses a SOAP, PO, or TO for Bob to learn to solve, when can Alice determine Bob has solved the task? A: Bob can be said to be doing better on a given task if his behavior improves, as is typical in evaluating behavior under reward. The difference with SOAPs, POs, and TOs is that we measure improvement relative to the task rather than reward. For instance, given a SOAP, we might say that Bob has solved the task once he has found one of the good policies, and we might measure Bob's progress on a task in terms of the distance of his greedy policy to one of the good policies (as done in our learning experiments). The same reasoning applies to POs and TOs: Bob is doing better on a task in so far as his greedy policy (or trajectories) is (are) higher up the ordering.\n(Q2) These notions of inexpressibility all come about due to the Markov restriction on reward functions. That is, the studied reward functions must be a function of s, (s, a), or (s, a, s ).\nBut, what about history-based reward functions? A: Indeed, as discussed in our introduction, our goal is to examine the expressivity of Markov rewards in the context of finite MDPs. We assume the environment is fixed and given to Alice with the state and action spaces already determined. While it is sensible to consider history-based rewards, this opens up new considerations: Must the state space also change so as to retain the Markov property? Instead, we suggest that for a given CMP, it is natural to be interested in Markov rewards, but acknowledge the importance of going beyond such functions. As discussed in the main text, we suspect that there is a coherent account of which tasks are and are not expressible as a consequence of some of the axioms for rationality. We hope to study these directions in future work.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Apprenticeship learning via inverse reinforcement learning", "journal": "", "year": "2004", "authors": "Pieter Abbeel; Andrew Y Ng"}, {"ref_id": "b1", "title": "Interactions between learning and evolution", "journal": "Artificial Life II", "year": "1992", "authors": "David Ackley; Michael L Littman"}, {"ref_id": "b2", "title": "The steady-state control problem for Markov decision processes", "journal": "", "year": "2013", "authors": "Sundararaman Akshay; Nathalie Bertrand; Serge Haddad; Loic Helouet"}, {"ref_id": "b3", "title": "Repeated inverse reinforcement learning", "journal": "", "year": "2017", "authors": "Kareem Amin; Nan Jiang; Satinder Singh"}, {"ref_id": "b4", "title": "A distributional perspective on reinforcement learning", "journal": "", "year": "2017", "authors": "G Marc; Will Bellemare; R\u00e9mi Dabney;  Munos"}, {"ref_id": "b5", "title": "The Alignment Problem: Machine Learning and Human Values", "journal": "Atlantic Books", "year": "", "authors": "Brian Christian"}, {"ref_id": "b6", "title": "Deep reinforcement learning from human preferences", "journal": "", "year": "2017", "authors": "Paul F Christiano; Jan Leike; Tom B Brown; Miljan Martic; Shane Legg; Dario Amodei"}, {"ref_id": "b7", "title": "Representation of a preference ordering by a numerical function", "journal": "", "year": "1954", "authors": "Gerard Debreu"}, {"ref_id": "b8", "title": "Reinforcement learning and the reward engineering principle", "journal": "", "year": "2014", "authors": "Daniel Dewey"}, {"ref_id": "b9", "title": "Reinforcement learning with a corrupted reward channel", "journal": "", "year": "2017", "authors": "Tom Everitt; Victoria Krakovna; Laurent Orseau; Marcus Hutter; Shane Legg"}, {"ref_id": "b10", "title": "Hyperbolic discounting and learning over multiple horizons", "journal": "", "year": "2019", "authors": "William Fedus; Carles Gelada; Yoshua Bengio; Marc G Bellemare; Hugo Larochelle"}, {"ref_id": "b11", "title": "The free-energy principle: a unified brain theory?", "journal": "Nature reviews neuroscience", "year": "2010", "authors": "Karl J Friston"}, {"ref_id": "b12", "title": "Reinforcement learning or active inference?", "journal": "PloS One", "year": "2009", "authors": "Karl J Friston; Jean Daunizeau; Stefan J Kiebel"}, {"ref_id": "b13", "title": "Cooperative inverse reinforcement learning", "journal": "", "year": "2016", "authors": "Dylan Hadfield-Menell; Anca Dragan; Pieter Abbeel; Stuart Russell"}, {"ref_id": "b14", "title": "The off-switch game", "journal": "", "year": "2017", "authors": "Dylan Hadfield-Menell; Anca Dragan; Pieter Abbeel; Stuart Russell"}, {"ref_id": "b15", "title": "Inverse reward design", "journal": "", "year": "2017", "authors": "Dylan Hadfield-Menell; Smitha Milli; Pieter Abbeel; Stuart Russell; Anca Dragan"}, {"ref_id": "b16", "title": "Action and perception as divergence minimization", "journal": "", "year": "2020", "authors": "Danijar Hafner; Pedro A Ortega; Jimmy Ba; Thomas Parr; Karl J Friston; Nicolas Heess"}, {"ref_id": "b17", "title": "Multi-agent reinforcement learning with temporal logic specifications", "journal": "", "year": "2021", "authors": "Lewis Hammond; Alessandro Abate; Julian Gutierrez; Michael Wooldridge"}, {"ref_id": "b18", "title": "Using reward machines for high-level task specification and decomposition in reinforcement learning", "journal": "", "year": "2018", "authors": "Rodrigo Toro Icarte; Toryn Klassen; Richard Valenzano; Sheila Mcilraith"}, {"ref_id": "b19", "title": "Reward-rational (implicit) choice: A unifying formalism for reward learning", "journal": "", "year": "2020", "authors": "Smitha Hong Jun Jeon; Anca Milli;  Dragan"}, {"ref_id": "b20", "title": "A composable specification language for reinforcement learning tasks", "journal": "", "year": "2020", "authors": "Kishor Jothimurugan; Rajeev Alur; Osbert Bastani"}, {"ref_id": "b21", "title": "A new polynomial-time algorithm for linear programming", "journal": "", "year": "1984", "authors": "Narendra Karmarkar"}, {"ref_id": "b22", "title": "Interactively shaping agents via human reinforcement: The TAMER framework", "journal": "", "year": "2009", "authors": "W ; Bradley Knox; Peter Stone"}, {"ref_id": "b23", "title": "Stationary ordinal utility and impatience", "journal": "Econometrica: Journal of the Econometric Society", "year": "1960", "authors": "C Tjalling;  Koopmans"}, {"ref_id": "b24", "title": "Notes on the Theory of Choice", "journal": "Westview Press", "year": "1988", "authors": "David Kreps"}, {"ref_id": "b25", "title": "Maximum reward formulation in reinforcement learning", "journal": "", "year": "2020", "authors": "Yashaswi Sai Krishna Gottipati; Rohan Pathak; Raviteja Nuttall; Ahmed Chunduru;  Touati; Ganapathi Sriram; Matthew E Subramanian; Sarath Taylor;  Chandar"}, {"ref_id": "b26", "title": "REALab: An embedded perspective on tampering", "journal": "", "year": "2020", "authors": "Ramana Kumar; Jonathan Uesato; Richard Ngo; Tom Everitt; Victoria Krakovna; Shane Legg"}, {"ref_id": "b27", "title": "Reinforcement learning with temporal logic rewards", "journal": "", "year": "2017", "authors": "Xiao Li; Cristian-Ioan Vasile; Calin Belta"}, {"ref_id": "b28", "title": "The reward hypothesis", "journal": "", "year": "2017", "authors": "L Michael;  Littman"}, {"ref_id": "b29", "title": "Environment-independent task specifications via GLTL", "journal": "", "year": "2017", "authors": "Michael L Littman; Ufuk Topcu; Jie Fu; Charles Isbell; Min Wen; James Macglashan"}, {"ref_id": "b30", "title": "Grounding English commands to reward functions", "journal": "", "year": "2015", "authors": "James Macglashan; Monica Babes-Vroman; Marie Desjardins; Michael L Littman; Smaranda Muresan; Shawn Squire; Stefanie Tellex; Dilip Arumugam; Lei Yang"}, {"ref_id": "b31", "title": "Convergent actor critic by humans", "journal": "", "year": "2016", "authors": "James Macglashan; Michael L Littman; David L Roberts; Robert Loftin; Bei Peng; Matthew E Taylor"}, {"ref_id": "b32", "title": "Interactive learning from policy-dependent human feedback", "journal": "", "year": "2017", "authors": "James Macglashan; Mark K Ho; Robert Loftin; Bei Peng; Guan Wang; David L Roberts; Matthew E Taylor; Michael L Littman"}, {"ref_id": "b33", "title": "Reward functions for accelerated learning", "journal": "", "year": "1994", "authors": "J Maja;  Mataric"}, {"ref_id": "b34", "title": "Preference order dynamic programming", "journal": "Management Science", "year": "1974", "authors": "L G Mitten"}, {"ref_id": "b35", "title": "Policy invariance under reward transformations: Theory and application to reward shaping", "journal": "", "year": "1999", "authors": "Andrew Y Ng; Daishi Harada; Stuart Russell"}, {"ref_id": "b36", "title": "Algorithms for inverse reinforcement learning", "journal": "", "year": "2000", "authors": "Andrew Y Ng; Stuart J Russell"}, {"ref_id": "b37", "title": "Dueling posterior sampling for preference-based reinforcement learning", "journal": "", "year": "2020", "authors": "Ellen Novoseller; Yibing Wei; Yanan Sui; Yisong Yue; Joel Burdick"}, {"ref_id": "b38", "title": "Building safe artificial intelligence: specification, robustness, and assurance", "journal": "", "year": "2018", "authors": "Pedro A Ortega"}, {"ref_id": "b39", "title": "Rethinking the discount factor in reinforcement learning: A decision theoretic approach", "journal": "", "year": "2019", "authors": "Silviu Pitis"}, {"ref_id": "b40", "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "journal": "John Wiley & Sons", "year": "2014", "authors": "Martin L Puterman"}, {"ref_id": "b41", "title": "Artificial Intelligence: A Modern Approach", "journal": "Prentice-Hall", "year": "1994", "authors": "J Stuart; Peter Russell;  Norvig"}, {"ref_id": "b42", "title": "Benefits of assistance over reward learning", "journal": "", "year": "2021", "authors": "Rohin Shah; Pedro Freire; Neel Alex; Rachel Freedman; Dmitrii Krasheninnikov; Lawrence Chan; Michael D Dennis; Pieter Abbeel; Anca Dragan; Stuart Russell"}, {"ref_id": "b43", "title": "Reward is enough", "journal": "Artificial Intelligence", "year": "2021", "authors": "David Silver; Satinder Singh; Doina Precup; Richard S Sutton"}, {"ref_id": "b44", "title": "Intrinsically motivated reinforcement learning", "journal": "", "year": "2005", "authors": "Satinder Singh; Andrew G Barto; Nuttapong Chentanez"}, {"ref_id": "b45", "title": "Where do rewards come from?", "journal": "", "year": "2009", "authors": "Satinder Singh; L Richard; Andrew G Lewis;  Barto"}, {"ref_id": "b46", "title": "On separating agent designer goals from agent goals: Breaking the preferences-parameters confound", "journal": "", "year": "2010", "authors": "Satinder Singh; Richard L Lewis; Jonathan Sorg; Andrew G Barto; Akram Helou"}, {"ref_id": "b47", "title": "Ordinal dynamic programming", "journal": "Management science", "year": "1975", "authors": "Matthew J Sobel"}, {"ref_id": "b48", "title": "Discounting axioms imply risk neutrality", "journal": "Annals of Operations Research", "year": "2013", "authors": "Matthew J Sobel"}, {"ref_id": "b49", "title": "The Optimal Reward Problem: Designing Effective Reward for Bounded Agents", "journal": "", "year": "2011", "authors": "Jonathan Sorg"}, {"ref_id": "b50", "title": "Reward design via online gradient ascent", "journal": "", "year": "2010", "authors": "Jonathan Sorg; Richard L Lewis; Satinder Singh"}, {"ref_id": "b51", "title": "Axioms for rational reinforcement learning", "journal": "", "year": "2011", "authors": "Peter Sunehag; Marcus Hutter"}, {"ref_id": "b52", "title": "The reward hypothesis", "journal": "", "year": "2004", "authors": "Richard S Sutton"}, {"ref_id": "b53", "title": "Reinforcement Learning: An Introduction", "journal": "MIT Press", "year": "2018", "authors": "Richard S Sutton; Andrew G Barto"}, {"ref_id": "b54", "title": "Apprenticeship learning using linear programming", "journal": "", "year": "2008", "authors": "Umar Syed; Michael Bowling; Robert E Schapire"}, {"ref_id": "b55", "title": "Constrained MDPs and the reward hypothesis", "journal": "", "year": "2020", "authors": "Csaba Szepesv\u00e1ri"}, {"ref_id": "b56", "title": "A Boolean task algebra for reinforcement learning", "journal": "", "year": "2020", "authors": "Steven Geraud Nangue Tasse; Benjamin James;  Rosman"}, {"ref_id": "b57", "title": "Teaching multiple tasks to an RL agent using LTL", "journal": "", "year": "2018", "authors": "Rodrigo Toro Icarte; Toryn Q Klassen; Richard Valenzano; Sheila A Mcilraith"}, {"ref_id": "b58", "title": "Theory of Games and Economic Behavior", "journal": "Princeton University Press", "year": "1953", "authors": "Oskar John Von Neumann;  Morgenstern"}, {"ref_id": "b59", "title": "Markov decision processes with ordinal rewards: Reference point-based preferences", "journal": "", "year": "2011", "authors": "Paul Weng"}, {"ref_id": "b60", "title": "Unifying task specification in reinforcement learning", "journal": "", "year": "2017", "authors": "Martha White"}, {"ref_id": "b61", "title": "Learning to parse natural language to grounded reward functions with weak supervision", "journal": "", "year": "2018", "authors": "Edward C Williams; Nakul Gopalan; Mine Rhee; Stefanie Tellex"}, {"ref_id": "b62", "title": "A Bayesian approach for policy learning from trajectory preference queries", "journal": "", "year": "2012", "authors": "Aaron Wilson; Alan Fern; Prasad Tadepalli"}, {"ref_id": "b63", "title": "A survey of preference-based reinforcement learning methods", "journal": "The Journal of Machine Learning Research", "year": "2017", "authors": "Christian Wirth; Riad Akrour; Gerhard Neumann; Johannes F\u00fcrnkranz"}, {"ref_id": "b64", "title": "Preference-based reinforcement learning with finite-time guarantees", "journal": "Advances in Neural Information Processing Systems", "year": "2020", "authors": "Yichong Xu; Ruosong Wang; Lin Yang; Aarti Singh; Artur Dubrawski"}, {"ref_id": "b65", "title": "What can learned intrinsic rewards capture?", "journal": "", "year": "2020", "authors": "Zeyu Zheng; Junhyuk Oh; Matteo Hessel; Zhongwen Xu; Manuel Kroiss; David Hado Van Hasselt; Satinder Silver;  Singh"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Alice, Bob, and the artifacts of task definition (blue) and task expression (purple).", "figure_data": ""}, {"figure_label": "34", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Definition 3 . 4 .34A trajectory ordering (TO) of length N \u2208 N is a partial ordering L \u03c4,N , with each trajectory \u03c4 consisting of N state-action pairs, {(s 0 , a 0 ), . . . , (a N \u22121 , s N \u22121 )}, with s 0 the start state.", "figure_data": ""}, {"figure_label": "41", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Theorem 4 . 1 .41For each of SOAP, PO, and TO, there exist (E, T ) pairs for which no Markow reward function realizes T in E.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "2 H j x w s s s F i C a w P c o Y o c l 4 v s U A n o N j C 8 N A 8 t S E n e 7 K p R 4 w 1 0 P o 4 J r j y q + Z S U N m P 5 c I o x F O o i k c C y 8 R N r s S y Q m M k m 1 Z O x w A J r 2 9 f 4 d 3 9 Q 4 3 8 w Z G s 6 O x l h l R y c y W m p R m C 3 U m q 8 i W n 5 C y t a D S a S a x 0 4 o u k j / 2 7 L w C v l q R P K", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "B s c q w x r d t 5 X C z Q h u L J W r D / d S z b 6 P N x k t / y + t t u S + 6 6 / e O / i p v q t d b P 7 R + a t 1 p e a 1 + 6 1 7 r 1 9 Z + 6 6 g V t d 6 1 / m 7 9 0 / p 3 7 f u 1 e 2 u P 1 3 4 r o R 9 d m 9 1 u v 2 3 V f m s H / w O 5 n Z i u < / l a t e x i t > \u21e7 \u2327 = { 21 } t e x i t s h a 1 _ b a s e 6 4 = \" t p 9 e x u N 8 f E V L + K t c i U f w f r d B W 7 I = \"> A A A P m H i c j V d t b 9 s 2 E H b 3 2 n l b 0 2 7 7 t H 3 h F h T o A i W Q Z D u O C w T o m r 5 s Q F / S I G m L R l l A y b T N h h I 1 i k r i c g L 2 U / Z 1 + 0 f 7 N z t S f p Eo p 6 g B S 9 T d w + P D 4 / G O D F N G M + m 6 / 1 3 7 6 O N P P v 3 s 8 + t f t L / 8 6 u s b a z d v f f M y 4 7 m I y F H E G R e v Q 5 w R R h N y J K l k 5 H U q C I 5 D R l 6 F Z 3 t a / + q c i I z y 5 F B O U 3 I S 4 3", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "2 H j x w s s s F i C a w P c o Y o c l 4 v s U A n o N j C 8 N A 8 t S E n e 7 K p R 4 w 1 0 P o 4 J r j y q + Z S U N m P 5 c I o x F O o i k c C y 8 R N r s S y Q m M k m 1 Z O x w A J r 2 9 f 4 d 3 9 Q 4 3 8 w Z G s 6 O x l h l R y c y W m p R m C 3 U m q 8 i W n 5 C y t a D S a S a x 0 4 o u k j / 2 7 L w C v l q R P K", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Estimate of Expressible SOAPs vs. Entropy of T( |s, a) (e) Vary Entropy of T SOAPs vs. Variety of g G (f) Vary the Spread of \u03a0G", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 3 :3Figure3: The approximate fraction of SOAPs that are expressible by reward in CMPs with a handful of states and actions, with 95% confidence intervals. In each plot, we vary a different parameter of the environment or task to illustrate how this change impacts the expressivity of reward, showing both equal (color) and range (grey) realization of SOAP.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "< l a t e x i t s h a 1 _1b a s e 6 4 = \" X p l 4 g v L1 I B F Y 1 v h p h U H o V Y v o S 2 4 = \" > A A A Q o 3 i c j V d b b 9 s 2 F H Z 3 7 b x L 3 e 1 x L 9 y C D G 2 g B J Z s x 3 G B A F 3 T y 4 D 1 k g T p B Y 2 8 g J J p m 4 0 k a p S U x O P 0 u / Z b 9 r D X 7 W / s H M o X i X K K G r B E n f P x 8 O P h 4 T m k F w c 8 S d v t v 2 9 8 9 P E n n 3 7 2 + c 0 v m l 9 + 9 f U 3 t 1 q 3 v 3 2 V i E z 6 7 K U v A i H f e D R h A Y / Y y 5 S n A X s T S 0 Z D L 2 C v v f M D 1 L + + Y D L h I j p J Z z E b h n Q S 8T H 3 a Q q i s 9 b R 5 k 9 u z M / U x L L z O 8 l d 4 r q n 3 T g d N p d i Z 7 2 4 Q O + T M q r p H v K z J 2 e t j f Z O W / 9 I v W H P G x u N + e / w 7 P a t e + 5 I + F n I o t Q P a J K c 2 m 0 Y S 1 G Z c j 9 g e X O T b G 9 v k 0 P q n 9 M J S / C j 6 W Y", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "J B k M 9 O c 7 M 8 7 4 h d x l c p u 0 o t 3 c J J 6 j 7 b 6 F E d H Q m h 0 Y g 8 R 7 5 a C s q H j M X P O E g 1 l 6 T p j t g Y w kt / q R G E V s Z y d f z k Q a 7 a 1 q B j O X Y 3 N z F x J u N g g b L b P a v X t p x O u 4 Y L q s B d x x r 0 L a f X q w F H p V E d s N W z + g D a N F B l b v a e 1 e t a d r d f h w V l W L 9 t 2 Q N 7 7 a A T y V g 0 x / V 2 L R t M O g P w H / j o 2 W y 9 d 0 q W B 3 s W s b t t e A x q E y / j 7 L Y D G A e B / Y 4 J L F N 1 7 F 2 L O J 0 e P O p k J U T D 3 N 4 A E L a N R u 2 a v c s p T 5 f 2 w D 9 k 8 T C B Q t J o s k R 2 k N 3 u A B 4 d x 0 S W v b T X w a n 0 L b L 3 f m + i K 8 F g H 9 E 1 Z F B B I r 9 i S r 2 + i U z p A u Y 4 N k z D R t + 0 a 9 6 O e X S + j B 0 c c x c d 2 d 4 z c T M W B O J y 6 R 0 0 1 k W e g 0 F 9 z n Q 2 x 3 X B V n e A f 7 2 z n v L J N D W j o h J w 2 o 2 w b 4 B u z e 6 H T 7 3 K F U k 4 G G u O 0 8 E A d W G 7 + y I M Y W 8 r l 8 V J r t w L K q H B A x F h R j 2 Z x V A D A h L w k K f 3 q v g I h N A h i 0 Z Q O F i q I u J K n B W V U l w S l 0 f j F C b v I i o 3 e i Y Z 9 o T n m e J Q H e z 8 t 8 i E x F J A p L r 4 K o N w T 0 F 2 Q k 6 Y k L D t e d W u o Z + f 2 k N 4 F z i 1 Y Z v j h 9 4 K 4 X l r A J D V u J 8 v A K G q A b x H S 6 1 6 Z C o l B e X K F Y Y 2 Q G 3 A x m u V z 9 G s p 5 6 b J t 8 W 8 r e m / K i Q H 5 n y 4 0 J + D P K q Y g T R k Z 8 6 Q 9 V 0 P Q b 1 c F U j T q W A p f S F j J j c t 2 M o W R 5 U h 8 l c C s G 0 r 5 P C s O l e J F A 3 m L L D M G + C K 8 Z Q C S D i O J 4 U c n J n w 7 5 7 j 4 A L U 7 K B O Y D B q K s 6 h H Q e M i j f k u H S v Y i x 2 g q 5 p V y o z i G H k I a 3 a 2 H r f U B 6 t Q B C y 1 g Z D n Z G 8 9 W R L F D u A z 7 5 s + Y H P Q W s u Y A s H O H D u Y L J 3 J 0 u q r F y f I h S 8 E T x 0 D O Z g + b W 0 i k T k o Vb a t H I 1 U n R y s t 6 x S P w q Z 8 l q Q j h i G C A W H T B p Y j w W K N W G I j P J g E 3 S r Z i 7 Y K 9 q i k M X d e w r r s B V 1 P c J N d x g j W X u T o Q U g T g 7 9 l 1 t D T s A 3 g h z i S m + 6 5 h V s iv I 4 Y + U s c M T m 7 n 1 5 F C y A d w Q s M G J b 0 A d U Z a D F n m U I p Y J D q o r 6 M H u S n O V Q l 4 H U k N / A C W i D N p 6 r 5 r e B Z y P I b R C 0 Z O K J z k a 6 N D 4 Q O x H n k R 5 X A G z S S I p r C P i 2 D m 0 W S R C w C e Q Q T k m k M q Y r 0 / s K", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "m 2 C 9 2 Y + p B M H + O z m j 9 R s b V W U 5 j b W m 9 v / r 4 m 6 4 5 W V U m V K t S q A h 0 L D 7 b S w S I e D 6 F j wh O 8 H u V q E 7 Z c 8 X N R w C K 4 z U E M q x c n d m 7 o E t h D L F F h T T 6 l M V y t T P G Y h j y Y q T g O 3 p V V c D e H h da D L w m e w I 3 s M U i W F M s M K 3 y b R k H S q 7 i 1 X M Y m 3 L Z t 8 2 5 d b 7 x y d u z e T v u o u 3 H f u 1 / c u 2 8 2 v m / 8 2 L j T s B v 9 x v 3 G L 4 3 D x s u G 3 / i r 8 U / j 3 8 Z / r c 3 W r 6 3 j 1 k k B / e j G / K 7 + X a P y a w 3 / B w B v / h c = < / l a t e x i t > \u21e7 G < l a t e x i t s h a 1 _ b a s e 6 4 = \" T X a l m X D Z n Q D G Z x X 3 a c J Z q k 8 U E d o = \" > A A A Q o X i c j V f r b 9 s 2 E H f 3 7 L x H 3 e 3 j v n A L M r S B E l i y H c c F A n R N H x v Q R x q k D 9 T K A k q m b T a S q F F S E o / T n 7 U / Z t j X 7 f / Y H e W H R D l F D F i i 7 n 4 8 / n g 8 3 p F e H P A k b b f / v v H R x 5 9 8 + t n n N 7 9 o f v n V 1 9 / c a t 3 + 9 n U i M u m z V 7 4 I h H z r 0 Y Q F P G K v U p 4 G 7 G 0 s G Q 2 9 g L 3 x z g 5 Q / + a c y Y S L 6 D i d x e w k p J O I j 7 l P U x C d t l 7 8 5 M b 8 V E 0 s O 7 + T 3 C W u O + z G 6 U l z I X X W S g v s P i m D m p v u I T 9 9 c t r a a O + 0 9 Y / U G / a 8 s d G Y / w 5 P b 9 + 6 5 4 6 E n 4 U s S v 2 A J s n Q b s N Y i s q U + w H L m 5 t k e 3 u b H F L / j E 5 Y g h 9 N N 0", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "1 n E T 3 7 P R M p g h B F N p j I L Q J b y s z 8 s 4 n l g x x P i L K U e q P F r R S K T g U U u 9Y J W y A / H k o Y M J j U V o / 1 j f v b u R I U j L R v l T X B O T A M w E A m C f H Z g F U r z j t h F f J m y y 9 T S L Z y k 7 r O N H t W x k R A a j c h z 5 K u l o H z I W P y M g 1 R z S Z r u i I 0 h u P S X G k F g Z S x X R 0 8 e 5 K p t D T q W Y 3 d z E x N n M g 4 W K L v d s 3 p t y + m 0 a 7 i g C t x 1 r E H f c n q 9 G n B U G t U B W z 2 r D 6 B N A 1 X m Z u 9 Z v a 5 l d / t 1 W F C G 9 d u W P b D X D j q R j E V z X G / X s s G k M w D / g Y + e z d Z 7 p 2 R 5 s G c R u 9 u G x 6 A 2 8 T L O b j u A c R D Y 7 5 j A M l X H 3 r W I 0 + n B o 0 5 W Q j T M 7 Q 0 A Y d t o 1 K 7 Z u 5 j y d G k P / E M W D x M o J I 0 m S 2 Q H 2 e 0 O 4 N F x T G T Z S 3 s d n E r f I n s f 9 i a 6 E g z 2 E V 1 D B h U k 8 i u m 1 O u b y J Q u Y I 5 j w z R s 9 E 2 7 5 u 2 Y R 2 f L 2 M E x d 9 G R 7 T 0 T N 2 N B I C 6 W 3 k F j X e Q 5 G N T n T G d z X B d s d Q f 4 1 z v r K Z 9 M U z M q K g G n 3 Q j 7 B u j W 7 F 5 / 6 l W u S M L B W H O c D g a o C 9 v d F 2 E I e 1 u 5 L E 5 y 5 Z 5 T C Q 0 e i A g z 6 v E s h g o Q k I C H P L 1 X x U c g h A 5 Z N I K y w V I V E V f i r K i U 4 o K 4 P B q n M H k X U b n R M8 m w J z x P F Y f y Y O e / R S Y k l g I i 1 c V X G Y R 7 C r I T c s K E h G 3 P q 3 Y N / X x o n 8 C 7 w K k N 2 x w / 9 F Y I z 1 s D g K z G / X w B C F U N 4 D 1 a a t U j U y k p K F e u M L Q B a g M 2 X q t 8 j m Y 9 9 d w 0 + a 6 Q v z P l L w v 5 S 1 N + V M i P Q F 5 V j C A 6 8 q F z o p q u x 6 A e r m r E U A p Y S l / I i M l 9 O 4 a S 5 U F 1 m M y l E E z 7", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 4 :4Figure 4: A SOAP-designed reward function (left) and the resulting learning curves (right) for Qlearning compared to the traditional reward function for the Russell and Norvig [42] grid world. Each series presents average performance over 50 runs of the experiment with 95% confidence intervals.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Figure 5 :5Figure 5: A CMP that separates the two kinds of SOAP realizations.", "figure_data": ""}, {"figure_label": "41", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Theorem 4 . 1 .41For each of SOAP, PO, and TO, there exist (E, T ) pairs for which no reward function realizes T in E. Proof of Theorem 4.1.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "3 .3We proceed by providing constructive algorithms for each of SOAP, PO, or TO. All three are based on similar applications of a linear program (LP), though there is nuance that separates them. We present each as a Lemma (Lemma B.1, Lemma B.2, Lemma B.3), which together constitute the proof of this Theorem. Lemma B.1. The SOAP variant of REWARDDESIGN can be solved in polynomial time. Proof of Lemma B.1.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "\u03c1i (s, a) := \u221e t=0 \u03b3 t Pr(s t = s, a t = a | s 0 , \u03c0 i ). (B.4)", "figure_data": ""}, {"figure_label": "44", "figure_type": "figure", "figure_id": "fig_17", "figure_caption": "Corollary 4 . 4 .44For any task T and environment E, deciding whether T is expressible in E is solvable in polynomial time Proof of Corollary 4.4.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_19", "figure_caption": "Figure 6 :6Figure 6: A pair of CMPs with opposite action effects: On the left, a 1 keeps the agent in the same place, while a 2 flips the state. On the right, the effects are exactly inverted.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "SOAP \u03a0 G task-as-\u03c0 * equal: V \u03c0g (s 0 ) = V \u03c0 g (s 0 ) > V \u03c0 b (s 0 ), \u2200 \u03c0g,\u03c0 g \u2208\u03a0 G ,\u03c0 b \u2208\u03a0 B range: V \u03c0g (s 0 ) > V \u03c0 b (s 0 ), \u2200 \u03c0g\u2208\u03a0 G ,\u03c0 b \u2208\u03a0 B PO L \u03a0 SOAP (\u03c0 1 \u2295 \u03c0 2 ) \u2208 L \u03a0 =\u21d2 V \u03c01 (s 0 ) \u2295 V \u03c02 (s 0 ) TO L \u03c4,N task-as-goal (\u03c4 1 \u2295 \u03c4 2 ) \u2208 L \u03c4,N =\u21d2 G(\u03c4 1 ; s 0 ) \u2295 G(\u03c4 2 ; s 0 )", "formula_coordinates": [5.0, 115.53, 95.92, 379.71, 66.88]}, {"formula_id": "formula_1", "formula_text": "> A A A P m H i c j V d t b 9 s 2 E H b 3 2 n l b 0 2 7 7 t H 3 h F h T o A i W Q Z D u O C w T o m r 5 s Q F / S I G m L R l l A y b T N h h I 1 i k r i c g L 2 U / Z 1 + 0 f 7 N z t S f p E o p 6 g B S 9 T d w + P D 4 / G O D F N G M + m 6 / 1 3 7 6 O N P P v 3 s 8 + t f t L / 8 6 u s b a z d v f f M y 4 7 m I y F H E G R e v Q 5 w R R h N y J K l k 5 H U q C I 5 D R l 6 F Z 3 t a / + q c i I z y 5 F B O U 3 I S 4 3 F C R z T C E k S n N 7 8 L 9 u n p Y 7 S L A h W k 9 F T 5 X h E U p z f X 3 S 3 X / F C z 4 c 0 a 6 6 3 Z b / / 0 1 g 0 W D H m U x y S R E c N Z d u y 5 q T x R W E g a M V K 0 b 6 P N z U 2 0 j 6 M z P C a Z / m g H e U b S U n A c Y z G m y a 6 7 5 d P k R I 0 J j 4 k U 0 6 K K U S Q B 8 w J L 4 q A Y y 4 n I R p m D Y G o h z C R 2 E I 4 z L T Y N O S k F 2 T Q O o c H G X N B S x s Z p R n K g y o d g J 6 U j n k g H Z X k 4 o m M H j X L G U h j L Q V H 2 R 8 4 l g R G G O J u I n I F M 0 r N 3 D g p D s B N y f i Z x C G r 9 t S S R C + a g S 7 M q N f L H I 4 F j A p O a 8 O H u I T 1 7 c 6 L i o Z E N i z Y 4 J 8 U M D C Q c a T 5 b t W k n 5 C K 9 l O R S O q a l 5 2 i 6 b G q H m v X N E E 6 G 6 J m m a 6 S g f E B I + p S C 1 F D J 2 s G Q j C B A z J c a Q n D k p F A H j + 8 X y n U G H c f 3 u o W N S X O R s j n K c 3 t O z 3 X 8 j t v A s T p w 2 3 c G f c f v 9 R r A Y W V U H 2 z 1 n D 6 A b l u o K j d v x + l 1 H a / b b 8 J Y F d Z 3 H W / g r R x 0 L A h J Z r j e t u O B S X 8 A / g M f P Z 2 u 9 k 7 F 8 m D H Q V 7 X h c e g M f E q z n N 9 w P g a 2 O / Y w C p V 3 9 t 2 k N / p w a N J V k A w z O w N A O F 5 2 q j X s H c x o X J h D / y D 5 g 8 b y A V O x g t k R 7 P b H s C j 4 9 v I q p d 2 O n o q f Q f t v N + b 2 p V g s K / R D S S r I T W / c k q 9 v o 2 U e A 7 z f Q + m 4 W n f u A 1 v p z Q 5 W 8 S O H n N b O 9 L d s X F T w h i / W H h H G + t q n o N B c 8 5 4 O s N 1 w V Z 3 o P 9 m Z z 2 h 4 4 m 0 o 6 I W c M a N s G + A b s P u h 0 + 9 z l W T 8 H W s + X 5 H B 2 g A 2 z 3 i c Q x 7 W w U k z Q o V n G M B D c p 4 o h P q 4 T S F L M 4 Q o z G V d + v 4 B I T Q I U + G k P q J V A k K h J 4 V F o J f o I A m I w m T D z S q s H p m u e 4 J z 1 N F o S x 4 x e + J D U k F h 0 g N 9 K s K 0 n s K s p P m p B O S b o d h v W s c F c f e C b x L n F r 3 7 P H j c I k I w x U A y G o 0 K u a A W D U A 4 c O F V j 2 0 l Q K D c u k K S 8 u 0 l p H R S u U z b T Z U z 2 y T b 0 r 5 G 1 v + o p S / s O U H p f w A 5 H X F E K K j O P Z P V D s I C Z T D Z Y k 4 F h y W M u I i I W L X S 6 F i h V A d x j M p B N O u S Q o n 7 e A 8 g 7 p B l B f H R R t c M Y J K A B F H d a 0 v 0 J 1 1 7 + e 7 C F w o 0 b r O A Q R G X Z Y h T e c B g e o t i F 6 6 5 6 k u t l x s q A C K c 0 w h p O E d O L r 1 P i C + n A O h Z a 0 M B T v D 2 e o I w l R w n 4 7 / b P j B T E G X X E C W j o j g W E F E E U z m x V j 5 E U Q p e K J 8 m J n M Q D N r c k K 4 I P G G m j c K d V i 2 i q p e 0 Q R 8 G u W Z 5 D G c E C w Q S c 6 p 4 I k + 1 a g l B u K z j c C N g i x Z B 2 C v b k q H b m B Z N 9 2 A q y 1 u o 6 s 4 w Z q L Q u 1 x w R n 4 e 3 o V L Q P 7 A F 4 a Z x M z f V c w K + V X E d M + U g c E D m 5 n V 5 H S k A / g p A 1 b l M w C N B k Z s T 7 d 4 H O C D j E c c R s j Q z 0 B s R l", "formula_coordinates": [7.0, 140.48, 34.58, 5.65, 7.33]}, {"formula_id": "formula_2", "formula_text": "A i l W G S 1 W I G L g N 5 n J o E p H 5 Z t m u Y i C d v S S S 1 T y H W F 2 0 L M 9 8 D 9 n Z Y I B j W V W / P v G o a c o l j c 0 Z 9 O G v U t I y A O w v 1 x L x q G q h w K c / K 7 K n 2 K x 8 1 l C h 3 Q W 0 3 z H U j H E G O e q S f 9 b S k F R s r N a W 5 j d X 2 Z u 9 K n l r k 4 g M e w i 7 Z m 8 f a P k A z m u k L R a F u w 2 4 q f 4 E W k A T u P x C f 6 v m h V 1 i 6 D P Y H y V T c k E 9 w C r c R W z z C M W V T l a b s b V U F F 1 J Y R D P 4 g u A h X G I e g W R", "formula_coordinates": [7.0, 140.48, 34.58, 5.65, 7.33]}, {"formula_id": "formula_3", "formula_text": "F C R z T C E k S n N 7 8 L 9 u n p Y 7 S L A h W k 9 F T 5 X h E U p z f X 3 S 3 X / F C z 4 c 0 a 6 6 3 Z b / / 0 1 g 0 W D H m U x y S R E c N Z d u y 5 q T x R W E g a M V K 0 b 6 P N z U 2 0 j 6 M z P C a Z / m g H e U b S U n A c Y z G m y a 6 7 5 d P k R I 0 J j 4 k U 0 6 K K U S Q B 8 w J L 4 q A Y y 4 n I R p m D Y G o h z C R 2 E I 4 z L T Y N O S k F 2 T Q O o c H G X N B S x s Z p R n K g y o d g J 6 U j n k g H Z X k 4 o m M H j X L G U h j L Q V H 2 R 8 4 l g R G G O J u I n I F M 0 r N 3 D g p D s B N y f i Z x C G r 9 t S S R C + a g S 7 M q N f L H I 4 F j A p O a 8 O H u I T 1 7 c 6 L i o Z E N i z Y 4 J 8 U M D C Q c a T 5 b t W k n 5 C K 9 l O R S O q a l 5 2 i 6 b G q H m v X N E E 6 G 6 J m m a 6 S g f E B I + p S C 1 F D J 2 s G Q j C B A z J c a Q n D k p F A H j + 8 X y n U G H c f 3 u o W N S X O R s j n K c 3 t O z 3 X 8 j t v A s T p w 2 3 c G f c f v 9 R r A Y W V U H 2 z 1 n D 6 A b l u o K j d v x + l 1 H a / b b 8 J Y F d Z 3 H W / g r R x 0 L A h J Z r j e t u O B S X 8 A / g M f P Z 2 u 9 k 7 F 8 m D H Q V 7 X h c e g M f E q z n N 9 w P g a 2 O / Y w C p V 3 9 t 2 k N / p w a N J V k A w z O w N A O F 5 2 q j X s H c x o X J h D / y D 5 g 8 b y A V O x g t k R 7 P b H s C j 4 9 v I q p d 2 O n o q f Q f t v N + b 2 p V g s K / R D S S r I T W / c k q 9 v o 2 U e A 7 z f Q + m 4 W n f u A 1 v p z Q 5 W 8 S O H n N b O 9 L d s X F T w h i / W H h H G + t q n o N B c 8 5 4 O s N 1 w V Z 3 o P 9 m Z z 2 h 4 4 m 0 o 6 I W c M a N s G + A b s P u h 0 + 9 z l W T 8 H W s + X 5 H B 2 g A 2 z 3 i c Q x 7 W w U k z Q o V n G M B D c p 4 o h P q 4 T S F L M 4 Q o z G V d + v 4 B I T Q I U + G k P q J V A k K h J 4 V F o J f o I A m I w m T D z S q s H p m u e 4 J z 1 N F o S x 4 x e + J D U k F h 0 g N 9 K s K 0 n s K s p P m p B O S b o d h v W s c F c f e C b x L n F r 3 7 P H j c I k I w x U A y G o 0 K u a A W D U A 4 c O F V j 2 0 l Q K D c u k K S 8 u 0 l p H R S u U z b T Z U z 2 y T b 0 r 5 G 1 v + o p S / s O U H p f w A 5 H X F E K K j O P Z P V D s I C Z T D Z Y k 4 F h y W M u I i I W L X S 6 F i h V A d x j M p B N O u S Q o n 7 e A 8 g 7 p B l B f H R R t c M Y J K A B F H d a 0 v 0 J 1 1 7 + e 7 C F w o 0 b r O A Q R G X Z Y h T e c B g e o t i F 6 6 5 6 k u t l x s q A C K c 0 w h p O E d O L r 1 P i C + n A O h Z a 0 M B T v D 2 e o I w l R w n 4 7 / b P j B T E G X X E C W j o j g W E F E E U z m x V j 5 E U Q p e K J 8 m J n M Q D N r c k K 4 I P G G m j c K d V i 2 i q p e 0 Q R 8 G u W Z 5 D G c E C w Q S c 6 p 4 I k + 1 a g l B u K z j c C N g i x Z B 2 C v b k q H b m B Z N 9 2 A q y 1 u o 6 s 4 w Z q L Q u 1 x w R n 4 e 3 o V L Q P 7 A F 4 a Z x M z f V c w K + V X E d M + U g c E D m 5 n V 5 H S k A / g p A 1 b l M w C N B k Z s T 7 d 4 H O C D j E c c R s j Q z 0 B s R l", "formula_coordinates": [7.0, 68.33, 34.43, 5.65, 7.33]}, {"formula_id": "formula_4", "formula_text": "A i l W G S 1 W I G L g N 5 n J o E p H 5 Z t m u Y i C d v S S S 1 T y H W F 2 0 L M 9 8 D 9 n Z Y I B j W V W / P v G o a c o l j c 0 Z 9 O G v U t I y A O w v 1 x L x q G q h w K c / K 7 K n 2 K x 8 1 l C h 3 Q W 0 3 z H U j H E G O e q S f 9 b S k F R s r N a W 5 j d X 2 Z u 9 K n l r k 4 g M e w i 7 Z m 8 f a P k A z m u k L R a F u w 2 4 q f 4 E W k A T u P x C f 6 v m h V 1 i 6 D P Y H y V T c k E 9 w C r c R W z z C M W V T l a b s b V U F F 1 J Y R D P 4 g u A h X G I e g W R B s c q w x r d t 5 X C z Q h u L J W r D / d S z b 6 P N", "formula_coordinates": [7.0, 68.33, 34.43, 5.65, 7.33]}, {"formula_id": "formula_5", "formula_text": "\u21e7 \u2327 = { 21 } s1 s0 a2 a1 a1 a2 (b) Entailment Case", "formula_coordinates": [7.0, 68.19, 32.75, 386.51, 114.31]}, {"formula_id": "formula_6", "formula_text": "E \u00acT or (R, \u03b3) in E \u00ac\u03b3 .", "formula_coordinates": [7.0, 354.35, 293.03, 91.42, 10.32]}, {"formula_id": "formula_7", "formula_text": "1: \u03a0 fringe = compute_fringe(\u03a0 G ) 2: for \u03c0 g,i \u2208 \u03a0 G do", "formula_coordinates": [8.0, 112.98, 122.91, 144.99, 21.62]}, {"formula_id": "formula_8", "formula_text": "\u03c1 g,i = compute_exp_visit(\u03c0 g,i , E) 4: for \u03c0 f,i \u2208 \u03a0 fringe do 5: \u03c1 f,i = compute_exp_visit(\u03c0 f,i , E) 6: C eq = {} Make Equality Constraints. 7: for \u03c0 g,i \u2208 \u03a0 G do 8: C eq .add(\u03c1 g,0 (s 0 ) \u2022 X = \u03c1 g,i (s 0 ) \u2022 X) 9: C ineq = {}", "formula_coordinates": [8.0, 112.98, 144.73, 392.76, 101.64]}, {"formula_id": "formula_9", "formula_text": "C ineq .add(\u03c1 f,j (s 0 ) \u2022 X + \u2264 \u03c1 g,0 (s 0 ) \u2022 X) 12: R out , out = linear_programming(obj. = max , constraints = C ineq , C eq )", "formula_coordinates": [8.0, 108.5, 257.72, 318.34, 30.05]}, {"formula_id": "formula_10", "formula_text": "E = {E 1 , . . . , E n } such that T is realizable in each E i \u2208 E", "formula_coordinates": [8.0, 258.86, 565.56, 244.25, 10.71]}, {"formula_id": "formula_11", "formula_text": "J i w v B a U j l h E f 7 7 R 2 H R 0 M 1 Y S J k q Z z l Z Y x i E Z i X N G U W C W k 6 l c k 4 s Q j 4 x Q M 3 h B a h Y Y J i 3 U i n h S C Z h R 4 0 g o m Q v J A F k z h h G V A V I 7 A T 8 7 G I U o s k m T f m E 4 u M s y C I Y S y L + M n v m U g Z j D C i y V R m A c h S f v 6 H R T w P 7 H h C n K f U A z V + r U h k M r D I l V 7 S C v n T s a Q h g 0 l N x W j / h J + / H a p w p G W j v A n O i W k A B i", "formula_coordinates": [10.0, 137.61, 116.28, 4.98, 8.64]}, {"formula_id": "formula_12", "formula_text": "t I 0 T M Z D o G 7 Y I E r v u Y m C 6 9 l K a F k T C N / B g f Y K 0 J 1 + i D p F E Z J d o x U B A C d h 9 + f i r q Y i v T E g d H 8 E I 8 y L S q Y m V K d e 0 0 h p t y S b P U J t Q U F p U 5 z i Z n / s J r / 0 D M T I P h q T Z a D 0 l k E T F K J H r i 2 Z G G s Y 0 X 9 v G p X M L 6 I 3 j E / R Z / C p l y 2 D c x i s 5 r 7 d o k I K J b n A / 2 q a N g V D f V h + t G 8 U d E G D N y Z q 6 f 6 V d H E p U C v R / 0 C J Y v t W t", "formula_coordinates": [10.0, 137.61, 116.28, 4.98, 8.64]}, {"formula_id": "formula_13", "formula_text": "A n L g K o Y g Z 2 Y j 0 W U W i T J v D G f W G S c B U E M Y", "formula_coordinates": [10.0, 138.41, 128.42, 5.07, 5.7]}, {"formula_id": "formula_14", "formula_text": "O i m c N N 3 z B O o G U 3 Y Y 5 k 1 w x R g q A U Q c x 3 N C T u 5 s 2 H f v E X B h S j Y w B z A Y d V W H k M 5 D B u V b M l y 6 F z F W W y G 3 l A v V O e Q Q 0 v B 2 L W x 9 C E g v F 0 B o G S v", "formula_coordinates": [10.0, 138.41, 128.42, 5.07, 5.7]}, {"formula_id": "formula_15", "formula_text": "L \u03c4,N := {\u03c4 G , \u03c4 B }, (B.1) \u03c4 G = {{(s 0 , a 1 ), (s 1 , a 2 )}, {(s 0 , a 2 ), (s 1 , a 1 )}} , (B.2) \u03c4 B = {{(s 0 , a 1 ), (s 1 , a 1 )}, {(s 0 , a 2 ), (s 1 , a 2 )}} . (B.", "formula_coordinates": [16.0, 201.66, 258.55, 292.38, 38.5]}, {"formula_id": "formula_16", "formula_text": "E \u00acT or (R, \u03b3) in E \u00ac\u03b3 .", "formula_coordinates": [16.0, 354.35, 361.41, 91.42, 10.32]}, {"formula_id": "formula_17", "formula_text": "R \u2022 \u03c1 0 \u2265 R \u2022 \u03c1 1 \u2265 . . . . (B.5)", "formula_coordinates": [18.0, 264.42, 222.06, 229.62, 10.71]}, {"formula_id": "formula_18", "formula_text": "R \u2022 \u03c1 0 \u2265 R \u2022 \u03c1 1 + \u2265 . . . . (B.6", "formula_coordinates": [18.0, 256.31, 278.15, 233.58, 10.71]}, {"formula_id": "formula_19", "formula_text": "G(\u03c4 ; s 0 ) = N \u22121 i=0 \u03b3 i R(s i , a i ), (B.7)", "formula_coordinates": [18.0, 252.68, 565.34, 241.36, 31.18]}, {"formula_id": "formula_20", "formula_text": "\u03c4 i = {(s (i) 0 , a (i) 0 ), . . . , (s (i) N \u22121 , a (i) N \u22121 )}, \u03c4 j = {(s (j) 0 , a (j) 0 ), . . . , (s (j) N \u22121 , a(j)", "formula_coordinates": [18.0, 137.34, 647.92, 305.48, 15.08]}, {"formula_id": "formula_21", "formula_text": "\u03c4 i \u2022 R \u2212 \u03c4 j \u2022 R \u2264 , (B.9)", "formula_coordinates": [18.0, 271.1, 686.62, 222.94, 10.71]}], "doi": ""}