{"title": "HYPERPARAMETER TUNING WITH RENYI DIFFERENTIAL PRIVACY", "authors": "Nicolas Papernot; Thomas Steinke", "pub_date": "2022-03-14", "abstract": "For many differentially private algorithms, such as the prominent noisy stochastic gradient descent (DP-SGD), the analysis needed to bound the privacy leakage of a single training run is well understood. However, few studies have reasoned about the privacy leakage resulting from the multiple training runs needed to fine tune the value of the training algorithm's hyperparameters. In this work, we first illustrate how simply setting hyperparameters based on non-private training runs can leak private information. Motivated by this observation, we then provide privacy guarantees for hyperparameter search procedures within the framework of Renyi Differential Privacy. Our results improve and extend the work of Liu and Talwar (STOC 2019). Our analysis supports our previous observation that tuning hyperparameters does indeed leak private information, but we prove that, under certain assumptions, this leakage is modest, as long as each candidate training run needed to select hyperparameters is itself differentially private.", "sections": [{"heading": "INTRODUCTION", "text": "Machine learning (ML) systems memorize training data and regurgitate excerpts from it when probed (Carlini et al., 2020). If the training data includes sensitive personal information, then this presents an unacceptable privacy risk (Shokri et al., 2017). It may however still be useful to apply machine learning to such data, e.g., in the case of healthcare (Kourou et al., 2015;Wiens et al., 2019). This has led to a significant body of research on the development of privacy-preserving machine learning methods. Differential privacy (DP) (Dwork et al., 2006b;a) provides a robust and quantitative privacy guarantee. It has been widely accepted as the best framework for formally reasoning about the privacy guarantees of a machine learning algorithm.\nA popular method for ensuring DP is noisy (stochastic) gradient descent (a.k.a. DP-SGD) (Song et al., 2013;Bassily et al., 2014;Abadi et al., 2016). DP-SGD differs from standard (stochastic) gradient descent in three ways. First, gradients are computed on a per-example basis rather than directly averaged across a minibatch of training examples. Second, each of these individual gradients is clipped to ensure its 2-norm is bounded. Third, Gaussian noise is added to the gradients as they are averaged and applied to update model parameters. These modifications bound the sensitivity of each update so that the added noise ensures differential privacy. The composition (Dwork et al., 2010) and privacy amplification by subsampling (Balle et al., 2018) properties of differential privacy thus imply that the overall training procedure is differentially private. We can compute tight privacy loss bounds for DP-SGD using techniques like the Moments Accountant (Abadi et al., 2016) or the closely related framework of R\u00e9nyi DP (Mironov, 2017;Mironov et al., 2019).\nMachine learning systems have hyperparameters, such as the learning rate, minibatch size, or choice of a regularizer to prevent overfitting. Details of the model architecture can also be treated as hyperparameters of the optimization problem. Furthermore, learning within the constraints of differential privacy may introduce additional hyperparameters, as illustrated in the DP-SGD optimizer by the 2-norm bound value for clipping, the scale of the Gaussian noise, and the choice of stopping time. Typically the training procedure is repeated many times with different hyperparameter settings in order to select the best setting, an operation known as hyperparameter tuning. This methodology implies that even if each run of the training procedure is privacy-preserving, we need to take into Published as a conference paper at ICLR 2022 account the fact that the training procedure is repeated (possibly many times) when reasoning about the privacy of the overall learning procedure.\nCan the tuning of hyperparameters reveal private information? This question has received remarkably little attention and, in practice, it is often ignored entirely. We study this question and provide both positive and negative answers.", "publication_ref": ["b9", "b25", "b17", "b14", "b26", "b2", "b0", "b16", "b1", "b0", "b20", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "OUR CONTRIBUTIONS", "text": "\u2022 We show that, under certain circumstances, the setting of hyperparamters can leak private information. Hyperparameters are a narrow channel for private information to leak through, but they can still reveal information about individuals if we are careless. Specifically, if we tune the hyperparameters in an entirely non-private fashion, then individual outliers can noticeably skew the optimal hyperparameter settings. This is sufficient to reveal the presence or absence of these outliers \u00e0 la membership inference (Shokri et al., 2017); it shows that we must exercise care when setting hyperparameters. \u2022 We provide tools for ensuring that the selection of hyperparameters is differentially private. Specifically, if we repeat the training procedure multiple times (with different hyperparameters) and each repetition of the training procedure is differentially private on its own, then outputting the best repetition is differentially private. Of course, a basic version of such a result follows from the composition properties of differential privacy (that is the fact that one can \"sum\" the privacy loss bounds of multiple differentially private analyses performed on the same data to bound the overall privacy loss from analyzing this data). However, we provide quantitatively sharper bounds. Specifically, our privacy loss bounds are either independent of the number of repetitions or grow logarithmically in the number of repetitions, whereas composition would give linear bounds. Rather than repeating the training procedure a fixed number of times, our results require repeating the training procedure a random number of times. The privacy guarantees depend on the distribution of the number of runs; we consider several distributions and provide generic results. We discover a tradeoff between the privacy parameters and how heavy-tailed the distribution of the number of repetitions is.\n1.2 BACKGROUND AND RELATED WORK Differential privacy (DP) is a framework to reason about the privacy guarantees of randomized algorithms which analyze data (Dwork et al., 2006b;a). An algorithm is said to be DP if its outputs on any pair of datasets that only differ on one individual's data are indistinguishable. A bound on this indistinguishability serves as the quantification for privacy. Formally, a randomized algorithm M : X n \u2192 Y is (\u03b5, \u03b4)-DP if for any inputs x, x \u2208 X n differing only on the addition, removal, or replacement of one individual's records and for any subset of outputs S \u2286 Y, we have\nP [M (x) \u2208 S] \u2264 e \u03b5 P [M (x ) \u2208 S] + \u03b4.(1)\nHere, the parameter \u03b5 is known as the privacy loss bound -the smaller \u03b5 is, the stronger the privacy guarantee provided is, because it is hard for an adversary to distinguish the outputs of the algorithm on two adjacent inputs. The parameter \u03b4 is essentially the probability that the guarantee fails to hold. One of the key properties of DP is that it composes: running multiple independent DP algorithms is also DP and composition theorems allow us to bound the privacy parameters of such a sequence of mechanisms in terms of the individual mechanisms' privacy parameters (Dwork & Roth, 2014).\nThere is a vast literature on differential privacy in machine learning. A popular tool is the DP-SGD optimizer (Abadi et al., 2016). Because the noise added is Gaussian and DP-SGD applies the same (differentially private) training step sequentially, it is easier to reason about its privacy guarantees in the framework of R\u00e9nyi differential privacy (Mironov, 2017). R\u00e9nyi differential privacy (RDP) generalizes pure differential privacy (with \u03b4 = 0) as follows. An algorithm M is said to be (\u03bb, \u03b5)-RDP with \u03bb \u2265 1 and \u03b5 \u2265 0, if for any adjacent inputs x, x\nD \u03bb (M (x) M (x )) := 1 \u03bb \u2212 1 log E Y \u2190M (x) P [M (x) = Y ] P [M (x ) = Y ] \u03bb\u22121 \u2264 \u03b5,(2)\nwhere D \u03bb (P Q) is the R\u00e9nyi divergence of order \u03bb between distributions P and Q. In the framework of RDP, one obtains sharp and simple composition:\nIf each individual mechanism M i is (\u03bb, \u03b5 i )-RDP,\nthen the composition of running all of the mechanisms on the data satisfies (\u03bb, i \u03b5 i )-RDP. For instance, the privacy analysis of DP-SGD first analyzes the individual training steps then applies composition. Note that it is common to keep track of multiple orders \u03bb in the analysis. Thus \u03b5 should be thought of as a function \u03b5(\u03bb), rather than a single number. In many cases, such as Gaussian noise addition, this is a linear function -i.e., \u03b5(\u03bb) = \u03c1 \u2022 \u03bb for some \u03c1 \u2208 R -and such a linear bound yields the definition of zero-Concentrated DP with parameter \u03c1 (\u03c1-zCDP) (Bun & Steinke, 2016).\nOne could na\u00efvely extend this composition-based approach to analyze the privacy of a training algorithm which involves hyperparameter tuning. Indeed, if each training run performed to evaluate one candidate set of hyperparameter values is DP, the overall procedure is also DP by composition over all the hyperparameter values tried. However, this would lead to very loose guarantees of privacy. Chaudhuri & Vinterbo (2013) were the first to obtain improved DP bounds for hyperparameter tuning, but their results require a stability property of the learning algorithm. The only prior work that has attempted to obtain tighter guarantees for DP hyperparameter tuning in a black-box fashion is the work of Liu & Talwar (2019). Their work is the starting point for ours. Liu & Talwar (2019) show that, if we start with a (\u03b5, 0)-DP algorithm, repeatedly run it a random number of times following a geometric distribution, and finally return the best output produced by these runs, then this system satisfies (3\u03b5, 0)-differential privacy. 1 Liu & Talwar (2019) also consider algorithms satisfying (\u03b5, \u03b4)-DP for \u03b4 > 0. However, their analysis is restricted to the (\u03b5, \u03b4) formulation of DP and they do not give any results for R\u00e9nyi DP. This makes it difficult to apply these results to modern DP machine learning systems, such as models trained with DP-SGD.\nOur results directly improve on the results of Liu & Talwar (2019). We show that replacing the geometric distribution on the number of repetitions in their result with the logarithmic distribution yields (2\u03b5, 0)-differential privacy as the final result. We also consider other distributions on the number of repetitions, which give a spectrum of results. We simultaneously extend these results to the R\u00e9nyi DP framework, which yields sharper privacy analyses.\nIndependently Mohapatra et al. (2021) study adaptive hyperparameter tuning under DP with composition. In contrast, our results are for non-adaptive hyperparameter tuning, i.e., \"random search.\"\nA closely related line of work is on the problem of private selection. Well-known algorithms for private selection include the exponential mechanism (McSherry & Talwar, 2007) and the sparse vector technique (Dwork et al., 2009;Zhu & Wang, 2020). However, this line of work assumes that there is a low-sensitivity function determining the quality of each of the options. This is usually not the case for hyperparameters. Our results simply treat the ML algorithm as a black box; we only assume that its output is private and make no assumptions about how that output was generated. Our results also permit returning the entire trained model along with the selected hyperparameters.", "publication_ref": ["b25", "b14", "b11", "b0", "b20", "b5", "b10", "b18", "b18", "b18", "b22", "b19", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "MOTIVATION", "text": "A hyperparameter typically takes categorical values (e.g., the choice of activation function in a neural network layer), or is a single number (e.g., a real number for the learning rate or an integer for the number of epochs). Thus, it is intuitive that a hyperparameter provides little capacity as a channel to leak private information from the training data. Nevertheless, leakage can happen, in particular when training is done without preserving privacy. We illustrate how with the constructed example of hyperparameter tuning for a support vector machine learning (SVM) learned from a synthetic data distribution. We consider a SVM with a soft margin; we use stochastic gradient descent to minimize the corresponding objective involving the hinge loss and a weight penalty:\nl w,b (x, y) = w 2 2 + \u03b1 max{0, 1 \u2212 y(w \u2022 x + b)} where y \u2208 {\u22121, 1} indicates the label of training example x \u2208 R 2 .\nBecause our purpose here is to illustrate how leakage of private information can arise from hyperparameter tuning, we work with a synthetic data distribution for simplicity of exposition: we draw 40 inputs from isotropic 2D Gaussians of standard deviation 1.0 to form the training set D. The negative class is sampled from a Gaussian centered at \u00b5 \u22121 = (7.86, \u22123.36) and the positive at \u00b5 1 = (6.42, \u22129.17).\nOur learning procedure has a single hyperparameter \u03b1 controlling how much importance is given to the hinge loss, i.e., how much the SVM is penalized for using slack variables to misclassify some of the training data. We first tune the value of \u03b1 with the training set D and report training accuracy as a function of \u03b1 in Figure 1. Next, we repeat this experiment on a dataset D to which we added 8 outliers x o = (7.9, \u22128.0) to the negative class. The resulting hyperparameter tuning curve is added to Figure 1. By comparing both curves, it is clear that the choice of hyperparameter \u03b1 which maximizes accuracy differs in the two settings: the best performance is achieved around \u03b1 = 8 with outliers whereas increasing \u03b1 is detrimental to performance without outliers.\nFigure 1: Accuracy of the model as a function of the regularization weight \u03b1, with and without outliers. Note how the model performance exhibits a turning point with outliers whereas increasing the value of \u03b1 is detrimental without outliers. This difference can be exploited to perform a variant of a membership inference attack (Shokri et al., 2017): Here, one could infer from the value of the hyperparameter \u03b1 whether or not the outlier points x o were part of the training set or not. While the example is constructed, this shows how we must be careful when tuning hyperparameters: in corner cases such as the one presented here, it is possible for some information contained in the training data to leak in hyperparameter choices.\nIn particular, this implies that the common practice of tuning hyperparameters without differential privacy and then using the hyperparameter values selected to repeat training one last time with differential privacy is not ideal. In Section 3, we will in particular show how training with differential privacy when performing the different runs necessary to tune the hyperparameter can bound such leakage effectively if one carefully chooses the number of runs hyperparameters are tuned for.", "publication_ref": ["b25"], "figure_ref": ["fig_9", "fig_9", "fig_9"], "table_ref": []}, {"heading": "OUR POSITIVE RESULTS", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "PROBLEM FORMULATION", "text": "We begin by appropriately formalizing the problem of differentially private hyperparameter tuning, following the framework of Liu & Talwar (2019). Suppose we have m randomized base algorithms M 1 , M 2 , \u2022 \u2022 \u2022 , M m : X n \u2192 Y. These correspond to m possible settings of the hyperparameters. Ideally we would simply run each of these algorithms once and return the best outcome. For simplicity, we consider a finite set of hyperparameter possibilities; if the hyperparameters of interest are continuous, then we must pick a finite subset to search over (which is in practice sufficient).\nHere we make two simplifying assumptions: First, we assume that there is a total order on the range Y, which ensures that \"best\" is well-defined. In particular, we are implicitly assuming that the algorithm computes a quality score (e.g., accuracy on a test set) for the trained model it produces; this may require allocating some privacy budget to this evaluation. 2 Second, we are assuming that the output includes both the trained model and the corresponding hyperparameter values (i.e., the output of M j includes the index j). 3 These assumptions can be made without loss of generality.", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "STRAWMAN APPROACH: REPEAT THE BASE ALGORITHM A FIXED NUMBER OF TIMES", "text": "The obvious approach to this problem would be to run each algorithm once and to return the best of the m outcomes. From composition, we know that the privacy cost grows at most linearly with m. It turns out that this is in fact tight. There exists a (\u03b5, 0)-DP algorithm such that if we repeatedly run it m times and return the best output, the resultant procedure is not (m\u03b5 \u2212 \u03c4, 0)-DP for any \u03c4 > 0. This was observed by Liu & Talwar (2019, Appendix B) and we provide an analysis in Appendix D.1. This negative result also extends to R\u00e9nyi DP. To avoid this problem, we will run the base algorithms a random number of times. The added uncertainty significantly enhances privacy. However, we must carefully choose this random distribution and analyze it.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "OUR ALGORITHM FOR HYPERPARAMETER TUNING", "text": "To obtain good privacy bounds, we must run the base algorithms a random number of times. We remark that random search rather than grid search is often performed in practice (Bergstra & Bengio, 2012), so this is not a significant change in methodology. Specifically, we pick a total number of runs K from some distribution. Then, for each run k = 1, 2, \u2022 \u2022 \u2022 , K, we pick an index j k \u2208 [m] uniformly at random and run M j k . Then, at the end, we return the best of the K outcomes.\nThe privacy guarantee of this overall system then depends on the privacy guarantees of each of the mechanisms M j as well as the distribution of the number of runs K. Specifically, we assume that there exists a uniform (R\u00e9nyi) DP bound for all of the mechanisms M j . Note that DP is \"convex\" where \"convexity\" here means that if M 1 , M 2 , \u2022 \u2022 \u2022 , M m are each individually DP, then running M j k for a random j k \u2208 [m] is also DP with the same parameters.\nTo simplify notation, we also assume that there is a single mechanism Q : X n \u2192 Y that picks a random index j \u2208 [m] and then runs M j . In essence, our goal is to \"boost\" the success probability of Q by repeating it many times. The distribution of the number of runs K must be chosen to both ensure good privacy guarantees and to ensure that the system is likely to pick a good setting of hyperparameters. Also, the overall runtime of the system depends on K and we want the runtime to be efficient and predictable. Our results consider several distributions on the number of repetitions K and the ensuing tradeoff between these three considerations.", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "MAIN RESULTS", "text": "There are many possible distributions for the number of repetitions K. In this section, we first consider two -the truncated negative binomial and Poisson distributions -and state our main privacy results for these distributions. Later, in Section 3.5, we state a more general technical lemma which applies to any distribution on the number of repetitions K.\nDefinition 1 (Truncated Negative Binomial Distribution). Let \u03b3 \u2208 (0, 1) and \u03b7 \u2208 (\u22121, \u221e). Define a distribution D \u03b7,\u03b3 on N = {1, 2, \u2022 \u2022 \u2022 } as follows. If \u03b7 = 0 and K is drawn from D \u03b7,\u03b3 , then \u2200k \u2208 N P [K = k] = (1 \u2212 \u03b3) k \u03b3 \u2212\u03b7 \u2212 1 \u2022 k\u22121 =0 + \u03b7 + 1 (3) and E [K] = \u03b7\u2022(1\u2212\u03b3) \u03b3\u2022(1\u2212\u03b3 \u03b7 ) .\nIf K is drawn from D 0,\u03b3 , then\nP [K = k] = (1 \u2212 \u03b3) k k \u2022 log(1/\u03b3)(4)\nand\nE [K] = 1/\u03b3\u22121 log(1/\u03b3) .\nThis is called the \"negative binomial distribution,\" since\nk\u22121 =0 +\u03b7 +1 = k+\u03b7\u22121 k\nif we extend the definition of binomial coefficients to non-integer \u03b7. The distribution is called \"truncated\" because P [K = 0] = 0, whereas the standard negative binomial distribution includes 0 in its support. The \u03b7 = 0 case D 0,\u03b3 is known as the \"logarithmic distribution\". The \u03b7 = 1 case D 1,\u03b3 is simply the geometric distribution. Next, we state our main privacy result for this distribution.\nTheorem 2 (Main Privacy Result -Truncated Negative Binomial). Let Q : X n \u2192 Y be a randomized algorithm satisfying (\u03bb, \u03b5)-RDP and (\u03bb,\u03b5)-RDP for some \u03b5,\u03b5 \u2265 0, \u03bb \u2208 (1, \u221e), and\u03bb \u2208 [1, \u221e). 4\nAssume Y is totally ordered.\nLet \u03b7 \u2208 (\u22121, \u221e) and \u03b3 \u2208 (0, 1). Define an algorithm A : X n \u2192 Y as follows. Draw K from the truncated negative binomial distribution D \u03b7,\u03b3 (Definition 1). Run Q(x) repeatedly K times. Then A(x) returns the best value from the K runs.\nThen A satisfies (\u03bb, \u03b5 )-RDP where Theorem 2 shows a tradeoff between privacy and utility for the distribution D \u03b7,\u03b3 of the number of repetitions. Privacy improves as \u03b7 decreases and \u03b3 increases. However, this corresponds to fewer repetitions and thus a lower chance of success. We will study this aspect in Section 3.6.\n\u03b5 = \u03b5 + (1 + \u03b7) \u2022 1 \u2212 1 \u03bb \u03b5 + (1 + \u03b7) \u2022 log(1/\u03b3) \u03bb + log E [K] \u03bb \u2212 1 .(5)\nTheorem 2 assumes two RDP bounds, which makes it slightly hard to interpret. Thus we consider two illustrative special cases: We start with pure DP (a.k.a. pointwise DP) -i.e., (\u03b5, \u03b4)-DP with \u03b4 = 0, which is equivalent to (\u221e, \u03b5)-RDP. This corresponds to Theorem 2 with \u03bb \u2192 \u221e and\u03bb \u2192 \u221e. Corollary 3 (Theorem 2 for pure DP). Let Q : X n \u2192 Y be a randomized algorithm satisfying (\u03b5, 0)-DP. Let \u03b7 \u2208 (\u22121, \u221e) and \u03b3 \u2208 (0, 1). Define A : X n \u2192 Y as in Theorem 2. Then A satisfies\n((2 + \u03b7)\u03b5, 0)-DP.\nOur result is a generalization of the result of Liu & Talwar (2019) -they show that, if K follows a geometric distribution and Q satisfies (\u03b5, 0)-DP, then A satisfies (3\u03b5, 0)-DP. Setting \u03b7 = 1 in Corollary 3 recovers their result. If we set \u03b7 < 1, then we obtain an improved privacy bound.\nAnother example is if Q satisfies concentrated DP (Dwork & Rothblum, 2016;Bun & Steinke, 2016). This is the type of guarantee that is obtained by adding Gaussian noise to a bounded sensitivity function. In particular, this is the type of guarantee we would obtain from noisy gradient descent. 5 Corollary 4 (Theorem 2 for Concentrated DP). Let Q : X n \u2192 Y be a randomized algorithm satisfying \u03c1-zCDP -i.e., (\u03bb, \u03c1\n\u2022 \u03bb)-R\u00e9nyi DP for all \u03bb > 1. Let \u03b7 \u2208 (\u22121, \u221e) and \u03b3 \u2208 (0, 1). Define A : X n \u2192 Y and K \u2190 D \u03b7,\u03b3 as in Theorem 2. Assume \u03c1 \u2264 log(1/\u03b3). Then A satisfies (\u03bb, \u03b5 )-R\u00e9nyi DP for all \u03bb > 1 with \u03b5 = \uf8f1 \uf8f2 \uf8f3 2 \u03c1 \u2022 log (E [K]) + 2(1 + \u03b7) \u03c1 log(1/\u03b3) \u2212 \u03b7\u03c1 if \u03bb \u2264 1 + 1 \u03c1 log (E [K]) \u03c1 \u2022 (\u03bb \u2212 1) + 1 \u03bb\u22121 log (E [K]) + 2(1 + \u03b7) \u03c1 log(1/\u03b3) \u2212 \u03b7\u03c1 if \u03bb > 1 + 1 \u03c1 log (E [K])\n.\nFigure 2 shows what the guarantee of Corollary 4 looks like. Here we start with 0.1-zCDP and perform repetition following the logarithmic distribution (\u03b7 = 0) with varying scales (given by \u03b3) and plot the R\u00e9nyi DP guarantee attained by outputting the best of the repeated runs. The improvement over naive composition, which instead grows linearly, is clear. We also study other distributions on the number of repetitions, obtained by varying \u03b7, and Figure 3 gives a comparison. Figure 4 shows what these bounds look like if we convert to approximate (\u03b5, \u03b4)-DP with \u03b4 = 10 \u22126 . Remark 5. Corollary 4 uses the monotonicity property of R\u00e9nyi divergences:\nIf \u03bb 1 \u2264 \u03bb 2 , then D \u03bb1 (P Q) \u2264 D \u03bb2 (P Q) (Van Erven & Harremos, 2014, Theorem 3). Thus (\u03bb 2 , \u03b5)-RDP implies (\u03bb 1 , \u03b5)-RDP for any \u03bb 1 \u2264 \u03bb 2 .\nIn particular, the bound of Theorem 2 yields \u03b5 \u2192 \u221e as \u03bb \u2192 1, so we use monotonicity to bound \u03b5 for small \u03bb.\nPoisson Distribution. We next consider the Poisson distribution, which offers a different privacyutility tradeoff than the truncated negative binomial distribution. The Poisson distribution with mean \u00b5 \u2265 0 is given by P [K = k] = e \u2212\u00b5 \u00b5 k k! for all k \u2265 0. Note that P [K = 0] = e \u2212\u00b5 > 0 here, whereas the truncated negative binomial distribution does not include 0 in its support. We could condition on K \u2265 1 here too, but we prefer to stick with the standard definition. We remark that (modulo the issue around P [K = 0]) the Poisson distribution is closely related to the truncated negative binomial distribution. If we take the limit as \u03b7 \u2192 \u221e while the mean remains fixed, then the negative binomial distribution becomes a Poisson distribution. Conversely, the negative binomial distribution can be represented as a convex combination of Poisson distributions or as a compound of Poisson and logarithmic; see Appendix A.2 for more details. Theorem 6 (Main Privacy Result -Poisson Distribution). Let Q : X n \u2192 Y be a randomized algorithm satisfying (\u03bb, \u03b5)-RDP and (\u03b5,\u03b4)-DP for some \u03bb \u2208 (1, \u221e) and \u03b5,\u03b5,\u03b4 \u2265 0. Assume Y is totally ordered. Let \u00b5 > 0.\nDefine an algorithm A : X n \u2192 Y as follows. Draw K from a Poisson distribution with mean \u00b5i.e., P\n[K = k] = e \u2212\u00b5 \u2022 \u00b5 k k! for all k \u2265 0. Run Q(x)\nrepeatedly K times. Then A(x) returns the best value from the K runs. If K = 0, A(x) returns some arbitrary output independent from the input x.\nIf e\u03b5 \u2264 1 + 1 \u03bb\u22121 , then A satisfies (\u03bb, \u03b5 )-RDP where \u03b5 = \u03b5 + \u00b5 \u2022\u03b4 + log \u00b5 \u03bb \u2212 1 .\nThe assumptions of Theorem 6 are different from Theorem 2: We assume a R\u00e9nyi DP guarantee and an approximate DP guarantee on Q, rather than two R\u00e9nyi DP guarantees. We remark that a R\u00e9nyi DP guarantee can be converted into an approximate DP guarantee - Mironov, 2017;Canonne et al., 2020). Thus this statement can be directly compared to our other result. We show such a comparison in Figure 3 and Figure 4. The proofs of Theorems 2 and 6 are included in Appendix B.2.\n(\u03bb, \u03b5)-RDP implies (\u03b5,\u03b4)-DP for all\u03b5 \u2265 \u03b5 and\u03b4 = e (\u03bb\u22121)(\u03b5\u2212\u03b5) \u2022 1 \u03bb \u2022 1 \u2212 1 \u03bb \u03bb\u22121 (", "publication_ref": ["b18", "b12", "b5", "b20", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "GENERIC R\u00c9NYI DP BOUND FOR ANY DISTRIBUTION ON THE NUMBER OF REPETITIONS", "text": "We now present our main technical lemma, which applies to any distribution on the number of repetitions K. Theorems 2 and 6 are derived from this result. It gives a R\u00e9nyi DP bound for the repeated algorithm in terms of the R\u00e9nyi DP of the base algorithm and the probability generating function of the number of repetitions applied to probabilities derived from the base algorithm.\nLemma 7 (Generic Bound). Fix \u03bb > 1. Let K be a random variable supported on N \u222a {0}. Let f : [0, 1] \u2192 R be the probability generating function of K -i.e., f (x) :=\n\u221e k=0 P [K = k] \u2022 x k .\nLet Q and Q be distributions on Y. Assume Y is totally ordered. Define a distribution A on Y as follows. First sample K. Then sample from Q independently K times and output the best of these samples. 6 This output is a sample from A. We define A analogously with Q in place of Q. Then\nD \u03bb (A A ) \u2264 D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log f (q) \u03bb \u2022 f (q ) 1\u2212\u03bb ,(6)\nwhere applying the same postprocessing to Q and Q gives probabilities q and q respectively -i.e., there exists an arbitrary function g :\nY \u2192 [0, 1] such that q = E X\u2190Q [g(X)] and q = E X \u2190Q [g(X )].\nThe proof of this generic bound is found in Appendix B.1. To interpret the theorem, we should imagine adjacent inputs x, x \u2208 X n , and then the distributions correspond to the algorithms run on these inputs: A = A(x), A = A(x ), Q = Q(x), and Q = Q(x ). The bounds on R\u00e9nyi divergence thus correspond to R\u00e9nyi DP bounds. The derivative of the probability generating function\n-f (x) = E K \u2022 x K\u22121 -is somewhat mysterious. A first-order intuition is that, if q = q , then f (q) \u03bb \u2022 f (q ) 1\u2212\u03bb = f (q) \u2264 f (1) = E [K]\nand thus the last term in the bound ( 6) is simply\nlog E[K]\n\u03bb\u22121 . A second-order intuition is that q \u2248 q by DP and postprocessing and, if f is smooth, then f (q) \u2248 f (q ) and the first-order intuition holds up to these approximations. Vaguely, f being smooth corresponds to the distribution of K being spread out (i.e., not a point mass) and not too    Figure 7: Accuracy of the CNN model obtained at the end of the hyperparameter search, for the different distributions on the number of repetitions K we considered. We report the mean over 500 trials of the experiment.\nheavy-tailed (i.e. K is small most of the time). The exact quantification of this smoothness depends on the form of the DP guarantee q \u2248 q .\nIn our work, we primarily compare three distributions on the number of repetitions: a point mass (corresponding to na\u00efve repetition), the truncated negative binomial distribution, and the Poisson distribution. A point mass would have a polynomial as the probability generating function -i.e., if\nP [K = k] = 1, then f (x) = E x K = x k .\nThe probability generating function of the truncated negative binomial distribution (Definition 1) is\nf (x) = E K\u2190D\u03b7,\u03b3 x K = (1\u2212(1\u2212\u03b3)x) \u2212\u03b7 \u22121 \u03b3 \u2212\u03b7 \u22121 if \u03b7 = 0 log(1\u2212(1\u2212\u03b3)x) log(\u03b3) if \u03b7 = 0 . (7\n)\nThe probability generating function of the Poisson distribution with mean \u00b5 is given by f (x) = e \u00b5\u2022(x\u22121) . We discuss probability generating functions further in Appendix A.2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "UTILITY AND RUNTIME OF OUR HYPERPARAMETER TUNING ALGORITHM", "text": "Our analytical results thus far, Theorems 2 and 6 and Lemma 7, provide privacy guarantees for our hyperparameter tuning algorithm A when it is used with various distributions on the number of repetitions K. We now turn to the utility that this algorithm provides. The utility of a hyperparameter search is determined by how many times the base algorithm (denoted Q in the theorem statements) is run when we invoke the overall algorithm (A). The more often Q is run, the more likely we are to observe a good output and A is more likely to return the corresponding hyperparameter values. Note that the number of repetitions K also determines the algorithm's runtime, so these are closely linked.\nHow does this distribution on the number of repetitions K map to utility? As a first-order approximation, the utility and runtime are proportional to E [K]. Hence several of our figures compare the different distributions on K based on a fixed expectation and Figure 4 plots E [K] on the vertical axis. However, this first-order approximation ignores the fact that some of the distributions we consider are more concentrated than others; even if the expectation is large, there might still be a significant probability that K is small. Indeed, for \u03b7 \u2264 1, the mode of the truncated negative binomial distribution is K = 1. We found this to be an obstacle to using the (truncated) negative binomial distribution in practice in our experiments, and discuss this further in Appendix A.2.1.\nWe can formulate utility guarantees more precisely by looking at the expected quantile of the output to measure our algorithm's utility. If we run the base algorithm Q once, then the quantile of the output is (by definition) uniform on [0, 1] and has mean 0.5. If we repeat the base algorithm a fixed number of times k, then the quantile of the best output follows a Beta(k, 1) distribution, as it is the maximum of k independent uniform random variables. The expectation in this case is k k+1 = 1 \u2212 1 k+1 . If we repeat a random number of times K, the expected quantile of the best result is given by\nE K K + 1 = E 1 \u2212 1 K + 1 = 1 0 x \u2022 f (x)dx = 1 \u2212 1 0 f (x)dx,(8)\nwhere f (x) = E x K is the probability generating function of K; see Appendix A.2.1 for further details. Figure 5 plots this quantity against privacy. We see that the Poisson distribution performs very well in an intermediate range, while the negative binomial distribution with \u03b7 = 0.5 does well if we want a strong utility guarantee. This means that Poisson is best used when little privacy budget is available for the hyperparameter search. Instead, the negative binomial distribution with \u03b7 = 0.5 allows us to improve the utility of the solution returned by the hyperparameter search, but this only holds when spending a larger privacy budget (in our example, the budget has to be at least \u03b5 = 4 otherwise Poisson is more advantageous). The negative binomial with \u03b7 = \u22120.5 does very poorly.\nFrom a runtime perspective, the distribution of K should have light tails. All of the distributions we have considered have subexponential tails. However, larger \u03b7 corresponds to better concentration in the negative binomial distribution with the Poisson distribution having the best concentration.\nExperimental Evaluation. To confirm these findings, we apply our algorithm to a real hyperparameter search task. Specifically, we fine-tune the learning rate of a convolutional neural network trained on MNIST. We implement DP-SGD in JAX for an all-convolutional architecture with a stack of 32, 32, 64, 64, 64 feature maps generated by 3x3 kernels. We vary the learning rate between 0.025 and 1 on a logarithmic scale but fix all other hyperparameters: 60 epochs, minibatch size of 256, 2 clipping norm of 1, and noise multiplier of 1.1. In Figure 7, we plot the maximal accuracy achieved during the hyperparameter search for the different distributions considered previously as a function of the total privacy budget expended by the search. The experiment is repeated 500 times and the mean result reported. This experiment shows that the Poisson distribution achieves the best privacy-utility tradeoff for this relatively simple hyperparameter search. This agrees with the theoretical analysis we just presented above that shows that the Poisson distribution performs well in the intermediate range of utility, as this is a simple hyperparameter search.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "CONCLUSION", "text": "Our positive results build on the work of Liu & Talwar (2019) and show that repeatedly running the base algorithm and only returning the best output can incur much lower privacy cost than na\u00efve composition would suggest. This however requires that we randomize the number of repetitions, rather than repeating a fixed number of times. We analyze a variety of distributions for the number of repetitions, each of which gives a different privacy/utility tradeoff.\nWhile our results focused on the privacy implications of tuning hyperparameters with, and without, differential privacy, our findings echo prior observations that tuning details of the model architecture without privacy to then repeat training with DP affords suboptimal utility-privacy tradeoffs (Papernot et al., 2020); in this work, the authors demonstrated that the optimal choice of activation function in a neural network can be different when learning with DP, and that tuning it with DP immediately can improve the model's utility at no changes to the privacy guarantee. We envision that future work will be able to build on our algorithm for private tuning of hyperparameters to facilitate privacy-aware searches for model architectures and training algorithm configurations to effectively learn with them.\nLimitations. We show that hyperparameter tuning is not free from privacy cost. Our theoretical and experimental results show that, in the setting of interest, the privacy parameter may double or even triple after accounting for hyperparameter tuning, which could be prohibitive. In this case, one compromise would be to state both privacy guarantees -that of the base algorithm that does not account for hyperparameter tuning, and that of the overall system that does account for this. The reader may wonder whether our positive results can be improved. In Appendix D, we explore give some intuition for why they cannot (easily) be improved. We also note that our results are only immediately applicable to the hyperparameter tuning algorithm from Section 3.3. Other algorithms, in particular those that adaptively choose hyperparameter candidates will require further analysis.\nFinally, among the distributions on the number of repetitions K that we have analyzed, the distribution that provides the best privacy-utility tradeoffs will depend on the setting. While it is good to have choices, this does leave some work to be done by those using our results. Fortunately, the differences between the distributions seem to be relatively small, so this choice is unlikely to be critical.", "publication_ref": ["b18", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "REPRODUCIBILITY & ETHICS STATEMENTS", "text": "Reproducibility. We give precise theorem statements for our main results and we have provided complete proofs in the Appendix, as well as all the necessary calculations and formulas for plotting our figures. We have also fully specified the setup required to reproduce our experimental results, including hyperparameters. Our algorithm is simple, fully specified and can be easily implemented.\nEthics. Our work touches on privacy, which is an ethically sensitive topic. If differentially private algorithms -such as ours -are applied to real-world sensitive data, then potential harms to the people whose data is being used must be carefully considered. However, our work is not directly using real-world sensitive data. Our main results are theoretical and our experiments use either synthetic data or MNIST, which is a standard non-private dataset. Yuqing Zhu and Yu-Xiang Wang.\nImproving sparse vector technique with renyi differential privacy.\nIn Advances in Neural Information Processing Systems 33 preproceedings (NeurIPS 2020), 2020. URL https://papers.nips.cc/paper/2020/ hash/e9bf14a419d77534105016f5ec122d62-Abstract.html.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A FURTHER BACKGROUND", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 DIFFERENTIAL PRIVACY & R\u00c9NYI DP", "text": "For completeness, we provide some basic background on differential privacy and, in particular, R\u00e9nyi differential privacy. We start with the standard definition of differential privacy: Definition 8 (Differential Privacy). A randomized algorithm M : X n \u2192 Y is (\u03b5, \u03b4)-differentially private if, for all neighbouring pairs of inputs x, x \u2208 X n and all measurable S \u2282 Y,\nP [M (x) \u2208 S] \u2264 e \u03b5 \u2022 P [M (x ) \u2208 S] + \u03b4.\nWhen \u03b4 = 0, this is referred to as pure (or pointwise) differential privacy and we may abbreviate (\u03b5, 0)-DP to \u03b5-DP. When \u03b4 > 0, this is referred to as approximate differential privacy.\nThe definition of pure DP was introduced by Dwork et al. (2006b) and approximate DP was introduced by Dwork et al. (2006a). Note that the notion of neighbouring datasets is context-dependent, but this is often glossed over. Our results are general and can be applied regardless of the specifics of what is a neighbouring dataset. (However, we do require symmetry -i.e., if (x, x ) are a pair of neighbouring inputs, then so are (x x).) Usually two datasets are said to be neighbouring if they differ only by the addition/removal or replacement of the data corresponding to a single individual. Some papers only consider addition or removal of a person's records, rather than replacement. But these are equivalent up to a factor of two.\nIn order to define R\u00e9nyi DP, we first define the R\u00e9nyi divergences: Definition 9 (R\u00e9nyi Divergences). Let P and Q be probability distributions on a common space \u2126. Assume that P is absolutely continuous with respect to Q -i.e., for all measurable S \u2282 \u2126, if Q(S) = 0, then P (S) = 0. Let P (x) and Q(x) denote the densities of P and Q respectively. 7 The KL divergence from P to Q is defined as\nD 1 (P Q) := E X\u2190P log P (X) Q(X) = \u2126 P (x) log P (x) Q(x) dx.\nThe max divergence from P to Q is defined as\nD \u221e (P Q) := sup log P (S) Q(S) : P (S) > 0 .\nFor \u03bb \u2208 (1, \u221e), the R\u00e9nyi divergence from P to Q of order \u03bb is defined as 7 In general, we can only define the ratio P (x)/Q(x) to be the Radon-Nikodym derivative of P with respect to Q. To talk about P (x) and Q(x) separately we must assume some base measure with respect to which these are defined. In most cases the base measure is either the counting measure in the case of discrete distributions or the Lesbesgue measure in the case of continuous distributions.\nD \u03bb (P Q) := 1 \u03bb \u2212 1 log E X\u2190P P (X) Q(X) \u03bb\u22121 = 1 \u03bb \u2212 1 log E X\u2190Q P (X) Q(X) \u03bb = 1 \u03bb \u2212 1 log \u2126 P (x) \u03bb Q(x) 1\u2212\u03bb dx .\nWe state some basic properties of R\u00e9nyi divergences; for further information see, e.g., Van Erven & Harremos (2014). Lemma 10. Let P , Q, P , and Q be probability distributions. Let \u03bb \u2208 [1, \u221e]. The following hold.\n\u2022 Non-negativity: D \u03bb (P Q) \u2265 0.\n\u2022 Monotonicity & Continuity: D \u03bb (P Q) is a continuous and non-decreasing function of \u03bb.\n\u2022 Data processing inequality (a.k.a. Postprocessing): Let f (P ) denote the distribution obtained by applying some (possibly randomized) function to a sample from P and let f (Q) denote the distribution obtained by applying the same function to a sample from Q. Then D \u03bb (f (P ) f (Q)) \u2264 D \u03bb (P Q).\n\u2022 Finite case suffices: We have D \u03bb (P Q) = sup f D \u03bb (f (P ) f (Q)) even when f is restricted to functions with a finite range.\n\u2022 Chain rule (a.k.a. Composition): D \u03bb (P \u00d7 P Q \u00d7 Q ) = D \u03bb (P Q) + D \u03bb (P Q ), where P \u00d7 P and Q \u00d7 Q denote the product distributions of the individual distributions.\n\u2022 Convexity: The function (P, Q) \u2192 e (\u03bb\u22121)D \u03bb (P Q) is convex for all \u03bb \u2208 (1, \u221e). The function (P, Q) \u2192 D \u03bb (P Q) convex if and only if \u03bb = 1.\nNow we can state the definition of R\u00e9nyi DP (RDP), which is due to Mironov (2017). Definition 11 (R\u00e9nyi Differential Privacy). A randomized algorithm M :\nX n \u2192 Y is (\u03bb, \u03b5)-R\u00e9nyi differentially private if, for all neighbouring pairs of inputs x, x \u2208 X n , D \u03bb (M (x) M (x )) \u2264 \u03b5.\nA closely related definition is that of zero-concentrated differential privacy (Bun & Steinke, 2016) (which is based on an earlier definition (Dwork & Rothblum, 2016) of concentrated differential privacy that does not refer to R\u00e9nyi divergences). Definition 12 (Concentrated Differential Privacy). A randomized algorithm M : X n \u2192 Y is \u03c1-zCDP if, for all neighbouring pairs of inputs x, x \u2208 X n and all \u03bb \u2208 (1, \u221e),\nD \u03bb (M (x) M (x )) \u2264 \u03c1 \u2022 \u03bb.\nUsually, we consider a family of (\u03bb, \u03b5(\u03bb))-RDP guarantees, where \u03b5(\u03bb) is a function, rather than a single function. Concentrated DP is one example of such a family, where the function is linear, and this captures the behaviour of many natural algorithms. In particular, adding Gaussian noise to a bounded sensitivity function: If f : X n \u2192 R d has sensitivity \u2206 -i.e., f (x) \u2212 f (x ) 2 \u2264 \u2206 for all neighbouring x, x -and M : X n \u2192 R d is the algorithm that returns a sample from N (f (x), \u03c3 2 I), then M satisfies \u2206 2 2\u03c3 2 -zCDP. We can convert from pure DP to concentrated or R\u00e9nyi DP as follows (Bun & Steinke, 2016).\nLemma 13. If M satisfies (\u03b5, 0)-differential privacy, then M satisfies 1 2 \u03b5 2 -zCDP -i.e., (\u03bb, 1 2 \u03b5 2 \u03bb)- RDP for all \u03bb \u2208 (1, \u221e).\nConversely, we can convert from concentrated or R\u00e9nyi DP to approximate DP as follows (Canonne et al., 2020). Lemma 14. If M satisfies (\u03bb,\u03b5)-RDP, then M satisfies (\u03b5, \u03b4)-DP where \u03b5 \u2265 0 is arbitrary and\n\u03b4 = exp((\u03bb \u2212 1)(\u03b5 \u2212 \u03b5)) \u03bb \u2022 1 \u2212 1 \u03bb \u03bb\u22121 .", "publication_ref": ["b14", "b13", "b20", "b5", "b12", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "A.2 PROBABILITY GENERATING FUNCTIONS", "text": "Let K be a random variable supported on N \u222a {0}. The probability generating function (PGF) of K is defined by\nf (x) = E x K = \u221e k=0 P [K = k] \u2022 x k .\nThe PGF f (x) is always defined for x \u2208 [0, 1], but may or may not be defined for x > 1. The PGF characterizes K. In particular, we can recover the probability mass function from the derivatives of the PGF (hence the name):\nP [K = k] = f (k) (0) k! ,\nwhere f (k) (x) denotes the k th derivative of f (x) and, in particular, P [K = 0] = f (0). We remark that is is often easiest to specify the PGF and derive the probability distribution from it, rather than vice versa; indeed, we arrived at the truncated negative binomial distribution by starting with the PGF that we want and then differentiating.\nWe can also easily recover the moments of K from the PGF: We have\nf (k) (x) = \u221e =k P [K = ] \u2022 x \u2212k \u2022 \u2022 ( \u2212 1) \u2022 ( \u2212 2) \u2022 \u2022 \u2022 ( \u2212 k + 1). In particular, f (1) = E [1] = 1 and f (1) = E [K] and f (1) = E [K(K \u2212 1)].\nNote that the PGF is a rescaling of the moment generating function (MGF) g(t) := E e tK = f (e t ).\nThe PGF can be related to the MGF in another way: Suppose \u039b is a random variable on [0, \u221e). Now suppose we draw K \u2190 Poisson(\u039b). Then the PGF of K is the MGF of \u039b -i.e., E\nx K = E \u039b E K\u2190Poisson(\u039b) x K = E \u039b e \u039b\u2022(x\u22121) = g(x \u2212 1)\n, where g is the MGF of \u039b. In particular, if \u039b is drawn from a Gamma distribution, then this would yield K from a negative binomial distribution which has a PGF of the form f NB\n(x) = 1\u2212(1\u2212\u03b3)x \u03b3 \u2212\u03b7\n. 8 Note that our results work with a truncated negative binomial distribution, which is a negative binomial conditioned on K = 0. This corresponds to an affine rescaling of the PGF, namely f TNB (x) = fNB(x)\u2212fNB( 0 ) fNB(1)\u2212fNB(0) . We can also obtain a negative binomial distribution as a compound of a Poisson distribution and a logarithmic distribution. That is, if we draw T from a Poisson distribution and draw K 1 , K 2 , \u2022 \u2022 \u2022 , K T independently from a logarithmic distribution, then K = T t=1 K t follows a negative binomial distribution. The PGF of the logarithmic distribution is given by\nf Kt (x) = E x Kt = log(1\u2212(1\u2212\u03b3)x) log(\u03b3)\nand the PGF of Poisson is given by f T (x) = E x T = e \u00b5\u2022(x\u22121) . Hence\nf K (x) = E x K = E T T t=1 E Kt x Kt = E T f Kt (x) T = f T (f Kt (x)) = exp \u00b5 \u2022 log(1 \u2212 (1 \u2212 \u03b3)x) log(\u03b3) \u2212 1 , which is equivalent to f NB (x) = 1\u2212(1\u2212\u03b3)x \u03b3 \u2212\u03b7 with \u03b7 = \u00b5 log(1/\u03b3) .\nFinally we remark that we can also use the PGF to show convergence in probability. In particular,\nlim \u03b7\u2192\u221e,\u03b3= \u03b7 \u03b7+\u00b5 f NB (x) = lim \u03b7\u2192\u221e,\u03b3= \u03b7 \u03b7+\u00b5 1 \u2212 (1 \u2212 \u03b3)x \u03b3 \u2212\u03b7 = lim \u03b7\u2192\u221e,\u03b3= \u03b7 \u03b7+\u00b5 1 \u2212 \u00b5 \u03b7 (x \u2212 1) \u2212\u03b7 = e \u00b5(x\u22121) .\nThat is, if we take the limit of the negative binomial distribution as \u03b7 \u2192 \u221e but the mean \u00b5 = \u03b7 1\u2212\u03b3 \u03b3 remains fixed, then we obtain a Poisson distribution. If we take \u03b7 \u2192 0, then f NB (x) \u2192 1, which is to say that the negative binomial distribution converges to a point mass at 0 as \u03b7 \u2192 0. However, the truncated negative binomial distribution converges to a logarithmic distribution as \u03b7 \u2192 0.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2.1 PROBABILITY GENERATING FUNCTIONS AND UTILITY", "text": "Recall that in Section 3.6, we analyzed the expected utility and runtime of different distributions on the number of repetitions K. Given our discussion of probability generating functions for these distributions, we can offer an alternative perspective on the expected utility and runtime.\nSuppose each invocation of Q has a probability 1/m of producing a \"good\" output. This would be the case if we are considering m hyperparameter settings and only one is good-where here we consider the outcome to be binary (good or bad) for simplicity and what is a good or bad is determined only by the total order on the range Y and some threshold on the quality score (e.g., accuracy). Then A has a probability\n\u03b2 := 1 \u2212 P [A(x) \u2208 Bad] = 1 \u2212 E K P [Q(x) \u2208 Bad] K = 1 \u2212 E (1 \u2212 1/m) K = 1 \u2212 f (1 \u2212 1/m)\nof outputting a good output, where f (x) = E x K is the probability generating function of the distribution. If we make the first-order approximation\nf (1 \u2212 1/m) \u2248 f (1) \u2212 f (1) \u2022 1/m = 1 \u2212 E [K]/m, then we have \u03b2 \u2248 E [K]/m.\nIn other words, for small values of 1/m, the probability of success is amplified by a multiplicative factor of E [K].\nHowever, the above first-order approximation only holds for large m and, hence, small overall success probabilities \u03b2. In practice, we want \u03b2 \u2248 1. The different distributions (Poisson and truncated negative binomial with different values of \u03b7) have very different behaviours even with the same expectation. In the regime where we want the overall success probability to be high (i.e., \u03b2 \u2248 1), smaller \u03b7 performs worse, because the distribution is more heavy-tailed. The best performing distribution is the Poisson distribution, which is almost as concentrated as na\u00efve repetition. Figure 6 show the success probability \u03b2 as a function of the final (\u03b5, 10 \u22126 )-DP guarantee. This demonstrates that there is a tradeoff between distributions.\nMore generally, we can relate the PGF of K to the expected utility of our repeated algorithm. Let X \u2208 R be random variable corresponding to the utility of one run of the base algorithm Q. E.g. X could represent the accuracy, loss, AUC/AUROC, or simply the quantile of output. Now let Y \u2208 R be the utility of our repeated algorithm A which runs the base algorithm Q repeatedly K times for a random K. That is,\nY = max{X 1 , \u2022 \u2022 \u2022 , X K } where X 1 , X 2 , \u2022 \u2022 \u2022 are independent copies of X. Let cdf X (x) = P [X \u2264 x] and cdf Y (x) = P [Y \u2264 x] = E K P X [X \u2264 x] K = f (cdf X (x)),\nwhere f (x) = E x K is the PGF of the number of repetitions K. Assuming for the moment that X is a continuous random variable, we can derive the probability density function of Y from the cumulative distribution function:\npdf Y (x) = d dx cdf X (x) = d dx f (cdf X (x)) = f (cdf X (x)) \u2022 pdf X (x).\nThis allows us to compute the expected utility:\nE [Y ] = \u221e \u2212\u221e x \u2022 pdf Y (x)dx = \u221e \u2212\u221e x \u2022 f (cdf X (x)) \u2022 pdf X (x)dx = E [X \u2022 f (cdf X (X))].\nIn particular, we can compute the expected quantile (8) in which case X is uniform on [0, 1] and, hence, cdf X (x) = x and pdf X (x) = 1 for x \u2208 [0, 1]. Integration by parts gives\nE [Y ] = 1 0 x\u2022f (x)dx = 1 0 d dx xf (x) \u2212f (x)dx = 1f (1)\u22120f (0)\u2212 1 0 f (x)dx = 1\u2212 1 0 f (x)dx.\nNote that\n1 0 f (x)dx = 1 0 E K x K dx = E K 1 0 x K dx = E K 1 K + 1 .\nFinally, we also want to ensure that the runtime of our hyperparameter tuning algorithm is wellbehaved. In particular, we wish to avoid heavy-tailed runtimes. We can obtain tail bounds on the number of repetitions K from the PGF or MGF too: For all t > 0, we have\nP [K \u2265 k] = P e t\u2022(K\u2212k) \u2265 1 \u2264 E e t\u2022(K\u2212k) = f (e t ) \u2022 e \u2212t\u2022k .\nThus, if the PGF f (x) = E x K is finite for some x = e t > 1, then we obtain a subexponential tail bound on K.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "B PROOFS FROM SECTION 3 B.1 PROOF OF GENERIC BOUND", "text": "Proof of Lemma 7. We assume that Y is a finite set and that P [K = 0] = 0; this is, essentially, without loss of generality. 9 Denote Q(\u2264 y) := y \u2208Y I[y \u2264 y] \u2022 Q(y ) and similarly for Q(< y)\nand analogously for Q in place of Q. For each y \u2208 Y, we have\nA(y) = \u221e k=1 P [K = k] \u2022 (Q(\u2264 y) k \u2212 Q(< y) k ) = f (Q(\u2264 y)) \u2212 f (Q(< y)) = Q(\u2264y) Q(<y) f (x)dx = Q(y) \u2022 E X\u2190[Q(<y),Q(\u2264y)] uniform [f (X)]\nand, likewise,\nA (y) = Q (y) \u2022 E X \u2190[Q (<y),Q (\u2264y)] uniform [f (X )]\n. Thus\ne (\u03bb\u22121)D \u03bb (A A ) = y\u2208Y A(y) \u03bb \u2022 A (y) 1\u2212\u03bb = y\u2208Y Q(y) \u03bb \u2022 Q (y) 1\u2212\u03bb \u2022 E X\u2190[Q(<y),Q(\u2264y)] [f (X)] \u03bb \u2022 E X \u2190[Q (<y),Q (\u2264y)] [f (X )] 1\u2212\u03bb \u2264 y\u2208Y Q(y) \u03bb \u2022 Q (y) 1\u2212\u03bb \u2022 E X\u2190[Q(<y),Q(\u2264y)] X \u2190[Q (<y),Q (\u2264y)] f (X) \u03bb \u2022 f (X ) 1\u2212\u03bb \u2264 e (\u03bb\u22121)D \u03bb (Q Q ) \u2022 max y\u2208Y E X\u2190[Q(<y),Q(\u2264y)] X \u2190[Q (<y),Q (\u2264y)] f (X) \u03bb \u2022 f (X ) 1\u2212\u03bb .\nThe second inequality follows from H\u00f6lder's inequality. The first inequality follows from the fact that, for any \u03bb \u2208 R, the function h\n: (0, \u221e) 2 \u2192 (0, \u221e) given by h(u, v) = u \u03bb \u2022 v 1\u2212\u03bb is convex and, hence, E [U ] \u03bb E [V ] 1\u2212\u03bb = h(E [(U, V )]) \u2264 E [h(U, V )] = E U \u03bb \u2022 V 1\u2212\u03bb\nfor any pair of positive random variables (U, V ). Note that we require X to be uniform on [Q(< y), Q(\u2264 y)] and X to be uniform on [Q (< y), Q (\u2264 y)], but their joint distribution can be arbitrary. We will couple them so that X\u2212Q(<y) Q(y)\n= X \u2212Q (<y) Q (y)\n. In particular, this implies that, for each y \u2208 Y, there exists some\nt \u2208 [0, 1] such that E X\u2190[Q(<y),Q(\u2264y)] X \u2190[Q (<y),Q (\u2264y)] f (X) \u03bb \u2022 f (X ) 1\u2212\u03bb \u2264 f (Q(< y) + t \u2022 Q(y)) \u03bb \u2022 f (Q (< y) + t \u2022 Q (y)) 1\u2212\u03bb . Hence D \u03bb (A A ) \u2264 D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log max y\u2208Y t\u2208[0,1] f (Q(< y) + t \u2022 Q(y)) \u03bb \u2022 f (Q (< y) + t \u2022 Q (y)) 1\u2212\u03bb .\nTo prove the result, we simply fix y * \u2208 Y and t * \u2208 [0, 1] achieving the maximum above and define\ng(y) := 1 if y < y * t * if y = y * 0 if y > y * .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.2 PROOFS OF DISTRIBUTION-SPECIFIC BOUNDS", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Truncated Negative Binomial Distribution", "text": "Proof of Theorem 2. The probability generating function of the truncated negative binomial distribution is\nf (x) = E K\u2190D\u03b7,\u03b3 x K = (1\u2212(1\u2212\u03b3)x) \u2212\u03b7 \u22121 \u03b3 \u2212\u03b7 \u22121 if \u03b7 = 0 log(1\u2212(1\u2212\u03b3)x) log(\u03b3) if \u03b7 = 0 . Thus f (x) = (1 \u2212 (1 \u2212 \u03b3)x) \u2212\u03b7\u22121 \u2022 \u03b7\u2022(1\u2212\u03b3) \u03b3 \u2212\u03b7 \u22121 if \u03b7 = 0 1\u2212\u03b3 log(1/\u03b3) if \u03b7 = 0 = (1 \u2212 (1 \u2212 \u03b3)x) \u2212\u03b7\u22121 \u2022 \u03b3 \u03b7+1 \u2022 E [K].\nNow we delve into the privacy analysis: Let Q = Q(x) and Q = Q(x ) denote the output distributions of Q on two neighbouring inputs. Similarly, let A = A(x) and A = A(x ) be the corresponding pair of output distributions of the repeated algorithm. By Lemma 7, for appropriate values q, q \u2208 [0, 1] and for all \u03bb > 1 and all\u03bb > 1, 10 we have\nD \u03bb (A A ) \u2264 D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log f (q) \u03bb \u2022 f (q ) 1\u2212\u03bb = D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log \u03b3 \u03b7+1 \u2022 E [K] \u2022 (1 \u2212 (1 \u2212 \u03b3)q) \u2212\u03bb(\u03b7+1) \u2022 (1 \u2212 (1 \u2212 \u03b3)q ) \u2212(1\u2212\u03bb)(\u03b7+1) = D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log \u03b3 \u03b7+1 \u2022 E [K] \u2022 (\u03b3 + (1 \u2212 \u03b3)(1 \u2212 q)) 1\u2212\u03bb \u2022 (\u03b3 + (1 \u2212 \u03b3)(1 \u2212 q ))\u03bb \u03bd \u2022 (\u03b3 + (1 \u2212 \u03b3)(1 \u2212 q)) u (\u03bb\u03bd = (\u03bb \u2212 1)(1 + \u03b7) and (1 \u2212\u03bb)\u03bd + u = \u2212\u03bb(\u03b7 + 1)) \u2264 D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log \u03b3 \u03b7+1 \u2022 E [K] \u2022 \u03b3 + (1 \u2212 \u03b3) \u2022 e (\u03bb\u22121)D\u03bb(Q Q) \u03bd \u2022 (\u03b3 + (1 \u2212 \u03b3)(1 \u2212 q)) u\n(1 \u2212 q and 1 \u2212 q are postprocessings of Q and Q respectively and e (\u03bb\u22121)D\u03bb(\u2022 \u2022) is convex and \u03bd \u2265 0)\n\u2264 D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log \u03b3 \u03b7+1 \u2022 E [K] \u2022 \u03b3 + (1 \u2212 \u03b3) \u2022 e (\u03bb\u22121)D\u03bb(Q Q) \u03bd \u2022 \u03b3 u (\u03b3 \u2264 \u03b3 + (1 \u2212 \u03b3)(1 \u2212 q) and u \u2264 0) = D \u03bb (Q Q ) + \u03bd \u03bb \u2212 1 log \u03b3 + (1 \u2212 \u03b3) \u2022 e (\u03bb\u22121)D\u03bb(Q Q) + 1 \u03bb \u2212 1 log \u03b3 \u03b7+1 \u2022 E [K] \u2022 \u03b3 u = D \u03bb (Q Q ) + \u03bd \u03bb \u2212 1 (\u03bb \u2212 1)D\u03bb (Q Q) + log 1 \u2212 \u03b3 \u2022 1 \u2212 e \u2212(\u03bb\u22121)D\u03bb(Q Q) + 1 \u03bb \u2212 1 log \u03b3 u+\u03b7+1 \u2022 E [K] = D \u03bb (Q Q ) + (1 + \u03b7) 1 \u2212 1 \u03bb D\u03bb (Q Q) + 1 + \u03b7 \u03bb log 1 \u2212 \u03b3 \u2022 1 \u2212 e \u2212(\u03bb\u22121)D\u03bb(Q Q) + log (E [K]) \u03bb \u2212 1 + 1 + \u03b7 \u03bb log(1/\u03b3) (\u03bd = (\u03bb\u22121)(1+\u03b7) \u03bb and u = \u2212(1 + \u03b7)( \u03bb\u22121 \u03bb + 1)) = D \u03bb (Q Q ) + (1 + \u03b7) 1 \u2212 1 \u03bb D\u03bb (Q Q) + 1 + \u03b7 \u03bb log 1 \u03b3 \u2212 1 + e \u2212(\u03bb\u22121)D\u03bb(Q Q) + log (E [K]) \u03bb \u2212 1 \u2264 D \u03bb (Q Q ) + (1 + \u03b7) 1 \u2212 1 \u03bb D\u03bb (Q Q) + 1 + \u03b7 \u03bb log 1 \u03b3 + log (E [K]) \u03bb \u2212 1 .\nProof of Corollary 4. We assume that Q : X n \u2192 Y is a randomized algorithm satisfying \u03c1-zCDP -i.e., (\u03bb, \u03c1 \u2022 \u03bb)-R\u00e9nyi DP for all \u03bb > 1. Substituting this guarantee into Theorem 2 (i.e., setting\n\u03b5 = \u03c1 \u2022 \u03bb and\u03b5 = \u03c1 \u2022\u03bb) gives that the repeated algorithm A satisfies (\u03bb, \u03b5 )-RDP for \u03b5 \u2264 \u03c1 \u2022 \u03bb + (1 + \u03b7) \u2022 1 \u2212 1 \u03bb \u03c1 \u2022\u03bb + (1 + \u03b7) \u2022 log(1/\u03b3) \u03bb + log E [K] \u03bb \u2212 1 .\nThis holds for all \u03bb \u2208 (1, \u221e) and all\u03bb \u2208 [1, \u221e).\nWe set\u03bb = log(1/\u03b3)/\u03c1 to minimize this expression. Note that we assume \u03c1 \u2264 log(1/\u03b3) and hence this is a valid setting of\u03bb \u2265 1. This reduces the expression to\n\u03b5 \u2264 \u03c1 \u2022 \u03bb \u2212 (1 + \u03b7) \u2022 \u03c1 + 2(1 + \u03b7) \u2022 \u03c1 \u2022 log(1/\u03b3) + log E [K] \u03bb \u2212 1 .\nThis bound is minimized when \u03bb \u2212 1 = log(E [K])/\u03c1. If \u03bb \u2212 1 < log(E [K])/\u03c1, then we can apply the monotonicity property of Renyi DP (Remark 5 and Lemma 10) and substitute in the bound with this optimal \u03bb. That is, we obtain the bound\n\u03b5 \u2264 \u03c1 \u2022 \u03bb \u2212 (1 + \u03b7) \u2022 \u03c1 + 2(1 + \u03b7) \u2022 \u03c1 \u2022 log(1/\u03b3) + log E[K] \u03bb\u22121 if \u03bb > 1 + log(E [K])/\u03c1 2 \u03c1 \u2022 log E [K] + 2(1 + \u03b7) \u03c1 \u2022 log(1/\u03b3) \u2212 \u03b7\u03c1 if \u03bb \u2264 1 + log(E [K])/\u03c1 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of the Poisson Distribution Bound", "text": "Proof of Theorem 6. The probability generating function for the Poisson distribution is f (x) = E x K = e \u00b5(x\u22121) . Thus f (x) = \u00b5\u2022e \u00b5(x\u22121) . As in the previous proofs, let x and x be neighbouring inputs. Denote Q = Q(x), Q = Q(x ), A = A(x), and A = A(x). By Lemma 7,\nD \u03bb (A A ) \u2264 D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log f (q) \u03bb \u2022 f (q ) 1\u2212\u03bb = D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log \u00b5 \u2022 e \u00b5\u03bb(q\u22121)+\u00b5(1\u2212\u03bb)(q \u22121) = D \u03bb (Q Q ) + \u00b5(\u03bbq \u2212 (\u03bb \u2212 1)q \u2212 1) + log \u00b5 \u03bb \u2212 1 = D \u03bb (Q Q ) + \u00b5((\u03bb \u2212 1)(1 \u2212 q ) \u2212 \u03bb(1 \u2212 q)) + log \u00b5 \u03bb \u2212 1 \u2264 D \u03bb (Q Q ) + \u00b5((\u03bb \u2212 1)(e\u03b5(1 \u2212 q) +\u03b4) \u2212 \u03bb(1 \u2212 q)\n) + log \u00b5 \u03bb \u2212 1 (by our (\u03b5,\u03b4)-DP assumption on Q and since 1 \u2212 q and 1 \u2212 q are postprocessings)\n= D \u03bb (Q Q ) + \u00b5 \u2022 (1 \u2212 q) \u2022 e\u03b5 \u2212 \u03bb \u03bb \u2212 1 + \u00b5 \u2022\u03b4 + log \u00b5 \u03bb \u2212 1 \u2264 D \u03bb (Q Q ) + \u00b5 \u2022\u03b4 + log \u00b5 \u03bb \u2212 1 ,\nwhere the final inequality follows from our assumption that e\u03b5 \u2264 1 + 1 \u03bb\u22121 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C CONDITIONAL SAMPLING APPROACH", "text": "In the main text, we analyzed the approach where we run the underlying algorithm Q a random number of times according to a carefully-chosen distribution and output the best result from these independent runs. An alternative approach -also studied by Liu & Talwar (2019) -is to start with a pre-defined threshold for a \"good enough\" output and to run Q repeatedly until it produces such a result and then output that. This approach has some advantages, namely being simpler and avoiding the heavy-tailed behaviour of the logarithmic distribution while attaining the same kind of privacy guarantee. However, the disadvantage of this approach is that we must specify the acceptance threshold a priori. If we set the threshold too high, then we may have to keep running Q for a long time. 11 If we set the threshold too low, then we may end up with a suboptimal output.\nWe analyze this approach under R\u00e9nyi DP, thereby extending the results of Liu & Talwar (2019).\nOur algorithm A now works as follows. We start with a base algorithm Q and a set of good outputs S. Now A(x) computes y = Q(x) and, if y \u2208 S, then it returns y and halts. Otherwise, A repeats the procedure. This is equivalent to sampling from a conditional distribution Q(x)|Q(x) \u2208 S. The number of times Q is run will follow a geometric distribution with mean 1/Q(S). Proposition 15. Let \u03bb \u2208 (1, \u221e). Let Q and Q be probability distributions on \u2126 with D \u03bb (Q Q ) < \u221e. Let S \u2282 \u2126 have nonzero measure under Q and also under Q. Let Q S and Q S denote the conditional distributions of Q and Q respectively conditioned on being in the set S. That is, Q S (E) = Q(E \u2229 S)/Q(S) and Q S (E) = Q (E \u2229 S)/Q (S) for all measurable E \u2282 \u2126. Then, for all p, q, r \u2208 [1, \u221e] satisfying 1/p + 1/q + 1/r = 1, we have\nD \u03bb (Q S Q S ) \u2264 \u03bb \u2212 1/p \u2212 1/r \u03bb \u2212 1 D r\u2022(\u03bb\u22121/p) (Q Q )+ \u03bb + 1/q \u2212 2 \u03bb \u2212 1 D \u03bb+1/q\u22121 (Q Q)+ 1/r + 1 \u03bb \u2212 1 log 1 Q(S)\n.\nProof. For x \u2208 \u2126, denote the various distributional densities at x (relative to some base measure) by Q S (x), Q S (x), Q(x), and Q (x). We have Q \ne (\u03bb\u22121)D \u03bb (QS Q S ) = \u2126 Q S (x) \u03bb Q S (x) 1\u2212\u03bb dx = Q(S) \u2212\u03bb Q (S) \u03bb\u22121 \u2126 I[x \u2208 S]Q(x) \u03bb Q (x) 1\u2212\u03bb dx \u2264 Q(S) \u2212\u03bb Q (S) \u03bb\u22121 S Q(x)dx 1/p S Q (x)dx 1/q S Q(x) \u03bb\u22121/p Q (x) 1\u2212\u03bb\u22121/q r dx 1/r (H\u00f6lder's inequality) = Q(S) 1/p\u2212\u03bb Q (S) 1/q+\u03bb\u22121 S Q(x) r\u03bb\u2212r/p Q (x) r\u2212r\u03bb\u2212r/q dx 1/r = Q (S) \u03bb0 Q(S) 1\u2212\u03bb0 Q(S) \u22121/r\u22121 S Q(x) \u03bb1 Q (x) 1\u2212\u03bb1 dx 1/r (\u03bb 0 := \u03bb + 1/q \u2212 1, \u03bb 1 := r\u03bb \u2212 r/p) \u2264 e (\u03bb0\u22121)D \u03bb 0 (Q Q) \u2022 Q(S) \u22121/r\u22121 \u2022 e (\u03bb1\u22121)D \u03bb 1 (Q Q ) 1/r . (Postprocessing & non-negativity)\nThe number of parameters in Proposition 15 is excessive. Thus we provide some corollaries that simplify the expression somewhat. Corollary 16. Let \u03bb, Q, Q , S, Q S , Q S be as in Proposition 15. The following inequalities all hold.\nD \u221e (Q S Q S ) \u2264 D \u221e (Q Q ) + D \u221e (Q Q) . D \u03bb (Q S Q S ) \u2264 D \u03bb (Q Q ) + \u03bb \u2212 2 \u03bb \u2212 1 D \u03bb\u22121 (Q Q) + 2 \u03bb \u2212 1 log 1 Q(S) . D \u03bb (Q S Q S ) \u2264 D \u221e (Q Q ) + \u03bb \u2212 2 \u03bb \u2212 1 D \u03bb\u22121 (Q Q) + 1 \u03bb \u2212 1 log 1 Q(S) . D \u03bb (Q S Q S ) \u2264 \u03bb \u03bb \u2212 1 D \u221e (Q Q ) + D \u03bb (Q Q) + 1 \u03bb \u2212 1 log 1 Q(S) . \u2200r \u2265 1 D \u03bb (Q S Q S ) \u2264 D r(\u03bb\u22121)+1 (Q Q ) + \u03bb \u2212 2 \u03bb \u2212 1 D \u03bb\u22121 (Q Q) + 1/r + 1 \u03bb \u2212 1 log 1 Q(S)\n.\nThe first inequality in Corollary 16 is essentially the result given by Liu & Talwar (2019): If Q satisfies \u03b5-DP, then A satisfies 2\u03b5-DP. ", "publication_ref": ["b18", "b18", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "D NEGATIVE RESULTS ON IMPROVEMENTS TO OUR ANALYSIS", "text": "It is natural to wonder whether our results could be further improved. In this section, we give some examples that demonstrates that quantitatively there is little room for improvement.\nD.1 WHY A FIXED NUMBER OF REPETITIONS DOES NOT RESULT IN GOOD PRIVACY.\nWe first consider more closely the strawman approach discussed in Section 3.2: the base algorithm Q is repeated a fixed number of times k and we return the best output. This corresponds to picking k from a point mass distribution. To understand why it performs so poorly from a privacy standpoint, we first apply our main result from Section 3.5 to the resulting point mass distribution.\nPoint Mass: Suppose K is just a point mass -i.e., P [K = k] = 1. So A runs the algorithm Q a deterministic number of times. Then the probability generating function (PGF) is f (x) = x k and its derivative is f (x) = k \u2022 x k\u22121 . Let Q denote the base algorithm. We abuse notation and let Q = Q(x) and Q = Q(x ), where x and x are neighbouring inputs. Similarly, let A = A(x) and A = A(x ) be the final output distributions obtained by running Q and Q repeatedly k times and returning the best result. We follow the same pattern of analysis that we applied to the other distributions in Theorems 2 and 6: Lemma 7 gives the bound\nD \u03bb (A A ) \u2264 D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log k \u2022 q \u03bb \u2022 q 1\u2212\u03bb k\u22121 \u2264 D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log k \u2022 e (\u03bb\u22121)D \u03bb (Bern(q) Bern(q )) k\u22121 \u2264 D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log k \u2022 e (\u03bb\u22121)D \u03bb (Q Q ) k\u22121 = k \u2022 D \u03bb (Q Q ) + log k \u03bb \u2212 1 ,\nwhere the final inequality follows from the fact that Bern(q) and Bern(q ) are postprocessings of Q and Q respectively.\nThis bound is terrible. In fact, it is slightly worse than a na\u00efve composition analysis which would give\nD \u03bb (A A ) \u2264 D \u03bb Q \u2297k Q \u2297k = k \u2022 D \u03bb (Q Q ).\nIt shows that a deterministic number of repetitions does not yield good privacy parameters, at least with this analysis.\nIt is surprising that running the base algorithm Q a fixed number of times k and returning the best output performs so poorly from a privacy standpoint. We will now give a simple example that demonstrates that this is inherent and not just a limitation of our analysis. Liu & Talwar (2019, Appendix B) give a similar example. Proposition 17. For all \u03b5 > 0, there exists a \u03b5-DP algorithm Q : X n \u2192 Y such that the following holds. Define an algorithm A : X n \u2192 Y that runs Q a fixed number of times k and returns the best output from these runs. Then A is not\u03b5-DP for any\u03b5 < k\u03b5. Furthermore, for all \u03bb > 1, A is not (\u03bb,\u03b5(\u03bb))-R\u00e9nyi DP for any\u03b5(\u03bb) < \u03b5 (\u03bb), where\n\u03b5 (\u03bb) = k\u03b5 \u2212 k \u2022 log(1 + e \u2212\u03b5 ) \u03bb \u2212 1 .\nProof. The base algorithm is simply randomized response. We will let Y = {1, 2} with the total order preferring 1, then 2. We will define a pair of distributions Q and Q on {1, 2} and then the base algorithm is simply set so that these are its output distributions on a pair of neighbouring inputs.\nWe let\nQ = 1 1 + e \u03b5 , e \u03b5 1 + e \u03b5 , Q = e \u03b5 1 + e \u03b5 , 1 1 + e \u03b5 .\nThen D \u221e (Q Q ) = D \u221e (Q Q) = \u03b5. Thus we can ensure that the base algorithm yielding this pair of distributions is \u03b5-DP. Now we look at the corresponding pair of distributions from repeating the base algorithm k times. We have\nA = 1 \u2212 e \u03b5 1 + e \u03b5 k , e \u03b5 1 + e \u03b5 k , A = 1 \u2212 1 1 + e \u03b5 k , 1 1 + e \u03b5 k .\nThe first part of the result follows:\nD \u221e (A A ) \u2265 log \uf8eb \uf8ec \uf8ed e \u03b5 1+e \u03b5 k 1 1+e \u03b5 k \uf8f6 \uf8f7 \uf8f8 = k\u03b5. For all \u03bb > 1, e (\u03bb\u22121)D \u03bb (A A ) \u2265 e \u03b5 1 + e \u03b5 k \u03bb \u2022 1 1 + e \u03b5 k 1\u2212\u03bb = e \u03b5k\u03bb \u2022 (1 + e \u03b5 ) \u2212k Hence D \u03bb (A A ) \u2265 \u03b5k\u03bb \u2212 k \u2022 log(1 + e \u03b5 ) \u03bb \u2212 1 = k\u03b5 \u2212 k \u2022 log(1 + e \u2212\u03b5 ) \u03bb \u2212 1 .\nThe second part of Proposition 17 shows that this problem is not specific to pure DP. For \u03bb \u2265 1 + 1/\u03b5, we have \u03b5 (\u03bb) = \u2126(k\u03b5), so we are paying linearly in k.\nHowever, Proposition 17 is somewhat limited to pure \u03b5-DP or at least (\u03bb, \u03b5(\u03bb))-RDP with not-toosmall values of \u03bb. This is because the \"bad\" event is relatively low-probability. Specifically, the high privacy loss event has probability (1 + e \u2212\u03b5 ) \u2212k . This is small, unless \u03b5 \u2265 \u2126(log k).\nWe can change the example to make the bad event happen with constant probability. However, the base algorithm will also not be pure \u03b5-DP any more. Specifically, we can replace the two distributions in Proposition 17 with the following:\nQ = (1 \u2212 exp(\u22121/k), exp(\u22121/k)), Q = (1 \u2212 exp(\u2212\u03b5 0 \u2212 1/k), exp(\u2212\u03b5 0 \u2212 1/k)).\nIf we repeat this base algorithm a fixed number of times k, then the corresponding pair of distributions is given by\nA = (1 \u2212 exp(\u22121), exp(\u22121)), A = (1 \u2212 exp(\u2212k\u03b5 0 \u2212 1), exp(\u2212k\u03b5 0 \u2212 1)).\nNow we have D \u221e (A A ) = k\u03b5 0 and the bad event happens with probability e \u22121 \u2248 0.36. On the other hand,\nD \u221e (Q Q ) = \u03b5 0 like before, but D \u221e (Q Q) = log(1 \u2212 exp(\u2212\u03b5 0 \u2212 1/k)) \u2212 log(1 \u2212 exp(\u22121/k))\n\u2248 log(k\u03b5 0 + 1). But we still have a good guarantee in terms of R\u00e9nyi divergences. In particular, D 1 (Q Q) \u2264 (\u03b5 0 + 1/k) log(k\u03b5 0 + 1), and we can set \u03b5 0 \u2264 o(1/ log k) to ensure that we get reasonable (\u03bb, \u03b5(\u03bb))-RDP guarantees for small \u03bb.\nAt a higher level, it should not be a surprise that this negative example is relatively brittle. Our positive results show that it only takes a very minor adjustment to the number of repetitions to obtain significantly tighter privacy guarantees for hyperparameter tuning than what one would obtain from naive composition. In particular, running a fixed number of times k versus running Poisson(k) times is not that different, but our positive results show that it already circumvents this problem in general.\nWe also remark that composition behaves differently in the low-order RDP or approx DP regime relative to the pure-DP or high-order RDP regime covered by Proposition 17. Thus the na\u00efve composition baseline we compare to is also shifting from basic composition (\u03b5-DP becomes k\u03b5-DP under k-fold repetition (Dwork et al., 2006b)) to advanced composition (\u03b5-DP becomes (O(\u03b5 \u2022 k \u2022 log(1/\u03b4)), \u03b4)-DP (Dwork et al., 2010)). Proving tightness of basic composition is easy, but proving tightness for advanced composition is non-trivial (in general, it relies on the machinery of fingerprinting codes (Bun et al., 2014)). This means it is not straightforward to extend Proposition 17 to this regime.", "publication_ref": ["b14", "b16", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "D.2 TIGHT EXAMPLE FOR CONDITIONAL SAMPLING.", "text": "We are also interested in the tightness of our generic results. We begin by studying the conditional sampling approach outlined in Appendix C. This approach is simpler and it is therefore easier to give a tight example. This also proves to be instructive for the random repetition approach in Section 3.\nOur tight example for the conditional sampling approach consists of a pair of distributions Q and Q . These should be thought of as the output distributions of the algorithm Q on two neighbouring inputs. The distributions are supported on only three points. Such a small output space seems contrived, but it should be thought of as representing a partition of a large output space into three sets. The first set is where the privacy loss is large and the second set is where the privacy loss is very negative, while the third set is everything in between.\nFix s, t > 0 and a\n\u2208 [0, 1/4]. Note (1 \u2212 2a) \u22121 \u2264 e 3a . Let Q = a \u2022 e \u2212s , a \u2022 e \u2212s , 1 \u2212 2a \u2022 e \u2212s , Q = a \u2022 e \u2212s\u2212t , a, 1 \u2212 a \u2212 a \u2022 e \u2212s\u2212t , S = {1, 2} \u2282 {1, 2, 3}, Q S = 1 2 , 1 2 , Q S = e \u2212s\u2212t 1 + e \u2212s\u2212t , 1 1 + e \u2212s\u2212t .\nIntuitively, the set S corresponds to the outputs that (1) have large privacy loss or (2) very negative privacy loss and we exclude (3) the outputs with middling privacy loss. Once we condition on S we still have the outputs (1) with large privacy loss, but that privacy loss is further increased because of the renormalization. Specifically, the negative privacy loss means the renormalization constants are very different -Q(S) = a \u2022 2e \u2212s Q (S) = a \u2022 (1 + e \u2212s\u2212t ) -if s is large. In effect, the negative privacy loss becomes a positive privacy loss that is added to the already large privacy loss.  We make the above intuition more precise by calculating the various quantities. For all \u03bb > 1, we have\nQ(S) = 2a \u2022 e \u2212s , e (\u03bb\u22121)D \u03bb (Q Q ) = a \u2022 e \u2212s\u03bb\u2212(s+t)(1\u2212\u03bb) + a \u2022 e \u2212s\u03bb + (1 \u2212 2a \u2022 e \u2212s ) \u03bb (1 \u2212 a \u2212 a \u2022 e \u2212s\u2212t ) 1\u2212\u03bb \u2264 a \u2022 e (\u03bb\u22121)t\u2212s + ae \u2212s\u03bb + e \u22122ae \u2212s \u03bb (1 \u2212 2a) 1\u2212\u03bb \u2264 a \u2022 e (\u03bb\u22121)t\u2212s + a + e 3a(\u03bb\u22121) \u2248 a \u2022 e (\u03bb\u22121)t\u2212s (assuming t is large) = 1 2 Q(S) \u2022 e (\u03bb\u22121)t , =\u21d2 D \u03bb (Q Q ) t \u2212 log(2/Q(S)) \u03bb \u2212 1 , e (\u03bb\u22121)D \u03bb (Q Q) = a \u2022 e \u2212s(1\u2212\u03bb)\u2212(s+t)\u03bb + a \u2022 e \u2212s(1\u2212\u03bb) + (1 \u2212 2a \u2022 e \u2212s ) 1\u2212\u03bb (1 \u2212 a \u2212 a \u2022 e \u2212s\u2212t ) \u03bb \u2264 a \u2022 e \u2212t\u03bb\u2212s + a \u2022 e s(\u03bb\u22121) + e 3a\u2022e \u2212s \u2022(\u03bb\u22121) \u2022 e \u2212a\u03bb \u2248 a \u2022 e s(\u03bb\u22121) (assuming s is large) = 1 2 Q(S) \u2022 e s\u03bb =\u21d2 D \u03bb (Q Q) s + s \u2212 log(2/Q(S)) \u03bb \u2212 1 , e (\u03bb\u22121)D \u03bb (QS Q S ) = 2 \u2212\u03bb (1 + e \u2212s\u2212t ) \u03bb\u22121 (e (\u03bb\u22121)(s+t) + 1) \u2265 2 \u2212\u03bb \u2022 e (\u03bb\u22121)(s+t) , =\u21d2 D \u03bb (Q S Q S ) \u2265 s + t \u2212 \u03bb log 2 \u03bb \u2212 1 .\nWe contrast this with our upper bound from the second part of Corollary 16:\nD \u03bb (Q S Q S ) \u2264 D \u03bb (Q Q ) + \u03bb \u2212 2 \u03bb \u2212 1 D \u03bb\u22121 (Q Q) + 2 \u03bb \u2212 1 log 1 Q(S) . t \u2212 log(2/Q(S)) \u03bb \u2212 1 + \u03bb \u2212 2 \u03bb \u2212 1 s + s \u2212 log(2/Q(S)) \u03bb \u2212 2 + 2 log(1/Q(S)) \u03bb \u2212 1 = s + t \u2212 2 log 2 \u03bb \u2212 1 .\nThis example shows that our upper bound is tight up to small factors, namely the lower order terms we ignore with \u2248 and \u03bb\u22122 \u03bb\u22121 log 2. Figure 8 illustrates how the upper and lower bounds compare.", "publication_ref": [], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "D.3 TIGHTNESS OF OUR GENERIC RESULT.", "text": "Now we consider the setting from Section 3 where our base algorithm is run repeatedly a random number of times and the best result is given as output.\nLet Q : X n \u2192 Y denote the base algorithm. Assume Y is totally ordered. Let K \u2208 N be a random variable and let f (x) = E x K be its probability generating function. Define A : X n \u2192 Y to be the algorithm that runs Q repeatedly K times and returns the best output.\nFor a tight example, we again restrict our attention to distributions supported on three points:\nQ = Q(x) = (1 \u2212 b \u2212 c, b, c), Q = Q(x ) = (1 \u2212 b \u2212 c , b , c ), A = A(x) = (1 \u2212 f (b + c), f (b + c) \u2212 f (c), f (c)), A = A(x ) = (1 \u2212 f (b + c ), f (b + c ) \u2212 f (c ), f (c )).\nHere the total ordering prefers the first option (corresponding to the first coordinate probability), then the second, and then the third, which implies the expressions for A and A . Note that the probability values are not necessarily ordered the same way as the ordering on outcomes. Now we must set these four values to show tightness of our results.\nWe make the first-order approximation\nA \u2248 (1 \u2212 f (b + c), f (c) \u2022 b, f (c)), A \u2248 (1 \u2212 f (b + c ), f (c ) \u2022 b , f (c )).\nWe take this approximation and get\ne (\u03bb\u22121)D \u03bb (A A ) (f (c) \u2022 b) \u03bb \u2022 (f (c ) \u2022 b ) 1\u2212\u03bb = e (\u03bb\u22121)D \u03bb (b b ) \u2022 (f (c)) \u03bb \u2022 (f (c )) 1\u2212\u03bb \u2248 e (\u03bb\u22121)D \u03bb (Q Q ) \u2022 (f (c)) \u03bb \u2022 (f (c )) 1\u2212\u03bb ,\nwhere the final approximation assumes that the second term is dominant in the equation\ne (\u03bb\u22121)D \u03bb (Q Q ) = (1 \u2212 b \u2212 c) \u03bb \u2022 (1 \u2212 b \u2212 c ) 1\u2212\u03bb + b \u03bb \u2022 (b ) 1\u2212\u03bb + (c) \u03bb \u2022 (c ) 1\u2212\u03bb .\nContrast this with our upper bound (Lemma 7), which says\ne (\u03bb\u22121)D \u03bb( A A ) \u2264 e (\u03bb\u22121)D \u03bb (Q Q ) \u2022 (f (q)) \u03bb \u2022 (f (q )) 1\u2212\u03bb ,\nwhere q and q are arbitrary postprocessings of Q and Q . In particular, we can set the values so that q = c and q = c . This is not a formal proof, since we make imprecise approximations. But it illustrates that our main generic result (Lemma 7) is tight up to low order terms.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.4 SELECTION & LOWER BOUNDS", "text": "Private hyperparameter tuning is a generalization of the private selection problem. In the private selection problem we are given a utility function u : X n \u00d7 [m] \u2192 R which has sensitivity 1 in its first argument -i.e., for all neighbouring x, x \u2208 X n and all j \u2208 [m] = {1, 2, \u2022 \u2022 \u2022 , m}, we have |u(x, j) \u2212 u(x , j)| \u2264 1. The goal is to output and approximation to arg max j\u2208[m] u(x, j) subject to differential privacy.\nThe standard algorithm for private selection is the exponential mechanism (McSherry & Talwar, 2007). The exponential mechanism is defined by\n\u2200j \u2208 [m] P [M (x) = j] = exp \u03b5 2 u(x, j) \u2208[m] exp \u03b5 2 u(x, )\n.\nIt provides (\u03b5, 0)-DP and, at the same time, 1 8 \u03b5 2 -zCDP (Rogers & Steinke, 2021). On the utility side, we have the guarantees\nE [u(x, M (x))] \u2265 max j\u2208[m] u(x, j) \u2212 2 \u03b5 log m, P u(x, M (x)) \u2265 max j\u2208[m] u(x, j) \u2212 2 \u03b5 log m \u03b2 \u2265 1 \u2212 \u03b2\nfor all inputs x and all \u03b2 > 0 (Bassily et al., 2021, Lemma 7.1) (Dwork & Roth, 2014, Theorem 3.11).\nIt is also well-known that the exponential mechanism is optimal up to constants. That is, (\u03b5, 0)-DP selection entails an additive error of \u2126(log(m)/\u03b5) (Steinke & Ullman, 2017).\nOur results can be applied to the selection problem. The base algorithm Q(x) will simply pick an index j \u2208 [m] uniformly at random and the privately estimate u(x, j) by adding Laplace or Gaussian noise \u03be and output the pair (j, u(x, j) + \u03be). The total order on the output space [m] \u00d7 R simply selects for the highest estimated utility (breaking ties arbitrarily). If we take \u03be to be Laplace noise with scale 1/\u03b5, then Q is (\u03b5, 0)-DP.\nApplying Corollary 3 yields a ((2 + \u03b7)\u03b5, 0)-DP algorithm A with the following utility guarantee. Let K be the number of repetitions and let (j 1 , u(x, j 1 ) + \u03be 1 ), \u2022 \u2022 \u2022 , (j K , u(x, j K ) + \u03be K ) be the outputs from the runs of the base algorithm. The probability that the repeated algorithm A will consider j * := arg max j\u2208[m] u(x, j) is P [j * \u2208 {j 1 , \u2022 \u2022 \u2022 , j K }] = 1 \u2212 E K k\u2208[K] P [j * = j k ] = 1 \u2212 f (1 \u2212 1/m), where f (x) = E x K is the probability generating function of the number of repetitions K. For each noise sample, we have \u2200k P [|\u03be k | \u2264 t] \u2265 1\u2212e \u2212\u03b5t for all t > 0. Thus, for all t > 0, the probability that all noise samples are smaller than t is P [\u2200k \u2208 [K] |\u03be k | \u2264 t] = f (1 \u2212 e \u2212\u03b5t ). By a union bound, we have P [u(x, M (x)) \u2265 u(x, j * ) \u2212 2t] \u2265 f (1\u2212e \u2212\u03b5t )\u2212f (1\u22121/m) for all t > 0. Setting \u03b7 = 0, yields . That is, we can match the result of the exponential mechanism up to (large) constants. In particular, this means that the lower bounds for selection translate to our results -i.e., our results are tight up to constants.\nf (x) = log(1\u2212(1\u2212\u03b3)x) log \u03b3 , so P [u(x, M (x)) \u2265 u(x, j * ) \u2212 2t] \u2265 1 log(1/\u03b3) log 1+ 1\u2212\u03b3 \u03b3 \u2022 1 m 1+ 1\u2212\u03b3 \u03b3 \u2022", "publication_ref": ["b19", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "E EXTENDING OUR RESULTS TO APPROXIMATE DP", "text": "Our results are all in the framework of R\u00e9nyi DP. A natural question is what can be said if the base algorithm instead only satisfies approximate DP -i.e., (\u03b5, \u03b4)-DP with \u03b4 > 0. Liu & Talwar (2019) considered these questions and gave several results. We now briefly show how to our results can be extended in a black-box fashion to this setting.\nWe begin by defining approximate R\u00e9nyi divergences and approximate R\u00e9nyi DP: For all \u03bb \u2264 1 + 1 e \u03b5 \u22121 , the algorithm A satisfies \u03b4 -approximate (\u03bb, \u03b5 )-RDP where \u03b5 = \u03b5 0 + (e \u03b50 \u2212 1) \u2022 log \u00b5, \u03b4 = 1 \u2212 e \u2212\u00b5\u2022\u03b40 \u2264 \u00b5 \u2022 \u03b4 0 .\nProof of Lemma 20. For a distribution P on Y and an integer k \u2265 0, let max P k denote the distribution on Y obtained by taking k independent samples from P and returning the maximum value per the total ordering on Y. (If k = 0, this is some arbitrary fixed distribution.)\nUsing this notation, we can express A K Q as a convex combination:\nA K Q = \u221e k=0 P [K = k] max Q k .\nSuppose P = (1 \u2212 \u03b4)P + \u03b4P is a convex combination. We can view sampling from P as a two-step process: first we sample a Bernoulli random variable B \u2208 {0, 1} with expectation \u03b4; if B = 0, we return a sample from P and, if B = 1, we return a sample from P . Thus, if we draw k independent samples from P like this, then with probability (1 \u2212 \u03b4) k all of these Bernoullis are 0 and we generate k samples from P ; otherwise, we generate some mix of samples from P and P . Hence we can write max P k = (1 \u2212 \u03b4) k max(P ) k + (1 \u2212 (1 \u2212 \u03b4) k )P for some distribution P .\nIt follows that we can express\nA K Q = \u221e k=0 P [K = k] max Q k = \u221e k=0 P [K = k] (1 \u2212 \u03b4 0 ) k max Q k 1\u2212\u03b40 + (1 \u2212 (1 \u2212 \u03b4 0 ) k )P k = f (1 \u2212 \u03b4 0 )A K Q + (1 \u2212 f (1 \u2212 \u03b4 0 ))P *\nfor some distributions P 0 , P 1 , \u2022 \u2022 \u2022 and (1 \u2212 f (1 \u2212 \u03b4 0 ))P * = \u221e k=0 P [K = k](1 \u2212 (1 \u2212 \u03b4 0 ) k )P k . Similarly, we can express A K Q = f (1 \u2212 \u03b4 0 )A K Q 1\u2212\u03b4 0\n+ (1 \u2212 f (1 \u2212 \u03b4 0 ))P * for some distribution P * .\nUsing these convex combinations we have, by the definition of approximate R\u00e9nyi divergence, . The base algorithm, corresponding to the pair Q 1\u2212\u03b40 and Q 1\u2212\u03b40 satisfies (\u03b5 0 , 0)-DP. This yields\nD \u03b4 \u03bb A K Q A K Q \u2264 D \u03bb A K Q 1\u2212\u03b4 0 A K Q 1\u2212\u03b4 0 as \u03b4 = 1 \u2212 f (1 \u2212 \u03b4 0 ).\nD \u03bb A K Q 1\u2212\u03b4 0 A K Q 1\u2212\u03b4 0 \u2264 \u03b5 0 + log((1 \u2212 \u03b4 0 ) \u2022 \u00b5) \u03bb \u2212 1\nif e \u03b50 \u2264 1 + 1/(\u03bb \u2212 1). Setting \u03bb = 1 + 1/(e \u03b50 \u2212 1) and applying monotonicity (Remark 5 and Lemma 10) and we obtain the result.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.1 TRUNCATING THE NUMBER OF REPETITIONS", "text": "Another natural question is what happens if we truncate the distribution of the number of repetitions K. For example, we may have an upper limit on the acceptable runtime. This does not require relaxing to approximate DP, as done by Liu & Talwar (2019).\nLet K be the non-truncated number of repetitions and let f (x) = E x K be the PGF. Let m \u2208 N. Let K be the truncated number of repetitions. That is,\nP K = k = I[K \u2264 m] \u2022 P [K = k]/P [K \u2264 m].\nLetf (x) = E xK be the corresponding PGF.\nWe have\nf (x) = \u221e k=1 k \u2022 x k\u22121 \u2022 P [K = k] andf (x) = m k=1 k \u2022 x k\u22121 \u2022 P[K=k] P[K\u2264m] . Hence, for x \u2208 [0, 1], 0 \u2264f (x) \u2212f (x) \u2022 P [K \u2264 m] = \u221e k=m+1 k \u2022 x k\u22121 \u2022 P [K = k] \u2264 x m \u2022 \u221e k=m+1 k \u2022 P [K = k] = x m \u2022 E [K \u2022 I[K > m]]. Nowf (x) \u2022 P [K \u2264 m] = m k=1 k \u2022 x k\u22121 \u2022 P [K = k] \u2265 x m\u22121 \u2022 E [K \u2022 I[K \u2264 m]]. Thus 1 \u2264 f (x) f (x) \u2022 P [K \u2264 m] \u2264 1 + x m \u2022 E [K \u2022 I[K > m]] x m\u22121 \u2022 E [K \u2022 I[K \u2264 m]] = 1 + x \u2022 E [K \u2022 I[K > m]] E [K] \u2212 E [K \u2022 I[K > m]]\nNow we can bound the quantity of interest in Lemma 7: For all q, q \u2208 [0, 1], we hav\u1ebd\nf (q) \u03bb \u2022f (q ) 1\u2212\u03bb \u2264 f (q) P [K \u2264 m] \u03bb \u2022 \uf8eb \uf8ed f (q ) P [K \u2264 m] \u2022 1 + E[K\u2022I[K>m]] E[K]\u2212E[K\u2022I[K>m]] \uf8f6 \uf8f8 1\u2212\u03bb = f (q) \u03bb \u2022 f (q ) 1\u2212\u03bb \u2022 1 P [K \u2264 m] \u2022 1 + E [K \u2022 I[K > m]] E [K] \u2212 E [K \u2022 I[K > m]] \u03bb\u22121\nThis gives us a generic result for truncated distributions: Lemma 22 (Generic Bound (cf. Lemma 7) for Truncated distributions). Fix \u03bb > 1 and m \u2208 N. Let K be a random variable supported on N \u222a {0}. Let f : [0, 1] \u2192 R be the probability generating function of K -i.e., f (x) :=\n\u221e k=0 P [K = k] \u2022 x k .\nLet Q and Q be distributions on Y. Assume Y is totally ordered. Define a distribution A on Y as follows. First we sampleK which is K conditioned on K \u2264 m -i.e. P K = k = P [K = k|K \u2264 m].\nThen we sample from Q independentlyK times and output the best of these samples. 13 This output is a sample from A. We define A analogously with Q in place of Q.", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "Then", "text": "D \u03bb (A A ) \u2264 D \u03bb (Q Q )+ 1 \u03bb \u2212 1 log f (q) \u03bb \u2022 f (q ) 1\u2212\u03bb + log 1 1\u2212P[K>m] \u03bb \u2212 1 +log 1 + E [K \u2022 I[K > m]] E [K] \u2212 E [K \u2022 I[K > m]] ,(9)\nwhere q and q are probabilities attained by applying the same arbitrary postprocessing to Q and Q respectively -i.e., there exists a function g : Y \u2192 [0, 1] such that q = E X\u2190Q [g(X)] and q = E ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ACKNOWLEGMENTS", "text": "The authors would like to thank the reviewers for their detailed feedback and interactive discussion during the review period. We also thank our colleagues Abhradeep Guha Thakurta, Andreas Terzis, Peter Kairouz, and Shuang Song for insightful discussions about differentially private hyperparameter tuning that led to the present project, as well as their comments on early drafts of this document.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Definition 18 (Approximate R\u00e9nyi Divergence). Let P and Q be probability distributions over \u2126. Let \u03bb \u2208 [1, \u221e] and \u03b4 \u2208 [0, 1]. We define D \u03b4 \u03bb (P Q) = inf {D \u03bb (P Q ) : P = (1 \u2212 \u03b4)P + \u03b4P , Q = (1 \u2212 \u03b4)Q + \u03b4Q } , where P = (1 \u2212 \u03b4)P + \u03b4P denotes the fact that P can be expressed as a convex combination of two distributions P and P with weights 1 \u2212 \u03b4 and \u03b4 respectively. Definition 19 (Approximate R\u00e9nyi Differential Privacy). A randomized algorithm M :\nDefinition 19 is an extension of the definition of approximate zCDP (Bun & Steinke, 2016). Some remarks about the basic properties of approximate RDP are in order:\nOur results for R\u00e9nyi DP can be extended to approximate R\u00e9nyi DP by the following Lemma. Lemma 20. Assume Y is a totally ordered set. For a distribution Q on Y and a random variable K supported on N \u222a {0}, define A K Q as follows. First we sample K. Then we sample from Q independently K times and output the best of these samples. This output is a sample from A.\nLet K be a random variable on N \u222a {0} and let f (x) = E x K be the probability generating function of K. Define a random variable K on N \u222a {0} by P\nThen, for all \u03bb \u2265 1, we have\nHow do we use this lemma? We should think of A K Q as representing the algorithm we want to analyze. The base algorithm Q satisfies \u03b4 0 -approximate (\u03bb, \u03b5)-RDP. The above lemma says it suffices to analyze the algorithm A K Q whereQ satisfies (\u03bb, \u03b5)-RDP. We end up with a \u03b4-approximate RDP result, where the final \u03b4 depends on \u03b4 0 and the PGF of K.\nAs an example, we can combine Lemma 20 with Theorem 6 to obtain the following result for the approximate case. Corollary 21. Let Q : X n \u2192 Y be a randomized algorithm satisfying (\u03b5 0 , \u03b4 0 )-DP. Assume Y is totally ordered. Let \u00b5 > 0.\nDefine an algorithm A : X n \u2192 Y as follows. Draw K from a Poisson distribution with mean \u00b5. Run Q(x) repeatedly K times. Then A(x) returns the best value from the K runs. (If K = 0, A(x) returns some arbitrary output independent from the input x.)", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Deep learning with differential privacy", "journal": "", "year": "2016", "authors": "Martin Abadi; Andy Chu; Ian Goodfellow; Brendan Mcmahan; Ilya Mironov; Kunal Talwar; Li Zhang"}, {"ref_id": "b1", "title": "Privacy amplification by subsampling: Tight analyses via couplings and divergences", "journal": "", "year": "2018", "authors": "Borja Balle; Gilles Barthe; Marco Gaboardi"}, {"ref_id": "b2", "title": "Private empirical risk minimization: Efficient algorithms and tight error bounds", "journal": "IEEE 55th Annual Symposium on Foundations of Computer Science", "year": "2014", "authors": "Raef Bassily; Adam Smith; Abhradeep Thakurta"}, {"ref_id": "b3", "title": "Algorithmic stability for adaptive data analysis", "journal": "SIAM Journal on Computing", "year": "2021", "authors": "Raef Bassily; Kobbi Nissim; Adam Smith; Thomas Steinke; Uri Stemmer; Jonathan Ullman"}, {"ref_id": "b4", "title": "Random search for hyper-parameter optimization", "journal": "Journal of machine learning research", "year": "2012", "authors": "James Bergstra; Yoshua Bengio"}, {"ref_id": "b5", "title": "Concentrated differential privacy: Simplifications, extensions, and lower bounds", "journal": "Springer", "year": "2016", "authors": "Mark Bun; Thomas Steinke"}, {"ref_id": "b6", "title": "Fingerprinting codes and the price of approximate differential privacy", "journal": "", "year": "2014", "authors": "Mark Bun; Jonathan Ullman; Salil Vadhan"}, {"ref_id": "b7", "title": "Composable and versatile privacy via truncated cdp", "journal": "", "year": "2018", "authors": "Mark Bun; Cynthia Dwork; N Guy; Thomas Rothblum;  Steinke"}, {"ref_id": "b8", "title": "The discrete gaussian for differential privacy", "journal": "", "year": "2020", "authors": "Gautam Cl\u00e9ment L Canonne; Thomas Kamath;  Steinke"}, {"ref_id": "b9", "title": "Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models", "journal": "", "year": "2020", "authors": "Nicholas Carlini; Florian Tramer; Eric Wallace; Matthew Jagielski; Ariel Herbert-Voss; Katherine Lee; Adam Roberts; Tom Brown"}, {"ref_id": "b10", "title": "A stability-based validation procedure for differentially private machine learning", "journal": "Curran Associates, Inc", "year": "2013", "authors": "Kamalika Chaudhuri;  Vinterbo"}, {"ref_id": "b11", "title": "The algorithmic foundations of differential privacy", "journal": "Foundations and Trends in Theoretical Computer Science", "year": "2014", "authors": "Cynthia Dwork; Aaron Roth"}, {"ref_id": "b12", "title": "", "journal": "", "year": "2016", "authors": "Cynthia Dwork;  Guy N Rothblum"}, {"ref_id": "b13", "title": "Our data, ourselves: Privacy via distributed noise generation", "journal": "Springer", "year": "2006", "authors": "Cynthia Dwork; Krishnaram Kenthapadi; Frank Mcsherry; Ilya Mironov; Moni Naor"}, {"ref_id": "b14", "title": "Calibrating noise to sensitivity in private data analysis", "journal": "Springer", "year": "2006", "authors": "Cynthia Dwork; Frank Mcsherry; Kobbi Nissim; Adam Smith"}, {"ref_id": "b15", "title": "On the complexity of differentially private data release: efficient algorithms and hardness results", "journal": "", "year": "2009", "authors": "Cynthia Dwork; Moni Naor; Omer Reingold; N Guy; Salil Rothblum;  Vadhan"}, {"ref_id": "b16", "title": "Boosting and differential privacy", "journal": "IEEE 51st Annual Symposium on Foundations of Computer Science", "year": "2010", "authors": "Cynthia Dwork; N Guy; Salil Rothblum;  Vadhan"}, {"ref_id": "b17", "title": "Machine learning applications in cancer prognosis and prediction", "journal": "Computational and structural biotechnology journal", "year": "2015", "authors": "Konstantina Kourou; P Themis;  Exarchos; P Konstantinos;  Exarchos; V Michalis; Dimitrios I Karamouzis;  Fotiadis"}, {"ref_id": "b18", "title": "Private selection from private candidates", "journal": "", "year": "2019", "authors": "Jingcheng Liu; Kunal Talwar"}, {"ref_id": "b19", "title": "Mechanism design via differential privacy", "journal": "IEEE", "year": "2007", "authors": "Frank Mcsherry; Kunal Talwar"}, {"ref_id": "b20", "title": "R\u00e9nyi differential privacy", "journal": "IEEE", "year": "2017", "authors": "Ilya Mironov"}, {"ref_id": "b21", "title": "R\\'enyi differential privacy of the sampled gaussian mechanism", "journal": "", "year": "2019", "authors": "Ilya Mironov; Kunal Talwar; Li Zhang"}, {"ref_id": "b22", "title": "The role of adaptive optimizers for honest private hyperparameter selection", "journal": "", "year": "2021", "authors": "Shubhankar Mohapatra; Sajin Sasy; Xi He; Gautam Kamath; Om Thakkar"}, {"ref_id": "b23", "title": "Tempered sigmoid activations for deep learning with differential privacy", "journal": "", "year": "2020", "authors": "Nicolas Papernot; Abhradeep Thakurta; Shuang Song; Steve Chien; \u00dalfar Erlingsson"}, {"ref_id": "b24", "title": "A better privacy analysis of the exponential mechanism. DifferentialPrivacy.org", "journal": "", "year": "", "authors": "Ryan Rogers; Thomas Steinke"}, {"ref_id": "b25", "title": "Membership inference attacks against machine learning models", "journal": "IEEE", "year": "2017", "authors": "Reza Shokri; Marco Stronati; Congzheng Song; Vitaly Shmatikov"}, {"ref_id": "b26", "title": "Stochastic gradient descent with differentially private updates", "journal": "IEEE", "year": "2013", "authors": "Shuang Song; Kamalika Chaudhuri; Anand D Sarwate"}, {"ref_id": "b27", "title": "Tight lower bounds for differentially private selection", "journal": "IEEE", "year": "2017", "authors": "Thomas Steinke; Jonathan Ullman"}], "figures": [{"figure_label": "24", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :Figure 4 :24Figure 2: R\u00e9nyi DP guarantees from Corollary 4 for various expected numbers of repetitions of the logarithmic distribution (i.e., truncated negative binomial with \u03b7 = 0), compared with base algorithm (0.1-zCDP) and na\u00efve composition.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 5 :5Figure 5: Expected quantile of the repeated algorithm A as a function of the final privacy guarantee (\u03b5, 10 \u22126 )-DP for various distributions K, where each invocation of the base algorithm Q is 0.1-zCDP.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 6 :6Figure6: Final success probability (\u03b2) of the repeated algorithm A as a function of the final privacy guarantee (\u03b5, 10 \u22126 )-DP for various distributions, where each invocation of the base algorithm Q has a 1/100 probability of success and is 0.1-zCDP.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "S (x) = Q(x)I[x \u2208 S]/Q(S) and Q S (x) = Q (x)I[x \u2208 S]/Q (S). Now we have", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 88Figure 8 plots the guarantee of the second inequality in Corollary 16 when D \u03bb (Q Q ) = 0.1\u03bb and D \u03bb\u22121 (Q Q) = 0.1(\u03bb \u2212 1).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "S |Q S ) upper bound D (Q|Q ) = D (Q |Q)", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 8 :8Figure 8: Upper and lower bounds for R\u00e9nyi DP of conditional sampling. For each \u03bb, we pick the parameters s and t such that D \u03bb (Q Q ) = D \u03bb (Q Q) = 0.1 \u2022 \u03bb and we plot the upper bound from the second inequation in Corollary 16 along with the exact value of D \u03bb (Q S Q S ).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "1 m1e \u2212\u03b5t . Now set t = log(m 10 \u2212 1)/\u03b5 and \u03b3 = 1 exp(\u03b5t)+1 = m \u221210 so that 1\u2212\u03b3 \u03b3 e \u2212\u03b5t = 1 and 1\u2212\u03b3 \u03b3 = m 9 \u2212 1 m . Then P u(x, M (x)) \u2265 u(x, j * ) \u2212 20 \u03b5 log m \u2265", "figure_data": ""}, {"figure_label": "21", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Proof of Corollary 21 .21Fix neighbouring inputs x, x and let Q = Q(x) and Q = Q(x ) be the corresponding pair of output distributions from the base algorithm. Then, in the notation of Lemma 20,A(x) = A K Q and A(x ) = A K Q for K \u223c Poisson(\u00b5). Setting \u03b4 = 1\u2212f (1\u2212\u03b4 0 ) = 1\u2212e \u2212\u00b5\u2022\u03b40 \u2264 \u00b5\u2022\u03b4 0 Poisson(\u00b5 \u2022 (1 \u2212 \u03b4 0 )).Now we apply Theorem 6 to the algorithm corresponding to the pair of distributions A", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "X \u2190Q [g(X )]. This will give almost identical bounds to using the non-truncated distribution as long as P [K > m] 1 and E [K \u2022 I[K > m]] E [K], which should hold for large enough m.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Tim Van Erven and Peter Harremos. R\u00e9nyi divergence and kullback-leibler divergence. IEEE Transactions on Information Theory, 60(7):3797-3820, 2014. Jenna Wiens, Suchi Saria, Mark Sendak, Marzyeh Ghassemi, Vincent X Liu, Finale Doshi-Velez, Kenneth Jung, Katherine Heller, David Kale, Mohammed Saeed, Pilar N. Ossorio, Sonoo Thadaney-Israni, and Anna Goldenberg. Do no harm: a roadmap for responsible machine learning for health care. Nature medicine, 25(9):1337-1340, 2019.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "P [M (x) \u2208 S] \u2264 e \u03b5 P [M (x ) \u2208 S] + \u03b4.(1)", "formula_coordinates": [2.0, 227.58, 511.47, 276.43, 11.37]}, {"formula_id": "formula_1", "formula_text": "D \u03bb (M (x) M (x )) := 1 \u03bb \u2212 1 log E Y \u2190M (x) P [M (x) = Y ] P [M (x ) = Y ] \u03bb\u22121 \u2264 \u03b5,(2)", "formula_coordinates": [2.0, 157.27, 674.72, 346.73, 26.07]}, {"formula_id": "formula_2", "formula_text": "If each individual mechanism M i is (\u03bb, \u03b5 i )-RDP,", "formula_coordinates": [2.0, 312.43, 723.06, 192.82, 9.65]}, {"formula_id": "formula_3", "formula_text": "l w,b (x, y) = w 2 2 + \u03b1 max{0, 1 \u2212 y(w \u2022 x + b)} where y \u2208 {\u22121, 1} indicates the label of training example x \u2208 R 2 .", "formula_coordinates": [3.0, 107.64, 604.37, 297.64, 28.16]}, {"formula_id": "formula_4", "formula_text": "Definition 1 (Truncated Negative Binomial Distribution). Let \u03b3 \u2208 (0, 1) and \u03b7 \u2208 (\u22121, \u221e). Define a distribution D \u03b7,\u03b3 on N = {1, 2, \u2022 \u2022 \u2022 } as follows. If \u03b7 = 0 and K is drawn from D \u03b7,\u03b3 , then \u2200k \u2208 N P [K = k] = (1 \u2212 \u03b3) k \u03b3 \u2212\u03b7 \u2212 1 \u2022 k\u22121 =0 + \u03b7 + 1 (3) and E [K] = \u03b7\u2022(1\u2212\u03b3) \u03b3\u2022(1\u2212\u03b3 \u03b7 ) .", "formula_coordinates": [5.0, 108.0, 365.88, 396.0, 83.82]}, {"formula_id": "formula_5", "formula_text": "P [K = k] = (1 \u2212 \u03b3) k k \u2022 log(1/\u03b3)(4)", "formula_coordinates": [5.0, 253.05, 458.69, 250.95, 23.89]}, {"formula_id": "formula_6", "formula_text": "E [K] = 1/\u03b3\u22121 log(1/\u03b3) .", "formula_coordinates": [5.0, 125.44, 492.1, 70.37, 14.38]}, {"formula_id": "formula_7", "formula_text": "k\u22121 =0 +\u03b7 +1 = k+\u03b7\u22121 k", "formula_coordinates": [5.0, 344.85, 519.79, 87.78, 14.56]}, {"formula_id": "formula_8", "formula_text": "Theorem 2 (Main Privacy Result -Truncated Negative Binomial). Let Q : X n \u2192 Y be a randomized algorithm satisfying (\u03bb, \u03b5)-RDP and (\u03bb,\u03b5)-RDP for some \u03b5,\u03b5 \u2265 0, \u03bb \u2208 (1, \u221e), and\u03bb \u2208 [1, \u221e). 4", "formula_coordinates": [5.0, 107.67, 583.5, 396.33, 23.01]}, {"formula_id": "formula_9", "formula_text": "\u03b5 = \u03b5 + (1 + \u03b7) \u2022 1 \u2212 1 \u03bb \u03b5 + (1 + \u03b7) \u2022 log(1/\u03b3) \u03bb + log E [K] \u03bb \u2212 1 .(5)", "formula_coordinates": [5.0, 173.79, 683.23, 330.21, 23.93]}, {"formula_id": "formula_10", "formula_text": "((2 + \u03b7)\u03b5, 0)-DP.", "formula_coordinates": [6.0, 106.83, 409.21, 69.76, 8.74]}, {"formula_id": "formula_11", "formula_text": "\u2022 \u03bb)-R\u00e9nyi DP for all \u03bb > 1. Let \u03b7 \u2208 (\u22121, \u221e) and \u03b3 \u2208 (0, 1). Define A : X n \u2192 Y and K \u2190 D \u03b7,\u03b3 as in Theorem 2. Assume \u03c1 \u2264 log(1/\u03b3). Then A satisfies (\u03bb, \u03b5 )-R\u00e9nyi DP for all \u03bb > 1 with \u03b5 = \uf8f1 \uf8f2 \uf8f3 2 \u03c1 \u2022 log (E [K]) + 2(1 + \u03b7) \u03c1 log(1/\u03b3) \u2212 \u03b7\u03c1 if \u03bb \u2264 1 + 1 \u03c1 log (E [K]) \u03c1 \u2022 (\u03bb \u2212 1) + 1 \u03bb\u22121 log (E [K]) + 2(1 + \u03b7) \u03c1 log(1/\u03b3) \u2212 \u03b7\u03c1 if \u03bb > 1 + 1 \u03c1 log (E [K])", "formula_coordinates": [6.0, 107.64, 514.04, 397.66, 70.02]}, {"formula_id": "formula_12", "formula_text": "If \u03bb 1 \u2264 \u03bb 2 , then D \u03bb1 (P Q) \u2264 D \u03bb2 (P Q) (Van Erven & Harremos, 2014, Theorem 3). Thus (\u03bb 2 , \u03b5)-RDP implies (\u03bb 1 , \u03b5)-RDP for any \u03bb 1 \u2264 \u03bb 2 .", "formula_coordinates": [6.0, 106.83, 664.77, 397.16, 31.57]}, {"formula_id": "formula_13", "formula_text": "[K = k] = e \u2212\u00b5 \u2022 \u00b5 k k! for all k \u2265 0. Run Q(x)", "formula_coordinates": [7.0, 132.63, 252.14, 180.26, 15.64]}, {"formula_id": "formula_14", "formula_text": "If e\u03b5 \u2264 1 + 1 \u03bb\u22121 , then A satisfies (\u03bb, \u03b5 )-RDP where \u03b5 = \u03b5 + \u00b5 \u2022\u03b4 + log \u00b5 \u03bb \u2212 1 .", "formula_coordinates": [7.0, 107.83, 282.2, 246.1, 43.21]}, {"formula_id": "formula_15", "formula_text": "(\u03bb, \u03b5)-RDP implies (\u03b5,\u03b4)-DP for all\u03b5 \u2265 \u03b5 and\u03b4 = e (\u03bb\u22121)(\u03b5\u2212\u03b5) \u2022 1 \u03bb \u2022 1 \u2212 1 \u03bb \u03bb\u22121 (", "formula_coordinates": [7.0, 108.0, 361.73, 396.17, 25.93]}, {"formula_id": "formula_16", "formula_text": "\u221e k=0 P [K = k] \u2022 x k .", "formula_coordinates": [7.0, 413.15, 484.85, 78.17, 14.11]}, {"formula_id": "formula_17", "formula_text": "D \u03bb (A A ) \u2264 D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log f (q) \u03bb \u2022 f (q ) 1\u2212\u03bb ,(6)", "formula_coordinates": [7.0, 183.73, 542.96, 320.27, 22.31]}, {"formula_id": "formula_18", "formula_text": "Y \u2192 [0, 1] such that q = E X\u2190Q [g(X)] and q = E X \u2190Q [g(X )].", "formula_coordinates": [7.0, 252.46, 583.65, 253.28, 14.58]}, {"formula_id": "formula_19", "formula_text": "-f (x) = E K \u2022 x K\u22121 -is somewhat mysterious. A first-order intuition is that, if q = q , then f (q) \u03bb \u2022 f (q ) 1\u2212\u03bb = f (q) \u2264 f (1) = E [K]", "formula_coordinates": [7.0, 106.51, 652.35, 397.49, 23.81]}, {"formula_id": "formula_20", "formula_text": "log E[K]", "formula_coordinates": [7.0, 109.2, 677.62, 28.01, 6.51]}, {"formula_id": "formula_21", "formula_text": "P [K = k] = 1, then f (x) = E x K = x k .", "formula_coordinates": [8.0, 108.0, 370.27, 178.68, 10.87]}, {"formula_id": "formula_22", "formula_text": "f (x) = E K\u2190D\u03b7,\u03b3 x K = (1\u2212(1\u2212\u03b3)x) \u2212\u03b7 \u22121 \u03b3 \u2212\u03b7 \u22121 if \u03b7 = 0 log(1\u2212(1\u2212\u03b3)x) log(\u03b3) if \u03b7 = 0 . (7", "formula_coordinates": [8.0, 189.04, 400.81, 311.09, 30.88]}, {"formula_id": "formula_23", "formula_text": ")", "formula_coordinates": [8.0, 500.13, 412.5, 3.87, 8.64]}, {"formula_id": "formula_24", "formula_text": "E K K + 1 = E 1 \u2212 1 K + 1 = 1 0 x \u2022 f (x)dx = 1 \u2212 1 0 f (x)dx,(8)", "formula_coordinates": [9.0, 165.44, 97.95, 338.56, 26.29]}, {"formula_id": "formula_25", "formula_text": "P [M (x) \u2208 S] \u2264 e \u03b5 \u2022 P [M (x ) \u2208 S] + \u03b4.", "formula_coordinates": [12.0, 223.98, 317.72, 164.04, 11.37]}, {"formula_id": "formula_26", "formula_text": "D 1 (P Q) := E X\u2190P log P (X) Q(X) = \u2126 P (x) log P (x) Q(x) dx.", "formula_coordinates": [12.0, 173.06, 517.21, 265.87, 23.97]}, {"formula_id": "formula_27", "formula_text": "D \u221e (P Q) := sup log P (S) Q(S) : P (S) > 0 .", "formula_coordinates": [12.0, 205.57, 557.64, 200.85, 22.31]}, {"formula_id": "formula_28", "formula_text": "D \u03bb (P Q) := 1 \u03bb \u2212 1 log E X\u2190P P (X) Q(X) \u03bb\u22121 = 1 \u03bb \u2212 1 log E X\u2190Q P (X) Q(X) \u03bb = 1 \u03bb \u2212 1 log \u2126 P (x) \u03bb Q(x) 1\u2212\u03bb dx .", "formula_coordinates": [12.0, 202.08, 598.97, 204.6, 91.93]}, {"formula_id": "formula_29", "formula_text": "X n \u2192 Y is (\u03bb, \u03b5)-R\u00e9nyi differentially private if, for all neighbouring pairs of inputs x, x \u2208 X n , D \u03bb (M (x) M (x )) \u2264 \u03b5.", "formula_coordinates": [13.0, 108.0, 334.24, 396.0, 22.18]}, {"formula_id": "formula_30", "formula_text": "D \u03bb (M (x) M (x )) \u2264 \u03c1 \u2022 \u03bb.", "formula_coordinates": [13.0, 389.49, 414.27, 112.34, 9.65]}, {"formula_id": "formula_31", "formula_text": "Lemma 13. If M satisfies (\u03b5, 0)-differential privacy, then M satisfies 1 2 \u03b5 2 -zCDP -i.e., (\u03bb, 1 2 \u03b5 2 \u03bb)- RDP for all \u03bb \u2208 (1, \u221e).", "formula_coordinates": [13.0, 107.69, 521.41, 397.96, 21.58]}, {"formula_id": "formula_32", "formula_text": "\u03b4 = exp((\u03bb \u2212 1)(\u03b5 \u2212 \u03b5)) \u03bb \u2022 1 \u2212 1 \u03bb \u03bb\u22121 .", "formula_coordinates": [13.0, 220.77, 595.63, 170.46, 25.51]}, {"formula_id": "formula_33", "formula_text": "f (x) = E x K = \u221e k=0 P [K = k] \u2022 x k .", "formula_coordinates": [13.0, 229.06, 677.18, 153.89, 30.55]}, {"formula_id": "formula_34", "formula_text": "P [K = k] = f (k) (0) k! ,", "formula_coordinates": [14.0, 261.25, 95.07, 89.49, 23.88]}, {"formula_id": "formula_35", "formula_text": "f (k) (x) = \u221e =k P [K = ] \u2022 x \u2212k \u2022 \u2022 ( \u2212 1) \u2022 ( \u2212 2) \u2022 \u2022 \u2022 ( \u2212 k + 1). In particular, f (1) = E [1] = 1 and f (1) = E [K] and f (1) = E [K(K \u2212 1)].", "formula_coordinates": [14.0, 108.0, 169.56, 396.0, 35.67]}, {"formula_id": "formula_36", "formula_text": "x K = E \u039b E K\u2190Poisson(\u039b) x K = E \u039b e \u039b\u2022(x\u22121) = g(x \u2212 1)", "formula_coordinates": [14.0, 108.0, 236.2, 396.0, 35.55]}, {"formula_id": "formula_37", "formula_text": "(x) = 1\u2212(1\u2212\u03b3)x \u03b3 \u2212\u03b7", "formula_coordinates": [14.0, 238.6, 283.65, 87.11, 18.55]}, {"formula_id": "formula_38", "formula_text": "f Kt (x) = E x Kt = log(1\u2212(1\u2212\u03b3)x) log(\u03b3)", "formula_coordinates": [14.0, 361.14, 372.26, 141.66, 14.38]}, {"formula_id": "formula_39", "formula_text": "f K (x) = E x K = E T T t=1 E Kt x Kt = E T f Kt (x) T = f T (f Kt (x)) = exp \u00b5 \u2022 log(1 \u2212 (1 \u2212 \u03b3)x) log(\u03b3) \u2212 1 , which is equivalent to f NB (x) = 1\u2212(1\u2212\u03b3)x \u03b3 \u2212\u03b7 with \u03b7 = \u00b5 log(1/\u03b3) .", "formula_coordinates": [14.0, 107.64, 405.38, 450.49, 51.97]}, {"formula_id": "formula_40", "formula_text": "lim \u03b7\u2192\u221e,\u03b3= \u03b7 \u03b7+\u00b5 f NB (x) = lim \u03b7\u2192\u221e,\u03b3= \u03b7 \u03b7+\u00b5 1 \u2212 (1 \u2212 \u03b3)x \u03b3 \u2212\u03b7 = lim \u03b7\u2192\u221e,\u03b3= \u03b7 \u03b7+\u00b5 1 \u2212 \u00b5 \u03b7 (x \u2212 1) \u2212\u03b7 = e \u00b5(x\u22121) .", "formula_coordinates": [14.0, 108.0, 478.5, 417.42, 27.14]}, {"formula_id": "formula_41", "formula_text": "\u03b2 := 1 \u2212 P [A(x) \u2208 Bad] = 1 \u2212 E K P [Q(x) \u2208 Bad] K = 1 \u2212 E (1 \u2212 1/m) K = 1 \u2212 f (1 \u2212 1/m)", "formula_coordinates": [14.0, 108.0, 687.46, 396.0, 17.55]}, {"formula_id": "formula_42", "formula_text": "f (1 \u2212 1/m) \u2248 f (1) \u2212 f (1) \u2022 1/m = 1 \u2212 E [K]/m, then we have \u03b2 \u2248 E [K]/m.", "formula_coordinates": [15.0, 107.5, 96.99, 396.5, 20.25]}, {"formula_id": "formula_43", "formula_text": "Y = max{X 1 , \u2022 \u2022 \u2022 , X K } where X 1 , X 2 , \u2022 \u2022 \u2022 are independent copies of X. Let cdf X (x) = P [X \u2264 x] and cdf Y (x) = P [Y \u2264 x] = E K P X [X \u2264 x] K = f (cdf X (x)),", "formula_coordinates": [15.0, 108.0, 273.33, 396.0, 45.75]}, {"formula_id": "formula_44", "formula_text": "pdf Y (x) = d dx cdf X (x) = d dx f (cdf X (x)) = f (cdf X (x)) \u2022 pdf X (x).", "formula_coordinates": [15.0, 164.34, 361.85, 283.33, 22.31]}, {"formula_id": "formula_45", "formula_text": "E [Y ] = \u221e \u2212\u221e x \u2022 pdf Y (x)dx = \u221e \u2212\u221e x \u2022 f (cdf X (x)) \u2022 pdf X (x)dx = E [X \u2022 f (cdf X (X))].", "formula_coordinates": [15.0, 120.36, 401.9, 371.29, 26.29]}, {"formula_id": "formula_46", "formula_text": "E [Y ] = 1 0 x\u2022f (x)dx = 1 0 d dx xf (x) \u2212f (x)dx = 1f (1)\u22120f (0)\u2212 1 0 f (x)dx = 1\u2212 1 0 f (x)dx.", "formula_coordinates": [15.0, 108.0, 460.85, 412.52, 26.29]}, {"formula_id": "formula_47", "formula_text": "1 0 f (x)dx = 1 0 E K x K dx = E K 1 0 x K dx = E K 1 K + 1 .", "formula_coordinates": [15.0, 183.88, 500.02, 249.78, 26.29]}, {"formula_id": "formula_48", "formula_text": "P [K \u2265 k] = P e t\u2022(K\u2212k) \u2265 1 \u2264 E e t\u2022(K\u2212k) = f (e t ) \u2022 e \u2212t\u2022k .", "formula_coordinates": [15.0, 177.86, 574.95, 256.28, 11.37]}, {"formula_id": "formula_49", "formula_text": "A(y) = \u221e k=1 P [K = k] \u2022 (Q(\u2264 y) k \u2212 Q(< y) k ) = f (Q(\u2264 y)) \u2212 f (Q(< y)) = Q(\u2264y) Q(<y) f (x)dx = Q(y) \u2022 E X\u2190[Q(<y),Q(\u2264y)] uniform [f (X)]", "formula_coordinates": [16.0, 210.59, 101.8, 190.83, 100.41]}, {"formula_id": "formula_50", "formula_text": "A (y) = Q (y) \u2022 E X \u2190[Q (<y),Q (\u2264y)] uniform [f (X )]", "formula_coordinates": [16.0, 165.21, 225.57, 171.84, 19.46]}, {"formula_id": "formula_51", "formula_text": "e (\u03bb\u22121)D \u03bb (A A ) = y\u2208Y A(y) \u03bb \u2022 A (y) 1\u2212\u03bb = y\u2208Y Q(y) \u03bb \u2022 Q (y) 1\u2212\u03bb \u2022 E X\u2190[Q(<y),Q(\u2264y)] [f (X)] \u03bb \u2022 E X \u2190[Q (<y),Q (\u2264y)] [f (X )] 1\u2212\u03bb \u2264 y\u2208Y Q(y) \u03bb \u2022 Q (y) 1\u2212\u03bb \u2022 E X\u2190[Q(<y),Q(\u2264y)] X \u2190[Q (<y),Q (\u2264y)] f (X) \u03bb \u2022 f (X ) 1\u2212\u03bb \u2264 e (\u03bb\u22121)D \u03bb (Q Q ) \u2022 max y\u2208Y E X\u2190[Q(<y),Q(\u2264y)] X \u2190[Q (<y),Q (\u2264y)] f (X) \u03bb \u2022 f (X ) 1\u2212\u03bb .", "formula_coordinates": [16.0, 108.0, 254.25, 413.99, 109.91]}, {"formula_id": "formula_52", "formula_text": ": (0, \u221e) 2 \u2192 (0, \u221e) given by h(u, v) = u \u03bb \u2022 v 1\u2212\u03bb is convex and, hence, E [U ] \u03bb E [V ] 1\u2212\u03bb = h(E [(U, V )]) \u2264 E [h(U, V )] = E U \u03bb \u2022 V 1\u2212\u03bb", "formula_coordinates": [16.0, 108.0, 383.36, 397.25, 24.21]}, {"formula_id": "formula_53", "formula_text": "= X \u2212Q (<y) Q (y)", "formula_coordinates": [16.0, 179.65, 432.5, 53.86, 14.38]}, {"formula_id": "formula_54", "formula_text": "t \u2208 [0, 1] such that E X\u2190[Q(<y),Q(\u2264y)] X \u2190[Q (<y),Q (\u2264y)] f (X) \u03bb \u2022 f (X ) 1\u2212\u03bb \u2264 f (Q(< y) + t \u2022 Q(y)) \u03bb \u2022 f (Q (< y) + t \u2022 Q (y)) 1\u2212\u03bb . Hence D \u03bb (A A ) \u2264 D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log max y\u2208Y t\u2208[0,1] f (Q(< y) + t \u2022 Q(y)) \u03bb \u2022 f (Q (< y) + t \u2022 Q (y)) 1\u2212\u03bb .", "formula_coordinates": [16.0, 108.0, 448.94, 419.44, 93.9]}, {"formula_id": "formula_55", "formula_text": "g(y) := 1 if y < y * t * if y = y * 0 if y > y * .", "formula_coordinates": [16.0, 248.52, 572.12, 114.96, 31.57]}, {"formula_id": "formula_56", "formula_text": "f (x) = E K\u2190D\u03b7,\u03b3 x K = (1\u2212(1\u2212\u03b3)x) \u2212\u03b7 \u22121 \u03b3 \u2212\u03b7 \u22121 if \u03b7 = 0 log(1\u2212(1\u2212\u03b3)x) log(\u03b3) if \u03b7 = 0 . Thus f (x) = (1 \u2212 (1 \u2212 \u03b3)x) \u2212\u03b7\u22121 \u2022 \u03b7\u2022(1\u2212\u03b3) \u03b3 \u2212\u03b7 \u22121 if \u03b7 = 0 1\u2212\u03b3 log(1/\u03b3) if \u03b7 = 0 = (1 \u2212 (1 \u2212 \u03b3)x) \u2212\u03b7\u22121 \u2022 \u03b3 \u03b7+1 \u2022 E [K].", "formula_coordinates": [16.0, 189.04, 703.57, 233.92, 30.88]}, {"formula_id": "formula_57", "formula_text": "D \u03bb (A A ) \u2264 D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log f (q) \u03bb \u2022 f (q ) 1\u2212\u03bb = D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log \u03b3 \u03b7+1 \u2022 E [K] \u2022 (1 \u2212 (1 \u2212 \u03b3)q) \u2212\u03bb(\u03b7+1) \u2022 (1 \u2212 (1 \u2212 \u03b3)q ) \u2212(1\u2212\u03bb)(\u03b7+1) = D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log \u03b3 \u03b7+1 \u2022 E [K] \u2022 (\u03b3 + (1 \u2212 \u03b3)(1 \u2212 q)) 1\u2212\u03bb \u2022 (\u03b3 + (1 \u2212 \u03b3)(1 \u2212 q ))\u03bb \u03bd \u2022 (\u03b3 + (1 \u2212 \u03b3)(1 \u2212 q)) u (\u03bb\u03bd = (\u03bb \u2212 1)(1 + \u03b7) and (1 \u2212\u03bb)\u03bd + u = \u2212\u03bb(\u03b7 + 1)) \u2264 D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log \u03b3 \u03b7+1 \u2022 E [K] \u2022 \u03b3 + (1 \u2212 \u03b3) \u2022 e (\u03bb\u22121)D\u03bb(Q Q) \u03bd \u2022 (\u03b3 + (1 \u2212 \u03b3)(1 \u2212 q)) u", "formula_coordinates": [17.0, 108.0, 215.82, 494.08, 124.26]}, {"formula_id": "formula_58", "formula_text": "\u2264 D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log \u03b3 \u03b7+1 \u2022 E [K] \u2022 \u03b3 + (1 \u2212 \u03b3) \u2022 e (\u03bb\u22121)D\u03bb(Q Q) \u03bd \u2022 \u03b3 u (\u03b3 \u2264 \u03b3 + (1 \u2212 \u03b3)(1 \u2212 q) and u \u2264 0) = D \u03bb (Q Q ) + \u03bd \u03bb \u2212 1 log \u03b3 + (1 \u2212 \u03b3) \u2022 e (\u03bb\u22121)D\u03bb(Q Q) + 1 \u03bb \u2212 1 log \u03b3 \u03b7+1 \u2022 E [K] \u2022 \u03b3 u = D \u03bb (Q Q ) + \u03bd \u03bb \u2212 1 (\u03bb \u2212 1)D\u03bb (Q Q) + log 1 \u2212 \u03b3 \u2022 1 \u2212 e \u2212(\u03bb\u22121)D\u03bb(Q Q) + 1 \u03bb \u2212 1 log \u03b3 u+\u03b7+1 \u2022 E [K] = D \u03bb (Q Q ) + (1 + \u03b7) 1 \u2212 1 \u03bb D\u03bb (Q Q) + 1 + \u03b7 \u03bb log 1 \u2212 \u03b3 \u2022 1 \u2212 e \u2212(\u03bb\u22121)D\u03bb(Q Q) + log (E [K]) \u03bb \u2212 1 + 1 + \u03b7 \u03bb log(1/\u03b3) (\u03bd = (\u03bb\u22121)(1+\u03b7) \u03bb and u = \u2212(1 + \u03b7)( \u03bb\u22121 \u03bb + 1)) = D \u03bb (Q Q ) + (1 + \u03b7) 1 \u2212 1 \u03bb D\u03bb (Q Q) + 1 + \u03b7 \u03bb log 1 \u03b3 \u2212 1 + e \u2212(\u03bb\u22121)D\u03bb(Q Q) + log (E [K]) \u03bb \u2212 1 \u2264 D \u03bb (Q Q ) + (1 + \u03b7) 1 \u2212 1 \u03bb D\u03bb (Q Q) + 1 + \u03b7 \u03bb log 1 \u03b3 + log (E [K]) \u03bb \u2212 1 .", "formula_coordinates": [17.0, 110.77, 356.95, 422.95, 217.36]}, {"formula_id": "formula_59", "formula_text": "\u03b5 = \u03c1 \u2022 \u03bb and\u03b5 = \u03c1 \u2022\u03bb) gives that the repeated algorithm A satisfies (\u03bb, \u03b5 )-RDP for \u03b5 \u2264 \u03c1 \u2022 \u03bb + (1 + \u03b7) \u2022 1 \u2212 1 \u03bb \u03c1 \u2022\u03bb + (1 + \u03b7) \u2022 log(1/\u03b3) \u03bb + log E [K] \u03bb \u2212 1 .", "formula_coordinates": [17.0, 108.0, 634.54, 343.72, 43.36]}, {"formula_id": "formula_60", "formula_text": "\u03b5 \u2264 \u03c1 \u2022 \u03bb \u2212 (1 + \u03b7) \u2022 \u03c1 + 2(1 + \u03b7) \u2022 \u03c1 \u2022 log(1/\u03b3) + log E [K] \u03bb \u2212 1 .", "formula_coordinates": [18.0, 175.17, 115.66, 261.65, 22.31]}, {"formula_id": "formula_61", "formula_text": "\u03b5 \u2264 \u03c1 \u2022 \u03bb \u2212 (1 + \u03b7) \u2022 \u03c1 + 2(1 + \u03b7) \u2022 \u03c1 \u2022 log(1/\u03b3) + log E[K] \u03bb\u22121 if \u03bb > 1 + log(E [K])/\u03c1 2 \u03c1 \u2022 log E [K] + 2(1 + \u03b7) \u03c1 \u2022 log(1/\u03b3) \u2212 \u03b7\u03c1 if \u03bb \u2264 1 + log(E [K])/\u03c1 .", "formula_coordinates": [18.0, 109.01, 187.07, 393.97, 25.67]}, {"formula_id": "formula_62", "formula_text": "D \u03bb (A A ) \u2264 D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log f (q) \u03bb \u2022 f (q ) 1\u2212\u03bb = D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log \u00b5 \u2022 e \u00b5\u03bb(q\u22121)+\u00b5(1\u2212\u03bb)(q \u22121) = D \u03bb (Q Q ) + \u00b5(\u03bbq \u2212 (\u03bb \u2212 1)q \u2212 1) + log \u00b5 \u03bb \u2212 1 = D \u03bb (Q Q ) + \u00b5((\u03bb \u2212 1)(1 \u2212 q ) \u2212 \u03bb(1 \u2212 q)) + log \u00b5 \u03bb \u2212 1 \u2264 D \u03bb (Q Q ) + \u00b5((\u03bb \u2212 1)(e\u03b5(1 \u2212 q) +\u03b4) \u2212 \u03bb(1 \u2212 q)", "formula_coordinates": [18.0, 175.08, 310.84, 229.81, 134.59]}, {"formula_id": "formula_63", "formula_text": "= D \u03bb (Q Q ) + \u00b5 \u2022 (1 \u2212 q) \u2022 e\u03b5 \u2212 \u03bb \u03bb \u2212 1 + \u00b5 \u2022\u03b4 + log \u00b5 \u03bb \u2212 1 \u2264 D \u03bb (Q Q ) + \u00b5 \u2022\u03b4 + log \u00b5 \u03bb \u2212 1 ,", "formula_coordinates": [18.0, 177.85, 468.96, 246.19, 49.42]}, {"formula_id": "formula_64", "formula_text": "D \u03bb (Q S Q S ) \u2264 \u03bb \u2212 1/p \u2212 1/r \u03bb \u2212 1 D r\u2022(\u03bb\u22121/p) (Q Q )+ \u03bb + 1/q \u2212 2 \u03bb \u2212 1 D \u03bb+1/q\u22121 (Q Q)+ 1/r + 1 \u03bb \u2212 1 log 1 Q(S)", "formula_coordinates": [19.0, 108.0, 199.83, 429.88, 22.31]}, {"formula_id": "formula_65", "formula_text": "e (\u03bb\u22121)D \u03bb (QS Q S ) = \u2126 Q S (x) \u03bb Q S (x) 1\u2212\u03bb dx = Q(S) \u2212\u03bb Q (S) \u03bb\u22121 \u2126 I[x \u2208 S]Q(x) \u03bb Q (x) 1\u2212\u03bb dx \u2264 Q(S) \u2212\u03bb Q (S) \u03bb\u22121 S Q(x)dx 1/p S Q (x)dx 1/q S Q(x) \u03bb\u22121/p Q (x) 1\u2212\u03bb\u22121/q r dx 1/r (H\u00f6lder's inequality) = Q(S) 1/p\u2212\u03bb Q (S) 1/q+\u03bb\u22121 S Q(x) r\u03bb\u2212r/p Q (x) r\u2212r\u03bb\u2212r/q dx 1/r = Q (S) \u03bb0 Q(S) 1\u2212\u03bb0 Q(S) \u22121/r\u22121 S Q(x) \u03bb1 Q (x) 1\u2212\u03bb1 dx 1/r (\u03bb 0 := \u03bb + 1/q \u2212 1, \u03bb 1 := r\u03bb \u2212 r/p) \u2264 e (\u03bb0\u22121)D \u03bb 0 (Q Q) \u2022 Q(S) \u22121/r\u22121 \u2022 e (\u03bb1\u22121)D \u03bb 1 (Q Q ) 1/r . (Postprocessing & non-negativity)", "formula_coordinates": [19.0, 108.0, 274.46, 484.33, 197.47]}, {"formula_id": "formula_66", "formula_text": "D \u221e (Q S Q S ) \u2264 D \u221e (Q Q ) + D \u221e (Q Q) . D \u03bb (Q S Q S ) \u2264 D \u03bb (Q Q ) + \u03bb \u2212 2 \u03bb \u2212 1 D \u03bb\u22121 (Q Q) + 2 \u03bb \u2212 1 log 1 Q(S) . D \u03bb (Q S Q S ) \u2264 D \u221e (Q Q ) + \u03bb \u2212 2 \u03bb \u2212 1 D \u03bb\u22121 (Q Q) + 1 \u03bb \u2212 1 log 1 Q(S) . D \u03bb (Q S Q S ) \u2264 \u03bb \u03bb \u2212 1 D \u221e (Q Q ) + D \u03bb (Q Q) + 1 \u03bb \u2212 1 log 1 Q(S) . \u2200r \u2265 1 D \u03bb (Q S Q S ) \u2264 D r(\u03bb\u22121)+1 (Q Q ) + \u03bb \u2212 2 \u03bb \u2212 1 D \u03bb\u22121 (Q Q) + 1/r + 1 \u03bb \u2212 1 log 1 Q(S)", "formula_coordinates": [19.0, 114.76, 538.04, 369.53, 120.98]}, {"formula_id": "formula_67", "formula_text": "D \u03bb (A A ) \u2264 D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log k \u2022 q \u03bb \u2022 q 1\u2212\u03bb k\u22121 \u2264 D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log k \u2022 e (\u03bb\u22121)D \u03bb (Bern(q) Bern(q )) k\u22121 \u2264 D \u03bb (Q Q ) + 1 \u03bb \u2212 1 log k \u2022 e (\u03bb\u22121)D \u03bb (Q Q ) k\u22121 = k \u2022 D \u03bb (Q Q ) + log k \u03bb \u2212 1 ,", "formula_coordinates": [20.0, 149.03, 307.03, 306.1, 103.41]}, {"formula_id": "formula_68", "formula_text": "D \u03bb (A A ) \u2264 D \u03bb Q \u2297k Q \u2297k = k \u2022 D \u03bb (Q Q ).", "formula_coordinates": [20.0, 128.26, 455.97, 207.97, 11.23]}, {"formula_id": "formula_69", "formula_text": "\u03b5 (\u03bb) = k\u03b5 \u2212 k \u2022 log(1 + e \u2212\u03b5 ) \u03bb \u2212 1 .", "formula_coordinates": [20.0, 242.32, 583.99, 127.36, 23.89]}, {"formula_id": "formula_70", "formula_text": "Q = 1 1 + e \u03b5 , e \u03b5 1 + e \u03b5 , Q = e \u03b5 1 + e \u03b5 , 1 1 + e \u03b5 .", "formula_coordinates": [20.0, 253.81, 677.38, 104.38, 51.78]}, {"formula_id": "formula_71", "formula_text": "A = 1 \u2212 e \u03b5 1 + e \u03b5 k , e \u03b5 1 + e \u03b5 k , A = 1 \u2212 1 1 + e \u03b5 k , 1 1 + e \u03b5 k .", "formula_coordinates": [21.0, 224.48, 139.31, 163.04, 59.39]}, {"formula_id": "formula_72", "formula_text": "D \u221e (A A ) \u2265 log \uf8eb \uf8ec \uf8ed e \u03b5 1+e \u03b5 k 1 1+e \u03b5 k \uf8f6 \uf8f7 \uf8f8 = k\u03b5. For all \u03bb > 1, e (\u03bb\u22121)D \u03bb (A A ) \u2265 e \u03b5 1 + e \u03b5 k \u03bb \u2022 1 1 + e \u03b5 k 1\u2212\u03bb = e \u03b5k\u03bb \u2022 (1 + e \u03b5 ) \u2212k Hence D \u03bb (A A ) \u2265 \u03b5k\u03bb \u2212 k \u2022 log(1 + e \u03b5 ) \u03bb \u2212 1 = k\u03b5 \u2212 k \u2022 log(1 + e \u2212\u03b5 ) \u03bb \u2212 1 .", "formula_coordinates": [21.0, 108.0, 227.08, 325.25, 160.92]}, {"formula_id": "formula_73", "formula_text": "Q = (1 \u2212 exp(\u22121/k), exp(\u22121/k)), Q = (1 \u2212 exp(\u2212\u03b5 0 \u2212 1/k), exp(\u2212\u03b5 0 \u2212 1/k)).", "formula_coordinates": [21.0, 210.5, 522.1, 191.0, 24.91]}, {"formula_id": "formula_74", "formula_text": "A = (1 \u2212 exp(\u22121), exp(\u22121)), A = (1 \u2212 exp(\u2212k\u03b5 0 \u2212 1), exp(\u2212k\u03b5 0 \u2212 1)).", "formula_coordinates": [21.0, 215.68, 583.94, 180.64, 24.91]}, {"formula_id": "formula_75", "formula_text": "D \u221e (Q Q ) = \u03b5 0 like before, but D \u221e (Q Q) = log(1 \u2212 exp(\u2212\u03b5 0 \u2212 1/k)) \u2212 log(1 \u2212 exp(\u22121/k))", "formula_coordinates": [21.0, 108.0, 629.41, 396.0, 19.7]}, {"formula_id": "formula_76", "formula_text": "\u2208 [0, 1/4]. Note (1 \u2212 2a) \u22121 \u2264 e 3a . Let Q = a \u2022 e \u2212s , a \u2022 e \u2212s , 1 \u2212 2a \u2022 e \u2212s , Q = a \u2022 e \u2212s\u2212t , a, 1 \u2212 a \u2212 a \u2022 e \u2212s\u2212t , S = {1, 2} \u2282 {1, 2, 3}, Q S = 1 2 , 1 2 , Q S = e \u2212s\u2212t 1 + e \u2212s\u2212t , 1 1 + e \u2212s\u2212t .", "formula_coordinates": [22.0, 182.14, 414.38, 205.29, 178.12]}, {"formula_id": "formula_77", "formula_text": "Q(S) = 2a \u2022 e \u2212s , e (\u03bb\u22121)D \u03bb (Q Q ) = a \u2022 e \u2212s\u03bb\u2212(s+t)(1\u2212\u03bb) + a \u2022 e \u2212s\u03bb + (1 \u2212 2a \u2022 e \u2212s ) \u03bb (1 \u2212 a \u2212 a \u2022 e \u2212s\u2212t ) 1\u2212\u03bb \u2264 a \u2022 e (\u03bb\u22121)t\u2212s + ae \u2212s\u03bb + e \u22122ae \u2212s \u03bb (1 \u2212 2a) 1\u2212\u03bb \u2264 a \u2022 e (\u03bb\u22121)t\u2212s + a + e 3a(\u03bb\u22121) \u2248 a \u2022 e (\u03bb\u22121)t\u2212s (assuming t is large) = 1 2 Q(S) \u2022 e (\u03bb\u22121)t , =\u21d2 D \u03bb (Q Q ) t \u2212 log(2/Q(S)) \u03bb \u2212 1 , e (\u03bb\u22121)D \u03bb (Q Q) = a \u2022 e \u2212s(1\u2212\u03bb)\u2212(s+t)\u03bb + a \u2022 e \u2212s(1\u2212\u03bb) + (1 \u2212 2a \u2022 e \u2212s ) 1\u2212\u03bb (1 \u2212 a \u2212 a \u2022 e \u2212s\u2212t ) \u03bb \u2264 a \u2022 e \u2212t\u03bb\u2212s + a \u2022 e s(\u03bb\u22121) + e 3a\u2022e \u2212s \u2022(\u03bb\u22121) \u2022 e \u2212a\u03bb \u2248 a \u2022 e s(\u03bb\u22121) (assuming s is large) = 1 2 Q(S) \u2022 e s\u03bb =\u21d2 D \u03bb (Q Q) s + s \u2212 log(2/Q(S)) \u03bb \u2212 1 , e (\u03bb\u22121)D \u03bb (QS Q S ) = 2 \u2212\u03bb (1 + e \u2212s\u2212t ) \u03bb\u22121 (e (\u03bb\u22121)(s+t) + 1) \u2265 2 \u2212\u03bb \u2022 e (\u03bb\u22121)(s+t) , =\u21d2 D \u03bb (Q S Q S ) \u2265 s + t \u2212 \u03bb log 2 \u03bb \u2212 1 .", "formula_coordinates": [23.0, 112.48, 501.83, 391.52, 234.84]}, {"formula_id": "formula_78", "formula_text": "D \u03bb (Q S Q S ) \u2264 D \u03bb (Q Q ) + \u03bb \u2212 2 \u03bb \u2212 1 D \u03bb\u22121 (Q Q) + 2 \u03bb \u2212 1 log 1 Q(S) . t \u2212 log(2/Q(S)) \u03bb \u2212 1 + \u03bb \u2212 2 \u03bb \u2212 1 s + s \u2212 log(2/Q(S)) \u03bb \u2212 2 + 2 log(1/Q(S)) \u03bb \u2212 1 = s + t \u2212 2 log 2 \u03bb \u2212 1 .", "formula_coordinates": [24.0, 128.11, 167.07, 354.59, 77.31]}, {"formula_id": "formula_79", "formula_text": "Q = Q(x) = (1 \u2212 b \u2212 c, b, c), Q = Q(x ) = (1 \u2212 b \u2212 c , b , c ), A = A(x) = (1 \u2212 f (b + c), f (b + c) \u2212 f (c), f (c)), A = A(x ) = (1 \u2212 f (b + c ), f (b + c ) \u2212 f (c ), f (c )).", "formula_coordinates": [24.0, 190.11, 390.37, 231.77, 53.21]}, {"formula_id": "formula_80", "formula_text": "A \u2248 (1 \u2212 f (b + c), f (c) \u2022 b, f (c)), A \u2248 (1 \u2212 f (b + c ), f (c ) \u2022 b , f (c )).", "formula_coordinates": [24.0, 226.6, 523.73, 158.8, 24.0]}, {"formula_id": "formula_81", "formula_text": "e (\u03bb\u22121)D \u03bb (A A ) (f (c) \u2022 b) \u03bb \u2022 (f (c ) \u2022 b ) 1\u2212\u03bb = e (\u03bb\u22121)D \u03bb (b b ) \u2022 (f (c)) \u03bb \u2022 (f (c )) 1\u2212\u03bb \u2248 e (\u03bb\u22121)D \u03bb (Q Q ) \u2022 (f (c)) \u03bb \u2022 (f (c )) 1\u2212\u03bb ,", "formula_coordinates": [24.0, 185.85, 571.75, 240.31, 48.92]}, {"formula_id": "formula_82", "formula_text": "e (\u03bb\u22121)D \u03bb (Q Q ) = (1 \u2212 b \u2212 c) \u03bb \u2022 (1 \u2212 b \u2212 c ) 1\u2212\u03bb + b \u03bb \u2022 (b ) 1\u2212\u03bb + (c) \u03bb \u2022 (c ) 1\u2212\u03bb .", "formula_coordinates": [24.0, 140.1, 644.7, 331.81, 12.3]}, {"formula_id": "formula_83", "formula_text": "e (\u03bb\u22121)D \u03bb( A A ) \u2264 e (\u03bb\u22121)D \u03bb (Q Q ) \u2022 (f (q)) \u03bb \u2022 (f (q )) 1\u2212\u03bb ,", "formula_coordinates": [24.0, 185.35, 681.03, 241.3, 12.3]}, {"formula_id": "formula_84", "formula_text": "\u2200j \u2208 [m] P [M (x) = j] = exp \u03b5 2 u(x, j) \u2208[m] exp \u03b5 2 u(x, )", "formula_coordinates": [25.0, 197.97, 196.6, 207.53, 28.22]}, {"formula_id": "formula_85", "formula_text": "E [u(x, M (x))] \u2265 max j\u2208[m] u(x, j) \u2212 2 \u03b5 log m, P u(x, M (x)) \u2265 max j\u2208[m] u(x, j) \u2212 2 \u03b5 log m \u03b2 \u2265 1 \u2212 \u03b2", "formula_coordinates": [25.0, 191.99, 265.46, 227.49, 58.86]}, {"formula_id": "formula_86", "formula_text": "f (x) = log(1\u2212(1\u2212\u03b3)x) log \u03b3 , so P [u(x, M (x)) \u2265 u(x, j * ) \u2212 2t] \u2265 1 log(1/\u03b3) log 1+ 1\u2212\u03b3 \u03b3 \u2022 1 m 1+ 1\u2212\u03b3 \u03b3 \u2022", "formula_coordinates": [25.0, 108.0, 548.97, 342.47, 21.08]}, {"formula_id": "formula_87", "formula_text": "A K Q = \u221e k=0 P [K = k] max Q k .", "formula_coordinates": [27.0, 244.38, 210.97, 123.24, 30.55]}, {"formula_id": "formula_88", "formula_text": "A K Q = \u221e k=0 P [K = k] max Q k = \u221e k=0 P [K = k] (1 \u2212 \u03b4 0 ) k max Q k 1\u2212\u03b40 + (1 \u2212 (1 \u2212 \u03b4 0 ) k )P k = f (1 \u2212 \u03b4 0 )A K Q + (1 \u2212 f (1 \u2212 \u03b4 0 ))P *", "formula_coordinates": [27.0, 170.82, 346.3, 265.13, 81.3]}, {"formula_id": "formula_89", "formula_text": "D \u03b4 \u03bb A K Q A K Q \u2264 D \u03bb A K Q 1\u2212\u03b4 0 A K Q 1\u2212\u03b4 0 as \u03b4 = 1 \u2212 f (1 \u2212 \u03b4 0 ).", "formula_coordinates": [27.0, 108.0, 499.48, 274.47, 35.03]}, {"formula_id": "formula_90", "formula_text": "D \u03bb A K Q 1\u2212\u03b4 0 A K Q 1\u2212\u03b4 0 \u2264 \u03b5 0 + log((1 \u2212 \u03b4 0 ) \u2022 \u00b5) \u03bb \u2212 1", "formula_coordinates": [27.0, 207.07, 683.29, 196.67, 22.31]}, {"formula_id": "formula_91", "formula_text": "P K = k = I[K \u2264 m] \u2022 P [K = k]/P [K \u2264 m].", "formula_coordinates": [28.0, 306.35, 157.82, 199.4, 11.81]}, {"formula_id": "formula_92", "formula_text": "f (x) = \u221e k=1 k \u2022 x k\u22121 \u2022 P [K = k] andf (x) = m k=1 k \u2022 x k\u22121 \u2022 P[K=k] P[K\u2264m] . Hence, for x \u2208 [0, 1], 0 \u2264f (x) \u2212f (x) \u2022 P [K \u2264 m] = \u221e k=m+1 k \u2022 x k\u22121 \u2022 P [K = k] \u2264 x m \u2022 \u221e k=m+1 k \u2022 P [K = k] = x m \u2022 E [K \u2022 I[K > m]]. Nowf (x) \u2022 P [K \u2264 m] = m k=1 k \u2022 x k\u22121 \u2022 P [K = k] \u2265 x m\u22121 \u2022 E [K \u2022 I[K \u2264 m]]. Thus 1 \u2264 f (x) f (x) \u2022 P [K \u2264 m] \u2264 1 + x m \u2022 E [K \u2022 I[K > m]] x m\u22121 \u2022 E [K \u2022 I[K \u2264 m]] = 1 + x \u2022 E [K \u2022 I[K > m]] E [K] \u2212 E [K \u2022 I[K > m]]", "formula_coordinates": [28.0, 108.0, 199.81, 396.17, 172.03]}, {"formula_id": "formula_93", "formula_text": "f (q) \u03bb \u2022f (q ) 1\u2212\u03bb \u2264 f (q) P [K \u2264 m] \u03bb \u2022 \uf8eb \uf8ed f (q ) P [K \u2264 m] \u2022 1 + E[K\u2022I[K>m]] E[K]\u2212E[K\u2022I[K>m]] \uf8f6 \uf8f8 1\u2212\u03bb = f (q) \u03bb \u2022 f (q ) 1\u2212\u03bb \u2022 1 P [K \u2264 m] \u2022 1 + E [K \u2022 I[K > m]] E [K] \u2212 E [K \u2022 I[K > m]] \u03bb\u22121", "formula_coordinates": [28.0, 117.67, 396.5, 376.16, 69.7]}, {"formula_id": "formula_94", "formula_text": "\u221e k=0 P [K = k] \u2022 x k .", "formula_coordinates": [28.0, 235.49, 509.0, 78.17, 14.11]}, {"formula_id": "formula_95", "formula_text": "D \u03bb (A A ) \u2264 D \u03bb (Q Q )+ 1 \u03bb \u2212 1 log f (q) \u03bb \u2022 f (q ) 1\u2212\u03bb + log 1 1\u2212P[K>m] \u03bb \u2212 1 +log 1 + E [K \u2022 I[K > m]] E [K] \u2212 E [K \u2022 I[K > m]] ,(9)", "formula_coordinates": [28.0, 108.0, 601.94, 481.02, 38.41]}], "doi": ""}