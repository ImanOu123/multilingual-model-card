{"title": "SCOTT: Self-Consistent Chain-of-Thought Distillation", "authors": "Peifeng Wang; Zhengyang Wang; Zheng Li; Yifan Gao; Bing Yin; Xiang Ren;  {q", "pub_date": "", "abstract": "Large language models (LMs) beyond a certain scale, demonstrate the emergent capability of generating free-text rationales for their predictions via chain-of-thought (CoT) prompting. While CoT can yield dramatically improved performance, such gains are only observed for sufficiently large LMs. Even more concerning, there is little guarantee that the generated rationales are consistent with LM's predictions or faithfully justify the decisions. In this work, we propose SCOTT, a faithful knowledge distillation method to learn a small, self-consistent CoT model from a teacher model that is orders of magnitude larger. To form better supervision, we elicit rationales supporting the gold answers from a large LM (teacher) by contrastive decoding, which encourages the teacher to generate tokens that become more plausible only when the answer is considered. To ensure faithful distillation, we use the teacher-generated rationales to learn a student LM with a counterfactual reasoning objective, which prevents the student from ignoring the rationales to make inconsistent predictions. Experiments show that, while yielding comparable end-task performance, our method can generate CoT rationales that are more faithful than baselines do. Further analysis suggests that such a model respects the rationales more when making decisions; thus, we can improve its performance more by refining its rationales. * This work was done when Peifeng Wang was an intern at Amazon. Zheng Li and Xiang Ren are corresponding authors. Can a Bengal cat survive eating only pancakes? The answer is no. Why? Is material from an aloe plant sometimes enclosed in petroleum-derived products? The answer is yes. Why? A Bengal cat cannot survive eating only pancakes. Aloe is a plant Plants are made of cells. Cells are made of molecules. Molecules are made of atoms. Error 1 (42%): Do not provide new information. Error 2 (37%): Do not justify the answer.", "sections": [{"heading": "Introduction", "text": "Large language models (LMs) elicit strong reasoning capabilities through chain-of-thought (CoT) prompting (Wei et al., 2022b), which asks LMs to generate free-text rationale for explaining their multi-step reasoning. However, CoT prompting does not guarantee that the rationale is consistent with the prediction, rendering the rationale Figure 1: Vacuous rationales generated by a prompted LM (GPT-3) for StrategyQA. In both types of error cases, LM fails to give rationales consistent with the answers due to hallucination. useless for justifying the model's behavior. In this work, we present Self-Consistent Chain-Of-Thought DisTillation (SCOTT), a knowledge distillation (KD) method for eliciting faithful CoT reasoning, where a small student model learns from a large teacher model to generate CoT rationales that are consistent to its own predictions.\nExisting works (Shridhar et al., 2022;Li et al., 2022a) propose learning to reason from large LMs mainly for computation efficiency or task performance. They prompt a large LM (the teacher) to generate rationales for a downstream dataset, which is then used to train a small LM (the student). However, these works neglect the following two issues which could undermine the faithfulness of the rationales. First, LMs are prone to hallucination, meaning they often generate text that is not grounded by the input (Maynez et al., 2020;Ji et al., 2022). Therefore, the teacher may not generate on-topic rationales, which fully support the answer. In our pioneer study (Figure 1) over 100 random rationales generated by GPT-3, we found 42% of them not providing new information that is not stated in the task input and 37% of them not justifying the answer 1 . This inconsistency between the rationale and answer would then be inherited by the student. Second, the student may treat ra-<question> = Could the Great Wall of China connect the Dodgers to the White Sox? <answer> = yes\nThe Great Wall of China is about 5,500 miles long\u2026 tionale generation and answer prediction as two independent processes. This is due to the spurious correlations between the question and answer, which is exploited as a reasoning shortcut by the student (Branco et al., 2021). The two issues together would lead to an unfaithful student which learns to generate vacuous rationales and may make predictions inconsistent with the rationales.\nTo address these issues, we propose to enhance the vanilla KD process from two ends respectively. To elicit more on-topic rationales from the teacher, we propose to leverage contrastive decoding which aims to ground each rationale to the answer ( \u00a7 3.1). This technique encourages the teacher to generate tokens that are more plausible only when the answer is considered instead of the ones that are fairly plausible even without the answer during decoding. To train a faithful student, we ask the student to conduct counterfactual reasoning, i.e., predicting accordingly when the rationales are leading to different answers ( \u00a7 3.2). We obtain the training data by asking the teacher to generate a rationale for a sampled incorrect answer. The reasoning shortcut between the question and the gold answer is thus removed since now the student needs to give a different answer for the same question, according to the rationales provided during training.\nWe conduct experiments on several open-domain question answering tasks that require knowledgeintensive reasoning. Experiments show that: (1) Contrastive decoding can lead to a more consistent teacher which generates rationales that are more supportive of the gold answers. (2) Trained on the more consistent rationale-answer pairs, the student learns to better associate the answer prediction with the rationale generation. (3) With counterfactual reasoning as an auxiliary training objective, the student learns not to take the reasoning shortcut and instead respect the rationale more. (4) Despite being more faithful, our model performs comparably to the baselines. (5) Ablation study shows that although performing better, larger student models are more prone to being inconsistent. Our method robustly remedies the inconsistency regardless of the size of the student model. (6) With a more faithful student, we can better improve its performance by correcting its rationale, demonstrating the utility of our method in model refinement.", "publication_ref": ["b27", "b22", "b13", "b16", "b11", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Chain-of-Thought Distillation", "text": "Our goal is to 1) elicit consistent rationales, i.e., those well justifying the gold answers, from a large LM as supervision, and then 2) train a selfconsistent student model to reason faithfully, i.e., answer accordingly to its generated rationale. We consider the task of language-based reasoning where the required knowledge is not provided in the task input. Specifically, we focus on open-domain question answering (QA) which is the most general setting adopted by prior works: given a question q, a QA system is asked to predict the gold answer a * . For interpretability, we also require the model to provide a free-text rationale r, which justifies its prediction. Below we describe the overview of a vanilla KD framework as illustrated in Figure 2. We then discuss the limitations and propose our method in \u00a7 3.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Generating Rationale Annotation", "text": "Instead of asking humans to annotate a rationale for each question-answer tuple {q, a * }, we obtain the rationale from a teacher model automatically using in-context learning. The idea is to prompt a frozen LM as the teacher with only a few an-notated examples as demonstration before a new instance is provided. Each example consists of a question q randomly sampled from the training set, the gold answer a * and a human-annotated rationale r which justifies why a * is correct. The prompt p is structured in the format as shown in Figure 2 (the Prompt in the left part). To obtain the rationale for a new question q, one basic strategy could be greedy decoding, which selects the most plausible token at each step:\nt * i = arg max P (t i |p, q, a * , t <i ).(1)", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Training a Student Model", "text": "Now with the annotated training data {q, r, a * }, we can train a smaller model as the student. There are many ways to implement a QA model that can make a prediction as well as generate a rationale.\nIn this work, we focus on the self-rationalization paradigm, where the student firstly generates a rationale and then predicts the answer conditioning on the generated rationale. This is in contrast to related works which conduct post-rationalization, i.e., generating the rationale after the answer is predicted, or multi-task learning, which treats rationale generation as an auxiliary task besides answer prediction. The reason is that the generation of the rationale for the latter two paradigms does not affect the decision making by design, and therefore the faithfulness of the rationale is not guaranteed in the first place. Given a question q, the student model is trained to output a sequence of rationale tokens concatenated with the answer tokens as shown in Figure 2 (the output in the right part). One straightforward implementation is simply fine-tuning a text-to-text LM over the silver training data generated by the teacher using standard language modeling loss:\nL f actual = \u2212 i log P (t i |q, t <i ),(2)\nwhich we refer as factual reasoning loss.\n3 Distilling a Self-Consistent Student\nThere are two vital issues with the vanilla KD process described in the previous section. Firstly, neural LMs are known to suffer from the issue of hallucination, meaning they often generate text that is not grounded by the input (Maynez et al., 2020;Ji et al., 2022). This would lead to the generated rationale not supporting the given answer. The inconsistency between the rationale and the answer Q: Would someone with back pain enjoy picking strawberries?\nA: The answer is no.\nQ: Would someone with back pain enjoy picking strawberries?\nA: The answer is <perturbed_answer>.", "publication_ref": ["b16", "b11"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Greedy Decoding :", "text": "-The back is a part of the body. The back is not a fruit. Thus, someone with back pain would not enjoy picking strawberries.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Contrastive Decoding", "text": "-:\n(a) <perturbed_answer> = empty string -Manual labor can cause back pain. Thus, someone with back pain would not enjoy picking strawberries.\n(b) <perturbed_answer> = yes -The spine is needed to support the body. If someone has back pain, they would not be able to pick strawberries. would then be inherited by the student, which is misled to think that the answer prediction is independent of the rationale generation. Secondly, the student model would learn to predict the answer by taking a reasoning shortcut (Branco et al., 2021), without taking into account the generated rationale (even though the answer prediction is conditioned on the rationale). This is due to the spurious correlations between the question and the answer which are found in various implicit reasoning task datasets (Gururangan et al., 2018;Zellers et al., 2019;Blodgett et al., 2020).\nThe two issues mentioned above would result in an untrustworthy student whose generated rationales do not consistently justify its answers. To mitigate this, we propose two corresponding techniques as detailed below.", "publication_ref": ["b3", "b9", "b30", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "A Consistent Teacher: Contrastive Decoding", "text": "To encourage the teacher to generate a more ontopic rationale that supports the answer, our proposed method extends a prior technique called contrastive decoding for open-ended text generation (Li et al., 2022b). The core idea is to search rationale tokens that are more plausible only when the answer is considered instead of the ones that are fairly plausible even without the answer during decoding. To implement this idea, we firstly model the hallucinating behavior by providing a perturbed answer a to the same teacher and then obtain the plausibility growth of any token t i given the answer\na * as G(t i |a * ) = log P (t i |p, q, A, a * , t <i ) P (t i |p, q, A, a , t <i ) . (3)\nWe investigate two ways of perturbing the answer: setting a as an empty string or an incorrect answer other than a * 2 . The first way (with an empty string) punishes tokens that are generally plausible when the gold answer a * is not considered by a hallucinated LM. The second way (with an incorrect answer) takes a step further by encouraging the teacher to generate a rationale that is more distinctive between gold and wrong answers. Figure 3 shows the generations for an example question from greedy decoding and contrastive decoding.\nTo strike a balance between language fluency and the grounding with a * , we incorporate the plausibility growth into Eq. 1 by aggregation as our final contrastive decoding strategy:\nt * i = arg max P (t i |p, q, A, a * , t <i ) + G(t i |a * )(4)", "publication_ref": ["b14"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "A Faithful Student: Counterfactual Reasoning", "text": "To encourage the student to reason faithfully towards its generated rationale, we train the student to conduct counterfactual reasoning (Roese, 1997), i.e., answer accordingly when the rationale is leading to a different answer. This would help remove the reasoning shortcut between a question and the gold answer (Figure 4) since now the student is asked to answer differently for the same question.\nTo implement this idea, we firstly replace the gold answer fed to the teacher in Eq. 4 with a wrong answer a randomly (with the same sampling strategy as in \u00a7 3.1) as if a is correct. We thus obtain a counterfactual rationale r that leads to the wrong answer a . We then train the model to generate a when r is directly fed to the decoder as teacherforcing (the language modeling loss is only applied to the answer tokens t i \u2208 a ):\nL counterf actual = \u2212 i log P (t i |q, r , t <i ). (5)\nTo avoid confusing the student about the task, we indicate the training objective Eq. 2 (or Eq. 5) to the student by appending the keyword [Factual] (or [Counterfactual]) at the beginning of 2 For yes/no or true/false questions, we obtain the incorrect answer by flipping the gold answer. For multi-choice questions, we randomly pick one incorrect answer.  both the input sequence to the encoder and the output sequence to the decoder (see Figure 4 for an example input and output). The overall training loss is the sum of Eq. 2 and Eq. 5.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "Experiments", "text": "We aim to answer the following research questions in our experiments: (1) Can our contrastive decoding strategy lead to a more consistent teacher?\n(2) Can a more consistent teacher and the counterfactual reasoning objective lead to a student that reasons more faithfully? (3) Can we have more control over a self-consistent student's predictions by modifying its generated rationales?", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Datasets", "text": "We experiment with several language-based reasoning tasks that are knowledge-intensive: (1) CSQA (Talmor et al., 2018) ", "publication_ref": ["b24"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation Metrics", "text": "(1) To evaluate the consistency between the rationales generated by the teacher and the gold answers Figure 5: Simulatability (LAS) of the rationales generated from different teacher models as a measurement the consistency between the rationales and the gold answers. {Greedy, CD-Empty, CD-Wrong} refer respectively to using greedy decoding, contrastive decoding with empty/wrong answer to obtain rationale tokens from the teacher.\nprovided as input, we use the LAS metric (Hase et al., 2020), whose core idea is to measure how well the rationales assist a simulator to predict the gold answers a * , computed as the difference between the task performance when the rationale is provided as input vs. when it is not, namely Acc(qr \u2192 a * ) \u2212 Acc(q \u2192 a * ).\n(2) To evaluate the faithfulness of the rationales generated by the student, we use LAS to measure how well the rationales help a simulator to predict a student's predictions a , namely Acc(qr \u2192 a ) \u2212 Acc(q \u2192 a ).\nWe implement each simulators with a fine-tuned T5-large model (Raffel et al., 2020) respectively.\n(3) To evaluate how well the student preserves its task performance on the downstream datasets, we use accuracy as the metric.", "publication_ref": ["b10", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Implementation Details", "text": "We use GPT-neox (Black et al., 2022), a LM with 20B parameters as the teacher since the model checkpoint is publicly available, which allows us to host it offline and have access to token-wise probabilities as required in our contrastive decoding. We then implement two teacher variants by using an empty string or a wrong answer as the perturbed answer a in Eq. 4 respectively. The obtained rationales are then used to fine-tune two T5-3b LMs as the students respectively. For both variants, we train the student using the sum of factual training loss Eq. 2 and counterfactual training loss Eq. 5.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Baselines", "text": "Chain-of-Thought (CoT) Since we elicit the rationales from GPT-neox (with 20b parameters) (Black et al., 2022) to train the student, we prompt the same model (GPT-neox) to firstly explain and then predict using CoT prompting (Wei et al., 2022b).\nLearn from Human To demonstrate the advantage of our automatic way of generating rationale annotations, we implement this baseline as a finetuned T5-3b LM over human-annotated rationales, which are expensive to obtain and could be noisy.", "publication_ref": ["b1", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Learn from Greedy Decoding", "text": "We implement this baseline as a fine-tuned T5-3b LM over the rationales obtained by greedy decoding using the same LM as our main method. We also implement another variant by adding the counterfactual reasoning loss when fine-tuning the student, where the rationales for the wrong answers are obtained by greedy decoding. We also implement two baselines of our method by training the student with the rationales obtained by contrastive decoding with empty/wrong answers based on factual reasoning only. We run all the experiments for 5 times using a fixed set of random seeds and report the average results.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Main Results", "text": "Can contrastive decoding lead to a more consistent teacher? Figure 5 shows the consistency between the rationales generated by different teachers and the gold answers measured by LAS. Across four datasets, contrastive decoding with either empty or wrong answers yield more consistent rationales compared to human annotation and greedy decoding. This demonstrates the effectiveness of our contrastive decoding strategy in encouraging the teacher to generate more on-topic rationales. Moreover, using wrong answers is better than using empty strings for contrastive decoding. This shows that by contrasting with the wrong answers, the teacher can generate more distinguishable rationales that lead to the gold answers, thus obtain higher consistency. Greedy decoding yields less consistent rationales compared to human annotation, verifying our claim that LMs are prone to generating text not grounded by the gold answers.  We also conduct a human evaluation over 100 rationales generated by different decoding strategies for StrategyQA. Annotators are asked to judge the rationales by 3 dimensions: 1) Grammaticality (Is the rationale grammatical?) 2) New Info (Does the rationale provide new information not expressed in the question?) 3) Supports Answer (Does the rationale justify the answer?). Table 1 confirms that our two contrastive decoding strategies yield more informative and on-topic rationales than greedy de-coding, with a slightly worse grammaticality. We list examples in Table 2 (appendix) to showcase how rationales from contrative decoding are more consistent with gold answers than greedy decoding.\nCan a more consistent teacher train a more faithful student? Figure 6 (upper parts of each sub-figure ) shows the faithfulness of the students measured in LAS on the experimented datasets. First, the CoT method often achieves much lower LAS compared to the KD methods across four datasets, showing that the generated rationales do not faithfully reflect the decision making in CoT. Second, we observe that students trained with the rationales from contrastive decoding with either empty strings or wrong answers generally achieve higher LAS scores compared to the baselines. Together with the observation on the consistency of the teacher (Figure 5), this validates that a more consistent teacher train a more faithful student and the inconsistency in the training data generated by the teacher will be inherited by the student.\nCan couterfactual reasoning loss further improve the faithfulness? Figure 6 shows the students fine-tuned additionally with counterfactual training loss achieve higher faithfulness than their counterparts which are fine-tuned with factual training only. This validates that counterfactual reasoning can further improve the student's faithfulness, as it may still treat rationale generation and answer prediction as two independent processes. Can a faithful student still preserve its performance? Figure 6 (lower parts of each sub-figure ) shows the performance of the students measured in accuracy. First, CoT methods achieve lower accuracy compared to the KD methods, showing the benefit of combining the supervision from the teacher (the rationales) and the labeled datasets (the answers). Second, all the KD methods achieve comparable performance. Together with the observation over faithfulness, this demonstrates our method can improve faithfulness of the model while not hurting its performance. Note that the student which learns from human annotation achieves slightly better results compared to other students. This is because the human rationales are less consistent with the answers (as evidenced in Figure 5). Therefore, the student learns to generate the rationales and predict the answers more independently, which allows it to exploit the spurious correlation and achieve better performance. Our further analysis ( \u00a7 4.7) shows that such performance gain is suspicious as changing the rationales does not change the student's predictions mostly.", "publication_ref": [], "figure_ref": ["fig_4", "fig_4", "fig_4"], "table_ref": ["tab_2"]}, {"heading": "Ablation on the student model size", "text": "We ablate the student model size to see how its faithfulness and performance are affected. From Figure 7, we observe that larger student models achieve higher performance but lower faithfulness. This confirms that it requires sufficient capacity for storing knowledge necessary for reasoning (Wei et al., 2022a), but larger models are also better at answering the questions independently of the rationales. Still, our models are more faithful than baselines and comparable in performance with different model sizes.", "publication_ref": ["b26"], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "Controlling the behavior of the Student", "text": "One important utility of faithful rationales is that we can have more control over the behavior of the  student via changing its rationales. If the model can make predictions consistent with its rationales, we can either impair or improve the its performance by perturbing or refining its rationales. To verify this, we conduct two types of edition to the rationales generated by the student, namely perturbation and refinement as described below. We then feed the edited rationales to the decoder of the student directly (as teacher forcing) and see if the student will act accordingly, i.e., predict more badly (or accurately) due to the worse (or better) rationales.\nRationales Perturbation For perturbing the rationales, we randomly replace 50% of the tokens in the generated rationales from the student and then feed the perturbed rationales r back to the decoder of the student. We finally calculate the performance drop (or sensitivity), i.e., Acc(qr \u2192 a * ) \u2212 Acc(qr \u2192 a * ). Figure 8 (the lower parts) shows the results on CSQA and CREAK. First, perturbing the rationales from the student that is finetuned with human-annotation has little (down to 1.1% on CSQA) impact on its performance, meaning that the student largely ignores the rationales when making prediction. Second, learning from rationales obtained by contrastive decoding with empty or wrong answers leads to a student that is more sensitive to the rationale perturbation compared to learning from greedy decoding. This again verifies the necessity of having a consistent teacher in order to train a faithful student. Lastly, our counterfactual training loss further improves the sensi- tivity of the student, demonstrating that the student is more faithful towards the rationales.", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "Rationales Refinement", "text": "As a proxy refinement, we obtain the oracle rationales r * automatically by asking the teacher to rationalize for gold answers using each compared decoding strategy. For the student trained with human annotation, we directly use the annotated rationales as the oracle. We then calculate the performance gain, i.e., Acc(qr * \u2192 a * ) \u2212 Acc(qr \u2192 a * ). Figure 8 (the upper parts) shows the results on CSQA and CREAK. First, we observe that oracle human-annotated rationales do not bring as much performance gain as machinegenerated rationales do. This demonstrates that even trained with human annotation, the student is still prone to being unfaithful to its rationales. Second, we observe that contrastive decoding (with either empty strings or wrong answers) leads to higher performance gains from the student. By adding counterfactual training, the performance gains are further increased. This demonstrates the advantage brought by our method, which is that we can have more success in debugging a reasoning model by refining its rationales.", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "Related Works", "text": "Free-text Rationales A variety of datasets have been proposed to collect human-annotated rationales alongside each task instance (Camburu et al., 2018;Rajani et al., 2019;Aggarwal et al., 2021), aiming to train the downstream models to explain their predictions in natural language. However, human annotation is expensive and the resulting ratio-nales are reported to be of poor quality (Aggarwal et al., 2021;Sun et al., 2022). Our work leverages a prompted LM to obtain rationales automatically for supporting both correct and incorrect answers, using only a few annotated examples as demonstration. The rationales for supporting the incorrect answers further enable the student to conduct counterfactual reasoning, which is not available from existing human annotation.\nPrompted Self-Rationalization Models Recent works have been proposed to prompt large LMs to generate a free-text rationale before making the prediction (Nye et al., 2021;Wei et al., 2022b). However, this technique relies on extremely large LMs (with over 100B parameters) to work effectively (Wei et al., 2022b,a), which requires significant computation resources or expensive API calls (Shridhar et al., 2022). Meanwhile, the rationales generated by such models are shown to contradict the context (Ye and Durrett, 2022) and fail to faithfully represent the underlying reasoning process (Wang et al., 2022). In contrast, our student is trained to be more faithful towards its generated rationales using a smaller LM.\nKnowledge Distillation There exist some works that explore the idea of distilling rationales knowledge from a large LM to a small LM as the student. Chan et al. proposed to learn a student model that only predicts answers from a teacher model that is augmented with rationales. Eisenstein et al. proposed to train the student to extract the sentence containing the answer, which is not applicable to reasoning tasks that require background knowledge. Shridhar et al. proposed to train the student to ask and answer sub-questions necessary for decomposing the main question, which is tailored to solve math word problems (Cobbe et al., 2021) with an equation generator for guiding the student while we do not have such a constraint. Li et al. proposed to train the student on the joint task of generating the answers and the rationales, which only act as a regularization and do not affect the student's prediction during inference. More importantly, both Shridhar et al. and Li et al. do not consider the faithfulness of the rationales, which is critical for examining the behavior of the student.", "publication_ref": ["b4", "b20", "b0", "b0", "b17", "b27", "b22", "b29", "b25", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "This work presents a faithful KD framework for learning a small, self-consistent CoT model from a large teacher model. To ensure the student reason faithfully, we propose (1) contrastive decoding for obtaining a consistent teacher and (2) counterfactual reasoning for teaching a faithful student. Experiments show that these two techniques jointly lead to a more faithful student compared to the baselines, while preserving much performance accuracy. Our further analysis shows that changing the rationales has a larger impact on the student's behavior and thus we can have more success in debugging the model by refining its rationales.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Compared to a standard knowledge distillation process, our method requires additional computation when preparing training data and training the student. First, our contrastive decoding needs to perform forward pass in the teacher model one time more than greedy decoding does to obtain the perturbed plausibility for each token generated (Eq. 4). Second, our KD process introduces additional training data for training the student with the counterfactual reasoning objective (Eq.5). Besides computation cost, this work focuses on improving faithfulness of the rationales rather than performance, which is complementary to prior works which leverages rationales for improving the performance only.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "Our KD process leverages large LMs to obtain rationale annotation, which may expose social bias encoded in these models (Lucy and Bamman, 2021). The bias may be further inherited by the student model. Nevertheless, our method improves the faithfulness of the rationales, making the predictions from the student accountable. Without the faithful rationales, it would be unclear to users about whether the model is making predictions based on some unintended bias.", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "A Appendix", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Example generations from teachers", "text": "Table 2: Examples where rationales generated by contrastive decoding with empty/wrong answers are rated higher than rationales generated by greedy decoding in our human evaluation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ACL 2023 Responsible NLP Checklist", "text": "A For every submission: A1. Did you describe the limitations of your work?\nIn the \"Limitations\" section after Section 6. A2. Did you discuss any potential risks of your work?\nIn the Ethics Statements section before the References section.\nA3. Do the abstract and introduction summarize the paper's main claims?\nIn the abstract section and Section 1. B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Section 3.2 explains how the generated rationales are used as supervision.\nB4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. Left blank.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Section 4.1 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? Section 4.3.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "5557", "text": "C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? Models (language models) are fine-tuned with default hyperparameters specified by the original papers.\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? Section 4.4.\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? Not applicable. Left blank. D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? Not applicable. Left blank.\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? Not applicable. Left blank.\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? Not applicable. Left blank.\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board? Not applicable. Left blank.\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? Not applicable. Left blank.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "5558", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Explanations for commonsenseqa: New dataset and models", "journal": "", "year": "2021", "authors": "Shourya Aggarwal; Divyanshu Mandowara; Vishwajeet Agrawal; Dinesh Khandelwal; Parag Singla; Dinesh Garg"}, {"ref_id": "b1", "title": "GPT-NeoX-20B: An opensource autoregressive language model", "journal": "", "year": "2022", "authors": "Sid Black; Stella Biderman; Eric Hallahan; Quentin Anthony; Leo Gao; Laurence Golding; Horace He; Connor Leahy; Kyle Mcdonell; Jason Phang; Michael Pieler; Shivanshu Usvsn Sai Prashanth;  Purohit"}, {"ref_id": "b2", "title": "Language (technology) is power: A critical survey of\" bias", "journal": "", "year": "2020", "authors": " Su Lin; Solon Blodgett; Hal Barocas; Iii Daum\u00e9; Hanna Wallach"}, {"ref_id": "b3", "title": "Shortcutted commonsense: Data spuriousness in deep learning of commonsense reasoning", "journal": "", "year": "2021", "authors": "Ruben Branco; Ant\u00f3nio Branco; Joao Rodrigues; Joao Silva"}, {"ref_id": "b4", "title": "e-snli: Natural language inference with natural language explanations", "journal": "", "year": "2018", "authors": "Oana-Maria Camburu; Tim Rockt\u00e4schel; Thomas Lukasiewicz; Phil Blunsom"}, {"ref_id": "b5", "title": "Knife: Knowledge distillation with free-text rationales", "journal": "", "year": "2022", "authors": "Aaron Chan; Zhiyuan Zeng; Wyatt Lake; Brihi Joshi; Hanjie Chen; Xiang Ren"}, {"ref_id": "b6", "title": "Training verifiers to solve math word problems", "journal": "", "year": "2021", "authors": "Karl Cobbe; Vineet Kosaraju; Mohammad Bavarian; Jacob Hilton; Reiichiro Nakano; Christopher Hesse; John Schulman"}, {"ref_id": "b7", "title": "Honest students from untrusted teachers: Learning an interpretable question-answering pipeline from a pretrained language model", "journal": "", "year": "2022", "authors": "Jacob Eisenstein; Daniel Andor; Bernd Bohnet; Michael Collins; David Mimno"}, {"ref_id": "b8", "title": "Did aristotle use a laptop? a question answering benchmark with 5554 implicit reasoning strategies", "journal": "Transactions of the Association for Computational Linguistics", "year": "2021", "authors": "Mor Geva; Daniel Khashabi; Elad Segal; Tushar Khot; Dan Roth; Jonathan Berant"}, {"ref_id": "b9", "title": "Annotation artifacts in natural language inference data", "journal": "", "year": "2018", "authors": "Swabha Suchin Gururangan; Omer Swayamdipta; Roy Levy;  Schwartz; R Samuel; Noah A Bowman;  Smith"}, {"ref_id": "b10", "title": "Leakage-adjusted simulatability: Can models generate non-trivial explanations of their behavior in natural language?", "journal": "", "year": "2020", "authors": "Peter Hase; Shiyue Zhang; Harry Xie; Mohit Bansal"}, {"ref_id": "b11", "title": "Survey of hallucination in natural language generation", "journal": "", "year": "2022", "authors": "Ziwei Ji; Nayeon Lee; Rita Frieske; Tiezheng Yu; Dan Su; Yan Xu; Etsuko Ishii; Yejin Bang; Andrea Madotto; Pascale Fung"}, {"ref_id": "b12", "title": "QASC: A dataset for question answering via sentence composition", "journal": "AAAI Press", "year": "2020-02-07", "authors": "Tushar Khot; Peter Clark; Michal Guerquin; Peter Jansen; Ashish Sabharwal"}, {"ref_id": "b13", "title": "Explanations from large language models make small reasoners better", "journal": "", "year": "", "authors": "Shiyang Li; Jianshu Chen; Yelong Shen; Zhiyu Chen; Xinlu Zhang; Zekun Li; Hong Wang; Jing Qian; Baolin Peng; Yi Mao"}, {"ref_id": "b14", "title": "Contrastive decoding: Open-ended text generation as optimization", "journal": "", "year": "2022", "authors": "Lisa Xiang; Ari Li; Daniel Holtzman; Percy Fried; Jason Liang; Tatsunori Eisner; Luke Hashimoto; Mike Zettlemoyer;  Lewis"}, {"ref_id": "b15", "title": "Gender and representation bias in GPT-3 generated stories", "journal": "Virtual. Association for Computational Linguistics", "year": "2021", "authors": "Li Lucy; David Bamman"}, {"ref_id": "b16", "title": "On faithfulness and factuality in abstractive summarization", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Joshua Maynez; Shashi Narayan; Bernd Bohnet; Ryan Mcdonald"}, {"ref_id": "b17", "title": "Show your work: Scratchpads for intermediate computation with language models", "journal": "", "year": "2021", "authors": "Maxwell Nye; Anders Johan Andreassen; Guy Gur-Ari; Henryk Michalewski; Jacob Austin; David Bieber; David Dohan; Aitor Lewkowycz; Maarten Bosma; David Luan"}, {"ref_id": "b18", "title": "Creak: A dataset for commonsense reasoning over entity knowledge", "journal": "", "year": "2021", "authors": "Yasumasa Onoe; J Q Michael; Eunsol Zhang; Greg Choi;  Durrett"}, {"ref_id": "b19", "title": "Exploring the limits of transfer learning with a unified text-totext transformer", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b20", "title": "Explain yourself! leveraging language models for commonsense reasoning", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Bryan Nazneen Fatema Rajani; Caiming Mccann; Richard Xiong;  Socher"}, {"ref_id": "b21", "title": "Counterfactual thinking. Psychological bulletin", "journal": "", "year": "1997", "authors": "J Neal;  Roese"}, {"ref_id": "b22", "title": "Distilling multi-step reasoning capabilities of large language models into smaller models via semantic decompositions", "journal": "", "year": "2022", "authors": "Kumar Shridhar; Alessandro Stolfo; Mrinmaya Sachan"}, {"ref_id": "b23", "title": "2022. Investigating the benefits of freeform rationales", "journal": "", "year": "", "authors": "Jiao Sun; Swabha Swayamdipta; Jonathan May; Xuezhe Ma"}, {"ref_id": "b24", "title": "Commonsenseqa: A question answering challenge targeting commonsense knowledge", "journal": "", "year": "2018", "authors": "Alon Talmor; Jonathan Herzig; Nicholas Lourie; Jonathan Berant"}, {"ref_id": "b25", "title": "Pinto: Faithful language reasoning using prompt-generated rationales", "journal": "", "year": "2022", "authors": "Peifeng Wang; Aaron Chan; Filip Ilievski; Muhao Chen; Xiang Ren"}, {"ref_id": "b26", "title": "Emergent abilities of large language models", "journal": "", "year": "", "authors": "Jason Wei; Yi Tay; Rishi Bommasani; Colin Raffel; Barret Zoph; Sebastian Borgeaud; Dani Yogatama; Maarten Bosma; Denny Zhou; Donald Metzler"}, {"ref_id": "b27", "title": "Chain of thought prompting elicits reasoning in large language models", "journal": "", "year": "2022", "authors": "Jason Wei; Xuezhi Wang; Dale Schuurmans; Maarten Bosma; Ed Chi; Quoc Le; Denny Zhou"}, {"ref_id": "b28", "title": "Reframing human-ai collaboration for generating free-text explanations", "journal": "", "year": "2021", "authors": "Sarah Wiegreffe; Jack Hessel; Swabha Swayamdipta; Mark Riedl; Yejin Choi"}, {"ref_id": "b29", "title": "The unreliability of explanations in few-shot in-context learning", "journal": "", "year": "2022", "authors": "Xi Ye; Greg Durrett"}, {"ref_id": "b30", "title": "Hellaswag: Can a machine really finish your sentence?", "journal": "", "year": "2019", "authors": "Rowan Zellers; Ari Holtzman; Yonatan Bisk; Ali Farhadi; Yejin Choi"}, {"ref_id": "b31", "title": "Q: Do workers at Nissan's headquarters eat with chopsticks? A: Yes. Greedy: Workers at Nissan's headquarters eat with chopsticks. Chopsticks are used in China. Thus, workers at Nissan's headquarters eat with chopsticks", "journal": "", "year": "", "authors": ""}, {"ref_id": "b32", "title": "Contrastive-Empty: Workers at Nissan's headquarters are Japanese. Japanese eat with chopsticks", "journal": "", "year": "", "authors": ""}, {"ref_id": "b33", "title": "Japan is a country where people eat with chopsticks. Thus, workers at Nissan's headquarters eat with chopsticks", "journal": "", "year": "", "authors": " Contrastive-Wrong"}, {"ref_id": "b34", "title": "Greedy: PETA is an animal rights group. The Constitution of the United States is a document that protects the rights of animals. Thus, PETA would be offended by the Constitution of the United States", "journal": "", "year": "", "authors": ""}, {"ref_id": "b35", "title": "PETA is against the use of animal products. The Constitution of the United States is written on animal products. Thus, PETA would be offended by the Constitution of the United States", "journal": "", "year": "", "authors": " Contrastive-Empty"}, {"ref_id": "b36", "title": "PETA is against the use of animal products. The Constitution of the United States is written on animal products. Thus, PETA would be offended by the Constitution of the United States", "journal": "", "year": "", "authors": " Contrastive-Wrong"}, {"ref_id": "b37", "title": "Sam Harris worship Shiva? A: No. Greedy: Sam Harris is a Hindu. Shiva is a Hindu god. Thus, Sam Harris does not worship Shiva", "journal": "", "year": "", "authors": ""}, {"ref_id": "b38", "title": "Sam Harris is an atheist. Thus, Sam Harris does not worship Shiva", "journal": "", "year": "", "authors": " Contrastive-Empty"}, {"ref_id": "b39", "title": "Greedy: A dozen pickles have about 1,000 calories. A person needs about 2,000 calories a day. Thus, a dozen pickles would not be good for easing hypertension", "journal": "", "year": "", "authors": ""}, {"ref_id": "b40", "title": "Contrastive-Wrong: The recommended daily intake of sodium is 2,400 mg. A dozen pickles have about 0.5 g of sodium. Thus, a dozen pickles would not be good for easing hypertension", "journal": "", "year": "", "authors": " Contrastive-Empty"}, {"ref_id": "b41", "title": "it be unusual to see frost in September in Texas? A: Yes. Greedy: Frost can occur in September in Texas. However", "journal": "", "year": "", "authors": ""}, {"ref_id": "b42", "title": "Frost occurs in the winter. Thus, it would be unusual to see frost in September in Texas", "journal": "", "year": "", "authors": " Contrastive-Empty"}, {"ref_id": "b43", "title": "Frost is usually seen in the winter. Thus, it would be unusual to see frost", "journal": "", "year": "", "authors": " Contrastive-Wrong"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Overview of our knowledge distillation framework for faithful reasoning. (a) Teacher: A large LM prompted to generate a consistent rationale given a question and the gold answer in the training set via contrastive decoding. (b) Student: A small LM fine-tuned to generate a rationale and then answer via counterfactual reasoning.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure3: Contrastive decoding for obtaining rationales that are more grounded by the gold answers, by preferring tokens that are more plausible only when the answer is considered.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Counterfactual reasoning for teaching the student to reason faithfully, i.e., answer differently according to the rationale.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure 6: Faithfulness (LAS) and task performance (accuracy) of the compared methods on the experimented datasets. The x-axis represents the CoT baseline and knowledge distillation methods that use Human Annotation, Greedy Decoding, and Contrastive Decoding with empty strings/wrong answers as the teachers. For knowledge distllation methods, we apply Factual training and Factual+Counterfactual training to train the students.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 7 :7Figure 7: Faithfulness (LAS) and task performance (accuracy) of the compared methods with different student model sizes. Each model is named by the teacher it learns from and the training objective as teacher model:training objective.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 8 :8Figure 8: Performance gain (drop) of the compared methods when the oracle (perturbed) rationales are fed to the decoder of the model on CSQA and CREAK.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "The European wildcat is not a predator of the black-tailed jackrabbit. Thus, the black-tailed jackrabbit does not fear the European wildcat. So the answer is no.", "figure_data": "QReasoning shortcut!(a) Factual reasoning Input: [Facutal] Do black-tailed jackrabbits fearthe European wildcat?EQA Shortcut removed!(b) Counterfactual reasoning Output: [Factual] Input: [Counterfacutal] Do black-tailed jackrabbits fear the European wildcat?Output: [Counterfacutal] The European wildcat isa predator of the black-tailed jackrabbit. Thus, theE'A'European wildcat is a threat to the black-tailed jackrabbit. So the answer is yes."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Khot et al., 2020) is an eight-choice QA dataset which requires both knowledge facts retrieval and the common sense for composing the facts. Since the test labels for these datasets are not publicly available, we treat the official development set as our test set, while randomly splitting the official training set into a new training set and development set.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Human evaluation on the rationales generated by different teacher models for StrategyQA. A fair level of agreement measured by Fleiss Kappa (\u03ba=0.26) is obtained among three annotators.", "figure_data": "Teacher ModelGrammaticality New Info Supports AnswerGreedy Contrast.-Empty0.99 0.970.65 0.770.48 0.58Contrast.-Wrong0.970.820.63"}], "formulas": [{"formula_id": "formula_0", "formula_text": "t * i = arg max P (t i |p, q, a * , t <i ).(1)", "formula_coordinates": [3.0, 108.7, 215.31, 180.43, 21.19]}, {"formula_id": "formula_1", "formula_text": "L f actual = \u2212 i log P (t i |q, t <i ),(2)", "formula_coordinates": [3.0, 105.52, 590.87, 183.62, 25.16]}, {"formula_id": "formula_2", "formula_text": "a * as G(t i |a * ) = log P (t i |p, q, A, a * , t <i ) P (t i |p, q, A, a , t <i ) . (3)", "formula_coordinates": [4.0, 70.86, 71.64, 218.28, 55.12]}, {"formula_id": "formula_3", "formula_text": "t * i = arg max P (t i |p, q, A, a * , t <i ) + G(t i |a * )(4)", "formula_coordinates": [4.0, 78.08, 340.48, 211.05, 27.33]}, {"formula_id": "formula_4", "formula_text": "L counterf actual = \u2212 i log P (t i |q, r , t <i ). (5)", "formula_coordinates": [4.0, 78.1, 648.58, 211.03, 25.16]}], "doi": ""}