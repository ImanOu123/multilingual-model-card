{"title": "Mitigating Political Bias in Language Models Through Reinforced Calibration", "authors": "Ruibo Liu; Chenyan Jia; Jason Wei; Guangxuan Xu; Lili Wang; Soroush Vosoughi", "pub_date": "2021-04-30", "abstract": "Current large-scale language models can be politically biased as a result of the data they are trained on, potentially causing serious problems when they are deployed in realworld settings. In this paper, we describe metrics for measuring political bias in GPT-2 generation and propose a reinforcement learning (RL) framework for mitigating political biases in generated text. By using rewards from word embeddings or a classifier, our RL framework guides debiased generation without having access to the training data or requiring the model to be retrained. In empirical experiments on three attributes sensitive to political bias (gender, location, and topic), our methods reduced bias according to both our metrics and human evaluation, while maintaining readability and semantic coherence.", "sections": [{"heading": "Introduction", "text": "Large-scale language models (LMs) can generate humanlike text and have shown promise in many Natural Language Generation (NLG) applications such as dialogue generation (Zhang et al. 2020;Peng et al. 2020) and machine translation (Yang et al. 2020;Zhu et al. 2020). These models are often trained on large quantities of unsupervised datafor example, GPT-2 (Radford et al. 2019) is trained on a dataset of 8 million unlabeled web pages. Although training data is typically collected with content diversity in consideration, other factors, such as ideological balance, are often ignored. This raises a couple of important questions: Do current large-scale generative language models, such as GPT-2, perpetuate political biases towards a certain ideological extreme? And if so, can they be guided towards politically unbiased generation?\nLM generation typically relies on a given text prompt, e.g., \"I'm from Massachusetts. I will vote...\", and we notice that the demographic (i.e., \"Massachusetts\") and topic attributes within the prompts have substantial influence on the ideological tendencies of the generated texts. In this work, we study the ideological biases of texts generated by GPT-2 with respect to three attributes: gender, location and topic.\nWe propose and investigate two bias types: 1) Indirect Bias, which measures bias of texts generated using prompts Copyright \u00a9 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nwith particular keywords of the aforementioned attributes, and 2) Direct Bias, which measures bias in texts generated using prompts that have directly ideological triggers (e.g., democrat, republican) in addition to keywords of aforementioned attributes. Table 1 shows four samples of text generated by off-the-shelf GPT-2 with different attribute keywords in the prompts-all samples exhibit political bias. For example, when triggered with a prompt including marijuana, the generated text tends to present a favorable attitude (e.g., \"I believe it should be legal and not regulated.\"), which is mostly a liberal stance. More interestingly, even a prompt including a conservative trigger (republican) results in generation which leans to the liberal side (\"vote for Hillary...\").\nThe ethical implications of bias in NLG have started to receive considerable attention in discussions around the social impact of AI ( Sheng et al. 2020( Sheng et al. , 2019Wallace et al. 2019;Bordia and Bowman 2019). Given the ever-growing number of down-stream models that rely on GPT-2 (and other LMs), it is of utmost importance, and a matter of fairness, for these LMs to generate politically unbiased text.\nIn this paper, we define what political bias is in generative LMs and present how to mitigate such bias during generation. Specifically, our contributions are three-fold:\n\u2022 We propose two bias metrics (Indirect Bias and Direct Bias) to quantify the political bias in language model generation ( \u00a73). Although in this work we focus on political bias based on three attributes (gender, location and topic), our framework can be easily extended to other types of bias and different attributes.\n\u2022 We present a reinforcement learning based framework for mitigating political bias in two modes: word-embedding guided debias and classifier-guided debias ( \u00a74). Since our framework neither accesses the original training data nor retrains the model from scratch, it can be generalized to other large-scale LMs with minimum modification.\n\u2022 We systematically evaluate our methods with the proposed metrics, finding that it successfully reduces political bias while maintaining reasonable fluency ( \u00a76.1- \u00a76.3). Furthermore, human evaluation confirms that our methods successfully mitigate the political bias without sacrificing readability and semantic coherence ( \u00a76.4).", "publication_ref": ["b46", "b32", "b43", "b52", "b35", "b39", "b38", "b42", "b9"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Attribute", "text": "Bias [GENDER] Amy is a republican. About voting he/she will + vote for Hillary but doesn't want to be \"Hillary Clinton's Democrat\"! Table 1: Demo examples of Indirect Bias and Direct Bias existing in vanilla GPT-2 generation. For Indirect Bias, we fill in the blank [ATTR] with keywords representing the actual value of the demographic attribute. For Direct Bias, besides the keywords replacement, we also trigger the generation with a given ideology (L: liberal or C: conservative).", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Related Work", "text": "To mitigate LM bias, common approaches include modifying the training data through data augmentation, manipulating word embeddings, and adjusting predictions to produce more fair classifications. This section explores this prior art.  (Park, Shin, and Fung 2018;Liu et al. 2020), knowledge graph building (Mitchell et al. 2019) and machine translation (Stanovsky, Smith, and Zettlemoyer 2019).\nEmbedding Manipulation. Societal biases have also been reflected in word embeddings (Garg et al. 2018). To mitigate gender bias in Word2Vec (Mikolov et al. 2013 Prediction Adjustment. Finally, there is related art in machine learning fairness research seeking to produce \"fair\" classifiers or unbiased feature representations Donini et al. 2018;Misra et al. 2016;Kamishima et al. 2012). For instance, Zhang, Lemoine, and Mitchell use an adversarial network where the generator attempted to prevent the discriminator from identifying gender in an analogy completion task. All these works, however focus on classification tasks rather than exploring the bias in LM generation.\nAlthough these approaches can be effective, it can be challenging to apply them to pretrained large-scale LMs, since 1) the corpus used to train LMs is not always publicly available, and 2) it is often costly to retrain large-scale LMs with augmented data. In this paper, we will propose an approach that neither accesses the original training data and nor retrains the language model.", "publication_ref": ["b31", "b25", "b29", "b40", "b16", "b27", "b15", "b28", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Political Bias Measurement", "text": "We first introduce the notation used throughout the paper and briefly describe the problem setup. We then formally define the political bias in generative language models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Notation", "text": "Sensitive Attributes. In this paper, we explore three sensitive attributes: gender, location, topic. Each attribute contains multiple options (e.g., male is an option of gender, blue state is an option for location), each of which can be exemplified by keywords (e.g., Jacob is a keyword for male, Massachusetts is a keyword for blue states). Moving forward, we refer to a keyword as a, an option as o, and an attribute as A.\nLanguage Modeling. Auto-regressive LMs are typically triggered by a prompt (a span of of pre-defined tokens) (Radford et al. 2019). In our case, given a prompt \u03c8, a LM will generate a sequence of T tokens X = [x t ] for t \u2208 [1 : T ] where x t is given by:\nx t \u223c argmax xt Pr(x t ) = LM(x 1:t\u22121 |\u03c8) .\n(1)\nWhen computing indirect bias, each prompt is filled in with an keyword a. When computing direct bias, each prompt is filled in with both an keyword a and a liberal (L) or conservative (C) ideology injection.\nBias Judgement. To measure the extent of political bias in outputs generated by LMs, we pretrain a political ideology classifier f judge . For a given generated sequence of tokens X, it computes a score y = f judge (X) \u2208 [0, 1] where y \u2192 0 indicates liberal bias and y \u2192 1 indicates conservative bias. Following prior work on fairness in machine learning , we define the base rate of a given set of texts as the distribution of corresponding probabilities of each text being classified as class 1 by our pretrained classifier.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition", "text": "This section defines two methods for measuring the extent of bias in texts generated by a LM.\nINDIRECT BIAS For indirect prompts, which take in only a keyword without any specified political biases, indirect bias measures the amount of bias our pretrained classifier detects in texts generated using keywords from a specific option compared with the bias in texts generated using keywords from all options. Formally, we consider two variables in this metric: 1. X o is the set of texts generated with prompts using every keyword associated with a single given option o, and 2. X \u2200o\u2208A is the set of texts generated with prompts using every keyword from all options belonging to attribute A. Now, the indirect bias is computed using the distance between the base rates of X o and X \u2200o\u2208A :\nB indirect (o, A) := \u2206 BR (X o , X \u2200o\u2208A ) ,(2)\nwhere \u2206 BR is the second order Sliced Wasserstein Distance (SWD) (Jiang et al. 2019;Rabin et al. 2011) between the base rates (computed by f judge ) of two sets of texts. The theoretical underpinning of this bias is conditional independence: if the political bias of LM generation is independent of option o, we should have Pr(y = 1|\u03c8\u2229o) = Pr(y = 1|\u03c8).\nIn other words, if the LM is unbiased on option o, its base rate given o should equal the option-invariant base rate. Therefore, the distance between these two base rates measures the dependence of generation on a certain option o.\nDIRECT BIAS As another metric, we also consider direct bias, which measures the extent of bias in texts generated by LMs when given prompts that directly contain political ideology information. We define direct bias as the difference in indirect bias of generated texts when given liberal-leaning (L) versus conservative-leaning (C) prompts:\nB direct := |B L indirect (o, A) \u2212 B C indirect (o, A)| .\n(3) By \"leaking\" ideology information to the LM directly through prompts with political leanings, we expect generated text to be politically biased. If an LM is able to generate equally biased texts given both liberal and conservative prompts, then the direct bias should be close to 0. If the LM is not able to generate adequately-biased texts given prompts with a political leaning (e.g., if an LM is not able to generate conservative leaning texts given a conservative leaning prompt), however, our direct bias metric will be positive.\nUnlike indirect bias, which solely relies on the LM itself to establish connections between attributes and political ideology, directly-biased prompts explicitly guide generation in a specified direction, allowing us to examine the sensitiveness of LMs to political bias directly.", "publication_ref": ["b20", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "Debias through Reinforced Calibration", "text": "Different from existing methods that add fairness loss and retrain an unbiased LM from scratch (Huang et al. 2019), we keep the main architecture of GPT-2 unchanged but calibrate the bias during the generation. As shown in Figure 1, we add a debias stage (either using word embeddings or a classifier) between the softmax and argmax function, calibrating the vanilla generation in several iterations of reinforced optimization to produce unbiased tokens. In the framework of reinforcement learning, we define the state at step t as all the generated sequences before t (i.e., s t = x 1:t ), and the action at step t as the t-th output token (i.e., a t = x t ). We take the softmax output of the last hidden states as the policy \u03c0 \u03b8 , because it can be viewed as the probability we choose token x t (action a t ) given the state s t = x 1:t (Dai et al. 2019a;Dathathri et al. 2019). We also prepare 1) a pre-defined political biased words set w L (as for liberal) and w C (as for conservative) which are extracted from the Media Cloud dataset using TF-IDF, and 2) a pretrained GPT-2 based classifier f debias to provide guidance for debias, which differs the bias judgement classifier f judge previously defined. They will be used in MODE 1: Word Embedding Debias and MODE 2: Classifier Guided Debias respectively.", "publication_ref": ["b19", "b10", "b13"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Debias Reward", "text": "Inspired by the objective function used in PPO (Proximal Policy Optimal) algorithm (Schulman et al. 2017), we define the single-step debias reward as follows:\nR(x d t ) = E t \u03c0 \u03b8 d (a t |s t ) \u03c0 \u03b8 (a t |s t ) D [1,2] (x d t ) ,(4)\nwhere\nD [1,2] (x d t )\nis the debias gain that comes from either MODE 1 ( \u00a74.3) or MODE 2 ( \u00a74.4), which serves as a guide signal for the debias generation. As part of the off-policy tricks (Munos et al. 2016), we take the ratio of debias policy \u03c0 \u03b8 d and the vanilla policy \u03c0 \u03b8 as a coefficient, so that the reward is based on the trajectory (i.e., (s t , a t ) pairs) produced by the vanilla policy instead of the debiased one which is part of our optimization goal.", "publication_ref": ["b37", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "MODE 1: Word Embedding Debias", "text": "One of the proven methodologies used in the unbiased word embedding literature is to force the neutral words have equal distance to groups of sensitive words (e.g., male and female) in the embedding space (Zhao et al. 2018b;Park, Shin, and Fung 2018;Bolukbasi et al. 2016). Instead of using it as a goal to train unbiased LMs, we take it as the rule to pick the unbiased token at each step generation. Specifically, given the liberal and conservative words list w L and w C , the debias gain D [1] (x d t ) of token x d t is:\nD [1] (x d t ) = w\u2208w L dist(x d t , w) 2 2 + w\u2208w C dist(x d t , w) 2 2 \u2212 w\u2208w L dist(x d t , w) \u2212 w\u2208w C dist(x d t , w) 1 ,(5)\nwhere dist(x d t , w) measures the distance between the generated debiased token x d t and biased words from both groups. The distance in embedding space is estimated by the negative inner product of the t-th step hidden states h \u03b8 d 1:t (accumulated till t) and the embedded vector of w by the LM embedding layers:\ndist(x d t , w) = \u2212 log(softmax(h \u03b8 d 1:t ) \u2022 emb(w)).(6)\nIn general the L 2 terms in Equation 5will push the picked token far away from the bias words, and the negative L 1 term will penalize picking the word whose distance to two groups are not equal. At each step we maximize such gain to shift the current step hidden states h \u03b8 d 1:t towards the unbiased direction.", "publication_ref": ["b50", "b31", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "MODE 2: Classifier Guided Debias", "text": "Word embedding debias could be problematic if the bias is not purely word level (Bordia and Bowman 2019). Also, poor quality pre-defined bias words could affect the debias performance remarkably (Huang et al. 2019). Thus we present a more advanced mode that leverages the political bias classifier to guide the debias generation.\nFor a given span of generated text\nx d 1:t = [x d 1 , x d 2 , ... x d t ]\n, the total debias gain can be computed as a summation of weighted gain collected at each step generation:\nD [2] (x d 1:t ) = 1 t t i=1 \u03b3 t\u2212i r(x d i ) \u2248 1 \u03c4 + 1 t i=t\u2212\u03c4 \u03b3 t\u2212i r(x d i ),(7)\nwhere \u03b3 \u2208 (0, 1) is the discounting factor which assigns historical tokens less weights. To reduce the computational complexity during generation, we set a window size \u03c4 to limit the back-tracking history length, and use the generation during the period [t \u2212 \u03c4, t] to estimate the whole current sequence. The gain at i-th step is:\nr(x d i ) = \u2212 [y log Pr(y = 1|h d 1:i ) + (1 \u2212 y) log Pr(y = 0|h d 1:i )],(8)\nwhich is similar to cross-entropy loss but here we try to maximize it to penalize the generation resulting in one of the extremes, while to encourage neutral selection (i.e., Pr(y = 1) = Pr(y = 0) \u2192 0.5). The probability output of the bias classifier f debias (h d 1:t ) is within [0, 1] for either class, and y = {0, 1} depending on whether the probability is above threshold 0.5. As in MODE 1, we use the accumulated hidden states till t as a reasonable estimate of current step generation.", "publication_ref": ["b9", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Reinforced Calibration", "text": "Besides the debias reward, we also consider the Kullback-Leibler (KL) divergence between the vanilla distribution of \u03b8 and the debiased \u03b8 d as an auxiliary constraint in case the debias policy drifts too far away from the vanilla policy causing low readability. The procedure of our debias calibration is shown in Algorithm 1. \n\u03b8 d \u2190 argmax \u03b8 \u03bb t R(x d t )(\u03b8) \u2212 KL(\u03b8||\u03b8 d ) (9)\nby taking K steps of SGD (via Adam); if KL(\u03b8||\u03b8 d ) \u2265 2\u03c3 then \u03bb t+1 = \u03bb t / 2; else if KL(\u03b8||\u03b8 d ) \u2264 \u03c3/2 then \u03bb t+1 = 2\u03bb t ; end Return the debiased policy \u03c0 \u03b8 d ; end\nWe set the balance parameter \u03bb t and target divergence \u03c3 to adaptively balance the strength of debias (debias reward) and semantic coherence (KL constraint) based on the current step KL divergence. The debias algorithm is called \"calibration\" because it is not generating unbiased text from scratch but rather performing debias on the hidden states (with param \u03b8) of vanilla generation. The algorithm will produce a debiased policy \u03c0 \u03b8 d with which we can generate text conforming to political neutrality.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "In order to implement our framework, we train a generative LM, a political bias judgement classifier (f judge ), and a bias classifier for MODE 2 of our debiasing framework (f debias ). Media Cloud Dataset. We collect a large-scale political ideology dataset containing N\u2248260k (full) news articles from 10 liberal and conservative media outlets 1 through Media Cloud API. 2 The ideology of the news outlets is retrieved from a survey of news consumption by the Pew Research Center. 3 We removed all punctuation except ,.?! and the press names in the articles to avoid label leaking (e.g., \"(CNN) -\"). We only considered the first 100 tokens in each article and cut off the rest, since 100 was also the max sequence length for GPT-2 generation. We used a distribution-balanced version from our prior work ) (N\u2248120k, balanced) for better classifier performance and further split the data into training, validation, and test sets by the ratio {70%, 15%, 15%}, maintaining the original class distributions.\nModels. We chose the off-the-shelf GPT-2 medium (trained on a corpus of size 40GB, with 355M parameters) as the generative LM for our study. For f judge , we fine-tuned XLNet (2019) (using the default parameters) on the Media Cloud dataset achieving an F 1 of 0.984. We also tested GRN + attention (2016), FastText (2017), Transformer Network (2017), and BERT (2019, but none of them outperformed the fine-tuned XLNet.\nFor f debias , we trained a classifier using the Media Cloud dataset with the encoder of GPT-2 medium plus dense ([1024, 1024]) + activation (tanh) + dense ([1024, 2]) layers. Since we used GPT-2 as the generative LM, we chose the GPT-2 encoder for f debias as gradient consistency.\nParameters & Settings. We used the default GPT-2 settings. For each keyword a belonging to a certain option o, we generate 10 samples with length of 100 tokens on M =10 prompts. Thus, for a given option, we generate |a| \u2022 M \u2022 10 samples. (e.g., we picked 17 male names to represent male for the gender attribute, so in total we produce 1,700 sentences as the generation samples for male.) In total we gener-1 CNN, NYT, PBS, NPR, NBC, Fox News, Rush Limbaugh Show, ABC, CBS, and Breitbart News 2 https://mediacloud.org/ 3 https://www.journalism.org/2020/01/24/u-s-mediapolarization-and-the-2020-election-a-nation-divided/ ated 42,048 samples (evenly divided between vanilla, MODE 1 and MODE 2). The full list of attributes, keywords, and the prompts can be found in Appendix A and B.\nOn average, the vanilla generation of 100-token sequences took about 0.8s, debias by MODE 1 generation took about 1.1s and by MODE 2 took about 1.3s on a RTX 2080 GPU. The debias strength parameter \u03bb is set to 0.6 initially by default but we also explored the performance under \u03bb = {0.1, 0.3, 0.5, 0.7, 0.9} (see \u00a76.2). We picked 250 bias words for either ideology in MODE 1 and set the backtracking window size to 5 in MODE 2. There were 15 iterations of SGD calibration in both modes. The KL-divergence threshold \u03c3 is set to 0.02 and 0.05 for the two modes respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "In this section, we evaluate our proposed method in terms of mitigating political bias ( \u00a76.1) and retaining fluency ( \u00a76.2). Moreover, we also use manual human judgement to evaluate models in terms of bias, readability, and coherence ( \u00a76.4).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Mitigating Political Bias", "text": "We evaluate the generated texts from three models: vanilla GPT-2 (baseline), word embedding debiased GPT-2, and classifier guided debiased GPT-2. As a qualitative evaluation, we take a clustering approach to visualize the bias of sentences generated using indirect prompts. For quantitative evaluation, we compute indirect and direct bias before and after applying debias calibration. UMAP Visualization. We visualize XLNet embeddings of texts generated by three models: our baseline and our two RL-debias methods. For the baseline, we use two modes to embed generated texts: (1) pretrained XLNet without any political ideology fine-tuning (Figure 2(a)), and (2) pretrained XLNet with political ideology fine-tuning (Figure 2(b)). Notably, embeddings of baseline generations separate into noticeable clusters even when visualized using XL-Net without political ideology pretraining, and become even more clear when using an XLNet classifier that is fine-tuned for political ideology classification. Figure 2(c) and 2(d) visualize the embedding space for Modes 1 and 2 of our debias model respectively using the XLNet classifier fine-tuned for Table 2: The performance of our debias methods. Baseline: vanilla generation of GPT-2; Emb.: Word Embedding Debias; Cls.: Classifier Guided Debias. We report the indirect and direct bias before and after we apply debias calibration. The reduction of bias is marked with \u2193 regarding to the bias of baseline. As expected, politically contentious topics such as Immigration have higher bias.\npolitical ideology classification. Qualitatively, it appears that the clusters in (c) and (d) are much less separated, suggesting that sentences generated by our debiased models are less separable by the XLNet political ideology classifier.\nIndirect & Direct Bias Reduction. To quantify the effect of our debiasing method, we compute indirect and direct bias reduction of generated text from our two models compared with the baseline (Table 2). Foremost, we see that for all three attributes, overall, both our proposed methods significantly reduce indirect and direct bias, and the classifier guided debias generally outperforms the word embedding debias. It is interesting to see that in options Healthcare and Immigration, and in option Female, word embedding debias receives even lower direct bias score, which can be partially attributed to the last distance balancing term in Equation 5.", "publication_ref": [], "figure_ref": ["fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "Trade-off between Debias and Fluency", "text": "In preliminary experiments, we observed that debiased generations sometimes contain more syntactic errors when using larger debias strength parameter (\u03bb \u2192 1), meaning that the model mitigates the bias aggressively but sacrifices the semantic fluency to some extent. Thus, in this section, we examine the trade-off between the bias reduction and the generation fluency. To measure perplexity, we use kenLM (Heafield 2011) to train three separate LMs on the vanilla generation for our three attributes. Here, we focus on the classifier-guided debias method, which has the better performance of the two rewards we study. As shown in Table 3 we see that, in general, models trained with larger \u03bb generate texts that have higher both indirect and direct bias but also have higher perplexity (less fluency), which confirms our original observation. However, among our three attributes, even with the highest debias strength parameter  Debias strength parameter \u03bb starts from 0 (no debias, vanilla generation) and gradually increases to 0.9 (strongest debias). \u2193 indicates change compared with \u03bb = 0.\nwe study (\u03bb=0.9), the perplexity does not grow drastically, which is potentially the result of adaptive control of KL constraint from Algorithm 1.  be the only one that does not access to the original training data and there has potential to be adapted to LM generation debias. We add a simple retrieving stage upon the trained IN-GloVe model (Ideology-Neutral Glove, not original Gender-Neutral): every time the generation encounters the pre-defined biased words, replace them with one of the top-10 most similar word retrieved from the IN-GloVe. In this way we approximate using prior word embedding debias method in current generative LMs. We also prepare a Naive method, which just randomly replaces pre-defined bias words with the most similar word in terms of off-theshelf Word2Vec (Mikolov et al. 2013). Their performances compared with two proposed methods are shown in Table 5. Naive method marginally reduces the bias, while IN-GloVe performs similar to Naive method, suggesting that word-level rather than contextual method cannot truly debias. Compared with prior methods, which simply replace words in already generated text, our proposed method generates completely new unbiased text, which likely explains the increased perplexity.", "publication_ref": ["b17", "b27"], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "Comparison with Related Work", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Human Judgement", "text": "As further evaluation, we recruited N =170 MTurk participants to manually examine generated texts for 1) Debias (i.e., \"How biased is text you read?\" Answer is from 1extremely unbiased to 7-extremely biased); 2) Readability (i.e., \"How well-written is the text?\" Answer is from 1-not readable at all to 7-very readable); and 3) Coherence (i.e., \"Is the generated text coherent with the writing prompt?\" Table 6: Human evaluation results on bias reduction, readability, and coherence to the given prompts. All results are compared with the participants' perceptions of baseline. p value describes the significance of difference. (* corresponds to p < 0.05, ** to p < 0.01 and *** to p < 0.001.)\nAnswer is from 1-strongly disagree to 7-strongly agree). Each participant was randomly assigned eight paragraphs generated by four methods (Baseline, IN-GloVe, Emb., and Cls.). The participants were informed that the generations were continuations of the underlined prompts, but they did not know the exact method used to generate the paragraph. We used paired samples t-tests to examine the difference between baseline and other methods in terms of coherence, perceived bias, and readability. As Table 6 shows, our wordembedding debias method was the least biased (M=4.25), and the classifier-guided debias method had the best readability (M=4.93) and highest coherence score (M=4.55). IN-GloVe mitigated bias not as much as our methods and its readability was significantly worse than Baseline (M=3.81 vs. M=4.33, t=6.67, p < .001***). No significant difference existed for coherence among all four methods.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Although the bias metrics we study capture the purported phenomenon relatively well, they certainly have limitations. For instance, the indirect bias metric measures the extent of bias from texts generated by one option, but it does not tell us the directionality of the bias. Moreover, as we study political bias in this paper, our metrics focus on only binary classes (liberal and conservative) and would require nontrivial modification in order to be extended into types of bias that are non-binary (e.g., emotional bias, normally categorized by nine directions (Huang et al. 2018)).", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this work, we have discussed two metrics for measuring political bias in language model generation and presented a framework to mitigate such bias that requires neither extra data nor retraining. As more potentially-biased LMs are adopted in AI applications, it is a growing concern that the political bias will be amplified if fairness is not taken into considering. Our method is especially meaningful in such contexts, since the training data of LMs are normally not publicly available and training a new large-scale LM from scratch is costly. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We sincerely thank the reviewers for their insightful comments and suggestions that helped improve the paper. This research was supported in part by the Dartmouth Burke Research Initiation Award and the Amazon Research Award.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix A: Sensitive Attributes", "text": "In this paper, we consider the political bias of three sensitive attributes, gender, location and topic, which are detailed below.\nGender. We use male and female names used by Huang et al. (2019) ", "publication_ref": ["b19"], "figure_ref": [], "table_ref": []}, {"heading": "Appendix B: Writing Prompts", "text": "We consider ten writing prompts for each attribute we study. Tables 7, Table 8, and Table 9 show the prompts for gender, location, and topic respectively. As can be seen in the tables, we use different prompts for indirect bias (Ind.B.) and direct bias (D.B.). For direct bias, we further separated the prompts into liberal and conservative leaning versions.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "About voting he/she will 6", "journal": "", "year": "", "authors": ""}, {"ref_id": "b1", "title": "1. I am from [L]. I think 2. My best friend is from", "journal": "", "year": "", "authors": " B Ind"}, {"ref_id": "b2", "title": "He/She 3. It is reported that during election season [L] 4. About voting", "journal": "", "year": "", "authors": ""}, {"ref_id": "b3", "title": "", "journal": "", "year": "", "authors": "D B "}, {"ref_id": "b4", "title": "People from this state will 6", "journal": "", "year": "", "authors": ""}, {"ref_id": "b5", "title": "People from this state will 9", "journal": "", "year": "", "authors": ""}, {"ref_id": "b6", "title": "Writing prompts for the location attribute", "journal": "", "year": "", "authors": ""}, {"ref_id": "b7", "title": "], a national poll says democrat voters D.B.(C) 8. I am a republican. Talking about [T] issue I 9. I will vote for Trump, since in [T] topic he 10. About [T], a national poll says republican voters References Bojanowski", "journal": "", "year": "2017", "authors": "P Grave; \u00c9 Joulin; A Mikolov; T "}, {"ref_id": "b8", "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings", "journal": "", "year": "2016", "authors": "T Bolukbasi; K.-W Chang; J Y Zou; V Saligrama; A T Kalai"}, {"ref_id": "b9", "title": "Identifying and Reducing Gender Bias in Word-Level Language Models", "journal": "", "year": "2019", "authors": "S Bordia; S Bowman"}, {"ref_id": "b10", "title": "Style Transformer: Unpaired Text Style Transfer without Disentangled Latent Representation", "journal": "", "year": "2019", "authors": "N Dai; J Liang; X Qiu; X.-J Huang"}, {"ref_id": "b11", "title": "Annual Meeting of the Association for Computational Linguistics (ACL 19')", "journal": "", "year": "", "authors": ""}, {"ref_id": "b12", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context", "journal": "", "year": "2019", "authors": "Z Dai; Z Yang; Y Yang; J G Carbonell; Q Le; R Salakhutdinov"}, {"ref_id": "b13", "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "journal": "", "year": "2019", "authors": "S Dathathri; A Madotto; J Lan; J Hung; E Frank; P Molino; J Yosinski; R Liu"}, {"ref_id": "b14", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "journal": "", "year": "2019", "authors": "J Devlin; M.-W Chang; K Lee; K Toutanova"}, {"ref_id": "b15", "title": "Empirical risk minimization under fairness constraints", "journal": "", "year": "2018", "authors": "M Donini; L Oneto; S Ben-David; J S Shawe-Taylor; M Pontil"}, {"ref_id": "b16", "title": "Word embeddings quantify 100 years of gender and ethnic stereotypes", "journal": "Proceedings of the National Academy of Sciences", "year": "2018", "authors": "N Garg; L Schiebinger; D Jurafsky; J Zou"}, {"ref_id": "b17", "title": "KenLM: Faster and smaller language model queries", "journal": "", "year": "2011", "authors": "K Heafield"}, {"ref_id": "b18", "title": "Automatic Dialogue Generation with Expressed Emotions", "journal": "", "year": "2018", "authors": "C Huang; O Za\u00efane; A Trabelsi; N Dziri"}, {"ref_id": "b19", "title": "Reducing sentiment bias in language models via counterfactual evaluation", "journal": "", "year": "2019", "authors": "P.-S Huang; H Zhang; R Jiang; R Stanforth; J Welbl; J Rae; V Maini; D Yogatama; P Kohli"}, {"ref_id": "b20", "title": "Wasserstein fair classification", "journal": "", "year": "2019", "authors": "R Jiang; A Pacchiano; T Stepleton; H Jiang; S Chiappa"}, {"ref_id": "b21", "title": "Fairness-aware classifier with prejudice remover regularizer", "journal": "", "year": "2012", "authors": "T Kamishima; S Akaho; H Asoh; J Sakuma"}, {"ref_id": "b22", "title": "Counterfactual fairness", "journal": "", "year": "2017", "authors": "M J Kusner; J Loftus; C Russell; R Silva"}, {"ref_id": "b23", "title": "A Transformer-based Framework for Neutralizing and Reversing the Political Polarity of News Articles", "journal": "Proceedings of the ACM on Human-Computer Interaction", "year": "2021", "authors": "R Liu; C Jia; S Vosoughi"}, {"ref_id": "b24", "title": "Political Depolarization of News Articles Using Attribute-Aware Word Embeddings", "journal": "", "year": "2021", "authors": "R Liu; L Wang; C Jia; S Vosoughi"}, {"ref_id": "b25", "title": "Data Boost: Text Data Augmentation through Reinforcement Learning Guided Conditional Generation", "journal": "", "year": "2020", "authors": "R Liu; G Xu; C Jia; W Ma; L Wang; S Vosoughi"}, {"ref_id": "b26", "title": "It's All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution", "journal": "", "year": "2019", "authors": "R H Maudslay; H Gonen; R Cotterell; S Teufel"}, {"ref_id": "b27", "title": "Distributed representations of words and phrases and their compositionality", "journal": "", "year": "2013", "authors": "T Mikolov; I Sutskever; K Chen; G S Corrado; J Dean"}, {"ref_id": "b28", "title": "Seeing through the human reporting bias: Visual classifiers from noisy human-centric labels", "journal": "", "year": "2016", "authors": "I Misra; C Lawrence Zitnick; M Mitchell; R Girshick"}, {"ref_id": "b29", "title": "Model cards for model reporting", "journal": "", "year": "2019", "authors": "M Mitchell; S Wu; A Zaldivar; P Barnes; L Vasserman; B Hutchinson; E Spitzer; I D Raji; T Gebru"}, {"ref_id": "b30", "title": "Safe and efficient off-policy reinforcement learning", "journal": "", "year": "2016", "authors": "R Munos; T Stepleton; A Harutyunyan; M Bellemare"}, {"ref_id": "b31", "title": "Reducing Gender Bias in Abusive Language Detection", "journal": "", "year": "2018", "authors": "J H Park; J Shin; P Fung"}, {"ref_id": "b32", "title": "Few-shot Natural Language Generation for Task-Oriented Dialog", "journal": "", "year": "2002", "authors": "B Peng; C Zhu; C Li; X Li; J Chao Li; M Zeng; J Gao"}, {"ref_id": "b33", "title": "Glove: Global vectors for word representation", "journal": "", "year": "2014", "authors": "J Pennington; R Socher; C D Manning"}, {"ref_id": "b34", "title": "Wasserstein barycenter and its application to texture mixing", "journal": "", "year": "2011", "authors": "J Rabin; G Peyr\u00e9; J Delon; M Bernot"}, {"ref_id": "b35", "title": "Language models are unsupervised multitask learners", "journal": "OpenAI Blog", "year": "2019", "authors": "A Radford; J Wu; R Child; D Luan; D Amodei; I Sutskever"}, {"ref_id": "b36", "title": "Visualizing and Measuring the Geometry of BERT", "journal": "", "year": "2019", "authors": "E Reif; A Yuan; M Wattenberg; F B Viegas; A Coenen; A Pearce; B Kim"}, {"ref_id": "b37", "title": "Proximal policy optimization algorithms", "journal": "", "year": "2017", "authors": "J Schulman; F Wolski; P Dhariwal; A Radford; O Klimov"}, {"ref_id": "b38", "title": "The Woman Worked as a Babysitter: On Biases in Language Generation", "journal": "", "year": "2019", "authors": "E Sheng; K.-W Chang; P Natarajan; N Peng"}, {"ref_id": "b39", "title": "Towards Controllable Biases in Language Generation", "journal": "", "year": "2020", "authors": "E Sheng; K.-W Chang; P Natarajan; N Peng"}, {"ref_id": "b40", "title": "Evaluating Gender Bias in Machine Translation", "journal": "", "year": "2019", "authors": "G Stanovsky; N A Smith; L Zettlemoyer"}, {"ref_id": "b41", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "A Vaswani; N Shazeer; N Parmar; J Uszkoreit; L Jones; A N Gomez; \u0141 Kaiser; I Polosukhin"}, {"ref_id": "b42", "title": "Universal Adversarial Triggers for Attacking and Analyzing NLP", "journal": "", "year": "2019", "authors": "E Wallace; S Feng; N Kandpal; M Gardner; S Singh"}, {"ref_id": "b43", "title": "Towards Making the Most of BERT in Neural Machine Translation", "journal": "", "year": "2020", "authors": "J Yang; M Wang; H Zhou; C Zhao; Y Yu; W Zhang; L Li"}, {"ref_id": "b44", "title": "Xlnet: Generalized autoregressive pretraining for language understanding", "journal": "", "year": "2019", "authors": "Z Yang; Z Dai; Y Yang; J Carbonell; R R Salakhutdinov; Q V Le"}, {"ref_id": "b45", "title": "Mitigating unwanted biases with adversarial learning", "journal": "", "year": "2018", "authors": "B H Zhang; B Lemoine; M Mitchell"}, {"ref_id": "b46", "title": "DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation", "journal": "", "year": "2020", "authors": "Y Zhang; S Sun; M Galley; Y.-C Chen; C Brockett; X Gao; J Gao; J Liu; W Dolan"}, {"ref_id": "b47", "title": "Conditional Learning of Fair Representations", "journal": "", "year": "2019", "authors": "H Zhao; A Coston; T Adel; G J Gordon"}, {"ref_id": "b48", "title": "Inherent tradeoffs in learning fair representations", "journal": "", "year": "2019", "authors": "H Zhao; G Gordon"}, {"ref_id": "b49", "title": "Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods", "journal": "", "year": "2018", "authors": "J Zhao; T Wang; M Yatskar; V Ordonez; K.-W Chang"}, {"ref_id": "b50", "title": "Learning Gender-Neutral Word Embeddings", "journal": "", "year": "2018", "authors": "J Zhao; Y Zhou; Z Li; W Wang; K.-W Chang"}, {"ref_id": "b51", "title": "Attention-based bidirectional long short-term memory networks for relation classification", "journal": "", "year": "2016", "authors": "P Zhou; W Shi; J Tian; Z Qi; B Li; H Hao; B Xu"}, {"ref_id": "b52", "title": "Incorporating BERT into Neural Machine Translation", "journal": "", "year": "2020", "authors": "J Zhu; Y Xia; L Wu; D He; T Qin; W Zhou; H Li; T Liu"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Two modes of our RL-guided debias method.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: (a) and (b): The UMAP 2D visualization of 5,606 sentences generated by vanilla GPT-2 when the sentence embeddings are encoding output of (a) not pretrained XLNet, (b) pretrained XLNet on Media Cloud Dataset (F 1 =0.98). (c) and (d) are visualization of debiased sentences by MODE 1 and MODE 2. The embeddings of (c) (d) are both from pretrained XLNet. We mark the class of each sentence (L / C ) labeled by the pretrained XLNet classifier.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "talk about [TOPIC] marijuana, because + I believe it should be legal and not regulated. Virginia will + vote. But what if the Republican-controlled legislature ban voters who don't have an absentee ballot? Willie and his/her fellow democrats + were planning a massive gathering of anti-Trump, pro-Hillary Clinton supporters.", "figure_data": "-2 Generationhealthcare (Topic) I want to leaning blue Indirect (Location) Indirect About voting, people from [LOCATION] male (Gender) Direct (L) The news reported that [GENDER] female (Gender) Direct (C)"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Reinforced Political DebiasInput: Bias words lists w L and w C , pretrained bias classifier f debias , KL-divergence threshold \u03c3. for t = 1, 2, . . . do Generate (a t |s t ) by vanilla policy \u03c0", "figure_data": "\u03b8 astrajectories;if MODE 1 thenCompute D(x d t ) as in MODE 1 (Eq. 5);else if MODE 2 thenCompute D(x d t ) as in MODE 2 (Eq. 7);endEstimate reward R(x d t ) with D(x d t );Compute policy update"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Trade-off between bias reduction and perplexity (PPL). Ind.B.: Indirect Bias; D.B.: Direct Bias.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Bias: the bias type. We also mark the number of studied attributes next to the method.", "figure_data": "presents an overview of six debias methods andtheir requirements. GN-GloVe (Zhao et al. 2018b) seems to"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "", "figure_data": ": Averaged indirect bias, direct bias and perplexityof Naive (randomly Word2Vec synonym replacement), IN-GloVe (Ideology-Neutral GloVe, modified GN-GloVe witha retrieving add-on) and our two proposed debias methodsover the three studied attributes. PPL: perplexity."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "GloVe 4.38 .00*** 3.81 .00*** 4.20 .29 Ours: Emb. 4.15 .00*** 4.46 .20 4.46 .41 Ours: Cls. 4.25 .00*** 4.93 .00*** 4.55 .12", "figure_data": "DebiasReadability CoherenceMeanpMeanpMean pBaseline4.72-4.33-4.35-IN-"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Writing prompts for the topic attribute. [T] are topic keywords such as immigration ban, or marijuana.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "x t \u223c argmax xt Pr(x t ) = LM(x 1:t\u22121 |\u03c8) .", "formula_coordinates": [2.0, 359.33, 588.09, 158.83, 16.6]}, {"formula_id": "formula_1", "formula_text": "B indirect (o, A) := \u2206 BR (X o , X \u2200o\u2208A ) ,(2)", "formula_coordinates": [3.0, 97.65, 329.01, 194.85, 11.88]}, {"formula_id": "formula_2", "formula_text": "B direct := |B L indirect (o, A) \u2212 B C indirect (o, A)| .", "formula_coordinates": [3.0, 87.94, 536.88, 170.63, 12.85]}, {"formula_id": "formula_3", "formula_text": "R(x d t ) = E t \u03c0 \u03b8 d (a t |s t ) \u03c0 \u03b8 (a t |s t ) D [1,2] (x d t ) ,(4)", "formula_coordinates": [3.0, 363.17, 588.95, 194.83, 23.23]}, {"formula_id": "formula_4", "formula_text": "D [1,2] (x d t )", "formula_coordinates": [3.0, 346.99, 616.91, 41.93, 12.19]}, {"formula_id": "formula_5", "formula_text": "D [1] (x d t ) = w\u2208w L dist(x d t , w) 2 2 + w\u2208w C dist(x d t , w) 2 2 \u2212 w\u2208w L dist(x d t , w) \u2212 w\u2208w C dist(x d t , w) 1 ,(5)", "formula_coordinates": [4.0, 56.11, 180.97, 236.39, 80.2]}, {"formula_id": "formula_6", "formula_text": "dist(x d t , w) = \u2212 log(softmax(h \u03b8 d 1:t ) \u2022 emb(w)).(6)", "formula_coordinates": [4.0, 80.64, 343.21, 211.86, 13.26]}, {"formula_id": "formula_7", "formula_text": "x d 1:t = [x d 1 , x d 2 , ... x d t ]", "formula_coordinates": [4.0, 203.58, 515.19, 86.29, 12.2]}, {"formula_id": "formula_8", "formula_text": "D [2] (x d 1:t ) = 1 t t i=1 \u03b3 t\u2212i r(x d i ) \u2248 1 \u03c4 + 1 t i=t\u2212\u03c4 \u03b3 t\u2212i r(x d i ),(7)", "formula_coordinates": [4.0, 59.07, 562.21, 233.43, 40.74]}, {"formula_id": "formula_9", "formula_text": "r(x d i ) = \u2212 [y log Pr(y = 1|h d 1:i ) + (1 \u2212 y) log Pr(y = 0|h d 1:i )],(8)", "formula_coordinates": [4.0, 100.02, 678.59, 192.48, 28.92]}, {"formula_id": "formula_10", "formula_text": "\u03b8 d \u2190 argmax \u03b8 \u03bb t R(x d t )(\u03b8) \u2212 KL(\u03b8||\u03b8 d ) (9)", "formula_coordinates": [4.0, 360.79, 415.6, 182.27, 18.67]}], "doi": "10.18653/v1/N18-2008"}