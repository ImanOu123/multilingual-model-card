{"title": "Effective Bilingual Constraints for Semi-supervised Learning of Named Entity Recognizers", "authors": "Mengqiu Wang; Wanxiang Che; Christopher D Manning", "pub_date": "", "abstract": "Most semi-supervised methods in Natural Language Processing capitalize on unannotated resources in a single language; however, information can be gained from using parallel resources in more than one language, since translations of the same utterance in different languages can help to disambiguate each other. We demonstrate a method that makes effective use of vast amounts of bilingual text (a.k.a. bitext) to improve monolingual systems. We propose a factored probabilistic sequence model that encourages both crosslanguage and intra-document consistency. A simple Gibbs sampling algorithm is introduced for performing approximate inference. Experiments on English-Chinese Named Entity Recognition (NER) using the OntoNotes dataset demonstrate that our method is significantly more accurate than state-ofthe-art monolingual CRF models in a bilingual test setting. Our model also improves on previous work by Burkett et al. (2010), achieving a relative error reduction of 10.8% and 4.5% in Chinese and English, respectively. Furthermore, by annotating a moderate amount of unlabeled bi-text with our bilingual model, and using the tagged data for uptraining, we achieve a 9.2% error reduction in Chinese over the state-ofthe-art Stanford monolingual NER system.", "sections": [{"heading": "Introduction", "text": "Supervised learning algorithms have been met with great success in many areas of Natural Language Processing (NLP). It is well-known that the performance of supervised learners increases when more labeled training examples become available. In most application scenarios, however, manually labeled data are extremely limited in quantity and costly to produce. On the other hand, we live in an age of abundance of unannotated data -as regards NLP, there has been an explosion in the amount of freely available web and news texts. One would expect to greatly increase the coverage of a system if such large amounts of additional data can be incorporated in a judicious manner.\nA number of semi-supervised techniques have been introduced to tackle this problem, such as bootstrapping (Yarowsky 1995;Collins and Singer 1999;Riloff and Jones 1999), multi-view learning (Blum and Mitchell 1998;Ganchev et al. 2008) and structural learning (Ando and Zhang 2005). Most previous semi-supervised work is situated in a monolingual setting where all unannotated data are available only in a single language.\nHowever, in recent years, a vast amount of translated parallel texts have been generated in our increasingly connected multilingual world. While such bi-texts have primarily been leveraged to train statistical machine translation (SMT) systems, contemporary research has increasingly considered the possibilities of utilizing parallel corpora to improve systems outside of SMT. For example, Yarowsky and Ngai (2001) projects the part-of-speech labels assigned by a supervised model in one language (e.g. English) onto word-aligned parallel text in another language (e.g. Chinese) where less manually annotated data is available. Similar ideas were also employed by Das and Petrov (2011) and Fu, Qin, and Liu (2011).\nA severe limitation of methods employing bilingual projection is that they can only be applied to test scenarios where parallel sentence pairs are available. It is more desirable to improve monolingual system performance, which is more broadly applicable. Previous work such as Li et al. (2012) and Kim, Toutanova, and Yu (2012) successfully demonstrated that manually-labeled bilingual corpora can be used to improve monolingual system performance. This approach, however, encounters the difficulty that manually annotated bilingual corpora are even harder to come by than monolingual ones.\nIn this work, we consider a semi-supervised learning scheme using unannotated bi-text. For a given language pair (e.g., English-Chinese), we expect one language (e.g. English) to have more annotated training resources than the other (e.g. Chinese), and thus there exists a strong monolingual model (for English) and a weaker model (for Chinese). Since bi-text contains translations across the two languages, an aligned sentence pair would exhibit some semantic and syntactic similarities. Thus we can constrain the two models to agree with each other by making joint predictions that are skewed towards the more informed model. In general, errors made in the lower-resource model will be corrected by the higher-resource model, but we also anticipate that these joint predictions will have higher quality for both languages than the output of a monolingual model alone. We can then apply this bilingual annotation method to a large amount of  Burkett et al. (2010) proposed a similar framework with a \"multi-view\" learning scheme where k-best outputs of two monolingual taggers are reranked using a complex selftrained reranking model. In our work, we propose a simple decoding method based on Gibbs sampling that eliminates the need for training complex reranking models. In particular, we construct a new factored probabilistic model by chaining together two Conditional Random Field monolingual models with a bilingual constraint model, which encourages soft label agreements. We then apply Gibbs sampling to find the best labels under the new factored model. We can further improve the quality of bilingual prediction by incorporating an additional model, expanding upon Finkel, Grenager, and Manning (2005), that enforces global label consistency for each language.\nExperiments on Named Entity Recognition (NER) show that our bilingual method yields significant improvements over the state-of-the-art Stanford NER system. When evaluated over the standard OntoNotes English-Chinese dataset in a bilingual setting, our models achieve a F 1 error reduction of 18.6% in Chinese and 9.9% in English. Our method also improves over Burkett et al. (2010) with a relative error reduction of 10.8% and 4.5% in Chinese and English, respectively. Furthermore, we automatically label a moderate-sized set of 80k sentence pairs using our bilingual model, and train new monolingual models using an uptraining scheme. The resulting monolingual models demonstrate an error reduction of 9.2% over the Stanford NER systems for Chinese. 2", "publication_ref": ["b22", "b4", "b18", "b1", "b9", "b0", "b21", "b5", "b8", "b15", "b12", "b3", "b7", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Monolingual NER with CRF", "text": "Named Entity Recognition is an important task in NLP. It serves as a first step in turning unstructured text into structured data, and has broad applications in news aggregation, question answering, and bioNLP. Given an input sentence, an NER tagger identifies words that are part of a named entity, and assigns the entity type and relative position information. For example, in the commonly used BIO tagging scheme, a tag such as B-PERSON indicates the word is the beginning of a person name entity; and a I-LOCATION tag software/CRF-NER.shtml.\nmarks the word to be inside a location entity. All words marked with tag O are not part of any entity. Figure 1 illustrates a tagged sentence pair in English and Chinese.\nCurrent state-of-the-art supervised NER systems employ an undirected graphical model called Conditional Random Field (CRF) (Lafferty, McCallum, and Pereira 2001). Given an input sentence x, a linear-chain structured CRF defines the following conditional probability for tag sequence y:\nP mono (y|x) = 1 Z(x) i exp( j \u03bb j f j (y i , y i\u22121 |x)) (1)\nwhere f j is the jth feature function, \u03bb j is the feature weight, and Z(x) is the partition function.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "Bilingual NER Constraints", "text": "A pair of aligned sentences in two languages contain complementary cues to aid the analysis of each other. For example, in Figure 1, it is not immediately obvious whether the phrase \"Foreign Affairs\" on the English side refers to an organization (Ministry of Foreign Affairs), or general foreign affairs. But the aligned word on the Chinese side is a lot less ambiguous, and can be easily identified as an organization entity.\nAnother example is that in the Chinese training data we have never seen the translation of the name \"Kamyao\". As a result, the tagger cannot make use of lexical features, and so has to rely on less informative contextual features to predict if it is a geo-political entity (GPE) or a person. But we have seen the aligned word on the English side being tagged as person, and thus can infer that the Chinese aligned entity should also be a person.\nIt is straight-forward to see that accurate word alignment is essential in such an analysis. Fortunately, there are automatic word alignment systems used in MT research that produce robust and accurate alignment results, and our method will use the output of one (Liang, Taskar, and Klein 2006).", "publication_ref": ["b16"], "figure_ref": [], "table_ref": []}, {"heading": "Hard Agreement Constraints", "text": "Drawing on the above observations, we first propose a simple bilingual constraint model that enforces hard agreements.\nWe define the following probability for an output sequence pair y c and y e for Chinese and English input sentences x c and x e , respectively:\nP bi (y c , y e ) = A={a c ,a e } I(y a c , y a e )(2)\n. where A is the set of all aligned word pairs, and I(y a c , y a e ) is an indicator function that equals 1 if y a c = y a e , and 0 otherwise.\n. . the O earliest O established O bonded O area O . . . \u6700\u65e9 O \u521b\u529e O \u7684 O \u4fdd\u7a0e\u533a B-LOC", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Soft Agreement Constraints", "text": "If we apply hard agreement constraints, any output sequence pairs that disagree on any tag pair will be assigned zero probability. Such a hard constraint is not always satisfied in practice, since annotation standards in different languages can differ. An example is given in Figure 2, where the phrase mention of \"bonded area\" is considered a location in the Chinese gold-standard, but not in the English gold-standard.\nWe can soften these constraints by replacing the 1 and 0 values in indicator function I(y a c , y a e ) with a probability measure. We first tag a set of unannotated bilingual sentence pairs using two baseline monolingual CRF taggers. Then we collect counts of aligned entity tag pairs from the autogenerated tagged data. The value I(y a c , y a e ) is chosen to be the pairwise mutual information score of the entity pair (y a c , y a e ). This version of constraints is denoted as auto.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Alignment Uncertainty", "text": "When we consider the previous two sets of bilingual constraints, we assume the word alignments are given by some off-the-shelf alignment model which outputs a set of \"hard\" alignments. In practice, most statistical word alignment models assign a probability to each alignment pair, and \"hard\" alignments are produced by cutting off alignment pairs that fall below a threshold value.\nTo take into account alignment uncertainties, we modify function I(y a c , y a e ) by exponentiating its value to the power of the alignment probability to give a new function: U(y a c , y a e ) = I(y a c , y a e ) P (y a c ,y a e ) . The intuition behind this modification is that pairs with a higher alignment probability will reflect more probability fluctuation when different label assignments are considered.\nFor example, consider an extreme case where a particular pair of aligned words has alignment probability 0. Then the value of the U function will always be 1 regardless of what tags are assigned to the two words, thus reducing the impact of different choices of tags for this pair in the overall tag sequence assignment.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Gibbs Sampling with Factored Models", "text": "In a monolingual setting, exact inference in a standard linear-chain CRF can be done by applying the Viterbi algorithm to find the most likely output sequence. But when we consider the joint probability of an output sequence pair in a bilingual setting, especially when we apply the aforementioned bilingual constraints, cyclic cliques are introduced into the Markov random field which make exact inference algorithms intractable.\nMarkov Chain Monte Carlo (MCMC) methods offer a simple and elegant solution for approximate inference by constructing a Markov chain whose stationary distribution is the target distribution.\nIn this work, we adopt a specific MCMC sampling method called Gibbs sampling (Geman and Geman 1984). We define a Markov chain over output sequences by observing a simple transition rule: from a current sequence assignment at time t \u2212 1, we can transition into the next sequence at time t by changing the label at any position i. And the distribution over these transitions is defined as:\nP (y t |y t\u22121 ) = P (y t i |y t\u22121 \u2212i , x)(3)\nwhere y t\u22121 \u2212i is the set of all labels except y i at time t \u2212 1. To apply the bilingual constraints during decoding, we formulate a new factored model by combining the two monolingual CRF models (one for each language) with the bilingual constraint model via a simple product. 3 The resulting model is of the following form:\nP (y c , y e |x c , x e ) = P mono (y c |x c )P mono (y e |x e )P bi (y c , y e )(4)\nObtaining the state transition model P (y t i |y t\u22121 \u2212i , x) for the monolingual CRF models is straight-forward. In the case of a first order linear-chain CRF, the Markov blanket is the neighboring two cliques. Given the Markov blanket of state i, the label at position i is independent of all other states. Thus we can compute the transition model simply by normalizing the product of the neighboring clique potentials. Finkel, Grenager, and Manning (2005) gave a more detailed account of how to compute this quantity.\nThe transition probability of label y ci in the bilingual constraint model is defined as (yc i ,ye k )\u2208A U(y ci , y ek ), where y ek is a word aligned to y ci .\nAt decoding time, we walk the Markov chain by taking samples at each step. We start from some random assignment of the label sequence, and at each step we randomly sample a new value for y i at a randomly chosen position i. After a fixed number of steps, we output a complete sequence as our final solution. In practice, MCMC sampling could be quite slow and inefficient, especially when the input sentence is long. To speed up the sampling process, we initialize the state sequence from the best sequences found by Viterbi decoding using only the monolingual models.\nA bigger problem with vanilla Gibbs sampling is that the random samples we draw do not necessarily give us the most likely state sequence, as given by Viterbi in the exact inference case. One way to tackle this problem is to borrow the simulated annealing technique from optimization research (Kirkpatrick, Gelatt, and Vecchi 1983). We redefine the transition probability in Eqn. 3 as:\nP (y t |y t\u22121 ) = P (y t i |y t\u22121 \u2212i , x) 1/ct j P (y t j |y t\u22121 \u2212j , x) 1/ct (5)\nwhere c = {c 0 . . . c T } is the schedule of annealing \"temperature,\" with 0 \u2264 c i \u2264 1. The distribution becomes sharper as the value of c i move towards 0. In our experiments we adopted a linear cooling schedule, where c 0 = 1, and c t+1 = c t \u2212 1/T . This technique has been shown to be effective by Finkel, Grenager, and Manning (2005).", "publication_ref": ["b10", "b7", "b13", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Global Consistency Constraints", "text": "A distinctive feature of the proposed factored model and Gibbs sampling inference is the ability to incorporate nonlocal constraints that are not easily captured in a traditional Markov network model. The bilingual constraint model described earlier is certainly a benefactor of this unique characteristic.\nStill, there are further linguistic constraints that we can apply to improve the NER system. For example, many previous papers have made the observation that occurrences of the same word sequence within a given document are unlikely to take on different entity types (Bunescu and Mooney 2004;Sutton and McCallum 2004;Finkel, Grenager, and Manning 2005; inter alia) . Similar to Finkel, Grenager, and Manning (2005), we devise a global consistency model as follows:\nP glo (y|x) = \u03b3\u2208\u0393 \u03c6 #(\u03b3,y,x) \u03b3 (6)\n\u0393 is the set of all possible entity type violations, \u03c6 \u03b3 is the penalty parameter for violation type \u03b3, and #(\u03b3, y, x) is the count of violations \u03b3 in sequence y. For example, if the word sequence \"China Daily\" has occurred both as GPE and organization exactly once, then the penalty \u03c6 \u03b3 for GPEto-organization violation will apply once. The parameter values of \u03c6 \u03b3 are estimated empirically by counting the occurrences of entity pairs of the same word sequence in the training data.\nWe can now factor in one global consistency model for each language by taking the product of Eqn. 4 with Eqn. 6. The same Gibbs sampling procedure applies unchanged to this new factored model. At test time, instead of tagging one sentence at a time, we group together sentences that belong to the same document, and tag one document at a time.", "publication_ref": ["b2", "b20", "b7", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Enhancing Recall", "text": "A flaw of the Finkel, Grenager, and Manning (2005) model described above is that consistency is enforced by applying penalties to entity type violations. But if a word is not tagged with an entity type, it will not receive any penalty since no entity type violations would occur. Therefore, this model has the tendency of favoring null annotations, which can result in losses in model recall.\nWe fix this deficiency in Finkel, Grenager, and Manning ( 2005) by introducing a new \"reward\" parameter \u03b4, Chinese NER Templates 00: 1 (class bias param) 01:\nw i+k , \u22121 \u2264 k \u2264 1 02: w i+k\u22121 \u2022 w i+k , 0 \u2264 k \u2264 1 03: shape(w i+k ), \u22124 \u2264 k \u2264 4 04: prefix(wi, k), 1 \u2264 k \u2264 4 05: prefix(wi\u22121, k), 1 \u2264 k \u2264 4 06: suffix(wi, k), 1 \u2264 k \u2264 4 07: suffix(wi\u22121, k), 1 \u2264 k \u2264 4 08: radical(wi, k), 1 \u2264 k \u2264 len(wi) 09: distsim(w i+k ), \u22121 \u2264 k \u2264 1 Unigram Features yi\u2022 00 -09 Bigram Features yi\u22121 \u2022 yi\u2022 00 -09\nTable 1: Basic features of Chinese NER. \u2022 means string concatenation and y i is the named entity label of the i th word w i . shape(w i ) is the shape of w i , such as date and number. prefix/suffix(w i , k) denotes the k-characters prefix/suffix of w i . radical(w i , k) denotes the radical of the k th Chinese character of w i . 4 len(w i ) is the number of Chinese characters in w i . distsim(w i , k) denotes the distributional similarity features based on large word clusters. which has value > 0. \u03b4 is activated each time we see a matching pair of entities for the same word occurrence. The new P glo is modified as:\nP glo (y|x) = \u03b4 #(\u03b4,y,x) \u03b3\u2208\u0393 \u03c6 #(\u03b3,y,x) \u03b3 (7)\nwhere #(\u03b4, y, x) is the activation count of \u03b4 in sequence y.\nThis model is in fact a naive Bayes model, where the parameters \u03b4 and \u03c6 are empirically estimated (a value of 2 is used for \u03b4 in our experiments, based on tuning on a development set). A similar global consistency model was shown to be effective in Rush et al. (2012), where parameters were also tuned on a development set.", "publication_ref": ["b19"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "To compare the proposed bilingual constraint decoding algorithm against traditional monolingual methods, we evaluate on a large, manually annotated parallel corpus that contains named entity annotation in both Chinese and English. The corpus we use is the latest version (v4.0) of the OntoNotes corpus (Hovy et al. 2006), which includes 401 pairs of Chinese and English documents (chtb 0001-0325, ectb 1001-1078). We use odd-numbered documents as the development set and even-numbered documents as the blind test set.\nThese document pairs are aligned at document level, but not at sentence or word level. To obtain sentence alignment, we use the Champollion Tool Kit (CTK). 5 After discarding sentences with no aligned counterpart, a total of 8,249 sentence pairs were retained. We induce word alignment using the BerkeleyAligner toolkit (Liang, Taskar, and Klein 2006). 6 The aligner outputs the posterior probability for each aligned word pair. To increase efficiency, we prune away all alignments that have probability less than 0.1.\nWe adopt the state-of-the-art monolingual Stanford NER tagger as a strong baseline for both English and Chinese. For English, we use the default tagger setting from Finkel, Grenager, and Manning (2005). For Chinese, we use an improved set of features over the default tagger, which are listed in Table 1. Both models make use of distributional similarity features taken from word clusters trained on large amounts of non-overlapping data. We train the two CRF models on all portions of the OntoNotes corpus that are annotated with named entity tags, except the parallel-aligned portion which we reserve for development and test purposes. In total, there are about 660 documents (\u223c16k sentences) and 1,400 documents (\u223c39k sentences) for Chinese and English, respectively.\nOut of the 18 named entity types that are annotated in OntoNotes, which include person, location, date, money, and so on, we select the four most commonly seen named entity types for evaluation. They are person, location, organization and GPE. All entities of these four types are converted to the standard BIO format, and background tokens and all other entities types are marked with tag O.\nIn all of the Gibbs sampling experiments, a fixed number of 2000 sampling steps are taken, and a linear cooling schedule is used in the deterministic annealing procedure.\nIn order to compare our method with past work, we obtained code from Burkett et al. (2010) and reproduced their experiment setting for the OntoNotes data. An extra set of 5,000 unannotated parallel sentence pairs are used for training the reranker, and the reranker model selection was performed on the development dataset.\nWe report standard NER measures (entity precision (P), recall (R) and F 1 score) on the test set. Statistical significance tests are done using the paired bootstrap resampling method (Efron and Tibshirani 1993), where we repeatedly draw random samples with replacement from the output of the two systems, and compare the test statistics (e.g. absolute difference in F 1 score) of the new samples with the observed test statistics. We used 1000 sampling iterations in our experiments.", "publication_ref": ["b11", "b16", "b7", "b3", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Bilingual NER Results", "text": "The main results on Chinese and English test sets are shown in Table 2. The first row (CRF) shows the baseline monolingual model performance. As we can see, the performance on Chinese is much lower than on English. This is partially attributed to the fact that the Chinese NER tagger was trained on less than half as much data, but it is also because NER in Chinese is a harder problem (e.g., there are no capitalization features in Chinese, which is a very strong indicator of named entities in English).\nBy enforcing hard agreement constraints, we can see from row hard that there is an increase of about 1.4% in absolute F 1 score on the Chinese side, but at the expense of a 6 code.google.com/p/berkeleyaligner  When we loosen the bilingual constraint to allow softagreement by simply assigning a hand-picked value (0.02) to aligned entities of different types (row manual), we observe a significant increase in accuracy in both Chinese and English. This suggests that the soft alignment successfully accounted for the cases where annotation standards differ in the two languages. In particular, the Chinese results are 3.8% better than the monolingual baseline, a 12% relative error reduction.\nWhen we replace the arbitrary hand-picked softagreement probabilities with empirical counts from the autotagged dataset (row auto), we see a small increase in recall on both sides, but a drop in precision for Chinese. However, accounting for alignment uncertainty (row auto+aP) increases both precision and recall for Chinese, resulting in another 1.2% increase in absolute F 1 score over the auto model.\nComparing against Burkett et al. (2010) (second row from the top), we can see that both our method and Burkett et al. (2010) significantly outperform the monolingual CRF baseline. This suggests that methods that explore bilingual language cues do have great utility in the NER task. Our best model (auto+aP) gives a significant gain over Burkett et al. (2010) on Chinese (by 2.2%), but trails behind on English by 0.7%. However, we will show in the next section some further improvements to our method by modeling global label consistency, which allows us to outperform Burkett et al. (2010) on both languages.", "publication_ref": ["b3", "b3", "b3", "b3"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Results on Global Consistency", "text": "Table 3 shows results on the test set after factoring in a global consistency model. Adding global consistency to the monolingual baseline (mono) increases performance on English (consistent with results from previous work (Finkel, Grenager, and Manning 2005)), but hurts Chinese results, especially in recall.\nA possible explanation is that CRF models for English are more certain about which words are entities (by having strong indicative features such as word capitalization), and thus a penalty does not persuade the model to label a word as a non-entity. However, in the Chinese case, the CRF model is weaker, and thus less certain about words being an entity or not. It is also much more likely that the same word  (string) will be both an entity and a common word in Chinese than English. In some cases, the model will be better off marking a word as a non-entity, than risking taking a penalty for labeling it inconsistently. By applying the \"reward\" function, we see a drastic increase in recall on both Chinese and English, with a relatively small sacrifice in precision on Chinese. The overall F 1 score increases by about 3.1% and 0.8% in Chinese and English, respectively. Similar results can be found when we apply global consistency to the bilingual model (auto). Again we see a recall-precision tradeoff between models with or without a \"reward\" function. But overall, we observe a significant increase in performance when global consistency with a reward function is factored in.\nModeling alignment uncertainty continues to improve the Chinese results when the global consistency model is added, but shows a small performance decrease on the English side. But the gain on the Chinese side is more significant than the loss on English side.\nThe best overall F 1 scores are achieved when bilingual constraints, global consistency with reward, and alignment uncertainty are conjoined. The combined model outperforms the CRF monolingual baseline, with an error reduction of 18.6% for Chinese and 9.9% for English. This model also significantly improves over the method of Burkett et al. (2010) with an error reduction of 10.8% for Chinese and 4.5% for English.\nBeyond the difference in model performance, our method is much easier to understand and implement than Burkett et al. (2010). Their method involves simulating a multi-view learning environment using \"weakened\" monolingual models to train a reranking model, and transplanting the parameters of the \"weakened\" models to \"strong\" models at test time in a practical but ad-hoc manner.", "publication_ref": ["b7", "b3", "b3"], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Semi-supervised NER Results", "text": "In the previous section we demonstrated the utility of our proposed method in a bilingual setting, where parallel sentence pairs are tagged together and directly evaluated. In reality, this is not the common use case. Most down-stream NLP applications operate in a monolingual environment.  Therefore, in order to benefit general monolingual NLP systems, we propose a semi-supervised learning setting where we use the bilingual tagger to annotate a large amount of unannotated bilingual text, then we take the tagged sentences on the Chinese side to retrain a monolingual Chinese tagger.\nTo evaluate the effectiveness of this approach, we used the Chinese-English part of the Foreign Broadcast Information Service corpus (FBIS, LDC2003E14), and tagged it with the auto+aP model. Unlike the OntoNotes dataset, this corpus does not contain document boundaries. In order to apply the document-level label consistency model, we divide the test set into blocks of ten sentences, and use the blocks as pseudo-documents.\nResults from self-training, as well as results from uptraining using model outputs from Burkett et al. (2010) are shown in Table 4. We can see that by using 80,000 additional sentences, our method gives a significant boost (\u223c2.9%, an error reduction of \u223c9.2%) over the CRF baseline. Our method also improves over Burkett et al. (2010) by a significant margin.\nThe gains are more pronounced in recall than precision, which suggests that the semi-supervised approach using bilingual data is very effective in increasing the coverage of the monolingual tagger. On the other hand, monolingual self-training hurts performance in both precision and recall.\nWe also report results on the effect of using increasing amounts of unannotated bilingual data. When only 10k sentences are added to the Chinese side, we already see a 5.2% error reduction over the CRF baseline.", "publication_ref": ["b3", "b3"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "Conclusions", "text": "We introduced a factored model with a Gibbs sampling inference algorithm, that can be used to produce more accurate tagging results for a parallel corpus. Our model makes use of cross-language bilingual constraints and intra-document consistency constraints. We further demonstrated that unlabeled parallel corpora tagged with our bilingual model can then be used to improve monolingual tagging results, using an uptraining scheme. The model presented here is not restricted to the NER task only, but can be adopted to improve other natural language applications as well, such as syntactic parsing and semantic analysis.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "The authors would like to thank Rob Voigt and the four anonymous reviewers for their valuable comments and suggestions. We gratefully acknowledge the support of the National Natural Science Foundation of China (NSFC) via grant 61133012, the National \"863\" Project via grant 2011AA01A207 and 2012AA011102, the Ministry of Education Research of Social Sciences Youth funded projects via grant 12YJCZH304, the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-0181 and the support of the DARPA Broad Operational Language Translation (BOLT) program through IBM.\nAny opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DARPA, AFRL, or the US government.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A high-performance semi-supervised learning method for text chunking", "journal": "", "year": "2005", "authors": "R K Ando; T Zhang"}, {"ref_id": "b1", "title": "Combining labeled and unlabeled data with co-training", "journal": "", "year": "1998", "authors": "A Blum; T Mitchell"}, {"ref_id": "b2", "title": "Collective information extraction with relational markov networks", "journal": "", "year": "2004", "authors": "R Bunescu; R J Mooney"}, {"ref_id": "b3", "title": "Learning better monolingual models with unannotated bilingual text", "journal": "", "year": "2010", "authors": "D Burkett; S Petrov; J Blitzer; D Klein"}, {"ref_id": "b4", "title": "Unsupervised models for named entity classification", "journal": "", "year": "1999", "authors": "M Collins; Y Singer"}, {"ref_id": "b5", "title": "Unsupervised part-of-speech tagging with bilingual graph-based projections", "journal": "", "year": "2011", "authors": "D Das; S Petrov"}, {"ref_id": "b6", "title": "An Introduction to the Bootstrap", "journal": "Chapman & Hall", "year": "1993", "authors": "B Efron; R J Tibshirani"}, {"ref_id": "b7", "title": "Incorporating non-local information into information extraction systems by gibbs sampling", "journal": "", "year": "2005", "authors": "J R Finkel; T Grenager; C Manning"}, {"ref_id": "b8", "title": "Generating chinese named entity data from a parallel corpus", "journal": "", "year": "2011", "authors": "R Fu; B Qin; T Liu"}, {"ref_id": "b9", "title": "Multi-view learning over structured and non-identical outputs", "journal": "", "year": "2008", "authors": "K Ganchev; J Graca; J Blitzer; B Taskar"}, {"ref_id": "b10", "title": "Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images", "journal": "IEEE Transitions on Pattern Analysis and Machine Intelligence", "year": "1984", "authors": "S Geman; D Geman"}, {"ref_id": "b11", "title": "Ontonotes: the 90% solution", "journal": "", "year": "2006", "authors": "E Hovy; M Marcus; M Palmer; L Ramshaw; R Weischedel"}, {"ref_id": "b12", "title": "Multilingual named entity recognition using parallel data and metadata from wikipedia", "journal": "", "year": "2012", "authors": "S Kim; K Toutanova; H Yu"}, {"ref_id": "b13", "title": "Optimization by simulated annealing", "journal": "Science", "year": "1983", "authors": "S Kirkpatrick; C D Gelatt; M P Vecchi"}, {"ref_id": "b14", "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "journal": "", "year": "2001", "authors": "J D Lafferty; A Mccallum; F C N Pereira"}, {"ref_id": "b15", "title": "Joint bilingual name tagging for parallel corpora", "journal": "", "year": "2012", "authors": "Q Li; H Li; H Ji; W Wang; J Zheng; F Huang"}, {"ref_id": "b16", "title": "Alignment by agreement", "journal": "", "year": "2006", "authors": "P Liang; B Taskar; D Klein"}, {"ref_id": "b17", "title": "Uptraining for accurate deterministic question parsing", "journal": "", "year": "2010", "authors": "S Petrov; P.-C Chang; M Ringgaard; H Alshawi"}, {"ref_id": "b18", "title": "Learning dictionaries for information extraction by multi-level bootstrapping", "journal": "", "year": "1999", "authors": "E Riloff; R Jones"}, {"ref_id": "b19", "title": "Improved parsing and POS tagging using inter-sentence consistency constraints", "journal": "", "year": "2012", "authors": "A M Rush; R Reichert; M Collins; A Globerson"}, {"ref_id": "b20", "title": "Collective segmentation and labeling of distant entities in information extraction", "journal": "", "year": "2004", "authors": "C Sutton; A Mccallum"}, {"ref_id": "b21", "title": "Inducing multilingual POS taggers and NP brackets via robust projection across aligned corpora", "journal": "", "year": "2001", "authors": "D Yarowsky; G Ngai"}, {"ref_id": "b22", "title": "Unsupervised word sense disambiguation rivaling supervised methods", "journal": "", "year": "1995", "authors": "D Yarowsky"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Example of annotation standard inconsistency", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Vice O Foreign B-ORG Affairs I-ORG Minister O Huaqiu B-PER Liu I-PER held O talks O with O Kamyao B-PER", "figure_data": "\u5916\u4ea4\u90e8 B-ORG\u526f\u90e8\u957f O\u5218\u534e\u79cb B-PER\u4e0e O\u52a0\u7c73\u5965 B-PER\u4e3e\u884c O\u4e86 O\u4f1a\u8c08 OFigure 1: Example of NER labels between two word-aligned bilingual parallel sentences.unannotated bi-text, and use the resulting annotated data asadditional training data to train a new monolingual modelwith better coverage. 1"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "61.64 68.42 81.98 74.59 78.11 Burkett 77.52 65.84 71.20 82.28 76.64 79.36 hard 76.19 64.47 69.84 82.13 72.85 77.21 manual 80.02 65.85 72.24 82.87 74.56 78.50 auto 78.53 66.90 72.25 82.11 75.40 78.62 auto+aP 79.17 68.46 73.43 82.05 75.56 78.67", "figure_data": "ChineseEnglishPRF1PRF1CRF76.89"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Results on bilingual parallel test set. F 1 scores that are statistically significantly better than the CRF baseline is highlighted in bold. 0.9% drop on the English side. The tradeoff mainly occurs in recall.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "61.64 68.42 81.98 74.59 78.11 +global 77.30 58.96 66.90 83.89 74.88 79.13 +global' 75.23 68.12 71.50 82.31 77.63 79.90 auto 78.53 66.90 72.25 82.11 75.40 78.62 +global 79.02 64.57 71.07 84.02 75.73 79.66 +global' 76.17 71.04 73.52 82.87 78.84 80.81", "figure_data": "ChineseEnglishPRF1PRF1mono 76.89 auto+aP 79.17 68.46 73.43 82.05 75.56 78.67+global79.31 65.93 72.01 84.01 75.81 79.70+global'76.43 72.32 74.32 82.30 78.35 80.28"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Results of enforcing global consistency. global is the global consistency without \"reward\" parameter, and global' is the one with \"reward\" parameter. \"mono\" is the monolingual CRF baseline. Best number in each column is highlighted in bold.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Burkett +80k 76.30 63.46 69.29 Semi with auto+aP +80k 77.40 66.10 71.31 +40k 76.97 65.60 70.83 +10k 77.48 64.95 70.66", "figure_data": "Method# train sent PRF1CRF baseline\u223c16k76.89 61.64 68.42Self-training+80k75.15 59.06 66.14Semi with"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Semi-supervised results on Chinese test set. F 1 scores that are statistically significantly better than the CRF baseline is highlighted in bold.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "P mono (y|x) = 1 Z(x) i exp( j \u03bb j f j (y i , y i\u22121 |x)) (1)", "formula_coordinates": [2.0, 328.82, 270.97, 229.19, 27.32]}, {"formula_id": "formula_1", "formula_text": "P bi (y c , y e ) = A={a c ,a e } I(y a c , y a e )(2)", "formula_coordinates": [2.0, 367.02, 679.42, 190.98, 21.19]}, {"formula_id": "formula_2", "formula_text": ". . the O earliest O established O bonded O area O . . . \u6700\u65e9 O \u521b\u529e O \u7684 O \u4fdd\u7a0e\u533a B-LOC", "formula_coordinates": [3.0, 68.65, 58.61, 230.78, 65.57]}, {"formula_id": "formula_3", "formula_text": "P (y t |y t\u22121 ) = P (y t i |y t\u22121 \u2212i , x)(3)", "formula_coordinates": [3.0, 379.43, 238.48, 178.57, 13.15]}, {"formula_id": "formula_4", "formula_text": "P (y c , y e |x c , x e ) = P mono (y c |x c )P mono (y e |x e )P bi (y c , y e )(4)", "formula_coordinates": [3.0, 319.5, 334.94, 242.52, 20.58]}, {"formula_id": "formula_5", "formula_text": "P (y t |y t\u22121 ) = P (y t i |y t\u22121 \u2212i , x) 1/ct j P (y t j |y t\u22121 \u2212j , x) 1/ct (5)", "formula_coordinates": [4.0, 96.67, 85.28, 195.83, 28.52]}, {"formula_id": "formula_6", "formula_text": "P glo (y|x) = \u03b3\u2208\u0393 \u03c6 #(\u03b3,y,x) \u03b3 (6)", "formula_coordinates": [4.0, 119.08, 385.7, 173.42, 22.13]}, {"formula_id": "formula_7", "formula_text": "w i+k , \u22121 \u2264 k \u2264 1 02: w i+k\u22121 \u2022 w i+k , 0 \u2264 k \u2264 1 03: shape(w i+k ), \u22124 \u2264 k \u2264 4 04: prefix(wi, k), 1 \u2264 k \u2264 4 05: prefix(wi\u22121, k), 1 \u2264 k \u2264 4 06: suffix(wi, k), 1 \u2264 k \u2264 4 07: suffix(wi\u22121, k), 1 \u2264 k \u2264 4 08: radical(wi, k), 1 \u2264 k \u2264 len(wi) 09: distsim(w i+k ), \u22121 \u2264 k \u2264 1 Unigram Features yi\u2022 00 -09 Bigram Features yi\u22121 \u2022 yi\u2022 00 -09", "formula_coordinates": [4.0, 372.79, 75.17, 131.92, 128.41]}, {"formula_id": "formula_8", "formula_text": "P glo (y|x) = \u03b4 #(\u03b4,y,x) \u03b3\u2208\u0393 \u03c6 #(\u03b3,y,x) \u03b3 (7)", "formula_coordinates": [4.0, 365.8, 367.88, 192.2, 22.13]}], "doi": ""}