{"title": "Levin Tree Search with Context Models", "authors": "Laurent Orseau; Marcus Hutter; Levi H S Lelis; Google Deepmind", "pub_date": "", "abstract": "Levin Tree Search (LTS) is a search algorithm that makes use of a policy (a probability distribution over actions) and comes with a theoretical guarantee on the number of expansions before reaching a goal node, depending on the quality of the policy. This guarantee can be used as a loss function, which we call the LTS loss, to optimize neural networks representing the policy (LTS+NN). In this work we show that the neural network can be substituted with parameterized context models originating from the online compression literature (LTS+CM). We show that the LTS loss is convex under this new model, which allows for using standard convex optimization tools, and obtain convergence guarantees to the optimal parameters in an online setting for a given set of solution trajectories -guarantees that cannot be provided for neural networks. The new LTS+CM algorithm compares favorably against LTS+NN on several benchmarks: Sokoban (Boxoban), The Witness, and the 24-Sliding Tile puzzle (STP). The difference is particularly large on STP, where LTS+NN fails to solve most of the test instances while LTS+CM solves each test instance in a fraction of a second. Furthermore, we show that LTS+CM is able to learn a policy that solves the Rubik's cube in only a few hundred expansions, which considerably improves upon previous machine learning techniques.", "sections": [{"heading": "Introduction", "text": "We 1 consider the problem of solving a set of deterministic single-agent search problems of a given domain, by starting with little prior domain-specific knowledge. We focus on algorithms that learn from previously solved instances to help solve the remaining ones. We consider the satisficing setting where solvers should (learn to) quickly find a solution, rather than to minimize the cost of the returned solutions.\nLevin Tree Search (LevinTS, LTS) is a tree search algorithm for this setup that uses a policy, i.e., a probability distri-bution over actions, to guide the search . LTS has a guarantee on the number of search steps required before finding a solution, which depends on the probability of the corresponding sequence of actions as assigned by the policy. Orseau and Lelis [2021] showed that this guarantee can be used as a loss function. This LTS loss is used to optimize a neural-network (NN) policy in the context of the Bootstrap search-and-learn process [Jabbari Arfaee et al., 2011]: The NN policy is used in LTS (LTS+NN) to iteratively solve an increasing number of problems from a given set, optimizing the parameters of the NN when new problems are solved to improve the policy by minimizing the LTS loss.\nOne constant outstanding issue with NNs is that the loss function (whether quadratic, log loss, LTS loss, etc.) is almost never convex in the NN's parameters. Still, most of the time NNs are trained using online convex optimization algorithms, such as stochastic gradient descent, Adagrad [Duchi et al., 2011], and its descendants. Such algorithms often come with strong convergence or regret guarantees that only hold under convexity assumptions, and can help to understand the effect of various quantities (number of parameters, etc.) on the learning speed [Zinkevich, 2003;Hazan, 2016;Boyd and Vandenberghe, 2004]. In this paper we present parameterized context models for policies that are convex with respect to the model's parameters for the LTS loss. Such models guarantee that we obtain an optimal policy in terms of LTS loss for a given set of training trajectories -a guarantee NNs do not have.\nThe context models we introduce for learning policies are based on the models from the online data compression literature [Rissanen, 1983;Willems et al., 1995]. Our context models are composed of a set of contexts, where each context is associated with a probability distribution over actions. These distributions are combined using product-of-experts [Hinton, 2002] to produce the policy used during the LTS search. The expressive power of product-of-experts comes mainly from the ability of each expert to (possibly softly) veto a particular option by assigning it a low probability. A similar combination using geometric mixing [Mattern, 2013;Matthew, 2005] (a geometrically-parameterized variant of product-ofexperts) in a multi-layer architecture has already proved competitive with NNs in classification, regression and density modelling tasks [Veness et al., 2017;Veness et al., 2021;Budden et al., 2020]. In our work the context distributions arXiv:2305.16945v2 [cs.LG] 27 Jun 2023 are fully parameterized and we show that the LTS loss is convex for this parameterization.\nIn their experiments, Orseau and Lelis [2021] showed that LTS+NN performs well on two of the three evaluated domains (Sokoban and The Witness), but fails to learn a policy for the 24-Sliding Tile Puzzle (STP). We show that LTS with context models optimized with the LTS loss within the Bootstrap process is able to learn a strong policy for all three domains evaluated, including the STP. We also show that LTS using context models is able to learn a policy that allows it to find solutions to random instances of the Rubik's Cube with only a few hundred expansions. In the context of satisficing planning, this is a major improvement over previous machinelearning-based approaches, which require hundreds of thousands expansions to solve instances of the Rubik's Cube.\nWe start with giving some notation and the problem definition (Section 2), before describing the LTS algorithm, for which we also provide a new lower bound on the number of node expansions (Section 3). Then, we describe parameterized context models and explain why we can expect them to work well when using product-of-experts (Section 4), before showing that the LTS loss function is convex for this parameterization (Section 5) and considering theoretical implications. Finally we present the experimental results (Section 6) before concluding (Section 7).", "publication_ref": ["b8", "b4", "b11", "b2", "b11", "b11", "b7", "b10", "b11", "b11", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Notation and Problem Definition", "text": "A table of notation can be found in Appendix I. We write [t] = {1, 2, . . . t} for a natural number t. The set of nodes is N and is a forest, where each tree in the forest represents a search problem with the root being the initial configuration of the problem. The set of children of a node n \u2208 N is C(n) and its parent is par(n); if a node has no parent it is a root node. The set of ancestors of a node is anc(n) and is the transitive closure of par(\u2022); we also define anc + (n) = anc(n) \u222a {n}. Similarly, desc(n) is the set of the descendants of n, and desc + (n) = desc(n) \u222a {n}. The depth of a node is d(n) = |anc(n)|, and so the depth of a root node is 0. The root root(n) of a node n is the single node n 0 \u2208 anc + (n) such that n 0 is a root. A set of nodes N \u2032 is a tree in the forest N if and only if there is a node n 0 \u2208 N \u2032 such that n\u2208N \u2032 root(n) = {n 0 }. Let N 0 = n\u2208N root(n) be the set of all root nodes. We write n [j] for the node at depth\nj \u2208 [d(n)] on the path from root(n) = n [0] to n = n [d(n)] .\nLet N * \u2286 N be the set of all solution nodes, and we write N * (n) = N * \u2229 desc + (n) for the set of solution nodes under n. A policy \u03c0 is such that for all n \u2208 N and for all n\n\u2032 \u2208 C(n) : \u03c0(n \u2032 | n) \u2265 0 and n \u2032 \u2208C(n) \u03c0(n \u2032 | n) \u2264 1.\nThe policy is called proper if the latter holds as an equality. We define, for all\nn \u2032 \u2208 C(n), \u03c0(n \u2032 ) = \u03c0(n)\u03c0(n \u2032 | n) recursively and \u03c0(n) = 1 if n is a root node.\nEdges between nodes are labeled with actions and the children of any node all have different labels, but different nodes can have overlapping sets of actions. The set of all edge labels is A. Let a(n) be the label of the edge from par(n) to n, and let A(n) be the set of edge labels for the edges from node n to its children.\nThen n \u0338 = n \u2032 \u2227 par(n) = par(n \u2032 ) implies a(n) \u0338 = a(n \u2032 ).\nStarting at a given root node n 0 , a tree search algorithm expands a set N \u2032 \u2286 desc + (n 0 ) until it finds a solution node in N * (n 0 ). In this paper, given a set of root nodes, we are interested in parameterized algorithms that attempt to minimize the cumulative number of nodes that are expanded before finding a solution node for each root node, by improving the parameters of the algorithm from found solutions, and with only little prior domain-specific knowledge.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Levin Tree Search", "text": "Levin Tree Search (LevinTS, which we abbreviate to LTS here) is a tree/graph search algorithm based on best-first search [Pearl, 1984] that uses the cost function 2 n \u2192 d(n)/\u03c0(n) , which, for convenience, we abbreviate as d \u03c0 (n). That is, since d \u03c0 (\u2022) is monotonically increasing from parent to child, LTS expands all nodes by increasing order of d \u03c0 (\u2022) (Theorem 2, Orseau et al. [2018]). Theorem 1 (LTS upper bound, adapted from Orseau et al. [2018], Theorem 3). Let \u03c0 be a policy. For any node\nn * \u2208 N , let N (n * ) = {n \u2208 N : root(n) = root(n * ) \u2227 d \u03c0 (n) \u2264 d \u03c0 (n *\n)} be the set of nodes within the same tree with cost at most that of n * . Then\n|N (n * )| \u2264 1 + d(n * ) \u03c0(n * ) .\nProof. Let L be the set of leaves of N (n * ), then\n|N (n * )| \u2264 1 + n\u2208L d(n) = 1 + n\u2208L \u03c0(n) d \u03c0 (n) \u2264 1 + n\u2208L \u03c0(n) d \u03c0 (n * ) \u2264 1 + d \u03c0 (n * ) ,\nwhere we used Lemma 10 (in Appendix) on the last inequality.\nThe consequence is that LTS started at root(n * ) expands at most 1 + d \u03c0 (n * ) nodes before reaching n * . Orseau and Lelis [2021] also provides a related lower bound showing that, for any policy, there are sets of problems where any algorithm needs to expand \u2126( d \u03c0 (n * )) nodes before reaching some node n * in the worst case. They also turn the guarantee of Theorem 1 into a loss function, used to optimize the parameters of a neural network. Let N \u2032 be a set of solution nodes whose roots are all different, define the LTS loss function:\nL(N \u2032 ) = n\u2208N \u2032 d \u03c0 (n) (1)\nwhich upper bounds the total search time of LTS to reach all nodes in N \u2032 . Equation ( 1) is the loss function used in Algorithm 2 (Appendix A) to optimize the policy -but a more precise definition for context models will be given later.\nTo further justify the use of this loss function, we provide a lower bound on the number of expansions that LTS must perform before reaching an (unknown) target node.\nTheorem 2 (Informal lower bound). For a proper policy \u03c0 and any node n * , the number of nodes whose\nd \u03c0 cost is at most that of n * is at least [ 1 d d \u03c0 (n * ) \u2212 1]/(|A| \u2212 1), wher\u0113 d \u2212 1 is\nthe average depth of the leaves of those nodes.\nA more formal theorem is given in Appendix B. Example 3. For a binary tree with a uniform policy, sinc\u0113 d = d(n * ) + 1, the lower bound gives 2 d d/(d + 1) \u2212 1 nodes for a node n * at depth d and of probability 2 \u2212d , which is quite tight since the tree has 2 d \u22121 nodes. The upper bound 1+d2 d is slightly looser.\nRemark 4. Even though pruning (such as state-equivalence pruning) can make the policy improper, in which case the lower bound does not hold and the upper bound can be loose, optimizing the parameters of the policy for the upper bound still makes sense, since pruning can be seen as a feature placed on top of the policy -that is, the policy is optimized as if pruning is not used. It must be noted that for optimization Orseau and Lelis [2021] (Section 4) use the log gradient trick to replace the upper bound loss with the actual number of expansions in an attempt to account for pruning; as the results of this paper suggest, it is not clear whether one should account for the actual number of expansions while optimizing the model.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Context Models", "text": "Now we consider that the policy \u03c0 has some parameters \u03b2 \u2208 B (where B \u2286 R k for some k, which will be made more precise later) and we write \u03c0(\u2022; \u03b2) when the parameters are relevant to the discussion. As mentioned in the introduction, we want the LTS loss function of Eq. (1) to be convex in the policy's parameters, which means that we cannot use just any policy -in particular this rules out deep neural networks. Instead, we use context models, which have been widely used in online prediction and compression (e.g., [Rissanen, 1983;Willems et al., 1995;Matthew, 2005;Veness et al., 2021]).\nThe set of contexts is Q. A context is either active or inactive at a given node in the tree. At each node n, the set of active contexts is Q(n), and the policy's prediction at n depends only on these active contexts.\nSimilarly to patterns in pattern databases [Culberson and Schaeffer, 1998], we organize contexts in sets of mutually exclusive contexts, called mutex sets, and each context belongs to exactly one mutex set. The set of mutex sets is M. For every mutex set M \u2208 M, for every node n, at most one context is active per mutex set. In this paper we are in the case where exactly one context is active per mutex set, which is what happens when searching with multiple pattern databases, where each pattern database provides a single pattern for a given node in the tree. When designing contexts, it is often more natural to directly design mutex sets. See Figure 1 for an example, omitting the bottom parts of (b) and (d) for now.\nTo each context c \u2208 Q we associate a predictor p c : A \u2192 [0, 1] which is a (parameterized) categorical probability distribution over edge labels that will be optimized from training data -the learning part will be explained in Section 5.1.\nTo combine the predictions of the active contexts at some node n, we take their renormalized product, as an instance of product-of-experts [Hinton, 2002]:\n\u2200a \u2208 A(n) : p \u00d7 (n, a) = c\u2208Q(n) p c (a) a \u2032 \u2208A(n) c\u2208Q(n) p c (a \u2032 )(2)\nWe refer to the operation of Eq. (2) as product mixing, by relation to geometric mixing [Mattern, 2013], a closely related operation. Then, one can use p \u00d7 (n, a) to define the policy \u03c0(n \u2032 |n) = p \u00d7 (n, a(n \u2032 )) to be used with LTS. The choice of this particular aggregation of the individual predictions is best explained by the following example. Example 5 (Wisdom of the product-of-experts crowd). Figure 1 (a) and (b) displays a simple maze environment where the agent is coming from the left. The only sensible action is to go Up (toward the exit), but no single context sees the whole picture. Instead, they see only individual cells around the agent, and one context also sees (only) the previous action (which is Right). The first two contexts only see empty cells to the left and top of the agent, and are uninformative (uniform probability distributions) about which action to take. But the next three contexts, while not knowing what to do, know what not to do. When aggregating these predictions with product mixing, only one action remains with high probability: Up. Example 6 (Generalization and specialisation). Another advantage of product mixing is its ability to make use of both general predictors and specialized predictors. Consider a mutex set composed of m contexts, and assume we have a total of M data points (nodes on solution trajectories). Due to the mutual exclusion nature of mutex sets, these M data points must be partitioned among the m contexts. Assuming for simplicity a mostly uniform partitioning, then each context receives approximately M/m data points to learn from. Consider the mutex sets in Fig. 1 (b): The first 4 mutex sets have size 3 (each context can see a wall, an empty cell or the goal) and the last one has size 4. These are very small sizes and thus the parameters of the contexts predictors should quickly see enough data to learn an accurate distribution. However, while accurate, the distribution can hardly be specific, and each predictor alone is not sufficient to obtain a nearly-deterministic policy -though fortunately product mixing helps with that. Now compare with the 2-cross mutex set in Fig. 1 (d), and assume that cells outside the grid are walls. A quick calculation, assuming only one goal cell, gives that it should contain a little less than 1280 different contexts. Each of these contexts thus receives less data to learn from on average than the contexts in (b), but also sees more information from the environment which may lead to more specific (less entropic) distributions, as is the case in situation (c).\nRemark 7. A predictor that has a uniform distribution has no effect within a product mixture. Hence, adding new predictors initialized with uniform predictions does not change the policy, and similarly, if a context does not happen to be useful to learn a good policy, optimization will push its weights toward the uniform distribution, implicitly discarding it.\nHence, product mixing is able to take advantage of both general contexts that occur in many situations and specialised contexts tailored to specific situations -and anything inbetween. The specialized 2-cross context is certain that the correct action is Right, despite the two other contexts together giving more weight to action Down. The resulting product mixing gives high probability to Right, showing that, in product mixing, specialized contexts can take precedence over less-certain more-general contexts.\nOur LTS with context models algorithm is given in Algorithm 1, building upon the one by Orseau and Lelis [2021] with a few differences. As mentioned earlier, it is a bestfirst search algorithm and uses a priority queue to maintain the nodes to be expanded next.\nIt is also budgeted and returns \"budget_reached\" if too many nodes have been expanded. It returns \"no_solution\" if all nodes have been expanded without reaching a solution nodeassuming safe pruning or no pruning. Safe pruning (using visited_states) can be performed if the policy is Markovian [Orseau et al., 2018], which is the case in particular when the set of active contexts Q(n) depends only on state(n). The algorithm assumes the existence of application-specific state and state_transition functions, such that state(n \u2032 ) = state_transition (state(n), a(n \u2032 )) for all n \u2032 \u2208 C(n). Note that with context models the prediction \u03c0(n \u2032 | n) depends on the active contexts Q(n) but not on the state of a child node. This allows us to delay the state transition until the child is extracted from the queue, saving up to a branching factor of state transitions (see also [Agostinelli et al., 2021]).\nRemark 8. In practice, usually a mutex set can be implemented as a hashtable as for pattern databases: the active context is read from the current state of the environment, and the corresponding predictor is retrieved from the hashtable. This allows for a computational cost of O(log |M |) per mutex set M , or even O(1) with perfect hash functions, and thus O( M \u2208M log |M |) which is much smaller than |Q|. Using an imperfect hashtable, only the contexts that appear on the paths to the found solution nodes need to be stored.", "publication_ref": ["b11", "b11", "b10", "b11", "b3", "b7"], "figure_ref": ["fig_0", "fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Convexity", "text": "Because the LTS loss in Eq. ( 1) is different from the log loss [Cesa-Bianchi and Lugosi, 2006] (due to the sum inbetween the products), optimization does not reduce to maximum likelihood estimation. However, we show that convexity in the log loss implies convexity in the LTS loss. This means, in particular, that if a probability distribution is logconcave (such as all the members of the exponential family), that is, the log loss for such models is convex, then the LTS loss is convex in these parameters, too.\nFirst we show that every sequence of functions with a convex log loss also have convex inverse loss and LTS loss.\nTheorem 9 (Log loss to inverse loss convexity). Let f 1 , f 2 , . . . f s be a sequence of positive functions with f i :\nR n \u2192 (0, \u221e) for all i \u2208 [s] and such that \u03b2 \u2192 \u2212 log f i (\u03b2) is convex for each i \u2208 [s], then L(\u03b2) = k\nThe proof is in Appendix E.1. For a policy \u03c0(\u2022; \u03b2) parameterized by \u03b2, the LTS loss in Eq. ( 1) is L N \u2032 (\u03b2) = k\u2208N \u2032 d(n k )/\u03c0(n k ; \u03b2), and its convexity follows from Theorem 9 by taking f k,0\n(\u2022) = 1/d(n k ), and f k,t (\u03b2) = \u03c0(n k [t] |n k [t\u22121] ; \u03b2) such that d(n k )\nt=1 f k,t (\u03b2) = \u03c0(n k ; \u03b2). Theorem 9 means that many tools of compression and online prediction in the log loss can be transferred to the LTS loss case. In particular, when there is only one mutex set (|M| = 1), the f i are simple categorical distributions, that is, f i (\u03b2) = \u03b2 jt for some index j t , and thus \u2212 log f i is a convex function, so the corresponding LTS loss is convex too. Unfortunately, the LTS loss function for such a model is convex in \u03b2 only when there is only one mutex set, |M| = 1. Fortunately, it becomes convex for |M| \u2265 1 when we reparameterize the context predictors with \u03b2 \u21dd exp \u03b2.\nLet \u03b2 c,a \u2208 [ln \u03b5 low , 0] be the value of the parameter of the predictor for context c for the edge label a. Then the prediction of a context c is defined as\n\u2200a \u2208 A(n) : p c (a; \u03b2) = exp(\u03b2 c,a ) a \u2032 \u2208A(n) exp(\u03b2 c,a \u2032 ) .(3)\nWe can also now make precise the definition of B: B = [ln \u03b5 low , 0] |Q|\u00d7A , and note that p c (a; \u03b2) \u2265 \u03b5 low /|A(n)|. Similarly to geometric mixing [Mattern, 2013;Mattern, 2016], it can be proven that context models have a convex log loss, Algorithm 1 Budgeted LTS with context models. Returns a solution node if any is found, or \"budget_reached\" or \"no_solution\".\n# n 0 : root node # B: node expansion budget # \u03b2: parameters of the context models def LTS+CM(n 0 , B, \u03b2):\nq = priority_queue() # tuple: { d \u03c0 , d, \u03c0 n , node, state, action} tup = {0, 0, 1, n 0 , state(n 0 ), False} q.insert(tup) # insert root node/state visited_states = {} # dict: state(n) -> \u03c0(n) repeat forever:\nif q is empty: return \"no_solution\" # Extract the tuple with minimum cost\nd \u03c0 d \u03c0 n, d, \u03c0 n , n, s_parent, a = q.extract_min() if n \u2208 N * : return n # solution found s = state_transition(s_parent, a) if a else s_parent \u03c0 s = visited_states.get(s, default=0) # Note: BFS ensures d \u03c0 (n s ) \u2264 d \u03c0 (n); s = state(n s ) # Optional: Prune the search if s is better if \u03c0 s \u2265 \u03c0 n : continue else: visited_states.set(s, \u03c0 n ) # Node expansion expanded += 1 if expanded == B: return \"budget_reached\" Z = a\u2208A(n) c\u2208Q(n) p c (a; \u03b2) # normalizer for n \u2032 \u2208 C(n): a = a(n \u2032 ) # action # Product mixing of the active contexts' predictions p \u00d7,a = 1 Z c\u2208Q(n) p c (a; \u03b2) # See Eq. (3) # Action probability, \u03b5 mix ensures \u03c0 n \u2032 > 0 \u03c0 n \u2032 = \u03c0 n ((1 \u2212 \u03b5 mix )p \u00d7,a + \u03b5mix |A(n)| ) q.insert({(d+1)/\u03c0 n \u2032, d+1, \u03c0 n \u2032, n \u2032 , s, a})\nand thus their LTS loss is also convex by Theorem 9. In Appendix E.2 we provide a more direct proof, and a generalization to the exponential family for finite sets of actions.\nPlugging (3) into Eq. (2) and pushing the probabilities away from 0 with \u03b5 mix > 0  we obtain the policy's probability for a child n \u2032 of n (i.e., for the action a(n \u2032 ) at node n) with parameters \u03b2:\np \u00d7 (n, a; \u03b2) = exp( c\u2208Q(n) \u03b2 c,a ) a \u2032 \u2208A(n) exp c\u2208Q(n) \u03b2 c,a \u2032 ,(4)\n\u03c0(n \u2032 | n; \u03b2) = (1 \u2212 \u03b5 mix )p \u00d7 (n, a(n \u2032 ); \u03b2) + \u03b5 mix |A(n)| . (5)", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "Optimization", "text": "We can now give a more explicit form of the LTS loss function of Eq. (1) for context models with a dependency on the parameters \u03b2, for a set of solution nodes N \u2032 assumed to all have different roots:\nL(N \u2032 , \u03b2) = n\u2208N \u2032 \u2113(n, \u03b2) ,(6)\n\u2113(n, \u03b2) = d(n) \u03c0(n; \u03b2) = d(n) d(n)\u22121 j=0 \u03c0(n [j+1] |n [j] ; \u03b2) (7) = d(n) d(n)\u22121 j=0 a \u2032 \u2208A(n [j] ) exp \uf8eb \uf8ed c\u2208Q(n [j] ) \u03b2 c,a \u2032 \u2212 \u03b2 c,a(n [j+1] ) \uf8f6 \uf8f8\nwhere a(n [j+1] ) should be read as the action chosen at step j, and the last equality follows from Eqs. ( 4) and ( 5) where we take \u03b5 mix = 0 during optimization. Recall that this loss function L gives an upper bound on the total search time (in node expansions) required for LTS to find all the solutions N \u2032 for their corresponding problems (root nodes), and thus optimizing the parameters corresponds to optimizing the search time.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Online Search-and-Learn Guarantees", "text": "Suppose that at each time step t = 1, 2 . . . , the learner receives a problem n 0 t (a root node) and uses LTS with parameters \u03b2 t \u2208 B until it finds a solution node n t \u2208 N * (n 0 t ). The parameters are then updated using n t (and previous nodes) and the next step t + 1 begins.\nLet N t = (n 1 , . . . , n t ) be the sequence of found solution nodes. For the loss function of Eq. (6), after t found solution nodes, the optimal parameters in hindsight are \u03b2 * t = argmin \u03b2\u2208B L(N t , \u03b2). We want to know how the learner fares against \u03b2 * t -which is a moving target as t increases. The regret [Hazan, 2016] at step t is the cumulative difference between the loss incurred by the learner with its time varying parameters \u03b2 i , i = 1, 2, . . . , t, and the loss when using the optimum parameters in hindsight \u03b2 * t :\nR(N t ) = i\u2208[t] \u2113(n i , \u03b2 i ) \u2212 L(N t , \u03b2 * t ) .\nA straightforward implication of the convexity of Eq. ( 7) is that we can use Online Gradient Descent (OGD) [Zinkevich, 2003] or some of its many variants such as Adagrad [Duchi et al., 2011] and ensure that the algorithm incurs a regret of R(\nN t ) = O(|A| |Q|G \u221a t ln 1 \u03b5low ),\nwhere G is the largest observed gradient in infinite norm 3 and when using quadratic regularization. Regret bounds are related to the learning speed (the smaller the bound, the faster the learning), that is, roughly speaking, how fast the parameters converge to their optimal values for the same sequence of solution nodes. Such a regret bound (assuming it is tight enough) also allows to observe the impact of the different quantities on the regret, such as the number of contexts |Q|, or \u03b5 low .\nOGD and its many variants are computationally efficient as they take O(d(n)|A| |M|) computation time per solution node n, but they are not very data efficient, due to the linearization of the loss function -the so-called 'gradient trick' [Cesa-Bianchi and Lugosi, 2006]. To make the most of the data, we avoid linearization by sequentially minimizing the full regularized loss function L(N t , \u2022) + R(\u2022) where R(\u03b2) is a convex regularization function. That is, at each step, we set:\n\u03b2 t+1 = argmin \u03b2\u2208B L(N t , \u03b2) + R(\u03b2)(8)\nwhich can be solved using standard convex optimization techniques (see Appendix C) [Boyd and Vandenberghe, 2004]. This update is known as (non-linearized) Follow the Leader (FTL) which automatically adapts to local strong convexity and has a fast O(log T ) regret without tuning a learning rate [Shalev-Shwartz, 2007], except that we add regularization to avoid overfitting which FTL suffers from. Unfortunately, solving Eq. ( 8) even approximately at each step is too computational costly, so we amortize this cost by delaying updates (see below), which of course incurs a learning cost, e.g., [Joulani et al., 2013].", "publication_ref": ["b11", "b4", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "As with previous work, in the experiments we use the LTS algorithm with context models (Algorithm 1) within the searchand-learn loop of the Bootstrap process [Jabbari Arfaee et al., 2011] to solve a dataset of problems, then test the learned model on a separate test set. See Appendix A for more details. Note that the Bootstrap process is a little different from the online learning setting, so the theoretical guarantees mentioned above may not carry over strictly -this analysis is left for future work., This allows us to compare LTS with context models (LTS+CM) in particular with previous results using LTS with neural networks (LTS+NN) [Guez et al., 2019;Orseau and Lelis, 2021] on three domains. We also train LTS+CM to solve the Rubik's cube and compare with other approaches. LTS+NN's domains. We foremost compare LTS with context models (LTS+CM) with LTS with a convolutional neural network [Orseau and Lelis, 2021] (LTS+NN) on the three domains where the latter was tested: (a) Sokoban (Boxoban) [Guez et al., 2018] on the standard 1000 test problems, a PSPACE-hard puzzle [Culberson, 1999] where the player must push boxes onto goal positions while avoiding deadlocks, (b) The Witness, a color partitioning problem that is NP-hard in general [Abel et al., 2020], and (c) the 24 (5\u00d75) sliding-tile puzzle (STP), a sorting problem on a grid, for which finding short solutions is also NP-hard [Ratner and Warmuth, 1986]. As in previous work, we train LTS+CM on the same datasets of 50 000 problems each, with the same initial budget (2000 node expansions for Sokoban and The Witness, 7000 for STP) and stop as soon as the training set is entirely solved. Training LTS+CM for these domains took less than 2 hours each. Harder Sokoban. Additionally, we compare algorithms on the Boxoban 'hard' set of 3332 problems. Guez et al. [2019] trained a convLSTM network on the mediumdifficulty dataset (450k problems) with a standard actor-critic setup -not the LTS loss -and used LTS (hence LTS+NN) at test time. The more recent ExPoSe algorithm [Mittal et al., 2022] updates the parameters of a policy neural network 4 during the search, and is trained on both the medium set (450k problems) and the 'unfiltered' Boxoban set (900k problems) with solution trajectories obtained from an A* search.\nRubik's Cube. We also use LTS+CM to learn a fast policy for the Rubik's cube, with an initial budget of B 1 = 21000. We use a sequence of datasets containing 100k problems each, generated with a random walk of between m and m \u2032 = m + 5 moves from the solution, where m increases by steps of 5 from 0 to 50, after which we set m \u2032 = m = 50 for each new generated set. DeepCubeA [Agostinelli et al., 2019] uses a fairly large neural network to learn in a supervised fashion from trajectories generated with a backward model of the environment, and Weighted A* is used to solve random test cubes. Their goal is to learn a policy that returns solutions of near-optimal length. By contrast, our goal is to learn a fast-solving policy. Allen et al. [2021] takes a completely different approach (no neural network) by learning a set of 'focused macro actions' which are meant to change the state as little as possible so as to mimic the so-called 'algorithms' that human experts use to solve the Rubik's cube. They use a rather small budget of 2 million actions to learn the macro actions, but also use the more informative goal-count scoring function (how many variables of the state have the correct value), while we only assume access to the more basic solved/unsolved function. As with previous work, we report solution lengths in the quarter-turn metric. Our test set contains 1000 cubes scrambled 100 times each -this is likely more than enough to generate random cubes [Korf, 1997]and we expect the difficulty to match that of previous work.\nMachine description. We used a single EPYC 7B12 (64 cores, 128 threads) server with 512GB of RAM without GPU.\nDuring training and testing, 64 problems are attempted concurrently -one problem per CPU core. Optimization uses 128 threads to calculate the loss, Jacobian and updates.", "publication_ref": ["b8", "b6", "b10", "b10", "b5", "b4", "b0", "b11", "b6", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Hyperparameters.", "text": "For all experiments we use \u03b5 low = 10 \u22124 , \u03b5 mix = 10 \u22123 , a quadratic regularization R(\u03b2) = 5\u2225\u03b2 \u2212 \u03b2 0 \u2225 2 where \u03b2 0 = (1 \u2212 1/A) ln \u03b5 low (see Appendix F). The convex optimization algorithm we use to solve Eq. ( 8) is detailed in Appendix C.    [Guez et al., 2019]). DeepCubeA uses four high-end GPU cards. More results can be found in Table 2 in Appendix H. \u2020Does not account for the cost of macro-actions.\nMutex sets. For Sokoban, STP, and The Witness we use several mutex sets of rectangular shapes at various distances around the agent (the player in Sokoban, the tip of the 'snake' in The Witness, the blank in STP), which we call relative tilings. An example of relative tiling is given in Fig. 2, and a more information can be found in Appendix G. For the Rubik's cube, each mutex set {i, j} corresponds to the ordered colors of the two cubies (the small cubes that make up the Rubik's cube) at location i and j (such as the up-front-right corner and the back-right edge). There are 20 locations, hence 190 different mutex sets, and each of them contains at most 24 2 contexts (there are 8 corner cubies, each with 3 possible orientations, and 12 side cubies, each with 2 possible orientations). For all domains, to these mutex sets we add one mutex set for the last action, indicating the action the agent performed to reach the node; for Sokoban this includes whether the last action was a push. The first 3 domains all have 4 actions (up, down, left, right), and the Rubik's cube has 12 actions (a rotation of each face, in either direction).\nResults. The algorithms are tested on test sets that are separate from the training sets, see Table 1. For the first three domains, LTS+CM performs better than LTS+NN, even solving all test instances of the STP while LTS+NN solves less than 1% of them. On The Witness, LTS+CM learns a policy that allows it to expand 5 times fewer nodes than LTS+NN. LTS+CM also solves all instances of the Boxoban hard set, by contrast to previous published work, and despite being trained only on 50k problems. On the Rubik's cube, LTS+CM learns a policy that is hundreds of times faster than previous workthough recall that DeepCubeA's objective of finding short solutions differs from ours. This may be surprising given how simple the contexts are -each context 'sees' only two cubies -and is a clear sign that product mixing is taking full advantage of the learned individual context predictions.", "publication_ref": ["b6"], "figure_ref": ["fig_1"], "table_ref": ["tab_1"]}, {"heading": "Conclusion", "text": "We have devised a parameterized policy for the Levin Tree Search (LTS) algorithm using product-of-experts of context models that ensures that the LTS loss function is convex. While neural networks -where convexity is almost certainly lost -have achieved impressive results recently, we show that our algorithm is competitive with published results, if not better.\nConvexity allows us in particular to use convex optimization algorithms and to provide regret guarantees in the online learning setting. While this provides a good basis to work with, this notion of regret holds against any competitor that learns from the same set of solution nodes. The next question is how we can obtain an online search-and-learn regret guarantee against a competitor for the same set of problems (root nodes), for which the cumulative LTS loss is minimum across all sets of solution nodes for the same problems. And, if this happens to be unachievable, what intermediate regret setting could be considered? We believe these are important open research questions to tackle.\nWe have tried to design mutex sets that use only basic domain-specific knowledge (the input representation of agent-centered grid-worlds, or the cubie representation of the Rubik's cube), but in the future it would be interesting to also learn to search the space of possible context models -this would likely require more training data.\nLTS with context models, as presented here, cannot directly make use of a value function or a heuristic function, however they could either be binarized into multiple mutex sets, or be used as in PHS* [Orseau and Lelis, 2021] to estimate the LTS cost at the solution, or be used as features since the loss function would still be convex (see Appendix C).", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "A Bootstrap Algorithm Details", "text": "Algorithm 2 Bootstrap using LTS_CM (given in Algorithm 1), which returns \"budget_reached\" when the number of nodes expanded reaches B t , or returns a solution node n * \u2208 N * (n 0 ) if it reaches n * , or returns \" no_solution\" if all nodes have been expanded without exhausting the budget and without reaching a solution node, which means that the problem has no solution. # N 0 : set of root nodes = problems # B 1 : initial budget # \u03c0 1 : initial policy # Returns the set of solution nodes def Bootstrap_with_LTS(N 0 , B 1 , \u03c0 1 ):\nsolns = {} # dictionary of problem -> solution for t = 1, 2, ...:\nfor each n 0 \u2208 N 0 : result = LTS_CM(n 0 , B t , \u03b2 t ) # search if result is \"no_solution\": N 0 \u2190 N 0 \\ {n 0 } if result is a node n * : soln[n 0 ] = n * if len(soln) = |N 0 |: return soln # Update the parameters of the model \u03b2 t+1 \u2248 argmin \u03b2\u2208B L(soln.values(), \u03b2) + R(\u03b2) choose budget B t+1\nOrseau and Lelis [2021] use a variant of the Bootstrap process [Jabbari Arfaee et al., 2011] to iteratively solve a set of problems while improving the policy based on the solutions for the already solved problems. See Algorithm 2.\nAt each Bootstrap iteration t, LTS is run on each problem (even those already solved) with a budget of B t node expansions with the context-model policy with current parameters \u03b2 t . After collecting the set of solutions N t , the parameters \u03b2 t+1 are obtained from Eq. (8) to some approximation (see Appendix C), and the next bootstrap iteration t + 1 is started with budget B t+1 .\nAdjusting the budget is non trivial. Keep in mind that computation time during search is proportional to the number of expansions. Solving previously solved problems usually is fast, because the policy has been optimized for them. Each problem for which a solution is newly found usually takes a large fraction of the budget (since they couldn't be solved for the previous budget), and every problem that remains unsolved consumes the whole budget. While a larger budget means that more problems can be solved, for a fixed set of parameters it is usual to see this number grow only logarithmically with the budget (since we are tackling hard problems). Hence when only few problems have already been solved, a large budget will make the algorithm spend a lot of time in yet-unsolvable problems, wasting computation time. By contrast, a too small budget will prevent finding new solutions and improving the policy, requiring more Bootstrap iterations.\nJabbari Arfaee et al. [2011] double the budget at each new Bootstrap iteration. This can become wasteful in computation time if learning works well, but a larger budget does not help much, in which case it may be better to use a constant budget. It may also be not fast enough during the last iterations: suppose 95% of the problems are solved, but the remaining 5% ones require to double the budget k more times: then the 95% will be resolved k more times (possibly finding different solutions), and, if the found solutions change, optimization is also performed k times before any new problem can be solved. By contrast, Orseau and Lelis [2021] use a fixed budget and double the budget only if no new problem is solved, which can also be wasteful in computation time if learning does not manage to work well enough and just one more problem is solved at each step.\nTo alleviate these issues, first, if more than a factor (1 + b) (say b = 1/4) of problems are solved at iteration t compared to the previous iteration -formally, |N t | \u2265 (1 + b) t \u2032 <t N t \u2032 -we reduce the next budget in case the current budget is too high:\nB t+1 = max{B 1 , B t /2} .\nOtherwise, we increase B t+1 so as to approximately double the number of total expansions (in the worst case of no new problem solved), rather than merely doubling the budget. More precisely, at Bootstrap iteration t, say the total number of expansions of solved problems is T + t , and the number of remaining unsolved problems is\ns \u2212 t = |N * \\ t \u2032 \u2264t N t \u2032 |, then we set the next budget B t+1 = 2B t + T + t /s \u2212 t , which ensures that T + t + s \u2212 t B t+1 = 2(T + t + s \u2212 t B t ), where T + t + s \u2212 t B t\nis the actual number of expansions used during iteration t, and T + t + s \u2212 t B t+1 is the probable number of expansions in case no new problem is solved, and assuming that previous problems take about the same time to be solved again (which is likely to be an overestimate due to learning).", "publication_ref": ["b8", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "B Formal Statement of the Lower Bound", "text": "In this section we provide a formal version of the informal lower bound of Theorem 2 on the number of node expansions required before reaching a target node n * . This number is within a factor (A \u2212 1)d of the upper bound, showing that the upper bound is quite tight and can be meaningfully used as a loss function.\nThe lower bound in Theorem 11 below requires the following lemma, which shows that the probability mass at the root behaves like a liter of water that is distributed recursively (but unevenly) along all the branches, and that if we collect the water at all the leaves (assuming a finite tree) then it still amounts to one liter, as long as the policy is proper. This lemma can be found in a compact form in the proof of Theorem 3 [Orseau et al., 2018].\nA tree N \u2032 \u2286 N is said to be full if every node of the tree either has all its children in the tree, or none of them.\nLemma 10. Let N \u2032 \u2286 N be a finite tree with root n 0 , and let L \u2032 \u2286 N \u2032 be its leaves. Let \u03c0 be a policy with \u03c0(n 0 ) = 1. Then n\u2208L \u2032 \u03c0(n) \u2264 1. Furthermore, if the policy is proper and N \u2032 is a full tree, then n\u2208L \u2032 \u03c0(n) = 1.\nProof. We start with the equality case. Using the fact that the policy is proper on the second line, and the fact that the tree N \u2032 is full on the fourth line, we have\nn\u2208L \u2032 \u03c0(n) = n\u2208N \u2032 \u03c0(n) \u2212 n\u2208N \u2032 \\L \u2032 \u03c0(n) = n\u2208N \u2032 \u03c0(n) \u2212 n\u2208N \u2032 \\L \u2032 \u03c0(n) n \u2032 \u2208C(n) \u03c0(n \u2032 | n) = n\u2208N \u2032 \u03c0(n) \u2212 n\u2208N \u2032 \\L \u2032 n \u2032 \u2208C(n) \u03c0(n \u2032 ) = n\u2208N \u2032 \u03c0(n) \u2212 n\u2208N \u2032 \\{n0} \u03c0(n \u2032 ) = \u03c0(n 0 ) = 1 .\nIf the tree N \u2032 is not full, it suffices to assign probability 0 to children outside of N \u2032 , which reduces to an improper policy. If the policy is not proper, it can be made proper on N \u2032 by renormalization of \u03c0 to\u03c0. More precisely, if the tree N \u2032 is not full or the policy is not proper, define, for\nall n \u2208 N \u2032 \\ L \u2032 , for all n \u2032 \u2208 C(n) \u2229 N \u2032 :\u03c0(n \u2032 | n) = \u03c0(n \u2032 | n)/ n \u2032\u2032 \u2208C(n)\u2229N \u2032 \u03c0(n \u2032\u2032 | n), which ensures that \u03c0(n) \u2265 \u03c0(n) for all nodes n \u2208 N \u2032 , and thus n\u2208L \u2032 \u03c0(n) \u2264 n\u2208L \u2032\u03c0(n) = 1. For a node n * , define N (n * ) = {n \u2208 N : root(n) = root(n * ) \u2227 d \u03c0 (n) \u2264 d \u03c0 (n *\n)}, which is the set of nodes of the same tree of cost at most that of n * , and\nL \u2032 (n * ) = {n \u2208 N : d \u03c0 (n) > d \u03c0 (n * ) \u2227 par(n) \u2208 N (n * )}, which is the set of children right outside N (n * ) -L \u2032 (n *\n) would be the 'frontier' or the contents of the priority queue in Algorithm 1, disregarding tie breaking. Let A \u2265 2 be the maximal branching factor of the search tree N (n * ), that is, for all n \u2208 N : |C(n)| \u2264 A. Observe that N (n * ) may not be a full tree, but that N (n * ) \u222a L \u2032 (n * ) is a full tree. Theorem 11 (Lower bound). Let \u03c0 be a proper policy. Then, for a node n * , the number of nodes with cost at most that of n * is at least\n|N (n * )| \u2265 1 (A \u2212 1) 1 d d \u03c0 (n * ) \u2212 1 . whered = 1/ n\u2208L \u2032 (n * ) \u03c0(n) d(n) is the harmonic average of the depth at the leaves L \u2032 (n * ).\nAlso observe that by the harmonic-mean -arithmetic mean inequality,d \u2264 n\u2208L \u2032 (n * ) \u03c0(n)d(n), the average depth at the leaves of the search tree N (n * ).\nProof. First, using the fact that\n|L \u2032 (n * ) \u222a N (n * )| is a full tree, |L \u2032 (n * )| + |N (n * )| = |L \u2032 (n * ) \u222a N (n * )| = 1 + n\u2208N (n * ) n \u2032 \u2208C(n) 1 \u2264 1 + A|N (n * )| ,\nand by rearranging we obtain \n|N (n * )| \u2265 (|L \u2032 (n * )| \u2212 1)/(A \u2212 1) . n 0 n 1 n 1,1 1 A n 1,2 1 A . . . n 1,A 1 A 1 \u2212 2 A n 2 n 2,1 1 2 n 2,2 1 2 2 A\n|L \u2032 (n * )| = n\u2208L \u2032 (n * ) d \u03c0 (n) \u03c0(n) d(n) > d \u03c0 (n * ) n\u2208L \u2032 (n * ) \u03c0(n) d(n) = 1 d d \u03c0 (n * ) ,\nSince L \u2032 (n * ) are the leaves of a full tree, n\u2208L \u2032 (n * ) \u03c0(n) = 1 by Lemma 10 and thusd is indeed an harmonic mean of the depths of the leaves. Therefore,\n|N (n * )| \u2265 1 (A \u2212 1) 1 d d \u03c0 (n * ) \u2212 1 .\nRemark 12. It appears that the factor 1/(A \u2212 1) is necessary. Consider the tree in Fig. 3, and take A \u2265 3. Then\nd \u03c0 (n 2,1 ) = 2A while d \u03c0 (n 1,\u2022 ) = 2A 2 /(A \u2212 2) > d \u03c0 (n 2,1 ). Then |N (n 2,1 )| = 5 = 5 d \u03c0 (n 2,1 )/(2A) = O( d \u03c0 (n 2,1 )/(A \u2212 1)\n). Also note that replacing 2/A with 1/A in the tree leads to d \u03c0 (n 2,1 ) = 4A and\nd \u03c0 (n 1,\u2022 ) = 2A 2 /(A \u2212 1) < d \u03c0 (n 2,1 ), which means that |N (n 2,1 )| \u2265 5 + A = \u2126( d \u03c0 (n 2,1 )) instead.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "C Convex optimization algorithm", "text": "To minimize Eq. (6), many convex optimization algorithms can be considered. For a first simple implementation, we would recommend using Frank-Wolfe [Frank and Wolfe, 1956;Jaggi, 2013] while being mindful of numerical stability (see Appendix D).\nIn the following, we describe the convex optimization routine we use for the experiments, however we suspect better and possibly more principled algorithms might be applicable too.\nFor the optimizer, we use isoGD [Orseau and Hutter, 2021] projection onto B (see also SOLO-FTRL[Orabona and P\u00e1l, 2018]), a scale-free variant of Adagrad [Duchi et al., 2011], which is adapted to use a line search. We have observed empirically that these algorithms tend to close the duality gap [Jaggi, 2013] faster than other algorithms (Frank-Wolfe [Frank and Wolfe, 1956], normalized gradient descent [Cort\u00e9s, 2006], accelerated gradient descent [Nesterov, 1983]), but admittedly we did not try all variants of all algorithms). However, the well-known difficulty with Adagradstyle algorithms is that the learning rate is always smaller than 1/G where G is the amplitude of largest observed gradient. But in function optimization, almost always the largest gradients are observed early and decrease significantly near the optimum -in our case, the initial gradients can be exponentially large. Hence, to reduce this dependency, we reset the learning rates on steps that are powers of 2 -this is known as the doubling trick [Cesa-Bianchi and Lugosi, 2006] and we fully expect that a regret bound can be proven with resets too, paying only a constant factor in the regret bound for a significantly milder dependency on the usually-larger initial gradients. We use one learning rate per context.\nOptimization is stopped after 200 iterations: if this happens to not be enough to improve the policy significantly to solve more problems, 200 more iterations will be triggered anyway after the next Bootstrap iteration.\nOptimization is also stopped early if the duality gap [Jaggi, 2013] guarantees that the loss is within a factor 2 of the optimum. Recall that this roughly means that the bound on the search time is a factor 2 away from the bound on the search time for the optimal parameters, which can hardly be much better. The duality gap is calculated every 20 iterations to amortize the computation cost. See also Appendix F.\nFor the line search, we use some ideas from Truong and Nguyen [2021]: A line search is triggered at each update iteration t where 1 \u2264 t mod 20 \u2264 3, and the learning rate found by the line search is re-used for the next optimization steps as long as there is improvement -otherwise a line search is triggered too -and also as the first middle query of the line search in [0, 1]. We use a (quasi-)exact line search rather than a backtracking line search, as it can make a significant difference on the first iterations, but less so afterwards.\nSee also Appendix D on numerical stability.", "publication_ref": ["b4", "b8", "b8", "b4", "b3", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "D Numerical Stability Considerations", "text": "One should use a numerically stable 'softmax' function for product mixing, and a stable 'log-sum-exp' (LSE) to calculate the logarithm of the LTS loss -the LTS loss can be exponentially large for untuned parameters.\nLSE(X) = C + log x\u2208X exp(x \u2212 C) , C = max X .\nFor a set N \u2032 of nodes, We can rewrite Eqs. ( 6) and ( 7), and the scaled gradient as:\nlog \u2113(n, \u03b2) = log d(n) \u2212 d(n)\u22121 j=0 log p \u00d7 (n [j] , a(n [j+1] ), \u03b2) , log L(N \u2032 , \u03b2) = LSE({log \u2113(n, \u03b2) | n \u2208 N \u2032 }) log(L(N \u2032 , \u03b2) + R(\u03b2)) = LSE(log L(N \u2032 , \u03b2), log R(\u03b2)) .\nRecall that \u03b5 mix = 0 during optimization. During the line search, one can use log(L(N \u2032 , \u03b2)+R(\u03b2)) but note that, while still unimodal (quasiconvex), it may not be convex anymore (despite Theorem 14) with a quadratic regularizer.\nTo calculate the gradients, similar caution should be used, for example for some constant C:\n\u03b2 t+1 = argmin \u03b2 e \u2212C R(\u03b2) + \u03c4 \u2208T exp(log \u2113(\u03c4, \u03b2) \u2212 C) , \u2207 exp(log \u2113(\u03c4, \u03b2) \u2212 C) = exp(log \u2113(\u03c4, \u03b2) \u2212 C)\u2207 log \u2113(\u03c4, \u03b2) .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E Convexity", "text": "E.1 Log loss to LTS loss Proof of Theorem 9. We can write\nL(x) = k exp t \u2212 log f k,t (x) ,\nBy assumption, \u2212 log f k,t (x) is convex for all k, t, and convexity is preserved by both summation and exponentiation (convex and non-decreasing) [Boyd and Vandenberghe, 2004], hence L(x) is convex.\nRemark 13. Convexity in LTS loss does not imply convexity in log loss. For example, take f (x) = 1/x 2 for x > 0, then 1/f is (strongly) convex, but \u2212 log 1/x 2 = 2 log |x| is not only concave on (\u2212\u221e, 0) and on (0, \u221e) but also has a singularity at 0. In a sense, the LTS loss is 'nicer' for convex optimization than the log loss. Theorem 14. The logarithm of loss function L(\u2022) defined in Theorem 9 is convex.\nProof. Following the proof of Theorem 9 we can write\nlog L(x) = log k exp t \u2212 log f k,t (x) ,\nand the result follows by observing that log-sum-exp and summation preserve convexity [Boyd and Vandenberghe, 2004].", "publication_ref": ["b2", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "E.2 LTS loss convexity of product mixing of context predictors", "text": "Theorem 15. The function L(N t , \u03b2) defined in Eq. (6) is convex in \u03b2.\nProof. This function is of the form exp log exp f (\u03b2) where f is linear. The result follows by observing that summation, exponentiation, and log-sum-exp are all preserving convexity, since they all are convex and non-decreasing [Boyd and Vandenberghe, 2004].\nWe now provide a more general result that applies if the context predictors are members of the exponential family, rather than just categorical distributions. Lemma 16. Let A be a finite set of actions and canonical parameters \u03b2 \u2208 R Q\u00d7A , and let Q be a set of predictors. Let\np c (n, a; \u03b2) = exp[\u03b2 \u2022 T c (n, a) \u2212 A c (n, \u03b2) + B c (n, a)]\nand all node n \u2208 N and all actions a \u2208 A, with A c : N \u00d7 B \u2192 R and B c : N \u00d7 A \u2192 R and T c : N \u00d7 A \u2192 R Q\u00d7A for all predictors c \u2208 Q, be a set of members of the exponential family in canonical form, with a dependency on the current node n \u2208 N . Then the product mixing p \u00d7 (n, a; \u03b2) (Eq. (2)) of the {p c } c\u2208Q is also a member of the exponential family in canonical form:\np \u00d7 (n, a; \u03b2) = exp[\u03b2 \u2022 T (n, a) \u2212 A(n, \u03b2) + B(n, a)] , with T (n, a) = c\u2208Q T c (n, a) , B(n, a) = c\u2208Q B c (n, a) , A(n, \u03b2) = ln a \u2032 \u2208A exp [\u03b2 \u2022 T (n, a \u2032 ) + B(n, a \u2032 )] .\nProof. The result is a straightforward application of the definition of product mixing:\np \u00d7 (n, a; \u03b2) = c\u2208Q p c (n, a; \u03b2) a \u2032 \u2208A c\u2208Q p c (n, a \u2032 ; \u03b2) = exp \u03b2 \u2022 c\u2208Q T c (n, a) + c\u2208Q B c (n, a) a \u2032 \u2208A exp \u03b2 \u2022 c\u2208Q T c (n, a \u2032 ) + c\u2208Q B c (n, a \u2032 ) = exp[\u03b2 \u2022 T (n, a) \u2212 A(n, \u03b2) + B(n, a)] .\nCategorical context models can be expressed as members of the exponential family in canonical form, by setting T c (n, a) to a zero vector, with just a single 1 at index (c, a) for context c and action a, but only if the context c is active at node n, i.e., c \u2208 Q(n), that is\nT c (n, a) c \u2032 ,a \u2032 = [[c \u2032 = c]] \u2022 [[a \u2032 = a]] \u2022 [[c \u2208 Q(n)]] ,\nwhere [[test]] = 1 if test is true, 0 otherwise. This implies that the vector T (n, a) also has a 1 at index (c, a) for each active context c, for each action a \u2208 A. To select only the valid actions at node n, also set B c (n, a) = 0 if a \u2208 A(n) and B c (n, a) = \u2212\u221e otherwise. Then\np c (n, a; \u03b2) = exp(\u03b2 \u2022 T c (n, a) \u2212 A c (n, \u03b2) + B c (n, a)) = exp(\u03b2 c,a [[c \u2208 Q(n)]] \u2212 A c (n, \u03b2)) [[a \u2208 A(n)]] , A c (n, \u03b2) = ln a\u2208A(n) exp(\u03b2 c,a [[c \u2208 Q(n)]]) , that is, p c (n, a; \u03b2) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 exp \u03b2c,a a \u2032 \u2208A(n) exp \u03b2 c,a \u2032 if c \u2208 Q(n), a \u2208 A(n) , 0 if a / \u2208 A(n) , 1 |A(n)| otherwise.\nRecall that uniform distributions have no effects in the product mixing, and thus a context that is not active is in effect removed from the product mixing, as in Eq. (4).\nIt is interesting to note that the A c (n, \u03b2) do not appear directly in the resulting form of the product mixing and thus do not need to be calculated.\nBeyond simple context models, since the vector T c (n, a) can depend on the current node n, it can make use in particular of features of the corresponding state of the environment, such as a heuristic distance to the goal.\nFinally, members of the exponential family in canonical form are well-known to be log-concave in their natural parameters \u03b2, that is, their log loss is convex in their natural parameters: \u2212 log p c (\u2022, \u2022; \u03b2) is of the form h(\u03b2)+ln exp g(\u03b2) where h and g are linear, and since log-sum-exp is convex the result follows. Therefore, by Theorem 9, the LTS loss of the product mixing of members of the canonical exponential family is convex in their natural parameters.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "F Beta-Simplex", "text": "This section describes a small but computationally helpful improvement regarding the calculation of the duality gap [Jaggi, 2013], which is used to terminate the optimization procedure.\nThe domain of the parameters \u03b2 is defined in the main paper as B = [ln \u03b5 low , 0] |Q|\u00d7A , and wrote that p c (a; \u03b2) \u2265 \u03b5 low /|C(n)|.\nWhile the duality gap can be calculated on this set, it can also be calculated for a subset of B, which more closely relates to the probability distributions of the predictors. Furthermore, the regret can still be meaningfully compared to the best probability distributions for the context predictors, rather than the optimal parameters \u03b2 * t \u2208 B. The highest-entropy probability distribution is the uniform distribution and can be expressed with p c by setting all components \u03b2 c,\u2022 to the same value.\nThe lowest-entropy probability distribution that can be expressed with p c is such that \u03b2 c,a = 0 for some chosen a \u2208 A and \u03b2 c,a \u2032 = ln \u03b5 low for a \u2032 \u0338 = a, giving\np c (a; \u03b2) = 1/(1 + (A \u2212 1)\u03b5 low ) \u2265 1 \u2212 (A \u2212 1)\u03b5 low , p c (a \u2032 ; \u03b2) = \u03b5 low /(1 + (A \u2212 1)\u03b5 low ) \u2264 \u03b5 low .\nConsider the constrained simplex \u2206 \u03b5low such that if p \u2208 \u2206 \u03b5low , then for all a \u2208 A : p(a) \u2265 \u03b5 low . Hence the probability distributions expressed by B can express at least as much as \u2206 \u03b5low by convex combinations. Unfortunately, enforcing a exp \u03b2 \u2022,a = 1 on B does not lead to a convex set. Instead, we define the \u03b2-simplex as a (convex) subset of B by constraining a\u2208A \u03b2 \u2022,a = (A \u2212 1) ln \u03b5 low . Note that the \u03b2-simplex still contains the highest and lowest entropy distributions of \u2206 \u03b5low . The 'center' of the \u03b2-simplex is at \u03b2 .,a = ((A \u2212 1) ln \u03b5 low )/A, which we define as \u03b2 0 , and explains why we use the regularization \u2225\u03b2 \u2212 \u03b2 0 \u2225 2 .\nHence, instead of calculating the regret compared to \u03b2 * \u2208 B, we can also consider calculating the regret to the best distribution in \u2206 \u03b5low or the best point in the \u03b2-simplex.\nMore importantly, we calculate the duality gap [Jaggi, 2013] for the \u03b2-simplex, which experimentally is easier to reduce than the duality gap for the \u03b2-hypercube B. In particular the mutex sets used in the experiments for Sokoban, The Witness, and the Sliding-Tile Puzzle are as follows: The position (r 0 , c 0 ) is taken to be the position of the agent: the avatar in Sokoban, the blank tile in the sliding tile puzzle, and the tip of the 'snake' in The Witness.", "publication_ref": ["b8", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "G Mutex Sets: Relative Tilings", "text": "We used the following relative tilings. For Sokoban: R T (3, 3, 4, 4), R T (2, 4, 2, 3), R T (4, 2, 3, 2) R T (2, 2, 2, 2), R T (1, 2, 1, 1), R T (2, 1, 1, 1), the number of mutex sets is |M| = 125, and walls are used as padding value; For The Witness: R T (3, 3, 4, 4), R T (2, 2, 4, 4), R T (2, 1, 1, 1), R T (1, 2, 1, 1), |M| = 125, with one additional padding color, and the goal location is not encoded. For the STP: R T (2, 2, 3, 3), R T (2, 1, 2, 2), R T (1, 2, 2, 2), R T (1, 1, 2, 2), |M| = 102, with an additional padding value.\nFor The Witness, our implementation uses two grids: one for the (fixed) colors and one for the snake (the trajectory of the player). We perform the same relativing tiling on each grid in parallel, merging each pair of contexts into a single context.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "H Extended Table of Results", "text": "While the message of the main paper is to compare LTS+CM with LTS+NN, it is also interesting to compare LTS+CM with other algorithms for the same domains. See Table 2.\nIn particular, Orseau and Lelis [2021] introduce the PHS* algorithm, which is based on LTS, but also uses a heuristic function to speed up the search, and they also compare with Weighted A* (WA*) [Pohl, 1970;Ebendt and Drechsler, 2009] which uses only a heuristic function, both with similar neural networks as LTS+NN. We can see LTS+CM is competitive on all three domains, while being fast (recall that tests use only one CPU, and no GPU). Moreover, LTS+CM could also be extended with a value function, either to PHS*+CM, or by using the value function as input features to each context, or by binarizing the values into multiple (possibly overlapping) contexts.\nFor the STP, while the test set used for DeepCubeA is different from the one used by the other algorithms, 5 we 5 The training set is also different, and although the number of problems is not clearly specified, it appears to be much larger than still expect the results to be comparable since Orseau and Lelis [2021]'s test sets are composed of random instances rather than scrambled from the solution. The heavy cost in expansions paid by DeepCubeA shows the price of finding near-optimal solutions -which it is reported to find 97% of the time. WA* appears to give the best compromise on this problem. LTS+CM expands more nodes but, given that it is still fast in milliseconds, we can hope for better results possibly by adding more mutex sets.\nDeepCubeA has also been trained on the 900k unfiltered Boxoban set [Guez et al., 2018], and finds short solutions in few expansions. But because it is trained differently from the other algorithms, its results are not directly comparable. We trained LTS+CM on the 450k medium Boxoban set and obtained a slightly better average number of expansions, with a much faster algorithm in milliseconds -at the expense of solution length. The same LTS+CM also expands almost 4x fewer nodes on the Boxoban hard set than the one trained with only 50k problems.\nOn the Rubik's cube, we report results for LTS+CM at various stages of its training. After just 300k cubes (2 hours 10 minutes), scrambled at most 15 times (these are much easier than fully scrambled cubes) it already finds a policy that solves the whole random test set -scrambled 100 times, which is usually considered more than sufficient for generating random cubes [Korf, 1997]. At 400k cubes, the policy is already substantially faster than previous work. We trained the network for up to 9M cubes, but the average number of expansions stabilizes at around 380. The training curve is shown in Fig. 4.\nWe also compared LTS+CM on the same 100 hardest Rubik's Cube problems of B\u00fcchner et al. [2022] as used by [Allen et al., 2021]. This set appears slightly simpler than the test set we used because we are able to find shorter solutions with fewer expansions in this set than in our set of problems. Note also that Allen et al. [2021] do not account for the length of the macro actions in the number of expansions, because they use a logical representation of the problem to 'compress' the results of macro actions -this assumes access to more information about the environment than just a simulator. The two approaches, focused macro-actions and learning a policy, are very much composable, and it would be interesting to see whether such macro actions could help make LTS+CM more efficient in training time or converge to a faster policy. As far as we are aware, LTS+CM is the first machine-learning algorithm to learn a fast policy for the Rubik's cube -while using only little domain-specific knowledge.\nFinally, it must be noted that the timings reported in the table should be read with a grain of salt, as different algorithms have been tested on different machines and implemented using different libraries. LTS+CM has been implemented in Racket 6 and we report running times as a means of showing that the CM models are very fast in practice. 50k problems. ", "publication_ref": ["b11", "b5", "b9", "b2", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We would like to thank the following people for their useful help and feedback: Csaba Szepesvari, Pooria Joulani, Tor Lattimore, Joel Veness, Stephen McAleer.\nThe following people also helped with Racket-specific questions: Matthew Flatt, Sam Tobin-Hochstadt, Bogdan Popa, Jeffrey Massung, Jens Axel S\u00f8gaard, Sorawee Porncharoenwase, Jack Firth, Stephen De Gabrielle, Alex Hars\u00e1nyi, Shu-Hung You, and the rest of the quite helpful and reactive Racket community.\nThis research was supported by Canada's NSERC and the CIFAR AI Chairs program.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Single parent of n, may not exist root(n)\nTopmost ancestor of n, has no parent anc(n)\nSet of ancestors of\nNode at depth j on the path from\nSet of nodes after t problems\nProbability of the node n according to the policy \u03c0 \u03c0(n\nLoss function for a single node Set of edge labels at node n, possible actions at n a An action, edge label a(n)\nEdge label (action) from par(n) to n R(N t )\nRegret of the learner compared to the optimal parameters for a set of solution nodes N t\n=1 if test is true, 0 otherwise", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Who witnesses the witness? finding witnesses in the witness is hard and sometimes impossible", "journal": "Nature Machine Intelligence", "year": "2019", "authors": "Abel "}, {"ref_id": "b1", "title": "Efficient black-box planning using macro-actions with focused effects", "journal": "", "year": "2021", "authors": "Allen "}, {"ref_id": "b2", "title": "A comparison of abstraction heuristics for rubik's cube", "journal": "Cambridge University Press", "year": "2004", "authors": "Vandenberghe ; Stephen Boyd; Lieven Vandenberghe; ; B\u00fcchner"}, {"ref_id": "b3", "title": "Jorge Cort\u00e9s. Finite-time convergent gradient flows with applications to network consensus", "journal": "", "year": "1993", "authors": "Joseph C Cort\u00e9s; Jonathan Culberson;  Schaeffer"}, {"ref_id": "b4", "title": "R\u00fcdiger Ebendt and Rolf Drechsler. Weighted a* search -unifying view and application", "journal": "", "year": "1956", "authors": "C Joseph;  Culberson;  Duchi"}, {"ref_id": "b5", "title": "crap, and Victor Valdes. An investigation of model-free planning: boxoban levels", "journal": "", "year": "2018", "authors": ""}, {"ref_id": "b6", "title": "An investigation of model-free planning", "journal": "PMLR", "year": "2016", "authors": "[ Guez"}, {"ref_id": "b7", "title": "Training Products of Experts by Minimizing Contrastive Divergence", "journal": "Neural Computation", "year": "2002-08", "authors": "Geoffrey E Hinton;  Hinton"}, {"ref_id": "b8", "title": "Revisiting Frank-Wolfe: Projection-free sparse convex optimization", "journal": "PMLR", "year": "2011", "authors": "Arfaee [jabbari"}, {"ref_id": "b9", "title": "Finding optimal solutions to rubik's cube using pattern databases", "journal": "Christopher Mattern", "year": "1997", "authors": "Richard E Korf;  Korf"}, {"ref_id": "b10", "title": "Yurii Nesterov. A method for solving the convex programming problem with convergence rate o(1/k 2 )", "journal": "Addison-Wesley Longman Publishing Co., Inc", "year": "1983", "authors": "; Mattern ; Christopher Mattern; ; V Mahoney Matthew; Laurent Matthew;  Orseau; H S Levi;  Lelis"}, {"ref_id": "b11", "title": "Tuyen Truong and Hang-Tuan Nguyen. Backtracking gradient descent method and some applications in large scale optimisation", "journal": "MIT Press", "year": "1970", "authors": "Ira Pohl; ; Daniel Ratner; Manfred Warmuth; ; J Rissanen ; Richard; S Sutton; Andrew G Barto;  Veness"}, {"ref_id": "b12", "title": "Each point label should be read: \"training time (iteration) -expansions (length)\", where \"training time\" is the wall-clock time used for both solving and optimizing since the start, \"iteration\" is the training set iteration", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: (a) A simple maze environment. The dark gray cells are walls, the green circle is a goal. The blue arrow symbolizes the fact that the agent (red triangle) is coming from the left. (b) A simple context model with five mutex sets: One mutex set for each of the four cells around the triangle, and one mutex set for the last chosen action. Each of the first four mutex sets contains three contexts (wall, empty cell, goal), and the last mutex set contains four contexts (one for each action). The 5 active contexts (one per mutex set) for the situation shown in (a) are depicted at the top, while their individual probability predictions are the horizontal blue bars for each of the four actions. The last column is the resulting product mixing prediction of the 5 predictions. No individual context prediction exceeds 1/3 for any action, yet the product mixing prediction is close to 1 for the action Up. (c) Another situation. (d) A different set of mutex sets for the situation in (c): A 1-cross around the agent, a 2-cross around the agent, and the last action. The specialized 2-cross context is certain that the correct action is Right, despite the two other contexts together giving more weight to action Down. The resulting product mixing gives high probability to Right, showing that, in product mixing, specialized contexts can take precedence over less-certain more-general contexts.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure2: Example of a relative tiling of row span 2, column span 3, at maximum row distance 1 and maximum column distance 3 around the agent (red triangle). Each orange rectangle is a mutex set of at most 4 6 different contexts. A padding value can be chosen arbitrarily (such as the wall value) for cells outside the grid.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: A tree showing the necessity of the factor A \u2212 1 for the lower bound, with A \u2265 3. Nodes that have fewer than A children can be completed with children of probability 0.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "We now give a general and formal definition of the rectangular tiling example in Figure Figure2. It is closely related to tile coding[Sutton and Barto, 1998]. On a grid of dimension R \u00d7 C, relative to some position (r 0 , c 0 ) on the grid, we call a relative tile T (sr , s c , d r , d c ) a particular mutex set with row span s r , column span s c , row offset d r and column offset d c . The ordered values of the grid rectangle between (r 0 + d r , c 0 + d c ) and (r 0 + d r + s r \u2212 1, c 0 + d c + s c \u2212 1) identify a unique context within the relative tile. A padding value can be chosen arbitrarily for coordinates that are outside the grid. A relative tiling R T (s r , s c , D r , D c ) of row span s r , column span s c , row distance D r and column distance D c , relative to some position (r 0 , c 0 ) is a set of relative tile mutex sets {T (s r , s c , d r , d c )} dr,dc for d r \u2208 [\u2212D r , . . . , D r \u2212 s r + 1] and d c \u2208 [\u2212D c , . . . , D c \u2212s c +1] -this ensures that the last position of the last relative tile is at (r 0 + D r , c 0 + D c ), while the first position of the first relative tile is at (r 0 \u2212 D r , c 0 \u2212 D c ). There are (2D r + 2 \u2212 s r )(2D c + 2 \u2212 s c ) mutex sets in the relative tiling.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Results on the test sets. The last 3 columns are the averages over the test instances. The first three domains allow for a fair comparison between LTS with context models and LTS with neural networks [Orseau and Lelis, 2021] using the same 50k training instances and initial budget. For the last two domains, comparison to prior work is more cursory and is provided for information only, in particular because the", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "j \u2208 [d(n)] on the path from root(n) = n [0] to n = n [d(n)] .", "formula_coordinates": [2.0, 54.0, 539.73, 243.0, 9.96]}, {"formula_id": "formula_1", "formula_text": "\u2032 \u2208 C(n) : \u03c0(n \u2032 | n) \u2265 0 and n \u2032 \u2208C(n) \u03c0(n \u2032 | n) \u2264 1.", "formula_coordinates": [2.0, 54.0, 571.03, 243.0, 23.68]}, {"formula_id": "formula_2", "formula_text": "n \u2032 \u2208 C(n), \u03c0(n \u2032 ) = \u03c0(n)\u03c0(n \u2032 | n) recursively and \u03c0(n) = 1 if n is a root node.", "formula_coordinates": [2.0, 54.0, 605.47, 243.0, 21.49]}, {"formula_id": "formula_3", "formula_text": "Then n \u0338 = n \u2032 \u2227 par(n) = par(n \u2032 ) implies a(n) \u0338 = a(n \u2032 ).", "formula_coordinates": [2.0, 54.0, 682.66, 243.0, 21.27]}, {"formula_id": "formula_4", "formula_text": "n * \u2208 N , let N (n * ) = {n \u2208 N : root(n) = root(n * ) \u2227 d \u03c0 (n) \u2264 d \u03c0 (n *", "formula_coordinates": [2.0, 315.0, 271.71, 243.0, 25.42]}, {"formula_id": "formula_5", "formula_text": "|N (n * )| \u2264 1 + d(n * ) \u03c0(n * ) .", "formula_coordinates": [2.0, 388.95, 309.06, 95.09, 23.89]}, {"formula_id": "formula_6", "formula_text": "|N (n * )| \u2264 1 + n\u2208L d(n) = 1 + n\u2208L \u03c0(n) d \u03c0 (n) \u2264 1 + n\u2208L \u03c0(n) d \u03c0 (n * ) \u2264 1 + d \u03c0 (n * ) ,", "formula_coordinates": [2.0, 341.99, 355.62, 189.03, 50.01]}, {"formula_id": "formula_7", "formula_text": "L(N \u2032 ) = n\u2208N \u2032 d \u03c0 (n) (1)", "formula_coordinates": [2.0, 394.75, 554.97, 163.25, 22.13]}, {"formula_id": "formula_8", "formula_text": "d \u03c0 cost is at most that of n * is at least [ 1 d d \u03c0 (n * ) \u2212 1]/(|A| \u2212 1), wher\u0113 d \u2212 1 is", "formula_coordinates": [3.0, 54.0, 66.24, 431.29, 37.05]}, {"formula_id": "formula_9", "formula_text": "\u2200a \u2208 A(n) : p \u00d7 (n, a) = c\u2208Q(n) p c (a) a \u2032 \u2208A(n) c\u2208Q(n) p c (a \u2032 )(2)", "formula_coordinates": [3.0, 324.96, 75.92, 233.04, 26.6]}, {"formula_id": "formula_10", "formula_text": "R n \u2192 (0, \u221e) for all i \u2208 [s] and such that \u03b2 \u2192 \u2212 log f i (\u03b2) is convex for each i \u2208 [s], then L(\u03b2) = k", "formula_coordinates": [4.0, 315.0, 373.15, 243.0, 23.68]}, {"formula_id": "formula_11", "formula_text": "(\u2022) = 1/d(n k ), and f k,t (\u03b2) = \u03c0(n k [t] |n k [t\u22121] ; \u03b2) such that d(n k )", "formula_coordinates": [4.0, 315.0, 448.94, 243.0, 28.26]}, {"formula_id": "formula_12", "formula_text": "\u2200a \u2208 A(n) : p c (a; \u03b2) = exp(\u03b2 c,a ) a \u2032 \u2208A(n) exp(\u03b2 c,a \u2032 ) .(3)", "formula_coordinates": [4.0, 341.3, 627.34, 216.7, 24.72]}, {"formula_id": "formula_13", "formula_text": "q = priority_queue() # tuple: { d \u03c0 , d, \u03c0 n , node, state, action} tup = {0, 0, 1, n 0 , state(n 0 ), False} q.insert(tup) # insert root node/state visited_states = {} # dict: state(n) -> \u03c0(n) repeat forever:", "formula_coordinates": [5.0, 63.03, 145.01, 220.95, 65.48]}, {"formula_id": "formula_14", "formula_text": "d \u03c0 d \u03c0 n, d, \u03c0 n , n, s_parent, a = q.extract_min() if n \u2208 N * : return n # solution found s = state_transition(s_parent, a) if a else s_parent \u03c0 s = visited_states.get(s, default=0) # Note: BFS ensures d \u03c0 (n s ) \u2264 d \u03c0 (n); s = state(n s ) # Optional: Prune the search if s is better if \u03c0 s \u2265 \u03c0 n : continue else: visited_states.set(s, \u03c0 n ) # Node expansion expanded += 1 if expanded == B: return \"budget_reached\" Z = a\u2208A(n) c\u2208Q(n) p c (a; \u03b2) # normalizer for n \u2032 \u2208 C(n): a = a(n \u2032 ) # action # Product mixing of the active contexts' predictions p \u00d7,a = 1 Z c\u2208Q(n) p c (a; \u03b2) # See Eq. (3) # Action probability, \u03b5 mix ensures \u03c0 n \u2032 > 0 \u03c0 n \u2032 = \u03c0 n ((1 \u2212 \u03b5 mix )p \u00d7,a + \u03b5mix |A(n)| ) q.insert({(d+1)/\u03c0 n \u2032, d+1, \u03c0 n \u2032, n \u2032 , s, a})", "formula_coordinates": [5.0, 73.04, 224.35, 222.25, 268.76]}, {"formula_id": "formula_15", "formula_text": "p \u00d7 (n, a; \u03b2) = exp( c\u2208Q(n) \u03b2 c,a ) a \u2032 \u2208A(n) exp c\u2208Q(n) \u03b2 c,a \u2032 ,(4)", "formula_coordinates": [5.0, 66.32, 604.19, 230.68, 30.13]}, {"formula_id": "formula_16", "formula_text": "\u03c0(n \u2032 | n; \u03b2) = (1 \u2212 \u03b5 mix )p \u00d7 (n, a(n \u2032 ); \u03b2) + \u03b5 mix |A(n)| . (5)", "formula_coordinates": [5.0, 64.64, 637.91, 232.36, 22.31]}, {"formula_id": "formula_17", "formula_text": "L(N \u2032 , \u03b2) = n\u2208N \u2032 \u2113(n, \u03b2) ,(6)", "formula_coordinates": [5.0, 315.13, 83.2, 242.88, 22.14]}, {"formula_id": "formula_18", "formula_text": "\u2113(n, \u03b2) = d(n) \u03c0(n; \u03b2) = d(n) d(n)\u22121 j=0 \u03c0(n [j+1] |n [j] ; \u03b2) (7) = d(n) d(n)\u22121 j=0 a \u2032 \u2208A(n [j] ) exp \uf8eb \uf8ed c\u2208Q(n [j] ) \u03b2 c,a \u2032 \u2212 \u03b2 c,a(n [j+1] ) \uf8f6 \uf8f8", "formula_coordinates": [5.0, 323.25, 110.16, 234.75, 66.11]}, {"formula_id": "formula_19", "formula_text": "R(N t ) = i\u2208[t] \u2113(n i , \u03b2 i ) \u2212 L(N t , \u03b2 * t ) .", "formula_coordinates": [5.0, 360.69, 456.03, 151.61, 22.6]}, {"formula_id": "formula_20", "formula_text": "N t ) = O(|A| |Q|G \u221a t ln 1 \u03b5low ),", "formula_coordinates": [5.0, 328.66, 524.02, 121.26, 19.69]}, {"formula_id": "formula_21", "formula_text": "\u03b2 t+1 = argmin \u03b2\u2208B L(N t , \u03b2) + R(\u03b2)(8)", "formula_coordinates": [6.0, 107.65, 116.78, 189.35, 18.67]}, {"formula_id": "formula_22", "formula_text": "for each n 0 \u2208 N 0 : result = LTS_CM(n 0 , B t , \u03b2 t ) # search if result is \"no_solution\": N 0 \u2190 N 0 \\ {n 0 } if result is a node n * : soln[n 0 ] = n * if len(soln) = |N 0 |: return soln # Update the parameters of the model \u03b2 t+1 \u2248 argmin \u03b2\u2208B L(soln.values(), \u03b2) + R(\u03b2) choose budget B t+1", "formula_coordinates": [10.0, 73.07, 249.09, 223.84, 95.18]}, {"formula_id": "formula_23", "formula_text": "B t+1 = max{B 1 , B t /2} .", "formula_coordinates": [10.0, 384.08, 253.09, 104.85, 9.65]}, {"formula_id": "formula_24", "formula_text": "s \u2212 t = |N * \\ t \u2032 \u2264t N t \u2032 |, then we set the next budget B t+1 = 2B t + T + t /s \u2212 t , which ensures that T + t + s \u2212 t B t+1 = 2(T + t + s \u2212 t B t ), where T + t + s \u2212 t B t", "formula_coordinates": [10.0, 315.0, 325.77, 243.0, 76.95]}, {"formula_id": "formula_25", "formula_text": "n\u2208L \u2032 \u03c0(n) = n\u2208N \u2032 \u03c0(n) \u2212 n\u2208N \u2032 \\L \u2032 \u03c0(n) = n\u2208N \u2032 \u03c0(n) \u2212 n\u2208N \u2032 \\L \u2032 \u03c0(n) n \u2032 \u2208C(n) \u03c0(n \u2032 | n) = n\u2208N \u2032 \u03c0(n) \u2212 n\u2208N \u2032 \\L \u2032 n \u2032 \u2208C(n) \u03c0(n \u2032 ) = n\u2208N \u2032 \u03c0(n) \u2212 n\u2208N \u2032 \\{n0} \u03c0(n \u2032 ) = \u03c0(n 0 ) = 1 .", "formula_coordinates": [11.0, 60.18, 97.38, 230.64, 125.09]}, {"formula_id": "formula_26", "formula_text": "all n \u2208 N \u2032 \\ L \u2032 , for all n \u2032 \u2208 C(n) \u2229 N \u2032 :\u03c0(n \u2032 | n) = \u03c0(n \u2032 | n)/ n \u2032\u2032 \u2208C(n)\u2229N \u2032 \u03c0(n \u2032\u2032 | n), which ensures that \u03c0(n) \u2265 \u03c0(n) for all nodes n \u2208 N \u2032 , and thus n\u2208L \u2032 \u03c0(n) \u2264 n\u2208L \u2032\u03c0(n) = 1. For a node n * , define N (n * ) = {n \u2208 N : root(n) = root(n * ) \u2227 d \u03c0 (n) \u2264 d \u03c0 (n *", "formula_coordinates": [11.0, 54.0, 282.44, 450.6, 81.72]}, {"formula_id": "formula_27", "formula_text": "L \u2032 (n * ) = {n \u2208 N : d \u03c0 (n) > d \u03c0 (n * ) \u2227 par(n) \u2208 N (n * )}, which is the set of children right outside N (n * ) -L \u2032 (n *", "formula_coordinates": [11.0, 54.0, 361.95, 243.0, 36.05]}, {"formula_id": "formula_28", "formula_text": "|N (n * )| \u2265 1 (A \u2212 1) 1 d d \u03c0 (n * ) \u2212 1 . whered = 1/ n\u2208L \u2032 (n * ) \u03c0(n) d(n) is the harmonic average of the depth at the leaves L \u2032 (n * ).", "formula_coordinates": [11.0, 54.0, 497.03, 243.0, 50.63]}, {"formula_id": "formula_29", "formula_text": "|L \u2032 (n * ) \u222a N (n * )| is a full tree, |L \u2032 (n * )| + |N (n * )| = |L \u2032 (n * ) \u222a N (n * )| = 1 + n\u2208N (n * ) n \u2032 \u2208C(n) 1 \u2264 1 + A|N (n * )| ,", "formula_coordinates": [11.0, 54.0, 596.27, 243.0, 71.41]}, {"formula_id": "formula_30", "formula_text": "|N (n * )| \u2265 (|L \u2032 (n * )| \u2212 1)/(A \u2212 1) . n 0 n 1 n 1,1 1 A n 1,2 1 A . . . n 1,A 1 A 1 \u2212 2 A n 2 n 2,1 1 2 n 2,2 1 2 2 A", "formula_coordinates": [11.0, 102.01, 54.81, 427.42, 645.84]}, {"formula_id": "formula_31", "formula_text": "|L \u2032 (n * )| = n\u2208L \u2032 (n * ) d \u03c0 (n) \u03c0(n) d(n) > d \u03c0 (n * ) n\u2208L \u2032 (n * ) \u03c0(n) d(n) = 1 d d \u03c0 (n * ) ,", "formula_coordinates": [11.0, 345.2, 227.79, 182.6, 60.57]}, {"formula_id": "formula_32", "formula_text": "|N (n * )| \u2265 1 (A \u2212 1) 1 d d \u03c0 (n * ) \u2212 1 .", "formula_coordinates": [11.0, 359.35, 341.06, 154.3, 22.67]}, {"formula_id": "formula_33", "formula_text": "d \u03c0 (n 2,1 ) = 2A while d \u03c0 (n 1,\u2022 ) = 2A 2 /(A \u2212 2) > d \u03c0 (n 2,1 ). Then |N (n 2,1 )| = 5 = 5 d \u03c0 (n 2,1 )/(2A) = O( d \u03c0 (n 2,1 )/(A \u2212 1)", "formula_coordinates": [11.0, 315.0, 392.66, 243.0, 34.81]}, {"formula_id": "formula_34", "formula_text": "d \u03c0 (n 1,\u2022 ) = 2A 2 /(A \u2212 1) < d \u03c0 (n 2,1 ), which means that |N (n 2,1 )| \u2265 5 + A = \u2126( d \u03c0 (n 2,1 )) instead.", "formula_coordinates": [11.0, 315.0, 429.1, 243.0, 26.71]}, {"formula_id": "formula_35", "formula_text": "LSE(X) = C + log x\u2208X exp(x \u2212 C) , C = max X .", "formula_coordinates": [12.0, 65.12, 510.89, 220.77, 20.06]}, {"formula_id": "formula_36", "formula_text": "log \u2113(n, \u03b2) = log d(n) \u2212 d(n)\u22121 j=0 log p \u00d7 (n [j] , a(n [j+1] ), \u03b2) , log L(N \u2032 , \u03b2) = LSE({log \u2113(n, \u03b2) | n \u2208 N \u2032 }) log(L(N \u2032 , \u03b2) + R(\u03b2)) = LSE(log L(N \u2032 , \u03b2), log R(\u03b2)) .", "formula_coordinates": [12.0, 58.83, 575.81, 235.0, 61.35]}, {"formula_id": "formula_37", "formula_text": "\u03b2 t+1 = argmin \u03b2 e \u2212C R(\u03b2) + \u03c4 \u2208T exp(log \u2113(\u03c4, \u03b2) \u2212 C) , \u2207 exp(log \u2113(\u03c4, \u03b2) \u2212 C) = exp(log \u2113(\u03c4, \u03b2) \u2212 C)\u2207 log \u2113(\u03c4, \u03b2) .", "formula_coordinates": [12.0, 315.0, 75.67, 249.44, 35.89]}, {"formula_id": "formula_38", "formula_text": "L(x) = k exp t \u2212 log f k,t (x) ,", "formula_coordinates": [12.0, 358.06, 198.03, 156.89, 20.14]}, {"formula_id": "formula_39", "formula_text": "log L(x) = log k exp t \u2212 log f k,t (x) ,", "formula_coordinates": [12.0, 343.53, 403.45, 185.94, 20.14]}, {"formula_id": "formula_40", "formula_text": "p c (n, a; \u03b2) = exp[\u03b2 \u2022 T c (n, a) \u2212 A c (n, \u03b2) + B c (n, a)]", "formula_coordinates": [12.0, 326.99, 666.31, 219.02, 9.65]}, {"formula_id": "formula_41", "formula_text": "p \u00d7 (n, a; \u03b2) = exp[\u03b2 \u2022 T (n, a) \u2212 A(n, \u03b2) + B(n, a)] , with T (n, a) = c\u2208Q T c (n, a) , B(n, a) = c\u2208Q B c (n, a) , A(n, \u03b2) = ln a \u2032 \u2208A exp [\u03b2 \u2022 T (n, a \u2032 ) + B(n, a \u2032 )] .", "formula_coordinates": [13.0, 54.0, 118.56, 229.41, 117.21]}, {"formula_id": "formula_42", "formula_text": "p \u00d7 (n, a; \u03b2) = c\u2208Q p c (n, a; \u03b2) a \u2032 \u2208A c\u2208Q p c (n, a \u2032 ; \u03b2) = exp \u03b2 \u2022 c\u2208Q T c (n, a) + c\u2208Q B c (n, a) a \u2032 \u2208A exp \u03b2 \u2022 c\u2208Q T c (n, a \u2032 ) + c\u2208Q B c (n, a \u2032 ) = exp[\u03b2 \u2022 T (n, a) \u2212 A(n, \u03b2) + B(n, a)] .", "formula_coordinates": [13.0, 55.61, 272.07, 234.43, 84.24]}, {"formula_id": "formula_43", "formula_text": "T c (n, a) c \u2032 ,a \u2032 = [[c \u2032 = c]] \u2022 [[a \u2032 = a]] \u2022 [[c \u2208 Q(n)]] ,", "formula_coordinates": [13.0, 74.95, 424.46, 201.09, 11.72]}, {"formula_id": "formula_44", "formula_text": "p c (n, a; \u03b2) = exp(\u03b2 \u2022 T c (n, a) \u2212 A c (n, \u03b2) + B c (n, a)) = exp(\u03b2 c,a [[c \u2208 Q(n)]] \u2212 A c (n, \u03b2)) [[a \u2208 A(n)]] , A c (n, \u03b2) = ln a\u2208A(n) exp(\u03b2 c,a [[c \u2208 Q(n)]]) , that is, p c (n, a; \u03b2) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 exp \u03b2c,a a \u2032 \u2208A(n) exp \u03b2 c,a \u2032 if c \u2208 Q(n), a \u2208 A(n) , 0 if a / \u2208 A(n) , 1 |A(n)| otherwise.", "formula_coordinates": [13.0, 54.0, 505.49, 246.13, 126.93]}, {"formula_id": "formula_45", "formula_text": "p c (a; \u03b2) = 1/(1 + (A \u2212 1)\u03b5 low ) \u2265 1 \u2212 (A \u2212 1)\u03b5 low , p c (a \u2032 ; \u03b2) = \u03b5 low /(1 + (A \u2212 1)\u03b5 low ) \u2264 \u03b5 low .", "formula_coordinates": [13.0, 329.45, 434.38, 214.11, 25.07]}], "doi": ""}