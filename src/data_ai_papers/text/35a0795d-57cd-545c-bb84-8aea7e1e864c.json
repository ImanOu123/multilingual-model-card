{"title": "Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms", "authors": "Chris Thornton; Frank Hutter; Holger H Hoos; Kevin Leyton-Brown", "pub_date": "2013-03-06", "abstract": "Many different machine learning algorithms exist; taking into account each algorithm's hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that addresses these issues in isolation. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classification performance often much better than using standard selection/hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance.", "sections": [{"heading": "INTRODUCTION", "text": "Increasingly, users of machine learning tools are nonexperts who require off-the-shelf solutions. The machine learning community has much aided such users by making available a wide variety of sophisticated learning algorithms and feature selection methods through open source packages, such as WEKA [13] and PyBrain [22]. Each of these packages asks a user to make two kinds of choices: selecting a learning algorithm and customizing it by setting its hyperparameters (which also control any feature selection being performed). It can be challenging to make the right choice when faced with these degrees of freedom, leaving many users to select algorithms based on reputation or intuitive appeal, and/or to leave hyperparameters set to default values. Of course, this approach can yield performance far worse than that of the best method and hyperparameter settings.\nThis suggests a natural challenge for machine learning: given a dataset, to automatically and simultaneously choose a learning algorithm and set its hyperparameters to optimize empirical performance. We dub this the combined algorithm selection and hyperparameter optimization problem (short: CASH); we formally define it in Section 3. Despite the practical importance of this problem, we are surprised to find no evidence that it has previously been considered in the literature. A likely explanation is that the combined space of learning algorithms and their hyperparameters is very challenging to search: the response function is noisy and the space is high dimensional, involves both categorical and continuous choices, and contains hierarchical dependencies (e.g., the hyperparameters of a learning algorithm are only meaningful if that algorithm is chosen; the algorithm choices in an ensemble method are only meaningful if that ensemble method is chosen; etc). In contrast, we do note that there has been considerable past work separately addressing model selection [e.g., 1,5,6,7,9,20,21,29] and hyperparameter optimization [e.g., 2,3,4,12,25,27].\nIn what follows, we demonstrate that CASH can be viewed as a single hierarchical hyperparameter optimization problem, in which even the choice of algorithm itself is considered a hyperparameter. We also show that -based on this problem formulation -recent Bayesian optimization methods can obtain high quality results in reasonable time and with minimal human effort. After discussing some preliminaries (Section 2), we define the CASH problem and discuss methods for tackling it (Section 3). We then define a concrete CASH problem encompassing the full range of classifiers and feature selectors in the open source package WEKA (Section 4), and show that a search in the combined space of algorithms and hyperparameters yields better-performing models than standard algorithm selection/hyperparameter optimization methods (Section 5). More specifically, we show that the recent Bayesian optimization procedures TPE [3] and SMAC [14] find combinations of algorithms and hyperparameters that often outperform existing baseline methods, especially on large datasets.", "publication_ref": ["b12", "b21", "b0", "b4", "b5", "b6", "b8", "b19", "b20", "b28", "b1", "b2", "b3", "b11", "b24", "b26", "b2", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "PRELIMINARIES", "text": "This work focuses on classification problems: learning a function f : X \u2192 Y with finite Y. A learning algorithm A maps a set {d1, . . . , dn} of training data points di = (xi, yi) \u2208 X \u00d7 Y to such a function, which is often expressed via a vector of model parameters. Most learning algorithms A further expose hyperparameters \u03bb \u2208 \u039b, which change the way the learning algorithm A \u03bb itself works. For example, hyperparameters are used to describe a description-length penalty, the number of neurons in a hidden layer, the number of data points that a leaf in a decision tree must contain to be eligible for splitting, etc. These hyperparameters are typically optimized in an \"outer loop\" that evaluates the performance of each hyperparameter configuration using cross-validation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model Selection", "text": "Given a set of learning algorithms A and a limited amount of training data D = {(x1, y1), . . . , (xn, yn)}, the goal of model selection is to determine the algorithm A * \u2208 A with optimal generalization performance. Generalization performance is estimated by splitting D into disjoint training and validation sets D (i) train and D (i) valid , learning functions fi by applying A * to D (i) train , and evaluating the predictive performance of these functions on D (i) valid . This allows for the model selection problem to be written as:\nA * \u2208 argmin A\u2208A 1 k k i=1 L(A, D (i) train , D (i) valid ), where L(A, D (i) train , D (i)\nvalid ) is the loss (here: misclassification rate) achieved by A when trained on D (i) train and evaluated on D (i) valid . We use k-fold cross-validation [18], which splits the training data into k equal-sized partitions D\n(1) valid , . . . , D (k)\nvalid , and sets D\n(i) train = D \\ D (i) valid for i = 1, . . . , k. 1", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "Hyperparameter Optimization", "text": "The problem of optimizing the hyperparameters \u03bb \u2208 \u039b of a given learning algorithm A is conceptually similar to that of model selection. Some key differences are that hyperparameters are often continuous, that hyperparameter spaces are often high dimensional, and that we can exploit correlation structure between different hyperparameter settings \u03bb1, \u03bb2 \u2208 \u039b. Given n hyperparameters \u03bb1, . . . , \u03bbn with domains \u039b1, . . . , \u039bn, the hyperparameter space \u039b is a subset of the crossproduct of these domains:\n\u039b \u2282 \u039b1 \u00d7 \u2022 \u2022 \u2022 \u00d7 \u039bn.\nThis subset is often strict, such as when certain settings of one hyperparameter render other hyperparameters inactive. For example, the parameters determining the specifics of the third layer of a deep belief network are not relevant if the network depth is set to one or two. Likewise, the parameters of a support vector machine's polynomial kernel are not relevant if we use a different kernel instead.\nMore formally, following [15], we say that a hyperparameter \u03bbi is conditional on another hyperparameter \u03bbj, if \u03bbi is only active if hyperparameter \u03bbj takes values from a given set Vi(j) \u039bj; in this case we call \u03bbj a parent of \u03bbi. Conditional hyperparameters can in turn be parents of other conditional hyperparameters, giving rise to a tree-structured space [3] or, in some cases, a directed acyclic graph (DAG) [15]. Given such a structured space \u039b, the (hierarchical) hyperparameter optimization problem can be written as:\n\u03bb * \u2208 argmin \u03bb\u2208\u039b 1 k k i=1 L(A \u03bb , D (i) train , D (i) valid ).", "publication_ref": ["b14", "b2", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "COMBINED ALGORITHM SELECTION AND HYPERPARAMETER OPTIMIZA-TION (CASH)", "text": "Given a set of algorithms A = {A (1) , . . . , A (k) } with associated hyperparameter spaces \u039b (1) , . . . , \u039b (k) , we define the 1 K-fold cross-validation is not the only available method for estimating generalization performance. We also experimented with the technique of repeated random subsampling validation [18], with similar results. Compute\nc = L(A \u03bb , D (i) train , D (i) valid ) 5: H \u2190 H \u222a {(\u03bb, c)} 6:\nUpdate ML given H 7: end while 8: return \u03bb from H with minimal c combined algorithm selection and hyperparameter optimization problem (CASH) as computing\nA * \u03bb * \u2208 argmin A (j) \u2208A,\u03bb\u2208\u039b (j) 1 k k i=1 L(A (j) \u03bb , D (i) train , D (i) valid ). (1\n)\nWe note that this problem can be reformulated as a single combined hierarchical hyperparameter optimization problem with parameter space\n\u039b = \u039b (1) \u222a \u2022 \u2022 \u2022 \u222a \u039b (k) \u222a {\u03bbr},\nwhere \u03bbr is a new root-level hyperparameter that selects between algorithms A (1) , . . . , A (k) . The root-level parameters of each subspace \u039b (i) are made conditional on \u03bbr being instantiated to Ai.\nIn principle, Problem 1 can be tackled in various ways. A promising approach is Bayesian Optimization [8], and in particular Sequential Model-Based Optimization [SMBO; 14], a versatile stochastic optimization framework that can work explicitly with both categorical and continuous hyperparameters, and that can exploit hierarchical structure stemming from conditional parameters. SMBO (outlined in Algorithm 1) first builds a model ML that captures the dependence of loss function L on hyperparameter settings \u03bb (line 1 in Algorithm 1). It then iterates the following steps: use ML to determine a promising candidate configuration of hyperparameters \u03bb to evaluate next (line 3); evaluate the loss c of \u03bb (line 4); and update the model ML with the new data point (\u03bb, c) thus obtained (lines 5-6).\nIn order to select its next hyperparameter configuration \u03bb using model ML, SMBO uses a so-called acquisition function aM L : \u039b \u2192 R, which uses the predictive distribution of model ML at arbitrary hyperparameter configurations \u03bb \u2208 \u039b to quantify (in closed form) how useful knowledge about \u03bb would be. SMBO then simply maximizes this function over \u039b to select the most useful configuration \u03bb to evaluate next. Several prominent acquisition functions exist [17,23,26] that all aim to automatically trade off exploitation (locally optimizing hyperparameters in regions known to perform well) versus exploration (trying hyperparameters in a relatively unexplored region of the space) in order to avoid premature convergence. In this work, we use one of the most prominent acquisition functions, the positive expected improvement (EI) attainable over an existing given error rate cmin [23]. Let c(\u03bb) denote the error rate of hyperparameter configuration \u03bb. Then, the positive improvement function over cmin is defined as:\nIc min (\u03bb) := max{cmin \u2212 c(\u03bb), 0}.\nOf course, we do not know c(\u03bb). We can, however, compute its expectation with respect to the current model ML:\nEM L [Ic min (\u03bb)] := c min \u2212\u221e max{cmin \u2212 c, 0} \u2022 pM L (c | \u03bb) dc. (2)\nOne main difference between existing SMBO algorithms lies in the model class they employ. We now review the two SMBO algorithms whose models can handle hierarchical hyperparameters and that are thus suitable for the task of combined algorithm selection and hyperparameter optimization.", "publication_ref": ["b0", "b0", "b17", "b0", "b7", "b16", "b22", "b25", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Sequential Model-based Algorithm Configuration (SMAC)", "text": "Sequential model-based algorithm configuration [SMAC; 14] supports a variety of models p(c | \u03bb) to capture the dependence of the loss function c on hyper-parameters \u03bb, including approximate Gaussian processes and random forests. In this paper we use random forest models, since they tend to perform well with discrete and high-dimensional input data. SMAC handles conditional parameters by instantiating inactive conditional parameters in \u03bb to default values for model training and prediction. This allows the individual decision trees to include splits of the kind \"is hyperparameter \u03bbi active?\", allowing them to focus on active hyperparameters. While random forests are not usually treated as probabilistic models, SMAC obtains a predictive mean \u00b5 \u03bb and variance \u03c3 \u03bb 2 of p(c | \u03bb) as frequentist estimates over the predictions of its individual trees for \u03bb; it then models pM L (c | \u03bb) as a Gaussian N (\u00b5 \u03bb , \u03c3 \u03bb 2 ). SMAC uses the expected improvement criterion defined in Equation 2, instantiating cmin to the error rate of the best hyperparameter configuration measured so far. Under SMAC's predictive distribution pM L (c | \u03bb) = N (\u00b5 \u03bb , \u03c3 \u03bb 2 ), this expectation can be computed by the closed-form expression\nEM L [Ic min (\u03bb)] = \u03c3 \u03bb \u2022 [u \u2022 \u03a6(u) + \u03d5(u)],\nwhere u = c min \u2212\u00b5 \u03bb \u03c3 \u03bb , and \u03d5 and \u03a6 denote the probability density function and cumulative distribution function of a standard normal distribution, respectively [17].\nSMAC is designed for robust optimization under noisy function evaluations, and as such implements special mechanisms to keep track of its best known configuration and assure high confidence in its estimate of that configuration's performance. This robustness against noisy function evaluations can be exploited in combined algorithm selection and hyperparameter optimization, since the function to be optimized in Equation ( 1) is a mean over a set of loss terms (each corresponding to one pair of D (i) train and D (i) valid constructed from the training set). A key idea in SMAC is to make progressively better estimates of this mean by evaluating these terms one at a time, thus trading off accuracy against computational cost. In order for a new configuration to become a new incumbent, it must outperform the previous incumbent in every comparison made: considering only one fold, two folds, and so on up to the total number of folds previously used to evaluate the incumbent. (Furthermore, every time the incumbent survives such a comparison, it is evaluated on a new fold, up to the total number available, meaning that the number of folds used to evaluate the incumbent grows over time.) This means that a poorly performing configuration can be discarded after considering as little as a single fold.\nFinally, SMAC also implements a diversification mechanism to achieve robust performance even when its model is misled: every second configuration is selected at random. Because of the evaluation procedure just described, the over-head required by this safeguard is less than it might appear.", "publication_ref": ["b16"], "figure_ref": [], "table_ref": []}, {"heading": "Tree-structured Parzen Estimator (TPE)", "text": "While SMAC models p(c | \u03bb) explicitly, the Tree-structure Parzen Estimator [TPE; 3] uses separate models for p(c) and p(\u03bb | c). Specifically, it models p(\u03bb | c) as one of two density estimates, conditional on whether c is greater or less than a given threshold value c * :\np(\u03bb | c) = (\u03bb), if c < c * . g(\u03bb), if c \u2265 c * . (3)\nHere, c * is chosen as the \u03b3-quantile of the losses TPE obtained so far (where \u03b3 is an algorithm parameter with a default value of \u03b3 = 0.15), (\u2022) is a density estimate learned from all previous hyperparameters \u03bb with corresponding loss smaller than c * , and g(\u2022) is a density estimate learned from all previous hyperparameters \u03bb with corresponding loss greater than or equal to c * . Intuitively, this creates a probabilistic density estimator (\u2022) for hyperparameters that appear to do 'well', and a different density estimator g(\u2022) for hyperparameters that appear 'poor' with respect to the threshold. Bergstra et al. [3] showed that the expected improvement EM L [Ic min (\u03bb)] from Equation 2 is proportional to a quantity that can be computed in closed-form from \u03b3, g(\u03bb), and (\u03bb):\nE[Ic min (\u03bb)] \u221d \u03b3 + g(\u03bb) (\u03bb) \u2022 (1 \u2212 \u03b3) \u22121 .\nTPE maximizes this expression by generating many candidate hyperparameter configurations at random and picking \u03bb with the smallest value of g(\u03bb)/ (\u03bb). The density estimators (\u2022) and g(\u2022) have a hierarchical structure with discrete, continuous, and conditional variables reflecting the hyperparameters and their dependence relationships. For each node in this tree structure, a 1-D Parzen estimator is created to model the density of the node's corresponding hyperparameter. For a given hyperparameter configuration \u03bb that is being added to either or g, only the 1-D estimators corresponding to active hyperparameters in \u03bb are updated. For continuous hyperparameters, these 1-D estimators are constructed by placing density in the form of a Gaussian at each hyperparameter value \u03bbi, with standard deviation set to the larger of each point's left and right neighbour. Discrete hyperparameters are estimated with probabilities proportional to the number of times that a particular choice occurred in the set of observations. To evaluate a candidate hyperparameter \u03bb's probability estimate, TPE starts at the root of the tree and descends into the leaves by following paths that only use active hyperparameters. At each node in this traversal, the probability of the corresponding hyperparameter is computed according to its 1-D estimator, and the individual probabilities are combined on a pass back up to the root of the tree. Note that this means that TPE assumes independence for hyperparameters that do not appear together along any path from the tree's root to one of its leaves.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "AUTO-WEKA", "text": "To demonstrate the feasibility of an automatic approach to solving the CASH problem, we built a tool, Auto-WEKA, that solves this problem for all classification algorithms and feature selectors/evaluators implemented in the WEKA package [13]. Note that while we have focused on classification algorithms in WEKA, there is no obstacle to extending our approach to other settings. Table 1 provides a list of all 39 WEKA classification algorithms. Of these models, 27 are considered 'base' classifiers (which can be used independently), 10 of the remaining classifiers are meta methods (which take a single base classifier and its parameters as an input), and the final 2 ensemble classifiers can take any number of base classifiers as input. We allowed the meta-methods to use any base classifier with any hyperparameter settings, and allowed ensemble methods to use up to five base classifiers, again with any hyperparameter settings. Not all classifiers are applicable on all datasets (e.g., due to a classifier's inability to handle missing data). For a given dataset, our Auto-WEKA implementation automatically only considers the subset of applicable classifiers.\nTable 2 provides a list of WEKA's 3 feature search methods, as well its 8 feature evaluators, and their respective number of subparameters (up to 5 for search; up to 4 for evaluators). To perform feature selection, a search method is combined with a feature evaluator, and the subparameters of both of them need to be instantiated. Feature selection is run as a pre-processing phase before building any classifier.\nThe algorithms in Table 1 and 2 have a wide variety of hyperparameters, which take on values from continuous intervals, from ranges of integers, and from other discrete sets. We associated either a uniform or log uniform prior with each numerical parameter, depending on its semantics. For example, we set a log uniform prior for the ridge regression penalty, and a uniform prior for the maximum depth for a tree in a random forest. Auto-WEKA works with continuous hyperparameter values directly; nevertheless, to give a sense of the size of the hypothesis space we studied, we note that discretizing hyperparameter domains to a maximum of 10 values each would give rise to over 10 47 hyperparameter settings. We emphasize that this space is much larger than a simple union of the base learners' hypothesis spaces (whose size is roughly 10 8 ), since the ensemble methods allow up to 5 independent base learners, giving rise to a space with roughly (10 8 ) 5 = 10 40 elements. The feature selection part gives rise to another independent decision between roughly 10 6 choices, and several parameters on the meta and ensemble level contribute another order of magnitude to the total size of AutoWEKA's hypothesis space.\nAuto-WEKA can be understood as a single learning algorithm with a highly conditional parameter space, as depicted in Figure 1. Auto-WEKA has two top-level Boolean parameters, the first of which is is_base that selects among single base classifiers and ensemble or meta-classifiers. If is_base is true, then the parameter base determines which of the 27 base classifiers are to be used. If is_base is false, then class indicates either an ensemble or a metaclassifier. If class is a meta-classifier, then the parameter meta_base is chosen to be one of the 27 base classifiers. In the event that class is an ensemble classifier, an additional parameter num_classes is an integer chosen from {1, . . . , 5}. base_i variables are then selected according to the value of num_classes, which again select which of the 27 base classifiers to use. For each * base * parameter, conditional hyperparameters for every model are attached.\nThe second top level Boolean parameter feat_sel indicates if one of the feature selection methods is going to be applied. If feat_sel is false, then Auto-WEKA passes the unmodified dataset to the classifier. If it is true, then  feat_search selects the choice of feature search method, and feat_eval selects the choice of feature evaluator. This results in a very wide tree that captures all the hierarchical nature of the model hyperparameters, and allows the creation of a single hyperparameter optimization problem with four hierarchical layers of a total of 786 parameters.\nAuto-WEKA is agnostic to the choice of optimizer, so we implemented variants leveraging SMAC and TPE, respectively -the two Bayesian optimization algorithms that can handle hierarchical parameter spaces (see Sections 3.1 and 3.2. 2 We defined two Auto-WEKA variants, based on SMAC and TPE, respectively. We made both of these Auto-WEKA versions available to the public at www.cs.ubc. ca/labs/beta/Projects/autoweka/ and are committed to provide support for their widespread use in practice.\nBoth TPE and SMAC have their own parameters that influence their performance (such as TPE's choice of the \u03b3-quantile indicating 'good' or 'bad' performance, or the parameters of SMAC's random forest model). In Auto-Weka, we used the defaults for these meta-hyperparameters, as set by the authors (we assume further small improvements may be possible by meta-hyperparameter optimization, but a separate process with a meta-training/validation set split would be required to guard against over-fitting, and we did not attempt this).\nFinally, we note that both TPE and SMAC are randomized algorithms and thus expected to produce different results based on the random seed provided. As demonstrated in [16], this allows for trivial, yet effective parallelization of the optimization process: simply perform k independent runs of the optimization method in parallel and select the result of the run with the lowest cross-validation error to return. 3 We verified experimentally that the more parallel runs, the faster this process identifies high-quality configurations; nevertheless, we restricted the version of Auto-WEKA studied here to run only 4 parallel processes, in order to study a setting typical for commonly used workstations.", "publication_ref": ["b12", "b1", "b15", "b2"], "figure_ref": ["fig_0"], "table_ref": ["tab_1", "tab_2", "tab_1"]}, {"heading": "EVALUATING AUTO-WEKA", "text": "We now describe an experimental study of the performance that can be achieved by Auto-WEKA on various datasets. After specifying our experiment environment, we demonstrate the importance of addressing the algorithm selection and the CASH problems, and establish baselines for them (Section 5.2). We then demonstrate Auto-WEKA's ability to search its enormous hyperparameter space effectively to find algorithms and hyperparameters with low cross-validation error (Section 5.3). Then, we analyse its test performance and address concerns regarding overfitting (Section 5.4). Finally, we provide a synopsis of the classifiers and feature search/evaluators Auto-WEKA chose in our experiments (Section 5.5).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental setup", "text": "We evaluated Auto-WEKA on 21 prominent benchmark datasets (see Table 3): 15 sets from the UCI repository [11]; the 'convex', 'MNIST basic' and 'rotated MNIST with background images' tasks used in [4]; the appentency task from the KDD Cup '09; and two versions of the CIFAR-10 image classification task [19] (CIFAR-10-Small is a subset of CIFAR-10, where only the first 10 000 training data points are used rather than the full 50 000.) For datasets with a predefined training/test split, we used that split. Otherwise, we randomly split the dataset into 70% training and 30% test data. The test data was never seen by any optimization method; it was only used once in an offline analysis stage to evaluate the models found by the various optimization is no empirical evidence that these methods outperform the simple approach we use here when the cost of evaluating hyperparameter configurations varies across the space. methods. We denote datasets with at least 10 000 training data points as 'large' and all others as 'small'.\nAll of our experiments were run on Linux machines with Intel Xeon X5650 six-core processors, running at 2.66GHz. All datasets had a RAM limit of 3GB for classification; if training a classifier ever exceeded this memory limit, the classifier job was terminated, returning a misclassification rate of 100%. An additional 1GB of RAM was allocated for the SMBO method. While these limits are somewhat arbitrary, we believe them to be reasonably close to the resource limitations faced by any user of machine learning algorithms. We also limited the training time for each evaluation of a learning algorithm on each fold, to ensure that the optimization method had a chance to explore the search space. Once this training budget for a fold is consumed, Auto-WEKA sends an interrupt to the learning algorithm to terminate as soon as possible, and the (partially) trained model is then evaluated on the validation set to determine the error estimate of the fold. This timeout was set to 150 minutes for classification and 15 minutes for feature search and evaluation in our experiments. 4 For each dataset, we ran Auto-WEKA with each hyperparameter optimization algorithm with a total time budget of 30 hours. For each method, we performed 25 runs of this process with different random seeds and then -in order to simulate parallelization on a typical workstation -used bootstrap sampling to repeatedly select 4 random runs and report the performance of the one with best cross-validation performance.\nIn early experiments, we observed a few cases in which Auto-WEKA's SMBO method picked hyperparameters that had excellent training performance, but turned out to generalize poorly. To enable Auto-WEKA to detect cases of overfitting, we partitioned its training set into two subsets: 70% for use inside the SMBO method, and 30% of validation data that is only looked at after the SMBO method has finished.", "publication_ref": ["b10", "b3", "b18", "b3"], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Algorithm Selection and CASH: Baseline Methods", "text": "Auto-WEKA aims to aid non-expert users of machine learning techniques. The simplest approach for selecting a classifier that is widely adopted amongst non-experts is to use a classifier merely based on its popularity or intuitive appeal, without any empirical consideration of alternatives. To quantify the difference such a choice can make, we consider the 39 WEKA classifiers (each with default hyperparameter settings) for each dataset, train each on the training set, and measure its accuracy on the test set. For each dataset, the second and third columns in Table 4 present the best and worst \"oracle performance\" of these classifiers on the test set. We observe that the gap between the best and worst classifier was huge, e.g. misclassification rates of 4.93% vs 99.24% on the Dorothea dataset. Even when the set of classifiers was restricted to a few popular ones (we considered neural networks, random forests, SVMs, AdaBoost, C4.5 decision trees, logistic regression, and KNN), this gap still exceeded 20% on 14 out of the 21 datasets. Furthermore, there was no single method that achieved good performance across all datasets: every method was at least 22% worse than the best for at least one data set. We conclude that some form of algorithm selection is essential for achieving good performance.\nA straight-forward algorithm selection method is to perform exhaustive 10-fold cross-validation on the training set and to return the classifier with the smallest average misclassification error across folds. We will refer to this method applied to the set of 39 WEKA classifiers as Ex-Def ; it is the best choice that can be made among the 39 WEKA classifiers (with their default hyperparameters) based on an exhaustive cross-validation and will serve as a baseline to compare an optimal solution to the algorithm selection problem in WEKA to our solution for the CASH problem in WEKA.\nMore experienced users of machine learning algorithms would not only select between a fixed set of default algorithms, but would also consider different hyperparameter settings -for example by performing a grid search over the hyperparameter space of a single classifier (as, e.g., implemented in WEKA 5 ). Since different learning algorithms perform well for different problems, more experienced users optimally would also want to consider different hyperparameter settings for more than one learning algorithm. Therefore, a stronger baseline we will use is an approach that -in addition to the 39 WEKA default classifiers -considers various hyperparameter settings for all of WEKA's 27 base classifiers. More precisely, this baseline considers a grid of hyperparameter settings for each of these 27 base classifier, and performs a random search [4] in the union of these grids (plus the 39 WEKA default classifiers). We refer to this baseline as Random Grid and note that -as an optimization approach in the joint space of algorithms and hyperparameter settings -it is a simple CASH algorithm. We executed this Random Grid search for all our datasets in parallel, using 400 CPU hours on average per dataset (at least 120 hours for each). Table 4 (columns 4 and 5) shows the best and worst \"oracle performance\" on the test set across these Random Grid classifiers. Comparing these performances to the default performance, we note that in most cases even WEKA's best default algorithm could be improved by selecting better hyperparameter settings, sometimes rather substantially so: e.g., in the rotated MNIST with background images task, random grid search offered a 6% improvement over Ex-Def. We conclude that choosing hyperparameter settings appropriately can lead to substantial differences in performance and that because of this fact even a relatively simple (albeit computationally expensive) approach for CASH can outperform algorithm selection by itself.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": ["tab_5", "tab_5"]}, {"heading": "Results for Cross-Validation Performance", "text": "With 786 hierarchical hyperparameters, Auto-WEKA's combined algorithm / hyperparameter space is very complex. We now study how effectively SMAC and TPE could search this space to optimize 10-fold cross-validation performance, and compare their performance to that of the Ex-Def and Random Grid methods defined in the previous section.\nThe middle portion of Table 4 reports the results. First, we note that random grid search over the hyperparameters of all base-classifiers yielded better results than Ex-Def in 14/21 cases (and tied in the remaining seven), which underlines We conclude that by searching Auto-WEKA's combined algorithm/hyperparameter space, we can effectively find models with much better cross-validation performance than by grid search over WEKA's base classifiers.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Results for Test Performance", "text": "The results just shown demonstrate that Auto-WEKA is effective at optimizing its given objective function; however, this is not sufficient to allow us to conclude that it fits models that generalize well. As the hypothesis space of a machine learning algorithm grows, so does its potential for overfitting. The use of cross-validation substantially increases Auto-WEKA's robustness against overfitting, but 6 For the Amazon data set, WEKA's default implementation of support vector machines yielded a very strong error rate of 44%, which was below that of SMAC's median performance. One of SMAC's 25 runs actually reached an error rate of 36%, indicating that Auto-WEKA would be competitive given more time.\nsince its hypothesis space is much larger than that of standard classification algorithms, it is important to carefully study whether (and to what extent) overfitting poses a problem.\nTo evaluate generalization, we determined a combination of algorithm and hyperparameter settings A \u03bb by running Auto-WEKA as before (cross-validating on the training set), trained A \u03bb on the entire training set, and then evaluated the resulting model on the test set. The right portion of Table 4 reports the test performance obtained with all methods. Broadly speaking, similar trends held as for cross-validation performance: random grid search performed better than Ex-Def, and Auto-WEKA in turn outperformed random grid search. However, the performance differences were less pronounced: random grid search only yielded better results than Ex-Def in 9/21 cases, with 6/21 ties and 6/21 cases in which Ex-Def performed better. Auto-WEKA continued to outperform random grid search and Ex-Def in 15/21 cases, with 3 ties and 3 losses. Notably, Auto-WEKA performed best on all of the 11 largest datasets; we attribute this to the fact that the risk of overfitting decreases with dataset size. Sometimes, Auto-WEKA's performance improvements over the other methods were substantial, with relative reductions of the test error rate exceeding 15% in 5/21 cases. Comparing the different Auto-WEKA variants, SMAC outperformed TPE on 12 datasets and tied on 3, with TPE performing better on 6. Even when compared to the unrealistic \"oracle best\" Random Grid classifier (which has access to the test set!), Auto-WEKA found algorithms/hyperparameters with a smaller error on 9/21 datasets, tied on 2, and was outperformed on the remaining 10. For the 10 largest datasets, it performed better in 8 cases, tied in 1, and only lost in 1.\nAs mentioned in our experimental setup, Auto-WEKA only used 70% of its training set during the optimization of cross-validation performance, reserving the remaining 30%  for assessing the risk of overfitting. At any point in time Auto-WEKA's SMBO method keeps track of its incumbent (the hyperparameter configuration with the lowest crossvalidation error rate seen so far). After its SMBO method has finished, Auto-WEKA extracts a trajectory of these incumbents from it and computes their generalization performance on the withheld 30% validation data. It then computes the Spearman rank coefficient between the sequence of training performances (evaluated by the SMBO method through cross-validation) and this generalization performance. The rightmost columns in Table 4 (labelled SC) show the average correlation coefficient for each run of Auto-WEKA. We note a general trend: as the absolute gap between cross-validation and test performance grows, this correlation coefficient decreases. The GermanCredit dataset is a good example where Auto-WEKA can signal that it only has low confidence in how well its chosen hyperparameters will generalize. We do note, however, that this weak signal has to be used with caution: there is no guarantee that large correlation coefficients yield a small gap and vice versa.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Classifiers Selected by Auto-WEKA", "text": "Figure 2 shows the distribution of classifiers chosen by our two Auto-WEKA variants (aggregated across runs and datasets -both TPE and SMAC produced similar results when considered individually). We note that no single classifier clearly dominated the others: the most frequently used classifiers (random forests, the single layer perceptron, and SVMs) were only selected in roughly 12% of all cases each, and most classifiers were selected in at least a few percent of the cases. Furthermore, the selected methods differed considerably between the large and small datasets, demonstrating the need for dataset-specific methods; for example, the large datasets benefitted more from meta methods than the small ones. A more detailed investigation of the top two meta-methods in Figure 3 (left) shows which base methods were chosen. Note that AdaBoostM1 frequently used the single layer perceptron on the small datasets, but never for the large ones, while the REP tree was highly popular for the large datasets. In the random subspace, the two most prominent methods were naive Bayes and the decision table. It is interesting to note that these two methods, as well as the REP tree frequently selected by AdaBoost, were not often  selected as a base classifier on their own. This underlines the importance of searching Auto-WEKA's entire parameter space instead of, e.g., restricting one's attention to a small number of favourite base classifiers. Figure 3 (right) provides a breakdown of the feature search and evaluation methods Auto-WEKA selected. Overall, it used these feature selection methods more often on the smaller datasets than on the larger ones, and if it did use a feature selection method it clearly favored the ranker method. All feature evaluators were used with roughly the same frequency for small datasets; in contrast, if Auto-WEKA performed feature selection for a large dataset it clearly favored the information gain evaluator. We note that Auto-WEKA's data-dependent choices (based on its internal cross-validation evaluation) allow it to use feature selection as a regularization method for small data sets, while at the same time using all features to construct more complex hypotheses for large datasets.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "CONCLUSION AND FUTURE WORK", "text": "In this work, we have shown that the daunting problem of combined algorithm selection and hyperparameter optimization (short: CASH) can be solved by a practical, fully automated tool. This is made possible by the use of recent Bayesian optimization techniques that iteratively build models of the algorithm/hyperparameter landscape and leverage these models to identify new points in the space that deserve investigation.\nWe built a tool, Auto-WEKA, that utilizes the full range of classification algorithms in WEKA and makes it easy for non-experts to build high-quality classifiers for given application scenarios. An extensive empirical comparison on 21 prominent datasets showed that Auto-WEKA often outperformed standard algorithm selection/hyperparameter optimization methods, especially on large datasets. We empirically compared two different optimizers for searching Auto-WEKA's 786-dimensional parameter space and in the end recommend an Auto-WEKA variant based on the Bayesian optimization method SMAC [14]. We have written a freely-downloadable software package to make", "publication_ref": ["b13"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Auto-WEKA easy for end-users to access; it is available at www.cs.ubc.ca/labs/beta/Projects/autoweka/.\nWe see several promising avenues for future work. First, Auto-WEKA still shows larger improvements in cross-validation performance than on test data, suggesting the investigation of more sophisticated methods for detecting and avoiding overfitting than our simple correlation-based approach. Second, we see potential value in extending our current approach to allow parameter sharing between classifiers used within ensemble methods, likely increasing their chance of being selected by Auto-WEKA. Finally, we could use our approach as an inner loop for training ensembles of machine learning algorithms by iteratively adding algorithms with maximal marginal contribution (this idea is conceptually related to the Hydra approach for constructing algorithm selectors [28]).", "publication_ref": ["b27"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Model selection for the LS-SVM. application to handwriting recognition", "journal": "Pattern Recognition", "year": "2009", "authors": "M Adankon; M Cheriet"}, {"ref_id": "b1", "title": "Gradient-based optimization of hyperparameters", "journal": "Neural Computation", "year": "2000", "authors": "Y Bengio"}, {"ref_id": "b2", "title": "Algorithms for Hyper-Parameter Optimization", "journal": "", "year": "2011", "authors": "J Bergstra; R Bardenet; Y Bengio; B K\u00e9gl"}, {"ref_id": "b3", "title": "Random search for hyperparameter optimization", "journal": "JMLR", "year": "2012", "authors": "J Bergstra; Y Bengio"}, {"ref_id": "b4", "title": "A model selection criterion for classification: Application to HMM topology optimization", "journal": "IEEE", "year": "2003", "authors": "A Biem"}, {"ref_id": "b5", "title": "Model selection and Akaike's information criterion (AIC): The general theory and its analytical extensions", "journal": "Psychometrika", "year": "1987", "authors": "H Bozdogan"}, {"ref_id": "b6", "title": "Ranking learning algorithms: Using IBL and meta-learning on accuracy and time results", "journal": "Machine Learning", "year": "2003", "authors": "P Brazdil; C Soares; J. Da Costa"}, {"ref_id": "b7", "title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "journal": "", "year": "2009", "authors": "E Brochu; V M Cora; N De Freitas"}, {"ref_id": "b8", "title": "Model selection for small sample regression", "journal": "", "year": "2001", "authors": "O Chapelle; V Vapnik; Y Bengio"}, {"ref_id": "b9", "title": "Parallelizing exploration-exploitation tradeoffs with gaussian process bandit optimization", "journal": "", "year": "2012", "authors": "T Desautels; A Krause; J Burdick"}, {"ref_id": "b10", "title": "UCI machine learning repository", "journal": "", "year": "2010", "authors": "A Frank; A Asuncion"}, {"ref_id": "b11", "title": "A novel LS-SVMs hyper-parameter selection based on particle swarm optimization", "journal": "Neurocomputing", "year": "2008", "authors": "X Guo; J Yang; C Wu; C Wang; Y Liang"}, {"ref_id": "b12", "title": "The WEKA data mining software: an update", "journal": "ACM SIGKDD Explorations Newsletter", "year": "2009", "authors": "M Hall; E Frank; G Holmes; B Pfahringer; P Reutemann; I Witten"}, {"ref_id": "b13", "title": "Sequential model-based optimization for general algorithm configuration", "journal": "", "year": "2011", "authors": "F Hutter; H Hoos; K Leyton-Brown"}, {"ref_id": "b14", "title": "ParamILS: an automatic algorithm configuration framework", "journal": "JAIR", "year": "2009", "authors": "F Hutter; H Hoos; K Leyton-Brown; T St\u00fctzle"}, {"ref_id": "b15", "title": "Parallel algorithm configuration", "journal": "", "year": "2012", "authors": "F Hutter; H H Hoos; K Leyton-Brown"}, {"ref_id": "b16", "title": "Efficient global optimization of expensive black box functions", "journal": "Journal of Global Optimization", "year": "1998", "authors": "D R Jones; M Schonlau; W J Welch"}, {"ref_id": "b17", "title": "A study of cross-validation and bootstrap for accuracy estimation and model selection", "journal": "", "year": "1995", "authors": "R Kohavi"}, {"ref_id": "b18", "title": "Learning multiple layers of features from tiny images", "journal": "", "year": "2009", "authors": "A Krizhevsky; G Hinton"}, {"ref_id": "b19", "title": "Hoeffding races: Accelerating model selection search for classification and function approximation", "journal": "", "year": "1994", "authors": "O Maron; A Moore"}, {"ref_id": "b20", "title": "Regression and time series model selection", "journal": "World Scientific", "year": "1998", "authors": "A Mcquarrie; C Tsai"}, {"ref_id": "b21", "title": "", "journal": "PyBrain. JMLR", "year": "2010", "authors": "T Schaul; J Bayer; D Wierstra; Y Sun; M Felder; F Sehnke; T R\u00fcckstie\u00df; J Schmidhuber"}, {"ref_id": "b22", "title": "Global versus local search in constrained optimization of computer models", "journal": "", "year": "1998", "authors": "M Schonlau; W J Welch; D R Jones"}, {"ref_id": "b23", "title": "Opportunity cost in Bayesian optimization", "journal": "", "year": "2011", "authors": "J Snoek; H Larochelle; R Adams"}, {"ref_id": "b24", "title": "Practical bayesian optimization of machine learning algorithms", "journal": "", "year": "2012", "authors": "J Snoek; H Larochelle; R P Adams"}, {"ref_id": "b25", "title": "Gaussian process optimization in the bandit setting: No regret and experimental design", "journal": "", "year": "2010", "authors": "N Srinivas; A Krause; S Kakade; M Seeger"}, {"ref_id": "b26", "title": "Nonlinear regression model generation using hyperparameter optimization", "journal": "Computers & Mathematics with Applications", "year": "2010", "authors": "V Strijov; G Weber"}, {"ref_id": "b27", "title": "Hydra: Automatically configuring algorithms for portfolio-based selection", "journal": "", "year": "2010", "authors": "L Xu; H H Hoos; K Leyton-Brown"}, {"ref_id": "b28", "title": "On model selection consistency of lasso", "journal": "JMLR", "year": "2006-12", "authors": "P Zhao; B Yu"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Auto-WEKA's parameter space. Top: first top level Boolean, concerning Auto-WEKA's classification methods. The triangular items represent a parameter that selects one of the 27 base classifiers, and adds conditional classifier hyperparameters accordingly. Bottom: second top level Boolean, concerning Auto-WEKA's feature selection methods.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Left: distribution of chosen base classifiers for the two most frequently selected meta methods: AdaBoostM1 and random subspace. Right: distribution of chosen feature search and evaluator methods. Both plots are aggregated across TPE and SMAC, ranked on their frequency of being selected; None indicates that no feature selection was performed.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": "ClassifierCategorical NumericBayes Net20Naive Bayes20Naive Bayes Multinomial00Gaussian Process36Linear Regression21Logistic Regression01Single-Layer Perceptron52Stochastic Gradient Descent32SVM46Simple Linear Regression00Simple Logistic Regression21Voted Perceptron12KNN41K-Star21Decision Table40RIPPER31M5 Rules311-R01PART220-R00Decision Stump00C4.5 Decision Tree62Logistic Model Tree52M5 Tree31Random Forest23Random Tree44REP Tree23Locally Weighted Learning  *  AdaBoost M1  *  Additive Regression  *  Attribute Selected  *  Bagging  *  Classification via Regression  *  LogitBoost  *  MultiClass Classifier  *  Random Committee  *  Random Subspace  *3 2 1 2 1 0 4 3 0 00 2 2 0 2 0 4 0 1 2Voting +10Stacking +00"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": "Feature MethodCategorical NumericBest First  *  Greedy Stepwise  *  Ranker  *1 3 01 2 1CFS Subset Eval20Pearson Correlation Eval00Gain Ratio Eval00Info Gain Eval201-R Eval12Principal Components Eval22RELIEF Eval12Symmetrical Uncertainty Eval10"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": ". . .", "figure_data": "is_basetruefalsebaseclassAdaBoostM1BaggingVotingStackingiterations percentage use_resamplingiterations out_of_bag_err percentagecombination_rule (none)num_classes\u22651\u22652\u22655meta_basebase_1base_2. . .base_5feat_seltruefalsefeat_serfeat_evalGreedy StepwiseBest FirstCFS SubsetRELIEFfwd./bkwd. conservative threshold. . .direction non-improving nodes lookup cache. . . missing as separate include locally predictive. . .num neighbours weight by distance ..."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": "Dexter20 00002420180GermanCredit1372700300Dorothea100 00002805345Yeast08101 038446Amazon10 0000491 050450Secom059121 096471Semeion2560101 115478Car6041 209519Madelon500021 820780KR-vs-KP37022 237959Abalone17282 9231 254Wine Quality011113 4251 469Waveform04033 5001 500Gisette5 000024 9002 100Convex078428 00050 000CIFAR-10-Small3 07201010 000 10 000MNIST Basic07841012 000 50 000Rot. MNIST + BI07841012 000 50 000Shuttle90743 500 14 500KDD09-Appentency19040235 000 15 000CIFAR-103 07201050 000 10 000"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": "Oracle Perf. (%)10-Fold C.V. Performance (%)Test Performance (%)SCDatasetEx-Def Best WorstRand. Grid Best WorstEx-Def Rand. GridAuto-WEKA TPE SMACEx-Def Rand. GridAuto-WEKA TPE SMACTPE SMACDexter7.7852.785.0058.3310.207.489.905.488.895.009.447.220.82 0.25GermanCredit26.00 38.0025.00 63.6722.45 22.45 21.4319.5927.33 27.33 27.6728.330.31 0.20Dorothea4.93 99.244.9399.246.036.036.935.526.966.966.966.380.95 0.40Yeast40.00 68.9937.53 68.9939.43 38.87 35.0336.2740.45 40.90 41.12 40.450.36 0.49Amazon28.44 99.3328.44 99.3343.94 43.94 48.4348.3028.44 28.44 37.5637.560.92 0.97Secom7.8714.267.6640.646.256.126.255.348.098.307.877.87-0.10 -0.56Semeion8.1892.456.0892.456.526.526.914.868.188.188.185.030.84 0.73Car0.7729.150.1931.662.711.540.940.710.770.190.000.580.12 0.75Madelon17.05 50.2617.05 51.0325.98 24.26 24.2620.8721.38 20.77 20.77 21.150.44 0.43KR-vs-KP0.3148.960.2151.040.890.700.450.320.310.520.520.310.22 0.32Abalone73.18 84.0472.55 89.2373.33 72.45 72.2071.7673.18 72.79 72.71 73.020.15 0.10Wine Quality36.35 60.9936.08 81.6238.94 37.28 35.9434.7437.51 36.08 33.56 33.700.73 0.85Waveform14.27 68.8014.20 68.8012.73 12.73 12.5711.7114.40 14.40 14.20 14.400.36 0.26Gisette2.5250.912.3850.913.623.273.702.422.812.382.572.240.69 0.79Convex25.96 50.0025.96 50.5728.68 28.50 29.0424.7025.96 26.76 25.45 22.050.98 0.84CIFAR-10-Small65.91 90.0064.54 90.0066.59 65.11 57.9757.7665.91 64.54 56.65 55.930.93 0.80MNIST Basic5.1988.753.7988.755.124.0013.643.645.193.7918.033.561.00 0.87Rot. MNIST + BI63.14 88.8857.28 90.9666.15 59.75 73.0459.6163.14 58.16 69.86 55.840.50 0.95Shuttle0.0138 20.8414 0.0069 20.84140.0328 0.0263 0.0230 0.02300.0138 0.0276 0.0069 0.0069 0.60 0.73KDD09-Appentency1.746.971.6454.081.881.881.881.751.751.771.741.740.89 1.00CIFAR-1064.27 90.0064.27 90.0065.54 65.54 66.6863.2164.27 64.27 64.80 62.390.33 0.69the importance of not only choosing the right algorithm butof also setting its hyperparameters well. However, we notethat this performance of random grid search is based on avery large time budget of an average of 400 CPU hours perdataset (650 CPU hours on average for each of the largedatasets), making it a somewhat unrealistic alternative inpractice. In contrast, Auto-WEKA was only run for 4 \u00d7 30CPU hours per dataset, but still yielded substantially bet-ter performance than random grid search, outperformingit in 20/21 cases (and performing worse in one 6 ). Com-paring the two Auto-WEKA variants, SMAC outperformedTPE in 19/21 cases, with one tie. We note that sometimesAuto-WEKA's performance improvements over the othermethods were substantial, with relative reductions of thecross-validation error rate exceeding 15% in 12/21 cases."}], "formulas": [{"formula_id": "formula_0", "formula_text": "A * \u2208 argmin A\u2208A 1 k k i=1 L(A, D (i) train , D (i) valid ), where L(A, D (i) train , D (i)", "formula_coordinates": [2.0, 53.47, 198.02, 188.15, 43.78]}, {"formula_id": "formula_1", "formula_text": "(1) valid , . . . , D (k)", "formula_coordinates": [2.0, 232.32, 267.38, 53.54, 12.49]}, {"formula_id": "formula_2", "formula_text": "(i) train = D \\ D (i) valid for i = 1, . . . , k. 1", "formula_coordinates": [2.0, 96.86, 280.06, 136.36, 12.48]}, {"formula_id": "formula_3", "formula_text": "\u039b \u2282 \u039b1 \u00d7 \u2022 \u2022 \u2022 \u00d7 \u039bn.", "formula_coordinates": [2.0, 214.17, 398.63, 80.52, 7.92]}, {"formula_id": "formula_4", "formula_text": "\u03bb * \u2208 argmin \u03bb\u2208\u039b 1 k k i=1 L(A \u03bb , D (i) train , D (i) valid ).", "formula_coordinates": [2.0, 84.8, 575.87, 159.18, 29.15]}, {"formula_id": "formula_5", "formula_text": "c = L(A \u03bb , D (i) train , D (i) valid ) 5: H \u2190 H \u222a {(\u03bb, c)} 6:", "formula_coordinates": [2.0, 320.7, 105.72, 157.38, 29.75]}, {"formula_id": "formula_6", "formula_text": "A * \u03bb * \u2208 argmin A (j) \u2208A,\u03bb\u2208\u039b (j) 1 k k i=1 L(A (j) \u03bb , D (i) train , D (i) valid ). (1", "formula_coordinates": [2.0, 328.37, 197.1, 223.62, 29.15]}, {"formula_id": "formula_7", "formula_text": ")", "formula_coordinates": [2.0, 551.99, 207.21, 3.93, 7.92]}, {"formula_id": "formula_8", "formula_text": "\u039b = \u039b (1) \u222a \u2022 \u2022 \u2022 \u222a \u039b (k) \u222a {\u03bbr},", "formula_coordinates": [2.0, 408.63, 250.5, 119.81, 9.67]}, {"formula_id": "formula_9", "formula_text": "Ic min (\u03bb) := max{cmin \u2212 c(\u03bb), 0}.", "formula_coordinates": [2.0, 369.17, 644.92, 134.38, 9.4]}, {"formula_id": "formula_10", "formula_text": "EM L [Ic min (\u03bb)] := c min \u2212\u221e max{cmin \u2212 c, 0} \u2022 pM L (c | \u03bb) dc. (2)", "formula_coordinates": [2.0, 317.55, 684.5, 238.37, 34.55]}, {"formula_id": "formula_11", "formula_text": "EM L [Ic min (\u03bb)] = \u03c3 \u03bb \u2022 [u \u2022 \u03a6(u) + \u03d5(u)],", "formula_coordinates": [3.0, 93.56, 388.33, 159.58, 9.48]}, {"formula_id": "formula_12", "formula_text": "p(\u03bb | c) = (\u03bb), if c < c * . g(\u03bb), if c \u2265 c * . (3)", "formula_coordinates": [3.0, 378.34, 138.75, 177.57, 22.18]}, {"formula_id": "formula_13", "formula_text": "E[Ic min (\u03bb)] \u221d \u03b3 + g(\u03bb) (\u03bb) \u2022 (1 \u2212 \u03b3) \u22121 .", "formula_coordinates": [3.0, 356.38, 316.49, 159.97, 23.64]}], "doi": ""}