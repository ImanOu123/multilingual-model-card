{"title": "Revising with a Backward Glance: Regressions and Skips during Reading as Cognitive Signals for Revision Policies in Incremental Processing", "authors": "Brielen Madureira; Pelin \u00c7elikkol; David Schlangen", "pub_date": "", "abstract": "In NLP, incremental processors produce output in instalments, based on incoming prefixes of the linguistic input. Some tokens trigger revisions, causing edits to the output hypothesis, but little is known about why models revise when they revise. A policy that detects the time steps where revisions should happen can improve efficiency. Still, retrieving a suitable signal to train a revision policy is an open problem, since it is not naturally available in datasets. In this work, we investigate the appropriateness of regressions and skips in human reading eye-tracking data as signals to inform revision policies in incremental sequence labelling. Using generalised mixed-effects models, we find that the probability of regressions and skips by humans can potentially serve as useful predictors for revisions in BiLSTMs and Transformer models, with consistent results for various languages.", "sections": [{"heading": "Introduction", "text": "\"Supreme court plans an attack on independent judiciary, says Labour.\" This was the headline of a news article, 1 which sounds incongruous until one interprets it the way intended. That is a crash blossom, 2 a sentence that becomes ambiguous e.g. due to brevity. The correspondent later revised the headline to remove the ambiguity. You probably had to go back and read that sentence again. Such movement is called regression in the eye-tracking literature, when the eye makes a regressive, as opposed to progressive, saccade while reading a text.\nIn incremental NLP models, partial output hypotheses are built at each time step, based on incoming input prefixes, which renders revisability a desirable property to correct mistakes (Schlangen and Skantze, 2011). This mode takes place in interactive settings that require real-time processing, for instance disfluency detecion or reference resolution in dialogue (Hough and Schlangen, 2015;Kennington and Schlangen, 2017) and simultaneous translation (Cho and Esipova, 2016;Arivazhagan et al., 2020;Sen et al., 2023).\nFigure 1 depicts a constructed example for sequence labelling. For each new token, the model either just extends the current output prefix with a new label, or also edits the output by changing previous labels (here at time steps 3 and 5). Modelling a policy that predicts when revisions should occur is an open research problem, because this signal is not naturally available in the training data (K\u00f6hn, 2018;. Moreover, we currently lack evaluation methods to understand whether the revisions performed by a model are linguistically or cognitively motivated (i.e. being grounded in the linguistic input or resembling cognitive processes) or an idiosyncratic result of its internal processing patterns.\nIn eye-tracking experiments, many measures can be extracted per token while humans read texts (Rayner, 1998). Common data formats include variables representing whether each token, in first-pass reading, was skipped, fixated and left progressively or triggered a regressive eye movement. In Figure 1, the constructed scanpath shows regressions at tokens of and by and skips at one and us. Various theories exist to account for why humans regress (see \u00a73), but the fact that underlying cognitive processes cause the eyes to move forward or backward at each word (or skip it) lends itself as a cognitively motivated token-level signal.\nIn this paper, we bridge the concepts of revisions in incremental sequence labelling and regressions in human eye-tracking reading data. We investigate whether regressions and skips can aid the prediction of revisions in incremental processors, and conclude that eye-tracking measures are a potential cognitively-motivated learning signal to model revision policies.", "publication_ref": ["b47", "b19", "b27", "b10", "b1", "b49", "b29", "b44"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Motivation", "text": "Currently on-trend models like Bi-LSTMs (Schuster and Paliwal, 1997) and Transformers (Vaswani et al., 2017) operate in a non-incremental fashion, relying on the availability of complete input sentences or texts to deliver output. One workaround to employ non-incremental encoders in real-time is applying a restart-incremental interface (Schlangen and Skantze, 2011), enabling outputs to be revised as a by-product of recomputations, as explored by Madureira and Schlangen (2020) and Kahardipraja et al. (2021). Although possible, it forces recomputation from scratch at every new piece of input, which increases the computational load and can become infeasible for long sequences (Kahardipraja et al., 2021). On the other hand, inherently incremental models like RNNs have the disadvantage of not being able to recover from mistakes via revisions (at least their prototypical versions).\nThe sweet spot would be a model that can detect the need to revise. Initiatives in this direction are HEAR (Kaushal et al., 2023), which has a module that predicts the need to restart, and TAPIR , which integrates an RNN with a Transformer-revisor, predicting whether to recompute or to just extend the current output. A difficulty encountered in the latter is how to obtain a ground-truth signal for the revision policy. They derived silver labels from the outputs of another Transformer, which is possibly too model-specific and its linguistic motivation is not explored. HEAR compares partial outputs to the non-incremental gold standard which, however, does not encode locally valid hypotheses (which only future input will rule out) and does not accommodate the fact that the gold standard may differ from its final output, thus penalising the incremental metrics with the model's non-incremental deficits (Baumann et al., 2011;.\nWe usually do not have corpora containing annotation for the incremental hypotheses for input prefixes by humans, only the annotated gold labels for the final output. But there is vast literature using human reading data as a supervision signal in NLP tasks (Barrett and Hollenstein, 2020;Mathias et al., 2021). Inspired by that, we ask ourselves whether a model's revisions coincide with human regressions in eye-tracking reading data. A positive answer would mean that human reading data could help modelling a dedicated policy for revisions (as opposed to naive recomputations or restarts), and would serve as a cognitively motivated yardstick to judge a models' revisions. Among all revisions, some are effective, i.e. they edit the prefix into a better state, with respect to a gold standard or to the final output . Identifying them can contribute to reducing undesired revisions, which cause instability without bringing the advantage of improvement in output quality. Therefore, if human reading behaviour can help perform only effective revisions, the signal is even more useful for incremental processing.", "publication_ref": ["b53", "b47", "b35", "b24", "b24", "b7", "b3", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "Related Literature", "text": "During reading, humans fixate the gaze on some words and make saccades that can be progressive or regressive with respect to the order of the words in the text, so that scanpaths and various measures regarding gaze position, direction and duration can be extracted with eye-tracking devices (Rayner et al., 2012), a technique that is becoming more accessible at scale (Ribeiro et al., 2023).\nResearch based on eye-tracking reading data often rely on the eye-mind hypothesis, which assumes that the eye remains fixated on a word as long as it is being processed (Just and Carpenter, 1980). Various research fields rely on the temporal and spatial dimensions of human reading data. We identify at least three (non-mutually exclusive) uses. A consolidated line of research involves studying human cognition and verifying linguistic theories of sentence processing (e.g. Demberg and Keller (2008) and Shain et al. (2016)). Another field is occupied with understanding to what extent com-putational models like artificial neural networks resemble human cognition in how they process language, for example by estimating their psychometric predictive power (Wilcox et al., 2020;Hollenstein et al., 2021). A relationship commonly investigated is the surprisal of language models versus human reading time (Fernandez Monsalve et al., 2012;Goodkind and Bicknell, 2018;Wilcox et al., 2020). NLP has been incorporating eye-tracking data in recent years (Iida et al., 2013;Tokunaga et al., 2017), with the emerging use of human reading data both as input to enhance NLP models (see Barrett and Hollenstein (2020) and Mathias et al. (2021) for recent surveys) and as a means for their interpretability (Ikhwantri et al., 2023).\nIn this work, the phenomenon of interest is regressions, i.e. eye movements that move backwards in the text and can be shorter or longer-range (Rayner et al., 2012). They are a common topic in psycholinguistics research (Paape et al., 2022(Paape et al., , 2021 and various hypotheses account for their role, such as comprehension or word identification difficulties, low-level visuomotor processes, rereading, memory cues and tools for language processing (see Vitu (2005), Lopopolo et al. (2019) and Booth and Weger (2013) for comprehensive discussions and references). Relevant measures are at which word a regression initiates, at which word it lands, regression path duration (how long the reader remains in past text before progressing to unseen text), and how many regressions are initiated for each word. We can also differentiate between firstpass and subsequent regressions.\nRegressions in NLP Reading data has been used as a source of psycholinguistic information for various NLP tasks. When it comes to regressions, Barrett and S\u00f8gaard (2015a) used eye-movements to predict syntactic categories, an idea further explored in Barrett et al. (2016), who augmented PoStaggers with various gaze features, among which was the number of regressions originating from a word. Barrett and S\u00f8gaard (2015b) used the number of regressions from and to a word as features to predict grammatical functions. The number of total regressions per word was also used as a feature by  for sarcasm understandability prediction. Regression duration, i.e. the total time spent on a word after the first pass over it, was a useful feature for sentence compression proposed by Klerke et al. (2016). Regressions during coreference resolution annotation were investigated by Cheri et al. (2016), who used it to propose a heuristic for pruning candidates in a coreference resolution model. In Hollenstein and Zhang (2019), the total duration of regressions from a word was used as a context feature in named-entity recognition.\nWe draw inspiration from the work by Lopopolo et al. (2019), who hypothesised that backward saccades are involved in online syntactic analysis, in which case regressions should coincide, at least partially, with the edges of the relations computed by a dependency parser. They found a significant effect of the number of left-hand side dependency relations on the number of backward saccades. While the authors were interested at predicting human regressions from a model instantiating a parsing theory, we are conversely interested in using human regressions as a signal to train an NLP model. 3", "publication_ref": ["b45", "b46", "b23", "b12", "b50", "b56", "b17", "b15", "b16", "b56", "b20", "b52", "b3", "b37", "b21", "b45", "b41", "b40", "b55", "b32", "b8", "b4", "b2", "b5", "b28", "b9", "b18", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Method", "text": "To perform the analysis, we use binomial generalised linear mixed models (GLMM) with a logit link function to predict model revisions. Similar to the approach by Lopopolo et al. (2019), for each combination of dataset and NLP model/task, we fit two GLMMs: The baseline model (1) only includes the token position variable as a fixed effect and texts as random effects. Since a model's revisions may vary depending on the word's position in the text, we add token position as a baseline predictor and include texts to account for any variability due to different types of texts. We fit model (2) with the same structure, adding the predictors of regression probability and skipping probability as fixed effects. The binary dependent variable is a token's revise/not-revise label. model revision \u223c token position\n+ (1|text) (1) model revision \u223c token position + p(regression) + p(skip) + (1|text)(2)\nWe use likelihood ratio tests (LRT) between the null and the full models to evaluate the goodness of fit. LRTs are used to compare a baseline model to \n! 0 0 0 - 0 0 - 0 0 - 0 0 - 0 subject 2 ! - 0 - - - ! ! - 0 0 - 0 0 0 ! subject 3 - ! ! - - - - ! - - - 0 - ! 0 ! subject 4 - 0 - - 0 - 0 - - 0 - 0 - 0 ! ! subject 5 - 0 0 - 0 - 0 - - 0 - - 0 0 - ! Figure 2\n: An example of our data structure for a portion of a text in the Provo corpus, processed by a restartincremental Transformer predicting dependency relations. Each token is annotated with the reading variable for each subject (eyes: regressed, 0: not regressed, -: skipped) and the model's decision (r: revised, /: not revised).\na more complex one with more predictors and decide if certain predictors should be included, consequently selecting the model that fits the data better.\nTo infer statistical significance, we obtain p-values using the \u03c7 2 distribution. We do not intend to make claims about why regressions occur. For our purposes, we take at face value that they did occur in the eye-tracking experiments (and when). We are interested in words at which regressions are initiated when they are first read, knowing that, for some reason, the reader went to past input before continuing (as a consequence, we also analyse words that are not fixated in the first pass). Still, the hypothesis that regressions occur due to reanalysis, when humans encounter garden path sentences (Altmann et al., 1992), is at our favour, since revisions represent updates in the current model's interpretation caused by input seen for the first time.", "publication_ref": ["b32", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Data", "text": "In this section, we explain the data structure constructed for the analysis. We then introduce the eye-tracking corpora and the models selected for this study, and discuss how we extract the incremental outputs from non-incremental, pre-trained sequence labelling models. 4 Procedure Our method requires knowing, for each token w in a text, what was the behaviour of the model while performing sequence labelling and of the humans while reading the text. More specifically, we need to know whether the model revised its hypothesis upon processing w and whether humans skipped w, fixated it but moved forward, or fixated it and regressed. We thus construct an annotation mapping tokens to human and model data as illustrated in Figure 2. The texts come from the eye-tracking corpora, from which we also extract the human skips or regressions. The revisions are retrieved by feeding the same texts to the NLP models, prefix by prefix in a restart-incremental fashion, and checking if labels change at each time step.   J\u00e4ger et al., 2020), Provo (Luke andChristianson, 2018) and RastrOS (Vieira, 2020;Leal et al., 2022). Table 1 presents their language and size. The distribution of regressions and skips (per token and per subject) is shown in Figure 3. Although many other corpora exist, we opted to use those that had firstpass regression and first-pass skip measures already available or easy to infer from other measures. For each interest area, 5 we retrieve the label for each subject as follows: If the token was skipped in the first-pass reading, we label it as skipped. Otherwise, we retrieve a variable which is 1 if a first-pass MECO (du) MECO (enl2) Nicenboim (es) PoTeC (de) Provo (en) RastrOS (ptbr) all-r eff-r all-r eff-r all-r eff-r all-r eff-r all-r eff-r all-r eff-r Table 2: % of timesteps that trigger revisions (all-r) and effective revisions (eff-r) for each model and task.\nregression was initiated at that interest area, and 0 otherwise. Although regressions can occur later, we only consider what happens in the first-pass reading to approximate what the model does (revisions happen when a token is integrated for the first time in the sequence). The probabilities are estimated by computing the proportion of regressions and skips per token (excluding subjects with missing data), following existing literature in terms of using average human behaviour as a feature (Barrett et al., 2016;Hollenstein and Zhang, 2019). We checked that they are only moderately (negatively) correlated (\u22120.59 < \u03c1 < \u22120.44). See Appendix for details about the measures and pre-processing.  Models' Revisions We opt to evaluate pretrained sequence labelling models with a restartincremental paradigm. Models were selected according to the availability of languages to match the eye-tracking corpora. We evaluate Stanza's BiLSTM models (Qi et al., 2020) 6 and Explosion's pre-trained multi-task Transformer architectures. 7 These families of models were selected due to the availability of all languages and comparability in terms of similar training data, as both were trained on the Universal Dependencies corpora (de Marneffe et al., 2021). The model checkpoints for each language are listed in Table 3. We extract the incremental outputs for dependency parsing (prediction of the head position and the relation) and POStagging. We also inspected NER, but revisions were extremely sparse in these datasets (possibly due to the genres of the texts), so we did not analyse it further. The same texts from the eye-tracking data are fed to each model, one prefix after another, as illustrated in Figure 1, following previous works (Madureira and Schlangen, 2020;Kahardipraja et al., 2021). At each time step, we extend the input with one interest area (i.e., sometimes it means more than one token). If the output prefix at time t (apart from the recently added label(s), which refer to the last interest area) differs from the output at time t \u2212 1, a revision occurred. If more labels match the final output than in the previous prefix, the revision is effective. The percentage of (effective) revisions over tokens/timesteps is shown in Table 2.   ", "publication_ref": ["b33", "b54", "b31", "b2", "b18", "b42", "b11", "b35", "b24"], "figure_ref": ["fig_2", "fig_0"], "table_ref": ["tab_2", "tab_5"]}, {"heading": "Results", "text": "We summarise the full GLMM results in Table 4 for Provo and MECO-L2 datasets. Due to a large number of experiments, we only present results for the English models in this table; the complete results are in the Appendix. In every (dataset, NLP model, task) combination, the likelihood ratio test between the baseline and full models revealed that the full model, including the two predictors of interest, is a better fit to the data than the baseline model with only token position and text.\nThe token position was a significant predictor of revisions in most models. For the few cases in which it did not significantly affect revisions (i.e., MECO-L2-Transformer-head and BiLSTM-head, MECO-L1-BiLSTM-head, Provo-BiLSTM-pos), we fitted models without this predictor instead.\nWe found that average human gaze patterns, namely the estimated word's regression and skip probability, were significant predictors of revisions. This was a consistent result across all eye-tracking corpora, for the BiLSTM and the Transformer, both for dependency parsing and POS-tagging. On the one hand, human regressions were often positively related to revisions, so that words with a higher regression probability were more likely to be revised by models (MECO-L2-Transformer-pos was the only exception where regression probability negatively affected revisions). Conversely, a word's skip probability decreased the probability of it triggering a revision in most cases (with the exceptions of Potec and Provo-Transformer-pos and Nicenboim-BiLSTM-pos). These relationships are illustrated in Figure 4. The magnitude of the regression coefficient did not follow a general pattern for the tasks, but the skip coefficient was more often larger for the task of predicting the head than for the dependency relation, which was usually larger than for POS-tagging (exceptions to this is RastrOS-Transformer and MECO-L1-BiLSTM).\nIn a further analysis, we repeated the same procedure to predict only the effective revisions and observed the same trend in regression and skip coefficients when predicting effective revisions, in terms of direction and significance, in all experiments. However, the magnitude of the coefficients differed, sometimes being larger in one or the other, which does not allow us to draw general conclusions at this point. The coefficient of token position was, in most cases, smaller in the model that predicts effective revisions. Similarly, in many models the magnitude of the coefficient of skips was larger for models predicting effective revisions.\nTo assess the fit of the model to the data in more detail, we evaluated its predictions by running permutation tests with the null hypothesis that the probabilities assigned to (effective) revisions and to not-revisions are randomly sampled from the same distribution. Besides, we computed the area under the ROC curve in each model. As we can see in Table 5, most of the differences were significant (except for many cases in POS-tagging), but their magnitude was relatively small. The AUC was around 0.7 for all datasets, and in some experiments the models of effective revisions had higher AUC. Examples with considerable improvements are RastrOS-head and Nicenboim-head.\n7 Do models revise when humans regress?\nWe have gathered evidence that there is a relationship between NLP restart-incremental models' revisions and human gaze behaviour in reading, which manifests as the probability of revision at a given token being partially predictable from it being often skipped or triggering regressions, when token position and text are accounted for. Interestingly, the overall findings hold for BiLSTM and Transformers, even though their encoding mechanisms are different, and also for all five languages, despite the eye-tracking data having been collected from different text genres and the readers having performed different tasks (or no additional task beyond reading for comprehension, as in Provo).\nFor this conclusion, we did not rely on any assumptions for the connection between human regressions and incremental models' revisions beyond the analogy of what we factually know: When seeing text areas for the first time, humans made decisions to skip or fixate, and possibly to revisit past text, and at some words, models \"decided\" to revisit past decisions. Some exceptions to the general trend in predicting model revisions occurred in POS-tagging, for which relatively fewer revisions occur (see Table 2). The sparsity of revisions may cause the signal to be harder to model well without more data. For dependency parsing, more revisions are expected, especially because in the beginning of the sentence abs. mean diff AUC all-r eff-r all-r eff-r  the model has to wait until the root is processed to make good predictions. There may also be a difference in processing, since the humans could regress to previous sentences in the text, whereas the NLP models depend on their internal tokenisation and sentence boundary detection.\nThis suggests that eye-tracking measures can be transformed into a useful signal to inform the decision of when to revise in mixed restart-incremental processors, especially when the model's task entails more syntactic tasks with frequent revisions to the input. Still, preliminary investigation of the revision probabilities predicted by the model did not yield a straightforward threshold for binary classification, despite the difference in means being statistically significant. This invites a more detailed extrinsic evaluation, by incorporating the human predictors into a revision controller like TAPIR , and assessing the revisions with the evaluation methods discussed by . One approach is to train an incremental sequence labelling model whose revision policy relies on eye-tracking data as part of the input and comparing its performance against a model without it. Since skips had a negative effect, it may also be possible to use other variables that relate to the probability of a token being skipped, like POS-tags or word frequency and length, as additional input, which are cheaper to obtain. The analysis should also be done with larger datasets and other models and tasks.\nThe usefulness of our findings presupposes the availability of eye-tracking measures during inference on truly unseen data, which is an open problem because such signal is not always available in real time. One possibility is to use pretrained eyetracking models to predict regressions and skips, as in approaches discussed in the literature (Engbert et al., 2005;Deng et al., 2023).\nDown the road, a revision policy should not only detect times to revise, but times to revise effectively, since wrong revisions make the partial outputs less reliable for downstream processors. Our experiments showed that regressions and skips are also good predictors for effective revisions. Identifying ways to filter this more specific signal demands further investigation. An immediate next step is to evaluate the predictions of each model in unseen data for all revisions and for effective revisions.", "publication_ref": ["b14", "b13"], "figure_ref": ["fig_3"], "table_ref": ["tab_6", "tab_8"]}, {"heading": "Conclusion", "text": "Let us conclude with a backward glance to our contribution. We have addressed the open question of whether pre-trained sequence labelling models, when employed incrementally, perform revisions in a similar fashion as humans skip words or make regressive eye movements while reading. We have found a significant effect in all the experiments, supporting the use of human reading data as a cognitive signal to inform revision policies. This is a valuable finding: BiLSTMs and Transformers are bidirectional, trained on full sequences, but if we make them process linguistic input incrementally, their revisions can be partially predicted by human reading behaviour. This is also a step forward towards understanding why these models change hypotheses at some tokens, when only partial prefixes are available.\nBesides advancing the research on eye-trackingaugmented NLP, this study also opens the door to exploring other cognitive perspectives with restartincremental NLP models. We see a potential to go the other direction and investigate to what extent a \"mixed incrementality\" model (architectures relying on an incremental processor with occasional restarts) would capture the patterns of human gaze in reading, and hence function as a model of that. In this case, revisions would serve as predictors of human regressions, with control variables like word frequency, surprisal and word length. Other possibility for future work is to investigate whether other measures, like number of fixations or regressions to a token, are related to the edits per label.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Here we summarise a few known limitations that we have mentioned throughout the text. We have analysed various datasets which differ both in the ways they were collected (the task humans were performing, e.g. only reading or also answering to comprehension questions) as well as the length and genre of the texts. The size of the eye-tracking datasets is, in general, small. Ideally, larger amounts of data are necessary to train a revision policy than what we had available for the analysis. Some preprocessing steps had to be made; in particular, some decisions were necessary on had how to merge tokens and interpret documentation, so that a mapping could be created. This is documented in the Appendix, but alternative ways are also possible. We limited the study to families of pre-trained models and tasks for which all languages were available. There can be a mismatch between the humans having the full text available at any point and the models performing sentence segmentation internally in different ways. For models that are trained on sequence level, it may be better if the human reading is also performed the same way. Further research expanding these aspects is desired. Other models beyond GLLMs, e.g. with non-linearity, may be examined, because the probability of regression is within a narrow range in most of the cases. Using models' revisions to predict human behaviour is also a possible research question which was not addressed in this work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Appendix B Pre-processing Human Data", "text": "We pre-process all datasets to combine the measures into a common format, with one token per row and one column for each subject. If no data was available for a subject, the cell is filled with a NaN value, so that it is later ignored. We partition the measure into three groups: interest areas that were skipped in the first-pass reading (and, consequently, also interest areas that were skipped altogether) are assigned a skipped category (label \u22121). For the remaining interest areas, i.e. those that had a first-pass fixation, we extract either a regressed (label 1) or not regressed (label 0) category. Here we document some necessary decisions. The measures we rely on are documented in Table 6 and the pre-processing scripts are available at https://github.com/briemadu/revreg. For further details about the data collections, please refer to the original publications.\n\u25b7 RastrOS: Participants read paragraphs, one by one in a random order, from journalistic, literary and popular science sources. There was a yes/no comprehension question after 20 of the paragraphs. We get the tokens from the columns Word and IA_LABEL. We solve inconsistencies as follows: if Word contains a comma and IA_LABEL contains a full stop, we use the former (in accordance to personal communication with the author). If there are mismatches in quotation marks, we also use the former. For other inconsistencies (33 tokens), we use the latter. \u25b7 PoTeC: Participants read scientific texts on biology and physics from textbooks. Three multiplechoice comprehension questions were presented after each text in a separate screen. We use the negation of FPF as an auxiliary to detect tokens that were skipped in the first pass. The raw text files do not contain punctuation in a straightforward format. We thus only extract commas, and final sentence punctuation is considered to be always a full stop, except for two cases that we noticed were not end of sentences, so a ; was used. We follow the list of 13 subjects ids (in the original script mergeFixationsWordFeatures.py) that were removed due to poor calibration (according to J\u00e4ger et al. ( 2020)) and exclude them from our sample.\n\u25b7 Provo: Participants read the texts from various sources in a random order, without any additional task. For the tokens, we rely on IA_LABEL, due to inconsistencies in the Word column. Four tokens do not match the raw texts (apparently due to encoding), so we use the text instead of the IA_LABEL. \u25b7 MECO-L1: Wikipedia style texts, each on a separate screen. After each text, there were four yes/no comprehension questions. We could only use the Dutch version, as the other languages had mismatches between the source texts and the interest area column.\n\u25b7 MECO-L2: Texts are from training materials for English tests. Participants answered four yes/no questions after each text. 5 subjects were excluded due to unexplained repetitions. \u25b7 Nicenboim: Participants read stimuli (sentences). True/false statements appeared randomly after half of them. We use the filler sentences (as the others had varying conditions across participants). We use FPRT, assuming it is first-pass reading time, to infer first-pass fixations: if it is NaN, we consider it to be a skip (because otherwise it is always a number higher than 0).", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_10"]}, {"heading": "C Pre-processing Models' Data", "text": "We use off-the-shelf implementations of sequence labelling models. To extract the outputs, we loop over the interest areas for each text in the eyetracking corpus for the corresponding language. At each time step t, a string is created with the interest areas up to position t, joined with a blank space. The models output a list of labels, which we take to be the output prefix for that time step. Due to the internal tokenization, it can happen in a few cases that tokenization changes slightly or that more than one new label is added. We use the number of labels in the previous time step as a reference, all new labels beyond that length are considered an addition and do not affect revisions. A revision happens if the output prefix at time t differs from the output at time t \u2212 1; and it is effective if the number of labels that match the final output labels up to that time step increased. For Stanza BiLSTM, we extract the labels from the attributes upos_, deprel, head. For Explosion's Transformers, we extract the labels from the attributes pos_, dep_, head_i.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Modelling Details", "text": "We fit generalized linear mixed models using the lme4 (Bates et al., 2015)  PoTeC FPReg 1 if a regression was initiated in the first-pass reading of the word, otherwise 0 (sign(RPD exc))\nnegation of FPF 1 if the word was fixated in the first-pass, otherwise 0\nProvo IA_REGRESSION_OUT Whether the current interest area received at least one regression from later interest areas (e.g., later parts of the sentence). 1 if interest area was entered from a higher IA_ID (from the right in English); 0 if not. (...) Note that IA_REGRESSION_OUT only considers first-pass regressions.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "IA_SKIP", "text": "An interest area is considered skipped (i.e., IA_SKIP = 1) if no fixation occurred in first-pass reading.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "RastrOS", "text": "IA_REGRESSION_OUT Whether regression(s) was made from the current interest area to earlier interest areas (e.g., previous parts of the sentence) prior to leaving that interest area in a forward direction. 1 if a saccade exits the current interest area to a lower IA_ID (to the left in English) before a later interest area was fixated; 0 if not. (...) Note that IA_REGRESSION_OUT only considers first-pass regressions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "IA_SKIP", "text": "An interest area is considered skipped (i.e.,IA_SKIP = 1) if no fixation occurred in first-pass reading. computing environment (R Core Team, 2022). All baseline and full models were initially fit with the same structure described in the Methods section. We made changes to the model structure in 6 cases to tackle with convergence issues: Model fits to the Nicenboim-TRF-Pos and Nicenboim-BiLSTM-Pos datasets revealed low text-level variance and random effects were excluded in these datasets in further analyses. Token position was not a significant predictor of model revision in MECOL1-BiLSTM-Head, MECOL2-TRF-Head, MECOL2-BiLSTM-Head, and Provo-BiLSTM-Pos models, thus, we refitted these models without the token positions variable.", "publication_ref": ["b43"], "figure_ref": [], "table_ref": []}, {"heading": "E Detailed Results", "text": "Tables 7 and 8 show all the estimated coefficients, standard errors, z and p-values for all models. Table 9 presents the results of the likelihood ratio tests for the full models in relation to their corresponding null model. All results in the paper have been rounded to to decimal places programatically.    ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_12", "tab_14"]}, {"heading": "Acknowledgements", "text": "We thank Nora Hollenstein for some helpful advice on using eye-tracking measures, as well as the authors of the eye-tracking datasets who replied to our clarification requests. Thanks to Patrick Kahardipraja for initial discussions on using surprisal as a signal for revisions policies. We are also thankful for the valuable feedback and suggestions provided by the anonymous reviewers.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Avoiding the garden path: Eye movements in context", "journal": "Journal of Memory and Language", "year": "1992", "authors": "Alan Gerry Tm Altmann; Yvette Garnham;  Dennis"}, {"ref_id": "b1", "title": "Re-translation versus streaming for simultaneous translation", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Naveen Arivazhagan; Colin Cherry; Wolfgang Macherey; George Foster"}, {"ref_id": "b2", "title": "Weakly supervised part-ofspeech tagging using eye-tracking data", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Maria Barrett; Joachim Bingel; Frank Keller; Anders S\u00f8gaard"}, {"ref_id": "b3", "title": "Sequence labelling and sequence classification with gaze: Novel uses of eye-tracking data for natural language processing", "journal": "Language and Linguistics Compass", "year": "2020", "authors": "Maria Barrett; Nora Hollenstein"}, {"ref_id": "b4", "title": "Reading behavior predicts syntactic categories", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Maria Barrett; Anders S\u00f8gaard"}, {"ref_id": "b5", "title": "Using reading behavior to predict grammatical functions", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Maria Barrett; Anders S\u00f8gaard"}, {"ref_id": "b6", "title": "Fitting linear mixed-effects models using lme4", "journal": "Journal of Statistical Software", "year": "2015", "authors": "Douglas Bates; Martin M\u00e4chler; Ben Bolker; Steve Walker"}, {"ref_id": "b7", "title": "Evaluation and Optimisation of Incremental Processors", "journal": "Dialogue and Discourse", "year": "2011", "authors": "Timo Baumann; Okko Bu\u00df; David Schlangen"}, {"ref_id": "b8", "title": "The function of regressions in reading: Backward eye movements allow rereading", "journal": "Memory & cognition", "year": "2013", "authors": "W Robert; Ulrich W Booth;  Weger"}, {"ref_id": "b9", "title": "Leveraging annotators' gaze behaviour for coreference resolution", "journal": "", "year": "2016", "authors": "Joe Cheri; Abhijit Mishra; Pushpak Bhattacharyya"}, {"ref_id": "b10", "title": "Can neural machine translation do simultaneous translation?", "journal": "", "year": "2016", "authors": "Kyunghyun Cho; Masha Esipova"}, {"ref_id": "b11", "title": "Universal Dependencies", "journal": "Computational Linguistics", "year": "2021", "authors": "Marie-Catherine De Marneffe; Christopher D Manning; Joakim Nivre; Daniel Zeman"}, {"ref_id": "b12", "title": "Data from eyetracking corpora as evidence for theories of syntactic processing complexity", "journal": "Cognition", "year": "2008", "authors": "Vera Demberg; Frank Keller"}, {"ref_id": "b13", "title": "Eyettention: An attention-based dual-sequence model for predicting human scanpaths during reading", "journal": "Proc. ACM Hum.-Comput. Interact", "year": "2023", "authors": "Shuwen Deng; David R Reich; Paul Prasse; Patrick Haller; Tobias Scheffer; Lena A J\u00e4ger"}, {"ref_id": "b14", "title": "Swift: a dynamical model of saccade generation during reading", "journal": "Psychological review", "year": "2005", "authors": "Ralf Engbert; Antje Nuthmann; M Eike; Reinhold Richter;  Kliegl"}, {"ref_id": "b15", "title": "Lexical surprisal as a general predictor of reading time", "journal": "Association for Computational Linguistics", "year": "2012", "authors": "Irene Fernandez Monsalve; Stefan L Frank; Gabriella Vigliocco"}, {"ref_id": "b16", "title": "Predictive power of word surprisal for reading times is a linear function of language model quality", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Adam Goodkind; Klinton Bicknell"}, {"ref_id": "b17", "title": "Multilingual language models predict human reading behavior", "journal": "", "year": "2021", "authors": "Nora Hollenstein; Federico Pirovano; Ce Zhang; Lena J\u00e4ger; Lisa Beinborn"}, {"ref_id": "b18", "title": "Entity recognition at first sight: Improving NER with eye movement information", "journal": "", "year": "2019", "authors": "Nora Hollenstein; Ce Zhang"}, {"ref_id": "b19", "title": "Recurrent Neural Networks for Incremental Disfluency Detection", "journal": "", "year": "2015", "authors": "Julian Hough; David Schlangen"}, {"ref_id": "b20", "title": "Investigation of annotator's behaviour using eyetracking data", "journal": "Association for Computational Linguistics", "year": "2013", "authors": "Ryu Iida; Koh Mitsuda; Takenobu Tokunaga"}, {"ref_id": "b21", "title": "Looking deep in the eyes: Investigating interpretation methods for neural models on reading tasks using human eyemovement behaviour", "journal": "Information Processing and Management", "year": "2023", "authors": "Fariz Ikhwantri; Jan Wira Gotama; Hiroaki Putra; Takenobu Yamada;  Tokunaga"}, {"ref_id": "b22", "title": "Deep eyedentification: Biometric identification using micro-movements of the eye", "journal": "Springer", "year": "2019", "authors": "A Lena; Silvia J\u00e4ger; Paul Makowski; Sascha Prasse; Maximilian Liehr; Tobias Seidler;  Scheffer"}, {"ref_id": "b23", "title": "A theory of reading: from eye fixations to comprehension", "journal": "Psychological review", "year": "1980", "authors": "A Marcel; Patricia A Just;  Carpenter"}, {"ref_id": "b24", "title": "Towards incremental transformers: An empirical analysis of transformer models for incremental NLU", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Patrick Kahardipraja; Brielen Madureira; David Schlangen"}, {"ref_id": "b25", "title": "Tapir: Learning adaptive revision for incremental natural language understanding with a two-pass model", "journal": "", "year": "2023", "authors": "Patrick Kahardipraja; Brielen Madureira; David Schlangen"}, {"ref_id": "b26", "title": "Shyam Upadhyay, and Manaal Faruqui. 2023. Efficient encoders for streaming sequence tagging", "journal": "", "year": "", "authors": "Ayush Kaushal; Aditya Gupta"}, {"ref_id": "b27", "title": "A simple generative model of incremental reference resolution for situated dialogue", "journal": "Computer Speech & Language", "year": "2017", "authors": "Casey Kennington; David Schlangen"}, {"ref_id": "b28", "title": "Improving sentence compression by learning to predict gaze", "journal": "", "year": "2016", "authors": "Sigrid Klerke; Yoav Goldberg; Anders S\u00f8gaard"}, {"ref_id": "b29", "title": "Incremental natural language processing: Challenges, strategies, and evaluation", "journal": "", "year": "2018", "authors": "Arne K\u00f6hn"}, {"ref_id": "b30", "title": "Text reading in english as a second language: Evidence from the multilingual eye-movements corpus. Studies in Second Language Acquisition", "journal": "", "year": "2023", "authors": "Noam Victor Kuperman; Sascha Siegelman; Cengiz Schroeder; Svetlana Acart\u00fcrk; Simona Alexeeva; Raymond Amenta; Rolando Bertram; Marc Bonandrini; Daria Brysbaert;  Chernova"}, {"ref_id": "b31", "title": "Rastros project: Natural language processing contributions to the development of an eye-tracking corpus with predictability norms for brazilian portuguese. Language Resources and Evaluation", "journal": "", "year": "2022", "authors": "Sidney Evaldo Leal; Katerina Lukasova; Maria Teresa Carthery-Goulart; Sandra Maria Alu\u00edsio"}, {"ref_id": "b32", "title": "Dependency parsing with your eyes: Dependency structure predicts eye regressions during reading", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Alessandro Lopopolo; Stefan L Frank"}, {"ref_id": "b33", "title": "The provo corpus: A large eye-tracking corpus with predictability norms", "journal": "", "year": "2018", "authors": "G Steven; Kiel Luke;  Christianson"}, {"ref_id": "b34", "title": "The road to quality is paved with good revisions: A detailed evaluation methodology for revision policies in incremental sequence labelling", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Brielen Madureira; Patrick Kahardipraja; David Schlangen"}, {"ref_id": "b35", "title": "Incremental processing in the age of non-incremental encoders: An empirical assessment of bidirectional models for incremental NLU", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Brielen Madureira; David Schlangen"}, {"ref_id": "b36", "title": "A discriminative model for identifying readers and assessing text comprehension from eye movements", "journal": "Springer", "year": "2018", "authors": "Silvia Makowski; A Lena; Ahmed J\u00e4ger; Niels Abdelwahab; Tobias Landwehr;  Scheffer"}, {"ref_id": "b37", "title": "A survey on using gaze behaviour for natural language processing", "journal": "", "year": "2021", "authors": "Sandeep Mathias; Diptesh Kanojia; Abhijit Mishra; Pushpak Bhattacharyya"}, {"ref_id": "b38", "title": "Predicting readers' sarcasm understandability by modeling gaze behavior", "journal": "", "year": "2016", "authors": "Abhijit Mishra; Diptesh Kanojia; Pushpak Bhattacharyya"}, {"ref_id": "b39", "title": "Working memory differences in long-distance dependency resolution", "journal": "Frontiers in Psychology", "year": "2015", "authors": "Bruno Nicenboim; Shravan Vasishth; Carolina Gattei; Mariano Sigman; Reinhold Kliegl"}, {"ref_id": "b40", "title": "Does local coherence lead to targeted regressions and illusions of grammaticality?", "journal": "Open Mind", "year": "2021", "authors": "Dario Paape; Shravan Vasishth; Ralf Engbert"}, {"ref_id": "b41", "title": "Is reanalysis selective when regressions are consciously controlled?", "journal": "Glossa Psycholinguistics", "year": "2022", "authors": "Dario Paape; Shravan Vasishth; Dario Paape; Shravan Vasishth"}, {"ref_id": "b42", "title": "Stanza: A python natural language processing toolkit for many human languages", "journal": "", "year": "2020", "authors": "Peng Qi; Yuhao Zhang; Yuhui Zhang; Jason Bolton; Christopher D Manning"}, {"ref_id": "b43", "title": "R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing", "journal": "", "year": "2022", "authors": " R Core Team"}, {"ref_id": "b44", "title": "Eye movements in reading and information processing: 20 years of research", "journal": "Psychological bulletin", "year": "1998", "authors": "Keith Rayner"}, {"ref_id": "b45", "title": "Psychology of reading", "journal": "Psychology Press", "year": "2012", "authors": "Keith Rayner; Alexander Pollatsek; Jane Ashby; Charles Clifton"}, {"ref_id": "b46", "title": "Webqamgaze: A multilingual webcam eye-tracking-while-reading dataset", "journal": "", "year": "2023", "authors": "Tiago Ribeiro; Stephanie Brandl; Anders S\u00f8gaard; Nora Hollenstein"}, {"ref_id": "b47", "title": "A General, Abstract Model of Incremental Dialogue Processing", "journal": "Dialogue and Discourse", "year": "2011", "authors": "David Schlangen; Gabriel Skantze"}, {"ref_id": "b48", "title": "Bidirectional recurrent neural networks", "journal": "IEEE transactions on Signal Processing", "year": "1997", "authors": "Mike Schuster; K Kuldip;  Paliwal"}, {"ref_id": "b49", "title": "Self-training reduces flicker in retranslation-based simultaneous translation", "journal": "", "year": "2023", "authors": "Sukanta Sen; Rico Sennrich; Biao Zhang; Barry Haddow"}, {"ref_id": "b50", "title": "Memory access during incremental sentence processing causes reading time latency", "journal": "", "year": "2016", "authors": "Cory Shain; Marten Van Schijndel; Richard Futrell; Edward Gibson; William Schuler"}, {"ref_id": "b51", "title": "Expanding horizons of cross-linguistic research on reading: The multilingual eye-movement corpus (meco)", "journal": "", "year": "2022", "authors": "Noam Siegelman; Sascha Schroeder; Cengiz Acart\u00fcrk; Hee-Don Ahn; Svetlana Alexeeva; Simona Amenta; Raymond Bertram; Rolando Bonandrini; Marc Brysbaert; Daria Chernova"}, {"ref_id": "b52", "title": "An eye-tracking study of named entity annotation", "journal": "", "year": "2017", "authors": "Takenobu Tokunaga; Hitoshi Nishikawa; Tomoya Iwakura"}, {"ref_id": "b53", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b54", "title": "The Brazilian Portuguese eye tracking corpus with a predictability study focusing on lexical and partial prediction", "journal": "", "year": "2020", "authors": "Jo\u00e3o Marcos Munguba Vieira"}, {"ref_id": "b55", "title": "Visual extraction processes and regressive saccades in reading. Cognitive processes in eye guidance", "journal": "", "year": "2005", "authors": "Fran\u00e7oise Vitu"}, {"ref_id": "b56", "title": "On the predictive power of neural language models for human real-time comprehension behavior", "journal": "", "year": "2020", "authors": "Ethan Gotlieb Wilcox; Jon Gauthier; Jennifer Hu; Peng Qian; Roger Levy"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "1Figure 1 :1Figure1: A constructed example of incremental sequence labelling where revisions occur at time steps 3 and 5. If tokens where humans initiate regressions in reading align with tokens that trigger revisions, it can be a cognitive signal to model a revision policy.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Distributions of the probabilities of regression and skips, by token (left) and by subject (right) estimated from the human reading data for each dataset.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: The full GLMM predictions of the revision probability are shown. Each plot presents the predictions for BiLSTM and Transformer models given regression and skip probability in the corresponding dataset. Error bars represent 95% confidence interval.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "", "figure_data": "model/rrr//rrrr/rrr/rsubject 1-"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Human reading eye-tracking corpora.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "BiLSTM deprel 58.45 47.20 60.74 54.52 55.75 50.32 53.56 44.27 60.99 53.70 54.01 46.75 head 65.76 38.32 66.95 38.60 61.31 43.36 67.28 40.37 67.92 39.30 60.34 43.70 pos 12.95 11.52 11.70 10.68 6.32 5.44 17.89 15.51 12.65 11.27 29.19 27.11 Transformer deprel 63.92 52.44 67.97 57.66 48.93 44.37 73.67 56.36 66.68 58.77 52.81 44.23 head 67.55 38.01 69.06 37.21 57.27 41.47 74.56 43.38 69.30 38.46 61.39 42.", "figure_data": "98pos9.82 6.28 7.846.091.901.645.01 4.12 8.09 6.56 9.227.62"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Specification of the pre-trained NLP models.", "figure_data": "estimateSEzpMECO-L2ProvoMECO-L2 Provo MECO-L2 Provo MECO-L2 ProvoBiLSTMdeprel intercept1.29*** 1.22***0.05 0.0524.1824.29<0.001 <0.001p(reg)3.41*** 3.30***0.05 0.0973.3938.56<0.001 <0.001p(skip)-2.80*** -3.68***0.02 0.03-178.47 -133.52<0.001 <0.001position-0.03*** 0.21***0.00 0.01-8.9438.87<0.001 <0.001head intercept1.59*** 1.76***0.06 0.0527.4433.12<0.001 <0.001p(reg)4.32*** 2.18***0.05 0.1081.0521.84<0.001 <0.001p(skip)-3.23*** -4.92***0.02 0.03-193.35 -155.18<0.001 <0.001position-0.40***-0.01-68.85-<0.001posintercept-2.62*** -1.92***0.07 0.08-36.21 -22.77<0.001 <0.001p(reg)1.25*** 1.42***0.05 0.0827.5318.61<0.001 <0.001p(skip)-1.16*** -0.66***0.02 0.04-52.26 -18.63<0.001 <0.001position0.20***-0.00-42.18-<0.001-Transformer deprel intercept1.22*** 1.28***0.09 0.0514.2824.39<0.001 <0.001p(reg)4.39*** 3.26***0.05 0.0982.9134.39<0.001 <0.001p(skip)-2.53*** -3.75***0.02 0.03-154.71 -129.34<0.001 <0.001position0.03*** 0.30***0.00 0.0111.3754.95<0.001 <0.001head intercept1.45*** 1.45***0.08 0.0518.1329.17<0.001 <0.001p(reg)4.40*** 2.27***0.05 0.1082.2423.76<0.001 <0.001p(skip)-2.64*** -4.01***0.02 0.03-160.14 -133.24<0.001 <0.001position-0.37***-0.01-64.92-<0.001posintercept-2.64*** -2.69***0.17 0.14-15.28 -19.71<0.001 <0.001p(reg)-0.62*** 3.00***0.06 0.10-9.4931.11<0.001 <0.001p(skip)-0.77*** 0.80***0.03 0.04-29.3318.07<0.001 <0.001position0.08*** -0.25***0.01 0.0115.56 -30.18<0.001 <0.001"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Overview of the GLMM results, showing the estimated coefficients for each variable and their statistical significance, for the English corpora. See Appendix for the the complete table.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Left block: Absolute difference of sample means in the predictions of the models between time steps with and without revisions. * means p-value < 0.001. Right block: Area Under the ROC Curve when the fitted models' predictions are used for binary classification of revision time steps in the data.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Measures used for each eye-tracking corpus and their definition according to the available documentation.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Overview of all results (part I).", "figure_data": "estimateSEzpall-reff-rall-r eff-rall-reff-rall-reff-r"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Overview of all results (part 2).", "figure_data": "BIC\u03c7 2Dfpall-reff-rall-reff-rall-r eff-rall-reff-rMECO-L1 (du)BiLSTMdeprel83546.5082400.757670.4911010.8422<0.001 <0.001head72210.5771300.38 14407.5918647.4622<0.001 <0.001pos48702.7144738.043086.693257.6322<0.001 <0.001Transformer deprel78422.5984143.929326.019114.9322<0.001 <0.001head73396.7874729.62 11051.1415030.0722<0.001 <0.001pos40292.3730157.89104.20188.2522<0.001 <0.001MECO-L2 (en-l2) BiLSTMdeprel 786910.36 813266.48 80770.8080710.3522<0.001 <0.001head721580.34 719579.16 99023.63 143061.5522<0.001 <0.001pos453665.09 428746.076319.526111.9122<0.001 <0.001Transformer deprel 733070.61 810697.01 71546.5169594.7822<0.001 <0.001head721451.14 700433.46 74786.38 156206.7522<0.001 <0.001pos339878.15 289129.13891.56320.9322<0.001 <0.001Nicenboim (es)BiLSTMdeprel62038.5661705.57 12760.8014065.3522<0.001 <0.001head57808.4647871.65 14703.7828111.0922<0.001 <0.001pos----22<0.001 <0.001Transformer deprel67874.1867156.037930.098486.2422<0.001 <0.001head59810.3850487.63 14257.2325095.6222<0.001 <0.001pos----22<0.001 <0.001Potec (de)BiLSTMdeprel 145892.59 146622.24 15375.9614146.1922<0.001 <0.001head121763.91 123406.25 26247.0735086.7022<0.001 <0.001pos101606.8992595.028237.358481.0522<0.001 <0.001Transformer deprel 119666.13 148027.04 15136.5012789.5122<0.001 <0.001head116103.04 133947.43 16464.6726794.8522<0.001 <0.001pos45668.1439611.90122.3469.3322<0.001 <0.001Provo (en)BiLSTMdeprel 265555.00 274893.44 38215.0339111.6522<0.001 <0.001head235978.14 258861.40 47478.2346669.4622<0.001 <0.001pos166971.65 155560.741373.301651.3622<0.001 <0.001Transformer deprel 252130.08 275598.83 35534.9933376.3822<0.001 <0.001head243848.75 255250.75 34344.1149079.4722<0.001 <0.001pos118204.33 103652.19886.80843.6422<0.001 <0.001RastrOS (pt-br)BiLSTMdeprel 106976.51 106122.58 13333.0314659.6222<0.001 <0.001head99489.2989499.43 16638.7030213.5422<0.001 <0.001pos92050.1687854.00 12436.1413744.6322<0.001 <0.001Transformer deprel 108432.79 106773.67 11382.0413219.4322<0.001 <0.001head99987.6291261.27 15027.1927893.1622<0.001 <0.001pos49397.8344548.61169.15371.7222<0.001 <0.001"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Overview of likelihood ratio tests, showing how each full model compares to the null model.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "+ (1|text) (1) model revision \u223c token position + p(regression) + p(skip) + (1|text)(2)", "formula_coordinates": [3.0, 316.41, 581.57, 208.73, 103.08]}, {"formula_id": "formula_1", "formula_text": "! 0 0 0 - 0 0 - 0 0 - 0 0 - 0 subject 2 ! - 0 - - - ! ! - 0 0 - 0 0 0 ! subject 3 - ! ! - - - - ! - - - 0 - ! 0 ! subject 4 - 0 - - 0 - 0 - - 0 - 0 - 0 ! ! subject 5 - 0 0 - 0 - 0 - - 0 - - 0 0 - ! Figure 2", "formula_coordinates": [4.0, 70.87, 120.77, 432.73, 103.58]}], "doi": "10.18653/v1/2020.iwslt-1.27"}