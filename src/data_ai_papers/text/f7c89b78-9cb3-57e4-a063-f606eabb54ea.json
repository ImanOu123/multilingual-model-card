{"title": "Bayesian Model Selection, the Marginal Likelihood, and Generalization", "authors": "Sanae Lotfi; Pavel Izmailov; Gregory Benton; Micah Goldblum; Andrew Gordon Wilson", "pub_date": "2023-05-02", "abstract": "How do we compare between hypotheses that are entirely consistent with observations? The marginal likelihood (aka Bayesian evidence), which represents the probability of generating our observations from a prior, provides a distinctive approach to this foundational question, automatically encoding Occam's razor. Although it has been observed that the marginal likelihood can overfit and is sensitive to prior assumptions, its limitations for hyperparameter learning and discrete model comparison have not been thoroughly investigated. We first revisit the appealing properties of the marginal likelihood for learning constraints and hypothesis testing. We then highlight the conceptual and practical issues in using the marginal likelihood as a proxy for generalization. Namely, we show how marginal likelihood can be negatively correlated with generalization, with implications for neural architecture search, and can lead to both underfitting and overfitting in hyperparameter learning. We also re-examine the connection between the marginal likelihood and PAC-Bayes bounds and use this connection to further elucidate the shortcomings of the marginal likelihood for model selection. We provide a partial remedy through a conditional marginal likelihood, which we show is more aligned with generalization, and practically valuable for large-scale hyperparameter learning, such as in deep kernel learning.", "sections": [{"heading": "Introduction", "text": "The search for scientific truth is elusive. No matter how consistent a theory may be with all available data, it is always possible to propose an alternative theory that is equally consistent. Moreover, no theory is entirely correct: there will always be missed nuances, or phenomena we have not or cannot measure. To decide between different possible explanations, we heavily rely on a notion of Occam's razor -that the \"simplest\" explanation of data consistent with our observations is most likely to be true. For example, there are alternative theories of gravity to general relativity that are similarly consistent with observations, but general relativity is preferred because of its simplicity and intuitive appeal. Jeffreys (1939), and many follow up works, showed that Occam's razor is not merely an ad-hoc rule of thumb, but a rigorous quantifiable consequence of probability theory. MacKay (2003, Chapter 28) arguably makes this point most clearly. Suppose we observe what appears to be a block behind a tree. If we had x-ray vision, perhaps we would see that there are in fact two blocks of equal height standing next to each other. The two block model can generate many more observations, but as a consequence, has to assign these observations lower probabilities. For what we do observe, the one block hypothesis is significantly more likely (see Figure 1(c)), even if we believe each hypothesis is equally likely before we observe the data. This probability of generating a dataset from a prior model is called the marginal likelihood, or Bayesian evidence. The marginal likelihood is widely applied to hypothesis testing, and model selection, where we wish to know which trained model is most likely to provide the best generalization. Marginal likelihood optimization has also been applied with great success for hyperparameter learning, where it is known as empirical Bayes, often outperforming cross-validation.\nThere is a strong polarization in the way marginal likelihood is treated. Advocates make compelling arguments about its philosophical benefits for hypothesis testing, its ability to learn constraints, and its practical successes, especially in Gaussian process kernel learning -often embracing the marginal likelihood as a nearly all-encompassing solution to model selection (e.g., MacKay, 1992d;Minka, 2001;Rasmussen and Williams, 2006;Wilson et al., 2016a). Critics tend to focus narrowly on its sensitivity to prior assumptions, without appreciating its many strengths (e.g., Domingos, 1999;Gelman, 2011;Gelman et al., 2013). There is a great need for a more comprehensive exposition, clearly demonstrating the limits of the marginal likelihood, while acknowledging its unique strengths, especially given the rise of the marginal likelihood in deep learning.\nRather than focus on a specific feature of the marginal likelihood, such as its sensitivity to the prior in isolation, in this paper we aim to fundamentally re-evaluate whether the marginal likelihood is the right metric for predicting the generalization of trained models, and learning hyperparameters. We argue that it does a good job of prior hypothesis testing, which is exactly aligned with the question it is designed to answer. However, we show that the marginal likelihood is only peripherally related to the question of which model we expect to generalize best after training, with significant implications for its use in model selection and hyperparameter learning.\nWe first highlight the strengths of the marginal likelihood, and its practical successes, in Section 3. We then describe several practical and philosophical issues in using the marginal likelihood for selecting between trained models in Section 4, and present a conditional marginal likelihood as a partial remedy for these issues. We exemplify these abstract considerations throughout the remainder of the paper, with several significant findings. We show that the marginal likelihood can lead to both underfitting and overfitting in data space, explaining the fundamental mechanisms behind each. In Section 5, we discuss practical approximations of the marginal likelihood, given its intractability in the general case. In particular, we discuss the Laplace approximation used for neural architecture search, the variational ELBO, and sampling-based approaches. We then highlight the advantages of these approximations, and how their drawbacks affect the relationship between the marginal likelihood and generalization. In Section 6, we re-examine the relationship between the marginal likelihood and training efficiency, where we show that a conditional marginal likelihood, unlike the marginal likelihood, is correlated with generalization for a range of datasizes. In Section 7, we demonstrate that the marginal likelihood can be negatively correlated with the generalization of trained neural network architectures. In Section 8, we show that the conditional marginal likelihood provides particularly promising performance for deep kernel hyperparameter learning. In Section 9, we revisit the connection between the marginal likelihood and PAC-Bayes generalization bounds in theory and practice. We show that while such intuitive and formal connection exists, it does not imply that the marginal likelihood should be used for hyperparameter tuning or model selection. We also use this connection to understand the pitfalls of the marginal likelihood from a different angle. We make our code available here.\nThis paper extends a shorter version of this work, particularly with additional discussion and experiments regarding approximate inference, PAC-Bayes, and neural architecture search.", "publication_ref": ["b38", "b59", "b65", "b73", "b96", "b11", "b17", "b18"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Related Work", "text": "As as early as Jeffreys (1939), it has been known that the log marginal likelihood (LML) encodes a notion of Occam's razor arising from the principles of probability, providing a foundational approach to hypothesis testing (Good, 1968(Good, , 1977Jaynes, 1979;Gull, 1988;Smith and Spiegelhalter, 1980;Loredo, 1990;Berger and Jeffreys, 1991;Jefferys and Berger, 1991;Kass and Raftery, 1995). In machine learning, Bayesian model selection was developed and popularized by the pioneering works of David MacKay (MacKay, 1992d,c,b,a). These works develop early Bayesian neural networks, and use a Laplace approximation of the LML for neural architecture design, and learning hyperparameters such as weight-decay (MacKay, 1992c(MacKay, , 1995.\nIn addition to the compelling philosophical arguments, the practical success of the marginal likelihood is reason alone to study it closely. For example, LML optimization is now the de facto procedure for kernel learning with Gaussian processes, working much better than other approaches such as standard cross-validation and covariogram fitting, and can be applied in many cases where these standard alternatives are simply intractable (e.g., Rasmussen and Williams, 2006;Wilson, 2014;Lloyd et al., 2014;Wilson et al., 2016a).\nMoreover, in variational inference, the evidence lower bound (ELBO) to the LML is often used for automatically setting hyperparameters (Hoffman et al., 2013;Kingma and Welling, 2013;Kingma et al., 2015;Alemi et al., 2018). Notably, in variational auto-encoders (VAE), the whole decoder network (often, with millions of parameters) is treated as a model hyperparameter and is trained by maximizing the ELBO (Kingma and Welling, 2014).\nRecently, the Laplace approximation (LA) and its use in marginal likelihood model selection has quickly regained popularity in Bayesian deep learning (Kirkpatrick et al., 2017;Ritter et al., 2018;Daxberger et al., 2021;Immer et al., 2021Immer et al., , 2022a. Notably, Immer et al. (2021) use a scalable Laplace approximation of the marginal likelihood to predict which architectures will generalize best, and for automatically setting hyperparameters in deep learning, in the vein of MacKay (1992d), but with much larger networks. MacKay (2003) uses the Laplace approximation to make connections between the marginal likelihood and the minimum description length framework. MacKay (1995) also notes that structural risk minimization (Guyon et al., 1992) has the same scaling behaviour as the marginal likelihood. In recent years, PAC-Bayes (e.g., Alquier, 2021) has provided a popular framework for generalization bounds on stochastic networks (e.g. Dziugaite and Roy, 2017;Zhou et al., 2018;Lotfi et al., 2022). Notably, Germain et al. (2016) derive PAC-Bayes bounds that are tightly connected with the marginal likelihood. We discuss these works in detail in Section 9, where we use the PAC-Bayes bounds to provide further insights into the limitations of the marginal likelihood for model selection and hyperparameter tuning.\nCritiques of the marginal likelihood often note its inability to manage improper priors for hypothesis testing, sensitivity to prior assumptions, lack of uncertainty representation over hyperparameters, and its potential misuse in advocating for models with fewer parameters (e.g., Domingos, 1999;Gelman et al., 2013;Gelman, 2011;Ober et al., 2021). To address such issues, Berger and Pericchi (1996) propose the intrinsic Bayes factor to enable Bayesian hypothesis testing with improper priors. Decomposing the LML into a sum over the data, Fong and Holmes (2020) use a similar measure to help reduce sensitivity to prior assumptions when comparing trained models. Lyle et al. (2020a) also use this decomposition to suggest that LML is connected to training speed. Rasmussen and Ghahramani (2001) additionally note that the LML operates in function space, and can favour models with many parameters, as long as they do not induce a distribution over functions unlikely to generate the data.\nOur work complements the current understanding of the LML, and has many features that distinguish it from prior work: (1) We provide a comprehensive treatment of the strengths and weaknesses of the LML across hypothesis testing, model selection, architecture search, and hyperparameter optimization; (2) While it has been noted that LML model selection can be sensitive to prior specification, we argue that the LML is answering an entirely different question than \"will my trained model provide good generalization?\", even if we have a reasonable prior; (3) We differentiate between LML hypothesis testing of fixed priors, and predicting which trained model will generalize best; (4) We also show that LML optimization can lead to underfitting or overfitting in function space; (5) We show the recent characterization in Lyle et al. (2020a) that \"models which train faster will obtain a higher LML\" is not generally true, and revisit the connection between LML and training efficiency; (6) We show that in modern deep learning, the Laplace LML is not well-suited for architecture search and hyperparameter learning despite its recent use; (7) We study a conditional LML (CLML), related to the metrics in Berger and Pericchi (1996) and Fong and Holmes (2020), but with a different rationale and application. We are the first to consider the CLML for hyperparameter learning, model selection for neural networks, approximate inference, and classification. We also do not consider prior sensitivity a drawback of the LML, but argue instead that the LML is answering a fundamentally different question than whether a trained model provides good generalization, and contrast this setting with hypothesis testing. Compared to cross-validation, the CLML can be more scalable and can be conveniently used to learn thousands of hyperparameters.", "publication_ref": ["b38", "b22", "b21", "b36", "b24", "b81", "b50", "b6", "b37", "b41", "b56", "b57", "b73", "b93", "b49", "b96", "b29", "b42", "b44", "b1", "b43", "b45", "b77", "b31", "b32", "b31", "b59", "b58", "b26", "b2", "b13", "b100", "b51", "b20", "b11", "b18", "b17", "b70", "b5", "b15", "b52", "b74", "b52", "b5", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "The Case for the Marginal Likelihood", "text": "While we are primarily focused on exploring the limitations of the marginal likelihood, we emphasize that the marginal likelihood distinctively addresses foundational questions in hypothesis testing and constraint learning. By encoding a notion of Occam's razor, the marginal likelihood can outperform cross-validation, without intervention and using training data alone. Since we can directly take gradients of the marginal likelihood with respect to hyperparameters on the training data, it can also be applied where standard cross-validation cannot, for computational reasons.\nDefinition. The marginal likelihood is the probability that we would generate a dataset D with a model M if we randomly sample from a prior over its parameters p(w): p(D|M) = p(D|M, w)p(w|M)dw.\n(1)\nIt is named the marginal likelihood, because it is a likelihood formed from marginalizing parameters w. It is also known as the Bayesian evidence. Maximizing the marginal likelihood is sometimes referred to as empirical Bayes, type-II maximum likelihood estimation, or maximizing the evidence. We can also decompose the marginal likelihood as\np(D|M) = n i p(D i |D <i , M),(2)\nwhere it can equivalently be understood as how good the model is at predicting each data point in sequence given every data point before it.\nOccam factors. In the definition of the marginal likelihood in Eq. (1), the argument of the integral is the posterior p(w|D, M) up to a constant of proportionality. If we assume the posterior is relatively concentrated around\u0175 = argmax w p(w|D, M), then we can perform a rectangular approximation of the integral, as the height of the posterior times its width, \u03c3 w|D , to find\np(D|M) \u2248 p(D|\u0175, M) \u2022 \u03c3 w|D \u03c3 w ,(3)\nwhere p(D|\u0175, M) is the data fit and \u03c3 w|D \u03c3w is the Occam factor -the width of the posterior over the width of the prior. If the posterior contracts significantly from the prior, there will be a large Occam penalty, leading to a low LML.\nOccam's Razor. The marginal likelihood automatically encapsulates a notion of Occam's razor, as in Figure 1(c). If a model can only generate a small number of datasets, it will generate those datasets with high probability, since the marginal likelihood is a normalized probability density. By the same reasoning, a model which can generate many datasets cannot assign significant probability density to all of them. For a given dataset, the marginal likelihood will automatically favour the most constrained model that is consistent with the data. For example, suppose we have f 1 (x, w) = w 1 x, and f 2 (x, w) = 100 i=1 w i x i , with p(w) = N (0, I) in both cases, and data given by a straight line with a particular slope. Both models have parameters consistent with the data, yet the first model is significantly more likely to generate this dataset from its prior over functions.\nHypothesis Testing. The marginal likelihood provides an elegant mechanism to select between fixed hypotheses, even if each hypothesis is entirely consistent with our observations, and the prior odds of these hypotheses are equal. For example, in the early twentieth century, it was believed that the correct explanation for the irregularities in Mercury's orbit was either an undiscovered planet, orbital debris, or a modification to Newtonian gravity, but not general relativity. Since the predictions of general relativity are unable to explain other possible orbital trajectories, and thus easy to falsify, but consistent with Mercury's The complex model spreads its mass thinly on a broad support, while the appropriate model concentrates its mass on a particular class of problems. The overfit model is a \u03b4-distribution on the target datasetD.\norbit, Jefferys and  show it has a significantly higher marginal likelihood than the alternatives. We emphasize here we are comparing fixed prior hypotheses. We are not interested in how parameters of general relativity update based on orbital data, and then deciding whether the updated general relativity is the correct description of orbital trajectories.\nHyperparameter Learning. In practice, the LML is often used to learn hyperparameters of the prior to find argmax \u03b8 p(D|\u03b8) where p(D|\u03b8) = p(D|w)p(w|\u03b8)dw. Gaussian processes (GPs) provide a particularly compelling demonstration of LML hyperparameter learning. The LML does not prefer a small RBF length-scale that would optimize the data fit. Instead, as we show empirically in Figure 22 (Appendix), the LML chooses a value that would make the distribution over functions likely to generate the training data. We note that the LML can be used to learn many such kernel parameters (Rasmussen and Williams, 2006;Wilson and Adams, 2013;Wilson et al., 2016a). Since we can take gradients of the LML with respect to these hypers using only training data, the LML can also be used where cross-validation would suffer from a curse of dimensionality.\nConstraint Learning. Typical learning objectives like maximum likelihood are never incentivized to select for constraints, because a constrained model will be a special case of a more flexible model that is more free to increase likelihood. The LML, on the other hand, can provide a consistent estimator for such constraints, automatically selecting the most constrained solution that fits the data, and collapsing to the true value of the constraint in the limit of infinite observations, from training data alone. Bayesian PCA is a clear example of LML constraint learning (Minka, 2001). Suppose the data are generated from a linear subspace, plus noise. While maximum likelihood always selects for the largest possible subspace dimensionality, and cross-validation tends to be cumbersome and inaccurate, the LML provides a consistent and practically effective estimator for the true dimensionality. Another clear example is in automatically learning symmetries, such as rotation invariance (van der Wilk et al., 2018;Immer et al., 2022a).", "publication_ref": ["b73", "b94", "b96", "b65", "b87", "b32"], "figure_ref": ["fig_0", "fig_1"], "table_ref": []}, {"heading": "Pitfalls of the Marginal Likelihood", "text": "We now discuss general conceptual challenges in working with the marginal likelihood, and present the conditional marginal likelihood as a partial remedy. The remainder of this paper concretely exemplifies each of these challenges.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Marginal Likelihood is not Generalization", "text": "The marginal likelihood answers the question \"what is the probability that a prior model generated the training data?\". This question is subtly different from asking \"how likely is the posterior, conditioned on the training data, to have generated withheld points drawn from the same distribution?\". Although the marginal likelihood is often used as a proxy for generalization (e.g. MacKay, 1992d;Immer et al., 2021;Daxberger et al., 2021), it is the latter question we wish to answer in deciding whether a model will provide good generalization performance.\nIndeed, if after observing data, prior A leads to posterior B, and prior C leads to posterior D, it can be the case that the same data are less probable under B than D, and also that D provides better generalization on fresh points from the same distribution, even if the prior A explains the data better than C. Consider, for example, the situation where we have a prior over a diffuse set of solutions which are easily identifiable from the data. We will then observe significant posterior contraction, as many of these solutions provide poor likelihood. While the marginal likelihood will be poor, the posterior could be perfectly reasonable for making predictions: in the product decomposition of the marginal likelihood in Section 3, the first terms will have low probability density, even if the posterior updates quickly to become a good description of the data. A different prior, which allocates significant mass to moderately consistent solutions, could then give rise to a much higher marginal likelihood, but a posterior which provides poorer generalization. We illustrate this effect in Figure 1(a) and provide concrete examples in Section 6.\nThere are several ways of understanding why the marginal likelihood will be poor in this instance: (1) the diffuse prior is unlikely to generate the data we observe, since it allocates significant mass to generating other datasets; (2) we pay a significant Occam factor penalty, which is the width of the posterior over the width of the prior, in the posterior contraction;\n(3) in the product decomposition of the marginal likelihood in Section 3, the first terms will have low probability density, even if the posterior updates quickly to become a good description of the data.\nModel Selection. In hypothesis testing, our interest is in evaluating priors, whereas in model selection we wish to evaluate posteriors. In other words, in model selection we are not interested in a fixed hypothesis class A corresponding to a prior (such as the theory of general relativity in the example of Section 3), but instead the posterior B that arises when A is combined with data. Marginal likelihood is answering the question most pertinent to hypothesis testing, but is not generally well-aligned with model selection. We provide several examples in Sections 6, 7. Train data is shown with circles, test data is shown with crosses and the shaded region visualizes the 2\u03c3-predictive region of the GP. Given enough flexibility with the prior mean, the marginal likelihood overfits the data, providing poor overconfident predictions outside of the train region.", "publication_ref": ["b59", "b31"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Marginal Likelihood Optimization and Overfitting", "text": "Marginal likelihood optimization for hyperparameter learning, also known as type-II maximum likelihood or empirical Bayes, is a special case of model selection. In this setting, we are typically comparing between many models -often a continuous spectrum of modelscorresponding to different hyperparameter settings. In practice the marginal likelihood can be effective for tuning hyperparameters, as discussed in Section 3. However, marginal likelihood optimization can be prone to both underfitting and overfitting.\nOverfitting by ignoring uncertainty. We can overfit the marginal likelihood, as we can overfit the likelihood. Indeed, a likelihood for one model can always be seen as a marginal likelihood for another model. For example, suppose we include in our search space a prior model concentrated around a severely overfit maximum likelihood solution. Such a model would be \"simple\" in that it is extremely constrained -it can essentially only generate the dataset under consideration -and would thus achieve high marginal likelihood, but would provide poor generalization (Figure 1(c)).\nAs an example, we parameterize the mean function in an RBF GP with a small multilayer perceptron (MLP) and learn the parameters of the MLP by optimizing the LML. We show the results in Figure 2, where the learned mean function overfits the train data, leading to poor and overconfident predictions outside of the train region. We note that the mean of a GP does not appear in the Occam factor of the marginal likelihood. Therefore the Occam factor does not directly influence neural network hyperparameter learning in this instance, which is different from deep kernel learning (Wilson et al., 2016b). We provide additional details in Appendix A.\nWhile it may not appear surprising that we can overfit the marginal likelihood, the narratives relating the marginal likelihood to Occam's razor often give the impression that it is safe to expand our model search, and that by favouring a \"constrained model\", we are protected from conventional overfitting. For example, Iwata and Ghahramani (2017) proposed a model analogous to the example in Figure 2. where a neural network serves as a mean function of a GP and argued that \"since the proposed method is based on Bayesian inference, it can help alleviate overfitting\". Furthermore, MacKay (1992d, Chapter 3.4) argues that if the correlation between the marginal likelihood and generalization is poor for a set of models under consideration, then we should expand our search to find new model structures that can achieve better marginal likelihood. While a mismatch between generalization and marginal likelihood can indicate that we should revisit our modelling assumptions, this advice could easily lead to overfitting.\nUnderfitting in hyperparameter selection. The above example involves overfitting that arises by ignoring uncertainty. The marginal likelihood also has a bias towards underfitting. This bias arises because supporting a good solution could involve also supporting many solutions that are unlikely to generate the training data from the prior. As an example, consider a zero-centred Gaussian prior on a set of parameters, p(w) = N (0, \u03c3 2 I). Now suppose the parameters w that provide the best generalization have large norm, w , but there are several settings of the parameters that provide moderate fits to the data with smaller norms b w . Further suppose that parameters with norms b < w < w provide very poor fits to the data. The marginal likelihood will not favour a large value of \u03c3 that makes w likely under the prior -even though such a value could lead to a posterior with much better generalization, as in Figure 1(b). With more data, the likelihood signal for w will dominate, and the underfitting bias disappears.", "publication_ref": ["b97", "b34"], "figure_ref": ["fig_0", "fig_1", "fig_1", "fig_0"], "table_ref": []}, {"heading": "The Conditional Marginal Likelihood", "text": "Using the product decomposition of the marginal likelihood in Eq. (2), we can write the LML as\nlog p(D|M) = n i=1 log p(D i |D <i , M). (4\n)\nEach term log p(D i |D <i , M) is the predictive log-likelihood of the data point D i under the Bayesian model average after observing the data D <i . The terms for i close to n are clearly indicative of generalization of the model to new test data: we train on the available data, and test on the remaining, unseen data. On the other hand, the terms corresponding to small i have an equally large effect on the marginal likelihood, but may have little to do with generalization. Inspired by the reasoning above, we consider the conditional log marginal likelihood (CLML):\nn i=m log p(D i |D <i , M) = log p(D \u2265m |D <m ),(5)\nwhere m \u2208 {1, . . . , n} is the cut-off number, and D \u2265m is the set of datapoints D m , . . . , D n .\nIn CLML, we simply drop the first m \u2212 1 terms of the LML decomposition, to obtain a metric that is more aligned with generalization. In Appendix B, we provide further details on the CLML, including a permutation-invariant version, and study how performance varies with the choice of m in Figure 24 (Appendix J). We note the CML can be written as p(D m:n |w)p(w|D 1:m\u22121 )dw, and thus can be more easily estimated by Monte Carlo sampling than the LML, since samples from the posterior over m \u2212 1 points will typically have much greater likelihood than samples from the prior (see Section 5.3 for a discussion of sampling-based estimates of the LML).\nVariants of the CLML were considered in Berger and Pericchi (1996) as an intrinsic Bayes factor for handling improper uniform priors in hypothesis testing, and Fong and Holmes (2020) to show a connection with cross-validation and reduce the sensitivity to the prior. Our rationale and applications are different, motivated by understanding how the marginal likelihood can be fundamentally misaligned with generalization. We do not consider prior sensitivity a deficiency of the marginal likelihood, since the marginal likelihood is evaluating the probability the data were generated from the prior. We also are the first to consider the CLML for neural architecture comparison, hyperparameter learning, approximate inference, and transfer learning. We expect the CLML to address the issues we have presented in this section, with the exception of overfitting, since CLML optimization is still fitting to withheld points. For hyperparameter optimization, we expect the CLML to be at its best relative to the LML for small datasets. As in Figure 1(b), the LML suffers because it has to assign mass to parameters that are unlikely to generate the data in order to reach parameters that are likely to generate the data. But as we get more data, the likelihood signal for the good parameters becomes overwhelming, and the marginal likelihood selects a reasonable value. Even for small datasets, the CLML is more free to select parameters that provide good generalization, since it is based on the posterior p(w|D <m ) that is re-centred from the prior, as shown in Figure 1(b).", "publication_ref": ["b5", "b15"], "figure_ref": ["fig_1", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Marginal Likelihood is Not Aligned with Posterior Model Averaging", "text": "In Bayesian inference, we are concerned with the performance of the Bayesian model average (BMA) in which we integrate out the parameters according to the posterior, to form the posterior predictive distribution:\np(d * |D, M) = w p(d * |w)p(w|D)dw,(6)\nwhere d * is a test datapoint. In other words, rather than use a single setting of parameters, we combine the predictions of models corresponding to every setting of parameters, weighting these models by their posterior probabilities.\nThe marginal likelihood, according to its definition in Eq. (1), measures the expected performance of the prior model average on the training data, i.e. it integrates out the parameters according to the prior. It is thus natural to assume that the marginal likelihood is closely connected to the BMA performance. Indeed, MacKay (1992d) argues that \"the evidence is a measure of plausibility of the entire posterior ensemble\". However, the marginal likelihood is not aligned with posterior model averaging. Consider the following representation of the marginal likelihood, using the standard derivation of the evidence lower bound (ELBO, see Section 5. \nwhere KL represents the Kullback-Leibler divergence. From Eq. (7), we can see that the marginal likelihood is in fact more closely related to the average performance (train likelihood) of individual samples from the posterior, and not the Bayesian model average. This discrepancy is practically important for several reasons. First, those using the marginal likelihood will typically be using the posterior predictive for making predictions, and unconcerned with the average performance of individual posterior samples. Second, the discrepancy between these measures is especially pronounced in deep learning.\nFor example, Wilson and Izmailov (2020) argue that Bayesian marginalization is especially useful in flexible models such as Bayesian neural networks, where different parameters, especially across different modes of the posterior, can correspond to functionally different solutions, so that the ensemble of these diverse solutions provides strong performance. However, this functional diversity does not affect the marginal likelihood in Eq. (7), as the marginal likelihood is only concerned with the average performance of a random sample from the posterior and the degree of posterior contraction.\nIn particular, using a prior that only provides support for a single mode of the BNN posterior may significantly affect the BMA performance, as it would limit the functional diversity of the posterior samples, but it would not hurt the marginal likelihood as long as the average training likelihood of a posterior sample within that mode is similar to the average sample likelihood across the full posterior. We provide an illustration of this behaviour in Figure 3, where the marginal likelihood has no preference between a prior that leads to a unimodal posterior, and a prior that leads to a highly multimodal posterior. We discuss this example in more detail in the next section on approximations of the marginal likelihood.", "publication_ref": ["b95"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Marginal Likelihood Approximations", "text": "Outside of a few special cases, such as Gaussian process regression, the marginal likelihood is intractable. Because the marginal likelihood is integrating with respect to the prior, and we thus cannot effectively perform simple Monte Carlo, it is also harder to approximate than the posterior predictive distribution. Moreover, modern neural networks contain millions of parameters, leaving few practical options. In this section, we discuss several strategies for approximating the marginal likelihood, including the Laplace approximation, variational ELBO, and sampling-based methods, and their limitations. For an extensive review of how to compute approximations to the marginal likelihood, see Llorente et al. (2020). As Laplace only captures a single mode, the Laplace estimate of marginal likelihood decreases linearly with \u03b1 while the true marginal likelihood is roughly constant.", "publication_ref": ["b48"], "figure_ref": [], "table_ref": []}, {"heading": "Laplace Approximation", "text": "The Laplace approximation (LA) for model selection in Bayesian neural networks was originally proposed by MacKay (1992d), and has recently seen a resurgence of popularity (e.g. Immer et al., 2021). Moreover, several generalization metrics, such as the Bayesian Information Criterion (BIC), can be derived by further approximating the Laplace approximate marginal likelihood (Bishop, 2006a). The Laplace approximation represents the parameter posterior with a Gaussian distribution centred at a local optimum of the posterior aka the maximum a posteriori (MAP) solution, w MAP , with covariance matrix \u03a3 given by the inverse Hessian at w MAP :\nq(w) = N (w MAP , \u03a3), \u03a3 \u22121 = \u2207 2 w log (p(D|w)p(w)) w=w MAP . (8\n)\nThe covariance matrix \u03a3 captures the sharpness of the posterior. The marginal likelihood estimate (see e.g. Bishop, 2006a) is then given by log p(D|M) \u2248 log p(D|w MAP ) + log p(w\nMAP ) + D 2 log(2\u03c0) + 1 2 log (det \u03a3) ,(9)\nwhere D is the dimension of the parameter vector w. Drawbacks of the Laplace approximation. The actual posterior distribution for a modern neural network is highly multimodal. By representing only a single mode, the Laplace approximation provides a poor representation of the true Occam factor in Eq. (3), which is the posterior volume divided by the prior volume. As a consequence, the Laplace marginal likelihood will overly penalize diffuse priors that capture multiple reasonable parameter settings across different modes. We provide an example in Figure 3.\nWe generate data from x \u223c N (sin(w), 1) with uniform prior w \u223c U [\u2212\u03b1, \u03b1], then estimate the posterior on w and evaluate the marginal likelihood to estimate the parameter \u03b1. The posterior is periodic with a period of 2\u03c0. Consequently, as we increase \u03b1, the marginal likelihood will be roughly constant for \u03b1 > w M AP , where w M AP is the lowest norm maximum a posteriori solution, as the ratio of the posterior volume to the prior volume (Occam factor) is roughly constant in this regime. We visualize the posterior and the true LML in Figure 3. However, the Laplace approximation only captures a single mode of the posterior, and thus greatly underestimates the posterior volume. As a result, the Laplace marginal likelihood estimate decreases linearly with \u03b1. This toy example shows that Laplace can be problematic for tuning the prior scale in Bayesian neural networks, where covering multiple diverse modes is beneficial for generalization.\nThere are several additional drawbacks to the Laplace approximation:\n\u2022 The Laplace approximation is highly local : it only depends on the value and curvature of the unnormalized posterior log-density log(p(D|w)p(w)) at the MAP solution. The curvature at that point may describe the structure of the posterior poorly, even within a single basin of attraction, causing the Laplace approximation to differ dramatically from the true marginal likelihood.\n\u2022 In practice, computing the covariance matrix \u03a3 \u2208 R D\u00d7D in Eq. ( 8) is intractable for large models such as Bayesian neural networks, as it amounts to computing and inverting the D \u00d7 D Hessian matrix of the loss function. Consequently, practical versions of the Laplace approximation construct approximations to \u03a3, e.g. in diagonal (MacKay, 1992c;Kirkpatrick et al., 2017) or block-Kronecker factorized (KFAC) (Ritter et al., 2018;Martens and Grosse, 2015) form. These approximations further separate the Laplace estimates of the marginal likelihood from the true marginal likelihood.\n\u2022 As evident from Eq. ( 9), the Laplace approximation of the marginal likelihood also highly penalizes models with many parameters D, even though such models could be simple (Maddox et al., 2020). Unlike the standard marginal likelihood, which operates purely on the properties of functions, the marginal likelihood is sensitive to the number of parameters, and is not invariant to (non-linear) reparametrization. Recent work aims to help mitigate this issue (Antor\u00e1n et al., 2022).\nIn Sections 6 and 7 we show examples of misalignment between the Laplace marginal likelihood and generalization in large Bayesian neural networks.\nInformation criteria. While the Laplace approximation provides a relatively cheap estimate of the marginal likelihood, it still requires estimating the Hessian of the posterior density, which may be computationally challenging. We can further simplify the approximation in Eq. ( 9) by dropping all the terms which do not scale with the number of datapoints n, arriving at the Bayesian information criterion (BIC) (Schwarz, 1978):\nlog p(D|M) \u2248 log p(D|w MLE ) \u2212 D 2 log n, (10\n)\nwhere n is the number of datapoints in D and w MLE is the maximum likelihood estimate (MLE) of the parameter w, which replaces the MAP solution. For a detailed derivation, see Chapter 4.4.1 of Bishop (2006b). The BIC is cheap and easy to compute, compared to the other approximations of the marginal likelihood. However, it is also a crude approximation, as it removes all information about the model except for the number of parameters D and the value of the maximum likelihood, completely ignoring the prior and the amount of posterior contraction. In practice, BIC tends to be more dominated by the maximum likelihood term than other more faithful marginal likelihood approximations, causing it to prefer overly unconstrained models (e.g., Minka, 2001). Other related information criteria include AIC (Akaike, 1974), DIC (Spiegelhalter et al., 2002), and WAIC (Watanabe and Opper, 2010). For a detailed discussion of these criteria, see e.g., Gelman et al. (2014).", "publication_ref": ["b31", "b7", "b7", "b56", "b45", "b77", "b61", "b60", "b3", "b78", "b8", "b65", "b0", "b88", "b19"], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Variational Inference and ELBO", "text": "In variational inference (VI), the evidence lower bound (ELBO), a lower bound on logmarginal likelihood, is often used for automatically setting hyperparameters (Hoffman et al., 2013;Kingma and Welling, 2013;Kingma et al., 2015;Alemi et al., 2018). In variational auto-encoders (VAE), the whole decoder network (often, with millions of parameters) is treated as a model hyper-parameter and is trained by maximizing the ELBO (Kingma and Welling, 2014).\nThe ELBO is given by\nlog p(D|M) \u2265 E q(w) log p(D|w) data fit \u2212 KL(q(w)||p(w)) complexity penalty ,(11)\nwhere q(w) is an approximate posterior. Note that the ELBO generalizes the decomposition of marginal likelihood in Eq. ( 7): the inequality in Eq. ( 11) becomes an equality if q(w) = p(w|D).\nIn VI for Bayesian neural networks, the posterior is often approximated with a unimodal Gaussian distribution with a diagonal covariance matrix. For a complex model, the ELBO will suffer from some of the same drawbacks described in Section 5.1 and the example in Figure 3. However, the ELBO is not as locally defined as the Laplace approximation, as it takes into account the average performance of the samples from the posterior and not just the local curvature of the posterior at the MAP solution. Consequently, the ELBO can be preferable for models with highly irregular posteriors. Moreover, the ELBO in principle allows for non-Gaussian posterior approximations (e.g. Rezende and Mohamed, 2015), making it more flexible than the Laplace approximation. However, the KL term can be exactly evaluated if the prior p(w) and the approximate posterior q(w) are both Gaussian, and must typically be approximated otherwise. Similarly, the ELBO is in principle invariant to reparametrization, unlike Laplace, but in practice one often works with a parametrization where q(w) and p(w) are Gaussian to retain tractability.\nOn the downside, the ELBO generally requires multiple epochs of gradient-based optimization to find the optimal variational distribution q, while the Laplace approximation can be computed as a simple post-processing step for any pretrained model. Moreover, optimizing the ELBO can generally suffer from the same overfitting behaviour as the marginal likelihood in general (see Section 4.2). Indeed, if we can set the prior p(w) to be highly concentrated on a solution that is overfit to the training data, e.g. by tuning the mean of the prior to fit the data as we did in the example in Figure 2, we can set the posterior to match the prior q(w) = p(w) achieving very low ELBO, without improving generalization.", "publication_ref": ["b29", "b42", "b44", "b1", "b43", "b76"], "figure_ref": ["fig_3", "fig_1"], "table_ref": []}, {"heading": "Sampling-Based Methods", "text": "Another important group of methods for estimating the marginal likelihood are based on sampling. In the likelihood weighting approach, we form a simple Monte Carlo approximation to the integral in Eq. (1):\np(D|M) = E w\u223cp(w) p(D|w) \u2248 1 m m i=1 p(D|w i ), w i \u223c p(w), i = 1, . . . , m.(12)\nWhile Eq. ( 12) provides an unbiased estimate for the marginal likelihood, its variance can be very high. Indeed, for complex models such as Bayesian neural networks, we are unlikely to encounter parameters that are consistent with the data by randomly sampling from the prior with a computationally tractable number of samples. Consequently, we will not achieve a meaningful estimate of the marginal likelihood.\nIn order to reduce the variance of the simple Monte Carlo estimate in Eq. ( 12), we can use importance sampling, where the samples come from a proposal distribution q(w), rather than the prior. Specifically, in the simple importance sampling approach, the marginal likelihood is estimated as\np(D|M) = p(D|w)p(w) q(w) q(w)dw = E w\u223cq(w) p(D|w)p(w) q(w) \u2248 1 m m i=1 p(D|w i )p(w i ) q(w i ) ,(13)\nwhere {w i } m i=1 are sampled from an arbitrary proposal distribution q. In particular, if we use the true posterior as the proposal distribution q(w) = p(w|D) = p(D|w)p(w) p(D|M) , we have\np(D|w i )p(w i ) q(w i ) = p(D|w i )p(w i ) p(w i |D)\n= p(D|M). Generally, we do not have access to the posterior in a closed-form, so we have to use approximations to the posterior in place of the proposal distribution q(w), retaining a high variance of the LML estimate.\nMultiple approaches that aim to reduce the variance of the sampling-based estimates of the marginal likelihood have been developed. Llorente et al. (2020) provide an extensive discussion of many of these methods. Notably, annealed importance sampling (AIS) (Neal, 2001) constructs a sequence of distributions transitioning from the prior to the posterior so that the difference between each consecutive pair of distributions is not very stark.  derive both lower and upper bounds on the marginal likelihood based on AIS, making it possible to guarantee the accuracy of the estimates.\nWhile AIS and related ideas provide an improvement over the simple Monte Carlo in Eq. ( 12), these approaches are still challenging to apply to large models such as Bayesian neural networks. In particular, these methods typically require full gradient evaluations in order to perform Metropolis-Hastings correction steps, and using stochastic gradients is an open problem (Zhang et al., 2021). Moreover, these methods are generally not differentiable, and do not provide an estimate of the gradient of the marginal likelihood with respect to the prior parameters. This limitation prevents the sampling-based methods from being generally used for hyperparameter learning, which is a common practice with the Laplace and variational approximations. Several works attempt to address this limitation (e.g. Tomczak and Turner, 2020; Zhang et al., 2021). However, in general sampling-based approaches are yet to be applied successfully to estimating and optimizing the marginal likelihood in high-dimensional large-scale Bayesian models containing millions of parameters, such as Bayesian neural networks.  1 3 8 0   -1 3 7 9   -1 3 7 8 Test LL  \n\u22122 \u22121 0 1 2 3 x 0.0 0.2 0.4 p(x) M MML : LML=\u221260.8 M 1 \u03c3 2 =10 6 \u00b5=1 , LML=\u221266.9 M 2 \u03c3 2 =0.07 \u00b5=\u22120.4 , LML=\u221266.3 Empirical mean 0 20 40 Number of datapoints, n \u22124 \u22123 \u22122 \u22121 p(D n |D <n ) M MML M 1 M 2 (a) Fixed \u00b5 = 0 (b) LML prefers M 2 (c)", "publication_ref": ["b48", "b69", "b98", "b98"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Training Speed and Learning Curves", "text": "The remainder of this paper will now concretely exemplify and further elucidate many of the conceptual issues we have discussed, regarding the misalignment between the marginal likelihood and generalization.\nHow a model updates based on new information is a crucial factor determining its generalization properties. We will explore this behaviour with learning curves -graphs showing how log p(D n |D <n ) changes as a function of n. The LML can be thought of as the area under the learning curve (Lyle et al., 2020a). We will see that the first few terms in the learning curve corresponding to small n often decide which model is preferred by the LML. These terms are typically maximized by small, inflexible models, biasing the LML towards underfitting. We illustrate this behaviour in Figure 1(a): the marginal likelihood penalizes models with vague priors, even if after observing a few datapoints the posterior collapses, and generalizes well to the remaining datapoints.\nDensity Estimation. Consider the process where x is generated from a Gaussian distribution N (u, 1) and the mean parameter is in turn generated from a Gaussian distribution u \u223c N (\u00b5, \u03c3 2 ). Figure 4(a) shows the LML and the test predictive log likelihood as a function of the prior variance \u03c3 2 . The posterior over u and the predictive distribution are stable above a threshold of the prior variance \u03c3 2 , as the likelihood of the training data constrains the model and outweighs the increasingly weak prior. However, as we increase \u03c3 2 , the training data becomes increasingly unlikely according to the prior, so the marginal likelihood sharply decreases with \u03c3 2 . We provide analytical results in Appendix E.\nA direct consequence of this behaviour is that two models may have the same generalization performance but very different values of the marginal likelihood; or worse, the marginal likelihood might favor a model with a poor generalization performance. We can see this effect in Figure 4 Moreover, we can design a third model, M 2 , with a prior variance 0.07 and prior mean 2 which leads to a poor fit of the data but achieves higher marginal likelihood than M 1 . This simple example illustrates the general point presented in Section 4.1: LML measures the likelihood of the data according to the prior, which can be very different from the generalization performance of the corresponding posterior.\nIn Figure 4(c) we show log p(D n |D <n ) as a function of n, averaged over 100 orderings of the data. We see that M 1 trains faster than M 2 -where the training speed is defined by Lyle et al. (2020a) as \"the number of data points required by a model to form an accurate posterior\" -but achieves a lower LML, contradicting recent claims that \"models which train faster will obtain a higher LML\" (Lyle et al., 2020a). These claims seem to implicitly rely on the assumption that all models start from the same log p(D 1 |M), which is not true in general as we demonstrate in Figure 4(c).\nFourier Model. Consider the Fourier model\nf (x, a, b) = D d=1 a d sin(d \u2022 x) + b d cos(d \u2022 x), where {a d , b d } D d=1\nare the parameters of the model, and D is the order of the model. To generate the data, we use a model of order D = 9. We sample the model parameters\u00e2 d ,b d \u223c N (0, (1/d 2 ) 2 ). We sample 100 data points x \u223c Uniform[0, 1], and compute the corresponding y = f (x,\u00e2,b) + , with noise \u223c N (0, 0.1 2 ). We then compare an order-9 model M 9 and an order-3 M 3 model on this dataset using LML and CLML. For both models, we use the prior p(a d ) = p(b d ) = N (0, 1). Note that the M 9 model includes ground truth, while the M 3 model does not. We show the fit for both models in Figure 4(e) (top and middle). M 9 provides a much better fit of the true function, while M 3 finds an overly simple solution. However, the LML strongly prefers the simpler M 3 model, which achieves a value of 53.8 compared to 28.9 for the model M 9 . We additionally evaluate the CLML using 200 random orders and conditioning on m = 85 datapoints. CLML strongly prefers the flexible M 9 model with a value of 28.9 compared to 11.45 for M 3 .\nWe can understand the behaviour of LML and CLML by examining the decomposition of LML into a sum over data in Eq. (4) and Figure 1(a). In Figure 4(d) we plot the terms log p(D n |D <n ) of the decomposition as a function of n, averaged over 200 orderings of the data. For n > 50 observed datapoints, the more flexible model M 9 achieves a better generalization log-likelihood log p(D n |D <n ). However, for small n the simpler M 3 model achieves better generalization, where the difference between M 3 and M 9 is more pronounced. As a result, LML prefers the simpler M 3 for up to n = 297 datapoints! For n \u2208 [50,296] the LML picks the model with suboptimal generalization performance. We can achieve the best of both worlds with the corrected model M 9c with the parameter prior a d , b d \u223c N (0, (1/d 2 ) 2 ): strong generalization performance both for small and large training dataset sizes n. These results are qualitatively what we expect: for small datasizes, the prior, and thus the LML, are relatively predictive of generalization. For intermediate size data, the first terms in the LML decomposition have a negative effect on how well LML predicts generalization. For asymptotically large data sizes, the first terms have a diminishing effect, and the LML becomes a consistent estimator for the true model if it is contained within its support. For further details, please see Appendix C.\nNeural Networks. We show the rank of 6 different neural network architectures on their BMA test accuracy on CIFAR-10 for different dataset sizes in Figure 10(b) (Appendix). We see that DenseNet121 and GoogLeNet train faster than ResNet-18 and VGG19, but rank worse with more data. In Figure 10(a) (Appendix), we show the correlation of the BMA test log-likelihood with the LML is positive for small datasets and negative for larger datasets, whereas the correlation with the CLML is consistently positive. As above, the LML will asymptotically choose the correct model if it is in the considered options as we increase the datasize, but for these architectures we are nowhere near any regime where these asymptotic properties could be realized. Finally, Figure 10(a) (Appendix) shows that the Laplace LML heavily penalizes the number of parameters, as in Section 5. We provide additional details in Appendix D.\nSummary. In contrast with Lyle et al. (2020a), we find that models that train faster do not necessarily have higher marginal likelihood, or better generalization. Indeed, the opposite can be true: fast training is associated with rapid posterior contraction, which can incur a significant Occam factor penalty (Section 3), because the first few terms in the LML expansion are very negative. We also show that, unlike the LML, the CLML is positively correlated with generalization in both small and large n regimes, and that it is possible for a single model to do well in both regimes.", "publication_ref": ["b52", "b52", "b52", "b52"], "figure_ref": ["fig_0", "fig_5", "fig_5", "fig_5", "fig_5", "fig_5", "fig_0", "fig_5", "fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Model Selection and Architecture Search", "text": "In Section 4.1, we discussed how the marginal likelihood is answering a fundamentally different question than \"will my trained model provide good generalization?\". In model selection and architecture search, we aim to find the model with the best predictive distribution, not the prior most likely to generate the training data. Here, we consider neural architecture selection. In a way, the marginal likelihood for neural architecture search has come full circle: it was the most prominent example of the marginal likelihood in seminal work by MacKay (1992d), and it has seen a resurgence of recent popularity for this purpose with the Laplace approximation (Immer et al., 2021;Daxberger et al., 2021). We investigate the correlation between LML and generalization performance across 25 convolutional (CNN) and residual (ResNet) architectures of varying depth and width on CIFAR-10 and CIFAR-100, following the setup of Immer et al. (2021). See Appendix F for more details. First, we investigate the correlation between the Laplace marginal likelihood and BMA test accuracy, when the prior precision (aka weight decay) \u03bb is fixed. Figure 5(a) shows the results for fixed prior precision \u03bb = 10 2 , 10 \u22121 , and 10 \u22123 . In each panel, we additionally report the Spearman's correlation coefficient \u03c1 (Spearman, 1961) between the model rankings according to the BMA test accuracy and the LML. LML is positively correlated with the BMA test accuracy when the prior precision is high, \u03bb = 10 2 , but the correlation becomes increasingly negative as \u03bb decreases. While the prior precision has little effect on the BMA test accuracy, it has a significant effect on the approximation of the LML values and model ranking! As discussed in Section 4.1, the marginal likelihood heavily penalizes vague priors, especially in large, flexible models. Moreover, as discussed in Section 5, the Laplace approximation is especially sensitive to the prior variance, and the number of parameters in the model. By the same rationale, we expect the conditional marginal likelihood to help alleviate this problem, since it evaluates the likelihood of the data under the posterior, rather than the prior. Moreover, CLML is evaluated in function space rather than in parameter space (see Appendix B for details), and consequently is not sensitive to the number of parameters in the model, unlike the Laplace approximation (Section 5.1). Indeed, in Figure 5 Correlation between the BMA test accuracy and the CLML for CNNs before and after we perform temperature scaling in order to calibrate the models. Calibrating the models significantly improves the correlation between the CLML and the BMA test accuracy, especially for large models.\nCLML exhibits a positive correlation with the generalization performance for both large and small values of the prior precision. In Appendix F, we show that unlike the LML, the CLML is positively correlated with BMA accuracy, BMA log-likelihood, MAP accuracy and MAP log-likelihood across a wide range of prior precision values both on CIFAR-10 and CIFAR-100.\nPrior precision optimization. In Figure 5(c), we show that optimizing the global or layer-wise prior precision leads to a positive correlation between the LML and the BMA test accuracy, following the online procedure in Immer et al. (2021). This optimization selects high-precision priors, leading to a positive correlation between the LML estimate and the test performance. Notably, optimizing a separate prior scale for each layer leads to higher correlation, an observation that was also made in Chapter 3.4 of MacKay (1992d). In particular, if we only optimize a global prior precision, the correlation between the LML and the BMA test accuracy is negative for the CNN models, and we only recover a positive correlation by including the ResNet models.\nOn the effect of calibration. Although the conditional marginal likelihood correlates much better than the marginal likelihood with the BMA accuracy, we notice in Figure 5(b) that large CNN models appear to represent outliers of this trend. To further investigate this behaviour, we plot the BMA test likelihood as a function of the BMA test accuracy in Figure 6(a). We observe in this figure that the largest CNN models have higher accuracy but are poorly calibrated compared to other models. These findings are compatible with the conclusions of Guo et al. (2017) which argue that larger vision models are more miscalibrated than smaller models. Incidentally, while Bayesian methods can help improve calibration (Wilson and Izmailov, 2020), it appears the Laplace approximation is still clearly susceptible to overconfidence with large CNNs. Prompted by this observation, we calibrate these models via temperature scaling (Guo et al., 2017) and find that the correlation between the CLML and BMA test accuracy for these well-calibrated models improves in Figure 6(b).\nTo understand why model calibration is important for the alignment of CLML and BMA test accuracy and likelihood, let us examine the definition of CLML in Eq. (5). The CLML represents the joint likelihood of the held-out datapoints D \u2265m for the model conditioned on the datapoints D <m . In particular, for miscalibrated models the test likelihood is not predictive of accuracy: highly accurate models can achieve poor likelihood due to making overconfident mistakes (Guo et al., 2017).\nMoreover, the test likelihood can also be misaligned with the CLML for miscalibrated models. This difference is caused by the discrepancy between the marginal and joint predictive distributions: the CLML evaluates the joint likelihood of D \u2265m , while the test likelihood only depends on the marginal predictive distribution on each test datapoint. The difference between the marginal and joint predictive likelihoods is discussed in detail in Osband et al. and Wen et al. (2021). We provide further intuition for this discrepancy in Appendix G. In particular, Figure 21 shows that while the CLML and BMA validation loss correlate positively with the BMA test accuracy, the MAP validation loss correlates negatively with the BMA test accuracy. One possible explanation for the difference between the correlation factors for the BMA validation loss in contrast with the MAP validation loss is that the BMA solutions generally tend to be less overconfident and better calibrated than MAP solutions, hence the positive correlation with the BMA test accuracy. This discrepancy between the marginal and joint predictive distributions also explains the difference between the CLML and standard cross-validation.\nEstimating CLML with MCMC. A key practical advantage of the CLML compared to the LML is that we can reasonably estimate the CLML directly with posterior samples produced by MCMC (see also the discussion in Appendix B), which means working in function-space and avoiding some of the drawbacks (such as parameter counting properties) of the Laplace approximation. MCMC is difficult to directly apply to estimate the LML, because simple Monte Carlo integration to compute the LML would require sampling from a typically uninformative prior, leading to a high variance estimate. The CLML, on the other hand, can be viewed as a marginal likelihood formed on a subset of the data using an informative prior, corresponding to a posterior formed with a different subset of the data (Section 4.3).\nIn Figure 7, we use the approximate posterior samples produced by the SGLD method (Welling and Teh, 2011) to estimate the CLML, BMA test accuracy, and log-likelihood. We follow the setup from  and use a cosine annealing learning rate schedule with initial learning rate 10 \u22127 and momentum 0.9. We also remove data augmentation and use posterior temperature T = 1, since data augmentation does not have a clear Bayesian interpretation (Wenzel et al., 2020;Izmailov et al., 2021;Kapoor et al., 2022, e.g.,).\nWe achieve results consistent with our observations using the Laplace estimates of CLML in Figure 5: the CLML is closely aligned with both BMA accuracy and log-likelihood, with the exception of the largest models which are poorly calibrated. These results suggest that the CLML can be estimated efficiently with Monte Carlo methods, which are significantly more accurate than the alternatives such as the Laplace approximation for models with complex posteriors such as Bayesian neural networks (Izmailov et al., 2021).\nSummary. Claims that \"the marginal likelihood can be used to choose between two discrete model alternatives after training\" and that \"we only need to choose the model with a higher LML value\" (Immer et al., 2021) do not hold universally: we see in Figure 5(a) that the marginal likelihood can be negatively correlated with generalization in practice! In Figure 5(c), we have seen that this correlation can be fixed by optimizing the prior precision, but in general there is no recipe for how many prior hyperparameters we should be optimizing to ensure a positive correlation. For example, in Figure 5(c) optimizing the global prior precision leads to a positive correlation for ResNet models but not for CNNs. The CLML on the other hand consistently provides a positive correlation with the generalization performance.", "publication_ref": ["b31", "b31", "b82", "b31", "b25", "b25", "b25", "b90", "b89", "b91", "b35", "b35", "b31"], "figure_ref": ["fig_7", "fig_7", "fig_7", "fig_7", "fig_8", "fig_8", "fig_0", "fig_9", "fig_7", "fig_7", "fig_7", "fig_7"], "table_ref": []}, {"heading": "Hyperparameter Learning", "text": "We want to select hyperparameters that provide the best possible generalization. We have argued that LML optimization is not always aligned with generalization. As in Section 4.2, there are two ways LML optimization can go awry. The first is associated with overfitting through ignoring uncertainty. The second is associated with underfitting as a consequence of needing to support many unreasonable functions. CLML optimization can help address this second issue, but not the first, since it still ignores uncertainty in the hyperparameters.\nWe provide examples of both issues in GP kernel hyperparameter learning. Curiously, overfitting the marginal likelihood through ignoring uncertainty can lead to underfitting in function space, which is not a feature of standard maximum likelihood overfitting. We then demonstrate that the CLML provides a highly practical mechanism for deep kernel hyperparameter learning, significantly improving performance over LML optimization. The performance gains can be explained as a consequence of the second issue, where we accordingly see the biggest performance gains on smaller datasets, as we predict in the discussion in Section 4.2. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Two issues with LML Optimization", "text": "Using Gaussian process (GP) kernel learning, we provide illustrative examples of two conceptually different ways LML optimization can select hyperparameters that provide poor generalization, discussed in Section 4.2.\nIf we severely overfit the GP LML by optimizing with respect to the covariance matrix itself, subject to no constraints, the solution is the empirical covariance of the data, which is degenerate and biased. Figure 8(a) shows RBF kernel learning inherits this bias by over-estimating the length-scale parameter, which pushes the eigenvalues of the covariance matrix closer to the degenerate unconstrained solution. As we observe more data, the RBF kernel becomes increasingly constrained, and the bias disappears (Wilson et al., 2015). This finding is curious in that it shows how ignoring uncertainty in LML can lead to underfitting in data space, since a larger length-scale will lead to a worse fit of the data. This behaviour is not a feature of standard maximum likelihood overfitting, and also not a property of the LML overfitting in the example of Figure 2. But since it is overfitting arising from a lack of uncertainty representation, the CLML suffers from the same issue.\nIn our next experiment, we generate data from a GP with a rational quadratic (RQ) kernel. Figure 8(b) shows that if we overestimate the observation noise, then the LML is completely misaligned with the shape of the test log-likelihood as a function of the \u03b1 hyper-parameter of the RQ kernel, whereas the CLML is still strongly correlated with the test likelihood. We see here the underfitting bias of Figure 1(b), where supporting an \u03b1 of any reasonable size leads to a prior over functions unlikely to generate the training data. In Appendix H, we show that under the ground truth observation noise both LML and CLML provide adequate representations of the test log-likelihood in this instance. Indeed, the CLML is additionally more robust to misspecification than the LML. Test likelihood results (Appendix) are qualitatively similar.\nWe provide further details in Appendix H.", "publication_ref": ["b92"], "figure_ref": ["fig_10", "fig_1", "fig_10", "fig_0"], "table_ref": []}, {"heading": "Deep Kernel Learning", "text": "Deep kernel learning (DKL) (Wilson et al., 2016b) presents a scenario in which a large number of hyperparameters are tuned through marginal likelihood optimization. While it has been noted that DKL can overfit through ignoring hyperparameter uncertainty (Ober et al., 2021), in this section we are primarily concerned with the underfitting described in Section 4.2, where the CLML will lead to improvements.\nHere we showcase CLML optimization as a practical tool in both UCI regression tasks and transfer learning tasks from Patacchiola et al. (2020). In UCI regression tasks, we examine the performance of LML vs CLML in terms of test performance when training with limited amounts of training data. In Figure 9 we see a common trend: when we are restricted to a small number of training examples, LML optimization is outperformed by CLML optimization. As the number of training examples increases, the gap between LML and CLML optimized models closes. We provide further details, with complete results including a comparison of negative log-likelihoods in Appendix I.\nIn transfer learning tasks we are typically concerned with how well our method performs on unseen data, which may be from a different distribution than the training data, rather than how well aligned our prior is to the training data. In Figure 9 we reproduce the Deep Kernel Transfer (DKT) transfer learning experiments from Patacchiola et al. (2020), replacing LML optimization with CLML optimization. In these experiments DKL models are trained on one task with either LML or CLML optimization, and then evaluated on a separate but related task. Figure 4(a), and Table 4 (Appendix), shows a comparison of methods on a transfer learning task in which we train on the Omniglot dataset and test on the EMNIST dataset. In both experiments CLML optimization provides a clear improvement over LML optimization. Figure 9(b), and Table 3 ( ", "publication_ref": ["b70", "b72", "b72"], "figure_ref": ["fig_11", "fig_11", "fig_5", "fig_11"], "table_ref": ["tab_6", "tab_4"]}, {"heading": "The Marginal Likelihood and PAC-Bayes Generalization Bounds", "text": "PAC-Bayes provides a compelling approach for constructing state-of-the-art generalization bounds for deep neural networks (McAllester, 1999;Dziugaite and Roy, 2017;Zhou et al., 2018;Alquier, 2021;Lotfi et al., 2022). Like the marginal likelihood, PAC-Bayes bounds depend on how well the model can fit the data, and the amount of posterior contraction: models where the posterior differs significantly from the prior are heavily penalized (see Section 6). In fact, the similarity between PAC-Bayes and the marginal likelihood can be made precise: Germain et al. (2016) show that for models where the likelihood is bounded, it is possible to derive PAC-Bayes generalization bounds that are monotonically related to the marginal likelihood (McAllester, 1998;Germain et al., 2016). In other words, it is possible to construct formal generalization bounds based on the value of the marginal likelihood, with higher values of marginal likelihood implying stronger guarantees on generalization. This result may initially appear at odds with our argument that the marginal likelihood is not the right tool for predicting generalization. In this section, we reconcile these two observations, and consider to what extent one can take comfort from the connection with PAC-Bayes in using the marginal likelihood for model comparison and hyperparameter tuning.\nIn Section 9.1, we provide a brief introduction to PAC-Bayes bounds and their connection to the marginal likelihood. In Section 9.2, we explain how PAC-Bayes generalization bounds provide insight into the underfitting and overfitting behaviour of the marginal likelihood. In Section 9.3, we discuss the connection between conditional marginal likelihood and data-dependent priors in PAC-Bayes generalization bounds. In Section 9.4, we show that the PAC-Bayes bounds are typically not prescriptive of model construction. In Section 9.5, we show that the connections between state-of-the-art PAC-Bayes bounds in deep learning both to generalization and to marginal likelihood are limited. Finally, we summarize our observations in Section 9.6.", "publication_ref": ["b64", "b13", "b100", "b2", "b51", "b20", "b63", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "PAC-Bayes and its Relation to the Marginal Likelihood", "text": "Generalization bounds are often based on the following idea: if we select the parameters w of a model from a fixed set of possible values \u2126, then the difference between the performance of the model with weights w on the training data and the test data can be bounded by a term that depends on the size of the set \u2126 (see e.g., Chapter 2 of Mohri et al., 2018). In particular, if the set \u2126 of possible parameters is small, and we find a value w \u2208 \u2126 that performs well on the training data, we can expect it to perform well on the test data. At the same time, if the set \u2126 is infinite, then we cannot provide strong guarantees on the test performance.\nPAC-Bayes generalizes this idea: we put a prior distribution p(w) on the possible parameter values, and we provide guarantees for the expected performance of a random sample w \u223c q from an arbitrary posterior distribution q (not necessarily the Bayesian posterior) over the parameters. Then, we can provide non-trivial generalization bounds even if the set of possible parameter values \u2126 is infinite, as long as the distribution q does not differ too much from the prior: if we come up with a distribution q which is similar to a fixed prior, and such that on average samples from this distribution perform well on the training data, we can expect these samples to perform well on test data.\nFormally, suppose we have a model, defined by a likelihood p(d|w), where d are data, and a prior p(w) over the parameters w. Suppose that we are given a dataset of n points d i sampled randomly from the data distribution p D . Let R D (w) denote the risk (average loss) on the training dataset D for the model with weights w, and let R p D (w) denote the true risk, i.e. the expected loss on test datapoints sampled from the same distribution as D:\nR D (w) = 1 n n i=1 (d i , w), R p D (w) = E d * \u223cp D (d * , w),(14)\nfor some loss function . We are especially interested in the case when the loss is given by the negative log-likelihood (d i , w) = \u2212 log p(d i |w). PAC-Bayes bounds are typically structured as a sum of the expected loss (negative log-likelihood) of a posterior sample on the training data and a complexity term which measures the amount of posterior contraction. Here the term \"posterior\" refers to an arbitrary distribution q(w) over the parameters, and not necessarily the Bayesian posterior.\nFor example, the early bound introduced in McAllester (1999) can be written as follows. For any distribution q over the parameters w, with probability at least 1 \u2212 \u03b4 over the training sample D, we can bound the true risk:\nE w\u223cq R p D (w) true risk \u2264 E w\u223cq R D (w) risk on training data + KL(q(w)||p(w)) + log(n/\u03b4) + 2 2n \u2212 1 complexity penalty ,(15)\nwhere KL denotes the Kullback-Leibler divergence. In particular, the complexity term heavily penalizes cases where q(w) differs significantly from the prior p(w), such as when there is significant posterior contraction. Multiple variations of the bound in Eq. ( 15) follow the same general form (Langford and Seeger, 2001;Maurer, 2004;Catoni, 2007;Thiemann et al., 2017).\nAs shown in Section 4.4 and Eq. ( 7), the log marginal likelihood can be written in a similar form: \nwhere the negative log-likelihood plays the role of the loss, and the Bayesian posterior p(w|D) replaces q. Eq. ( 16) is a special case of the ELBO in Eq. ( 11) where the posterior p(w|D) takes place of the variational distribution, in which case the ELBO equals the marginal likelihood.\nEq. ( 15) and ( 16) above formalize the intuitive connection between the marginal likelihood and PAC-Bayes bounds. Indeed, the difference between the log marginal likelihood in Eq. ( 16) and the PAC-Bayes bound in Eq. ( 15) with negative log-likelihood loss for q = p(w|D) is then only in the specific form of complexity penalty: the complexity penalty in the marginal likelihood is KL(p(w|D)||p(w)), while in the PAC-Bayes bound of McAllester (1999) the complexity penalty is KL(p(w|D)||p(w))+log(n/\u03b4)+2 2n\u22121 .\nIn some cases, it is possible to construct PAC-Bayes bounds that explicitly depend on the value of the marginal likelihood. Germain et al. (2016) show that for models with bounded likelihood, the PAC-Bayes bound of Catoni (2007) on the generalization of a sample from the Bayesian posterior is a monotonic function of the marginal likelihood. Specifically, they show that if the log-likelihood is bounded as a \u2264 \u2212 log p(d|w) \u2264 b for all d, w, then with probability at least 1 \u2212 \u03b4 over the dataset D of n datapoints sampled from p D , we have\nE w\u223cp(w|D) E d * \u223cp D [\u2212 log p(d * |w)] \u2264 a + b \u2212 a 1 \u2212 e a\u2212b 1 \u2212 e a \u2022 n p(D|M)\u03b4 . (17\n)\nWe note that Germain et al. (2016) also derive other variants of this bound applicable to some unbounded likelihoods. McAllester (1998) also derives PAC-Bayes bounds which explicitly depend on the marginal likelihood in a different setting.\nNote that the right hand side of the bound in Eq. ( 17) is a monotonic function of the marginal likelihood p(D|M). Eq. (17) implies that model selection based on the value of the marginal likelihood is, in this case, equivalent to model selection based on the value of a PAC-Bayes generalization bound. However, we have observed how the marginal likelihood is in many ways misaligned with generalization. In the following subsections, we reconcile these observations and provide insight into the limitations of marginal likelihood from the perspective of PAC-Bayes bounds.\nWhat do PAC-Bayes bounds guarantee? We note that the PAC-Bayes bounds, e.g., in Eq. (15) and Eq. (17), provide performance guarantees for the expected performance of a random posterior sample. In particular, these guarantees are not concerned with the performance of the Bayesian model average, where the parameters are integrated out. This observation supports our argument in Section 4.4, where we argue that the marginal likelihood targets the average posterior sample rather than the BMA performance. We note that Morningstar et al. (2022) discuss the distinction between targeting the BMA performance and average sample performance from the perspective of the PAC-Bayes and variational inference. In particular, they propose PAC m -Bayes bounds, which explicitly target the predictive performance of the BMA.", "publication_ref": ["b66", "b46", "b62", "b9", "b85", "b20", "b9", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Underfitting, Overfitting, and PAC-Bayes", "text": "In Section 4, we identified underfitting and overfitting as two pitfalls of the marginal likelihood. We now revisit these limitations, but from the perspective of the connections between the marginal likelihood and PAC-Bayes generalization bounds in Section 9.1 and Eq. (17).\nDiffuse Priors and Underfitting. In Section 4, we have seen that model selection based on the marginal likelihood can overly penalize diffuse priors, which can often contract to posteriors that provide good generalization. Penalizing these priors can also lead to underfitting, where overly simple solutions are favoured in preference to a prior that can support much better solutions. While the exact complexity penalties in the marginal likelihood and PAC-Bayes differ, PAC-Bayes will have the same behaviour: for models with diffuse priors, the penalty in the PAC-Bayes bound of Eq. (15), KL(q(w)||p(w)), will necessarily be large even for posteriors q which achieve low values of risk on the training data, including the Bayes posterior p(w|D).\nOverfitting. Above we have seen that both the marginal likelihood and PAC-Bayes bounds can be unreliable for comparing models with poor marginal likelihood. We would expect a model with poor marginal likelihood to have a loose PAC-Bayes bound, and a loose bound simply does not say anything about generalization. On the other hand, it may be tempting to assume, based on the connection with PAC-Bayes, that models with \"good\" marginal likelihood will provide good generalization, since the bounds can monotonically improve with improved marginal likelihood. However, we have seen in Section 4.2 that optimizing the marginal likelihood to learn hyperparameters can lead to overfitting, where models with arbitrarily high marginal likelihood can generalize poorly.\nTo reconcile these observations, we note that we cannot simply optimize PAC-Bayes bounds (e.g., Eq. (15) or Eq. ( 17)) with respect to the prior and expect to have the same guarantees on generalization, as those bounds do not hold simultaneously for all priors p(w).\nIndeed, optimization is a form of model selection, and in order to perform model selection while preserving generalization guarantees, we need to expand the guaranteed generalization error using a union bound, relying on the property that the probability of a union of events is bounded above by the sum of their probabilities. Using this property, we have that a PAC-Bayes generalization bound holds simultaneously for k models under consideration with probability at least 1 \u2212 k\u03b4, where \u03b4 is as described in Eq. (15) for bounding the generalization error of each model individually. Note that even though 1 \u2212 k\u03b4 may be much lower than 1 \u2212 \u03b4, we can choose a very low value of \u03b4, for example by dividing it by k so that the k generalization bounds hold simultaneously with high probability. By decreasing \u03b4 by a factor of k, we obtain a looser bound.\nWe can understand how much looser the bound becomes when we are comparing k models. Noting that the logarithmic term involving \u03b4 from Eq. (15) becomes log( nk \u03b4 ) = log( n \u03b4 )+log(k), we see we pay a cost equivalent to adding exactly log(k) to the KL-divergence KL(q(w)||p(w)) by bounding k models simultaneously if we fix the probability 1 \u2212 \u03b4 with which the bound holds for each individual model simultaneously.\nEven though the logarithm may scale slowly in the number of models we compare, this term accumulates. If we tune real-valued prior hyperparameters using gradient-based optimizers on the marginal likelihood, as is common practice (e.g., MacKay, 1992d;Rasmussen and Williams, 2006;Wilson and Adams, 2013;Hensman et al., 2013;Wilson et al., 2016a;Molchanov et al., 2017;Daxberger et al., 2021;Immer et al., 2021Immer et al., , 2022aSchw\u00f6bel et al., 2022), we are searching over an uncountably infinite continuous space of models, and lose any guarantee that the resulting model which maximizes the marginal likelihood will generalize at all. Does good marginal likelihood imply good generalization? Not in general. The bound in Eq. ( 17) implies that under certain conditions we can provide formal guarantees on generalization based on the value of the marginal likelihood. The specific value of the marginal likelihood required for such guarantees will depend on the model and size of the dataset. However, the bound in Eq. (17) will only hold if we fix the model a priori without looking at the data, and then evaluate the marginal likelihood for this model. In particular, we cannot optimize the prior or model class as suggested for example in MacKay (1992d, Chapter 3.4) to find models with high marginal likelihood and still expect generalization guarantees to hold.", "publication_ref": ["b59", "b73", "b94", "b28", "b96", "b67", "b31", "b32", "b79"], "figure_ref": [], "table_ref": []}, {"heading": "PAC-Bayes Bounds with Data-dependent Priors and the CLML", "text": "We have considered the conditional log marginal likelihood (CLML) as an alternative to the LML. As argued in Section 4.2, the CLML is intuitively more aligned with generalization, and indeed outperforms the LML across several experimental settings in Sections 6, 7 and 8.\nThe conditional log marginal likelihood (CLML) in Eq. ( 5) is equivalent to replacing the prior distribution p(w) with the data-dependent prior p(w|D <m ), and computing the marginal likelihood on the remainder of the training data. The CLML is related to the PAC-Bayes bounds with data-dependent priors in the same way as the marginal likelihood is related to PAC-Bayes bounds with conventional fixed priors:\nE w\u223cp(w|D) E d * \u223cp D [\u2212 log p(d * |w)] \u2264 a + b \u2212 a 1 \u2212 e a\u2212b 1 \u2212 e a \u2022 (p(D \u2265m |D <m , M)\u03b4) 1 n\u2212m+1 . (18)\nThe data-dependent bound of Eq. ( 18) is found by replacing the prior p(w) in Eq. (17) with the data-dependent prior p(w|D <m ) and subtracting the m \u2212 1 datapoints used to form the prior from the size of the observed dataset n.\nData-dependent PAC-Bayes bounds are often tighter than what can be obtained with the optimal data-independent prior (Dziugaite et al., 2021), providing further motivation for the CLML.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "Prescriptions for Model Construction", "text": "While PAC-Bayes generalization bounds can be used to provide precise theoretical guarantees on generalization, they are not typically prescriptive of model construction. In particular, achieving state-of-the-art generalization bounds typically involves severe model compression with pruning, quantization, and restricting the parameters to a subspace of the parameter space (Zhou et al., 2018;Lotfi et al., 2022). These interventions reduce the complexity penalty in Eq. (15) and lead to significantly improved generalization bounds, but they hurt generalization. Conversely, increasing the number of parameters in deep models typically improves generalization, but loosens the PAC-Bayes bounds.\nIt is not uncommon to see claims that the marginal likelihood should be prescriptive of model construction. For example, MacKay (1992d) argues that poor correlation between generalization and marginal likelihood implies we should re-think our model construction to achieve better marginal likelihood. In particular, he observes that there is a weak correlation between marginal likelihood and generalization for networks that have a global prior over all parameters, but both improved marginal likelihood and generalization if we have priors with different variance scales in each layer. In recent work, the marginal likelihood is similarly used for architecture search, and for learning invariances (van der Wilk et al., 2018;Immer et al., 2021Immer et al., , 2022a. However, we have seen many examples of how better marginal likelihood can lead to worse generalization, especially if we expand our model search broadly, or optimize hyperparameters, leading to overfitting (Section 4.2, 6, 7). Moreover, it may be reasonable to design models specifically to encourage posterior contraction -e.g., specify priors over easily identifiable solutions, enabling fast contraction around a desirable solution. Such models would have poor marginal likelihood, but would be explicitly designed for good generalization.\nIn short, neither PAC-Bayes nor the marginal likelihood can be relied upon as a prescriptive guide to model construction. Indeed, more often than not PAC-Bayes prescribes the opposite of what we know to provide good generalization in deep learning, and the bounds can even have a negative correlation with generalization across architectures (Maddox et al., 2020). The connection with PAC-Bayes thus provides the opposite of a reassurance that the marginal likelihood would provide reliable prescriptions.\nHowever, there is work investigating PAC bounds that are more informative about modelling decisions (Lyle et al., 2020b;Lotfi et al., 2022;Behboodi et al., 2022). And it may be reasonable in some cases to cautiously use the marginal likelihood as a guide, as it does provide a consistent estimator for constraints, and may not be at major risk of overfitting if we carefully limit the number of models we are comparing. One should just proceed with extreme care, and not generally read too much into what is prescribed by the marginal likelihood.", "publication_ref": ["b100", "b51", "b59", "b87", "b31", "b32", "b60", "b53", "b51", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Connections in Practice", "text": "The theoretical relationships between the marginal likelihood and PAC-Bayes only precisely hold when the PAC-Bayes bounds use the Bayesian posterior p(w|D). In practice, state-ofthe-art PAC-Bayes bounds in deep learning make use of posteriors that are wildly divergent from the Bayesian posterior. For example, it is standard to use a single Gaussian distribution as q, while the true Bayesian posterior is highly multimodal. Moreover, there is not even an attempt to have this Gaussian approximate the Bayesian posterior, in contrast to approximate Bayesian inference (e.g., Dziugaite and Roy, 2017;Zhou et al., 2018;Jiang et al., 2019;Lotfi et al., 2022). The prior choices for PAC-Bayes are also often chosen somewhat artificially, for the sake of good bounds rather than good generalization. For instance, the first non-vacuous PAC-Bayes bounds for overparameterized neural networks (Dziugaite and Roy, 2017) were computed by choosing the prior distribution to be a multivariate normal in order to obtain the KL divergence in closed form.\nFurthermore, in practice we often approximate the marginal likelihood. Indeed, an approximation is unavoidable for Bayesian neural networks. There are a variety of approximations, with different properties. As we have discussed in Section 5, these approximations tend to further weaken the connection between marginal likelihood and generalization, and would increase the already significant discrepancy between the marginal likelihood and PAC-Bayes in practice.\nMoreover, those working with the marginal likelihood are typically interested in the predictive performance of the posterior weighted model average. However, as we discuss in Sections 4.4 and 9.1 respectively, the marginal likelihood and the PAC-Bayes bounds target the expected generalization of a single sample from the posterior. There are often major discrepancies between these quantities. For example, a multimodal mixture of Gaussians posterior confers significant generalization benefits for Bayesian neural networks, but has a negligible effect on the PAC-Bayes bounds (Wilson and Izmailov, 2020). This result is intuitive: the solutions with different basins of the posterior are similarly good, but complementary. These solutions can therefore be combined to great effect, but if we are only taking one sample, it won't matter so much which basin it is from.\nWhile the marginal likelihood and PAC-Bayes are closely related in theory, the relationship in practice is much less clear. It is also worth emphasizing that although the marginal likelihood and PAC-Bayes share some limitations, the way the marginal likelihood and PAC-Bayes are used in practice are very different. Generally, PAC-Bayes is used more cautiously than the marginal likelihood. It is not assumed, for instance, that PAC-Bayes is prescriptive of model construction, or would be closely aligned with generalization in optimizing for hyperparameters.", "publication_ref": ["b13", "b100", "b39", "b51", "b95"], "figure_ref": [], "table_ref": []}, {"heading": "Summary", "text": "We conclude our discussion of the PAC-Bayes bounds with a summary of the points presented in this section.\n\u2022 There exist both intuitive and formal connections between the marginal likelihood and PAC-Bayes generalization bounds. In particular, Germain et al. (2016) bound the expected test performance of a random posterior sample with a monotonic transformation of the marginal likelihood, under certain conditions.\n\u2022 While the connection between PAC-Bayes bounds and the marginal likelihood exists, it does not justify the use of the marginal likelihood for model selection with the goal of generalization. Indeed, PAC-Bayes bounds are not intended to be used in the ways the marginal likelihood is used for these purposes.\n\u2022 In particular, the PAC-Bayes bounds and the marginal likelihood both heavily penalize diffuse priors leading to the underfitting behaviour: selecting overly simple models to avoid posterior contraction.\n\u2022 Moreover, the PAC-Bayes bounds provide insight into the marginal likelihood overfitting behaviour: in order to perform model selection based on the PAC-Bayes bounds, we need to pay a penalty based on the logarithm of the size of the model class. Consequently, we lose generalization guarantees based on the marginal likelihood if we optimize the hyper-parameters of the prior.\n\u2022 In principle, \"high\" values of the marginal likelihood can provide certain guarantees on generalization, but only in the case when the model is chosen and fixed a priori, without looking at the data.\n\u2022 The state-of-the-art PAC-Bayes bounds in deep learning are not prescriptive for model construction, further suggesting caution in using the marginal likelihood as a guide for model construction.\n\u2022 The LML is related to the PAC-Bayes bounds in the same way as the CLML is related to PAC-Bayes bounds with data-dependent priors, which are known to be significantly tighter. This connection provides additional theoretical motivation for CLML.\n\u2022 Despite their relationship, the LML and PAC-Bayes are focused on different quantities: the LML is computing performance of a Bayesian model average, while PAC-Bayes measures the performance of a single posterior sample.\n\u2022 In practice, the marginal likelihood and PAC-Bayes may only be loosely connected. The formal connection only exists when PAC-Bayes uses the Bayesian posterior, which is not the posterior used in practice. The marginal likelihood is also typically approximated in practice.", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "Discussion", "text": "While the marginal likelihood provides a powerful mechanism for hypothesis testing, and can be practical for hyperparameter tuning, we show that it is in many ways misaligned with generalization. These results are particularly timely in light of recent work proposing the marginal likelihood for model selection and hyperparameter tuning in deep learning. We show that a conditional marginal likelihood retains the convenient properties of the marginal likelihood, but helps resolve the misalignment between the marginal likelihood and generalization, but is still susceptible to overfitting. We find that the conditional marginal likelihood provides particularly compelling performance in learning deep kernel hyperparameters, especially on smaller datasets, and transfer learning problems.\nTo what extent does approximate inference affect our results? As we discuss in Section 5, approximating the marginal likelihood can further weaken its ability to predict generalization. However, almost all of our experiments use the exact LML and CLML: the density model, Fourier features, Gaussian processes, and deep kernel learning. While the neural architecture search experiments in Section 7 necessarily use approximate inference, the results are qualitatively similar to the exact experiments, and these results are also what we expect from Section 4. A key advantage of working with the CLML is that it can be effectively approximated by sampling. However, what we observe about the LML behaviour stands on its own, independently of the CLML.\nHow does the correlation between LML and generalization vary with dataset size n? The relationship between the LML and generalization is non-monotonic with dataset size. For very small datasets, the first (and only) terms in the LML decomposition are typically predictive of generalization. For intermediate datasets, these terms have a negative effect on the correlation with generalization, as the posterior can differ significantly from the prior. Finally, for asymptotically large datasets, the first terms have a diminishing effect, and the LML becomes a consistent estimator for the correct model, assuming it is within the set of considered models. We observe these results empirically in Figure 4(d),\nwhere LML picks the better generalizing model for n < 50 and n > 296. For n in [50,296] it picks the wrong model.\nCan we construct a model which performs well for both small and large n? While we are primarily concerned with model selection, model construction is intimately related. There is a conventional wisdom that one should use small models for small datasets, and large models for large datasets. We show in Figure 4(e) that this prescription is incorrect: we can achieve the best of both worlds, a model which is good in both small and large n regimes, by combining flexibility with reasonable inductive biases, aligned with the discussion in Wilson and Izmailov (2020).\nIs the CLML \"just\" cross-validation? The LML itself is arguably a form of crossvalidation, although it is not standard cross-validation (Fong and Holmes, 2020). The CLML can be significantly more efficient and practical than standard cross-validation for gradient-based learning of many hyperparameters. However, our goal with the CLML was not to explore a measure that is starkly different from cross-validation, nor do we consider any arguable similarity with cross-validation a deficiency. Instead, we show how a minor modification to the LML can improve alignment with generalization, and be practical for hyperparameter learning. We also show in Appendix F and Figure 21 that the CLML correlates better than negative of the MAP validation loss with the BMA test accuracy, while the BMA validation loss achieves a similar degree of correlation. In Section 7 and Appendix G we also discuss the relationship between the CLML and the validation likelihood in more detail. We show that these two metrics can deviate from each other significantly for models with miscalibrated uncertainty estimates.\nCan we take comfort in using the marginal likelihood for model selection and hyperparameter learning, due to its connection with PAC-Bayes? Generally no. We summarize our observations about these connections in Section 9.6.", "publication_ref": ["b95", "b15"], "figure_ref": ["fig_5", "fig_5", "fig_0"], "table_ref": []}, {"heading": "Should we use the CLML instead of the LML?", "text": "The DKL hyperparameter learning with the CLML is of particular practical significance. These experiments, in Section 8.2, show that the CLML can be a promising drop-in replacement for the LML for learning many hyperparameters, especially for transfer learning and small datasets, and are our most substantial experiments involving DNNs. Future work in this area could have a substantial impact on the way we estimate hyperparameters in probabilistic models. Do we expect to see these issues with the marginal likelihood arising in common practice? Yes. For example, as above, we see clear improvements in using the CLML in place of the LML for deep kernel learning, which is a widespread application of the LML. More broadly, it is not safe to use the marginal likelihood as a proxy for generalization. Predicting the generalization of trained models is simply not the question the marginal likelihood is intended to answer. If we had a good prior, then these issues would be less severe. But if we are doing model selection, it is precisely because we are particularly uncertain which prior is reasonable. Moreover, if we attempt to compare too many models, for example by optimizing the LML with respect to the parameters of the prior, we can easily see marginal likelihood overfitting. We note that even when there is a correlation between marginal likelihood and generalization, the correlation will often be adversely affected by the conceptual issues described in Figure 1, as we see with DKL.\nAre neural architecture search results addressed by optimizing the prior scale? In Section 7, we show that the marginal likelihood is correlated with BMA test accuracy once we optimize a layer-wise prior scale parameter. At the same time, if we do not optimize the prior scale, this correlation can be negative, and if we only optimize a global prior scale shared across all layers, the correlation is negative if we restrict our attention to CNN models. It is important to note, however, that the amount of flexibility in the prior specification is completely arbitrary here: if we optimize too few parameters (a global prior scale) or too many parameters (a prior mean and variance separately for each weight), the correlation between the marginal likelihood and the generalization performance will break. The specific choice of layer-wise prior scale optimization is not in any way suggested by our conceptual understanding of the marginal likelihood. The fact that the marginal likelihood is only aligned with generalization for some subset of possible models, without a clear specification, is a major limitation for its use for model selection in practice. Moreover, even positive correlations with generalization are often adversely affected by the issues in Section 4.\nIs prior sensitivity the main drawback of the marginal likelihood? We do not consider prior sensitivity a drawback of the marginal likelihood. The marginal likelihood tells us how likely our data were generated from our prior. It should be sensitive to the prior. Prior sensitivity can sometimes be problematic in using the marginal likelihood to predict generalization, but the marginal likelihood should not be used for that purpose.\nIn Figure 2 (left) we see that by fitting the training data with a GP with constant mean and an RBF kernel we do not necessarily extrapolate well, but our prediction is reasonable and our uncertainty appears to be well-calibrated. In Figure 2 (right) we see that in training a GP with an overly flexible mean function we are able to overfit to the training data, and produce extrapolation predictions that are both incorrect, and very confident. By building a model with an incredibly flexible prior, we are able to optimize out LML to concentrate heavily around a single solution. This model has a high likelihood of generating the data, but does not extrapolate well, similar to the effect presented in Figure 1(c).", "publication_ref": [], "figure_ref": ["fig_0", "fig_1", "fig_1", "fig_0"], "table_ref": []}, {"heading": "B. Details on the Conditional Marginal Likelihood", "text": "Note that unlike the LML, the CLML depends on the ordering of the datapoints. To remove this undesirable dependence, we can average the CLML over all possible orderings of the data:\n1\nn! \u03c3\u2208Sn n i=m log p(D \u03c3(i) |D \u03c3(1) , . . . , D \u03c3(i) , M),(19)\nwhere S n is the set of all the possible permutations of n elements. Using all the n! permutations is prohibitively expensive in most practical scenarios, so we approximate Eq. (\n) as 1 |\u015c| \u03c3\u2208\u015c n i=m log p(D \u03c3(i) |D \u03c3(1) , . . . , D \u03c3(i) , M),19\nwhere\u015c \u2282 S n is a set containing several random permutations of the dataset. When D is a large dataset such that D <m and D \u2265m are both sufficiently large, a single permutation may suffice.\nImplementation. For all experiments involving the Laplace approximation, we compute the conditional marginal likelihood as follows:\n1. We train a model on 80% of the training data, and fit the LA approximation on the same subset of the data.\n2. We tune a hyperparameter T that we use to re-scale the Laplace posterior covariance matrix to ensure that it does not lead to very low BMA accuracies. We choose the value of T that achieves the highest BMA accuracy (average over 20 samples) on 5% of the training data. Our experimental results show that the optimal values of T generally ranges between 0.1 and 0.001, so the LA posterior does not collapse on the MAP solution even as we use this re-scaling parameter.\n3. Finally, we directly compute the CLML p(D \u2265m |D <m ) using the remaining 15% of the training data. This quantity corresponds simply to the log predictive likelihood of the 15% of the data approximated using a Bayesian model average of LA over 20 samples.\nIt is important to note that in all our plots, we show the BMA test accuracy and loglikelihood of the model trained on the entire training data and for the Laplace approximation fit on the entire training data as well, and not just the 80% subset that we condition the CLML on.\nFunction space. Note that in the procedure described above, we approximate CLML purely in function space: the estimate only depends on the predictions made by the Bayesian model average and not the values of individual parameters. The standard Laplace approximation of the LML is on the other hand quite sensitive to the number of parameters in the model. Approximating the LML directly in function space is hard, because it would require approximating the integral over the prior with simple Monte Carlo, but sampling from the prior over the weights of a neural network we will never randomly sample the parameters that are likely to generate the data.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C. Details on the Fourier model", "text": "We can construct a model that achieves the best of both worlds: strong generalization performance both for small and large training dataset sizes n. To do so, we consider the corrected model M 9c , an order-9 model with a prior a d , b d \u223c N (0, (1/d 2 ) 2 ), following the data generating process. We show the data fit and the learning curve for M 9c in Figure 4(d) and (e) (bottom). While the predictive distribution of M 9c is almost identical to that of M 9 (see Figure 4(e) (middle)), M 9c achieves the best marginal likelihood (comparable to M 3 ) and the best CLML (comparable to M 9 ) when evaluated on 100 datapoints. In the learning curve, M 9c provides comparable performance to M 3 for small n, and comparable performance to M 9 for large n.", "publication_ref": [], "figure_ref": ["fig_5", "fig_5"], "table_ref": []}, {"heading": "D. Training speed of Deep Neural Networks", "text": "Experimental details We consider 6 deep neural networks with architectures shown in Table D (LeCun et al., 1998;Simonyan and Zisserman, 2014;Szegedy et al., 2015;He et al., 2016;Huang et al., 2017). We also consider 8 different sizes of training datasets, {250, 500, 1000, 2000, 5000, 10, 000, 20, 000, 45, 000}, each constructed by randomly sampling a subset of CIFAR-10. To produce a MAP estimate, we train a neural network using hyperparameters found in Table 2. All models are trained using SGD with weight decay coefficient 0.0005, momentum coefficient of 0.9, initial learning rate 0.1, and learning rate drops by a factor of 10 after 1 2 and 3 4 of the epochs. Data augmentations include random horizontal flips and crops. We use the diagonal Laplace approximation to approximate the marginal likelihood and perform a Bayesian model average over 20 samples to obtain the BMA test accuracy and log-likelihood. The CLML is computed using a 80% \u2212 20% split of the training data as described in detail in Section B. We note that the BMA test accuracy and log-likelihood that we show in Figure 10 are computed with respect to all available training data and not just 80% of it. Figure 10 (b) shows the ranking of the models according to their generalization performance, where a lower ranking indicates a higher value of the BMA test accuracy. In particular, we see that VGG19 and GoogLeNet train faster than ResNet-18 and DenseNet121 but generalize worse for bigger sizes of the CIFAR-10 dataset. This observation extends to many neural architectures that can perform better or worse depending on the size of the dataset (Dosovitskiy et al., 2020). This proves that a faster training speed does not necessarily imply a better generalization performance. Results in Figure 10 (a) (left) are coherent with our conclusions from the Fourier example: the correlation of the BMA test log-likelihood with the LML is positive for small sizes of the training data and negative for higher sizes, whereas the correlation with CLML is consistently positive. Finally, Figure 10 (a ", "publication_ref": ["b80", "b84", "b27", "b30", "b12"], "figure_ref": ["fig_0", "fig_0", "fig_0"], "table_ref": ["tab_3"]}, {"heading": "Discussion", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E. The Density Estimation Example", "text": "Consider the generative process where x is generated using a Gaussian distribution N (u, 1) and the mean parameter is in turn generated using a Gaussian distribution N (\u00b5, \u03c3 2 ). Given dataset D = {x i } N 1 , we can show that the marginal likelihood is equal to, p(D|\u03c3, \u00b5) = N \u00b5 N , I + \u03c3 2 1 N,N , where\n\u00b5 N = \u00b5 \u00d7 1 N = \u00b5 . . . \u00b5 T N\n, and 1 N \u2208 R N is a column vector with all N elements equal to 1.\nProof Indeed, we have x i = u + i , where i \u223c N (0, 1), and u \u223c N (\u00b5, \u03c3 2 ). Thus, the observations x i are jointly Gaussian with a mean Ex i = E(u + i ) = Eu = \u00b5. The covariance structure is given by\ncov(x i , x j ) = E(x i \u2212 \u00b5) \u2022 (x j \u2212 \u00b5) = E(u + i \u2212 \u00b5) \u2022 (u + j \u2212 \u00b5) = E(u \u2212 \u00b5) \u2022 (u \u2212 \u00b5) + E i j = cov(u, u) + cov( i , j ) = \u03c3 2 + \u03b4 ij ,\nwhere \u03b4 ij is equal to 1 if i = j and 0 otherwise. Thus, we get D \u223c N (\u00b5\nN , I + \u03c3 2 1 N,N ).\nThe posterior distribution is equal to,\np(u|D, \u03c3, \u00b5) = N 1 1/\u03c3 2 + N N i=1 x i + 1 \u03c3 2 \u00b5 , 1 1/\u03c3 2 + N . (20\n)\nProof Let us denote x = x 1 . . . x N T . We can write down the joint distribution over x and u, following from the derivation marginal distribution of x above: Using the properties of Gaussian distributions, we can compute the posterior p(u|x) as a Gaussian conditional (see e.g. Ch. 3 of Bishop, 2006b):\nx u \u223c N \u00b5 N \u00b5 , 1 N 1 T N \u03c3 2 + I 1 N \u2022 \u03c3 2 1 T N \u2022 \u03c3 2 \u03c3 2 .\nu | x \u223c N \u00b5 + 1 T N \u2022 \u03c3 2 \u2022 (1 N 1 T N \u03c3 2 + I) \u22121 \u2022 (x \u2212 \u00b5 N ), \u03c3 2 \u2212 1 T N \u2022 \u03c3 2 \u2022 (1 N 1 T N \u03c3 2 + I) \u22121 1 N \u2022 \u03c3 2 . (21\n)\nNow, note that (1 N 1 T N \u03c3 2 + I) \u22121 = I \u2212 \u03c3 2 1+N \u03c3 2 1 N 1 T N\nwhich can be verified by direct multiplication. Substituting the expression for the inverse in Eq. (21), we recover Eq. (20).\nThe predictive distribution is equal to, , but due to \u03c3 2 = 0.6 providing the best fit for n = 1 datapoint, marginal likelihood strongly prefers this choice.\np(x * |D, \u03c3, \u00b5) = N 1 1/\u03c3 2 + N N i=1 x i + 1 \u03c3 2 \u00b5 , 1 + 1 1/\u03c3 2 + N . (22\n)\nProof\nConditioned on u, the observations are Gaussian: p(x * |u) = N (u, 1). Furthermore, we have shown that the posterior p(u|D, \u03c3, \u00b5) = N (\u03bc,\u03c3 2 ) is also Gaussian, with the parameter\u015d \u00b5,\u03c3 2 given by Eq. (20). Then, the predictive distribution is simply p(p(x * |D, \u03c3, \u00b5) = N (\u03bc,\u03c3 2 + 1), recovering Eq. (22).\nWe see that as the variance of the prior mean \u03c3 2 \u2192 +\u221e, both the predictive distribution and the posterior distribution do not depend on this hyperparameter, whereas the marginal likelihood depends on it. This is another example whereas the marginal likelihood is more sensitive to a hyperparameter that has little influence on the quality of future predictions. Hence, the potential mismatch between marginal likelihood and generation.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "F. Neural Architecture Search Details", "text": "We investigate the correlation between the log marginal likelihood (LML) and generalization in the context of image classification using the CIFAR-10 and CIFAR-100 datasets. In particular, we consider two tasks: (1) model selection with fixed prior precision, and (2) tuning the prior precision then performing a similar model selection task.\nExperimental details We reconstruct the neural architecture search experiments with convolutional (CNN) and residual (ResNet) networks for CIFAR-100 and CIFAR-10 from Immer et al. (2021). We use the same architectures:\n\u2022 The CNNs consist of up to 5 blocks of 33 convolutions, followed by a ReLU activation function, and MaxPooling, except in the first layer. BatchNorm is replaced by the fixup initialization (Zhang et al., 2019) as in Immer et al. (2021). The width (number of channels) of the first channel varies from 2 to 32 for both datasets. The last layer is a fully-connected layer to the class logit.\n\u2022 ResNets of depths varying from 8 to 32 are used for CIFAR-10 and from 20 to 101 for CIFAR-100. The width varies from 16 to 48 for CIFAR-10 and from 32 to 64 for CIFAR-100.\nAll models were trained for 250 epochs with an SGD optimizer and an initial learning rate of 0.01. The batch-size was fixed to 128. For experiments where the prior precision was optimized, we used online optimization where the prior precision was updated every 5 epochs for 100 iterations using an Adam optimizer with an initial learning rate equal to 1.0.\nFor all experiments in this section, we used the Kronecker Laplace approximation and computed the BMA test accuracy and log-likelihood by averaging over 20 samples. The CLML was computed using a 80% \u2212 20% split of the training data as described in detail in Section B. We note that the BMA test accuracy and log-likelihood that we show in all figures are computed using all available training data and not just 80% of it that we condition CLML on.\nDiscussion We visualize the correlation of the LML and the CLML in the top and bottom rows of Figure 12, respectively for CIFAR-100. We also report the Spearman's correlation coefficient \u03c1 (Spearman, 1961), which measures the correlation between the model rankings according to the BMA test accuracy and the LML/CLML. We see that the LML correlates positively with the BMA test accuracy for high values of the prior precision, but negatively for lower values. This can be understood in the light of what we discussed in Section 4.1; the LML penalizes low values of the prior precision because they correspond to diffuse priors. A similar trend is observed in Figure 13 for the MAP test accuracy, Figure 14 for the BMA test log-likelihood, and Figure 15 for the MAP test log-likelihood for the CIFAR-100 dataset. Similar results are obtained for CIFAR-10 in Figures 16,17,18,19. Figure 20 shows that optimizing the global and layerwise prior precision helps improve the correlation between LML and the BMA test accuracy. To understand this effect, consider Figure 4(a): two models with the same test performance can have very different values of the marginal likelihood depending on the prior variance. However, if we update the prior variance such that it maximizes the LML, we can expect that the final prior variance to be low without a major effect on the accuracy, therefore leading to a positive correlation between the LML and the BMA test accuracy.\nComparison to the validation loss Figure 21 shows the correlation between the BMA test accuracy and: the CLML (a), the BMA validation loss (b), and the MAP validation loss (c). The negative MAP validation loss correlates positively overall with the BMA test The CLML on the other hand is less sensitive to the value of the prior precision and consistently achieves a positive correlation with the BMA test accuracy.\naccuracy for CNNs, but not for ResNets, resulting in a negative total correlation for low prior precision values. In contrast, the CLML and BMA validation loss exhibit a positive correlation with the BMA test accuracy. This difference in the correlation factors of the BMA validation loss as opposed to the MAP validation loss might be due to the fact that the BMA solution tends to be generally less overconfident and better calibrated than MAP solution, hence increasing its correlation with the BMA test accuracy.", "publication_ref": ["b31", "b99", "b31", "b82"], "figure_ref": ["fig_0", "fig_0", "fig_0", "fig_0", "fig_1", "fig_5", "fig_0"], "table_ref": []}, {"heading": "G. Model Calibration and CLML", "text": "In this Section, we explain the difference between the CLML and the test (or validation) likelihood. The CLML is a likelihood under the joint predictive distribution, i.e. it takes into account the dependence in the predictions on the different datapoints. The test likelihood on the other hands treats datapoints as independent, and only evaluates the marginal predictive distributions. Suppose the test set consists of two datapoints D = {d 1 , d 2 }, and suppose we have two parameter samples w 1 and w 2 . Suppose for example that the predictive likelihoods are given by p(d 1 |w 1 ) = 0.1, p(d 2 |w 1 ) = 0.9, p(d 1 |w 2 ) = 0.9, p(d 2 |w 2 ) = 0.1. Then the joint likelihood estimate will be given by  The average likelihood of a test datapoint will on the other hand be\np(d 1 , d 2 ) = E w p(d 1 |w)\u2022p(d 2 |w) \u2248 1 2 (p(d 1 |w 1 ) \u2022 p(d 2 |w 1 ) + p(d 1 |w 2 ) \u2022 p(d 2\np(d 1 ) \u2022 p(d 2 ) = E w p(d 1 |w) \u2022 E w p(d 2 |w) \u2248 1 2 (p(d 1 |w 1 ) + p(d 1 |w 2 )) \u2022 1 2 (p(d 2 |w 1 ) + p(d 2 |w 2 )) = 0.25. (24\n)\nOn the other hand, if all samples provide equal likelihood p(d 1 |w 1 ) = p(d 2 |w 1 ) = p(d 1 |w 2 ) = p(d 2 |w 2 ) = 0.5, both Eq. (23) and Eq. (24) result in 0.25. In other words, the two configurations of the predictive distributions provide the same average test likelihood but different joint test likelihoods. As the CLML evaluates the joint likelihood of held-out data D \u2265m , it may not be aligned with the test likelihood.\nThe values of Eq. (23) and Eq. (24) will be identical if p(d i |w 1 ) = p(d i |w 2 ) for all i \u2208 {1, 2}. If each sample is tempered to optimize calibration, as in Figure 6, then p(d i |w j ) should differ less across w j , providing a higher correlation between the two measures. Indeed if p(d i |w 1 ) \u2248 0 and p(d i |w 2 ) \u2248 1, the tempered distributions p T (d i |w 1 ) > 0 and p T (d i |w 2 ) < 1 would shift away from these extreme values, so that and\n|p T (d i |w 1 ) \u2212 p T (d i |w 2 )| < |p(d i |w 1 ) \u2212 p(d i |w 2 )|.", "publication_ref": [], "figure_ref": ["fig_8"], "table_ref": []}, {"heading": "H. Extended Gaussian Process Results", "text": "GPs: RBF kernel. In Figure 22(a) we illustrate the bias of LML towards underfitting. We follow the experiment presented in Wilson et al. (2015), and generate 100 datasets from an RBF Gaussian process prior with a lengthscale of l = 4. The datapoints are located  at positions {1, . . . , 150}, the output scale is 1, and the observation noise is 0.2. For each dataset and each n \u2208 {1, . . . , 150}, we fit a new GP model to the the first n datapoints of the dataset: we maximize the LML or CLML with respect to the lengthscale of the RBF kernel, using the ground truth value l = 4 as the initialization. We plot the learned lengthscales averaged over the datasets as a function of n in 22(a). This experiment illustrates a unique quality of marginal likelihood that distinguishes it from conventional maximum likelihood training: while low lengthscales would lead to a better fit of the training data, marginal likelihood has a bias towards underfitting in the data space. Indeed, LML consistently selects lengthscales that are larger than the the lengthscale that was used to generate the data, especially for small n. We note that CLML does not remove this bias, and provides a very similar curve. GPs: RQ kernel. Above, we have seen how marginal likelihood can over-estimate the lengthscale of an RBF kernel leading to underfitting in data space. Here, we construct a more extreme example of this behaviour using the rational quadratic (RQ) kernel (see Rasmussen and Nickisch (2010)\n): k RQ (x 1 , x 2 ) = 1 + x 1 \u2212x 2 2 2\u03b1l 2 \u2212\u03b1 .\nThe hyperparameters are the lengthscale l and \u03b1; lower values of \u03b1 correspond to higher prior correlations, while as \u03b1 \u2192 \u221e the kernel approaches the RBF kernel with lengthscale l.\nWe generate the data from a GP with an RQ kernel with hyperparameters\u03b1 = 0.05, l = 0.5, and observation noise standard deviation\u03c3 = 0.1. The dataset is shown in Appendix  Figure 23. We then evaluate the LML and CLML as a function of \u03b1 and compare them to the true BMA predictive likelihood of test data. For this experiment we set the lengthscale l =l to its ground truth value, and we consider two values of the observation noise standard deviation: ground truth \u03c3 =\u03c3 = 0.1 and over-estimated noise \u03c3 = 2 \u2022\u03c3 = 0.2. We show the results in Figure 22(b). For the ground-truth noise scale, both LML and CLML provide an adequate representation of the test likelihood, although they both peak at a lower \u03b1 value than the test likelihood surface. However, for \u03c3 = 0.2 the marginal likelihood is completely misaligned with the test log-likelihood: LML peaks at \u03b1 \u2248 0, and then sharply decreases, while test LL is the lowest near \u03b1 = 0, and increases with \u03b1. The CLML does a much better job of tracking the test LL curve.\nIn Figure 23 (a), (b) we show the fit of the model with over-estimated observation noise \u03c3 = 0.2 for the \u03b1 parameter chosen by maximizing the marginal likelihood and CML respectively. For the CML, we condition on m = 45 datapoints (the training dataset size is n = 50), and we average the results over 20 random orderings of the data.\nIn panel (c) of Figure 23 we show the learning curve averaged over 100 random orderings of the data. While for large n the \u03b1 = 0.3 model generalizes better, the small-n terms in the marginal likelihood decomposition dominate, so that marginal likelihood prefers the simpler \u03b1 = 0.001 model.  ", "publication_ref": ["b92", "b75"], "figure_ref": ["fig_1", "fig_1", "fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "I. Deep Kernel Learning Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "I.1 UCI Regression", "text": "For the UCI regression datasets we use a DKL model with a fully-connected ReLU architecture of [D, 50, 50, 2], where D is the dimensionality of the data, and train using random subsets of the full UCI datasets ranging in size from 100 to 700 training points. We use the Bayesian Benchmarks library 1 to obtain the datasets, with a modification to ensure test data are not included in the normalization statistics. Models are trained using the closed form LML and CLML forms known for Gaussian process regression.\nIn figure 24 we show how the RMSE (normalized by dataset) varies for CLML optimization as a function of the fraction of data used used to condition the conditional marginal likelihood. As a general trend, the performance of CLML optimization increases as we use a larger fraction of the available data to condition on and a smaller fraction to compute the likelihood.\nIn Figure 25 we show the negative log likelihoods (normalized by dataset) of the N = 100 models on the UCI regression problems. While there is some variance in the relative gap in performance between CLML and LML optimization, in all cases we see that for very restricted train set sizes CLML not only produces more accurate predictions, but is more performant in terms of NLL.   Patacchiola et al. (2020). In this limited data regime the focus on test performance of CLML leads to stronger performance.", "publication_ref": ["b72"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "I.2 DKT Transfer Learning", "text": "Tables 3 and 4 give the numerical results accompanying Figure 9. From these tables we see that CLML optimization with the same model configuration consistently outperforms LML optimization, and in both experiments leads to the highest performing model. For full experimental details see Patacchiola et al. (2020).", "publication_ref": ["b72"], "figure_ref": ["fig_11"], "table_ref": ["tab_4"]}, {"heading": "J. Choice of m for the Conditional Marginal Likelihood", "text": "The hyperparameter m -the number of datapoints that we condition on in CLML -has an important effect on the conditional marginal likelihood. Indeed, if we set m = 0, we   recover the marginal likelihood. Setting m = n \u2212 1, we recover leave-one-out cross-validation likelihood for the BMA model, assuming we average the CLML over all possible orderings of the data. Generally, we find that CLML works best for relatively large values of m. However, The LML almost does not correlate with the MAP test log-likelihood for high values of the prior precision, but shows a negative correlation for low values of the prior precision (vague priors). We can that the correlation shift occurs around \u03bb = 10 \u22121 as the correlation remains positive for ResNets but becomes negative for CNNs. The CLML on the other hand is less sensitive to the value of the prior precision and consistently achieves a positive correlation with the MAP test log-likelihood.\nsetting m << n (for example, in the architecture search experiments in Section 7) allows us to estimate CLML without averaging over multiple orderings.\nIn Figure 24 we show the effect of m on the final RMSE for Deep Kernel Learning models trained with CLML. We find that larger values of m lead to better performance, but the results are relatively stable with respect to m. ", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Acknowledgements", "text": "We thank Sam Stanton, Marc Finzi, Rich Turner, Andreas Kirsch, Sebastian Farquhar, and Tom Minka for helpful discussions. This research is supported by NSF CAREER IIS-2145492, NSF I-DISRE 193471, NIH R01DA048764-01A1, NSF IIS-1910266, NSF 1922658  NRT-HDR, Meta Core Data Science, Google AI Research, BigHat Biosciences, Capital One, and an Amazon Research Award", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix Appendix Outline", "text": "This appendix is organized as follows:\n\u2022 In Appendix A, we demonstrate overfitting in Gaussian processes where the mean function is parameterized with a small multi-layer perceptron and we learn the parameters of the MLP by optimizing the LML.\n\u2022 In Appendix B, we provide details about the CLML approximation in our experiments and how it can be approximated directly in the function space in general.\n\u2022 In Appendix C, we provide additional details on the Fourier model.\n\u2022 In Appendix D, we demonstrate that faster training speed does not necessarily imply a better generalization performance for deep neural networks.\n\u2022 In Appendix E, we provide analytical details and additional experiments for the density estimation example.\n\u2022 In Appendix G, we discuss the difference between the joint and marginal predictive likelihoods and provide an example where CLML deviates from the test likelihood.\n\u2022 In Appendix F, we provide the experimental details for neural architecture search as well as additional results for CIFAR-10 and CIFAR-100.\n\u2022 In Appendix H, we provide additional results on Gaussian processes that show that the CLML is more robust to misspecification than the LML.\n\u2022 In Appendix I, we give details on the deep kernel learning experiments alongside additional results.\n\u2022 Lastly, in Appendix J, we study the effect of the choice of m on the CLML.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Overfitting in Gaussian Processes", "text": "In Figure 2 we provide a simple example in which LML optimization leads to severe overfitting. We generate a set of 100 evenly spaced points from a GP prior with an RBF kernel with a lengthscale of 0.75 and observation noise of 0.02. We then use LML optimization to train two GP models on the first 50 data points: the first model is a standard GP with constant mean and an RBF kernel, and the second is a GP with an RBF kernel, but where we have replaced the mean function with a small neural network. The mean function of the second model is a feed forward ReLU network with two hidden layers each with 50 units.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A new look at the statistical model identification", "journal": "IEEE transactions on automatic control", "year": "1974", "authors": "Hirotugu Akaike"}, {"ref_id": "b1", "title": "Fixing a broken elbo", "journal": "PMLR", "year": "2018", "authors": "Alexander Alemi; Ben Poole; Ian Fischer; Joshua Dillon; A Rif; Kevin Saurous;  Murphy"}, {"ref_id": "b2", "title": "User-friendly introduction to", "journal": "", "year": "2021", "authors": "Pierre Alquier"}, {"ref_id": "b3", "title": "Adapting the linearised laplace model evidence for modern deep learning", "journal": "", "year": "2022", "authors": "Javier Antor\u00e1n; David Janz; Erik James U Allingham; Riccardo Rb Daxberger; Eric Barbano; Jos\u00e9 Miguel Hern\u00e1ndez-Lobato Nalisnick"}, {"ref_id": "b4", "title": "A pac-bayesian generalization bound for equivariant networks", "journal": "", "year": "2022", "authors": "Arash Behboodi; Gabriele Cesa; Taco Cohen"}, {"ref_id": "b5", "title": "The intrinsic Bayes factor for model selection and prediction", "journal": "Journal of the American Statistical Association", "year": "1996", "authors": "O James;  Berger;  Luis R Pericchi"}, {"ref_id": "b6", "title": "Minimal bayesian testing of precise hypotheses, model selection, and ockham's razor", "journal": "", "year": "1991", "authors": "J O Berger; W H Jeffreys"}, {"ref_id": "b7", "title": "Pattern Recognition and Machine Learning", "journal": "Springer", "year": "2006", "authors": "Christopher M Bishop"}, {"ref_id": "b8", "title": "Pattern recognition and machine learning", "journal": "springer", "year": "2006", "authors": "M Christopher;  Bishop"}, {"ref_id": "b9", "title": "Pac-bayesian supervised classification: the thermodynamics of statistical learning", "journal": "", "year": "2007", "authors": "Olivier Catoni"}, {"ref_id": "b10", "title": "Laplace redux-effortless bayesian deep learning", "journal": "Advances in Neural Information Processing Systems", "year": "", "authors": "Erik Daxberger; Agustinus Kristiadi; Alexander Immer; Runa Eschenhagen; Matthias Bauer; Philipp Hennig"}, {"ref_id": "b11", "title": "The role of occam's razor in knowledge discovery", "journal": "Data mining and knowledge discovery", "year": "1999", "authors": "Pedro Domingos"}, {"ref_id": "b12", "title": "An image is worth 16x16 words: Transformers for image recognition at scale", "journal": "", "year": "2020", "authors": "Alexey Dosovitskiy; Lucas Beyer; Alexander Kolesnikov; Dirk Weissenborn; Xiaohua Zhai; Thomas Unterthiner; Mostafa Dehghani; Matthias Minderer; Georg Heigold; Sylvain Gelly"}, {"ref_id": "b13", "title": "Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data", "journal": "", "year": "2017", "authors": "Karolina Gintare; Daniel M Dziugaite;  Roy"}, {"ref_id": "b14", "title": "On the role of data in pac-bayes bounds", "journal": "PMLR", "year": "2021", "authors": "Kyle Gintare Karolina Dziugaite; Waseem Hsu; Gabriel Gharbieh; Daniel Arpino;  Roy"}, {"ref_id": "b15", "title": "On the marginal likelihood and cross-validation", "journal": "Biometrika", "year": "2020", "authors": "Edwin Fong; Chris C Holmes"}, {"ref_id": "b16", "title": "Bayesian neural network priors revisited", "journal": "", "year": "2021", "authors": "Adri\u00e0 Vincent Fortuin; Florian Garriga-Alonso; Gunnar Wenzel; Richard R\u00e4tsch; Mark Turner; Laurence Van Der Wilk;  Aitchison"}, {"ref_id": "b17", "title": "", "journal": "", "year": "2011", "authors": "Andrew David Gelman; Occam's Mackay;  Razor"}, {"ref_id": "b18", "title": "", "journal": "", "year": "2013", "authors": "Andrew Gelman; B John; Hal S Carlin;  Stern; B David; Aki Dunson; Donald B Vehtari;  Rubin"}, {"ref_id": "b19", "title": "Understanding predictive information criteria for bayesian models", "journal": "Statistics and computing", "year": "2014", "authors": "Andrew Gelman; Jessica Hwang; Aki Vehtari"}, {"ref_id": "b20", "title": "Pac-bayesian theory meets bayesian inference", "journal": "Advances in Neural Information Processing Systems", "year": "2016", "authors": "Pascal Germain; Francis Bach; Alexandre Lacoste; Simon Lacoste-Julien"}, {"ref_id": "b21", "title": "Explicativity: a mathematical theory of explanation with statistical applications", "journal": "Proceedings of the Royal Society of London. A. Mathematical and Physical Sciences", "year": "1678", "authors": " Ij Good"}, {"ref_id": "b22", "title": "Corroboration, explanation, evolving probability, simplicity and a sharpened razor", "journal": "The British Journal for the Philosophy of Science", "year": "1968", "authors": "Irving John Good"}, {"ref_id": "b23", "title": "Sandwiching the marginal likelihood using bidirectional monte carlo", "journal": "", "year": "2015", "authors": "Zoubin Roger B Grosse; Ryan P Ghahramani;  Adams"}, {"ref_id": "b24", "title": "Bayesian inductive inference and maximum entropy", "journal": "Springer", "year": "1988", "authors": "F Stephen;  Gull"}, {"ref_id": "b25", "title": "On calibration of modern neural networks", "journal": "PMLR", "year": "2017", "authors": "Chuan Guo; Geoff Pleiss; Yu Sun; Kilian Q Weinberger"}, {"ref_id": "b26", "title": "Structural risk minimization for character recognition", "journal": "", "year": "1992", "authors": "Isabelle Guyon; Vladimir Vapnik; Bernhard Boser; Leon Bottou; Sara A Solla"}, {"ref_id": "b27", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b28", "title": "Gaussian processes for big data", "journal": "AUAI Press", "year": "2013", "authors": "J Hensman; N D Fusi;  Lawrence"}, {"ref_id": "b29", "title": "Stochastic variational inference", "journal": "The Journal of Machine Learning Research", "year": "2013", "authors": "D Matthew;  Hoffman; M David; Chong Blei; John Wang;  Paisley"}, {"ref_id": "b30", "title": "Densely connected convolutional networks", "journal": "", "year": "2017", "authors": "Gao Huang; Zhuang Liu; Laurens Van Der Maaten; Kilian Q Weinberger"}, {"ref_id": "b31", "title": "Gunnar R\u00e4tsch, and Mohammad Emtiyaz Khan. Scalable marginal likelihood estimation for model selection in deep learning", "journal": "", "year": "2021", "authors": "Alexander Immer; Matthias Bauer; Vincent Fortuin"}, {"ref_id": "b32", "title": "Invariance learning in deep neural networks with differentiable laplace approximations", "journal": "", "year": "2022-12", "authors": "Alexander Immer; F A Tycho; Gunnar Van Der Ouderaa; Vincent R\u00e4tsch; Mark Fortuin;  Van Der Wilk"}, {"ref_id": "b33", "title": "Invariance learning in deep neural networks with differentiable laplace approximations", "journal": "", "year": "2022", "authors": "Alexander Immer; F A Tycho; Vincent Van Der Ouderaa; Gunnar Fortuin; Mark R\u00e4tsch;  Van Der Wilk"}, {"ref_id": "b34", "title": "Improving output uncertainty estimation and generalization in deep learning via neural network gaussian processes", "journal": "", "year": "2017", "authors": "Tomoharu Iwata; Zoubin Ghahramani"}, {"ref_id": "b35", "title": "What are Bayesian neural network posteriors really like", "journal": "", "year": "2021", "authors": "Pavel Izmailov; Sharad Vikram; D Matthew; Andrew Gordon Hoffman;  Wilson"}, {"ref_id": "b36", "title": "Inference, method, and decision: Towards a bayesian philosophy of science", "journal": "", "year": "1979", "authors": "T Edwin;  Jaynes"}, {"ref_id": "b37", "title": "Sharpening ockham's razor on a bayesian strop", "journal": "", "year": "1991", "authors": "H William; James O Jefferys;  Berger"}, {"ref_id": "b38", "title": "The theory of probability", "journal": "The Clarendon Press", "year": "1939", "authors": "Harold Jeffreys"}, {"ref_id": "b39", "title": "Fantastic generalization measures and where to find them", "journal": "", "year": "2019", "authors": "Yiding Jiang; Behnam Neyshabur; Hossein Mobahi; Dilip Krishnan; Samy Bengio"}, {"ref_id": "b40", "title": "On uncertainty, tempering, and data augmentation in bayesian classification", "journal": "", "year": "2022", "authors": "Sanyam Kapoor; J Wesley; Pavel Maddox; Andrew Gordon Izmailov;  Wilson"}, {"ref_id": "b41", "title": "Bayes factors", "journal": "Journal of the American Statistical Association", "year": "1995", "authors": "E Robert; Adrian E Kass;  Raftery"}, {"ref_id": "b42", "title": "", "journal": "", "year": "2013", "authors": "P Diederik; Max Kingma;  Welling"}, {"ref_id": "b43", "title": "Auto-encoding variational Bayes. International Conference on Learning Representations", "journal": "", "year": "2014", "authors": "P Diederik; Max Kingma;  Welling"}, {"ref_id": "b44", "title": "Variational dropout and the local reparameterization trick", "journal": "", "year": "2015", "authors": "P Diederik; Tim Kingma; Max Salimans;  Welling"}, {"ref_id": "b45", "title": "Overcoming catastrophic forgetting in neural networks", "journal": "Proceedings of the national academy of sciences", "year": "2017", "authors": "James Kirkpatrick; Razvan Pascanu; Neil Rabinowitz; Joel Veness; Guillaume Desjardins; Andrei A Rusu; Kieran Milan; John Quan; Tiago Ramalho; Agnieszka Grabska-Barwinska"}, {"ref_id": "b46", "title": "Bounds for averaging classifiers", "journal": "", "year": "2001", "authors": "John Langford; Matthias Seeger"}, {"ref_id": "b47", "title": "Gradient-based learning applied to document recognition", "journal": "Proceedings of the IEEE", "year": "1998", "authors": "Yann Lecun; L\u00e9on Bottou; Yoshua Bengio; Patrick Haffner"}, {"ref_id": "b48", "title": "Marginal likelihood computation for model selection and hypothesis testing: an extensive review", "journal": "", "year": "2020", "authors": "Fernando Llorente; Luca Martino; David Delgado; Javier Lopez-Santiago"}, {"ref_id": "b49", "title": "Automatic construction and natural-language description of nonparametric regression models", "journal": "", "year": "2014", "authors": "James Lloyd; David Duvenaud; Roger Grosse; Joshua Tenenbaum; Zoubin Ghahramani"}, {"ref_id": "b50", "title": "From laplace to supernova sn 1987a: Bayesian inference in astrophysics", "journal": "Springer", "year": "1990", "authors": "J Thomas;  Loredo"}, {"ref_id": "b51", "title": "Pac-bayes compression bounds so tight that they can explain generalization", "journal": "", "year": "2022", "authors": "Sanae Lotfi; Marc Finzi; Sanyam Kapoor; Andres Potapczynski; Micah Goldblum; Andrew Gordon Wilson"}, {"ref_id": "b52", "title": "Yarin Gal, and Mark van der Wilk. A bayesian perspective on training speed and model selection", "journal": "", "year": "2020", "authors": "Clare Lyle; Lisa Schut; Binxin Ru"}, {"ref_id": "b53", "title": "On the benefits of invariance in neural networks", "journal": "", "year": "2020", "authors": "Clare Lyle; Mark Van Der Wilk; Marta Kwiatkowska; Yarin Gal; Benjamin Bloem-Reddy"}, {"ref_id": "b54", "title": "Bayesian interpolation", "journal": "Neural Computation", "year": "1992", "authors": "D J Mackay"}, {"ref_id": "b55", "title": "The evidence framework applied to classification networks", "journal": "Neural Computation", "year": "1992", "authors": "J C David;  Mackay"}, {"ref_id": "b56", "title": "A practical Bayesian framework for backpropagation networks", "journal": "Neural computation", "year": "1992", "authors": "J C David;  Mackay"}, {"ref_id": "b57", "title": "Probable networks and plausible predictions?a review of practical Bayesian methods for supervised neural networks", "journal": "Network: computation in neural systems", "year": "1995", "authors": "J C David;  Mackay"}, {"ref_id": "b58", "title": "Information theory, inference and learning algorithms", "journal": "Cambridge university press", "year": "2003", "authors": "J C David;  Mackay"}, {"ref_id": "b59", "title": "Bayesian methods for adaptive models", "journal": "", "year": "1992", "authors": "David John Cameron Mackay"}, {"ref_id": "b60", "title": "Rethinking parameter counting in deep models: Effective dimensionality revisited", "journal": "", "year": "2020", "authors": "J Wesley; Gregory Maddox; Andrew Gordon Benton;  Wilson"}, {"ref_id": "b61", "title": "Optimizing neural networks with kronecker-factored approximate curvature", "journal": "", "year": "2015", "authors": "James Martens; Roger Grosse"}, {"ref_id": "b62", "title": "A note on the pac bayesian theorem", "journal": "", "year": "2004", "authors": "Andreas Maurer"}, {"ref_id": "b63", "title": "Some pac-bayesian theorems", "journal": "", "year": "1998", "authors": "A David;  Mcallester"}, {"ref_id": "b64", "title": "Pac-bayesian model averaging", "journal": "", "year": "1999", "authors": "A David;  Mcallester"}, {"ref_id": "b65", "title": "Automatic choice of dimensionality for pca", "journal": "", "year": "2001", "authors": "P Thomas;  Minka"}, {"ref_id": "b66", "title": "Foundations of machine learning", "journal": "", "year": "2018", "authors": "Mehryar Mohri; Afshin Rostamizadeh; Ameet Talwalkar"}, {"ref_id": "b67", "title": "Variational dropout sparsifies deep neural networks", "journal": "PMLR", "year": "2017", "authors": "Dmitry Molchanov; Arsenii Ashukha; Dmitry Vetrov"}, {"ref_id": "b68", "title": "Pacm-bayes: Narrowing the empirical risk gap in the misspecified bayesian regime", "journal": "PMLR", "year": "2022", "authors": "Alex Warren R Morningstar; Joshua V Alemi;  Dillon"}, {"ref_id": "b69", "title": "Annealed importance sampling", "journal": "Statistics and computing", "year": "2001", "authors": "M Radford;  Neal"}, {"ref_id": "b70", "title": "The promises and pitfalls of deep kernel learning", "journal": "", "year": "2021", "authors": "W Sebastian; Carl E Ober; Mark Rasmussen;  Van Der Wilk"}, {"ref_id": "b71", "title": "The neural testbed: Evaluating joint predictions", "journal": "", "year": "", "authors": "Ian Osband; Zheng Wen; Seyed Mohammad Asghari; Vikranth Dwaracherla; Xiuyuan Lu; Morteza Ibrahimi; Dieterich Lawson; Botao Hao; O' Brendan; Benjamin Donoghue;  Van Roy"}, {"ref_id": "b72", "title": "Bayesian meta-learning for the few-shot setting via deep kernels", "journal": "", "year": "2020", "authors": "Massimiliano Patacchiola; Jack Turner; J Elliot;  Crowley; O' Michael; Amos Boyle;  Storkey"}, {"ref_id": "b73", "title": "Gaussian processes for Machine Learning", "journal": "The MIT Press", "year": "2006", "authors": "C E Rasmussen; C K I Williams"}, {"ref_id": "b74", "title": "Occam's razor", "journal": "", "year": "2001", "authors": "Carl Edward Rasmussen; Zoubin Ghahramani"}, {"ref_id": "b75", "title": "Gaussian processes for machine learning (GPML) toolbox", "journal": "Journal of Machine Learning Research", "year": "2010-11", "authors": "Carl Edward Rasmussen; Hannes Nickisch"}, {"ref_id": "b76", "title": "Variational inference with normalizing flows", "journal": "PMLR", "year": "2015", "authors": "Danilo Rezende; Shakir Mohamed"}, {"ref_id": "b77", "title": "A scalable Laplace approximation for neural networks", "journal": "", "year": "2018", "authors": "Hippolyt Ritter; Aleksandar Botev; David Barber"}, {"ref_id": "b78", "title": "Estimating the dimension of a model. The annals of statistics", "journal": "", "year": "1978", "authors": "Gideon Schwarz"}, {"ref_id": "b79", "title": "Last layer marginal likelihood for invariance learning", "journal": "PMLR", "year": "2022", "authors": "Pola Schw\u00f6bel; Martin J\u00f8rgensen; W Sebastian; Mark Ober;  Van Der;  Wilk"}, {"ref_id": "b80", "title": "Very deep convolutional networks for large-scale image recognition", "journal": "", "year": "2014", "authors": "Karen Simonyan; Andrew Zisserman"}, {"ref_id": "b81", "title": "Bayes factors and choice criteria for linear models", "journal": "Journal of the Royal Statistical Society: Series B (Methodological)", "year": "1980", "authors": "F M Adrian; David J Smith;  Spiegelhalter"}, {"ref_id": "b82", "title": "The proof and measurement of association between two things", "journal": "", "year": "1961", "authors": "Charles Spearman"}, {"ref_id": "b83", "title": "Bayesian measures of model complexity and fit", "journal": "Journal of the royal statistical society: Series b (statistical methodology)", "year": "2002", "authors": "J David; Nicola G Spiegelhalter;  Best; P Bradley; Angelika Carlin;  Van Der Linde"}, {"ref_id": "b84", "title": "Going deeper with convolutions", "journal": "", "year": "2015", "authors": "Christian Szegedy; Wei Liu; Yangqing Jia; Pierre Sermanet; Scott Reed; Dragomir Anguelov; Dumitru Erhan; Vincent Vanhoucke; Andrew Rabinovich"}, {"ref_id": "b85", "title": "A strongly quasiconvex pac-bayesian bound", "journal": "PMLR", "year": "2017", "authors": "Niklas Thiemann; Christian Igel; Olivier Wintenberger; Yevgeny Seldin"}, {"ref_id": "b86", "title": "Marginal likelihood gradient for bayesian neural networks", "journal": "", "year": "2020", "authors": "B Marcin; Richard E Tomczak;  Turner"}, {"ref_id": "b87", "title": "Learning invariances using the marginal likelihood", "journal": "", "year": "2018", "authors": "Matthias Mark Van Der Wilk;  Bauer; James St John;  Hensman"}, {"ref_id": "b88", "title": "Asymptotic equivalence of bayes cross validation and widely applicable information criterion in singular learning theory", "journal": "Journal of machine learning research", "year": "2010", "authors": "Sumio Watanabe; Manfred Opper"}, {"ref_id": "b89", "title": "Bayesian learning via stochastic gradient Langevin dynamics", "journal": "", "year": "2011", "authors": "Max Welling; Yee W Teh"}, {"ref_id": "b90", "title": "From predictions to decisions: The importance of joint predictive distributions. arXiv e-prints", "journal": "", "year": "2021", "authors": "Zheng Wen; Ian Osband; Chao Qin; Xiuyuan Lu; Morteza Ibrahimi; Vikranth Dwaracherla; Mohammad Asghari; Benjamin Van Roy"}, {"ref_id": "b91", "title": "How good is the Bayes posterior in deep neural networks really? arXiv preprint", "journal": "", "year": "2020", "authors": "Florian Wenzel; Kevin Roth; S Bastiaan;  Veeling; Linh Jakub\u015bwiatkowski; Stephan Tran; Jasper Mandt; Tim Snoek; Rodolphe Salimans; Sebastian Jenatton;  Nowozin"}, {"ref_id": "b92", "title": "The human kernel", "journal": "", "year": "2015", "authors": "Christoph Andrew G Wilson; Chris Dann; Eric P Lucas;  Xing"}, {"ref_id": "b93", "title": "Covariance kernels for fast automatic pattern discovery and extrapolation with Gaussian processes", "journal": "", "year": "2014", "authors": "Andrew Gordon; Wilson "}, {"ref_id": "b94", "title": "Gaussian process kernels for pattern discovery and extrapolation", "journal": "", "year": "2013", "authors": "Andrew Gordon ; Wilson ; Ryan Prescott Adams"}, {"ref_id": "b95", "title": "Bayesian deep learning and a probabilistic perspective of generalization", "journal": "", "year": "2020", "authors": "Andrew Gordon ; Wilson ; Pavel Izmailov"}, {"ref_id": "b96", "title": "Deep kernel learning", "journal": "", "year": "2016", "authors": "Zhiting Andrew Gordon Wilson; Ruslan Hu; Eric P Salakhutdinov;  Xing"}, {"ref_id": "b97", "title": "Deep kernel learning", "journal": "", "year": "2016", "authors": "Zhiting Andrew Gordon Wilson; Ruslan Hu; Eric P Salakhutdinov;  Xing"}, {"ref_id": "b98", "title": "Differentiable annealed importance sampling and the perils of gradient noise", "journal": "Advances in Neural Information Processing Systems", "year": "2021", "authors": "Guodong Zhang; Kyle Hsu; Jianing Li; Chelsea Finn; Roger B Grosse"}, {"ref_id": "b99", "title": "Fixup initialization: Residual learning without normalization", "journal": "", "year": "2019", "authors": "Hongyi Zhang; Tengyu Yann N Dauphin;  Ma"}, {"ref_id": "b100", "title": "Nonvacuous generalization bounds at the imagenet scale: a pac-bayesian compression approach", "journal": "", "year": "2018", "authors": "Wenda Zhou; Victor Veitch; Morgane Austern; P Ryan; Peter Adams;  Orbanz"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Pitfalls of marginal likelihood. (a): Prior B is vague, but contains easily identifiable solutions and quickly collapses to posterior D after observing a small number of datapoints. Prior A describes the data better than prior B, but posterior D describes the data better than posterior B. The marginal likelihood will prefer model A, but model C generalizes better. (b): Example of misalignment between marginal likelihood and generalization. The marginal likelihood will pick prior scale b, and not include the best solution w * , leading to suboptimal generalization performance. (c): The complex model spreads its mass thinly on a broad support, while the appropriate model concentrates its mass on a particular class of problems.The overfit model is a \u03b4-distribution on the target datasetD.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: LML overfitting in Gaussian Processes. Left: A fit of a GP regression model with a constant prior mean. Right: a prior mean parameterized with an MLP and trained via marginal likelihood optimization. Train data is shown with circles, test data is shown with crosses and the shaded region visualizes the 2\u03c3-predictive region of the GP. Given enough flexibility with the prior mean, the marginal likelihood overfits the data, providing poor overconfident predictions outside of the train region.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Laplace approximation. Model x \u223c N (sin(w), 1) with a uniform prior w \u223c U [\u2212\u03b1, \u03b1]. (Left): Posterior density and a Laplace approximation to the posterior (scaled for visualization); (Right): True marginal likelihood and the Laplace estimate as a function of \u03b1. As Laplace only captures a single mode, the Laplace estimate of marginal likelihood decreases linearly with \u03b1 while the true marginal likelihood is roughly constant.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 :4Figure 4: Training speed and learning curves. (a): The prior variance continues to affect the marginal likelihood as its value increases whereas the test predictive distribution becomes insensitive to its value starting at a certain threshold. (b): While M 1 and M M M L provide identical fits of the data, LML favors M M M L . Moreover, LML prefers the model M 2 with a poor data fit to M 1 . (c): M 1 trains faster than M 2 , but has a worse LML than M 2 . (d): M 9 provides a better fit after observing 60 datapoints, but the LML prefers M 3 until n = 297. The model M 9c provides a near-identical fit to M 9 after observing 50 datapoints, but is preferred by the LML. (e): Data fit for M 3 , M 9 and M 9c . M 3 undefits, while the other two models get identical fits.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "(b), where the predictive distributions of M 1 and the maximum marginal likelihood (MML) model M M M L almost coincide, but the LML values are very different.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 5 :5Figure 5: Neural hyperparameter optimization for CIFAR-100. The correlation (Spearman's \u03c1) between the model rankings and generalization. For panels (a), (b), we consider a fixed prior precision \u03bb = 10 2 , 10 \u22121 , and 10 \u22123 . (a) Correlation between the Laplace BMA test accuracy and the LML. (b): Correlation between the BMA test accuracy and the CLML. (c): Correlation between the BMA test accuracy and the LML for optimized global (left) and layer-wise (right) prior precision. The correlation between the LML and test accuracy highly depends on the value of the prior precision. The CLML does not suffer from this sensitivity to the prior precision.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 6 :6Figure 6: The effect of calibration on neural architecture search. For both panels (a), (b), we consider a fixed prior precision \u03bb = 10 \u22121 and focus on convolutional neural networks (CNNs), which exhibit outliers in Figure 5. (a) Correlation between the Laplace BMA test accuracy and the Laplace BMA test log-likelihood (LL). The models with a large number of parameters have a relatively good BMA test accuracy but a bad BMA test log-likelihood, and are therefore miscalibrated. (b):Correlation between the BMA test accuracy and the CLML for CNNs before and after we perform temperature scaling in order to calibrate the models. Calibrating the models significantly improves the correlation between the CLML and the BMA test accuracy, especially for large models.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 7 :7Figure 7: Estimating CLML with MCMC. CLML estimates produced with SGLD plotted against (a): BMA test accuracy and (b): BMA test likelihood for CNN models of varying width and depth, with different fixed values of prior precision \u03bb \u2208 {10 2 , 1, 10 \u22124 }. Each plot also shows the corresponding Spearman's correlation coefficient \u03c1.With SGLD CLML estimates we observe similar results to the results for Laplace estimates in Figure5: CLML correlates with both the BMA test accuracy and likelihood with the exception of the largest models which are poorly calibrated.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 8 :8Figure 8: LML for hyperparameter learning in Gaussian processes. (a): The loglengthscale learned by LML and CLML in a GP regression model averaged over 100 datasets generated from a GP model with a lengthscale of 4. Unlike the train likelihood, LML has a bias towards underfitting, consistently overestimating the lengthscale, particularly for small n < 20. (b): Test log-likelihood, LML and CLML as a function of the \u03b1 hyper-parameter in the rational quadratic kernel with noise variance \u03c3 2 = 0.2. The CLML is closely aligned with test log likelihood, unlike the LML.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 9 :9Figure 9: Deep kernel learning (DKL). (a): DKL trained with LML and CLML optimization with the cutoff m at 90% of the training data. CLML optimization outperforms LML optimization in low data regimes. (b): Classification error (\u00b1 stderr) for Deep Kernel Transfer (DKT) models trained on the Omniglot dataset and evaluated on EMNIST. For both Cosine similarity and BatchNorm Cosine similarity kernels, CLML optimization outperforms LML optimization. (c): MSE for DKT (\u00b1 stderr) applied to the QMUL dataset shift problem of recognizing head poses. For both RBF and SM kernels, CLML optimization outperforms LML optimization. In all panels results are collected over 10 random initializations.Test likelihood results (Appendix) are qualitatively similar.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Appendix), shows a comparison of methods on the QMUL head pose regression problem for estimating the angular pose of gray-scale images of faces, where the individuals in the test set are distinct from those in the training set leading to dataset shift. For experimental details see Patacchiola et al. (2020).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "\u2212log p(D|M) = E w\u223cp(w|D) \u2212 log p(D|w) risk on training data + KL(p(w|D)||p(w)) complexity penalty ,", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Figure 10 :10Figure 10: Training speed, generalization, and LML for deep neural networks (DNN). (a): (Left): The correlation of the CLML and the LML with the BMA test log-likelihood. We show that the LML correlated positively with the BMA test log-likelihood (LL) for small sizes of the training data, but negatively for larger sizes, whereas CLML is correlated consistently positively with the BMA test log-likelihood for all sizes of the data. (Right): The correlation of of the CLML, LML and the BMA test log-likelihood with the inverse number of parameters. The LML approximated with LA correlates negatively with number of parameters and assigns higher values to more constrained models. This negative correlation might reflect one of the sensitivity of the Laplace approximation to the number of parameters as discussed in Section 5. (b): Ranking of DNNs according to their Bayesian model average (BMA) test accuracy approximated with LA for different sizes of the CIFAR-10 training data. A lower ranking indicates a higher BMA test accuracy. VGG19 and GoogLeNet, in contrast with ResNet-18 and DenseNet121, train faster but generalize worse for bigger sizes of the CIFAR-10 dataset.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "Figure 11 :11Figure 11: Density estimation (details). (a): Marginal log-likelihood, (b): test loglikelihood and (c): mean and (d): variance of the predictive distribution as a function of the prior variance \u03c3 2 . The predictive distribution is virtually constant with respect to prior variance for \u03c3 2 > 10, while marginal likelihood sharply decreases. (e): Learning curves for three different choices of prior standard deviation \u03c3 2 . After observing n = 5 datapoints, the predictive distributions are almost indistinguishable between the different values of \u03c3 2, but due to \u03c3 2 = 0.6 providing the best fit for n = 1 datapoint, marginal likelihood strongly prefers this choice.", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "Figure 12 :12Figure 12: Neural architecture search for CIFAR-100. Visualization of the correlation between the model rankings according to different metrics for fixed prior precision \u03bb \u2208 {10 2 , 10 \u22121 , 10 \u22122 , 10 \u22123 , 10 \u22124 , 10 \u22126 }. We report the Spearman's correlation coefficient \u03c1 in each figure. (Top row): Correlation between the BMA test accuracy and the log marginal likelihood (LML). (Top row): Correlation between the BMA test accuracy and the conditional log marginal likelihood (CLML). The LML correlates positively with the BMA test accuracy for high values of the prior precision, and negatively for low values of the prior precision (vague priors).The CLML on the other hand is less sensitive to the value of the prior precision and consistently achieves a positive correlation with the BMA test accuracy.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_17", "figure_caption": "|w 2 )) = 0.09. (23)", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_18", "figure_caption": "Figure 13 :13Figure 13: Neural architecture search for CIFAR-100. Visualization of the correlation between the model rankings according to different metrics for fixed prior precision \u03bb \u2208 {10 2 , 10 \u22121 , 10 \u22122 , 10 \u22123 , 10 \u22124 , 10 \u22126 }. We report the Spearman's correlation coefficient \u03c1 in each figure. (Top row): Correlation between the maximum-aposterior (MAP) test accuracy and the LML. (Top row): Correlation between the MAP test accuracy and the CLML. The LML correlates positively with the MAP test accuracy for high values of the prior precision, and negatively for low values of the prior precision, which correspond to vague priors. The CLML on the other hand is less sensitive to the value of the prior precision and consistently achieves a positive correlation with the MAP test accuracy.", "figure_data": ""}, {"figure_label": "14", "figure_type": "figure", "figure_id": "fig_20", "figure_caption": "Figure 14 :14Figure 14: Neural architecture search for CIFAR-100. Visualization of the correlation between the model rankings according to different metrics for fixed prior precision \u03bb \u2208 {10 2 , 10 \u22121 , 10 \u22122 , 10 \u22123 , 10 \u22124 , 10 \u22126 }. We report the Spearman's correlation coefficient \u03c1 in each figure. (Top row): Correlation between the BMA test log-likelihood and the log marginal likelihood (LML). (Top row): Correlation between the BMA test log-likelihood and the conditional log marginal likelihood (CLML). The LML correlates positively with the BMA test log-likelihood for high values of the prior precision, and negatively for low values of the prior precision (vague priors). The correlation shifts around \u03bb = 10 \u22121 as it remains positive for ResNets but becomes negative for CNNs. The CLML on the other hand is less sensitive to the value of the prior precision and consistently achieves a positive correlation with the BMA test log-likelihood.", "figure_data": ""}, {"figure_label": "15", "figure_type": "figure", "figure_id": "fig_22", "figure_caption": "Figure 15 :15Figure 15: Neural architecture search for CIFAR-100. Visualization of the correlation between the model rankings according to different metrics for fixed prior precision \u03bb \u2208 {10 2 , 10 \u22121 , 10 \u22122 , 10 \u22123 , 10 \u22124 , 10 \u22126 }. We report the Spearman's correlation coefficient \u03c1 in each figure. (Top row): Correlation between the MAP test log-likelihood and the log marginal likelihood (LML). (Top row): Correlation between the MAP test log-likelihood and the conditional log marginal likelihood (CLML). The LML correlates positively with the MAP test log-likelihood for high values of the prior precision, and negatively for low values of the prior precision (vague priors).We can that the correlation shift occurs around \u03bb = 10 \u22121 as the correlation remains positive for ResNets but becomes negative for CNNs. The CLML on the other hand is less sensitive to the value of the prior precision and consistently achieves a positive correlation with the MAP test log-likelihood.", "figure_data": ""}, {"figure_label": "16", "figure_type": "figure", "figure_id": "fig_24", "figure_caption": "Figure 16 :16Figure 16: Neural architecture search for CIFAR-10. Visualization of the correlation between the model rankings according to different metrics for fixed prior precision \u03bb \u2208 {10 2 , 10 \u22121 , 10 \u22122 , 10 \u22124 }. We report the Spearman's correlation coefficient \u03c1 in each figure. (Top row): Correlation between the BMA test accuracy and the log marginal likelihood (LML). (Top row): Correlation between the BMA test accuracy and the conditional log marginal likelihood (CLML). The LML correlates positively with the BMA test accuracy for high values of the prior precision, and negatively for low values of the prior precision (vague priors). The CLML on the other hand is less sensitive to the value of the prior precision and consistently achieves a positive correlation with the BMA test accuracy.", "figure_data": ""}, {"figure_label": "17", "figure_type": "figure", "figure_id": "fig_25", "figure_caption": "Figure 17 :17Figure 17: Neural architecture search for CIFAR-10. Visualization of the correlation between the model rankings according to different metrics for fixed prior precision \u03bb \u2208 {10 2 , 10 \u22121 , 10 \u22122 , 10 \u22124 }. We report the Spearman's correlation coefficient \u03c1 in each figure. (Top row): Correlation between the maximum-a-posterior (MAP) test accuracy and the LML. (Top row): Correlation between the MAP test accuracy and the CLML. The LML correlates positively with the MAP test accuracy for high values of the prior precision, and negatively for low values of the prior precision, which correspond to vague priors. The CLML on the other hand is less sensitive to the value of the prior precision and consistently achieves a positive correlation with the MAP test accuracy.", "figure_data": ""}, {"figure_label": "18", "figure_type": "figure", "figure_id": "fig_26", "figure_caption": "Figure 18 :18Figure 18: Neural architecture search for CIFAR-10. Visualization of the correlation between the model rankings according to different metrics for fixed prior precision\u03bb \u2208 {10 2 , 10 \u22121 , 10 \u22122 , 10 \u22124 }. We report the Spearman's correlation coefficient \u03c1 in each figure. (Top row): Correlation between the BMA test log-likelihood and the log marginal likelihood (LML). (Top row): Correlation between the BMA test log-likelihood and the conditional log marginal likelihood (CLML). The LML almost does not correlate with the BMA test log-likelihood for high values of the prior precision, but shows a negative correlation for low values of the prior precision (vague priors). The correlation shifts around \u03bb = 10 \u22121 as it remains positive for ResNets but becomes negative for CNNs. The CLML on the other hand is less sensitive to the value of the prior precision and consistently achieves a positive correlation with the BMA test log-likelihood.", "figure_data": ""}, {"figure_label": "19", "figure_type": "figure", "figure_id": "fig_27", "figure_caption": "Figure 19 :19Figure 19: Neural architecture search for CIFAR-10. Visualization of the correlation between the model rankings according to different metrics for fixed prior precision \u03bb \u2208 {10 2 , 10 \u22121 , 10 \u22122 , 10 \u22124 }. We report the Spearman's correlation coefficient \u03c1 in each figure. (Top row): Correlation between the MAP test log-likelihood and the log marginal likelihood (LML). (Top row): Correlation between the MAP test log-likelihood and the conditional log marginal likelihood (CLML).The LML almost does not correlate with the MAP test log-likelihood for high values of the prior precision, but shows a negative correlation for low values of the prior precision (vague priors). We can that the correlation shift occurs around \u03bb = 10 \u22121 as the correlation remains positive for ResNets but becomes negative for CNNs. The CLML on the other hand is less sensitive to the value of the prior precision and consistently achieves a positive correlation with the MAP test log-likelihood.", "figure_data": ""}, {"figure_label": "202122232425", "figure_type": "figure", "figure_id": "fig_28", "figure_caption": "Figure 20 :Figure 21 :Figure 22 :Figure 23 :Figure 24 :Figure 25 :202122232425Figure 20: Neural network hyperparameter optimization for CIFAR-10 and CIFAR-100. Correlation between the log marginal likelihood (LML) and the BMA test accuracy for (Left) optimized global prior precision, and (Right)optimized layerwise prior precision for CIFAR-10. We report the Spearman's correlation coefficient \u03c1 in each figure. We observe that the layerwise optimization further improves the correlation between the LML and the BMA test accuracy.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": "# Samples Epochs Batch Size2509003250090032100090032200060064500060064100003001282000030012845000300128: Neural architectures that we considerin Section 6."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ": Training hyperparameters for experi-ments that we consider for trainingneural networks in Section 6."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "CLML and LML optimization of deep kernel transfer models on the QMUL head pose trajectory task of", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "CLML and LML optimization of deep kernel transfer models on a transfer learning task in which the training data is from the Omniglot dataset, and the test data from the EMNIST dataset. In this transfer learning setting we should be more focused on the test performance, as opposed to the alignment of our model with the training data, which is a focus more closely aligned with the biases of CLML than LML optimization.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "p(D|M) = n i p(D i |D <i , M),(2)", "formula_coordinates": [5.0, 237.36, 266.98, 284.64, 33.71]}, {"formula_id": "formula_1", "formula_text": "p(D|M) \u2248 p(D|\u0175, M) \u2022 \u03c3 w|D \u03c3 w ,(3)", "formula_coordinates": [5.0, 235.7, 403.09, 286.3, 26.58]}, {"formula_id": "formula_2", "formula_text": "log p(D|M) = n i=1 log p(D i |D <i , M). (4", "formula_coordinates": [9.0, 220.54, 395.14, 296.82, 33.71]}, {"formula_id": "formula_3", "formula_text": ")", "formula_coordinates": [9.0, 517.35, 406.04, 4.65, 10.91]}, {"formula_id": "formula_4", "formula_text": "n i=m log p(D i |D <i , M) = log p(D \u2265m |D <m ),(5)", "formula_coordinates": [9.0, 207.86, 544.33, 314.15, 33.71]}, {"formula_id": "formula_5", "formula_text": "p(d * |D, M) = w p(d * |w)p(w|D)dw,(6)", "formula_coordinates": [10.0, 221.28, 483.47, 300.73, 21.39]}, {"formula_id": "formula_7", "formula_text": "q(w) = N (w MAP , \u03a3), \u03a3 \u22121 = \u2207 2 w log (p(D|w)p(w)) w=w MAP . (8", "formula_coordinates": [12.0, 159.67, 446.25, 357.69, 20.42]}, {"formula_id": "formula_8", "formula_text": ")", "formula_coordinates": [12.0, 517.35, 448.02, 4.65, 10.91]}, {"formula_id": "formula_9", "formula_text": "MAP ) + D 2 log(2\u03c0) + 1 2 log (det \u03a3) ,(9)", "formula_coordinates": [12.0, 317.33, 509.87, 204.67, 25.77]}, {"formula_id": "formula_10", "formula_text": "log p(D|M) \u2248 log p(D|w MLE ) \u2212 D 2 log n, (10", "formula_coordinates": [13.0, 210.81, 567.61, 306.34, 25.3]}, {"formula_id": "formula_11", "formula_text": ")", "formula_coordinates": [13.0, 517.15, 574.26, 4.85, 10.91]}, {"formula_id": "formula_12", "formula_text": "log p(D|M) \u2265 E q(w) log p(D|w) data fit \u2212 KL(q(w)||p(w)) complexity penalty ,(11)", "formula_coordinates": [14.0, 187.63, 275.93, 334.37, 30.09]}, {"formula_id": "formula_13", "formula_text": "p(D|M) = E w\u223cp(w) p(D|w) \u2248 1 m m i=1 p(D|w i ), w i \u223c p(w), i = 1, . . . , m.(12)", "formula_coordinates": [15.0, 135.77, 121.45, 386.24, 33.71]}, {"formula_id": "formula_14", "formula_text": "p(D|M) = p(D|w)p(w) q(w) q(w)dw = E w\u223cq(w) p(D|w)p(w) q(w) \u2248 1 m m i=1 p(D|w i )p(w i ) q(w i ) ,(13)", "formula_coordinates": [15.0, 104.37, 307.96, 417.63, 33.71]}, {"formula_id": "formula_15", "formula_text": "p(D|w i )p(w i ) q(w i ) = p(D|w i )p(w i ) p(w i |D)", "formula_coordinates": [15.0, 91.2, 390.05, 115.95, 17.1]}, {"formula_id": "formula_16", "formula_text": "\u22122 \u22121 0 1 2 3 x 0.0 0.2 0.4 p(x) M MML : LML=\u221260.8 M 1 \u03c3 2 =10 6 \u00b5=1 , LML=\u221266.9 M 2 \u03c3 2 =0.07 \u00b5=\u22120.4 , LML=\u221266.3 Empirical mean 0 20 40 Number of datapoints, n \u22124 \u22123 \u22122 \u22121 p(D n |D <n ) M MML M 1 M 2 (a) Fixed \u00b5 = 0 (b) LML prefers M 2 (c)", "formula_coordinates": [16.0, 148.44, 93.72, 335.99, 128.26]}, {"formula_id": "formula_17", "formula_text": "f (x, a, b) = D d=1 a d sin(d \u2022 x) + b d cos(d \u2022 x), where {a d , b d } D d=1", "formula_coordinates": [17.0, 106.94, 474.79, 303.98, 20.94]}, {"formula_id": "formula_18", "formula_text": "R D (w) = 1 n n i=1 (d i , w), R p D (w) = E d * \u223cp D (d * , w),(14)", "formula_coordinates": [26.0, 174.78, 407.05, 347.22, 33.71]}, {"formula_id": "formula_19", "formula_text": "E w\u223cq R p D (w) true risk \u2264 E w\u223cq R D (w) risk on training data + KL(q(w)||p(w)) + log(n/\u03b4) + 2 2n \u2212 1 complexity penalty ,(15)", "formula_coordinates": [26.0, 142.28, 585.4, 379.72, 41.74]}, {"formula_id": "formula_21", "formula_text": "E w\u223cp(w|D) E d * \u223cp D [\u2212 log p(d * |w)] \u2264 a + b \u2212 a 1 \u2212 e a\u2212b 1 \u2212 e a \u2022 n p(D|M)\u03b4 . (17", "formula_coordinates": [27.0, 138.17, 401.27, 378.98, 33.79]}, {"formula_id": "formula_22", "formula_text": ")", "formula_coordinates": [27.0, 517.15, 408.92, 4.85, 10.91]}, {"formula_id": "formula_23", "formula_text": "E w\u223cp(w|D) E d * \u223cp D [\u2212 log p(d * |w)] \u2264 a + b \u2212 a 1 \u2212 e a\u2212b 1 \u2212 e a \u2022 (p(D \u2265m |D <m , M)\u03b4) 1 n\u2212m+1 . (18)", "formula_coordinates": [29.0, 95.46, 427.4, 426.55, 33.79]}, {"formula_id": "formula_24", "formula_text": "n! \u03c3\u2208Sn n i=m log p(D \u03c3(i) |D \u03c3(1) , . . . , D \u03c3(i) , M),(19)", "formula_coordinates": [36.0, 204.49, 275.82, 317.51, 33.89]}, {"formula_id": "formula_25", "formula_text": ") as 1 |\u015c| \u03c3\u2208\u015c n i=m log p(D \u03c3(i) |D \u03c3(1) , . . . , D \u03c3(i) , M),19", "formula_coordinates": [36.0, 93.57, 341.06, 259.13, 20.94]}, {"formula_id": "formula_26", "formula_text": "\u00b5 N = \u00b5 \u00d7 1 N = \u00b5 . . . \u00b5 T N", "formula_coordinates": [38.0, 121.43, 427.52, 137.22, 31.59]}, {"formula_id": "formula_27", "formula_text": "cov(x i , x j ) = E(x i \u2212 \u00b5) \u2022 (x j \u2212 \u00b5) = E(u + i \u2212 \u00b5) \u2022 (u + j \u2212 \u00b5) = E(u \u2212 \u00b5) \u2022 (u \u2212 \u00b5) + E i j = cov(u, u) + cov( i , j ) = \u03c3 2 + \u03b4 ij ,", "formula_coordinates": [38.0, 90.0, 516.29, 434.49, 30.91]}, {"formula_id": "formula_28", "formula_text": "N , I + \u03c3 2 1 N,N ).", "formula_coordinates": [38.0, 428.81, 554.61, 73.51, 12.64]}, {"formula_id": "formula_29", "formula_text": "p(u|D, \u03c3, \u00b5) = N 1 1/\u03c3 2 + N N i=1 x i + 1 \u03c3 2 \u00b5 , 1 1/\u03c3 2 + N . (20", "formula_coordinates": [38.0, 160.98, 606.84, 356.17, 33.71]}, {"formula_id": "formula_30", "formula_text": ")", "formula_coordinates": [38.0, 517.15, 617.74, 4.85, 10.91]}, {"formula_id": "formula_31", "formula_text": "x u \u223c N \u00b5 N \u00b5 , 1 N 1 T N \u03c3 2 + I 1 N \u2022 \u03c3 2 1 T N \u2022 \u03c3 2 \u03c3 2 .", "formula_coordinates": [38.0, 206.74, 681.32, 204.28, 33.42]}, {"formula_id": "formula_32", "formula_text": "u | x \u223c N \u00b5 + 1 T N \u2022 \u03c3 2 \u2022 (1 N 1 T N \u03c3 2 + I) \u22121 \u2022 (x \u2212 \u00b5 N ), \u03c3 2 \u2212 1 T N \u2022 \u03c3 2 \u2022 (1 N 1 T N \u03c3 2 + I) \u22121 1 N \u2022 \u03c3 2 . (21", "formula_coordinates": [39.0, 90.0, 523.87, 435.41, 33.52]}, {"formula_id": "formula_33", "formula_text": ")", "formula_coordinates": [39.0, 517.15, 546.48, 4.85, 10.91]}, {"formula_id": "formula_34", "formula_text": "Now, note that (1 N 1 T N \u03c3 2 + I) \u22121 = I \u2212 \u03c3 2 1+N \u03c3 2 1 N 1 T N", "formula_coordinates": [39.0, 90.0, 568.25, 240.4, 21.66]}, {"formula_id": "formula_35", "formula_text": "p(x * |D, \u03c3, \u00b5) = N 1 1/\u03c3 2 + N N i=1 x i + 1 \u03c3 2 \u00b5 , 1 + 1 1/\u03c3 2 + N . (22", "formula_coordinates": [39.0, 151.04, 652.28, 366.11, 33.71]}, {"formula_id": "formula_36", "formula_text": ")", "formula_coordinates": [39.0, 517.15, 663.17, 4.85, 10.91]}, {"formula_id": "formula_37", "formula_text": "Proof", "formula_coordinates": [39.0, 90.0, 695.38, 30.47, 9.6]}, {"formula_id": "formula_38", "formula_text": "p(d 1 , d 2 ) = E w p(d 1 |w)\u2022p(d 2 |w) \u2248 1 2 (p(d 1 |w 1 ) \u2022 p(d 2 |w 1 ) + p(d 1 |w 2 ) \u2022 p(d 2", "formula_coordinates": [42.0, 95.46, 685.3, 340.17, 26.03]}, {"formula_id": "formula_39", "formula_text": "p(d 1 ) \u2022 p(d 2 ) = E w p(d 1 |w) \u2022 E w p(d 2 |w) \u2248 1 2 (p(d 1 |w 1 ) + p(d 1 |w 2 )) \u2022 1 2 (p(d 2 |w 1 ) + p(d 2 |w 2 )) = 0.25. (24", "formula_coordinates": [43.0, 102.1, 439.84, 415.05, 42.97]}, {"formula_id": "formula_40", "formula_text": ")", "formula_coordinates": [43.0, 517.15, 454.65, 4.85, 10.91]}, {"formula_id": "formula_41", "formula_text": "|p T (d i |w 1 ) \u2212 p T (d i |w 2 )| < |p(d i |w 1 ) \u2212 p(d i |w 2 )|.", "formula_coordinates": [43.0, 301.71, 614.79, 222.41, 18.93]}, {"formula_id": "formula_42", "formula_text": "): k RQ (x 1 , x 2 ) = 1 + x 1 \u2212x 2 2 2\u03b1l 2 \u2212\u03b1 .", "formula_coordinates": [44.0, 241.9, 635.15, 169.87, 17.86]}], "doi": "10.48550/ARXIV.2202.10638"}