{"title": "The Importance of Non-Markovianity in Maximum State Entropy Exploration", "authors": "Mirco Mutti; Riccardo De Santi; Marcello Restelli", "pub_date": "", "abstract": "In the maximum state entropy exploration framework, an agent interacts with a reward-free environment to learn a policy that maximizes the entropy of the expected state visitations it is inducing. Hazan et al. (2019) noted that the class of Markovian stochastic policies is sufficient for the maximum state entropy objective, and exploiting non-Markovianity is generally considered pointless in this setting. In this paper, we argue that non-Markovianity is instead paramount for maximum state entropy exploration in a finite-sample regime. Especially, we recast the objective to target the expected entropy of the induced state visitations in a single trial. Then, we show that the class of non-Markovian deterministic policies is sufficient for the introduced objective, while Markovian policies suffer non-zero regret in general. However, we prove that the problem of finding an optimal non-Markovian policy is NP-hard. Despite this negative result, we discuss avenues to address the problem in a tractable way and how non-Markovian exploration could benefit the sample efficiency of online reinforcement learning in future works.", "sections": [{"heading": "Introduction", "text": "Several recent works have addressed Maximum State Entropy (MSE) exploration (Hazan et al., 2019;Tarbouriech & Lazaric, 2019;Lee et al., 2019;Mutti & Restelli, 2020;Mutti et al., 2021;Zhang et al., 2021;Guo et al., 2021;Liu & Abbeel, 2021b;a;Seo et al., 2021;Yarats et al., 2021;Mutti et al., 2022;Nedergaard & Cook, 2022) as an objective for unsupevised Reinforcement Learning (RL) (Sutton & Barto, 2018). In this line of work, an agent interacts with a rewardfree environment (Jin et al., 2020) in order to maximize an entropic measure of the state distribution induced by its behavior over the environment, effectively targeting a uniform exploration of the state space. Previous works motivated this MSE objective in two main directions. On the one hand, this learning procedure can be seen as a form of unsupervised pre-training of the base model (Laskin et al., 2021), which has been extremely successful in supervised learning (Erhan et al., 2009;2010;Brown et al., 2020). In this view, a MSE policy can serve as an exploratory initialization to standard learning techniques, such as Q-learning (Watkins & Dayan, 1992) or policy gradient (Peters & Schaal, 2008), and this has been shown to benefit the sample efficiency of a variety of RL tasks that could be specified over the pre-training environment (e.g., Mutti et al., 2021;Liu & Abbeel, 2021b;Laskin et al., 2021). On the other hand, pursuing a MSE objective leads to an even coverage of the state space, which can be instrumental to address the sparse reward discovery problem (Tarbouriech et al., 2021). Especially, even when the fine-tuning is slow (Campos et al., 2021), the MSE policy might allow to solve hard-exploration tasks that are out of reach of RL from scratch (Mutti et al., 2021;Liu & Abbeel, 2021b). As we find these premises fascinating, and of general interest to the RL community, we believe it is worth providing a theoretical reconsideration of the MSE problem. Specifically, we aim to study the minimal class of policies that is necessary to optimize a well-posed MSE objective, and the general complexity of the resulting learning problem.\nAll of the existing works pursuing a MSE objective solely focus on optimizing Markovian exploration strategies, in which each decision is conditioned on the current state of the environment rather than the full history of the visited states. The resulting learning problem is known to be provably efficient in tabular domains (Hazan et al., 2019;Zhang et al., 2020). Moreover, this choice is common in RL, as it is well-known that an optimal deterministic Markovian strategy maximizes the usual cumulative sum of rewards objective (Puterman, 2014). Similarly, Hazan et al. (2019, Lemma 3.3) note that the class of Markovian strategies is sufficient for the standard MSE objective. A carefully constructed Markovian strategy is able to induce the same state distribution of any history-based (non-Markovian) one by exploiting randomization. Crucially, this result does not hold only for asymptotic state distributions, but also for state arXiv:2202.03060v2 [cs.LG] 8 Jul 2022 distributions that are marginalized over a finite horizon (Puterman, 2014). Hence, there is little incentive to consider more complicated strategies as they are not providing any benefit on the value of the entropy objective.\nHowever, the intuition suggests that exploiting the history of the interactions is useful when the agent's goal is to uniformly explore the environment: If you know what you have visited already, you can take decisions accordingly. To this point, let us consider an illustrative example in which the agent finds itself in the middle of a two-rooms domain (as depicted in Figure 1), having a budget of interactions that is just enough to visit every state within a single episode. It is easy to see that an optimal Markovian strategy for the MSE objective would randomize between going left and right in the initial position, and then would follow the optimal route within a room, finally ending in the initial position again. An episode either results in visiting the left room twice, or the right room twice, or each room once, and all of this outcomes have the same probability. Thus, the agent might explore poorly when considering a single episode, but the exploration is uniform in the average of infinite trials. Arguably, this is quite different from how a human being would tackle this problem, i.e., taking intentional decisions in the middle position to visit a room before the other. This strategy leads to uniform exploration of the environment in any trial, but it is inherently non-Markovian.\nBacked by this intuition, we argue that prior work does not recognize the importance of non-Markovianity in MSE exploration due to an hidden infinite-samples assumption in the objective formulation, which is in sharp contrast with the objective function it is actually optimized by empirical methods, i.e., the state entropy computed over a finite batch of interactions. In this paper, we introduce a new finite-sample MSE objective that is akin to the practical formulation, as it targets the expected entropy of the state visitation frequency induced within an episode instead of the entropy of the expected state visitation frequency over infinite samples. In this finite-sample formulation non-Markovian strategies are crucial, and we believe they can benefit a significant range of relevant applications. For example, collecting task-specific samples might be costly in some real-world domains, and a pre-trained non-Markovian strategy is essential to guarantee quality exploration even in a single-trial setting. In another instance, one might aim to pre-train an exploration strategy for a class of multiple environments instead of a single one. A non-Markovian strategy could exploit the history of interactions to swiftly identify the structure of the environment, then employing the environment-specific optimal strategy thereafter. Unfortunately, learning a non-Markovian strategy is in general much harder than a Markovian one, and we are able to show that it is NP-hard in this setting. Nonetheless, this paper aims to highlight the importance of non-Markovinaity to fulfill the promises of maximum state entropy exploration, thereby motivating the development of tractable formulations of the problem as future work.\nThe contributions are organized as follows. First, in Section 3, we report a known result (Puterman, 2014) to show that the class of Markovian strategies is sufficient for any infinite-samples MSE objective, including the entropy of the induced marginal state distributions in episodic settings. Then, in Section 4, we propose a novel finite-sample MSE objective and a corresponding regret formulation. Especially, we prove that the class of non-Markovian strategies is sufficient for the introduced objective, whereas the optimal Markovian strategy suffers a non-zero regret. However, in Section 5, we show that the problem of finding an optimal non-Markovian strategy for the finite-sample MSE objective is NP-hard in general. Despite the hardness result, we provide a numerical validation of the theory (Section 6), and we comment some potential options to address the problem in a tractable way (Section 7). In Appendix A, we discuss the related work in the MSE literature, while the missing proofs can be found in Appendix B.", "publication_ref": ["b13", "b36", "b21", "b26", "b27", "b41", "b11", "b23", "b33", "b40", "b28", "b29", "b35", "b15", "b20", "b9", "b10", "b4", "b38", "b31", "b27", "b23", "b20", "b37", "b5", "b27", "b23", "b13", "b42", "b32", "b32", "b32"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Preliminaries", "text": "In the following, we will denote with \u2206(X ) the simplex of a space X , with [T ] the set of integers {0, . . . , T \u2212 1}, and with v \u2295 u a concatenation of the vectors v, u.\nControlled Markov Process A Controlled Markov Process (CMP) is a tuple M := (S, A, P, \u00b5), where S is a finite state space (|S| = S), A is a finite action space (|A| = A), P : S \u00d7 A \u2192 \u2206(S) is the transition model, such that P (s |a, s) denotes the probability of reaching state s \u2208 S when taking action a \u2208 A in state s \u2208 S, and \u00b5 \u2208 \u2206(S) is the initial state distribution.\nPolicies A policy \u03c0 defines the behavior of an agent interacting with an environment modelled by a CMP. It consists of a sequence of decision rules \u03c0 := (\u03c0 t ) \u221e t=0 . Each of them is a map between histories h := (s j , a j ) t j=0 \u2208 H t and actions \u03c0 t : H t \u2192 \u2206(A), such that \u03c0 t (a|h) defines the conditional probability of taking action a \u2208 A having experienced the history h \u2208 H t . We denote as H the space of the histories of arbitrary length. We denote as \u03a0 the set of all the policies, and as \u03a0 D the set of deterministic policies \u03c0 = (\u03c0 t ) \u221e t=1 such that \u03c0 t : H t \u2192 A. We further define: \u2022 Non-Markovian (NM) policies \u03a0 NM , where each \u03c0 \u2208 \u03a0 NM collapses to a single time-invariant decision rule \u03c0 = (\u03c0, \u03c0, . . .) such that \u03c0 : H \u2192 \u2206(A);\n\u2022 Markovian (M) policies \u03a0 M , where each \u03c0 \u2208 \u03a0 M is defined through a sequence of Markovian decision rules \u03c0 = (\u03c0 t ) \u221e t=0 such that \u03c0 t : S \u2192 \u2206(A). A Markovian policy that collapses into a single time-invariant decision rule \u03c0 = (\u03c0, \u03c0, . . .) is called a stationary policy. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "State Distributions and Visitation Frequency", "text": "J M R (\u03c0) = E h\u223cp \u03c0 T [R(h)]\n, and \u03c0 * J \u2208 arg max \u03c0\u2208\u03a0 J M R (\u03c0) is called an optimal policy. For any MDP M R , there always exists a deterministic Markovian policy \u03c0 \u2208 \u03a0 D M that is optimal (Puterman, 2014).  (Astrom, 1965;Kaelbling et al., 1998) is described by M R \u2126 := (S, A, P, R, \u00b5, \u2126, O), where S, A, P, R, \u00b5 are defined as in an MDP, \u2126 is a finite observation space, and O : S \u00d7 A \u2192 \u2206(\u2126) is the observation function, such that O(o|s , a) denotes the conditional probability of the observation o \u2208 \u2126 when selecting action a \u2208 A in state s \u2208 S. Crucially, while interacting with a POMDP the agent cannot observe the state s \u2208 S, but just the observation o \u2208 \u2126. The performance of a policy \u03c0 is defined as in an MDP.", "publication_ref": ["b32", "b2", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Infinite Samples: Non-Markovianity Does Not Matter", "text": "Previous works pursuing maximum state entropy exploration of a CMP consider an objective of the kind\nE \u221e (\u03c0) := H d \u03c0 (\u2022) = \u2212 E s\u223cd \u03c0 log d \u03c0 (s) ,(1)\nwhere d \u03c0 (\u2022) is either a stationary state distribution (Mutti & Restelli, 2020), a discounted state distribution (Hazan et al., 2019;Tarbouriech & Lazaric, 2019), or a marginal state distribution (Lee et al., 2019;Mutti et al., 2021). While it is well-known (Puterman, 2014) that there exists an optimal deterministic policy \u03c0 * \u2208 \u03a0 D M for the common average return objective J M R , it is not pointless to wonder whether the objective in (1) requires a more powerful policy class than \u03a0 M . Hazan et al. (2019, Lemma 3.3) confirm that the set of (randomized) Markovian policies \u03a0 M is indeed sufficient for E \u221e defined over asymptotic (stationary or discounted) state distributions. In the following theorem and corollary, we report a common MDP result (Puterman, 2014) to show that \u03a0 M suffices for E \u221e defined over (nonasymptotic) marginal state distributions as well. \nTheorem 3.1. Let x \u2208 {\u221e, \u03b3, T }, and let D x NM = {d \u03c0 x (\u2022) : \u03c0 \u2208 \u03a0 NM }, D x M = {d \u03c0 x (\u2022) : \u03c0 \u2208 \u03a0 M }\n(\u2022) = d \u03c0 \u221e (\u2022), d \u03c0 \u03b3 (\u2022) = d \u03c0 \u03b3 (\u2022), d \u03c0 T (\u2022) = d \u03c0 T (\u2022), from which D x NM \u2261 D x M follows. See Appendix B.1 for a de- tailed proof.\nFrom the equivalence of the sets of induced distributions, it is straightforward to derive the optimality of Markovian policies for objective (1).\nCorollary 3.2. For every CMP, there exists a Markovian policy \u03c0 * \u2208 \u03a0 M such that \u03c0 * \u2208 arg max \u03c0\u2208\u03a0 E \u221e (\u03c0).\nAs a consequence of Corollary 3.2, there is little incentive to consider non-Markovian policies when optimizing objective ( 1), since there is no clear advantage to make up for the additional complexity of the policy. This result might be unsurprising when considering asymptotic distributions, as one can expect a carefully constructed Markovian policy to be able to tie the distribution induced by a non-Markovian policy in the limit of the interaction steps. However, it is less evident that a similar property holds for the expectation of final-length interactions alike. Yet, we were able to show that a Markovian policy that properly exploits randomization can always achieve equivalent state distributions w.r.t. non-Markovian counterparts. Note that state distributions are actually expected state visitation frequency, and the expectation practically implies an infinite number of realizations. In this paper, we show that this underlying infinite-sample regime is the reason why the benefit of non-Markovianity, albeit backed up by intuition, does not matter. Instead, we propose a relevant finite-sample entropy objective in which non-Markovianity is crucial.", "publication_ref": ["b26", "b13", "b36", "b21", "b27", "b32", "b32", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "Finite Samples: Non-Markovianity Matters", "text": "In this section, we reformulate the typical maximum state entropy exploration objective of a CMP (1) to account for a finite-sample regime. Crucially, we consider the expected entropy of the state visitation frequency rather than the entropy of the expected state visitation frequency, which results in\nE(\u03c0) := E h\u223cp \u03c0 T H d h (\u2022) = \u2212 E h\u223cp \u03c0 T E s\u223cd h log d h (s) . (2)\nWe note that E(\u03c0) \u2264 E \u221e (\u03c0) for any \u03c0 \u2208 \u03a0, which is trivial by the concavity of the entropy function and the Jensen's inequality. Whereas ( 2) is ultimately an expectation as it is ( 1), the entropy is not computed over the infinite-sample state distribution d \u03c0 T (\u2022) but its finite-sample realization d h (\u2022). Thus, to maximize E(\u03c0) we have to find a policy inducing high-entropy state visits within a single trajectory rather than high-entropy state visits over infinitely many trajectories. Crucially, while Markovian policies are as powerful as any other policy class in terms of induced state distributions (Theorem 3.1), this is no longer true when looking at induced trajectory distributions p \u03c0 T . Indeed, we will show that non-Markovianity provides a superior policy class for objective (2). First, we define a performance measure to formally assess this benefit, which we call the regret-to-go. 1 Definition 4.1 (Expected Regret-to-go). Consider a policy \u03c0 \u2208 \u03a0 interacting with a CMP over T \u2212 t steps starting from the trajectory h t . We define the expected regret-to-go R T \u2212t , i.e., from step t onwards, as\nR T \u2212t (\u03c0, h t ) = H * \u2212 E h T \u2212t \u223cp \u03c0 T \u2212t H d ht\u2295h T \u2212t (\u2022) ,\nwhere\nH * = max \u03c0 * \u2208\u03a0 E h * T \u2212t \u223cp \u03c0 * T \u2212t H d ht\u2295h * T \u2212t (\u2022)\nis the expected entropy achieved by an optimal policy \u03c0 * . The term R T (\u03c0) denotes the expected regret-to-go of a T -step trajectory h T starting from s \u223c \u00b5.\nThe intuition behind the regret-to-go is quite simple. Suppose to have drawn a trajectory h t upon step t. If we take the subsequent action with the (possibly sub-optimal) policy \u03c0, by how much would we decrease (in expectation) the entropy of the state visits H(d h T (\u2022)) w.r.t. an optimal policy \u03c0 * ? In particular, we would like to know how limiting the policy \u03c0 to a specific policy class would affect the expected regret-to-go and the value of E(\u03c0) we could achieve. The following theorem and subsequent corollary, which constitute the main contribution of this paper, state that an optimal non-Markovian policy suffers zero expected regret-to-go in any case, whereas an optimal Markovian policy suffers non-zero expected regret-to-go in general. \nto-go R T \u2212t (\u03c0 NM , h t ) = 0, whereas for any \u03c0 M \u2208 \u03a0 M we have R T \u2212t (\u03c0 M , h t ) \u2265 0.\nCorollary 4.3 (Sufficient Condition). For every CMP M and trajectory h t \u2208 H [T ] for which any optimal Markovian policy \u03c0 M \u2208 \u03a0 M is randomized (i.e., stochastic) in s t , we have strictly positive regret-to-go R T \u2212t (\u03c0 M , h t ) > 0.\nThe result of Theorem 4.2 highlights the importance of non-Markovianity for optimizing the finite-sample MSE objective (2), as the class of Markovian policies is dominated by the class of non-Markovian policies. Most importantly, Corollary 4.3 shows that non-Markovian policies are strictly better than Markovian policies in any CMP of practical interest, i.e., those in which any optimal Markovian policy has to be randomized (Hazan et al., 2019) in order to maximize (2). The intuition behind this result is that a Markovian policy would randomize to make up for the uncertainty over the history, whereas a non-Markovian policy does not suffer from this partial observability, and it can deterministically select an optimal action. Clearly, this partial observability is harmless when dealing with the standard RL objective, in which the reward is fully Markovian and does not depend on the history, but it is instead relevant in the peculiar MSE setting, in which the objective is a concave function of the state visitation frequency. In the following section, we report a sketch of the derivation underlying Theorem 4.2 and Corollary 4.3, while we refer to the Appendix B.2 for complete proofs.", "publication_ref": ["b44", "b43", "b13", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "Regret Analysis", "text": "To the purpose of the regret analysis, we will consider the following assumption to ease the notation. 2Assumption 1 (Unique Optimal Action). For every CMP M and trajectory h t \u2208 H [T ] , there exists a unique optimal action a * \u2208 A w.r.t. the objective (2).\nFirst, we show that the class of deterministic non-Markovian policies is sufficient for the minimization of the regret-to-go, and thus for the maximization of (2). 2 Note that this assumption could be easily removed by partitioning the action space in ht as A(ht) = Aopt(ht) \u222a A sub\u2212opt (ht), such that Aopt(ht) are optimal actions and A sub\u2212opt (ht) are sub-optimal, and substituting any term \u03c0(a * |ht) with a\u2208A opt (h t ) \u03c0(a|ht) in the results.\nThen, in order to prove that the class of non-Markovian policies is also necessary for regret minimization, it is worth showing that Markovian policies can instead rely on randomization to optimize objective (2). For a fixed history h t \u2208 H t ending in state s, the variance of the event of an optimal Markovian policy \u03c0 M \u2208 arg max \u03c0\u2208\u03a0M E(\u03c0) taking a * = \u03c0 NM (h t ) in s is given by\nVar B(\u03c0 M (a * |s, t)) = Var hs\u223cp \u03c0 NM t E B(\u03c0 NM (a * |hs)) ,\nwhere hs \u2208 H t is any history of length t such that the final state is s, i.e., hs := (h t\u22121 \u2208 H t\u22121 ) \u2295 s, and B(x) is a Bernoulli with parameter x.\nProof Sketch. We can prove the result through the Law of Total Variance (LoTV) (see Bertsekas & Tsitsiklis, 2002), which gives\nVar B(\u03c0 M (a * |s, t)) = E hs\u223cp \u03c0 NM t Var B(\u03c0 NM (a * |hs)) + Var hs\u223cp \u03c0 NM t E B(\u03c0 NM (a * |hs)) , \u2200s \u2208 S.\nThen, exploiting the determinism of \u03c0 NM (through Lemma 4.4), it is straightforward to see that\nE hs\u223cp \u03c0 NM t\nVar B(\u03c0 NM (a * |hs)) = 0, which concludes the proof. 3 Unsurprisingly, Lemma 4.5 shows that, whenever the optimal strategy for (2) (i.e., the non-Markovian \u03c0 NM ) requires to adapt its decision in a state s according to the history that led to it (hs), an optimal Markovian policy for the same objective (i.e., \u03c0 M ) must necessarily be randomized. This is crucial to prove the following result, which establishes lower and upper bounds R T \u2212t , R T \u2212t to the expected regret-to-go of any Markovian policy that optimizes (2).\nLemma 4.6. Let \u03c0 M be an optimal Markovian policy \u03c0 M \u2208 arg max \u03c0\u2208\u03a0M E(\u03c0) on a CMP M. For any h t \u2208 H [T ] , it holds R T \u2212t (\u03c0 M ) \u2264 R T \u2212t (\u03c0 M ) \u2264 R T \u2212t (\u03c0 M ) such that R T \u2212t (\u03c0 M ) = H * \u2212 H * 2 \u03c0 M (a * |s t ) Var hst\u223cp \u03c0 NM t E B(\u03c0 NM (a * |hs t )) , R T \u2212t (\u03c0 M ) = H * \u2212 H * \u03c0 M (a * |s t ) Var hst\u223cp \u03c0 NM t E B(\u03c0 NM (a * |hs t )) ,\nwhere \u03c0 NM \u2208 arg max  \nH * = min h\u2208H T \u2212t H(d ht\u2295h (\u2022)), H * 2 = max h\u2208H T \u2212t \\H * T \u2212t H(d ht\u2295h (\u2022)) s.t. H * T \u2212t = arg max h\u2208H T \u2212t H(d ht\u2295h (\u2022)).\nProof Sketch. The crucial idea to derive lower and upper bounds to the regret-to-go is to consider the impact of a sub-optimal action in the best-case and the worst-case CMP respectively (see Lemma B.2,B.1). This gives\nR T \u2212t (\u03c0 M ) \u2265 H * \u2212 \u03c0 M (a * |s t )H * \u2212 1 \u2212 \u03c0 M (a * |s t ) H * 2 and R T \u2212t (\u03c0 M ) \u2264 H * \u2212 \u03c0 M (a * |s t )H * \u2212 1 \u2212 \u03c0 M (a * |s t ) H * .\nThen, with Lemma 4.5 we get\nVar B(\u03c0 M (a * |s t )) = \u03c0 M (a * |s t ) 1 \u2212 \u03c0 M (a * |s t ) = Var hs\u223cp \u03c0 NM t E B(\u03c0 NM (a * |hs t ))\n, which concludes the proof.\nFinally, the result in Theorem 4.2 is a direct consequence of Lemma 4.6. Note that the upper and lower bounds on the regret-to-go are strictly positive whenever \u03c0 M (a * |s t ) < 1, as it is stated in Corollary 4.3.", "publication_ref": ["b44", "b44", "b3", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "Complexity Analysis", "text": "Having established the importance of non-Markovianity in dealing with MSE exploration in a finite-sample regime, it is worth considering how hard it is to optimize the objective 2 within the class of non-Markovian policies. Especially, we aim at characterizing the complexity of the problem:\n\u03a8 0 := maximize \u03c0\u2208\u03a0NM E(\u03c0),\ndefined over a CMP M. Before going into the details of the analysis, we provide a couple of useful definitions for the remainder of the section, whereas we leave to (Arora & Barak, 2009) an extended review of complexity theory. Definition 5.1 (Many-to-one Reductions). We denote as A \u2264 m B a many-to-one reduction from A to B.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Definition 5.2 (Polynomial Reductions). We denote as", "text": "A \u2264 p B a polynomial-time (Turing) reduction from A to B.\nThen, we recall that \u03a8 0 can be rewritten as the problem of finding a reward-maximizing Markovian policy, i.e., \u03c0 M \u2208 arg max \u03c0\u2208\u03a0M J M R T (\u03c0), over a convenient extended MDP M R T obtained from CMP M (see the proof of Lemma 4.4 for further details). We call this problem \u03a8 0 and we note that \u03a8 0 \u2208 P, as the problem of finding a reward-maximizing Markovian policy is well-known to be in P for any MDP (Papadimitriou & Tsitsiklis, 1987). However, the following lemma shows that it does not exist a many-to-one reduction from \u03a8 0 to \u03a8 0 .\nLemma 5.3. A reduction \u03a8 0 \u2264 m \u03a8 0 does not exist.\nProof. In general, coding any instance of \u03a8 0 in the representation required by \u03a8 0 , which is an extended MDP M R T , holds exponential complexity w.r.t. the input of the initial instance of \u03a8 0 , i.e., a CMP M. Indeed, to build the extended MDP M R T from M, we need to define the transition probabilities P ( s | s, a) for every s \u2208 S, a \u2208 A, s \u2208 S. Whereas the action space remains unchanged A = A, the extended state space S has cardinality | S| = S T in general, which grows exponentially in T .\nThe latter result informally suggests that \u03a8 0 / \u2208 P. Indeed, we can now prove the main theorem of this section, which shows that \u03a8 0 is NP-hard under the common assumption that P = NP.\nTheorem 5.4. \u03a8 0 is NP-hard.\nProof Sketch. To prove the theorem, it is sufficient to show that there exists a problem \u03a8 c \u2208 NP-hard so that \u03a8 c \u2264 p \u03a8 0 . We show this by reducing 3SAT, which is a well-known NP-complete problem, to \u03a8 0 . To derive the reduction we consider two intermediate problems, namely \u03a8 1 and \u03a8 2 . Especially, we aim to show that the following chain of reductions holds In particular, we define \u2126 = S (note that S is the state space of the original CMP M), and O( o| s) = s \u22121 . Then, the reduction \u03a8 0 \u2265 m \u03a8 1 works as follows. We denote as I \u03a8i the set of possible instances of problem \u03a8 i . We show that \u03a8 0 is harder than \u03a8 1 by defining the polynomial-time functions \u03c8 and \u03c6 such that any instance of \u03a8 1 can be rewritten through \u03c8 as an instance of \u03a8 0 , and a solution \u03c0 * NM \u2208 \u03a0 NM for \u03a8 0 can be converted through \u03c6 into a solution \u03c0 * M \u2208 \u03a0 M for the original instance of  In (a, d), we illustrates the 3State and River Swim CMPs. Then, we report the average entropy induced by an optimal (stationary) Markovian policy \u03c0M and an optimal non-Markovian policy \u03c0NM in the 3State (T = 9) (b) and the River Swim (T = 10) (e). In (c) we report the entropy frequency in the 3State, in (f) the state visitation frequency in the River Swim. We provide 95% c.i. over 100 runs.\n\u03a8 0 \u2265 m \u03a8 1 \u2265 p \u03a8 2 \u2265 p 3SAT.\nMarkovian policy \u03c0 * M \u2208 arg max \u03c0\u2208\u03a0M J M R \u2126 (\u03c0) is greater than 0. Since computing an optimal policy in POMDPs is in general harder than the relative policy existence problem (Lusena et al., 2001, Section 3), we have that \u03a8 1 \u2265 p \u03a8 2 . For the last reduction, i.e., \u03a8 2 \u2265 p 3SAT, we extend the proof of Theorem 4.13 in (Mundhenk et al., 2000), which states that the policy existence problem for POMDPs is NP-complete. In particular, we show that this holds within the restricted class of POMDPs defined in \u03a8 1 . Since the chain \u03a8 0 \u2265 m \u03a8 1 \u2265 p \u03a8 2 \u2265 p 3SAT holds, we have that \u03a8 0 \u2265 p 3SAT. Since 3SAT \u2208 NP-complete, we can conclude that \u03a8 0 is NP-hard.\nHaving established the hardness of the optimization of \u03a8 0 , one could now question whether the problem \u03a8 0 is instead easy to verify (\u03a8 0 \u2208 NP), from which we would conclude that \u03a8 0 \u2208 NP-complete. Whereas we doubt that this problem is significantly easier to verify than to optimize, the focus of this work is on its optimization version, and we thus leave as future work a finer analysis to show that \u03a8 0 / \u2208 NP.", "publication_ref": ["b30", "b24", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Numerical Validation", "text": "Despite the hardness result of Theorem 5.4, we provide a brief numerical validation around the potential of non-Markovianity in MSE exploration. Crucially, the reported analysis is limited to simple domains and short time horizons, and it has to be intended as an illustration of the theoretical claims reported in previous sections. For the sake of simplicity, in this analysis we consider stationary policies for the Markovian set, though similar results can be obtained for time-variant strategies as well (in stochastic environments). Whereas a comprehensive evaluation of the practical benefits of non-Markovianity in MSE exploration is left as future work, we discuss in Section 7 why we believe that the development of scalable methods is not hopeless even in this challenging setting.\nIn this section, we consider a 3State (S = 3, A = 2, T = 9), which is a simple abstraction of the two-rooms in Figure 1, and a River Swim (Strehl & Littman, 2008) (S = 3, A = 2, T = 10) that are depicted in Figure 2a, 2d respectively. Especially, we compare the expected entropy (2) achieved by an optimal non-Markovian policy \u03c0 NM \u2208 arg max \u03c0\u2208\u03a0NM E(\u03c0), which is obtained by solving the extended MDP as described in the proof of Lemma 4.4, against an optimal Markovian policy \u03c0 M \u2208 arg max \u03c0\u2208\u03a0M E(\u03c0). In confirmation of the result in Theorem 4.2, \u03c0 M cannot match the performance of \u03c0 NM (see Figure 2b, 2e). In 3State, an optimal strategy requires going left when arriving in state 0 from state 2 and vice versa. The policy \u03c0 NM is able to do that, and it always realizes the optimal trajectory (Figure 2c). Instead, \u03c0 M is uniform in 0 and it often runs into sub-optimal trajectories. In the River Swim, the main hurdle is to reach state 2 from the initial one. Whereas \u03c0 M and \u03c0 NM are equivalently good in doing so, as reported in Figure 2f, only the non-Markovian strategy is able to balance the visitations in the previous states when it eventually reaches 2. The difference is already noticeable with a short horizon and it would further increase with a longer T .", "publication_ref": ["b34", "b44"], "figure_ref": ["fig_0", "fig_11", "fig_11", "fig_11", "fig_11"], "table_ref": []}, {"heading": "Discussion and Conclusion", "text": "In the previous sections, we detailed the importance of non-Markovianity when optimizing a finite-sample MSE objective, but we also proved that the corresponding optimization problem is NP-hard in its general formulation. Despite the hardness result, we believe that it is not hopeless to learn exploration policies with some form of non-Markovianity, while still preserving an edge over Markovian strategies.\nIn the following paragraphs, we discuss potential avenues to derive practical methods for relevant relaxations to the general class of non-Markovian policies.\nFinite-Length Histories Throughout the paper, we considered non-Markovian policies that condition their decisions on histories of arbitrary length, i.e., \u03c0 : H \u2192 \u2206(A). However, the complexity of optimizing such policies grows exponentially with the length of the history. To avoid this exponential blowup, one can define a class of non-Markovian policies \u03c0 : H H \u2192 \u2206(A) in which the decisions are conditioned on histories of a finite length H > 1 that are obtained from a sliding window on the full history. The optimal policy within this class would still retain better regret guarantees than an optimal Markovian policy, but it would not achieve zero regret in general. With the length parameter H one can trade-off the learning complexity with the regret according to the structure of the domain. For instance, H = 2 would be sufficient to achieve zero regret in the 3State domain, whereas in the River Swim domain any H < T would cause some positive regret.\nCompact Representations of the History Instead of setting a finite length H, one can choose to perform function approximation on the full history to obtain a class of policies \u03c0 : f (H) \u2192 \u2206(A), where f is a function that maps an history h to some compact representation. An interesting option is to use the notion of eligibility traces (Sutton & Barto, 2018) to encode the information of h in a vector of length S, which is updated as z t+1 \u2190 \u03bbz t + 1 st , where \u03bb \u2208 (0, 1) is a discount factor, 1 st is a vector with a unit entry at the index s t , and z 0 = 0. The discount factor \u03bb acts as a smoothed version of the length parameter H, and it can be dynamically adapted while learning. Indeed, this eligibility traces representation is particularly convenient for policy optimization (Deisenroth et al., 2013), in which we could optimize in turn a parametric policy over actions \u03c0 \u03b8 (\u2022|z, \u03bb) and a parametric policy over the discount \u03c0 \u03bd (\u03bb).\nTo avoid a direct dependence on S, one can define the vector z over a discretization of the state space.\nDeep Recurrent Policies Another noteworthy way to do function approximation on the history is to employ recurrent neural networks (Williams & Zipser, 1989;Hochreiter & Schmidhuber, 1997) to represent the non-Markovian policy. This kind of recurrent architecture is already popular in RL. In this paper we are providing the theoretical ground to motivate the use of deep recurrent policies to address maximum state entropy exploration.\nNon-Markovian Control with Tree Search In principle, one can get a realization of actions from the optimal non-Markovian policy without ever computing it, e.g., by employing a Monte-Carlo Tree Search (MCTS) (Kocsis & Szepesv\u00e1ri, 2006) approach to select the next action to take. Given the current state s t as a root, we can build the tree of trajectories from the root through repeated simulations of potential action sequences. With a sufficient number of simulations and a sufficiently deep tree, we are guaranteed to select the optimal action at the root. If the horizon is too long, we can still cut the tree at any depth and approximately evaluate a leaf node with the entropy induced by the path from the root to the leaf. The drawback of this procedure is that we require to access a simulator with reset (or a reliable estimate of the transition model) to actually build the tree.\nHaving reported interesting directions to learn non-Markovian exploration policies in practice, we would like to mention some relevant online RL settings that might benefit from such exploration policies. We leave as future work a formal definition of the settings and an empirical study.\nSingle-Trial RL In many relevant real-world scenarios, where data collection might be costly or non-episodic in nature, we cannot afford multiple trials to achieve the desired exploration of the environment. Non-Markovian exploration policies guarantee a good coverage of the environment in a single trial and they are particularly suitable for online learning processes.", "publication_ref": ["b35", "b8", "b39", "b14", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Learning in Latent MDPs", "text": "In a latent MDP scenario (Hallak et al., 2015;Kwon et al., 2021) an agent interacts with an (unknown) environment drawn from a class of MDPs to solve an online RL task. A non-Markovian exploration policy pre-trained on the whole class could exploit the memory to perform a fast identification of the specific context that has been drawn, quickly adapting to the optimal environmentspecific policy.\nIn this paper we focus on the gap between non-Markovian and Markovian policies, which can be either stationary or time-variant. Future works might consider the role of stationarity (see also Akshay et al., 2013;Laroche et al., 2022), such as establishing under which conditions stationary strategies are sufficient in this setting. Finally, here we focus on state distributions, which is most common in the MSE literature, but similar results could be extended to state-action distributions with minor modifications.\nTo conclude, we believe that this work sheds some light on the, previously neglected, importance of non-Markovianity to address maximum state entropy exploration. Although it brings a negative result about the computational complexity of the problem, we believe it can provide inspiration for future empirical and theoretical contributions on the matter.\nTable 1. Overview of the methods addressing MSE exploration in a controlled Markov process. For each method, we report the nature of the corresponding MSE objective, i.e., the entropy function (Entropy), whether it considers stationary, discounted, or marginal distributions (Distribution), and if it accounts for the state space S or the state-action space SA (Space). We also specify if the method learns a single policy rather than a mixture of policies (Mixture), and if it supports non-parametric entropy estimation (Non-Parametric).  (Mutti & Restelli, 2020) Shannon stationary state-action MEPOL (Mutti et al., 2021) Shannon marginal state MaxR\u00e9nyi (Zhang et al., 2021) R\u00e9nyi discounted state-action GEM (Guo et al., 2021) geometry-aware marginal state APT (Liu & Abbeel, 2021b) Shannon marginal state RE3 (Seo et al., 2021) Shannon marginal state Proto-RL (Yarats et al., 2021) Shannon marginal state APS (Liu & Abbeel, 2021a) Shannon marginal state A. Related Work Hazan et al. (2019) were the first to consider an entropic measure over the state distribution as a sensible learning objective for an agent interacting with a reward-free environment (Jin et al., 2020). Especially, they propose an algorithm, called MaxEnt, that learns a mixture of policies that collectively maximize the Shannon entropy of the discounted state distribution, i.e., (1). The final mixture is learned through a conditional gradient method, in which the algorithm iteratively estimates the state distribution of the current mixture to define an intrinsic reward function, and then identifies the next policy to be added by solving a specific RL sub-problem with this reward. A similar methodology has been obtained by Lee et al. (2019) from a game-theoretic perspective on the MSE exploration problem. Their algorithm, called SMM, targets the Shannon entropy of the marginal state distribution instead of the discounted distribution of MaxEnt. Another approach based on the conditional gradient method is FW-AME (Tarbouriech & Lazaric, 2019), which learns a mixture of policies to maximize the entropy of the stationary state-action distribution. As noted in (Tarbouriech & Lazaric, 2019), the mixture of policies might suffer a slow mixing to the asymptotic distribution for which the entropy is maximized. In (Mutti & Restelli, 2020), the authors present a method (IDE 3 AL) to learn a single exploration policy that simultaneously accounts for the entropy of the stationary state-action distribution and the mixing time.\nEven if they are sometimes evaluated on continuous domains (especially (Hazan et al., 2019;Lee et al., 2019)), the methods we mentioned require an accurate estimate of either the state distribution (Hazan et al., 2019;Lee et al., 2019) or the transition model (Tarbouriech & Lazaric, 2019;Mutti & Restelli, 2020), which hardly scales to high-dimensional domains. A subsequent work by Mutti et al. (2021) proposes an approach to estimate the entropy of the state distribution through a non-parametric method, and then to directly optimize the estimated entropy via policy optimization. Whereas all of the previous approaches accounts for the Shannon entropy in their objectives, recent works (Zhang et al., 2021;Guo et al., 2021) consider alternative formulations. Zhang et al. (2021) argues that the R\u00e9nyi entropy provides a superior incentive to cover all of the corresponding space than the Shannon entropy, and they propose a method to optimize the R\u00e9nyi of the state-action distribution via gradient ascent (MaxR\u00e9nyi). On an orthogonal direction, the authors of (Guo et al., 2021) consider a reformulation of the entropy function that accounts for the underlying geometry of the space. They present a method, called GEM, to learn an optimal policy for the geometry-aware entropy objective.", "publication_ref": ["b12", "b18", "b0", "b19", "b26", "b27", "b41", "b11", "b23", "b33", "b40", "b22", "b13", "b15", "b43", "b21", "b36", "b36", "b26", "b13", "b21", "b13", "b21", "b36", "b26", "b27", "b41", "b11", "b41", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "B.2. Proofs of Section 4", "text": "Theorem 4.2 (Non-Markovian Optimality). For every CMP M and trajectory h t \u2208 H [T ] , there exists a deterministic non-Markovian policy \u03c0 NM \u2208 \u03a0 D NM that suffers zero regret-to-go R T \u2212t (\u03c0 NM , h t ) = 0, whereas for any \u03c0 M \u2208 \u03a0 M we have R T \u2212t (\u03c0 M , h t ) \u2265 0.\nProof. The result R T \u2212t (\u03c0 NM , h t ) = 0 for a policy \u03c0 NM \u2208 \u03a0 NM D is a direct implication of Lemma 4.4, whereas R T \u2212t (\u03c0 M , h t ) \u2265 0 for any \u03c0 M \u2208 \u03a0 M is given by Lemma 4.6, which states that even an optimal Markovian policy \u03c0 * M \u2208 arg max \u03c0\u2208\u03a0M E(\u03c0) suffers expected regret-to-go R T \u2212t (\u03c0 * M ) \u2265 0.\nCorollary 4.3 (Sufficient Condition). For every CMP M and trajectory h t \u2208 H [T ] for which any optimal Markovian policy \u03c0 M \u2208 \u03a0 M is randomized (i.e., stochastic) in s t , we have strictly positive regret-to-go R T \u2212t (\u03c0 M , h t ) > 0.\nProof. This result is a direct consequence of the combination of Lemma 4.5 and Lemma 4.6. Indeed, if the policy \u03c0 M \u2208 arg max \u03c0\u2208\u03a0M E(\u03c0) is randomized in s t we have\n0 < Var B(\u03c0 M (a * |s t )) = Var hst\u223cp \u03c0 NM t E B(\u03c0 NM (a * |hs t )) ,\nfrom Lemma 4.5, which gives a lower bound to the expected regret-to-go R T \u2212t (\u03c0 M , h t ) > 0 through Lemma 4.6. where hs \u2208 H t is any history of length t such that the final state is s, i.e., hs := (h t\u22121 \u2208 H t\u22121 ) \u2295 s, and B(x) is a Bernoulli with parameter x.\nProof. Let us consider the random variable A \u223c P denoting the event \"the agent takes action a * \u2208 A\". Through the law of total variance (Bertsekas & Tsitsiklis, 2002), we can write the variance of A given s \u2208 S and t \u2265 0 as Var A|s, t = E A 2 |s, t \u2212 E A|s, t\n2 = E h E A 2 |s, t, h \u2212 E h E A|s, t, h 2 = E h Var A|s, t, h + E A|s, t, h 2 \u2212 E h E \u03c0 A|s, t, h 2 = E h Var A|s, t, h + E h E A|s, t, h 2 \u2212 E h E A|s, t, h 2 = E h Var A|s, t, h + Var h E A|s, t, h .(3)\nNow let the conditioning event h be distributed as h \u223c p \u03c0NM t\u22121 , so that the condition s, t, h becomes hs where hs = (s 0 , a 0 , s 1 , . . . , s t = s) \u2208 H t , and let the variable A be distributed according to P that maximizes the objective (2) given the conditioning. Hence, we have that the variable A on the left hand side of (3) is distributed as a Bernoulli B(\u03c0 M (a * |s, t)), where \u03c0 M \u2208 arg max \u03c0\u2208\u03a0M E(\u03c0), and the variable A on the right hand side of (4) is distributed as a Bernoulli B(\u03c0 NM (a * |hs)), where \u03c0 NM \u2208 arg max \u03c0\u2208\u03a0NM E(\u03c0). Thus, we obtain \nUnder Assumption 1, we know from Lemma 4.4 that the policy \u03c0 NM is deterministic, i.e., \u03c0 NM \u2208 \u03a0 NM D , so that Var B(\u03c0 NM (a * |hs)) = 0 for every hs, which concludes the proof. Proof. From the definition of the expected regret-to-go (Definition 4.1), we have that\nR T \u2212t (\u03c0 M , h t ) = H * \u2212 E h T \u2212t \u223cp \u03c0 M T \u2212t H d ht\u2295h T \u2212t (\u2022) ,\nin which we will omit h t in the regret-to-go R T \u2212t (\u03c0 M , h t ) = R T \u2212t (\u03c0 M ) as h t is fixed by the statement. To derive a lower bound and an upper bound to R T \u2212t (\u03c0 M ) we consider the impact that taking a sub-optimal action a \u2208 A \\ {a * } in state s t would have in a best-case and a worst-case CMP respectively, which is detailed in Lemma B. Then, we note that the event of taking a sub-optimal action a \u2208 A \\ {a * } with a policy \u03c0 M can be modelled by a ", "publication_ref": ["b3", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We would like to thank the reviewers of this paper for their feedbacks and useful advices. We also thank Romain Laroche and Remi Tachet des Combes for having signalled a technical error in a previous draft of the manuscript.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1. Online Learning of Global Concave Rewards", "text": "Another interesting pair of related works (Cheung, 2019a; addresses a reinforcement learning problem for the maximization of concave functions of vectorial rewards, which in a special case (Cheung, 2019a, Section 6.2) is akin to our objective function (2). Beyond this similarity in the objective definition, those works and our paper differ for some crucial aspects, and the contributions are essentially non-overlapping. On the one hand, (Cheung, 2019a; deal with an online learning problem, in which they care for the performance of the policies deployed during the learning process, whereas we only consider the performance of the optimal policy. On the other hand, we aim to compare the classes of non-Markovian and Markovian policies respectively, whereas they consider non-stationary or adaptive strategies to maximize the online objective. Finally, their definition of regret is based on an infinite-samples relaxation of the problem, whereas we account for the performance of the optimal general policy w.r.t. the finite-sample objective (2) to define our regret-to-go (Definition 4.1). , \u2200s \u2208 S, \u2200a \u2208 A.", "publication_ref": ["b6", "b44", "b6", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "B. Missing Proofs", "text": "For t = 0, we have that d \u03c0 0 (\u2022) = d \u03c0 0 (\u2022) = \u00b5(\u2022), which is the initial state distribution. We proceed by induction to show that if d \u03c0 t\u22121 (\u2022) = d \u03c0 t\u22121 (\u2022), then we have\nSince d \u03c0 t (s) = d \u03c0 t (s) holds for any t \u2265 0 and \u2200s \u2208 S, we have\nCorollary 3.2. For every CMP, there exists a Markovian policy \u03c0 * \u2208 \u03a0 M such that \u03c0 * \u2208 arg max \u03c0\u2208\u03a0 E \u221e (\u03c0).\nProof. The result is straightforward from Theorem 3.1 and noting that the set of non-Markovian policies \u03a0 NM with arbitrary history-length is as powerful as the general set of policies \u03a0. Thus, for every policy \u03c0 \u2208 \u03a0 there exists a (possibly randomized) policy \u03c0 \u2208 \u03a0 M inducing the same (stationary, discounted or marginal) state distribution of \u03c0, i.e., d\nIf it holds for any \u03c0 \u2208 \u03a0, then it holds for \u03c0 * \u2208 arg max \u03c0\u2208\u03a0 H d \u03c0 (\u2022) .\nLemma B.1 (Worst-Case CMP). For any t-step trajectory h t \u2208 H [T ] , taking a sub-optimal action a \u2208 A \\ {a * } at step t in the worst-case CMP M gives a final entropy\nwhere the minimum is attained by\nProof. The worst-case CMP M is designed such that the agent cannot recover from taking a sub-optimal action a t \u2208 A\\{a * } as it is absorbed by a worst-case state given the trajectory h t . A worst-case state is one that maximizes the visitation frequency in h t , i.e., s \u2208 arg max s\u2208S d ht (s), so that the visitation frequency becomes increasingly unbalanced. A sub-optimal action at the first step in M leads to T \u2212 1 visits to the initial state s 0 \u223c \u00b5, and the final entropy is zero.\n, taking a sub-optimal action a \u2208 A \\ {a * } at step t in the best-case CMP M gives a final entropy\nwhere the maximum is attained by\nin which s * 2 is any state that is the second-closest to a uniform entry in d ht\u2295h T \u2212t .\nProof. The best-case CMP M is designed such that taking a sub-optimal action a \u2208 A \\ {a * } at step t minimally decreases the final entropy. Especially, instead of reaching at step t + 1 an optimal state s * , i.e., a state that maximally balances the state visits of the final trajectory, the agent is drawn to the second-to-optimal state s * 2 , from which it gets back on track on the optimal trajectory for the remaining steps. Note that visiting s * 2 cannot lead to the optimal final entropy, achieved when s * is visited at step t + 1, due to the sub-optimality of action a at step t and Assumption 1.\nInstantaneous Regret Although the objective ( 2) is non-additive across time steps, we can still define a notion of pseudoinstantaneous regret by comparing the regret-to-go of two subsequent time steps. In the following, we provide the definition of this expected pseudo-instantaneous regret along with lower and upper bounds to the regret suffered by an optimal Markovian policy.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition B.3 (Expected Pseudo-Instantaneous Regret).", "text": "Consider a policy \u03c0 \u2208 \u03a0 interacting with a CMP over T \u2212 t steps starting from the trajectory h t . We define the expected pseudo-instantaneous regret of \u03c0 at step t as r t (\u03c0) :\n. \nfrom Lemma 4.6. Then, we can write", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.3. Proofs of Section 5", "text": "Theorem 5.4. \u03a8 0 is NP-hard.\nProof. To prove the theorem, it is sufficient to show that there exists a problem \u03a8 c \u2208 NP-hard so that \u03a8 c \u2264 p \u03a8 0 . We show this by reducing 3SAT, a well-known NP-complete problem, to \u03a8 0 . To derive the reduction we consider two intermediate problems, namely \u03a8 1 and \u03a8 2 . Especially, we aim to show that the following chain of reductions hold:\nFirst, we define \u03a8 1 and we prove that Then, the reduction \u03a8 0 \u2265 m \u03a8 1 works as follows. We denote as I \u03a8i the set of possible instances of problem \u03a8 i . We show that \u03a8 0 is harder than \u03a8 1 by defining the polynomial-time functions \u03c8 and \u03c6 such that any instance of \u03a8 1 can be rewritten through \u03c8 as an instance of \u03a8 0 , and a solution \u03c0 * NM \u2208 \u03a0 NM for \u03a8 0 can be converted through \u03c6 into a solution \u03c0 * M \u2208 \u03a0 M for the original instance of \u03a8 1 .\nThe function \u03c8 sets S = \u2126 and derives the transition model of M from the one of M R \u2126 , while \u03c6 converts the optimal solution of \u03a8 0 by computing\nwhere H o stands for the set of histories h \u2208 H t ending in the observation o \u2208 \u2126. Thus, we have that \u03a8 0 \u2265 m \u03a8 1 holds. We now define \u03a8 2 as the policy existence problem w.r.t. the problem statement of \u03a8 1 . Hence, \u03a8 2 is the problem of determining whether the value of a reward-maximizing Markovian policy \u03c0 * M \u2208 arg max \u03c0\u2208\u03a0M J M R \u2126 (\u03c0) is greater than 0. Since computing an optimal policy in POMDPs is in general harder than the relative policy existence problem (Lusena et al., 2001, Section 3), we have that \u03a8 1 \u2265 p \u03a8 2 .\nFor the last reduction, i.e., \u03a8 2 \u2265 p 3SAT, we extend the proof of Theorem 4.13 in (Mundhenk et al., 2000), which states that the policy existence problem for POMDPs is NP-complete. In particular, we show that this holds within the restricted class of POMDPs defined in \u03a8 1 .\nThe restrictions on the POMDPs class are the following:", "publication_ref": ["b24", "b25"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "The steady-state control problem for Markov decision processes", "journal": "", "year": "2013", "authors": "S Akshay; N Bertrand; S Haddad; L Helouet"}, {"ref_id": "b1", "title": "Computational complexity: a modern approach", "journal": "Cambridge University Press", "year": "2009", "authors": "S Arora; B Barak"}, {"ref_id": "b2", "title": "Optimal control of Markov decision processes with incomplete state estimation", "journal": "Journal Mathematical Analysis and Applications", "year": "1965", "authors": "K J Astrom"}, {"ref_id": "b3", "title": "Introduction to probability", "journal": "", "year": "2002", "authors": "D P Bertsekas; J N Tsitsiklis"}, {"ref_id": "b4", "title": "Language models are few-shot learners", "journal": "", "year": "2020", "authors": "T Brown; B Mann; N Ryder; M Subbiah; J D Kaplan; P Dhariwal; A Neelakantan; P Shyam; G Sastry; A Askell; S Agarwal; A Herbert-Voss; G Krueger; T Henighan; R Child; A Ramesh; D Ziegler; J Wu; C Winter; C Hesse; M Chen; E Sigler; M Litwin; S Gray; B Chess; J Clark; C Berner; S Mccandlish; A Radford; I Sutskever; Amodei ; D "}, {"ref_id": "b5", "title": "Coverage as a principle for discovering transferable behavior in reinforcement learning", "journal": "", "year": "2021", "authors": "V Campos; P Sprechmann; S Hansen; A Barreto; S Kapturowski; A Vitvitskyi; A P Badia; C Blundell"}, {"ref_id": "b6", "title": "Exploration-exploitation trade-off in reinforcement learning on online Markov decision processes with global concave rewards", "journal": "", "year": "2019", "authors": "W C Cheung"}, {"ref_id": "b7", "title": "Regret minimization for reinforcement learning with vectorial feedback and complex objectives", "journal": "", "year": "2019", "authors": "W C Cheung"}, {"ref_id": "b8", "title": "A survey on policy search for robotics. Foundations and Trends in Robotics", "journal": "", "year": "2013", "authors": "M P Deisenroth; G Neumann; J Peters"}, {"ref_id": "b9", "title": "The difficulty of training deep architectures and the effect of unsupervised pre-training", "journal": "", "year": "2009", "authors": "D Erhan; P.-A Manzagol; Y Bengio; S Bengio; P Vincent"}, {"ref_id": "b10", "title": "Why does unsupervised pre-training help deep learning?", "journal": "", "year": "2010", "authors": "D Erhan; A Courville; Y Bengio; P Vincent"}, {"ref_id": "b11", "title": "Geometric entropic exploration", "journal": "", "year": "2021", "authors": "Z D Guo; M G Azar; A Saade; S Thakoor; B Piot; B A Pires; M Valko; T Mesnard; T Lattimore; R Munos"}, {"ref_id": "b12", "title": "", "journal": "", "year": "2015", "authors": "A Hallak; D Di Castro; S Mannor"}, {"ref_id": "b13", "title": "Provably efficient maximum entropy exploration", "journal": "", "year": "2019", "authors": "E Hazan; S Kakade; K Singh; A Van Soest"}, {"ref_id": "b14", "title": "Long short-term memory", "journal": "Neural Computation", "year": "1997", "authors": "S Hochreiter; J Schmidhuber"}, {"ref_id": "b15", "title": "Reward-free exploration for reinforcement learning", "journal": "", "year": "2020", "authors": "C Jin; A Krishnamurthy; M Simchowitz; Yu ; T "}, {"ref_id": "b16", "title": "Planning and acting in partially observable stochastic domains", "journal": "Artificial Intelligence", "year": "1998", "authors": "L P Kaelbling; M L Littman; Cassandra ; A R "}, {"ref_id": "b17", "title": "Bandit based Monte-Carlo planning", "journal": "", "year": "2006", "authors": "L Kocsis; C Szepesv\u00e1ri"}, {"ref_id": "b18", "title": "RL for latent MDPs: Regret guarantees and a lower bound", "journal": "", "year": "2021", "authors": "J Kwon; Y Efroni; C Caramanis; S Mannor"}, {"ref_id": "b19", "title": "Non-Markovian policies occupancy measures", "journal": "", "year": "2022", "authors": "R Laroche; R T Combes; J Buckman"}, {"ref_id": "b20", "title": "URLB: Unsupervised reinforcement learning benchmark", "journal": "", "year": "2021", "authors": "M Laskin; D Yarats; H Liu; K Lee; A Zhan; K Lu; C Cang; L Pinto; P Abbeel"}, {"ref_id": "b21", "title": "Efficient exploration via state marginal matching", "journal": "", "year": "2019", "authors": "L Lee; B Eysenbach; E Parisotto; E Xing; S Levine; R Salakhutdinov"}, {"ref_id": "b22", "title": "Active pretraining with successor features", "journal": "", "year": "2021", "authors": "H Liu; P Abbeel;  Aps"}, {"ref_id": "b23", "title": "Behavior from the void: Unsupervised active pre-training", "journal": "", "year": "2021", "authors": "H Liu; P Abbeel"}, {"ref_id": "b24", "title": "Nonapproximability results for partially observable Markov decision processes", "journal": "Journal of Artificial Intelligence Research", "year": "2001", "authors": "C Lusena; J Goldsmith; M Mundhenk"}, {"ref_id": "b25", "title": "Complexity of finite-horizon Markov decision process problems", "journal": "Journal of the ACM (JACM)", "year": "2000", "authors": "M Mundhenk; J Goldsmith; C Lusena; Allender ; E "}, {"ref_id": "b26", "title": "An intrinsically-motivated approach for learning highly exploring and fast mixing policies", "journal": "", "year": "2020", "authors": "M Mutti; M Restelli"}, {"ref_id": "b27", "title": "Task-agnostic exploration via policy gradient of a non-parametric state entropy estimate", "journal": "", "year": "2021", "authors": "M Mutti; L Pratissoli; M Restelli"}, {"ref_id": "b28", "title": "Unsupervised reinforcement learning in multiple environments", "journal": "", "year": "2022", "authors": "M Mutti; M Mancassola; M Restelli"}, {"ref_id": "b29", "title": "k-means maximum entropy exploration", "journal": "", "year": "2022", "authors": "A Nedergaard; M Cook"}, {"ref_id": "b30", "title": "The complexity of Markov decision processes", "journal": "Mathematics of Operations Research", "year": "1987", "authors": "C H Papadimitriou; J N Tsitsiklis"}, {"ref_id": "b31", "title": "Reinforcement learning of motor skills with policy gradients", "journal": "Neural networks", "year": "2008", "authors": "J Peters; S Schaal"}, {"ref_id": "b32", "title": "Markov decision processes: discrete stochastic dynamic programming", "journal": "John Wiley & Sons", "year": "2014", "authors": "M L Puterman"}, {"ref_id": "b33", "title": "State entropy maximization with random encoders for efficient exploration", "journal": "", "year": "2021", "authors": "Y Seo; L Chen; J Shin; H Lee; P Abbeel; K Lee"}, {"ref_id": "b34", "title": "An analysis of modelbased interval estimation for Markov decision processes", "journal": "Journal of Computer and System Sciences", "year": "2008", "authors": "A L Strehl; M L Littman"}, {"ref_id": "b35", "title": "Reinforcement learning: An introduction", "journal": "MIT press", "year": "2018", "authors": "R S Sutton; A G Barto"}, {"ref_id": "b36", "title": "Active exploration in Markov decision processes", "journal": "", "year": "2019", "authors": "J Tarbouriech; A Lazaric"}, {"ref_id": "b37", "title": "A provably efficient sample collection strategy for reinforcement learning", "journal": "", "year": "2021", "authors": "J Tarbouriech; M Pirotta; M Valko; A Lazaric"}, {"ref_id": "b38", "title": "", "journal": "Machine learning", "year": "1992", "authors": "C J Watkins; P Dayan"}, {"ref_id": "b39", "title": "A learning algorithm for continually running fully recurrent neural networks", "journal": "Neural computation", "year": "1989", "authors": "R J Williams; D Zipser"}, {"ref_id": "b40", "title": "Reinforcement learning with prototypical representations", "journal": "", "year": "2021", "authors": "D Yarats; R Fergus; A Lazaric; L Pinto"}, {"ref_id": "b41", "title": "Exploration by maximizing R\u00e9nyi entropy for reward-free RL framework", "journal": "", "year": "2021", "authors": "C Zhang; Y Cai; L Huang; J Li"}, {"ref_id": "b42", "title": "Variational policy gradient method for reinforcement learning with general utilities", "journal": "", "year": "2020", "authors": "J Zhang; A Koppel; A S Bedi; C Szepesvari; Wang ; M "}, {"ref_id": "b43", "title": "It suffices to add states with deterministic transitions so that T = m \u2022 n can be defined a priori, where T is the number of steps needed to reach the state with positive reward through every possible path. Here m is the number of clauses", "journal": "", "year": "2000", "authors": ""}, {"ref_id": "b44", "title": "Noticing that the set of observations corresponds with the set of variables and that from the previous point T = m \u2022 n, we have that | \u2126| T = n m\u2022n , while the POMDPs class used by the proof hereinabove has S = m \u2022 n 2 . Notice that n \u2265 2 and m \u2265 1 implies that n m\u2022n \u2265 m \u2022 n 2 . Moreover, notice that every instance of 3SAT has m \u2265 1 and n \u2265 3. Hence, to extend the proof to the POMDPs class defined by \u03a8 1 it suffices to add a set of states S", "journal": "", "year": "", "authors": "S = | \u2126| T"}, {"ref_id": "b45", "title": "Since the chain \u03a8 0 \u2265 m \u03a8 1 \u2265 p \u03a8 2 \u2265 p 3SAT holds", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. Illustrative two-rooms domain. The agent starts in the middle, colored traces represent optimal strategies to explore the left and the right room.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "A policy \u03c0 \u2208 \u03a0 interacting with a CMP induces a t-step state distribution d \u03c0 t (s) := P r(s t = s|\u03c0) over S(Puterman, 2014). This distribution is described by the temporal relation d \u03c0 t (s) = S A d \u03c0 t\u22121 (s , a )P (s|s , a ) ds da , whered \u03c0 t (\u2022, \u2022) \u2208 \u2206(S \u00d7 A) is the t-step state-action distribution.We call the asymptotic fixed point of this temporal relation the stationary state distribution d \u03c0 \u221e (s) := lim t\u2192\u221e d \u03c0 t (s), and we denote asd \u03c0 \u03b3 (s) := (1 \u2212 \u03b3) \u221e t=0 \u03b3 t d \u03c0 t (s) its \u03b3discounted counterpart, where \u03b3 \u2208 (0, 1) is the discount factor. A marginalization of the t-step state distribution over a finite horizon T , i.e., d \u03c0 T (s) := 1 T t\u2208[T ] d \u03c0 t(s), is called the marginal state distribution. The state visitation frequency d h (s) = 1 T t\u2208[T ] 1(s t = s|h) is a realization of the marginal state distribution, such that E h\u223cp \u03c0 T d h (s) = d \u03c0 T (s), where the distribution over histories p \u03c0 T \u2208 \u2206(H T ) is defined as p \u03c0 T (h) = \u00b5(s 0 ) t\u2208[T \u22121] \u03c0(a t |h t )P (s t+1 |a t , s t ). Markov Decision Process A CMP M paired with a reward function R : S \u00d7 A \u2192 R is called a Markov Decision Process (MDP) (Puterman, 2014) M R := M \u222a R. We denote with R(s, a) the expected immediate reward when taking action a \u2208 A in s \u2208 S, and with R(h) = t\u2208[T ] R(s t , a t ) the return over the horizon T . The performance of a policy \u03c0 over the MDP M R is defined as the average return", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "ExtendedMDP The problem of finding an optimal non-Markovian policy with history-length T in an MDP M R , i.e., \u03c0 * NM \u2208 arg max \u03c0\u2208\u03a0NM J M R (\u03c0), can be reformulated as the one of finding an optimal Markovian policy \u03c0 * M \u2208 arg max \u03c0\u2208\u03a0M J M R T (\u03c0) in an extended MDP M R T . The extended MDP is defined as M R T := ( S, A, P , R, \u00b5), in which S \u2286 H [T ] = H 1 \u222a . . . \u222a H T , and s := ( s 0 , . . . , s \u22121 ) corresponds to a history in M R of length | s|, A = A, P ( s | s, a) = P (s = s \u22121 |s = s \u22121 , a = a), R( s, a) = R(s = s \u22121 , a = a), and \u00b5( s) = \u00b5(s = s) for any s \u2208 S of unit length. Partially Observable MDP A Partially Observable Markov Decision Process (POMDP)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "the corresponding sets of state distributions over a CMP. We can prove that:(i) The sets of stationary state distributions are equivalent D \u221e NM \u2261 D \u221e M ; (ii) The sets of discounted state distributions are equivalent D \u03b3 NM \u2261 D \u03b3 M for any \u03b3; (iii) The sets of marginal state distributions are equivalent D T NM \u2261 D T M for any T . Proof Sketch. For any non-Markovian policy \u03c0 \u2208 \u03a0 NM inducing distributions d \u03c0 t (\u2022), d \u03c0 t (\u2022, \u2022) over the states and the state-action pairs of the CMP, we can build a Markovian policy \u03c0 \u2208 \u03a0 M , \u03c0 = (\u03c0 t ) \u221e t=0 through the construction \u03c0 t (a|s) = d \u03c0 t (s, a) d \u03c0 t (s), \u2200s \u2208 S, \u2200a \u2208 A. From (Puterman, 2014, Theorem 5.5.1) we know that d \u03c0 t (s) = d \u03c0 t (s) holds for any t \u2265 0 and \u2200s \u2208 S. This implies that d \u03c0 \u221e", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Theorem 4.2 (Non-Markovian Optimality). For every CMP M and trajectory h t \u2208 H [T ] , there exists a deterministic non-Markovian policy \u03c0 NM \u2208 \u03a0 D NM that suffers zero regret-", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Lemma 4.4. For every CMP M and trajectory h t \u2208 H [T ] , there exists a deterministic non-Markovian policy \u03c0 NM \u2208 \u03a0 D NM such that \u03c0 NM \u2208 arg max \u03c0\u2208\u03a0NM E(\u03c0), which suffers zero regret-to-go R T \u2212t (\u03c0 NM , h t ) = 0. Proof. The result R T \u2212t (\u03c0 NM , h t ) = 0 is straightforward by noting that the set of non-Markovian policies \u03a0 NM with arbitrary history-length is as powerful as the general set of policies \u03a0. To show that there exists a deterministic \u03c0 NM , we consider the extended MDP M R T obtained from the CMP M as in Section 2, in which the extended reward function is R( s, a) = H(d s (\u2022)) for every a \u2208 A, and every s \u2208 S such that | s| = T , and R( s, a) = 0 otherwise. Since a Markovian policy \u03c0 M \u2208 \u03a0 D M on M R T can be mapped to a non-Markovian policy \u03c0 NM \u2208 \u03a0 D NM on M, and it is well-known (Puterman, 2014) that for any MDP there exists an optimal deterministic Markovian policy, we have that \u03c0 M \u2208 arg max \u03c0\u2208\u03a0M J M R T (\u03c0) implies \u03c0 NM \u2208 arg max \u03c0\u2208\u03a0NM E(\u03c0).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Lemma 4.5. Let \u03c0 NM \u2208 \u03a0 D NM a non-Markovian policy such that \u03c0 NM \u2208 arg max \u03c0\u2208\u03a0 E(\u03c0) on a CMP M.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "\u03c0\u2208\u03a0 D NM E(\u03c0), and H * , H * 2 are given 3 Note that the determinism of \u03c0NM does not also imply Var hs\u223cp \u03c0 NM t E B(\u03c0NM(a * |hs)) = 0, as the optimal action a = \u03c0NM(hs) may vary for different histories, which results in the inner expectations E B(\u03c0NM(a * |hs)) being either 1 or 0.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "by", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "First, we define \u03a8 11and we prove that \u03a8 0 \u2265 m \u03a8 1 . Informally, \u03a8 1 is the problem of finding a reward-maximizing Markovian policy \u03c0 M \u2208 \u03a0 M w.r.t. the entropy objective (2) encoded through a reward function in a convenient POMDP M R \u2126 . We can build M R \u2126 from the CMP M similarly as the extended MDP M R T (see Section 2 and the proof of Lemma 4.4 for details), except that the agent only access the observation space \u2126 instead of the extended state space S.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "\u03a8 1 .1The function \u03c8 sets S = \u2126 and derives the transition model of M from the one of M R \u2126 , while \u03c6 converts the optimal solution of \u03a8 0 by computing\u03c0 * M (a|o, t) = ho\u2208Ho p \u03c0 * NM T (ho)\u03c0 * NM (a|ho), where H o stands for the set of histories h \u2208 H t ending in the observation o \u2208 \u2126. Thus, we have that \u03a8 0 \u2265 m \u03a8 1 holds. We now define \u03a8 2 as the policy existence problem w.r.t. the problem statement of \u03a8 1 . Hence, \u03a8 2 is the problem of determining whether the value of a reward-maximizingThe Importance of Non-Markovianity in Maximum State Entropy Exploration", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 2 .2Figure2. In (a, d), we illustrates the 3State and River Swim CMPs. Then, we report the average entropy induced by an optimal (stationary) Markovian policy \u03c0M and an optimal non-Markovian policy \u03c0NM in the 3State (T = 9) (b) and the River Swim (T = 10) (e). In (c) we report the entropy frequency in the 3State, in (f) the state visitation frequency in the River Swim. We provide 95% c.i. over 100 runs.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Lemma 4.5. Let \u03c0 NM \u2208 \u03a0 D NM a non-Markovian policy such that \u03c0 NM \u2208 arg max \u03c0\u2208\u03a0 E(\u03c0) on a CMP M. For a fixed history h t \u2208 H t ending in state s, the variance of the event of an optimal Markovian policy \u03c0 M \u2208 arg max \u03c0\u2208\u03a0M E(\u03c0) taking a * = \u03c0 NM (h t ) in s is given by Var B(\u03c0 M (a * |s, t)) = Var hs\u223cp \u03c0 NM t E B(\u03c0 NM (a * |hs)) ,", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Var B(\u03c0 M (a * |s, t)) = E hs\u223cp \u03c0 NM t Var B(\u03c0 NM (a * |hs)) + Var hs\u223cp \u03c0 NM t E B(\u03c0 NM (a * |hs)) .", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "2 \u03c02Lemma 4.6. Let \u03c0 M be an optimal Markovian policy \u03c0 M \u2208 arg max \u03c0\u2208\u03a0M E(\u03c0) on a CMP M. For any h t \u2208 H[T ] , it holds R T \u2212t (\u03c0 M ) \u2264 R T \u2212t (\u03c0 M ) \u2264 R T \u2212t (\u03c0 M ) such that R T \u2212t (\u03c0 M ) = H * \u2212 H * M (a * |s t ) Var hst\u223cp \u03c0 NM t E B(\u03c0 NM (a * |hs t )) , R T \u2212t (\u03c0 M ) = H * \u2212 H * \u03c0 M (a * |s t ) Var hst\u223cp \u03c0 NM t E B(\u03c0 NM (a * |hs t )) ,where \u03c0 NM \u2208 arg max \u03c0\u2208\u03a0 D NM E(\u03c0), and H * , H * 2", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "2 =22 and Lemma B.1. Especially, we can writeR T \u2212t (\u03c0 M ) = H * \u2212 E h T \u2212t \u223cp \u03c0 M T \u2212t H d ht\u2295h T \u2212t (\u2022) \u2265 H * \u2212 \u03c0 M (a * |s t )H * \u2212 1 \u2212 \u03c0 M (a * |s t ) H * (H * \u2212 H * 2 ) 1 \u2212 \u03c0 M (a * |s t )andR T \u2212t (\u03c0 M ) = H * \u2212 E h T \u2212t \u223cp \u03c0 M T \u2212t H d ht\u2295h T \u2212t (\u2022) \u2264 H * \u2212 \u03c0 M (a * |s t )H * \u2212 1 \u2212 \u03c0 M (a * |s t ) H * = (H * \u2212 H * ) 1 \u2212 \u03c0 M (a * |s t ) .", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "2 \u03c02Bernoulli distribution B with parameter 1 \u2212 \u03c0 M (a * |s t ) . By combining the equation of the variance of a Bernoulli random variable with Lemma 4.5 we obtainVar B(\u03c0 M (a * |s t )) = \u03c0 M (a * |s t ) 1 \u2212 \u03c0 M (a * |s t ) = Var hs\u223cp \u03c0 NM t E B(\u03c0 NM (a * |hs t )) which gives R T \u2212t (\u03c0 M ) \u2265 H * \u2212 H * M (a * |s t ) Var hs\u223cp \u03c0 NM t E B(\u03c0 NM (a * |hs t )) := R T \u2212t (\u03c0 M ) R T \u2212t (\u03c0 M ) \u2264 H * \u2212 H * \u03c0 M (a * |s t ) Varhs\u223cp \u03c0 NM t E B(\u03c0 NM (a * |hs t )) := R T \u2212t (\u03c0 M )", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Their algorithm, called MEPOL, is able to learn a single exploration policy that maximizes the entropy of the marginal state distribution in challenging continuous control domains.Liu & Abbeel (2021b) combine non-parametric entropy estimation with learned state representations into an algorithm, called APT, that successfully addresses MSE exploration problems in visual-inputs domains.Seo et al. (2021) shows that even random state representations are sufficient to learn MSE exploration policies from visual inputs. On a similar line, Yarats et al. (2021) consider simultaneously learning state representations and a basis for the latent space (or prototypical representations) to help reducing the variance of the entropy estimates. Finally,Liu & Abbeel (2021a) consider a method, called APS, to learn a set of code-conditioned policies that collectively maximizes the MSE objective by coupling non-parametric entropy estimation and successor representation.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "J M R (\u03c0) = E h\u223cp \u03c0 T [R(h)]", "formula_coordinates": [3.0, 135.12, 566.15, 104.92, 16.07]}, {"formula_id": "formula_1", "formula_text": "E \u221e (\u03c0) := H d \u03c0 (\u2022) = \u2212 E s\u223cd \u03c0 log d \u03c0 (s) ,(1)", "formula_coordinates": [3.0, 336.22, 346.52, 205.22, 17.59]}, {"formula_id": "formula_2", "formula_text": "Theorem 3.1. Let x \u2208 {\u221e, \u03b3, T }, and let D x NM = {d \u03c0 x (\u2022) : \u03c0 \u2208 \u03a0 NM }, D x M = {d \u03c0 x (\u2022) : \u03c0 \u2208 \u03a0 M }", "formula_coordinates": [3.0, 307.11, 555.91, 235.72, 25.29]}, {"formula_id": "formula_3", "formula_text": "(\u2022) = d \u03c0 \u221e (\u2022), d \u03c0 \u03b3 (\u2022) = d \u03c0 \u03b3 (\u2022), d \u03c0 T (\u2022) = d \u03c0 T (\u2022), from which D x NM \u2261 D x M follows. See Appendix B.1 for a de- tailed proof.", "formula_coordinates": [4.0, 55.08, 129.51, 236.01, 35.91]}, {"formula_id": "formula_4", "formula_text": "E(\u03c0) := E h\u223cp \u03c0 T H d h (\u2022) = \u2212 E h\u223cp \u03c0 T E s\u223cd h log d h (s) . (2)", "formula_coordinates": [4.0, 62.02, 606.26, 227.42, 27.92]}, {"formula_id": "formula_5", "formula_text": "R T \u2212t (\u03c0, h t ) = H * \u2212 E h T \u2212t \u223cp \u03c0 T \u2212t H d ht\u2295h T \u2212t (\u2022) ,", "formula_coordinates": [4.0, 318.09, 239.0, 212.7, 22.7]}, {"formula_id": "formula_6", "formula_text": "H * = max \u03c0 * \u2208\u03a0 E h * T \u2212t \u223cp \u03c0 * T \u2212t H d ht\u2295h * T \u2212t (\u2022)", "formula_coordinates": [4.0, 334.97, 271.06, 187.91, 18.46]}, {"formula_id": "formula_7", "formula_text": "to-go R T \u2212t (\u03c0 NM , h t ) = 0, whereas for any \u03c0 M \u2208 \u03a0 M we have R T \u2212t (\u03c0 M , h t ) \u2265 0.", "formula_coordinates": [4.0, 307.44, 529.0, 234.0, 27.55]}, {"formula_id": "formula_8", "formula_text": "Var B(\u03c0 M (a * |s, t)) = Var hs\u223cp \u03c0 NM t E B(\u03c0 NM (a * |hs)) ,", "formula_coordinates": [5.0, 309.13, 190.48, 230.61, 20.71]}, {"formula_id": "formula_9", "formula_text": "Var B(\u03c0 M (a * |s, t)) = E hs\u223cp \u03c0 NM t Var B(\u03c0 NM (a * |hs)) + Var hs\u223cp \u03c0 NM t E B(\u03c0 NM (a * |hs)) , \u2200s \u2208 S.", "formula_coordinates": [5.0, 307.44, 319.06, 237.28, 44.05]}, {"formula_id": "formula_10", "formula_text": "E hs\u223cp \u03c0 NM t", "formula_coordinates": [5.0, 307.44, 400.48, 40.35, 11.07]}, {"formula_id": "formula_11", "formula_text": "Lemma 4.6. Let \u03c0 M be an optimal Markovian policy \u03c0 M \u2208 arg max \u03c0\u2208\u03a0M E(\u03c0) on a CMP M. For any h t \u2208 H [T ] , it holds R T \u2212t (\u03c0 M ) \u2264 R T \u2212t (\u03c0 M ) \u2264 R T \u2212t (\u03c0 M ) such that R T \u2212t (\u03c0 M ) = H * \u2212 H * 2 \u03c0 M (a * |s t ) Var hst\u223cp \u03c0 NM t E B(\u03c0 NM (a * |hs t )) , R T \u2212t (\u03c0 M ) = H * \u2212 H * \u03c0 M (a * |s t ) Var hst\u223cp \u03c0 NM t E B(\u03c0 NM (a * |hs t )) ,", "formula_coordinates": [5.0, 307.44, 541.02, 241.37, 100.65]}, {"formula_id": "formula_12", "formula_text": "H * = min h\u2208H T \u2212t H(d ht\u2295h (\u2022)), H * 2 = max h\u2208H T \u2212t \\H * T \u2212t H(d ht\u2295h (\u2022)) s.t. H * T \u2212t = arg max h\u2208H T \u2212t H(d ht\u2295h (\u2022)).", "formula_coordinates": [6.0, 100.28, 90.68, 144.31, 63.27]}, {"formula_id": "formula_13", "formula_text": "R T \u2212t (\u03c0 M ) \u2265 H * \u2212 \u03c0 M (a * |s t )H * \u2212 1 \u2212 \u03c0 M (a * |s t ) H * 2 and R T \u2212t (\u03c0 M ) \u2264 H * \u2212 \u03c0 M (a * |s t )H * \u2212 1 \u2212 \u03c0 M (a * |s t ) H * .", "formula_coordinates": [6.0, 55.44, 213.97, 234.0, 43.1]}, {"formula_id": "formula_14", "formula_text": "Var B(\u03c0 M (a * |s t )) = \u03c0 M (a * |s t ) 1 \u2212 \u03c0 M (a * |s t ) = Var hs\u223cp \u03c0 NM t E B(\u03c0 NM (a * |hs t ))", "formula_coordinates": [6.0, 55.44, 252.83, 234.0, 28.07]}, {"formula_id": "formula_15", "formula_text": "\u03a8 0 := maximize \u03c0\u2208\u03a0NM E(\u03c0),", "formula_coordinates": [6.0, 125.93, 455.18, 93.03, 17.49]}, {"formula_id": "formula_16", "formula_text": "\u03a8 0 \u2265 m \u03a8 1 \u2265 p \u03a8 2 \u2265 p 3SAT.", "formula_coordinates": [6.0, 365.07, 389.82, 118.74, 12.44]}, {"formula_id": "formula_17", "formula_text": "0 < Var B(\u03c0 M (a * |s t )) = Var hst\u223cp \u03c0 NM t E B(\u03c0 NM (a * |hs t )) ,", "formula_coordinates": [13.0, 172.85, 261.53, 251.17, 20.71]}, {"formula_id": "formula_18", "formula_text": "2 = E h E A 2 |s, t, h \u2212 E h E A|s, t, h 2 = E h Var A|s, t, h + E A|s, t, h 2 \u2212 E h E \u03c0 A|s, t, h 2 = E h Var A|s, t, h + E h E A|s, t, h 2 \u2212 E h E A|s, t, h 2 = E h Var A|s, t, h + Var h E A|s, t, h .(3)", "formula_coordinates": [13.0, 194.07, 468.43, 347.37, 113.43]}, {"formula_id": "formula_20", "formula_text": "R T \u2212t (\u03c0 M , h t ) = H * \u2212 E h T \u2212t \u223cp \u03c0 M T \u2212t H d ht\u2295h T \u2212t (\u2022) ,", "formula_coordinates": [14.0, 188.44, 299.67, 220.01, 24.08]}], "doi": ""}