{"title": "On Adversarial Robustness: A Neural Architecture Search perspective", "authors": "Chaitanya Devaguptapu; Devansh Agarwal; Gaurav Mittal; Pulkit Gopalani; Vineeth N Balasubramanian", "pub_date": "", "abstract": "Adversarial robustness of deep learning models has gained much traction in the last few years. Various attacks and defenses are proposed to improve the adversarial robustness of modern-day deep learning architectures. While all these approaches help improve the robustness, one promising direction for improving adversarial robustness is unexplored, i.e., the complex topology of the neural network architecture. In this work, we address the following question: \"Can the complex topology of a neural network give adversarial robustness without any form of adversarial training?\". We answer this empirically by experimenting with different hand-crafted and NAS-based architectures. Our findings show that, for small-scale attacks, NAS-based architectures are more robust for small-scale datasets and simple tasks than hand-crafted architectures. However, as the size of the dataset or the complexity of task increases, hand-crafted architectures are more robust than NAS-based architectures. Our work is the first large-scale study to understand adversarial robustness purely from an architectural perspective. Our study shows that random sampling in the search space of DARTS (a popular NAS method) with simple ensembling can improve the robustness to PGD attack by nearly 12%. We show that NAS, which is popular for achieving SoTA accuracy, can provide adversarial accuracy as a free add-on without any form of adversarial training. Our results show that leveraging the search space of NAS methods with methods like ensembles can be an excellent way to achieve adversarial robustness without any form of adversarial training. We also introduce a metric that can be used to calculate the trade-off between clean accuracy and adversarial robustness. Code and pre-trained models will be made available at https: //github.com/tdchaitanya/nas-robustness", "sections": [{"heading": "Introduction", "text": "The choice of neural network architecture and its complex topology play a crucial role in improving the performance of several deep learning based applications. However, in most cases, these architectures are typically designed by experts in an ad-hoc, trial-and-error fashion. Early efforts on Neural Architecture Search (NAS) [48] alleviate the pain of hand-designing these architectures by partially automating the process of finding the right topology that can result in best-performing architectures. Since the work by [48], there has been much interest in this space. Many researchers have come up with unique approaches [45,4,31] to improve the performance besides decreasing the computational cost. Current SoTA (state-of-the-art) on image classification and object detection [38,39] are developed using NAS, which shows how important a role NAS plays in solving standard learning tasks, especially in computer vision.\nAdversarial robustness is defined as the accuracy of a model when adversarial examples (images perturbed with some imperceptible noise) are provided as input. Adversarial examples have the potential to be dangerous. [29] discusses an example where attackers could target autonomous arXiv:2007.08428v4 [cs.LG] 26 Aug 2021 vehicles by using stickers or paint to create an adversarial stop sign that the vehicle could interpret as a yield or other sign. One commonly used technique to improve the adversarial robustness of neural networks is adversarial training, but in most of the cases adversarial training decreases the accuracy on clean (un-perturbed) samples [23,47]. So, it is essential to develop architectures that are inherently robust without any form of adversarial training. This forms the primary motivation of our work, Can the complex topology of a neural network architecture provide adversarial robustness without any form of adversarial training?\nIn an attempt to understand adversarial robustness purely from an architectural perspective, we seek to answer the following questions,\n\u2022 In the absence of adversarial training, how do NASbased architectures compare with hand-crafted architectures (like ResNets [13], DenseNets [14], etc.) in terms of adversarial robustness?\n\u2022 Does an increase in the number of parameters of the architecture help improve robustness?\n\u2022 Where does the source of adversarial vulnerability lie for NAS? Is it in the search space or in the way the current methods are performing the search?\nTo the best of our knowledge, our work is the first attempt at understanding adversarial robustness purely from an architectural perspective. We show that the complex topology of neural network architectures can be leveraged to achieve robustness without adversarial training. Additionally, we introduce two simple metrics, Harmonic Robustness Score (HRS) and Per-parameter HRS (PP-HRS) that combine: (1) the total number of parameters in a model; and (2) accuracy on both clean and perturbed samples, to convey how robust and deployment-ready a given model is when no adversarial training is performed.\nWe examine the adversarial robustness of different handcrafted and NAS-based architectures in a wide range of scenarios and find that for large-scale datasets, complex tasks and stronger attacks (like PGD [23]), traditional handcrafted architectures like ResNets and DenseNets are more robust than NAS-based architectures (Figure 1). This suggests that the adversarial robustness of a model depends significantly on network topology. Results of our study can be used to design network architectures that can give adversarial robustness with no additional adversarial training along with SoTA performance on unperturbed samples.", "publication_ref": ["b48", "b48", "b45", "b3", "b30", "b38", "b39", "b28", "b22", "b47", "b12", "b13", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Adversarial Attacks and Robustness: Adversarial examples, in general, refer to samples that are imperceptible to the human eye but can fool a deep classifier to predict a non-true class with high confidence. Adversarial examples can result in degraded performance even in the presence of perturbations too subtle to be perceived by humans.\nExisting adversarial attacks can be broadly classified into white-box and black-box attacks. The difference between these lies in the knowledge of the adversaries. In white-box attacks, the adversaries have the full knowledge of the target model, including the model architecture and parameters. In a black-box setting, the adversaries knowledge is very limited and may not know details about the model.\nIn the frameworks of these threat models, several effective adversarial attacks have been proposed over the years such as L-BFGS [37], FGSM [10], BIM [19], C&W attacks [2] JSMA [30], Deep-Fool [25], R-FGSM [40],\nStepLL [18], PGD [23] and most recently SparseFool [24], F-FGSM [42] and AutoPGD [6]. For more information on adversarial attacks and defenses, please see [3,33]. Whitebox is a stronger setting where attackers can access the model parameters and architecture. It is also closely related to the network topology aspect of our study. So we mainly focus on the white-box setting in our work and present some results on black-box attacks in the Appendix.\nOne popular way to improve the adversarial robustness of deep learning models is adversarial training (AT) [11]. The basic idea of AT is to create and incorporate adversarial samples during training. A critical downside of AT is that it is time-consuming [35]. In addition to the gradient computation needed to update the network parameters, each stochastic gradient descent (SGD) iteration requires multiple gradients computations to produce adversarial images.\nNeural Architecture Search (NAS) automates the design of neural network architectures for a given task. Over the years, several approaches have emerged to search architectures using methods ranging from Reinforcement Learning (RL) [48], Neuro-evolutionary approaches [32], Sequential Decision Processes [20], Oneshot methods [31] and fully differentiable Gradient-based methods [21]. While most of these algorithms attempt to search a cell architecture (micro search) due to the computational cost involved and repeat the cell a fixed number of times, few recent approaches have also demonstrated searching the full architecture (macro search).\nMost of the early approaches are based on RL and neuroevolutionary algorithms, making the search process computationally intensive. Recently these have been replaced by one-shot fully-differentiable gradient-based NAS methods, such as DARTS [21], which are orders of magnitude faster than non-differentiable techniques and have gained much traction recently. P-DARTS [5] bridges the gap between search and evaluation by progressively increasing search depth. Partially-Connected DARTS [44], a SoTA approach in NAS, significantly improves the efficiency of one-shot NAS by sampling parts of the super-network and adding edge normalization to reduce redundancy and uncertainty in search. DenseNAS [9], a more recent method, attempts to improve search space design by further searching block counts and block widths in a densely connected search space. Despite a plethora of these methods and their applications, there has been minimal effort to understand the adversarial robustness of final learned architectures.\nAdversarial Robustness of Architectures: [23] is one of the early papers to talk about adversarial robustness of network architectures. It shows that when training with unperturbed samples, increasing the capacity of the network in terms of width, depth, and the number of parameters can alone help improve the robustness for datasets like MNIST and CIFAR-10. Recently, [43] echoes this observation by showing the depth of the network helps to improve the adversarial robustness during adversarial training. Both [23,43] talk about robustness mainly in the context of adversarial training. However, our results show that when no adversarial training is performed, increasing parameters alone only helps to a certain point and beyond that, it reduces the adversarial robustness of the model.\nVery recently, there have been limited efforts to improve adversarial robustness using architecture search [12,41]. [12] proposes a robust architecture search framework by leveraging one-shot NAS. However, the proposed method adversarially trains the entire NAS search space before starting the search process, making it harder to assess the contribution of just the architecture to the adversarial robustness. [41] uses black-box attacks to generate a fixed set of adversarial examples on CIFAR-10 and uses these examples to search for a robust architecture using NAS. The experimental setting is constrained and does not reflect the true robustness of the model as the adversarial examples are fixed a priori. No study is done on white-box attacks. Both [12] and [41] do not make any comparisons with existing NAS methods (which, as per our study, are already robust to an extent).\nIn this work, we mainly focus on evaluating the robustness of SoTA NAS methods on white-box attacks across datasets of different sizes, including large-scale datasets such as ImageNet [7] and compare them with hand-crafted models like ResNets and DenseNets. As a part of our study, we introduce metrics that can be used to estimate the tradeoff between clean accuracy and adversarial robustness when comparing architectures within and across different families.", "publication_ref": ["b37", "b9", "b18", "b1", "b29", "b24", "b40", "b17", "b22", "b23", "b42", "b5", "b2", "b32", "b10", "b34", "b48", "b31", "b19", "b30", "b20", "b20", "b4", "b44", "b8", "b22", "b43", "b22", "b43", "b11", "b41", "b11", "b41", "b11", "b41", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Robustness of NAS models: A study", "text": "We carefully design our experimental setting to answer the questions stated in Section 1. We begin by describing the design of our experiments, providing details about datasets, models, attacks, and metrics.\nDatasets: Since we want to compare the robustness of architectures across different dataset scales and complexities, we choose four different image classification datasets. In addition to the standard CIFAR-10 [16] dataset, which consists of 60K images of 32 \u00d7 32 resolution, we also choose CIFAR-100 [17] to test if the same robustness trends hold when the labels turn from coarse to more fine-grained and the number of classes increase by a factor of 10.\nTo study the robustness trend for tougher tasks like finegrained image classification where the classes are semantically and perceptually more similar, we choose Flowers-102 dataset [26], which consists of 8189 flowers images split across 102 categories with number of images in each category being between 40 and 258. Since most real-world applications deal with large-scale datasets, we also test robustness on ImageNet [7] dataset, consisting of \u223c1.3M images from 1000 classes. This makes our study more complete when compared to earlier works.\nArchitectures: We select most commonly used NAS methods including DARTS [21], P-DARTS [5], Proxyless-NAS [1], NSGA-Net [22], along with recent methods like PC-DARTS [44] and DenseNAS [9]. We evaluate five wellknown handcrafted architectures and at least four NAS architectures on each dataset mentioned above for a fair comparison. For all experiments, we either use pre-trained models made available by the respective authors or train the models from scratch until we obtain the performance reported in the respective papers. For the results on Flowers-102 dataset, we explicitly search for an architecture using the code provided by [46]. The results for NSGA-Net are only available for CIFAR-10/100 because its implementation does not support Imagenet. Similarly, the implementation of DenseNAS does not support CIFAR-10/100, so the results are shown only for ImageNet. ProxylessNAS provides pre-trained models for only CIFAR-10 and ImageNet, so we show results only for these two datasets.\nEnsemble of Architectures: When compared with single architecture, an ensemble of architectures are known to be adversarially more robust [27]. To understand the effectiveness of ensembling, in Section 4.4, we random sample cells from the DARTS search space using the code provided by [46]; and stack these cells to create small architectures, since this is randomly sampling, the search cost associated with building these architectures is zero. After sampling, we follow the standard DARTS training protocol to train these architectures. In general, for the CIFAR-10 dataset, DARTS architectures having 20 cells are trained for 600 epochs. Following this, the number of epochs for training each network in the ensemble is determined based on the number of cells in that network. Effectively the ensemble as a whole is trained for 600 epochs to ensure we make a fair comparison with existing approaches. After training each of these networks separately, we train a simple linear model to combine the individual model outputs. This linear model is trained only for two epochs. The difference between standard DARTS and ensembling by sampling from DARTS search space is visually shown in Figure 2. More details on the structure of the linear model are discussed in Section 4.4 Adversarial Attacks: For adversarial robustness, we test against the standard attacks like FGSM [10], PGD [23] and also report results on recently introduced F-FGSM [42] and AutoPGD [6]. For all these attacks, we use a perturbation value of 8/255(0.03) which denotes the maximum noise added to each pixel in the input image as perturbation. The step size is 2/255 (0.007) with the attack iterations set as 10. Moreover, we run the AutoPGD attack with 10 random restarts. All these parameter choices are standard and widely used in the community [28,8,42]. Architectures are trained using standard training protocols, and no adversarial training is performed. We use the library provided by [15] for all the adversarial attacks in our experiments. Metrics: We use Clean Accuracy and Adversarial Accuracy as our performance metrics. Clean accuracy refers to the accuracy on the unperturbed test set as provided in the dataset. For each attack, we measure Adversarial accuracy by perturbing the test set examples using various attacks in the methods listed in the above section (FGSM, F-FGSM, PGD and AutoPGD).\nOne of the main problems with adversarially trained models is that their clean accuracy is usually less than standard non-adversarially trained models. Adversarial vulnerability is a side-effect of overfitting to the training set [34]. While this overfitting gives good performance on the clean test set, it makes the model vulnerable to adversarial examples. If the accuracy of a model on clean samples is not good, it is not useful when deployed in situations where unperturbed samples are more frequent. On the other hand, if the model has SoTA performance on a clean test-set, it becomes vulnerable to adversarial examples. There is no well-defined metric to capture this trade-off between clean and adversarial accuracy.\nTo this end, we introduce a metric, called Harmonic Robustness Score (HRS), that is defined as the harmonic mean of the clean and adversarial accuracy of a given model. HRS captures the balance of a model's performance to unperturbed inputs and robustness to an adversarial attack . Consider a model with clean accuracy C and Adversarial accuracy A (both in percentage), HRS for that model is calculated as follows:\nHRS = 2 \u2022 C \u2022 A C + A (1)\nThe harmonic mean is better at reflecting extreme differences in input values, compared to Arithmetic mean. Therefore, if one of the clean or adversarial accuracy is very low, then the harmonic mean of C and A would be more reflective of the same. A weighted version of HRS, the HRS \u03b2 score, can also be used. This is a measure of the model's performance to clean as well as perturbed images, weighted according to what the end-user prefers -clean accuracy, or the adversarial accuracy. For a given use case, one might be preferred more to the other, and hence this metric can be used accordingly. HRS \u03b2 is given by,\nHRS \u03b2 = (\u03b2 2 + 1) \u2022 C \u2022 A \u03b2 2 C + A (2)\nwhere \u03b2 can be interpreted as the importance of adversarial accuracy over clean accuracy. Since we do not have any particular preference to adversarial accuracy over clean accuracy (or vice-versa), we use \u03b2 = 1 for reporting HRS values. The adversarial accuracy for all models is measured on perturbed inputs obtained using the PGD [23] attack. (Choice of PGD is arbitrary, and can be replaced with any other attack).\nWhen comparing performances of architectures belonging to the same family, number of parameters play an important role. A huge parameter difference can easily improve clean and adversarial accuracy, but this comes with huge training and inference time. So we further define perparameter harmonic robustness score (PP-HRS) to measure the clear accuracy verses adversarial robustness trade-off within a family of architectures. PP-HRS compares the parameters of the model with the parameters in the baseline model of that family. In a family of architectures ( F ), consider a baseline model m b having p b number of parameters, now for a model m i \u2208 F with p i number of parameters, PP-HRS is calculated as follows,\nPP-HRS = HRS * p b f (p i )(3)\nwhere the function f (p i ) can be defined as per requirement. We use f (p i ) = p i as our function of choice for PP-HRS.\nThe main motivation behind this choice is to consider the improvement of accuracy with number of parameters. For example, in the EfficientNet [38] family, we can compute the average increase in accuracy per unit increase in the number of parameters, for the purpose of comparison.", "publication_ref": ["b16", "b25", "b6", "b20", "b4", "b0", "b21", "b44", "b8", "b46", "b26", "b46", "b9", "b22", "b42", "b5", "b27", "b7", "b42", "b14", "b33", "b22", "b38"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Analysis and Results", "text": "In this section, we compare and contrast the robustness of different architectures in a wide-range of scenarios and answer questions listed in Section 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "How do NAS-based models compare with hand-", "text": "crafted models in terms of architectural robustness?\nThe HRS and robustness of different hand-crafted and NAS-based architectures on CIFAR-10, CIFAR-100, Ima-geNet and Flowers-102 datasets are shown in Tables 1, 2, 3, 4 respectively.\nIn the case of CIFAR-10 and CIFAR-100, NAS-based architectures outperform hand-crafted architectures in terms of architectural robustness for attacks like FGSM and F-FGSM by a significant margin. However, for stronger and most commonly used attacks like PGD and AutoPGD, NAS-based architectures fail significantly compared to hand-crafted models. In terms of HRS, for CIFAR-10 dataset, the difference in the best-performing NAS and hand-crafted models is 21%.\nThis trend seen in CIFAR-10/100 for attacks like FGSM and F-FGSM do not hold for large-scale datasets like Ima-geNet and relatively complex tasks like fine-grained classification. In the case of Imagenet, handcrafted models are more robust than NAS-based architectures for all the attacks. Similarly, for the task of fine-grained classification on Flowers-102 dataset, handcrafted models like DenseNet-169 and VGG- 16      In summary, as the dataset size (in terms of both samples and number of classes) or the complexity of the task increases, NAS-based architectures are more vulnerable to adversarial attacks than hand-crafted models when no explicit adversarial training is performed.", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "Does an increase in the number of parameters", "text": "of architecture help improve robustness? [36] and [23] observed that within the same family of architectures, increasing the number of network parameters helps improve robustness. We therefore hypothesize that increasing model capacity benefits network robustness. To study this claim, we compare the robustness of five families of architectures on the ImageNet dataset with respect to the parameter count. For comparing the trends, we use PGD accuracy along with the Per-parameter Harmonic Robustness Score (PP-HRS). The five different families of architectures we considered for this study are mentioned below.\nFirst, we choose all the eight different variants of the EfficientNet family [38]. EfficientNet is a family of models that are developed by taking a NAS-based base model and scaling its width, depth and input image resolution proportionately using a set of compound-scaling coefficients which are searched via extensive grid search. We also study  4. In 4 out of 5 families considered for this study, an increase in parameters increases both clean and adversarial accuracy. The maximum value of the parameter count in these four families in nearly 26 million. This trend of increase in robustness with parameter count is also seen in the fifth family (EfficientNet) but only up to a parameter count of 20 million. Increasing the parameters alone beyond 20 million results in a decrease of both clean and adversarial accuracy. This is probably why EfficientNet considers different image sizes for each of the eight networks. After a certain point, increasing the parameters alone will not help improve robustness, and EfficientNet, which has the best adversarial accuracy in the case of ImageNet dataset, conveys this.\n\"In what family of architectures, is the increase in parameter count helping the performance?\" To better understand this, we report PP-HRS in Table 5. In the case of DenseNAS models developed using MobileNet-V2 search space, an increase in parameters from DenseNAS-A to DenseNAS-Large is improving both clean accuracy and adversarial robustness, which as a result lead to improved PP-HRS. For all the other families, the increased parameter count does not give a significant and sufficient improvement in the PP-HRS and adversarial robustness.\nIn summary, adversarial robustness can be improved by increasing the number of parameters, but this holds only to an extent. Beyond a certain point (approximately 20-25 million as per our analysis), increasing parameters alone cannot improve adversarial robustness.", "publication_ref": ["b35", "b22", "b38"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "What makes EfficientNets more robust than other architectures?", "text": "In comparison to the best performing NAS and handcrafted architectures in Table 5 (and as discussed in Section 4.1), the family of EfficientNet models are significantly better in terms of robustness to adversarial attacks. Among all the architectures compared, EfficientNet-B0 has the highest PP-HRS of 14.90. In case of PGD, the best performing EfficientNet model, EfficientNet-B4, outperforms all hand-crafted architectures by atleast 6%. This is a significant improvement particularly at the scale of ImageNet dataset. Further, for AutoPGD, all EfficientNet models perform better than all hand-crafted architectures (except for DenseNet-169 which is still worse than EfficientNet-B4 and B7).\nOne significant difference between EffcientNet and existing NAS and hand-crafted models is the scaling factor. Most of the hand-crafted and NAS-based architectures are developed in a micro-style, i.e., a small cell (like the ResNet block or DARTS cell) is developed/searched, and it is stacked to build the full architectures of varying depths and parameter sizes. In the case of EfficientNet, this scaling is done systematically using a compound scaling method using a coefficient (\u03c6) to scale width, depth, and resolution in a principled way [38]. This \u03c6 is specified by the user to control the resources available for scaling the model proportionately in terms of width, depth and resolution. In our analysis, for consistency, we keep the image resolution fixed at 224 \u00d7 224.\nLetting NAS figure out the optimal way to scale a neural network would alleviate the compute required for grid-search (for hyperparameters \u03b1, \u03b2, \u03b3 in EfficientNet) and makes the complete process of finding an adversarially robust architecture end-to-end. But since Section 4.2 shows that NAS-based architectures are more vulnerable than hand-crafted ones for larger and complex datasets, it is important to better understand the source of this vulnerability to find more effective ways to scale neural networks that are also adverasarially robust. We address this in the next section. In Section 4.2, we see that NAS-based architectures are more robust than hand-crafted architectures for small-scale datasets and simpler attacks. However, for stronger attacks like PGD, NAS-based architectures are not robust even at the scale of CIFAR-10. Most of the existing NAS methods perform the search on CIFAR-10 or a subset of Ima-geNet, and the discovered cell is stacked and trained for other datasets. To understand whether the problem lies in the search space or in the way search is being performed by the existing methods, we performed two simple experiments.\nOur first experiment is motivated by [46]. [46] shows that a randomly sampled cell in the DARTS search space gives as good a clean accuracy as a searched cell. To test if this fact also holds for the case of adversarial robustness, we sampled random cells from the DARTS search space, stacked and trained them using the standard procedure, and tested their robustness on the CIFAR-10 dataset. Due to the randomness involved, we report the value over four different runs. Results of this experiment are shown in Table 6. We can observed that randomly sampled cells have a better PGD accuracy than the searched architecture. But the variance is very high which shows that relying on randomly sampled architectures for adversarial robustness is not a good idea. This leads us to our second experiment.\nFor the second experiment, we randomly sample cells from the DARTS search space to build small models (please refer to Figure 2 that illustrates this procedure). After training these models independently, we ensemble the outputs of all these models using a simple linear network. This linear model consists of 2 linear layers with batch normalization and one fully connected classifier layer towards the end that outputs logits based on the number of classes in the dataset. This linear model is just fine-tuned for two epochs. Entire ensemble is treated as one single-network when generating the adversarial examples. To make a fair comparison, we ensure that the ensemble as a whole has the same number of cells as the standard DARTS networks. Since the procedure uses randomly sampled architectures, we run the entire sample-train-ensemble procedure four times and report the mean value in Table 6. Due to the randomness involved, this is a computationally expensive procedure. Therefore, we restrict our experiments to CIFAR-10 dataset and DARTS search space.  Surprisingly this simple ensemble of randomly sampled architectures can improve the PGD accuracy of DARTS based models by nearly 12% and can decrease the variance by \u223c10%. Now, this leads to the following interesting conclusions: (1) Learning to build a simple network to combine the outputs of randomly sampled architectures can give clean accuracy with adversarial robustness as an add-on. In this case, we used a simple linear model; replacing this with a searched NAS based architecture can improve the results further. (2) Using NAS to search for an ensemble of architectures can be a potential way to achieve adversarial robustness as an add-on to SoTA clean accuracy. In this case, the NAS objective should be modified to find small models that can complement each other. We plan to explore this in our future work. (3) Both random and ensemble-based topologies are able to provide significantly better adversarial robustness than existing NAS algorithms. This suggests that the search space itself is not the source of vulnerability. Rather, we need better search algorithms, potentially ensemble-based, that can leverage the same search space to build architectures that are inherently more robust even without any explicit adversarial training.", "publication_ref": ["b38", "b46", "b46"], "figure_ref": ["fig_0"], "table_ref": ["tab_5", "tab_5"]}, {"heading": "Conclusion", "text": "We present a detailed analysis of the adversarial robustness of NAS and hand-crafted models and show how the complex topology of neural networks can be leveraged to achieve adversarial robustness without any form of adversarial training. We also introduce a metric that can be used to calculate the trade-off between clean and adversarial accuracy within and across different families of architectures. Finally, we show that using NAS to find an ensemble of architectures can be one potential way to build robust and reliable models without any form of adversarial training.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Supplementary Material", "text": "In this supplementary section, we discuss the following details, which could not be included in the paper owing to space constraints.\n\u2022  DARTS search space consists of 5 operations: Max Pooling, Average Pooling, Skip Connections, Separable and Dilated convolutions. We report the number of times these operations are used in the normal and reduce cells of different DARTS architectures/micros in Tables 8 and 9. When compared with searched cells, randomly sampled ones, in general, have more number of unique operations. In a searched micro, the maximum number of unique operations for the normal, reduce cells in three, four, respectively. For a randomly sampled architecture, the count is five for both normal and reduce cells. Searched cells have many occurrences of a single operation (Separable convolution), which is not the case in randomly sampled architectures. We hypothesize that the presence of diverse set operations is a plausible for improved adversarial accuracy of randomly sampled DARTS architectures. While we only show 3-randomly chosen sub-networks in Tables 8, 9, we observe similar inferences with other random choices for sub-networks too. A qualitative comparison of these operations is shown in Figures 5 and 6.  Table 10 shows the robustness of different architectures for black-box attacks on CIFAR-10. In a black-box setting, the robustness of a model is tested on adversarial examples generated using a source model. For the source model, we use two variants of hand-crafted models and two variants of NAS models. Since we use no adversarial training in our experiments, the accuracy numbers are relatively lower. In general, the average adversarial accuracy on hand-crafted architectures is higher than NAS methods, having complex operations in the topology (DenseNet, ProxylessNAS) or a large number of parameters (VGG-16) provide the best adversarial accuracy in a black-box setting. Either way, our hypothesis that the complexity of the architecture (in terms of operations or parameters) plays a major role is corroborated in this study too. Understanding specific details of the nature and origin of such complexity in topology may be an interesting direction of future work.   ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_9", "tab_9", "tab_0"]}, {"heading": "Acknowledgement", "text": "This work has been partly supported by the funding received from DST, Govt of India, through the Data Science cluster of the ICPS program (DST/ICPS/CLUSTER/Data Science/2018/General).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "ProxylessNAS: Direct neural architecture search on target task and hardware", "journal": "", "year": "2019", "authors": "Han Cai; Ligeng Zhu; Song Han"}, {"ref_id": "b1", "title": "Towards Evaluating the Robustness of Neural Networks", "journal": "", "year": "2002", "authors": "Nicholas Carlini; David Wagner"}, {"ref_id": "b2", "title": "Adversarial Attacks and Defences: A Survey. arXiv e-prints", "journal": "", "year": "2002", "authors": "Anirban Chakraborty; Manaar Alam; Vishal Dey; Anupam Chattopadhyay; Debdeep Mukhopadhyay"}, {"ref_id": "b3", "title": "Progressive differentiable architecture search: Bridging the depth gap between search and evaluation", "journal": "", "year": "2019", "authors": "Xin Chen; Lingxi Xie; Jun Wu; Qi Tian"}, {"ref_id": "b4", "title": "Progressive Differentiable Architecture Search: Bridging the Depth Gap between Search and Evaluation", "journal": "", "year": "2006", "authors": "Xin Chen; Lingxi Xie; Jun Wu; Qi Tian"}, {"ref_id": "b5", "title": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks", "journal": "", "year": "2004", "authors": "Francesco Croce; Matthias Hein"}, {"ref_id": "b6", "title": "ImageNet: A Large-Scale Hierarchical Image Database", "journal": "", "year": "2009", "authors": "J Deng; W Dong; R Socher; L.-J Li; K Li; L Fei-Fei"}, {"ref_id": "b7", "title": "Adversarial Training and Provable Robustness: A Tale of Two Objectives. arXiv eprints", "journal": "", "year": "2004", "authors": "Jiameng Fan; Wenchao Li"}, {"ref_id": "b8", "title": "Densely Connected Search Space for More Flexible Neural Architecture Search. arXiv e-prints", "journal": "", "year": "2007", "authors": "Jiemin Fang; Yuzhu Sun; Qian Zhang; Yuan Li; Wenyu Liu; Xinggang Wang"}, {"ref_id": "b9", "title": "Jonathon Shlens, and Christian Szegedy. Explaining and Harnessing Adversarial Examples. arXiv eprints", "journal": "", "year": "2004", "authors": "Ian J Goodfellow"}, {"ref_id": "b10", "title": "Jonathon Shlens, and Christian Szegedy. Explaining and Harnessing Adversarial Examples. arXiv eprints", "journal": "", "year": "2002", "authors": "Ian J Goodfellow"}, {"ref_id": "b11", "title": "When nas meets robustness: In search of robust architectures against adversarial attacks", "journal": "", "year": "", "authors": "Minghao Guo; Yuzhe Yang; Rui Xu; Ziwei Liu"}, {"ref_id": "b12", "title": "Deep Residual Learning for Image Recognition", "journal": "", "year": "2002", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b13", "title": "Densely Connected Convolutional Networks. arXiv e-prints", "journal": "", "year": "2002", "authors": "Gao Huang; Zhuang Liu; Laurens Van Der Maaten; Kilian Q Weinberger"}, {"ref_id": "b14", "title": "Torchattacks: A pytorch repository for adversarial attacks", "journal": "", "year": "2020", "authors": "Hoki Kim"}, {"ref_id": "b15", "title": "Cifar-10 (canadian institute for advanced research", "journal": "", "year": "", "authors": "Alex Krizhevsky; Vinod Nair; Geoffrey Hinton"}, {"ref_id": "b16", "title": "Cifar-100 (canadian institute for advanced research", "journal": "", "year": "", "authors": "Alex Krizhevsky; Vinod Nair; Geoffrey Hinton"}, {"ref_id": "b17", "title": "Adversarial examples in the physical world", "journal": "", "year": "2002", "authors": "Alexey Kurakin; Ian Goodfellow; Samy Bengio"}, {"ref_id": "b18", "title": "Adversarial Machine Learning at Scale. arXiv e-prints", "journal": "", "year": "2002", "authors": "Alexey Kurakin; Ian Goodfellow; Samy Bengio"}, {"ref_id": "b19", "title": "Progressive neural architecture search", "journal": "", "year": "2018", "authors": "Chenxi Liu; Barret Zoph; Maxim Neumann; Jonathon Shlens; Wei Hua; Li-Jia Li; Li Fei-Fei; Alan Yuille; Jonathan Huang; Kevin Murphy"}, {"ref_id": "b20", "title": "DARTS: Differentiable Architecture Search. arXiv e-prints", "journal": "", "year": "2006", "authors": "Hanxiao Liu; Karen Simonyan; Yiming Yang"}, {"ref_id": "b21", "title": "Neural Architecture Search using Multi-Objective Genetic Algorithm. arXiv e-prints", "journal": "", "year": "2005", "authors": "Zhichao Lu; Ian Whalen; Vishnu Boddeti; Yashesh Dhebar; Kalyanmoy Deb; Erik Goodman; Wolfgang Banzhaf;  Nsga-Net"}, {"ref_id": "b22", "title": "Towards Deep Learning Models Resistant to Adversarial Attacks. arXiv e-prints", "journal": "", "year": "2006", "authors": "Aleksander Madry; Aleksandar Makelov; Ludwig Schmidt; Dimitris Tsipras; Adrian Vladu"}, {"ref_id": "b23", "title": "Sparsefool: A few pixels make a big difference", "journal": "", "year": "2002", "authors": "Apostolos Modas; Pascal Seyed-Mohsen Moosavi-Dezfooli;  Frossard"}, {"ref_id": "b24", "title": "DeepFool: a simple and accurate method to fool deep neural networks", "journal": "", "year": "2002", "authors": "Alhussein Seyed-Mohsen Moosavi-Dezfooli; Pascal Fawzi;  Frossard"}, {"ref_id": "b25", "title": "Automated flower classification over a large number of classes", "journal": "", "year": "2003", "authors": "Maria-Elena Nilsback; Andrew Zisserman"}, {"ref_id": "b26", "title": "Improving adversarial robustness via promoting ensemble diversity", "journal": "Proceedings of Machine Learning Research", "year": "2019-06", "authors": "Tianyu Pang; Kun Xu; Chao Du; Ning Chen; Jun Zhu"}, {"ref_id": "b27", "title": "Bag of Tricks for Adversarial Training. arXiv e-prints", "journal": "", "year": "2004", "authors": "Tianyu Pang; Xiao Yang; Yinpeng Dong; Hang Su; Jun Zhu"}, {"ref_id": "b28", "title": "Practical Black-Box Attacks against Machine Learning. arXiv e-prints", "journal": "", "year": "2001", "authors": "Nicolas Papernot; Patrick Mcdaniel; Ian Goodfellow; Somesh Jha; Z Berkay Celik; Ananthram Swami"}, {"ref_id": "b29", "title": "The Limitations of Deep Learning in Adversarial Settings. arXiv e-prints", "journal": "", "year": "2002", "authors": "Nicolas Papernot; Patrick Mcdaniel; Somesh Jha; Matt Fredrikson; Z Berkay Celik; Ananthram Swami"}, {"ref_id": "b30", "title": "Efficient Neural Architecture Search via Parameter Sharing. arXiv e-prints", "journal": "", "year": "2002", "authors": "Hieu Pham; Melody Y Guan; Barret Zoph; Quoc V Le; Jeff Dean"}, {"ref_id": "b31", "title": "Regularized evolution for image classifier architecture search", "journal": "", "year": "2019", "authors": "Esteban Real; Alok Aggarwal; Yanping Huang; Quoc V Le"}, {"ref_id": "b32", "title": "Adversarial attacks and defenses in deep learning", "journal": "Engineering", "year": "", "authors": "Tianhang Kui Ren; Zhan Zheng; Xue Qin;  Liu"}, {"ref_id": "b33", "title": "How benign is benign overfitting? arXiv e-prints", "journal": "", "year": "2004", "authors": "Amartya Sanyal; K Puneet; Varun Dokania; Philip H S Kanade;  Torr"}, {"ref_id": "b34", "title": "Adversarial training for free", "journal": "Curran Associates, Inc", "year": "2019", "authors": "Ali Shafahi; Mahyar Najibi; Mohammad Amin Ghiasi; Zheng Xu; John Dickerson; Christoph Studer; S Larry; Gavin Davis; Tom Taylor;  Goldstein"}, {"ref_id": "b35", "title": "Is Robustness the Cost of Accuracy? -A Comprehensive Study on the Robustness", "journal": "", "year": "", "authors": "Dong Su; Huan Zhang; Hongge Chen; Jinfeng Yi; Pin-Yu Chen; Yupeng Gao"}, {"ref_id": "b36", "title": "Deep Image Classification Models", "journal": "", "year": "2006", "authors": ""}, {"ref_id": "b37", "title": "triguing properties of neural networks. arXiv e-prints", "journal": "", "year": "2002", "authors": "Christian Szegedy; Wojciech Zaremba; Ilya Sutskever; Joan Bruna; Dumitru Erhan; Ian Goodfellow; Rob Fergus"}, {"ref_id": "b38", "title": "Rethinking Model Scaling for Convolutional Neural Networks. arXiv e-prints", "journal": "", "year": "2007", "authors": "Mingxing Tan; V Quoc;  Le;  Efficientnet"}, {"ref_id": "b39", "title": "Scalable and Efficient Object Detection. arXiv e-prints", "journal": "", "year": "2001", "authors": "Mingxing Tan; Ruoming Pang; Quoc V Le;  Efficientdet"}, {"ref_id": "b40", "title": "Ensemble Adversarial Training: Attacks and Defenses. arXiv e-prints", "journal": "", "year": "2002", "authors": "Florian Tram\u00e8r; Alexey Kurakin; Nicolas Papernot; Ian Goodfellow; Dan Boneh; Patrick Mcdaniel"}, {"ref_id": "b41", "title": "Evolving robust neural architectures to defend from adversarial attacks", "journal": "", "year": "2019", "authors": "Danilo Vasconcellos Vargas; Shashank Kotyan; Spm Iiit-Nr"}, {"ref_id": "b42", "title": "Fast is better than free: Revisiting adversarial training", "journal": "", "year": "2004", "authors": "Eric Wong; Leslie Rice; J Zico Kolter"}, {"ref_id": "b43", "title": "Intriguing properties of adversarial training at scale", "journal": "", "year": "2003", "authors": "Cihang Xie; Alan Yuille"}, {"ref_id": "b44", "title": "Pc-darts: Partial channel connections for memory-efficient architecture search", "journal": "", "year": "2006", "authors": "Yuhui Xu; Lingxi Xie; Xiaopeng Zhang; Xin Chen; Guo-Jun Qi; Qi Tian; Hongkai Xiong"}, {"ref_id": "b45", "title": "Efficient Neural Architecture Search via Hierarchical Masking. arXiv e-prints", "journal": "", "year": "2001", "authors": "Biyi Shen Yan; Faen Fang; Yu Zhang; Xiao Zheng; Hui Zeng; Mi Xu;  Zhang;  Hm-Nas"}, {"ref_id": "b46", "title": "Nas evaluation is frustratingly hard", "journal": "", "year": "2020", "authors": "Antoine Yang; Pedro M Esperan\u00e7a; Fabio M Carlucci"}, {"ref_id": "b47", "title": "Theoretically Principled Trade-off between Robustness and Accuracy. arXiv e-prints", "journal": "", "year": "2002", "authors": "Hongyang Zhang; Yaodong Yu; Jiantao Jiao; Eric P Xing; Laurent El Ghaoui; Michael I Jordan"}, {"ref_id": "b48", "title": "Neural Architecture Search with Reinforcement Learning. arXiv e-prints", "journal": "", "year": "2002", "authors": "Barret Zoph; Quoc V Le"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 .2Figure 2. Left: Standard procedure for building architectures from DARTS search space; Right: Procedure for building ensembles using DARTS search space. 12, 6, 2 can be replaced with any values that sum to 20.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 .4Figure 4. Comparison of PGD accuracy and Parameter count across different family of architectures Model Clean % FGSM F-FGSM PGD AutoPGD HRS", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "4. 4 .4Where does the source of adversarial vulnerability lie for NAS? Is it in the search space or in the way the current methods are performing the search?", "figure_data": ""}, {"figure_label": "56", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 .Figure 6 .56Figure 5. Qualitative comparison of different operations in normal cells of searched, randomly sampled micros from DARTS search space; Left: Searched micros, Right: Randomly sampled micros; Randomly sampled ones, in general, have more unique operations than searched onesSearchedRandomly sampled", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "beat NAS based architectures by a signifi-For simple attacks, NAS based architectures are robust, but for strong attacks hand-crafted architectures are better. Quantitative comparison of clean accuracy and adversarial robustness on CIFAR-10 dataset (Top-1 Accuracy) 5% for the Flowers-102 dataset.This trend of robustness for all four datasets is clearly shown in Figure4. As the dataset size or the task complexity increases, hand-crafted models start to be better for all the three adversarial attacks. For stronger attacks like PGD, handcrafted models are more robust when compared to NAS-based architectures at any given dataset scale. While NAS-based architectures achieve SoTA clean accuracy in general, the robustness of these architectures is very erratic.", "figure_data": "ModelClean % FGSM F-FGSM PGD AutoPGD HRSResNet-1893.4852.4348.3324.2723.1338.53ResNet-5094.3850.0545.7823.4522.3537.57DenseNet-12194.7650.9447.1424.0622.6638.38DenseNet-16994.7453.5349.4726.2124.3541.06VGG16 BN94.0752.4246.1620.0318.6333.03DARTS [21]97.0358.5345.037.096.1013.21PDARTS [5]97.1258.6747.629.317.9816.99NSGA Net [22]96.9466.0856.1611.19.8219.92Proxyless-NAS [1]97.9251.7358.383.224.246.23PC-DARTS [44]97.0560.5548.659.848.3617.87ModelClean % FGSM F-FGSM PGD AutoPGD HRSResNet-1863.8717.0817.126.055.3911.05ResNet-5073.091918.125.635.1610.45DenseNet-12178.7122.922.227.286.6813.33DenseNet-16982.4422.7321.667.376.9013.53VGG16 BN72.0517.0915.154.273.818.06DARTS [21]82.4324.9116.342.321.894.51PDARTS [5]83.0727.6920.233.092.665.96NSGA Net [22]85.4434.9324.12.261.944.40PC-DARTS [44]81.8326.2218.352.932.515.66"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": "CIFAR-10CIFAR-100ImageNetTest AccuracyFigure 3. DenseNets are always more robust. Qualitative comparison of accuracy of different models for PGD and AutoPGD attacks onCIFAR-10, CIFAR-100 and ImageNet datasets70.0018.0060.0016.0014.0050.0012.0040.0010.0030.008.006.0020.004.0010.002.000.000.00121 1691850ABCLarge R1R2R3B0B1B2B3B4B5B6B7DenseNetResNetDenseNASDenseNASEfficient-NetMobileNet SearchResNet Searchspacespace"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "These networks are listed in the increasing order of their parameters. Lastly, to also understand the trend in hand-crafted models, we study the robustness of standard DenseNet and ResNet models.All the results of this comparison are shown in Table5and Figure", "figure_data": "FamilyVariant Params (M) Clean % PGD AutoPGD PP-HRSB05.2991.368.1125.6114.90B17.7988.895.4724.907.00B29.1192.7711.4025.7711.79Efficient-NetB3 B412.23 19.3493.04 92.7313.37 16.9927.15 29.6410.11 7.86B530.3990.959.3725.282.96B643.0491.8611.7126.172.55B766.3591.5711.2027.821.59A4.7790.941.8419.673.61B5.5891.892.1319.373.56C6.1392.312.2919.223.48DenseNASLarge6.4892.802.9719.624.24R111.0991.332.0119.773.93R219.4792.473.1919.603.51R324.6693.814.3219.943.71ResNet18 5011.69 25.5689.08 92.862.41 4.6821.65 20.934.69 4.08DenseNet121 1697.98 14.1591.97 92.816.93 10.4624.20 27.1512.89 10.60Table 5. Comparison of parameter count vs Adversarial accuracyfor five different family of architectures on ImageNet dataseta recent family of SoTA NAS-based models called Dense-NAS [9]. DenseNAS architectures are developed using twodifferent search spaces. DenseNAS-A/B/C and Large aredeveloped using a MobileNetV2-based search space, andDenseNAS-R1, R2, R3 are developed using a ResNet-basedsearch space."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "\u00b1 0.41 93.77 \u00b1 0.39 21.68 \u00b1 0.35 20.78 \u00b1 0.39 Value reported over four runs. Randomly picked architectures from DARTS search-space. Value reported over four runs. \u2020 Ensemble of small, randomly picked architectures from DARTS search space.", "figure_data": "Model# cells Params (M)Clean %PGDAutoPGDDARTS [21]203.3597.037.096.10P-DARTS [5]203.4397.129.317.98PC-DARTS [44]203.6397.059.848.36RANDOM202.73 \u00b1 0.49 95.57 \u00b1 0.40 14.47 \u00b1 4.70 12.56 \u00b1 4.16ENSEMBLE  \u2020202.74"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Ensemble of randomly sampled DARTS cells is significantly more robust than a searched architecture. Adversarial accuracy comparison of DARTS-based architectures on CIFAR-10.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Details of the randomly sampled architectures used in Section 4.4. \u2022 A quantitative and qualitative comparison of searched, randomly sampled architectures used in Section 4.4. \u2022 Comparison of NAS and hand-crafted architectures for black-box attacks on CIFAR-10 dataset.", "figure_data": "A.1. Details of Randomly sampled architecturesfrom DARTS search spaceIn Section 4.4, we report results over four random runs,details of each of these runs are shown in Table 7. '# Net-works' denotes the number of sub-networks in a given en-semble, and '# Cells' denote the number of cells in eachsub-networks. '# Epochs' denotes the number of epochseach sub-network is trained for.Run # # Networks# Cells# Epochs13{12, 6, 2}{360, 180, 40}25{4, 4, 4, 4, 4} {120, 120, 120, 120, 120}35{6, 5, 4, 3, 2}{180, 150, 120, 90, 60}43{16, 2, 2}{480, 60, 60}"}, {"figure_label": "78", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Details of sub-networks in each ensemble across four runsA.2. Comparison of Searched and Randomly sampled architecturesIn this section, we make a qualitative and quantitative comparison between the randomly sampled architectures (used in Section 4.4) with standard SoTA DARTS based architectures like DARTS[21], P-DARTS[5], and PC-DARTS[44]. For this study, we randomly choose 3 of the 16 randomly sampled sub-networks shown in Table7. Comparison on usage of different operations in normal cell of micros from DARTS search space; R1, R2, R3 denote three randomly sampled micros.", "figure_data": "MaxAvgSkipSeparableDilated# uniquePoolPoolconnectionConvConvoperationsDARTS002513P-DARTS002423PC-DARTS011424R1112135R2113215R3111235"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Comparison on usage of different operations in reduce cell of micros from DARTS search space; R1, R2, R3 denote three randomly sampled micros;", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Target \u2193 Source \u2192 ResNet-18 DesneNet-169 DARTS NSGA Net", "figure_data": "ResNet-189.599.519.599.60ResNet-5010.9610.9810.9710.92DenseNet-12111.6911.6711.6611.66DenseNet-1699.749.719.749.71VGG16 BN13.9613.6413.8513.94DARTS10.0510.0410.0410.05PDARTS9.959.9810.0710.11NSGA Net9.859.869.909.88Proxyless-NAS11.9711.8911.9511.97PC-DARTS7.968.028.17.94"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "HRS = 2 \u2022 C \u2022 A C + A (1)", "formula_coordinates": [4.0, 390.37, 411.03, 154.75, 22.53]}, {"formula_id": "formula_1", "formula_text": "HRS \u03b2 = (\u03b2 2 + 1) \u2022 C \u2022 A \u03b2 2 C + A (2)", "formula_coordinates": [4.0, 367.09, 589.85, 178.03, 22.53]}, {"formula_id": "formula_2", "formula_text": "PP-HRS = HRS * p b f (p i )(3)", "formula_coordinates": [5.0, 118.11, 233.51, 168.26, 23.22]}], "doi": ""}