{"title": "Discrete-Continuous Optimization for Large-Scale Structure from Motion", "authors": "David Crandall; Andrew Owens; Mit Cambridge; Noah Snavely; Dan Huttenlocher", "pub_date": "", "abstract": "Recent work in structure from motion (SfM) has successfully built 3D models from large unstructured collections of images downloaded from the Internet. Most approaches use incremental algorithms that solve progressively larger bundle adjustment problems. These incremental techniques scale poorly as the number of images grows, and can drift or fall into bad local minima. We present an alternative formulation for SfM based on finding a coarse initial solution using a hybrid discrete-continuous optimization, and then improving that solution using bundle adjustment. The initial optimization step uses a discrete Markov random field (MRF) formulation, coupled with a continuous Levenberg-Marquardt refinement. The formulation naturally incorporates various sources of information about both the cameras and the points, including noisy geotags and vanishing point estimates. We test our method on several large-scale photo collections, including one with measured camera positions, and show that it can produce models that are similar to or better than those produced with incremental bundle adjustment, but more robustly and in a fraction of the time.", "sections": [{"heading": "Introduction", "text": "Structure from motion (SfM) techniques have recently been used to build 3D models from unstructured and unconstrained image collections, including images downloaded from Internet photo-sharing sites such as Flickr [1,6,11,25]. Most approaches to SfM from unstructured image collections operate incrementally, starting with a small seed reconstruction, then growing through repeated adding of additional cameras and scene points. While such incremental approaches have been quite successful, they have two significant drawbacks. First, these methods tend to be computationally intensive, making repeated use of bundle adjustment [29] (a non-linear optimization method that jointly refines camera parameters and scene structure) as well as outlier rejection to remove inconsistent measurements. Second, these methods do not treat all images equally, producing different results depending on the order in which pho-tos are considered. This sometimes leads to failures due to local minima or cascades of misestimated cameras. Such methods can also suffer from drift as large scenes with weak visual connections grow over time.\nIn this paper we propose a new SfM method for unstructured image collections which considers all the photos at once rather than incrementally building up a solution. This method is faster than current incremental bundle adjustment (IBA) approaches and more robust to reconstruction failures. Our approach computes an initial estimate of the camera poses using all available photos, and then refines that estimate and solves for scene structure using bundle adjustment. This approach is reminiscent of earlier work in SfM (prior to recent work on unstructured collections) where a good initialization was obtained and bundle adjustment was used as a final nonlinear refinement step yielding accurate camera parameters and scene structure. Thus one can think of our approach as a means of providing a good initialization for highly unstructured image sets, one that is readily refined using bundle adjustment.\nOur initialization technique uses a two-step process combining discrete and continuous optimization techniques. In the first step, discrete belief propagation (BP) is used to estimate camera parameters based on a Markov random field (MRF) formulation of constraints between pairs of cameras or between cameras and scene points. This formulation naturally incorporates additional noisy sources of constraint including geotags (camera locations) and vanishing points. The second step of our initialization process is a Levenberg-Marquardt nonlinear optimization, related to bundle adjustment, but involving additional constraints. This hybrid discrete-continuous optimization allows for an efficient search of a very large parameter space of camera poses and 3D points, while yielding a good initialization for bundle adjustment. The method is highly parallelizable, requiring a fraction of the time of IBA. By using all of the available data at once (rather than incrementally), and by allowing additional forms of constraint, we find that the approach is quite robust on large, challenging problems.\nWe evaluate our approach on several large datasets, find-ing that it produces comparable reconstructions-and in the case of a particularly challenging dataset, a much better reconstruction-to those produced by the state-of-the-art IBA approach of [1], in significantly less time. We have also created a dataset of several thousand photos, including some with very accurate ground-truth positions taken at surveyed points. On this dataset our method and IBA have similar accuracy with respect to the ground truth, and thus our method not only can yield similar results to IBA, but the two achieve comparably accurate reconstructions.", "publication_ref": ["b0", "b5", "b10", "b24", "b28", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Related work", "text": "Current techniques for large-scale SfM from unordered photo collections (e.g, [1,11,21,25]) make heavy use of nonlinear optimization (bundle adjustment), which is sensitive to initialization. Thus, these methods are run iteratively, starting with a small set of photos, then repeatedly adding photos and refining 3D points and camera poses. While generally successful, incremental approaches are time-consuming for large image sets, with a worst-case running time O(n 4 ) in the number of images. 1 Hence recent work has used clustering or graph-based techniques to reduce the number of images that must be considered in SfM [1,3,11,26,30]. These techniques make SfM more tractable, but the graph algorithms themselves can be costly, the number of remaining images can be large, and the effects on solution robustness are not well understood. Other approaches to SfM solve the full problem in a single batch optimization. These include factorization methods [28], which in some cases can solve SfM in closed form. However, it is difficult to apply factorization to perspective cameras with significant outliers and missing data (which are the norm in Internet photo collections).\nOur work is most closely related to batch SfM methods that solve for a global set of camera poses given local estimates of geometry, such as pairwise relative camera poses. These include linear methods for solving for global camera orientations or translations [8,14,20], and L \u221e methods for solving for camera (and possibly point) positions given known rotations and pairwise geometry or point correspondence [9,22]. While fast, these methods do not have built-in robustness to outliers, and it can be difficult to integrate noisy prior pose information into the optimization. In contrast, our MRF formulation can easily incorporate both robust error functions and priors. Some very recent work has incorporated geotags and other prior information into SfM, as we do here. Sinha et al. [24] proposed a linear SfM method that incorporates vanishing points (but not geotags) in estimating camera orientations. They use only a small number of pairwise estimates of geometry (forming a spanning tree on an image graph) for initializing translations, while our method incorporates all available information. Prior information has also been used as a postprocess for SfM, e.g., by applying vanishing point or map constraints to straighten out a model [12,23], using sparse geotags to georegister an existing reconstruction [10], or using geotags, terrain maps, and GIS data to register different connected components of a reconstruction [27]. In our work, we incorporate such geotag and vanishing point information into the optimization itself.\nFinally, other techniques for accelerating SfM have been proposed, including methods for hierarchical reconstruction or bundle adjustment [7,11,15]. These methods still depend on an incremental approach for initialization, but structure the computation more efficiently. We present an alternative that avoids incremental reconstruction entirely.", "publication_ref": ["b0", "b10", "b20", "b24", "b0", "b0", "b2", "b10", "b25", "b29", "b27", "b7", "b13", "b19", "b8", "b21", "b23", "b11", "b22", "b9", "b26", "b6", "b10", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Global estimation of cameras and points", "text": "Our approach represents a set of images as a graph modeling geometric constraints between pairs of cameras or between cameras and scene points (as binary constraints), as well as single-camera pose information such as geotags (as unary constraints). This set of binary and unary constraints can be modeled as a Markov random field (MRF) with an associated energy function on configurations of cameras and points. A key contribution of our work is to use both discrete and continuous optimization to minimize this energy function; in particular, we use belief propagation (BP) on a discretized space of camera and point parameters to find a good initialization, and non-linear least squares (NLLS) to refine the estimate. The power and generality of this combination of techniques allow us to efficiently optimize a more general class of energy functions than previous batch techniques. This class includes robust error functions, which are critical to obtaining good results in the presence of noisy binary and unary constraints.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Problem formulation", "text": "The input to our problem is a set of images I = {I 1 , . . . , I n }, relative pose estimates between some pairs of images (computed using two-frame SfM, described in Section 4), point correspondences between the images, and noisy absolute pose estimates for a subset of cameras (derived from sources like geotags). Our goal is to estimate an absolute pose for each camera, and a location for each scene point, consistent with the input measurements and in a geo-referenced coordinate system. We denote the absolute pose of camera I i as a pair (R i , t i ), where R i is a 3D rotation specifying the camera orientation and t i is the position of the camera's optical center in a global coordinate frame. The 3D position of a scene point is denoted X k .\nEach pairwise estimate of relative pose between two cameras I i and I j has the form (R ij , t ij ), where R ij is a relative orientation and t ij is a translation direction (in the coordinate system of camera I i ). Given perfect pairwise pose estimates, the absolute poses (R i , t i ) and (R j , t j ) of the two cameras would satisfy\nR ij = R i R j (1) \u03bb ij t ij = R i (t j \u2212 t i ),(2)\nwhere \u03bb ij is an unknown scaling factor (due to the gauge ambiguity in SfM). We can also write constraints between cameras and scene points. For a scene point X k visible to camera I i , let x ik denote the 2D position of the point in I i 's image plane. Then we can relate the absolute pose of the camera and the 3D location of the point:\n\u00b5 ik x ik = K i R i (X k \u2212 t i )(3)\nwhere K i is the matrix of intrinsics for image I i (assumed known, see Section 4), and \u00b5 ik is an unknown scale factor (the depth of the point). Equation ( 3) is the basis for the standard reprojection error used in bundle adjustment. The above three constraints can be defined on a reconstruction graph G = (V, E C \u222a E P ) having a node for each camera and each point, a set E C of edges between pairs of cameras with estimated relative pose, and a set E P of edges linking each camera to its visible points. Bundle adjustment typically only uses point-camera constraints, but in batch techniques constraints between cameras have proven useful. These constraints are unlikely to be satisfied exactly because of noise and outliers in relative pose estimates, so we pose the problem as an optimization which seeks absolute poses most consistent with the constraints according to a cost function. Ideally, one would minimize an objective on camera poses and points simultaneously, as in bundle adjustment, but in practice many batch techniques solve for camera rotations and translations separately [14,22,24]. We follow this custom and define an MRF for each of these two subproblems. A key concern will be to use objectives that are robust to incorrect two-frame geometry and point correspondence.\nRotations. From equation ( 1) we see that for neighboring images I i and I j in the reconstruction graph, we seek absolute camera poses R i and R j such that d R (R ij , R i R j ) is small, for some choice of distance function d R . This choice of distance function is tightly linked with the choice of parameterization of 3D rotations. Previous linear approaches to this problem have used a squared L 2 distance between 3 \u00d7 3 rotations matrices (i.e., the Frobenius norm) or between quaternions. Such methods relax the orthonormality constraints on these representations, which allows for an approximate least squares solution. In our case, we instead define d R to be a robustified distance,\nd R (R a , R b ) = \u03c1 R (||R a \u2212 R b ||),(4)\nfor some parameterization of rotations (detailed below), and a robust error function \u03c1 R (we use a truncated quadratic).\nFor some cameras we may have noisy pose information from evidence like vanishing point detection and camera orientation sensors. To incorporate this evidence into our optimization, we assume that for each camera I i there is a distance function d O i (R) that gives a cost for assigning the camera to any absolute orientation R. This function can have any form, including uniform if no prior information is available; we propose a particular cost function in Section 4.\nWe combine the unary and binary distances into a total rotational error function D R ,\nD R (R) = eij \u2208E C d R R ij , R i R j + \u03b1 1 Ii\u2208I d O i (R i ), (5\n)\nwhere R is an assignment of absolute rotations to the entire image collection, E C is the set of camera-camera edges, and \u03b1 1 is a constant. We minimize D R using a combination of BP and NLLS, as described in Section 4.\nCamera and point positions. Having solved for camera rotations, we fix them and estimate the positions of cameras and a subset of scene points by solving another MRF inference problem on the graph G. As with the rotations, we define an error function using a combination of binary and unary terms, where the binary terms correspond to the pairwise constraints in equations ( 2) and ( 3), and the unary terms correspond to prior pose information from geotags. Equation (2) implies that for a pair of adjacent images I i and I j we seek absolute camera positions t i and t j such that the relative displacement induced by those absolute camera positions, t j \u2212t i , is close to the relative translation estimat\u00ea t ij = R i t ij . Similarly, for a point X k visible in image I i , we want the displacement X k \u2212 t i to be close to the \"ray direction\"x ik derived from the 2D position of that point in the image (wherex ik = R i K \u22121 i x ik given observed position x ik and known intrinsics K i ). Thus, we can utilize both camera-camera constraints and camera-point constraints.\nPrevious linear approaches have considered one or the other of these constraints, by observing thatt ij \u00d7(t j \u2212t i ) = 0 for camera-camera constraints [8], or thatx ik \u00d7 (X k \u2212 t i ) = 0 for camera-point constraints [20]. These constraints form a homogeneous linear system, but the corresponding least squares problem minimizes a non-robust cost function that disproportionately weights distant points. Alternatively, L \u221e formulations to this problem have been defined [9,22], but these too lack robustness. In contrast, we explicitly handle outliers by defining a robust distance on the angle between displacement vectors,\nd T (t a , t b , t ab ) = \u03c1(angleof(t b \u2212 t a , t ab )),(6)\nwhere \u03c1 again denotes a robust distance function.\nWe also integrate geotags into the optimization. For now, we simply assume that there is a cost function d G i (t i ) for each camera I i over the space of translations, which may be uniform if no geotag is available; we propose a particular form for d G i in Section 4. We define the translational error of an assignment of absolute positions T to cameras and points as a combination of binary and unary terms,\nD T (T ) = \u03b1 2 eij \u2208E C d T (t i , t j ,t ij ) + d T (t j , t i ,t ji ) + \u03b1 3 e ik \u2208E P d T (X k , t i ,x ik ) + Ii\u2208I d G i (t i )(7)\nwhere E C denotes the set of camera-camera edges in G, E P is the set of camera-point edges, and \u03b1 2 and \u03b1 3 are weighting constants. We could ignore one of these sets by fixing \u03b1 2 or \u03b1 3 to 0; we evaluate these options in Section 5.", "publication_ref": ["b13", "b21", "b23", "b7", "b19", "b8", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Initial poses and points via discrete BP", "text": "The objectives in equations ( 5) and ( 7) can be minimized directly using Levenberg-Marquardt with reweighting for robustness, as we discuss in section 3.3, but this algorithm requires a good initial estimate of the solution. We tried using raw geotags to initialize the camera positions, for example, but we have found that they alone are too noisy for this purpose. In this section, we show how to compute a coarse initial estimate of camera poses and point positions using discrete belief propagation on an MRF.\nThe reconstruction graph G can be viewed as a firstorder MRF with hidden variables corresponding to absolute camera orientations and camera and point positions, observable variables corresponding to prior camera pose information, and constraints between pairs of cameras and between cameras and points. Finding an optimal labeling of an MRF is NP-hard in general, but approximate methods work well on problems like stereo [4]. However compared with those problems, our MRF is highly non-uniform (dense in some places, sparse in others) and the label space is very large. To do inference on this MRF efficiently, we use discrete belief propagation (BP) [19], computing the messages in linear time using distance transforms [5]. We use BP to solve both the rotations in equation ( 5) and the translations in (7).\nEstimating rotations. We first solve for absolute camera rotations R by minimizing equation (5) using discrete BP. Instead of solving for full 3D rotations, we reduce the state space by assuming that most cameras have little twist (inplane rotation) because most photos are close to landscape or portrait orientations and most digital cameras automatically orient images correctly. (We estimate that about 80% of photos in our datasets have less than 5 \u2022 twist, and 99% have less than 10 \u2022 twist. The no-twist assumption is made only during the BP stage; in the later NLLS and bundle adjustment stages we allow twist angles to vary.) Under this assumption, camera orientations R i can be represented as a single unit 3-vector v i (the viewing direction). The distance function in equation ( 5) then simplifies to\nd R0 (v i , v j ) = \u03c1 R (||v ij \u2212 R 0 (v i ) \u22121 v j ||),(8)\nwhere v ij is the expected difference in viewing directions (which can be computed from R ij ) and R 0 (v) is a 3D orientation with viewing direction v and no twist. 2 We define \u03c1 R (x) = min(x 2 , K R ), for constant K R (we use 1.0).\nEstimating translations and points. Having solved for absolute camera orientations, estimating camera and point positions involves minimizing Eq. (7). We use a modified pairwise distance function d T based on the cross product between vectors, which allows us to efficiently compute BP messages using distance transforms [5]:\nd T approx (t a , t b , t ab ) = \u03c1 T (||t ab \u00d7 (t b \u2212 t a )||) (9) = \u03c1 T (||t b \u2212 t a || ||t ab || sin(\u03b8 ab )),\nwith \u03b8 ab = angleof(t b \u2212 t a , t ab ) and \u03c1 T (x)= min(x, K T ) 2 with K T set to about 10m. This approximation is related to the linear approach of [8], which uses a non-robust version of d T approx and estimates translations by solving a sequence of reweighted least squares problems. We note that such approaches are sensitive to outliers, as without the truncation each term is unbounded and grows with ||t j \u2212 t i || 2 .", "publication_ref": ["b3", "b18", "b4", "b6", "b1", "b6", "b4", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Refining poses using non-linear least squares", "text": "Using the coarse estimates of rotations or translations determined by BP, we apply continuous optimization to the objective functions in equations ( 5) and ( 7), using the Levenberg-Marquardt (LM) algorithm for non-linear least squares [18]. Instead of defining a robust objective for LM, we simply remove edges and geotags from the reconstruction graph that disagree with the BP estimates more than a threshold, then run LM with a sum-of-square residual objective. These NLLS steps are related to bundle adjustment in that both minimize a non-linear objective by joint estimation of camera and (in the case of translations) point parameters. However, our NLLS stages separate rotation estimation from translation estimation, and integrate cameracamera constraints in addition to point-camera constraints.", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "A large-scale reconstruction system", "text": "We now show how to use the approach described in the last section to perform SfM on large unstructured image collections. Our method consists of the following main steps:\n1. Build the reconstruction graph G through image matching and two-view relative pose estimation. 2. Compute priors from geotags and vanishing points. 3. Solve for camera orientations, R, using discrete BP followed by continuous optimization.\n4. Estimate the camera and 3D point positions, T , again using BP followed by continuous optimization. 5. Perform a single stage of bundle adjustment, with the pose estimates from steps 3 and 4 as initialization.\nWe now describe these five steps in detail.\nStep 1: Producing pairwise transformations. We use SIFT matching [13] and two-frame SfM [17] to estimate correspondence and pairwise constraints between images. We tried two approaches to avoid matching all pairs of images: first, a simplification of [1] that uses a vocabulary tree [16] to find, for each image, a set of 80 candidate images to match; second, using geotags to find, for each image, 20 nearby images as initial candidates [6], sampling additional pairs from different connected components of the match graph, and densifying the graph using query expansion [1]. For matched pairs, we use the 5-point algorithm [17] followed by bundle adjustment to estimate relative pose. Since the 5-point algorithm requires intrinsically calibrated cameras, we only use images having focal lengths in the Exif metadata. We also apply a heuristic to remove high-twist images by finding images for which the relative twist of most pairwise transformations is above 20 \u2022 . In addition, we remove images with unusual aspect ratios, as these are often panoramas or cropped images.\nStep 2: Computing prior evidence. We compute unary cost functions on camera pose using geotags and vanishing points. For an image I i with geotag g i , we define the positional cost function d G i as a robust distance from the geotag,\nd G i (t i ) = \u03c1 T (|| en(g i ) \u2212 \u03c0(t i )||),(10)\nwhere \u03c1 T is a truncated quadratic, \u03c0 is a projection of 3D camera positions into a local Cartesian plane tangent to the surface of the earth, and en maps geotags in latitudelongitude coordinates to this plane. 3 The robustified distance function is essential because geotags are typically quite noisy and contaminated with outliers [27]. For images without geotags we use a uniform function for d G i . For rotations, we use a cost function d O i for image I i that is a sum of distances over the three rotation dimensions,\nd O i (R i ) = d \u03b8 i (R i ) + d \u03c8 i (R i ) + d \u03c6 i (R i ),(11)\nwhere and d \u03c6 i measure the error between an absolute camera rotation R i and prior pose information in pan, twist, and tilt, respectively. For d \u03c6 i (R i ), we estimate the tilt \u03c6 i using vertical vanishing point (VP) detection and penalize the tilt of R i as a function of angular distance to \u03c6 i . We detect vertical VPs as in [24], except that we use Hough voting instead of RANSAC to find VPs. Given a vertical VP estimate with sufficient support, we compute the corresponding tilt angle \u03c6 i ; if no vertical VP is found, we use a uniform function for d \u03c6 i . To estimate pan angle we observe that equation (2) constrains the absolute orientation R i of camera I i , given absolute positions of cameras I i and I j and the relative translation between them. Using geotags as estimates of the camera positions, we obtain a weak cost distribution for camera pan (heading direction),\nd \u03b8 i , d \u03c8 i ,\nd \u03b8 i (R i ) = j\u2208N (i) w g i w g j min(||R i t ij \u2212 g ij ||g ij || ||, K G ) 2 ,\nwhere N (i) are the neighboring cameras of I i , g ij = en(g j ) \u2212 en(g i ), w g i and w g j indicate whether I i and I j have geotags, and K G is a constant set empirically to 0.7. Our current BP implementation assumes that cameras have zero twist (see sec. 3), so we ignore the twist error term d \u03c8 i .\nStep 3: Solving for absolute rotations. We use discrete loopy belief propagation (BP) [19] to perform inference on our MRFs. For rotations, we parameterize the unit sphere into a 3D grid with 10 cells in each dimension, for a total of L = 1000 labels for each camera. The advantage of this parameterization is that the distance function in equation ( 8) becomes separable into a sum over dimensions, which allows the use of distance transforms to compute each message in linear time [5]. (Note that cells not intersecting the surface of the unit sphere are invalid and thus are assigned infinite cost.) We then run non-linear least squares to optimize equation (4) (using a squared distance), initializing the twist angles to 0 and the viewing directions to those estimated by BP. Inside this optimization, we represent displacement rotations using Rodrigues parameters, allowing the twist angles to vary. We used Matlab's lsqnonlin, using its sparse preconditioned conjugate gradients solver.\nStep 4: Solving for translations and points. Having estimated rotations, we next apply discrete BP to estimate camera and point positions. To reduce the label space, during BP we solve for 2D positions, as for most scenes camera and point positions vary predominantly over the two dimensions in the ground plane. (The later NLLS and BA stages remove this constraint.) We discretize this space depending on the geographic size of the region being reconstructed, using a 300 \u00d7 300 grid where each cell represents an area of about 1-4 meters square, for a total of L = 90000 labels. We use discrete BP to minimize (7) using the approximate distance function (9), with a modification to allow the use of the distance transform: when sending a message from camera i to j, instead of using the pairwise distance function \u03b1 2 (d T (t i , t j ,t ij ) + d T (t j , t i ,t ji )) suggested by Eq. (7), we use 2\u03b1 2 d T (t i , t j ,t ij ). For the NLLS optimization, we used lsqnonlin to minimize the squared residuals in Eq. (7), allowing cameras and points to vary in height as well as ground position. We generate a set of scene points by finding point tracks [1]; to reduce the size of the optimization problem, we greedily select a subset of tracks that covers each camera-camera edge in the reconstruction Figure 2. CentralRome reconstruction, using incremental bundle adjustment (left) and our technique (right), shown as top views projected on a map. Black points are cameras; blue points are scene points. There is a large drift in scale in the IBA solution (left), due to several weak connections between different parts of the reconstruction. For instance, the Colosseum (lower right) is smaller than it should be given the scale of the reconstructed Il Vittoriano monument (upper left). In addition, the inside and outside of the Colosseum do not align. The scale and alignment of the scene in our solution (right) is much more consistent. graph at least k 1 times, and that covers each image at least k 2 \u2265 k 1 times (we used k 1 = 5 and k 2 = 10).\nStep 5: Bundle adjustment. We use the estimates for the cameras and a sparse set of 3D points obtained in the last step as initialization to a global bundle adjustment stage in which all parameters including camera twist and height are refined simultaneously. We bundle adjust the cameras and the subset of 3D points selected in the previous step, triangulate the remaining points with reprojection error below a threshold, and run a final bundle adjustment. We use the preconditioned conjugate gradients bundle adjuster of [2] and a robust Huber norm on the reprojection error.", "publication_ref": ["b12", "b16", "b0", "b15", "b5", "b0", "b16", "b2", "b26", "b23", "b18", "b4", "b8", "b0", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We have applied our approach to four large datasets, summarized in Table 1, including one with over 15,000 images in the largest connected component of the reconstruction graph. The Acropolis, Dubrovnik, and CentralRome datasets consist of images downloaded from Flickr via the public API, while Quad consists of photos of the Arts Quad at Cornell University taken by several photographers over several months. For each dataset we ran the approach described in Section 4, including the discrete BP, continuous NLLS, and a final bundle adjustment. For these problems, we note that simple initializations to BA or NLLS perform poorly. We tried both random initialization of parameters, as well as initializing translations using the geotags, but both resulted in reconstructions with large errors. This highlights the fact that good initialization is critical, as well as the large degree of noise in the geotags.\nComparison to Incremental BA (IBA). To compare our approach to a state-of-the-art technique that uses IBA, we ran the datasets through a version of Bundler [25] that uses an efficient bundle adjuster based on preconditioned conjugate gradients [2], then georegistered the results by using RANSAC to align the model with the geotags. Table 2 summarizes results of this comparison, including distances between corresponding camera positions and viewing directions. It is important to note that the IBA solution has errors and is thus not ground truth, but it does represent the stateof-the-art in SfM and is thus a useful comparison. These results show that the raw geotags are quite noisy, with a median translation error of over 100 meters for some datasets. The estimates from BP are significantly better, and results from the full process (including a final bundle adjustment step) agree with the IBA solution within a meter for all datasets except CentralRome. The differences for Central-Rome are large because IBA produced poor results for this set, as discussed below. The median differences between point positions for the two methods are also less than 1m for all sets except CentralRome. For the camera orienta- tions, the median angle between viewing directions of the IBA solution and the output of BP is between about 5 \u2022 and 14 \u2022 , with the continuous optimization decreasing the difference below 5 \u2022 , and the final BA step further reducing it to less than 1.5 \u2022 (and below 0.5 \u2022 for all datasets except CentralRome). We thus see that our approach produces reconstructions that are quantitatively similar to incremental methods in cases where IBA produces reasonable results. We also tried the batch approach of [8] on these datasets. The rotation estimates produced by this linear technique were reasonable for the densely-connected Acropolis and Dubrovnik sets, but poor for the other two sets (as shown in the table), even when we ran NLLS on the output of [8]. The translations estimates were very poor for all of the datasets, even when we modified [8] to include geotag priors. This suggests that the robustness used by our approach is important in getting good results on large, noisy datasets (as existing evaluations of linear approaches like [8] and [24] were on much simpler, more homogeneous datasets).\nRunning times. As shown in Table 3, our approach is significantly faster than incremental bundle adjustment on all of the datasets that we study. The improvement is particularly dramatic for the larger datasets; for CentralRome for example, our approach took about 13 hours compared to about 82 hours for IBA, or a more than 6x speed-up. One of the reasons for this speed-up is that BP (unlike IBA) is easily parallelizable. The running times reported here used a multi-threaded implementation of rotations BP on a single 16-core 3.0GHz machine and a map-reduce implementation of translations BP on a 200-core 2.6GHz Hadoop cluster. NLLS was single-threaded and run on a 3.0GHz machine. For BA and IBA we used the highly-optimized implementation of [1], which uses a parallel BLAS library to achieve some parallelism, on a single 16-core 3.0GHz machine.\nThe asymptotic running time of our approach also com-pares favorably to that of IBA. In contrast to the the worst case O(n 4 ) running time of IBA (using dense linear algebra), where n is the number of images, our approach is O(n 3 ): each application of belief propagation takes time O(n 2 L) per iteration, where L is the size of the label space, and the final bundle adjustment step takes O(n 3 ) time in the worst case. Memory use of BP is also O(n 2 L), although messages can be compressed and stored on disk between iterations (as our Hadoop implementation does).\nComparison to ground truth. To evaluate our results against ground truth, we collected highly accurate geotags (with error less than 10cm) for a subset of 348 photos for the Quad, based on survey points found using differential GPS. We also collected geotags using consumer GPS (an iPhone 3G); the precise geotags are used for ground-truth, while the consumer geotags are used as priors in the optimization. Table 4 compares the error of camera pose estimates produced by IBA to those of the various stages of our method. IBA produces slightly better estimates than our approach, but the difference is quite small (1.01m versus 1.16m). The table also studies the sensitivity of our approach to the fraction of photos having geotags. As the fraction of geotagged images decreases below about 10%, the accuracy starts to decrease. This seems to be due to less accurate global rotation estimates, indicating that weak orientation information is helpful for getting good results. We also tested using only the camera-camera edges or only the camera-point edges during the translations estimation with 40% of images geotagged (by setting \u03b1 2 or \u03b1 3 to 0 in equation ( 7)); using only camera-point edges increased error from 1.21m to 1.9m, while using only camera-camera edges increased error by a factor of 3 (from 1.21m to 3.93m).\nQualitative results. Figure 1 shows views of the Cen-tralRome dataset at different stages of our approach. Because our recovered cameras (and points) are reconstructed in an absolute coordinate system, they can be displayed on a map. Figure 2 shows the CentralRome reconstruction for both our approach and IBA. IBA produced a poor reconstruction for this dataset, while our approach produced much more reasonable results, likely because prior information like geotags helped to avoid problems with sparselyconnected components of the reconstruction graph.", "publication_ref": ["b24", "b1", "b7", "b7", "b7", "b7", "b23", "b0"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Conclusion", "text": "We have presented a new approach to SfM that avoids solving sequences of larger and larger bundle adjustment problems by initializing all cameras at once using hybrid discrete-continuous optimization on an MRF. It also integrates prior pose evidence from geotags and vanishing points into the optimization. Our approach is faster than incremental SfM both in practice and asymptotically, and gives better reconstructions on some scenes, especially when the reconstruction graph is weakly connected. As future work, we would like to further characterize the performance and tradeoffs of our algorithm, including studying its scalability to even larger collections (with hundreds of thousands of images) and characterizing its robustness to various properties of the scene and dataset. We would also like to study improvements to our approach, including solving for rotations and translations in a single optimization step.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgments. This work was supported by the National Science Foundation (grants IIS-0705774 and IIS-0964027), the Indiana University Data to Insight Center, the Lilly Endowment, Quanta Computer, MIT Lincoln Labs, Google, and Intel, and used computational resources of the Cornell Center for Advanced Computing and Indiana University (funded by NSF grant EIA-0202048 and by IBM).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Building Rome in a Day", "journal": "ICCV", "year": "2009", "authors": "S Agarwal; N Snavely; I Simon; S Seitz; R Szeliski"}, {"ref_id": "b1", "title": "Bundle Adjustment in the Large", "journal": "ECCV", "year": "2009", "authors": "S Agarwal; N Snavely; S Seitz; R Szeliski"}, {"ref_id": "b2", "title": "Global uncertainty-based selection of relative poses for multi-camera calibrations", "journal": "BMVC", "year": "2008", "authors": "F Bajramovic; J Denzler"}, {"ref_id": "b3", "title": "Fast approximate energy minimization via graph cuts", "journal": "IEEE Trans. PAMI", "year": "2001", "authors": "Y Boykov; O Veksler; R Zabih"}, {"ref_id": "b4", "title": "Efficient belief propagation for early vision. IJCV", "journal": "", "year": "2006", "authors": "P Felzenszwalb; D Huttenlocher"}, {"ref_id": "b5", "title": "Building Rome on a cloudless day", "journal": "ECCV", "year": "2010", "authors": "J-M Frahm; P Georgel; D Gallup; T Johnson; R Raguram; C Wu; Y-H Jen; E Dunn; B Clipp; S Lazebnik; M Pollefeys"}, {"ref_id": "b6", "title": "Improving the efficiency of hierarchical structure-and-motion", "journal": "CVPR", "year": "2010", "authors": "R Gherardi; M Farenzena; A Fusiello"}, {"ref_id": "b7", "title": "Lie-algebraic averaging for globally consistent motion estimation. CVPR", "journal": "", "year": "2004", "authors": "V Govindu"}, {"ref_id": "b8", "title": "Multiple-view geometry under the L-infinity norm", "journal": "IEEE TPAMI", "year": "2007", "authors": "F Kahl; R Hartley"}, {"ref_id": "b9", "title": "Alignment of 3D Point Clouds to Overhead Images. CVPR Workshop on Internet Vision", "journal": "", "year": "2009", "authors": "R Kaminsky; N Snavely; S Seitz; R Szeliski"}, {"ref_id": "b10", "title": "Modeling and Recognition of Landmark Image Collections Using Iconic Scene Graphs. ECCV", "journal": "", "year": "2008", "authors": "X Li; C Wu; C Zach; S Lazebnik; J Frahm"}, {"ref_id": "b11", "title": "Towards geographical referencing of monocular SLAM reconstruction using 3D city models", "journal": "", "year": "2009", "authors": "P Lothe; S Bourgeois; F Dekeyser; E Royer; M Dhome"}, {"ref_id": "b12", "title": "Distinctive image features from scale-invariant keypoints. IJCV", "journal": "", "year": "2004", "authors": "D G Lowe"}, {"ref_id": "b13", "title": "Robust Rotation and Translation Estimation in Multiview Reconstruction. CVPR", "journal": "", "year": "2007", "authors": "D Martinec; T Pajdla"}, {"ref_id": "b14", "title": "Out-of-Core Bundle Adjustment for Large-Scale 3D Reconstruction, ICCV", "journal": "", "year": "2007", "authors": "K Ni; D Steedly; F Dellaert"}, {"ref_id": "b15", "title": "Scalable Recognition with a Vocabulary Tree", "journal": "CVPR", "year": "2006", "authors": "D Nist\u00e9r; H Stew\u00e9nius"}, {"ref_id": "b16", "title": "An efficient solution to the five-point relative pose problem", "journal": "Trans. PAMI", "year": "2004", "authors": "D Nist\u00e9r"}, {"ref_id": "b17", "title": "Numerical optimization", "journal": "Springer-Verlag", "year": "1999", "authors": "J Nocedal; S J Wright"}, {"ref_id": "b18", "title": "Probabilistic Reasoning in Intelligent Systems", "journal": "Morgan Kaufmann", "year": "1988", "authors": "J Pearl"}, {"ref_id": "b19", "title": "Linear multi-view reconstruction of points, lines, planes and cameras, using a reference plane", "journal": "ICCV", "year": "2003", "authors": "C Rother"}, {"ref_id": "b20", "title": "Multi-view matching for unordered image sets, or How do I organize my holiday snaps? ECCV", "journal": "", "year": "2002", "authors": "F Schaffalitzky; A Zisserman"}, {"ref_id": "b21", "title": "Recovering Camera Motion Using L\u221e Minimization", "journal": "", "year": "2006", "authors": "K Sim; R Hartley"}, {"ref_id": "b22", "title": "Interactive 3D Architectural Modeling from Unordered Photo Collections", "journal": "SIGGRAPH ASIA", "year": "2008", "authors": "S Sinha; D Steedly; R Szeliski; M Agarwala; M Pollefeys"}, {"ref_id": "b23", "title": "A multi-stage linear approach to structure from motion", "journal": "", "year": "2010", "authors": "S Sinha; D Steedly; R Szeliski"}, {"ref_id": "b24", "title": "Photo tourism: exploring photo collections in 3D. SIGGRAPH", "journal": "", "year": "2006", "authors": "N Snavely; S Seitz; R Szeliski"}, {"ref_id": "b25", "title": "Skeletal graphs for efficient structure from motion", "journal": "CVPR", "year": "2008", "authors": "N Snavely; S Seitz; R Szeliski"}, {"ref_id": "b26", "title": "Dynamic and Scalable Large Scale Image Reconstruction. CVPR", "journal": "", "year": "2010", "authors": "C Strecha; P Pylvanalnen; P Fua"}, {"ref_id": "b27", "title": "Shape and motion from image streams under orthography: a factorization method", "journal": "IJCV", "year": "2008", "authors": "C Tomasi; T Kanade"}, {"ref_id": "b28", "title": "Bundle adjustment -a modern synthesis", "journal": "", "year": "2000", "authors": "B Triggs; P Mclauchlan; R Hartley; A Fitzgibbon"}, {"ref_id": "b29", "title": "A new reliability measure for essential matrices suitable in multiple view calibration", "journal": "VISAPP", "year": "2008", "authors": "J Verg\u00e9s-Llah\u00ed; D Moldovan; T Wada"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. Translation estimates for CentralRome. Camera positions after BP, after NLLS refinement, and after final bundle adjustment.", "figure_data": ""}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Summary of datasets: Total number of photos; number of images, camera-camera edges, and camera-point edges in the largest connected component; fraction of images with geotags; approximate scene size; and number of reconstructed images using our approach. Median differences between our camera pose estimates and those produced by incremental bundle adjustment.", "figure_data": "TotalImages in largest Cam-cam edges Cam-pt edges%Scene sizeReconstructedDatasetimagesCC (|V |)(|EC |)(|EP |)geotagged(km 2 )imagesAcropolis2,96146322,84242,255100.0%0.1\u00d70.1454Quad6,5145,520444,064551,67077.2%0.4\u00d70.35,233Dubrovnik12,0926,8541,000,178835,31056.7%1.0\u00d70.56,532CentralRome74,39415,242864,7581,393,658100.0%1.5\u00d70.814,754Rotational differenceTranslational differencePoint differenceOur approachLinear approach [8]Our approachOur approachDatasetBPNLLS Final BALinearNLLSGeotagsBPNLLS Final BAFinal BAAcropolis14.1 \u20221.5 \u20220.2 \u20221.6 \u20221.6 \u202212.9m8.1m2.4m0.1m0.2mQuad4.7 \u20224.6 \u20220.2 \u202241 \u202241 \u202215.5m 16.6m 14.2m0.6m0.5mDubrovnik9.1 \u20224.9 \u20220.1 \u202211 \u20226 \u2022127.6m 25.7m 15.1m1.0m0.9mCentralRome6.2 \u20223.3 \u20221.3 \u202227 \u202225 \u2022413.0m 27.3m 27.7m25.0m24.5m"}, {"figure_label": "34", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Running times of our approach compared to incremental bundle adjustment. Median error in camera position with respect to ground truth for the Quad dataset, with geotags for about 40% of images. The median error of IBA was 1.01m.", "figure_data": "Our approachIncremental"}], "formulas": [{"formula_id": "formula_0", "formula_text": "R ij = R i R j (1) \u03bb ij t ij = R i (t j \u2212 t i ),(2)", "formula_coordinates": [3.0, 115.03, 150.84, 171.33, 25.59]}, {"formula_id": "formula_1", "formula_text": "\u00b5 ik x ik = K i R i (X k \u2212 t i )(3)", "formula_coordinates": [3.0, 115.12, 259.44, 171.25, 9.68]}, {"formula_id": "formula_2", "formula_text": "d R (R a , R b ) = \u03c1 R (||R a \u2212 R b ||),(4)", "formula_coordinates": [3.0, 100.59, 702.1, 185.78, 11.74]}, {"formula_id": "formula_3", "formula_text": "D R (R) = eij \u2208E C d R R ij , R i R j + \u03b1 1 Ii\u2208I d O i (R i ), (5", "formula_coordinates": [3.0, 313.84, 222.41, 227.4, 22.77]}, {"formula_id": "formula_4", "formula_text": ")", "formula_coordinates": [3.0, 541.24, 224.83, 3.87, 8.64]}, {"formula_id": "formula_5", "formula_text": "d T (t a , t b , t ab ) = \u03c1(angleof(t b \u2212 t a , t ab )),(6)", "formula_coordinates": [3.0, 339.81, 659.31, 205.31, 11.74]}, {"formula_id": "formula_6", "formula_text": "D T (T ) = \u03b1 2 eij \u2208E C d T (t i , t j ,t ij ) + d T (t j , t i ,t ji ) + \u03b1 3 e ik \u2208E P d T (X k , t i ,x ik ) + Ii\u2208I d G i (t i )(7)", "formula_coordinates": [4.0, 56.12, 138.49, 230.24, 52.14]}, {"formula_id": "formula_7", "formula_text": "d R0 (v i , v j ) = \u03c1 R (||v ij \u2212 R 0 (v i ) \u22121 v j ||),(8)", "formula_coordinates": [4.0, 341.51, 89.01, 203.61, 11.74]}, {"formula_id": "formula_8", "formula_text": "d T approx (t a , t b , t ab ) = \u03c1 T (||t ab \u00d7 (t b \u2212 t a )||) (9) = \u03c1 T (||t b \u2212 t a || ||t ab || sin(\u03b8 ab )),", "formula_coordinates": [4.0, 313.6, 235.01, 231.52, 26.69]}, {"formula_id": "formula_9", "formula_text": "d G i (t i ) = \u03c1 T (|| en(g i ) \u2212 \u03c0(t i )||),(10)", "formula_coordinates": [5.0, 99.88, 429.01, 186.48, 12.71]}, {"formula_id": "formula_10", "formula_text": "d O i (R i ) = d \u03b8 i (R i ) + d \u03c8 i (R i ) + d \u03c6 i (R i ),(11)", "formula_coordinates": [5.0, 86.81, 558.09, 199.55, 13.68]}, {"formula_id": "formula_11", "formula_text": "d \u03b8 i , d \u03c8 i ,", "formula_coordinates": [5.0, 77.64, 577.47, 29.25, 13.68]}, {"formula_id": "formula_12", "formula_text": "d \u03b8 i (R i ) = j\u2208N (i) w g i w g j min(||R i t ij \u2212 g ij ||g ij || ||, K G ) 2 ,", "formula_coordinates": [5.0, 317.87, 146.36, 218.24, 27.27]}], "doi": ""}