{"title": "The Max K-Armed Bandit: A New Model of Exploration Applied to Search Heuristic Selection", "authors": "Vincent A Cicirello; Stephen F Smith", "pub_date": "", "abstract": "The multiarmed bandit is often used as an analogy for the tradeoff between exploration and exploitation in search problems. The classic problem involves allocating trials to the arms of a multiarmed slot machine to maximize the expected sum of rewards. We pose a new variation of the multiarmed bandit-the Max K-Armed Bandit-in which trials must be allocated among the arms to maximize the expected best single sample reward of the series of trials. Motivation for the Max K-Armed Bandit is the allocation of restarts among a set of multistart stochastic search algorithms. We present an analysis of this Max K-Armed Bandit showing under certain assumptions that the optimal strategy allocates trials to the observed best arm at a rate increasing double exponentially relative to the other arms. This motivates an exploration strategy that follows a Boltzmann distribution with an exponentially decaying temperature parameter. We compare this exploration policy to policies that allocate trials to the observed best arm at rates faster (and slower) than double exponentially. The results confirm, for two scheduling domains, that the double exponential increase in the rate of allocations to the observed best heuristic outperforms the other approaches.", "sections": [{"heading": "Introduction", "text": "The K-Armed Bandit often serves as an analogy for balancing exploration and exploitation in search domains (Berry & Fristedt 1985). The problem is to allocate trials to the arms of a k-armed bandit (i.e., slot machine with k arms, each with a different pay-out distribution) with the goal of maximizing expected total reward. Many have analyzed variations of the bandit problem (e.g., (Agrawal 1995;Auer, Cesa-Bianchi, & Fischer 2002;Berry & Fristedt 1985;Holland 1975)). Others have used bandits as inspiration for, or justification of, exploration strategies-e.g., for genetic algorithms (Holland 1975) and reinforcement learning (Sutton & Barto 1998).\nIn this paper, a new variation of the multiarmed bandit is posed-the Max K-Armed Bandit Problem. The problem, simply stated, is to allocate trials among the k arms so as to maximize the expected best single sample reward. Our motivation is the problem of allocating restarts among multistart stochastic search algorithms to maximize over-all search results. Consider an NP-hard combinatorial optimization problem, a stochastic search algorithm that can be biased by a search heuristic, and a set of heuristics which perform differentially on different problem instances. In solving any given problem instance, one would like to dynamically determine and exploit the heuristic that yields the best search performance on this instance. At any point during the search, the goal of future restarts is to find a solution that is better than the current best found. The original multiarmed bandit is concerned with maximizing the expected sum of rewards. However, this does not match the goal in our stochastic search example. In the stochastic search case, we have our current reward (i.e., the best solution found so far) and need to find some reward that is better yet.\nPrior research has argued that extreme value theory offers a good model for the distribution of solutions that would be produced across iterations of a heuristic biased stochastic sampling procedure when using the bias of a strong domain heuristic (Cicirello & Smith 2004). Starting from this assumption, we show theoretically that the optimal exploration policy for the Max K-armed Bandit allocates a double exponentially increasing number of trials to the observed best heuristic. We then empirically validate this exploration policy in two complex scheduling domains: (1) weighted tardiness sequencing; and (2) resource constrained project scheduling with time windows.", "publication_ref": ["b3", "b0", "b2", "b3", "b10", "b10", "b15", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "The K-Armed Bandit: Holland's Analysis", "text": "The k-armed bandit is a major part of the theoretical underpinning of the genetic algorithm (GA). Holland (1975) uses the k-armed bandit analogy to show that the GA achieves a near-optimal tradeoff of exploration and exploitation.\nFor the two-armed bandit, the expected reward for arm one is \u00b5 1 with variance \u03c3 2 1 (\u00b5 2 and \u03c3 2 2 for arm two). Furthermore, \u00b5 1 \u2265 \u00b5 2 , but it is not known which arm is which. The problem is to maximize expected reward for a series of trials. One must determine the optimal tradeoff of exploratory actions (i.e., to discover the payoffs) and exploitation actions (i.e., playing the apparent best). The k-armed bandit is the obvious generalization. Let R(\u00b5, \u03c3) be a reward function that samples a normal distribution with mean \u00b5 and standard deviation \u03c3, and let n i be the number of samples given the i-th arm. The objective is to allocate the n i to optimize:\nmax k i=1 n i R(\u00b5 i , \u03c3 i ).\n(1)\nIf the \u00b5 i are known, then all N trials should be allocated to the arm with the largest \u00b5 i to maximize the expected value of this objective. Without knowledge of the \u00b5 i , it is necessary to perform some exploration to solve the problem. For the two-armed bandit, Holland showed the optimal policy (to minimize expected loss from trials of the worst arm) allocates n * trials to the worst arm, and N \u2212 n * to the best arm where in the limit:\n1 N \u2212 n * \u223c \u0398(exp(cn * )), (2\n)\nwhere c is a constant. The trials allocated the observed best arm should increase exponentially relative to the allocation to the observed worst arm. Holland generalized this to the k-arm case, showing the worst-case expected loss for the problem occurs when \u00b5 2 = \u00b5 3 = . . . = \u00b5 k and \u03c3 2 = \u03c3 3 = . . . = \u03c3 k ; and further showing that the best arm should be allocated N \u2212 (k \u2212 1)m * trials where N is the total number of trials and where each of the other k \u2212 1 arms are allocated m * trials. The optimal number of trials, in the limit, is:\nN \u2212 (k \u2212 1)m * \u223c \u0398(exp(cm * )).(3)\nThe number of trials allocated to the observed best arm in the optimal allocation should increase exponentially with the number of trials allocated to each of the other k \u2212 1 arms.", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "The Max K-Armed Bandit", "text": "We now pose a new variation of the multiarmed bandit called the Max K-Armed Bandit. In the Max K-Armed Bandit Problem, we are faced with a series of N trials. In any given trial, we can choose any of the k arms. For each of the arms there is an expected payoff according to some probability distribution. The goal is to maximize the value of the best single reward received over the N trials. This new objective is to allocate N trials among the arms to optimize:\nmax k max i=1 ni max j=1 R j (D i ),(4)\nwhere R j (D i ) is the reward of the j-th trial of arm i with reward distribution D i .\nIn the following subsections, we develop a solution to the Max K-Armed Bandit problem. Under certain assumptions about the distribution of samples of an arm, we show that to maximize the expected max single sample reward over N trials, the number of samples taken from the observed best arm should grow double exponentially in the number of samples taken from the observed second best. We proceed in three steps. First, we make some assumptions about the payoff distributions associated with each arm. Then we consider the special case of two arms. Finally, we generalize this solution to K arms. 1 See Holland (1975) for complete derivation.", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "Payoff Distribution Assumptions", "text": "To analyze the Max K-Armed Bandit, it is necessary to specify the type of distribution that each of the arms follow. In the classic version of the bandit problem, this is not necessary. Since the classic problem concerns the maximization of the expected sum of rewards, it is sufficient to make assumptions about the means and standard deviations of the arms. In the Max K-Armed Bandit case, we require an expression for the expected max of a series of N trials as a function of N . This necessitates an assumption about the form of the underlying distribution of trials. The extremal types theorem tells us that the distribution of the max of a series of independent and identically distributed trials (as the length of the series grows large) belongs to one of three distribution families independent of the underlying distribution of the trials: the Gumbel, the Fr\u00e9chet, or the Weibull (Coles 2001). This seems to allow us to carry through with an analysis independent of the form of the distributions of the samples drawn from the arms of the bandit. However, an expression is needed in terms of the length of the series of trials, requiring an assumption on the underlying distribution.\nTo make an appropriate assumption we consider the target application-allocating restarts among a set of multistart stochastic search heuristics for combinatorial optimization. Cicirello and Smith (2004) argue that a stochastic search procedure that is biased by strong domain heuristics samples from the extreme of the solution quality distribution of the underlying problem space. They showed that such an algorithm generally finds \"good\" solutions for combinatorial optimization and that \"good\" solutions are statistically rare in the overall solution space (i.e., extremely low probability of drawing a \"good\" solution at random). If we randomly sample N solutions, then for large N , the best sample (or maximum element) must follow the extremal types theorem-by definition. The assumption is that the behavior of a stochastic search procedure that is biased by a strong domain heuristic is equivalent to taking the best solution from a sufficiently large series of unbiased random samples. Following extreme value theory, we assume that individual solutions given by the stochastic search are drawn from one of three distribution families: Gumbel, Fr\u00e9chet, or Weibull (generalized as the Generalized Extreme Value (GEV) distribution).\nSince an assumption of the most general GEV distribution prevents a closed form analysis, let us instead assume that each of the arms samples from a type I extreme value distribution (or the Gumbel distribution). This distribution has a cumulative probability of:\nP (Z \u2264 z) = G(z) = exp \u2212 exp \u2212 z \u2212 b a , (5\n)\nwhere b is the location parameter and a the scale parameter.\nThe probability density function of the Gumbel is:\nP (Z = z) = 1 a exp \u2212 z\u2212b a exp \u2212 exp \u2212 z\u2212b a . (6\n)\nThe Max 2-Armed Bandit Case Let there be two arms, M 1 and M 2 , with the rewards of M i drawn from a Gumbel distribution G i (x) with location parameter b i and scale parameter a i . The mean reward of a single sample of M i is: \u00b5 i = b i + 0.5772a i , where 0.5772 is Euler's number, and the standard deviation is: \u03c3 i = ai\u03c0 \u221a 6 .\nProof Given that the expected largest sample of a series of trials must be maximized, an expression is needed for the expected value of the maximum of a series of samples. Given N samples {X 1 , . . . , X N } from a distribution, the probability that the maximum of these samples equals x is:\nP (max(X i ) = x) = N P (X = x) P (X \u2264 x) N \u22121 . (7)\nWith the assumption of samples drawn from a Gumbel distribution, we have:\nP (max(X i ) = x) = N a exp(\u2212 x\u2212b a ) exp(\u2212 exp(\u2212 x\u2212b a )) exp(\u2212(N \u2212 1) exp(\u2212 x\u2212b a ))\n.\n(8) This simplifies to:\nP (max(X i ) = x) = 1 a exp(\u2212 x\u2212b\u2212a ln N a ) exp(\u2212 exp(\u2212 x\u2212b\u2212a ln N a )). (9\n)\nFrom this we see that the distribution of the max of N samples drawn from a Gumbel distribution with location parameter b and scale parameter a is also a Gumbel distribution with location parameter, b max = b + a ln N and scale parameter a max = a. Thus the expected max reward of N samples from each of the two arms in the problem is:\nb i + 0.5772a i + a i ln N. (10\n)\nConsider that M 1 is the better of the two arms in the problem. This necessitates a definition for \"better\". Let:\nb 1 + 0.5772a 1 + a 1 ln N > b 2 + 0.5772a 2 + a 2 ln N (11)\nwhich implies that a 1 \u2265 a 2 . Otherwise, for great enough N this inequality would fail to hold.\nIn the two-armed problem, where we do not know with certainty which arm is M 1 and which is M 2 , the expected max reward if we had access to an omniscient oracle is clearly b 1 + 0.5772a 1 + a 1 ln N -the expected max reward of giving all N trials to the better arm. However, given that we cannot know with certainty which arm is which, some exploration is necessary. Consider that we draw n samples from the observed second best arm, and N \u2212n samples from the observed best arm. Now consider the loss of reward associated with sampling from the second best arm. There are two cases to consider: 1. The observed best is really the best. In this case, the loss comes from giving n less samples to the best arm-with an expected loss equal to: a 1 (ln N \u2212 ln (N \u2212 n)).", "publication_ref": ["b7", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "2.", "text": "The observed best arm is really second best. The loss in this case depends on whether the expected value of giving N \u2212 n samples to the second best arm is greater than giving n samples to the best arm. That is, the expected loss is:\nmin{a 1 (ln N \u2212 ln n), (b 1 \u2212 b 2 ) + 0.5772(a 1 \u2212 a 2 ) + a 1 ln N \u2212a 2 ln (N \u2212 n)}.\nThis form of loss is maximized when the expected value of the max of n samples of the best arm equals that of N \u2212 n samples of the second best. This allows us to consider a simplification of the expected loss in this case: a 1 (ln N \u2212 ln n). 2\nLet q be the probability that the observed best arm is really second best. Therefore, (1\u2212q) is the probability that the observed best really is the best. The expected loss of sampling n times from the observed second best and N \u2212n times from the observed best arm, as a function of n is therefore:\nl(N ) = q(a 1 (ln N \u2212ln n))+(1\u2212q)(a 1 (ln N \u2212ln (N \u2212 n))).\n(12) This can be simplified to:\nl(N ) = q(a 1 (ln (N \u2212 n) \u2212 ln n)) + a 1 (ln N \u2212 ln (N \u2212 n)).\n(13) To select a value for n that minimizes the expected loss, we need to define q as a function of n. Let M b be the arm that is perceived as best (i.e., the arm perceived to have the highest expected max single sample reward over a series of N trials) and M w be the arm that is perceived as second best. The probability q can be stated as the probability that the expected max value of N samples of M w is greater than the expected max value of N samples of M b . If we note that the parameters of a Gumbel distribution can be estimated (see (NIST/SEMATECH 2003)) from the data by\u00e3 = s \u221a 6 \u03c0 andb =X \u2212 0.5772\u00e3, whereX and s are the sample mean and sample standard deviation, then we can define:\nq(n) = P \u239b \u239d (b b + 0.5772\u00e3 b +\u00e3 b ln N ) \u2212(b w + 0.5772\u00e3 w +\u00e3 w ln N ) < 0 \u239e \u23a0 (14) = P (X b + s b \u221a 6 \u03c0 ln N ) \u2212(X w + sw \u221a 6 \u03c0 ln N ) < 0 (15) = P X b \u2212X w < (s w \u2212 s b ) \u221a 6 \u03c0 ln N . (16\n)\nThe central limit theorem says thatX b approaches a normal distribution with mean \u00b5 b and variance\n\u03c3 2 b N \u2212n .\nSimilarly,X w approaches a normal distribution with mean \u00b5 w and variance \n\u03c3 2 b N \u2212n + \u03c3 2 w n .\nUsing an approximation for the tail of a normal distribution, we can define q(n) as:\nq(n) < \u223c 1 \u221a 2\u03c0 exp(\u2212x 2 /2) x (17\n)\nwhere\nx = (\u00b5 b \u2212 \u00b5 w ) + \u221a 6 \u03c0 ln (N )( \u03c3 b \u221a N \u2212n \u2212 \u03c3w \u221a n ) \u03c3 2 b N \u2212n + \u03c3 2 w n . (18\n)\nGiven the expressions for q(n) and x, note that q(n) decreases exponentially in n. Using the same simplification made by Holland (1975), note that no matter the value for \u03c3 b , there is a large enough N such that for n close to its optimal value,\n\u03c3 2 b N \u2212n \u03c3 2 w n\n. This leads to:\nx < \u223c (\u00b5 b \u2212 \u00b5 w ) \u221a n \u2212 \u03c3w \u221a 6 \u03c0 ln N \u03c3 w (19)\nTo select the value of n that will minimize the loss l(n) we begin by taking the derivative of l(n) with respect to n:\ndl dn = dq dn (a 1 (ln (N \u2212 n) \u2212 ln n)) \u2212q(n)( a1 N \u2212n + a1 n ) + a1 N \u2212n , (20\n)\nwhere dq dn\n< \u223c \u2212q(n) x 2 + 1 x dx dn , (21\n)\nand dx dn\n< \u223c \u00b5 b \u2212 \u00b5 w 2\u03c3 w \u221a n . (22\n)\nThe optimal value of n occurs when dl dn = 0 so we can get a bound on the optimal n by solving the following inequality:\n0 < \u223c a 1 N \u2212 n \u2212 q(n) a 1 N N \u2212 n \u2212q(n) x 2 + 1 x dx dn (a 1 (ln (N \u2212 n) \u2212 ln n)). (23\n)\nWe can collect the logarithmic terms on the left to obtain\nln (N \u2212 n) \u2212 ln n < \u223c (1 \u2212 q(n)N )x (N \u2212 n)q(n) dx dn (x 2 + 1) (24\n)\nRecalling that q(n) decreases exponentially in n, (1 \u2212 q(n)N ) rapidly approaches 1. Noting x x 2 +1 < \u223c 1 x , obtain: ln (N \u2212 n) \u2212 ln n < \u223c 1 (N \u2212 n)q(n) dx dn x (25)\nSubstituting expressions for q(n) and dx dn we get:\nln (N \u2212 n) \u2212 ln n < \u223c \u03c3w \u221a 8\u03c0 \u221a n (N \u2212n)(\u00b5 b \u2212\u00b5w) exp (\u00b5 b \u2212\u00b5w\u2212 \u03c3w \u221a 6 \u03c0 \u221a n ln (N )) 2 n 2\u03c3 2 w (26)\nFinally, exponentiate both sides of the inequality, to obtain:\nN \u2212 n < \u223c exp \u239b \u239d ln (n)+ \u03c3w \u221a 8\u03c0 \u221a n (N \u2212n)(\u00b5 b \u2212\u00b5w) exp (\u00b5 b \u2212\u00b5w\u2212 \u03c3w \u221a 6 \u03c0 \u221a n ln (N )) 2 n 2\u03c3 2 w \u239e \u23a0 (27)\nThe question that remains is which term in the exponential dominates the expression. We can take the fraction involving N \u2212 n up into the exponential and gain insight into the answer to this question:\nN \u2212 n < \u223c exp \u239b \u239c \u239c \u239d ln (n)+ exp \u239b \u239d (\u00b5 b \u2212\u00b5w\u2212 \u03c3w \u221a 6 \u03c0 \u221a n ln (N )) 2 n 2\u03c3 2 w + ln \u03c3w \u221a 8\u03c0 \u221a n (N \u2212n)(\u00b5 b \u2212\u00b5w) \u239e \u23a0 \u239e \u239f \u239f \u23a0 . (28)\nWe must now determine which part of this double exponential dominates as the total number of samples N grows large. Consider the following limits:\nlim N \u2192\u221e \u00b5 b \u2212 \u00b5 w \u2212 \u03c3w \u221a 6 \u03c0 \u221a n ln (N ) 2 n 2\u03c3 2 w = \u221e (29) lim N \u2192\u221e ln \u03c3 w \u221a 8\u03c0 \u221a n (N \u2212 n)(\u00b5 b \u2212 \u00b5 w ) = \u2212\u221e (30)\nNote that the first expression is dominated by the (ln N ) 2 and that within the logarithm of the second expression, the N in the denominator dominates. For large enough N , it is sufficient to consider which of (ln N ) 2 and ln (1/N ) dominates. Consider the following:\nlim N \u2192\u221e {(ln N ) 2 + ln (1/N )} = lim N \u2192\u221e {(ln N ) 2 + ln 1 \u2212 ln N } = lim N \u2192\u221e {ln N (ln N \u2212 1)} = \u221e (31\n) Taking this into account and making a few other obvious simplifications, we can arrive at:\nN \u2212 n \u223c \u0398(exp(exp(cn)))(32)\nThis shows that the number of trials N \u2212 n given to the observed best arm should grow double exponentially in n to maximize the expected max single sample reward.", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "Generalization to the K-Armed Case", "text": "Theorem 2 The Multiarmed Double Exponential Sampling Theorem: To optimize the Max K-Armed Bandit (samples drawn from Gumbel distributions), the observed best arm should be sampled at a rate increasing double exponentially relative to the number of samples given the other k \u22121 arms.\nProof To make this inductive leap from the result of the two-arm case to the k-arm case, observe the following. The worst case loss in the k-armed case occurs when the k \u2212 1 worst arms are identical (as is the case in Holland's analysis of the original k-armed bandit). If these k \u2212 1 arms are identical then it doesn't matter how we allocate trials among them-the result is equally poor. But, if any of these k \u2212 1 arms is better than any of the other k \u2212 2 arms, then we can improve our expected reward by allocating more trials to it. Assume the worst case that the k \u2212 1 arms are identical. With m * trials given to each of these k \u2212 1 worst arms, the analysis of the k-armed case can be considered a special case of the analysis of the two-armed problem. Specifically, we have the observed best arm and a meta-arm comprised of the aggregation of the other k \u2212 1 arms. The meta-arm is given n * = m * (k \u2212 1) trials uniformly distributed across the k \u2212 1 arms. Since the k \u2212 1 arms are identical in the worst case, the meta-arm behaves identically to the second best arm in the two-arm case. Thus, the number of samples N \u2212 m * (k \u2212 1) given the observed best arm should grow double exponentially in n * = m * (k \u2212 1).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Exploration Strategy", "text": "Recall that our goal is to find a good exploration strategy for allocating trials to different heuristics. To design an exploration policy that follows the double exponential sampling theorems, consider Boltzmann exploration (Sutton & Barto 1998). Let the temperature parameter T decay exponentially, choosing heuristic h i with probability:\nP (h i ) = exp((R i )/T ) H j=1 exp((R j )/T ) . (33\n)\nThe R i is some indicator/estimator of the expected max of a series of trials of heuristic h i . For example, R i can be an estimator for the expected max for some fixed length series of trials given some distribution assumption. To derive the double exponentially increasing allocation of trials to the observed best arm, the temperature parameter must follow an exponentially decreasing cooling schedule (e.g., T j = exp(\u2212j) where j is the trial number). That is, on iteration j choose heuristic h i with probability:\nP (h i |j) = exp((R i )/ exp(\u2212j)) H k=1 exp((R k )/ exp(\u2212j)) . (34\n)\nNext we present results from NP-hard scheduling domains that contrast the performance of this exploration policy with policies that allocate trials to the observed best heuristic at rates greater (and lesser) than double exponentially.", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "Weighted Tardiness Scheduling", "text": "Problem Formalization: The Weighted Tardiness Scheduling Problem is a sequencing problem. A set of jobs J = {j 1 , . . . , j N } must be sequenced on a single machine. Each of the N jobs j has a weight w j , duedate d j , and process time p j . Preempting a job during processing is not permitted. Only one job at a time can be processed. The objective is to sequence the set of jobs J on a machine to minimize the total weighted tardiness: T = j\u2208J w j T j = j\u2208J w j max (c j \u2212 d j , 0), where T j is the tardiness of job j; and c j , d j is the completion time and duedate of job j. The completion time of job j is equal to the sum over the process times of all jobs that come before it in the sequence plus that of the job j itself. Specifically, let \u03c0(j) be the position in the sequence of job j. We can now define c j as: c j = i\u2208J,\u03c0(i)<=\u03c0(j) p i .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Value-Biased Stochastic Sampling (VBSS):", "text": "VBSS is an iterative stochastic heuristic search algorithm (Cicirello & Smith 2005). A search heuristic is used to bias a random decision at each decision point. We use VBSS here to generate biased initial configurations for a local search for the weighted tardiness problem known as Multistart Dynasearch (Congram, Potts, & van de Velde 2002). The original Multistart Dynasearch used unbiased initial solutions.", "publication_ref": ["b6", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Dispatch Policies as Search Heuristic:", "text": "Many dispatch policies exist for this problem (Morton & Pentico 1993). A few of the best are used here as candidate search heuristics:\n\u2022 weighted shortest process time, WSPT i = wi pi ; \u2022 earliest duedate, EDD i = 1 di ;\n\u2022 COVERT i (t) = wi pi (1 \u2212 max (0,di\u2212pi\u2212t) kpi\n), with current time t and parameter k; and\n\u2022 R&M i (t) = wi pi exp(\u22121 * max (0,di\u2212pi\u2212t) kp\n), with average process timep.\nExperimental Setup: In this experiment, these heuristics are combined across multiple restarts of the dynasearch algorithm. On any given restart, VBSS is used, along with one of these heuristics to construct an initial solution, which is then locally optimized using dynasearch. The following exploration policies are considered: double exponentially increasing rate of allocations to the observed best heuristic (D-Exp); faster than double exponentially increasing allocation rate (Faster); and exponentially increasing allocation rate (Exp). We also compare to multistart dynasearch (M-Dyna) as originally specified by Congram et al. result depends on the assumption that each of the arms follows a Gumbel distribution. This seems restrictive. However, the extremal types theorem tells us that the distribution of the max of a series of trials belongs to one of three distribution families (Gumbel, Fr\u00e9chet, or Weibull) independent of the underlying distribution of trials. We assumed that the underlying samples were drawn from a Gumbel, but could more generally assume that the distribution of the max of a series of samples from each arm is a Gumbel. The most general assumption that the max of a series of trials from each arm follows a GEV distribution would not have allowed for a closed form analysis, necessitating an approximation.\nWe showed how the result of the Double Exponential Sampling Theorem can be applied to the problem of allocating iterations of a heuristic guided stochastic sampling algorithm to alternative search heuristics. This approach was validated in two NP-hard scheduling domains, showing that a faster than double exponential rate of allocations to the observed best heuristic results in over exploitation of the model of heuristic performance; while a slower rate results in over exploration. The double exponential increase in the rate of allocation to the observed best heuristic provides the balance between exploration and exploitation that leads to effective problem solving in these domains.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This work was supported in part by the National Aeronautics and Space Administration under contract NCC2-1243 and the CMU Robotics Institute.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Results: The results presented here are for the 100 job instances from the benchmark problem set from the OR-Library (Beasley 1998). The set contains 125 instances. Results are shown in Table 1. NB is the number of best known solutions found (no further improvement is made). ARPD (and MRPD) are the average (and maximum) relative percentage deviation from the best known solutions. The results shown are averages of 10 runs for all 125 problem instances.\nM-DYNA is the worst of the four variations considered. There is clearly benefit to biasing the initial configurations of the M-DYNA local search, contrary to the untested hypothesis of Congram et al. The trend for any number of iterations considered is that the double exponentially increasing rate of allocations finds the most best known solutions, with smallest percentage deviation from the best knowns. The next best in terms of these criteria is when the observed best heuristic is given an exponentially increasing allocation of trials, followed by the variation with a faster than double exponentially increasing rate of allocations.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "Resource Constrained Project Scheduling with Time Windows (RCPSP/max)", "text": "Problem Formalization: The RCPSP/max problem is defined as follows. Define P =< A,\u2206, R > as an instance of RCPSP/max. Let A be the set of activities A = {a 0 , a 1 , a 2 , . . . , a n , a n+1 }. Activity a 0 is a dummy activity representing the start of the project and a n+1 is similarly the project end. Each activity a j has a fixed duration p j , a start-time S j , and a completion-time C j which satisfy the constraint S j + p j = C j . Let \u2206 be a set of temporal constraints between activity pairs < a i , a j > of the form\nThe \u2206 are generalized precedence relations between activities. The T min i,j and T max i,j are minimum and maximum time-lags between the start times of pairs of activities. Let R be the set of renewable resources R = {r 1 , r 2 , . . . r m }. Each resource r k has an integer capacity c k \u2265 1. Execution of an activity a j requires one or more resources. For each resource r k , the activity a j requires an integer capacity rc j,k for the duration of its execution. An assignment of start-times to activities in A is time-feasible if all temporal constraints are satisfied and is resource-feasible if all resource constraints are satisfied. A schedule is feasible if both sets of constraints are satisfied. The problem is to find a feasible schedule with minimum makespan M where M (S) = max{C i }. That is, find a set of assignments to S such that S sol = arg min S M (S). The maximum time-lag constraints are the source of difficultye.g., finding feasible solutions alone is NP-Hard.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup:", "text": "We begin with a backtracking CSP heuristic search procedure for the problem (Franck, Neumann, & Schwindt 2001). We modify this algorithm to use VBSS to bias the choice made by the heuristic at each decision point. Five priority rules for the RCPSP/max problem are used as candidate search heuristics:  (Cesta, Oddi & Smith 2002) 8.0 670 1057\n\u2022 \"most total successors\" first,\nwhere Successors i is the set of not necessarily immediate successors of a i in the project network;\n\u2022 \"longest path following\" first, LPF i = lpath(i, n + 1), where lpath(i, n+1) is the length of the longest path from a i to a n+1 ; and\n\u2022 \"resource scheduling method\", RSM i = 1 1+max (0,max g\u2208eligible set,g =i (ESi+pi\u2212LSg)) . LS i and ES i are the latest and earliest start times. A few have been redefined from Neumann et al.'s definitions so that the eligible activity with the highest heuristic value is chosen. Eligible activities are those that can be time-feasibly scheduled given constraints involving already scheduled activities. We consider the alternative exploration policies: double exponentially increasing rate of allocations to the observed best heuristic (D-Exp); faster than double exponentially increasing rate (Faster); and exponentially increasing rate (Exp).\nResults: Table 2 shows the results. We use the benchmark problem instances of Schwindt (2003). \u2206 LB is the average relative deviation from the known lower bounds. NO and NF are the number of optimal solutions and feasible solutions found. The results are comparable to the first problem domain. The exponential rate of allocations to the observed best heuristic leads to over-exploration and the faster than double exponential rate leads to over-exploitationboth outperformed by the policy that allocates a double exponentially increasing number of trials to the observed best heuristic. These results are competitive with the current best known heuristic approaches to this NP-Hard problem (e.g., (Cesta, Oddi & Smith 2002;Smith & Pyle 2004;Cicirello & Smith 2004)).", "publication_ref": ["b9", "b4", "b13", "b4", "b14", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions", "text": "In learning domains with a reward structure as in the Max K-Armed Bandit, where the goal is to maximize the best single sample reward received over time, we have seen that the optimal strategy is to allocate a double exponentially increasing number of trials to the observed best action. This", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "The continuum-armed bandit problem", "journal": "SIAM Journal on Control and Optimization", "year": "1995", "authors": "R Agrawal"}, {"ref_id": "b1", "title": "The nonstochastic multiarmed bandit problem", "journal": "SIAM Journal on Computing", "year": "2002", "authors": "P Auer; N Cesa-Bianchi; Y Freund; R E Schapire"}, {"ref_id": "b2", "title": "Finitetime analysis of the multiarmed bandit problem", "journal": "Machine Learning", "year": "2002", "authors": "P Auer; N Cesa-Bianchi; P Fischer"}, {"ref_id": "b3", "title": "Bandit Problems: Sequential Allocation of Experiments", "journal": "Chapman-Hall", "year": "1985", "authors": "J E Beasley; D A Berry; B Fristedt"}, {"ref_id": "b4", "title": "A constraintbased method for project scheduling with time windows", "journal": "Journal of Heuristics", "year": "2002", "authors": "A Cesta; A Oddi; S F Smith"}, {"ref_id": "b5", "title": "Heuristic selection for stochastic search optimization: Modeling solution quality by extreme value theory", "journal": "", "year": "2004", "authors": "V A Cicirello; S F Smith"}, {"ref_id": "b6", "title": "Enhancing stochastic search performance by value-biased randomization of heuristics", "journal": "Journal of Heuristics", "year": "2005", "authors": "V A Cicirello; S F Smith"}, {"ref_id": "b7", "title": "An Introduction to Statistical Modeling of Extreme Values", "journal": "Springer-Verlag", "year": "2001", "authors": "S Coles"}, {"ref_id": "b8", "title": "An iterated dynasearch algorithm for the single-machine total weighted tardiness scheduling problem", "journal": "INFORMS Journal on Computing", "year": "2002", "authors": "R K Congram; C N Potts; S L Van De Velde"}, {"ref_id": "b9", "title": "Truncated branch-and-bound, schedule-construction, and schedule-improvement procedures for resourceconstrained project scheduling", "journal": "OR Spektrum", "year": "2001", "authors": "B Franck; K Neumann; C Schwindt"}, {"ref_id": "b10", "title": "Adaptation in Natural and Artificial Systems", "journal": "", "year": "1975", "authors": "J H Holland"}, {"ref_id": "b11", "title": "Heuristic Scheduling Systems", "journal": "John Wiley and Sons", "year": "1993", "authors": "T E Morton; D W Pentico"}, {"ref_id": "b12", "title": "NIST/SEMATECH. 2003. e-Handbook of Statistical Methods", "journal": "", "year": "", "authors": ""}, {"ref_id": "b13", "title": "", "journal": "", "year": "2003", "authors": "C Schwindt"}, {"ref_id": "b14", "title": "An effective algorithm for project scheduling with arbitrary temporal constraints", "journal": "", "year": "2004", "authors": "T B Smith; J M Pyle"}, {"ref_id": "b15", "title": "Reinforcement Learning: An Introduction", "journal": "MIT Press", "year": "1998", "authors": "R S Sutton; A G Barto"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "distribution ofX b \u2212X w is the convolution of the distributionsX b and \u2212X w . The convolution of these distributions is by definition a normal distribution with mean \u00b5 b \u2212 \u00b5 w and variance", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "The Two-Armed Double Exponential Sampling Theorem: To optimize the Max 2-Armed Bandit, where the arm samples are drawn from Gumbel distributions, the observed best arm should be sampled at a rate that increases double exponentially relative to the observed second best.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Weighted tardiness: For each number of restarts [N ], bold indicates the most best known solutions found.", "figure_data": "AlgorithmNB ARPD MRPDD-EXP[400] EXP[400] FASTER[400] M-DYNA[400]94.3 85 78.7 620.12 0.14 0.19 1.6610.07 11.28 13.51 76.18D-EXP[800] EXP[800] FASTER[800] M-DYNA[800]100.7 89.3 83.6 68.30.12 0.14 0.17 1.2910.07 11.28 13.51 74.28D-EXP[1600] EXP[1600] FASTER[1600] M-DYNA[1600]107.3 95 87.5 73.30.11 0.12 0.16 1.168.47 9.75 11.28 71.06"}], "formulas": [{"formula_id": "formula_0", "formula_text": "max k i=1 n i R(\u00b5 i , \u03c3 i ).", "formula_coordinates": [2.0, 130.01, 76.26, 86.47, 30.44]}, {"formula_id": "formula_1", "formula_text": "1 N \u2212 n * \u223c \u0398(exp(cn * )), (2", "formula_coordinates": [2.0, 122.64, 189.59, 165.98, 31.37]}, {"formula_id": "formula_2", "formula_text": ")", "formula_coordinates": [2.0, 288.62, 209.59, 3.87, 11.03]}, {"formula_id": "formula_3", "formula_text": "N \u2212 (k \u2212 1)m * \u223c \u0398(exp(cm * )).(3)", "formula_coordinates": [2.0, 104.67, 335.02, 187.83, 11.69]}, {"formula_id": "formula_4", "formula_text": "max k max i=1 ni max j=1 R j (D i ),(4)", "formula_coordinates": [2.0, 126.03, 514.4, 166.47, 18.91]}, {"formula_id": "formula_5", "formula_text": "P (Z \u2264 z) = G(z) = exp \u2212 exp \u2212 z \u2212 b a , (5", "formula_coordinates": [2.0, 326.17, 597.35, 227.96, 22.91]}, {"formula_id": "formula_6", "formula_text": ")", "formula_coordinates": [2.0, 554.13, 602.48, 3.87, 11.03]}, {"formula_id": "formula_7", "formula_text": "P (Z = z) = 1 a exp \u2212 z\u2212b a exp \u2212 exp \u2212 z\u2212b a . (6", "formula_coordinates": [2.0, 335.2, 660.33, 218.93, 34.75]}, {"formula_id": "formula_8", "formula_text": ")", "formula_coordinates": [2.0, 554.12, 670.22, 3.87, 11.03]}, {"formula_id": "formula_9", "formula_text": "P (max(X i ) = x) = N P (X = x) P (X \u2264 x) N \u22121 . (7)", "formula_coordinates": [3.0, 64.61, 254.91, 227.88, 12.14]}, {"formula_id": "formula_10", "formula_text": "P (max(X i ) = x) = N a exp(\u2212 x\u2212b a ) exp(\u2212 exp(\u2212 x\u2212b a )) exp(\u2212(N \u2212 1) exp(\u2212 x\u2212b a ))", "formula_coordinates": [3.0, 64.58, 303.77, 214.58, 57.91]}, {"formula_id": "formula_11", "formula_text": "P (max(X i ) = x) = 1 a exp(\u2212 x\u2212b\u2212a ln N a ) exp(\u2212 exp(\u2212 x\u2212b\u2212a ln N a )). (9", "formula_coordinates": [3.0, 64.3, 388.49, 224.33, 29.78]}, {"formula_id": "formula_12", "formula_text": ")", "formula_coordinates": [3.0, 288.62, 404.51, 3.87, 11.03]}, {"formula_id": "formula_13", "formula_text": "b i + 0.5772a i + a i ln N. (10", "formula_coordinates": [3.0, 123.39, 495.73, 164.96, 11.81]}, {"formula_id": "formula_14", "formula_text": ")", "formula_coordinates": [3.0, 288.35, 495.73, 4.15, 11.03]}, {"formula_id": "formula_15", "formula_text": "b 1 + 0.5772a 1 + a 1 ln N > b 2 + 0.5772a 2 + a 2 ln N (11)", "formula_coordinates": [3.0, 58.98, 543.27, 233.52, 11.82]}, {"formula_id": "formula_16", "formula_text": "min{a 1 (ln N \u2212 ln n), (b 1 \u2212 b 2 ) + 0.5772(a 1 \u2212 a 2 ) + a 1 ln N \u2212a 2 ln (N \u2212 n)}.", "formula_coordinates": [3.0, 329.46, 137.94, 228.54, 21.17]}, {"formula_id": "formula_17", "formula_text": "l(N ) = q(a 1 (ln N \u2212ln n))+(1\u2212q)(a 1 (ln N \u2212ln (N \u2212 n))).", "formula_coordinates": [3.0, 319.5, 270.52, 238.86, 10.21]}, {"formula_id": "formula_18", "formula_text": "l(N ) = q(a 1 (ln (N \u2212 n) \u2212 ln n)) + a 1 (ln N \u2212 ln (N \u2212 n)).", "formula_coordinates": [3.0, 319.5, 310.9, 238.49, 10.21]}, {"formula_id": "formula_19", "formula_text": "q(n) = P \u239b \u239d (b b + 0.5772\u00e3 b +\u00e3 b ln N ) \u2212(b w + 0.5772\u00e3 w +\u00e3 w ln N ) < 0 \u239e \u23a0 (14) = P (X b + s b \u221a 6 \u03c0 ln N ) \u2212(X w + sw \u221a 6 \u03c0 ln N ) < 0 (15) = P X b \u2212X w < (s w \u2212 s b ) \u221a 6 \u03c0 ln N . (16", "formula_coordinates": [3.0, 335.2, 469.96, 222.8, 107.26]}, {"formula_id": "formula_20", "formula_text": ")", "formula_coordinates": [3.0, 553.85, 559.43, 4.15, 11.03]}, {"formula_id": "formula_21", "formula_text": "\u03c3 2 b N \u2212n .", "formula_coordinates": [3.0, 477.15, 600.38, 21.89, 16.74]}, {"formula_id": "formula_22", "formula_text": "\u03c3 2 b N \u2212n + \u03c3 2 w n .", "formula_coordinates": [4.0, 145.67, 53.81, 46.91, 16.73]}, {"formula_id": "formula_23", "formula_text": "q(n) < \u223c 1 \u221a 2\u03c0 exp(\u2212x 2 /2) x (17", "formula_coordinates": [4.0, 117.66, 88.59, 170.69, 25.17]}, {"formula_id": "formula_24", "formula_text": ")", "formula_coordinates": [4.0, 288.35, 94.86, 4.15, 11.03]}, {"formula_id": "formula_25", "formula_text": "x = (\u00b5 b \u2212 \u00b5 w ) + \u221a 6 \u03c0 ln (N )( \u03c3 b \u221a N \u2212n \u2212 \u03c3w \u221a n ) \u03c3 2 b N \u2212n + \u03c3 2 w n . (18", "formula_coordinates": [4.0, 74.46, 132.12, 213.89, 41.57]}, {"formula_id": "formula_26", "formula_text": ")", "formula_coordinates": [4.0, 288.35, 147.37, 4.15, 11.03]}, {"formula_id": "formula_27", "formula_text": "\u03c3 2 b N \u2212n \u03c3 2 w n", "formula_coordinates": [4.0, 114.44, 225.07, 45.74, 16.74]}, {"formula_id": "formula_28", "formula_text": "x < \u223c (\u00b5 b \u2212 \u00b5 w ) \u221a n \u2212 \u03c3w \u221a 6 \u03c0 ln N \u03c3 w (19)", "formula_coordinates": [4.0, 103.99, 244.78, 188.52, 32.08]}, {"formula_id": "formula_29", "formula_text": "dl dn = dq dn (a 1 (ln (N \u2212 n) \u2212 ln n)) \u2212q(n)( a1 N \u2212n + a1 n ) + a1 N \u2212n , (20", "formula_coordinates": [4.0, 99.57, 313.14, 188.79, 36.48]}, {"formula_id": "formula_30", "formula_text": ")", "formula_coordinates": [4.0, 288.35, 324.51, 4.15, 11.03]}, {"formula_id": "formula_31", "formula_text": "< \u223c \u2212q(n) x 2 + 1 x dx dn , (21", "formula_coordinates": [4.0, 139.99, 365.12, 148.36, 24.05]}, {"formula_id": "formula_32", "formula_text": ")", "formula_coordinates": [4.0, 288.35, 371.39, 4.15, 11.03]}, {"formula_id": "formula_33", "formula_text": "< \u223c \u00b5 b \u2212 \u00b5 w 2\u03c3 w \u221a n . (22", "formula_coordinates": [4.0, 156.26, 400.61, 132.09, 23.82]}, {"formula_id": "formula_34", "formula_text": ")", "formula_coordinates": [4.0, 288.35, 405.74, 4.15, 11.03]}, {"formula_id": "formula_35", "formula_text": "0 < \u223c a 1 N \u2212 n \u2212 q(n) a 1 N N \u2212 n \u2212q(n) x 2 + 1 x dx dn (a 1 (ln (N \u2212 n) \u2212 ln n)). (23", "formula_coordinates": [4.0, 71.6, 459.49, 216.75, 48.99]}, {"formula_id": "formula_36", "formula_text": ")", "formula_coordinates": [4.0, 288.35, 490.7, 4.15, 11.03]}, {"formula_id": "formula_37", "formula_text": "ln (N \u2212 n) \u2212 ln n < \u223c (1 \u2212 q(n)N )x (N \u2212 n)q(n) dx dn (x 2 + 1) (24", "formula_coordinates": [4.0, 68.33, 532.16, 220.03, 26.55]}, {"formula_id": "formula_38", "formula_text": ")", "formula_coordinates": [4.0, 288.35, 537.29, 4.15, 11.03]}, {"formula_id": "formula_39", "formula_text": "Recalling that q(n) decreases exponentially in n, (1 \u2212 q(n)N ) rapidly approaches 1. Noting x x 2 +1 < \u223c 1 x , obtain: ln (N \u2212 n) \u2212 ln n < \u223c 1 (N \u2212 n)q(n) dx dn x (25)", "formula_coordinates": [4.0, 54.0, 564.16, 238.54, 62.65]}, {"formula_id": "formula_40", "formula_text": "ln (N \u2212 n) \u2212 ln n < \u223c \u03c3w \u221a 8\u03c0 \u221a n (N \u2212n)(\u00b5 b \u2212\u00b5w) exp (\u00b5 b \u2212\u00b5w\u2212 \u03c3w \u221a 6 \u03c0 \u221a n ln (N )) 2 n 2\u03c3 2 w (26)", "formula_coordinates": [4.0, 68.11, 653.67, 224.39, 50.5]}, {"formula_id": "formula_41", "formula_text": "N \u2212 n < \u223c exp \u239b \u239d ln (n)+ \u03c3w \u221a 8\u03c0 \u221a n (N \u2212n)(\u00b5 b \u2212\u00b5w) exp (\u00b5 b \u2212\u00b5w\u2212 \u03c3w \u221a 6 \u03c0 \u221a n ln (N )) 2 n 2\u03c3 2 w \u239e \u23a0 (27)", "formula_coordinates": [4.0, 324.59, 72.79, 233.41, 61.59]}, {"formula_id": "formula_42", "formula_text": "N \u2212 n < \u223c exp \u239b \u239c \u239c \u239d ln (n)+ exp \u239b \u239d (\u00b5 b \u2212\u00b5w\u2212 \u03c3w \u221a 6 \u03c0 \u221a n ln (N )) 2 n 2\u03c3 2 w + ln \u03c3w \u221a 8\u03c0 \u221a n (N \u2212n)(\u00b5 b \u2212\u00b5w) \u239e \u23a0 \u239e \u239f \u239f \u23a0 . (28)", "formula_coordinates": [4.0, 322.84, 179.17, 235.16, 65.74]}, {"formula_id": "formula_43", "formula_text": "lim N \u2192\u221e \u00b5 b \u2212 \u00b5 w \u2212 \u03c3w \u221a 6 \u03c0 \u221a n ln (N ) 2 n 2\u03c3 2 w = \u221e (29) lim N \u2192\u221e ln \u03c3 w \u221a 8\u03c0 \u221a n (N \u2212 n)(\u00b5 b \u2212 \u00b5 w ) = \u2212\u221e (30)", "formula_coordinates": [4.0, 341.6, 283.44, 216.4, 65.84]}, {"formula_id": "formula_44", "formula_text": "lim N \u2192\u221e {(ln N ) 2 + ln (1/N )} = lim N \u2192\u221e {(ln N ) 2 + ln 1 \u2212 ln N } = lim N \u2192\u221e {ln N (ln N \u2212 1)} = \u221e (31", "formula_coordinates": [4.0, 332.54, 420.02, 221.31, 87.83]}, {"formula_id": "formula_45", "formula_text": "N \u2212 n \u223c \u0398(exp(exp(cn)))(32)", "formula_coordinates": [4.0, 382.62, 535.84, 175.39, 11.37]}, {"formula_id": "formula_46", "formula_text": "P (h i ) = exp((R i )/T ) H j=1 exp((R j )/T ) . (33", "formula_coordinates": [5.0, 111.17, 352.63, 177.19, 26.7]}, {"formula_id": "formula_47", "formula_text": ")", "formula_coordinates": [5.0, 288.35, 357.35, 4.15, 11.03]}, {"formula_id": "formula_48", "formula_text": "P (h i |j) = exp((R i )/ exp(\u2212j)) H k=1 exp((R k )/ exp(\u2212j)) . (34", "formula_coordinates": [5.0, 91.83, 492.37, 196.52, 26.7]}, {"formula_id": "formula_49", "formula_text": ")", "formula_coordinates": [5.0, 288.35, 497.08, 4.15, 11.03]}, {"formula_id": "formula_50", "formula_text": "\u2022 COVERT i (t) = wi pi (1 \u2212 max (0,di\u2212pi\u2212t) kpi", "formula_coordinates": [5.0, 319.5, 505.9, 176.65, 14.5]}, {"formula_id": "formula_51", "formula_text": "\u2022 R&M i (t) = wi pi exp(\u22121 * max (0,di\u2212pi\u2212t) kp", "formula_coordinates": [5.0, 319.5, 538.18, 176.89, 14.5]}], "doi": ""}