{"title": "Experimental Design for Causal Effect Identification", "authors": "Sina Akbari; Negar Kiyavash", "pub_date": "2023-08-17", "abstract": "Pearl's do calculus is a complete axiomatic approach to learn the identifiable causal effects from observational data. When such an effect is not identifiable, it is necessary to perform a collection of often costly interventions in the system to learn the causal effect. In this work, we consider the problem of designing the collection of interventions with the minimum cost to identify the desired effect. First, we prove that this problem is NP-complete and subsequently propose an algorithm that can either find the optimal solution or a logarithmic-factor approximation of it. This is done by establishing a connection between our problem and the minimum hitting set problem. Additionally, we propose several polynomial time heuristic algorithms to tackle the computational complexity of the problem. Although these algorithms could potentially stumble on sub-optimal solutions, our simulations show that they achieve small regrets on random graphs.", "sections": [{"heading": "Introduction", "text": "Causal inference plays a key role in many applications such as psychology (Foster, 2010), econometrics (Hoover, 1990), education, social sciences (Murnane and Willett, 2010;Gangl, 2010), etc. Causal effect identification, one of the most fundamental topics in causal inference, is concerned with estimating the effect of intervening on a set of variables, say X on another set of variables, say Y denoted by P (Y |do(X)). The estimation is performed having access to a set of observational and/or interventional distributions under causal assumptions that are usually encoded in the form of a causal graph. The causal graph of a system of variables captures the interconnection among the variables and can be inferred from a combination of observations, experiments, and expert knowledge about the phenomenon under investigation (Spirtes et al., 2000). Throughout this work, we assume that the causal graph is given as side information.\nGiven a causal graph, it is known that in the absence of unobserved (latent) variables, every causal effect is identifiable from mere observational data (Robins, 1987;Spirtes et al., 2000). On the other hand, inferring causal effects from data becomes challenging in the presence of latent variables. In the setting where only observational data is available, the do calculus, introduced by Pearl (1995), has been shown to be complete. That is, it provides a complete set of rules to compute a causal effect (if identifiable) given a causal graph and observational data (Huang and Valtorta, 2006). Moreover, polynomial time algorithms exist that can determine the identifiability of a causal effect using do-calculus (Shpitser and Pearl, 2006).\nIn recent years, there has been an increase in the effort to generalize Pearl's do-calculus to the setting in which data from both observational and interventional data are available for identifying a causal effect. For instance, Bareinboim and Pearl (2012) studied the problem of estimating the causal effect of intervening on a set of variables X on the outcome Y when we experiment on a different set Z. This problem is known as z-identifiability, and Bareinboim and Pearl (2012) provide a complete algorithm for computing P (Y |do(X)) using information provided by experiments on all subsets of Z. A slightly more general version of z-identifiability is called g-identifiability, which considers the problem of identifying P (Y |do(X)) from an arbitrary collection of distributions. Lee et al. (2020) and Kivva et al. (2022) studied the g-identifiability problem and showed that Pearl's do-calculus is also complete in this setting. All three of the aforementioned works study the identifiability of P (Y |do(X)). There are also various works that consider the more general problem of identifying a conditional causal effect of the form P (Y |do(X), W ) from a combination of distributions. However, most of these works do not manage to provide complete results a la Pearl's do-calculus. See (Tikka et al., 2019), for a complete review on causal effect identification.\nWhen a causal effect is not identifiable from observations, it is necessary to perform a collection of interventions to infer the effect of interest. However, such interventions could be costly, impossible, or unethical to perform. Therefore, naturally we are interested in the problem of designing a collection of low cost permitted interventions to identify a causal effect. This is the focus of our paper. A closely related work to ours is (Kandasamy et al., 2019), in which, the authors considered the problem of finding the minimum number of interventions to identify every possible causal query. Their approach is based on two limiting assumptions, namely that all interventions have the same cost and that we are allowed to intervene on any variable. More importantly, the result in (Kandasamy et al., 2019) guarantees to render every causal effect identifiable, which makes the solution sub-optimal for a specific query. In other words, a set of interventions that makes all causal effects identifiable might have higher aggregate cost than a set of interventions designed for identifying a specific causal effect.\nDesigning minimum-cost interventions has also received attention in the causal discovery literature, under the term experimental design. In causal discovery, the goal is to infer the causal graph from a dataset (Spirtes et al., 2000;Colombo et al., 2012;Akbari et al., 2021). It is known that mere observational data cannot fully recover the causal graph, and thus additional interventional data is required to precisely learn the graph. Lindgren et al. (2018) considered the problem of designing a set with minimum number of interventions to learn a causal graph given the essential graph (assuming no latent variable), and showed that this problem is NP-hard. Addanki et al. (2020) studied a similar problem in the presence of latent variables. The problem of orienting the maximum number of edges using a fixed number of interventions was studied in (Hauser and B\u00fchlmann, 2014;Ghassami et al., 2018;Agrawal et al., 2019). Addanki et al. (2021) studied designing interventions for causal discovery when the goal is to learn a portion of the edges in the causal graph instead of all of them.\nIn this work, we study the problem of designing the set of minimum cost interventions for identifying a specific causal effect, where intervening on each variable may induce a different cost, and we are not necessarily allowed to intervene on every variable. Extending the findings of our ICML paper, Akbari et al. (2022), we outline our contributions as follows.\n\u2022 We prove that finding a minimum-cost intervention set for identifying a specific causal effect is NP-complete. Further, we show that approximating the solution to this problem within a sub-logarithmic factor of the optimal solution is NP-hard.\n\u2022 We formulate the minimum cost intervention problem in terms of a minimum hitting set problem, and propose an algorithm based on this formulation that can find the optimal solution to the minimum cost intervention problem. This algorithm can also be used to approximate the solution up to a logarithmic-factor 1 .\n\u2022 We propose several heuristic algorithms to solve the minimum cost intervention problem in polynomial time, and through empirical evaluations show that they achieve low-regret solutions in randomly generated causal graphs. We also provide an upper bound on the regret of these algorithms in the average case when the graph is generated due to Eros-Renyi model.\ns 1 s 2 x v 3 v 2 v 1\n\u2022 We analyze several special cases of the minimum cost intervention problem that can be solved in polynomial time, and provide efficient algorithms for these cases.", "publication_ref": ["b16", "b23", "b38", "b17", "b45", "b43", "b45", "b40", "b24", "b44", "b6", "b6", "b35", "b30", "b47", "b28", "b28", "b45", "b10", "b4", "b36", "b0", "b22", "b20", "b2", "b1", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Terminology & Problem Description", "text": "We briefly introduce the notations used in this paper. We begin with the definition of structural causal model (SCM) (Pearl et al., 2000), which is the framework we use throughout this paper.\nAn SCM is a tuple M = (U, V, F, P (U )), where U is the set of exogenous variables which are not observed but affect the relationship among the variables of the system, V = {v 1 , ..., v n } is the set of observed endogenous variables where each v i \u2208 V is a function of a subset of V \u222a U denoted by pa(v i ) \u222a pa U (v i ), F = {f 1 , ..., f n } is a set of functions where each f i determines the value of v i = f i (pa(v i ), pa U (v i )), and P (U ) is the joint probability distribution over the variables U . An intervention is defined through a mathematical operator do(X =X), which replaces the functions corresponding to variables X in the model M with a constant function f =X. Denoting this model by MX , the interventional distribution P (Y |do(X =X)) is then given by P MX (Y ), or PX (Y ) in short (Pearl, 2012). For a subset S of variables V , we denote by Q[S] = P (S|do(V \\ S)), the interventional distribution of the variables S after intervention on the rest of the variables (Tian and Pearl, 2002). The causal graph corresponding to the SCM M is a semi-Markovian graph G with one vertex for each v i \u2208 V , where there is a directed edge from v i to v j if the value of v j is a function of v i (v i \u2208 pa(v j )), and there is a bidirected edge between v i and v j if the values of v i and v j are both functions of a common exogenous variable u. See Figure 1 for an example. We use V to denote the set of vertices of G throughout the paper. We use the words vertex and variable interchangeably throughout this work, as each vertex represents a variable.\nWe use small letters for variables, capital letters for sets of variables, and bold letters for collections of subsets of variables (set families), respectively. We utilise common graph-theoretic terms such as parents of a vertex x (denoted by pa(x)), as well as children, ancestors, and descendants of a vertex. We denote by biD(x), the set of vertices that have a bidirected edge to x. We also denote by pa \u2194 (x) = pa(x) \u2229 biD(x) the set of parents of x that have a bidirected edge to x. For a set X, pa(X) is defined as pa(x) = \u222a x\u2208X pa(x) \\ X. The rest of the aforementioned sets are defined analogously for a set of variables X. For a set of variables X, we denote by G [X] the induced vertex subgraph of G over the vertices X. The connected components of the edge-induced subgraph of G over its bidirected edges are called c-components (aka districts) of G (Tian and Pearl, 2002). For example, the causal graph in Figure 1 consists of only one c-component. However, its induced subgraph over {s 1 , x, v 2 } consists of two c-components {x, s 1 } and {v 2 }. Definition 1 (Identifiability). We say a causal effect P (Y |do(X)) is identifiable in G, if it is uniquely computable from P (V ), the joint distribution of the observed variables. More precisely, for any positive models M 1 and M 2 that are compatible with the causal graph G and P M1 (V ) = P M2 (V ) (admit the same joint distribution), P M1 (Y |do(X)) = P M2 (Y |do(X)).\nAnalogously, for a given set of interventional distributions P = {P (Y 1 |do(X 1 )), ..., P (Y k |do(X k ))}, we say P (S|do(T )) is identifiable in the causal graph G from P, if for any positive model M that is compatible with G, P M (S|do(T )) is uniquely computable from P. Letting P be P = {P (V |do(\u2205))}, this generalization reduces to Definition 1.", "publication_ref": ["b42", "b41", "b46", "b46"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Problem Description", "text": "Let G be a semi-Markovian graph on the vertex set V along with a cost function C : V \u2192 R \u22650 , where C(x) for some x \u2208 V denotes the cost of intervening on variable x. With slight abuse of notation, we denote the cost of intervening on a set of variables X \u2286 V by C(X). In this work, we assume that the intervention cost is additive, unless otherwise stated (we shall discuss non-additive cost models in Section 5.) More precisely, we make the following assumption.\nAssumption 1. For a set X \u2286 V , the cost of intervening on X is C(X) := x\u2208X C(x), and for a collection X of subsets of V , the cost of intervention on X is C(X) := X\u2208X C(X).\nMoreover, we assume that there is no cost for observing a variable, i.e., C(\u2205) = 0. Therefore, when intervening on set X, we have access to Q[V \\ X] = P (V \\ X|do(X)) at the cost of C(X).\nRemark 2. In this setting, we can model a non-intervenable variable x by assigning the cost\nC(x) = \u221e.\nFor a given causal graph G and disjoint subsets S, T \u2286 V , our goal is to find a collection\nA = {A 1 , A 2 , ..., A m } of subsets of V with minimum C(A) such that P (S|do(T )) is identifiable in G given {Q[V \\ A 1 ], ..., Q[V \\ A m ]}.\nMore precisely, let ID G (S, T ) denote the set of all collections of subsets of V , e.g., A = {A 1 , A 2 , ..., A m }, where\nA i \u2286 V, 1 \u2264 i \u2264 m, such that P (S|do(T )) is identifiable in G given {Q[V \\ A 1 ], ..., Q[V \\ A m ]}. Note that |ID G (S, T )| \u2264 2 2 |V | .\nThus, the minimumcost intervention design problem to identify P (S|do(T )) can be cast as the following optimization problem,\nA * S,T \u2208 arg min A\u2208ID G (S,T ) A\u2208A C(A).(1)\nWe say A * S,T is the minimum-cost intervention for identifying P (S|do(T )) in G. Note that additional constraints or regularization terms can be added to target a specific minimum-cost intervention set within ID G (S, T ).\nIt has been shown that P (S|do(T )) is identifiable in G if and only if Q[Anc G\\T (S)] is identifiable in G, where Anc G\\T (S) are ancestors of S in G after deleting vertices T (Kivva et al., 2022;Lee et al., 2020;Jaber et al., 2019;Shpitser and Pearl, 2006)\n. That is, ID G (S, T ) = ID G (Anc G\\T (S), V \\ Anc G\\T (S)).\nIn other words, any causal query of the form P (S|do(T )) can be transformed into a causal query that is in the form of Q[\u2022]. Therefore, in what follows, we focus on the minimum-cost intervention problem for identifying causal queries of the form Q[S] = P (S|do(V \\ S)). Throughout the rest of this work, we will assume T = V \\ S in Equation (1). In Section 3, we study the above problem when G [S] is a single c-component. In Section 4, we generalize our results to an arbitrary subset S. We discuss the problem under non-additve costs in Section 5. We evaluate our proposed algorithms in terms of runtime and optimality in Section 6.", "publication_ref": ["b30", "b35", "b25", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "Single C-component Identification", "text": "The main challenge in solving the optimization problem in Equation ( 1) is that the number of elements in ID G (S, T ) is possibly super-exponential. Throughout this section, we assume that S is a subset of variables in G such that G [S] is a single c-component, unless stated otherwise. Under this assumption, we first show 2 in Theorem 5 that ID G (S, T ) in Equation ( 1) can be replaced with a substantially smaller subset without changing the solution to the problem in (1). Next, we prove in Theorems 8 and 12 that even after this substitution, the minimum-cost intervention problem remains NP-complete.\nLemma 3. Suppose S is a subset of variables such that G [S] is a single c-component. Let A = {A 1 , A 2 , ..., A m } be a collection of subsets of V such that A \u222a \u2229 S = \u2205, where\nA \u222a := \u222a m i=1 A i . If A \u2208 ID G (S, V \\ S), then the singleton collection A \u222a = {A \u222a } also belongs to ID G (S, V \\ S). Remark 4. The cost of A \u222a in Lemma 3 is at most C(A), C(A) = Ai\u2208A a\u2208Ai C(a) \u2265 a\u2208A\u222a C(a) = C(A \u222a ),\nwhere the inequality holds because each a \u2208 A \u222a appears exactly once in the right-hand-side summation, whereas it appears at least once on the left hand side.\nNext, we prove that for a given subset S where G [S] is a c-component, the collection A * S,V \\S is singleton, that is, it contains exactly one intervention set.\nTheorem 5. Suppose S is a subset of variables such that G [S] is a c-component. Let A = {A 1 , A 2 , ..., A m } be a collection of subsets such that A \u2208 ID G (S, V \\ S) and m > 1. Then, there exists a subset\u00c3 \u2286 V such that\u00c3 = {\u00c3} \u2208 ID G (S, V \\ S) and C(\u00c3) \u2264 C(A).\nTheorem 5 indicates that when G [S] is a c-component, the minimum-cost intervention problem in Equation 1 reduces to the problem of finding a single intervention set\nA * such that Q[S] is identifiable from Q[V \\ A * ].\nMore formally, the optimization in (1) reduces to the following problem,\nA * S \u2208 arg min A\u2208ID1(S) a\u2208A C(a),(2)\nwhere ID 1 (S) is the set of all subsets\nA of V such that Q[S] is identifiable from Q[V \\A]. Note that |ID 1 (S)| \u2264 2 |V | .\nFor the rest of this section, we discuss the solution to Equation (2). The following lemma further constrains the worst-case cardinality of ID 1 (S) to at most 2 |V |\u2212|S| elements, noting that intervening on variables in S does not help in identifying Q [S]. That is, we need only to consider all subsets\nA of V , such that Q[S] is identifiable from Q[V \\ A] and A \u2229 S = \u2205. Lemma 6. Suppose S is a subset of variables such that G [S] is a c-component. If A \u2208 ID 1 (S), then A \u2229 S = \u2205.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Hardness", "text": "In this section, we study the complexity of the minimum-cost intervention design problem. We show that despite the substantial decrease in the search space complexity of the optimization achieved through Theorem 5 (from Eq. 1 to Eq. 2), the minimum-cost intervention problem in Equation (2) remains NP-hard. More precisely, we show that there exists a polynomial-time reduction from the Weighted Minimum Vertex Cover (WMVC) problem to the min-cost intervention problem. For the sake of completeness, we formally define the WMVC problem.\nDefinition 7 (WMVC). Given an undirected graph H = (V H , E H ) and a weight function \u03c9 : V H \u2192 R \u22650 , a vertex cover is a subset A \u2286 V H such that A covers all the edges of H, i.e., for any edge {x, y} \u2208 E H , at least one of x or y is a member of A. The weighted minimum vertex cover problem's objective is to find a set A * among all vertex covers that minimizes a\u2208A \u03c9(a).\nWMVC is known to be NP-hard . Even finding an approximation within a factor of 1.36 to this problem is NP-hard (Dinur and Safra, 2005). In fact, there is no known polynomial-time algorithm to approximate WMVC problem within a constant factor less than two 3 . Indeed, WMVC remains NP-hard even for bounded-degree graphs (Garey et al., 1974). The following theorem shows that all these statements also hold for the min-cost intervention problem.\nTheorem 8. WMVC problem is reducible to a minimum-cost intervention problem in polynomial time.\nNote that since the ID algorithm proposed by Shpitser and Pearl (2006) can be employed to verify a solution to our problem in polynomial time, the minimum-cost intervention problem is in NP. Therefore, we have the following corollary.\nCorollary 9. The minimum-cost intervention problem is NP-complete.\nRemark 10. The unweighted version of WMVC problem (i.e., when the weight function is given by \u03c9(\u2022) = 1) can be reduced to a minimum-cost intervention problem with the constant cost function C(\u2022) = 1 in polynomial time. Consequently, the NP-completeness does not stem from arbitrary choice of cost functions. This claim is formally proved in Appendix C.\nA result more general than Theorem 8 is provided below, which shows that the minimum-weight hitting set (MWHS) problem can also be reduced to the min-cost intervention problem.\nDefinition 11 (MWHS). Let V = {v 1 , ..., v n } be a set of objects along with a weight function\n\u03c9 : V \u2192 R \u22650 . Given a collection of subsets of V such as F = {F 1 , ..., F k }, F i \u2286 V, 1 \u2264 i \u2264 k, a hitting set for F is a subset A \u2286 V such that A hits all the sets in F, i.e., for any 1 \u2264 i \u2264 k, A \u2229 F i \u0338 = \u2205.\nThe weighted minimum hitting set problem's objective is to find a set A * among all hitting sets that minimizes a\u2208A \u03c9(a).\nTheorem 12. MWHS problem is reducible to minimum-cost intervention problem in polynomial time.\nThe significance of Theorem 12 compared to Theorem 8 is that MWHS is not only NP-hard to solve, but also NP-hard to approximate within a factor better than a logarithmic factor (Feige, 1998). This results in the following corollary.\nCorollary 13. There is no (1 \u2212 o(1)) ln |V | approximation scheme for the minimum-cost intervention problem, unless NP has quasi-polynomial-time algorithms.\nOn the other hand, as with any other NP-complete problem, certain instances of the minimum-cost intervention problem can be solved in polynomial-time. An interesting group of such instances are discussed in Appendix D. Despite being restrictive, these special cases might provide useful insights for finding efficient algorithms in more general settings. Naturally, Theorem 12 implies that the algorithms proposed in this paper can aid to solve some other problems in the NP class.", "publication_ref": ["b11", "b19", "b44", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Minimum Hitting Set Formulation", "text": "In this section, we propose a formulation of the minimum-cost intervention problem in terms of the minimum-weight hitting set (MWHS) problem. This formulation will allow us to find algorithms to solve or approximate our problem in later sections.\nIt is known that special structures, called hedges that are formed for Q [S] in G prevent the identifiability of the causal effect Q[S] (Shpitser and Pearl, 2006). On the other hand, intervening on a vertex of a hedge allows us to eliminate it from the graph. Hence, the problem of identifying Q[S] is equivalent to finding a subset of vertices that hits all the hedges formed for Q [S]. In other words, the minimum-cost intervention problem can be reformulated as a MWHS problem. For simplicity, here, we use a slightly modified definition of a hedge. In Appendix A, we show that it is equivalent to the original definition in (Shpitser and Pearl, 2006). Definition 14. (Hedge) Let G be a semi-Markovian graph and S be a subset of its vertices such that G\n[S] is a c-component. A subset F is a hedge formed for Q[S] in G if S \u228a F , F is the set of ancestors of S in G [F ] , and G [F ] is a c-component.", "publication_ref": ["b44", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "As an example, suppose", "text": "S = {s 1 , s 2 } in the causal graph of Figure 1. In this case, G [S] is a c-component and {s 1 , s 2 , v 1 , v 2 } and {s 1 , s 2 , v 2 } are two hedges formed for Q[S].\nUsing the result of (Shpitser and Pearl, 2006), the following Lemma connects the minimum-cost intervention problem to the minimum-weight hitting set problem.\nLemma 15. Let G be a semi-Markovian graph with vertex set V , along with a cost function\nC : V \u2192 R \u22650 . Let S be a subset of V such that G [S] is a c-component. Suppose the set of all hedges formed for Q[S] in G is {F 1 , ..., F m }. Then A *\nS is a solution to Equation (2) if and only if it is a solution to the MWHS problem for the sets {F 1 \\ S, ..., F m \\ S}, with the weight function \u03c9(\u2022) := C(\u2022).\nLemma 15 suggests that designing an intervention to identify Q[S] can be cast as finding a set that intersects (hits) with all the hedges formed for Q [S]. A brute-force algorithm to find the minimum-cost intervention (Equation (2)) is then to first enumerate all hedges formed for Q[S] in G and solve the corresponding minimum hitting set problem.\nSolving MWHS, which itself is equivalent to the set cover problem, is known to be NP-hard Bernhard and Vygen, 2008). However, there exist greedy algorithms that can approximate the optimal solution up to a logarithmic factor Chvatal, 1979), which has been shown to be optimum in the sense that they achieve the best approximation ratio (Feige, 1998). Another approach for tackling MWHS is via linear programming relaxation which achieves the similar approximation ratio as the greedy approach (Lov\u00e1sz, 1975). But even in the case that we use an approximation algorithm for the minimum hitting set formulation of the minimum-cost intervention problem, the task of enumerating all hedges formed for Q[S] in G requires exponential number of computations in terms of number of the variables.", "publication_ref": ["b44", "b8", "b9", "b14", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "Properties of A *", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "S", "text": "In Section 3.1, we proved that the minimum-cost intervention design problem in (2) is NP-complete. Herein, we shall study certain properties of the solution A * S that allow us to reduce the complexity of solving (2). We begin with characterizing a set of variables that we must intervene upon to identify Q [S].\nRecall that pa \u2194 (S) is the set of parents of S that have a bidirected edge to a variable in S. Note that for a given set S, we can construct pa \u2194 (S) in linear time. The following Lemma indicates that Q[S] is not identifiable unless all of the variables in pa \u2194 (S) are intervened upon.\nLemma 16. Let G be a semi-Markovian graph with the vertex set V , and for S \u2286 V , let G [S] be a c-component. For any subset A \u2286 V , if A \u2208 ID 1 (S), then pa \u2194 (S) \u2286 A.\nAs a counterpart to Lemma 16, below, we characterize a subset of vertices that do not belong to A * S . Initialize F \u2190 set of vertices of G 3:\nwhile True do 4:\nF 1 \u2190 connected component of S via bidirected edges in G [F ] 5: F 2 \u2190 ancestors of S in G [F1] 6:\nif F 2 \u0338 = F then 7:\nF \u2190 F 2 8: else 9: return F If G [S]\nis not a c-component, it can be uniquely partitioned into maximal c-components (Tian and Pearl, 2002). Let S 1 , ..., S k be the partition of S such that G\n[S1] , ..., G [S k ] are the maximal c-components of G [S] . We define Hhull(S, G) as Hhull(S, G) = k i=1 Hhull(S i , G). Lemma 18. Consider A * S in Equation (2), then A * S \u2286 Hhull(S, G) \\ S.\nFor a given subset S and a semi-Markovian graph G, Lemmas 16 and 18 bound the solution to the minimum-cost intervention problem as pa \u2194 (S) \u2286 A * S \u2286 Hhull(S, G). In Algorithm 1, we propose a method to construct the hedge hull of a given subset S. Lines 4 and 5 of this algorithm can be performed via depth first search (DFS) algorithm, which is quadratic in the number of vertices in the worst-case scenario 4 . On the other hand, the while loop of line 3 can run at most |V | times in the worst case (as long as F 2 \u0338 = F , at least one vertex will be eliminated from F .) Hence, the complexity of this algorithm is 5 O(|V | 3 ). \nH := Hhull(S, G [V \\pa \u2194 (S)] ).(3)\nThis result suggests that solving (2) can be done by first identifying pa \u2194 (S), and then solving a reduced size minimum-cost intervention problem to identify\nQ[S] in G [H] , where H is given in Equation (3). Note that all the minimal hedges of S in G [H] can be enumerated in O(2 (|H|\u2212|S|) ).\nTherefore, if |H| is small, the hedge enumeration task of the brute-force approach in Section 3.2 can be done efficiently. However, the performance of this method deteriorates as the size of H increases. Next, we propose an algorithm that circumvents the hedge enumeration task to solve the minimum-cost intervention problem more efficiently.", "publication_ref": ["b46"], "figure_ref": [], "table_ref": []}, {"heading": "Exact Algorithmic Solution to Minimum-cost Intervention Problem", "text": "In this section, we propose an algorithm that can be used both to exactly solve the minimum-cost intervention problem and to approximate it within a logarithmic factor. 4. Lines 4 and 5 can also be swapped, as the order in which we execute them does not affect the output. 5. To be more precise, Algorithm 2 Exact algorithm for minimum-cost intervention(S, G), where G [S] is a c-component. if A \u222a pa \u2194 (S) \u2208 ID 1 (S) then 16:\nreturn (A \u222a pa \u2194 (S))\n17:\nH \u2190 Hhull(S, G [V \\(A\u222apa \u2194 (S))] )\nAs we discussed earlier, the minimum-cost intervention problem can be formulated as a combination of two tasks: enumerating the hedge structures and solving a minimum hitting set for the hedges. Although the minimum hitting set problem can be solved with polynomial-time approximation algorithms, enumerating all hedges requires exponential computational complexity. To reduce this complexity, we propose Algorithm 2, that avoids enumerating all hedges formed for Q[S] by utilizing the notion of minimality defined below, and Theorem 20. We will next explain this.\nDefinition 21 (Minimal hedge). A hedge F formed for Q[S] in G is said to be minimal if no subset of F (clearly excluding F ) is a hedge formed for Q[S] in G.\nAs an example, in Figure 1, Q[{s 1 , s 2 }] has two hedges: {s 1 , s 2 , v 1 , v 2 } and {s 1 , s 2 , v 2 }. In this case, {s 1 , s 2 , v 2 } is a minimal hedge. Clearly, every non-minimal hedge formed for Q[S] has a subset which is a minimal hedge. Therefore, hitting all the minimal hedges would suffice to identify Q[S]. As a result, for hedge enumeration, whenever we find a subset F that is a hedge formed for Q[S], it is not necessary to consider any super-set of F .\nAlgorithm 2 summarizes our proposed exact algorithm to solve the minimum-cost intervention problem. It begins with identifying pa \u2194 (S) and the subset H given in (3). The main idea of this algorithm is to iterate between discovering a new hedge formed for Q[S] in G, and solving the MWHS problem for the set of already discovered hedges, denoted by F. The set F grows each time a new hedge is discovered, up to a point where the minimum hitting set solution for F is exactly the solution to the original minimum-cost intervention problem. This is where the algorithm returns the result by solving the MWHS for F.\nThe inner loop of Algorithm 2 (lines 7-13) corresponds to the hedge discovery phase. Within this loop, the algorithm selects a vertex a in H \\S with the minimum cost, and removes a from H (resolves the hedge H). If this hedge elimination makes Q[S] identifiable (i.e., Hhull(S, G [H\\{a}] ) = S), it updates F in line 10. Otherwise, it updates H by Hhull(S, G H\\{a} ) in line 13 using Algorithm 1. The reason for updating F only when Q[S] becomes identifiable is that the hedge discovered in the last step of the inner loop H is a subset of all the hedges discovered earlier within the loop. Therefore, hitting (eliminating) H, also hits all its super-sets.\nAt the end of the inner loop, it solves a minimum hitting set problem for the constructed F to find A in line 14. If A \u222a pa \u2194 (S) \u2208 ID 1 (S), the algorithm terminates and outputs A \u222a pa \u2194 (S) as the optimal intervention set. Otherwise, it updates H using Algorithm 1 in line 19 and repeats the outer loop by going back to line 6 to discover new hedges formed for Q[S].\nIn the worst-case scenario, Algorithm 2 requires exponential number of iterations to form F. However, as illustrated in our empirical evaluations in Section 6, the algorithm often finds the solution to the minimum-cost intervention after only a few number of iterations. This is to say, in practice, discovering only a few hedges and solving the minimum hitting set problem for them suffices to solve the original minimum-cost intervention problem.\nLemma 22. Let G be a semi-Markovian graph and S \u2286 V . Algorithm 2 returns an optimal solution to (2).\nRemark 23. It is noteworthy that this result holds even if S is not a c-component. In other words, Algorithm 2 always returns an optimal solution in ID 1 (S). We will use this result in Section 4 to introduce an algorithm for the general setting in which S is an arbitrary subset of variables.\nApproximation version. Note that the minimum hitting set problem in line 15 can be solved approximately using a greedy algorithm Chvatal, 1979), which guarantees a logarithmicfactor approximation 6 . In this case, if polynomially many hedges are discovered before the algorithm stops 7 , Algorithm 2 returns a logarithmic-factor approximation of the solution in polynomial time.", "publication_ref": ["b9"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Anytime version.", "text": "As even the approximation version of Algorithm 2 has exponential time complexity in the worst case, we also propose an anytime version of this algorithm as follows. Suppose a runtime threshold \u03c4 is specified by the user. We run Algorithm 2, and as soon as the time threshold \u03c4 is hit, we return the following set as a solution:\nA \u222a pa \u2194 (S) \u222a Hhull(S, G [V \\(A\u222apa \u2194 (S))] ) \\ S,(4)\nwhere A is the latest set computed in line 14 of the algorithm (the minimum hitting set from the very last iteration of the algorithm). The following result shows that this set is indeed sufficient to identify Q[S], and provides an upper bound on how far this solution can be from the optimal one.\nProposition 24. Let A * be an optimal solution to the minimum-cost intervention problem of Equation (2). Let A be the minimum hitting set computed in line 14 of Algorithm 2 in an arbitrary iteration. Also, define H = Hhull(S, G [V \\(A\u222apa \u2194 (S))] ) \\ S. Then, (A \u222a pa \u2194 (S) \u222a H) \u2208 ID 1 (S), and\nC(A * ) \u2264 C(A \u222a pa \u2194 (S) \u222a H) \u2264 C(A * ) + C(H).\nSince the set H and therefore the regret bound C(H) can be computed efficiently, one can run the algorithm until a desired upper bound is achieved.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Heuristic Algorithms", "text": "The algorithm discussed in the previous Section provides an exact solution for finding the minimumcost intervention. However, it has an exponential runtime in the worst case. Herein, we develop and present two heuristic algorithms to approximate the solution to the minimum-cost intervention problem in polynomial time. We discuss their average-case performance on random graphs. Also, in Section 6, we evaluate the performance of these algorithms in terms of their runtimes and the optimality of their solutions. Further analysis of these heuristic algorithms are provided in Appendix E. It is noteworthy that these two algorithms utilize the result of Theorem 20, i.e., they initiate with identifying pa \u2194 (S), H in (3), and then find a minimum-cost intervention set that identifies\nQ[S] in G [H] .\nTheses two algorithms approximate the minimum-cost intervention problem via a minimum-weight vertex cut (a.k.a. vertex separator) problem.\n6. See Appendix F for further details. 7. We propose a slightly modified version of Algorithm 2 in Appendix F with lower number of calls to the hitting set solver.\nDefinition 25. (Minimum-weight vertex cut) Let H be a (un)directed graph over the vertices V , with a weight function \u03c9 : V \u2192 R \u22650 . For two non-adjacent vertices x, y \u2208 V , a subset A \u2282 V \\ {x, y} is said to be a vertex cut for x \u2212 y, if there is no (un)directed path that connects x to y in H V \\A . The objective of minimum-weight vertex cut problem is to identify a vertex cut for x \u2212 y that minimizes a\u2208A \u03c9(a). Minimum-weight vertex cut problem can be solved in polynomial time by, for instance, casting it as a max-flow problem 8 and then using algorithms such as Ford-Fulkerson, Edmonds-Karp, or push-relabel algorithm (Ford and Fulkerson, 1956;Edmonds and Karp, 1972;Goldberg and Tarjan, 1988).\nHeuristic Algorithm 1: For a given graph G and a subset S \u2286 V , this algorithm builds an undirected graph H with the vertex set H \u222a {x, y}, where H is given in (3), and x and y are two auxiliary vertices. For any pair of vertices {v 1 , v 2 } \u2208 H, if v 1 and v 2 are connected with a bidirected edge in G, they will be connected in H. Vertex x is connected to all the vertices in pa(S) \u2229 H, and y is connected to all vertices in S. The output of the algorithm is the minimum-weight vertex cut for x \u2212 y, with the weight function \u03c9(\u2022) := C(\u2022). Algorithm 8 in Appendix E presents the pseudo code for this procedure. Next result shows that intervening on the output set of this algorithm will identify Q[S], although this set is not necessarily minimum-cost.\nLemma 26. Let G be a semi-Markovian graph on V and S be a subset of V such that G [S] is a c-component. Heuristic Algorithm 1 returns an intervention set A in O(|V | 3 ) such that A \u2208 ID 1 (S).\nHeuristic Algorithm 2: Given a graph G and a subset S, this algorithm builds a directed graph J as follows: the vertex set is H \u222a {x, y}, where H is given in (3), and x and y are two auxiliary vertices. For any pair of vertices A major difference between the two heuristic algorithms is that Algorithm 2 solves a minimum vertex cut on a directed graph, whereas Algorithm 1 solves the same problem on an undirected graph. Since the equivalent max-flow problem is easier to solve on directed graphs, Algorithm 2 is preferred, unless the directed edges of G are considerably denser than its bidirected edges. As we shall see in experimental evaluations of Section 6, both of these heuristic algorithms perform outstandingly well on randomly generated graphs according to Erdos-Renyi generative model (Erd\u0151s and R\u00e9nyi, 1960), i.e., when every edge is sampled independently. The following result indicates that our simulation results are theoretically justified.\n{v 1 , v 2 } \u2208 H, if v 1 is a parent of v 2 in G, then v 1 will be a parent of v 2 in J .\nProposition 28. Let G be a random semi-Markovian graph, where each directed edge exists with probability p and each bidirected edge exists with probability q, mutually independently (generalized Erdos-Renyi generative model). Suppose S = {s}, where s is an arbitrary vertex. Let c * be the random variable of the cost of the optimal solution to Equation (2). Also let c 1 and c 2 be the random variables of the cost of the solution returned by Heuristic Algorithm 1 and Heuristic Algorithm 2, respectively. Then under this generative model, and with equal cost for vertices, (2) is not a solution to (1). In this example, the cost of intervening on each of {s 1 , s 2 , s 3 } is 1, whereas the cost of intervening on each of {v 1 , v 2 , v 3 , v 4 } is 5.\nE[c 1 ] \u2264 q \u22121 E[c * ], E[c 2 ] \u2264 p \u22121 E[c * ]. 8. See Appendix E for details. s 1 s 2 s 3 v 2 v 1 v 3 v 4\nCorollary 29. Consider an algorithm that runs both of these heuristic algorithms and picks the best solution out of the two. Let the cost of this solution be denoted by c. This algorithm runs in time O(|V | 3 ) in the worst case, and the cost of its solution satisfies the following inequality in the Erdos-Renyi model (see Appendix C for the proof.)\nE[c * ] \u2264 E[c] \u2264 min{p \u22121 , q \u22121 }E[c * ].\nWe propose another algorithm which uses a greedy approach to solve the minimum-cost intervention problem and discuss its complexity in Appendix E. This greedy algorithm is preferable to the two aforementioned algorithms in certain special settings. Additionally, we propose a polynomial-time post-process in Appendix E to improve the solution returned by our three heuristic algorithms.", "publication_ref": ["b15", "b12", "b21", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "General Subset Identification", "text": "So far we have discussed the minimum-cost intervention design problem for subset S, where the induced subgrah G [S] is a c-component. In this section, we study the general case in which S is an arbitrary subset of variables and show that the minimum-cost intervention design problem for S requires solving a set of instances of the problem for subsets of S such as S i where G [Si] is a single c-component.\nThe main challenge in the general case is that Theorem 5 is no longer valid. Thus, the minimumcost intervention design problem in (1) cannot be reduced to (2). As an example, consider Figure 2. In this causal graph, the minimum-cost intervention to identify Q\n[S] for S := {s 1 , s 2 , s 3 }, is A * S,V \\S = {{s 1 }, {s 2 }} with the cost C(s 1 ) + C(s 2 ) = 2.\nHowever, any singleton intervention that can identify Q[S], i.e., A \u2208 ID 1 (S) has a cost of at least 10. More importantly, the union of the sets in A * S,V \\S , i.e., {s 1 , s 2 } does not belong to ID 1 (S). In other words, intervening on {s 1 , s 2 } does not identify Q[S] (Lemma 6).\nIn many applications, it is reasonable to assume that in order to identify Q[S] = P (S|do(V \\ S)), intervening on elements of S, i.e., the outcome variables, is not desirable. In other words, C(s) = \u221e, for all s \u2208 S. Under this assumption, we show that instances similar to Figure 2 cannot occur and results analogous to Theorem 5 can be established.\nTheorem 30. Suppose S is a subset of variables such that C(s) = \u221e for any s \u2208 S. Let A = {A 1 , A 2 , ..., A m } be a collection of subsets such that A \u2208 ID G (S, V \\ S) and m > 1. Then there exists a singleton intervention\u00c3 such that\u00c3 = {\u00c3} \u2208 ID G (S, V \\ S) and C(\u00c3) \u2264 C(A).\nTheorem 30 implies that the general problem can be solved exactly analogous to the case where G [S] is a c-component, if intervention on S is not allowed.\nAlgorithm 3 Naive general algorithm for minimum-cost intervention, when G [S] is not necessarily a c-component.\n1: function NaiveMinCost(S, G, C(\u2022)) 2: A * \u2190 null 3: minCost \u2190 \u221e 4: {S 1 , ..., S k } \u2190 maximal c-components of G [S] 5:\nfor any partition of {S 1 , ..., S k } as S (1) , ..., S (t) do 6:\nA \u2190 {} 7: cost \u2190 0 8: for i from 1 to t do 9: A i \u2190 min-cost intervention set in ID 1 (S (i) ) 10: A \u2190 A \u222a {A i } 11: cost \u2190 cost +C(A i ) 12:\nif cost < minCost then 13:\nA * \u2190 A 14: minCost \u2190 cost 15: return A *\nWe now turn to proposing an exact solution to the minimum-cost intervention problem in (1) in the general case. Let S 1 , ..., S k be subsets of S such that i Tian and Pearl, 2002). This observation is formalized below.\nS i = S and G [S1] , ..., G [S k ] are the maximal c-components of G [S] . It is known that Q[S] is identifiable in G if and only if Q[S i ]s are identifiable in G for all 1 \u2264 i \u2264 k (\nObservation 1. Let G be a semi-Markovian graph and S be a subset of its vertices. Suppose\nA * S,V \\S is a min-cost interventions collection to identify Q[S] in G. If G [Sj ] is a maximal c-component of G [S]\n, then there exists A \u2208 A * S,V \\S such that A \u2208 ID 1 (S j ) (see Theorem 1 of Kivva et al., 2022.) Recall that S 1 , . . . , S k are the subsets of S such that G [Si] is a maximal c-component for each 1 \u2264 i \u2264 k. Let S (1) , ..., S (\u2113) denote a partitioning of {S 1 , ..., S k }. That is, for each j, S (j) is a subset of the set {S 1 , ..., S k }, S (j) \u2229S (i) =\u2205 for i\u0338 =j, and \u2113 j=1 S (j) = {S 1 , ..., S k }. Furthermore, we denote the set of all vertices in partition S (j) by S (j) . As an example, in Figure 2\n, set S = {s 1 , s 2 , s 3 } consists of two maximal c-components G [S1] and G [S2]\n, where S 1 ={s 1 , s 3 } and S 2 ={s 2 }. There are two different ways to partition {S 1 , S 2 }. One is S (1) = {S 1 , S 2 }. The other is S (1) = {S 1 } and S (2) = {S 2 }. For the first partition, we have S (1) = {s 1 , s 2 , s 3 }. Similarly, the second partition will result in S (1) ={s 1 , s 3 } and S (2) ={s 2 }. Suppose A * S,V \\S = {A 1 , . . . , A t }. Based on Observation 1, the set {S 1 , . . . , S k } can be partitioned into t groups, denoted by S (1) , . . . , S (t) , such that each partition S (j) is identified by intervention on\nA j for 1 \u2264 j \u2264 t. More precisely, if S i \u2208 S (j) , then A j \u2208 ID 1 (S i ). Lemma 31. Suppose A * S,V \\S = {A 1 , . . . , A t } is the minimum-cost intervention to identify Q[S]. Let S (1) , . . . , S (t) be the partitioning of maximal c-components of G [S]\n, such that c-components of each partition S (j) are identified by intervention on A j for 1 \u2264 j \u2264 t. Then,\nA j \u2208 arg min A\u2208ID1(S (j) ) C(A), \u22001 \u2264 j \u2264 t.\nLemma 31 illustrates the fact that if the partitioning S (1) , . . . , S (t) were known a priori, then the task of recovering the optimal collection of interventions in ID G (S, V \\ S) would reduce to t independent instances of finding optimal intervention sets in ID 1 (S (j) ) for each 1 \u2264 j \u2264 t. We already know from Remark 23 that these instances can be solved through Algorithm 2. However, the optimal partitioning is not known. Even the number of groups (t) is not known in advance. A naive approach would be to enumerate every possible partitioning of the maximal c-components {S 1 , . . . , S k }, and form the optimal intervention collection corresponding to each partitioning through successive runs of Algorithm 2. This approach is summarized as Algorithm 3. The following result proves the soundness of this algorithm.\nProposition 32. Given a semi-Markovian graph G and a subset S of its vertices, Algorithm 3 with Algorithm 2 used as a subroutine in line ( 9) returns an optimal solution to the min-cost interventions collection to identify Q[S] in G.\nThe number of ways to partition the maximal c-components grows as O(m log log m ), where m = 2 k is the number of subsets of {S 1 , . . . , S k } 9 . Further, Algorithm 2 has to be run as many times as the number of groups in each partition to find the optimal collection of interventions corresponding to that partitioning; that is, if {S 1 , . . . , S k } is partitioned into \u2113 groups, Algorithm 2 is run \u2113 times for one such partitioning. Although the number of c-components and therefore number of different partitionings is small in general, it is crucial to minimize the number of runs of Algorithm 2 as its runtime might be limiting. In what follows, we introduce two algorithms, including an exact, and an approximation algorithm which require running Algorithm 2 only m \u2212 1 times.", "publication_ref": ["b46", "b30"], "figure_ref": ["fig_4", "fig_4", "fig_4"], "table_ref": []}, {"heading": "Minimum Set Cover Formulation", "text": "Let \u0393 = {S 1 , ..., S k } be the subsets of S such that G [Sj ] for 1 \u2264 j \u2264 k is a maximal c-component in G [S] . Also let \u0393 1 , . . . , \u0393 2 k \u22121 = \u0393 m\u22121\ndenote the non-empty subsets of \u0393 in an arbitrary order. Define\nA * i = arg min A\u2208ID1(\u222a S j \u2208\u0393 i Sj ) C(A), \u22001 \u2264 i \u2264 (m \u2212 1),(5)\nthat is, A * i is the optimal single intervention set to identify Q[S j ] for every S j \u2208 \u0393 i . Having access to A)i * s, the minimum-cost intervention problem can be cast as a weighted minimum set cover (WMSC) problem.\nDefinition 33 (WMSC). Let \u0393 = {S 1 , ..., S k } be a set of objects along with a weight function \u03c9 : 2 \u0393 \u2192 R \u22650 , which assigns a weight to each subset of \u0393. A set cover for \u0393 is a subset B \u2286 2 \u0393 (a subset of the power set of \u0393) such that B covers the set \u0393, i.e., \u222a A\u2208B A = \u0393. The weighted minimum set cover problem's objective is to find a set B * among all set covers that minimizes A\u2208B \u03c9(A).\nThe following Lemma indicates that after computing A * i s as defined in Equation ( 5), the rest of the problem is exactly an instance of WMSC.\nLemma 34. Let A * i be defined as in Equation (5). Let B denote the solution to the WMSC problem with \u03c9(\u0393 i ) = A * i for every non-empty subset of \u0393 and \u03c9(\u2205) = 1. Then A = {A * i |\u0393 i \u2208 B} is an optimal solution to Equation (1).\nAlthough Lemma 34 suggests an exact algorithm to solve the minimum-cost intervention problem in the general case, the WMSC problem itself is NP-hard 10 (Karp, 1972), and requires time exponential in m = 2 k \u2212 1 in the worst case. In what follows, we suggest an approximation algorithm which requires time polynomial in m, and guarantees a k-factor approximation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Approximation Algorithm Based on Minimum-cost Flow", "text": "In this section, we describe an algorithm to approximate the optimal collection of interventions to identify Q[S] for an arbitrary set S, where G [S] might comprise multiple c-components. This 9. To be more precise, the number of partitions (generally referred to as Bell number) has been shown to have a growth rate of O(m log( 0.792 log m log log m ) ) (Berend and Tassa, 2010). 10. The greedy approximation \nS 1 S 2 S 3 \u0393 1 = {S 1 } \u0393 3 = {S 2 } \u0393 7 = {S 3 } \u0393 2 = {S 1 , S 2 } \u0393 5 = {S 1 , S 3 } \u0393 6 = {S 2 , S 3 } \u0393 4 = {S 1 , S 2 , S 3 } w z 1 \\ C ( A * 1 ) 1\\ C (A * 3 ) 1 \\ C ( A * 7 ) 2 \\ C (A * 2 ) 2 2\\ C (A * 5 ) 2 2 \\ C (A * 6 ) 2 3\\ C(A * 4 ) 3\nmin f (\u2022,\u2022) (x,y)\u2208E f (x, y)\u03b6(x, y), s.t. 0 \u2264 f (x, y) \u2264 \u03b3(x, y), \u2200(x, y) \u2208 E, x\u2208V f (x, v) \u2212 y\u2208V f (v, y) = d(v), \u2200v \u2208 V, where d(z) = \u2212d(w) = d * , and d(v) = 0 for v \u0338 = w, z.\nWith this definition, we now describe an instance of the minimum-cost flow problem, which is in the core of our approximation algorithm. We draw a graph where each S j is represented by a vertex. Moreover, each \u0393 i is also represented by a vertex. We also add two auxiliary vertices w and z. For each subset \u0393 i , we draw a directed edge with capacity \u03b3 = 1 and cost \u03b6 = 0 from \u0393 i to S j if S j \u2208 \u0393 i . Analogously, we draw a directed edge with capacity 1 and cost 0 from each S j to z. Finally, for each \u0393 i , we draw a directed edge from w to \u0393 i with capacity |\u0393 i | and cost\nC(A * i )\n|\u0393i| . An example is shown in Figure 3, where k = 3. The pair of numbers on each edge represent the capacity and the cost corresponding to that edge, respectively. We then solve MCFP on this network, with flow d * = k from w to z.\nAlgorithm 4 MCFP-based approximation algorithm for minimum-cost intervention, when G [S] is not necessarily a c-component.", "publication_ref": ["b7"], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "1: function", "text": "2: \u0393 = {S 1 , ..., S k } \u2190 maximal c-components of G [S] 3: \u0393 1 , . . . , \u0393 2 k \u22121 \u2190 non-empty subsets of \u0393 4:\nH \u2190 a directed graph with one vertex for each S j , one vertex for each \u0393 i , and two auxiliary vertices w, z 5:\nfor j from 1 to k do add edge (\u0393 i , S j ) for each j if S j \u2208 \u0393 i , with capacity \u03b3 = 1 and cost \u03b6 = 0 11: f * \u2190 integral solution for MCFP on H, sending an amount of flow k from w to z\n12:B \u2190 {\u0393 i |f * (w, \u0393 i ) > 0} 13: B * \u2190 {\u0393 i \u2032 = {S j |1 \u2264 j \u2264 k, f * (\u0393 i , S j ) > 0}|\u0393 i \u2208B} 14: A \u2190 {A * i \u2032 |\u0393 i \u2032 \u2208 B * } 15: return A Remark 36.\nBy definition, all of the capacities are integer values, and the demanded flow d * is also an integer value. Moreover, this problem has a trivial feasible solution where an amount of flow d * = k is sent from w to the vertex corresponding to the full set {S 1 , . . . , S k }, and then it is split to each S j with value of 1, and then sent from each S j to z. As a result, it follows from the integrality theorem that this problem has an integral solution which can be computed in polynomial time (Ahuja et al., 1988;Kov\u00e1cs, 2015).\nWe first find the integral solution (f * (\u2022, \u2022)) to the MCFP described above. Let the set of \u0393 i s that receive non-zero in-flow in this solution beB = {\u0393 1 , . . . , \u0393 t } without loss of generality. We then construct B * as follows.\nB * = {\u0393 i \u2032 = {S j |1 \u2264 j \u2264 k, f * (\u0393 i , S j ) > 0}|1 \u2264 i \u2264 t}.\nIn words, we replace each subset \u0393 i with a subset \u0393 i \u2032 \u2286 \u0393 i such that only those S j s are kept in \u0393 i \u2032 that f * (\u0393 i , S j ) > 0. Finally, the solution A to identify Q[S] is the set of\nA * i \u2032 s that their corresponding \u0393 i \u2032 appears in B * . That is, A = {A * i \u2032 |\u0393 i \u2032 \u2208 B * }.\nThis approach is summarized as Algorithm 4. We refer to the proof of Proposition 37 in Appendix C for detailed discussion on correctness and approximation ratio of this algorithm.", "publication_ref": ["b3", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "Proposition 37. Algorithm 4 returns a solution", "text": "A \u2208 ID G (S, V \\ S), such that C(A) \u2264 kC(A * S,V \\S ), where k and A * S,V \\S are the number of c-components in G [S]\n, and an optimal solution to Equation (1), respectively.\nIt is noteworthy that Algorithm 4 requires calling Algorithm 2 exactly (m \u2212 1) times, and then solving MCFP which has an overhead of O(m 2.5 ) operations, as it can be solved as a linear program (Vaidya, 1989). Since the number of maximal c-components of G [S] is in general small compared to the total number of variables in the system, the bottleneck of the computational complexity would be determining A * i s.", "publication_ref": ["b48"], "figure_ref": [], "table_ref": []}, {"heading": "Non-linear Cost Models", "text": "The results derived so far rely on the core assumption that the intervention costs are linear; that is, C(A) = A\u2208A C(A) = A\u2208A a\u2208A C(a), for every collection of subsets of variables A. In general, however, the intervention costs can be arbitrarily defined for each subset of variables. As an example, take for instance a study where the examiner can combine two costly experiments using a similar experimental setup to reduce the experiment cost up to fifty percent; or on the contrary, if these two experiments affect each other, they can prevent the examiner from performing them simultaneously, which can be modeled as an infinite cost for the simultaneous experiment. In the most general form, instead of defining the cost function C on the set of variables (as we did in Section 2), it must be defined as C : 2 V \u2192 R, i.e., for each subset of variables separately 11 . In an even more general setup, one could relax the additivity of costs for collections of subsets: C(A) \u0338 = A\u2208A C(A).\nIn this section, we discuss the minimum-cost intervention problem under nonlinear (non-additive) costs, which is a relaxation of our assumptions. This is in contrast to what we discuss in Appendix D, where we show how certain improvements can be achieved if further assumptions are made on the cost function. We first note that certain results of the previous sections can be extended under assumptions milder than linearity of costs. For instance, Lemma 3 and Remark 4 hold under sub-additivity, i.e., if C(\u2022) is such that C(A \u222a B) \u2264 C(A) + C(B) for any A, B \u2286 V . Applying the definition of sub-additivity to A i s in Remark 4 recursively yields the same conclusion.\nMore interestingly, Theorem 5 holds under the milder (and more plausible) assumption of non-decreasing costs over collections of interventions 12 : Assumption 2 (Non-decreasing costs). Performing a set of interventions does not decrease the cost of other interventions. More precisely, for any collection of subsets of V such as A and any subset\nA \u2032 \u2286 V , C(A \u222a {A \u2032 }) \u2265 C(A).\nProposition 38. Theorem 5 holds under Assumption 2, even if Assumption 1 is violated.\nAs a result, analogous to what we did in Section 3, the search for minimum-cost intervention can still be restricted from ID G (S, V \\ S) to ID 1 (S) under Assumption 2. It is also worth mentioning that the minimum hitting set formulation of Section 3.2 is valid regardless of the non-linearity of costs. As a result, Algorithm 2 is still applicable, with the only difference that the weights of minimum hitting set problem must also be defined for subsets rather than objects. Note that the greedy algorithm to solve the minimum hitting set does not guarantee logarithmic-factor approximation anymore. To sum up, although presenting the results and algorithms discussed in this paper was more straightforward under Assumption 1, most of these results are applicable in general settings with non-linear costs, under mild assumptions such as Assumption 2.\nThere are limited settings where Assumption 2 can be violated, such as when an intervention has negative cost. This can for instance happen when a recommender system receives sponsorship profits for recommending certain products. Under linearity of costs (Assumption 1), one would simply perform every possible intervention with negative cost, and reduce the remaining problem into a smaller instance of the same problem with positive costs. However, if costs are not linear, we might have to investigate every possible combination of interventions in a brute-force manner.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluations", "text": "Our evaluation consists of three parts. First, we evaluate our heuristic algorithms and the approximation version of Algorithm 2 against the exact algorithm, in terms of runtime and normalized regret, when solving the target query Q[S] is such that G [S] is a c-component. Throughout this section, the normalized regret of a solution A is defined as (C(A) \u2212 C * )/C * , where C * denotes the cost of the optimal minimum-cost solution. Second, we evaluate our exact approach, Algorithm 2 in terms of the number of hedges it requires to discover for recovering the optimal solution, against the number of total existing hedges formed for Q[S] in G. This indeed translates to the number of iterations of Algorithm 2. Finally, we compare the two algorithms proposed for minimum-cost intervention design when G [S] comprises multiple c-components, namely Algorithms 3 and 4, in terms of runtime.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Heuristic and Approximation Algorithms", "text": "For evaluation, we generated causal graphs using the Erdos-Renyi generative model (Erd\u0151s and R\u00e9nyi, 1960) as follows. For a given number of vertices |V | = n, we fixed a causal order over the vertices.\nThen, directed edges were sampled with probability p = 0.35 and bidirected edges were sampled with probability q = 0.25 between the vertices, mutually independently. The set S was selected randomly among the last 5% of the vertices in the causal order such that G [S] is a c-component. Intervention costs of vertices were chosen independently at random from {1, 2, 3, 4}. For various n, we sampled different causal graphs and different subsets S using the above procedure and ran our algorithms 13 on them to find the minimum-cost intervention set for identifying Q[S]. Our performance measures are runtime and normalized regret. The results are depicted in Figure 4. Each curve and its confidence interval is obtained by averaging over 40 trials. As illustrated in Figure 4, our proposed heuristic algorithms achieve negligible regret in most of the cases while their runtime are considerably faster than the exact algorithm, i.e., Algorithm 2. It is noteworthy that the regret is not necessarily a monotone function of n, and it depends on the structure of the causal graph and intervention costs. For further evaluations of our algorithms (e.g., their sensitivities with respect to p and q), see Appendix G.", "publication_ref": ["b13"], "figure_ref": ["fig_7", "fig_7"], "table_ref": []}, {"heading": "Exact Algorithm", "text": "Recall that Algorithm 2 discovers hedges formed for Q[S] iteratively, one hedge per iteration. This process continues until enough hedges are discovered. That is, instead of enumerating all of the hedges, Algorithm 2 enumerates only a portion of them, which suffices to find the optimal solution. Figure 5 demonstrates the number of hedges formed for Q[S] in random graphs of different sizes generated with parameters p = 0.35 and q = 0.25 (same setting as above), as opposed to the number of hedges that Algorithm 2 discovered before finding the optimal minimum-cost intervention solution in logarithmic scale. The number of hedges formed for Q[S] was counted after removing pa \u2194 (S). ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "General Algorithms", "text": "We discussed two exact algorithms for solving the minimum-cost intervention design problem, namely Algorithm 3 and the WMSC-based algorithm suggested by Lemma 34. We also proposed an approximation version of the latter as Algorithm 4. The time-consuming complex computations in these algorithms boil down to the use of Algorithm 2 as a sub-routine, which is an exponential-time algorithm called exponentially many times. Herein, we compare their number of calls to Algorithm 2 as a measure of time complexity (line 9 of Algorithm 3 and line 8 of Algorithm 4, where the latter shares the same number of calls with the WMSC-based algorithm). Figure 6 illustrates the number of calls to Algorithm 2 versus the number of maximal c-components in G [S] in logarithmic scale. As can be seen in this figure, the number of calls to Algorithm 2 becomes considerably different as the number of c-components grows.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Concluding Remarks", "text": "We discussed the problem of designing the minimum-cost intervention for causal effect identification in this paper. We established the NP-completeness of this problem by relating it to two well-known NP-complete problems: minimum vertex cover and minimum hitting set. When the target query consisted of a single c-component, we proposed a formulation of the minimum-cost intervention problem based on minimum hitting set. We utilized this formulation to devise an algorithm to solve the minimum-cost intervention problem exactly. Highlighting the computational complexity of the problem, we also proposed several heuristic algorithms to design an intervention in polynomial time.\nFor a general query, we reduced the problem to several instances of the single-c-component case. We proposed an algorithm based on minimum set cover to solve the minimum-cost intervention problem, and a similar algorithm based on minimum-cost flow to approximate it within a factor of the number of c-components.\nThis appendix is organized as follows.\n\u2022 In Section A, we discuss our definition of a hedge formed for a causal query, and show it is equivalent to the original definition used in the literature.\n\u2022 In Section B, we recite the three rules of Pearl's do calculus for the sake of completeness.\n\u2022 In Section C, we provide the proofs for all of our results stated in the main text and the appendix. It is noteworthy that the results stated later in the appendix are also proved in this section.\n\u2022 In Section D, we discuss various cases where the minimum-cost intervention problem can be solved more efficiently (i.e., we provide polynomial time algorithms) under assumptions on the structure of the causal graph, or the cost function.\n\u2022 In Section E, we provide further details of our proposed heuristic algorithms.\n\u2022 Section F includes further details on the approximation algorithm used to solve the hitting set algorithm, as well as a slight modification to Algorithm 2 to reduce the number of calls to solve the hitting set problem.\n\u2022 Section G provides further details of the experimental setup of this paper, along with further evaluations of the proposed algorithms in this work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Definition of Hedge", "text": "We used a definition of hedge (Definition 14) throughout the paper which was slightly different from the original definition in (Shpitser and Pearl, 2006). We provide a formal proof that these two definitions are equivalent. Following Pearl's notation, for two sets of variables X and Y , the graph G XY is defined as the edge subgraph of G, where the edges going into X and the edges going out of Y are deleted. The definitions of root set, c-component, c-forest and hedge are all adopted from (Shpitser and Pearl, 2006).\nDefinition 39 (C-component). Let G be a semi-Markovian graph. G is a c-component (confoundedcomponent) if a subset of its bidirected edges form a spanning tree over all vertices of G.\nRemark 40. If G is not a c-component, it can be uniquely partitioned into maximal c-components (Tian and Pearl, 2002).\nDefinition 41 (Root set). We say R is a root set in G if for every r \u2208 R, the set of descendants of r in G is empty.\nDefinition 42 (C-forest). Let G be a semi-Markovian graph with the maximal root set R. G is a R-rooted c-forest if G is a c-component and all the observed variables have at most one child.\nDefinition 43 (Hedge). Let X, Y be two set of vertices in the semi-Markovian graph G. Also let F, F \u2032 be two R-rooted c-forests such that F \u2032 \u2282 F , F \u2229 X \u0338 = \u2205, F \u2032 \u2229 X = \u2205 and R is a subset of ancestors of Y in G X . Then F, F \u2032 form a hedge for P (Y |do(X)) in G.\nTheorem 44 (Shpitser and Pearl, 2006). If there exists a hedge formed for P (Y |do(X)) in G, then P (Y |do(X)) is not identifiable in G.\nRemark 45. As mentioned in Theorem 6 of (Shpitser and Pearl, 2006), if an edge subgraph of G contains a hedge formed for P (Y |do(X)), then P (Y |do(X)) is not identifiable in G. In other words, if P (Y |do(X)) is not identifiable in G, it is not identifiable in any edge super-graph of G either.\nFor the purposes of this paper where we are predominantly considering interventional distributions of the form Q[S] = P (S|do(V \\S)), we adapt the definition of hedge and the hedge criterion (Theorem 44) as follows. Let S be a subset of the vertices of G such that G [S] is a c-component. First note that if F, F \u2032 form a hedge for P (Y |do(X)), then (F \u222a Y ), (F \u2032 \u222a Y ) clearly form a hedge for P (Y |do(X)) by definition. Further, if the two R-rooted c-forests F, F \u2032 form a hedge for Q[S], the set R must be S itself, as S has no other ancestors in G V \\S that can be a member of the root set. Consequently, We also make use of the following theorem along our proofs.\nF \u2032 = R = S,\nTheorem 46 (Tian and Pearl, 2002). Let G be a semi-Markovian graph, and let H be a subset of the observable vertices. Let H 1 , ..., H k denote the maximal c-components of G\n[H] . Then Q[H] is identifiable in G, if and only if Q[H 1 ], ..., Q[H k ] are identifiable in G.", "publication_ref": ["b44", "b44", "b46", "b44", "b44", "b46"], "figure_ref": [], "table_ref": []}, {"heading": "B. Pearl's do Calculus", "text": "For the sake of completeness, we recite the three rules of Pearl's do calculus (Pearl, 2012).\nRule 1 ) Insertion or deletion of observations. If (Y \u22a5 \u22a5 Z|X, W ) G X , then P (Y |do(X), Z, W ) = P (Y |do(X), W ).\nRule 2 ) Action and observation exchange.\nIf (Y \u22a5 \u22a5 Z|X, W ) G XZ , then P (Y |do(X, Z), W ) = P (Y |do(X), Z, W ). Rule 3 ) Insertion or deletion of actions. If (Y \u22a5 \u22a5 Z|X, W ) G XZ(W ) , where Z(W ) are vertices in Z that have no descendants in W in G X , then P (Y |do(X, Z), W ) = P (Y |do(X), W ).", "publication_ref": ["b41"], "figure_ref": [], "table_ref": []}, {"heading": "C. Proofs", "text": "The proofs of the results are presented in the order that they appear in the main text. The proofs of the statements that appear later in the appendix are also included in this section.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 Results Appearing in the Main Text", "text": "Lemma 3. Suppose S is a subset of variables such that G [S] is a single c-component. Let A = {A 1 , A 2 , ..., A m } be a collection of subsets of V such that A \u222a \u2229 S = \u2205, where \nA \u222a := \u222a m i=1 A i . If A \u2208 ID G (S, V \\ S), then the singleton collection A \u222a = {A \u222a } also belongs to ID G (S, V \\ S). Proof Suppose B i = V \\ A i for 1 \u2264 i \u2264 m,\nV such that Q M1 [B \u2229 ] = Q M2 [B \u2229 ], but Q M1 [S] \u0338 = Q M2 [S]\n, where Q Mj is the interventional distribution under model M j . Now we build two models M \u2032 1 and M \u2032 2 as follows. For any x \u2208 B \u2229 , x has the same equation in\nM \u2032 j as in M j . Both in M \u2032 1 and M \u2032 2 , any x / \u2208 B \u2229 , x is uniformly distributed in D(x)\n, where D(x) is the domain of the variable x. Since every x / \u2208 B \u2229 is drawn independently of every other variable, for 1 \u2264 i \u2264 m we can write:\nQ M \u2032 1 [B i ] = Q M \u2032 1 [B \u2229 ] \u2022 \u03a0 x\u2208Bi\\B\u2229 Q M \u2032 1 [x] = Q M1 [B \u2229 ] \u2022 \u03a0 x\u2208Bi\\B\u2229 1 |D(x)| = Q M2 [B \u2229 ] \u2022 \u03a0 x\u2208Bi\\B\u2229 1 |D(x)| = Q M \u2032 2 [B \u2229 ] \u2022 \u03a0 x\u2208Bi\\B\u2229 Q M \u2032 2 [x] = Q M \u2032 2 [B i ],(6)\nwhere the second equality follows from the fact that every variable in B \u2229 has the same model in M 1 and M \u2032 1 , the third equality follows from the assumption that\nQ M1 [B \u2229 ] = Q M2 [B \u2229 ]\n, and the fourth one is because every variable in B \u2229 has the same model in M 2 and M \u2032 2 . With the same line of reasoning as above,\nQ M \u2032 1 [S] = Q M1 [S] \u0338 = Q M2 [S] = Q M \u2032 2 [S].(7)\nEquations ( 6) and ( 7 \n, A = {A \u222a } \u2208 ID G (S, V \\ S). Theorem 5. Suppose S is a subset of variables such that G [S] is a c-component. Let A = {A 1 , A 2 , ..., A m } be a collection of subsets such that A \u2208 ID G (S, V \\ S) and m > 1. Then, there exists a subset\u00c3 \u2286 V such that\u00c3 = {\u00c3} \u2208 ID G (S, V \\ S) and C(\u00c3) \u2264 C(A).\nProof Suppose without loss of generality that\nA i \u2229 S = \u2205 for 1 \u2264 i \u2264 k, and A i \u2229 S \u0338 = \u2205 for k < i \u2264 m, for some integer k. We first claim that the following collection is in ID G (S, V \\ S). A = {A 1 , ..., A k } \u2208 ID G (S, V \\ S).\nSuppose this claim does not hold. Then from theorem 1 of (Kivva et al., 2022), Q[S] is not identifiable from any of Q[A i ]s for 1 \u2264 i \u2264 k. Since for i > k, we have S \u0338 \u2286 A i , applying Theorem 1 of (Kivva et al., 2022) again, A / \u2208 ID G (S, V \\ S), which is a contradiction. Now defining A \u222a = (\u222a k i=1 A i ), from Lemma 3, we know that It suffices to show that C(\u00c3) \u2264 C(A), which follows from an identical reasoning to Remark 4:\nA = {A \u222a } \u2208 ID G (S, V \\ S).\nC(\u00c3) = C(A \u222a ) = a\u2208A\u222a C(a) \u2264 k i=1 a\u2208Ai C(a) = k i=1 C(A i ) = C(\u00c2) \u2264 C(A).\nLemma 6. Suppose S is a subset of variables such that G [S] is a c-component. If A \u2208 ID 1 (S), then A \u2229 S = \u2205.\nProof Suppose A \u2229 S is nonempty, and s \u2208 A \u2229 S is an arbitrary variable. Define two models M 1 and M 2 as follows. Every variable in M 1 is uniformly drawn from {0, 1}. Also, every variable in M 2 except s is uniformly drawn from {0, 1}, and s is drawn from {0, 1} with probabilities 0.4 and 0.6, respectively. Let Q Mi denote the interventional distributions under model\nM i , for i \u2208 {1, 2}. Clearly, Q M1 [V \\ A] = Q M2 [V \\ A] = 1 2 |V |\u2212|A| , whereas Q M1 [S s=0 ] = 1 2 |S| \u0338 = 1 0.4 * 2 |S|\u22121 = Q M2 [S s=0 ], which shows that Q[S] is not identifiable in G [V \\A] .\nTheorem 8. WMVC problem is reducible to a minimum-cost intervention problem in polynomial time.\nProof Suppose an undirected graph H = (V H , E H ) along with a weight function \u03c9 : V H \u2192 R \u22650 is given. We construct a semi-Markovian graph G along with a cost function C and prove that the min vertex cover problem in H is equivalent to the min-cost intervention problem in G for some set S. The construction is as follows.\nWe first begin with defining the vertex set of G. For any vertex x \u2208 V H , add a vertex x in G. For any edge {x, y} \u2208 E H , add two vertices u xy and w xy in G. We will denote the set of all such vertices by U and W , respectively. Finally, add a vertex s. The number of vertices of G (denoted by\nV = V H \u222a U \u222a W \u222a {s}) is therefore equal to (|V H | + 2|E H | + 1).\nAssume a random ordering \u03c3 on the vertices of H. Now take an edge {x, y} \u2208 E H , and assume without loss of generality that x precedes y in \u03c3. Add the directed edges u xy \u2192 x, x \u2192 y, y \u2192 w xy , and w xy \u2192 s. Also draw a bidirected edge between u xy and all of the vertices {x, y, w xy , s}. Graph G has therefore 4|E H | directed and 4|E H | bidirected edges (4 edges for each edge in H). Figure 7 demonstrates the structure corresponding to the the edge {x, y} constructed in G. Finally, the cost function C is defined as follows. For x \u2208 V H , C(x) = \u03c9(x). for every other vertex y \u2208 U \u222a W \u222a {s}, C(y) = z, where\nz = |V H | \u2022 max x\u2208V H \u03c9(x) + 1.\nFirst note that constructing the graph G and the cost function C given H and \u03c9 can be done in polynomial time, as it only needs a sweep over the vertices and the edges of H, which can be performed in time O(V H + E H ). To complete the proof of the theorem, we will show that a subset A \u2286 V H is a weighted minimal vertex cover for H if and only if A is a minimum-cost intervention to identify Q[{s}] in G. We begin with the following claims.\nClaim 1: {V H } \u2208 ID G ({s}). To see this, we simply provide the identification formula.\nQ[{s}] = P (s|do(V H , U, W )) = P (s|W, do(V H , U )) (do calculus rule 2) = P (s|W, do(V H )).\n(do calculus rule 3)\nAs seen in the expression above, Q[{s}] can be identified by intervening on (fixing) only the variables V H . Claim 2: if A is a minimum-cost intervention to identify Q[{s}], then A \u2286 V H . First note that from claim 1, we know that the cost of the min-cost intervention is at most\nC(V H ) \u2264 |V H |\u2022max x\u2208V H C(x) \u2264 (z \u2212 1)\n. Since the cost of every variable in V \\ V H is equal to z, the min-cost intervention clearly does not include any such variable.\nClaim 3: if A is a min-cost intervention to identify Q[{s}] in G, then A is a vertex cover for H. Take an arbitrary edge {x, y} \u2208 E H . To prove this claim, it suffices to show that either x \u2208 A or y \u2208 A. The structure G [x,y,uxy,wxy,s] (as depicted in Figure 7) is a hedge formed for Q[{s}] in G. Since {A} \u2208 ID G (S, V \\ S), at least one of the variables {x, y, u xy , w xy } is included in A, as otherwise the aforementioned hedge precludes the identification of Q[{s}]. However, from claim 2 we know that A \u2286 V H , and therefore at least one of x, y is in A, which completes the proof of the claim.\nclaim 4: if A is a vertex cover for H, then {A} \u2208 ID G ({s}, V \\ {s}) in G. Again an identification formula based on the do-calculus rules can be derived. We first begin with the third rule of do calculus to derive Q[{s}] = P (s|do(V H , U, W )) = P (s|do(A, U, W )). This is based on the fact that s \u22a5 \u22a5 (V \\ A)|A, U, W in the graph where incoming edges to U, W, V \\ A are deleted. Now similar to the arguments of claim 1, Q[{s}] = P (s|do(V H , U, W )) = P (s|do(A, U, W )) = P (s|W, do(A, U )) (do calculus rule 2) = P (s|W, do(A)).\n(do calculus rule 3)\nIn the last equality, we used the fact that A is a vertex cover for H, and therefore in every structure like the one shown in Figure 7, at least one of the vertices x or y is included in A, i.e., non of the vertices in U have a direct path to s in the graph where the incoming edges to A are deleted. Equation 8 proves claim 4, as intervention on A suffices to identify Q[{s}]. Now suppose A is a minimum vertex cover for H. From claim 4, {A} \u2208 ID G ({s}, V \\ {s}). We claim that this intervention is a minimum-cost intervention to identify Q[{s}]. Suppose not. Then there exists a min-cost intervention\u00c2 and C(\u00c2) < C(A). From claim 3,\u00c2 is a vertex cover for H. By definition of C(\u2022), a\u2208\u00c2 \u03c9(a) = C(\u00c2) < C(A) = a\u2208A \u03c9(a), which contradicts the assumption that A is the min vertex cover.\nConversely, suppose A is the min-cost intervention to identify Q[{s}] in G. From claim 3, A is also a vertex cover for H. We claim that this vertex cover is a minimum vertex cover. Suppose not.\nThen there exists a minimum vertex cover\u00c2 for H and a\u2208\u00c2 \u03c9(a) < a\u2208A \u03c9(a). From claim 4, {\u00c2} \u2208 ID G ({s}, V \\ {s}). By definition of C(\u2022), C(\u00c2) = a\u2208\u00c2 \u03c9(a) < a\u2208A \u03c9(a) = C(A), which contradicts the assumption that A is the min-cost intervention.\nRemark 10. The unweighted version of WMVC problem (i.e., when the weight function is given by \u03c9(\u2022) = 1) can be reduced to a minimum-cost intervention problem with the constant cost function C(\u2022) = 1 in polynomial time. Consequently, the NP-completeness does not stem from arbitrary choice of cost functions. This claim is formally proved in Appendix C.\nProof The proof is analogous to the proof of Theorem 8 with slight modifications as follows. The graph G is constructed exactly in the same manner, but the cost function is forced to the constant C(\u2022) = 1. With the exact same arguments of the proof of Theorem 8, A is a minimum vertex cover for H only if it is a min-cost intervention to identify Q[{s}] in G. The other direction does not necessarily hold, as claim 2 of that proof does not hold anymore. However, we show how a min-cost intervention solution A can be turned into a minimum vertex cover for H. Suppose A is a min-cost intervention to identify Q[{s}] in G. Substitute any vertex u xy \u2208 A \u2229 U or w xy \u2208 A \u2229 W with one of the vertices x, y arbitrarily, to form the set\u00c2. First note that C(\u00c2) \u2264 C(A), since the cost of all variables are the same. Also\u00c2 \u2286 V H . We claim that\u00c2 is a vertex cover for H. Take an arbitrary edge {x, y} \u2208 E H . It suffices to show that at least one of x, y is included in\u00c2, and follows from the fact that at least one of the variables x, y, u xy , w xy must appear in A, since otherwise {x, y, u xy , w xy , s} is a hedge formed for Q [{s}] in G after intervention on A, which contradicts the fact that\n{A} \u2208 ID G ({s}, V \\ {s}).\nNote that\u00c2 is also a minimum vertex cover for H, since otherwise any vertex cover with smaller weight would also be an intervention with smaller cost than A to identify Q[{s}], which is a contradiction.\nTheorem 12. MWHS problem is reducible to minimum-cost intervention problem in polynomial time.\nProof Suppose we have a universe V = {v 1 , v 2 , . . . , v n }, a set of subsets of V denoted as F 1 , ..., F k , and a weight function \u03c9 : V \u2192 R \u22650 . We consider the problem of finding the subset of V with the minimum aggregate weight such as A such that for any 1\n\u2264 i \u2264 k, A \u2229 F i \u0338 = \u2205.\nWe propose a polynomial time reduction from this problem to an instance of the minimum-cost intervention problem. To this end, we construct a semi-Markovian graph G as follows.\nFor each member of V such as v i , we add a vertex representing v i in G. We fix an arbitrary ordering between the members of V , which without loss of generality we assume is\nv 1 \u227a v 2 \u227a \u2022 \u2022 \u2022 \u227a v n .\nWe draw directed edges between each pair of nodes in {v 1 , . . . , v n } to build a complete graph over the nodes representing V , with respect to the ordering. We add an auxiliary node s, for which we will consider the hedges formed in G. Then for each subset\nF i = {f i,1 \u227a ... \u227a f i,m } \u2286 {v 1 \u227a \u2022 \u2022 \u2022 \u227a v n },\n\u2022 we add (m + 1) new nodes to G, denoted by f \u2032 i,j for 1 \u2264 j \u2264 (m + 1). \u2022 We draw a directed edge from f \u2032 i,j to f i,j for 1 \u2264 j \u2264 m. \u2022 We draw a directed edge from f i,m to f \u2032 i,m+1 , and a directed edge from f \u2032 i,m+1 to s. \u2022 For 1 \u2264 j \u2264 m, we draw a bidirected edge between f \u2032 i,j and f \u2032 i,j+1 . \u2022 Finally, we draw m bidirected edges from f \u2032 i,1 to all nodes in F i , as well as one bidirected edge to s.\ns f \u2032 1,4 f \u2032 1,3 f \u2032 1,2 f \u2032 1,1 f 1,1 f 1,2 f 1,3\nFigure 8: Reduction from the minimum weighted hitting set problem to the minimum-cost intervention problem. Each set F i = {f i,1 , . . . , f i,m } is represented by a hedge structure in the semi-Markovian graph G.\nWe assign C(v) = \u03c9(v) for any v \u2208 V , and C(w) = \u221e for the rest of (auxiliary) vertices. An example for 8. First note that building the graph G requires only traversing V and each set F i once, and performing at most O(|F i |) operations for each of the subsets. As a result, the reduction is polynomial time (the size of the resulting graph is 1 +\nF 1 = {f 1,1 \u227a f 1,2 \u227a f 1,3 } is visualized in Figure\n|V | + k i=1 (|F i | + 1)\n.) It suffices now to prove that the minimum-cost intervention to identify Q[{s}] in G is exactly the minimum hitting set for {F 1 , . . . , F k }. This is proved through the following claims.\nClaim 1. For each\nF i = {f i,1 , . . . , f i,m }, the set X i = F i \u222a {f \u2032 i,j |1 \u2264 j \u2264 (m + 1)} \u222a {s} forms a hedge for Q[{s}].\nThe reason to this is straightforward. Since a complete graph is formed on the vertices corresponding to V , every vertex in F i is a parent of f i,m , which itself has a directed path to s through f \u2032 i,m+1 . Every f i,j for 1 \u2264 j \u2264 m is also a parent of a vertex in F i . As a result, every vertex in X i is an ancestor of s in G [Xi] . On the other hand, every vertex in F i along with s is connected to f \u2032 i,1 by a bidirected edge, and by construction, every vertex f \u2032 i,j has a bidirected path to\nf \u2032 i,1 , which means G [Xi] is a c-component.\nClaim 2. Any finite-cost intervention set A that suffices to identify Q[S] is a valid hitting set for {F 1 , . . . , F k }. That is, if we solve the minimum-cost intervention problem for Q[{s}], the solution will be a valid hitting set for all the sets F i for 1 \u2264 i \u2264 k. To see this, note that A has non-empty intersection with every hedge formed for Q[{s}] in G such as X i . However, every vertex in X i except those in F i have infinite cost. Therefore, A \u2229\nF i \u0338 = \u2205 for every 1 \u2264 i \u2264 m.\nClaim 3. If A is a valid hitting set for F i s, there is no hedge formed for Q[{s}] in G after intervention on A. That is, any valid hitting set for F i s is an intervention set which suffices to identify Q[{s}] in G. To prove this, it suffices to show that if an arbitrary hedge X is formed for Q[{s}], then there exists an index 1 \u2264 i \u2264 k such that no variable in F i is intervened upon. Since by construction the only parents of s are the vertices f \u2032 j,m+1 corresponding to some F j , X includes one of these vertices (a hedge cannot be formed without including a parent of s.) Suppose without loss of generality that F 1 = {f 1,1 , . . . , f 1,m } and that f \u2032 1,m+1 \u2208 X. Since f \u2032 1,m+1 has a bidirected edge only to f \u2032 1,m , f \u2032 1,m \u2208 X. Applying the same argument recursively, we have that {f \u2032 1,1 , . . . , f \u2032 1,m } \u2286 X. However, note that each vertex f \u2032 1,j has only one child by construction of G, which is f 1,j . As a result, f 1,j is not intervened upon for 1 \u2264 j \u2264 m (as otherwise the hedge X would be resolved as at least one of f \u2032 1,j s would not be an ancestor of s in G [X] .) This completes the proof of Claim 3, as non of vertices in F 1 are intervened upon, that is, A \u2229 F 1 = \u2205.\nCombining the three claims above, solving the minimum hitting set for F i s is equivalent to solving the minimum cost intervention set for Q[{s}].\nLemma 15. Let G be a semi-Markovian graph with vertex set V , along with a cost function\nC : V \u2192 R \u22650 . Let S be a subset of V such that G [S] is a c-component. Suppose the set of all hedges formed for Q[S] in G is {F 1 , ..., F m }. Then A *\nS is a solution to Equation (2) if and only if it is a solution to the MWHS problem for the sets {F 1 \\ S, ..., F m \\ S}, with the weight function \u03c9(\u2022) := C(\u2022).\nProof First note that if A is an intervention set that makes Q[S] identifiable, it hits all the hedges formed for Q[S] in G, as otherwise from hedge criterion (Theorem 44) Q[S] would not be identifiable. Conversely, if A hits all the hedges formed for Q[S] in G, intervening on A makes Q[S] identifiable. As a result, a set A is an intervention to identify Q[S] in G if and only if it is a hitting set for {F 1 \\ S, ..., F m \\ S}. Since the set of interventions and the hitting sets coincide, a minimum intervention set is a minimum hitting set and vice-versa.\nLemma 16. Let G be a semi-Markovian graph with the vertex set V , and for\nS \u2286 V , let G [S] be a c-component. For any subset A \u2286 V , if A \u2208 ID 1 (S), then pa \u2194 (S) \u2286 A.\nProof First, from Lemma 6 we know that \nS \u2229 A = \u2205. Now define B = S \u222a (pa \u2194 (S) \\ A). If B \\ S \u0338 = \u2205,\nV such that G [S] is a c-component, Algorithm 1 returns Hhull(S, G) in O(|V | 3 ).\nProof First note that F 2 is always a subset of F throughout the algorithm. Therefore, every time that F 2 \u0338 = F , at least one vertex is excluded from F to form F 2 . Hence, the while loop is performed at most |V | times, and the algorithm terminates. Inside every loop, two depth-first searches are executed, one to find the connected component of S in the edge-induced subgraph of G [F ] over its bidirected edges, and the other to find the ancestors of S in G [F ] . DFS is quadratic-time in the worst case, i.e., each iteration runs in time 2|F | 2 \u2264 2|V | 2 in the worst case. Therefore, the algorithm ends in time O(|V | 3 ).\nLetF be the output of Algorithm 1. Since in the last iterationF = F 1 = F 2 ,F is the set of ancestors of S in G [F ] , and also G [F ] is a c-component. By definition,F is a hedge formed for Q[S] in G, and thereforeF \u2286 Hhull(S, G). Now suppose F \u2032 is a hedge formed for Q[S] in G. It suffices to show that F \u2032 \u2286F . At the beginning of the algorithm, F = V , that is, F \u2032 is included in F . At each iteration, since every vertex in F \u2032 has a bidirected path to S through only the vertices in F \u2032 , it also has a bidirected path to S in every subgraph of G which includes F \u2032 . As a result, when constructing the connected component of S in line 3 of Algorithm 1, F \u2032 \u2286 F 1 . Further, by definition of hedge, every vertex in F \u2032 has a directed path to S that goes through only vertices of F \u2032 . By the same argument, every vertex in F \u2032 is included in F 2 in line 4. Therefore, F \u2032 \u2286F .\nTheorem 20. Let S be a subset of variables such that G [S] is a c-component. Then, A * S is a solution to (2) if and only if both pa \u2194 (S) \u2286 A * S and A * S \\ pa \u2194 (S) is a minimum-cost intervention to identify\nQ[S] in G [H]\n, where\nH := Hhull(S, G [V \\pa \u2194 (S)] ).\n(\n)3\nProof First note that the set of hedges formed for Q[S] in G can be partitioned into hedges that intersect with pa \u2194 (S), and the hedges that do not intersect with pa \u2194 (S), denoted by F 1 and F 2 , respectively. The set of hedges formed for\nQ[S] in G [H \u2032 ] is then F 2 .\nUsing Lemma 15, the lemma is equivalent to the claim that A * is a minimum hitting set for the hedges F 1 \u222a F 2 if and only if A * \\ pa \u2194 (S) is a minimum hitting set for hedges F 2 . Suppose A * is a min-cost intervention to identify Q[S] in G. From Lemma 15, A * hits all the hedges formed for Q[S] in G H \u2032 , i.e., F 2 . However, since none of these hedges intersect with pa \u2194 (S), A * \\ pa \u2194 (S) hits all of these hedges. We claim that A * \\ pa \u2194 (S) is the minimum hitting set for the hedges F 2 . Suppose there exists another set\u00c3 such that\u00c3 hits all the hedges F 2 , and C(\u00c3) < C(A * \\ pa \u2194 (S)). Since all the hedges F 1 intersect with pa \u2194 (S),\u00c3 \u222a pa \u2194 (S) hits all the hedges formed for Q[S] in G, and\nC(\u00c3 \u222a pa \u2194 (S)) \u2264 C(\u00c3) + C(pa \u2194 (S)) < C(A * \\ pa \u2194 (S)) + C(pa \u2194 (S)) = C(A * ),\nwhich contradicts the fact that A * is the minimum-cost intervention to identify Q[S] in G.\nConversely, let A * \\ pa \u2194 (S) be a minimum hitting set for hedges F 2 . If A * is not the min-cost intervention to identify Q[S] in G, then there exists\u00c3 such that C(\u00c3) < C(A * ) and\u00c3 hits the hedges F 1 \u222a F 2 . From Lemma 16, pa \u2194 (S) \u2286\u00c3. Since hedges F 2 do not intersect with pa \u2194 (S), A \\ pa \u2194 (S) hits all the hedges F 2 , and\nC(\u00c3 \\ pa \u2194 (S)) = C(\u00c3) \u2212 C(pa \u2194 (S)) < C(A * ) \u2212 C(pa \u2194 (S)) = C(A * \\ pa \u2194 (S)), which contradicts the fact that A * \\ pa \u2194 (S) is the minimum-cost intervention to identify Q[S] in G [H \u2032 ] .\nLemma 22. Let G be a semi-Markovian graph and S \u2286 V . Algorithm 2 returns an optimal solution to (2).\nProof First let G [S] be a c-component. Note that every time that the set A is constructed in line 14 and intervening on A \u222a pa \u2194 (S) does not suffice to identify Q[S] in G, the algorithm continues on the subgraph of G over Hhull(S, G V \\(A\u222apa \u2194 (S)) ). As a result, the newly discovered hedges do not intersect with A, i.e., Algorithm 2 never discovers redundant hedges. Since every hedge is a subset of the graph and the number of such subsets is finite, the algorithm halts within finite time. At each iteration (while loop of lines 7-13), a new hedge is added to the set of hedges formed for Q[S] a b S Figure 9: Hedges of size 2 must be in the form depicted above. Exactly one vertex a is a member of pa(S) \\ biD(S), and the other vertex b is a member of biD(S) \\ pa(S).\nand adds it to the set F. From the minimum hitting set formulation, it is clear that any min-cost intervention for identifying Q[S] in G must intersect with all the hedges in F. As a result, the output of Algorithm 2 (A) is always a subset of a min-cost intervention. Further, by construction A \u2208 ID 1 (S). As a result, A is a min-cost intervention for identifying\nQ[S] in G. Now suppose G [S] is not a c-component. G [S] can be uniquely partitioned into its maximal c-components G [S1] , ..., .G [S k ] .\nThe arguments above hold for any of the maximal c-components G [Si] . The result follows from the fact that Q[S] is identifiable in G if and only if all of its maximal c-components are identifiable in G.\nProposition 24. Let A * be an optimal solution to the minimum-cost intervention problem of Equation (2). Let A be the minimum hitting set computed in line 14 of Algorithm 2 in an arbitrary iteration. Also, define\nH = Hhull(S, G [V \\(A\u222apa \u2194 (S))] ) \\ S. Then, (A \u222a pa \u2194 (S) \u222a H) \u2208 ID 1 (S), and C(A * ) \u2264 C(A \u222a pa \u2194 (S) \u222a H) \u2264 C(A * ) + C(H).\nProof Let A be the minimum hitting set computed in line 14 of Algorithm 2 in i-th iteration. Let F and F \u2032 denote the set of all hedges formed for Q[S] in G [V \\pa \u2194 (S)] , and the set of hedges discovered up to i-th iteration. Clearly, A hits all the hedges in F \u2032 . Moreover, by definition of hedge hull, any hedge formed for Q[S] which is hit neither by A nor by pa \u2194 (S), is a subset of H. As a result, any hedge formed for Q[S] in G is hit by at least one of the variables in A, pa \u2194 (S), or H. Therefore,\nA \u222a pa \u2194 (S) \u222a H \u2208 ID 1 (S).\nFor the second part of the claim, note that the first inequality is trivial since A * is the optimal set in ID 1 (S). As for the second inequality, recall that A * is the minimum hitting set for F along with pa \u2194 (S), whereas A is the minimum hitting set for F \u2032 . Since F \u2032 \u2286 F, any hitting set for F \u2032 is also a hitting set for F \u2032 . This is to say, A * \\ pa \u2194 (S) is a hitting set for F \u2032 , whereas A is the minimum one. As a result, C(A) \u2264 C(A * \\ pa \u2194 (S)). From linearity of the cost function,\nC(A \u222a pa \u2194 (S) \u222a H) = C(A) + C(pa \u2194 (S)) + C(H) \u2264 C(A * \\ pa \u2194 (S))) + C(pa \u2194 (S)) + C(H) = C(A * ) + C(H) Lemma 26. Let G be a semi-Markovian graph on V and S be a subset of V such that G [S] is a c-component. Heuristic Algorithm 1 returns an intervention set A in O(|V | 3 ) such that A \u2208 ID 1 (S).\nProof Correctness. Let the output of the algorithm be A. We will utilize the minimum hitting set formulation to show the correctness of the algorithm. Let F be a hedge formed for Q[S] in G.", "publication_ref": ["b30", "b30"], "figure_ref": ["fig_11", "fig_11", "fig_11"], "table_ref": []}, {"heading": "It suffices to show that", "text": "F \u2229 A \u0338 = \u2205. If F \u2229 pa \u2194 (S) \u0338 = \u2205, then the claim holds since pa \u2194 (S) \u2286 A.\nOtherwise, F is a hedge formed for Q [S] in G [V \\pa \u2194 (S)] , i.e., F \u2286 H, where H is given by Equation (3). Now let a be an arbitrary vertex in (F \\ S) \u2229 pa(S). Such a vertex exists by definition of hedge. Further, G [F ] is a c-component, i.e., there exists a path from a to S through bidirected edges in F . As a result, in the undirected graph H built in heuristic Algorithm 1, there exists a path from x to y that passes only through vertices in F . Any solution to minimum vertex cut for x \u2212 y must include at least one vertex of F . Therefore, F \u2229 A \u0338 = \u2205.\nRuntime. Heuristic Algorithm 1 begins with constructing the set pa \u2194 (S), and the set H given by Equation (3), which are performed in time O(|V |), and O(|V | 3 ) in the worst case. Constructing the graph H requires iterating over the bidirected edges of G [H] , which can be done in time O(|H| 2 ) in the worst case. The reduction from minimum vertex cut to minimum edge cut discussed in Appendix E is linear-time. The final step of the algorithm is to solve a minimum edge cut, which can be done in time O(|H| 3 ) using the push-relabel algorithm to solve the equivalent maximum flow problem (Goldberg and Tarjan, 1988). Noting that |H| \u2264 |V |, the runtime of the algorithm is O(|V | 3 ).\nLemma 27. Let G be a semi-Markovian graph on V and S be a subset of V such that G [S] is a c-component. Heuristic Algorithm 2 returns an intervention set A in O(|V | 3 ) such that A \u2208 ID 1 (S).\nProof Correctness. Let the output of the algorithm be A. We will utilize the hitting set formulation to show the correctness of the algorithm. Let F be a hedge formed for Q , which can be done in time O(|H| 2 ) in the worst case. The reduction from minimum vertex cut to minimum edge cut discussed in Appendix E is linear-time. The final step of the algorithm is to solve a minimum edge cut, which can be done in time O(|H| 3 ) using the push-relabel algorithm to solve the equivalent maximum flow problem (Goldberg and Tarjan, 1988). Noting that |H| \u2264 |V |, the runtime of the algorithm is O(|V | 3 ).\n[S] in G. It suffices to show that F \u2229 A \u0338 = \u2205. If F \u2229 pa \u2194 (S) \u0338 = \u2205, then the claim holds since pa \u2194 (S) \u2286 A. Otherwise, F is a hedge formed for Q[S] in G [V \\pa \u2194 (S)] , i.e., F \u2286 H,\nProposition 28. Let G be a random semi-Markovian graph, where each directed edge exists with probability p and each bidirected edge exists with probability q, mutually independently (generalized Erdos-Renyi generative model). Suppose S = {s}, where s is an arbitrary vertex. Let c * be the random variable of the cost of the optimal solution to Equation (2). Also let c 1 and c 2 be the random variables of the cost of the solution returned by Heuristic Algorithm 1 and Heuristic Algorithm 2, respectively. Then under this generative model, and with equal cost for vertices,\nE[c 1 ] \u2264 q \u22121 E[c * ], E[c 2 ] \u2264 p \u22121 E[c * ].\nProof Since the costs of intervention are assumed to be equal for every vertex, both sides of the inequalities can be normalized with the constant cost. Therefore, without loss of generality, suppose that the cost of intervention on each vertex is 1. That is, the cost of intervention on a set X is |X|. Let N be the set of vertices of G that precede s in the causal order. Each of these vertices s in biD(S) with probability q, and is in pa(S) with probability p. Since these two events are independent, any such vertex appears in pa \u2194 (S) with probability pq. Let 1 pa \u2194 (S) (v) be the indicator function corresponding to the membership of a vertex v \u2208 N in pa \u2194 (S). Then,\nE[|pa \u2194 (S)|] = E[ v\u2208N 1 pa \u2194 (S) (v)] = v\u2208N P r(v \u2208 pa \u2194 (S)) = v\u2208V pq = |N |pq.\nFrom Lemma 16, pa \u2194 (S) \u2286 A * , where A * is the optimal solution to minimum-cost intervention (and c * = C(A * )). Therefore, |pa \u2194 (S)| \u2264 |A * |, and consequently,\nE[c * ] = E[C(A * )] = E[|A * |] \u2265 E[|pa \u2194 (S)|] = |N |pq.(9)\nNow consider the minimum-weight vertex cut problem that Heuristic algorithm 1 solves. The vertex x is only connected to pa(S) \u2229 H, and therefore pa(S) \u2229 H is a trivial cut. As a result,\nE[c 1 ] \u2264 E[|pa(S) \u2229 H|] \u2264 E[|pa(S)|] = |N |p.(10)\nBy a similar argument, we can write\nE[c 2 ] \u2264 E[|biD(S) \u2229 H|] \u2264 E[|biD(S) \u2229 N |] = |N |q.(11)\nThe result follows from Equations ( 9), ( 10) and (11).\nCorollary 29. Consider an algorithm that runs both of these heuristic algorithms and picks the best solution out of the two. Let the cost of this solution be denoted by c. This algorithm runs in time O(|V | 3 ) in the worst case, and the cost of its solution satisfies the following inequality in the Erdos-Renyi model (see Appendix C for the proof.)\nE[c * ] \u2264 E[c] \u2264 min{p \u22121 , q \u22121 }E[c * ].\nProof Since both heuristic algorithms run in time O(|V | 3 ), running both of them also requires time O(|V | 3 ). Also, since we are choosing the better solution among the two, if the costs of the solutions returned by Heuristic Alg. 1 and Heuristic Alg. 2 are denoted by c 1 and c 2 , respectively, then\nc = min{c 1 , c 2 }. Therefore, E[c] \u2264 min{E[c 1 ], E[c 2 ]}. Plugging in the inequalities from Proposition 28, E[c] \u2264 min{E[c 2 ], E[c 1 ]} \u2264 min{p \u22121 E[c * ], q \u22121 E[c * ]} = min{p \u22121 , q \u22121 }E[c * ],\nwhich proves the second inequality. The first inequality is trivial since c * is the optimal cost. Theorem 30. Suppose S is a subset of variables such that C(s) = \u221e for any s \u2208 S. Let A = {A 1 , A 2 , ..., A m } be a collection of subsets such that A \u2208 ID G (S, V \\ S) and m > 1. Then there exists a singleton intervention\u00c3 such that\u00c3 = {\u00c3} \u2208 ID G (S, V \\ S) and C(\u00c3) \u2264 C(A).\nProof First note that if there exists 1 \u2264 i \u2264 m such that A i \u2229 S \u0338 = \u2205, then C(A) = \u221e. In this case, A = V \\ S satisfies the desired property. Otherwise, we can assume that S \u2229 (\u222a m i=1 A i ) = \u2205. We claim that\u00c3 = A \u222a = \u222a m i=1 A i is the desired intervention set. To prove this claim, first note that\nC({\u00c3}) = C(A \u222a ) = a\u2208A\u222a C(a) \u2264 m i=1 a\u2208Ai C(a) = m i=1 C(A i ) = C(A).\nTherefore, it suffices to show that {\u00c3} \u2208 ID G (S, V \\ S). Let S 1 , ..., S k denote the maximal ccomponents of G [S] . From lemma 2 of (Tian and Pearl, 2002) (restated here as Theorem 46), Q[S] is identifiable in G V \\\u00c3 , if and only if Q[S 1 ], ..., Q[S k ] are identifiable in G V \\\u00c3 . Therefore, it suffices to show that {\u00c3} \u2208 ID G (S i , V \\ S i ), for all 1 \u2264 i \u2264 k. This result follows from Lemma 3, since G [Si] is a c-component, {A 1 , ..., A m } \u2208 ID G (S i , V \\S i ) (from Theorem 46), and A \u222a \u2229S i = \u2205.\nLemma 31. Suppose A * S,V \\S = {A 1 , . . . , A t } is the minimum-cost intervention to identify Q[S]. Let S (1) , . . . , S (t) be the partitioning of maximal c-components of G [S] , such that c-components of each partition S (j) are identified by intervention on A j for 1 \u2264 j \u2264 t. Then,\nA j \u2208 arg min A\u2208ID1(S (j) ) C(A), \u22001 \u2264 j \u2264 t.\nProof By definition of S (j) s, for each 1 \u2264 j \u2264 t, we have A j \u2208 ID 1 (S (j) ). Suppose that the claim does not hold. That is, there exists an index j such that A j / \u2208 arg min A\u2208ID1(S (j) ) C(A). From a new set family (collection of subsets) A by replacing the set A j in A * S,V \\S with A \u2032 j , where\nA \u2032 j \u2208 arg min A\u2208ID1(S (j) ) C(A).\nClearly, the c-components in S (j) are identified from intervention on A \u2032 j , and the rest of the ccomponents (in all other partitions) are identified from the sets in A \\ {A j }. As a result,\nA \u2208 ID G (S, V \\ S). (12\n)\nSince A j , A \u2032 j \u2208 ID 1 (S) and A \u2032 j has the optimal cost, C(A \u2032 j ) < C(A j ) (equality cannot happen since A j is assumed not to be optimal.) Since the rest of the sets in A and A * S,V \\S are the same, and since the costs are additive,\nC(A) = C(A \\ {A \u2032 j }) + C(A \u2032 j ) < C(A * S,V \\S \\ {A j }) + C(A j ) = C(A * S,V \\S ). (13\n)\nEquations ( 12) and ( 13) contradict the optimality of A * S,V \\S , which completes the proof.\nProposition 32. Given a semi-Markovian graph G and a subset S of its vertices, Algorithm 3 with Algorithm 2 used as a subroutine in line ( 9) returns an optimal solution to the min-cost interventions collection to identify Q[S] in G.\nProof Let A be the output of Algorithm 3. We first claim that\nA \u2208 ID G (S, V \\ S). For any maximal c-component of G [S] such as G [Si]\n, there exists at least one set A j \u2208 A such that A j \u2208 ID 1 (S). Therefore,\nA \u2208 ID G (S i , V \\S i ). Since Q[S i ] is identifiable from interventions on A for any c-component of G [S]\n, by the result form (Tian and Pearl, 2002), Q[S] is also identifiable, i.e., A \u2208 ID G (S, V \\ S). Now suppose A * S = {A * 1 , ..., A * t } is a min-cost intervention collection to identify Q[S] in G. It suffices to show that C(A) \u2264 C(A * S ). Consider an arbitrary partitioning of the c-components of S to t parts such as S (1) , ...S (t) , such that A * i \u2208 ID 1 (S (i) ) for 1 \u2264 i \u2264 t. Note that such a partitioning is possible due to Observation 1. Algorithm 3 considers this partitioning in one of its iterations, and due to optimality of Algorithm 2 (see Lemma 22), it constructs an intervention collection\u00c3 = {\u00c3 1 , ...,\u00c3 t } where\u00c3 i is the optimal intervention set in ID 1 (S (i) ) for 1 \u2264 i \u2264 t. As a result,\nC(\u00c3) = t i=1 C(\u00c3 i ) \u2264 t i=1 C(A * i ) = C(A * S ).\nSince Algorithm 3 outputs the minimum-cost collection among all constructed intervention collections, clearly C(A) \u2264 C(\u00c3) \u2264 C(A * S ).\nLemma 34. Let A * i be defined as in Equation (5). Let B denote the solution to the WMSC problem with \u03c9(\u0393 i ) = A * i for every non-empty subset of \u0393 and \u03c9(\u2205) = 1. Then A = {A * i |\u0393 i \u2208 B} is an optimal solution to Equation (1).\nProof It suffices to show that A \u2208 ID G (S, V \\ S), and C(A) \u2264 C(A * S,V \\S ), where A * S,V \\S is any solution to Equation (1). Since B is a set cover for \u0393, for every 1 \u2264 j \u2264 k, there exists \u0393 i \u2208 B such that S j \u2208 \u0393 i , or equivalently, A * i \u2208 A such that A * i \u2208 ID 1 (S j ). As a result, for any 1 \u2264 j \u2264 k, Q[S j ] is identifiable from A, and therefore A \u2208 ID G (S, V \\ S) (Tian and Pearl, 2002). To show optimality, let A * S,V \\S be an arbitrary solution to Equation (1). From Lemma 31, there exists a partitioning of the c-components such as S (1) , . . . , S (t) such that A * S,V \\S = {A 1 , . . . , A t }, where\nA j \u2208 arg min A\u2208ID1(S (j) ) C(A), \u22001 \u2264 j \u2264 t.\nThe partitions S (j) are disjoint subsets of the set of c-components \u0393 = {S 1 , . . . , S k }. Without loss of generality, assume \u0393 j = S (j) for 1 \u2264 j \u2264 t. Then the set {\u0393 1 , . . . , \u0393 t } is a set cover for \u0393. Since B is the minimum-weight set cover for \u0393,\nC(A) = A\u2208B \u03c9(A) \u2264 1\u2264i\u2264t \u03c9(\u0393 i ) = 1\u2264i\u2264t C(A i ) = C(A * S,V \\S ).\nProposition 37. Algorithm 4 returns a solution A \u2208 ID G (S, V \\ S), such that C(A) \u2264 kC(A * S,V \\S ), where k and A * S,V \\S are the number of c-components in G [S] , and an optimal solution to Equation (1), respectively.\nProof Let {S 1 , . . . , S k } be subsets of S such that G [Si] for 1 \u2264 i \u2264 k are the maximal c-components in G [S]\n. The proof consists of two parts. First, we show that the set family A returned by Algorithm 4 is a member of ID G (S, V \\ S) (identifiability). Then, we prove that it has a cost at most k times the cost of the minimum cost among all members of ID G (S, V \\ S) (approximation ratio).\nProof of identifiability. Let f * (\u2022, \u2022) be the optimal integral solution to the MCFP in line 11 of Algorithm 4. The vertex z receives a flow of d * = k, and it has exactly k incoming edges with capacity 1 in the flow network H. As a result, f * (S j , z) = 1 for every 1 \u2264 j \u2264 k. By conservation of flow, and from integrality of the solution, we know that for every j, there exists exactly one vertex \u0393 i such that f * (\u0393 i , S j ) = 1. Therefore, for each 1 \u2264 j \u2264 k, there exists exactly one set \u0393 i in the set familyB constructed in line 12 such that f * (\u0393 i , S j ) = 1 > 0. But by construction of the flow network, an edge exists between \u0393 i and S j if and only if S j \u2208 \u0393 i . As a result, for each 1 \u2264 j \u2264 k, there exists exactly one set \u0393 i inB such that S j \u2208 \u0393 i , and f * (\u0393 i , S j ) > 0. Therefore, by replacing \u0393 i s with \u0393 i \u2032 s in line 13, we know that S j will appear in the corresponding \u0393 i \u2032 ,. That is, for each 1 \u2264 j \u2264 k, there exists exactly one set \u0393 i \u2032 \u2208 B * such that S j \u2208 \u0393 i \u2032 . By definition of A * i s, the intervention set A i \u2032 corresponding to \u0393 i \u2032 (which is included in A * ) suffices to identify Q[S j ] in G. The identifiability result then follows from the fact that Q[S] is identifiable in G from A * if and only if Q[S j ] is identifiable from A for any 1 \u2264 j \u2264 k (Tian and Pearl, 2002).\nProof of approximation ratio. We first claim that the cost of the optimal flow f * () is at least as large as a fraction 1 k of the cost of intervention collection A returned by the algorithm. To see this, note that the cost of a flow is implied by the amount of flow going through edges between the source w and the vertices \u0393 i , since all other edges have 0 flow cost. As a result, the cost of the optimal flow\nf * is \u0393i\u2208B f * (w, \u0393 i ) C(A * i ) |\u0393 i | = \u0393i\u2208B |\u0393 i \u2032 | |\u0393 i | C(A * i ). (14\n)\nWe explain why the equality above holds. Replacing \u0393 i with the corresponding \u0393 i \u2032 is in a way such that the number of elements in \u0393 i \u2032 is equal to the flow into \u0393 i (which is equal to out-flow of \u0393 i , which in turn is the number of S j s such that f * (\u0393 i , S j ) > 0.) As a result, f * (w, \u0393 i ) = |\u0393 i \u2032 |. Now note that |\u0393 i | \u2264 k, since \u0393 i \u2286 \u0393. Moreover, |\u0393 i \u2032 \u2265 1, since \u0393 i has positive flow and therefore at least one S j receives positive flow from \u0393 i . Finally, since \u0393 i \u2032 \u2286 \u0393 i , from linearity of costs, C(A * i ) \u2265 C(A * i \u2032 ). Plugging these inequalities into Equation ( 14), the cost of the optimal flow f * is at least\n\u0393i\u2208B 1 k C(A * i \u2032 ) = 1 k \u0393 i \u2032 \u2208B * C(A * i \u2032 ) = 1 k A i \u2032 \u2208A C(A * i \u2032 ) = 1 k C(A). (15\n)\nNow from Lemma 34, there exists a subset of A * i s, namely A * which is an optimal solution to the minimum-cost intervention problem. Let the corresponding \u0393 i s be denoted by B \u2032 . Define a flow f \u2032 (\u2022, \u2022) as follows.\nf \u2032 (w, \u0393 i ) = |\u0393 i |, if \u0393 i \u2208 B \u2032 , 0, o.w. , f \u2032 (\u0393 i , S j ) = 1, if \u0393 i \u2208 B \u2032 , S j \u2208 \u0393 i 0, o.w. , f \u2032 (S j , z) = 1.\nFlow f \u2032 satisfies all the flow constraints, and induces cost:\n\u0393i\u2208B \u2032 f \u2032 (w, \u0393 i ) C(A * i ) |\u0393 i | = A * i \u2208A * C(A * i ) = C(A * ).\nSince f * is the minimum-cost flow, the cost of f * is at most equal to f \u2032 , that is, C(A * ). Combining this result with Equation (15), we get\n1 k C(A) \u2264 C(A * ).\nMultiplying both sides by k completes the proof.\nProposition 38. Theorem 5 holds under Assumption 2, even if Assumption 1 is violated. Kivva et al. (2022), there exists\nProof Let A = {A 1 , . . . A m } be a collection of interventions in ID G (S, V \\ S) for a set S such that G [S] is a c-component. From Theorem 1 of\n1 \u2264 i \u2264 m such that Q[S] is identifiable from Q[V \\ A i ]. That is, A i \u2208 ID 1 (S). Without loss of generality, suppose i = 1. Define\u00c3 = {A 1 }. Clearly,\u00c3 \u2208 ID G (S, V \\ S).\nAs for the cost, applying Assumption 2 successively, we get: Proof First note that from Lemma 48, all the sets in F in line 9 of the algorithm are hedges formed for Q[{s}] in G. Therefore, any intervention to identify Q[{s}] must hit all of these sets. On the other hand, if all of these sets are hit, by definition of N C s (\u2022), a hedge formed for Q[{s}] cannot include any of the vertices in Hhull(s), i.e., there does not exist any hedge formed for\nC(\u00c3) = C({A 1 }) \u2264 C({A 1 , A 2 }) \u2264 \u2022 \u2022 \u2022 \u2264 C({A 1 , A 2 , . . . , A m }) = C(A).\nQ[{s}] in G [V \\A]\n, where A is a hitting set for F. As a result, the solution to min-cost intervention problem is the solution to minimum hitting set for F in line 9 of the algorithm. Further, we can eliminate any hedge F \u2032 from F, if there is a hedge F \u2208 F such that F \u2286 F \u2032 . This is due to the fact that if F is hit, F \u2032 is also hit. Observing that if x \u2208 N C s (y) then N C s (x) \u2286 N C s (y), we can eliminate all such sets N C s (y) from F. At the end of this process (after the for loop of lines 10-13), we claim that for any two sets F, F \u2032 remaining in F, F \u2229 F \u2032 = {s}. Suppose not. Then there exists x \u0338 = s such that x \u2208 F \u2229 F \u2032 . Since F and F \u2032 are both sets of the Form N C s (\u2022), as mentioned above, N C s (x) \u2286 F and N C s (x) \u2286 F \u2032 . Now if N C s (x) \u2208 F, then both sets F and F \u2032 (or at least one of them, in the case that one of them is N C s (x) itself) would be eliminated from F during the for loop of lines 12-13. Otherwise, N C s (x) / \u2208 F, which means that there exists a vertex y \u2208 H such that N C s (y) \u2286 N C s (x), and therefore N C s (x) has been removed from F. But in this case, N C s (y) \u2286 N C s (x) \u2286 F, F \u2032 , and in the same iteration where N C s (x) was removed from F, both F and F \u2032 would also be eliminated. The contradiction shows that when the algorithm reaches line 14, all the sets {F \\ {s}|F \u2208 F} are disjoint. Clearly, the minimum hitting set for disjoint sets includes the minimum-cost member of each of the sets, which is the output of Algorithm 5, along with pa \u2194 (s) (Lemma 16).\nAs discussed earlier, constructing the hedge hull of s (Hhull(s)) requires at most O(|V |) times running a depth-first search, which has linear complexity in trees. Therefore, constructing the hedge hull has a worst-case time complexity of O(|V | 2 ). pa \u2194 (s) can also be constructed in linear time, using two one-step breadth-first searches. Let H denote the hedge hull of s in the graph G V \\pa \u2194 (s) . Constructing nec s (x) for the vertices in H can be performed by solving two all-pair shortest paths, which requires two breadth-first search from each vertex with time complexity O(|H| 2 ) in the worst case (BFS in trees requires linear time.) The while loop of lines 6-8 is performed at most |H| times, each with linear complexity. As a result, the N C s sets and therefore F are also constructed in time O(|H| 3 ). The for loop of lines 12-13 requires a sweep over the sets in F, which are at most |H| many sets, each with at most |H| members; which can be performed in time O(|H| 2 ), and therefore the for loop of lines 10-13 has complexity O(|H| 3 ). Finally, calculating the minimum of a set with at most |H| members can be done in (sub)linear time. Therefore, the complexity of Algorithm 5 is O(|V | 3 ) in the worst case. As a result, all hedges of size 2 are in the form depicted in Figure 9, where a \u2208 pa(S) \\ biD(S) and b \u2208 biD(S) \\ pa(S). Accordingly, for any edge drawn in H such as {a, b}, exactly one of them is in biD(S) \\ pa(S), and the other one is in pa(S) \\ biD(S). Partitioning the vertices of H into the aforementioned sets, it is clear that H is bipartite.  (Tian and Pearl, 2002). The result follows immediately. It is worthy to note that the only overhead in the case that G [S] is not a c-component is to partition S into its c-components, which can be done using DFS in time O(|V | 3 ) in the worst case, i.e., it does not alter the computational complexity of any of the heuristic algorithms.", "publication_ref": ["b21", "b21", "b46", "b46", "b46", "b46", "b30", "b46"], "figure_ref": [], "table_ref": []}, {"heading": "D. Special Cases & Improvements", "text": "In this section, we discuss a few special cases of the min-cost intervention problem, and how these cases can be solved efficiently. We show that under the assumption that the expert has certain knowledge about the structure of the causal graph G, or the cost function C(\u2022), the problem of designing the minimum-cost intervention can be solved efficiently in polynomial time. Some of these assumptions might seem restrictive. However, as we shall discuss, they provide useful insight towards solving the min-cost intervention problem efficiently in more practical settings.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.1 Tree-like structure of G", "text": "We begin with a special structure of the semi-Markovian graph G, where both the edge induced subgraphs of G over the directed edges and over the bidirected edges are trees. Between any pair of vertices in a tree, there is a unique path. As a result, for any two vertices a, b in G, there is a unique path using bidirected edges, and if a is an ancestor of b, there is also a unique path from a Algorithm 6 Polynomial algorithm for bounded hedges. draw an edge between a and b in H 5: A \u2190 the min-weight vertex cover for H 6: return A \u222a pa \u2194 (S) y \u2208 Hhull(S) and the set N C s (x) is not eliminated yet, we eliminate N C s (y). At the end of this procedure, we are left with a collection of hedges F that satisfies the following properties.\n1. The min-cost intervention to identify Q[{s}] in G is the min-cost hitting set solution to {F \\ {s}|F \u2208 F}.\n2. For any two hedges F, F \u2032 \u2208 F, F \u2229 F \u2032 = {s}. There are further considerations to Algorithm 5 that we would like to mention. The first one is that this algorithm together with the definitions of nec s and N C s , suggest an alternative formulation of the min-cost intervention problem, which is taking into account the set of variables that must be combined together with each variable x to form a hedge for Q [S]. The definition of nec s (x) can be generalized to the case that G is not a tree anymore, although nec s (x) will not be a set anymore, but a collection of sets where if an intervention is made upon at least one vertex of all of these sets, no remaining hedge formed for Q[{s}] includes x. This indeed suggests a method to enumerate the hedges formed for Q[S] in G. As we saw in this section, for tree-like structures, this enumeration can be executed in polynomial time. However, in general structures, this enumeration method would still take exponential time in the worst case. Another point to mention is that one-step generalizations of the assumption of tree-ness and Algorithm 5 can be thought of, such as the assumption that the number of paths between each pair of vertices in G is at most 2 (or k, where k is a constant.) Although the tree assumption made in this section might appear restrictive, such generalizations might yield efficient solutions of the min-cost intervention problem that can be used in practice.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.2 Bounded hedge size", "text": "Following the hitting set formulation for the min-cost intervention problem, the two main challenges were enumerating the hedges and solving the hitting set problem afterwards. For a hedge F formed for Q[S] in G, let (|F | \u2212 |S|) be the size of this hedge, which is exactly the size of the set to be hit in the hitting set equivalent. If an upper bound on the size of the hedges formed for Q[S] such as (|F | \u2212 |S|) \u2264 k is know where k is a constant, then the task of enumerating the hedges can be performed in polynomial time, as we only need to check the subsets of up to size k. Note that as discussed in Section 3.2, this argument is still valid if the upper bound works for the set of minimal Algorithm 8 Heuristic algorithm 1. , and every edge that goes out of v to v 2 . The resulting problem can be solved using any of the standard max-flow-min-cut algorithms. We used the push-relabel algorithm to solve the max-flows throughout our simulations (Goldberg and Tarjan, 1988).\nThe second heuristic algorithm, depicted as Algorithm 9, relies on similar ideas. Again, we begin with removing pa \u2194 (S) from the graph, as we already know that this set must be included in the output. We then build a directed graph H over the vertices of H = Hhull(S, G [V \\pa \u2194 (S)] ), along with two extra vertices x and y. For every directed edge v 1 \u2192 v 2 in G [H] , we draw a corresponding edge between v 1 \u2192 v 2 in H. Finally, we draw an edge from x to all vertices in biD(S) \u2229 H and from all vertices in S to y. Note that every directed path from x to y in H corresponds to a directed path that connects a vertex in biD(S) to a vertex in S in G. If we intervene on a subset of variables A such that no such path exists anymore, the hedge hull of S in the remaining graph will be S itself, as none of the vertices biD(S) have a directed path to S. Consequently, the effect Q[S] becomes identifiable. With that being said, we solve for the minimum-weight vertex cut for x \u2212 y in H in line (4) of the algorithm. We set the weights of the vertices in S to infinity to ensure that we do not intervene on them. As mentioned above, we reduce the min-weight vertex cut to min-weight edge cut, and then use max-flow algorithms to solve it.\nFinally, we proceed to our third heuristic algorithm, which is based on a greedy approach. First, note that if we intervene on every variable in the hedge hull of S except S, Q[S] becomes identifiable. That is, defining H = Hhull(S, G [V \\pa \u2194 (S)] ), one trivial set in ID G (S, V \\ S) is {(H \\ S) \u222a pa \u2194 (S)}. Similarly, if we intervene on a set of variables A, then A \u222a Hhull(S, G [V \\A] \\ S) is a trivial solution. In our greedy approach, we minimize the cost of this trivial solution at each iteration. We proceed as follows. We maintain an intervention set A, which is initialized as pa \u2194 (S). At each iteration, we find the vertex x \u2208 Hhull(S, G [V \\A] ) \\ S that minimizes the objective function f (x) = C(x) + C(Hhull(S, G [V \\(A\u222a{x})] )), and add this vertex to A. Note that the function f (x) is exactly the cost of the trivial solution in graph G \\ (A \u222a {x}). We add one vertex in each iteration until we reach a point where Q[S] becomes identifiable. The following result indicates the correctness of Algorithm 10 along with its computational complexity. than an undirected graph. However, if the graph G [H] is dense on its directed edges, we choose Algorithm 8. In certain cases, as shown by our empirical evaluation, the greedy approach achieves lower regret despite the higher time complexity.", "publication_ref": ["b21"], "figure_ref": [], "table_ref": []}, {"heading": "F. Hitting Set & Algorithm 2", "text": "F.1 Greedy approach for minimum hitting set\nIn this section, we present the greedy weighted minimum hitting set algorithm mentioned in the main text . This greedy approach is depicted in Algorithm 11. Let V , F, and \u03c9(\u2022) be the universe of objects, the collection of sets for which we want to find a hitting set, and the weight function respectively. For an object v \u2208 V , we denote by N (v) the number of sets F \u2208 F such that v \u2208 F , that is, the number of sets v hits. We begin with an empty hitting set A. At each iteration, we choose the variable v \u2208 V that maximizes N (v) \u03c9(v) , and add it to A. We then remove all the sets F that include v from F. The algorithm runs until F becomes empty. The resulting set A is a hitting set for F. It has been shown that this greedy algorithm achieves a logarithmic-factor approximation of the optimal hitting set in the worst case Chvatal, 1979). Note that using certain data structures, we can avoid recalculating N (v) at each iteration in line (4).\nAlgorithm 11 Greedy weighted minimum hitting set algorithm. 1: input: universe V , collection of sets F, weights \u03c9(v) for v \u2208 V , output: a hitting set for F 2: while F \u0338 = \u2205 do 3: for all v \u2208 V do 4:\nN (v) \u2190 |{F \u2208 F|v \u2208 F }| 5: v \u2190 arg min v\u2208V N (v) \u03c9(v)", "publication_ref": ["b9"], "figure_ref": [], "table_ref": []}, {"heading": "6:", "text": "A \u2190 A \u222a {v} 7:\nF \u2190 F \\ {F \u2208 F|v \u2208 F } 8: return A", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F.2 On Algorithm 2", "text": "In this section, we provide a slight modification of Algorithm 2. One caveat to Algorithm 2 is that it might call numerous times as a subroutine, a solution to the minimum hitting set problem (line ( 13)). Although we propose using the greedy approach mentioned above as the subroutine, we also provide a modification, depicted as Algorithm 12, which reduces the number of calls to this subroutine as follows. At the end of each iteration (inner loop, that is, lines (7-13)), instead of solving the minimum hitting set problem, we simply add the vertex a found in the last step to a set of interventions A. We postpone the call to minimum hitting set to when A grows large enough so that {A} \u2208 ID G (S, V \\ S). Through this modification, we discover more hedges and add them to F before calling for the solution of the minimum hitting set problem. Therefore, this modification reduces the number of calls to the subroutine of solving the min hitting set in certain cases.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G. Further Empirical Evaluation", "text": "In this section, we provide further details of the experimental setup of the paper. We also provide complementary evaluations of our proposed algorithms.\nSetup. We have evaluated our algorithms in two different settings. In Appendix G.1, we evaluate our algorithms on a set of well-known graphs, which are the benchmark causal graphs in the causality literature. These graphs are obtained under the assumption of no latent variables. However, often the ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This research was in part supported by the Swiss National Science Foundation under NCCR Automation, grant agreement 51NF40 180545 and Swiss SNF project 200021 204355 /1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Algorithm 5 Polynomial time algorithm for tree-like structures. ). Now suppose we want to solve the min-cost intervention set problem for Q [{s}]. Take an arbitrary variable x \u0338 = s from Hhull(s). Let F be a hedge formed for Q [{s}] such that x \u2208 F . Since F is a c-component and x is an ancestor of s in G [F ] , all of the variables on both a p u \u2190\u2192 b and a p u \u2212\u2192 b must be members of F . We therefore call the union of all these variables, the necessary set of x to form a hedge for s, and we denote this set by nec s (x). Clearly, if we intervene on at least one vertex from nec s (x), then no hedge formed for Q[{s}] contains x. Further, we observe that if y \u2208 nec s (x), then with the same arguments, if a hedge formed for Q[{s}] contains x, it must contain y and therefore all the variables in nec s (y) as well. We define the closure of necessary variables for x to form a hedge for Q[{s}] as follows.\nDefinition 47 (Necessary closure). Let G be a semi-Markovian graph such that the edge induced subgraphs of G over its directed edges and over its bidirected edges are trees. We say a subset A of vertices of G is a closure of necessary variables for x to form a hedge for Q[{s}], if x \u2208 A, and for every y \u2208 A, nec s (y) \u2286 A. We denote the minimum closure of necessary variables for x by N C s (x).\nThe following lemma indicates that minimum closure of necessary variables for x is a hedge formed for Q[{s}].\nLemma 48. Let G be a semi-Markovian graph such that the edge induced subgraphs of G over its directed edges and over its bidirected edges are trees. For an arbitrary vertex s and a vertex\nAll of the proofs are provided in Appendix C. One observation is that to solve the min-cost intervention, we can enumerate N C s (x) for every x \u2208 Hhull(s) and solve the hitting set problem for these hedge. Although the number of such hedges is exactly |Hhull(s)| \u2212 1, the hitting set problem is still complex to solve. However, we can further reduce the complexity of the problem as follows. First note that if y \u2208 N C s (x), by definition of N C s (\u2022), N C s (y) \u2286 N C s (x). Therefore, when considering the hitting set problem, if N C s (y) is hit, N C s (x) will also be hit. As a result, we can eliminate N C s (x) from the sets we are considering. Using the same argument, we begin with some random ordering over the variables Hhull(S) and for every x \u2208 Hhull(S), if x appears in N C s (y) for some Algorithm 7 Polynomial time algorithm for special C(\u2022).\nwhile true do 10:  if I \u2208 ID 1 (S) then 16: return I hedges. However, the hitting set task still remains exponential in the worst case. On the other hand, for certain values of k, the min-cost intervention problem can be solved in polynomial time without using the hitting set formulation. For k = 1, the set of minimal hedges formed for Q[S] reduces to the hedge structures composed of S and one variable in pa \u2194 (S). From lemma 16, we know that in such a structure, the optimal intervention is A * = pa \u2194 (S), and pa \u2194 (S) can be constructed in linear time. In this section, we show that for k = 2, that is, given that every minimal hedge formed for Q[S] has size at most 2, the min-cost intervention problem can be solved in polynomial time. We begin with the following property of the formed hedges, which will help us model the min-cost intervention problem as a maximum matching problem in a bipartite graph through Konig's theorem (Konig, 1931).\nLemma 50. Let G be a semi-Markovian graph and S be a subset of its vertices such that G [S] is a c-component. Construct an undirected graph H on the same set of vertices as G \\ pa \u2194 (S) as follows. For any hedge of size 2 formed for Q[S] such as F , connect the two vertices in F \\ S with an edge. The resulting graph H is bipartite.\nFirst, note that for any hedge F formed for Q[S] in G [V \\pa \u2194 (S)] , there exists an edge between the two vertices F \\ S in the undirected graph H. Since the min-cost intervention to identify Q[S] in G is the union of pa \u2194 (S) and the minimum hitting set for the sets F \\ S (Lemma 15), the min-cost intervention can also be given as the union of pa \u2194 (S) and the minimum vertex cover for the undirected graph H. Lemma 50 states that H is bipartite. It is known that in bipartite graphs, the minimum-weight vertex cover problem is equivalent to a maximum matching (when the costs are uniform), or a maximum flow problem (when the costs are not uniform) (Konig, 1931). There are various polynomial time algorithms to solve these problems, such as Ford-Fulkerson, Edmonds-Karp and push-relabel algorithms to name a few (Ford and Fulkerson, 1956;Edmonds and Karp, 1972;Goldberg and Tarjan, 1988). Consequently, under the assumption that for any minimal hedge F formed for Q[S], |F | \u2212 |S| \u2264 2, we propose Algorithm 6 to solve the min-cost intervention problem in polynomial time. Any appropriate algorithm can be used as a subroutine in line (5) of Algorithm 6.", "publication_ref": ["b15", "b12", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "D.3 Special cost functions", "text": "We have discussed special graph structures so far. However, in certain cases, knowledge about the form of the cost function C(\u2022) can help us solve the min-cost intervention problem efficiently. One such case is when the costs of intervening on variables are far enough from each other. As a concrete example, let the vertices of G be v 1 , ..., v n , with the cost function C(v i ) = 2 i for 1 \u2264 i \u2264 n. We begin with testing the sets {v 1 }, {v 1 , v 2 }, ..., {v 1 , v 2 , ..., v n }, until we reach at the first set I j = {v 1 , ..., v j } \u2208 ID 1 (S). Since the cost of this intervention is C(I j ) = j i=1 2 i < 2 j+1 , the min-cost intervention does not include any of the variables v j+1 , ..., v n , as the cost of any of these variables is at least 2 j+1 . Further, as intervening on more variables cannot induce new hedges, and by definition of I j , no subset of {v 1 , ..., v j\u22121 } is in ID 1 (S). This implies that if A * is a min-cost intervention to identify Q[S] in G, then v j \u2208 A * and v l / \u2208 A * for any l > j. We then restart the procedure, testing the sets {v 1 }\u222a{v j }, {v 1 , v 2 }\u222a{v j }, ..., {v 1 , ..., v j\u22121 }\u222a{v j } to find the first set I k = {v 1 , ..., v k }\u222a{v j } \u2208 ID 1 (S). Again with the same arguments, we can conclude that v k \u2208 A * and v l / \u2208 A * \\ {v j } for any l > k. Continuing in the same manner, we construct the min-cost vertex cover (which is unique in this setting) after O(|V |) iterations in the worst case. Each iteration tests whether a set is a hedge at most |V | times, which can be performed using two depth-first searches (O(|V | 2 )). As a result, the min-cost intervention can be solved in time O(|V | 4 ) in the worst case.\nNote that the property we used throughout our reasoning was the fact that having sorted the variables based on their intervention costs as v 1 , ..., v n , for any 1\nWith such a cost function, Algorithm 7 solves the min-cost intervention problem in time O(|V | 4 ) in the worst case, i.e., regardless of the structure of G. Note that this algorithm has the same worst-case time complexity for both when G [S] is a c-component and when it is not. Also note that as an optional step, we can begin with constructing the hedge hull of S in\nis a c-component. In this case, sorting the variables in H based on their cost as h 1 , ..., h m , we only need the assumption that ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E. Heuristic Algorithms", "text": "In this section, we first present the three heuristic algorithms proposed in Section 3.5. We discuss their correctness, their running times, and how they compare to each other. Later, we propose a polynomial-time improvement that can be utilized as a post-process to improve the output of these algorithms.\nThe first heuristic algorithm is depicted as Algorithm 8. We begin with removing pa \u2194 (S) from the graph, as we already know that this set must be included in the output. We then build an undirected graph H over the vertices of H = Hhull(S, G [V \\pa \u2194 (S)] ), along with two extra vertices x and y. For every bidirected edge {v 1 , v 2 } in G [H] , we draw a corresponding edge between v 1 and v 2 in H. Finally, we connect x to pa(S) \u2229 H and y to S with an edge. Note that every undirected path between x and y in H corresponds to a bidirected path that connects a vertex in S to a vertex in pa(S) \u2229 H in G. If we intervene on a subset of variables A such that no such path exists anymore, the hedge hull of S in the remaining graph will be S itself, as none of the vertices pa(S) are in the same c-component of S. Consequently, the effect Q[S] becomes identifiable. With that being said, we solve for the minimum-weight vertex cut for x \u2212 y in H in line (4) of the algorithm. We set the weights of the vertices in S to infinity to ensure that we do not intervene on them. Note that the min-weight vertex cut in an undirected graph can be turned into an equivalent problem in a directed graph, by simply substituting every undirected edge with two directed edges in the opposite direction. Further, min-weight vertex cut can be reduced to min-weight edge cut through a trivial reduction: We replace every vertex v with two vertices v 1 , v 2 , add an edge from v 1 to v 2 with the same weight as the weight of v in the original graph, and connect every edge that goes into v to Lemma 52. Given a semi-Markovian graph G on V and a subset of its vertices S such that G [S] is a c-component, Algorithm 10 returns a set A such that {A} \u2208 ID G (S, V \\ S) in time O(|V | 5 ) in the worst case.\nAlgorithm 10 Heuristic greedy algorithm. Note that we Lemma 53 does not require that G [S] be a c-component, unlike Lemmas 26 and 27. As a result, all of these algorithms can also be utilized as a subroutine in line (7) of Algorithm 3, the general algorithm proposed in this work.\nPost-process. In many cases, when the output of the proposed heuristic algorithms is not optimal, it is a super-set of the optimal intervention. As a result, we propose greedily deleting such extra variables from the intervention set A while Q[S] remains identifiable. That is, assuming A is the output of one of the Algorithms 8,9,10, we start with the vertex a \u2208 A with the highest cost, and while there exists a \u2208 A \\ pa \u2194 (S) such that {A \\ {a}} \u2208 ID G (S, V \\ S), we remove a from A. Testing whether a set is in ID G (S, V \\ S) requires time O(|V | 3 ) in the worst case. As a result, the proposed post-process does not alter the worst-case complexity of the algorithms.\nDiscussion. The proposed algorithms have no theoretical guarantee of how well they can approximate the solution to the min-cost intervention problem. However, their performances as well as their runtimes are dependent on the structure of the graph G [H] , where H = Hhull(S, G [V \\pa \u2194 (S)] ). For instance, if the edge-induced subgraph of G [H] on its bidirected edges is much more dense than the edge-induced subgraph of G [H] on its directed edges, Algorithm 8 will need to solve a more complex min-weight vertex cover problem compared to Algorithm 9. It will also add potentially many extra vertices that are not needed in the intervention set. Since G [H] is constructed as a pre-process of all three algorithms, we propose choosing the heuristic algorithm after constructing G [H] as follows. Algorithm 9 is preferred over the other two, as it solves a min vertex cut in a directed graph rather Algorithm 12 Modified algorithm to reduce the calls to minimum hitting set. \nobserved variables of a system are confounded by a hidden variable. We added a common confounder for each pair of variables in these graphs with probability q. We then ran our algorithms to find the min-cost intervention for identifying Q[S], where S is the last vertex in the causal order. We assumed that the cost of intervening on each variable is uniformly sampled from {1, 2, 3, 4}.\nIn the second setting considered throughout our evaluations, we generated random graphs based on Erdos-Renyi generative model. The directed and bidirected edges of the graph in this model are sampled mutually independently, with probabilities p and q respectively. We then assigned a random cost of intervening to each variable, sampled from the uniform distribution over {1, 2, 3, 4}. Set S in these set of evaluations is randomly chosen among the last 5% vertices of the graph, such that G [S] is a c-component. Appendix G.2 provides empirical results of our algorithms on the randomly generated graphs. Finally, an evaluation of the hedge enumeration task of Algorithm 2 is given in Figure 5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G.1 Benchmark Structures", "text": "In this section, we evaluate our algorithms on graphs corresponding to real-world problems, namely the Barley (Kristensen andRasmussen, 1997), Water (Jensen et al., 1989) and Mehra (Vitolo et al., 2018) structures 15 . These structures are formed as causal DAGs under the assumption of no hidden confounder. However, often hidden variables confound observed variables. In our experiments, we randomly added a latent confounder for every pair of variables with probability q \u2208 {0.05, 0.15, 0.25, 0.35}, and evaluated the performance of our algorithms. The intervention costs are assigned uniformly at random from {1, 2, 3, 4}, and the set S is chosen to be the last vertex in the causal ordering. The results are depicted in Figure 10.\n15. See https://www.bnlearn.com/bnrepository/ for details.", "publication_ref": ["b34", "b49"], "figure_ref": [], "table_ref": []}, {"heading": "G.2 Randomly Generated Graphs", "text": "Figure 11 illustrates the runtime and the normalized regret (as defined in Section 6) of our algorithms on randomly generated graphs, with different values of p and q over random graphs of size n = 10 to n = 200. Figure 12 shows the effect of the density of the bidirected edges on the performance of the algorithms. Random graphs of size n = 30 are generated with different values of p. Figure 13 shows the effect of the density of the directed edges on the performance of the algorithms. Random graphs of size n = 30 are generated with different values of the parameter q. An important observation in all of these figures is that the normalized regret is not necessarily a monotone function of the graph size. This measure depends on the structure of the graph, size and location of the desired set S, and the random cost assignments. Another observation is that the runtime of the algorithms is not a monotone funciton of the graph density. This is due to the fact that the denser the graph becomes, the larger the set pa \u2194 (S) grows. As a result, the set H defined in Equation 3 becomes smaller and after a certain threshold, the problem becomes even simpler for denser graphs.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Efficient intervention design for causal discovery with latents", "journal": "PMLR", "year": "2020", "authors": "Raghavendra Addanki; Shiva Kasiviswanathan; Andrew Mcgregor; Cameron Musco"}, {"ref_id": "b1", "title": "Intervention efficient algorithms for approximate learning of causal graphs", "journal": "PMLR", "year": "2021", "authors": "Raghavendra Addanki; Andrew Mcgregor; Cameron Musco"}, {"ref_id": "b2", "title": "Abcdstrategy: Budgeted experimental design for targeted causal structure discovery", "journal": "PMLR", "year": "2019", "authors": "Raj Agrawal; Chandler Squires; Karren Yang; Karthikeyan Shanmugam; Caroline Uhler"}, {"ref_id": "b3", "title": "Network flows", "journal": "European Physical Journal B", "year": "1988", "authors": "K Ravindra;  Ahuja; L Thomas; James B Magnanti;  Orlin"}, {"ref_id": "b4", "title": "Recursive causal structure learning in the presence of latent variables and selection bias", "journal": "Advances in Neural Information Processing Systems", "year": "2021", "authors": "Sina Akbari; Ehsan Mokhtarian; Amiremad Ghassami; Negar Kiyavash"}, {"ref_id": "b5", "title": "Minimum cost intervention design for causal effect identification", "journal": "PMLR", "year": "2022", "authors": "Sina Akbari; Jalal Etesami; Negar Kiyavash"}, {"ref_id": "b6", "title": "Causal inference by surrogate experiments: z-identifiability", "journal": "", "year": "2012", "authors": "Elias Bareinboim; Judea Pearl"}, {"ref_id": "b7", "title": "Improved bounds on bell numbers and on moments of sums of random variables", "journal": "Probability and Mathematical Statistics", "year": "2010", "authors": "Daniel Berend; Tamir Tassa"}, {"ref_id": "b8", "title": "Combinatorial optimization: Theory and algorithms", "journal": "Springer", "year": "2005", "authors": "Korte Bernhard; Jens Vygen"}, {"ref_id": "b9", "title": "A greedy heuristic for the set-covering problem", "journal": "Mathematics of operations research", "year": "1979", "authors": "Vasek Chvatal"}, {"ref_id": "b10", "title": "Learning highdimensional directed acyclic graphs with latent and selection variables", "journal": "The Annals of Statistics", "year": "2012", "authors": "Diego Colombo; H Marloes; Markus Maathuis; Thomas S Kalisch;  Richardson"}, {"ref_id": "b11", "title": "On the hardness of approximating minimum vertex cover", "journal": "Annals of mathematics", "year": "2005", "authors": "Irit Dinur; Samuel Safra"}, {"ref_id": "b12", "title": "Theoretical improvements in algorithmic efficiency for network flow problems", "journal": "Journal of the ACM (JACM)", "year": "1972", "authors": "Jack Edmonds; M Richard;  Karp"}, {"ref_id": "b13", "title": "On the evolution of random graphs", "journal": "Publications of the Mathematical Institute of the Hungarian Academy of Sciences", "year": "1960", "authors": "Paul Erd\u0151s; Alfr\u00e9d R\u00e9nyi"}, {"ref_id": "b14", "title": "A threshold of ln n for approximating set cover", "journal": "Journal of the ACM (JACM)", "year": "1998", "authors": "Uriel Feige"}, {"ref_id": "b15", "title": "Maximal flow through a network", "journal": "Canadian journal of Mathematics", "year": "1956", "authors": "Randolph Lester; Delbert R Ford;  Fulkerson"}, {"ref_id": "b16", "title": "Causal inference and developmental psychology", "journal": "Developmental psychology", "year": "2010", "authors": "E Michael Foster"}, {"ref_id": "b17", "title": "Causal inference in sociological research", "journal": "Annual review of sociology", "year": "2010", "authors": "Markus Gangl"}, {"ref_id": "b18", "title": "Computers and intractability: a guide to the theory of NP-completeness", "journal": "W. H. Freeman", "year": "1979", "authors": "R Michael; David S Johnson Garey"}, {"ref_id": "b19", "title": "Some simplified np-complete problems", "journal": "", "year": "1974", "authors": "R Michael; David S Garey; Larry Johnson;  Stockmeyer"}, {"ref_id": "b20", "title": "Budgeted experiment design for causal structure learning", "journal": "PMLR", "year": "2018", "authors": "Amiremad Ghassami; Saber Salehkaleybar; Negar Kiyavash; Elias Bareinboim"}, {"ref_id": "b21", "title": "A new approach to the maximum-flow problem", "journal": "Journal of the ACM (JACM)", "year": "1988", "authors": "V Andrew; Robert E Goldberg;  Tarjan"}, {"ref_id": "b22", "title": "Two optimal strategies for active learning of causal models from interventional data", "journal": "International Journal of Approximate Reasoning", "year": "2014", "authors": "Alain Hauser; Peter B\u00fchlmann"}, {"ref_id": "b23", "title": "The logic of causal inference: Econometrics and the conditional analysis of causation", "journal": "Economics & Philosophy", "year": "1990", "authors": "D Kevin;  Hoover"}, {"ref_id": "b24", "title": "Pearl's calculus of intervention is complete", "journal": "", "year": "2006", "authors": "Yimin Huang; Marco Valtorta"}, {"ref_id": "b25", "title": "Causal identification under markov equivalence: Completeness results", "journal": "PMLR", "year": "2019", "authors": "Amin Jaber; Jiji Zhang; Elias Bareinboim"}, {"ref_id": "b26", "title": "An expert system for control of waste water treatment-a pilot project", "journal": "Judex Datasystemer A/S", "year": "1989", "authors": " Fv Jensen;  Kjaerulff; J Olesen;  Pedersen"}, {"ref_id": "b27", "title": "Approximation algorithms for combinatorial problems", "journal": "Journal of computer and system sciences", "year": "1974", "authors": "S David;  Johnson"}, {"ref_id": "b28", "title": "Minimum intervention cover of a causal graph", "journal": "", "year": "2019", "authors": "Saravanan Kandasamy; Arnab Bhattacharyya; G Vasant;  Honavar"}, {"ref_id": "b29", "title": "Reducibility among combinatorial problems", "journal": "Springer", "year": "1972", "authors": "M Richard;  Karp"}, {"ref_id": "b30", "title": "Revisiting the general identifiability problem", "journal": "", "year": "2022", "authors": "Yaroslav Kivva; Ehsan Mokhtarian; Jalal Etesami; Negar Kiyavash"}, {"ref_id": "b31", "title": "Graphok es matrixok (hungarian)[graphs and matrices", "journal": "", "year": "", "authors": "D\u00e9nes Konig"}, {"ref_id": "b32", "title": "", "journal": "", "year": "1931", "authors": "Lapok Matematikai\u00e9s Fizikai"}, {"ref_id": "b33", "title": "Minimum-cost flow algorithms: an experimental evaluation. Optimization Methods and Software", "journal": "", "year": "2015", "authors": "P\u00e9ter Kov\u00e1cs"}, {"ref_id": "b34", "title": "A decision support system for mechanical weed control in malting barley", "journal": "", "year": "1997", "authors": "Kristian Kristensen;  Rasmussen"}, {"ref_id": "b35", "title": "General identifiability with arbitrary surrogate experiments", "journal": "PMLR", "year": "2020", "authors": "Sanghack Lee; D Juan; Elias Correa;  Bareinboim"}, {"ref_id": "b36", "title": "Experimental design for cost-aware learning of causal graphs", "journal": "", "year": "2018", "authors": "M Erik; Murat Lindgren;  Kocaoglu; G Alexandros; Sriram Dimakis;  Vishwanath"}, {"ref_id": "b37", "title": "On the ratio of optimal integral and fractional covers", "journal": "Discrete mathematics", "year": "1975", "authors": "L\u00e1szl\u00f3 Lov\u00e1sz"}, {"ref_id": "b38", "title": "Methods matter: Improving causal inference in educational and social science research", "journal": "Oxford University Press", "year": "2010", "authors": "J Richard; John B Murnane;  Willett"}, {"ref_id": "b39", "title": "Combinatorial optimization: algorithms and complexity. Courier Corporation", "journal": "", "year": "1998", "authors": "H Christos; Kenneth Papadimitriou;  Steiglitz"}, {"ref_id": "b40", "title": "Causal diagrams for empirical research", "journal": "Biometrika", "year": "1995", "authors": "Judea Pearl"}, {"ref_id": "b41", "title": "The do-calculus revisited", "journal": "", "year": "2012", "authors": "Judea Pearl"}, {"ref_id": "b42", "title": "Models, reasoning and inference", "journal": "CambridgeUniversityPress", "year": "2000", "authors": "Judea Pearl"}, {"ref_id": "b43", "title": "A graphical approach to the identification and estimation of causal parameters in mortality studies with sustained exposure periods", "journal": "Journal of chronic diseases", "year": "1987", "authors": "James Robins"}, {"ref_id": "b44", "title": "Identification of joint interventional distributions in recursive semimarkovian causal models", "journal": "MIT Press", "year": "1999", "authors": "Ilya Shpitser; Judea Pearl"}, {"ref_id": "b45", "title": "Causation, prediction, and search", "journal": "MIT press", "year": "2000", "authors": "Peter Spirtes; N Clark; Richard Glymour; David Scheines;  Heckerman"}, {"ref_id": "b46", "title": "On the testable implications of causal models with hidden variables", "journal": "", "year": "2002", "authors": "Jin Tian; Judea Pearl"}, {"ref_id": "b47", "title": "Causal effect identification from multiple incomplete data sources: A general search-based approach", "journal": "", "year": "2019", "authors": "Santtu Tikka; Antti Hyttinen; Juha Karvanen"}, {"ref_id": "b48", "title": "Speeding-up linear programming using fast matrix multiplication", "journal": "IEEE Computer Society", "year": "1989", "authors": "M Pravin;  Vaidya"}, {"ref_id": "b49", "title": "Modeling air pollution, climate, and health data using bayesian networks: A case study of the english regions", "journal": "Earth and Space Science", "year": "2018", "authors": "Claudia Vitolo; Marco Scutari; Mohamed Ghalaieny; Allan Tucker; Andrew Russell"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: An example of a semi-Markovian graph. In this example, pa(x) = {v 3 , s 2 }, biD(x) = {v 3 , s 1 , v 1 }, and pa \u2194 (x) = {v 3 }.", "figure_data": ""}, {"figure_label": "19", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Lemma 19 .19Given a semi-Markovian graph G over V and a subset S \u2286 V such that G [S] is a c-component, Algorithm 1 returns Hhull(S, G) in O(|V | 3 ). Next theorem summarizes the results of this Section. Theorem 20. Let S be a subset of variables such that G [S] is a c-component. Then, A * S is a solution to (2) if and only if both pa \u2194 (S) \u2286 A * S and A * S \\ pa \u2194 (S) is a minimum-cost intervention to identify Q[S] in G [H] , where", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "DFS takes time O(|V | + |E|), where |E| is the number of edges. Therefore, Alg. 1 runs in time O(|V | 2 + |V | \u2022 |E|).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Vertex x is added to the parent set of all vertices in biD(S) \u2229 H, and all vertices of S are added to the parent set of y. The output of this algorithm is the minimum-weight vertex cut for x \u2212 y, with the weight function \u03c9(\u2022) := C(\u2022). Algorithm 9 in Appendix E summarizes this procedure. The following result indicates that intervening on the output set of this algorithm identifies Q[S]. Lemma 27. Let G be a semi-Markovian graph on V and S be a subset of V such that G [S] is a c-component. Heuristic Algorithm 2 returns an intervention set A in O(|V | 3 ) such that A \u2208 ID 1 (S).", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 2 :2Figure 2: An example where the optimal interventions collection is not a singleton, i.e., a solution to(2) is not a solution to (1). In this example, the cost of intervening on each of {s 1 , s 2 , s 3 } is 1, whereas the cost of intervening on each of {v 1 , v 2 , v 3 , v 4 } is 5.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 3 :3Figure 3: Minimum cost flow problem formulation for the general subset S, comprising maximal c-components S = S 1 \u222a S 2 \u222a S 3 . All the non-specified edges have capacity of 1 and cost of 0, i.e., \u03b3\\\u03b6 = 1\\0.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "S j , z) with capacity \u03b3 = 1 and cost \u03b6 = 0 7: for i from 1 to 2 k \u2212 1 do 8: compute A * i based on Eq. (5), using Algorithm 2 9: add edge (w, \u0393 i ) with capacity \u03b3 = |\u0393 i | and cost \u03b6 = C(A * i ) |\u0393i| 10:", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 4 :4Figure 4: Evaluation of our algorithms in terms of runtime (Left) and normalized regret (Right). nrepresents the number of vertices in the causal graph, Alg2-exact and Alg2-approx. solve the minimum hitting set exactly and approximately, respectively, and the greedy algorithm is presented in Appendix E. Alg2-exact obtains zero regret.", "figure_data": ""}, {"figure_label": "56", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 5 :Figure 6 :56Figure 5: Number of the total hedges formed for Q[S] vs the number of hedges Algorithm 2 discovers until finding the optimal solution.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "and F is a subset of G containing S. Also taking Remark 45 into consideration, a hedge formed for Q[S] in G can be thought of as the following structure, which is the definition used throughout this paper. Definition 14. (Hedge) Let G be a semi-Markovian graph and S be a subset of its vertices suchthat G [S] is a c-component. A subset F is a hedge formed for Q[S] in G if S \u228a F , F is the set of ancestors of S in G [F ] , and G [F ] is a c-component.Definitions 43 and 14 coincide when G [S] is a c-component, and we used Definition 14 to simplify the text. Claim 1. A hedge w.r.t. Definition 14 is formed for Q[S] if and only if a hedge w.r.t. Definition 43 is formed for Q[S]. Proof Let F be a hedge formed for Q[S] w.r.t. Definition 14. Taking F \u2032 = S, X = V \\ S, Y = S, and R = S, the pair F, F \u2032 forms a hedge for Q[S] = P X (Y ) w.r.t. Definition 43. Conversely, if the pair F, F \u2032 is a hedge by definition 43, as argued above, F \u2032 = R = S. In this case, by definition of R-rooted c-forest, F is the set of ancestors of S in G [F ] , and G [F ] is a c-component, which means that F forms a hedge for Q[S] w.r.t. Defintion 14.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "and define B \u2229 = \u2229 k i=1 B i . Also suppose that Q[S] is identifiable from the collection {Q[B 1 ], ..., Q[B m ]}. We claim that Q[S] is also identifiable from Q[B \u2229 ]. Suppose not. Then there exists two structural equation models M 1 and M 2 on the set of variables", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 7 :7Figure 7: Reduction from the weighted vertex cover problem to the minimum-cost intervention problem. Each edge {x, y} in the undirected graph H is represented by a hedge structure in the semi-Markovian graph G.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "then by definition, B is a hedge formed for Q[S] in G [V \\A] , and from Theorem 44, Q[S] is not identifiable in G [V \\A] , which is a contradiction. As a result, B \\ S = \u2205, or equivalently, pa \u2194 (S) \\ A = \u2205. Lemma 18. Consider A * S in Equation (2), then A * S \u2286 Hhull(S, G) \\ S. Proof By definition of hedge hull, all the hedges formed for Q[S] are a subset of Hhull(S). From Lemma 15, A * \u2286 Hhull(S). The result now follows from Lemma 6, which states that A * \u2229 S = \u2205. Lemma 19. Given a semi-Markovian graph G over V and a subset S \u2286", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "where H is given by Equation (3). Now let b be an arbitrary vertex in (F \\ S) \u2229 biD(S). Such a vertex exists by definition of hedge. Further, F are the ancestors of S in G [F ] , i.e., there exists a directed path from b to S through directed edges in G [F ] . As a result, in the directed graph J built in heuristic Algorithm 2, there exists a path from x to y that passes only through vertices in F . Any solution to minimum vertex cut for x \u2212 y must include at least one vertex of F . Therefore, F \u2229 A \u0338 = \u2205. Runtime. Heuristic Algorithm 2 begins with constructing the set pa \u2194 (S), and the set H given by Equation (3), which are performed in time O(|V |), and O(|V | 3 ) in the worst case. Constructing the graph J requires iterating over the directed edges of G [H]", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "C. 22Results Appearing in the Appendix Lemma 48. Let G be a semi-Markovian graph such that the edge induced subgraphs of G over its directed edges and over its bidirected edges are trees. For an arbitrary vertex s and a vertex x \u2208 Hhull(s) \\ {s}, N C s (x) is a hedge formed for Q[{s}] in G. Proof It suffices to show that G [N Cs(x)] is a c-component and every vertex in N C s (x) is an ancestor of s in G [N Cs(x)] . Take an arbitrary vertex y \u2208 N C s (x). By definition of N C s (\u2022), nec s (y) \u2286 N C s (x). As a result, y has both a directed and a bidirected path to s in G [N Cs(x)] , i.e., y is an ancestor of s in G [N Cs(x)] , and in the same c-component as s in G [N Cs(x)] . Repeating the same argument for every vertex in N C s (x) completes the proof. Lemma 49. Let G be a semi-Markovian graph over V such that the edge induced subgraphs of G over its directed edges and over its bidirected edges are trees. For a vertex s in G, Algorithm 5 returns the min-cost intervention to identify Q[{s}] in G in time O(|V | 3 ).", "figure_data": ""}, {"figure_label": "50", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "Lemma 50 .50Let G be a semi-Markovian graph and S be a subset of its vertices such that G [S] is a c-component. Construct an undirected graph H on the same set of vertices as G \\ pa \u2194 (S) as follows. For any hedge of size 2 formed for Q[S] such as F , connect the two vertices in F \\ S with an edge. The resulting graph H is bipartite.Proof First, let F = {a, b} \u222a S be a hedge formed for Q[S] in the graph G \\ pa \u2194 (S). By definition of hedge, both vertices a, b are ancestors of S in G[F ]  . Therefore, at least one of these vertices must be a parent of S. Without loss of generality, assume a \u2208 pa(S). Since a / \u2208 pa \u2194 (S), a does not have a bidirected edge to any vertex in S. However, by definition of hedge, F is a c-component. Therefore, a must have a bidirected edge to b, and b has a bidirected edge to S. Further, b / \u2208 pa \u2194 (S), and therefore b / \u2208 pa(S). Since b must be an ancestor of S in G [F ] , b \u2208 pa(a).", "figure_data": ""}, {"figure_label": "52", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "Lemma 52 .52Given a semi-Markovian graph G on V and a subset of its vertices S such that G [S] is a c-component, Algorithm 10 returns a set A such that {A} \u2208 ID G (S, V \\ S) in time O(|V | 5 ) in the worst case. Proof By construction, Algorithm 10 outputs a set A such that there is no hedge formed for Q[S] in G [V \\A] . As a result, {A} \u2208 ID G (S, V \\ S). It only suffices to show that the algorithm halts in time O(|V | 5 ). Constructing the hedge hull in line 1 is performed in cubic time in the worst case. The while loop of lines 3-12 can only be executed |H| times in the worst case, as at each iteration at least one vertex is removed from H. At each iteration of this loop, at most |H| hedge hulls are constructed, where each of these operations can be done in time O(|H| 3 ). Summing these up, the algorithm runs in time O(|V | 3 + |Hhull(S, G)| 5 ). Lemma 53. Given a semi-Markovian graph on V and a subset S of its vertices, Algorithms 8, 9 and 10 return a subset A of the vertices of G such that {A} \u2208 ID G (S, V \\ S), in time O(|V | 3 ), O(|V | 3 ) and O(|V | 5 ), respectively. Proof It is straightforward that the arguments used to prove the correctness of these algorithms for the case where G [S] is a c-component still hold for any maximal c-component of G [S] for an arbitrary subset S (see the proofs of Lemmas 26, 27 and 52.) Also, Q[S] is identifiable in G if and only if all of its maximal c-components are identifiable", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_17", "figure_caption": "1: H \u2190 empty undirected graph on V \\ pa \u2194 (S) 2: for any pair of vertices {a, b} \u2286 V \\ (S \u222a pa \u2194 (S)) do 3:if {a, b} \u222a S is a hedge formed for Q[S] then 4:", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_18", "figure_caption": "3. |F| \u2264 |Hhull(s)| 14 . Now we observe that the collection F of hedges are mutually disjoint, and thus the minimum hitting set is simply the union of the minimum cost vertex in each hedge. The following result indicates the correctness and the time complexity of algorithm 5, under the assumption that G has a tree-like structure.Lemma 49. Let G be a semi-Markovian graph over V such that the edge induced subgraphs of G over its directed edges and over its bidirected edges are trees. For a vertex s in G, Algorithm 5 returns the min-cost intervention to identify Q[{s}] in G in time O(|V | 3 ).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_19", "figure_caption": "1: input: G, S, C(\u2022), output: A \u2208 ID 1 (S) 2: H \u2190 Hhull(S, G V \\pa \u2194 (S) ) 3: Build H on H \u222a {x, y}: draw an undirected edge between v 1 , v 2 \u2208 H \\ S if there is a bidirected edge between them in G. Connect x to pa(S) \u2229 H and y to S.4: M C \u2190 minimum-weight vertex cut for x \u2212 y in H, with weights \u03c9(v) = C(v) for v / \u2208 S & \u03c9(s) = \u221e for s \u2208 S 5: A \u2190 M C \u222a pa \u2194 (S) 6: return A Algorithm 9 Heuristic algorithm 2. 1: input: G, S, C(\u2022), output: A \u2208 ID 1 (S) 2: H \u2190 Hhull(S, G V \\pa \u2194 (S) ) 3: Build H on H \u222a {x, y}: for v 1 , v 2 \u2208 H \\ S, draw v 1 \u2192 v 2 in H if this edge exists in G.Draw the edges from x to pa(S) \u2229 H and from S to y 4: M C \u2190 minimum-weight vertex cut for x \u2212 y in H, with weights \u03c9(v) = C(v) for v / \u2208 S & \u03c9(s) = \u221e for s \u2208 S 5: A \u2190 M C \u222a pa \u2194 (S) 6: return A v 1", "figure_data": ""}, {"figure_label": "10111213", "figure_type": "figure", "figure_id": "fig_20", "figure_caption": "Figure 10 :Figure 11 :Figure 12 :Figure 13 :10111213Figure 10: The performance of the proposed algorithms on three real-world structures.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Find Hhull(S, G), where G [S] is a c-component.", "figure_data": "Algorithm 1 1: function Hhull(S, G)2:Definition 17 (Hedge hull). Let G be a semi-Markovian graph and S be a subset of its vertices suchthat G [S] is a c-component. The union of all hedges formed for Q[S] is called hedge hull of S anddenoted by Hhull(S, G)."}], "formulas": [{"formula_id": "formula_0", "formula_text": "s 1 s 2 x v 3 v 2 v 1", "formula_coordinates": [3.0, 216.33, 91.78, 178.77, 90.67]}, {"formula_id": "formula_1", "formula_text": "C(x) = \u221e.", "formula_coordinates": [4.0, 90.0, 362.11, 48.02, 8.77]}, {"formula_id": "formula_2", "formula_text": "A = {A 1 , A 2 , ..., A m } of subsets of V with minimum C(A) such that P (S|do(T )) is identifiable in G given {Q[V \\ A 1 ], ..., Q[V \\ A m ]}.", "formula_coordinates": [4.0, 89.57, 391.23, 432.43, 21.64]}, {"formula_id": "formula_3", "formula_text": "A i \u2286 V, 1 \u2264 i \u2264 m, such that P (S|do(T )) is identifiable in G given {Q[V \\ A 1 ], ..., Q[V \\ A m ]}. Note that |ID G (S, T )| \u2264 2 2 |V | .", "formula_coordinates": [4.0, 90.0, 415.17, 432.0, 23.5]}, {"formula_id": "formula_4", "formula_text": "A * S,T \u2208 arg min A\u2208ID G (S,T ) A\u2208A C(A).(1)", "formula_coordinates": [4.0, 234.83, 464.25, 288.34, 19.67]}, {"formula_id": "formula_5", "formula_text": ". That is, ID G (S, T ) = ID G (Anc G\\T (S), V \\ Anc G\\T (S)).", "formula_coordinates": [4.0, 273.31, 550.0, 250.62, 9.99]}, {"formula_id": "formula_6", "formula_text": "A \u222a := \u222a m i=1 A i . If A \u2208 ID G (S, V \\ S), then the singleton collection A \u222a = {A \u222a } also belongs to ID G (S, V \\ S). Remark 4. The cost of A \u222a in Lemma 3 is at most C(A), C(A) = Ai\u2208A a\u2208Ai C(a) \u2265 a\u2208A\u222a C(a) = C(A \u222a ),", "formula_coordinates": [5.0, 90.0, 170.46, 432.0, 73.65]}, {"formula_id": "formula_7", "formula_text": "Theorem 5. Suppose S is a subset of variables such that G [S] is a c-component. Let A = {A 1 , A 2 , ..., A m } be a collection of subsets such that A \u2208 ID G (S, V \\ S) and m > 1. Then, there exists a subset\u00c3 \u2286 V such that\u00c3 = {\u00c3} \u2208 ID G (S, V \\ S) and C(\u00c3) \u2264 C(A).", "formula_coordinates": [5.0, 89.18, 314.08, 432.82, 33.59]}, {"formula_id": "formula_8", "formula_text": "A * such that Q[S] is identifiable from Q[V \\ A * ].", "formula_coordinates": [5.0, 90.0, 366.64, 432.0, 22.27]}, {"formula_id": "formula_9", "formula_text": "A * S \u2208 arg min A\u2208ID1(S) a\u2208A C(a),(2)", "formula_coordinates": [5.0, 246.19, 399.9, 276.97, 19.06]}, {"formula_id": "formula_10", "formula_text": "A of V such that Q[S] is identifiable from Q[V \\A]. Note that |ID 1 (S)| \u2264 2 |V | .", "formula_coordinates": [5.0, 90.0, 429.87, 432.27, 21.61]}, {"formula_id": "formula_11", "formula_text": "A of V , such that Q[S] is identifiable from Q[V \\ A] and A \u2229 S = \u2205. Lemma 6. Suppose S is a subset of variables such that G [S] is a c-component. If A \u2208 ID 1 (S), then A \u2229 S = \u2205.", "formula_coordinates": [5.0, 90.0, 477.69, 432.0, 50.89]}, {"formula_id": "formula_12", "formula_text": "\u03c9 : V \u2192 R \u22650 . Given a collection of subsets of V such as F = {F 1 , ..., F k }, F i \u2286 V, 1 \u2264 i \u2264 k, a hitting set for F is a subset A \u2286 V such that A hits all the sets in F, i.e., for any 1 \u2264 i \u2264 k, A \u2229 F i \u0338 = \u2205.", "formula_coordinates": [6.0, 89.24, 362.8, 434.14, 35.76]}, {"formula_id": "formula_13", "formula_text": "[S] is a c-component. A subset F is a hedge formed for Q[S] in G if S \u228a F , F is the set of ancestors of S in G [F ] , and G [F ] is a c-component.", "formula_coordinates": [7.0, 89.24, 173.72, 432.0, 21.91]}, {"formula_id": "formula_14", "formula_text": "S = {s 1 , s 2 } in the causal graph of Figure 1. In this case, G [S] is a c-component and {s 1 , s 2 , v 1 , v 2 } and {s 1 , s 2 , v 2 } are two hedges formed for Q[S].", "formula_coordinates": [7.0, 90.0, 205.6, 432.0, 21.61]}, {"formula_id": "formula_15", "formula_text": "C : V \u2192 R \u22650 . Let S be a subset of V such that G [S] is a c-component. Suppose the set of all hedges formed for Q[S] in G is {F 1 , ..., F m }. Then A *", "formula_coordinates": [7.0, 90.0, 271.15, 432.0, 23.81]}, {"formula_id": "formula_16", "formula_text": "F 1 \u2190 connected component of S via bidirected edges in G [F ] 5: F 2 \u2190 ancestors of S in G [F1] 6:", "formula_coordinates": [8.0, 95.37, 147.57, 304.8, 32.26]}, {"formula_id": "formula_17", "formula_text": "F \u2190 F 2 8: else 9: return F If G [S]", "formula_coordinates": [8.0, 95.37, 183.44, 98.0, 68.14]}, {"formula_id": "formula_18", "formula_text": "[S1] , ..., G [S k ] are the maximal c-components of G [S] . We define Hhull(S, G) as Hhull(S, G) = k i=1 Hhull(S i , G). Lemma 18. Consider A * S in Equation (2), then A * S \u2286 Hhull(S, G) \\ S.", "formula_coordinates": [8.0, 90.0, 253.58, 432.0, 45.22]}, {"formula_id": "formula_19", "formula_text": "H := Hhull(S, G [V \\pa \u2194 (S)] ).(3)", "formula_coordinates": [8.0, 245.43, 497.11, 277.73, 9.96]}, {"formula_id": "formula_20", "formula_text": "Q[S] in G [H] , where H is given in Equation (3). Note that all the minimal hedges of S in G [H] can be enumerated in O(2 (|H|\u2212|S|) ).", "formula_coordinates": [8.0, 90.0, 530.98, 433.93, 23.34]}, {"formula_id": "formula_21", "formula_text": "H \u2190 Hhull(S, G [V \\(A\u222apa \u2194 (S))] )", "formula_coordinates": [9.0, 136.83, 311.52, 136.97, 9.96]}, {"formula_id": "formula_22", "formula_text": "A \u222a pa \u2194 (S) \u222a Hhull(S, G [V \\(A\u222apa \u2194 (S))] ) \\ S,(4)", "formula_coordinates": [10.0, 208.45, 366.07, 314.72, 12.03]}, {"formula_id": "formula_23", "formula_text": "C(A * ) \u2264 C(A \u222a pa \u2194 (S) \u222a H) \u2264 C(A * ) + C(H).", "formula_coordinates": [10.0, 200.22, 473.51, 211.56, 10.81]}, {"formula_id": "formula_24", "formula_text": "Q[S] in G [H] .", "formula_coordinates": [10.0, 90.0, 647.68, 57.37, 9.96]}, {"formula_id": "formula_25", "formula_text": "Lemma 26. Let G be a semi-Markovian graph on V and S be a subset of V such that G [S] is a c-component. Heuristic Algorithm 1 returns an intervention set A in O(|V | 3 ) such that A \u2208 ID 1 (S).", "formula_coordinates": [11.0, 89.21, 312.22, 434.16, 21.64]}, {"formula_id": "formula_26", "formula_text": "{v 1 , v 2 } \u2208 H, if v 1 is a parent of v 2 in G, then v 1 will be a parent of v 2 in J .", "formula_coordinates": [11.0, 90.0, 367.56, 432.0, 21.61]}, {"formula_id": "formula_27", "formula_text": "E[c 1 ] \u2264 q \u22121 E[c * ], E[c 2 ] \u2264 p \u22121 E[c * ]. 8. See Appendix E for details. s 1 s 2 s 3 v 2 v 1 v 3 v 4", "formula_coordinates": [11.0, 94.59, 649.89, 247.78, 54.52]}, {"formula_id": "formula_28", "formula_text": "E[c * ] \u2264 E[c] \u2264 min{p \u22121 , q \u22121 }E[c * ].", "formula_coordinates": [12.0, 229.18, 328.18, 153.65, 12.01]}, {"formula_id": "formula_29", "formula_text": "[S] for S := {s 1 , s 2 , s 3 }, is A * S,V \\S = {{s 1 }, {s 2 }} with the cost C(s 1 ) + C(s 2 ) = 2.", "formula_coordinates": [12.0, 89.57, 522.06, 432.44, 23.32]}, {"formula_id": "formula_30", "formula_text": "1: function NaiveMinCost(S, G, C(\u2022)) 2: A * \u2190 null 3: minCost \u2190 \u221e 4: {S 1 , ..., S k } \u2190 maximal c-components of G [S] 5:", "formula_coordinates": [13.0, 95.37, 121.48, 221.61, 57.63]}, {"formula_id": "formula_31", "formula_text": "A \u2190 {} 7: cost \u2190 0 8: for i from 1 to t do 9: A i \u2190 min-cost intervention set in ID 1 (S (i) ) 10: A \u2190 A \u222a {A i } 11: cost \u2190 cost +C(A i ) 12:", "formula_coordinates": [13.0, 91.13, 182.68, 252.6, 82.6]}, {"formula_id": "formula_32", "formula_text": "A * \u2190 A 14: minCost \u2190 cost 15: return A *", "formula_coordinates": [13.0, 91.13, 267.31, 131.04, 34.22]}, {"formula_id": "formula_33", "formula_text": "S i = S and G [S1] , ..., G [S k ] are the maximal c-components of G [S] . It is known that Q[S] is identifiable in G if and only if Q[S i ]s are identifiable in G for all 1 \u2264 i \u2264 k (", "formula_coordinates": [13.0, 90.0, 338.55, 432.0, 32.65]}, {"formula_id": "formula_34", "formula_text": "A * S,V \\S is a min-cost interventions collection to identify Q[S] in G. If G [Sj ] is a maximal c-component of G [S]", "formula_coordinates": [13.0, 89.36, 380.34, 431.87, 36.91]}, {"formula_id": "formula_35", "formula_text": ", set S = {s 1 , s 2 , s 3 } consists of two maximal c-components G [S1] and G [S2]", "formula_coordinates": [13.0, 90.0, 468.38, 432.0, 21.91]}, {"formula_id": "formula_36", "formula_text": "A j for 1 \u2264 j \u2264 t. More precisely, if S i \u2208 S (j) , then A j \u2208 ID 1 (S i ). Lemma 31. Suppose A * S,V \\S = {A 1 , . . . , A t } is the minimum-cost intervention to identify Q[S]. Let S (1) , . . . , S (t) be the partitioning of maximal c-components of G [S]", "formula_coordinates": [13.0, 90.0, 545.02, 431.97, 45.77]}, {"formula_id": "formula_37", "formula_text": "A j \u2208 arg min A\u2208ID1(S (j) ) C(A), \u22001 \u2264 j \u2264 t.", "formula_coordinates": [13.0, 221.81, 615.51, 168.39, 17.82]}, {"formula_id": "formula_38", "formula_text": "Let \u0393 = {S 1 , ..., S k } be the subsets of S such that G [Sj ] for 1 \u2264 j \u2264 k is a maximal c-component in G [S] . Also let \u0393 1 , . . . , \u0393 2 k \u22121 = \u0393 m\u22121", "formula_coordinates": [14.0, 90.0, 321.27, 432.0, 22.13]}, {"formula_id": "formula_39", "formula_text": "A * i = arg min A\u2208ID1(\u222a S j \u2208\u0393 i Sj ) C(A), \u22001 \u2264 i \u2264 (m \u2212 1),(5)", "formula_coordinates": [14.0, 201.82, 352.98, 321.35, 21.28]}, {"formula_id": "formula_40", "formula_text": "S 1 S 2 S 3 \u0393 1 = {S 1 } \u0393 3 = {S 2 } \u0393 7 = {S 3 } \u0393 2 = {S 1 , S 2 } \u0393 5 = {S 1 , S 3 } \u0393 6 = {S 2 , S 3 } \u0393 4 = {S 1 , S 2 , S 3 } w z 1 \\ C ( A * 1 ) 1\\ C (A * 3 ) 1 \\ C ( A * 7 ) 2 \\ C (A * 2 ) 2 2\\ C (A * 5 ) 2 2 \\ C (A * 6 ) 2 3\\ C(A * 4 ) 3", "formula_coordinates": [15.0, 170.26, 93.66, 271.18, 185.59]}, {"formula_id": "formula_41", "formula_text": "min f (\u2022,\u2022) (x,y)\u2208E f (x, y)\u03b6(x, y), s.t. 0 \u2264 f (x, y) \u2264 \u03b3(x, y), \u2200(x, y) \u2208 E, x\u2208V f (x, v) \u2212 y\u2208V f (v, y) = d(v), \u2200v \u2208 V, where d(z) = \u2212d(w) = d * , and d(v) = 0 for v \u0338 = w, z.", "formula_coordinates": [15.0, 89.35, 481.07, 309.23, 101.87]}, {"formula_id": "formula_42", "formula_text": "C(A * i )", "formula_coordinates": [15.0, 437.71, 651.51, 22.88, 9.37]}, {"formula_id": "formula_43", "formula_text": "2: \u0393 = {S 1 , ..., S k } \u2190 maximal c-components of G [S] 3: \u0393 1 , . . . , \u0393 2 k \u22121 \u2190 non-empty subsets of \u0393 4:", "formula_coordinates": [16.0, 95.37, 132.91, 241.12, 32.26]}, {"formula_id": "formula_44", "formula_text": "12:B \u2190 {\u0393 i |f * (w, \u0393 i ) > 0} 13: B * \u2190 {\u0393 i \u2032 = {S j |1 \u2264 j \u2264 k, f * (\u0393 i , S j ) > 0}|\u0393 i \u2208B} 14: A \u2190 {A * i \u2032 |\u0393 i \u2032 \u2208 B * } 15: return A Remark 36.", "formula_coordinates": [16.0, 90.0, 272.27, 257.29, 77.59]}, {"formula_id": "formula_45", "formula_text": "B * = {\u0393 i \u2032 = {S j |1 \u2264 j \u2264 k, f * (\u0393 i , S j ) > 0}|1 \u2264 i \u2264 t}.", "formula_coordinates": [16.0, 188.43, 458.15, 235.14, 11.72]}, {"formula_id": "formula_46", "formula_text": "A * i \u2032 s that their corresponding \u0393 i \u2032 appears in B * . That is, A = {A * i \u2032 |\u0393 i \u2032 \u2208 B * }.", "formula_coordinates": [16.0, 90.0, 488.95, 432.0, 24.43]}, {"formula_id": "formula_47", "formula_text": "A \u2208 ID G (S, V \\ S), such that C(A) \u2264 kC(A * S,V \\S ), where k and A * S,V \\S are the number of c-components in G [S]", "formula_coordinates": [16.0, 89.35, 541.88, 434.03, 24.9]}, {"formula_id": "formula_48", "formula_text": "A \u2032 \u2286 V , C(A \u222a {A \u2032 }) \u2265 C(A).", "formula_coordinates": [17.0, 90.0, 327.56, 264.36, 22.27]}, {"formula_id": "formula_49", "formula_text": "F \u2032 = R = S,", "formula_coordinates": [25.0, 90.0, 165.94, 57.95, 10.31]}, {"formula_id": "formula_50", "formula_text": "[H] . Then Q[H] is identifiable in G, if and only if Q[H 1 ], ..., Q[H k ] are identifiable in G.", "formula_coordinates": [25.0, 89.36, 416.28, 432.64, 21.61]}, {"formula_id": "formula_51", "formula_text": "If (Y \u22a5 \u22a5 Z|X, W ) G XZ , then P (Y |do(X, Z), W ) = P (Y |do(X), Z, W ). Rule 3 ) Insertion or deletion of actions. If (Y \u22a5 \u22a5 Z|X, W ) G XZ(W ) , where Z(W ) are vertices in Z that have no descendants in W in G X , then P (Y |do(X, Z), W ) = P (Y |do(X), W ).", "formula_coordinates": [25.0, 89.45, 546.36, 432.82, 89.12]}, {"formula_id": "formula_52", "formula_text": "A \u222a := \u222a m i=1 A i . If A \u2208 ID G (S, V \\ S), then the singleton collection A \u222a = {A \u222a } also belongs to ID G (S, V \\ S). Proof Suppose B i = V \\ A i for 1 \u2264 i \u2264 m,", "formula_coordinates": [26.0, 90.0, 122.12, 432.0, 43.11]}, {"formula_id": "formula_53", "formula_text": "V such that Q M1 [B \u2229 ] = Q M2 [B \u2229 ], but Q M1 [S] \u0338 = Q M2 [S]", "formula_coordinates": [26.0, 130.72, 189.86, 246.36, 11.23]}, {"formula_id": "formula_54", "formula_text": "M \u2032 j as in M j . Both in M \u2032 1 and M \u2032 2 , any x / \u2208 B \u2229 , x is uniformly distributed in D(x)", "formula_coordinates": [26.0, 90.0, 225.73, 362.69, 12.33]}, {"formula_id": "formula_55", "formula_text": "Q M \u2032 1 [B i ] = Q M \u2032 1 [B \u2229 ] \u2022 \u03a0 x\u2208Bi\\B\u2229 Q M \u2032 1 [x] = Q M1 [B \u2229 ] \u2022 \u03a0 x\u2208Bi\\B\u2229 1 |D(x)| = Q M2 [B \u2229 ] \u2022 \u03a0 x\u2208Bi\\B\u2229 1 |D(x)| = Q M \u2032 2 [B \u2229 ] \u2022 \u03a0 x\u2208Bi\\B\u2229 Q M \u2032 2 [x] = Q M \u2032 2 [B i ],(6)", "formula_coordinates": [26.0, 221.51, 269.7, 301.66, 101.23]}, {"formula_id": "formula_56", "formula_text": "Q M1 [B \u2229 ] = Q M2 [B \u2229 ]", "formula_coordinates": [26.0, 360.45, 393.12, 91.21, 11.23]}, {"formula_id": "formula_57", "formula_text": "Q M \u2032 1 [S] = Q M1 [S] \u0338 = Q M2 [S] = Q M \u2032 2 [S].(7)", "formula_coordinates": [26.0, 220.54, 436.8, 302.63, 12.46]}, {"formula_id": "formula_58", "formula_text": ", A = {A \u222a } \u2208 ID G (S, V \\ S). Theorem 5. Suppose S is a subset of variables such that G [S] is a c-component. Let A = {A 1 , A 2 , ..., A m } be a collection of subsets such that A \u2208 ID G (S, V \\ S) and m > 1. Then, there exists a subset\u00c3 \u2286 V such that\u00c3 = {\u00c3} \u2208 ID G (S, V \\ S) and C(\u00c3) \u2264 C(A).", "formula_coordinates": [26.0, 89.18, 486.32, 432.82, 71.14]}, {"formula_id": "formula_59", "formula_text": "A i \u2229 S = \u2205 for 1 \u2264 i \u2264 k, and A i \u2229 S \u0338 = \u2205 for k < i \u2264 m, for some integer k. We first claim that the following collection is in ID G (S, V \\ S). A = {A 1 , ..., A k } \u2208 ID G (S, V \\ S).", "formula_coordinates": [26.0, 90.0, 567.73, 647.4, 43.52]}, {"formula_id": "formula_60", "formula_text": "A = {A \u222a } \u2208 ID G (S, V \\ S).", "formula_coordinates": [26.0, 245.28, 681.28, 121.44, 9.68]}, {"formula_id": "formula_61", "formula_text": "C(\u00c3) = C(A \u222a ) = a\u2208A\u222a C(a) \u2264 k i=1 a\u2208Ai C(a) = k i=1 C(A i ) = C(\u00c2) \u2264 C(A).", "formula_coordinates": [27.0, 208.22, 316.56, 195.57, 66.5]}, {"formula_id": "formula_62", "formula_text": "M i , for i \u2208 {1, 2}. Clearly, Q M1 [V \\ A] = Q M2 [V \\ A] = 1 2 |V |\u2212|A| , whereas Q M1 [S s=0 ] = 1 2 |S| \u0338 = 1 0.4 * 2 |S|\u22121 = Q M2 [S s=0 ], which shows that Q[S] is not identifiable in G [V \\A] .", "formula_coordinates": [27.0, 90.0, 496.44, 433.38, 33.87]}, {"formula_id": "formula_63", "formula_text": "V = V H \u222a U \u222a W \u222a {s}) is therefore equal to (|V H | + 2|E H | + 1).", "formula_coordinates": [27.0, 90.0, 684.1, 273.6, 9.65]}, {"formula_id": "formula_64", "formula_text": "z = |V H | \u2022 max x\u2208V H \u03c9(x) + 1.", "formula_coordinates": [28.0, 252.58, 161.93, 106.85, 15.2]}, {"formula_id": "formula_65", "formula_text": "Q[{s}] = P (s|do(V H , U, W )) = P (s|W, do(V H , U )) (do calculus rule 2) = P (s|W, do(V H )).", "formula_coordinates": [28.0, 197.07, 266.18, 216.35, 39.54]}, {"formula_id": "formula_66", "formula_text": "C(V H ) \u2264 |V H |\u2022max x\u2208V H C(x) \u2264 (z \u2212 1)", "formula_coordinates": [28.0, 88.83, 352.09, 433.17, 20.72]}, {"formula_id": "formula_68", "formula_text": "{A} \u2208 ID G ({s}, V \\ {s}).", "formula_coordinates": [29.0, 403.89, 346.26, 110.08, 9.68]}, {"formula_id": "formula_69", "formula_text": "\u2264 i \u2264 k, A \u2229 F i \u0338 = \u2205.", "formula_coordinates": [29.0, 370.19, 475.49, 93.74, 9.65]}, {"formula_id": "formula_70", "formula_text": "v 1 \u227a v 2 \u227a \u2022 \u2022 \u2022 \u227a v n .", "formula_coordinates": [29.0, 440.9, 523.31, 83.04, 9.65]}, {"formula_id": "formula_71", "formula_text": "F i = {f i,1 \u227a ... \u227a f i,m } \u2286 {v 1 \u227a \u2022 \u2022 \u2022 \u227a v n },", "formula_coordinates": [29.0, 331.22, 559.18, 183.75, 9.65]}, {"formula_id": "formula_72", "formula_text": "s f \u2032 1,4 f \u2032 1,3 f \u2032 1,2 f \u2032 1,1 f 1,1 f 1,2 f 1,3", "formula_coordinates": [30.0, 265.0, 115.61, 93.9, 189.18]}, {"formula_id": "formula_73", "formula_text": "F 1 = {f 1,1 \u227a f 1,2 \u227a f 1,3 } is visualized in Figure", "formula_coordinates": [30.0, 145.32, 398.95, 209.09, 9.65]}, {"formula_id": "formula_74", "formula_text": "|V | + k i=1 (|F i | + 1)", "formula_coordinates": [30.0, 106.87, 433.23, 89.18, 14.11]}, {"formula_id": "formula_75", "formula_text": "F i = {f i,1 , . . . , f i,m }, the set X i = F i \u222a {f \u2032 i,j |1 \u2264 j \u2264 (m + 1)} \u222a {s} forms a hedge for Q[{s}].", "formula_coordinates": [30.0, 90.0, 470.49, 432.0, 22.27]}, {"formula_id": "formula_76", "formula_text": "f \u2032 i,1 , which means G [Xi] is a c-component.", "formula_coordinates": [30.0, 90.0, 530.27, 432.0, 23.49]}, {"formula_id": "formula_77", "formula_text": "F i \u0338 = \u2205 for every 1 \u2264 i \u2264 m.", "formula_coordinates": [30.0, 288.35, 603.57, 119.37, 9.65]}, {"formula_id": "formula_78", "formula_text": "C : V \u2192 R \u22650 . Let S be a subset of V such that G [S] is a c-component. Suppose the set of all hedges formed for Q[S] in G is {F 1 , ..., F m }. Then A *", "formula_coordinates": [31.0, 90.0, 212.87, 432.0, 23.81]}, {"formula_id": "formula_79", "formula_text": "S \u2286 V , let G [S] be a c-component. For any subset A \u2286 V , if A \u2208 ID 1 (S), then pa \u2194 (S) \u2286 A.", "formula_coordinates": [31.0, 89.21, 355.87, 432.75, 21.61]}, {"formula_id": "formula_80", "formula_text": "S \u2229 A = \u2205. Now define B = S \u222a (pa \u2194 (S) \\ A). If B \\ S \u0338 = \u2205,", "formula_coordinates": [31.0, 90.0, 386.0, 432.0, 22.27]}, {"formula_id": "formula_81", "formula_text": "V such that G [S] is a c-component, Algorithm 1 returns Hhull(S, G) in O(|V | 3 ).", "formula_coordinates": [31.0, 89.21, 529.87, 432.75, 20.69]}, {"formula_id": "formula_82", "formula_text": "Q[S] in G [H]", "formula_coordinates": [32.0, 90.0, 191.35, 53.37, 9.96]}, {"formula_id": "formula_83", "formula_text": "H := Hhull(S, G [V \\pa \u2194 (S)] ).", "formula_coordinates": [32.0, 245.43, 213.26, 121.14, 9.96]}, {"formula_id": "formula_84", "formula_text": ")3", "formula_coordinates": [32.0, 514.68, 213.26, 8.49, 8.74]}, {"formula_id": "formula_85", "formula_text": "Q[S] in G [H \u2032 ] is then F 2 .", "formula_coordinates": [32.0, 280.29, 259.06, 108.37, 9.99]}, {"formula_id": "formula_86", "formula_text": "C(\u00c3 \u222a pa \u2194 (S)) \u2264 C(\u00c3) + C(pa \u2194 (S)) < C(A * \\ pa \u2194 (S)) + C(pa \u2194 (S)) = C(A * ),", "formula_coordinates": [32.0, 197.02, 375.28, 217.95, 40.7]}, {"formula_id": "formula_87", "formula_text": "C(\u00c3 \\ pa \u2194 (S)) = C(\u00c3) \u2212 C(pa \u2194 (S)) < C(A * ) \u2212 C(pa \u2194 (S)) = C(A * \\ pa \u2194 (S)), which contradicts the fact that A * \\ pa \u2194 (S) is the minimum-cost intervention to identify Q[S] in G [H \u2032 ] .", "formula_coordinates": [32.0, 89.64, 497.63, 432.36, 75.89]}, {"formula_id": "formula_88", "formula_text": "Q[S] in G. Now suppose G [S] is not a c-component. G [S] can be uniquely partitioned into its maximal c-components G [S1] , ..., .G [S k ] .", "formula_coordinates": [33.0, 90.0, 275.09, 432.0, 34.54]}, {"formula_id": "formula_89", "formula_text": "H = Hhull(S, G [V \\(A\u222apa \u2194 (S))] ) \\ S. Then, (A \u222a pa \u2194 (S) \u222a H) \u2208 ID 1 (S), and C(A * ) \u2264 C(A \u222a pa \u2194 (S) \u222a H) \u2264 C(A * ) + C(H).", "formula_coordinates": [33.0, 188.2, 382.45, 333.51, 31.79]}, {"formula_id": "formula_90", "formula_text": "A \u222a pa \u2194 (S) \u222a H \u2208 ID 1 (S).", "formula_coordinates": [33.0, 90.0, 485.19, 121.03, 11.23]}, {"formula_id": "formula_91", "formula_text": "C(A \u222a pa \u2194 (S) \u222a H) = C(A) + C(pa \u2194 (S)) + C(H) \u2264 C(A * \\ pa \u2194 (S))) + C(pa \u2194 (S)) + C(H) = C(A * ) + C(H) Lemma 26. Let G be a semi-Markovian graph on V and S be a subset of V such that G [S] is a c-component. Heuristic Algorithm 1 returns an intervention set A in O(|V | 3 ) such that A \u2208 ID 1 (S).", "formula_coordinates": [33.0, 89.21, 565.85, 434.16, 96.37]}, {"formula_id": "formula_92", "formula_text": "F \u2229 A \u0338 = \u2205. If F \u2229 pa \u2194 (S) \u0338 = \u2205, then the claim holds since pa \u2194 (S) \u2286 A.", "formula_coordinates": [33.0, 196.85, 694.48, 327.09, 10.31]}, {"formula_id": "formula_93", "formula_text": "Lemma 27. Let G be a semi-Markovian graph on V and S be a subset of V such that G [S] is a c-component. Heuristic Algorithm 2 returns an intervention set A in O(|V | 3 ) such that A \u2208 ID 1 (S).", "formula_coordinates": [34.0, 89.21, 286.96, 434.16, 21.64]}, {"formula_id": "formula_94", "formula_text": "[S] in G. It suffices to show that F \u2229 A \u0338 = \u2205. If F \u2229 pa \u2194 (S) \u0338 = \u2205, then the claim holds since pa \u2194 (S) \u2286 A. Otherwise, F is a hedge formed for Q[S] in G [V \\pa \u2194 (S)] , i.e., F \u2286 H,", "formula_coordinates": [34.0, 90.0, 330.82, 432.35, 33.87]}, {"formula_id": "formula_95", "formula_text": "E[c 1 ] \u2264 q \u22121 E[c * ], E[c 2 ] \u2264 p \u22121 E[c * ].", "formula_coordinates": [34.0, 266.27, 616.85, 76.11, 26.36]}, {"formula_id": "formula_96", "formula_text": "E[|pa \u2194 (S)|] = E[ v\u2208N 1 pa \u2194 (S) (v)] = v\u2208N P r(v \u2208 pa \u2194 (S)) = v\u2208V pq = |N |pq.", "formula_coordinates": [35.0, 140.2, 139.25, 331.6, 22.26]}, {"formula_id": "formula_97", "formula_text": "E[c * ] = E[C(A * )] = E[|A * |] \u2265 E[|pa \u2194 (S)|] = |N |pq.(9)", "formula_coordinates": [35.0, 191.7, 204.21, 331.46, 12.01]}, {"formula_id": "formula_98", "formula_text": "E[c 1 ] \u2264 E[|pa(S) \u2229 H|] \u2264 E[|pa(S)|] = |N |p.(10)", "formula_coordinates": [35.0, 208.49, 259.23, 314.67, 12.01]}, {"formula_id": "formula_99", "formula_text": "E[c 2 ] \u2264 E[|biD(S) \u2229 H|] \u2264 E[|biD(S) \u2229 N |] = |N |q.(11)", "formula_coordinates": [35.0, 192.12, 302.29, 331.05, 12.01]}, {"formula_id": "formula_100", "formula_text": "E[c * ] \u2264 E[c] \u2264 min{p \u22121 , q \u22121 }E[c * ].", "formula_coordinates": [35.0, 229.18, 418.45, 153.65, 12.01]}, {"formula_id": "formula_101", "formula_text": "c = min{c 1 , c 2 }. Therefore, E[c] \u2264 min{E[c 1 ], E[c 2 ]}. Plugging in the inequalities from Proposition 28, E[c] \u2264 min{E[c 2 ], E[c 1 ]} \u2264 min{p \u22121 E[c * ], q \u22121 E[c * ]} = min{p \u22121 , q \u22121 }E[c * ],", "formula_coordinates": [35.0, 89.75, 475.85, 432.24, 35.92]}, {"formula_id": "formula_102", "formula_text": "C({\u00c3}) = C(A \u222a ) = a\u2208A\u222a C(a) \u2264 m i=1 a\u2208Ai C(a) = m i=1 C(A i ) = C(A).", "formula_coordinates": [35.0, 203.83, 643.54, 204.34, 64.66]}, {"formula_id": "formula_103", "formula_text": "A j \u2208 arg min A\u2208ID1(S (j) ) C(A), \u22001 \u2264 j \u2264 t.", "formula_coordinates": [36.0, 221.81, 231.92, 168.39, 17.82]}, {"formula_id": "formula_104", "formula_text": "A \u2032 j \u2208 arg min A\u2208ID1(S (j) ) C(A).", "formula_coordinates": [36.0, 256.88, 310.03, 98.24, 19.86]}, {"formula_id": "formula_105", "formula_text": "A \u2208 ID G (S, V \\ S). (12", "formula_coordinates": [36.0, 263.57, 377.55, 255.16, 9.68]}, {"formula_id": "formula_106", "formula_text": ")", "formula_coordinates": [36.0, 518.74, 377.58, 4.43, 8.74]}, {"formula_id": "formula_107", "formula_text": "C(A) = C(A \\ {A \u2032 j }) + C(A \u2032 j ) < C(A * S,V \\S \\ {A j }) + C(A j ) = C(A * S,V \\S ). (13", "formula_coordinates": [36.0, 140.93, 442.69, 377.81, 12.94]}, {"formula_id": "formula_108", "formula_text": ")", "formula_coordinates": [36.0, 518.74, 444.76, 4.43, 8.74]}, {"formula_id": "formula_109", "formula_text": "A \u2208 ID G (S, V \\ S). For any maximal c-component of G [S] such as G [Si]", "formula_coordinates": [36.0, 90.0, 547.29, 432.0, 21.94]}, {"formula_id": "formula_110", "formula_text": "A \u2208 ID G (S i , V \\S i ). Since Q[S i ] is identifiable from interventions on A for any c-component of G [S]", "formula_coordinates": [36.0, 90.0, 571.2, 432.27, 21.94]}, {"formula_id": "formula_111", "formula_text": "C(\u00c3) = t i=1 C(\u00c3 i ) \u2264 t i=1 C(A * i ) = C(A * S ).", "formula_coordinates": [36.0, 213.63, 677.88, 184.75, 30.32]}, {"formula_id": "formula_112", "formula_text": "A j \u2208 arg min A\u2208ID1(S (j) ) C(A), \u22001 \u2264 j \u2264 t.", "formula_coordinates": [37.0, 222.03, 274.07, 167.94, 17.82]}, {"formula_id": "formula_113", "formula_text": "C(A) = A\u2208B \u03c9(A) \u2264 1\u2264i\u2264t \u03c9(\u0393 i ) = 1\u2264i\u2264t C(A i ) = C(A * S,V \\S ).", "formula_coordinates": [37.0, 171.79, 350.92, 268.42, 22.15]}, {"formula_id": "formula_114", "formula_text": "Proof Let {S 1 , . . . , S k } be subsets of S such that G [Si] for 1 \u2264 i \u2264 k are the maximal c-components in G [S]", "formula_coordinates": [37.0, 90.0, 466.7, 432.0, 21.94]}, {"formula_id": "formula_115", "formula_text": "f * is \u0393i\u2208B f * (w, \u0393 i ) C(A * i ) |\u0393 i | = \u0393i\u2208B |\u0393 i \u2032 | |\u0393 i | C(A * i ). (14", "formula_coordinates": [38.0, 90.0, 104.4, 428.74, 40.49]}, {"formula_id": "formula_116", "formula_text": ")", "formula_coordinates": [38.0, 518.74, 123.12, 4.43, 8.74]}, {"formula_id": "formula_117", "formula_text": "\u0393i\u2208B 1 k C(A * i \u2032 ) = 1 k \u0393 i \u2032 \u2208B * C(A * i \u2032 ) = 1 k A i \u2032 \u2208A C(A * i \u2032 ) = 1 k C(A). (15", "formula_coordinates": [38.0, 173.32, 231.24, 345.42, 28.51]}, {"formula_id": "formula_118", "formula_text": ")", "formula_coordinates": [38.0, 518.74, 237.98, 4.43, 8.74]}, {"formula_id": "formula_119", "formula_text": "f \u2032 (w, \u0393 i ) = |\u0393 i |, if \u0393 i \u2208 B \u2032 , 0, o.w. , f \u2032 (\u0393 i , S j ) = 1, if \u0393 i \u2208 B \u2032 , S j \u2208 \u0393 i 0, o.w. , f \u2032 (S j , z) = 1.", "formula_coordinates": [38.0, 105.67, 316.6, 400.65, 24.66]}, {"formula_id": "formula_120", "formula_text": "\u0393i\u2208B \u2032 f \u2032 (w, \u0393 i ) C(A * i ) |\u0393 i | = A * i \u2208A * C(A * i ) = C(A * ).", "formula_coordinates": [38.0, 202.36, 373.38, 207.29, 30.28]}, {"formula_id": "formula_121", "formula_text": "1 k C(A) \u2264 C(A * ).", "formula_coordinates": [38.0, 268.25, 445.87, 76.7, 22.31]}, {"formula_id": "formula_122", "formula_text": "Proof Let A = {A 1 , . . . A m } be a collection of interventions in ID G (S, V \\ S) for a set S such that G [S] is a c-component. From Theorem 1 of", "formula_coordinates": [38.0, 90.0, 533.1, 432.27, 21.94]}, {"formula_id": "formula_123", "formula_text": "1 \u2264 i \u2264 m such that Q[S] is identifiable from Q[V \\ A i ]. That is, A i \u2208 ID 1 (S). Without loss of generality, suppose i = 1. Define\u00c3 = {A 1 }. Clearly,\u00c3 \u2208 ID G (S, V \\ S).", "formula_coordinates": [38.0, 90.0, 545.08, 433.93, 33.56]}, {"formula_id": "formula_124", "formula_text": "C(\u00c3) = C({A 1 }) \u2264 C({A 1 , A 2 }) \u2264 \u2022 \u2022 \u2022 \u2264 C({A 1 , A 2 , . . . , A m }) = C(A).", "formula_coordinates": [38.0, 149.06, 592.88, 313.88, 9.68]}, {"formula_id": "formula_125", "formula_text": "Q[{s}] in G [V \\A]", "formula_coordinates": [39.0, 90.0, 257.55, 431.99, 21.91]}, {"formula_id": "formula_126", "formula_text": "N (v) \u2190 |{F \u2208 F|v \u2208 F }| 5: v \u2190 arg min v\u2208V N (v) \u03c9(v)", "formula_coordinates": [47.0, 95.37, 371.5, 149.57, 26.07]}], "doi": ""}