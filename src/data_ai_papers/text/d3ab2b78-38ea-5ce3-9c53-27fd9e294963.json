{"title": "Blockwise Coordinate Descent Procedures for the Multi-task Lasso, with Applications to Neural Semantic Basis Discovery", "authors": "Han Liu; Mark Palatucci; Jian Zhang", "pub_date": "", "abstract": "We develop a cyclical blockwise coordinate descent algorithm for the multi-task Lasso that efficiently solves problems with thousands of features and tasks. The main result shows that a closed-form Winsorization operator can be obtained for the sup-norm penalized least squares regression. This allows the algorithm to find solutions to very largescale problems far more efficiently than existing methods. This result complements the pioneering work of Friedman, et al. (2007) for the single-task Lasso. As a case study, we use the multi-task Lasso as a variable selector to discover a semantic basis for predicting human neural activation. The learned solution outperforms the standard basis for this task on the majority of test participants, while requiring far fewer assumptions about cognitive neuroscience. We demonstrate how this learned basis can yield insights into how the brain represents the meanings of words.", "sections": [{"heading": "Introduction", "text": "The cyclical coordinate descent algorithm has been proposed to solve the 1 -regularized least squares regression (or the Lasso) almost ten years ago (Fu, 1998), but not until very recently was their power fully appreciated (Friedman et al., 2007;Wu & Lange, 2008). In particular, Friedman et al.(2007) show that the coordinate descent method, if implemented appropriately, can be used to evaluate the entire regularization path remarkably faster than almost all the existing stateof-the-art methods. The main reasons for such a surprising performance of the coordinate descent algorithm can be summarized as: (i) During each iteration, the coordinate-wise update can be written as a closed-form soft-thresholding operator, thus an inner loop is avoided; (ii) If the underlying feature vector is very sparse, the soft-thresholding operator can very efficiently detect the zeros by a simple check, thus only a small number of updates are needed. (iii) Many computational tricks, like the covariance update or warm start, can be easily incorporated into the coordinate descent procedure (Friedman et al., 2008).\nIn this paper, we consider the computational aspect of the multi-task Lasso (Zhang, 2006), which generalizes the Lasso to the multi-task setting by replacing the 1 -norm regularization with the sum of sup-norm regularization. A scalable cyclical blockwise coordinate descent algorithm is designed which can evaluate the entire regularization path efficiently. In particular, we show that the sub-problem within each iteration can be very efficiently solved by aWinsorization operator, 1 i.e. a proportion of the extreme values of the given vector are truncated while the others remain the same. This extends the result of (Friedman et al., 2007) to the multi-task setting. A similar result also appeared in (Fornasier & Rauhut, 2008) under the more general linear inverse problem framework.\nThe main contribution of this work is that we formulate a non-trivial learning task from the cognitive neuroscience community into a multi-task Lasso problem and solve it using the obtained blockwise coordinate descent algorithm. Compared with the most stateof-the-art results (Mitchell et al., 2008), our solution outperforms the standard hand-crafted features in the majority of test participants while using far fewer assumptions. We also discuss how our methods can be used to refine current theory in cognitive neuroscience.", "publication_ref": ["b4", "b2", "b11", "b2", "b3", "b12", "b2", "b1", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "The Multi-task Lasso", "text": "In this section, we introduce the multi-task Lasso and some related work. We start with some notations. Consider a K-task linear regression, for k = 1, . . . , K,\nY (k) = p j=1 \u03b2 (k) j X (k) j + (k) where Y (k) , X (k) j , (k) \u2208 R n k .\nThe superscript k indexes the tasks, p is the number of features, and n k is the number of instances within the k-th task. We assume the data is standardized so the constant terms can be dropped, i.e. X (k) j and Y (k) have mean 0 and X\n(k) j 2 = 1 where \u2022 2 is the 2 -Euclidean norm. Let \u03b2 j \u2261 (\u03b2 (1) j , . . . , \u03b2 (K) j ) T\n(1)\nbe the vector of all coefficients for the j th feature across different tasks, the multi-task Lasso estimate is formulated as the solution to the optimization problem\nmin \u03b2 1 2 K k=1 Y (k) \u2212 p j=1 \u03b2 (k) j X (k) j 2 2 +\u03bb p j=1 \u03b2 j \u221e , (2)\nwhere\n\u03b2 j \u221e \u2261 max k |\u03b2 (k)\nj | is the sup-norm in the Euclidean space. It has the effect of \"grouping\" the elements in \u03b2 j such that they can achieve zeros simultaneously. If all tasks share a common design matrix, the multi-task regression reduces to a multivariateresponse regression. In this case, Turlach et al. (2005) proposes the same sum of sup-norm regularization and name the resulting estimate in (2) as the simultaneous Lasso. It's obvious that any solver for the multi-task Lasso also solves the simultaneous Lasso. Existing methods to solve (2) from the machine learning and statistics communities include the double coordinate descent method from (Zhang, 2006), the interior-point method from (Turlach et al., 2005), and the geometric solution path method from (Zhao et al., 2009). These methods, however, have difficulty scaling to thousands of features and tasks.\nOne alternative worth noting is the multi-task feature selection work of Argyriou, Evgeniou, and Pontil (2008). Compared with (2), although both methods can be used to learn features over many tasks, their work uses a different penalty term in the optimization problem. Our work, by contrast, focuses on the multitask Lasso which uses the sum of sup-norm penalty.\nRemark 1. Equation (2) treats all tasks equally, which can be sensitive to abnormal or outlier tasks. To address this, we can build an adaptive version of this algorithm. After obtaining the initial estimate by treating all the tasks equally, we could calculate the residual sum of squares for the fit of different tasks. A second step can then be conducted by weighting these tasks differently according to their initial fit. This provides extra performance gain and robustness.", "publication_ref": ["b10", "b12", "b10", "b13", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Blockwise Coordinate Descent", "text": "For a fixed regularization parameter \u03bb, the blockwise coordinate descent algorithm for the multi-task Lasso problem in Equation ( 2) is given in Figure 1, where\n\u2022, \u2022 denotes the inner product operator of two vectors.\nRecall that \u03b2 j in (1) represents the coefficient vector of the j-th feature across all the K tasks. We call \u03b2 j a block. The algorithm consists of simultaneously updating the coefficients within each block while holding all the others fixed, then cycling through this process. Therefore, if the current estimates are \u03b2 , = 1, . . . , p, then \u03b2 j is updated by the following sub-problem: k) denotes the partial residual vector.\n\u03b2 j = arg min \u03b2j 1 2 K k=1 R (k) j \u2212\u03b2 (k) j X (k) j 2 2 +\u03bb \u03b2 j \u221e (3) where R (k) j \u2261 Y (k) \u2212 =j \u03b2 (k) X(\nIf the regularization parameter \u03bb = 0, the problem in (3) decouples and the least squares solution \u03b1\n(k) j is \u03b1 (k) j = R (k) j , X (k) j , for \u2200j, k. (4) Since R (k) j , X (k) j = Y (k) , X (k) j \u2212 =j \u03b2 (k) X (k) , X(k) j\n, we can pre-calculate the quantities c\n(k) j = Y (k) , X (k) j and d (k) ij = X (k) i , X (k) j\n. This is the same covariance update idea as in (Friedman et al., 2008) and corresponds to the first double loop in the algorithm in Figure 1. If we have a decreasing sequence of the regularization parameters \u03bb's, the initial values of \u03b2 (k) j for each fixed \u03bb comes from the solutions \u03b2 (k) j calculated from the previous \u03bb value. This is the same warm start trick as in (Friedman et al., 2008) and can significantly speedup the algorithm performance for evaluating the entire solution path. Since the quantities c do not depend on \u03bb, they only need to be calculated once and can serve for the whole pathwise evaluation.\nFor \u03bb > 0, (3) becomes a sup-norm penalized least squares regression. If we use the Newton's method or coordinate descent procedure to solve it as in (Zhang, 2006), an inner loop will be needed. This turns out not to be scalable if the number of tasks K is very large. Fortunately, Theorem 2 below shows that the solution to (3) is equivalent to a closed-form Winsorization operation applied on the previously calculated unpenalized least squares results \u03b1 (k) j 's. This corresponds to the main loop of the algorithm in Figure 1.", "publication_ref": ["b3", "b3", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Blockwise coordinate descent algorithm", "text": "Input: Data (X\n(k) 1 , . . . , X (k)\np , Y (k) ), k = 1, . . . , K and the regularization parameter \u03bb;\nIterate over k \u2208 {1, . . . , K} and j \u2208 {1, . . . , p}\n(1) c (k) j \u2190 Y (k) , X (k) j ; (2) \u03b2 (k) j \u2190 initial values (either 0 or b \u03b2 (k) j\nfor the previous \u03bb if doing a warm-start);\n(3) For each i \u2208 {1, . . . , p}:\nd (k) ij \u2190 X (k) i , X (k) j ; End;\nIterate until convergence (Main Loop):\nFor each j \u2208 {1, . . . , p}:\n(1) \u2200k \u2208 {1, . . . , K}, \u03b1\n(k) j \u2190 c (k) j \u2212 X i =j \u03b2 (k) i d (k) ij ; (2) If P K k=1 |\u03b1 (k) j | \u2264 \u03bb Then \u03b2j \u2190 0 Else (a) Sort the indices according to |\u03b1 (k 1 ) j | \u2265 |\u03b1 (k 2 ) j | \u2265 . . . \u2265 |\u03b1 (k K ) j |; (b) m * \u2190 arg max 1\u2264m\u2264K \" P m i=1 |\u03b1 (k i ) j |\u2212\u03bb \" /m; (c) For each i \u2208 {1, . . . , K} If i > m * Then \u03b2 (k i ) j \u2190 \u03b1 (k i ) j Else \u03b2 (k i ) j \u2190 sign(\u03b1 (k i ) j ) m * \" m * X =1 |\u03b1 (k ) j | \u2212 \u03bb # ; End; Output: b \u03b2 (k) j \u2190 \u03b2 (k) j\nfor j = 1, . . . , p and k = 1, . . . , K;\nFigure 1. The algorithm for the multi-task Lasso.\nTheorem 2. Let \u03b1\n(k) j\nas defined in (4) and order the indices according to |\u03b1 Proof: The proof proceeds by discussing several cases separately: (i) All the elements in the sup-norm are zeros; (ii) One unique element in the sup-norm achieves the maximum; (iii) At least two elements in the supnorm achieve the maximum. These cases correspond to Propositions 5, 6, and 8 respectively.\n(k1) j | \u2265 |\u03b1 (k2) j | \u2265 . . . \u2265 |\u03b1 (k K ) j |. Then the solution to (3) is \u03b2 (ki) j = sign(\u03b1 (ki) j ) m * m * i =1 |\u03b1 (k i ) j |\u2212\u03bb + \u20221 {i\u2264m * } +\u03b1 (ki) j \u20221 {i>m * } where m * = arg max m 1 m m i =1 |\u03b1 (k i ) j | \u2212 \u03bb , 1 {\u2022} is\nSince the given objective function in ( 3) is convex, its solution can be characterized by the Karush-Kuhn-Tucker conditions as the following\nR (k) j \u2212 \u03b2 (k) j X (k) j T X (k) j = \u03bb\u03b7 k \u2200k \u2208 {1, . . . , K}, (5) where {\u03b7 k } K k=1 satisfy \u03b7 \u2261 (\u03b7 1 , . . . , \u03b7 K ) T \u2208 \u2202 \u2022 \u221e \u03b2j .\nHere, \u2202 \u2022 \u221e \u03b2j denotes the subdifferential of the convex functional \u2022 \u221e evaluated at \u03b2 j , it lies in a K-dimensional Euclidean space. Next, the following proposition from (Rockafellar & Wets, 1998) can be used to characterize the subdifferential of sup-norms.\nLemma 3. The subdifferential of \u2022 \u221e in R K is \u2202 \u2022 \u221e x = {\u03b7 : \u03b7 1 \u2264 1} x = 0 conv{sign(x i )e i : |x i | = x \u221e } o.w.(6)\nwhere conv(A) denotes the convex hull, and e i is the i-th canonical unit vector in R K .", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Proposition 4. Let \u03b2", "text": "(k) j be solution to (3) and \u03b1\n(k) j in (4), if \u03b2 (k) j = 0, then sign( \u03b2 (k) j ) = sign(\u03b1 (k) j ).\nProof to Proposition 4: Since \u03b2 (k) j = 0, the result trivially follows from the convexity and continuity of the objective function in (3).\nFirstly, we consider the case that K k=1 |\u03b1 (k) j | \u2264 \u03bb and show that 0 must be a solution.\nProposition 5. \u03b2 j = 0 if and only if\nK k=1 |\u03b1 (k) j | \u2264 \u03bb.\nProof to Proposition 5: From (5), we know that \u03b2 j = 0 if and only if \u2203\u03b7 1 , . . . , \u03b7 K such that\nK k=1 |\u03b7 k | \u2264 1 and \u03bb\u03b7 k = R (k) j T X (k) j = \u03b1 (k) j . (7\n)\nIf K k=1 |\u03b1 (k) j | \u2264 \u03bb, choosing \u03b7 k as in (7) would guar- antee that K k=1 |\u03b7 k | \u2264 1, therefore \u03b2 j = 0.\nOn the other hand, If \u03b2 j = 0, from (7), we know that\n\u03bb\u03b7 k = \u03b1 (k) j , k = 1, . . . , K and K k=1 |\u03b7 k | \u2264 1. This implies that K k=1 |\u03b1 (k) j | \u2264 \u03bb.\nNext, we consider the case that\nK k=1 |\u03b1 (k) j | > \u03bb and |\u03b1 (k1) j | \u2212 \u03bb > |\u03b1 (k2) j |.\nHere we show that \u03b2\n(k) j = \u03b1 (k) j for \u2200k = k 1 , while \u03b2 (k1) j = sign \u03b1 (k1) j |\u03b1 (k1) j | \u2212 \u03bb . Proposition 6. | \u03b2 (k1) j | > | \u03b2 (k) j | for \u2200k = k 1 if and only if |\u03b1 (k1) j | \u2212 \u03bb > |\u03b1 (k2) j |.\nProof to Proposition 6:\nIf | \u03b2 (k1) j | > | \u03b2 (k) j | for \u2200k = k 1 , this implies that \u2202 \u2022 \u221e \u03b2j = e k1\n, where e k1 is the k 1 -th canonical vector. Therefore, from (5),\nR (k) j \u2212 \u03b2 (k) j X (k) j T X (k) j = \u03bbsign( \u03b2 (k1) j )1 {k=k1} .\nTherefore, we know \u03b2\n(k1) j = \u03b1 (k1) j \u2212 \u03bbsign( \u03b2 (k1) j\n) and\n\u03b2 (k) j = \u03b1 (k) j for \u2200k = k 1 . Combined with the fact | \u03b2 (k1) j | > | \u03b2 (k) j | for \u2200k = k 1 , we get |\u03b1 (k1) j \u2212 \u03bbsign( \u03b2 (k1) j )| > |\u03b1 (k) j | for \u2200k = k 1 .\nFrom Proposition 4, we have sign(\u03b1\n(k1) j ) = sign( \u03b2 (k1) j ). Further, if \u03b2 (k1) j > 0, then |\u03b1 (k1) j | > \u03bb, we have |\u03b1 (k1) j \u2212 \u03bbsign(\u03b1 (k1) j )| = |\u03b1 (k1) j | \u2212 \u03bb. Therefore, |\u03b1 (k1) j | \u2212 \u03bb > |\u03b1 (k2) j |. This is also true for \u03b2 (k1) j < 0. On the other hand, assuming |\u03b1 (k1) j | \u2212 \u03bb > |\u03b1 (k2) j | but for some n > 1, there exist | \u03b2 (k1) j | = . . . = | \u03b2 (kn) j | = \u03b2 j \u221e . (8\n)\nThen, by ( 6) and ( 5), there must exist a 1 , a 2 \u2208 [0, 1] and\na 1 + a 2 \u2264 1, for h = 1, 2, R (k h ) j \u2212 \u03b2 (k h ) j X (k h ) j T X (k h ) j = \u03bba h sign( \u03b2 (k h ) j\n).\nCombine this result and (8), we get |\u03b1\n(k1) j \u2212 \u03bba 1 sign( \u03b2 (k1) j )| = |\u03b1 (k2) j \u2212 \u03bba 2 sign( \u03b2 (k2) j )|. By Proposition 4 and |\u03b1 (k1) j | > \u03bba 1 , we have |\u03b1 (k1) j \u2212 \u03bba 1 sign( \u03b2 (k1) j )| = |\u03b1 (k1) j |\u2212\u03bba 1 . If |\u03b1 (k2) j | > \u03bba 2 , we get |\u03b1 (k1) j | \u2212 \u03bb sign(a 1 \u03b2 (k1) j ) + a 2 sign( \u03b2 (k2) j ) = |\u03b1 (k2) j |. Since a 1 + a 2 \u2264 1, this obviously contradicts with the assumption that |\u03b1 (k1) j | \u2212 \u03bb > |\u03b1 (k2) j |. The same result also hold for the case |\u03b1 (k2) j | \u2264 \u03bba 2 .\nLastly, for the case\nK k=1 |\u03b1 (k) j | > \u03bb and |\u03b1 (k1) j | \u2212 \u03bb \u2264 |\u03b1 (k2) j |.\nWe start with an auxiliary proposition. | achieve \u03b2 j \u221e > 0 , by ( 6), there must exist m nonnegative numbers a 1 , . . . , a m , such that m =1 a = 1 and for each \u2208 {1, . . . , m}, R\nProposition 7. For m > 1, if there are precisely m entries | \u03b2 (k1) j |, . . . , | \u03b2 (km) j | achieve \u03b2 j \u221e > 0, then \u03b2 (ki) j = sign(\u03b1 (ki) j ) m m =1 |\u03b1 (k ) j | \u2212 \u03bb \u2200i = 1, . . . , m.\n(k ) j \u2212 \u03b2 (k ) j X (k ) j T X (k ) j = \u03bba sign( \u03b2 (k ) j\n).\nWhich can be re-written as\n\u03b1 (k ) j = \u03bba sign( \u03b2 (k ) j ) + \u03b2 (k ) j \u2200 \u2208 {1, . . . , m}. (9)\nUsing the fact that | \u03b2\n(k1) j | = . . . = | \u03b2 (km) j\n|, summing over the absolute value of both sides of all the equations in ( 9), we obtain\nm =1 |\u03b1 (k ) j | = m =1 |\u03bba sign( \u03b2 (k ) j ) + \u03b2 (k ) j |. Since |\u03bba sign( \u03b2 (k ) j ) + \u03b2 (k ) j | = \u03bba + | \u03b2 (k ) j | and m =1 a = 1, we have | \u03b2 (ki) j | = 1 m m =1 |\u03b1 (ki) j | \u2212 \u03bb . \u2200i \u2208 {1, . . . , m}. (10)\nFinally, by Proposition 4, we know that sign(\u03b1\n(ki) j ) = sign( \u03b2 (ki) j ) for i = 1, . . . , m, therefore \u03b2 (ki) j = sign(\u03b1 (ki) j ) m m =1 |\u03b1 (k ) j | \u2212 \u03bb i = 1, . . . , m.\nTo finish the proof of Theorem 2, we still need to describe the exact conditions that there are exactly m > 1 elements that achieve the sup-norm. This is given in the following Proposition 8. A similar result was also given in (Fornasier & Rauhut, 2008).\nProposition 8. For m > 1, there exist precisely m entries | \u03b2 (k1) j |, . . . , | \u03b2 (km) j | that achieve \u03b2 j \u221e > 0 if and only if |\u03b1 (k1) j | \u2212 \u03bb \u2264 |\u03b1 (k2) j | and |\u03b1 (km) j | \u2265 1 m\u22121 m\u22121 =1 |\u03b1 (k ) j | \u2212 \u03bb and |\u03b1 (km+1) j | < 1 m m =1 |\u03b1 (k ) j | \u2212 \u03bb . Proof to Proposition 8: Assume exactly m > 1 entries | \u03b2 (k1) j |, . . . , | \u03b2 (km) j | achieve \u03b2 j \u221e > 0, from Proposi- tion 7, we know that for i = 1, . . . , m, \u03b2 (ki) j = sign(\u03b1 (ki) j ) m m =1 |\u03b1 (k ) j | \u2212 \u03bb .\n(11) By ( 9) and Proposition 4, we have\na = \u03b1 (k ) j \u2212 \u03b2 (k ) j \u03bbsign( \u03b2 (k ) j ) = |\u03b1 (k ) j | \u2212 | \u03b2 (k ) j | \u03bb \u2208 {1, . . . , m}.\nPlugging (11) into a m , since a m \u2265 0, we get\n|\u03b1 (km) j | \u2265 1 m \u2212 1 m\u22121 =1 |\u03b1 (k ) j | \u2212 \u03bb . (12) Further, since | \u03b2 (km) j | > | \u03b2 (km+1) j |, the necessity follows from |\u03b1 (km+1) j | = | \u03b2 (km+1) j | < | \u03b2 (km) j | = 1 m m =1 |\u03b1 (k ) j | \u2212 \u03bb .\nFor the sufficiency, we assume that precisely n = m\nentries | \u03b2 (k1) j |, . . . , | \u03b2 (kn) j\n| achieve \u03b2 j \u221e > 0, then follow exactly the same argument as the necessity part to obtain a contradiction.\nTo prove Theorem 2, we know from Proposition 8 there are precisely m * entries in \u03b2 j that achieve\n\u03b2 j \u221e > 0 if and only if m * = arg max m 1 m m =1 |\u03b1 (k ) j | \u2212 \u03bb .\nThe result follows by combining Propositions 5 and 6.\nRemark 9. We also conducted experiments to quantitatively compare the performance of the blockwise coordinate descent algorithm with the Log-barrier interior-point method in a similar setting as in (Friedman et al., 2007). The blockwise coordinate descent algorithm is significantly faster. Due to the lack of space, we do not report the simulation details here.\nThe complexity analysis of the algorithm is straightforward. Within the main loop, the most expensive step is sorting the K elements, which can be done in O(K log K). This makes the algorithm scalable to very large number of tasks. From the Winsorization operator, we do not need to update a block if K k=1 |\u03b1 (k) j | \u2264 \u03bb, which happens frequently if the problem is sparse. This makes the algorithm scalable to very large number of features. Furthermore, since many quantities can be pre-calculated, the algorithm can be also applied to large sample datasets. The numerical convergence of this algorithm is summarized in the following theorem.\nTheorem 10. The solution sequence generated by the blockwise coordinate descent algorithm in Figure 1 is bounded and every cluster point is a solution of the multi-task Lasso defined in (2).\nProof The optimization objective function in (2) is continuous on a compact level set, and is convex (but not strictly convex) and nondifferentiable. Furthermore, notice that the nondifferentiable part \u03bb p j=1 \u03b2 j \u221e is separable, i.e. it can be decomposed into a sum of individual functions, one for each block of variables. By Theorem 4.1 in (Tseng, 2001) we obtain the claimed results.", "publication_ref": ["b1", "b2", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Neural Semantic Basis Discovery", "text": "In this section, we present a case study of the multitask Lasso by applying it to a problem in cognitive neuroscience. Specifically, we consider the task of predicting a person's neural activity in response to an arbitrary word in English as described in (Mitchell et al., 2008). Their approach is to predict the neural image that would be recorded using functional magnetic resonance imaging (fMRI) when a person thinks about a given word. In their work, the semantic meaning of a word is encoded by co-occurrence statistics with other words in a very large text corpus. They use a small number of training words to learn a linear model that maps these co-occurrence statistics to images of neural activity recorded while a person is thinking about those words. Their model can then predict images for new words that were not included in the training set and shows that the predicted images are similar to the observed images for those words. In their initial model each word is encoded by a vector of co-occurrences with 25 sensory-action verbs (e.g. eat, ride, wear). For example, words related to foods such as \"apples\" and \"oranges\" would have frequent co-occurrences with the word \"eat\" but few cooccurrences with the word \"wear\". Conversely, words related to clothes such as \"shirt\" or \"dress\" would cooccur frequently with the word \"wear\" but not the word \"eat\". Thus \"eat\" and \"wear\" are example basis words used to encode relationships of a broad set of other words. These 25 sensory-motor verbs (shown in Table 1) were hand-crafted based on domain knowledge from the cognitive neuroscience literature and are considered a semantic basis of latent word meaning. A natural question is: What is the optimal basis of words to represent semantic meaning across many concepts?\nRather than relying on models that require manual selection of a set of words, our research tries to build models that will perform variable selection to automatically learn a semantic basis of word meaning. In this way, we not only want to predict neural activity well, but also give insights into how the brain represents the meaning of different concepts. The hope is that learning directly from data could lead to new theories in cognitive neuroscience.\nFor our study, we utilize the two datasets described in (Mitchell et al., 2008). The first dataset was collected using fMRI. Nine participants were presented with 60 different words and were asked to think about each word for several seconds while their neural activities were recorded. The 60 words are composed of nouns from 12 categories with 5 exemplars per category. For example, a bodypart category includes Arm, Eye, Foot, Hand, Leg, a tools category includes the words Chisel, Hammer, Pliers, Saw, Screwdriver, and a furniture category includes Bed, Chair, Dresser, Desk, Table , etc.\nThe second dataset is a symmetric matrix of text cooccurrences between the 50,000 most frequent words in English. These co-occurrences are derived from the Google Trillion Word Corpus 2 . The goal is to use these co-occurrences to construct a feature vector that represents word meaning. The hope is that two words that cause similar neural activites would also have high inner product between their co-occurrence vectors. One simple representation of each word is to use the raw 50,000 dimensional feature vector of co-occurrences (normalized to unit length row norm). Typically a much smaller representation is desired such as the hand-crafted 25-word basis described above.\nFor each participant, there are altogether n = 60 fMRI images taken 3 , each one corresponds to one stimulus word. A typical fMRI image contains over K = 20, 000 different neural responses. Each neural response is the activity in a voxel (volume-element) in the brain. Each voxel represents a 1-3 mm 3 volume in the brain and is the basic spatial unit of measurement in fMRI.\nHere we show the problem of learning a semantic basis can be formulated into a multi-task Lasso as in (2). Since the goal of learning a semantic basis is to find a common set of predictor variables that will predict the neural response well across multiple voxels, where each predictor variable is the text co-occurences with a particular word from the Google Trillion Word Corpus. Therefore, for each participant, we have roughly K = 20, 000 tasks, all these tasks share the common design columns {X j } p j=1 \u2208 R n , representing the cooccurrences of n = 60 training words with p = 50, 000 other English words in the Google Corpus. The response vector Y (k) for each task contains the neural activations for a single voxel (volume-element) across the n = 60 fMRI images. Therefore, this is a multitask learning problem with a very large number of tasks K = 20, 000 and a very large number of features p = 50, 000. While the algorithm we develop can solve a problem of this scale in only a few minutes, our primary results use a smaller dataset where p = 5, 000 and K = 500, so that our experiments are directly comparable with other published results. We also provide additional results where K = 4500.", "publication_ref": ["b7", "b7"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Experimental Results", "text": "To evaluate our methods and compare them to existing results, we use exactly the same experimental protocols described in (Mitchell et al., 2008). As a small additional step we use the multi-task Lasso to perform a variable selection. Note that the multi-task Lasso is used in this context only to learn which variables should be input into the final model. Specifically, the leave-two-out-cross-validation procedure is as follows:\na Create a 60 \u00d7 5, 000 design matrix of semantic features using co-occurences of the 5,000 most frequent words in English (minus 100 stop words). A stop word is a high frequency common word like \"the\".\nb Select 2 words out of 60 for testing and use the other 58 words for training. Using (2), learn the coefficients \u03b2 by setting each Xj to be the 58 \u00d7 1 vector of co-occurences for each of the 5000 basis words and each Y (k) to be the 58 \u00d7 1 column vector for each of the top K = 500 voxel responses selected using the stability criterion score described in (Mitchell et al., 2008). In the language of multi-task Lasso, this problem corresponds to the scale n = 58, p = 5000, K = 500. The regularization parameter here can be set to choose the desired number of non-zero coefficients. We set this parameter to yield basis sets with 25, 140, and 350 elements so the model is easier to interpret and compare to existing results.\nc Create a new matrix of semantic features that is 58 \u00d7 d, where d is the number of selected feature blocks in \u03b2.\nIn our case d will be either 25, 140, or 350. Each nonzero block should correspond to a word selected from the original set of 5,000. This word shall now become a semantic feature in the new basis.\nd Train a linear model using ridge regularization to predict each of the 500 voxels from the semantic feature basis. The solution can be obtained using the standard normal equations for ridge regression.\ne For each of the two test examples, predict the neural response of the 500 selected voxels. Compute the cosine similarity of each prediction with each of the held out images. Based on the combined similarity scores, choose which prediction goes with each held out image. Test if the joint labeling was correct. This leads to an output of 0 or 1. For more details, see (Mitchell et al., 2008).\nf Repeat steps b-e for all`6 0 2\u00b4p ossible pairs of words (1,770 total). Count the number of incorrect labelings in step e to determine the accuracy of the basis set.  We repeat this experiment for each of the nine different participants in the fMRI study and use the same method in Mitchell et al. (2008) to ensure consistency while testing various semantic features. The regularization parameter for the ridge regression in step d is chosen automatically using the cross-validation error score described in (Hastie et al., 2001, Page 216).\nWe performed several experiments using the above protocols to compare the performance of the semantic feature sets learned using the multi-task Lasso with the hand-crafted features described earlier. Using these results we now pose and answer several questions:\nQ1: Can we automatically learn a semantic basis?\nAs in Figure 2, we use the multi-task Lasso to learn a semantic basis from the 5,000 most frequent words in English and adjust the regularization parameter to obtain different basis sizes. We denote MTL25, MTL140, MTL350 as models that have 25,140, and 350 words respectively in their basis sets each time they were trained. Note that we train the models on each iteration of the cross-validation and keep the regularization parameter the same between iterations. As a result, there can be a slight variation in the actual number of features selected on each iteration.\nWe see in Table 2 the results of the 4 models. The result for the 25 features hand-crafted by cognitive neuroscientists is also reported. Chance accuracy for this prediction task is 0.5 and statistically significant accuracy at the 5% level is 0.61 (Mitchell et al., 2008). We see that in 8 of 9 subjects, all three multi-task Lasso models learn a semantic basis that leads to statistically significant accuracies. This suggests that we can indeed learn a semantic basis directly from data.\nThe MTL140 and MTL350 models achieve higher accuracies than the hand-crafted features in 6 of 9 participants. It is exciting that the model can often meet or exceed the performance of the hand-crafted features using far fewer assumptions about neuroscience. It is also useful that our learned basis is different than the handcrafted features, which suggests potential benefits from a model averaging approach. For the MTL25 model, we report accuracies higher than the handcrafted features in 4 of 9 participants. We also see that two larger basis sets outperform the MTL25 set, suggesting that more than 25 features are necessary to capture the variance of the data.\nAn interesting observation comes from participant 4. On this participant all three learned models performed worse than the hand-crafted model. The very abnormal behavior of the learned models on this participant versus the other participants suggests that this particular participant might be an outlier or a very noisy observation (as is common in fMRI studies). However, the hand-crafted feature does not suffer from this case. More investigation is suggested to study this.\nQ2: Is there any advantage to learning the basis across multiple subjects simultaneously?\nAn interesting question is whether we could ameliorate this problem by learning the basis simultaneously across all subjects at once. Table 3 shows the results of learning the basis by combining the fMRI data for all participants (thereby yielding a 58 x 4500) target matrix. This corresponds to a multi-task Lasso where K = 4500. On average, we found a slight advantage on the MTL140 and MTL350 models, and slight disadvantage for the MT25 model. Although encouraging, this is hardly conclusive evidence, and we feel this is an interesting direction for future work. In particular, it would be worth studying the impact of noisy data by removing an outlier such as participant 4.\nQ3: What is the learned semantic basis? Table 4 shows one example of 25 basis words learned using the MTL25. It is easy to see relationships between many of the words in the basis set and the 60 stimulus words described before. For example, the model learned Tools as a basis word, which is interest-ing because 5 different instances of tools were shown (e.g. Screwdriver, Hammer, etc.). The basis word Bedroom clearly refers to words in the furniture category (Bed, Dresser, etc.) and Arms is related to body parts (Leg, Hand, etc.). For a given basis word, we can train a simple linear model to predict neural activations across all 20,000 voxels in the brain from this single basis word. Note that this is a multiple output regression and each learned coefficient corresponds to a physical location in the brain. By plotting the coefficients, we can discover how different basis words activate different regions of the brain. Figure 3 shows a 3D map of these coefficients for the basis word Tools. We see strong activation (red) in the superior temporal sulcus which is believed to be associated with the perception of biological motion. We also see strong activation in the postcentral gyrus which is believed to be associated with premotor planning. ", "publication_ref": ["b7", "b7", "b7", "b7", "b7"], "figure_ref": ["fig_3", "fig_4"], "table_ref": ["tab_1", "tab_2"]}, {"heading": "Conclusion", "text": "We present a blockwise coordinate descent algorithm to fit the entire regularization path of the multi-task Lasso in a highly efficient way. This algorithm uses a closed-form Winsorization operator which allows it easy to implement and perform far more efficiently than prevous methods. We believe that the multi-task Lasso is useful for a large class of sparse, multi-task regression problems and demonstrated its power on a neural semantics discovery problem.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Convex multi-task feature learning", "journal": "", "year": "2008", "authors": "A Argyriou; T Evgeniou; M Pontil"}, {"ref_id": "b1", "title": "Recovery algorithms for vector valued data with joint sparsity constraints", "journal": "SIAM Journal of Numerical Analysis", "year": "2008", "authors": "M Fornasier; H Rauhut"}, {"ref_id": "b2", "title": "Pathwise coordinate optimization", "journal": "The Annals of Applied Statistics", "year": "2007", "authors": "J Friedman; T Hastie; H H\u00fcdotofling; R Tibshirani"}, {"ref_id": "b3", "title": "Regularized paths for generalized linear models via coordinate descent", "journal": "", "year": "2008", "authors": "J H Friedman; T Hastie; R Tibshirani"}, {"ref_id": "b4", "title": "Penalized regressions: The bridge versus the lasso", "journal": "Journal of Computational and Graphical Statistics", "year": "1998", "authors": "W J Fu"}, {"ref_id": "b5", "title": "The elements of statistical learning: Data mining, inference, and prediction", "journal": "Springer-Verlag", "year": "2001", "authors": "T Hastie; R Tibshirani; J H Friedman"}, {"ref_id": "b6", "title": "The collected works of john w. tukey. volume vi: More mathematical", "journal": "Wadsworth & Brooks/Cole", "year": "1938", "authors": "C L Mallows"}, {"ref_id": "b7", "title": "Predicting human brain activity associated with the meanings of nouns", "journal": "Science", "year": "2008", "authors": "T Mitchell"}, {"ref_id": "b8", "title": "Variational analysis", "journal": "Springer-Verlag Inc", "year": "1998", "authors": "R T Rockafellar; R J Wets; .-B "}, {"ref_id": "b9", "title": "Convergence of a block coordinate descent method for nondifferentiable minimization", "journal": "", "year": "2001", "authors": "P Tseng"}, {"ref_id": "b10", "title": "Simultaneous variable selection", "journal": "Technometrics", "year": "2005", "authors": "B Turlach; W N Venables; S J Wright"}, {"ref_id": "b11", "title": "Coordinate descent algorithms for lasso penalized regression", "journal": "The Annals of Applied Statistics", "year": "2008", "authors": "T T Wu; K Lange"}, {"ref_id": "b12", "title": "A probabilistic framework for multitask learning", "journal": "", "year": "2006", "authors": "J Zhang"}, {"ref_id": "b13", "title": "The grouped and hierarchical model selection through composite absolute penalties", "journal": "The Annals of Statistics", "year": "2009", "authors": "P Zhao; G Rocha; B Yu"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "the indicator function, and [\u2022] + denotes the positive part.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 .2Figure 2. The leave-two-out-cross-validation protocols", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 .3Figure 3. Regression weights to each voxel for word Tools.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "The semantic basis used inMitchell et al. (2008) ", "figure_data": "SeeEatRunSayEnterHearTouchPushFearDriveListen RubFillOpen WearTasteApproachMove LiftBreakSmellManipulate RideNearClean"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Accuracies for 9 fMRI Participants. Learned per-subject", "figure_data": "Subject 1 Subject 2 Subject 3 Subject 4 Subject 5 Subject 6 Subject 7 Subject 8 Subject 9Handcraft 0.749718 0.705085 0.726554 0.715254 0.792655 0.771751 0.684746 0.729379 0.787006MTL250.863842 0.713559 0.718079 0.608475 0.787006 0.649153 0.714124 0.730508 0.679661MTL1400.863842 0.741243 0.727119 0.545763 0.831638 0.654237 0.733333 0.751977 0.717514MTL3500.881921 0.757627 0.754802 0.567232 0.840678 0.692090.762147 0.784746 0.738983Table 3. Accuracies for 9 fMRI Participants. Learned with combined fMRISubject 1 Subject 2 Subject 3 Subject 4 Subject 5 Subject 6 Subject 7 Subject 8 Subject 9MTL250.815254 0.718079 0.679096 0.588701 0.732203 0.661017 0.737288 0.755932 0.690960MTL1400.849718 0.736158 0.733333 0.553107 0.810734 0.678531 0.761017 0.753107 0.732768MTL3500.880791 0.761017 0.758757 0.575141 0.845198 0.691525 0.762712 0.783051 0.738983"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "An example of 25 learned semantic basis words.", "figure_data": "ToolsCarDogWinePotteryModel Station Bedroom Breakfast CupMadRentals FishingCakeTipArmsWalkCleaningCheeseGayRightWhiteFrontContentsResult"}], "formulas": [{"formula_id": "formula_0", "formula_text": "Y (k) = p j=1 \u03b2 (k) j X (k) j + (k) where Y (k) , X (k) j , (k) \u2208 R n k .", "formula_coordinates": [2.0, 55.44, 224.0, 234.0, 23.83]}, {"formula_id": "formula_1", "formula_text": "(k) j 2 = 1 where \u2022 2 is the 2 -Euclidean norm. Let \u03b2 j \u2261 (\u03b2 (1) j , . . . , \u03b2 (K) j ) T", "formula_coordinates": [2.0, 55.44, 287.36, 234.0, 45.17]}, {"formula_id": "formula_2", "formula_text": "min \u03b2 1 2 K k=1 Y (k) \u2212 p j=1 \u03b2 (k) j X (k) j 2 2 +\u03bb p j=1 \u03b2 j \u221e , (2)", "formula_coordinates": [2.0, 56.12, 383.73, 233.32, 31.02]}, {"formula_id": "formula_3", "formula_text": "\u03b2 j \u221e \u2261 max k |\u03b2 (k)", "formula_coordinates": [2.0, 90.88, 424.18, 86.71, 12.79]}, {"formula_id": "formula_4", "formula_text": "\u03b2 j = arg min \u03b2j 1 2 K k=1 R (k) j \u2212\u03b2 (k) j X (k) j 2 2 +\u03bb \u03b2 j \u221e (3) where R (k) j \u2261 Y (k) \u2212 =j \u03b2 (k) X(", "formula_coordinates": [2.0, 307.44, 355.8, 234.0, 56.74]}, {"formula_id": "formula_5", "formula_text": "(k) j is \u03b1 (k) j = R (k) j , X (k) j , for \u2200j, k. (4) Since R (k) j , X (k) j = Y (k) , X (k) j \u2212 =j \u03b2 (k) X (k) , X(k) j", "formula_coordinates": [2.0, 307.44, 441.48, 234.0, 66.95]}, {"formula_id": "formula_6", "formula_text": "(k) j = Y (k) , X (k) j and d (k) ij = X (k) i , X (k) j", "formula_coordinates": [2.0, 307.44, 509.89, 229.63, 29.6]}, {"formula_id": "formula_7", "formula_text": "(k) 1 , . . . , X (k)", "formula_coordinates": [3.0, 138.22, 185.87, 49.66, 12.1]}, {"formula_id": "formula_8", "formula_text": "(1) c (k) j \u2190 Y (k) , X (k) j ; (2) \u03b2 (k) j \u2190 initial values (either 0 or b \u03b2 (k) j", "formula_coordinates": [3.0, 78.53, 232.05, 177.41, 27.82]}, {"formula_id": "formula_9", "formula_text": "d (k) ij \u2190 X (k) i , X (k) j ; End;", "formula_coordinates": [3.0, 75.37, 273.22, 196.52, 28.34]}, {"formula_id": "formula_10", "formula_text": "(k) j \u2190 c (k) j \u2212 X i =j \u03b2 (k) i d (k) ij ; (2) If P K k=1 |\u03b1 (k) j | \u2264 \u03bb Then \u03b2j \u2190 0 Else (a) Sort the indices according to |\u03b1 (k 1 ) j | \u2265 |\u03b1 (k 2 ) j | \u2265 . . . \u2265 |\u03b1 (k K ) j |; (b) m * \u2190 arg max 1\u2264m\u2264K \" P m i=1 |\u03b1 (k i ) j |\u2212\u03bb \" /m; (c) For each i \u2208 {1, . . . , K} If i > m * Then \u03b2 (k i ) j \u2190 \u03b1 (k i ) j Else \u03b2 (k i ) j \u2190 sign(\u03b1 (k i ) j ) m * \" m * X =1 |\u03b1 (k ) j | \u2212 \u03bb # ; End; Output: b \u03b2 (k) j \u2190 \u03b2 (k) j", "formula_coordinates": [3.0, 75.37, 339.44, 219.78, 188.72]}, {"formula_id": "formula_11", "formula_text": "(k) j", "formula_coordinates": [3.0, 145.9, 576.86, 10.67, 14.07]}, {"formula_id": "formula_12", "formula_text": "(k1) j | \u2265 |\u03b1 (k2) j | \u2265 . . . \u2265 |\u03b1 (k K ) j |. Then the solution to (3) is \u03b2 (ki) j = sign(\u03b1 (ki) j ) m * m * i =1 |\u03b1 (k i ) j |\u2212\u03bb + \u20221 {i\u2264m * } +\u03b1 (ki) j \u20221 {i>m * } where m * = arg max m 1 m m i =1 |\u03b1 (k i ) j | \u2212 \u03bb , 1 {\u2022} is", "formula_coordinates": [3.0, 55.44, 592.4, 234.79, 114.55]}, {"formula_id": "formula_13", "formula_text": "R (k) j \u2212 \u03b2 (k) j X (k) j T X (k) j = \u03bb\u03b7 k \u2200k \u2208 {1, . . . , K}, (5) where {\u03b7 k } K k=1 satisfy \u03b7 \u2261 (\u03b7 1 , . . . , \u03b7 K ) T \u2208 \u2202 \u2022 \u221e \u03b2j .", "formula_coordinates": [3.0, 307.44, 191.78, 236.15, 45.24]}, {"formula_id": "formula_14", "formula_text": "Lemma 3. The subdifferential of \u2022 \u221e in R K is \u2202 \u2022 \u221e x = {\u03b7 : \u03b7 1 \u2264 1} x = 0 conv{sign(x i )e i : |x i | = x \u221e } o.w.(6)", "formula_coordinates": [3.0, 307.44, 311.03, 234.0, 50.4]}, {"formula_id": "formula_15", "formula_text": "(k) j in (4), if \u03b2 (k) j = 0, then sign( \u03b2 (k) j ) = sign(\u03b1 (k) j ).", "formula_coordinates": [3.0, 307.44, 407.26, 233.5, 29.6]}, {"formula_id": "formula_16", "formula_text": "K k=1 |\u03b1 (k) j | \u2264 \u03bb.", "formula_coordinates": [3.0, 479.48, 527.21, 61.09, 30.55]}, {"formula_id": "formula_17", "formula_text": "K k=1 |\u03b7 k | \u2264 1 and \u03bb\u03b7 k = R (k) j T X (k) j = \u03b1 (k) j . (7", "formula_coordinates": [3.0, 360.61, 582.16, 180.83, 40.8]}, {"formula_id": "formula_18", "formula_text": ")", "formula_coordinates": [3.0, 537.2, 612.02, 4.24, 8.74]}, {"formula_id": "formula_19", "formula_text": "If K k=1 |\u03b1 (k) j | \u2264 \u03bb, choosing \u03b7 k as in (7) would guar- antee that K k=1 |\u03b7 k | \u2264 1, therefore \u03b2 j = 0.", "formula_coordinates": [3.0, 307.44, 640.59, 234.0, 29.23]}, {"formula_id": "formula_20", "formula_text": "\u03bb\u03b7 k = \u03b1 (k) j , k = 1, . . . , K and K k=1 |\u03b7 k | \u2264 1. This implies that K k=1 |\u03b1 (k) j | \u2264 \u03bb.", "formula_coordinates": [3.0, 307.44, 689.59, 234.0, 29.81]}, {"formula_id": "formula_21", "formula_text": "K k=1 |\u03b1 (k) j | > \u03bb and |\u03b1 (k1) j | \u2212 \u03bb > |\u03b1 (k2) j |.", "formula_coordinates": [4.0, 55.44, 67.53, 234.0, 29.6]}, {"formula_id": "formula_22", "formula_text": "(k) j = \u03b1 (k) j for \u2200k = k 1 , while \u03b2 (k1) j = sign \u03b1 (k1) j |\u03b1 (k1) j | \u2212 \u03bb . Proposition 6. | \u03b2 (k1) j | > | \u03b2 (k) j | for \u2200k = k 1 if and only if |\u03b1 (k1) j | \u2212 \u03bb > |\u03b1 (k2) j |.", "formula_coordinates": [4.0, 55.44, 83.06, 234.0, 73.54]}, {"formula_id": "formula_23", "formula_text": "If | \u03b2 (k1) j | > | \u03b2 (k) j | for \u2200k = k 1 , this implies that \u2202 \u2022 \u221e \u03b2j = e k1", "formula_coordinates": [4.0, 55.44, 167.55, 234.0, 28.87]}, {"formula_id": "formula_24", "formula_text": "R (k) j \u2212 \u03b2 (k) j X (k) j T X (k) j = \u03bbsign( \u03b2 (k1) j )1 {k=k1} .", "formula_coordinates": [4.0, 61.39, 208.99, 203.32, 17.89]}, {"formula_id": "formula_25", "formula_text": "(k1) j = \u03b1 (k1) j \u2212 \u03bbsign( \u03b2 (k1) j", "formula_coordinates": [4.0, 152.06, 236.67, 112.91, 14.07]}, {"formula_id": "formula_26", "formula_text": "\u03b2 (k) j = \u03b1 (k) j for \u2200k = k 1 . Combined with the fact | \u03b2 (k1) j | > | \u03b2 (k) j | for \u2200k = k 1 , we get |\u03b1 (k1) j \u2212 \u03bbsign( \u03b2 (k1) j )| > |\u03b1 (k) j | for \u2200k = k 1 .", "formula_coordinates": [4.0, 55.44, 252.2, 234.0, 45.14]}, {"formula_id": "formula_27", "formula_text": "(k1) j ) = sign( \u03b2 (k1) j ). Further, if \u03b2 (k1) j > 0, then |\u03b1 (k1) j | > \u03bb, we have |\u03b1 (k1) j \u2212 \u03bbsign(\u03b1 (k1) j )| = |\u03b1 (k1) j | \u2212 \u03bb. Therefore, |\u03b1 (k1) j | \u2212 \u03bb > |\u03b1 (k2) j |. This is also true for \u03b2 (k1) j < 0. On the other hand, assuming |\u03b1 (k1) j | \u2212 \u03bb > |\u03b1 (k2) j | but for some n > 1, there exist | \u03b2 (k1) j | = . . . = | \u03b2 (kn) j | = \u03b2 j \u221e . (8", "formula_coordinates": [4.0, 55.44, 304.78, 234.0, 116.22]}, {"formula_id": "formula_28", "formula_text": ")", "formula_coordinates": [4.0, 285.2, 410.07, 4.24, 8.74]}, {"formula_id": "formula_29", "formula_text": "a 1 + a 2 \u2264 1, for h = 1, 2, R (k h ) j \u2212 \u03b2 (k h ) j X (k h ) j T X (k h ) j = \u03bba h sign( \u03b2 (k h ) j", "formula_coordinates": [4.0, 61.39, 442.73, 228.05, 28.18]}, {"formula_id": "formula_30", "formula_text": "(k1) j \u2212 \u03bba 1 sign( \u03b2 (k1) j )| = |\u03b1 (k2) j \u2212 \u03bba 2 sign( \u03b2 (k2) j )|. By Proposition 4 and |\u03b1 (k1) j | > \u03bba 1 , we have |\u03b1 (k1) j \u2212 \u03bba 1 sign( \u03b2 (k1) j )| = |\u03b1 (k1) j |\u2212\u03bba 1 . If |\u03b1 (k2) j | > \u03bba 2 , we get |\u03b1 (k1) j | \u2212 \u03bb sign(a 1 \u03b2 (k1) j ) + a 2 sign( \u03b2 (k2) j ) = |\u03b1 (k2) j |. Since a 1 + a 2 \u2264 1, this obviously contradicts with the assumption that |\u03b1 (k1) j | \u2212 \u03bb > |\u03b1 (k2) j |. The same result also hold for the case |\u03b1 (k2) j | \u2264 \u03bba 2 .", "formula_coordinates": [4.0, 55.44, 480.7, 234.0, 120.52]}, {"formula_id": "formula_31", "formula_text": "K k=1 |\u03b1 (k) j | > \u03bb and |\u03b1 (k1) j | \u2212 \u03bb \u2264 |\u03b1 (k2) j |.", "formula_coordinates": [4.0, 55.44, 608.66, 234.0, 29.6]}, {"formula_id": "formula_32", "formula_text": "Proposition 7. For m > 1, if there are precisely m entries | \u03b2 (k1) j |, . . . , | \u03b2 (km) j | achieve \u03b2 j \u221e > 0, then \u03b2 (ki) j = sign(\u03b1 (ki) j ) m m =1 |\u03b1 (k ) j | \u2212 \u03bb \u2200i = 1, . . . , m.", "formula_coordinates": [4.0, 55.44, 649.61, 234.0, 65.81]}, {"formula_id": "formula_33", "formula_text": "(k ) j \u2212 \u03b2 (k ) j X (k ) j T X (k ) j = \u03bba sign( \u03b2 (k ) j", "formula_coordinates": [4.0, 320.96, 118.26, 177.48, 17.89]}, {"formula_id": "formula_34", "formula_text": "\u03b1 (k ) j = \u03bba sign( \u03b2 (k ) j ) + \u03b2 (k ) j \u2200 \u2208 {1, . . . , m}. (9)", "formula_coordinates": [4.0, 310.58, 165.44, 230.86, 14.07]}, {"formula_id": "formula_35", "formula_text": "(k1) j | = . . . = | \u03b2 (km) j", "formula_coordinates": [4.0, 411.11, 198.0, 97.12, 14.07]}, {"formula_id": "formula_36", "formula_text": "m =1 |\u03b1 (k ) j | = m =1 |\u03bba sign( \u03b2 (k ) j ) + \u03b2 (k ) j |. Since |\u03bba sign( \u03b2 (k ) j ) + \u03b2 (k ) j | = \u03bba + | \u03b2 (k ) j | and m =1 a = 1, we have | \u03b2 (ki) j | = 1 m m =1 |\u03b1 (ki) j | \u2212 \u03bb . \u2200i \u2208 {1, . . . , m}. (10)", "formula_coordinates": [4.0, 307.44, 223.29, 234.0, 87.95]}, {"formula_id": "formula_37", "formula_text": "(ki) j ) = sign( \u03b2 (ki) j ) for i = 1, . . . , m, therefore \u03b2 (ki) j = sign(\u03b1 (ki) j ) m m =1 |\u03b1 (k ) j | \u2212 \u03bb i = 1, . . . , m.", "formula_coordinates": [4.0, 307.44, 323.39, 234.0, 73.4]}, {"formula_id": "formula_38", "formula_text": "Proposition 8. For m > 1, there exist precisely m entries | \u03b2 (k1) j |, . . . , | \u03b2 (km) j | that achieve \u03b2 j \u221e > 0 if and only if |\u03b1 (k1) j | \u2212 \u03bb \u2264 |\u03b1 (k2) j | and |\u03b1 (km) j | \u2265 1 m\u22121 m\u22121 =1 |\u03b1 (k ) j | \u2212 \u03bb and |\u03b1 (km+1) j | < 1 m m =1 |\u03b1 (k ) j | \u2212 \u03bb . Proof to Proposition 8: Assume exactly m > 1 entries | \u03b2 (k1) j |, . . . , | \u03b2 (km) j | achieve \u03b2 j \u221e > 0, from Proposi- tion 7, we know that for i = 1, . . . , m, \u03b2 (ki) j = sign(\u03b1 (ki) j ) m m =1 |\u03b1 (k ) j | \u2212 \u03bb .", "formula_coordinates": [4.0, 307.44, 486.08, 234.0, 168.41]}, {"formula_id": "formula_39", "formula_text": "a = \u03b1 (k ) j \u2212 \u03b2 (k ) j \u03bbsign( \u03b2 (k ) j ) = |\u03b1 (k ) j | \u2212 | \u03b2 (k ) j | \u03bb \u2208 {1, . . . , m}.", "formula_coordinates": [4.0, 307.44, 688.64, 233.63, 31.4]}, {"formula_id": "formula_40", "formula_text": "|\u03b1 (km) j | \u2265 1 m \u2212 1 m\u22121 =1 |\u03b1 (k ) j | \u2212 \u03bb . (12) Further, since | \u03b2 (km) j | > | \u03b2 (km+1) j |, the necessity follows from |\u03b1 (km+1) j | = | \u03b2 (km+1) j | < | \u03b2 (km) j | = 1 m m =1 |\u03b1 (k ) j | \u2212 \u03bb .", "formula_coordinates": [5.0, 55.44, 91.89, 234.0, 80.9]}, {"formula_id": "formula_41", "formula_text": "entries | \u03b2 (k1) j |, . . . , | \u03b2 (kn) j", "formula_coordinates": [5.0, 55.44, 191.77, 104.57, 14.07]}, {"formula_id": "formula_42", "formula_text": "\u03b2 j \u221e > 0 if and only if m * = arg max m 1 m m =1 |\u03b1 (k ) j | \u2212 \u03bb .", "formula_coordinates": [5.0, 55.44, 248.71, 234.0, 26.9]}], "doi": ""}