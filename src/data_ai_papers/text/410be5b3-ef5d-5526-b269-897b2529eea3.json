{"title": "DICNet: Deep Instance-Level Contrastive Network for Double Incomplete Multi-View Multi-Label Classification", "authors": "Chengliang Liu; Jie Wen; Xiaoling Luo; Chao Huang; Zhihao Wu; Yong Xu", "pub_date": "2023-03-23", "abstract": "In recent years, multi-view multi-label learning has aroused extensive research enthusiasm. However, multi-view multilabel data in the real world is commonly incomplete due to the uncertain factors of data collection and manual annotation, which means that not only multi-view features are often missing, and label completeness is also difficult to be satisfied. To deal with the double incomplete multi-view multilabel classification problem, we propose a deep instance-level contrastive network, namely DICNet. Different from conventional methods, our DICNet focuses on leveraging deep neural network to exploit the high-level semantic representations of samples rather than shallow-level features. First, we utilize the stacked autoencoders to build an end-to-end multiview feature extraction framework to learn the view-specific representations of samples. Furthermore, in order to improve the consensus representation ability, we introduce an incomplete instance-level contrastive learning scheme to guide the encoders to better extract the consensus information of multiple views and use a multi-view weighted fusion module to enhance the discrimination of semantic features. Overall, our DICNet is adept in capturing consistent discriminative representations of multi-view multi-label data and avoiding the negative effects of missing views and missing labels. Extensive experiments performed on five datasets validate that our method outperforms other state-of-the-art methods.", "sections": [{"heading": "Introduction", "text": "As one of the important tasks of multi-label learning, the purpose of multi-label classification task is to label the observed samples with various category tags (Zhu et al. 2018;Herrera et al. 2016). For example, in the field of image recognition, a natural picture can be labeled with multiple labels such as 'wild', 'bird', and 'sky'. Or in a text classification task, a piece of text can be classified into different semantic sets such as 'soccer', 'news', 'advertising', etc. The broad application prospect of multi-label classification has aroused great research enthusiasm in both industry and academia (Liu et al. 2015). In addition, with the explosive growth of data sources and feature extraction methods, only describing, analyzing, and processing samples from a single perspective can no longer meet the needs of more complex and comprehensive analysis (Hu, Shi, and Ye 2020;Yuan et al. 2021;Fang et al. 2021). Doubtlessly, multi-view data collected from multiple sources is able to describe the observed objects more integrally and accurately (Li, Wan, and He 2021;Li and He 2020;Wang et al. 2022). For example, in clinical practice, multiple indicators such as height, weight, and average hemoglobin are often used to synthetically diagnose whether a person is malnourished (Hu and Chen 2019;Luo et al. 2021;Zhu et al. 2022). Obviously, multi-view data is rich in more semantic information, which greatly facilitates the learning of multi-label semantic content (Huang et al. 2015;Wang et al. 2020;Hu, Lou, and Ye 2021). Therefore, different from the simple single-label classification task (Zhao et al. 2022), this paper focuses on the multi-view multi-label classification task, namely MVMLC.\nFor MVMLC, a few meaningful methods have been proposed in recent years. Zhu et al. proposed a multi-view label embedding model, which learns the intermediate latent space through the Hilbert-Schmidt Independence Criterion (Gretton et al. 2005) to indirectly bridge the feature space and the label space (Zhu et al. 2018). Another representative matrix factorization (MF) based MVMLC method, named latent semantic aware multi-view multi-label learning (LSA-MML), aligns the semantic spaces by maximizing the dependencies of basis matrices corresponding to different views in the kernel space (Zhang et al. 2018). In addition, deep neural networks (DNN) have also been developed to handle this issue (Liu et al. 2023). For example, Fang et al. proposed the simultaneously combining multiview multi-label learning (SIMM) neural network framework, which exploits adversarial loss and label loss to learn shared semantics and imposes the regularization constraint to obtain view-specific information (Fang and Zhang 2012). It is worth noting that these methods are invariably based on the unreasonable assumption that all views and labels are available. However, in practice, the data used for MVMLC is often incomplete. We consider this incompleteness from two aspects: On the one hand, the heterogenous data collected from multiple sources may be with missing views due to various reasons. For example, the media forms of records stored in archive may include text, audio, video, etc (Chen, Wang, and Lai 2022;Chen et al. 2022b). These diverse media of information regarded as different views are not ubiquitously present in all records, so the multi-view features extracted from them are naturally incomplete; On the other hand, since it is difficult and expensive to manually tag all labels, label information belonging to real datasets is often missing to varying degrees, especially for observed objects with numerous strongly correlated labels (Fang and Zhang 2012;Zhang et al. 2018). To sum up, unlike most existing works designed for the single-missing case, we contribute to dealing with the double-missing case, where random view-missing and label-missing occur simultaneously, i.e., double incomplete multi-view multi-label classification issue (DIMVMLC).\nObviously, both missing views and missing labels have serious impacts on multi-view multi-label learning (Xu, Tao, and Xu 2015;Liu et al. 2020;Liu, Sun, and Feng 2022). From the perspective of multi-view, missing views not only weaken the rich semantics of the original multi-view information, but also make the information fusion of unaligned multi-view more difficult due to the uncertain missing distribution compared with intact and aligned multiview data. From the perspective of multi-label, the absence of non-specific labels not only impairs its supervision, but also poses challenges to the construction of unified learning model (Tan et al. 2017;Huang et al. 2019;Zhao and Guo 2015). Even so, some methods for incomplete multiview learning (IMVL) and incomplete multi-label learning (IMLL) have been gradually proposed over the last few years, such as iMSF (Yuan et al. 2012), iMvWL (Tan et al. 2018), NAIML (Li and Chen 2021), etc (more detailed introduction in next section). Although these conventional methods represented by iMvWL have achieved certain results in the fields of IMVL and IMLL, it is the learning mode, which requires hand-designed feature extraction rules and is hard to generalize, that restricts the further development of incomplete multi-view multi-label learning. With the widespread popularity of deep learning, DNNs are increasingly applied in feature extraction and data analysis domain (Huang et al. 2022b,a). Compared with conventional multi-view learning methods, DNN shows irreplaceable natural advantages (Wen et al. 2020;Li et al. 2022). Specifically, for one thing, traditional methods, whether based on MF, spectral clustering or kernel learning, are only capable of exploiting the shallow features of data (Zong, Zhang, and Liu 2018;Wang et al. 2019;Liu et al. 2019). However, capturing relatively high-level semantic content, which is DNNfriendly, is increasingly proving to be necessary, especially in complex multi-label classification tasks (Wen et al. 2020); for another, the performance of traditional multi-view learning models is heavily dependent on the parameter settings, and usually requires searching for optimal parameter combinations for different datasets. On the contrary, DNN enjoys the advantages of parameter adaptive learning, and the model design is more concise. In addition, the trained DNN has end-to-end reasoning capability, which makes it more suitable for application scenarios that require fast prediction instead of re-learning classification in database like most of traditional methods (Luo et al. 2023).\nTherefore, in this paper, we propose a method named deep instance-level contrastive network (DICNet) for DIMVMLC. The DICNet consists of four parts: viewspecific representation learning framework, instance-level contrastive learning module, weighted fusion module, and incomplete multi-label classification module. Inspired by (Zhang, Liu, and Fu 2019), our view-specific representation learning framework builds the basic high-level feature extraction and reconstruction network. On this basis, considering that instances corresponding to the same sample but from different views should contain consistent semantic features (i.e. consensus assumption), we introduce an incomplete instance-level contrastive loss on the extracted viewspecific high-level representations to improve the cross-view consensus. Furthermore, in order to leverage the complementarity across views to obtain more discriminative semantic features, we design a weighted representations fusion module with prior missing-view information, which greatly mitigates the impact of the absence. Finally, a missing-label indicator matrix is introduced into the classifier to avoid invalid supervision information. Overall, compared with existing approaches, our DICNet proposed in this paper has the following outstanding contributions:\n\u2022 To the best of our knowledge, our DICNet is the first DNN framework for the DIMVMLC task, which can cope with all sorts of incomplete cases, including missing labels and missing views. Furthermore, as a flexible end-to-end neural network, our approach is capable of training in a supervised or semi-supervised manner and performing real-time predictions, which is not possible with conventional methods. \u2022 Our DICNet focuses on extracting and learning highlevel semantic features. On the one hand, the powerful incomplete instance-level contrastive learning guides the encoders to extract cross-view semantic features with better consensus. On the other hand, the weighted fusion strategy fully integrates complementary information without negative effects of missing views. \u2022 We conduct extensive experiments on five datasets, and the results establish that our DICNet significantly outperforms other benchmark methods on four key metrics.", "publication_ref": ["b53", "b13", "b29", "b17", "b45", "b8", "b24", "b23", "b38", "b15", "b33", "b54", "b20", "b39", "b16", "b52", "b10", "b53", "b48", "b27", "b9", "b3", "b2", "b9", "b48", "b43", "b30", "b31", "b35", "b21", "b51", "b46", "b34", "b25", "b41", "b26", "b55", "b37", "b32", "b41", "b34", "b47"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries Problem Formulation", "text": "For convenience, we define X (v) \u2208 R n\u00d7mv l v=1 as input data with l views, where n and m v denote the number of samples and dimensionality of v-th view. And we let Y \u2208 0, 1 n\u00d7c represents the label matrix, where c is the number of tags. Y i,: is the label vector of the sample i and Y i,j = 1 if the sample i belongs to class j, otherwise Y i,j = 0. Additionally, for missing-view and missing-label, we use indicator matrix W \u2208 0, 1 n\u00d7l and G \u2208 0, 1 n\u00d7c to describe the missing instances distribution, respectively. Specifically, we set W i,j = 1 if the instance of j-th view corresponding to i-th sample is available. Otherwise, we let W i,j = 0 for the missing j-th view of i-th sample and set 'NaN' or random noise at the missing position in raw data. Similarly, G i,j = 1 means j-th tag of i-th sample is existed and G i,j = 0 for the absence or uncertainty of such\n(\") ($) (%)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Weighted fusion", "text": "Fusion:\n= ( :,# ) (#) + ( :,& ) (&) +\u2026+ ( :,' ) (')", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "classifier", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "BCE loss", "text": "Incomplete Instance-level contrastive loss\n(\") ($) (%) Encoder Encoder Encoder Decoder Decoder Decoder & (\") & ($) & (%) (\") ($) (%)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MSE loss", "text": "Encoder\nFC ReLU FC BN FC ReLU FC ReLU Decoder FC classifier FC Sigmoid\nFigure 1: The main framework of our DICNet. The input data is processed by the encoders and then output to the weighted fusion module, the incomplete instance-level contrastive module, and the decoders, respectively. The structures of the encoders, decoders and classifier are shown at the bottom.\na tag. The goal of our DICNet is to learn a reliable classifier to predict multi-class labels for unlabeled test samples by given input, i.e., multi-view data X (v) l v=1 , mutli-label Y , missing-view indicator W, and missing-label indicator G.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Relevant IMVL or IMLL Methods", "text": "In the past few years, some methods for IMVL and IMLL have been developed. Yuan et al. proposed a multi-source feature learning method (iMSF), which cleverly divides the incomplete dataset into multiple complete subsets according to missing prior information and obtains a joint representation by imposing the structural sparse regularization constraint (Yuan et al. 2012). However, iMSF can only handle single-label classification tasks rather than multi-label tasks. Another commendable IMLL algorithm is MvEL-ILD (i.e., multi-view embedding learning for incompletely labeled data), which leverages canonical correlation analysis to map original features to common space and construct similarity matrix to fuse multi-view information . For incomplete semantic labels, MvEL-ILD attempts to construct the graph neighbor constraint based on the correlation of labels' semantic content for consistent prediction results. But MvEL-ILD is only suitable for complete multi-view data, ignoring possible missing-view scenarios. To cope with the DIMVMLC issue, Tan et al. designed a model named iMvWL, which consists of two parts-nonnegative MF based IMVL model and label correlation learning based IMLL model (Tan et al. 2018). IMVML bridges the feature space and the semantic space by learning a common representation and imposes low-rank constraint on the label correlation matrix to enhance the predictive power of the model. In addition, Li et al. proposed a non-aligned DIMVMLC model, named NAIML, which introduce a nonaligned constraint to complicate the classification task (Li and Chen 2021). The NAIML is the first to consider the global high-rank property of entire label matrix and the lowrank property of sub-label matrix synchronously.", "publication_ref": ["b46", "b34", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "In this section, we propose a novel deep neural network framework named DICNet for the DIMVMLC task. We will explain our model from the following four aspects: viewspecific representation learning, instance-level contrastive learning, incomplete multi-view weighted fusion strategy, and weighted multi-label classification module.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "View-specific Representation Learning", "text": "It is well known that the raw data contains non-ignorable noise and redundant information, which is not conducive to the learning of semantic content Wen et al. 2022). Therefore, both traditional methods and deep learning methods are devoted to capturing discriminative representation from original feature. Similar to other deep multiview learning works (Wen et al. 2020), we utilize the autoencoder to extract high-level feature instead of focusing on the shallow-level feature like traditional methods. Concretely, our autoencoder is composed of an encoder and a decoder, which are used to extract high-level feature and reconstruct the original data respectively. Each view enjoys its own coder-decoder for capturing the view-specific discriminative information independently. For the v-th view, we can define\nZ (v) = E (v) X (v) , \u03b8 (v) andX (v) = D (v) Z (v) , \u03c8 (v) ,\nwhere Z (v) \u2208 R n\u00d7d is view-specific high-level feature and X (v) denotes the reconstructed feature. d is the pre-defined dimensionality of Z (v) . E (v) and D (v) represent the encoder and decoder, respectively. \u03b8 (v) and \u03c8 (v) are network parameters corresponding to E (v) and D (v) . As shown in Fig. 1, our encoder and decoder can be regarded as two Multilayer Perceptrons (MLPs) with several fully connected (FC) layers.\nBesides, considering the incomplete multi-view data, following (Wen et al. 2020), we introduce the missing-view index matrix W to avoid the negative effects during the training process, so the weighted reconstruction loss is formulated as:\nL F R = 1 l l v=1 L (v) F R = 1 l l v=1 1 mv n i=1 x (v) i \u2212 x (v) i 2 2 Wi,v ,(1)\nwhere\nx (v) i andx (v) i\ndenote the i-th instance of view v and its reconstructed feature. L (v) F R is the reconstruction loss between input X (v) and outputX (v) , and L F R represents the mean reconstruction loss of all views.", "publication_ref": ["b40", "b41", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Incomplete Instances-level Contrastive Learning", "text": "Through the view-specific representation learning network, we can obtain l high-level representations Z (v) l v=1 output by the encoders. However, these representations inevitably retain a lot of view-private information since minimizing the reconstruction error will force the encoders to capture the complete information of each view as much as possible, which is obviously not conducive to learning discriminative representations based on the consensus assumption. Considering that the instances, which is belonging to the same sample but from different views, should enjoy similar semantic representation and inspired by (Hinton, Osindero, and Teh 2006;Chen et al. 2020;Xu et al. 2022), we introduce the incomplete instance-level contrastive learning to help the encoders extract more consistent high-level features.\nSpecifically, Z (v) l v=1 learned from the encoders contains l \u00d7 n high-level features corresponding to the instances across all views (including missing instances), and we refer to z (v) i \u2208 R d as instance of sample i in view v for convenience. First, we mark all instances in Z (v) l v=1 with three categories: (1) anchor instance A = z\n(v) i , (2) positive in- stance A + = z (u) i u =v\nthat belongs to the sample i but not in view v, and (3) negative instance A \u2212 = z (u) j j =i for remainders. It should be noted that the positive and negative instances here are relative to the anchor instance, and each instance can be selected as the anchor instance. Then, we let the anchor instance be paired with another positive or negative instance, so we can get n-1 positive instance-pairs A, A + and n \u00d7 l \u2212 n negative instance-pairs A, A \u2212 , respectively. As shown in Fig. 2, it is our motivation that seeks to minimize the distance of available positive instancepairs and maximize that of available negative instance-pairs in feature space. In our incomplete instance-level contrastive learning, we adopt cosine similarity to measure the distance of instance-pairs (Chen et al. 2020;Xu et al. 2022):\nS(z (v) i , z (u) j ) = z (v) i \u2022 z (u) j z (v) i z (u) j ,(2)\nwhere \u2022 denotes the dot product operator, and our optimization goal is S(A, A + ) S(A, A \u2212 ). It is worth noting that, unlike (Xu et al. 2022), we introduce the missingview indicator matrix to exclude unavailable positive instance pairs in the process of calculating contrastive loss. To do this, the contrastive loss between Z (v) and Z (u) is:\nl (vu) IC = \u2212 1 n n i=1 Wi,vWi,u log exp(S z (v) i , z (u) i \u03c4 ) exp(S z (v) i , z (u) i \u03c4 ) + Sneg ,(3)\nwhere\nS neg = r=u,v n j=1,j =i exp(S z (v) i , z (r)j\n\u03c4 )W j,r , and \u03c4 represents the temperature parameter that controls the concentration extent of the distribution (Wu et al. 2018). Further, the total incomplete instance-level contrastive loss for all view-pairs are as follows:\nL IC = 1 2 l v=1 u =v l (vu) IC .(4)\nAs can be seen from Eq.(3), the contrastive loss for views v and u is in the form of cross-entropy loss, i.e., minimizing the negative log-likelihood estimator about the similarity distribution of instance pairs. In other words, the contrastive loss of i-th sample with respect to v-th view and u-th view will be computed only if both instances in the positive instance pair z\n(v) i , z (u) i are available.", "publication_ref": ["b14", "b4", "b44", "b4", "b44", "b44", "b42"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Incomplete Multi-view Weighted Fusion", "text": "From the perspective of views, each view inherently enjoys a unique description of the objectives that means the complementarity of view-level information should be exploited to learn a comprehensive sample representation. Indeed, obtaining a view-federated representation is necessary for masses of multi-view learning methods. However, some simple ways, such as concatenating or accumulating the individual representations of all views, are not suitable for incomplete multi-view data due to the possibility of random missing. Therefore, following (Wen et al. 2020;Chen et al. 2022a), a weighted fusion strategy is introduced to combine the multi-view complementary information while avoiding the negative effects of missing views:\nhi = l v=1 z (v) i Wi,v l v=1 Wi,v,(5)\nwhere z\n(v) i\nis the view-specific high-level discriminative representation extracted by encoder for v-th view of sample i. h i \u2208 R d is the fusion feature for i-th sample, i.e., samplespecific representation. Combining all the sample representations from 1 to n, we can obtain the united high-level semantic representation matrix H = h T 1 , h T 2 , . . . , h T n , which is also the input for next classification layer.", "publication_ref": ["b41", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Weighted Multi-label Classification Module", "text": "To obtain end-to-end multi-label prediction results, we design a simple classifier to connect the common semantic feature space and label space. Specifically, we expect the classifier to learn a proprietary 'template' for each category, which is used to match the sample feature and output the corresponding score. Based on this, we adopt an FC layer with c neurons as the main body of the classifier. Besides, a Sigmoid activate function is applied to ensure that the value of prediction is located in the range of [0, 1]. We formalize the classifier as follows:\nP = Sigmoid(F c (H, \u03c9)),(6)\nwhere the \u03c9 denotes the learnable parameters of the FC layer, and P \u2208 R n\u00d7c is our prediction matrix. Finally, following (Chen et al. 2019), we utilize the binary crossentropy loss, which is widely used in multi-label classification tasks, as the multi-label classification loss L M C to evaluate the difference between prediction and ground truth:\nL M C = \u2212 1 nc n i=1 c j=1\nYi,j log(Pi,j )\n+ (1 \u2212 Yi,j ) log(1 \u2212 Pi,j ) Gi,j ,(7)\nwhere Y i,j and P i,j represent the real label and prediction, respectively. It is worth noting that we introduce the missing-label indicator G to filter invalid missing tags, which is similar to the application of missing-view indicator matrix W in the weighted fusion module.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "Overall Loss and Complexity Analysis", "text": "The overall training loss of our DICNet can be calculated as:\nL = L M C + \u03b2L IC + \u03b3L F R ,(8)\nwhere \u03b2 and \u03b3 are penalty parameters for L IC and L F R , respectively. These losses are calculated only during the training phase, and the weight parameters of the DICNet are updated via back propagation. Our DICNet can be trained in a semi-supervised or supervised case, and Algorithm 1 shows the training process of DICNet in a semi-supervised manner.\nFor convenience, we reiterate the relevant symbols, i.e., nthe number of samples, l-the number of views, B-the batch size, c-the number of categories, D-the maximum number of neurons, d-the dimensionality of view-specific representation, and M -the maximum dimensionality of raw data. We adopt the min-batch training strategy, so for autoencoders in DNN, the computational complexity is O(BD 2 l). Besides, the complexity of L F R , L IC , and L M C is O(BM l), O(B 2 dl 2 ), and O(Bc), respectively. Therefore, the total computational complexity for training of our DICNet is O n B (B 2 dl 2 +BD 2 l+BM l+Bc . We can see that the total complexity increases linearly with the number of samples n.\nAlgorithm 1: Semi-supervised training process of DICNet Input: Incomplete multi-view data X (v) l v=1 with missing-view indicator matrix W \u2208 {0, 1} n\u00d7l , and corresponding multi-label matrix Y with missing-label indicator matrix G \u2208 {0, 1} n\u00d7c ; Batch size B; Hype-parameters \u03c4 , \u03b2, and \u03bb; Training epochs T ; Stopping threshold \u03c3. Initialization: Fill the missing elements of the multi-view data and multi-lable data with '0', and randomly initialize the network weights; Set L last = 0; Initialize prediction label P last of n t test samples. Output: Parameters of trained model.\nfor k=1 to T do 1.Compute the view-specific representations Z (v) l v=1 and L F R using (1). 2.Compute the instance-level contrastive loss L IC according to (4). 3.Compute the fusion representation H using (5). 4.Compute multi-classification loss according to (7). 5.Compute total loss L according to (8) and use the optimizer to update the network parameters batch to batch. 6.Input test samples and obtain prediction P . \nif |L last \u2212 L| < \u03c3 or 1 ntc i,j P i,j \u2295 P last i,j <", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section, we introduce our experimental setup and analysis to evaluate our method in detail. And the implementation of our DICNet is based on MindSpore and Pytorch.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "Datasets: Following (Tan et al. 2018;Guillaumin, Verbeek, and Schmid 2010;Li and Chen 2021), we select five popular multi-view multi-label datasets in our experiments, i.e., Corel 5k (Duygulu et al. 2002), VOC 2007(Everingham et al. 2009, ESP Game (Von Ahn and Dabbish 2004), IAPR TC-12 (Grubinger et al. 2006), and MIR FLICKR (Huiskes and Lew 2008). For all five multi-view multi-label datasets, we uniformly select six types of features as six views, i.e., GIST, HSV, DenseHue, DenseSift, RGB, and LAB.\nDouble incomplete multi-view and multi-label data preparation: Following (Tan et al. 2018), we construct double incomplete multi-view multi-label datasets for training and testing based on the above five datasets. For all samples, we first randomly disable p% of instances of every view to build incomplete data (at least one view per sample is available to keep the total number of samples constant). Then, we randomly select m percent of the samples as the training set and the rest as the test set. Finally, we randomly remove q% of positive labels and q% of negative labels. After above three steps, we construct a dataset with p% missing-view rate, q% missing-label rate, and m% training samples.\nCompared with related methods: In our experiments, we select six state-of-the-art methods to compare with our DICNet. Four of them are introduced in the preliminaries, i.e., MvEL-ILD, iMSF, iMvWL, and NAIML. In addition, we briefly introduce the other two comparison methods: (1) lrMMC (Liu et al. 2015), a complete MF based MvMLC method, attempt to preserve the low-rank property of original features. (2) MVL-IV (Xu, Tao, and Xu 2015) is an IMVL model based on the missing-view recovery strategy. Notably, only iMVWL and NAIML can handle both incomplete views and incomplete tags, so we have to make some adjustments to the other four methods as (Tan et al. 2018) did: For MvEL-ILD and lrMMC, we fill missing-view with average value; For iMSF and MVL-IV, corresponding missing tags are set as negative tags. In addition, for a fair comparison, optimal parameters for the six approaches are selected as mentioned in their papers, and ten replicates are conducted to reduce the randomness of results.\nEvaluation metrics: Similar to (Tan et al. 2018) and (Li and Chen 2021), four popular metrics commonly used in the multi-label learning field are adopted to evaluate these approaches. i.e., Ranking Loss (RL), Average Precision (AP), Hamming Loss (HL), and adapted area under curve (AUC) (Bucak, Jin, and Jain 2011;Zhang and Zhou 2013). In particular, to facilitate the observation and comparison of the performance of different methods, we show the value of 1-RL and 1-HL instead of RL and HL in our report. Thus, a more intuitive criterion for comparison is: higher values of the four metrics mean better performance.", "publication_ref": ["b34", "b12", "b25", "b6", "b7", "b11", "b22", "b34", "b29", "b43", "b34", "b34", "b25", "b0", "b49"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results and Analysis", "text": "The statistical results of repeated experiments of seven methods on five aforementioned databases with 50% missing-view rate, 50% missing-label rate, and 70% training samples are listed in Table 1. And the results of the comparison methods are quoted from (Li and Chen 2021) and (Tan et al. 2018). Values in parentheses represent the standard deviation. From the Table 1, it is easy to obtain the following information:\n\u2022 Compared with the first four methods, which are not designed for DIMVMLC tasks, the iMvWL, NAIML, and our DICNet enjoy obvious performance advantages on all five datasets. For instance, the values about AP of iMvWL and DICNet exceed lrMMC by 7 and 14 percentage points on the Corel5k dataset, respectively. We have reason to believe that the weighted fusion strategy based on prior missing information plays a positive role in reducing the negative effects of missing views and labels. It is this comprehensive consideration that helps the model adapt to DIMVMLC tasks. \u2022 In comparison with the other six methods, our proposed DICNet has a bright performance, which is top in almost all metrics. In particular, on the most representative AP value, our DICNet is about 8 percentage points higher than the second-best NAIML on the corel5k database.\nEven on large-scale MIR FLICKR dataset with 25, 000 samples, its lead remained significant. These results illustrate that DNN is able to extract high-level discriminative features more effectively than traditional methods.\nTo further investigate the impact of different missing-view and missing-label ratios on classification performance, we conduct our DICNet on the Corel5k dataset and report the results in Fig. 3. Specifically, we fix one incomplete ratio at 50%, and then alter another incomplete ratio to 0, 30%, 50%, and 70%, respectively to observe the variation trends of four metrics. From Fig. 3, we can distinctly see that: (1) As the incompleteness rate of views or labels increases, the values of four metrics, especially the AP value, gradually decrease.\n(2) At the same miss rate, partial views have a greater impact on performance than partial tags to some extent. These intuitive phenomena once again verify the harmfulness of missing views and missing labels. Meanwhile, it can be inferred that our model is more dependent on the extraction and learning of high-level features than the supervision information in labels, which is still an open question.", "publication_ref": ["b25", "b34"], "figure_ref": ["fig_1", "fig_1"], "table_ref": ["tab_2", "tab_2"]}, {"heading": "Hyper-parameter Analysis and Ablation Study", "text": "In our DICNet, there are three hyper-parameters, i.e., \u03b2, \u03b3, and \u03c4 that need to be set before training. In order to study the sensitivity of our DICNet to the three hyper-parameters, we experiment on the corel5k dataset and pascal07 dataset with 50% available instances, 50% missing labels, and 70% training samples. Fig. 4a and Fig. 4b show the AP value versus hyper-parameters \u03b2 and \u03b3. Fig. 4c and Fig. 4d plot the curves of AP w.r.t the selection of \u03c4 respectively. Irrelevant hyper-parameters are fixed to guarantee the validity of all experiments. Obviously, when the hyper-parameters \u03b2 and \u03b3 are correspondingly selected from the ranges of [5e-4, 5e-3] and [5e-2, 1e-1] for the corel5k dataset and [5e-3, 1e-5] and [1e-3, 5e-1] for the pascal07 dataset, our DICNet can achieve relatively stable and satisfactory performance. As for temperature parameter \u03c4 , it seems to have an inappreciable impact on performance, so we set it to 0.5 for all datasets.\nTo verify the effectiveness of various parts of our method, we perform ablation experiments on the corel5k and pas-cal07 datasets with 50% instances, 50% missing labels, and 70% training samples. First, we select the multi-label classification loss L M C , which is essential for supervised or semi-supervised classification tasks, as our benchmark, and AP 0.240\u00b10.002 0.240\u00b10.001 0.204\u00b10.002 0.189\u00b10.002 0.283\u00b10.007 0.309\u00b10.004 0.381\u00b10.004 1-HL 0.954\u00b10.000 0.954\u00b10.000 0.946\u00b10.000 0.943\u00b10.000 0.978\u00b10.000 0.987\u00b10.000 0.988\u00b10.000 1-RL 0.762\u00b10.002 0.756\u00b10.001 0.638\u00b10.003 0.709\u00b10.005 0.865\u00b10.003 0.878\u00b10.002 0.882\u00b10.004 AUC 0.763\u00b10.002 0.762\u00b10.001 0.715\u00b10.001 0.663\u00b10.005 0.868\u00b10.003 0.881\u00b10.002 0.884\u00b10.004 VOC 2007 AP 0.425\u00b10.003 0.433\u00b10.002 0.358\u00b10.003 0.325\u00b10.000 0.441\u00b10.017 0.488\u00b10.003 0.505\u00b10.012 1-HL 0.882\u00b10.000 0.883\u00b10.000 0.837\u00b10.000 0.836\u00b10.000 0.882\u00b10.004 0.928\u00b10.001 0.929\u00b10.001 1-RL 0.698\u00b10.003 0.702\u00b10.001 0.643\u00b10.004 0.568\u00b10.000 0.737\u00b10.009 0.783\u00b10.001 0.783\u00b10.008 AUC 0.728\u00b10.002 0.730\u00b10.001 0.686\u00b10.005 0.620\u00b10.001 0.767\u00b10.012 0.811\u00b10.001 0.809\u00b10.006 ESP GAME AP 0.188\u00b10.000 0.189\u00b10.000 0.132\u00b10.000 0.108\u00b10.000 0.242\u00b10.003 0.246\u00b10.002 0.297\u00b10.002 1-HL 0.970\u00b10.000 0.970\u00b10.000 0.967\u00b10.000 0.964\u00b10.000 0.972\u00b10.000 0.983\u00b10.000 0.983\u00b10.000 1-RL 0.777\u00b10.001 0.778\u00b10.000 0.683\u00b10.002 0.722\u00b10.002 0.807\u00b10.001 0.818\u00b10.002 0.832\u00b10.001 AUC 0.783\u00b10.001 0.784\u00b10.000 0.734\u00b10.001 0.674\u00b10.003 0.813\u00b10.002 0.824\u00b10.002 0.836\u00b10.001 IAPR TC-12 AP 0.197\u00b10.000 0.198\u00b10.000 0.141\u00b10.000 0.101\u00b10.000 0.235\u00b10.004 0.261\u00b10.001 0.323\u00b10.001 1-HL 0.967\u00b10.000 0.967\u00b10.000 0.963\u00b10.000 0.960\u00b10.000 0.969\u00b10.000 0.981\u00b10.000 0.981\u00b10.000 1-RL 0.801\u00b10.000 0.799\u00b10.001 0.725\u00b10.001 0.631\u00b10.000 0.833\u00b10.003 0.848\u00b10.001 0.873\u00b10.001 AUC 0.805\u00b10.000 0.804\u00b10.001 0.746\u00b10.001 0.665\u00b10.001 0.836\u00b10.002 0.850\u00b10.001 0.874\u00b10.001 MIR FLICKR AP 0.441\u00b10.001 0.449\u00b10.001 0.375\u00b10.000 0.323\u00b10.000 0.495\u00b10.012 0.551\u00b10.002 0.589\u00b10.005 1-HL 0.839\u00b10.000 0.839\u00b10.000 0.778\u00b10.000 0.775\u00b10.000 0.840\u00b10.003 0.882\u00b10.001 0.888\u00b10.002 1-RL 0.802\u00b10.001 0.808\u00b10.001 0.771\u00b10.001 0.641\u00b10.001 0.806\u00b10.011 0.844\u00b10.001 0.863\u00b10.004 AUC 0.806\u00b10.001 0.807\u00b10.000 0.761\u00b10.000 0.715\u00b10.001 0.794\u00b10.015 0.837\u00b10.001 0.849\u00b10.004  the benchmark shows comparable performance compared to NAIML. Then, we superimpose L F R and L IC , step by step on the benchmark. Finally, from  provement comes from the addition of L IC . These phenomena illustrate that all parts of our model have gains for highperformance multi-label classification, especially for our incomplete instance-level contrastive learning.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we propose an ingenious neural network model for the DIMVMLC tasks. Most notably, we design the instance-level contrastive loss to guide the autoencoders to learn the cross-view high-level representation based on the consensus assumption. Moreover, a partial multi-view weighted fusion strategy is developed to exploit complementary information and enhance the discriminative ability. Throughout the learning model, we utilize the view and label missing indicator matrices to cleverly avoid the deleterious effects of incompleteness. Finally, complete and convincing experimental results confirm that our method is reliable and advanced compared to other state-of-the-art methods.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "This work is supported by Shenzhen Science and Technology Program under Grant RCBS20210609103709020, GJHZ20210705141812038, National Natural Science Foundation of China under Grant 62006059, and CAAI-Huawei MindSpore Open Fund under Grant CAAIXSJLJJ-2022-011C.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Multi-label learning with incomplete class assignments", "journal": "", "year": "2011", "authors": "S S Bucak; R Jin; A K Jain"}, {"ref_id": "b1", "title": "Adaptively-weighted Integral Space for Fast Multiview Clustering", "journal": "", "year": "2022", "authors": "M.-S Chen; T Liu; C.-D Wang; D Huang; J.-H Lai"}, {"ref_id": "b2", "title": "\u00e7\u00e7 Multi-view Subspace Clustering", "journal": "", "year": "", "authors": "M.-S Chen; C.-D Wang; D Huang; J.-H Lai; P S Yu"}, {"ref_id": "b3", "title": "Low-rank Tensor Based Proximity Learning for Multi-view Clustering", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2022", "authors": "M.-S Chen; C.-D Wang; J.-H Lai"}, {"ref_id": "b4", "title": "A simple framework for contrastive learning of visual representations", "journal": "PMLR", "year": "2020", "authors": "T Chen; S Kornblith; M Norouzi; G Hinton"}, {"ref_id": "b5", "title": "Learning semantic-specific graph representation for multi-label image recognition", "journal": "", "year": "2019", "authors": "T Chen; M Xu; X Hui; H Wu; L Lin"}, {"ref_id": "b6", "title": "Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary", "journal": "", "year": "2002", "authors": "P Duygulu; K Barnard; J F De Freitas; D A Forsyth"}, {"ref_id": "b7", "title": "The pascal visual object classes (voc) challenge", "journal": "International journal of computer vision", "year": "2009", "authors": "M Everingham; L Van Gool; C K Williams; J Winn; A Zisserman"}, {"ref_id": "b8", "title": "ANIMC: A Soft Approach for Autoweighted Noisy and Incomplete Multiview Clustering", "journal": "IEEE Transactions on Artificial Intelligence", "year": "2021", "authors": "X Fang; Y Hu; P Zhou; D Wu"}, {"ref_id": "b9", "title": "Simultaneously combining multi-view multi-label learning with maximum margin classification", "journal": "", "year": "2012", "authors": "Z Fang; Z Zhang"}, {"ref_id": "b10", "title": "Measuring statistical dependence with Hilbert-Schmidt norms", "journal": "Springer", "year": "2005", "authors": "A Gretton; O Bousquet; A Smola; B Sch\u00f6lkopf"}, {"ref_id": "b11", "title": "The iapr tc-12 benchmark: A new evaluation resource for visual information systems", "journal": "", "year": "2006", "authors": "M Grubinger; P Clough; H M\u00fcller; T Deselaers"}, {"ref_id": "b12", "title": "Multimodal semi-supervised learning for image classification", "journal": "", "year": "2010", "authors": "M Guillaumin; J Verbeek; C Schmid"}, {"ref_id": "b13", "title": "Multilabel classification", "journal": "", "year": "2016", "authors": "F Herrera; F Charte; A J Rivera; Jesus ; M J "}, {"ref_id": "b14", "title": "A fast learning algorithm for deep belief nets", "journal": "Neural computation", "year": "2006", "authors": "G E Hinton; S Osindero; Y.-W Teh"}, {"ref_id": "b15", "title": "Doubly aligned incomplete multi-view clustering", "journal": "", "year": "2019", "authors": "M Hu; S Chen"}, {"ref_id": "b16", "title": "View-Wise Versus Cluster-Wise Weight: Which Is Better for Multi-View Clustering?", "journal": "IEEE Transactions on Image Processing", "year": "2021", "authors": "S Hu; Z Lou; Y Ye"}, {"ref_id": "b17", "title": "DMIB: Dual-correlated multivariate information bottleneck for multiview clustering", "journal": "IEEE Transactions on Cybernetics", "year": "2020", "authors": "S Hu; Z Shi; Y Ye"}, {"ref_id": "b18", "title": "Weakly Supervised Video Anomaly Detection via Self-Guided Temporal Discriminative Transformer", "journal": "IEEE Transactions on Cybernetics", "year": "2022", "authors": "C Huang; C Liu; J Wen; L Wu; Y Xu; Q Jiang; Y Wang"}, {"ref_id": "b19", "title": "Pixel-Level Anomaly Detection via Uncertainty-Aware Prototypical Transformer", "journal": "", "year": "2022", "authors": "C Huang; C Liu; Z Zhang; Z Wu; J Wen; Q Jiang; Y Xu"}, {"ref_id": "b20", "title": "Learning label specific features for multi-label classification", "journal": "", "year": "2015", "authors": "J Huang; G Li; Q Huang; X Wu"}, {"ref_id": "b21", "title": "Improving multi-label classification with missing labels by learning label-specific features", "journal": "Information Sciences", "year": "2019", "authors": "J Huang; F Qin; X Zheng; Z Cheng; Z Yuan; W Zhang; Q Huang"}, {"ref_id": "b22", "title": "The mir flickr retrieval evaluation", "journal": "", "year": "2008", "authors": "M J Huiskes; M S Lew"}, {"ref_id": "b23", "title": "Bipartite graph based multi-view clustering", "journal": "", "year": "2020", "authors": "L Li; H He"}, {"ref_id": "b24", "title": "Incomplete multi-view clustering with joint partition and graph learning", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2021", "authors": "L Li; Z Wan; H He"}, {"ref_id": "b25", "title": "A Concise yet Effective Model for Non-Aligned Incomplete Multi-view and Missing Multilabel Learning", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2021", "authors": "X Li; S Chen"}, {"ref_id": "b26", "title": "EmoCaps: Emotion Capsule based Model for Conversational Emotion Recognition", "journal": "", "year": "2022", "authors": "Z Li; F Tang; M Zhao; Y Zhu"}, {"ref_id": "b27", "title": "Incomplete Multi-View Multi-Label Learning via Label-Guided Masked View-and Category-Aware Transformers", "journal": "", "year": "2023", "authors": "C Liu; J Wen; X Luo; Y Xu"}, {"ref_id": "b28", "title": "Localized Sparse Incomplete Multi-view Clustering", "journal": "IEEE Transactions on Multimedia", "year": "2022", "authors": "C Liu; Z Wu; J Wen; Y Xu; C Huang"}, {"ref_id": "b29", "title": "Low-Rank Multi-View Learning in Matrix Completion for Multi-Label Image Classification", "journal": "", "year": "2015", "authors": "M Liu; Y Luo; D Tao; C Xu; Y Wen"}, {"ref_id": "b30", "title": "Efficient and effective regularized incomplete multi-view clustering", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2020", "authors": "X Liu; M Li; C Tang; J Xia; J Xiong; L Liu; M Kloft; E Zhu"}, {"ref_id": "b31", "title": "Incomplete multi-view partial multi-label learning", "journal": "Applied Intelligence", "year": "2022", "authors": "X Liu; L Sun; S Feng"}, {"ref_id": "b32", "title": "Multiple kernel k kmeans with incomplete kernels", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2019", "authors": "X Liu; X Zhu; M Li; L Wang; E Zhu; T Liu; M Kloft; D Shen; J Yin; W Gao"}, {"ref_id": "b33", "title": "MVDRNet: Multi-view diabetic retinopathy detection by combining DCNNs and attention mechanisms", "journal": "Pattern Recognition", "year": "2021", "authors": "X Luo; Z Pu; Y Xu; W K Wong; J Su; X Dou; B Ye; J Hu; L Mou"}, {"ref_id": "b34", "title": "A deep convolutional neural network for diabetic retinopathy detection via mining local and long-range dependence", "journal": "", "year": "2018", "authors": "X Luo; W Wang; Y Xu; Z Lai; X Jin; B Zhang; D Zhang; Q Tan; G Yu; C Domeniconi; J Wang; Z Zhang"}, {"ref_id": "b35", "title": "Semi-supervised multi-label classification using incomplete label information", "journal": "Neurocomputing", "year": "2017", "authors": "Q Tan; Y Yu; G Yu; J Wang"}, {"ref_id": "b36", "title": "Labeling images with a computer game", "journal": "", "year": "2004", "authors": "L Von Ahn; L Dabbish"}, {"ref_id": "b37", "title": "Spectral Perturbation Meets Incomplete Multi-view Data", "journal": "", "year": "2019", "authors": "H Wang; L Zong; B Liu; Y Yang; W Zhou"}, {"ref_id": "b38", "title": "IS-MVSNet: Importance Sampling-Based MVS-Net", "journal": "Springer", "year": "2022", "authors": "L Wang; Y Gong; X Ma; Q Wang; K Zhou; L Chen"}, {"ref_id": "b39", "title": "Multi-label classification with label graph superimposing", "journal": "", "year": "2020", "authors": "Y Wang; D He; F Li; X Long; Z Zhou; J Ma; S Wen"}, {"ref_id": "b40", "title": "A survey on incomplete multiview clustering", "journal": "IEEE Transactions on Systems, Man, and Cybernetics: Systems", "year": "2022", "authors": "J Wen; Z Zhang; L Fei; B Zhang; Y Xu; Z Zhang; J Li"}, {"ref_id": "b41", "title": "Dimc-net: Deep incomplete multi-view clustering network", "journal": "", "year": "2020", "authors": "J Wen; Z Zhang; Z Zhang; Z Wu; L Fei; Y Xu; B Zhang"}, {"ref_id": "b42", "title": "Unsupervised feature learning via non-parametric instance discrimination", "journal": "", "year": "2018", "authors": "Z Wu; Y Xiong; S X Yu; D Lin"}, {"ref_id": "b43", "title": "Multi-view learning with incomplete views", "journal": "IEEE Transactions on Image Processing", "year": "2015", "authors": "C Xu; D Tao; C Xu"}, {"ref_id": "b44", "title": "Multi-Level Feature Learning for Contrastive Multi-View Clustering", "journal": "", "year": "2022", "authors": "J Xu; H Tang; Y Ren; L Peng; X Zhu; L He"}, {"ref_id": "b45", "title": "Adaptive reverse graph learning for robust subspace learning", "journal": "Information Processing & Management", "year": "2021", "authors": "C Yuan; Z Zhong; C Lei; X Zhu; R Hu"}, {"ref_id": "b46", "title": "Multi-source feature learning for joint analysis of incomplete multiple heterogeneous neuroimaging data", "journal": "NeuroImage", "year": "2012", "authors": "L Yuan; Y Wang; P M Thompson; V A Narayan; J Ye; A D N Initiative"}, {"ref_id": "b47", "title": "Ae2-nets: Autoencoder in autoencoder networks", "journal": "", "year": "2019", "authors": "C Zhang; Y Liu; H Fu"}, {"ref_id": "b48", "title": "Latent semantic aware multi-view multi-label classification", "journal": "", "year": "2018", "authors": "C Zhang; Z Yu; Q Hu; P Zhu; X Liu; X Wang"}, {"ref_id": "b49", "title": "A review on multilabel learning algorithms", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2013", "authors": "M.-L Zhang; Z.-H Zhou"}, {"ref_id": "b50", "title": "Multi-view embedding learning for incompletely labeled data", "journal": "", "year": "2013", "authors": "W Zhang; K Zhang; P Gu; X Xue"}, {"ref_id": "b51", "title": "Semi-supervised multi-label learning with incomplete labels", "journal": "", "year": "2015", "authors": "F Zhao; Y Guo"}, {"ref_id": "b52", "title": "Shared-Private Memory Networks for Multimodal Sentiment Analysis", "journal": "IEEE Transactions on Affective Computing", "year": "2022", "authors": "X Zhao; Y Chen; S Liu; B Tang"}, {"ref_id": "b53", "title": "Multi-view label embedding", "journal": "Pattern Recognition", "year": "2018", "authors": "P Zhu; Q Hu; Q Hu; C Zhang; Z Feng"}, {"ref_id": "b54", "title": "Interpretable learning based Dynamic Graph Convolutional Networks for Alzheimer's Disease analysis", "journal": "Information Fusion", "year": "2022", "authors": "Y Zhu; J Ma; C Yuan; X Zhu"}, {"ref_id": "b55", "title": "Multi-view clustering on unmapped data via constrained non-negative matrix factorization", "journal": "Neural Networks", "year": "2018", "authors": "L Zong; X Zhang; X Liu"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: The motivation of instance-level contrastive learning.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure3: The results on Corel5k dataset with (a) different missing-view rates and a 50% missing-label rate and (b) a 50% missing-view rate and different missing-label rates.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: The AP value versus \u03b2 and \u03b3 on the (a) Corel5k dataset and (b) VOC2007 dataset; the AP value versus \u03c4 on the (c) Corel5k dataset and (d) VOC2007 dataset. All datasets are with 50% available instances, 50% missing labels, and 70% training samples.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "10 \u22127 then Stop training. else Update L last with L. Update P last with P .", "figure_data": "end ifend for"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Experimental results of different methods on the five datasets with 50% missing instances, 70% training samples, and 50% missing labels for training samples. The 1st/2nd best resluts are marked in bold/underline.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": ", we can find that:(1) with the superposition of each loss component, the performance metrics increase significantly; (2) the biggest im-", "figure_data": "LMC LF R LICCorel5k AP AUCVOC2007 AP AUC0.336 0.858 0.484 0.7880.352 0.872 0.492 0.8020.368 0.876 0.504 0.8090.381 0.884 0.505 0.809"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Ablation experimental results of our DICNet on the Corel5k and VOC2007 datasets with 50% instances, 50% missing labels, and 70% training samples.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "(\") ($) (%)", "formula_coordinates": [3.0, 149.13, 59.47, 10.53, 120.78]}, {"formula_id": "formula_1", "formula_text": "= ( :,# ) (#) + ( :,& ) (&) +\u2026+ ( :,' ) (')", "formula_coordinates": [3.0, 222.12, 60.27, 170.92, 8.64]}, {"formula_id": "formula_2", "formula_text": "(\") ($) (%) Encoder Encoder Encoder Decoder Decoder Decoder & (\") & ($) & (%) (\") ($) (%)", "formula_coordinates": [3.0, 94.2, 74.8, 430.02, 128.29]}, {"formula_id": "formula_3", "formula_text": "FC ReLU FC BN FC ReLU FC ReLU Decoder FC classifier FC Sigmoid", "formula_coordinates": [3.0, 154.86, 253.96, 349.48, 22.03]}, {"formula_id": "formula_4", "formula_text": "Z (v) = E (v) X (v) , \u03b8 (v) andX (v) = D (v) Z (v) , \u03c8 (v) ,", "formula_coordinates": [3.0, 319.5, 677.3, 238.5, 10.53]}, {"formula_id": "formula_5", "formula_text": "L F R = 1 l l v=1 L (v) F R = 1 l l v=1 1 mv n i=1 x (v) i \u2212 x (v) i 2 2 Wi,v ,(1)", "formula_coordinates": [4.0, 65.68, 323.03, 226.82, 21.4]}, {"formula_id": "formula_6", "formula_text": "x (v) i andx (v) i", "formula_coordinates": [4.0, 81.39, 353.21, 53.3, 14.07]}, {"formula_id": "formula_7", "formula_text": "(v) i , (2) positive in- stance A + = z (u) i u =v", "formula_coordinates": [4.0, 54.0, 652.7, 238.5, 33.5]}, {"formula_id": "formula_8", "formula_text": "S(z (v) i , z (u) j ) = z (v) i \u2022 z (u) j z (v) i z (u) j ,(2)", "formula_coordinates": [4.0, 386.38, 194.67, 171.62, 25.2]}, {"formula_id": "formula_9", "formula_text": "l (vu) IC = \u2212 1 n n i=1 Wi,vWi,u log exp(S z (v) i , z (u) i \u03c4 ) exp(S z (v) i , z (u) i \u03c4 ) + Sneg ,(3)", "formula_coordinates": [4.0, 337.77, 300.26, 220.23, 22.47]}, {"formula_id": "formula_10", "formula_text": "S neg = r=u,v n j=1,j =i exp(S z (v) i , z (r)j", "formula_coordinates": [4.0, 346.71, 328.23, 151.47, 24.57]}, {"formula_id": "formula_11", "formula_text": "L IC = 1 2 l v=1 u =v l (vu) IC .(4)", "formula_coordinates": [4.0, 398.69, 404.71, 159.31, 21.54]}, {"formula_id": "formula_12", "formula_text": "(v) i , z (u) i are available.", "formula_coordinates": [4.0, 375.31, 499.48, 93.3, 14.07]}, {"formula_id": "formula_13", "formula_text": "hi = l v=1 z (v) i Wi,v l v=1 Wi,v,(5)", "formula_coordinates": [4.0, 379.37, 685.66, 178.63, 21.29]}, {"formula_id": "formula_14", "formula_text": "(v) i", "formula_coordinates": [5.0, 84.94, 54.47, 10.9, 14.07]}, {"formula_id": "formula_15", "formula_text": "P = Sigmoid(F c (H, \u03c9)),(6)", "formula_coordinates": [5.0, 119.51, 273.91, 173.0, 9.65]}, {"formula_id": "formula_16", "formula_text": "L M C = \u2212 1 nc n i=1 c j=1", "formula_coordinates": [5.0, 78.53, 363.69, 71.84, 21.4]}, {"formula_id": "formula_17", "formula_text": "+ (1 \u2212 Yi,j ) log(1 \u2212 Pi,j ) Gi,j ,(7)", "formula_coordinates": [5.0, 159.45, 371.05, 133.05, 12.84]}, {"formula_id": "formula_18", "formula_text": "L = L M C + \u03b2L IC + \u03b3L F R ,(8)", "formula_coordinates": [5.0, 114.13, 490.9, 178.37, 9.65]}, {"formula_id": "formula_19", "formula_text": "if |L last \u2212 L| < \u03c3 or 1 ntc i,j P i,j \u2295 P last i,j <", "formula_coordinates": [5.0, 339.43, 310.32, 171.85, 18.81]}], "doi": ""}