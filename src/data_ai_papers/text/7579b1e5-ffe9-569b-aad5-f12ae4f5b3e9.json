{"title": "Acquiring language from speech by learning to remember and predict", "authors": "Cory Shain; Micha Elsner", "pub_date": "", "abstract": "Classical accounts of child language learning invoke memory limits as a pressure to discover sparse, language-like representations of speech, while more recent proposals stress the importance of prediction for language learning. In this study, we propose a broadcoverage unsupervised neural network model to test memory and prediction as sources of signal by which children might acquire language directly from the perceptual stream. Our model embodies several likely properties of real-time human cognition: it is strictly incremental, it encodes speech into hierarchically organized labeled segments, it allows interactive top-down and bottom-up information flow, it attempts to model its own sequence of latent representations, and its objective function only recruits local signals that are plausibly supported by human working memory capacity. We show that much phonemic structure is learnable from unlabeled speech on the basis of these local signals. We further show that remembering the past and predicting the future both contribute to the linguistic content of acquired representations, and that these contributions are at least partially complementary.", "sections": [{"heading": "Introduction", "text": "How children acquire language from the environment is one of the fundamental mysteries of cognitive science. Much theoretical, experimental, and computational research into this question has focused on acquiring abstractions over lowerorder symbols, such acquiring morphemes from phoneme sequences or syntactic structures from word sequences (Chomsky, 1965;Gold, 1967;Elman, 1991;Saffran et al., 1996;Albright, 2002;Klein and Manning, 2004;Goldwater et al., 2009;Christodoulopoulos et al., 2012, inter alia). Children, however, do not get symbolic input; symbolic representations at any level of granularity constitute abstractions inferred from highly variable, noisy, and information-rich perceptual signals like audition and vision. This work joins a growing computational literature exploring the kinds of architectures and learning objectives that best support acquisition of linguistic representations directly from the speech signal without supervision (Versteegh et al., 2015;Dunbar et al., 2017). Such models can be used to test questions about language acquisition under more realistic assumptions about the input signal, especially to the extent that they reflect known constraints on human cognition (Shain and Elsner, 2019;Begu\u0161, 2020).\nThis study uses computational modeling to examine two influential and possibly complementary ideas about how people learn abstract representations, including language, from data: learning to remember, and learning to predict. Both hypotheses have been advocated by prior work in language acquisition, cognitive neuroscience, and computational modeling, yet their relative contributions to language learning are not yet clear. Our model permits precise manipulation of memory and prediction pressures during acquisition, allowing direct comparison of these hypotheses.\nIn so doing, we implement several constraints on real-time language processing that have not been simultaneously present in prior modeling of this domain: (1) we jointly segment and label the speech signal without supervision; (2) the learning objective is applied incrementally during real-time processing using only locally available feedback;\n(3) the encoded signal is segmental, sparse, and hierarchically organized; (4) segments are represented featurally as patterns of activation, rather than discrete category symbols; and (5) the system is optimized by modeling its own state at multiple timescales, rather than by modeling the data alone.\nResults show a systematic improvement along multiple measures of phoneme induction quality from both learning to remember and learning to predict, suggesting that these two kinds of signals may play complementary roles during child language acquisition. The contributions of this work are as follows:\n\u2022 We propose a novel deep neural encoderdecoder for unsupervised speech processing that is incremental, segmental, and useful for testing hypothesized cognitive constraints.\n\u2022 We show empirically that memory-based and prediction-based signals contribute separately to the acquisition of linguistic regularities, simultaneously supporting two existing classes of theories about the learning pressures that underlie human language acquisition.\n2 Background", "publication_ref": ["b22", "b47", "b37", "b115", "b1", "b76", "b50", "b127", "b36", "b120", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Memory, Prediction, and Learning", "text": "Many proposals from the language acquisition literature appeal to memory pressures as a learning signal (Newport, 1990;Pinker, 1991;Carstairs-McCarthy, 1994;Rissanen and Ristad, 1994;Baddeley et al., 1998;Goldsmith, 2003;Yang, 2005, inter alia). For example, Baddeley et al. (1998) invoke constraints on working memory, arguing that because the speech signal is too rich to support full retention during real-time language processing (Baddeley and Hitch, 1974), infants are guided toward phonemic representations, which constitute an efficient encoding of that signal. Meanwhile, classical theories of language acquisition such as Newport (1990) and Pinker (1991) invoke constraints on long-term memory, arguing that linguistic regularities constitute compressed descriptions of the learner's input and that their discovery reduces the amount of information that must by idiosyncratically stored. Artificial language learning patterns in humans (Kersten and Earles, 2001) and recent computational modeling of the speech domain (e.g. Lee and Glass, 2012;Lee et al., 2015;Elsner and Shain, 2017;Kamper et al., 2017a;Shain and Elsner, 2019) have supported a contribution from memory constraints to language learning. This position also aligns with an extensive computational neuroscience literature on sparse coding, which holds that biological neurons are tuned for memory-efficient representations of recent stimuli (Attneave, 1954;Field, 1996, 2004;Sheridan et al., 2017).\nNonetheless, debate exists about the role of memory in language learning. For example, Rohde and Plaut (1999) fail to replicate findings from Elman (1993) in favor of Newport (1990). In addition, Perfors (2012) fails to find evidence that memory bottlenecks encourage discovery of underlying linguistic regularities in adults and argues that such limitations only support language learning in concert with strong inductive priors. Furthermore, evidence suggests that mental representations during language processing preserve acoustic details over and above symbolic codes (Andruski et al., 1994;McMurray et al., 2002). Related work has called into question both the memory efficiency of human mental representations and the severity of long-term memory limits. For example, experimental evidence indicates that human mental representations contain redundant information, both of language (Baayen et al., 1997) and of other constructs such as logical relations (Piantadosi et al., 2016). In addition, recent estimates of mental storage requirements indicate that lexical information, especially semantics, already requires vastly more storage than e.g. phonemes and syntax, suggesting little added memory benefit from optimizing the efficiency with which regularities are stored (Mollica and Piantadosi, 2019). Finally, recent computational evidence linking memory bottlenecks to success in unsupervised speech processing has relied on storage of arbitrarily long acoustic sequences in their full detail in order to compute reconstruction losses Elsner and Shain, 2017). This design is inconsistent with known constraints on the storage duration (< 1s) of unanalyzed acoustic traces in human working memory (Baddeley and Hitch, 1974;Cowan, 1984). It is thus not yet clear (1) how strongly memory pressures constrain mental representations of speech or (2) how much they encourage language learning. Memory efficiency is not the only objective that can be constructed to learn abstractions over data without supervision. It has also been proposed that language learning may be driven by optimizing prediction of future input (Rohde and Plaut, 1999;Phillips and Ehrenhofer, 2015;Apfelbaum and McMurray, 2017). This proposal aligns with an extensive neuroscience literature arguing that predictive coding for future inputs is a \"canonical computation\" of the human brain (Keller and Mrsic-Flogel, 2018) and may better characterize the tuning of biological neurons than sparse coding (Singer et al., 2018), possibly because prediction affords advantages in critical tasks (Nijhawan, 1994) and may help organisms filter noise from the perceptual signal by focusing attention on features relevant to prediction (Bialek et al., 2001). Additional support for a role of prediction in language learning comes from the success of incremental language models in natural language processing, which optimize prediction of future words (Ney et al., 1994;Heafield et al., 2013;Jozefowicz et al., 2016;Radford et al., 2019). Language models support dramatic performance improvements in language processing tasks (Radford et al., 2019) and have been shown to both (1) acquire linguistic abstractions without direct supervision (Linzen et al., 2016) and (2) covary with human language comprehension measures (Frank and Bod, 2011;Goodkind and Bicknell, 2018;van Schijndel and Linzen, 2018). Finally, experimental evidence indicates that infants chunk the speech stream at points of low transition probability, suggesting that predictive signals are exploited to learn word-like units (Saffran et al., 1996).\nWe address these questions computationally by manipulating the presence or absence of memory and prediction pressures in the joint objective of an unsupervised incremental speech processing model, allowing us to quantify the contributions of these two hypothesized learning signals under realistic constraints on real-time processing.", "publication_ref": ["b95", "b108", "b18", "b113", "b8", "b49", "b128", "b8", "b9", "b95", "b108", "b73", "b79", "b80", "b40", "b70", "b120", "b6", "b121", "b114", "b95", "b103", "b2", "b90", "b7", "b107", "b93", "b40", "b9", "b32", "b114", "b106", "b5", "b72", "b123", "b97", "b15", "b96", "b58", "b67", "b110", "b110", "b81", "b43", "b51", "b119", "b115"], "figure_ref": [], "table_ref": []}, {"heading": "Recurrent, Hierarchical, and Segmental", "text": "Speech Processing in Humans Artificial recurrent neural networks such as those employed here were initially proposed as algorithmic-level (Marr, 1982) models of activity in biological neural networks (Little, 1974;Hopfield, 1982), and subsequent studies support ubiquitous recurrence in the cortex (Harris and Mrsic-Flogel, 2013). In addition, influential theories of biological neural information processing argue that biological neural circuits integrate information at multiple hierarchically-organized timescales (Kiebel et al., 2008;Hasson et al., 2015;Norman-Haignere et al., 2020). Further neuroscientific evidence indicates that segmentation of the time dimension plays a critical role in human cognition, both in domain-general event processing (Zacks et al., 2001;Jensen, 2006, inter alia) and in speech processing specifically (Sanders and Neville, 2003;Cunillera et al., 2006Cunillera et al., , 2009Kooijman et al., 2013;Lee and Cho, 2016, inter alia). Segmentation or \"chunking\" also plays a central role in several theories of language comprehension (Sanford and Sturt, 2002;Hale, 2006;Frank and Christiansen, 2018) and learning (Monaghan and Christiansen, 2010;McCauley and Christiansen, 2019). Our model incorporates these notions architecturally, with segment boundaries implemented by \"detector neurons\" that govern information flow between neural populations at larger and smaller timescales (Masquelier, 2018).", "publication_ref": ["b84", "b82", "b63", "b55", "b74", "b56", "b130", "b116", "b34", "b33", "b77", "b117", "b54", "b44", "b94", "b86", "b85"], "figure_ref": [], "table_ref": []}, {"heading": "Modeling the Mental State", "text": "Many theories of linguistic structure posit multiple, hierarchically organized levels of representation (Chomsky, 1957;Goldsmith, 1976). Such theories predict the existence of abstractions over abstractions, latent structures that describe the distribution of other latent structures. This idea accords with recent theories of generalized Bayesian learning in biological agents, in which neural populations are thought to model the activity of other neural populations within their Markov blanket (Friston, 2010). The notion of learning through modeling other elements of the agent's own mental state has been exploited in symbolic computational models of language acquisition (Lee and Glass, 2012;Lee et al., 2015), but not in the context of artificial neural zero-resource speech models, which have so far derived their objective exclusively from the data (Kamper et al., 2017a;Elsner and Shain, 2017).\nOur approach incorporates this idea by optimizing higher layers to predict the sequence of activations at lower layers.", "publication_ref": ["b21", "b48", "b45", "b79", "b80", "b70", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "Related Computational Approaches", "text": "This work is part of a growing interest in unsupervised representation learning from raw speech, especially the Zerospeech 2015 (Versteegh et al., 2015) and 2017 (Dunbar et al., 2017) shared tasks and participating systems (Badino et al., 2015;Renshaw et al., 2015;Agenbag and Niesler, 2015;Baljekar et al., 2015;R\u00e4s\u00e4nen et al., 2015;Lyzinski et al., 2015;Zeghidour et al., 2016;Heck et al., 2016;Srivastava and Shrivastava, 2016;Kamper et al., 2017b;Yuan et al., 2017;Heck et al., 2017;Shibata et al., 2017;Ansari et al., 2017a,b), as well as subsequent deep neural autoencoders (Van Den Oord et al., 2017;Chorowski et al., 2019) inspired by the WaveNet architecture (van den Oord et al., 2016). Much of this work concerns the discovery of word-like units, while our analyses focus on learning at the phoneme level (see section 4.3). A symbolic Bayesian framework for joint unsupervised phoneme segmentation and clustering is proposed by Lee and Glass (2012) and extended by Lee et al. (2015). Their system infers a Dirichlet process hidden Markov model to learn a symbolic sequential encoding of the speech stream. A disadvantage of this approach for the present research question is that the categorically distributed phone labels lack any notion of featural relatedness, contrary to widely held assumptions about natural language phonology (Clements, 1985). In addition, the learning signal derives from a next-frame prediction objective, making it difficult to use the model to factorially manipulate memory and prediction pressures. Another recent framework for unsupervised phone segmentation identifies boundaries at points of high surprisal in a frame-level language model (Michel et al., 2017). This approach does not generate segment encodings and cannot straightforwardly be used to test claims about the role of memory in language learning.", "publication_ref": ["b127", "b36", "b10", "b112", "b0", "b11", "b111", "b83", "b132", "b59", "b124", "b71", "b129", "b60", "b122", "b125", "b24", "b101", "b79", "b80", "b29", "b92"], "figure_ref": [], "table_ref": []}, {"heading": "Model", "text": "Like many prior ANN zero-resource speech processing models (e.g. Kamper et al., , 2017aElsner and Shain, 2017;Shain and Elsner, 2019), we employ an encoder-decoder framework. However, unlike previous approaches, our model decodes incrementally and hierarchically, with each layer decoding its inputs at their own timescale over a short window backward into the past and/or forward into the future. The model is thus required not only to describe the input signal (speech), but also its own sequence of latent representations (e.g. phones, words, etc.), much as people are implicitly thought to do in prior symbolic work on unsupervised language learning (Goldwater et al., 2009;Lee et al., 2015). Our encoder model closely follows Chung et al. (2017), and thus the primary technical contribution of this work lies in the cascaded incremental decoder and the layerwise incremental objective described below, both of which are designed to encourage repurposable segment representations based on locally available information. Although encodings are ultimately the quantity of interest in unsupervised encoder-decoder models, prior work has shown that decoder design can be a major determinant of acquired representations (McCoy et al., 2018(McCoy et al., , 2020. The overall design is schematized in Figure 1. Code is available at https://github.com/coryshain/dnnseg.", "publication_ref": ["b70", "b40", "b120", "b50", "b80", "b28", "b87", "b88"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Encoder", "text": "Our encoder closely follows a hierarchical multiscale extension (HM-LSTM, Chung et al., 2017) of long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997). The encoder consists of multiple LSTM layers linked by discrete boundary neurons that govern memory retention and information flow between layers. When a boundary neuron fires in layer l, it terminates a segment. Layer l then ejects its hidden state representation to layer l + 1, receives top-down input from the hidden state of layer l + 1, and resets its cell state (incremental memory) in order to process the next segment. The hidden state at a boundary thus constitutes a label for the segment terminating at that boundary, which is used to summarize the content of the segment when communicating with other layers. When the boundary neuron at layer l does not fire, layer l + 1 is inert and simply copies its representation forward. As a result, higher layers track information at longer timescales than lower layers, and the segmentation behavior at l determines the input timescale at l + 1. Each layer proceeds by segmenting and labeling its input signal at a timescale learned from data, resulting in a hierarchical sequence of labeled segments. As argued in Chung et al. (2017), this design enforces a trade-off between recurrent information (which is erased by segmentation) and top-down information (which is made available by segmentation). 1 Although the linguistic quality of discovered HM-LSTM segments is not systematically examined in the original proposal (Chung et al., 2017) and recent analysis has called it into question (K\u00e1d\u00e1r et al., 2018), our results indicate that HM-LSTMs can discover segmental structure from speech, at least at the phonemic level.", "publication_ref": ["b28", "b62", "b28", "b28", "b68"], "figure_ref": [], "table_ref": []}, {"heading": "Decoder", "text": "The decoder consists of two multi-layer attentional sequence-to-sequence (seq2seq) LSTMs with L layers each, one backward-directional (memory) and one forward directional (prediction). The LSTMs respectively decode the B previous input segment labels and the F following input segment labels given an encoder representation at layer l and time t. In addition, the predicted sequence from the layer above serves as attention values to inform decoding at the current layer, at a timescale determined by the segmentation patterns of the current layer and the layer below. The internal behavior of the decoder is thus tightly coupled with the segmental behavior of the encoder, providing direct feedback into the encoder decisions. In addition, the label sequence of the decoder at all layers must support both (1) decoding of the perceptual signal (the data), since top-down connections allow higherlevel representations to inform lower-level ones, and (2) decoding of lower-level state sequences. 2", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Objective", "text": "We employ an incremental layerwise objective that both reconstructs backward and predicts forward from time t at layer l over the segment labels from layer l \u2212 1 at a timescale defined by the segmentation behavior of layer l\u22121. Thus only the first layer decodes at the timescale of the data; higher layers l decode at the timescale of l\u22121, and representations associated with non-boundaries in l \u2212 1 are ignored by the objective. The objective scans incrementally over the time dimension and imposes a forward and backward cost at every segment boundary identified by layer l \u2212 1. As a result, the first layer (\"phonemes\") is responsible for incrementally decoding the local past and future realization of the acoustic stream, the second layer is responsible for decoding the local past and future realization of the 2 See Appendix C for definition of the decoder.\n\"phoneme\" sequence, etc. 3 Although it is possible to backpropagate into the decoding targets (i.e. encoder representations) at higher layers, thereby encouraging the encoder to discover more predictable segment sequences, we found in practice that doing so resulted in a form of mode collapse where labels became insensitive to the data and converged to a single value for all timesteps. For this reason, we stop the gradients into decoding targets and backpropagate only into the decoder predictions. Thus, the objective encourages encodings at higher layers to change to better predict structures at lower layers-but does not alter the representations at those lower layers to make them more uniform and therefore easier to predict.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Design", "text": "We assess the contribution of memory and prediction pressures to phoneme acquisition by (1) manipulating these pressures on models exposed to speech data from two unrelated languages (Xitsonga and English) and (2) evaluating the effect of these manipulations on multiple measures of phoneme induction quality.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data", "text": "We use the Zerospeech 2015 (Versteegh et al., 2015) challenge data in English and Xitsonga, a Bantu language spoken in South Africa. The Xitsonga data come from the NCHLT corpus (De Vries et al., 2014) and contain 2h29m07s of read speech from 24 speakers. The English data come from the Buckeye Corpus (Pitt et al., 2005) and contain 4h59m05s of spontaneous speech from 12 speakers. For English, we additionally include the official development set in training, which contains 1h39m45s of spontaneous speech from 2 speakers, also from the Buckeye Corpus. English development set performance was used for model development and tuning, but the development set is not included in the evaluations presented here. Xitsonga lacks a development set, so designs selected on the English development set are applied directly to Xitsonga for evaluation. Before fitting, we convert the source audio files into a cochleagram-based spectral representation that approximates the signal generated by the human auditory system (McDermott and Simoncelli, 2011). 4 200", "publication_ref": ["b127", "b35", "b109"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Manipulations", "text": "We seek to assess the contribution of both memory and prediction pressures to the content of model representations. To this end, our principal manipulations are the backward (B \u2208 {0, 5, 25, 50}) and forward (F \u2208 {0, 1, 5, 10}) window lengths used by the decoder, which respectively impose a pressure to efficiently remember and accurately predict. Note that the condition B = 0, F = 0 (no reconstruction or prediction) is not well defined because the objective is 0 at any parameterization and thus has no gradient, and we therefore exclude it from consideration in these results. In addition, we manipulate the number of encoder layers (L \u2208 {2, 3, 4}). This is because it is unclear a priori which layers of the encoder are expected to correspond to quantities of interest like phonemes or words, since the representations are unsupervised and the model could additionally or instead discover e.g. subphonemic, morphemic, phrasal, intonational, and other kinds of structures. Although detection of these and other levels of linguistic representation is of interest and is the target of future work, the annotations provided by the Zerospeech 2015 data support phoneme-level and word-level analyses only, and we concentrate our evaluation there. Varying the number of layers allows us to investigate which layers emergently discover more phoneme-like units, and under what conditions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "Because our model generates a segmental encoding of the speech signal, we apply two classes of evaluation in this study: phoneme segmentation and phoneme-level probing classification. The segmentation evaluation measures the degree of correspondence between the model-generated segment boundaries and expert-annotated phoneme boundaries, using a boundary F-measure which assigns a true positive for up to one predicted boundary that falls within some tolerance of each gold boundary, false positives for all other predicted boundaries, and false negatives for all gold boundaries that lack a predicted boundary within the tolerance. Following Lee and Glass (2012), we use a tolerance of 20ms. The classification evaluation measures the amount of signal in model-generated encodings as to (1) the true identity of the phoneme being encoded and (2) the cluster of phonological features associated with that phoneme (Hayes, 2011). Following e.g. Shain and Elsner (2019) and Chrupa\u0142a et al. (2020), we do so using probing classifiers. In particular, for each layer of each model's encoder, we fit linear classifiers to (1) the phoneme labels and (2) the phonological feature labels associated with the gold phoneme segment corresponding to each phone boundary. We extract the gold and predicted phoneme encodings at the human-annotated phoneme boundaries, regardless of whether the model segmented at that location. This supports direct comparison of metrics across models, since the set of evaluated segments is held constant. Phonological features are extracted at the same timepoints, following the procedure described in Shain and Elsner (2019). 5 Although our model is designed to support joint discovery of multiple layers of representation, we find empirically that no model appreciably improves at any layer in word boundary F-score over a baseline that segments only at the ends of voice activity regions, and qualitative inspection does not indicate systematic correspondence to an unannotated level of representation such as syllables, morphemes, or intonational units. Despite differences in segmentation rate, and thereby in word boundary precision-recall trade-off, models generally converge on similar (low) word boundary F-scores, and thus our manipulations are not informative about word learning. Probe-based classification metrics are not well suited to word-level evaluation due to the size of the vocabulary. Though human speech processing involves units between the phoneme and word level, detailed analysis of such units is difficult due to the lack of annotation in the corpus. We believe poor word discovery at higher layers may be due in part to the fact that noninitial layers have both a non-stationary objective (the evolving representations of the layer below) and slower learning dynamics, perhaps making it difficult for these layers to \"catch up\" with moving targets (Ioffe and Szegedy, 2015). We leave exploration of possible remedies to future research and focus here only on the phoneme level.\nWhile it is a priori unclear which layer of the encoder is expected to encode phonemes (for example, the initial layers may encode sub-phonemic units), we find systematically better phoneme segmentation and classification performance in the first layer of the network. For simplicity, we therefore only present metrics from this layer.\nIn addition to reporting raw model performance,  we report performance improvements from each model relative to (1) baseline U (untrained), an architecturally matched model left at random initialization (Chrupa\u0142a et al., 2020), and (2) baseline X (cross-language), the architecturally matched model trained on the opposite language. 6 These two baselines quantify different contributions of the acquisition process. Baseline U quantifies architectural inductive bias: how well does the architecture alone guide linguistic representations, without learning? Baseline X quantifies modality inductive bias: how well does general knowledge of human speech guide linguistic representations, without exposure to the target language? Improvement against either of these baselines supports language learning from experience, over and above any prior knowledge that might more efficiently be innately encoded. 7", "publication_ref": ["b79", "b57", "b120", "b27", "b120", "b64", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Results and Discussion", "text": "Boundary and macro-averaged phoneme and feature classification F-measures from the bestperforming configuration on the English development set (B = 25, F = 1, L = 3) are given in Table 1. English boundary performance (F = 65.3) approaches previously reported unsupervised phoneme segmentation scores on different and therefore not directly comparable datasets (Lee and Glass, 2012;Michel et al., 2017, both around 6 For Xitsonga, baseline X is the architecturally matched English-trained model. For English, baseline X is the architecturally matched Xitsonga-trained model. 7 We do not evaluate directly against a previous state of the art because no state of the art exists for unsupervised phoneme segmentation and classification in the Zerospeech 2015 data. A previous model that performed the same task (Lee and Glass, 2012) achieved an average boundary F-score of 76.1 on a different dataset that used a different boundary annotation standard (automatic forced alignment instead of human annotation). To our knowledge, the dataset is no longer publicly available. A recent segmentation-only model (Michel et al., 2017) achieved a boundary F of 75 on the TIMIT dataset (Fisher et al., 1986). However, because TIMIT is restricted to 10 unique utterances of English, we believe Zerospeech 2015, which contains more linguistically diverse speech from two unrelated languages, is a better dataset for investigating language acquisition patterns. F = 75). The overall segmentation performance in Xitsonga is considerably worse than that of English, consistent with prior evidence that word segmentation in the Zerospeech 2015 Xitsonga partition is harder than English (e.g. Kamper et al., 2017a). By contrast, classification metrics in Xitsonga are better than in English, which is again consistent with prior findings of stronger unsupervised classification performance in Xitsonga (Shain and Elsner, 2019). The difference in relative performance between segmentation and classification in the two languages could be due in part to differences in register: the English data is spontaneously produced, while the Xitsonga data is read speech. Longer average phoneme duration (100ms vs 70ms) and cleaner articulations in Xitsonga could plausibly give rise to this asymmetry, and further investigation is left to future work. The model substantially outperforms the untrained baseline (U) on all metrics and outperforms the cross-language baseline (X) on all metrics but boundary F in Xitsonga, which could be due in part to the larger size of the English-language training set. Results therefore indicate that the reconstruction and prediction objectives have contributed to unsupervised discovery of phonemic patterns in both languages.\nSegmentation and macro-averaged classification F-measures by language and experimental condition are given in Figure 2. Results show a contribution of both memory (B > 0) and prediction (F > 0), with a similar distribution of relative performance between the two languages, supporting the existence of language-general influences of prediction and memory on phoneme learning.\nAs shown in Figure 2, models without memory pressures (B = 0) find substantially worse boundaries than models with memory pressures. There also appears to be a ceiling effect of backward reconstruction size, with a jump in performance at B = 25 but no systematic improvement at B = 50. Importantly, at layer 1, B = 25 covers a 250ms interval, which falls within even conservative estimates of the storage duration of unanalyzed auditory traces in humans (Cowan, 1984). The B = 25 objective could therefore plausibly be used during online speech processing. Prediction pressures also support discovery of phoneme boundaries, as shown by the generally worse boundary performance of F = 0 vs. F > 0 in both languages.\nIn addition, Figure 2 shows that memory and prediction both modulate phoneme classification performance, with a roughly convex performance surface around a peak at B = 25, F = 10 for English and B = 25, F = 5 for Xitsonga. A similar peak emerges in the feature classification results for English, along with a local feature classification peak in Xitsonga for L > 2. A 250ms auditory memory window thus supports both phoneme segmentation and classification in our models, with additional benefits from predicting over short intervals (Singer et al., 2018). For feature classification, the primary determinant of performance across languages is the prediction objective, with performance generally increasing up to F = 5. There is also an effect of encoder depth in these results, such that encoders with more layers (L > 2) tend to perform better across metrics, despite the fact that all metrics reflect performance at the first layer. This result supports a contribution of multiscale modeling, even if the segmentation behavior at higher layers does not clearly correspond to a theory-driven level of representation (see section 4.3). absence of these characteristics. Memory and prediction therefore modulate not only absolute performance, but also the utility of language experience. Figure 4 reports performance differences by metric against baseline X (cross-language). English segmentation is substantially helped by experience with (i.e. training on) English, especially under strong memory pressures. However, Xitsonga segmentation is generally worse for the Xitsongatrained model than the English-trained one. This might be due to the fact that the English training set is larger, and/or to low overall levels of segmentation performance in Xitsonga. While we leave further investigation of this exception to future work, the classification metrics still show a clear benefit of in-domain training in both languages, but only in the presence of prediction pressures.\nThe baseline X results bear on the degree to which speech processing patterns can plausibly be innately specified. Although the set of phonological categories and features are classically regarded as universal (Chomsky and Halle, 1968;Clements, 1985), it is well known that the \"same\" phonological abstraction (e.g. voicing) can be phonetically cached out in different ways depending on the language (e.g. Gordon and Ladefoged, 2001;Gordon et al., 2002). Our results suggest that, at least between Xitsonga and English, this variation is both (1) constrained enough to permit recognition of non-trivial patterns from speech in other languages on the basis of general, possibly innate processing biases, and (2) substantial enough to give rise to a benefit of direct experience with the target language, even for language-general constructs like phoneme categories and phonological features. We use linear regression on the combined metrics to quantitatively evaluate the contribution of both memory and prediction pressures to phoneme acquisition. Results show significant positive contributions to acquisition from memory pressures (p = 0.006), prediction pressures (p < 0.001), and multiscale encoding (p < 0.001). 8 The boundary precision/recall trade-off illuminates the mechanisms by which memory and prediction pressures affect learning (Figure 5). Without memory pressures (B = 0), segmentation rates are high, resulting in high recall and low precision. Introducing memory pressures (B > 0) slows the segmentation rate, resulting in a more balanced P/R trade-off. Without prediction pressures (F = 0), segmentation rates are generally low, resulting in higher precision and lower recall. Introducing prediction pressures (F > 0) increases the segmentation rate, again resulting in a more balanced trade-off. To understand this pattern, recall that a boundary in our model represents both a cost 8 See Appendix G for details. (flushing the memory cell) and a benefit (injecting top-down feedback). The cost of forgetting is plausibly greater for reconstruction than prediction, since only the current layer has had direct access to the sequence of reconstruction targets. By contrast, the benefit of top-down feedback is plausibly greater for prediction than reconstruction, since the prediction can condition on contextual representations at multiple timescales. In our segmental model of speech processing, the objectives therefore induce countervailing biases that boost signal for phonological constructs, supporting their joint influence on phoneme acquisition from speech.", "publication_ref": ["b79", "b79", "b92", "b42", "b70", "b120", "b32", "b123", "b23", "b29", "b53", "b52"], "figure_ref": ["fig_1", "fig_1", "fig_1", "fig_3", "fig_4"], "table_ref": ["tab_1"]}, {"heading": "Conclusion", "text": "We proposed an unsupervised deep neural model of speech processing that is incremental, segmental, and optimized by local feedback. We manipulated the model's objective function in order to investigate prior hypotheses about the role in human language acquisition of memory constraints on the one hand and predictive processing on the other. Results support a role for both memory and prediction pressures for acquiring phonemes from speech.\nBoth objectives inform the model's segmentation behavior and the content of its segment encodings.\nIn addition, results suggest that these two mechanisms coordinate to support phoneme discovery by introducing countervailing pressures toward retention of previously encountered signals (memory) and consultation of top-down signals (prediction).\n\u2022 The recurrent connection includes both the previous segment label and the current segment length in addition to the previous hidden state. We found this to be helpful during model development, and we hypothesize that this is because doing so removes the need for this information to be encoded by the model.\n\u2022 We implement the case-wise reasoning of the segmentation decisions using multiplicative masking rather than logical selection. This is intended to boost signal into the boundary decisions.\n\u2022 We enforce hierarchical segmentation behavior by multiplicatively masking the segmentation decision at layer l with the segmentation decision at layer l \u2212 1, thus preventing higher layers from segmenting where lower layers do not.\n\u2022 We compute boundaries during training via Bernoulli sampling rather than rounding. We found this to substantially improve performance on the development set, and we hypothesize that sampling may improve the straightthrough gradient estimates by ensuring that the segmentation decision is unbiased with respect to the underlying segmentation probability.\n\u2022 We renormalize the preactivations s (l)\nt by the incoming boundary decisions (eq. A7). We found this to be helpful during model development, and we hypothesize that this is because it avoids fluctuation in the scale of preactivations as a function of the boundaries.\n\u2022 We do not apply the Chung et al. (2017) technique of slope annealing, i.e. gradually increasing the steepness of the sigmoid activation function to reduce bias in the straightthrough estimator. We did not find an appreciable benefit from slope annealing during development, and it had a tendency to produce training instability. Eliminating it also reduces experimenter degrees of freedom by removing design decisions about the annealing function.", "publication_ref": ["b28"], "figure_ref": [], "table_ref": []}, {"heading": "C Decoder Definition", "text": "The decoder consists of two attentional seq2seq LSTMs with L layers each, one backwarddirectional (memory) and one forward directional (prediction). Given a backward window size B and a forward window size F , each backward decoder layer generates reconstructions Y B(l) t \u2208 R B\u00d7D l\u22121 and each forward decoder layer generates predictions Y F(l) t \u2208 R F \u00d7D l\u22121 , corresponding respectively to the B preceding and F following segment labels of layer l \u2212 1 at time t. The initial decoder hidden and cell states -h dB(l) t,0 and c dB(l) t,0 for the backward decoder and h dF(l) t,0 and c dF(l) t,0 for the forward decoder -are generated using multilayer feedforward transforms f hB(l) , f cB(l) , f hF(l) , and f cF(l) :\nh dB(l) t,0 def = f hB(l) h e(l) t (A22) c dB(l) t,0 def = f cB(l) h e(l) t (A23) h dF(l) t,0 def = f hF(l) h e(l) t (A24) c dF(l) t,0 def = f cF(l) h e(l) t (A25)\nDecoder states are doubly time indexed by t, i, where t indexes the encoder timestamp (i.e. the input timestep at which decoding begins) and i indexes the decoder timestamp (i.e. progress through the B or F decoder frames). Given decoder states h\ndB(l) t,i , h dF(l) t,i , predictions Y B(l) t [i] , Y F(l) t [i] \u2208 R D l\u22121 are generated using multilayer feedforward transforms f yB(l) , f yF(l) : Y B(l) t [i] def = f yB(l) h dB(l) t,i(A26)\nY F(l) t [i] def = f yF(l) h dF(l) t,i(A27)\nThe decoder takes as input a periodic positional encoding e i , generated following Vaswani et al. (2017). Non-final layers additionally take as an attention values the predictions from the layer above, i.e. Y B(l+1) t (for backward reconstruction) and Y F(l+1) t (for forward prediction) and compute a weighted sum of these values over time with attention weight vectors a B(l) t,i \u2208 (0, 1) B and a F(l) t,i \u2208 (0, 1) F to generate context vectors w B(l) t,i and w\nF(l) t,i : w B(l) t,i def = Y B(l+1) t a B(l) t,i (A28) w F(l) t,i def = Y F(l+1) t a F(l) t,i (A29)\nAttention weights a B(l) and a F(l) are computed using Gaussian kernel k(i; \u00b5, \u03c3 2 ):\nk(i; \u00b5, \u03c3 2 ) def = exp (i \u2212 \u00b5) 2 \u03c3 2 (A30)\nKernel k is applied to decoder time, with concentration \u03c3 B(l) , \u03c3 F(l) = 0.25 and with location \u00b5\nB(l) t,i \u00b5 F(l)\nt,i \u2208 R + computed by transforming the previous decoder state using a feedforward transform f qB(l) , f qF(l) and adding the result to the previous attention location:\n\u00b5 B(l) t,i def = abs f qB(l) h dB(l) t,i\u22121 + \u00b5 B(l) t,i\u22121 (A31) \u00b5 F(l) t,i def = abs f qF(l) h dF(l) t,i\u22121 + \u00b5 F(l) t,i\u22121 (A32)\nwhere \u00b5 B(l) t,0 = 1. Unit-normalized attention vectors are computed from timestamp vectors t B def = (1, . . . , B) and t F def = (1, . . . , F ) as:\na B(l) t,i def = k t B ; \u03c3 B(l) , \u00b5 B(l) t,i B j=1 k t B [j] ; \u03c3 B(l) , \u00b5 B(l) t,j(A33)\na\nF(l) t,i def = k t F ; \u03c3 F(l) , \u00b5 F(l) t,i B j=1 k t F [j] ; \u03c3 F(l) , \u00b5 F(l) t,j(A34)\nThe attention weights are thus constrained to march monotonically in time from t into the decoded past or future predicted segment labels from the layer above. Using fixed concentration 0.25 yields an effective kernel width [\u22122\u03c3, 2\u03c3] of one timestep, ensuring that the bulk of the attention kernel either falls on a single segment label or straddles two consecutive segment labels and preventing the decoder from spreading its attention over many higher-level segments. This design encourages one-to-many temporal alignment between decoded segment labels and decoded inputs, while allowing the decoder to determine how long to attend to a predicted segment label before moving on to the next one. At the final (top) layer, no top-down predictions are available, so the context vectors are omitted (or, equivalently, set to 0). The inputs to the decoder x dB(l) t,i , x dF(l) t,i are constructed as the vertical concatenation of e, w, and the previously generated decoder output, and a standard LSTM state update is applied: t,i\u22121 , i > 0 (A38) study because the model is unsupervised. Since we wish to test theories about cognition by extracting features from the acoustic stream without supervision, it is critical not only that the speech representation contain features that support identification of linguistic units, but that the representation emphasize those features in a plausibly similar manner to that of the human auditory system. Cochleagrams support this goal by incorporating more recent insights about human auditory perception (Mc-Dermott and Simoncelli, 2011). Our implementation uses the pycochleagram library https: //github.com/mcdermottLab/pycochleagram.\nx dB(l) t,i def = \uf8eb \uf8ec \uf8ed e i w B(l) t,i Y B(l) t [i\u22121] \uf8f6 \uf8f7 \uf8f8 (A35) x dF(l) t,i def = \uf8eb \uf8ec \uf8ed e i w F(l) t,i Y F(l) t [i\u22121] \uf8f6 \uf8f7 \uf8f8 (A36)\nWe L2 normalize the cochleagrams in order to encourage the decoder to focus on the spectral power envelope rather than absolute variation in loudness, since the former plausibly contains more linguistic signal. This procedure is supported by evidence of loudness constancy in human auditory perception, suggesting that similar kinds of normalization may take place in the brain (Zahorik and Wightman, 2001). We additionally z-transform the normalized cochleagrams over time within each audio file, since this proved beneficial during model development.\nThe source audio files contain many non-speech regions that are not of direct relevance for this study. We use the voice activity detection (VAD) intervals provided with the Zerospeech 2015 challenge data to remove these regions as a preprocess, and we force boundaries at the ends of VAD intervals. This greatly speeds training by removing irrelevant data, and it aligns with neuroscientific evidence of a prelinguistic capacity to detect human voices (Belin et al., 2000;Fecteau et al., 2005;Blasi et al., 2011;Pernet et al., 2015).", "publication_ref": ["b126", "b131", "b13", "b41", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "G Regression Model Design and Results", "text": "We use linear regression to test the relationship between performance and memory pressures, prediction pressures, and multiscale encoding. To do so, we combine raw boundary, phoneme classification, and feature classification metrics, along with deltas in these metrics over baselines U and X, into a single vector of performance statistics, each of which measures one aspect of the contribution of these dimensions to phoneme learning in our unsupervised models. To improve normality of performance metrics which are bounded on the interval [0, 1], as well as comparability of performance across metrics, we first (1) cast the metrics onto the interval  [\u22121, 1], (2) apply Fisher's Z transformation (i.e. arctanh), and (3) Z-score the transformed vectors within each metric type. We use binary coding for our predictors of interest: presence/absence of memory pressures (B > 0), presence/absence of prediction pressures (F > 0), and presence/absence of multiscale segmental encoding (L > 2). We also include categorical controls for comparison type (full, full -baseline U, full -baseline X) and metric type (boundary, phoneme, feature). Results, shown in Table A1, support a contribution of all three critical variables to phoneme acquisition.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Acknowledgements", "text": "We thank Aren Jansen and the Clippers discussion group at Ohio State for providing valuable feedback on this research. This work was funded in part by a Google Faculty Research Award to Micha Elsner.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Encoder Definition", "text": "Given encoder inputs x e t \u2208 R D 0 for 0 < t \u2264 T , an encoder with L layers is a recurrent neural network that computes its D l -dimensional hidden state h e(l) t \u2208 R D l at timestep t and layer l from the hidden state below h e(l\u22121) t (bottom-up connection), the previous hidden state h e(l) t\u22121 (recurrent connection), and the previous hidden state above h e(l+1) t\u22121 , where the layer zeroth state h e(0) t \u2208 R D 0 is the data x e t . The hidden state h e(l) t serves as a label at layer l and timestep t. Information flow between these layers is governed by a discrete boundary neuron z (l) t \u2208 {0, 1} at each layer. Let t (l) be the location of the most recent segment boundary preceding time t in layer l:\n) be a filter function dropping labels of l at non-boundaries between timepoints a and b:\nt at layer l and timestep t is defined as:\nIn other words, the segment S (l) t consists of the sequence of segment labels from layer l \u2212 1 at boundaries from l \u2212 1 that intervene since the last segment boundary at l.\nThe bottom-up, recurrent, and top-down inputs are respectively linearly transformed into vectors in s b(l) , s r(l) , s t(l) \u2208 R 4D l +1 using weight matrices W j i mapping from layer i to layer j, and masked using the boundary decisions z:\nwhere h e(l) t\nrecords the label at the preceding segment boundary h e(l) t (l) and n (l) t is the number of timesteps since the preceding segment boundary at l. We pass this additional information into the recurrent connection to relieve pressure on the cell state to encode it. These vectors are summed together with a bias b (l) to create a vector of preactivations s (l) , normalized by the boundary decisions so that the weights on active connections sum to 1:\nIn this way, information is only passed upward and downward at boundaries, and the boundaries thus govern information flow between adjacent layers. The vector s\nand deterministically during evaluation:\nWe additionally require that z (0) t = 1 (the input always \"segments\") and z (L) t = 0 (the top layer never segments). To enforce hierarchical segmentation behavior, we mask the boundaries by the boundaries at the layer below:\nGradients through these discrete decisions are approximated using straight-through estimation (Hinton, 2012;Bengio et al., 2013;Courbariaux et al., 2016;Chung et al., 2017;Shain and Elsner, 2019;Eloff et al., 2019).\nThe forget gates f ", "publication_ref": ["b61", "b14", "b31", "b28", "b120", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "211", "text": "The cell state c e(l) t is a weighted sum of three terms: a flush operation c f(l) t that erases the cell memory, a standard LSTM update c u(l) , and a copy operation c c(l) that copies the preceding cell state forward:\nThese terms are weighted by the boundary decision such that a flush occurs when the preceding timestep finds a boundary, an update otherwise occurs when the layer below finds a boundary, and a copy occurs when neither layer finds a boundary:\nThe hidden state h e(l) t is computed as:\nThe previous segment encoding h e(l) t is updated following a boundary and copied forward otherwise:\n= 1, and copied forward otherwise:\nB Comparison of Encoder Model to Chung et al. (2017) Although our encoder model closely follows the definition in Chung et al. (2017), it differs in the following ways:\nThe decoder is only applied to elements of f f(l) (1, T ) (i.e. only to frames where layer l segments) and only decodes the last B elements of f f(l\u22121) (1, t) and the first F elements of f f(l\u22121) (t + 1, T ); that is, it decodes only the B preceding segment labels and F following segment labels from layer l \u2212 1, ignoring labels at non-boundaries. Therefore, like encoding, decoding is also multiscale, taking place at the timescale of the encoder representations.", "publication_ref": ["b28", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "D Objective", "text": "Each decoder layer contributes two terms to the objective, a forward objective and a backward objective. Layer 1 decodes the data and uses a squared error loss:\nLayers 2, . . . , L decode the representations from the layer below, which are tanh-activated and thus constrained to the interval (\u22121, 1). Encoder features h e(l) t are deterministically cast into bitwise feature probabilities p e(l) t and decoded using sigmoid cross-entropy loss:\nLet T (l) denote the number of segment boundaries in layer l. Let Z B(l)\nt,i,d respectively denote the backward and forward targets and model predictions at encoder time t, decoder time i, and dimension d, defined as follows:\nThe backward and forward loss components L B(l) and L F(l) are computed as:\nThe overall loss L is:", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E Implementation Details", "text": "We apply the following implementation decisions in this study:\n\u2022 One hidden layer of 128 units for all feedforward transforms\n\u2022 Positional encoding dimensionality of 128\n\u2022 Exponential linear unit (elu) activations for all internal feedforward layers (Clevert et al., 2015) \u2022 Glorot uniform initialization for bottom-up, top-down, and feedforward encoder and decoder weight matrices (Glorot and Bengio, 2010) \u2022 Orthogonal initialization for recurrent weight matrices (Saxe et al., 2013)\n\u2022 Adam optimizer (Kingma and Ba, 2014) with learning rate 0.001, a minibatch size of 8, and default TensorFlow parameters.\n\u2022 Probing classifier implementation -Logistic regression using scikit-learn (Pedregosa et al., 2011) -Phoneme prediction is multinomial, feature prediction is binary -Minority feature class is always coded as positive -2-fold cross-validation -L2 \u03bb = 1 -100 LBFGS iterations (Zhu et al., 1997) per fold", "publication_ref": ["b30", "b46", "b102", "b133"], "figure_ref": [], "table_ref": []}, {"heading": "F Data Preprocessing", "text": "We convert the audio recordings into sequences of 50-dimensional cochleagrams (Brown and Cooke, 1994;McDermott and Simoncelli, 2011), each representing 10ms of audio data. Although this differs from the standard automatic speech recognition pipeline based on Mel frequency cepstral coefficients (Mermelstein, 1976), it is motivated for our", "publication_ref": ["b17", "b89", "b91"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Automatic segmentation and clustering of speech using sparse coding and metaheuristic search", "journal": "", "year": "2015", "authors": "Wiehan Agenbag; Thomas Niesler"}, {"ref_id": "b1", "title": "Islands of reliability for regular morphology: Evidence from Italian. Language", "journal": "", "year": "2002", "authors": "Adam Albright"}, {"ref_id": "b2", "title": "The effect of subphonetic differences on lexical access", "journal": "Cognition", "year": "1994", "authors": "Sheila E Jean E Andruski; Martha Blumstein;  Burton"}, {"ref_id": "b3", "title": "Deep learning methods for unsupervised acoustic modeling-Leap submission to ZeroSpeech challenge", "journal": "IEEE", "year": "2017", "authors": "Rajath T K Ansari; Sonali Kumar; Sriram Singh;  Ganapathy"}, {"ref_id": "b4", "title": "Unsupervised HMM posteriograms for language independent acoustic modeling in zero resource conditions", "journal": "IEEE", "year": "2017", "authors": "Rajath T K Ansari; Sonali Kumar; Sriram Singh; Susheela Ganapathy;  Devi"}, {"ref_id": "b5", "title": "Learning during processing: Word learning doesn't wait for word recognition to finish", "journal": "Cognitive science", "year": "2017", "authors": "S Keith; Bob Apfelbaum;  Mcmurray"}, {"ref_id": "b6", "title": "Some informational aspects of visual perception", "journal": "Psychological review", "year": "1954", "authors": "Fred Attneave"}, {"ref_id": "b7", "title": "Singulars and plurals in Dutch: Evidence for a parallel dual-route model", "journal": "Journal of Memory and Language", "year": "1997", "authors": "Ton R Harald Baayen; Robert Dijkstra;  Schreuder"}, {"ref_id": "b8", "title": "The Phonological Loop as a Language Learning Device", "journal": "Psychological Review", "year": "1998", "authors": "Alan Baddeley; Susan Gathercole; Costanza Papagno"}, {"ref_id": "b9", "title": "Working Memory", "journal": "", "year": "1974", "authors": "D Alan; Graham Baddeley;  Hitch"}, {"ref_id": "b10", "title": "Discovering discrete subword units with binarized autoencoders and hidden-markovmodel encoders", "journal": "", "year": "2015", "authors": "Leonardo Badino; Alessio Mereta; Lorenzo Rosasco"}, {"ref_id": "b11", "title": "Using articulatory features and inferred phonological segments in zero resource speech processing", "journal": "", "year": "2015", "authors": "Pallavi Baljekar; Sunayana Sitaram; Prasanna Kumar Muthukumar; Alan W Black"}, {"ref_id": "b12", "title": "Modeling unsupervised phonetic and phonological learning in Generative Adversarial Phonology", "journal": "", "year": "2020", "authors": "Ga\u0161per Begu\u0161"}, {"ref_id": "b13", "title": "Voice-selective areas in human auditory cortex", "journal": "Nature", "year": "2000", "authors": "Pascal Belin; J Robert; Philippe Zatorre; Pierre Lafaille; Bruce Ahad;  Pike"}, {"ref_id": "b14", "title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "journal": "", "year": "2013", "authors": "Yoshua Bengio; Nicholas L\u00e9onard; Aaron Courville"}, {"ref_id": "b15", "title": "Predictability, complexity, and learning. Neural computation", "journal": "", "year": "2001", "authors": "William Bialek; Ilya Nemenman; Naftali Tishby"}, {"ref_id": "b16", "title": "Early specialization for voice and emotion processing in the infant brain", "journal": "Current biology", "year": "2011", "authors": "Anna Blasi; Evelyne Mercure; Sarah Lloyd-Fox; Alex Thomson; Michael Brammer; Disa Sauter; Quinton Deeley; J Gareth; Ville Barker;  Renvall"}, {"ref_id": "b17", "title": "Computational auditory scene analysis. Computer speech and language", "journal": "", "year": "1994", "authors": "J Guy; Martin Brown;  Cooke"}, {"ref_id": "b18", "title": "Inflection classes, gender, and the principle of contrast. Language", "journal": "", "year": "1994", "authors": "Andrew Carstairs-Mccarthy"}, {"ref_id": "b19", "title": "Parallel inference of Dirichlet process Gaussian mixture models for unsupervised acoustic modeling: A feasibility study", "journal": "", "year": "2015", "authors": "Hongjie Chen; Cheung-Chi Leung; Lei Xie; Bin Ma; Haizhou Li"}, {"ref_id": "b20", "title": "Multilingual bottle-neck feature learning from untranscribed speech", "journal": "IEEE", "year": "2017", "authors": "Hongjie Chen; Cheung-Chi Leung; Lei Xie; Bin Ma; Haizhou Li"}, {"ref_id": "b21", "title": "Syntactic Structures", "journal": "", "year": "1957", "authors": "Noam Chomsky"}, {"ref_id": "b22", "title": "Aspects of the Theory of Syntax", "journal": "MIT Press", "year": "1965", "authors": "Noam Chomsky"}, {"ref_id": "b23", "title": "The Sound Pattern of English", "journal": "Harper \\& Row", "year": "1968", "authors": "Noam Chomsky; Morris Halle"}, {"ref_id": "b24", "title": "Unsupervised speech representation learning using wavenet autoencoders", "journal": "", "year": "2019", "authors": "Jan Chorowski; Ron J Weiss; Samy Bengio; A\u00e4ron Van Den Oord"}, {"ref_id": "b25", "title": "speech, and language processing", "journal": "", "year": "", "authors": ""}, {"ref_id": "b26", "title": "Turning the pipeline into a loop: Iterated unsupervised dependency parsing and PoS induction", "journal": "", "year": "2012", "authors": "Christos Christodoulopoulos; Sharon Goldwater; Mark Steedman"}, {"ref_id": "b27", "title": "Analyzing analytical methods: The case of phonology in neural models of spoken language", "journal": "", "year": "2020", "authors": "Grzegorz Chrupa\u0142a; Bertrand Higy; Afra Alishahi"}, {"ref_id": "b28", "title": "Hierarchical Multiscale Recurrent Neural Networks", "journal": "", "year": "2017", "authors": "Junyoung Chung; Sungjin Ahn; Yoshua Bengio"}, {"ref_id": "b29", "title": "The geometry of phonological features", "journal": "Phonology", "year": "1985", "authors": "N George;  Clements"}, {"ref_id": "b30", "title": "Fast and accurate deep network learning by exponential linear units (elus)", "journal": "", "year": "2015", "authors": "Djork-Arn\u00e9 Clevert; Thomas Unterthiner; Sepp Hochreiter"}, {"ref_id": "b31", "title": "Ran El-Yaniv, and Yoshua Bengio", "journal": "", "year": "2016", "authors": "Matthieu Courbariaux; Itay Hubara; Daniel Soudry"}, {"ref_id": "b32", "title": "On short and long auditory stores", "journal": "Psychological bulletin", "year": "1984", "authors": "Nelson Cowan"}, {"ref_id": "b33", "title": "Time course and functional neuroanatomy of speech segmentation in adults", "journal": "", "year": "2009", "authors": "Toni Cunillera; Estela C\u00e0mara; M Juan; Josep Toro;  Marco-Pallares"}, {"ref_id": "b34", "title": "The effects of stress and statistical cues on continuous speech segmentation: an event-related brain potential study", "journal": "", "year": "2006", "authors": "Toni Cunillera; M Juan;  Toro"}, {"ref_id": "b35", "title": "A smartphone-based ASR data collection tool for under-resourced languages", "journal": "", "year": "2014", "authors": " Nic J De;  Vries; H Marelie; Jaco Davel; Willem D Badenhorst; Febe Basson; Etienne De Wet; Alta De Barnard;  Waal"}, {"ref_id": "b36", "title": "The zero resource speech challenge", "journal": "IEEE", "year": "2017", "authors": "Ewan Dunbar; Xuan Nga Cao; Juan Benjumea; Julien Karadayi; Mathieu Bernard; Laurent Besacier; Xavier Anguera; Emmanuel Dupoux"}, {"ref_id": "b37", "title": "Distributed representations, simple recurrent networks, and grammatical structure", "journal": "", "year": "1991", "authors": " Jeffrey L Elman"}, {"ref_id": "b38", "title": "Learning and development in neural networks: The importance of starting small", "journal": "Cognition", "year": "1993", "authors": " Jeffrey L Elman"}, {"ref_id": "b39", "title": "Unsupervised Acoustic Unit Discovery for Speech Synthesis Using Discrete Latent-Variable Neural Networks", "journal": "", "year": "2019", "authors": "Ryan Eloff; Andr\u00e9 Nortje; Avashna Benjamin Van Niekerk; Leanne Govender; Arnu Nortje;  Pretorius; Ewald Elan Van Biljon; Lisa Van Der Westhuizen; Herman Van Staden;  Kamper"}, {"ref_id": "b40", "title": "Speech segmentation with a neural encoder model of working memory", "journal": "", "year": "2017", "authors": "Micha Elsner; Cory Shain"}, {"ref_id": "b41", "title": "Sensitivity to voice in human prefrontal cortex", "journal": "Journal of Neurophysiology", "year": "2005", "authors": "Shirley Fecteau; Jorge L Armony; Yves Joanette; Pascal Belin"}, {"ref_id": "b42", "title": "The DARPA Speech Recognition Research Database: Specifications and Status", "journal": "", "year": "1986", "authors": "M William;  Fisher; Kathleen M Goudie-Marshall George R Doddington"}, {"ref_id": "b43", "title": "Insensitivity of the Human Sentence-Processing System to Hierarchical Structure", "journal": "Psychological Science", "year": "2011", "authors": "L Stefan; Rens Frank;  Bod"}, {"ref_id": "b44", "title": "Hierarchical and sequential processing of language. Language", "journal": "Cognition and Neuroscience", "year": "2018", "authors": "L Stefan; Morten H Frank;  Christiansen"}, {"ref_id": "b45", "title": "The free-energy principle: a unified brain theory?", "journal": "Nature reviews neuroscience", "year": "2010", "authors": "Karl Friston"}, {"ref_id": "b46", "title": "Understanding the difficulty of training deep feedforward neural networks", "journal": "", "year": "2010", "authors": "Xavier Glorot; Yoshua Bengio"}, {"ref_id": "b47", "title": "Language Identification in the Limit", "journal": "Information and Control", "year": "1967", "authors": "Mark E Gold"}, {"ref_id": "b48", "title": "Autosegmental phonology", "journal": "MIT Press London", "year": "1976", "authors": "John Goldsmith"}, {"ref_id": "b49", "title": "Unsupervised learning of the morphology of a natural language", "journal": "Computational Linguistics", "year": "2003", "authors": "John Goldsmith"}, {"ref_id": "b50", "title": "A {Bayesian} framework for word segmentation: {Exploring} the effects of context. Cognition", "journal": "", "year": "2009", "authors": "Sharon Goldwater; Thomas Griffiths; Mark Johnson"}, {"ref_id": "b51", "title": "Predictive power of word surprisal for reading times is a linear function of language model quality", "journal": "", "year": "2018", "authors": "Adam Goodkind; Klinton Bicknell"}, {"ref_id": "b52", "title": "A cross-linguistic acoustic study of voiceless fricatives", "journal": "Journal of the International Phonetic Association", "year": "2002", "authors": "Matthew Gordon; Paul Barthmaier; Kathy Sands"}, {"ref_id": "b53", "title": "Phonation types: a cross-linguistic overview", "journal": "Journal of phonetics", "year": "2001", "authors": "Matthew Gordon; Peter Ladefoged"}, {"ref_id": "b54", "title": "Uncertainty about the rest of the sentence", "journal": "Cognitive Science", "year": "2006", "authors": "John Hale"}, {"ref_id": "b55", "title": "Cortical connectivity and sensory coding", "journal": "Nature", "year": "2013", "authors": "D Kenneth; Thomas D Mrsic-Flogel Harris"}, {"ref_id": "b56", "title": "Hierarchical process memory: memory as an integral component of information processing", "journal": "Trends in cognitive sciences", "year": "2015", "authors": "Uri Hasson; Janice Chen; Christopher J Honey"}, {"ref_id": "b57", "title": "Introductory phonology", "journal": "John Wiley \\& Sons", "year": "2011", "authors": "Bruce Hayes"}, {"ref_id": "b58", "title": "Scalable modified Kneser-Ney language model estimation", "journal": "Bulgaria", "year": "2013", "authors": "Kenneth Heafield; Ivan Pouzyrevsky; Jonathan H Clark; Philipp Koehn"}, {"ref_id": "b59", "title": "Unsupervised linear discriminant analysis for supporting DPGMM clustering in the zero resource scenario", "journal": "Procedia Computer Science", "year": "2016", "authors": "Michael Heck; Sakriani Sakti; Satoshi Nakamura"}, {"ref_id": "b60", "title": "Feature optimized dpgmm clustering for unsupervised subword modeling: A contribution to zerospeech", "journal": "IEEE", "year": "2017", "authors": "Michael Heck; Sakriani Sakti; Satoshi Nakamura"}, {"ref_id": "b61", "title": "Neural Networks for Machine Learning. Coursera, video lectures", "journal": "", "year": "2012", "authors": "Geoffrey Hinton"}, {"ref_id": "b62", "title": "Long Short-Term Memory", "journal": "Neural Comput", "year": "1997", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"ref_id": "b63", "title": "Neural networks and physical systems with emergent collective computational abilities", "journal": "", "year": "1982", "authors": "J John;  Hopfield"}, {"ref_id": "b64", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "journal": "", "year": "2015", "authors": "Sergey Ioffe; Christian Szegedy"}, {"ref_id": "b65", "title": "Maintenance of multiple working memory items by temporal segmentation. Neuroscience", "journal": "", "year": "2006", "authors": "Ole Jensen"}, {"ref_id": "b66", "title": "Prediction plays a key role in language development as well as processing", "journal": "Behavioral and Brain Sciences", "year": "2013", "authors": "A Matt; Nicholas B Johnson; Adele E Turk-Browne;  Goldberg"}, {"ref_id": "b67", "title": "Exploring the limits of language modeling", "journal": "", "year": "2016", "authors": "Rafal Jozefowicz; Oriol Vinyals; Mike Schuster; Noam Shazeer; Yonghui Wu"}, {"ref_id": "b68", "title": "Revisiting the Hierarchical Multiscale LSTM", "journal": "", "year": "2018", "authors": "Akos K\u00e1d\u00e1r; Marc-Alexandre C\u00f4t\u00e9; Grzegorz Chrupa\u0142a; Afra Alishahi"}, {"ref_id": "b69", "title": "Unsupervised neural network based feature extraction using weak top-down constraints", "journal": "IEEE", "year": "2015", "authors": "Herman Kamper; Micha Elsner; Aren Jansen; Sharon Goldwater"}, {"ref_id": "b70", "title": "A segmental framework for fullyunsupervised large-vocabulary speech recognition", "journal": "Computer Speech & Language", "year": "2017", "authors": "Herman Kamper; Aren Jansen; Sharon Goldwater"}, {"ref_id": "b71", "title": "An embedded segmental k-means model for unsupervised segmentation and clustering of speech", "journal": "IEEE", "year": "2017", "authors": "Herman Kamper; Karen Livescu; Sharon Goldwater"}, {"ref_id": "b72", "title": "Predictive Processing: A Canonical Cortical Computation", "journal": "Neuron", "year": "2018", "authors": "B Georg; Thomas D Mrsic-Flogel Keller"}, {"ref_id": "b73", "title": "Less really is more for adults learning a miniature artificial language", "journal": "Journal of Memory and Language", "year": "2001", "authors": "W Alan; Julie L Kersten;  Earles"}, {"ref_id": "b74", "title": "A hierarchy of time-scales and the brain", "journal": "PLoS Comput Biol", "year": "2008", "authors": "J Stefan; Jean Kiebel; Karl J Daunizeau;  Friston"}, {"ref_id": "b75", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b76", "title": "Corpusbased induction of syntactic structure: Models of dependency and constituency", "journal": "", "year": "2004", "authors": "Dan Klein; Christopher D Manning"}, {"ref_id": "b77", "title": "Predictive brain signals of linguistic development. Frontiers in psychology", "journal": "", "year": "2013", "authors": "Caroline Valesca Kooijman; Elizabeth K Junge; Peter Johnson; Anne Hagoort;  Cutler"}, {"ref_id": "b78", "title": "Braininspired speech segmentation for automatic speech recognition using the speech envelope as a temporal reference", "journal": "Scientific reports", "year": "2016", "authors": "Byeongwook Lee; Kwang-Hyun Cho"}, {"ref_id": "b79", "title": "A Nonparametric {Bayesian} Approach to Acoustic Model Discovery", "journal": "", "year": "2012", "authors": "Chia-Ying Lee; James Glass"}, {"ref_id": "b80", "title": "Unsupervised Lexicon Discovery from Acoustic Input", "journal": "", "year": "2015", "authors": "Chia-Ying Lee; J Timothy; James O'donnell;  Glass"}, {"ref_id": "b81", "title": "Assessing the ability of LSTMs to learn syntax-sensitive dependencies", "journal": "Transactions of the Association for Computational Linguistics", "year": "2016", "authors": "Tal Linzen; Emmanuel Dupoux; Yoav Goldberg"}, {"ref_id": "b82", "title": "The existence of persistent states in the brain", "journal": "Mathematical biosciences", "year": "1974", "authors": " William A Little"}, {"ref_id": "b83", "title": "An evaluation of graph clustering methods for unsupervised term discovery", "journal": "", "year": "2015", "authors": "Vince Lyzinski; Gregory Sell; Aren Jansen"}, {"ref_id": "b84", "title": "Vision: A Computational Investigation into the Human Representation and Processing of Visual Information", "journal": "W.H. Freeman and Company", "year": "1982", "authors": "David Marr"}, {"ref_id": "b85", "title": "STDP allows close-tooptimal spatiotemporal spike pattern detection by single coincidence detector neurons", "journal": "Neuroscience", "year": "2018", "authors": "Timoth\u00e9e Masquelier"}, {"ref_id": "b86", "title": "Language learning as language use: A cross-linguistic model of child language development", "journal": "Psychological review", "year": "2019", "authors": "M Stewart; Morten H Mccauley;  Christiansen"}, {"ref_id": "b87", "title": "Revisiting the poverty of the stimulus: Hierarchical generalization without a hierarchical bias in recurrent neural networks", "journal": "", "year": "2018", "authors": "Thomas Mccoy; Robert Frank; Tal Linzen"}, {"ref_id": "b88", "title": "Does syntax need to grow on trees? Sources of hierarchical inductive bias in sequence-to-sequence networks", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "Thomas Mccoy; Robert Frank; Tal Linzen"}, {"ref_id": "b89", "title": "Sound texture perception via statistics of the auditory periphery: evidence from sound synthesis", "journal": "Neuron", "year": "2011", "authors": "H Josh;  Mcdermott; P Eero;  Simoncelli"}, {"ref_id": "b90", "title": "Gradient effects of within-category phonetic variation on lexical access", "journal": "Cognition", "year": "2002", "authors": "Bob Mcmurray; K Michael; Richard N Tanenhaus;  Aslin"}, {"ref_id": "b91", "title": "Distance measures for speech recognition, psychological and instrumental. Pattern recognition and artificial intelligence", "journal": "", "year": "1976", "authors": "Paul Mermelstein"}, {"ref_id": "b92", "title": "Blind Phoneme Segmentation With Temporal Prediction Errors", "journal": "", "year": "2017", "authors": "Paul Michel; Okko Rasanen; Roland Thiolli\u00e8re; Emmanuel Dupoux"}, {"ref_id": "b93", "title": "Humans store about 1.5 megabytes of information during language acquisition", "journal": "Royal Society open science", "year": "2019", "authors": "Francis Mollica; T Steven;  Piantadosi"}, {"ref_id": "b94", "title": "Words in puddles of sound: Modelling psycholinguistic effects in speech segmentation", "journal": "Journal of child language", "year": "2010", "authors": "Padraic Monaghan; Morten H Christiansen"}, {"ref_id": "b95", "title": "Maturational constraints on language learning", "journal": "Cognitive Science", "year": "1990", "authors": "Elissa Newport"}, {"ref_id": "b96", "title": "On structuring probabilistic dependences in stochastic language modelling", "journal": "Computer Speech and Language", "year": "1994", "authors": "Hermann Ney; Ute Essen; Reinhard Kneser"}, {"ref_id": "b97", "title": "Motion extrapolation in catching", "journal": "Nature", "year": "1994", "authors": " R Nijhawan"}, {"ref_id": "b98", "title": "Adeen Flinker, and Nima Mesgarani. 2020. Hierarchical integration across multiple timescales in human auditory cortex", "journal": "", "year": "", "authors": "Laura K Sam V Norman-Haignere; Orrin Long; Werner Devinsky; Ifeoma Doyle; Edward Irobunda;  Merricks; A Neil;  Feldstein; M V Guy; Catherine Mckhann;  Schevon"}, {"ref_id": "b99", "title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "journal": "Nature", "year": "1996", "authors": "A Bruno; David J Olshausen;  Field"}, {"ref_id": "b100", "title": "Sparse coding of sensory inputs. Current opinion in neurobiology", "journal": "", "year": "2004", "authors": "A Bruno; David J Olshausen;  Field"}, {"ref_id": "b101", "title": "WaveNet: A Generative Model for Raw Audio", "journal": "", "year": "2016", "authors": "A\u00e4ron Van Den Oord; Sander Dieleman; Heiga Zen; Karen Simonyan; Oriol Vinyals; Alex Graves; Nal Kalchbrenner; Andrew Senior; Koray Kavukcuoglu"}, {"ref_id": "b102", "title": "Scikitlearn: Machine learning in Python", "journal": "Journal of machine learning research", "year": "2011-10", "authors": "Fabian Pedregosa; Ga\u00ebl Varoquaux; Alexandre Gramfort; Vincent Michel; Bertrand Thirion; Olivier Grisel; Mathieu Blondel; Peter Prettenhofer; Ron Weiss"}, {"ref_id": "b103", "title": "When do memory limitations lead to regularization? An experimental and computational investigation", "journal": "Journal of Memory and Language", "year": "2012", "authors": "Amy Perfors"}, {"ref_id": "b104", "title": "", "journal": "Mitchell Valdes-Sosa", "year": "", "authors": "Phil Cyril R Pernet; Marianne Mcaleer;  Latinus; J Krzysztof; Ian Gorgolewski; Patricia E Charest;  Bestelmeyer"}, {"ref_id": "b105", "title": "The human voice areas: Spatial organization and inter-individual variability in temporal and extratemporal cortices", "journal": "Neuroimage", "year": "", "authors": ""}, {"ref_id": "b106", "title": "The role of language processing in language acquisition. Linguistic approaches to bilingualism", "journal": "", "year": "2015", "authors": "Colin Phillips; Lara Ehrenhofer"}, {"ref_id": "b107", "title": "The logical primitives of thought: Empirical foundations for compositional cognitive models", "journal": "Psychological review", "year": "2016", "authors": "Joshua B Steven T Piantadosi; Noah D Tenenbaum;  Goodman"}, {"ref_id": "b108", "title": "Rules of language", "journal": "Science", "year": "1991", "authors": "Steven Pinker"}, {"ref_id": "b109", "title": "The Buckeye corpus of conversational speech: labeling conventions and a test of transcriber reliability", "journal": "Speech Communication", "year": "2005", "authors": "A Mark; Keith Pitt; Elizabeth Johnson; Scott Hume; William Kiesling;  Raymond"}, {"ref_id": "b110", "title": "Language models are unsupervised multitask learners", "journal": "OpenAI Blog", "year": "2019", "authors": "Alec Radford; Jeffrey Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"ref_id": "b111", "title": "Unsupervised word discovery from speech using automatic segmentation into syllable-like units", "journal": "", "year": "2015", "authors": "Okko R\u00e4s\u00e4nen; Gabriel Doyle; Michael C Frank"}, {"ref_id": "b112", "title": "A comparison of neural network methods for unsupervised representation learning on the zero resource speech challenge", "journal": "", "year": "2015", "authors": "Daniel Renshaw; Herman Kamper; Aren Jansen; Sharon Goldwater"}, {"ref_id": "b113", "title": "Language acquisition in the MDL framework. Language computations", "journal": "", "year": "1994", "authors": "Jorma Rissanen; Eric Sven Ristad"}, {"ref_id": "b114", "title": "Language acquisition in the absence of explicit negative evidence: How important is starting small? Cognition", "journal": "", "year": "1999", "authors": "L T Douglas; David C Rohde;  Plaut"}, {"ref_id": "b115", "title": "Statistical learning by 8-month-old infants", "journal": "Science", "year": "1996", "authors": " Jenny R Saffran; Elissa L Richard N Aslin;  Newport"}, {"ref_id": "b116", "title": "An ERP study of continuous speech processing: I. Segmentation, semantics, and syntax in native speakers", "journal": "Cognitive Brain Research", "year": "2003", "authors": "D Lisa; Helen J Sanders;  Neville"}, {"ref_id": "b117", "title": "Depth of processing in language comprehension: Not noticing the evidence", "journal": "Trends in cognitive sciences", "year": "2002", "authors": "J Anthony; Patrick Sanford;  Sturt"}, {"ref_id": "b118", "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "journal": "", "year": "2013", "authors": "M Andrew; James L Saxe; Surya Mcclelland;  Ganguli"}, {"ref_id": "b119", "title": "A Neural Model of Adaptation in Reading", "journal": "", "year": "2018", "authors": "Marten Van Schijndel; Tal Linzen"}, {"ref_id": "b120", "title": "Measuring the perceptual availability of phonological features during language acquisition using unsupervised binary stochastic autoencoders", "journal": "", "year": "2019", "authors": "Cory Shain; Micha Elsner"}, {"ref_id": "b121", "title": "Sparse coding with memristor networks", "journal": "Nature nanotechnology", "year": "2017", "authors": "Fuxi Patrick M Sheridan; Chao Cai; Wen Du; Zhengya Ma; Wei D Zhang;  Lu"}, {"ref_id": "b122", "title": "Composite embedding systems for ZeroSpeech2017 Track1", "journal": "IEEE", "year": "2017", "authors": "Hayato Shibata; Taku Kato; Takahiro Shinozaki; Shinji Watanabet"}, {"ref_id": "b123", "title": "Sensory cortex is optimized for prediction of future input", "journal": "", "year": "2018", "authors": "Yosef Singer; Yayoi Teramoto; D B Ben; Jan W H Willmore; Andrew J Schnupp; Nicol S King;  Harper"}, {"ref_id": "b124", "title": "Articulatory gesture rich representation learning of phonological units in low resource settings", "journal": "Springer", "year": "2016", "authors": "Lal Brij Mohan; Manish Srivastava;  Shrivastava"}, {"ref_id": "b125", "title": "Neural discrete representation learning", "journal": "", "year": "2017", "authors": "Aaron Van Den;  Oord"}, {"ref_id": "b126", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b127", "title": "The zero resource speech challenge", "journal": "", "year": "2015", "authors": "Maarten Versteegh; Roland Thiolli\u00e8re; Thomas Schatz; Xuan Nga Cao; Xavier Anguera; Aren Jansen; Emmanuel Dupoux"}, {"ref_id": "b128", "title": "On productivity. Linguistic variation yearbook", "journal": "", "year": "2005", "authors": "Charles Yang"}, {"ref_id": "b129", "title": "Extracting bottleneck features and word-like pairs from untranscribed speech for feature representation", "journal": "IEEE", "year": "2017", "authors": "Yougen Yuan; Cheung-Chi Leung; Lei Xie; Hongjie Chen; Bin Ma; Haizhou Li"}, {"ref_id": "b130", "title": "Human brain activity time-locked to perceptual event boundaries", "journal": "Nature neuroscience", "year": "2001", "authors": "M Jeffrey;  Zacks; S Todd; Margaret A Braver; David I Sheridan; Abraham Z Donaldson; John M Snyder; Randy L Ollinger; Marcus E Buckner;  Raichle"}, {"ref_id": "b131", "title": "Loudness constancy with varying sound source distance", "journal": "Nature neuroscience", "year": "2001", "authors": "Pavel Zahorik;  Frederic L Wightman"}, {"ref_id": "b132", "title": "A deep scattering spectrum-deep siamese network pipeline for unsupervised acoustic modeling", "journal": "IEEE", "year": "2016", "authors": "Neil Zeghidour; Gabriel Synnaeve; Maarten Versteegh; Emmanuel Dupoux"}, {"ref_id": "b133", "title": "L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization", "journal": "ACM Transactions on Mathematical Software (TOMS)", "year": "1997", "authors": "Ciyou Zhu; H Richard; Peihuang Byrd; Jorge Lu;  Nocedal"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Incremental layerwise encoder-decoder framework. Shown here with 3 layers and a forward/backward window size of 3. Segment boundaries are shown in cyan. Gray arrows indicate information flow through the encoder, as governed by the boundary decisions. Colored arrows indicate information flow from encodings to decoder targets in the backward (orange) and forward (green) directions, starting from the encoded timestep at the center of the figure.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Phoneme acquisition scores. F-measures for boundary detection (left), phoneme classification (center), and phonological feature classification (right).", "figure_data": ""}, {"figure_label": "33", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 Figure 3 :33Figure 3 reports performance differences by metric against baseline U (untrained). Training yields consistent and often substantial improvements across metrics in multiscale (L > 0) encoders with both memory (B > 0) and prediction (F > 0) pressures, but can fail to improve in the", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Effect of language. Change in F-measure by metric over baseline X.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Boundary P/R trade-off. Boundary precision (left) and recall (right) by experimental configuration in Xitsonga and English.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "22.9 49.3 39.3 28.6 53.8 Baseline U 30.4 12.3 42.2 22.1 15.4 46.2 Baseline X 52.4 20.5 47.1 44.8 27.8 53.2", "figure_data": "EnglishXitsongaModelBdPcFcBdPcFcFull 65.3201"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "F-measures for boundary discovery (Bd), phoneme classification (Pc), and phonological feature classification (Fc), using B = 25, F = 1, L = 3.", "figure_data": ""}, {"figure_label": "A1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Linear regression results", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "h dB(l) t,0 def = f hB(l) h e(l) t (A22) c dB(l) t,0 def = f cB(l) h e(l) t (A23) h dF(l) t,0 def = f hF(l) h e(l) t (A24) c dF(l) t,0 def = f cF(l) h e(l) t (A25)", "formula_coordinates": [18.0, 128.98, 132.27, 161.29, 87.96]}, {"formula_id": "formula_1", "formula_text": "dB(l) t,i , h dF(l) t,i , predictions Y B(l) t [i] , Y F(l) t [i] \u2208 R D l\u22121 are generated using multilayer feedforward transforms f yB(l) , f yF(l) : Y B(l) t [i] def = f yB(l) h dB(l) t,i(A26)", "formula_coordinates": [18.0, 72.0, 301.79, 218.27, 73.92]}, {"formula_id": "formula_2", "formula_text": "Y F(l) t [i] def = f yF(l) h dF(l) t,i(A27)", "formula_coordinates": [18.0, 110.49, 383.04, 179.78, 16.56]}, {"formula_id": "formula_3", "formula_text": "F(l) t,i : w B(l) t,i def = Y B(l+1) t a B(l) t,i (A28) w F(l) t,i def = Y F(l+1) t a F(l) t,i (A29)", "formula_coordinates": [18.0, 99.54, 547.69, 190.73, 71.34]}, {"formula_id": "formula_4", "formula_text": "k(i; \u00b5, \u03c3 2 ) def = exp (i \u2212 \u00b5) 2 \u03c3 2 (A30)", "formula_coordinates": [18.0, 99.92, 672.52, 190.35, 26.38]}, {"formula_id": "formula_5", "formula_text": "B(l) t,i \u00b5 F(l)", "formula_coordinates": [18.0, 78.57, 738.48, 35.24, 15.74]}, {"formula_id": "formula_6", "formula_text": "\u00b5 B(l) t,i def = abs f qB(l) h dB(l) t,i\u22121 + \u00b5 B(l) t,i\u22121 (A31) \u00b5 F(l) t,i def = abs f qF(l) h dF(l) t,i\u22121 + \u00b5 F(l) t,i\u22121 (A32)", "formula_coordinates": [18.0, 315.78, 95.61, 209.76, 40.32]}, {"formula_id": "formula_7", "formula_text": "a B(l) t,i def = k t B ; \u03c3 B(l) , \u00b5 B(l) t,i B j=1 k t B [j] ; \u03c3 B(l) , \u00b5 B(l) t,j(A33)", "formula_coordinates": [18.0, 324.6, 203.65, 200.95, 39.14]}, {"formula_id": "formula_8", "formula_text": "F(l) t,i def = k t F ; \u03c3 F(l) , \u00b5 F(l) t,i B j=1 k t F [j] ; \u03c3 F(l) , \u00b5 F(l) t,j(A34)", "formula_coordinates": [18.0, 331.58, 249.96, 193.96, 39.14]}, {"formula_id": "formula_9", "formula_text": "x dB(l) t,i def = \uf8eb \uf8ec \uf8ed e i w B(l) t,i Y B(l) t [i\u22121] \uf8f6 \uf8f7 \uf8f8 (A35) x dF(l) t,i def = \uf8eb \uf8ec \uf8ed e i w F(l) t,i Y F(l) t [i\u22121] \uf8f6 \uf8f7 \uf8f8 (A36)", "formula_coordinates": [18.0, 343.9, 588.42, 181.64, 97.24]}], "doi": "10.18653/v1/P17"}