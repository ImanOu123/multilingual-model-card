{"title": "SGM: Sequence Generation Model for Multi-Label Classification", "authors": "Pengcheng Yang; Xu Sun; Wei Li; Shuming Ma; Wei Wu; Houfeng Wang", "pub_date": "", "abstract": "Multi-label classification is an important yet challenging task in natural language processing. It is more complex than single-label classification in that the labels tend to be correlated. Existing methods tend to ignore the correlations between labels. Besides, different parts of the text can contribute differently to predicting different labels, which is not considered by existing models. In this paper, we propose to view the multi-label classification task as a sequence generation problem, and apply a sequence generation model with a novel decoder structure to solve it. Extensive experimental results show that our proposed methods outperform previous work by a substantial margin. Further analysis of experimental results demonstrates that the proposed methods not only capture the correlations between labels, but also select the most informative words automatically when predicting different labels. 1   ", "sections": [{"heading": "Introduction", "text": "Multi-label classification (MLC) is an important task in the field of natural language processing (NLP), which can be applied in many real-world scenarios, such as text categorization (Schapire and Singer, 2000), tag recommendation (Katakis et al., 2008), information retrieval (Gopal and Yang, 2010), and so on. The target of the MLC task is to assign multiple labels to each instance in the dataset.\nBinary relevance (BR) (Boutell et al., 2004) is one of the earliest attempts to solve the MLC task by transforming the MLC task into multiple single-label classification problems. However, it neglects the correlations between labels. Classifier chains (CC) proposed by Read et al. (2011) converts the MLC task into a chain of binary classification problems to model the correlations between labels. However, it is computationally expensive for large datasets. Other methods such as ML-DT (Clare and King, 2001), Rank-SVM (Elisseeff and Weston, 2002), and ML-KNN (Zhang and Zhou, 2007) can only be used to capture the first or second order label correlations or are computationally intractable when high-order label correlations are considered.\nIn recent years, neural networks have achieved great success in the field of NLP. Some neural network models have also been applied in the MLC task and achieved important progress. For instance, fully connected neural network with pairwise ranking loss function is utilized in Zhang and Zhou (2006). Kurata et al. (2016) propose to perform classification using the convolutional neural network (CNN). Chen et al. (2017) use CNN and recurrent neural network (RNN) to capture the semantic information of texts. However, they either neglect the correlations between labels or do not consider differences in the contributions of textual content when predicting labels.\nIn this paper, inspired by the tremendous success of the sequence-to-sequence (Seq2Seq) model in machine translation (Bahdanau et al., 2014;Luong et al., 2015;Sun et al., 2017), abstractive summarization (Rush et al., 2015;, style transfer (Shen et al., 2017; and other domains, we propose a sequence generation model with a novel decoder structure to solve the MLC task. The proposed sequence generation model consists of an encoder and a decoder with the attention mechanism. The decoder uses an LSTM to generate labels sequentially, and predicts the next label based on its previously predicted labels. Therefore, the proposed model can consider the correlations between labels by processing label sequence dependencies through the LSTM structure. Furthermore, the attention mechanism considers the contributions of different parts of text when the model predicts different labels. In addition, a novel decoder structure with global embedding is proposed to further improve the performance of the model by incorporating overall informative signals.\nThe contributions of this paper are listed as follows:\n\u2022 We propose to view the MLC task as a sequence generation problem to take the correlations between labels into account.\n\u2022 We propose a sequence generation model with a novel decoder structure, which not only captures the correlations between labels, but also selects the most informative words automatically when predicting different labels.\n\u2022 Extensive experimental results show that our proposed methods outperform the baselines by a large margin. Further analysis demonstrates the effectiveness of the proposed methods on correlation representation.\nThe whole paper is organized as follows. We describe our methods in Section 2. In Section 3, we present the experiments and make analysis and discussions. Section 4 introduces the related work. Finally in Section 5 we conclude this paper and explore the future work.", "publication_ref": ["b26", "b11", "b9", "b3", "b23", "b5", "b6", "b39", "b38", "b14", "b4", "b0", "b18", "b30", "b24", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Proposed Method", "text": "We introduce our proposed methods in detail in this section. First, we give an overview of the model in Section 2.1. Second, we explain the details of the proposed sequence generation model in Section 2.2. Finally, Section 2.3 presents our novel decoder structure.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Overview", "text": "First of all, we define some notations and describe the MLC task. Given the label space with L labels L = {l 1 , l 2 , \u2022 \u2022 \u2022 , l L }, a text sequence x containing m words, the task is to assign a subset y containing n labels in the label space L to x. Unlike traditional single-label classification where only one label is assigned to each sample, each sample in the MLC task can have multiple labels. From the perspective of sequence generation, the MLC task can be modeled as finding an optimal label sequence y * that maximizes the conditional probability p(y|x), which is calculated as follows:\np(y|x) = n i=1 p(y i |y 1 , y 2 , \u2022 \u2022 \u2022 , y i\u22121 , x) (1)\nAn overview of our proposed model is shown in Figure 1. First, we sort the label sequence of each sample according to the frequency of the labels in the training set. High-frequency labels are placed in the front. In addition, the bos and eos symbols are added to the head and tail of the label sequence, respectively.\nThe text sequence x is encoded to the the hidden states, which are aggregated to a context vector c t by the attention mechanism at time-step t. The decoder takes the context vector c t , the last hidden state s t\u22121 of the decoder and the embedding vector g(y t\u22121 ) as the inputs to produce the hidden state s t at time-step t. Here y t\u22121 is the predicted probability distribution over the label space L at time-step t \u2212 1. The function g takes y t\u22121 as input and produces the embedding vector which is then passed to the decoder. Finally, the masked softmax layer is used to output the probability distribution y t .", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Sequence Generation", "text": "In this subsection, we introduce the details of our proposed model. The whole sequence generation model consists of an encoder and a decoder with the attention mechanism. Encoder: Let (w 1 , w 2 , \u2022 \u2022 \u2022 , w m ) be a sentence with m words and w i is the one-hot representation of the i-th word. We first embed w i to a dense embedding vector x i by an embedding matrix E \u2208 R k\u00d7|V| . Here |V| is the size of the vocabulary, and k is the dimension of the embedding vector.\nWe use a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) to read the text sequence x from both directions and compute the hidden states for each word,\n\u2212 \u2192 h i = \u2212 \u2212\u2212\u2212 \u2192 LSTM( \u2212 \u2192 h i\u22121 , x i ) (2) \u2190 \u2212 h i = \u2190 \u2212\u2212\u2212 \u2212 LSTM( \u2190 \u2212 h i+1 , x i )(3)\nWe obtain the final hidden representation of the i-th word by concatenating the hidden states from both directions,\nh i = [ \u2212 \u2192 h i ; \u2190 \u2212 h i ]\n, which embodies the information of the sequence centered around the i-th word.\nAttention: When the model predicts different labels, not all text words make the same contribution. The attention mechanism produces a context vector by focusing on different portions of the text sequence and aggregating the hidden representations of those informative words. Specially, the attention mechanism assigns the weight \u03b1 ti to the i-th word at time-step t as follows:\ne ti = v T a tanh(W a s t + U a h i )(4)\n\u03b1 ti = exp(e ti ) m j=1 exp(e tj )(5)\nwhere W a , U a , v a are weight parameters and s t is the current hidden state of the decoder at time-step t.\nFor simplicity, all bias terms are omitted in this paper. The final context vector c t which is passed to the decoder at time-step t is calculated as follows:\nc t = m i=1 \u03b1 ti h i(6)\nDecoder: The hidden state s t of the decoder at time-step t is computed as follows:\ns t = LSTM(s t\u22121 , [g(y t\u22121 ); c t\u22121 ])(7)\nwhere [g(y t\u22121 ); c t\u22121 ] means the concatenation of the vectors g(y t\u22121 ) and c t\u22121 . g(y t\u22121 ) is the embedding of the label which has the highest probability under the distribution y t\u22121 . y t\u22121 is the probability distribution over the label space L at time-step t \u2212 1 and is computed as follows:\no t = W o f (W d s t + V d c t ) (8) y t = sof tmax(o t + I t )(9)\nwhere W o , W d , and V d are weight parameters, I t \u2208 R L is the mask vector that is used to prevent the decoder from predicting repeated labels, and f is a nonlinear activation function.\n(I t ) i = \u2212\u221e if the label l i has been predicted at previous t \u2212 1 time steps. 0 otherwise. (10\n)\nAt the training stage, the loss function is the cross-entropy loss function. We employ the beam search algorithm (Wiseman and Rush, 2016) to find the top-ranked prediction path at inference time. The prediction paths ending with the eos are added to the candidate path set.", "publication_ref": ["b10", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "Global Embedding", "text": "In the sequence generation model mentioned above, the embedding vector g(y t\u22121 ) in Equation ( 7) is the embedding of the label that has the highest probability under the distribution y t\u22121 . However, this calculation only takes advantage of the maximum value of y t\u22121 greedily. The proposed sequence generation model generates labels sequentially and predicts the next label conditioned on its previously predicted labels. Therefore, it is likely that we would get a succession of wrong label predictions in the following time steps if the prediction is wrong at time-step t, which is also called exposure bias. To a certain extent, the beam search algorithm alleviates this problem. However, it can not fundamentally solve the problem because the exposure bias phenomenon is likely to occur for all candidate paths. y t\u22121 represents the predicted probability distribution at time-step t \u2212 1, so it is obvious that all information in y t\u22121 is helpful when we predict the current label at time-step t. The exposure bias problem ought to be relieved by considering all informative signals contained in y t\u22121 .\nBased on this motivation, we propose a new decoder structure, where the embedding vector g(y t\u22121 ) at time-step t is capable of representing the overall information at (t\u22121)-th time step. Inspired by the idea of the adaptive gate in highway network (Srivastava et al., 2015), here we introduce our global embedding. Let e denotes the embedding of the label which has the highest probability under the distribution y t\u22121 . e is the weighted average embedding at time t, which is calculated as follows:\ne = L i=1 y (i) t\u22121 e i (11\n)\nwhere y\n(i)\nt\u22121 is the i-th element of y t\u22121 and e i is the embedding vector of the i-th label. Then the proposed global embedding g(y t\u22121 ) passed to the decoder at time-step t is as follows:\ng(y t\u22121 ) = (1 \u2212 H) e + H \u0113 (12\n)\nwhere H is the transform gate controlling the proportion of the weighted average embedding:\nH = W 1 e + W 2\u0113(13)\nwhere W 1 , W 2 \u2208 R L\u00d7L are weight matrices. The global embedding g(y t\u22121 ) is the optimized combination of the original embedding and the weighted average embedding by using transform gate H, which can automatically determine the combination factor in each dimension. y t\u22121 contains the information of all possible labels. By considering the probability of every label, the model is capable of reducing damage caused by mispredictions made in the previous time steps. This enables the model to predict label sequences more accurately.  ", "publication_ref": ["b29"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section, we evaluate our proposed methods on two datasets. We first introduce the datasets, evaluation metrics, experimental details, and all baselines. Then, we compare our methods with the baselines. Finally, we provide the analysis and discussions of experimental results.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Datasets", "text": "Reuters Corpus Volume I (RCV1-V2) 2 : This dataset is provided by Lewis et al. (2004). It consists of over 800,000 manually categorized newswire stories made available by Reuters Ltd for research purposes. Multiple topics can be assigned to each newswire story and there are 103 topics in total.\nArxiv Academic Paper Dataset (AAPD) 3 : We build a new large dataset for the multi-label text classification. We collect the abstract and the corresponding subjects of 55,840 papers in the computer science field from the website 4 . An academic paper may have multiple subjects and there are 54 subjects in total. The target is to predict corresponding subjects of an academic paper according to the content of the abstract.\nWe divide each dataset into training, validation and test sets. The statistics of the two datasets are shown in Table 1.", "publication_ref": ["b15"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Evaluation Metrics", "text": "Following the previous work (Zhang and Zhou, 2007;Chen et al., 2017), we adopt hamming loss and micro-F 1 score as our main evaluation metrics. Micro-precision and micro-recall are also reported to assist the analysis.\n\u2022 Hamming-loss (Schapire and Singer, 1999) evaluates the fraction of misclassified instance-label pairs, where a relevant label is missed or an irrelevant is predicted.\n\u2022 Micro-F 1 (Manning et al., 2008) can be interpreted as a weighted average of the precision and recall. It is calculated globally by counting the total true positives, false negatives, and false positives.", "publication_ref": ["b39", "b4", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Details", "text": "We extract the vocabularies from the training sets. For the RCV1-V2 dataset, the size of the vocabulary is 50,000 and out-of-vocabulary (OOV) words are replaced with unk. Each document is truncated at the length of 500 and the beam size is 5 at the inference stage. Besides, we set the word embedding size to 512. The hidden sizes of the encoder and the decoder are 256 and 512, respectively. The number of LSTM layers of encoder and decoder is 2.\nFor the AAPD dataset, the size of word embedding is 256. There are two LSTM layers in the encoder and its size is 256. For the decoder, there is one LSTM layer of size 512. The size of the vocabulary is 30,000 and OOV words are also replaced with unk. Each document is truncated at the length of 500. The beam size is 9 at the inference stage.\nWe use the Adam (Kingma and Ba, 2014) optimization method to minimize the cross-entropy loss over the training data. For the hyper-parameters of the Adam optimizer, we set the learning rate \u03b1 = 0.001, two momentum parameters \u03b2 1 = 0.9 and \u03b2 2 = 0.999 respectively, and = 1 \u00d7 10 \u22128 . Additionally, Table 2: Comparison between our methods and all baselines on two datasets. GE denotes the global embedding. HL, P, R, and F1 denote hamming loss, micro-precision, micro-recall, and micro-F 1 , respectively. The symbol \"+\" indicates that the higher the value is, the better the model performs. The symbol \"-\" is the opposite.\nwe make use of the dropout regularization (Srivastava et al., 2014) to avoid overfitting and clip the gradients (Pascanu et al., 2013) to the maximum norm of 10.0. During training, we train the model for a fixed number of epochs and monitor its performance on the validation set. Once the training is finished, we select the model with the best micro-F 1 score on the validation set as our final model and evaluate its performance on the test set.", "publication_ref": ["b28", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Baselines", "text": "We compare our proposed methods with the following baselines:\n\u2022 Binary Relevance (BR) (Boutell et al., 2004) transforms the MLC task into multiple single-label classification problems by ignoring the correlations between labels.\n\u2022 Classifier Chains (CC) (Read et al., 2011) transforms the MLC task into a chain of binary classification problems and takes high-order label correlations into consideration.\n\u2022 Label Powerset (LP) (Tsoumakas and Katakis, 2006) transforms a multi-label problem to a multiclass problem with one multi-class classifier trained on all unique label combinations.\n\u2022 CNN (Kim, 2014) uses multiple convolution kernels to extract text features, which are then inputted to the linear transformation layer followed by a sigmoid function to output the probability distribution over the label space. The multi-label soft margin loss is optimized.\n\u2022 CNN-RNN (Chen et al., 2017) utilizes CNN and RNN to capture both the global and local textual semantics and model the label correlations.\nFollowing the previous work (Chen et al., 2017), we adopt the linear SVM as the base classifier in BR, CC and LP. We implement BR, CC and LP by means of Scikit-Multilearn (Szyma\u0144ski, 2017), an opensource library for the MLC task. We tune hyper-parameters of all baseline algorithms on the validation set based on the micro-F 1 score. In addition, training strategies mentioned in Zhang and Wallace (2015) are used to tune hyper-parameters for the baselines CNN and CNN-RNN.", "publication_ref": ["b3", "b23", "b12", "b4", "b4", "b32", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "For the purpose of simplicity, we denote the proposed sequence generation model as SGM. We report the evaluation results of our methods and all baselines on the test sets.\nThe experimental results of our methods and the baselines on dataset RCV1-V2 are shown in Table 2a. Results show that our proposed methods give the best performance in the main evaluation metrics. Our proposed SGM model using global embedding achieves a reduction of 12.79% hamming-loss and an improvement of 2.33% micro-F 1 score over the most commonly used baseline BR. Besides, our methods outperform other traditional deep-learning models by a large margin. For instance, the proposed SGM model with global embedding achieves a reduction of 15.73% hamming-loss and an improvement Figure 2: The performance of the SGM model when using different \u03bb. The red dotted line represents the results of using the adaptive gate. The symbol \"+\" indicates that the higher the value is, the better the model performs. The symbol \"-\" is the opposite. of 2.69% micro-F 1 score over the traditional CNN model. Even without the global embedding, our proposed SGM model is still able to outperform all baselines.\nIn addition, the SGM model is significantly improved by using global embedding. The SGM model with global embedding achieves a reduction of 7.41% hamming loss and an improvement of 1.04% micro-F 1 score on the test set compared with the model without global embedding.\nTable 2b presents the results of the proposed methods and the baselines on the AAPD test set. Similar to the experimental results on the RCV1-V2 test set, our proposed methods still outperform all baselines by a large margin in main evaluation metrics. This further confirms that our methods have significant advantages over previous work on large datasets. Besides, the proposed SGM achieves a reduction of 2.39% hamming loss and an improvement of 1.57% micro-F 1 score on the test set by using global embedding. This further testifies that the global embedding is capable of helping the model to predict label sequences more accurately.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Analysis and Discussion", "text": "Here we perform further analysis on the model and experimental results. We report the evaluation results in terms of hamming loss and micro-F 1 score.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Exploration of Global Embedding", "text": "As is shown in Table 2, global embedding can significantly improve the performance of the model. The global embedding g(y t\u22121 ) at time-step t takes advantage of all information of possible labels contained in y t\u22121 , so it is able to enrich the source information when the model predicts the current label, which leads to the performance of the model significantly improved. The global embedding is the combination of original embedding e and the weighted average embedding\u0113 by using the transform gate H. Here we conduct experiments on the RCV1-V2 dataset to explore how the performance of our model is affected by the proportion between two kinds of embeddings. In the exploratory experiment, the final embedding vector at time-step t is calculated as follows:\ng(y t\u22121 ) = (1 \u2212 \u03bb) * e + \u03bb * \u0113 (14)\nThe proportion between two kinds of embeddings is controlled by coefficient \u03bb. \u03bb = 0 denotes the proposed SGM model without global embedding. The proportion of weighted average embedding increases when we increase \u03bb. The experimental results using different \u03bb values in the decoder are shown in Figure 2.\nAs is shown in Figure 2, the performance of the model varies when different \u03bb is used. Overall, the model using the adaptive gate performs the best, which achieves the best results in both hamming loss and micro-F 1 . The models with \u03bb = 0 outperform the model with \u03bb = 0, which shows that the weighted average embedding contains richer information, leading to the improvement in the performance Table 3: Ablation study on the RCV1-V2 test set. GE denotes the global embedding. HL and F1 denote hamming loss and micro-F 1 , respectively. The symbol \"+\" indicates that the higher the value is, the better the model performs. The symbol \"-\" is the opposite. \u2191 means that the performance of the model improves and \u2193 is the opposite.\nof the model. Without using the adaptive gate, the performance of the model improves at first and then deteriorates as \u03bb increases. It reveals the reason why the model with the adaptive gate performs the best: the adaptive gate can automatically determine the most appropriate \u03bb value according to the actual condition.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Impact of Mask and Sorting", "text": "Our proposed methods are developed based on traditional Seq2Seq models. However, the mask module is added to the proposed methods, which is used to prevent the models from predicting repeated labels.\nIn addition, we sort the label sequence of each sample according to the frequency of appearance of labels in the training set. In order to explore the impact of the mask module and sorting, we conduct ablation experiments on the RCV1-V2 dataset. The experimental results are shown in Table 3. \"w/o mask\" means that we do not perform mask operation and \"w/o sorting\" means that we randomly shuffle the label sequence in order to perturb its original order.\nAs is shown in Table 3, the performance decline of the SGM model with global embedding is more significant compared with that of the SGM model without global embedding. In addition, the decline in the performance of the two models is more significant when we randomly shuffle the label sequence of the sample compared with removing mask module. The label cardinality of the RCV1-V2 dataset is small, so our proposed methods are less prone to predicting repeated labels. This explains the reason why experimental results indicate that the mask module has little impact on the models' performance. In addition, the proposed models are trained using the maximum likelihood estimation method and the cross-entropy loss function, which requires humans to predefine the order of the output labels. Therefore, the sorting of labels is very important for the models' performance. Besides, the performance of both models declines when we do not use the mask module. This shows that the performance of the model can be improved by using the mask operation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Error Analysis", "text": "In the experiment, we find that the performance of all methods deteriorates when the length of the label sequence increases (for simplicity, we denote the length of the label sequence as LLS). In order to explore the influence of the value of the LLS, we divide the test set into different subsets based on different LLS. Figure 3 shows the performance of the SGM model and the most commonly used baseline BR on different subsets of the RCV1-V2 test set. As is shown in Figure 3, generally, the performance of both models deteriorates as the LLS increases. This shows that when the label sequence of the sample is particularly long, it is difficult to accurately predict all labels. Because more information is needed when the model predicts more labels. It is easy to ignore some true labels whose feature information is insufficient.\nHowever, as is shown in Figure 3, the proposed SGM model outperforms BR with any value of LLS, and the advantages of our model are more significant when LLS is large. The traditional BR method predicts all labels at once only based on the sample input. Therefore, it tends to ignore some true labels whose feature information contained in the sample is insufficient. The SGM model generates labels sequentially, and predicts the next label based on its previously predicted labels. Therefore, even if the sample contains less information of some true labels, the SGM model is capable of generating these true labels by considering relevant labels that have been predicted.\n\u2022 Generating descriptions for videos has many applications including human robot interaction.\n\u2022 Many methods for image captioning rely on pretrained object classifier CNN and Long Short Term Memory recurrent networks.\n\u2022 How to learn robust visual classifiers from the weak annotations of the sentence descriptions.\n(a) Visual analysis when the SGM model predicts \"CV\".\n\u2022 Generating descriptions for videos has many applications including human robot interaction.\n\u2022 Many methods for image captioning rely on pretrained object classifier CNN and Long Short Term Memory recurrent networks.\n\u2022 How to learn robust visual classifiers from the weak annotations of the sentence descriptions.\n(b) Visual analysis when the SGM model predicts \"CL\". Table 4: An example abstract in the AAPD dataset, from which we extract three informative sentences. This abstract is assigned two labels: \"CV\" and \"CL\". They denote computer vision and computational language, respectively.\nTable 5: Several examples of the generated label sequences on the RCV1-V2 dataset. The red bold labels in each example indicate that they are highly correlated.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "Visualization of Attention", "text": "When the model predicts different labels, there exist differences in the contributions of different words. The SGM model is able to select the most informative words by utilizing the attention mechanism. The visualization of the attention layer is shown in Table 4. According to Table 4, when the SGM model predicts the label \"CV\", it can automatically assign larger weights to more informative words, like image, visual, captioning, and so on. For the label \"CL\", the selected informative words are sentence, memory, recurrent, etc. This shows that our proposed models are able to consider the differences in the contributions of textual content when predicting different labels and select the most informative words automatically.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Case Study", "text": "We give several examples of the generated label sequences on the RCV1-V2 dataset in Table 5, where we compare the proposed methods with the most commonly used baseline BR. The red bold labels in each example indicate that they are highly correlated. For instance, the correlation coefficient between E51 and E512 is 0.7664. Therefore, these highly correlated labels are likely to appear together in the predicted label sequence. The BR algorithm fails to capture this label correlation, leaving many true labels unpredicted. However, our proposed methods accurately predict almost all highly correlated true labels. The proposed SGM captures the correlations between labels by utilizing LSTM to generate labels sequentially. Therefore, for some true labels whose feature information is insufficient, the proposed SGM is still able to generate them by considering relevant labels that have been predicted. In addition, label sequences that are more accurate are predicted by using global embedding. The SGM model with global embedding predicts more true labels compared with the SGM model without global embedding. The reason is that the source information is further enriched by incorporating overall informative signals in the probability distribution y t\u22121 when the model predicts the label at time-step t. Enriched information makes global embedding more smooth, which enables the model to reduce damage caused by mispredictions made in the previous time steps.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "The MLC task studies the problem where multiple labels are assigned to each sample. There are four main types of methods for the MLC task: problem transformation methods, algorithm adaptation methods, ensemble methods, and neural network models.\nProblem transformation methods map the MLC task into multiple single-label learning tasks. Binary relevance (BR) (Boutell et al., 2004) decomposes the MLC task into independent binary classification problems by ignoring the correlations between labels. In order to model label correlations, label powerset (LP) (Tsoumakas and Katakis, 2006) transforms a multi-label problem to a multi-class problem with a classifier trained on all unique label combinations. Classifier chains (CC) (Read et al., 2011) transforms the MLC task into a chain of binary classification problems, where subsequent binary classifiers in the chain are built upon the predictions of preceding ones. However, the computational efficiency and performance of these methods are challenged by applications with a large number of labels and samples.\nAlgorithm adaptation methods extend specific learning algorithms to handle multi-label data directly. Clare and King (2001) construct decision tree based on multi-label entropy to perform classification. Elisseeff and Weston (2002) optimize the empirical ranking loss by using maximum margin strategy and kernel tricks. Collective multi-label classifier (CML) (Ghamrawi and McCallum, 2005) adopts maximum entropy principle to deal with multi-label data by encoding label correlations as constraint conditions. Zhang and Zhou (2007) adopt k-nearest neighbor techniques to deal with multi-label data. F\u00fcrnkranz et al. (2008) make ranking among labels by utilizing pairwise comparison. Li et al. (2015) propose a novel joint learning algorithm that allows the feedbacks to be propagated from the classifiers for latter labels to the classifier for the current label. Most methods, however, can only be used to capture the first or second order label correlations or are computationally intractable in considering high-order label correlations.\nAmong ensemble methods, Tsoumakas et al. (2011) break the initial set of labels into a number of small random subsets and employ the LP algorithm to train a corresponding classifier. Szyma\u0144ski et al. (2016) propose to construct a label co-occurrence graph and perform community detection to partition the label set.\nIn recent years, some neural network models have also been used for the MLC task. Zhang and Zhou (2006) propose the BP-MLL that utilizes a fully-connected neural network and a pairwise ranking loss function. Nam et al. (2013) propose a neural network using cross-entropy loss instead of ranking loss. Benites and Sapozhnikova (2015) increase classification speed by adding an extra ART layer for clustering. Kurata et al. (2016) utilize word embeddings based on CNN to capture label correlations. Chen et al. (2017) propose to represent semantic information of text and model high-order label correlations by combining CNN with RNN. Baker and Korhonen (2017) initialize the final hidden layer with rows that map to co-occurrence of labels based on the CNN architecture to improve the performance of the model.  propose to use the multi-label classification algorithm for machine translation to handle the situation where a sentence can be translated into more than one correct sentences.", "publication_ref": ["b3", "b23", "b5", "b6", "b8", "b39", "b7", "b16", "b31", "b38", "b21", "b2", "b14", "b4", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions and Future Work", "text": "In this paper, we propose to view the multi-label classification task as a sequence generation problem to model the correlations between labels. A sequence generation model with a novel decoder structure is proposed to improve the performance of classification. Extensive experimental results show that the proposed methods outperform the baselines by a substantial margin. Further analysis of experimental results demonstrates that our proposed methods not only capture the correlations between labels, but also select the most informative words automatically when predicting different labels.\nAs analyzed in Section 3.6.3, when a large number of labels are assigned to a sample, how to predict all these true labels accurately is an intractable problem. Our proposed methods alleviate this problem to some extent, but more effective solutions need to be further explored in the future.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This work is supported in part by National Natural Science Foundation of China (No. 61673028, No. 61333018) and the National Thousand Young Talents Program. Xu Sun is the corresponding author of this paper.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Neural machine translation by jointly learning to align and translate", "journal": "CoRR", "year": "2014", "authors": "Dzmitry Bahdanau; Kyunghyun Cho; Yoshua Bengio"}, {"ref_id": "b1", "title": "Initializing neural networks for hierarchical multi-label text classification", "journal": "", "year": "2017", "authors": "Simon Baker; Anna Korhonen"}, {"ref_id": "b2", "title": "Haram: a hierarchical aram neural network for large-scale text classification", "journal": "IEEE", "year": "2015", "authors": "Fernando Benites; Elena Sapozhnikova"}, {"ref_id": "b3", "title": "Learning multi-label scene classification", "journal": "Pattern Recognition", "year": "2004", "authors": "Matthew R Boutell; Jiebo Luo; Xipeng Shen; Christopher M Brown"}, {"ref_id": "b4", "title": "Ensemble application of convolutional and recurrent neural networks for multi-label text categorization", "journal": "", "year": "2017-05-14", "authors": "Guibin Chen; Deheng Ye; Zhenchang Xing; Jieshan Chen; Erik Cambria"}, {"ref_id": "b5", "title": "Knowledge discovery in multi-label phenotype data", "journal": "Springer", "year": "2001", "authors": "Amanda Clare; D Ross;  King"}, {"ref_id": "b6", "title": "A kernel method for multi-labelled classification", "journal": "", "year": "2002", "authors": "Andr\u00e9 Elisseeff; Jason Weston"}, {"ref_id": "b7", "title": "Multilabel classification via calibrated label ranking", "journal": "Machine learning", "year": "2008", "authors": "Johannes F\u00fcrnkranz; Eyke H\u00fcllermeier; Eneldo Loza Menc\u00eda; Klaus Brinker"}, {"ref_id": "b8", "title": "Collective multi-label classification", "journal": "ACM", "year": "2005", "authors": "Nadia Ghamrawi; Andrew Mccallum"}, {"ref_id": "b9", "title": "Multilabel classification with meta-level features", "journal": "ACM", "year": "2010", "authors": "Siddharth Gopal; Yiming Yang"}, {"ref_id": "b10", "title": "Long short-term memory", "journal": "Neural computation", "year": "1997", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"ref_id": "b11", "title": "Multilabel text classification for automated tag suggestion", "journal": "", "year": "2008", "authors": "Ioannis Katakis"}, {"ref_id": "b12", "title": "Convolutional neural networks for sentence classification", "journal": "", "year": "2014-10-25", "authors": "Yoon Kim"}, {"ref_id": "b13", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b14", "title": "Improved neural network-based multi-label classification with better initialization leveraging label co-occurrence", "journal": "", "year": "2016-06-12", "authors": "Gakuto Kurata; Bing Xiang; Bowen Zhou"}, {"ref_id": "b15", "title": "RCV1: A new benchmark collection for text categorization research", "journal": "Journal of Machine Learning Research", "year": "2004", "authors": "David D Lewis; Yiming Yang; Tony G Rose; Fan Li"}, {"ref_id": "b16", "title": "Multi-label text categorization with joint learning predictions-as-features method", "journal": "", "year": "2015", "authors": "Li Li; Houfeng Wang; Xu Sun; Baobao Chang; Shi Zhao; Lei Sha"}, {"ref_id": "b17", "title": "Global encoding for abstractive summarization", "journal": "", "year": "2018", "authors": "Junyang Lin; Xu Sun; Shuming Ma; Qi Su"}, {"ref_id": "b18", "title": "Effective approaches to attention-based neural machine translation", "journal": "", "year": "2015", "authors": "Minh-Thang Luong; Hieu Pham; Christopher D Manning"}, {"ref_id": "b19", "title": "Bag-of-words as target for neural machine translation", "journal": "", "year": "2018", "authors": "Shuming Ma; Xu Sun; Yizhong Wang; Junyang Lin"}, {"ref_id": "b20", "title": "Introduction to information retrieval", "journal": "Cambridge university press", "year": "2008", "authors": "D Christopher; Prabhakar Manning; Hinrich Raghavan;  Sch\u00fctze"}, {"ref_id": "b21", "title": "Large-scale multi-label text classification -revisiting neural networks", "journal": "CoRR", "year": "2013", "authors": "Jinseok Nam; Jungi Kim; Iryna Gurevych; Johannes F\u00fcrnkranz"}, {"ref_id": "b22", "title": "On the difficulty of training recurrent neural networks", "journal": "", "year": "2013", "authors": "Razvan Pascanu; Tomas Mikolov; Yoshua Bengio"}, {"ref_id": "b23", "title": "Classifier chains for multi-label classification", "journal": "Machine learning", "year": "2011", "authors": "Jesse Read; Bernhard Pfahringer; Geoff Holmes; Eibe Frank"}, {"ref_id": "b24", "title": "A neural attention model for abstractive sentence summarization", "journal": "", "year": "2015-09-17", "authors": "Alexander M Rush; Sumit Chopra; Jason Weston"}, {"ref_id": "b25", "title": "Improved boosting algorithms using confidence-rated predictions", "journal": "Machine learning", "year": "1999", "authors": "E Robert; Yoram Schapire;  Singer"}, {"ref_id": "b26", "title": "Boostexter: A boosting-based system for text categorization", "journal": "Machine learning", "year": "2000", "authors": "E Robert; Yoram Schapire;  Singer"}, {"ref_id": "b27", "title": "Style transfer from non-parallel text by cross-alignment", "journal": "CoRR", "year": "2017", "authors": "Tianxiao Shen; Tao Lei; Regina Barzilay; Tommi S Jaakkola"}, {"ref_id": "b28", "title": "Dropout: a simple way to prevent neural networks from overfitting", "journal": "Journal of Machine Learning Research", "year": "2014", "authors": "Nitish Srivastava; Geoffrey E Hinton; Alex Krizhevsky; Ilya Sutskever; Ruslan Salakhutdinov"}, {"ref_id": "b29", "title": "", "journal": "", "year": "2015", "authors": "Klaus Rupesh Kumar Srivastava; J\u00fcrgen Greff;  Schmidhuber"}, {"ref_id": "b30", "title": "Label embedding network: Learning label representation for soft training of deep networks", "journal": "CoRR", "year": "2017", "authors": "Xu Sun; Bingzhen Wei; Xuancheng Ren; Shuming Ma"}, {"ref_id": "b31", "title": "How is a data-driven approach better than random choice in label space division for multi-label classification?", "journal": "Entropy", "year": "2016", "authors": "Piotr Szyma\u0144ski; Tomasz Kajdanowicz; Kristian Kersting"}, {"ref_id": "b32", "title": "A scikit-based python environment for performing multi-label classification", "journal": "", "year": "2017", "authors": "Piotr Szyma\u0144ski"}, {"ref_id": "b33", "title": "Grigorios Tsoumakas and Ioannis Katakis", "journal": "International Journal of Data Warehousing and Mining", "year": "2006", "authors": ""}, {"ref_id": "b34", "title": "Random k-labelsets for multilabel classification", "journal": "", "year": "2011", "authors": ""}, {"ref_id": "b35", "title": "Sequence-to-sequence learning as beam-search optimization", "journal": "", "year": "2016", "authors": "Sam Wiseman; Alexander M Rush"}, {"ref_id": "b36", "title": "Unpaired sentiment-to-sentiment translation: A cycled reinforcement learning approach", "journal": "", "year": "2018", "authors": "Jingjing Xu; Xu Sun; Qi Zeng; Xuancheng Ren; Xiaodong Zhang; Houfeng Wang; Wenjie Li"}, {"ref_id": "b37", "title": "A sensitivity analysis of (and practitioners' guide to) convolutional neural networks for sentence classification", "journal": "CoRR", "year": "2015", "authors": "Ye Zhang; Byron C Wallace"}, {"ref_id": "b38", "title": "Multilabel neural networks with applications to functional genomics and text categorization", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2006", "authors": "Min-Ling Zhang; Zhi-Hua Zhou"}, {"ref_id": "b39", "title": "ML-KNN: A lazy learning approach to multi-label learning", "journal": "Pattern recognition", "year": "2007", "authors": "Min-Ling Zhang; Zhi-Hua Zhou"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: The overview of our proposed model. MS denotes the masked softmax layer. GE denotes the global embedding.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: The performance of the SGM model on different subsets of the RCV1-V2 test set. LLS represents the length of label sequence of each sample in the subset. The explanations of symbol \"+\" and \"-\" can be found in Figure 2.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Summary of datasets. Total Samples, Label Sets denote the total number of samples and labels, respectively. Words/Sample is the average number of words per sample and Labels/Sample is the average number of labels per sample.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Ablation study for SGM model with global embedding.", "figure_data": "ModelsHL(-)F1(+)ModelsHL(-)F1(+)SGM0.00810.869SGM + GE0.00750.878w/o mask0.0083(\u2193 2.47%)0.866(\u2193 0.35%)w/o mask0.0078(\u2193 4.00%)0.873(\u2193 0.57%)w/o sorting0.0084(\u2193 3.70%)0.858(\u2193 1.27%)w/o sorting0.0083(\u2193 10.67%)0.859(\u2193 2.16%)(a) Ablation study for the SGM model.(b)"}], "formulas": [{"formula_id": "formula_0", "formula_text": "p(y|x) = n i=1 p(y i |y 1 , y 2 , \u2022 \u2022 \u2022 , y i\u22121 , x) (1)", "formula_coordinates": [2.0, 213.91, 534.5, 311.64, 33.71]}, {"formula_id": "formula_1", "formula_text": "\u2212 \u2192 h i = \u2212 \u2212\u2212\u2212 \u2192 LSTM( \u2212 \u2192 h i\u22121 , x i ) (2) \u2190 \u2212 h i = \u2190 \u2212\u2212\u2212 \u2212 LSTM( \u2190 \u2212 h i+1 , x i )(3)", "formula_coordinates": [3.0, 244.46, 377.34, 281.08, 37.84]}, {"formula_id": "formula_2", "formula_text": "h i = [ \u2212 \u2192 h i ; \u2190 \u2212 h i ]", "formula_coordinates": [3.0, 144.26, 437.79, 67.28, 18.21]}, {"formula_id": "formula_3", "formula_text": "e ti = v T a tanh(W a s t + U a h i )(4)", "formula_coordinates": [3.0, 232.04, 536.49, 293.51, 14.19]}, {"formula_id": "formula_4", "formula_text": "\u03b1 ti = exp(e ti ) m j=1 exp(e tj )(5)", "formula_coordinates": [3.0, 230.14, 555.22, 295.41, 27.09]}, {"formula_id": "formula_5", "formula_text": "c t = m i=1 \u03b1 ti h i(6)", "formula_coordinates": [3.0, 266.09, 644.31, 259.45, 33.71]}, {"formula_id": "formula_6", "formula_text": "s t = LSTM(s t\u22121 , [g(y t\u22121 ); c t\u22121 ])(7)", "formula_coordinates": [3.0, 221.8, 716.39, 303.74, 10.67]}, {"formula_id": "formula_7", "formula_text": "o t = W o f (W d s t + V d c t ) (8) y t = sof tmax(o t + I t )(9)", "formula_coordinates": [4.0, 240.31, 92.54, 285.24, 27.2]}, {"formula_id": "formula_8", "formula_text": "(I t ) i = \u2212\u221e if the label l i has been predicted at previous t \u2212 1 time steps. 0 otherwise. (10", "formula_coordinates": [4.0, 128.15, 176.46, 392.85, 26.07]}, {"formula_id": "formula_9", "formula_text": ")", "formula_coordinates": [4.0, 521.0, 184.41, 4.54, 9.46]}, {"formula_id": "formula_10", "formula_text": "e = L i=1 y (i) t\u22121 e i (11", "formula_coordinates": [4.0, 265.12, 517.97, 255.88, 33.71]}, {"formula_id": "formula_11", "formula_text": ")", "formula_coordinates": [4.0, 521.0, 529.95, 4.54, 9.46]}, {"formula_id": "formula_12", "formula_text": "(i)", "formula_coordinates": [4.0, 106.69, 565.47, 9.47, 6.99]}, {"formula_id": "formula_13", "formula_text": "g(y t\u22121 ) = (1 \u2212 H) e + H \u0113 (12", "formula_coordinates": [4.0, 222.35, 608.98, 298.65, 10.67]}, {"formula_id": "formula_14", "formula_text": ")", "formula_coordinates": [4.0, 521.0, 609.36, 4.54, 9.46]}, {"formula_id": "formula_15", "formula_text": "H = W 1 e + W 2\u0113(13)", "formula_coordinates": [4.0, 256.33, 661.48, 269.21, 10.67]}, {"formula_id": "formula_16", "formula_text": "g(y t\u22121 ) = (1 \u2212 \u03bb) * e + \u03bb * \u0113 (14)", "formula_coordinates": [7.0, 231.04, 635.3, 294.5, 10.67]}], "doi": ""}