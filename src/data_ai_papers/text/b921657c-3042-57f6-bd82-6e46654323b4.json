{"title": "Efficient SimRank Computation via Linearization 1", "authors": "Takanori Maehara; Mitsuru Kusumoto; Ken-Ichi Kawarabayashi", "pub_date": "2014-11-26", "abstract": "SimRank, proposed by Jeh and Widom, provides a good similarity measure that has been successfully used in numerous applications. While there are many algorithms proposed for computing SimRank, their computational costs are very high. In this paper, we propose a new computational technique, \"SimRank linearization,\" for computing SimRank, which converts the SimRank problem to a linear equation problem. By using this technique, we can solve many SimRank problems, such as single-pair compuation, single-source computation, all-pairs computation, top k searching, and similarity join problems, efficiently.", "sections": [{"heading": "INTRODUCTION", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Background and motivation", "text": "Very large-scale networks are ubiquitous in today's world, and designing scalable algorithms for such huge network has become a pertinent problem in all aspects of compute science. The primary problem is the vast size of modern graph datasets. For example, the World Wide Web currently consists of over one trillion links and is expected to exceed tens of trillions in the near future, and Facebook embraces over 800 million active users, with hundreds of billions of friend links.\nLarge graphs arise in numerous applications where both the basic entities and the relationships between these entities are given. A graph stores the objects of the data on its vertices, and represents the relations among these objects by its edges. For example, the vertices and edges of the World Wide Web graph correspond to the webpages and hyperlinks, respectively. Another typical graph is a social network, whose vertices and edges correspond to personal information and friendship relations, respectively.\nWith the rapidly increasing amount of graph data, the similarity search problem, which identifies similar vertices in a graph, has become an important problem with many applications, including web analysis [Jeh and Widom 2002;Liben-Nowell and Kleinberg 2007], graph clustering [Yin et al. 2006;Zhou et al. 2009], spam detection [Gy\u00f6ngyi et al. 2004], computational advertisement [Antonellis et al. 2008], recommender systems [Abbassi and Mirrokni 2007;, and natural language processing [Scheible 2010].\nSeveral similarity measures have been proposed. For example, bibliographic coupling [Kessler 1963], co-citation [Small 1973], P-Rank [Zhao et al. 2009], PageSim [Lin et al. 2006], Extended Nearest Neighborhood Structure [Lin et al. 2007], Match-Sim , and so on. In this paper, we consider SimRank, a link-based similarity measure proposed by Jeh and Widom [Jeh and Widom 2002] for searching web pages. SimRank supposes that \"two similar pages are linked from many similar pages.\" This intuitive concept is formulated by the following recursive definition: For 1 2 3 4 5 6 7\ni j s(i, j) 1 2 0.260 1 3 0.142 1 4 0.120 1 5 0.162 1 6 0.069 1 7 0.219 2 3 0.121 i j s(i, j) 2 4 0.141 2 5 0.132 2 6 0.069 2 7 0.226 3 4 0.128 3 5 0.230 3 6 0.236 i j s(i, j) 3 7 0.101 4 5 0.107 4 6 0.080 4 7 0.125 5 6 0.271 5 7 0.110 6 7 0.061 a graph G = (V, E), the SimRank score s(i, j) of a pair of vertices (i, j) \u2208 V \u00d7 V is recursively defined by\ns(i, j) := \uf8f1 \uf8f2 \uf8f3 1, i = j, c |\u03b4(i)||\u03b4(j)| i \u2032 \u2208\u03b4(i),j\u2208\u03b4(j) s(i \u2032 , j \u2032 ), i = j, (1.1)\nwhere \u03b4(i) = {j \u2208 V : (j, i) \u2208 E} is the set of in-neighbors of i, and c \u2208 (0, 1) is a decay factor usually set to c = 0.8 [Jeh and Widom 2002] or c = 0.6 [Lizorkin et al. 2010]. See Figure 1.1 for an example of the SimRank on a small graph. SimRank can be regarded as a label propagation [Zhu and Ghahramani 2002] on the squared graph. Let us consider a squared graph G 2 = (V (2) , E (2) ) whose vertices are the pair of vertices V (2) = V \u00d7 V and edges are defined by E (2) = {((i, j), (i \u2032 , j \u2032 )) : (i, i \u2032 ), (j, j \u2032 ) \u2208 E}.\n(1.2)\nThen the SimRank is a label propagation method with trivial relations s(i, i) = 1 (i.e., i is similar to i) for all i \u2208 V on G (2) . SimRank also has a \"random-walk\" interpretation. Let us consider two random walks that start from vertices i and j, respectively, and follow the in-links. Let i (t) and j (t) be the t-th position of each random walk, respectively. The first meeting time \u03c4 i,j is defined by \u03c4 ij = min{t : i (t) = j (t) }.\n(1.3)\nThen SimRank score is obtained by s(i, j) = E[c \u03c4i,j ].\n(1.4)\nSimRank and its related measures (e.g., SimRank++ [Antonellis et al. 2008], S-SimRank [Cai et al. 2008], P-Rank [Zhao et al. 2009], and SimRank * [Yu et al. 2013]) give high-quality scores in activities such as natural language processing [Scheible 2010], computational advertisement [Antonellis et al. 2008], collaborative filtering , and web analysis [Jeh and Widom 2002]. As implied in its definition, SimRank exploits the information in multihop neighborhoods. In contrast, most other similarity measures utilize only the one-step neighborhoods. Consequently, SimRank is more effective than other similarity measures in real applications.\nAlthough SimRank is naturally defined and gives high-quality similarity measure, it is not so widely used in practice, due to high computational cost. While there are several algorithms proposed so far to compute SimRank scores, unfortunately, their computation costs (in both time and space) are very expensive. The difficulty of computing SimRank may be viewed as follows: to compute a SimRank score s(u, v) for two vertices u, v, since (1.1) is defined recursively, we have to compute SimRank scores for all O(n 2 ) pairs of vertices. Therefore it requires O(n 2 ) space and O(n 2 ) time, where n is the number of vertices. In order to reduce this computation cost, several approaches have been proposed. We review these approaches in the following subsection.", "publication_ref": ["b18", "b25", "b42", "b49", "b15", "b2", "b1", "b32", "b19", "b34", "b47", "b26", "b27", "b18", "b18", "b29", "b50", "b2", "b7", "b47", "b45", "b32", "b2", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "In order to reduce this computation cost, several approaches have been proposed [Fogaras and R\u00e1cz 2005;He et al. 2010;Li et al. 2010a;Li et al. 2010b;Lizorkin et al. 2010;Yu et al. 2013;Yu et al. 2012]. Here, we briefly survey some existing computational techniques for SimRank. We summarize the existing results in Table I. Let us point out that there are three fundamental problems for SimRank: (1) single-pair SimRank to compute s(u, v) for given two vertices u and v, (2) single-source SimRank to compute s(u, v) for a given vertex u and all other vertices v, and (3) allpairs SimRank to compute s(u, v) for all pair of vertices u and v.\nIn the original paper by Jeh and Widom [Jeh and Widom 2002], all-pairs SimRank scores are computed by recursively evaluating the equation (1.1) for all u, v \u2208 V . This \"naive\" computation yields an O(T d 2 n 2 ) time algorithm, where T denotes the number of iterations and d denotes the average degree of a given network. Lizorkin et al. [Lizorkin et al. 2010] proposed a \"partial sum\" technique, which memorizes partial calculations of Jeh and Widom's algorithm to reduce the time complexity of their algorithm. This leads to an O(T min{nm, n 3 / log n}) algorithm. Yu et al. [Yu et al. 2012] applied the fast matrix multiplication [Strassen 1969;Williams 2012] and then obtained an O(T min{nm, n \u03c9 }) algorithm to compute all pairs SimRank scores, where \u03c9 < 2.373 is the exponent of matrix multiplication. Note that the space complexity of these algorithms is O(n 2 ), since they have to maintain all SimRank scores for each pair of vertices to evaluate the equation (1.1). This results is, so far, the state-of-the-art algorithm to compute SimRank scores for all pairs of vertices.\nThere are some algorithms based on a random-walk interpretation (1.4). Fogaras and R\u00e1cz [Fogaras and R\u00e1cz 2005] evaluate the right-hand side by Monte-Carlo simulation with a fingerprint tree data structure, and they obtained a faster algorithm to compute single pair SimRank score for given two vertices i, j. Li et al. [Li et al. 2010b] also proposed an algorithm based on the random-walk iterpretation; however their algorithm is an iterative algorithm to compute the first meeting time and computes all-pairs SimRank deterministically. Some papers proposed spectral decomposition based algorithms (e.g., [Fujiwara et al. 2013;He et al. 2010;Li et al. 2010a;Yu et al. 2013]), but there is a mistake in the formulation of SimRank. On the other hand, their algorithms may output reasonable results. We shall mention more details about these algorithms in Remark 2.2.", "publication_ref": ["b12", "b16", "b22", "b24", "b29", "b45", "b46", "b18", "b29", "b46", "b37", "b40", "b12", "b24", "b13", "b16", "b22", "b45"], "figure_ref": [], "table_ref": []}, {"heading": "Contribution", "text": "In this paper, we propose a novel computational technique for SimRank, called Sim-Rank linearlization. This technique allows us to solve many kinds of SimRank problems such as the following:\nSingle-pair SimRank. We are given two vertices i, j \u2208 V , compute SimRank score s(i, j). Single-source SimRank. We are given a vertex i \u2208 V , compute SimRank scores s(i, j) for all j \u2208 V . All-pairs SimRank. Compute SimRank scores s(i, j) for all i, j \u2208 V . Top k SimRank search. We are given a vertex i \u2208 V , return k vertices j with k highest SimRank scores. SimRank join. We are given a threshold \u03b4, return all pairs (i, j) such that s(i, j) \u2265 \u03b4.\nFor all problems, the proposed algorithm outperforms the existing methods.\nTable I. Complexity of SimRank algorithms. n denotes the number of vertices, m denotes the number of edges, d denotes the average degree, T denotes the number of iterations, R is the number of Monte-Carlo samples, and r denotes the rank for low-rank approximation. Note that * -marked method is based on an incorect formula: see Remark 2.2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm", "text": "Type Time Space Technique Proposed (Section 3.5)\nSingle-pair O(T m) O(m) Linearization Proposed (Section 3.5) Single-source O(T 2 m) O(m) Linearization Proposed (Section 3.5) All-pairs O(T 2 nm) O(m) Linearization Proposed (Section 3.5) Top-k search \u226a O(T R) O(m) Linearization & Monte Carlo Proposed (Section 3.5) Join \u2248 O(output) O(m + output) Linearization & Gauss-Southwell [Li et al. 2010b] Single-pair O(T d 2 n 2 ) O(n 2 )\nRandom surfer pair (Iterative) [Fogaras and R\u00e1cz 2005] \nSingle-pair O(T R) O(m + nR)\nRandom surfer pair (Monte Carlo) [Jeh and Widom 2002] All-pairs Lizorkin et al. 2010] All-pairs O(T min{nm, n 3 / log n}) O(n 2 ) Partial sum [Yu et al. 2012] All-pairs O(min{nm, n \u03c9 }) O(n 2 ) Fast matrix multiplication [Li et al. 2009] All-pairs O( 4/3 ) O(n 4/3 ) Block partition [Li et al. 2010a] All-pairs O(r 4 n 2 ) O(n 2 ) Singular value decomposition * [Fujiwara et al. 2013] All-pairs\nO(T n 2 d 2 ) O(n 2 ) Naive [\nO(r 4 n) O(r 2 n 2 ) Singular value decomposition * [Yu et al. 2010] All-pairs O(n 3 ) O(n 2 ) Eigenvalue decomposition * Table II. List of symbols symbol description G directed unweighted graph, G = (V, E) V set of vertices E set of edges n number of vertices, n = |V | m number of edges, m = |E| i, j vertex e edge \u03b4(i) in-neighbors of of i, \u03b4(i) = {j \u2208 V : (j, i) \u2208 E} P transition matrix, P ij = 1/|\u03b4(j)| for (i, j) \u2208 E s(i, j)\nSimRank of i and j S SimRank matrix, S ij = s(i, j) D diagonal correction matrix, S = cP \u22a4 SP + D", "publication_ref": ["b12", "b18", "b29", "b46", "b23", "b22", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Organization", "text": "The paper consists of four parts. In Section 2, we introduce the SimRank linearization technique and show that how to use the linearization to solve single-pair, single-source, and all-pairs problem. In Section 3, we describe how to solve top k SimRank search problem. In Section 4, we describe how to solve SimRank join problem. Each section contains computational experiments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "LINEARIZED SIMRANK", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Concept of linearized SimRank", "text": "Let us first observe the difficulty in computing SimRank. Let G = (V, E) be a directed graph, and let P = (P ij ) be a transition matrix of transpose graph G \u22a4 defined by\nP ij := 1/|\u03b4(j)|, (i, j) \u2208 E, 0, (i, j) \u2208 E,\nwhere\n\u03b4(i) = {j \u2208 V : (j, i) \u2208 E} denotes the in-neighbors of i \u2208 V . Let S = (s(i, j\n)) be the SimRank matrix, whose (i, j) entry is the SimRank score of i and j. Then the SimRank equation (1.1) is represented [Yu et al. 2012] by:\nS = (cP \u22a4 SP ) \u2228 I, (2.1)\nwhere I is the identity matrix, and \u2228 denotes the element-wise maximum, i.e., (i, j) entry of the matrix A \u2228 B is given by max{A ij , B ij }.\nIn our view, the difficulty in computing SimRank via equation (2.1) comes from the element-wise maximum, which is a non-linear operation. To avoid the element-wise maximum, we introduce a new formulation of SimRank as follows. By observing (2.1), since S and cP \u22a4 SP only differ in their diagonal elements, there exists a diagonal matrix D such that S = cP \u22a4 SP + D.\n(2.2)\nWe call such a matrix D the diagonal correction matrix. The main idea of our approach here is to split a SimRank problem into the following two subproblems:\n(1) Estimate diagonal correction matrix D.\n(2) Solve the SimRank problem using D and the linear recurrence equation (2.2).\nFor efficient computation, we must estimate D without computing the whole part of S.\nTo simplify the discussion, we introduce the notion of linearized SimRank. Let \u0398 be an n \u00d7 n matrix. A linearized SimRank S L (\u0398) is a matrix that satisfies the following linear recurrence equation:\nS L (\u0398) = cP \u22a4 S L (\u0398)P + \u0398.\n(2.3) Below, we provide an example that illustrates what linearized SimRank is.\nExample 2.1 (Star graph of order 4). Let G be a star graph of order 4 (i.e., G has one vertex of degree three and three vertices of degree one). The transition matrix (of the transposed graph) is Thus, the diagonal correction matrix D is obtained by D = S \u2212 cP \u22a4 SP = diag(23/75, 1/5, 1/5, 1/5), Remark 2.2. Some papers have used the following formula for SimRank (e.g., equation (2) in [Fujiwara et al. 2013], equation (2) in [He et al. 2010], equation (2) in [Li et al. 2010a], and equation (3) in [Yu et al. 2013]):\nP = \uf8ee \uf8ef \uf8f0 0 1 1 1 1/3 0 0 0 1/3 0 0 0 1/3 0 0 0 \uf8f9 \uf8fa \uf8fb ,\nS = cP \u22a4 SP + (1 \u2212 c)I.\n(2.4) However, this formula does not hold; (2.4) requires diagonal correction matrix D to have the same diagonal entries, but Example 2.1 is a counterexample. In fact, matrix S defined by (2.4) is a linearized SimRank S L (\u0398) for a matrix \u0398 = (1 \u2212 c)I.\nWe provide some basic properties of linearized SimRank in Appendix.", "publication_ref": ["b46", "b13", "b16", "b22", "b45"], "figure_ref": [], "table_ref": []}, {"heading": "Solving SimRank problems via linearization", "text": "In this section, we present our proposed algorithms for SimRank by assuming that the diagonal correction matrix D has already been obtained. All algorithms are based on Algorithm 1 Single-pair SimRank 1: procedure SINGLEPAIRSIMRANK(i,j) 2:\n\u03b1 \u2190 0, x \u2190 e i , y \u2190 e j 3:\nfor t = 0, 1, . . . , T \u2212 1 do 4:\n\u03b1 \u2190 \u03b1 + c t x \u22a4 Dy, x \u2190 P x, y \u2190 P y 5:\nend for 6:\nReport S ij = \u03b1 7: end procedure Algorithm 2 Single-source SimRank\n1: procedure SINGLESOURCESIMRANK(i) 2: \u03b3 \u2190 0, x \u2190 e i 3: for t = 0, 1, . . . , T \u2212 1 do 4: \u03b3 \u2190 \u03b3 + c t P \u22a4t Dx, x \u2190 P x 5: end for 6:\nReport S ij = \u03b3 j for j = 1, . . . , n 7: end procedure the same fundamental idea; i.e., in (2.2), by recursively substituting the left hand side into the right hand side, we obtain the following series expansion:\nS = D + cP \u22a4 DP + c 2 P \u22a42 DP 2 + \u2022 \u2022 \u2022 .\n(2.5)\nOur algorithms compute SimRank by evaluating the first T terms of the above series.\nThe time complexity of the algorithms are O(T m) for the single-pair problem, O(T 2 m) for the single-source problem, and O(T 2 nm) for the all-pairs problem. For all problems, the space complexity is O(m).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Single-pair SimRank", "text": ". Let e i be the i-th unit vector (i = 1, . . . , n); then SimRank score s(i, j) is obtained via the (i, j) component of SimRank matrix S, i.e., s(i, j) = e \u22a4 i Se j . Thus, by applying e \u22a4 i and e j to both sides of (2.5), we obtain e \u22a4 i Se j = e \u22a4 i De j + c(P e i ) \u22a4 DP e j + c 2 (P 2 e i ) \u22a4 DP 2 e j + \u2022 \u2022 \u2022 .\n(2.6)\nOur single-pair algorithm (Algorithm 1) evaluates the right-hand side of (2.6) by maintaining P t e i and P t e j . The time complexity is O(T m) since the algorithm performs O(T ) matrix vector products for P t e i and P t e j (t = 1, . . . , T \u2212 1).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Single-source SimRank.", "text": "For the single-source problem, to obtain s(i, j) for all j \u2208 V , we need only compute vector Se i , because its j-th component is s(i, j). By applying e i to (2.5), we obtain\nSe i = De i + cP \u22a4 DP e i + c 2 P \u22a42 DP 2 e i + \u2022 \u2022 \u2022 .\n(2.7)\nOur single-source algorithm (Algorithm 2) evaluates the right hand side of (2.7) by maintaining P t e i and P t e j . The time complexity is O(T 2 m) since it performs O(T 2 ) matrix vector products for P \u22a4t DP t e i (t = 1, . . . , T \u2212 1).\nNote that, if we can use an additional O(T n) space, the single-source problem can be solved in O(T m) time. We first compute u t = DP t e i for all t = 1, . . . , T , and store them; this requires O(T m) time and O(T n) additional space. Then, we have\nSe i = u 0 + cP \u22a4 (u 1 + \u2022 \u2022 \u2022 (cP \u22a4 (u t\u22121 + cP \u22a4 u T )) \u2022 \u2022 \u2022 ),\n(2.8) for i = 1, . . . , n do 3:\nCompute SingleSourceSimRank(i) 4:\nend for 5: end procedure which can be computed in O(T m) time. We do not use this technique in our experiment because we assume that the network is very large and hence O(T n) is expensive.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "All-pairs SimRank.", "text": "Computing all-pairs SimRank is an expensive task for a large network, because it requires O(n 2 ) time since the number of pairs is n 2 . To compute all-pairs SimRank, it is best to avoid using O(n 2 ) space.\nOur all-pairs SimRank algorithm applies the single-source SimRank algorithm (Algorithm 2) for all initial vertices, as shown in Algorithm 3. The complexity is O(T 2 nm) time and requires only O(m) space. Since the best-known all-pairs SimRank algorithm [Lizorkin et al. 2010] requires O(T nm) time and O(n 2 ) space, our algorithm significantly improves the space complexity and has almost the same time complexity (since the cost of factor T is much smaller than n or m).\nIt is worth noting that this algorithm is distributed computing friendly. If we have M machines, we assign initial vertices to each machine and independently compute the single-source SimRank. Then the computational time is reduced to O(T 2 nm/M ). This shows the scalability of our all-pairs algorithm.", "publication_ref": ["b29"], "figure_ref": [], "table_ref": []}, {"heading": "Diagonal correction matrix estimation", "text": "We first observe that the diagonal correction matrix is uniquely determined from the diagonal condition. PROOF. See Appendix.\nThis proposition shows that the diagonal correction matrix can be estimated by solving equation (2.9). Furthermore, we observe that, since S L is a linear operator, (2.9) is a linear equation with n real variables D 11 , . . . , D nn where D = diag(D 11 , . . . , D nn ). Therefore, we can apply a numerical linear algebraic method to estimate matrix D.\nThe problem for solving (2.9) lies in the complexity. To reduce the complexity, we combine an alternating method (a.k.a. the Gauss-Seidel method) with Monte Carlo simulation. The complexity of the obtained algorithm is O(T LRn) time, where L is the number of iterations for the alternating method, and R is the number of Monte Carlo samples. We analyze the upper bound of parameters L and R for sufficient accuracy in Subsection 2.3.3 below.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Alternating method for diagonal estimation.", "text": "Our algorithm is motivated by the following intuition:\nA (k, k) diagonal entry S L (D) kk is the most affected by the (k, k) diagonal entry D kk of D.\n(2.10) Set initial guess of D.\n3:\nfor \u2113 = 1, . . . , L do 4:\nfor k = 1, . . . , n do\n5: \u03b4 \u2190 (1 \u2212 S L (D) kk )/S L (E (k,k) ) kk 6: D kk \u2190 D kk + \u03b4 7:\nend for 8:\nend for 9: return D 10: end procedure This intuition leads to the following iterative algorithm. Let D be an initial guess 5 ; for each k = 1, . . . , n, the algorithm iteratively updates D kk to satisfy S L (D) kk = 1. The update is performed as follows. Let E (k,k) be the matrix whose (k, k) entry is one, with the other entries being zero. To update D kk , we must find \u03b4 \u2208 R such that\nS L (D + \u03b4E (k,k) ) kk = 1.\nSince S L is linear, the above equation is solved as follows:\n\u03b4 = 1 \u2212 S L (D) kk S L (E (k,k) ) kk . (2.11)\nThis algorithm is shown in Algorithm 4. Mathematically, the intuition (2.10) shows the diagonally dominant property of operator S L . Furthermore, the obtained algorithm (i.e., Algorithm 4) is the Gauss-Seidel method for a linear equation. Since the Gauss-Seidel method converges for a diagonally dominant operator [Golub and Van Loan 2012], Algorithm 4 converges to the diagonal correction matrix 6 .", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "Monte Carlo based evaluation.", "text": "For an efficient implementation of our diagonal estimation algorithm (Algorithm 4), we must establish an efficient method to estimate S L (D) kk and S L (E (k,k) ) kk .\nConsider a random walk that starts at vertex k and follows its in-links. Let k (t) denote the location of the random walk after t steps. Then we have\nE[e k (t) ] = P t e k .\nWe substitute this representation into (2.6) and evaluate the expectation via Monte Carlo simulation. Let\nk (t) 1 , . . . , k (t)\nR be R independent random walks. Then for each step t, we have estimation\n(P t e k ) i \u2248 #{r = 1, . . . , R : k (t) r = i}/R =: p (t)\nki .\n(2.12)\nThus the t-th term of (2.6) for i = j = k is estimated as\n(P t e k ) \u22a4 DP t e k \u2248 n i=1 p (t)2 ki D ii . (2.13)\nWe therefore obtain Algorithm 5 for estimating S L (D) kk and S L (E (k,k) ) kk .\nAlgorithm 5 Estimate S L (D) kk and S L (E (k,k) ) kk .\n1: \u03b1 \u2190 0, \u03b2 \u2190 0, k 1 \u2190 k, k 2 \u2190 k, . . . , k R \u2190 k 2: for t = 0, 1, . . . , T \u2212 1 do 3: for i \u2208 {k 1 , k 2 , . . . , k R } do 4: p (t)\nki \u2190 #{r = 1, . . . , R : k r = i}/R 5:\nif i = k then 6: \u03b1 \u2190 \u03b1 + c t p (t)2 ki 7: end if 8: \u03b2 \u2190 \u03b2 + c t p (t)2 ki D ii 9:\nend for 10:\nfor r = 1, . . . , R do 11:\nk r \u2190 \u03b4 \u2212 (k r ) randomly 12:\nend for 13: end for 14: return S L (D) kk \u2248 \u03b1, S L (E (k,k) ) kk \u2248 \u03b2.\nUsing a hash table, we can implement Algorithm 5 in O(T R) time, where R denotes the number of samples and T denotes the maximum steps of random walks that are exactly the number of SimRank iterations. Therefore Algorithm 4 is performed in O(T LRn) time, where L denotes the number of iterations required for Algorithm 4.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Correctness: Accuracy of the algorithm.", "text": "To complete the algorithm, we provide a theoretical estimation of parameters L and R that are determined in relation to the desired accuracy. In Section 2.4, we experimentally evaluate the accuracy.\nEstimation of the number of iterations L. The convergence rate of the Gauss-Seidel method is linear; i.e., the squared error n k=1 (S L (D) kk \u2212 1) 2 at l-th iteration of Algorithm 4 is estimated as O(\u03c1 l ), where 0 \u2264 \u03c1 < 1 is a constant (i.e., the spectral radius of the iteration matrix). Therefore, since the error of an initial solution is O(n), the number of iterations L of Algorithm 4 is estimated as O(log(n/\u01eb)) for desired accuracy \u01eb.\nEstimation of the number of samples R. Since the algorithm is a Monte Carlo simulation, there is a trade-off between accuracy and the number of samples R. The dependency is estimated by the Hoeffding inequality, which is described below. PROPOSITION 2.4. Let p (t) ki (i = 1, . . . , n) be defined by (2.12), and let p\n(t) k := (p (t) k1 , . . . , p (t) kn ). Then P P t e k \u2212 p (t) k > \u01eb \u2264 2n exp \u2212 (1 \u2212 c)R\u01eb 2 2 .\nwhere P denotes the probability.\nLEMMA 2.5. Let k (t) 1 , . . . , k(t)\nR be positions of t-th step of independent random walks that start from a vertex k and follow ln-links. Let X (t) \nk := (1/R) R r=1 e k (t) r . Then for all l = 1, . . . , n, P e \u22a4 l X (t) k \u2212 P t e k \u2265 \u01eb \u2264 2 exp \u22122R\u01eb 2 . PROOF. Since E[e k (t) r ] = P t e k ,\n(t) ki = e \u22a4 i X (t)\nk , by Lemma 5.6, we have\nP P t e k \u2212 p (t) k > \u01eb \u2264 nP e \u22a4 i P t e k \u2212 p (t) ki > \u01eb \u2264 2n exp \u22122R\u01eb 2 .\nThis shows that we need R = O((log n)/\u01eb 2 ) samples to accurately estimate P t e k via Monte Carlo simulation.\nBy combining all estimations, we conclude that diagonal correction matrix D is estimated in O(T n log(n/\u01eb)(log n)/\u01eb 2 ) time. Since this is nearly linear time, the algorithm scales well.\nNote that the accuracy of our framework only depends on the accuracy of the diagonal estimation, i.e., if D is accurately estimated, the SimRank matrix S are accurately estimated by S L (D); see Proposition 5.7 in Appendix. Therefore, if we want accurate SimRank scores, we only need to spend more time in the preprocessing phase and fortunately do not need to increase the time required in the query phase.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section, we evaluate our algorithm via experiments using real networks. The datasets we used are shown in Table III; These are obtained from \"Stanford Large Network Dataset Collection 7 ,\", \"Laboratory for Web Algorithmics 8 ,\" and \"Social Computing Research 9 .\" We first evaluate the accuracy in Section 2.4.1, then evaluate the efficiency in Section 2.4.2; and finally, we compare our algorithm with some existing ones in Section 3.7.2.\nFor all experiments, we used decay factor c = 0.6, as suggested by Lizorkin et al. [Lizorkin et al. 2010], and the number of SimRank iterations T = 11, which is the same as Fogaras and R\u00e1cz [Fogaras and R\u00e1cz 2005].\nAll experiments were conducted on an Intel Xeon E5-2690 2.90GHz CPU with 256GB memory running Ubuntu 12.04. Our algorithm was implemented in C++ and was compiled using g++v4.6 with the -O3 option.\n2.4.1. Accuracy. The accuracy of our framework depends on the accuracy of the estimated diagonal correction matrix, computed via Algorithm 4. As discussed in Section 2.3.3, our algorithm has two parameters, L and R, the number of iterations for the Gauss-Seidel method, and the number of samples for Monte Carlo simulation, respectively. We evaluate the accuracy by changing these parameters.\nTo evaluate the accuracy, we first compute the exact SimRank matrix S by Jeh and Widom's original algorithm [Jeh and Widom 2002], and then compute the mean error [Yu et al. 2012] defined as follows:\nME = 1 n 2 i,j S L (D) ij \u2212 s(i, j) . (2.14)\nSince this evaluation is expensive (i.e., it requires SimRank scores for O(n 2 ) pairs), we used the following smaller datasets: ca-GrQc, as20000102, wiki-Vote, and ca-HepTh. Results are shown in Figure 2.1, and we summarize our results below. -For mean error ME \u2264 10 \u22125 \u223c 10 \u22126 , we need only R = 100 samples with L = 3 iterations. Note that this is the same accuracy level as [Yu et al. 2012]. -If we want more accurate SimRank scores, we need much more samples R and little more iterations L. This coincides with the analysis in Section 2.3.3 in which we estimated L = O(log(n/\u01eb)) and R = O((log n)/\u01eb 2 ). Table IV. Computational results of our proposed algorithm and existing algorithms; single-pair and single-source results are the average of 10 trials; we omitted results of the all-pairs computation of our proposed algorithm for a network larger than in-2004 since runtimes exceeded three days; other omitted results (-) mean that the algorithms failed to allocate memory.", "publication_ref": ["b29", "b12", "b18", "b46", "b46"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Dataset", "text": "Proposed [Yu et al. 2012 We next evaluate the efficiency of our algorithm. We first performed preprocessing with parameters R = 100 and L = 3, respectively. We then performed single-pair, single-source and all-pairs queries for real networks. Results are shown in Table VII; we omitted results of the all-pairs computation for a network larger than in-2004 since runtimes exceeded three days. We summarize our results below.\n-For small networks (n \u2264 1, 000, 000), only a few minutes of preprocessing time were required; furthermore, answers to single-pair queries were obtained in 100 milliseconds, while answers to single-source queries were obtained in 300 milliseconds. This efficiency is certainly acceptable for online services. We were also able to solve allpairs query in a few days. -For large networks, n \u2265 40, 000, 000, a few hours of preprocessing time were required; furthermore, answers to single-pair queries were obtained approximately in 10 seconds, while answers to single-source queries were obtained in a half minutes. To the best of our knowledge, this is the first time that such an algorithm is successfully scaled up to such large networks. -The space complexity is proportional to the number of edges, which enables us to compute SimRank values for large networks.", "publication_ref": ["b46"], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Comparisons with existing algorithms.", "text": "In this section, we compare our algorithm with two state-of-the-art algorithms for computing SimRank. We used the same parameters (R = 100, L = 3) as the above.\nComparison with the state-of-the-art all-pairs algorithm Yu et al. [Yu et al. 2012] proposed an efficient all-pairs algorithm; the time complexity of their algorithm is O(T nm), and the space complexity is O(n 2 ). They computed SimRank via matrix-based iteration (2.1) and reduced the space complexity by discarding entries in SimRank matrix that are smaller than a given threshold. We implemented their algorithm and evaluated it in comparison with ours. We used the same parameters presented in [Yu et al. 2012] that attain the same accuracy level as our algorithm.\nResults are shown in Table VII; the omitted results (-) mean that their algorithm failed to allocate memory. From the results, we observe that their algorithm performs a little faster than ours, because the time complexity of their algorithm is O(T nm), whereas the time complexity of our algorithm is O(T 2 nm); however, our algorithm uses much less space. In fact, their algorithm failed for a network with n \u2265 300,000 vertices because of memory allocation. More importantly, their algorithm cannot estimate the memory usage before running the algorithm. Thus, our algorithm significantly outperforms their algorithm in terms of scalability.\nComparison with the state-of-the-art single-pair and single-source algorithm Fogaras and R\u00e1cz [Fogaras and R\u00e1cz 2005] proposed an efficient single-pair algorithm that estimates SimRank scores by using first meeting time formula (1.4) with Monte Carlo simulation. Like our approach, their algorithm also consists of two phases, a preprocessing phase and a query phase. In the preprocessing phase, their algorithm generates R \u2032 random walks and stores the walks efficiently; this phase requires O(nR \u2032 ) time and O(nR \u2032 ) space. In the query phase, their algorithm computes scores via formula (1.4); this phase requires O(T nR \u2032 ) time. We implemented their algorithm and evaluated it in comparison with ours.\nWe first checked the accuracy of their algorithm by computing all-pairs SimRank for the smaller datasets used in Section 2.4.1; results are shown in Table V. From the table, we observe that in order to obtain the same accuracy as our algorithm, their algorithm requires R \u2032 \u2265 100,000 samples, which are much larger than our random samples R = 100. This is because their algorithm estimates all O(n 2 ) entries by Monte Carlo simulation, but our algorithm only estimates O(n) diagonal entries by Monte Carlo simulation.\nWe then evaluated the efficiency of their algorithm with R \u2032 = 100,000 samples. These results are shown in Table VII. This shows that their algorithm needs much more memory, thus it only works for small networks. This concludes that in order to obtain accurate scores, our algorithm is much more efficient than their algorithm.\nTable V. Accuracy of the single-pair algorithm proposed by Fogaras and R\u00e1cz [Fogaras and R\u00e1cz 2005]; accuracy is shown as mean error.", "publication_ref": ["b46", "b46", "b12", "b12"], "figure_ref": [], "table_ref": ["tab_7", "tab_7"]}, {"heading": "Dataset", "text": "Samples Accuracy ca-GrQc 100 1.59 \u00d710 \u22124 1,000 5.87 \u00d710 \u22125 10,000 1.32 \u00d710 \u22125 100,000 6.43 \u00d710 \u22126 (Proposed 4.77 \u00d710 \u22126 ) as20000102 100 2.51 \u00d710 \u22123 1,000 7.87 \u00d710 \u22124 10,000 2.54 \u00d710 \u22124 100,000 8.69 \u00d710 \u22125 (Proposed 1.19 \u00d710 \u22127 ) wiki-Vote 100 1.03 \u00d710 \u22123 1,000 3.57 \u00d710 \u22124 10,000 1.13 \u00d710 \u22124 100,000 3.63 \u00d710 \u22125 (Proposed 2.81 \u00d710 \u22126 ) ca-HepTh 100 1.36 \u00d710 \u22124 1,000 5.58 \u00d710 \u22125 10,000 1.18 \u00d710 \u22125 100,000 6.04 \u00d710 \u22126 (Proposed 4.56 \u00d710 \u22126 )", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "TOP K -COMPUTATION 1", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Motivation and overview", "text": "In the previous section, we describe the SimRank linearization technique and show how to use the linearization to solve single-pair, single-source, and all-pairs SimRank problems. In this section, we consider an top k search problem; we are given a vertex i and then find k vertices with the k highest SimRank scores with respect to i. This problem is interested in many applications because, usually, highly similar vertices for a given vertex i are very few (e.g., 10-20), and in many applications, we are are only interested in such highly similar vertices. This problem can be solved in O(T 2 m) time by using the single-source SimRank algorithm; however, since we only need k highly-similar vertices, we can develop more efficient algorithm.\nHere, we propose a Monte-Carlo algorithm based on SimRank linearization for this problem; the complexity is independent of the size of networks. Note that Fogaras and Racz [Fogaras and R\u00e1cz 2005] also proposed the Monte-Carlo based single-pair computation algorithm. By comparing their algorithm, our main ingredients is the \"pruning technique\" by utilizing the SimRank linearization. We observe that SimRank score s(i, j) decays very rapidly as distance of the pair i, j increases. To exploit this phenomenon, we establish upper bounds of SimRank score s(i, j) that only depend on distance d(i, j). The upper bounds can be efficiently computed by Monte-Carlo simulation (in our preprocess). These upper bounds, together with some adaptive sample technique, allow us to effectively prune the similarity search procedure.\nOverall, the proposed algorithm runs as follows.\n(1) We first perform preprocess to compute the auxiliary values for upper bounds of SimRank s(i, j) for all i \u2208 V (see Section 3.4). In addition, we construct an auxiliary bipartite graph H, which allows us to enumerate \"candidates\" of highly similar vertices j more accurate. This is our preprocess phase. The time complexity is O(n).\n(2) We now perform our query phase. We compute SimRank scores s(i, j) by the Monte-Carlo simulation for each vertex i in the ascending order of distance from a given vertex i, and at the same time, we perform \"pruning\" by the upper bounds. We also combine the adaptive sampling technique for the Monte-Carlo simulation; specifically, for a query vertex i, we first estimate SimRank scores roughly for each candidate j by using a small number of Monte-Carlo samples, and then we re-compute more accurate SimRank scores for each candidate j that has high estimated Sim-Rank scores.", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "Monte Carlo algorithm for single-pair SimRank", "text": "Let us consider a random walk that starts from i \u2208 V and that follows its in-links, and let i (t) be a random variable for the t-th position of this random walk. Then we observe that\nP t e i = E[e i (t) ]. (3.1)\nTherefore, by plugging (3.1) to (2.6), we obtain\ns (T ) (i, j) =D ij + cE[e i (1) ] \u22a4 DE[e j (1) ] + \u2022 \u2022 \u2022 + c T \u22121 E[e i (T \u22121) ] \u22a4 DE[e j (T \u22121) ]. (3.2)\nOur algorithm computes the expectations in the right hand side of (3.2) by Monte-Carlo simulation as follows: Consider R independent random walks i\n(t) 1 , . . . , i(t)\nR that Algorithm 6 Monte-Carlo Single-pair SimRank 1: procedure SINGLEPAIR(i,j)\n2: i 1 \u2190 i, . . . , i R \u2190 i, j 1 \u2190 j, . . . , j R \u2190 j 3: \u03c3 = 0 4: for t = 0, 1, . . . , T \u2212 1 do 5: for w \u2208 {i 1 , . . . , i R } \u2229 {i 1 . . . , i R } do 6: \u03b1 \u2190 #{r : i r = w, r = 1, . . . , R} 7:\n\u03b2 \u2190 #{r : j r = w, r = 1, . . . , R} 8:\n\u03c3 \u2190 \u03c3 + c t D ww \u03b1\u03b2/R 2 9:\nend for 10:\nfor r = 1, . . . , R do return \u03c3 15: end procedure start from i \u2208 V , and R independent random walks j\n11: i r \u2190 \u03b4 \u2212 (i r ), j r \u2190 \u03b4 \u2212 (j r ),\n(t) 1 , . . . , j (t)\nR that start from j \u2208 V with i = j. Then each t-th term of (3.2) can be estimated as\nc t E[e i (t) ] \u22a4 DE[e j (t) ] \u2243 c t R 2 R r=1 R r \u2032 =1 D i (t) r j (t) r \u2032 . (3.3)\nWe compute the right hand side of (3.3). Specifically by maintaining the positions of\ni (t) 1 , . . . , i(t)\nR and j\n(t) 1 , . . . , j(t)\nR by hash tables, it can be evaluated in O(R) time. Therefore the total time complexity to evaluate (3.2) is O(T R). The algorithm is shown in Algorithm 6. We emphasize that this time complexity is independent of the size of networks (i.e, n, m) Hence this algorithm can scale to very large networks.\nWe give estimation of the number of samples to compute (3.2) accurately, with high probability. We use the Hoeffding inequality to show the following. PROPOSITION 3.1. Lets (T ) (i, j) be the output of the algorithm. Then\nP |s (T ) (i, j) \u2212 s (T ) (i, j)| \u2265 \u01eb \u2264 4nT exp \u2212\u01eb 2 R/2(1 \u2212 c) 2 . (3.4) LEMMA 3.2. P X (t)\u22a4 u DX (t) v \u2212 (P t e u ) \u22a4 DP t e v \u2265 \u01eb \u2264 4n exp(\u2212\u01eb 2 R/2). PROOF. P X (t)\u22a4 u DX (t) v \u2212 (P t e u ) \u22a4 DP t e v \u2265 \u01eb \u2264 P X (t)\u22a4 u D X (t) v \u2212 P t e v \u2265 \u01eb/2 + P X (t) u \u2212 P t e u \u22a4 DP t e v \u2265 \u01eb/2 \u2264 4n exp(\u2212\u01eb 2 R/2).\nPROOF OF PROPOSITION 3.1. By Lemma 5.11, we have\nP T \u22121 t=0 c t X (t)\u22a4 u DX (t) v \u2212 s (T ) (u, v) \u2265 \u01eb \u2264 T \u22121 t=0 P c t X (t)\u22a4 u DX (t) v \u2212 c t (P t e u ) \u22a4 DP t e v \u2265 c t \u01eb/(1 \u2212 c) \u22644nT exp \u2212\u01eb 2 R/2(1 \u2212 c) 2 .\nBy Proposition 3.1, we have the following.\nCOROLLARY 3.3. Algorithm 6 computes the SimRank score s (T ) (u, v) with accuracy 0 < \u01eb < 1 with probability 0 < \u03b4 < 1 by setting R = 2(1 \u2212 c) 2 log(4nT /\u03b4)/\u01eb 2 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Distance correlation of SimRank", "text": "Our top k similarity search algorithm performs single-pair SimRank computations for a given source vertex i and for other vertices j, but we save the time complexity by pruning. In order to perform this pruning, we need some upper bounds. This section and the next section are devoted to establish the upper bounds.\nThe important observation of SimRank is SimRank score s(i, j) decays very fast as the pair i, j goes away.\nIn this section, we empirically verify this fact in some real networks, and in the next section, we develop the upper bounds that only depend on distance.\nLet us look at Figure 3.1. We randomly chose 100 vertices i and enumerate top-1000 similar vertices with respect to to a query vertex u (note that these top-1000 vertices are \"exact\", not 'approximate\"). Each point denotes the average distance of the k-th similar vertex. To convince the reader, we also give the average distance between two vertices for each network by the blue line.\nFigure 3.1 clearly shows much intuitive information. If we only need to compute top-10 vertices, all of them are within distance two, three, or four. In real applications, it is unlikely that we need to compute top-1000 vertices, but even for this case, most of them are within distance four or five. We emphasize that these distances are smaller than the average distance of two vertices in each network. Thus we can conclude that the \"candidates\" of highly similar vertices are screened by distances very well.\nThere is one remark we would like to make from Figure 3.1. The top-10 highest Sim-Rank vertices in Web graphs are much closer to a query vertex than social networks. Thus we can also claim that our algorithm would work better for Web graphs than for social networks, because we only look at subgraphs induced by vertices of distance within three (or even two) from a query vertex. This claim is verified in Section 3.6.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Tight upper bounds", "text": "In the previous section, we observe that highly similar vertices with respect to a query vertex are within small distance from u. This observation allows us to propose our efficient algorithm for the top-k similarity search problem for a single vertex. In order to obtain this algorithm, we need to establish the upper bounds of SimRank that depend only on distance, which will be done in this section.\nLet us observe that by definition, SimRank score is bounded by the decay factor to the power of the distance: Since almost all high SimRank score vertices with respect to a query vertex u are located within distance three from u (see Figure 3.1), we obtain s(u, v) \u2264 c 3 = 0.216. But this is too large for our purpose (indeed, our further experiments to compare actual SimRank scores with this bound confirm that it is too large).\ns(u, v) \u2264 c d(u,v)\nHere we propose two upper bounds, called \"L1 bound\" and \"L2 bound\". Our algorithm, described in a later section, combines these two bounds to perform \"pruning\", which results in a much faster algorithm. \ns (T ) (u, v) \u2264 \u03b2(u, d) (3.8)\nThe proof will be given in Appendix.\nRemark 3.5. \u03b1(u, d, t) has the following probabilistic representation:\n\u03b1(u, d, t) = max d(u,w)=d D ww P{u (t) = w}\nwhere u (t) denotes the position of a random walk that starts from u and follows its in-links.\nTo compute \u03b1(u, d, t) and \u03b2(u, d), we can use Monte-Carlo simulation for P t e u as shown in Algorithm 7.\nSimilar to Proposition 3.1, we obtain the following proposition, whose proof will be given in Appendix. This proposition shows that Algorithm 7 can compute \u03b1(u, d, t) and \u03b2(u, d).\nPROPOSITION 3.6. Let\u03b2(u, d) be computed by Algorithm 7. Then\nP |\u03b2(d, t) \u2212 \u03b2(d, t)| \u2265 \u01eb \u2264 2nd max T exp(\u22122\u01eb 2 R)\nBy Proposition 3.6, we have the following. COROLLARY 3.7. Algorithm 7 computes \u03b2(u, d) with accuracy less than 0 < \u01eb < 1 with probability at least 0 < \u03b4 < 1 by setting R = log(2nd max T /\u03b4)/(2\u01eb 2 ). (u, d(u, w), t), \u00b5} where\nAlgorithm 7 Monte-Carlo \u03b1(u, d, t), \u03b2(u, d) computation 1: procedure COMPUTEALPHABETA(u) 2: u 1 \u2190 u, . . . , u R \u2190 u 3: for t = 0, 1, . . . , T \u2212 1 do 4: for w \u2208 {u 1 , . . . , u R } do 5: \u00b5 \u2190 D ww #{r : u r = w, r \u2208 [1, R]}/R 6: \u03b1(u, d(u, w), t) \u2190 max{\u03b1\n\u221a D = diag( \u221a D 11 , . . . , \u221a D nn ). Note that, since D is a nonnegative diagonal ma- trix, \u221a D is well-defined.\nPROPOSITION 3.8. For two vertices u and v, we have\ns (T ) (u, v) \u2264 T t=0 c t \u03b3(u, t)\u03b3(v, t) (3.11)\nThe proof of Proposition 3.8 will be given in Appendix.\nTo compute \u03b3(u, d) for each u, we can use Monte-Carlo simulation. Let us emphasize that we can compute \u03b3(u, d) for each u and d \u2264 d max in preprocess.\nThe following proposition, whose proof will be given in Appendix, shows that Algorithm 8 can compute \u03b3(u, d).\nPROPOSITION 3.9. Let\u03b3(u, t) be computed by Algorithm 8. Then\nP {|\u03b3(u, t) \u2212 \u03b3(u, t)| \u2265 \u01eb} \u2264 4n exp \u2212\u01eb 2 R/8 .\nThe proof of of Proposition 3.9 will be given in Appendix. By Proposition 3.9, we have the following. COROLLARY 3.10. Algorithm 8 computes \u03b3(u, t) with accuracy less than 0 < \u01eb < 1 with probability at least 0 < \u03b4 < 1 by setting R = 8 log(4n/\u03b4)/\u01eb 2 .\nAlgorithm 8 Monte-Carlo \u03b3(u, t) computation 1: procedure COMPUTEGAMMA(u) 2: u 1 \u2190 u, . . . , u R \u2190 u 3: for t = 0, 1, . . . , T \u2212 1 do 4: \u00b5 = 0 5: for w \u2208 {u 1 , . . . , u R } do 6: \u00b5 \u2190 \u00b5 + D ww #{r : u r = w, r \u2208 [1, R]} 2 /R 2 7: end for 8: \u03b3(u, t) \u2190 \u221a \u00b5 9:\nfor r = 1, . . . , R do 10: end for 13: end procedure 3.4.3. Comparison of two bounds. The reason why we need both L1 and L2 bounds is the following: The L1 bound is more effective for a low degree query vertex u. This is because if u has low degree then P t e u is sparse. Therefore the bound (5.7) becomes tighter.\nu r \u2190 \u03b4(u r )\nOn the other hand, the L2 bound is more effective for high degree vertex u. This is because if u has high degree then P t e u spreads widely, and hence each entry is small. Therefore \u221a DP t e u decrease rapidly.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm", "text": "We are now ready to provide our whole algorithm for top-k similarity search. Our algorithm consists of two phases: preprocess phase and query phase. In the query phase, for a given vertex u, we compute single-pair SimRanks s(u, v) for some vertices v that may have high SimRank value (we call such vertices candidates that are computed in the preprocess phase), and output k highly similar vertices.\nIn order to obtain similar vertices accurately, we have to perform many Monte Carlo simulations in single-pair SimRank computation (Algorithm 6). Thus, the key of our algorithm is the way to reduce the number of candidates that are computed in the preprocess phase in Section 7.1. end for 13: end procedure 3.5.1. Preprocess phase. In the preprocess phase, we precompute \u03b3 in (3.10) for the L2 bound as described in Algorithm 8. Note that we compute \u03b1, \u03b2 in (3.6), (3.7) for the L1 bound in query phase.\nAfter that, for each vertex u, we enumerate \"candidates\" of highly similar vertices v. For this purpose, we consider the following auxiliary bipartite graph H. The left and right vertices of H are copy of V (i.e., H has 2n vertices). Let u left be the copy of u \u2208 V in the left vertices and let v right be the copy of v \u2208 V in the right vertices. There is an edge (u left , v right ) if a random walk that starts from u frequently reaches v. By this construction, a pair of vertices u and v has high SimRank score if u left and v left share many neighbors. We construct this bipartite graph H by performing Monte-Carlo simulations in the original graph G as follows. For each vertex u, we iterate the following procedure P times to construct an index for u. We perform a random walk W 0 of length T from u in G. We further perform Q random walks W 1 , . . . , W Q from u. Let v be t-th vertex on W 0 . Then we put an edge (u left , v right ) in H if there are at least two random walks in W 1 , . . . W Q that contain v at t-th step. The whole procedure is described in Algorithm 3 below. Here, for a random walk W j and t \u2265 0, we denote the vertex at the t-th step of W j by W jt .\nIn our experiment, we set P = 10, T = 11 and Q = 5. The time complexity of this preprocess phase is O(n(R + P Q)T ), where R comes from Algorithm 3 and we set R = 100 in our experiment. The space complexity is O(nP ), but in practice, since the number of candidates are usually small, the space is less smaller than this bound.\n3.5.2. Query phase. We now describe our query phase. For a given vertex u, we first traverse the auxiliary bipartite graphs H and enumerate all the vertices v that share the neighbor in H. We then prune some vertices v by L1 and L2 bounds. After that, for each candidate v, we compute SimRank scores s(u, v) by Algorithm 6. Finally we output top k similar vertices as the solution of similarity search.\nTo accelerate the above procedure, we use the adaptive sample technique. For a query vertex u, we first set R = 10 (in Algorithm 6) and estimate SimRank scores roughly for each candidate v by Monte Carlo simulation (i.e, we only perform 10 random walks for v by Algorithm 6). Then, we change R = 100 and re-compute more accurate SimRank scores for each candidate v that has high estimated SimRank scores by Monte Carlo simulation (i.e, we perform 100 random walks for v by Algorithm 6). The whole procedure is described in Algorithm 5. Perform Algorithm 6 R = 10 times to roughly estimate s(u, v).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "6:", "text": "if The estimated score s(u, v) is not small then 7:\nPerform Algorithm 6 R = 100 times to estimate s(u, v) more accurately Output top k similar vertices 11: end procedure", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiment", "text": "We perform our proposed algorithm for several real networks and evaluate performance of our algorithm. We also compare our algorithm with some state-of-the-art algorithms.\nAll experiments are conducted on an Intel Xeon E5-2690 2.90GHz CPU with 256GB memory and running Ubuntu 12.04. Our algorithm is implemented in C++ and compiled with g++v4.6 with -O3 option.\nAccording to discussion in the previous section, we set the parameters as follows: decay factor c = 0.6, T = 11, R = 100 for \u03b3 (Algorithm 8) and for s(\u2022, \u2022) (Algorithm 6), and R = 10000 for \u03b1 and \u03b2 (Algorithm 7) that is optimized by pre-experiment 10 . We also set P = 10, T = 11 and Q = 5 in our preprocess phase as in Section 7.1.\nIn addition, we set k = 20 since we are only interested in small number of similar vertices. To avoid searching vertices of very small SimRank scores, we set a threshold \u03b8 = 0.01 to terminate the procedure when upper bounds become smaller than \u03b8.\nWe use the datasets shown in Table III.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Results", "text": "We evaluate our proposed algorithm for several real networks. The results are summarized in Tables VII. We first observe that our proposed algorithm can find top-20 similar vertices in less than a few seconds for graphs of billions edges (i.e., \"it-2004\") and in less than a second for graphs of one hundred millions edges, respectively.\nWe can also observe that the query time for our algorithm does not much depend on the size of networks. For example, \"indochina-2004\" has 8 times more edges than \"flickr\" but the query time is twice faster than that. Hence the computational time of our algorithm depends on the network structure rather than the network size. Specifically, our algorithm works better for web graphs than for social networks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Analysis of Accuracy.", "text": "In this subsection, we shall investigate performance of our algorithm in terms of accuracy. In many applications, we are only interested in very similar vertices. Hence we only look at vertices that have high SimRank scores.\nSpecifically, what we do is the following. We first compute, for a query vertex u, the single source SimRank scores s(u, v) for all the vertices v (for the whole graph) by the exact method. Then we pick up all \"high score\" vertices v with score at least t from this computation (for t = 0.04, 0.05, 0.06, 0.07). Finally, we compute \"high score\" vertices v with respect to the query vertex u by our proposed algorithm. Let us point out that our algorithm can be easily modified so that we only output high SimRank score vertices(because we just need to set up the threshold to prune the similarity search). We then compute the following value: # of our high score vertices # of the optimal high score vertices .\nWe also do the same thing for high score vertices computed by Fogaras and R\u00e1cz [Fogaras and R\u00e1cz 2005](we used the same parameter R \u2032 = 100 presented in [Fogaras and R\u00e1cz 2005]). We perform this operation 100 times, and take the average. The result is in Table VI. We can see that our algorithm actually gives very accurate results. In addition, our algorithm gives better accuracy than Fogaras and R\u00e1cz [Fogaras and R\u00e1cz 2005].", "publication_ref": ["b12", "b12", "b12"], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Comparison with existing results.", "text": "In this subsection, we compare our algorithm with two state-of-the-art algorithms for computing SimRank, and show that our algorithm outperforms significantly in terms of scalability.\nComparison with the state-of-the-art all-pairs algorithm. Yu et al. [Yu et al. 2012] proposed an efficient all-pairs algorithm; the time complexity of their algorithm is O(T nm), and the space complexity is O(n 2 ), where T is the number of the iterations. We implemented their algorithm and evaluated it in comparison with ours. We used the same parameters presented in [Yu et al. 2012].\nResults are shown in Table VII; the omitted results (-) mean that their algorithm failed to allocate memory. From the results, we observe that their algorithm is a little faster than ours in query time, but our algorithm uses much less space(15-30 times). In fact, their algorithm failed for graphs with a million edges, because of memory allocation. More importantly, their algorithm cannot estimate the memory usage before running the algorithm. Moreover, since our all-pairs algorithm can easily be parallelized to multiple machines, if there are 100 machines, even for graphs of billions size, our all-pairs algorithm can output all top-20 vertices in less than 5 days. Thus, our algorithm significantly outperforms their algorithm in terms of scalability.\nComparison with the state-of-the-art single-pair and single-source algorithm. Fogaras and R\u00e1cz [Fogaras and R\u00e1cz 2005] proposed an efficient single-pair algorithm that estimates SimRank scores with Monte Carlo simulation. Like our approach, their algorithm also consists of two phases, a preprocessing phase and a query phase. In the preprocessing phase, their algorithm generates R \u2032 random walks and stores the walks efficiently; this phase requires O(nR \u2032 ) time and O(nR \u2032 ) space. The query phase phase requires O(T nR \u2032 ) time, where T is the number of iterations. We implemented their algorithm and evaluated it in comparison with ours. We used the same parameter R \u2032 = 100 presented in [Fogaras and R\u00e1cz 2005].\nWe can see that their algorithm is faster in query time. But we suspect that this is due to relaxing accuracy, as in the previous subsection. In order to obtain the same accuracy as our algorithm, we suspect that R \u2032 should be 500-1000, which implies that their algorithm would be at least 5-10 times slower, and require at leats 5-10 times more space.\nIn this case, their algorithm would fail for graphs with more than ten millions edges because of memory allocation. Even for the case R \u2032 = 100, our algorithm uses much less space(10-20 times), and their algorithm failed for graphs with more than 70 millions edges because of memory allocation. Therefore we can conclude that our algorithm significantly outperforms their algorithm in terms of scalability. ", "publication_ref": ["b46", "b46", "b12", "b12"], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Motivation and overview", "text": "Finally, in this section, we describe the SimRank join problem, which is formulated as follows.\nPROBLEM 4.1 (SIMRANK JOIN). Given a directed graph G = (V, E) and a threshold \u03b8 \u2208 [0, 1], find all pairs of vertices (i, j) \u2208 V \u00d7 V for which the SimRank score of (i, j) is greater than the threshold, i.e., s(i, j) \u2265 \u03b8, This problem is useful in the near-duplication detection problem [Chen et al. 2002;Arasu et al. 2009]. Let us consider the World Wide Web, which contains many \"very similar pages.\" These pages are produced by activities such as file backup, caching, spamming, and authors moving to different institutions. Clearly, these very similar pages are not desirable for data mining, and should be isolated from the useful pages by near-duplication detection algorithms.\nNear duplication detection problem is solved by the similarity join query. Let s(i, j) be a similarity measure, i.e., for two objects i and j, they are (considered as) similar if and only if s(i, j) is large. The similarity join query with respect to s finds all pairs of objects (i, j) with similarity score s(i, j) exceeding some specified threshold \u03b8 [White and Jain 1996]. 1112 The similarity join is a fundamental query for a database, and is used in applications, such as merge/purge [Hern\u00e1ndez and Stolfo 1995], record linkage [Fellegi and Sunter 1969], object matching [Sivic and Zisserman 2003], and reference reconciliation [Dong et al. 2005].\nSelecting the similarity measure s(i, j) is an important component of the similarity join problem. Similarity measures on graphs have been extensively investigated. Here, we are interested in link-based similarity measures, which are determined by sorely the link structure of the network. For applications in the World Wide Web, by comparing content-based similarity measures, which are determined by the content data stored on vertices (e.g., text and images), link-based similarity measures are more robust against spam pages and/or machine-translated pages.", "publication_ref": ["b8", "b3", "b17", "b11", "b33", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Difficulty of the problem.", "text": "In solving the SimRank join problem, the following obstacles must be overcome.\n(1) There are many similar-pair candidates.\n(2) Computationally, SimRank is very expensive.\nTo clarify these issues, we compare the SimRank with the Jaccard similarity, where the Jaccard similarity between two vertices i and j is given by\nJ(i, j) := |\u03b4(i) \u2229 \u03b4(j)| |\u03b4(i) \u222a \u03b4(j)| .\nRegarding the first issue, since the Jaccard similarity satisfies J(i, j) = 0 for all pairs of vertices (i, j) with d(i, j) \u2265 3 (i.e., their distance is at least three), the number of possibly similar pairs (imposing the Jaccard similarity) is easily reduced to much smaller than all possible pairs. This simple but fundamental concept is adopted in many existing similarity join algorithms [Sarawagi and Kirpal 2004]. However, since the SimRank exploits the information in multihop neighborhoods and hence scans the entire graph, it must consider all O(n 2 ) pairs, where n is the number of vertices. Therefore, whereas the Jaccard similarity is adopted in \"local searching,\" the SimRank similarity must look at the global influence of all vertices, which requires tracking of all O(n 2 ) pairs.\nRegarding the second issue, the Jaccard similarity of two vertices can be very efficiently computed (e.g., in O(|\u03b4(i)| + |\u03b4(j)|) time using a straightforward method or in O(1) time using MinHash [Broder 1997;Lee et al. 2011]). Conversely, SimRank computation is very expensive.\nNotably, until recently, most SimRank algorithms have computed the all-pairs Sim-Rank scores [Jeh and Widom 2002;Lizorkin et al. 2010;, which requires at least O(n 2 ) time, and if we have the all-pairs SimRank scores, the SimRank join problem is solved in O(n 2 ) time. For example, in one investigation of the SimRank join problem [Zheng et al. 2013], the all-pairs SimRank scores were first computed by an existing algorithm and an index for the SimRank join was then constructed. Therefore, developing scalable algorithm for the SimRank join problem is a newer challenging problem.", "publication_ref": ["b31", "b5", "b21", "b18", "b29", "b48"], "figure_ref": [], "table_ref": []}, {"heading": "Contribution and overview.", "text": "Here, we propose a scalable algorithm for the SimRank join problem, and perform experiments on large real datasets. The computational cost of the proposed algorithm only depends on the number of similar pairs, but does not depend on all pairs O(n 2 ). The proposed algorithm scales up to the network of 5M vertices and 70M edges. By comparing with the state-of-the-art algorithms, it is about 10 times faster, and requires about only 10 times smaller memory.\nThis section overviews our algorithm, which consists of two phases: filter and verification. The former enumerates the similar pair candidates, and then the latter decides whether each candidate pair is actually similar. Note that this framework is commonly adopted in similarity join algorithms [Xiao et al. 2011;Deng et al. 2014]. A more precise description of the two phases is given below.\nThe filter phase is the most important phase of the proposed algorithm because it must overcome both (1) and (2) difficulties, discussed in previous subsection. We combines the following three techniques for this phase. The details are discussed in Section 4.2.\n(1) We adopt the SimRank linearization (Section 2) by which the SimRank is computed as a solution to a linear equation.\n(2) We solve the linear equation approximately by the Gauss-Southwell algorithm [Southwell 1940;1946], which avoids the need to compute the SimRank scores for non-similar pairs (Sections 4.2.1,4.2.2). (3) We adopt the stochastic thresholding to reduce the memory used in the Gauss-Southwell algorithm (Section 4.2.3).\nThe verification phase is simpler than the filter phase. We adopt the following two techniques for this phase. The details are discussed in Section 4.3.\n(1) We run a Monte-Carlo algorithm for each candidate to decide whether the candidate is actually similar. This can be performed in parallel.\n(2) We control the number of Monte-Carlo samples adaptively to reduce the computational time.\nIt should be emphasized that, we give theoretical guarantees for all techniques used in the algorithm. All omitted proofs are given in Appendix.\nThe proposed algorithm is evaluated by experiments on real datasets (Section 4.4). The algorithm is 10 times faster, and requires only 10 times smaller memory that of Algorithm 11 SimRank join algorithm.\n1: procedure SIMRANKJOIN(\u03b8) 2:\nCompute two sets J L and J H .", "publication_ref": ["b41", "b9", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "3:", "text": "Output all (i, j) \u2208 J L .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4:", "text": "for (i, j) \u2208 J H \\ J L do 5: if i and j are really similar then 6:\nOutput (i, j).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "7:", "text": "end if 8: end for 9: end procedure the state-of-the-arts algorithms, and scales up to the network of 5M vertices and 70M edges. Since the existing study [Zheng et al. 2013] only performed in 338K vertices and 1045K edges, our experiment scales up to the 10 times larger network. Also, we empirically show that the all techniques used in the algorithm works effectively.\nWe also verified that the distribution of the SimRank scores on a real-world network follows a power-law distribution ], which has been verified only on small networks.", "publication_ref": ["b48"], "figure_ref": [], "table_ref": []}, {"heading": "Filter phase", "text": "In this section, we discuss the details of the filter phase for enumerating similar-pair candidates. We will discuss the details of the verification phase in the next section. Let us give overview of the filter phase.\nLet J(\u03b8) = {(i, j) : s(i, j) \u2265 \u03b8} be the set of similar pairs. The filter phase produces two subsets J L (\u03b8, \u03b3) and J H (\u03b8, \u03b3) such that\nJ L (\u03b8, \u03b3) \u2286 J(\u03b8) \u2286 J H (\u03b8, \u03b3), (4.1)\nwhere \u03b3 \u2208 [0, 1] is an accuracy parameter. Here, J L (\u03b8, \u03b3) is monotone increasing in \u03b3, J H (\u03b8, \u03b3) is monotone decreasing in \u03b3, and J L (\u03b8, 1) = J(\u03b8) = J H (\u03b8, 1). Note that, our filter phase gives 100%-precision solution J L (\u03b8, \u03b3) and 100%-recall solution J H (\u03b8, \u03b3). These two sets might be useful in some applications.\nLet us consider how to implement the filter phase. The basic idea is that \"compute only relevant entries of SimRank.\" In order to achieve this idea, we combine the two techniques, the linearization of the SimRank [Kusumoto et al. 2014; ?], and the Gauss-Southwell algorithm [Southwell 1940;1946] for solving a linear equation. The linearization of the SimRank is a technique to convert a SimRank problem to a linear equation problem. By using this technique, the problem of computing large entries of SimRank is transformed to the problem of computing large entries of a solution of a linear equation. To solve this linear algebraic problem, we can use the Guass-Southwell algorithm. By error analysis of the Guass-Southwell algorithm (given later in this paper), our filter algorithm obtains the lower and the upper bound of the SimRank of each pair (i, j). Using these bounds, the desired sets J L (\u03b8, \u03b3) and J H (\u03b8, \u03b3) are obtained, where \u03b3 is an accuracy parameter used in the Guass-Southwell algorithm.\nThe above procedure is already much efficient; however, we want to scale up the algorithm for large networks. The bottleneck of the above procedure is the memory allocation. We resolve this issue by introducing the stochastic thresholding technique. This is the overview of the proposed filter phase. In the following subsections, we give details of the filter phase.", "publication_ref": ["b20", "b35", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "Gauss-Southwell algorithm.", "text": "By the linearization of the SimRank, we obtained the linear equation (2.2). We want to solve this linear equation in S; however, since it has O(n 2 ) variables, we must keep track only on large entries of S. For this purpose, we adopt the Gauss-Southwell algorithm [Southwell 1940;1946], which is a very classical algorithm for solving a linear equation. In this subsection, we describe the Gauss-Southwell algorithm for a general linear equation Ax = b, and in the next subsection, we apply this method for the linearized SimRank equation (2.2).\nSuppose we desire an approximate solution to the linear system Ax = b, where A is an n \u00d7 n matrix with unit diagonal entries, i.e., A ii = 1 for all i = 1, . . . , n. Let \u01eb > 0 be an accuracy parameter. The Gauss-Southwell algorithm is an iterative algorithm. Let x (t) be the t-th solution and r (t) := b \u2212 Ax (t) be the corresponding residual. At each step, the algorithm chooses an index i such that |r (t) i | \u2265 \u01eb, and updates the solution as\nx (t+1) = x (t) + r (t) i e i ,(4.2)\nwhere e i denotes the i-th unit vector. The corresponding residual becomes\nr (t+1) = b \u2212 Ax (t+1) = r (t) \u2212 r (t) i Ae i . (4.3)\nSince A has unit diagonals, the i-th entry of r (t+1) is zero. Repeating this process until r (t) i\n< \u01eb for all i = 1, . . . , n, we obtain a solution x such that b \u2212 Ax \u221e < \u01eb. This algorithm is called the Gauss-Southwell algorithm.\nNote that this algorithm is recently rediscovered and applied to the personalized PageRank computation. See Section 1.2 for related work.", "publication_ref": ["b35", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "Filter phase.", "text": "We now propose our filtering algorithm. First, we compute the diagonal correction matrix D by Algorithm 4, reducing the SimRank computation problem to the linear equation (2.2). The Gauss-Southwell algorithm is then applied to the equation.\nThe t-th solution S (t) and the corresponding residual R (t) are maintained such that t) .\nD \u2212 (S (t) \u2212 cP \u22a4 S (t) P ) = R (\n(4.4) Initial conditions are S (0) = O and R (0) = D. At each step, the algorithm finds an entry (i, j) such that |R (t) ij | \u2265 \u01eb, and then updates the current solution as\nS (t+1) = S (t) + R (t)\nij e i e \u22a4 j .\n(4.5)\nThe corresponding residual becomes\nR (t+1) = R (t) \u2212 R (t) ij e i e \u22a4 j + cR (t) ij (P \u22a4 e i )(P \u22a4 e j ) \u22a4 . (4.6)\nSince we have assumed that G is simple, the i-th entry of P \u22a4 e i is zero, the (i, j)-th entry of R (t+1) is also zero. The algorithm repeats this process until\n|R (t) ij | < \u01eb for all i, j \u2208 V .\nThe procedure is outlined in Algorithm 12.\nWe first show the finite convergence of the algorithm, whose proof will be given in Appendix.\nPROPOSITION 4.2. Algorithm 12 terminates at most t = \u03a3/\u01eb steps, where \u03a3 = ij S ij is the sum of all SimRank scores. Since the (i, j) step is performed in O(|\u03b4(i)||\u03b4(j)|) time with O(|\u03b4(i)||\u03b4(j)|) memory allocation, the overall time and space complexity is O(I 2 max \u03a3/\u01eb), where I max is the maximum in-degree of G.\nWe now show that Algorithm 6 guarantees an approximate solution (whose proof will be given in Appendix).\nAlgorithm 12 Gauss-Southwell algorithm used in Algorithm 13. 1: procedure GAUSSSOUTHWELL(\u01eb) 2:\nS (0) = O, R (0) = D, t = 0 3: while there is (i, j) such that |R (t) ij | > \u01eb do 4: S (t+1) = S (t) + R (t) ij e i e \u22a4 j 5: R (t+1) = R (t) \u2212 R (t) ij e i e \u22a4 j + cR (t) ij (P \u22a4 e i )(P \u22a4 e j ) \u22a4 6: t \u2190 t + 1 7:\nend while 8:\nReturn S (t) as an approximate SimRank. 9: end procedure Algorithm 13 Filter procedure.\n1: procedure FILTER(\u03b8, \u03b3)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "2:", "text": "Compute diagonal correction matrix D.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "3:", "text": "Compute approximate solutionS of linear equation S = cP \u22a4 SP + D by Gauss-Southwell algorithm with accuracy \u01eb = (1 \u2212 c)\u03b3\u03b8.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4:", "text": "Output J L = {(i, j) :S ij \u2265 \u03b8} and J H = {(i, j) :S ij \u2265 \u03b3\u03b8}. 5: end procedure PROPOSITION 4.3. Let \u03b3 \u2208 [0, 1) be an accuracy parameter andS be the approximate SimRank obtained by Algorithm 12 with\n\u01eb = (1 \u2212 c)(1 \u2212 \u03b3)\u03b8. Then we have 0 \u2264 S ij \u2212S ij \u2264 (1 \u2212 \u03b3)\u03b8 (4.7) for all i, j \u2208 V .\nThe left inequality states ifS ij \u2265 \u03b8, S ij \u2265 \u03b8. Similarly, the right inequality states that if S ij \u2265 \u03b8,S ij \u2265 \u03b3\u03b8. Thus, letting\nJ L (\u03b8, \u03b3) := {(i, j) :S ij \u2265 \u03b8}, (4.8) J H (\u03b8, \u03b3) := {(i, j) :S ij \u2265 \u03b3\u03b8}, (4.9)\nwe obtain (4.1). J L and J H can be accurately estimated by letting \u03b3 \u2192 1. However, since the complexity is proportional to O(1/\u01eb) = O(1/(1 \u2212 \u03b3)), a large \u03b3 is precluded. Therefore, in practice, we set to some small value (e.g., \u03b3 = 0) and verify whether the pairs (i, j) \u2208 J H (\u03b8, \u03b3) \\ J L (\u03b8, \u03b3) are actually similar by an alternative algorithm.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Stochastic thresholding for reducing memory.", "text": "We cannot predict the required memory before running the algorithm (which depends on the SimRank distribution); therefore the memory allocation is a real bottleneck in the filter procedure (Algorithm 13). To scale up the procedure, we must reduce the waste of memory. Here, we develop a technique that reduces the space complexity.\nWe observe that, in Algorithm 13, some entries R ij are only used to store the values, and never used in future because they do not exceed \u01eb; therefore we want to reduce storing of these values. Here, a thresholding technique, which skips memory allocations for very small values, seems effective. This kind of heuristics is frequently used in SimRank computations Lizorkin et al. 2010;]. However, a simple thresholding technique may cause large errors because it ignores the accumulation of small values. Algorithm 14 Stochastic thresholding for R ij \u2190 R ij + a.\n1: procedure STOCHASTICTHRESHOLDING(R,i,j,a;\u03b2) 2: if R ij is already allocated then 3: allocate memory for R ij and store R ij = a. To guarantee the theoretical correctness of this heuristics, we use the stochastic thresholding instead of the deterministic thresholding. When the algorithm requires to allocate a memory, it skip the allocation with some probability depending on the value (a small value should be skipped with high probability). Intuitively, if the value is very small, the allocation may be skipped. However, if there are many small values, one of them may be allocated, and hence the error is bounded stochastically. The following proposition shows this fact (whose proof will be given in Appendix). PROPOSITION 4.4. Let a 1 , a 2 , . . . be a nonnegative sequence and let A = \u221e i=1 a i < \u221e. Let \u03b2 > 0. Let\u00c3 be the sum of these values with the stochastic thresholding where the skip probability is p(a i ) = min{1, \u03b2a i }. Then we have\nR ij \u2190 R ij + a.\nP{A \u2212\u00c3 \u2265 \u03b4} \u2264 exp(\u2212\u03b2\u03b4).\n(4.10)\nWe implement the stochastic thresholding (Algorithm 14) in Gauss-Southwell algorithm (Algorithm 12). Then, theoretical guarantee in Proposition 4.3 is modified as follows.\nPROPOSITION 4.5. Let \u03b3 \u2208 [0, 1) be an accuracy parameter, \u03b2 be a skip parameter, andS be the approximate SimRank obtained by Algorithm 12 using stochastic thresholding with\n\u01eb = (1 \u2212 c)(1 \u2212 \u03b3)\u03b8. Then we have 0 \u2264 S ij \u2212S ij and P{S ij \u2212S ij \u2264 (1 \u2212 \u03b3)\u03b8 + \u03b4} \u2264 exp(\u2212(1 \u2212 c)\u03b2\u03b4) (4.11) for all i, j \u2208 V .\nThus, by letting \u03b2 \u221d 1/\u03b4, we can reduce the misclassification probability arbitrary small. We experimentally evaluate the effect of this technique in Section 4.4", "publication_ref": ["b29"], "figure_ref": [], "table_ref": []}, {"heading": "Verification phase", "text": "In the previous section, we described the filter phase for enumerating the similar-pair candidates. In this section, we discuss the verification phase for deciding whether each candidate (i, j) is actually similar. It should be mentioned that this procedure for each pair can be performed in parallel, i.e., if there are M machines, the computational time is reduced to 1/M . Our verification algorithm is a Monte-Carlo algorithm based on the representation of the first meeting time (1.4). R samples of the first meeting time \u03c4 (1) (i, j), . . . , \u03c4 (R) (i, j) are obtained from R random walks and the SimRank score is then estimated by the Algorithm 15 Verification procedure. 1: procedure VERIFICATION(i, j; \u03b8, p, R max ) 2: for R = 1, . . . , R max do 3:\nPerform two independent random walks to obtain the first meeting time \u03c4 (r) (i, j).\n4: s (R) = (1/R) R\nk=1 \u03c4 (k) (i, j).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "5:", "text": "\u03b4 (R) = |s (R) \u2212 \u03b8|.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "6:", "text": "if R\u03b4 (R)2 is large, i.e., (4.15) holds then end if 15: end procedure sample average: ,j) . (4.12) To accurately estimate s(i, j) by (4.12), many samples (i.e., r \u2265 1000) are required [Fogaras and R\u00e1cz 2005]. However, to decide whether s(i, j) is greater than or smaller than the threshold \u03b8, much fewer samples are required.\ns(i, j) \u2248 s (R) (i, j) := 1 R R k=1 c \u03c4 (k) (i\nPROPOSITION 4.6. Let \u03b4 (R) = |s (R) (i, j) \u2212 \u03b8|. If s(i, j) \u2265 \u03b8 then\nP{s (R) (i, j) < \u03b8} \u2264 exp \u22122R\u03b4 (R)2 1 \u2212 c c 2 .\n(4.13) Similarly, if s(i, j) \u2264 \u03b8 then\nP{s (R) (i, j) > \u03b8} \u2264 exp \u22122R\u03b4 (R)2 1 \u2212 c c 2 .\n(4.14)\nThe proof will be given in Appendix. This shows that, if s(i, j) is far from the threshold \u03b8, it can be decided with a small number of Monte-Carlo samples. Now we describe our algorithm. We optimize the number of Monte-Carlo samples, by adaptively increasing the samples. Starting from R = 1, our verification algorithm loops through the following procedure. The algorithm obtains s (R) (i, j) by performing two random walks and computing \u03c4 (R) (i, j). It then checks the condition\nR\u03b4 (R)2 \u2265 log(1/p) 2 c 1 \u2212 c 2 , (4.15)\nwhere p \u2208 (0, 1) is a tolerance probability for misclassification. If this condition is satisfied, we determine s(i, j) < \u03b8 or s(i, j) > \u03b8, The misclassification probability of this decision is at most p by Proposition 4.6. We now evaluate the number of samples required in this procedure. By taking expectation of (4.15), we obtain the expected number of iterations R end as\nE[R end ] \u2265 1 (s(i, j) \u2212 \u03b8) 2 log(1/p) 2 c 1 \u2212 c 2 , (4.16)\nThis shows that the number of samples quadratically depends on the inverse of the difference between the score s(i, j) and the threshold \u03b8. In practice, we set an upper bound R max of R, and terminate the iterations after R max steps. Our verification algorithm is shown in Algorithm 15.", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section, we perform numerical experiments to evaluate the proposed algorithm. We used the datasets shown in Table VIII. These are obtained by Stanford Large Network Dataset Collection 13 .\nAll experiments were conducted on an Intel Xeon E5-2690 2.90GHz CPU (32 cores) with 256GB memory running Ubuntu 12.04. The proposed algorithm was implemented in C++ and was compiled using g++v4.6 with the -O3 option. We have used OpenMP for parallel implementation.\n4.4.1. Scalability. We first show that the proposed algorithm is scalable. For real datasets, we compute the number of pairs that has the SimRank score greater than \u03b8 = 0.2, where the parameters of the algorithm are set to the following.\n-For the filter phase, accuracy parameter for the Gauss-Southwell algorithm (Algorithm 12) is set to \u03b3 = 0, and the skip parameter \u03b2 for the stochastic thresholding (Algorithm 14) is set to \u03b2 = 100. -For the verification phase, the tolerance probability p is set to p = 0.01 and the maximum number of Monte-Carlo samples is set to R max = 1000.\nThe result is shown in Table VIII, which is the main result of this paper. Here, \"dataset\", \"|V |\", and \"|E|\" denote the statistics of dataset, \"|J L |\" and \"|J H |\" denote the size of J L and J H in (4.1), \"estimate\" denotes the number of similar pairs estimated by the algorithm, \"filter\" and \"verification\" denote the real time needed to compute filter and verification step, and \"memory\" denotes the allocated memory during the algorithm. This shows the proposed algorithm is very scalable. In fact, it can find the SimRank join for the networks of 5M vertices and 68M edges (\"soc-LiveJournal\") in a 1000 seconds with 23 GB memory.\nLet us look into the details. The computational time and the allocated memory depend on the number of the similar pairs, and even if the size of two networks are similar, the number of similar pairs in these networks can be very different. For example, the largest instance examined in the experiment is \"soc-LiveJournal\", which has 5M vertices and 68M edges. However, since it has a small number of similar pairs, we can compute the SimRank join. On the other hand, \"web-BerkStan\" dataset, which has only 0.6M vertices and 7M edges, we cannot compute the SimRank join because it has too many similar pairs. 4.4.2. Accuracy. Next, we verify the accuracy (and the correctness) of the proposed algorithm. We first compute the exact SimRank scores S * by using the original SimRank algorithm. Then, we compare the exact scores with the solution S obtained by the proposed algorithm. Here, the accuracy of the solution is measured by the precision, the recall, and the F-score [Baeza-Yates et al. 1999], defined by\nprecision = |S \u2229 S * | |S| , recall = |S \u2229 S * | |S * | , F = 2|S \u2229 S * | |S| + |S * | .\nWe use the same parameters as the previous subsection. Since all-pairs SimRank computation is expensive, we used relatively small datasets for this experiment.\nThe result is shown in Table IX. This shows the proposed algorithm is very accurate; the obtained solutions have about precision \u2248 97%, recall \u2248 92%, and F-score \u2248 95%. Since the precision is higher than the recall, the algorithm produces really similar pairs.", "publication_ref": ["b4"], "figure_ref": [], "table_ref": ["tab_3", "tab_3", "tab_9"]}, {"heading": "4.4.3.", "text": "Comparison with the state-of-the-arts algorithms. We then compare the proposed algorithm with some state-of-the-arts algorithms. For the proposed algorithm, we used the same parameters described in the previous section. For the state-of-the-arts algorithms, we implemented the following two algorithms 14 :\n(1) Yu et al.  We chose the parameters of these algorithms to hold similar accuracy (i.e., F-score \u2248 95%). We used 7 datasets (3 small and 4 large datasets). For small datasets, we also compute the exact SimRank scores and evaluate the accuracy. For large datasets, we only compare the scalability.\nThe result is shown in Table X; each cell denotes the computational time, the allocated memory, and the F-score (for small datasets) or the number of similar pairs (for large datasets), respectively. \"-\" denotes the algorithm did not return a solution within 3 hour and 256 GB memory.\nThe results of small instances show that the proposed algorithm performs the same level of accuracy to the existing algorithms with requiring much smaller memory and comparable time. For large instances, the proposed algorithm outperforms the existing algorithms both in time and space, because the complexity of the proposed algorithm depends on the number of similar pairs whereas the complexity of existing algorithms depends on the number of pairs, O(n 2 ). This shows the proposed algorithm is very scalable than the existing algorithms.\nMore precisely, the algorithm is efficient when the number of similar pairs is relatively small. For example, in email-Enron and web-Google datasets, which have many similar pairs, the performance of the proposed algorithm close to the existing algorithms. On the other hand, in p2p-Gnutella31 and cit-patent dataset, it clearly outperforms the existing algorithms.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_11"]}, {"heading": "Experimental analysis of our algorithm.", "text": "In the previous subsections, we observed that the proposed algorithm is scalable and accurate. Moreover, it outperforms the existing algorithms. In this subsection, we experimentally evaluate the behavior of the algorithm. We first evaluate the dependence with the accuracy parameter \u03b3 used in the Gauss-Southwell algorithm. We vary \u03b3 and compute the lower set J L and the upper set J H . The result is shown in Table XI; each cell denotes the computational time, the allocated memory, and the size of J L and J H , respectively. This shows, to obtain an accurate upper and lower sets, we need to set \u03b3 \u2265 0.9. However, it requires 5 times longer computational time and 10 times larger memory. Since the verification phase can be performed in parallel, to scale up for large instances, it would be nice to set a small \u03b3 (e.g., \u03b3 = 0).\nNext, we evaluate the dependence with the probability parameter \u03b2 in the stochastic thresholding. We vary the parameter \u03b2 and evaluate the used memory. The result is shown in Table XII. The column for \u03b2 = \u221e shows the result without this technique. Thus we compare other columns with this column. The result shows that the effect of memory-reducing technique greatly depends on the network structure. However, for an effective case, it reduces memory about 1/2. By setting \u03b2 = 100, we can obtain almost the same result as the result without the technique.  Finally, we evaluate the effectiveness of the adaptive Monte-Carlo samples technique in the verification phase. We plot the histogram of the number of required samples in Figure 4.1. Here, the bar at 1,000 denotes the candidates that cannot be decided in 1,000 samples.\nThe result shows that most of small candidates are decided in 200 samples, and 1/3 of samples cannot decided in 1,000 samples. Therefore, this implies the technique of adaptive Monte-Carlo samples reduces the computational cost in factor about 1/3. 4.4.5. Number of similar pairs. Cai et al. ] claimed that the SimRank scores of a network follows power-law distributions. However, they only verified this claim in small networks (at most 10K vertices).\nWe here verify this conjecture in larger networks. We first enumerate the pairs with SimRank greater than \u03b8 = 0.001, and then compute the SimRank score by the Monte-Carlo algorithm. The result is shown in Figure 4.2. This shows, for each experimented dataset, the number of similar pairs follows power-law distributions in this range; however, these exponent (i.e., the slope of each curve) are different. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_11", "tab_11"]}, {"heading": "APPENDIX", "text": "In this appendix, we provide details of propositions and proofs mentioned in the main body of our paper. We first introduce a vectorized form of SimRank, which is convenient for analysis. Let \u2297 be the Kronecker product of matrices, i.e., for n \u00d7 n matrices A = (A ij ) and B,\nA \u2297 B = \uf8ee \uf8ef \uf8f0 A 11 B \u2022 \u2022 \u2022 A 1n B . . . . . . . . . A n1 B \u2022 \u2022 \u2022 A nn B \uf8f9 \uf8fa \uf8fb .\nLet \"vec\" be the vectorization operator, which reshapes an n \u00d7 n matrix to an n 2 vector, i.e., vec By applying the vectorization operator and using (5.1), we obtain I \u2212 cP \u22a4 \u2297 P \u22a4 vec(S L (\u0398)) = vec(\u0398).\n(5.2) Thus, to prove Proposition 5.1, we only have to prove that the coefficient matrix I \u2212 cP \u22a4 \u2297 P \u22a4 is non-singular. Since P \u22a4 \u2297 P \u22a4 is a (left) stochastic matrix, its spectral radius is equal to one. Hence all eigenvalues of I \u2212 cP \u22a4 \u2297 P \u22a4 are contained in the disk with center 1 and radius c in the complex plane. Therefore I \u2212 cP \u22a4 \u2297 P \u22a4 does not have a zero eigenvalue, and hence I \u2212 cP \u22a4 \u2297 P \u22a4 is nonsingular.\nWe now prove Proposition 2.3, which is the basis of our diagonal estimation algorithm.\nPROOF OF PROPOSITION 2.3. Let us consider linear system (5.2) and let Q := P \u22a4 \u2297 P \u22a4 . We partite the system (5.2) into 2\u00d72 blocks that correspond to the diagonal entries and the others:\nI \u2212 cQ DD \u2212cQ DO \u2212cQ OD I \u2212 cQ OO 1 X = diag(D) 0 , (5.3)\nwhere Q DD , Q DO , Q OD , and Q OO are submatrices of Q that denote contributions of diagonals to diagonals, diagonals to off-diagonals, off-diagonals to diagonals, and offdiagonals to off-diagonals, respectively. X is the off-diagonal entries of S L (D). To prove Proposition 2.3, we only have to prove that there is a unique diagonal matrix D that satisfies (5.3).\nWe observe that the block-diagonal component I \u2212 cQ OO of (5.3) is non-singular, which can be proved similarly as in Proposition 5.1. Thus, the equation (5.3) is uniquely solved as (I \u2212 cQ DD \u2212 c 2 Q DO (I \u2212 cQ OO ) \u22121 Q OD )1 = diag(D),\n(5.4) which is a closed form solution of diagonal correction matrix D.\nRemark 5.2. It is hard to compute D via the closed form formula (5.4) because the evaluation of the third term of the left hand side of (5.4) is too expensive.\nOn the other hand, we can use (5.4) to obtain a reasonable initial solution for Algorithm 4. By using first two terms of (5.4), we have diag(D) \u2248 1 \u2212 cQ DD 1, (5.5) which can be computed in O(m) time.\nOur additional experiment shows that the initial solution (5.5) gives a slightly better (at most twice) results than the trivial guesses D = I and D = 1 \u2212 c.\nWe give a convergence proof of our diagonal estimation algorithm (Algorithm 4). As mentioned in Subsection 2. (5.6) LEMMA 5.3. Consider two independent random walks start from the same vertex i and follow their in-links. Let p i (t) be the probability that two random walks meet t-th step (at some vertex). Let \u2206 := max i { \u221e t=1 c t p i (t)}. If \u2206 < 1 then the coefficient matrix of (5.6) is diagonally dominant. PROOF. By definition, each diagonal entry S L (E (j,j) ) jj is greater than or equal to one. For the off diagonals, we have i:i =j S L (E (i,i) ) jj = i:i =j \u221e t=1 c t (P t e j ) \u22a4 E (i,i) (P t e j ) \u2264 \u221e t=1 c t (P t e j ) \u22a4 (P t e j ) = \u221e t=1 c t p j (t) \u2264 \u2206.\nThis shows that if \u2206 < 1 then the matrix is diagonally dominant. Remark 5.5. Let us observe that, in practice, the assumption \u2206 < 1 is not an issue. For a network of average degree d, the probability p i (t) is expected to 1/d t . Therefore \u2206 = t c t p i (t) \u2243 (c/d)/(1 \u2212 (c/d)) \u2264 1/(d \u2212 1) < 1. This implies that Algorithm 4 converges quickly when the average degree is large.\nWe now prove Proposition 2.4. We use the following lemma.\nLEMMA 5.6. Let k (t) 1 , . . . , k (t) R be positions of t-th step of independent random walks that start from a vertex k and follow ln-links. Let X (t)   By applying e i and e j , we have\nk := (1/R)\nS L (\u2206) ij = c(P e i ) \u22a4 S L (\u2206)(P e j ) + \u2206 ij \u2264 c sup i \u2032 ,j \u2032 S L (\u2206) i \u2032 j \u2032 + \u01eb.\nHere, we used p \u22a4 Aq \u2264 sup ij A ij for any stochastic vectors p and q. Therefore\n(1 \u2212 c) sup i,j S L (\u2206) ij \u2264 \u01eb.\nBy the same argument, we have\n(1 \u2212 c) inf i,j S L (\u2206) ij \u2265 \u2212\u01eb.\nBy combining them, we obtain the proposition. Therefore P {|\u03b3(u, t) \u2212 \u03b3(u, t)| \u2265 \u01eb} \u2264 P |\u03b3(u, t) 2 \u2212 \u03b3(u, t) 2 | \u2265 \u01eb \u03b3(u, t) + \u03b3(u, t) \u2264 P |\u03b3(u, t) 2 \u2212 \u03b3(u, t) 2 | \u2265 \u01eb/2 \u2264 4n exp(\u2212\u01eb 2 R/8).\nHere we use the fact that both\u03b3(u, t) and \u03b3(u, t) are smaller than \u221a max w D ww = 1.\nPROOF PROOF OF PROPOSITION 4.2. We prove that Algorithm 12 converges and also estimate the number of iterations.\nLet A, B := ij A ij B ij = tr(A \u22a4 B) be the inner product of matrices. Note that AB, C = B, A \u22a4 C = A, CB \u22a4 . We introduce a potential function of the form \u03a6(t) := U, R (t) ,\n(5.8)\nwhere U is a strictly positive matrix (determined later). Since both U and R (t) are nonnegative, the potential function \u03a6(t) is also nonnegative. Moreover, \u03a6(t) = 0 if and only if R (t) = O since U is strictly positive. To prove the convergence, we prove that the potential function monotonically decreases. More precisely, we prove that a strictly positive matrix U exists for which the corresponding potential function decreases monotonically.\nLet us observe that\n\u03a6(t + 1) \u2212 \u03a6(t) = U, \u2212R (t) ij E ij + cR (t) ij P \u22a4 E ij P = \u2212R (t) ij U \u2212 cP U P \u22a4 , E ij .\n(5.9)\nRecall that, by the algorithm, R\nij > \u01eb. Thus, to guarantee \u03a6(t + 1) \u2212 \u03a6(t) < 0, it suffices to prove the existence of matrix U satisfying U > 0, U \u2212 cP U P \u22a4 > O,\n(5.10)\nwhere U > O denotes that all entries of the matrix U is strictly positive. We explicitly construct this matrix. Let E be the all-one matrix. Then the matrix U := E + cP EP \u22a4 + c 2 P 2 EP \u22a42 + \u2022 \u2022 \u2022 .\n(5.11) satisfies (5.10) as follows. First, U is strictly positive because the first term in (5.11) is strictly positive and the other terms are nonnegative. Second, since U \u2212 cP U P \u22a4 = E, (5.12) it is also strictly positive. Therefore, using this matrix U in (5.8), we can obtain that the potential function \u03a6(t) is nonnegative and is strictly monotonically decreasing. This proves the convergence of \u03a6(t) \u2192 0. Furthermore, since \u03a6(t) = 0 implies R (t) = O, this proves the convergence of R (t) \u2192 O.\nLet us bound the number of iterations. From the above analysis, we obtain\n\u03a6(t + 1) \u2212 \u03a6(t) = \u2212R (t)\nij < \u2212\u01eb.\n(5.13) Thus the number of iterations is bounded by O(\u03a6(0)/\u01eb). The rest of the proof, we bound \u03a6(0). For the initial solution R (0) = D, we have U, R (0) = U, D = If \u03b2a i \u2265 1 for some 1 \u2264 i \u2264 k, it must not be skipped, therefore the proposition holds. Otherwise, the probability is given as\nP{A \u2212\u00c3 \u2265 \u03b4} = (1 \u2212 \u03b2a 1 ) \u2022 \u2022 \u2022 (1 \u2212 \u03b2a k ).\n(5.16)\nBy the arithmetic mean-geometric mean inequality, we have\n(1 \u2212 \u03b2a 1 ) \u2022 \u2022 \u2022 (1 \u2212 \u03b2a k ) \u2264 1 \u2212 1 k k i=1 \u03b2a i k \u2264 1 \u2212 \u03b2\u03b4 k k \u2264 exp(\u2212\u03b2\u03b4).\nTherefore the proposition holds.\nPROOF PROOF OF PROPOSITION 4.5. For each iteration, we have the following invariant: D \u2212 S (t) \u2212 cP \u22a4 S (t) P = R (t) +R (t) ,\n(5.17) whereR (t) is the skipped values by the stochastic thresholding. Using this invariant, this proposition follows from the similar proof as Proposition 4.3 with Proposition 4.4.\nPROOF PROOF OF PROPOSITION 4.6. Since E[s (R) (i, j)] = s(i, j), by the Hoeffding inequality, we have P{s (R) (i, j) \u2265 s(i, j) + \u01eb} \u2264 exp(\u22122R\u01eb 2 ).\n(5.18) Therefore P{s (R) (i, j) \u2265 \u03b8} \u2264 exp(\u22122R\u01eb 2 ).\n(5.19)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "The above proposition shows that if diagonal correction matrix D is accurately estimated, all entries of SimRank matrix S is accurately computed. PROOF OF PROPOSITION 3.4. Consider (P t e u ) \u22a4 D(P t e v ). Since P t e v is a stochastic vector, by (3.5), we have (P t e u ) \u22a4 D(P t e v ) \u2264 max w\u2208supp(P t ev ) (P t e u ) \u22a4 De w , (5.7) Since P t e v corresponds to a t-step random walk, the support is contained by a ball of radius t centered at v. Therefore we have\nBy plugging (3.6), we have\nSubstitute the above to (2.6), we obtain (3.8).\nPROOF OF PROPOSITION 3.8. Consider c t (P t e u ) \u22a4 D(P t e v ). By (3.9), we have\nSubstitute the above to (2.6), we obtain (3.11).\nLet us start the proof of concentration bounds. We first prepare some basic probablistic inequalities.\nLEMMA 5.8 (HOEFFDING'S INEQUALITY). Let X 1 , . . . , X R be independent random variables with\nLEMMA 5.9 (MAX-HOEFFDING'S INEQUALITY). For each f = 1, . . . , F , let X r (f ) (r = 1, . . . , R) be independent random variables with\nWe write u (t) r and v (t) r (r = 1, . . . , R) for the t-th positions of independent random walks start from u and v and follow the in-links, respectively, and\nLEMMA 5.11.\nPROOF OF PROPOSITION 3.1. By Lemma 5.11, we have\nPROOF OF PROPOSITION 3.6. We first prove the bound of \u03b1 (u, d, t). Note that \u03b1(u, d, t) = max w {P t e u De w } and the algorithm computes\u03b1(u, d, t) = max w {X (t)\u22a4 u De w }. By Lemmas 5.10 and 5.9, we have\nUsing the above estimation, we bound \u03b2 as\nPROOF OF PROPOSITION 3.9. We first observe that \u03b3(u, t) 2 = (P t e u ) \u22a4 D(P t e u ) and the algorithm estimates this value by (\u03b3(u, t))\nHence, by the same proof as Lemma 5.11, we have P |\u03b3(u, t) 2 \u2212 \u03b3(u, t) 2 | \u2265 \u01eb \u2264 4n exp(\u2212\u01eb 2 R/2).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Matrix Algebra", "journal": "Cambridge University Press", "year": "2005", "authors": "M Karim; Jan R Abadir;  Magnus"}, {"ref_id": "b1", "title": "A Recommender System Based on Local Random Walks and Spectral Methods", "journal": "Springer", "year": "2007", "authors": "Zeinab Abbassi; S Vahab;  Mirrokni"}, {"ref_id": "b2", "title": "Simrank++: query rewriting through link analysis of the click graph", "journal": "Proceedings of the VLDB Endowment", "year": "2008", "authors": "Ioannis Antonellis; Hector Garcia-Molina; Chi-Chao Chang"}, {"ref_id": "b3", "title": "Large-scale deduplication with constraints using dedupalog", "journal": "", "year": "2009", "authors": "Arvind Arasu; Christopher R\u00e9; Dan Suciu"}, {"ref_id": "b4", "title": "Berthier Ribeiro-Neto, and others", "journal": "ACM press New York", "year": "1999", "authors": "Ricardo Baeza-Yates"}, {"ref_id": "b5", "title": "On the resemblance and containment of documents", "journal": "", "year": "1997", "authors": "Z Andrei;  Broder"}, {"ref_id": "b6", "title": "Efficient Algorithm for Computing Link-Based Similarity in Real World Networks", "journal": "", "year": "2009", "authors": "Yuanzhe Cai; Gao Cong; Xu Jia; Hongyan Liu; Jun He; Jiaheng Lu; Xiaoyong Du"}, {"ref_id": "b7", "title": "S-SimRank: Combining Content and Link Information to Cluster Papers Effectively and Efficiently", "journal": "Springer", "year": "2008-06", "authors": "Yuanzhe Cai; Pei Li; Hongyan Liu"}, {"ref_id": "b8", "title": "The origin of power laws in Internet topologies revisited", "journal": "", "year": "2002", "authors": "Qian Chen; Hyunseok Chang; Ramesh Govindan; Sugih Jamin"}, {"ref_id": "b9", "title": "MassJoin: A mapreducebased method for scalable string similarity joins", "journal": "", "year": "2014", "authors": "Dong Deng; Guoliang Li; Shuang Hao; Jiannan Wang; Jianhua Feng"}, {"ref_id": "b10", "title": "Reference reconciliation in complex information spaces", "journal": "", "year": "2005", "authors": "Xin Dong; Alon Halevy; Jayant Madhavan"}, {"ref_id": "b11", "title": "A theory for record linkage", "journal": "J. Amer. Statist. Assoc", "year": "1969", "authors": "P Ivan; Alan B Fellegi;  Sunter"}, {"ref_id": "b12", "title": "Scaling link-based similarity search", "journal": "", "year": "2005", "authors": "D\u00e1niel Fogaras; Bal\u00e1zs R\u00e1cz"}, {"ref_id": "b13", "title": "Efficient search algorithm for SimRank", "journal": "", "year": "2013", "authors": "Yasuhiro Fujiwara; Makoto Nakatsuji; Hiroaki Shiokawa; Makoto Onizuka"}, {"ref_id": "b14", "title": "", "journal": "Matrix computations", "year": "2012", "authors": "H Gene; Charles F Golub;  Van Loan"}, {"ref_id": "b15", "title": "Combating Web Spam with TrustRank", "journal": "Morgan Kaufmann", "year": "2004", "authors": "Zolt\u00e1n Gy\u00f6ngyi; Hector Garcia-Molina; Jan O Pedersen"}, {"ref_id": "b16", "title": "Parallel SimRank computation on large graphs with iterative aggregation", "journal": "", "year": "2010", "authors": "Guoming He; Haijun Feng; Cuiping Li; Hong Chen"}, {"ref_id": "b17", "title": "The merge/purge problem for large databases", "journal": "In ACM SIGMOD Record", "year": "1995", "authors": "A Mauricio; Salvatore J Hern\u00e1ndez;  Stolfo"}, {"ref_id": "b18", "title": "SimRank: a measure of structural-context similarity", "journal": "", "year": "2002", "authors": "Glen Jeh; Jennifer Widom"}, {"ref_id": "b19", "title": "Bibliographic coupling extended in time: Ten case histories", "journal": "Information Storage and Retrieval", "year": "1963", "authors": " Maxwell Mirton;  Kessler"}, {"ref_id": "b20", "title": "Scalable similarity search for SimRank", "journal": "", "year": "2014", "authors": "Mitsuru Kusumoto; Takanori Maehara; Ken-Ichi Kawarabayashi"}, {"ref_id": "b21", "title": "Similarity join size estimation using locality sensitive hashing", "journal": "", "year": "2011", "authors": "Hongrae Lee; T Raymond; Kyuseok Ng;  Shim"}, {"ref_id": "b22", "title": "Fast computation of SimRank for static and dynamic information networks", "journal": "", "year": "2010", "authors": "Cuiping Li; Jiawei Han; Guoming He; Xin Jin; Yizhou Sun; Yintao Yu; Tianyi Wu"}, {"ref_id": "b23", "title": "Exploiting the Block Structure of Link Graph for Efficient Similarity Computation", "journal": "", "year": "2009-06", "authors": "Pei Li; Yuanzhe Cai; Hongyan Liu"}, {"ref_id": "b24", "title": "Fast Single-Pair SimRank Computation", "journal": "", "year": "2010", "authors": "Pei Li; Hongyan Liu; Jeffrey Xu Yu; Jun He; Xiaoyong Du"}, {"ref_id": "b25", "title": "The link-prediction problem for social networks", "journal": "JASIST", "year": "2007", "authors": "David Liben; - Nowell; Jon M Kleinberg"}, {"ref_id": "b26", "title": "PageSim: A Novel Link-Based Similarity Measure for the World Wide Web", "journal": "", "year": "2006", "authors": "Zhenjiang Lin; Irwin King; Michael R Lyu"}, {"ref_id": "b27", "title": "Extending Link-based Algorithms for Similar Web Pages with Neighborhood Structure", "journal": "", "year": "2007", "authors": "Zhenjiang Lin; Michael R Lyu; Irwin King"}, {"ref_id": "b28", "title": "MatchSim: a novel similarity measure based on maximum neighborhood matching", "journal": "Knowledge and Information Systems", "year": "2012", "authors": "Zhenjiang Lin; Michael R Lyu; Irwin King"}, {"ref_id": "b29", "title": "Accuracy estimate and optimization techniques for SimRank computation", "journal": "The VLDB Journal", "year": "2010", "authors": "Dmitry Lizorkin; Pavel Velikhov; Maxim N Grinev; Denis Turdakov"}, {"ref_id": "b30", "title": "Scalable SimRank join algorithm", "journal": "", "year": "2015", "authors": "Takanori Maehara; Mitsuru Kusumoto; Ken-Ichi Kawarabayashi"}, {"ref_id": "b31", "title": "Efficient set joins on similarity predicates", "journal": "", "year": "2004", "authors": "Sunita Sarawagi; Alok Kirpal"}, {"ref_id": "b32", "title": "Sentiment Translation through Lexicon Induction", "journal": "", "year": "2010", "authors": "Christian Scheible"}, {"ref_id": "b33", "title": "Video Google: A text retrieval approach to object matching in videos", "journal": "", "year": "2003", "authors": "Josef Sivic; Andrew Zisserman"}, {"ref_id": "b34", "title": "Co-citation in the scientific literature: A new measure of the relationship between two documents", "journal": "Journal of the American Society for Information Science", "year": "1973", "authors": "Henry Small"}, {"ref_id": "b35", "title": "Relaxation Methods in Engineering Science", "journal": "Oxford University Press", "year": "1940", "authors": "Richard Vynne; Southwell "}, {"ref_id": "b36", "title": "Relaxation Methods in Theoretical Physics", "journal": "Oxford University Press", "year": "1946", "authors": "Richard Vynne; Southwell "}, {"ref_id": "b37", "title": "Gaussian Elimination is not Optimal", "journal": "Numer. Math", "year": "1969", "authors": " Volker Strassen"}, {"ref_id": "b38", "title": "On link-based similarity join. Proceedings of the VLDB Endowment", "journal": "", "year": "2011", "authors": "Liwen Sun; Xiang Cheng;  Li; Jiawei Cheung;  Han"}, {"ref_id": "b39", "title": "Similarity indexing with the SS-tree", "journal": "", "year": "1996", "authors": "A David; Ramesh White;  Jain"}, {"ref_id": "b40", "title": "Multiplying matrices faster than Coppersmith-Winograd", "journal": "ACM", "year": "2012", "authors": "Virginia Vassilevska; Williams "}, {"ref_id": "b41", "title": "Efficient similarity joins for near-duplicate detection", "journal": "ACM Transactions on Database Systems", "year": "2011", "authors": "Chuan Xiao; Wei Wang; Xuemin Lin; Jeffrey Xu Yu; Guoren Wang"}, {"ref_id": "b42", "title": "LinkClus: Efficient Clustering via Heterogeneous Semantic Links", "journal": "ACM", "year": "2006", "authors": "Xiaoxin Yin; Jiawei Han; Philip S Yu"}, {"ref_id": "b43", "title": "SimRate: Improve Collaborative Recommendation Based on Rating Graph for Sparsity", "journal": "Springer", "year": "2010", "authors": "Li Yu; Zhaoxin Shu; Xiaoping Yang"}, {"ref_id": "b44", "title": "Taming Computational Complexity: Efficient and Parallel SimRank Optimizations on Undirected Graphs", "journal": "", "year": "2010", "authors": "Weiren Yu; Xuemin Lin; Jiajin Le"}, {"ref_id": "b45", "title": "More is Simpler: Effectively and Efficiently Assessing Node-Pair Similarities Based on Hyperlinks", "journal": "", "year": "2013", "authors": "Weiren Yu; Xuemin Lin; Wenjie Zhang; Lijun Chang; Jian Pei"}, {"ref_id": "b46", "title": "A space and time efficient algorithm for SimRank computation", "journal": "World Wide Web", "year": "2012", "authors": "Weiren Yu; Wenjie Zhang; Xuemin Lin; Qing Zhang; Jiajin Le"}, {"ref_id": "b47", "title": "P-Rank: a comprehensive structural similarity measure over information networks", "journal": "", "year": "2009", "authors": "Peixiang Zhao; Jiawei Han; Yizhou Sun"}, {"ref_id": "b48", "title": "Efficient simrank-based similarity join over large graphs", "journal": "", "year": "2013", "authors": "Weiguo Zheng; Lei Zou; Yansong Feng; Lei Chen; Dongyan Zhao"}, {"ref_id": "b49", "title": "Graph Clustering Based on Structural/Attribute Similarities", "journal": "", "year": "2009", "authors": "Yang Zhou; Hong Cheng; Jeffrey Xu Yu"}, {"ref_id": "b50", "title": "Learning from labeled and unlabeled data with label propagation", "journal": "", "year": "2002", "authors": "Xiaojin Zhu; Zoubin Ghahramani"}], "figures": [{"figure_label": "11", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig. 1 . 1 .11Fig. 1.1. Example of the SimRank. c = 0.6.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "PROPOSITION 2.3. A diagonal matrix D is the diagonal correction matrix, i.e., S L (D) = S if and only if D satisfies S L (D) kk = 1, (k = 1, . . . , n), (2.9) where S L (D) kk denotes (k, k) entry of the linearized SimRank matrix S L (D).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Fig. 2.1. Number of iterations L vs. mean error of computed SimRank.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "FigFig. 3.1. Distance correlation of similarity ranking. The red points are for distance of top-1000 similar vertices and the blue line is for average distance of two vertices in each network", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "3.4.1. L1 bound.The first bound is based on the following inequality: for a vector x and a stochastic vector y,x \u22a4 y \u2264 max w\u2208supp(y)x \u22a4 e w , (3.5)where supp(y) := {w \u2208 V : y(w) > 0} is a positive support of y. We bound (P t e u ) \u22a4 D(P t e v ) by this inequality.Fix a query vertex u. Let us define \u03b1(u, d, t) := max w\u2208V,d(u,w)=d (P t e u ) \u22a4 De w (3.6) for d = 1, . . . , d max and t = 1, . . . , T , and \u03b2(u, d) := \u2032 \u2264d+t \u03b1(u, d \u2032 , t) (3.7) for t = 1, . . . , T . Here d max is distance such that if d(u, v) > d max then s(u, v) is too small to take into account. (We usually set d max = T ). PROPOSITION 3.4. For a vertex v with d(u, v) = d, we have", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "d) = T \u22121 t=0 c t max d\u2212t\u2264d \u2032 \u2264d+t \u03b1(u, d \u2032 , t) 2. L2 bound.The second bound is based on the Cauchy-Schwartz inequality: for nonnegative vectors x and y, x \u22a4 y \u2264 x y .(3.9)We also bound (P t e u )D(P t e v ) by this inequality. Let \u03b3(u, t) := \u221a DP t e u , (3.10)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "W 1 ,. . .,W Q from u 6: for t = 1, . . . , T do 7:if W jt = W kt for some j = k then", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "The overall time complexity of the query phase is O(RT |S|) where |S| is the number of candidates, since computing SimRank scores s(u, v) by Algorithm 6 for two vertices u, v takes O(RT ). The space complexity is O(m + nP ). Algorithm 10 Proposed algorithm (query) 1: procedure QUERY(u) 2: Enumerate S := {v|\u03b4 H (u left ) \u2229 \u03b4 H (v left ) = \u2205}", "figure_data": ""}, {"figure_label": "41", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Fig. 4 . 1 .41Fig. 4.1. Histogram of the number of required samples.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Fig. 4.2. The number of similar pairs.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "(A) n\u00d7i+j = a ij . Then we have the following relation, which is well known in linear algebra [Abadir and Magnus 2005]: vec(ABC) = (C \u22a4 \u2297 A)vec(B). (5.1) PROPOSITION 5.1. Linearized SimRank operator S L is a non-singular linear operator. PROOF. A linearized SimRank S L (\u0398) for a matrix \u0398 is a matrix satisfies relation S L (\u0398) = cP \u22a4 S L (\u0398)P + \u0398.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "3.1, Algorithm 4 is the Gauss-Seidel method [Golub and Van Loan 2012] for the linear system\uf8ee \uf8ef \uf8f0 S L (E (1,1) ) 11 \u2022 \u2022 \u2022 S L (E (n,n) ) 11 . . . . . . . . . S L (E (1,1) ) nn \u2022 \u2022 \u2022 S L (E(n,n)  ) nn", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_17", "figure_caption": "COROLLARY 5.4. If a graph G satisfies the condition of Lemma 5.3, Algorithm 4 converges with convergence rate O(\u2206 l ).PROOF. This follows the standard theory of the Gauss-Seidel method[Golub and Van Loan 2012].", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_18", "figure_caption": "k \u2212 P t e k \u2265 \u01eb \u2264 2 exp \u22122R\u01eb 2 .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_19", "figure_caption": "PROPOSITION 5.7. Let D = diag(D 11 , . . . , D nn ) andD = diag(D 11 , . . . ,D nn ) be diagonal matrices. If they satisfy sup k |D kk \u2212D kk | \u2264 \u01eb then sup i,j |S L (D) ij \u2212 S L (D) ij | \u2264 \u01eb 1 \u2212 c . PROOF. Let \u2206 := D \u2212D. Since S L is linear, we have S L (\u2206) = S L (D) \u2212 S L (D). Consider S L (\u2206) = cP \u22a4 S L (\u2206)P + \u2206.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_20", "figure_caption": "\u22a4t DP t = E, S = ij S ij .(5.14)For the third equality, we usedS = D + cP \u22a4 DP + \u2022 \u2022 \u2022 , (5.15)which follows from (2.2). This shows \u03a6(0) = ij S ij .PROOF PROOF OF PROPOSITION 4.3. When the algorithm terminates, we obtain a solutionS with residualR satisfying the following bound:R ij = D \u2212 (S \u2212 cP \u22a4S P ) ij \u2264 \u01eb.Recall thatR ij \u2265 0 by construction. We establish an error bound for the solutionS from the above bound of the residualR. Recall also that the SimRank matrix satisfies S \u2212 cP \u22a4 SP = D. Thus we have(S \u2212S) \u2212 cP \u22a4 (S \u2212S)P ij \u2264 \u01eb.We can evaluate the second term asP \u22a4 (S \u2212S)P ij \u2264 max ij (S ij \u2212S ij ).Therefore we obtainmax ij (S ij \u2212S ij ) \u2264 \u01eb 1 \u2212 c . PROOF PROOF OF PROPOSITION 4.4. Let k be the smallest index such that a 1 +\u2022 \u2022 \u2022+ a k > \u03b4. Then the error, A \u2212\u00c3, exceeds \u03b4 if and only if the first k values are skipped.", "figure_data": ""}, {"figure_label": "III", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Dataset information.", "figure_data": "Dataset|V ||E|ca-GrQc5,24214,496as200001026,47413,895Wiki-Vote7,155103,689ca-HepTh9,87725,998email-Enron36,692183,831soc-Epinions175,879508,837soc-Slashdot081177,360905,468soc-Slashdot090282,168948,464email-EuAll265,214400,045web-Stanford281,9032,312,497web-NotreDame325,7281,497,134web-BerkStan685,2307,600,505web-Google875,7135,105,049dblp-2011933,2586,707,236in-20041,382,90817,917,053flickr1,715,25522,613,981soc-LiveJournal4,847,57168,993,773indochina-20047,414,866194,109,311it-200441,291,549 1,150,725,436twitter-201041,652,230 1,468,365,182uk-2007-05105,896,555 3,738,733,648"}, {"figure_label": "VI", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Accuracy.", "figure_data": "4. SIMRANK JOIN 1DatasetThreshold Proposed Fogaras and R\u00e1cz [Fogaras and R\u00e1cz 2005]ca-GrQc0.040.986650.923290.050.988540.924670.060.994610.952250.070.995540.92881as200001020.040.978310.946430.050.987270.947830.060.991770.947130.070.995500.94760wiki-Vote0.040.818620.934910.050.886290.937600.060.908010.942150.070.947850.97916ca-HepTh0.040.971420.889640.050.987820.943540.060.996730.918480.070.997460.93647DatasetProposed[Fogaras and R\u00e1cz 2005][Yu et al. 2012]Preproc. Query AllPairsIndex Preproc. QueryIndex AllPairs Memoryca-GrQc1.5 s 2.6 ms13.5 s 2.4 MB 110 ms 0.11 ms 22.7 MB2.97 s 66 MBas200001021.6 s 18 ms115 s 3.3 MB81 ms 1.3 ms 28.0 MB0.13 s 51 MBWiki-Vote1.9 s 3.8 ms26.9 s 5.3 MB 110 ms 0.41 ms 31.1 MB8.74 s 138 MBca-HepTh1.8 s 3.3 ms32.3 s 4.5 MB 253 ms 0.44 ms 43.3 MB23.3 s 312 MBemail-Enron7.8 s 24 ms864 s 21.6 MB 949 ms 1.1 ms 158 MB302 s 3.45 GBsoc-Epinions119.5 s 44 ms 3335 s 18.9 MB1.8 s 1.4 ms 332 MB777 s 6.91 GBsoc-Slashdot081120.2 s 53 ms 4081 s 48.6 MB1.9 s 1.2 ms 341 MB747 s 7,34 GBsoc-Slashdot090221.3 s 55 ms 4515 s 51.2 MB2.0 s 1.3 ms 363 MB694 s 7.21 GBemail-EuAll55.6 s 226 ms-103 MB3.6 s14 ms 1.1 GB--web-Stanford69.6 s 103 ms-141 MB10.4 s10 ms 1.2 GB--web-NotreDame60.6 s 73 ms-163 MB7.6 s 2.8 ms 1.4 GB--web-BerkStan240.2 s 93 ms-288 MB47.4 s 4.3 ms 3.8 GB--web-Google156.7 s 199 ms-211 MB24.0 s13 ms 3.0 GB--dblp-201182.2 s 16 ms-144 MB16.4 s 1.3 ms 1.4 GB--in-2004292.9 s 95 ms-451 MB46.4 s 6.8 ms 6.0 GB--flickr622.7 s1.5 s-513 MB90.1 s 7.6 ms 7.4 GB--soc-LiveJournal 2335.9 s 431 ms-1.2 GB 397.9 s27 ms 21.6 GB--indochina-20041585.2 s 714 ms-2.1 GB-----it-20043.1 h2.3 s-11.2 GB-----twitter-20107.7 h 17.4 s-8.4 GB------"}, {"figure_label": "VIII", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Scalability of the proposed algorithm.", "figure_data": "dataset|V ||E||J L ||J H |estimatefilter verification memoryamazon0302261,1111,234,8771,0591,338,6773,3555.3 s5.3 s505 MBamazon0312400,7273,200,4403,2291,579,3316,56340.8 s4.8 s2.6 GBamazon0505410,2363,356,8243,8981,174,5407,95115.6 s3.8 s1.8 GBamazon0601403,3943,387,3885,1391,289,19010,37516.3 s4.3 s1.8 GBas-caida26,475106,7622,141,6949,281,2512,601,6206.1 s84.0 s776 MBas-skitter1,696,41611,095,2983,386,71323,240,9124,327,637223.3 s54.7 s3.8 GBas200001026,47413,895276,5861,309,773378,3120.8 s9.2 s108 MBca-AstroPh18,772396,1601,33131,2572,5521.3 s0.4 s44 MBca-CondMat23,133186,9363,10075,6075,9690.2 s0.5 s32 MBca-GrQc5,24228,9801,49717,6202,6961.4 s0.1 s3 MBca-HepPh12,008237,0101,94832,4363,4461.4 s0.4 s19 MBca-HepTh9,87751,9712,46132,7504,3490.05 s0.1 s8 MBcit-HepPh34,546421,5787,827218,29110,7209.5 s0.5 s262 MBcit-HepTh27,770352,8075,441126,0575,4414.6 s0.3 s250 MBcit-Patents3,774,76816,518,948151,0964,406,829209,79342.7 s7.1 s3.6 GBcom-amazon334,863925,87296,2572,193,701164,9674.0 s14.3353 MBcom-dblp317,0801,049,866136,1761,862,027222,8534.2 s14.0 s448 MBemail-Enron36,692367,6621,204,9422,919,1241,348,7434.1 s14.3 s470 MBemail-EuAll265,214420,045131,109,661151,333,618133,693,68584.5 s116.4 s10.6 GBp2p-Gnutella0410,87639,9943,05332,2263,9700.1 s0.2 s14 MBp2p-Gnutella058,84631,8392,52727,1013,3520.06 s0.1 s12 MBp2p-Gnutella068,71731,5252,90226,6393,6910.07 s0.2 s11 MBp2p-Gnutella086,30120,7773,58225,7724,5210.05 s0.1 s8 MBp2p-Gnutella098,11426,0135,09134,1426,2340.06 s0.3 s10 MBp2p-Gnutella2426,51865,36923,894131,93930,0720.2 s0.8 s26 MBp2p-Gnutella2522,68754,70521,893114,38826,9390.1 s1.0 s20 MBp2p-Gnutella3036,68288,32841,164192,30749,6440.5 s1.4 s33 MBp2p-Gnutella3162,586147,89271,054334,95086,3670.6 s2.1 s57 MBsoc-Epinions175,879508,837205,6501,201,677252,0126.0 s7.9 s462 MBsoc-LiveJournal4,847,57168,993,7733,098,59730,715,4793,975,507640.4 s347.8 s23 GBsoc-Slashdot081177,360905,4682,1261,099,58415,4667.1 s9.7 s639 MBsoc-Slashdot090282,168948,4641,9041,072,62511,4477.4 s9.2 s627 MBsoc-pokec1,632,80330,622,564314,4763,739,302406,539134.4 s43.1 s6.5 GBweb-BerkStan685,2307,600,595------web-Google875,7135,105,0398,054,397100,247,73711,416,471262.9 s361.0 s24.7 GBweb-NotreDame325,7291,497,13410,206,00978,006,94910,895,99077.7 s188.5 s9.3 GBweb-Stanford281,9032,312,49721,438,881427,602,78825,890,0221519.6 s1688.2 s61.3 GBwiki-Talk2,394,3865,021,4102,647,9676,703,4682,965,75223.4 s16.5 s1.2 GBwiki-Vote7,115103,58910,42054,61312,5810.3 s0.1 s26 MB"}, {"figure_label": "IX", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Accuracy of the proposed algorithm.", "figure_data": "datasetexact obtained precision recall F-scoreas20000102394026 37787499%95%97%as-caida2727608 260178499%95%97%ca-CondMat6188596496%93%95%ca-GrQc2707269497%96%97%ca-HepTh4454434099%97%98%p2p-Gnutella30 537524949999%92%95%p2p-Gnutella31 986988680999%87%93%wiki-Vote131991256994%90%92%"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "]'s all-pairs SimRank algorithm. This algorithm computes all-pairs SimRank in O(nm) time and O(n 2 ) space. As discussed in, we combine thresholding heuristics that discard small values (say 0.01) in the Sim-Rank matrix for each iteration. For the SimRank join, we first apply this algorithm and then output the similar pairs.(2) Fogaras and Racz [Fogaras and R\u00e1cz 2005]'s random-walk based single-source Sim-Rank algorithm. This algorithm computes single-source SimRank in O(m) time and O(nR) space, where R is the number of Monte-Carlo samples.We set the number of Monte-Carlo samples R = 1000. For the SimRank join, we perform single-source SimRank computations for all seed vertices in parallel, and then output the similar pairs.", "figure_data": ""}, {"figure_label": "X", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Comparison of algorithms.", "figure_data": "datasetproposedFogaras&RaczYu et al.10.0 s18.4 s7.4 sas20000102108 MB764 MB289 MB97%98%99%1.5 s1.9 s1.9 sca-GrQc3 MB232 MB48 MB97%97%99%0.4 s1.7 s12.1 swiki-Vote26.MB293 MB343 MB91%91%88%2.7 s19.6 s33.8 sp2p-Gnutella3157 MB2.8 GB1.2 GB86,53783,66686,665633.0 s1569.3 s6537.6 sweb-Google24 GB36.6 GB151.6 GB11,414,97111,444,00910,980,706182.9 s2538.7 s-soc-pokec6 GB76.6 GB-406,981404,840-52.7 s--cit-patent3.6 GB--209,900--Table XI. Dependency of accuracy parameter \u03b3.dataset0.0\u03b3 0.50.96.8 s10.4 s33.6 sas-caida777 MB1.1 GB2.0 GB[2.1M, 9.2M] [2.2M, 5.7M] [2.5M, 2.7M]4.1 s9.0 s20.2 scom-amazon355 MB485 MB677 MB[96K, 2.1M] [119K, 529K] [144K, 178K]4.0 s5.1 s15.8 semail-Enron470 MB500 MB586 MB[1.2M, 2.9M] [1.2M, 1.8M] [1.2M, 1.4M]6.1 s6.9 s22.5 ssoc-Epinions1462 MB508 MB785 MB[205K, 1.2M] [206K, 472K] [208K, 291K]0.3 s0.5 s1.8 swiki-Vote26 MB27 MB33 MB[10K, 54K][10K, 22K][10K, 14K]"}, {"figure_label": "XII", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Dependency of the parameter \u03b2 for the stochastic thresholding.", "figure_data": "dataset\u221e1000\u03b3100107.0 s6.4 s5.8 s4.8 sas-caida879 MB868 MB776 MB681 MB[2.1M,9.3M] [2.1M,9.3M] [2.1M,9.2M] [2.1M,8.3M]4.8 s4.3 s3.9 s2.0 scom-amazon458 MB437 MB355 MB156 MB[97K,2.2M] [97K,2.2M] [96K,2.1M] [90K,1.3M]4.7 s4.5 s3.6 s2.4 semail-Enron746 MB677 MB470 MB245 MB[1.2M,2.9M] [1.2M,2.9M] [1.2M,2.9M] [1.2M,2.5M]9.9 s8.8 s5.6 s3.2 ssoc-Epinions11.1 GB954 MB462 MB135 MB[205K,1.2M] [205K,1.2M] [205K,1.2M] [204K,968K]0.6 s0.5 s0.5 s0.5 swiki-Vote68 MB56 MB26 MB7 MB[10K,54K] [10K,54K] [10K,54K] [10K,43K]700,000email-Enron600,000soc-Epinions1Number of samples200,000 300,000 400,000 500,000p2p-Gnutella31100,00001002003004005006007008009001,000Frequency"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "PROOF. Since E[e k (t)r ] = P t e k , this is a direct application of the Hoeffding's inequality.PROOF OF PROPOSITION 2.4. Since p", "figure_data": "e \u22a4 i X(t) k . Thus we have(t) ki defined by (2.12) is represented by p(t) ki =P P t e k \u2212 p(t) k> \u01eb \u2264 nP e \u22a4 i P t e k \u2212 p(t) ki > \u01eb\u2264 2n exp \u22122R\u01eb 2 ."}], "formulas": [{"formula_id": "formula_0", "formula_text": "s(i, j) := \uf8f1 \uf8f2 \uf8f3 1, i = j, c |\u03b4(i)||\u03b4(j)| i \u2032 \u2208\u03b4(i),j\u2208\u03b4(j) s(i \u2032 , j \u2032 ), i = j, (1.1)", "formula_coordinates": [2.0, 199.08, 216.32, 305.72, 39.36]}, {"formula_id": "formula_1", "formula_text": "Single-pair O(T m) O(m) Linearization Proposed (Section 3.5) Single-source O(T 2 m) O(m) Linearization Proposed (Section 3.5) All-pairs O(T 2 nm) O(m) Linearization Proposed (Section 3.5) Top-k search \u226a O(T R) O(m) Linearization & Monte Carlo Proposed (Section 3.5) Join \u2248 O(output) O(m + output) Linearization & Gauss-Southwell [Li et al. 2010b] Single-pair O(T d 2 n 2 ) O(n 2 )", "formula_coordinates": [4.0, 54.72, 134.27, 505.26, 61.05]}, {"formula_id": "formula_2", "formula_text": "Single-pair O(T R) O(m + nR)", "formula_coordinates": [4.0, 168.24, 196.32, 225.82, 8.97]}, {"formula_id": "formula_3", "formula_text": "O(T n 2 d 2 ) O(n 2 ) Naive [", "formula_coordinates": [4.0, 54.72, 205.84, 390.76, 20.29]}, {"formula_id": "formula_4", "formula_text": "O(r 4 n) O(r 2 n 2 ) Singular value decomposition * [Yu et al. 2010] All-pairs O(n 3 ) O(n 2 ) Eigenvalue decomposition * Table II. List of symbols symbol description G directed unweighted graph, G = (V, E) V set of vertices E set of edges n number of vertices, n = |V | m number of edges, m = |E| i, j vertex e edge \u03b4(i) in-neighbors of of i, \u03b4(i) = {j \u2208 V : (j, i) \u2208 E} P transition matrix, P ij = 1/|\u03b4(j)| for (i, j) \u2208 E s(i, j)", "formula_coordinates": [4.0, 54.72, 259.55, 492.57, 139.04]}, {"formula_id": "formula_5", "formula_text": "P ij := 1/|\u03b4(j)|, (i, j) \u2208 E, 0, (i, j) \u2208 E,", "formula_coordinates": [4.0, 244.2, 556.79, 125.52, 30.46]}, {"formula_id": "formula_6", "formula_text": "\u03b4(i) = {j \u2208 V : (j, i) \u2208 E} denotes the in-neighbors of i \u2208 V . Let S = (s(i, j", "formula_coordinates": [4.0, 141.96, 587.15, 354.53, 17.26]}, {"formula_id": "formula_7", "formula_text": "S = (cP \u22a4 SP ) \u2228 I, (2.1)", "formula_coordinates": [4.0, 267.48, 624.54, 237.32, 18.62]}, {"formula_id": "formula_8", "formula_text": "S L (\u0398) = cP \u22a4 S L (\u0398)P + \u0398.", "formula_coordinates": [5.0, 246.6, 280.86, 118.92, 11.55]}, {"formula_id": "formula_9", "formula_text": "P = \uf8ee \uf8ef \uf8f0 0 1 1 1 1/3 0 0 0 1/3 0 0 0 1/3 0 0 0 \uf8f9 \uf8fa \uf8fb ,", "formula_coordinates": [5.0, 264.24, 356.73, 83.64, 42.84]}, {"formula_id": "formula_10", "formula_text": "S = cP \u22a4 SP + (1 \u2212 c)I.", "formula_coordinates": [5.0, 254.76, 548.34, 102.48, 18.74]}, {"formula_id": "formula_11", "formula_text": "1: procedure SINGLESOURCESIMRANK(i) 2: \u03b3 \u2190 0, x \u2190 e i 3: for t = 0, 1, . . . , T \u2212 1 do 4: \u03b3 \u2190 \u03b3 + c t P \u22a4t Dx, x \u2190 P x 5: end for 6:", "formula_coordinates": [6.0, 115.56, 213.46, 199.04, 63.82]}, {"formula_id": "formula_12", "formula_text": "S = D + cP \u22a4 DP + c 2 P \u22a42 DP 2 + \u2022 \u2022 \u2022 .", "formula_coordinates": [6.0, 224.52, 328.13, 165.96, 18.99]}, {"formula_id": "formula_13", "formula_text": "Se i = De i + cP \u22a4 DP e i + c 2 P \u22a42 DP 2 e i + \u2022 \u2022 \u2022 .", "formula_coordinates": [6.0, 208.68, 553.73, 197.64, 18.87]}, {"formula_id": "formula_14", "formula_text": "Se i = u 0 + cP \u22a4 (u 1 + \u2022 \u2022 \u2022 (cP \u22a4 (u t\u22121 + cP \u22a4 u T )) \u2022 \u2022 \u2022 ),", "formula_coordinates": [6.0, 194.04, 647.94, 226.92, 18.62]}, {"formula_id": "formula_15", "formula_text": "A (k, k) diagonal entry S L (D) kk is the most affected by the (k, k) diagonal entry D kk of D.", "formula_coordinates": [7.0, 201.72, 637.5, 208.69, 22.36]}, {"formula_id": "formula_16", "formula_text": "5: \u03b4 \u2190 (1 \u2212 S L (D) kk )/S L (E (k,k) ) kk 6: D kk \u2190 D kk + \u03b4 7:", "formula_coordinates": [8.0, 115.56, 152.69, 199.59, 32.43]}, {"formula_id": "formula_17", "formula_text": "S L (D + \u03b4E (k,k) ) kk = 1.", "formula_coordinates": [8.0, 256.44, 281.09, 102.12, 12.29]}, {"formula_id": "formula_18", "formula_text": "\u03b4 = 1 \u2212 S L (D) kk S L (E (k,k) ) kk . (2.11)", "formula_coordinates": [8.0, 268.08, 315.42, 236.72, 25.24]}, {"formula_id": "formula_19", "formula_text": "E[e k (t) ] = P t e k .", "formula_coordinates": [8.0, 273.48, 478.26, 68.16, 12.76]}, {"formula_id": "formula_20", "formula_text": "k (t) 1 , . . . , k (t)", "formula_coordinates": [8.0, 211.2, 506.93, 52.19, 14.77]}, {"formula_id": "formula_21", "formula_text": "(P t e k ) i \u2248 #{r = 1, . . . , R : k (t) r = i}/R =: p (t)", "formula_coordinates": [8.0, 206.16, 536.69, 199.43, 19.95]}, {"formula_id": "formula_22", "formula_text": "(P t e k ) \u22a4 DP t e k \u2248 n i=1 p (t)2 ki D ii . (2.13)", "formula_coordinates": [8.0, 242.64, 571.14, 262.16, 30.64]}, {"formula_id": "formula_23", "formula_text": "1: \u03b1 \u2190 0, \u03b2 \u2190 0, k 1 \u2190 k, k 2 \u2190 k, . . . , k R \u2190 k 2: for t = 0, 1, . . . , T \u2212 1 do 3: for i \u2208 {k 1 , k 2 , . . . , k R } do 4: p (t)", "formula_coordinates": [9.0, 114.12, 111.47, 196.46, 45.94]}, {"formula_id": "formula_24", "formula_text": "if i = k then 6: \u03b1 \u2190 \u03b1 + c t p (t)2 ki 7: end if 8: \u03b2 \u2190 \u03b2 + c t p (t)2 ki D ii 9:", "formula_coordinates": [9.0, 114.12, 158.49, 123.01, 52.91]}, {"formula_id": "formula_25", "formula_text": "k r \u2190 \u03b4 \u2212 (k r ) randomly 12:", "formula_coordinates": [9.0, 111.0, 224.03, 146.59, 20.25]}, {"formula_id": "formula_26", "formula_text": "(t) k := (p (t) k1 , . . . , p (t) kn ). Then P P t e k \u2212 p (t) k > \u01eb \u2264 2n exp \u2212 (1 \u2212 c)R\u01eb 2 2 .", "formula_coordinates": [9.0, 108.84, 481.97, 394.5, 59.43]}, {"formula_id": "formula_27", "formula_text": "LEMMA 2.5. Let k (t) 1 , . . . , k(t)", "formula_coordinates": [9.0, 120.96, 565.13, 134.99, 14.77]}, {"formula_id": "formula_28", "formula_text": "k := (1/R) R r=1 e k (t) r . Then for all l = 1, . . . , n, P e \u22a4 l X (t) k \u2212 P t e k \u2265 \u01eb \u2264 2 exp \u22122R\u01eb 2 . PROOF. Since E[e k (t) r ] = P t e k ,", "formula_coordinates": [9.0, 108.84, 579.9, 394.52, 71.16]}, {"formula_id": "formula_29", "formula_text": "(t) ki = e \u22a4 i X (t)", "formula_coordinates": [10.0, 434.52, 93.77, 53.27, 14.81]}, {"formula_id": "formula_30", "formula_text": "P P t e k \u2212 p (t) k > \u01eb \u2264 nP e \u22a4 i P t e k \u2212 p (t) ki > \u01eb \u2264 2n exp \u22122R\u01eb 2 .", "formula_coordinates": [10.0, 196.08, 125.21, 216.19, 39.04]}, {"formula_id": "formula_31", "formula_text": "ME = 1 n 2 i,j S L (D) ij \u2212 s(i, j) . (2.14)", "formula_coordinates": [10.0, 235.2, 549.69, 269.6, 27.38]}, {"formula_id": "formula_32", "formula_text": "P t e i = E[e i (t) ]. (3.1)", "formula_coordinates": [15.0, 273.96, 539.22, 229.28, 12.76]}, {"formula_id": "formula_33", "formula_text": "s (T ) (i, j) =D ij + cE[e i (1) ] \u22a4 DE[e j (1) ] + \u2022 \u2022 \u2022 + c T \u22121 E[e i (T \u22121) ] \u22a4 DE[e j (T \u22121) ]. (3.2)", "formula_coordinates": [15.0, 204.36, 575.21, 298.88, 35.56]}, {"formula_id": "formula_34", "formula_text": "(t) 1 , . . . , i(t)", "formula_coordinates": [15.0, 435.12, 622.73, 44.51, 14.89]}, {"formula_id": "formula_35", "formula_text": "2: i 1 \u2190 i, . . . , i R \u2190 i, j 1 \u2190 j, . . . , j R \u2190 j 3: \u03c3 = 0 4: for t = 0, 1, . . . , T \u2212 1 do 5: for w \u2208 {i 1 , . . . , i R } \u2229 {i 1 . . . , i R } do 6: \u03b1 \u2190 #{r : i r = w, r = 1, . . . , R} 7:", "formula_coordinates": [16.0, 115.56, 120.95, 202.65, 64.17]}, {"formula_id": "formula_36", "formula_text": "\u03c3 \u2190 \u03c3 + c t D ww \u03b1\u03b2/R 2 9:", "formula_coordinates": [16.0, 115.56, 185.57, 153.85, 21.39]}, {"formula_id": "formula_37", "formula_text": "11: i r \u2190 \u03b4 \u2212 (i r ), j r \u2190 \u03b4 \u2212 (j r ),", "formula_coordinates": [16.0, 112.44, 219.59, 170.77, 17.26]}, {"formula_id": "formula_38", "formula_text": "(t) 1 , . . . , j (t)", "formula_coordinates": [16.0, 355.92, 284.57, 46.31, 14.77]}, {"formula_id": "formula_39", "formula_text": "c t E[e i (t) ] \u22a4 DE[e j (t) ] \u2243 c t R 2 R r=1 R r \u2032 =1 D i (t) r j (t) r \u2032 . (3.3)", "formula_coordinates": [16.0, 217.08, 318.9, 287.72, 31.08]}, {"formula_id": "formula_40", "formula_text": "i (t) 1 , . . . , i(t)", "formula_coordinates": [16.0, 110.28, 369.89, 47.99, 14.77]}, {"formula_id": "formula_41", "formula_text": "(t) 1 , . . . , j(t)", "formula_coordinates": [16.0, 185.52, 369.89, 46.43, 14.77]}, {"formula_id": "formula_42", "formula_text": "P |s (T ) (i, j) \u2212 s (T ) (i, j)| \u2265 \u01eb \u2264 4nT exp \u2212\u01eb 2 R/2(1 \u2212 c) 2 . (3.4) LEMMA 3.2. P X (t)\u22a4 u DX (t) v \u2212 (P t e u ) \u22a4 DP t e v \u2265 \u01eb \u2264 4n exp(\u2212\u01eb 2 R/2). PROOF. P X (t)\u22a4 u DX (t) v \u2212 (P t e u ) \u22a4 DP t e v \u2265 \u01eb \u2264 P X (t)\u22a4 u D X (t) v \u2212 P t e v \u2265 \u01eb/2 + P X (t) u \u2212 P t e u \u22a4 DP t e v \u2265 \u01eb/2 \u2264 4n exp(\u2212\u01eb 2 R/2).", "formula_coordinates": [16.0, 122.4, 470.21, 382.4, 179.44]}, {"formula_id": "formula_43", "formula_text": "P T \u22121 t=0 c t X (t)\u22a4 u DX (t) v \u2212 s (T ) (u, v) \u2265 \u01eb \u2264 T \u22121 t=0 P c t X (t)\u22a4 u DX (t) v \u2212 c t (P t e u ) \u22a4 DP t e v \u2265 c t \u01eb/(1 \u2212 c) \u22644nT exp \u2212\u01eb 2 R/2(1 \u2212 c) 2 .", "formula_coordinates": [17.0, 176.52, 110.22, 252.39, 87.14]}, {"formula_id": "formula_44", "formula_text": "s(u, v) \u2264 c d(u,v)", "formula_coordinates": [17.0, 272.28, 651.18, 67.07, 18.62]}, {"formula_id": "formula_45", "formula_text": "s (T ) (u, v) \u2264 \u03b2(u, d) (3.8)", "formula_coordinates": [18.0, 266.16, 590.33, 238.64, 18.99]}, {"formula_id": "formula_46", "formula_text": "\u03b1(u, d, t) = max d(u,w)=d D ww P{u (t) = w}", "formula_coordinates": [18.0, 228.0, 645.29, 158.94, 17.69]}, {"formula_id": "formula_47", "formula_text": "P |\u03b2(d, t) \u2212 \u03b2(d, t)| \u2265 \u01eb \u2264 2nd max T exp(\u22122\u01eb 2 R)", "formula_coordinates": [19.0, 198.24, 204.05, 215.67, 18.88]}, {"formula_id": "formula_48", "formula_text": "Algorithm 7 Monte-Carlo \u03b1(u, d, t), \u03b2(u, d) computation 1: procedure COMPUTEALPHABETA(u) 2: u 1 \u2190 u, . . . , u R \u2190 u 3: for t = 0, 1, . . . , T \u2212 1 do 4: for w \u2208 {u 1 , . . . , u R } do 5: \u00b5 \u2190 D ww #{r : u r = w, r \u2208 [1, R]}/R 6: \u03b1(u, d(u, w), t) \u2190 max{\u03b1", "formula_coordinates": [19.0, 108.84, 286.53, 257.49, 86.04]}, {"formula_id": "formula_49", "formula_text": "\u221a D = diag( \u221a D 11 , . . . , \u221a D nn ). Note that, since D is a nonnegative diagonal ma- trix, \u221a D is well-defined.", "formula_coordinates": [19.0, 108.84, 575.39, 394.52, 31.42]}, {"formula_id": "formula_50", "formula_text": "s (T ) (u, v) \u2264 T t=0 c t \u03b3(u, t)\u03b3(v, t) (3.11)", "formula_coordinates": [19.0, 240.36, 630.18, 262.88, 30.52]}, {"formula_id": "formula_51", "formula_text": "P {|\u03b3(u, t) \u2212 \u03b3(u, t)| \u2265 \u01eb} \u2264 4n exp \u2212\u01eb 2 R/8 .", "formula_coordinates": [20.0, 208.8, 172.85, 197.4, 18.87]}, {"formula_id": "formula_52", "formula_text": "Algorithm 8 Monte-Carlo \u03b3(u, t) computation 1: procedure COMPUTEGAMMA(u) 2: u 1 \u2190 u, . . . , u R \u2190 u 3: for t = 0, 1, . . . , T \u2212 1 do 4: \u00b5 = 0 5: for w \u2208 {u 1 , . . . , u R } do 6: \u00b5 \u2190 \u00b5 + D ww #{r : u r = w, r \u2208 [1, R]} 2 /R 2 7: end for 8: \u03b3(u, t) \u2190 \u221a \u00b5 9:", "formula_coordinates": [20.0, 110.28, 274.17, 249.37, 111.11]}, {"formula_id": "formula_53", "formula_text": "u r \u2190 \u03b4(u r )", "formula_coordinates": [20.0, 173.4, 386.87, 48.27, 17.26]}, {"formula_id": "formula_54", "formula_text": "J(i, j) := |\u03b4(i) \u2229 \u03b4(j)| |\u03b4(i) \u222a \u03b4(j)| .", "formula_coordinates": [25.0, 257.16, 507.35, 97.8, 30.82]}, {"formula_id": "formula_55", "formula_text": "J L (\u03b8, \u03b3) \u2286 J(\u03b8) \u2286 J H (\u03b8, \u03b3), (4.1)", "formula_coordinates": [27.0, 246.6, 377.99, 256.64, 17.26]}, {"formula_id": "formula_56", "formula_text": "x (t+1) = x (t) + r (t) i e i ,(4.2)", "formula_coordinates": [28.0, 261.72, 202.37, 243.08, 14.57]}, {"formula_id": "formula_57", "formula_text": "r (t+1) = b \u2212 Ax (t+1) = r (t) \u2212 r (t) i Ae i . (4.3)", "formula_coordinates": [28.0, 227.52, 241.37, 277.28, 20.07]}, {"formula_id": "formula_58", "formula_text": "D \u2212 (S (t) \u2212 cP \u22a4 S (t) P ) = R (", "formula_coordinates": [28.0, 240.72, 391.13, 124.16, 18.87]}, {"formula_id": "formula_59", "formula_text": "S (t+1) = S (t) + R (t)", "formula_coordinates": [28.0, 253.56, 446.81, 85.31, 12.88]}, {"formula_id": "formula_60", "formula_text": "R (t+1) = R (t) \u2212 R (t) ij e i e \u22a4 j + cR (t) ij (P \u22a4 e i )(P \u22a4 e j ) \u22a4 . (4.6)", "formula_coordinates": [28.0, 201.72, 487.37, 303.08, 20.07]}, {"formula_id": "formula_61", "formula_text": "|R (t) ij | < \u01eb for all i, j \u2208 V .", "formula_coordinates": [28.0, 110.28, 522.29, 394.46, 32.43]}, {"formula_id": "formula_62", "formula_text": "S (0) = O, R (0) = D, t = 0 3: while there is (i, j) such that |R (t) ij | > \u01eb do 4: S (t+1) = S (t) + R (t) ij e i e \u22a4 j 5: R (t+1) = R (t) \u2212 R (t) ij e i e \u22a4 j + cR (t) ij (P \u22a4 e i )(P \u22a4 e j ) \u22a4 6: t \u2190 t + 1 7:", "formula_coordinates": [29.0, 114.12, 119.81, 249.82, 76.83]}, {"formula_id": "formula_63", "formula_text": "\u01eb = (1 \u2212 c)(1 \u2212 \u03b3)\u03b8. Then we have 0 \u2264 S ij \u2212S ij \u2264 (1 \u2212 \u03b3)\u03b8 (4.7) for all i, j \u2208 V .", "formula_coordinates": [29.0, 108.84, 337.91, 394.4, 57.57]}, {"formula_id": "formula_64", "formula_text": "J L (\u03b8, \u03b3) := {(i, j) :S ij \u2265 \u03b8}, (4.8) J H (\u03b8, \u03b3) := {(i, j) :S ij \u2265 \u03b3\u03b8}, (4.9)", "formula_coordinates": [29.0, 240.84, 433.19, 262.4, 33.69]}, {"formula_id": "formula_65", "formula_text": "R ij \u2190 R ij + a.", "formula_coordinates": [30.0, 157.08, 132.71, 64.93, 17.26]}, {"formula_id": "formula_66", "formula_text": "P{A \u2212\u00c3 \u2265 \u03b4} \u2264 exp(\u2212\u03b2\u03b4).", "formula_coordinates": [30.0, 248.04, 382.67, 118.92, 17.26]}, {"formula_id": "formula_67", "formula_text": "\u01eb = (1 \u2212 c)(1 \u2212 \u03b3)\u03b8. Then we have 0 \u2264 S ij \u2212S ij and P{S ij \u2212S ij \u2264 (1 \u2212 \u03b3)\u03b8 + \u03b4} \u2264 exp(\u2212(1 \u2212 c)\u03b2\u03b4) (4.11) for all i, j \u2208 V .", "formula_coordinates": [30.0, 110.28, 468.47, 394.52, 58.3]}, {"formula_id": "formula_68", "formula_text": "4: s (R) = (1/R) R", "formula_coordinates": [31.0, 114.12, 153.9, 115.68, 12.39]}, {"formula_id": "formula_69", "formula_text": "s(i, j) \u2248 s (R) (i, j) := 1 R R k=1 c \u03c4 (k) (i", "formula_coordinates": [31.0, 227.64, 320.7, 144.36, 30.88]}, {"formula_id": "formula_70", "formula_text": "P{s (R) (i, j) < \u03b8} \u2264 exp \u22122R\u03b4 (R)2 1 \u2212 c c 2 .", "formula_coordinates": [31.0, 201.6, 421.73, 208.92, 26.79]}, {"formula_id": "formula_71", "formula_text": "P{s (R) (i, j) > \u03b8} \u2264 exp \u22122R\u03b4 (R)2 1 \u2212 c c 2 .", "formula_coordinates": [31.0, 201.6, 480.65, 208.92, 26.79]}, {"formula_id": "formula_72", "formula_text": "R\u03b4 (R)2 \u2265 log(1/p) 2 c 1 \u2212 c 2 , (4.15)", "formula_coordinates": [31.0, 241.32, 594.77, 261.92, 33.51]}, {"formula_id": "formula_73", "formula_text": "E[R end ] \u2265 1 (s(i, j) \u2212 \u03b8) 2 log(1/p) 2 c 1 \u2212 c 2 , (4.16)", "formula_coordinates": [32.0, 213.0, 500.69, 291.8, 33.51]}, {"formula_id": "formula_74", "formula_text": "precision = |S \u2229 S * | |S| , recall = |S \u2229 S * | |S * | , F = 2|S \u2229 S * | |S| + |S * | .", "formula_coordinates": [33.0, 181.92, 526.02, 248.28, 31.7]}, {"formula_id": "formula_75", "formula_text": "A \u2297 B = \uf8ee \uf8ef \uf8f0 A 11 B \u2022 \u2022 \u2022 A 1n B . . . . . . . . . A n1 B \u2022 \u2022 \u2022 A nn B \uf8f9 \uf8fa \uf8fb .", "formula_coordinates": [39.0, 239.64, 360.92, 132.72, 47.4]}, {"formula_id": "formula_76", "formula_text": "I \u2212 cQ DD \u2212cQ DO \u2212cQ OD I \u2212 cQ OO 1 X = diag(D) 0 , (5.3)", "formula_coordinates": [40.0, 219.96, 131.51, 284.84, 28.3]}, {"formula_id": "formula_77", "formula_text": "k := (1/R)", "formula_coordinates": [41.0, 344.4, 203.61, 52.71, 11.89]}, {"formula_id": "formula_78", "formula_text": "S L (\u2206) ij = c(P e i ) \u22a4 S L (\u2206)(P e j ) + \u2206 ij \u2264 c sup i \u2032 ,j \u2032 S L (\u2206) i \u2032 j \u2032 + \u01eb.", "formula_coordinates": [41.0, 224.04, 528.66, 163.01, 34.96]}, {"formula_id": "formula_79", "formula_text": "(1 \u2212 c) sup i,j S L (\u2206) ij \u2264 \u01eb.", "formula_coordinates": [41.0, 254.4, 588.9, 103.32, 18.64]}, {"formula_id": "formula_80", "formula_text": "(1 \u2212 c) inf i,j S L (\u2206) ij \u2265 \u2212\u01eb.", "formula_coordinates": [41.0, 251.88, 629.1, 108.36, 18.62]}, {"formula_id": "formula_81", "formula_text": "\u03a6(t + 1) \u2212 \u03a6(t) = U, \u2212R (t) ij E ij + cR (t) ij P \u22a4 E ij P = \u2212R (t) ij U \u2212 cP U P \u22a4 , E ij .", "formula_coordinates": [44.0, 203.16, 369.29, 203.44, 38.43]}, {"formula_id": "formula_83", "formula_text": "\u03a6(t + 1) \u2212 \u03a6(t) = \u2212R (t)", "formula_coordinates": [44.0, 241.2, 645.77, 104.39, 19.95]}, {"formula_id": "formula_84", "formula_text": "P{A \u2212\u00c3 \u2265 \u03b4} = (1 \u2212 \u03b2a 1 ) \u2022 \u2022 \u2022 (1 \u2212 \u03b2a k ).", "formula_coordinates": [45.0, 218.88, 497.75, 174.24, 17.26]}, {"formula_id": "formula_85", "formula_text": "(1 \u2212 \u03b2a 1 ) \u2022 \u2022 \u2022 (1 \u2212 \u03b2a k ) \u2264 1 \u2212 1 k k i=1 \u03b2a i k \u2264 1 \u2212 \u03b2\u03b4 k k \u2264 exp(\u2212\u03b2\u03b4).", "formula_coordinates": [45.0, 197.04, 530.46, 217.92, 63.86]}], "doi": ""}