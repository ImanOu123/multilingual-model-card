{"title": "GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields", "authors": "Michael Niemeyer; Andreas Geiger", "pub_date": "", "abstract": "Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects' shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.", "sections": [{"heading": "Introduction", "text": "The ability to generate and manipulate photorealistic image content is a long-standing goal of computer vision and graphics. Modern computer graphics techniques achieve impressive results and are industry standard in gaming and movie productions. However, they are very hardware expensive and require substantial human labor for 3D content creation and arrangement.\nIn recent years, the computer vision community has made great strides towards highly-realistic image generation. In particular, Generative Adversarial Networks (GANs) [24] emerged as a powerful class of generative models. They are able to synthesize photorealistic images at resolutions of 1024 2 pixels and beyond [6,14,15,39,40]. While training only on raw image collections, at test time we are able to control the image formation process wrt. camera pose, object poses, as well as the objects' shapes and appearances. Further, our model generalizes beyond the training data, e.g. we can synthesize scenes with more objects than were present in the training images. Note that for clarity we visualize volumes in color instead of features.\nDespite these successes, synthesizing realistic 2D images is not the only aspect required in applications of generative models. The generation process should also be controllable in a simple and consistent manner. To this end, many works [9,25,39,43,44,48,54,71,74,97,98] investigate how disentangled representations can be learned from data without explicit supervision. Definitions of disentanglement vary [5,53], but commonly refer to being able to control an attribute of interest, e.g. object shape, size, or pose, without changing other attributes. Most approaches, however, do not consider the compositional nature of scenes and operate in the 2D domain, ignoring that our world is three-dimensional. This often leads to entangled representations (Fig. 2) and control mechanisms are not built-in, but need to be discovered in the latent space a posteriori. These properties, however, are crucial for successful applications, e.g. a movie production where complex object trajectories Figure 2: Controllable Image Generation. While most generative models operate in 2D, we incorporate a compositional 3D scene representation into the generative model. This leads to more consistent image synthesis results, e.g. note how, in contrast to our method, translating one object might change the other when operating in 2D (Fig. 2a and  2b). It further allows us to perform complex operations like circular translations (Fig. 2c) or adding more objects at test time (Fig. 2d). Both methods are trained unsupervised on raw unposed image collections of two-object scenes.\nneed to be generated in a consistent manner. Several recent works therefore investigate how to incorporate 3D representations, such as voxels [32,63,64], primitives [46], or radiance fields [77], directly into generative models. While these methods allow for impressive results with built-in control, they are mostly restricted to singleobject scenes and results are less consistent for higher resolutions and more complex and realistic imagery (e.g. scenes with objects not in the center or cluttered backgrounds). Contribution: In this work, we introduce GIRAFFE, a novel method for generating scenes in a controllable and photorealistic manner while training from raw unstructured image collections. Our key insight is twofold: First, incorporating a compositional 3D scene representation directly into the generative model leads to more controllable image synthesis. Second, combining this explicit 3D representation with a neural rendering pipeline results in faster inference and more realistic images. To this end, we represent scenes as compositional generative neural feature fields (Fig. 1). We volume render the scene to a feature image of relatively low resolution to save time and computation. A neural renderer processes these feature images and outputs the final renderings. This way, our approach achieves high-quality images and scales to real-world scenes. We find that our method allows for controllable image synthesis of single-object as well as multi-object scenes when trained on raw unstructured image collections. Code and data is available at https://github.com/autonomousvision/giraffe.", "publication_ref": ["b24", "b5", "b13", "b14", "b41", "b42", "b8", "b25", "b41", "b45", "b46", "b50", "b56", "b74", "b77", "b100", "b101", "b4", "b55", "b34", "b65", "b66", "b48", "b80"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Related Work", "text": "GAN-based Image Synthesis: Generative Adversarial Networks (GANs) [24] have been shown to allow for photorealistic image synthesis at resolutions of 1024 2 pixels and beyond [6,14,15,39,40]. To gain better control over the synthesis process, many works investigate how factors of variation can be disentangled without explicit supervision. They either modify the training objective [9,40,71] or network architecture [39], or investigate latent spaces of well-engineered and pre-trained generative models [1,16,23,27,34,78,96]. All of these works, however, do not explicitly model the compositional nature of scenes. Recent works therefore investigate how the synthesis process can be controlled at the object-level [3,4,7,18,19,26,45,86,90]. While achieving photorealistic results, all aforementioned works model the image formation process in 2D, ignoring the three-dimensional structure of our world. In this work, we advocate to model the formation process directly in 3D for better disentanglement and more controllable synthesis.\nImplicit Functions: Using implicit functions to represent 3D geometry has gained popularity in learning-based 3D reconstruction [11,12,22,59,60,65,67,69,76] and has been extended to scene-level reconstruction [8,13,35,72,79]. To overcome the need of 3D supervision, several works [50,51,66,81,92] propose differentiable rendering techniques. Mildenhall et al. [61] propose Neural Radiance Fields (NeRFs) in which they combine an implicit neural model with volume rendering for novel view synthesis of complex scenes. Due to their expressiveness, we use a generative variant of NeRFs as our object-level representation. In contrast to our method, the discussed works require multi-view images with camera poses as supervision, train a single network per scene, and are not able to generate novel scenes. Instead, we learn a generative model from unstructured image collections which allows for controllable, photorealistic image synthesis of generated scenes.\n3D-Aware Image Synthesis: Several works investigate how 3D representations can be incorporated as inductive bias into generative models [21,[29][30][31][32]46,55,63,64,75,77]. While many approaches use additional supervision [2,10,87,88,99], we focus on works which are trained on raw image collections like our approach. Henzler et al. [32] learn voxel-based representations using differentiable rendering. The results are 3D controllable, but show artifacts due to the limited voxel resolutions caused by their cubic memory growth. Nguyen-Phuoc et al. [63,64] propose voxelized feature-grid representations which are rendered to 2D via a reshaping operation. While achieving impressive results, training becomes less stable and results less consistent for higher resolutions. Liao et al. [46] use abstract features in combination with primitives and differentiable rendering. While han-dling multi-object scenes, they require additional supervision in the form of pure background images which are hard to obtain for real-world scenes. Schwarz et al. [77] propose Generative Neural Radiances Fields (GRAF). While achieving controllable image synthesis at high resolutions, this representation is restricted to single-object scenes and results degrade on more complex, real-world imagery. In contrast, we incorporate compositional 3D scene structure into the generative model such that it naturally handles multiobject scenes. Further, by integrating a neural rendering pipeline [20,41,42,49,62,80,81,83,84], our model scales to more complex, real-world data.", "publication_ref": ["b24", "b5", "b13", "b14", "b41", "b42", "b8", "b42", "b74", "b41", "b0", "b16", "b23", "b28", "b36", "b81", "b99", "b2", "b3", "b6", "b18", "b19", "b26", "b47", "b89", "b93", "b10", "b11", "b22", "b61", "b62", "b67", "b69", "b71", "b79", "b7", "b12", "b37", "b75", "b82", "b52", "b53", "b68", "b84", "b95", "b63", "b21", "b31", "b32", "b33", "b34", "b48", "b57", "b65", "b66", "b78", "b80", "b1", "b9", "b90", "b91", "b102", "b34", "b65", "b66", "b48", "b80", "b20", "b43", "b44", "b51", "b64", "b83", "b84", "b86", "b87"], "figure_ref": [], "table_ref": []}, {"heading": "Method", "text": "Our goal is a controllable image synthesis pipeline which can be trained from raw image collections without additional supervision. In the following, we discuss the main components of our method. First, we model individual objects as neural feature fields (Sec. 3.1). Next, we exploit the additive property of feature fields to composite scenes from multiple individual objects (Sec. 3.2). For rendering, we explore an efficient combination of volume and neural rendering techniques (Sec. 3.3). Finally, we discuss how we train our model from raw image collections (Sec. 3.4). Fig. 3 contains an overview of our method.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Objects as Neural Feature Fields", "text": "Neural Radiance Fields: A radiance field is a continuous function f which maps a 3D point x \u2208 R 3 and a viewing direction d \u2208 S 2 to a volume density \u03c3 \u2208 R + and an RGB color value c \u2208 R 3 . A key observation in [61,82] is that the low dimensional input x and d needs to be mapped to higher-dimensional features to be able to represent complex signals when f is parameterized with a neural network. More specifically, a pre-defined positional encoding is applied element-wise to each component of x and d:\n\u03b3(t, L) = (sin(2 0 t\u03c0), cos(2 0 t\u03c0), . . . , sin(2 L t\u03c0), cos(2 L t\u03c0))(1)\nwhere t is a scalar input, e.g. a component of x or d, and L the number of frequency octaves. In the context of generative models, we observe an additional benefit of this representation: It introduces an inductive bias to learn 3D shape representations in canonical orientations which otherwise would be arbitrary (see Fig. 11). Following implicit shape representations [12,59,69], Mildenhall et al. [61] propose to learn Neural Radiance Fields (NeRFs) by parameterizing f with a multi-layer perceptron (MLP):\nf \u03b8 : R Lx \u00d7 R L d \u2192 R + \u00d7 R 3 (\u03b3(x), \u03b3(d)) \u2192 (\u03c3, c)(2)\nwhere \u03b8 indicate the network parameters and L x , L d the output dimensionalities of the positional encodings. Generative Neural Feature Fields: While [61] fits \u03b8 to multiple posed images of a single scene, Schwarz et al. [77] propose a generative model for Neural Radiance Fields (GRAF) that is trained from unposed image collections. To learn a latent space of NeRFs, they condition the MLP on shape and appearance codes z s , z a \u223c N (0, I):\ng \u03b8 : R Lx \u00d7 R L d \u00d7 R Ms \u00d7 R Ma \u2192 R + \u00d7 R 3 (\u03b3(x), \u03b3(d), z s , z a ) \u2192 (\u03c3, c)(3)\nwhere M s , M a are the dimensionalities of the latent codes.\nIn this work we explore a more efficient combination of volume and neural rendering. We replace GRAF's formulation for the three-dimensional color output c with a more generic M f -dimensional feature f and represent objects as Generative Neural Feature Fields:\nh \u03b8 : R Lx \u00d7 R L d \u00d7 R Ms \u00d7 R Ma \u2192 R + \u00d7 R M f (\u03b3(x), \u03b3(d), z s , z a ) \u2192 (\u03c3, f )(4)\nObject Representation: A key limitation of NeRF and GRAF is that the entire scene is represented by a single model. As we are interested in disentangling different entities in the scene, we need control over the pose, shape and appearance of individual objects (we consider the background as an object as well). We therefore represent each object using a separate feature field in combination with an affine transformation T = {s, t, R}\nwhere s, t \u2208 R 3 indicate scale and translation parameters, and R \u2208 SO(3) a rotation matrix. Using this representation, we transform points from object to scene space as follows:\nk(x) = R \u2022 \uf8ee \uf8f0 s 1 s 2 s 3 \uf8f9 \uf8fb \u2022 x + t(6)\nIn practice, we volume render in scene space and evaluate the feature field in its canonical object space (see Fig. 1):\n(\u03c3, f ) = h \u03b8 (\u03b3(k \u22121 (x)), \u03b3(k \u22121 (d)), z s , z a )(7)\nThis allows us to arrange multiple objects in a scene. All object feature fields share their weights and T is sampled from a dataset-dependent distribution (see Sec. 3.4).", "publication_ref": ["b63", "b85", "b11", "b61", "b71", "b63", "b63", "b80"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Scene Compositions", "text": "As discussed above, we describe scenes as compositions of N entities where the first N \u2212 1 are the objects in the scene and the last represents the background. We consider  s , z i a and affine transformations T i as input and synthesizes an image of the generated scene which consists of N \u2212 1 objects and a background. The discriminator D \u03c6 takes the generated image\u00ce and the real image I as input and our full model is trained with an adversarial loss. At test time, we can control the camera pose, the shape and appearance codes of the objects, and the objects' poses in the scene. Orange indicates learnable and blue non-learnable operations. two cases: First, N is fixed across the dataset such that the images always contain N \u2212 1 objects plus the background. Second, N is varied across the dataset. In practice, we use the same representation for the background as for objects except that we fix the scale and translation parameters s N , t N to span the entire scene, and to be centered at the scene space origin.\nComposition Operator: To define the composition operator C, let's recall that a feature field of a single entity h i \u03b8i predicts a density \u03c3 i \u2208 R + and a feature vector f i \u2208 R M f for a given point x and viewing direction d. When combining non-solid objects, a natural choice [17] for the overall density at x is to sum up the individual densities and to use the density-weighted mean to combine all features at (x, d):\nC(x, d) = \u03c3, 1 \u03c3 N i=1 \u03c3 i f i , where \u03c3 = N i=1 \u03c3 i (8)\nWhile being simple and intuitive, this choice for C has an additional benefit: We ensure gradient flow to all entities with a density greater than 0.", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "Scene Rendering", "text": "3D Volume Rendering: While previous works [47,57,61,77] volume render an RGB color value, we extend this formulation to rendering an M f -dimensional feature vector f . For given camera extrinsics \u03be, let {x j } Ns j=1 be sample points along the camera ray d for a given pixel, and (\u03c3 j , f j ) = C(x j , d) the corresponding densities and feature vectors of the field. The volume rendering operator \u03c0 vol [37] maps these evaluations to the pixel's final feature vector f :\n\u03c0 vol : (R + \u00d7 R M f ) Ns \u2192 R M f , {\u03c3 j , f j } Ns j=1 \u2192 f (9)\nUsing numerical integration as in [61], f is obtained as\nf = Ns j=1 \u03c4 j \u03b1 j f j \u03c4 j = j\u22121 k=1 (1 \u2212 \u03b1 k ) \u03b1 j = 1 \u2212 e \u2212\u03c3j \u03b4j(10)\nwhere \u03c4 j is the transmittance, \u03b1 j the alpha value for x j , and \u03b4 j = ||x j+1 \u2212 x j || 2 the distance between neighboring sample points. The entire feature image is obtained by evaluating \u03c0 vol at every pixel. For efficiency, we render feature images at resolution 16 2 which is lower than the output resolution of 64 2 or 256 2 pixels. We then upsample the lowresolution feature maps to higher-resolution RGB images using 2D neural rendering. As evidenced by our experiments, this has two advantages: increased rendering speed and improved image quality. 2D Neural Rendering: The neural rendering operator\n\u03c0 neural \u03b8 : R H V \u00d7W V \u00d7M f \u2192 R H\u00d7W \u00d73(11)\nwith weights \u03b8 maps the feature image\nI V \u2208 R H V \u00d7W V \u00d7M f\nto the final synthesized image\u00ce \u2208 R H\u00d7W \u00d73 . We parameterize \u03c0 neural \u03b8 as a 2D convolutional neural network (CNN) with leaky ReLU [56,89] activation (Fig. 4) and combine nearest neighbor upsampling with 3 \u00d7 3 convolutions to increase the spatial resolution. We choose small kernel sizes and no intermediate layers to only allow for spatially small refinements to avoid entangling global scene properties during image synthesis while at the same time allowing for increased output resolutions. Inspired by [40], we map the feature image to an RGB image at every spatial resolution, and add the previous output to the next via bilinear upsampling. These skip connections ensure a strong gradient flow to the feature fields. We obtain our final image prediction\u00ce by applying a sigmoid activation to the last RGB layer. We validate our design choices in an ablation study (Tab. 4). Figure 4: Neural Rendering Operator. The feature image I V is processed by n blocks of nearest neighbor upsampling and 3 \u00d7 3 convolutions with leaky ReLU activations. At every resolution, we map the feature image to an RGB image with a 3\u00d73 convolution and add it to the previous output via bilinear upsampling. We apply a sigmoid activation to obtain the final image\u00ce. Gray color indicates outputs, orange learnable, and blue non-learnable operations.", "publication_ref": ["b49", "b59", "b63", "b80", "b39", "b63", "b58", "b92", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "Training", "text": "Generator: We denote the full generative process formally as\nG \u03b8 ({z i s , z i a , T i } N i=1 , \u03be) = \u03c0 neural \u03b8 (I V )\nwhere\nI V = {\u03c0 vol ({C(x jk , d k )} Ns j=1 )} H V \u00d7W V k=1 (12\n)\nand N is the number of entities in the scene, N s the number of sample points along each ray, d k is the ray for the k-th pixel, and x jk the j-th sample point for the k-th pixel / ray.\nDiscriminator: We parameterize the discriminator D \u03c6 as a CNN [73] with leaky ReLU activation.\nTraining: During training, we sample the the number of entities in the scene N \u223c p N , the latent codes z i s , z i a \u223c N (0, I), as well as a camera pose \u03be \u223c p \u03be and object-level transformations T i \u223c p T . In practice, we define p \u03be and p T as uniform distributions over dataset-dependent camera elevation angles and valid object transformations, respectively. 1 The motivation for this choice is that in most realworld scenes, objects are arbitrarily rotated, but not tilted due to gravity. The observer (the camera in our case), in contrast, can freely change its elevation angle wrt. the scene.\nWe train our model with the non-saturating GAN objec- 1 Details can be found in the supplementary material.\ntive [24] and R 1 gradient penalty [58] V(\u03b8, \u03c6) =\nE z i s ,z i a \u223cN , \u03be\u223cp \u03be , Ti\u223cp T f (D \u03c6 (G \u03b8 ({z i s , z i a , T i } i , \u03be)) + E I\u223cp D f (\u2212D \u03c6 (I)) \u2212 \u03bb \u2207D \u03c6 (I) 2 (13)\nwhere f (t) = \u2212 log(1 + exp(\u2212t)), \u03bb = 10, and p D indicates the data distribution.", "publication_ref": ["b76", "b0", "b0", "b24", "b60"], "figure_ref": [], "table_ref": []}, {"heading": "Implementation Details", "text": "All object feature fields {h i \u03b8i } N \u22121 i=1 share their weights and we parametrize them as MLPs with ReLU activations. We use 8 layers with a hidden dimension of 128 and a density and a feature head of dimensionality 1 and M f = 128, respectively. For the background feature field h N \u03b8 N , we use half the layers and hidden dimension. We use L x = 2 \u2022 3 \u2022 10 and L d = 2 \u2022 3 \u2022 4 for the positional encodings. We sample M s = 64 points along each ray and render the feature image I V at 16 2 pixels. We use an exponential moving average [93] with decay 0.999 for the weights of the generator. We use the RMSprop optimizer [85] with a batch size of 32 and learning rates of 1 \u00d7 10 \u22124 and 5 \u00d7 10 \u22124 for the discriminator and generator, respectively. For experiments at 256 2 pixels, we set M f = 256 and half the generator learning rate to 2.5 \u00d7 10 \u22124 .", "publication_ref": ["b96", "b88"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "Datasets: We report results on commonly-used singleobject datasets Chairs [68], Cats [95], CelebA [52], and CelebA-HQ [38]. The first consists of synthetic renderings of Photoshape chairs [70], and the others are image collections of cat and human faces, respectively. The data complexity is limited as the background is purely white or only takes up a small part of the image. We further report results on the more challenging single-object, real-world datasets CompCars [91], LSUN Churches [94], and FFHQ [39]. For CompCars, we randomly crop the images to achieve more variety of the object's position in the image. 2 For these datasets, disentangling objects is more complex as the object is not always in the center and the background is more cluttered and takes up a larger part of the image. To test our model on multi-object scenes, we use the script from [36] to render scenes with 2, 3, 4, or 5 random primitives (Clevr-N). To test our model on scenes with a varying number of objects, we also run our model on the union of them (Clevr-2345). Baselines: We compare against voxel-based Platonic-GAN [32], BlockGAN [64], and HoloGAN [63], and radiance field-based GRAF [77]   of the methods). We further compare against HoloGAN w/o 3D Conv, a variant of [63] proposed in [77] for higher resolutions. We additionally report a ResNet-based [28] 2D GAN [58] for reference.", "publication_ref": ["b70", "b98", "b54", "b40", "b73", "b94", "b97", "b41", "b1", "b38", "b34", "b66", "b65", "b80", "b65", "b80", "b29", "b60"], "figure_ref": [], "table_ref": []}, {"heading": "Metrics:", "text": "We report the Frechet Inception Distance (FID) score [33] to quantify image quality. We use 20,000 real and fake samples to calculate the FID score.", "publication_ref": ["b35"], "figure_ref": [], "table_ref": []}, {"heading": "Controllable Scene Generation", "text": "Disentangled Scene Generation: We first analyze to which degree our model learns to generate disentangled scene representations. In particular, we are interested if objects are disentangled from the background. Towards this goal, we exploit the fact that our composition operator is a simple addition operation (Eq. 8) and render individual components and object alpha maps (Eq. 10). Note that while we always render the feature image at 16 2 during training, we can choose arbitrary resolutions at test time. Fig. 5 suggests that our method disentangles objects from the background. Note that this disentanglement emerges without any supervision, and the model learns to generate plausible backgrounds without ever having seen a pure background image, implicitly solving an inpainting task. We further observe that our model correctly disentangles individual objects when trained on multi-object scenes with fixed or varying number of objects. We further find that unsupervised disentanglement is a property of our model    which emerges already at the very beginning of training (Fig. 6). Note how our model synthesizes individual objects before spending capacity on representing the background. Controllable Scene Generation: As individual components of the scene are correctly disentangled, we analyze how well they can be controlled. More specifically, we are interested if individual objects can be rotated and translated, but also how well shape and appearance can be controlled. In Fig. 7, we show examples in which we control the scene during image synthesis. We rotate individual objects, translate them in 3D space, or change the camera elevation. By modeling shape and appearance for each entity with a different latent code, we are further able to change the objects' appearances without altering their shape.\nGeneralization Beyond Training Data: The learned compositional scene representations allow us to generalize outside the training distribution. For example, we can increase the translation ranges of objects or add more objects than there were present in the training data (Fig. 8).", "publication_ref": [], "figure_ref": ["fig_3", "fig_4"], "table_ref": []}, {"heading": "Comparison to Baseline Methods", "text": "Comparing to baseline methods, our method achieves similar or better FID scores at both 64 2 (Tab. 1) and 256 2 (Tab. 2) pixel resolutions. Qualitatively, we observe that while all approaches allow for controllable image synthesis on datasets of limited complexity, results are less consistent for the baseline methods on more complex scenes  with cluttered backgrounds. Further, our model disentangles the object from the background, such that we are able to control the object independent of the background (Fig. 9). We further note that our model achieves similar or better FID scores than the ResNet-based 2D GAN [58] despite fewer network parameters (0.41m compared to 1.69m).\n(a) 360 \u2022 Object Rotation for HoloGAN [63] (b) 360 \u2022 Object Rotation for GRAF [77] (c) 360 \u2022 Object Rotation for Our Method Figure 9: Qualitative Comparison. Compared to baseline methods, we achieve more consistent image synthesis for complex scenes with cluttered background at 64 2 (top rows) and 256 2 (bottom rows) pixel resolutions. Note that we disentangle the object from the background and are able to rotate only the object while keeping the background fixed. This confirms our initial hypothesis that using a 3D representation as inductive bias results in better outputs. Note that for fair comparison, we only report methods which are  ", "publication_ref": ["b60", "b65", "b80"], "figure_ref": [], "table_ref": []}, {"heading": "Ablation Studies", "text": "Importance of Individual Components: The ablation study in Tab. 4 shows that our design choices of RGB skip connections, final activation function, and selected upsampling types improve results and lead to higher FID scores.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Effect of Neural Renderer:", "text": "A key difference to [77] is that we combine volume with neural rendering. The quantitative (Tab. 1 and 2) and qualitative comparisons (Fig. 9) indicate that our approach leads to better results, in particular for complex, real-world data. Our model is more expressive and can better handle the complexity of real scenes, e.g. note how the neural renderer realistically adapts object appearances to the background (Fig. 10). Further, we observe a rendering speed up: compared to [77], total rendering time is reduced from 110.1ms to 4.8ms, and from 1595.0ms to 5.9ms for 64 2 and 256 2 pixels, respectively. Positional Encoding: We use axis-aligned positional encoding for the input point and viewing direction (Eq. 1). Surprisingly, this encourages the model to learn canoncial Figure 12: Dataset Bias. Eye and hair rotation are examples for dataset biases: They primarily face the camera, and our model tends to entangle them with the object rotation.\nrepresentations as it introduces a bias to align the object axes with highest symmetry with the canonical axes which allows the model to exploit object symmetry (Fig. 11).", "publication_ref": ["b80", "b80"], "figure_ref": ["fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Limitations", "text": "Dataset Bias: Our method struggles to disentangle factors of variation if there is an inherent bias in the data. We show an example in Fig. 12: In the celebA-HQ dataset, the eye and hair orientation is predominantly pointing towards the camera, regardless of the face rotation. When rotating the object, the eyes and hair in our generated images do not stay fixed but are adjusted to meet the dataset bias. Object Transformation Distributions: We sometimes observe disentanglement failures, e.g. for Churches where the background contains a church, or for CompCars where the foreground contains background elements (see Sup. Mat.). We attribute these to mismatches between the assumed uniform distributions over camera poses and object-level transformations and their real distributions.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Conclusion", "text": "We present GIRAFFE, a novel method for controllable image synthesis. Our key idea is to incorporate a compositional 3D scene representation into the generative model. By representing scenes as compositional generative neural feature fields, we disentangle individual objects from the background as well as their shape and appearance without explicit supervision. Combining this with a neural renderer yields fast and controllable image synthesis. In the future, we plan to investigate how the distributions over objectlevel transformations and camera poses can be learned from data. Further, incorporating supervision which is easy to obtain, e.g. predicted object masks, is a promising approach to scale to more complex, multi-object scenes.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Styleflow: Attribute-conditioned exploration of stylegan-generated images using conditional continuous normalizing flows. arXiv.org", "journal": "", "year": "2008", "authors": "Rameen Abdal; Peihao Zhu; Niloy J Mitra; Peter Wonka"}, {"ref_id": "b1", "title": "Geometric image synthesis", "journal": "", "year": "2018", "authors": "Hassan Alhaija; Siva Mustikovela; Andreas Geiger; Carsten Rother"}, {"ref_id": "b2", "title": "Object-centric image generation with factored depths, locations, and appearances. arXiv.org", "journal": "", "year": "2004", "authors": "Titas Anciukevicius; Christoph H Lampert; Paul Henderson"}, {"ref_id": "b3", "title": "Object discovery with a copy-pasting GAN. arXiv.org", "journal": "", "year": "1905", "authors": "Relja Arandjelovic; Andrew Zisserman"}, {"ref_id": "b4", "title": "Representation learning: A review and new perspectives", "journal": "IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI)", "year": "2013", "authors": "Yoshua Bengio; Aaron C Courville; Pascal Vincent"}, {"ref_id": "b5", "title": "Large scale GAN training for high fidelity natural image synthesis", "journal": "", "year": "2002", "authors": "Andrew Brock; Jeff Donahue; Karen Simonyan"}, {"ref_id": "b6", "title": "Unsupervised scene decomposition and representation. arXiv.org", "journal": "", "year": "1901", "authors": "Christopher P Burgess; Lo\u00efc Matthey; Nicholas Watters; Rishabh Kabra; Irina Higgins; M Matthew; Alexander Botvinick;  Lerchner;  Monet"}, {"ref_id": "b7", "title": "Newcombe. Deep local shapes: Learning local SDF priors for detailed 3d reconstruction", "journal": "", "year": "", "authors": "Rohan Chabra; Jan Eric Lenssen; Eddy Ilg; Tanner Schmidt; Julian Straub; Steven Lovegrove; Richard A "}, {"ref_id": "b8", "title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "journal": "", "year": "2002", "authors": "Xi Chen; Xi Chen; Yan Duan"}, {"ref_id": "b9", "title": "Neural graphics pipeline for controllable image generation. arXiv.org", "journal": "", "year": "2006", "authors": "Xuelin Chen; Daniel Cohen-Or; Baoquan Chen; Niloy J Mitra"}, {"ref_id": "b10", "title": "BAE-NET: branched autoencoder for shape co-segmentation", "journal": "", "year": "2019", "authors": "Zhiqin Chen; Kangxue Yin; Matthew Fisher; Siddhartha Chaudhuri; Hao Zhang"}, {"ref_id": "b11", "title": "Learning implicit fields for generative shape modeling", "journal": "", "year": "2019", "authors": "Zhiqin Chen; Hao Zhang"}, {"ref_id": "b12", "title": "Neural unsigned distance fields for implicit function learning", "journal": "", "year": "", "authors": "Julian Chibane; Aymen Mir; Gerard Pons-Moll"}, {"ref_id": "b13", "title": "Stargan: Unified generative adversarial networks for multi-domain image-to-image translation", "journal": "", "year": "2002", "authors": "Yunjey Choi; Min-Je Choi; Munyoung Kim; Jung-Woo Ha; Sunghun Kim; Jaegul Choo"}, {"ref_id": "b14", "title": "Stargan v2: Diverse image synthesis for multiple domains", "journal": "", "year": "", "authors": "Yunjey Choi; Youngjung Uh; Jaejun Yoo; Jung-Woo Ha"}, {"ref_id": "b15", "title": "Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)", "journal": "", "year": "2002", "authors": ""}, {"ref_id": "b16", "title": "Editing in style: Uncovering the local semantics of gans", "journal": "", "year": "", "authors": "Edo Collins; Raja Bala; Bob Price; Sabine S\u00fcsstrunk"}, {"ref_id": "b17", "title": "Volume rendering", "journal": "In ACM Trans. on Graphics", "year": "1988", "authors": "Robert A Drebin; Loren C Carpenter; Pat Hanrahan"}, {"ref_id": "b18", "title": "Martin Engelcke, Ingmar Posner, Niloy J. Mitra, and Andrea Vedaldi. RELATE: physically plausible multi-object scene synthesis using structured latent spaces. arXiv.org", "journal": "", "year": "1272", "authors": "S\u00e9bastien Ehrhardt; Oliver Groth; Aron Monszpart"}, {"ref_id": "b19", "title": "GENESIS: generative scene inference and sampling with object-centric latent representations", "journal": "", "year": "", "authors": "Martin Engelcke; Adam R Kosiorek; Oiwi Parker Jones; Ingmar Posner"}, {"ref_id": "b20", "title": "Neural scene representation and rendering", "journal": "", "year": "2018", "authors": "S M Ali Eslami; Danilo Jimenez Rezende; Fr\u00e9d\u00e9ric Besse; Fabio Viola; Ari S Morcos; Marta Garnelo; Avraham Ruderman; Andrei A Rusu; Ivo Danihelka; Karol Gregor; David P Reichert; Lars Buesing; Theophane Weber"}, {"ref_id": "b21", "title": "3d shape induction from 2d views of multiple objects", "journal": "", "year": "2017", "authors": "Matheus Gadelha; Subhransu Maji; Rui Wang"}, {"ref_id": "b22", "title": "Learning shape templates with structured implicit functions", "journal": "", "year": "2019", "authors": "Kyle Genova; Forrester Cole; Daniel Vlasic; Aaron Sarna; T William; Thomas Freeman;  Funkhouser"}, {"ref_id": "b23", "title": "Ganalyze: Toward visual definitions of cognitive image properties", "journal": "", "year": "2019", "authors": "Lore Goetschalckx; Alex Andonian; Aude Oliva; Phillip Isola"}, {"ref_id": "b24", "title": "Generative adversarial nets", "journal": "", "year": "2005", "authors": "Ian J Goodfellow; Jean Pouget-Abadie; Mehdi Mirza; Bing Xu; David Warde-Farley; Sherjil Ozair; Aaron C Courville; Yoshua Bengio"}, {"ref_id": "b25", "title": "Yoshua Bengio, and Bernhard Sch\u00f6lkopf. Recurrent independent mechanisms. arXiv.org", "journal": "", "year": "1909", "authors": "Anirudh Goyal; Alex Lamb; Jordan Hoffmann; Shagun Sodhani; Sergey Levine"}, {"ref_id": "b26", "title": "", "journal": "", "year": "", "authors": "Klaus Greff; Rapha\u00ebl Lopez Kaufmann; Rishabh Kabra; Nick Watters; Christopher Burgess; Daniel Zoran; Loic Matthey; Matthew Botvinick; Alexander Lerchner"}, {"ref_id": "b27", "title": "Multi-object representation learning with iterative variational inference", "journal": "", "year": "2019", "authors": ""}, {"ref_id": "b28", "title": "Ganspace: Discovering interpretable GAN controls. arXiv.org", "journal": "", "year": "2004", "authors": "Erik H\u00e4rk\u00f6nen; Aaron Hertzmann; Jaakko Lehtinen; Sylvain Paris"}, {"ref_id": "b29", "title": "Deep residual learning for image recognition", "journal": "", "year": "", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b30", "title": "Conf. on Computer Vision and Pattern Recognition (CVPR)", "journal": "", "year": "2016", "authors": ""}, {"ref_id": "b31", "title": "Learning single-image 3d reconstruction by generative modelling of shape, pose and shading", "journal": "International Journal of Computer Vision (IJCV)", "year": "2019", "authors": "Paul Henderson; Vittorio Ferrari"}, {"ref_id": "b32", "title": "Unsupervised object-centric video generation and decomposition in 3d. arXiv.org", "journal": "", "year": "2007", "authors": "Paul Henderson; Christoph H Lampert"}, {"ref_id": "b33", "title": "Leveraging 2d data to learn textured 3d mesh generation", "journal": "", "year": "", "authors": "Paul Henderson; Vagia Tsiminaki; Christoph H Lampert"}, {"ref_id": "b34", "title": "Escaping plato's cave: 3d shape from adversarial rendering", "journal": "", "year": "2005", "authors": "Philipp Henzler; J Niloy; Tobias Mitra;  Ritschel"}, {"ref_id": "b35", "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium", "journal": "", "year": "2017", "authors": "Martin Heusel; Hubert Ramsauer; Thomas Unterthiner; Bernhard Nessler; Sepp Hochreiter"}, {"ref_id": "b36", "title": "On the \"steerability\" of generative adversarial networks", "journal": "", "year": "", "authors": "Ali Jahanian; Lucy Chai; Phillip Isola"}, {"ref_id": "b37", "title": "Local implicit grid representations for 3d scenes", "journal": "", "year": "", "authors": "Max Chiyu; Avneesh Jiang; Ameesh Sud; Jingwei Makadia; Matthias Huang; Thomas A Nie\u00dfner;  Funkhouser"}, {"ref_id": "b38", "title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning", "journal": "", "year": "2017", "authors": "Justin Johnson; Bharath Hariharan; Laurens Van Der Maaten; Li Fei-Fei; Lawrence Zitnick; Ross Girshick"}, {"ref_id": "b39", "title": "Ray tracing volume densities", "journal": "In ACM Trans. on Graphics", "year": "1984", "authors": "T James; Brian Kajiya;  Von Herzen"}, {"ref_id": "b40", "title": "Progressive growing of GANs for improved quality, stability, and variation", "journal": "", "year": "2018", "authors": "Tero Karras; Timo Aila; Samuli Laine; Jaakko Lehtinen"}, {"ref_id": "b41", "title": "A style-based generator architecture for generative adversarial networks", "journal": "", "year": "2005", "authors": "Tero Karras; Samuli Laine; Timo Aila"}, {"ref_id": "b42", "title": "Analyzing and improving the image quality of StyleGAN", "journal": "", "year": "2004", "authors": "Tero Karras; Samuli Laine; Miika Aittala; Janne Hellsten; Jaakko Lehtinen; Timo Aila"}, {"ref_id": "b43", "title": "Toru Matsuoka, Wadim Kehl, and Adrien Gaidon. Differentiable rendering: A survey. arXiv.org", "journal": "", "year": "2006", "authors": "Hiroharu Kato; Deniz Beker; Mihai Morariu; Takahiro Ando"}, {"ref_id": "b44", "title": "Neural 3d mesh renderer", "journal": "", "year": "2018", "authors": "Hiroharu Kato; Yoshitaka Ushiku; Tatsuya Harada"}, {"ref_id": "b45", "title": "Generating images part by part with composite generative adversarial networks. arXiv.org, 1607.05387", "journal": "", "year": "2016", "authors": "Hanock Kwak; Byoung-Tak Zhang"}, {"ref_id": "b46", "title": "High-fidelity synthesis with disentangled representation. arXiv.org", "journal": "", "year": "2001", "authors": "Wonkwang Lee; Donggyun Kim; Seunghoon Hong; Honglak Lee"}, {"ref_id": "b47", "title": "Learning object-centric representations of multi-object scenes from multiple views", "journal": "", "year": "", "authors": "Nanbo Li; Robert Fisher"}, {"ref_id": "b48", "title": "Towards unsupervised learning of generative models for 3d controllable image synthesis", "journal": "", "year": "", "authors": "Yiyi Liao; Katja Schwarz; Lars Mescheder; Andreas Geiger"}, {"ref_id": "b49", "title": "Neural sparse voxel fields", "journal": "", "year": "", "authors": "Lingjie Liu; Jiatao Gu; Tat-Seng Kyaw Zaw Lin; Christian Chua;  Theobalt"}, {"ref_id": "b50", "title": "Generative adversarial networks for image and video synthesis: Algorithms and applications. arXiv.org", "journal": "", "year": "2001", "authors": "Ming-Yu Liu; Xun Huang; Jiahui Yu; Ting-Chun Wang; Arun Mallya"}, {"ref_id": "b51", "title": "Soft rasterizer: Differentiable rendering for unsupervised singleview mesh reconstruction", "journal": "", "year": "2019", "authors": "Shichen Liu; Weikai Chen; Tianye Li; Hao Li"}, {"ref_id": "b52", "title": "Learning to infer implicit surfaces without 3d supervision", "journal": "", "year": "2019", "authors": "Shichen Liu; Shunsuke Saito; Weikai Chen; Hao Li"}, {"ref_id": "b53", "title": "DIST: rendering deep implicit signed distance function with differentiable sphere tracing", "journal": "", "year": "", "authors": "Shaohui Liu; Yinda Zhang; Songyou Peng; Boxin Shi; Marc Pollefeys; Zhaopeng Cui"}, {"ref_id": "b54", "title": "Semantic image segmentation via deep parsing network", "journal": "", "year": "2015", "authors": "Ziwei Liu; Xiaoxiao Li; Ping Luo; Chen Change Loy; Xiaoou Tang"}, {"ref_id": "b55", "title": "Challenging common assumptions in the unsupervised learning of disentangled representations", "journal": "", "year": "2019", "authors": "Francesco Locatello; Stefan Bauer; Mario Lucic; Gunnar R\u00e4tsch; Sylvain Gelly; Bernhard Sch\u00f6lkopf; Olivier Bachem"}, {"ref_id": "b56", "title": "Objectcentric learning with slot attention", "journal": "", "year": "", "authors": "Francesco Locatello; Dirk Weissenborn; Thomas Unterthiner; Aravindh Mahendran; Georg Heigold; Jakob Uszkoreit; Alexey Dosovitskiy; Thomas Kipf"}, {"ref_id": "b57", "title": "Inverse graphics GAN: learning to generate 3d shapes from unstructured 2d data", "journal": "", "year": "", "authors": "Sebastian Lunz; Yingzhen Li; Andrew W Fitzgibbon; Nate Kushman"}, {"ref_id": "b58", "title": "Rectifier nonlinearities improve neural network acoustic models", "journal": "", "year": "2013", "authors": "Andrew L Maas; Awni Y Hannun; Andrew Y Ng"}, {"ref_id": "b59", "title": "Alexey Dosovitskiy, and Daniel Duckworth", "journal": "", "year": "2008", "authors": "Ricardo Martin-Brualla; Noha Radwan; S M Mehdi; Jonathan T Sajjadi;  Barron"}, {"ref_id": "b60", "title": "Which training methods for gans do actually converge?", "journal": "", "year": "2018", "authors": "Lars Mescheder; Andreas Geiger; Sebastian Nowozin"}, {"ref_id": "b61", "title": "Occupancy networks: Learning 3d reconstruction in function space", "journal": "", "year": "2019", "authors": "Lars Mescheder; Michael Oechsle; Michael Niemeyer; Sebastian Nowozin; Andreas Geiger"}, {"ref_id": "b62", "title": "Implicit surface representations as layers in neural networks", "journal": "", "year": "2019", "authors": "Mateusz Michalkiewicz; K Jhony; Dominic Pontes; Mahsa Jack; Anders Baktashmotlagh;  Eriksson"}, {"ref_id": "b63", "title": "NeRF: Representing scenes as neural radiance fields for view synthesis", "journal": "", "year": "2008", "authors": "Ben Mildenhall; P Pratul; Matthew Srinivasan; Jonathan T Tancik; Ravi Barron; Ren Ramamoorthi;  Ng"}, {"ref_id": "b64", "title": "Rendernet: A deep convolutional network for differentiable rendering from 3d shapes", "journal": "", "year": "2018", "authors": "Thu Nguyen-Phuoc; Chuan Li; Stephen Balaban; Yong-Liang Yang"}, {"ref_id": "b65", "title": "Hologan: Unsupervised learning of 3d representations from natural images", "journal": "", "year": "2005", "authors": "Thu Nguyen-Phuoc; Chuan Li; Lucas Theis; Christian Richardt; Yong-Liang Yang"}, {"ref_id": "b66", "title": "Blockgan: Learning 3d objectaware scene representations from unlabelled images", "journal": "", "year": "2005", "authors": "Thu Nguyen-Phuoc; Christian Richardt; Long Mai; Yong-Liang Yang; Niloy Mitra"}, {"ref_id": "b67", "title": "Occupancy flow: 4d reconstruction by learning particle dynamics", "journal": "", "year": "2019", "authors": "Michael Niemeyer; Lars Mescheder; Michael Oechsle; Andreas Geiger"}, {"ref_id": "b68", "title": "Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision", "journal": "", "year": "", "authors": "Michael Niemeyer; Lars Mescheder; Michael Oechsle; Andreas Geiger"}, {"ref_id": "b69", "title": "Texture fields: Learning texture representations in function space", "journal": "", "year": "2019", "authors": "Michael Oechsle; Lars Mescheder; Michael Niemeyer; Thilo Strauss; Andreas Geiger"}, {"ref_id": "b70", "title": "Learning implicit surface light fields", "journal": "", "year": "2020", "authors": "Michael Oechsle; Michael Niemeyer; Christian Reiser; Lars Mescheder; Thilo Strauss; Andreas Geiger"}, {"ref_id": "b71", "title": "Deepsdf: Learning continuous signed distance functions for shape representation", "journal": "", "year": "", "authors": "Jeong Joon Park; Peter Florence; Julian Straub; Richard A Newcombe; Steven Lovegrove"}, {"ref_id": "b72", "title": "Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)", "journal": "", "year": "2019", "authors": ""}, {"ref_id": "b73", "title": "Photoshape: Photorealistic materials for large-scale shape collections", "journal": "Communications of the ACM", "year": "2018", "authors": "Keunhong Park; Konstantinos Rematas; Ali Farhadi; Steven M Seitz"}, {"ref_id": "b74", "title": "The hessian penalty: A weak prior for unsupervised disentanglement", "journal": "", "year": "2002", "authors": "William S Peebles; John Peebles; Jun-Yan Zhu; Alexei A Efros; Antonio Torralba"}, {"ref_id": "b75", "title": "Convolutional occupancy networks", "journal": "", "year": "", "authors": "Songyou Peng; Michael Niemeyer; Lars Mescheder; Marc Pollefeys; Andreas Geiger"}, {"ref_id": "b76", "title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "journal": "", "year": "2016", "authors": "Alec Radford; Luke Metz; Soumith Chintala"}, {"ref_id": "b77", "title": "Learning to disentangle factors of variation with manifold interaction", "journal": "", "year": "2014", "authors": "Scott Reed; Kihyuk Sohn; Yuting Zhang; Honglak Lee"}, {"ref_id": "b78", "title": "Unsupervised learning of 3d structure from images", "journal": "", "year": "2016", "authors": "Danilo Jimenez Rezende; S M Ali Eslami; Shakir Mohamed; Peter Battaglia; Max Jaderberg; Nicolas Heess"}, {"ref_id": "b79", "title": "Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization", "journal": "", "year": "2019", "authors": "Shunsuke Saito; Zeng Huang; Ryota Natsume; Shigeo Morishima; Angjoo Kanazawa; Hao Li"}, {"ref_id": "b80", "title": "Graf: Generative radiance fields for 3d-aware image synthesis", "journal": "", "year": "2008", "authors": "Katja Schwarz; Yiyi Liao; Michael Niemeyer; Andreas Geiger"}, {"ref_id": "b81", "title": "Interpreting the latent space of gans for semantic face editing", "journal": "", "year": "", "authors": "Yujun Shen; Jinjin Gu; Xiaoou Tang; Bolei Zhou"}, {"ref_id": "b82", "title": "Implicit neural representations with periodic activation functions", "journal": "", "year": "", "authors": "Vincent Sitzmann; N P Julien; Alexander W Martel; David B Bergman; Gordon Lindell;  Wetzstein"}, {"ref_id": "b83", "title": "Deepvoxels: Learning persistent 3d feature embeddings", "journal": "", "year": "2019", "authors": "Vincent Sitzmann; Justus Thies; Felix Heide; Matthias Nie\u00dfner; Gordon Wetzstein; Michael Zollh\u00f6fer"}, {"ref_id": "b84", "title": "Scene representation networks: Continuous 3d-structure-aware neural scene representations", "journal": "", "year": "2019", "authors": "Vincent Sitzmann; Michael Zollh\u00f6fer; Gordon Wetzstein"}, {"ref_id": "b85", "title": "Fourier features let networks learn high frequency functions in low dimensional domains", "journal": "", "year": "2008", "authors": "Matthew Tancik; P Pratul; Ben Srinivasan; Sara Mildenhall; Nithin Fridovich-Keil; Utkarsh Raghavan; Ravi Singhal; Jonathan T Ramamoorthi; Ren Barron;  Ng"}, {"ref_id": "b86", "title": "State of the art on neural rendering", "journal": "Computer Graphics Forum", "year": "2020", "authors": "Ayush Tewari; Ohad Fried; Justus Thies; Vincent Sitzmann; Stephen Lombardi; Kalyan Sunkavalli; Ricardo Martin-Brualla; Tomas Simon; Jason M Saragih; Matthias Nie\u00dfner; Rohit Pandey; Sean Ryan Fanello; Gordon Wetzstein; Jun-Yan Zhu; Christian Theobalt; Maneesh Agrawala; Eli Shechtman; Dan B Goldman; Michael Zollh\u00f6fer"}, {"ref_id": "b87", "title": "Deferred neural rendering: image synthesis using neural textures", "journal": "ACM Trans. on Graphics", "year": "2019", "authors": "Justus Thies; Michael Zollh\u00f6fer; Matthias Nie\u00dfner"}, {"ref_id": "b88", "title": "Lecture 6.5-RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning", "journal": "", "year": "2012", "authors": "T Tieleman; G Hinton"}, {"ref_id": "b89", "title": "Investigating object compositionality in generative adversarial networks", "journal": "Neural Networks", "year": "", "authors": "Karol Sjoerd Van Steenkiste; J\u00fcrgen Kurach; Sylvain Schmidhuber;  Gelly"}, {"ref_id": "b90", "title": "Generative image modeling using style and structure adversarial networks", "journal": "", "year": "2016", "authors": "Xiaolong Wang; Abhinav Gupta"}, {"ref_id": "b91", "title": "Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling", "journal": "", "year": "2016", "authors": "Jiajun Wu; Chengkai Zhang; Tianfan Xue; Bill Freeman; Josh Tenenbaum"}, {"ref_id": "b92", "title": "Empirical evaluation of rectified activations in convolutional network", "journal": "", "year": "2015", "authors": "Bing Xu; Naiyan Wang; Tianqi Chen; Mu Li"}, {"ref_id": "b93", "title": "LR-GAN: layered recursive generative adversarial networks for image generation", "journal": "", "year": "2017", "authors": "Jianwei Yang; Anitha Kannan; Dhruv Batra; Devi Parikh"}, {"ref_id": "b94", "title": "Dense, accurate optical flow estimation with piecewise parametric model", "journal": "", "year": "2015", "authors": "Jiaolong Yang; Hongdong Li"}, {"ref_id": "b95", "title": "Multiview neural surface reconstruction by disentangling geometry and appearance", "journal": "", "year": "", "authors": "Lior Yariv; Yoni Kasten; Dror Moran; Meirav Galun; Matan Atzmon; Ronen Basri; Yaron Lipman"}, {"ref_id": "b96", "title": "The unusual effectiveness of averaging in GAN training", "journal": "", "year": "2019", "authors": "Yasin Yazici; Chuan-Sheng Foo; Stefan Winkler; Kim-Hui Yap; Georgios Piliouras; Vijay Chandrasekhar"}, {"ref_id": "b97", "title": "Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv.org", "journal": "", "year": "2015", "authors": "Fisher Yu; Yinda Zhang; Shuran Song; Ari Seff; Jianxiong Xiao;  Lsun"}, {"ref_id": "b98", "title": "Shape and motion under varying illumination: Unifying structure from motion, photometric stereo, and multiview stereo", "journal": "", "year": "2003", "authors": "Li Zhang; Brian Curless; Aaron Hertzmann; Steven M Seitz"}, {"ref_id": "b99", "title": "Image gans meet differentiable rendering for inverse graphics and interpretable 3d neural rendering. arXiv.org", "journal": "", "year": "2010", "authors": "Yuxuan Zhang; Wenzheng Chen; Huan Ling; Jun Gao; Yinan Zhang; Antonio Torralba; Sanja Fidler"}, {"ref_id": "b100", "title": "Modular generative adversarial networks", "journal": "", "year": "2018", "authors": "Bo Zhao; Bo Chang; Zequn Jie; Leonid Sigal"}, {"ref_id": "b101", "title": "Learning a discriminative model for the perception of realism in composite images", "journal": "", "year": "2015", "authors": "Jun-Yan Zhu; Philipp Kr\u00e4henb\u00fchl; Eli Shechtman; Alexei A Efros"}, {"ref_id": "b102", "title": "Visual object networks: Image generation with disentangled 3d representations", "journal": "", "year": "2018", "authors": "Jun-Yan Zhu; Zhoutong Zhang; Chengkai Zhang; Jiajun Wu; Antonio Torralba; Josh Tenenbaum; Bill Freeman"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Overview. We represent scenes as compositional generative neural feature fields. For a randomly sampled camera, we volume render a feature image of the scene based on individual feature fields. A 2D neural rendering network converts the feature image into an RGB image.While training only on raw image collections, at test time we are able to control the image formation process wrt. camera pose, object poses, as well as the objects' shapes and appearances. Further, our model generalizes beyond the training data, e.g. we can synthesize scenes with more objects than were present in the training images. Note that for clarity we visualize volumes in color instead of features.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "arXiv:2011.12100v2 [cs.CV] 29 Apr 2021 (a) Translation of Left Object (2D-based Method [71]) (b) Translation of Left Object (Ours) (c) Circular Translation (Ours) (d) Add Objects (Ours)", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure3: GIRAFFE. Our generator G \u03b8 takes a camera pose \u03be and N shape and appearance codes z i s , z i a and affine transformations T i as input and synthesizes an image of the generated scene which consists of N \u2212 1 objects and a background. The discriminator D \u03c6 takes the generated image\u00ce and the real image I as input and our full model is trained with an adversarial loss. At test time, we can control the camera pose, the shape and appearance codes of the objects, and the objects' poses in the scene. Orange indicates learnable and blue non-learnable operations.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: Scene Disentanglement. From top to bottom, we show only backgrounds, only objects, color-coded object alpha maps, and the final synthesized images at 64 2 pixel resolution. Disentanglement emerges without supervision, and the model learns to generate plausible backgrounds although the training data only contains images with objects.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure 6: Training Progression. We show renderings of our model on Clevr-2345 at 256 2 pixels after 0, 1, 2, 3, 10, and 100-thousand iterations. Unsupervised disentanglement emerges already at the very beginning of training.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Circular Translation of One Object Around Another Object", "figure_data": ""}, {"figure_label": "78", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 7 :Figure 8 :78Figure 7: Controllable Scene Generation at 256 2 Pixel Resolution.Controlling the generated scenes during image synthesis: Here we rotate or translate objects, change their appearances, and perform complex operations like circular translations.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Quantitative Comparison. We report the FID score (\u2193) at 64 2 pixels for baselines and our method.", "figure_data": "CelebA-HQ FFHQ Cars Churches Clevr-2HoloGAN [63]611923458241w/o 3D Conv33704966273GRAF [77]49599587106Ours2132263031"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Quantitative Comparison. We report the FID score (\u2193) at 256 2 pixels for the strongest 3D-aware baselines and our method.", "figure_data": "2D GAN Plat. GAN BlockGAN HoloGAN GRAF Ours1.69381.564.447.800.680.41"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Network Parameter Comparison. We report the number of generator network parameters in million.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Full -Skip -Act. +NN. RGB Up. +Bi. Feat. Up. 16.16 16.66 21.61 17.28 20.68", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Ablation Study. We report FID (\u2193) on CompCars without RGB skip connections (-Skip), without final activation (-Act.), with nearest neighbor instead of bilinear image upsampling (+ NN. RGB Up.), and with bilinear instead of nearest neighbor feature upsampling (+ Bi. Feat. Up.). Figure10: Neural Renderer. We change the background while keeping the foreground object fixed for our method at 256 2 pixel resolution. Note how the neural renderer realistically adapts the objects' appearances to the background. In contrast to random Fourier features[82], axis-aligned positional encoding (1) encourages the model to learn objects in a canonical pose. similar wrt. network size and training time (see Tab. 3).", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u03b3(t, L) = (sin(2 0 t\u03c0), cos(2 0 t\u03c0), . . . , sin(2 L t\u03c0), cos(2 L t\u03c0))(1)", "formula_coordinates": [3.0, 62.33, 519.31, 224.04, 25.19]}, {"formula_id": "formula_1", "formula_text": "f \u03b8 : R Lx \u00d7 R L d \u2192 R + \u00d7 R 3 (\u03b3(x), \u03b3(d)) \u2192 (\u03c3, c)(2)", "formula_coordinates": [3.0, 109.0, 683.59, 177.37, 25.75]}, {"formula_id": "formula_2", "formula_text": "g \u03b8 : R Lx \u00d7 R L d \u00d7 R Ms \u00d7 R Ma \u2192 R + \u00d7 R 3 (\u03b3(x), \u03b3(d), z s , z a ) \u2192 (\u03c3, c)(3)", "formula_coordinates": [3.0, 336.25, 181.82, 208.87, 26.67]}, {"formula_id": "formula_3", "formula_text": "h \u03b8 : R Lx \u00d7 R L d \u00d7 R Ms \u00d7 R Ma \u2192 R + \u00d7 R M f (\u03b3(x), \u03b3(d), z s , z a ) \u2192 (\u03c3, f )(4)", "formula_coordinates": [3.0, 325.88, 297.63, 219.23, 26.67]}, {"formula_id": "formula_5", "formula_text": "k(x) = R \u2022 \uf8ee \uf8f0 s 1 s 2 s 3 \uf8f9 \uf8fb \u2022 x + t(6)", "formula_coordinates": [3.0, 357.55, 519.71, 187.56, 35.13]}, {"formula_id": "formula_6", "formula_text": "(\u03c3, f ) = h \u03b8 (\u03b3(k \u22121 (x)), \u03b3(k \u22121 (d)), z s , z a )(7)", "formula_coordinates": [3.0, 340.33, 598.11, 204.78, 11.72]}, {"formula_id": "formula_7", "formula_text": "C(x, d) = \u03c3, 1 \u03c3 N i=1 \u03c3 i f i , where \u03c3 = N i=1 \u03c3 i (8)", "formula_coordinates": [4.0, 63.07, 475.21, 223.29, 30.32]}, {"formula_id": "formula_8", "formula_text": "\u03c0 vol : (R + \u00d7 R M f ) Ns \u2192 R M f , {\u03c3 j , f j } Ns j=1 \u2192 f (9)", "formula_coordinates": [4.0, 60.71, 697.68, 225.65, 13.32]}, {"formula_id": "formula_9", "formula_text": "f = Ns j=1 \u03c4 j \u03b1 j f j \u03c4 j = j\u22121 k=1 (1 \u2212 \u03b1 k ) \u03b1 j = 1 \u2212 e \u2212\u03c3j \u03b4j(10)", "formula_coordinates": [4.0, 316.37, 310.08, 228.74, 43.31]}, {"formula_id": "formula_10", "formula_text": "\u03c0 neural \u03b8 : R H V \u00d7W V \u00d7M f \u2192 R H\u00d7W \u00d73(11)", "formula_coordinates": [4.0, 354.1, 502.63, 191.01, 12.69]}, {"formula_id": "formula_11", "formula_text": "I V \u2208 R H V \u00d7W V \u00d7M f", "formula_coordinates": [4.0, 463.41, 523.29, 80.12, 11.23]}, {"formula_id": "formula_12", "formula_text": "G \u03b8 ({z i s , z i a , T i } N i=1 , \u03be) = \u03c0 neural \u03b8 (I V )", "formula_coordinates": [5.0, 70.51, 424.99, 148.91, 12.69]}, {"formula_id": "formula_13", "formula_text": "I V = {\u03c0 vol ({C(x jk , d k )} Ns j=1 )} H V \u00d7W V k=1 (12", "formula_coordinates": [5.0, 97.02, 435.89, 185.19, 19.11]}, {"formula_id": "formula_14", "formula_text": ")", "formula_coordinates": [5.0, 282.21, 435.89, 4.15, 8.64]}, {"formula_id": "formula_15", "formula_text": "E z i s ,z i a \u223cN , \u03be\u223cp \u03be , Ti\u223cp T f (D \u03c6 (G \u03b8 ({z i s , z i a , T i } i , \u03be)) + E I\u223cp D f (\u2212D \u03c6 (I)) \u2212 \u03bb \u2207D \u03c6 (I) 2 (13)", "formula_coordinates": [5.0, 319.83, 110.88, 225.28, 46.22]}], "doi": ""}