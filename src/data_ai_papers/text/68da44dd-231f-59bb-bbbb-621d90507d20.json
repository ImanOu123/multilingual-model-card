{"title": "ProFormer: Towards On-Device LSH Projection Based Transformers", "authors": "Chinnadhurai Sankar; Sujith Ravi; Zornitsa Kozareva", "pub_date": "", "abstract": "At the heart of text based neural models lay word representations, which are powerful but occupy a lot of memory making it challenging to deploy to devices with memory constraints such as mobile phones, watches and IoT. To surmount these challenges, we introduce ProFormer -a projection based transformer architecture that is faster and lighter making it suitable to deploy to memory constraint devices and preserve user privacy. We use LSH projection layer to dynamically generate word representations on-the-fly without embedding lookup tables leading to significant memory footprint reduction from O(V.d) to O(T ), where V is the vocabulary size, d is the embedding dimension size and T is the dimension of the LSH projection representation. We also propose a local projection attention (LPA) layer, which uses self-attention to transform the input sequence of N LSH word projections into a sequence of N/K representations reducing the computations quadratically by O(K 2 ). We evaluate ProFormer on multiple text classification tasks and observed improvements over prior state-of-the-art on-device approaches for short text classification and comparable performance for long text classification tasks. Pro-Former is also competitive with other popular but highly resource-intensive approaches like BERT and even outperforms small-sized BERT variants with significant resource savings -reduces the embedding memory footprint from 92.16 MB to 1.7 KB and requires 16\u00d7 less computation overhead, which is very impressive making it the fastest and smallest on-device model.", "sections": [{"heading": "Introduction", "text": "Transformers (Vaswani et al., 2017) based architectures like BERT (Devlin et al., 2018), XL-net (Yang et al., 2019), GPT-2 (Radford et al., 2019), MT-DNN (Liu et al., 2019a), RoBERTA (Liu et al., 2019b) reached state-of-the-art performance on tasks like machine translation (Arivazhagan et al., 2019), language modelling (Radford et al., 2019), text classification benchmarks like GLUE (Wang et al., 2018). However, these models require huge amount of memory and need high computational requirements making it hard to deploy to small memory constraint devices such as mobile phones, watches and IoT. Recently, there have been interests in making BERT lighter and faster (Sanh et al., 2019;McCarley, 2019). In parallel, recent on-device works like SGNN (Ravi and Kozareva, 2018), SGNN++  and (Sankar et al., 2019) produce lightweight models with extremely low memory footprint. They employ a modified form of LSH projection to dynamically generate a fixed binary projection representation, P(x) \u2208 [0, 1] T for the input text x using word or character n-grams and skip-grams features, and a 2-layer MLP + softmax layer for classification. As shown in (Ravi and Kozareva, 2018) these models are suitable for short sentence lengths as they compute T bit LSH projection vector to represent the entire sentence. However,  showed that such models cannot handle long text due to significant information loss in the projection operation.\nOn another side, recurrent architectures represent long sentences well, but the sequential nature of the computations increases latency requirements and makes it difficult to launch on-device. Recently, self-attention based architectures like BERT (Devlin et al., 2018) have demonstrated remarkable success in capturing long term dependencies in the input text via purely attention mechanisms. BERT's model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation in (Vaswani et al., 2017). The self-attention scores can be computed in parallel as they do not have recurrent mechanisms. But usually these architectures are very deep and the amount of computation is quadratic in the order of O(L \u2022 N 2 ), where L is the number of layers (Transformer blocks) and N is the input sentence length. Straightforward solutions like reducing the number of layers is insufficient to launch transformers on-device due to the large memory and quadratic computation requirements.\nIn this paper, we introduce a projection-based neural architecture ProFormer that is designed to (a) be efficient and learn compact neural representations (b) handle out of vocabulary words and misspellings (c) drastically reduce embedding memory footprint from hundreds of megabytes to few kilobytes and (d) reduce the computation overhead quadratically by introducing a local attention layer which reduces the intermediate sequence length by a constant factor, K. We achieve this by bringing the best of both worlds by combining LSH projection based representations (for low memory footprint) and self-attention based architectures (to model dependencies in long sentences). To tackle computation overheard in the transformer based models, we reduce the number of self-attention layers and additionally introduce an intermediate local projection attention (LPA) to quadratically reduce the number of self-attention operations. The main contributions of our paper are:\n\u2022 We propose novel on-device neural network called ProFormer which combines LSH projection based text representations, with transformer architecture and locally projected selfattention mechanism that captures long range sentence dependencies while yielding low memory footprint and low computation overhead.\n\u2022 ProFormer reduces the computation overhead O(L \u2022 N 2 ) and latency in multiple ways: by reducing the number of layers L from twelve to two and introducing new local projection attention layer that decreases number of selfattention operations by a quadratic factor.\n\u2022 ProFormer is light weigh compact on-device model, while BERT on-device still needs huge embedding table ( 92.16 MB for V = 30k, d = 768) with number of computation flops in the order of O(L \u2022 N 2 ), where L is the number of layers, N is the number of words in the input sentence.\n\u2022 We conduct empirical evaluations and comparisons against state-of-the-art on-device and prior deep learning approaches for short and long text classification. Our model ProFormer reached state-of-art performance for short text and comparable performance for long texts, while maintaining small memory footprint and computation requirements.", "publication_ref": ["b23", "b2", "b25", "b10", "b11", "b0", "b14", "b24", "b19", "b12", "b17", "b20", "b17", "b2", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "ProFormer: LSH Projection based Transformers", "text": "In this section, we show the overall architecture of ProFormer in Figure 1. ProFormer consists of multiple parts: (1) word-level Locality Sensitive Hashing (LSH) projection layer, (2) local projection attention (LPA) layer, (3) transformer layer (Devlin et al., 2018) and ( 4) a max-pooling + classifier layer. Next, we describe each layer in detail. ", "publication_ref": ["b2"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "LSH Projection Layer", "text": "It is a common practice to represent each word in the input sentence, x = [w 1 , w 2 , \u2022 \u2022 \u2022 , w N ] as an embedding vector based on its one-hot representation. Instead, we adopt LSH projection layer from (Ravi, 2017(Ravi, , 2019 which dynamically generates a T bit representation, P(w i ) \u2208 [0, 1] T for the input word, w i based on its morphological features like n-grams, skip-grams from the current and context words, parts-of-speech tags, etc. Since the LSH projection based approach does not rely on embedding lookup tables to compute word representation, we obtain significant memory savings of the order, O(V \u2022 d), where V is the vocabulary size and d is the embedding dimension. For instance, the embedding look-up table occupies 92.16 MB (V = 30k, d = 768 (Devlin et al., 2018)), while the LSH projection layer requires only \u2248 1.7 KB (T = 420) as shown in Table 1. ", "publication_ref": ["b15", "b16", "b2"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Models Embedding memory Computations", "text": "BERT O(V.d) O(N 2 ) ProFormer (our model) O(T ) O(N 2 /K 2 )", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Local Projection Attention (LPA) Layer", "text": "The LPA layer shown in Figure 2 consists of a single layer multi-headed self-attention layer similar to the Transformer architecture in (Vaswani et al., 2017) followed by a max-pooling layer yielding a compressed representation of K input words, The LPA layer transforms the N word-level projections, P(w i ) to a sequence of N/K representations as in Equation 1.\n[w 1 , w 2 , \u2022 \u2022 \u2022 w K ].\n[P(w 1 ),\n\u2022\u2022\u2022 P(w N )] N \u2212\u2192 [LPA(P(w 1:K )), \u2022\u2022\u2022 LPA(P(w N/K:N ))] N/K (1)\nwhere LPA consists of the self-attention and maxpooling operation, K is a Group factor 1 . We equally divide the N word-level LSH projection representations into N/K groups of size K. The LPA layer compresses each group of K word representations into LPA(P(w 1:K )) \u2208 R d yielding 1 We choose K such that N is divisible by K.\nN/K representations in total. The LPA layer reduces the self-attention computation overhead in the subsequent transformer layer (Vaswani et al., 2017) by O(K 2 ).", "publication_ref": ["b23", "b23"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Transformer Layer", "text": "This layer consists of 2-layer bidirectional Transformer encoder based on the original implementation described in (Vaswani et al., 2017). This layer transforms the N/K input representations from the LPA layer described in the previous sub-section into N/K output representations. In this layer, we reduce both the computation overhead and memory footprint by reducing the number of layers from L to 2 reducing the computation overhead by O(L/2) (6 times in the case of 12-layer BERT-base model).", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "Max-Pooling and Classification Layer", "text": "We summarize the N/K representations from the transformer layer to get a single d dimensional vector by max-pooling across the N/K time-steps, followed by a softmax layer to predict the output class Y .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Datasets & Experimental Setup", "text": "In this section, we describe our datasets and experimental setup. We use text classification datasets from state-of-the-art on-device evaluations such as: MRDA (Shriberg et al., 2004) and ATIS (T\u00fcr et al., 2010), AG News (Zhang et al., 2015a) and Yahoo! Answers (Zhang et al., 2015a). Table 2 shows the characteristics of each dataset.  We train ProFormer on multiple classification tasks individually and report Accuracy on corresponding test sets. We fix the projection size, T = 420, n-gram size=5, skip-gram size=1 for the LSH projection operation, P. For the LPA layer, We experiment with two values for K = 1, 4, where K = 1 corresponds to the null operation in the LPA layer which just passes the word LSH projection representation to the Transformer layer. For the transformer layer, we fix the number of layers, L = 2 and set all layer sizes, d = 768 (including the intermediate size for the dense layer). 2 We compare our model with previous state of the art neural architectures, including on-device approaches. We also fine-tune the pretrained 12layer BERT-base model (Devlin et al., 2018) on all classification tasks and compare to our model. BERT-base consists 12-layers of transformer blocks (Vaswani et al., 2017) and is pretrained in an unsupervised manner on a large corpus (BooksCorpus (Zhu et al., 2015) and English WikiPedia) using masked-language model objective. We fine-tune the pretrained BERT-base (Devlin et al., 2018) to each of the classification tasks. For training, we use Adam with learning rate of 1e-4, \u03b2 1 =0.9, \u03b2 2 =0.999, L2 weight decay of 0.01, learning rate warmup over the first 10, 000 steps, and linear decay of the learning rate. We use dropout probability of 0.1 on all layers and training batch size of 256. For further comparison, we also trained much smaller BERT baselines with 2-layers of transformer blocks and smaller input embedding sizes.", "publication_ref": ["b21", "b22", "b26", "b26", "b2", "b23", "b28", "b2"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Results", "text": "Tables 3 and 4 show the results on the ATIS & MRDA short text classification and AG & Y!A long text classification tasks. We compare our approach, ProFormer against prior state-of-the-art on-device works, fine-tuned BERT-base, smaller 2-layer BERT variants and other non-on-device neural approaches.\nOverall, our model ProFormer improved upon non-on-device neural models while keeping very small memory footprint and high accuracy. This is very impressive since ProFormer can be directly deployed to memory constraint devices like phones, watches and IoT while still maintaining high accuracy. ProFormer also improved upon prior on-device state-of-the-art neural approaches like SGNN (Ravi and Kozareva, 2018) and SGNN++  reaching over 35% improvement on long text classification. Similarly it improved over on-device ProSeqo ) models for all datasets and reached comparable performance on MRDA. In addition to the quality improvements, ProFormer also keeps smaller memory footprint than ProSeqo, SGNN and SGNN++.\nIn addition to the non-on-device and on-device neural comparisons, we also compare against BERT-base and other smaller variants. Our experiments show that ProFormer outperforms the bert conf ig.json in BERT-base model (Devlin et al., 2018) small BERT baselines on all tasks. Moreover, although the 12-layer fine-tuned BERT-base (Devlin et al., 2018) model converged to the state-of-the-art in almost all of the tasks, ProFormer converges to \u2248 97.2% BERT-base's performance on an average while occupying only 13% of BERT-base's memory. ProFormer has 14.4 million parameters, while BERT-base has 110 million. For fair comparison, we also test ProFormer with K = 4, which only occupies 38.4% the memory footprint of 2-layer BERT-base model and reduces the computation overhead by 16 times. The embedding look up table occupies nearly 23 million parameters out of 38 million parameters in the 2-layer BERT model. We notice that K=4 model performs slightly worse than K=1 indicating information loss in the LPA layer. Overall, our experiments demonstrate that ProFormer reaches better performances that prior non-on-device and on-device neural approaches, and comparable performance to BERT-base models while preserving smaller memory footprint.   (Ravi and Kozareva, 2018)(on-device) 57.6 36.5 FastText-full (Joulin et al., 2016) 92.5 72.3 CharCNNLargeWithThesau. (Zhang et al., 2015b) 90.6 71.2 CNN+NGM (Bui et al., 2018) 86.9 -LSTM-full (Zhang et al., 2015b) 86.1 70.8 ", "publication_ref": ["b17", "b2", "b2", "b5", "b27", "b1", "b27"], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Conclusion", "text": "We proposed a novel on-device neural network Pro-Former, which combines LSH projection based text representations, with trans-former architecture and locally projected self-attention mechanism that captures long range sentence dependencies. Overall, ProFormer yields low memory footprint and reduces computations quadratically. In series of experimental evaluations on short and long text classifications we show that ProFormer improved upon prior neural models and on-device work like SGNN (Ravi and Kozareva, 2018), SGNN++  and ProSeqo . ProFormer reached comparable performance to our BERT-base implementation, however it produced magnitudes more compact models than BERT-base. This is very impressive showing both effectiveness and compactness of our neural model.", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Massively multilingual neural machine translation in the wild: Findings and challenges", "journal": "", "year": "1907", "authors": "Naveen Arivazhagan; Ankur Bapna; Orhan Firat; Dmitry Lepikhin; Melvin Johnson; Maxim Krikun; Mia Xu Chen; Yuan Cao; George Foster; Colin Cherry; Wolfgang Macherey; Zhifeng Chen; Yonghui Wu"}, {"ref_id": "b1", "title": "Neural graph learning: Training neural networks using graphs", "journal": "", "year": "2018", "authors": "D Thang; Sujith Bui; Vivek Ravi;  Ramavajjala"}, {"ref_id": "b2", "title": "BERT: pre-training of deep bidirectional transformers for language understanding", "journal": "CoRR", "year": "2018", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b3", "title": "Slot-gated modeling for joint slot filling and intent prediction", "journal": "", "year": "2018", "authors": "Guang Chih-Wen Goo; Yun-Kai Gao; Chih-Li Hsu; Tsung-Chieh Huo; Keng-Wei Chen; Yun-Nung Hsu;  Chen"}, {"ref_id": "b4", "title": "Multi-domain joint semantic frame parsing using bi-directional rnn-lstm", "journal": "", "year": "2016", "authors": "Dilek Hakkani-Tur; Gokhan Tur; Asli Celikyilmaz; Yun-Nung Vivian Chen; Jianfeng Gao; Li Deng; Ye-Yi Wang"}, {"ref_id": "b5", "title": "Fasttext.zip: Compressing text classification models", "journal": "CoRR", "year": "2016", "authors": "Armand Joulin; Edouard Grave; Piotr Bojanowski; Matthijs Douze; Herv\u00e9 J\u00e9gou; Tomas Mikolov"}, {"ref_id": "b6", "title": "Dialogue act classification in domain-independent conversations using a deep recurrent neural network", "journal": "", "year": "2016", "authors": "Hamed Khanpour; Nishitha Guntakandla; Rodney Nielsen"}, {"ref_id": "b7", "title": "ProSeqo: Projection sequence networks for on-device text classification", "journal": "", "year": "2019", "authors": "Zornitsa Kozareva; Sujith Ravi"}, {"ref_id": "b8", "title": "Sequential short-text classification with recurrent and convolutional neural networks", "journal": "", "year": "2016", "authors": "Ji Young Lee; Franck Dernoncourt"}, {"ref_id": "b9", "title": "Attention-based recurrent neural network models for joint intent detection and slot filling", "journal": "", "year": "2016", "authors": "Bing Liu; Ian Lane"}, {"ref_id": "b10", "title": "Multi-task deep neural networks for natural language understanding. CoRR, abs", "journal": "", "year": "1901", "authors": "Xiaodong Liu; Pengcheng He; Weizhu Chen; Jianfeng Gao"}, {"ref_id": "b11", "title": "Roberta: A robustly optimized BERT pretraining approach", "journal": "", "year": "1907", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b12", "title": "Pruning a bert-based question answering model", "journal": "ArXiv", "year": "2019", "authors": "J ; Scott Mccarley"}, {"ref_id": "b13", "title": "Neuralbased context representation learning for dialog act classification", "journal": "", "year": "2017", "authors": "Daniel Ortega; Ngoc Thang Vu"}, {"ref_id": "b14", "title": "Language models are unsupervised multitask learners", "journal": "", "year": "2019", "authors": "Alec Radford; Jeffrey Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"ref_id": "b15", "title": "Projectionnet: Learning efficient on-device deep networks using neural projections", "journal": "CoRR", "year": "2017", "authors": "Sujith Ravi"}, {"ref_id": "b16", "title": "Efficient on-device models using neural projections", "journal": "", "year": "2019", "authors": "Sujith Ravi"}, {"ref_id": "b17", "title": "Selfgoverning neural networks for on-device short text classification", "journal": "", "year": "2018-10-31", "authors": "Sujith Ravi; Zornitsa Kozareva"}, {"ref_id": "b18", "title": "On-device structured and context partitioned projection networks", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Sujith Ravi; Zornitsa Kozareva"}, {"ref_id": "b19", "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter", "journal": "ArXiv", "year": "2019", "authors": "Victor Sanh; Lysandre Debut; Julien Chaumond; Thomas Wolf"}, {"ref_id": "b20", "title": "Transferable neural projection representations", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Chinnadhurai Sankar; Sujith Ravi; Zornitsa Kozareva"}, {"ref_id": "b21", "title": "The ICSI meeting recorder dialog act (MRDA) corpus", "journal": "", "year": "2004-04-30", "authors": "Elizabeth Shriberg; Rajdip Dhillon; Sonali Bhagat; Jeremy Ang; Hannah Carvey"}, {"ref_id": "b22", "title": "What is left to be understood in atis?", "journal": "", "year": "2010", "authors": "G\u00f6khan T\u00fcr; Dilek Hakkani-T\u00fcr; Larry P Heck"}, {"ref_id": "b23", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b24", "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding", "journal": "CoRR", "year": "2018", "authors": "Alex Wang; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel R Bowman"}, {"ref_id": "b25", "title": "Xlnet: Generalized autoregressive pretraining for language understanding", "journal": "", "year": "1906", "authors": "Zhilin Yang; Zihang Dai; Yiming Yang; Jaime G Carbonell; Ruslan Salakhutdinov; V Quoc;  Le"}, {"ref_id": "b26", "title": "Character-level convolutional networks for text classification", "journal": "", "year": "2015", "authors": "Xiang Zhang; Junbo Zhao; Yann Lecun"}, {"ref_id": "b27", "title": "Character-level convolutional networks for text classification", "journal": "MIT Press", "year": "2015", "authors": "Xiang Zhang; Junbo Zhao; Yann Lecun"}, {"ref_id": "b28", "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "journal": "", "year": "2015-12-07", "authors": "Yukun Zhu; Ryan Kiros; Richard S Zemel; Ruslan Salakhutdinov; Raquel Urtasun; Antonio Torralba; Sanja Fidler"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: ProFormer: Our Projection Transformer Network Architecture", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Local Projection Attention (LPA) layer.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Memory and computations overhead comparison between BERT (Devlin et al., 2018)  and ProFormer (our model). N is the number of words in the input. For V = 30k, d = 768, T = 420, BERT's embedding table occupies 92.16 MB while ProFormer requires only 1.7 KB. For K = 4, we reduce the BERT computation overhead by 16 times.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Short text classification results.", "figure_data": "ModelsAG Y!AProFormer (K=1) (our model)92.0 72.8ProFormer (K=4) (our model)91.5 71.1BERT-base + fine-tuned (Devlin et al., 2018)94.5 73.8(12-layers, embedding size = 768)BERT (2-layer, embedding size = 560)82.3-BERT (2-layer, embedding size = 840)83.3-ProSeqo (Kozareva and Ravi, 2019)(on-device)91.5 72.4SGNN"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Long text classification results.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "BERT O(V.d) O(N 2 ) ProFormer (our model) O(T ) O(N 2 /K 2 )", "formula_coordinates": [3.0, 78.09, 166.56, 196.16, 17.72]}, {"formula_id": "formula_1", "formula_text": "[w 1 , w 2 , \u2022 \u2022 \u2022 w K ].", "formula_coordinates": [3.0, 72.0, 391.17, 77.76, 10.68]}, {"formula_id": "formula_2", "formula_text": "\u2022\u2022\u2022 P(w N )] N \u2212\u2192 [LPA(P(w 1:K )), \u2022\u2022\u2022 LPA(P(w N/K:N ))] N/K (1)", "formula_coordinates": [3.0, 109.31, 626.96, 180.96, 24.88]}], "doi": "10.18653/v1/D19-1402"}