{"title": "", "authors": "Quentin Garrido; Yubei Chen; Adrien Bardes; Laurent Najman; Yann Lecun; Meta Ai -Fair", "pub_date": "2023-06-26", "abstract": "Recent approaches in self-supervised learning of image representations can be categorized into different families of methods and, in particular, can be divided into contrastive and non-contrastive approaches. While differences between the two families have been thoroughly discussed to motivate new approaches, we focus more on the theoretical similarities between them. By designing contrastive and covariance based non-contrastive criteria that can be related algebraically and shown to be equivalent under limited assumptions, we show how close those families can be. We further study popular methods and introduce variations of them, allowing us to relate this theoretical result to current practices and show the influence (or lack thereof) of design choices on downstream performance. Motivated by our equivalence result, we investigate the low performance of SimCLR and show how it can match VICReg's with careful hyperparameter tuning, improving significantly over known baselines. We also challenge the popular assumption that non-contrastive methods need large output dimensions. Our theoretical and quantitative results suggest that the numerical gaps between contrastive and noncontrastive methods in certain regimes can be closed given better network design choices and hyperparameter tuning. The evidence shows that unifying different SOTA methods is an important direction to build a better understanding of selfsupervised learning.", "sections": [{"heading": "INTRODUCTION", "text": "Self-supervised learning (SSL) of image representations has shown significant progress in the last few years (Chen et al., 2020a;Chen et al., 2020b;Grill et al., 2020;Lee et al., 2021b;Caron et al., 2020;Zbontar et al., 2021;Bardes et al., 2021;Tomasev et al., 2022;Caron et al., 2021;Chen et al., 2021b;Li et al., 2022a;Zhou et al., 2022a;b;HaoChen et al., 2021), approaching, and sometime even surpassing, the performance of supervised baselines on many downstream tasks. Most recent approaches are based on the joint-embedding framework with a siamese network architecture (Bromley et al., 1994) which are divided into two main categories, contrastive and non-contrastive methods. Contrastive methods bring together embeddings of different views of the same image while pushing away the embeddings from different images. Non-contrastive methods also attract embeddings of views from the same image but remove the need for explicit negative pairs, either by architectural design (Grill et al., 2020;Chen & He, 2020) or by regularization of the covariance of the embeddings (Zbontar et al., 2021;Bardes et al., 2021;Li et al., 2022b).\nWhile contrastive and non-contrastive approaches seem very different and have been described as such (Zbontar et al., 2021;Bardes et al., 2021;Ermolov et al., 2021;Grill et al., 2020), we pro-Published as a conference paper at ICLR 2023 pose to take a closer look at the similarities between the two, both from a theoretical and empirical point of view and show that there exists a close relationship between them. We focus on covariance regularization-based non-contrastive methods (Zbontar et al., 2021;Ermolov et al., 2021;Bardes et al., 2021) and demonstrate that these methods can be seen as contrastive between the dimensions of the embeddings instead of contrastive between the samples. We, therefore, introduce the term dimension-contrastive methods which we believe is better suited for them, and refer to the original contrastive methods as sample-contrastive methods. To show the similarities between the two, we define contrastive and non-contrastive criteria based on the Frobenius norm of the Gram and covariance matrices of the embeddings, respectively, and show the equivalence between the two under assumptions on the normalization of the embeddings. We then relate popular methods to these criteria, highlighting the links between them and further motivating the use of the sample-contrastive and dimension-contrastive nomenclature. Finally, we introduce variations of an existing dimensioncontrastive method (VICReg), and a sample-contrastive one (SimCLR). This allows us to verify this equivalence empirically and improve both VICReg and SimCLR through this lens. Our contributions can be summarized as follows:\n\u2022 We make a significant effort to unify several SOTA sample-contrastive and dimensioncontrastive methods and show that empirical performance gaps can be closed completely. By pinpointing its source, we consolidate our understanding of SSL methods. \u2022 We introduce two criteria that serve as representatives for sample-and dimensioncontrastive methods. We show that they are equivalent for doubly normalized embeddings, and then relate popular methods to them, highlighting their theoretical similarities. \u2022 We introduce methods that interpolate between VICReg and SimCLR to study the practical impact of precise components of their loss functions. This allows us to validate empirically our theoretical result by isolating the sample-and dimension-contrastive nature of methods. \u2022 Motivated by the equivalence, we show that advantages attributed to one family can be transferred to the other. We improve SimCLR's performance to match VICReg's, and improve VICReg to make it as robust to embedding dimension as SimCLR.", "publication_ref": ["b6", "b9", "b15", "b27", "b4", "b42", "b1", "b36", "b10", "b28", "b44", "b2", "b15", "b8", "b42", "b1", "b30", "b42", "b1", "b12", "b15", "b42", "b12", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORK", "text": "Sample-contrastive methods. In self-supervised learning of image representations, contrastive methods pull together embeddings of distorted views of a single image while pushing away embeddings coming from different images. Many works in this direction have recently flourished (Chen et al., 2020a;Chen et al., 2020b;Yeh et al., 2021), most of them using the InfoNCE criterion (Oord et al., 2018), except HaoChen et al. (2021, that uses squared similarities between the samples. Clustering-based methods (Caron et al., 2018; can be seen as contrastive between prototypes, or clusters, instead of samples.\nNon-contrastive methods. Recently, methods that deviate from contrastive learning have emerged and eliminate the use of negative samples in different ways. Distillation-based methods such as BYOL (Grill et al., 2020), SimSiam (Chen & He, 2020) or DINO (Caron et al., 2021) use architectural tricks inspired by distillation to avoid the collapse problem. Information maximization methods (Bardes et al., 2021;Zbontar et al., 2021;Ermolov et al., 2021;Li et al., 2022b) maximize the informational content of the representations and have also had significant success. They rely on regularizing the empirical covariance matrix of the embeddings so that their informational content is maximized. Our study of dimension-contrastive learning focuses on these covariance-based methods.\nUnderstanding contrastive and non-contrastive learning. Recent works tackle the task of understanding and characterizing methods. The fact that a method like SimSiam does not collapse is studied in Tian et al. (2021). The loss landscape of SimSiam is also compared to SimCLR's in Pokle et al. (2022), which shows that it learns bad minima. In Wang & Isola (2020), the optimal solutions of the InfoNCE criterion are characterized, giving a better understanding of the embedding distributions. A spectral graph point of view is taken in ; Shen et al. (2022) to analyze self-supervised learning methods. Practical properties of contrastive methods have been studied in Chen et al. (2021a). In  Barlow twins criterion is shown to be related to an upper bound of a sample-contrastive criterion. We go further and exactly quantify the gap between the criterion, which allows us to use the link between methods in practical scenarios. Barlow\nTwins' criterion is also linked to HSIC in Tsai et al. (2021). The use of data augmentation in samplecontrastive learning has also been studied from a theoretical standpoint in ; Wen & Li (2021). In Balestriero & LeCun (2022), popular self-supervised methods are linked to spectral methods, providing a unifying framework that highlights their differences. The gradient of various methods is also studied in Tao et al. (2021), where they show links and differences between them.\nIn Lee et al. (2021a), a link is made between CCA and SCL by showing similar error bounds on linear classifiers.", "publication_ref": ["b6", "b9", "b40", "b31", "b3", "b15", "b8", "b1", "b42", "b12", "b30", "b35", "b32", "b38", "b33", "b7", "b37", "b39", "b0", "b34", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "EQUIVALENCE OF THE CONTRASTIVE AND NON-CONTRASTIVE CRITERION", "text": "While our results only depend on the embeddings and not the architecture used to obtain them, nor do they depend on the data modality, all the studied methods are placed in a joint embedding framework and applied on images. Given a dataset D with individual datum d i \u2208 R c\u00d7h\u00d7w , this datum is augmented to obtain two views x i and x \u2032 i . These two views are then each fed through a pair of neural networks f \u03b8 and f \u2032 \u03b8 \u2032 . We obtain the representations f \u03b8 (x i ) and f \u2032 \u03b8 \u2032 (x \u2032 i ), which are fed through a pair of projectors p \u03b8 and p \u2032 \u03b8 \u2032 such that embeddings are defined as p \u03b8 (f \u03b8 (x i )) and\np \u2032 \u03b8 \u2032 (f \u2032 \u03b8 \u2032 (x \u2032 i )).\nWe denote the matrices of embeddings K and K \u2032 such that K \u2022,i = p \u03b8 (f \u03b8 (x i )), and similarly for K \u2032 , we have K \u2208 R M \u00d7N , with M the embedding size and N the batch size, and similarly for K \u2032 . These embedding matrices are the primary object of our study. In practice, we use\nf \u03b8 = f \u2032 \u03b8 \u2032 and p \u03b8 = p \u2032 \u03b8 \u2032 . While most self-supervised learning approaches use positive pairs (x i , x \u2032 i ) and negative pairs {\u2200j, j \u0338 = i, (x i , x j )} {\u2200j, j \u0338 = i, (x i , x \u2032 j )\n} for a given view x i , we focus on the simpler scenario where negative samples are just {\u2200j, j \u0338 = i, (x i , x j )}. There is no fundamental difference when \u03b8 = \u03b8 \u2032 and when the same distribution of augmentations is used for both branches, and we therefore make these simplifications to make the analysis less convoluted. We start by defining precisely which contrastive and non-contrastive criteria we will be studying throughout this work. These criteria will be used to classify methods in two classes, samplecontrastive, which corresponds to what is traditionally thought of as contrastive, and dimensioncontrastive, which will encompass non-contrastive methods relying on regularizing the covariance matrix of embeddings. Definition 3.1. Given a matrix A \u2208 R n\u00d7n . We define its extracted diagonal diag (A) \u2208 R n\u00d7n as:\ndiag (A) i,j = A i,i , if i = j 0, otherwise. (1\n)\nDefinition 3.2. A method is said to be sample-contrastive if it minimizes the contrastive criterion\nL c = \u2225K T K\u2212diag(K T K)\u2225 2 F .\nSimilarly, a method is said to be dimension-contrastive if it minimizes the non-contrastive criterion L nc = \u2225KK T \u2212 diag(KK T )\u2225 2 F . The sample-contrastive criterion can be seen as penalizing the similarity between different pairs of images, whereas the dimension-contrastive criterion can be seen as penalizing the off-diagonal terms of the covariance matrix of the embeddings. These criteria respectively try to make pairs of samples or dimensions orthogonal.\nInvariance criterion. While L c and L nc focus on regularizing the embedding space, they are not optimized alone. They are usually combined with an invariance criterion that aims at producing the same embedding for two views of the same image. As such, a complete self-supervised loss would look like L SSL = L inv + L reg with L reg being either L c or L nc for our prototypical samplecontrastive and dimension-contrastive methods. This invariance criterion is generally a similarity measure, such as the cosine similarity or the mean squared error of the difference between a positive pair of samples. Both are equivalent from an optimization point of view if using normalized embeddings, hence we focus on the regularization part which is the source of differences between these methods. Proposition 3.1. Considering an infinite amount of available negative samples, SimCLR and DCL's criteria lead to embeddings where for negative pairs (x, x \u2212 ) \u2208 R M we have\nE x T x \u2212 = 0 and Var x T x \u2212 = 1 M . (2\n)\nSimCLR and DCL cannot be easily linked to L c since they rely on cosine similarities instead of their square or absolute value. Indeed, while L c aims at making pairs of embeddings or dimensions orthogonal, SimCLR and DCL's criteria go a step further and aim at making them opposite. Both cannot be satisfied perfectly in practice, as we would need as many dimensions as samples for L c to have all negative pairs be orthogonal, and more than two vectors cannot be pairwise opposite for SimCLR and DCL's criterion. Nonetheless, as shown by Proposition 3.1, SimCLR and DCL's criteria will lead to dot products of negative pairs with a null mean, which is exactly the aim of L c . This shows that while the original formulations of DCL and SimCLR do not fit perfectly into our theoretical framework, they will still lead to results similar to other methods that we study. In order to complement this result, we introduce SimCLR-sq and SimCLR-abs as variations of SimCLR, which respectively use square or absolute values of cosine similarities. We define DCL-sq and DCL-abs similarly. We provide a study of SimCLR-sq and SimCLR-abs in supplementary section G, where we compare them to SimCLR. The main conclusion is that the distribution of off-diagonal terms of the Gram matrix is similar between all studied methods, with a high concentration of values around zero, as predicted by Proposition 3.1. We also see that changing SimCLR into these variations does not impact performance. We even see a small increase in top-1 accuracy on ImageNet (Deng et al., 2009) with linear evaluation when using SimCLR-abs, where we reach 68.71% top-1 accuracy, compared to 68.61% with our improved reproduction of SimCLR. Both of these theoretical and practical arguments reinforce the proximity of SimCLR to our framework. Proposition 3.2. SimCLR-abs/sq, DCL-sq/abs, and Spectral Contrastive Loss (HaoChen et al., 2021) are sample-contrastive methods. Barlow Twins (Zbontar et al., 2021), VICReg (Bardes et al., 2021) and TCR (Li et al., 2022b) are dimension-contrastive methods.\nEven though they do not fit perfectly in our framework, we discuss how methods such as DINO, SimSiam, or MoCo can be linked to L c and L nc in supplementary section B. From proposition 3.2 we can see that sample-contrastive and dimension-contrastive methods can respectively be linked together by L c and L nc . This alone is not enough to show the link between those two families of methods and we will now discuss the link between L c and L nc to show how close those families are. Theorem 3.3. The sample-contrastive and dimension-contrastive criteria L c and L nc are equivalent up to row and column normalization of the embedding matrix K. Consider a batch size of N and an embedding dimension of M . We have:\nL nc + M j=1 \u2225K j,\u2022 \u2225 4 2 = L c + N i=1 \u2225K \u2022,i \u2225 4 2 . (3\n)\nTheorem 3.3 is similar to lemma 3.2 from Le et al. (2011), where we consider matrices that are not doubly stochastic. It is worth noting that our result does not rely on any assumption about the embeddings themselves. A similar result was also used recently in , where they relate the spectral contrastive loss to L nc . The proof of theorem 3.3 hinges on the fact that the squared Frobenius norm of the Gram and Covariance matrix of the embeddings are equal, i.e., \u2225K T K\u2225 2 F = \u2225KK T \u2225 2 F . This means that penalizing all the terms of the Gram matrix (i.e., pairwise similarities) is the same as penalizing all of the terms of the Covariance matrix. While this gives an intuition for the similarity between the contrastive and non-contrastive criteria, it is not as representative of the criteria used in practice as L c and L nc are. While theorem 3.3 shows that sample-contrastive and dimension-contrastive approaches minimize similar criteria, for none of these methods can we conclude that both criteria can be used interchangeably. However, if both rows and columns of K were L2 normalized, we would have\nL nc = L c + N \u2212 M .\nIn this case, both criteria would be equivalent from an optimization point of view, and we could conclude that sample-contrastive and dimension-contrastive methods are all minimizing the same criterion.\nInfluence of normalization. The difference between the two criteria then lies in the embedding matrix row and column norms, and most approaches do normalize it in one direction. Since SimCLR relies on the cosine distance as a similarity measure between embeddings, we can effectively say that it uses normalized embeddings. Similarly, Spectral Contrastive Loss projects the embeddings on a ball of radius \u221a \u00b5, with \u00b5 a tuned parameter, meaning that the embeddings are normalized before the computation of the loss function. Barlow Twins normalizes dimensions such that they have a null mean and unit variance, so all dimensions will have a norm of \u221a N . VICReg takes a similar approach where dimensions are centered, but their variance is regularized by the variance criterion. This is similar to what is done for Barlow Twins and thus leads to dimensions with constant norm. However, for TCR, the embeddings are normalized and not the dimensions, contrasting with other dimension-contrastive methods. One of the main differences between normalizing embeddings or dimensions is that in the former case, embeddings are projected on a M \u2212 1 dimensional hypersphere, and in the latter, they are not constrained on a particular manifold; instead, their spread in the ambient space is limited.\nNonetheless, a constraint on the norm of the embeddings also constrains the norm of the dimensions indirectly, and vice versa, as illustrated in lemma 3.4. Lemma 3.4. If embeddings are normalized such that \u2200i, \u2225K \u2022,i \u2225 2 = a we have\nN 2 M a 4 \u2264 M j=1 \u2225K j,\u2022 \u2225 4 2 \u2264 N 2 a 4 .(4)\nConversely, if dimensions are normalized such that \u2200j, \u2225K j,\u2022 \u2225 2 = a we have\nM 2 N a 4 \u2264 N i=1 \u2225K \u2022,i \u2225 4 2 \u2264 M 2 a 4 .(5)\nFollowing the proof of lemma 3.4, the lower bounds can be constructed with a constant embedding matrix and the upper bounds with an embedding matrix where either the rows or columns contain only one non-zero element. Both correspond to collapsed representations and will thus not be attained in practice. While it is impossible to characterize non-collapsed embedding matrices and, as such, derive better practical bounds, these bounds can still be useful to derive the following corollary. We study how close methods are to these bounds in practice in section H of the supplementary material. The main conclusion is that in all practical scenarios, the sum of norms will be very close to the lower bounds, deviating by a single-digit factor. Corollary 3.4.1. If embeddings are L2-normalized we have\nL nc \u2212 N + N 2 M \u2264 L c \u2264 L nc \u2212 N + N 2 .(6)\nSimilarly, if dimensions are L2-normalized we have\nL c \u2212 M + M 2 N \u2264 L nc \u2264 L c \u2212 M + M 2 . (7\n)\nLemma 3.4 applied to Theorem 3.3 directly gives us corollary 3.4.1, which means that in practical scenarios, even when we compare methods where the embeddings are not doubly normalized, the contrastive and non-contrastive criteria can't be arbitrarily far apart. We further show experimentally in section 5.1 that the normalization strategy does not matter from a performance point of view on SimCLR, reinforcing this argument. Considering the previous discussions, we thus argue that the main differences between sample-contrastive and dimension-contrastive methods come from the optimization process as well as the implementation details.\nDisguising VICReg as a contrastive method. To illustrate theorem 3.3 we can rewrite VICReg's criterion to make L c appear. We first recall the different components of VICReg's criterion. The variance criterion v is a hinge loss that aims at making the variance along every dimension greater than 1, and the covariance criterion c is exactly defined as L nc applied to centered embeddings. For more details, confer Bardes et al. (2021). To make L c appear, we will still apply the invariance and variance criterion on the embeddings, but the covariance criterion will be applied to the transposed embeddings, effectively making it contrastive since we have:\nc(K T ) = \u2225K T K T T \u2212 diag K T K T T \u2225 2 F = \u2225K T K \u2212 diag(K T K)\u2225 2 F = L c (K). (8)\nWe then just need to add a regularization term on the norms of embeddings and dimensions as follows:\nL reg (K) = N i=1 \u2225K \u2022,i \u2225 4 2 \u2212 M j=1 \u2225K j,\u2022 \u2225 4 2 ,(9)\nand VICReg's loss function can then be written as\nL V ICReg = \u03bb N i=1 \u2225K \u2022,i \u2212K \u2032 \u2022,i \u2225 2 2 +\u00b5 (v(K) + v(K \u2032 ))+\u03bd (L c (K) + L reg (K) + L c (K \u2032 ) + L reg (K \u2032 )) .(10)\nThis rewriting can be seen as a variation of SCL to which is added L reg and that uses the variance loss for normalization. Being able to make VICReg's criterion sample-contrastive highlights the close relationship between existing sample-contrastive and dimension-contrastive methods and further shows that the difference in the behavior of different methods is not mainly due to whether they are contrastive or not.", "publication_ref": ["b11", "b42", "b1", "b30", "b25", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "INTERPOLATING BETWEEN METHODS: IMPACT OF THE LOSS FUNCTION.", "text": "While we have discussed the link between the contrastive and non-contrastive criteria, we can wonder how the design differences in popular criteria manifest themselves in practice. To do so we start by introducing variations on VICReg that will allow us to interpolate between VICReg and SimCLR while isolating precise components of the loss function. While our focus will be on performance, we provide an analysis of the optimization quality in supplementary section J. The conclusion is that while some design choices negatively impact the optimization process on the embeddings, there are no easily visible differences in the representations which are used in practice.\nVICReg variations. We introduce two variants of VICReg, one that is non-contrastive but inspired by the InfoNCE criterion and one that is contrastive and also inspired by the InfoNCE criterion. The former is motivated by one of the main differences between methods, which is the use of the LogSumExp (LSE) for the repulsive force (e.g., SimCLR) or the use of the sum of squares (e.g., SCL, VICreg, BT). The latter is motivated by the wish to design contrastive methods, where implementation details such as the negative pair sampling are as close as possible to another method. This way, comparing VICReg to either of those methods will yield a comparison that truly isolates specific components of the loss function. These two methods can also be seen as a transformation from VICReg to SimCLR, which allows us to see when the behavior of VICReg becomes akin to SimCLR's, as illustrated in the following diagram:\nVICReg LogSumExp \u2212 \u2212\u2212\u2212\u2212\u2212\u2192 VICReg-exp Contrastive \u2212 \u2212\u2212\u2212\u2212\u2212 \u2192 VICReg-ctr Neg. pair sampling \u2212 \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 \u2192 SimCLR\nThe first variant that we will introduce is VICReg-exp, which uses a repulsive force inspired by the InfoNCE criterion. We first define the exponential covariance regularization as:\nc exp (K) = 1 d i log \uf8eb \uf8ed j\u0338 =i e C(K)i,j /\u03c4 \uf8f6 \uf8f8 ,(11)\nVICReg-exp is then VICReg where we replace the covariance criterion by this exponential covariance criterion, giving an overall criterion of\nL V ICReg\u2212exp = \u03bb N i=1 \u2225K \u2022,i \u2212 K \u2032 \u2022,i \u2225 2 2 + \u00b5 (v(K) + v(K \u2032 )) + \u03bd (c exp (K) + c exp (K \u2032 )) .(12)\nWe then define VICReg-ctr, which is VICReg-exp where we transpose the embedding matrix before applying the variance and covariance regularization. This means that the variance regularization will regularize the norm of the embeddings, and the covariance criterion now penalizes the Gram matrix, with the same repulsive force as in DCL. Transposing the embedding matrix for the variance criterion leads to more stable training and enables the use of mixed precision. We thus have the following criterion:\nL V ICReg\u2212ctr = \u03bb N i=1 \u2225K \u2022,i \u2212 K \u2032 \u2022,i \u2225 2 2 + \u00b5 v(K T ) + v(K \u2032T ) + \u03bd c exp (K T ) + c exp (K \u2032T ) . (13\n)\nThis way, VICReg-exp will allow us to study the influence of the use of the LogSumExp operator in the repulsive force, and VICReg-ctr to study the difference between sample-contrastive and dimension-contrastive methods when comparing it to VICReg-exp. We will now be able to study the optimization of the two criteria and see how different design choices affect it.  Figure 1: VICReg, VICReg-exp, and VICReg-ctr perform similarly in 100 epochs training, validating empirically our theoretical result. While the original implementation of SimCLR performs significantly worse -which is unexpected per our theory -we are able to improve its performance to VICReg's level. This further validates our findings. While different projector architectures impact performance, behaviors are similar across methods. Confer supplementary section K for numerical values and hyperparameters.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "PRACTICAL DIFFERENCES BETWEEN SAMPLE-CONTRASTIVE AND DIMENSION-CONTRASTIVE METHODS", "text": "While we have discussed how close sample and dimension contrastive methods are in theory, one of the primary considerations when choosing or designing a method is the performance on downstream tasks. Linear classification on ImageNet has been the main focus in most SSL methods, so we will focus on this task. We will consider the two following aspects, which are responsible for most of the discrepancies between methods.\nLoss implementation. Thanks to VICReg-exp, we are able to study the difference between penalizing the Frobenius norm directly and using a LogSumExp to penalize it. Similarly, for VICReg-ctr we are able to study the practical differences between the contrastive and non-contrastive criteria. Finally, with SimCLR we will be able to see how the last details between VICReg-ctr and it can impact performance.\nProjector architecture. One of the main differences in methods is how the projector is designed. To describe projector architectures we use the following notation: X \u2212 Y \u2212 Z means that we use linear layers of dimensions X, then Y and Z. Each layer is followed by a ReLU activation and a batch normalization layer. The last layer has no activation, batch normalization, or bias. In order to study the impact that this has on performance with respect to embedding size, we study three scenarios. First, d \u2212 d \u2212 d, which is the scenario used for VICReg and BT, then 2048 \u2212 d which was originally used for SimCLR, and finally 8192 \u2212 8192 \u2212 d which was optimal for large embeddings with VICReg.\nDue to the extensive nature of the following experiments, we use a proxy of the classical linear evaluation on ImageNet, where the classifier is trained alongside the backbone and projector. Representations are fed to a linear classifier while keeping the gradient of this classifier's criterion from flowing back through the backbone. The addition of this linear classifier is extremely cheap and avoids a costly linear evaluation after training. The performance of this online classifier correlates almost perfectly with its offline counterpart, so we can rely on it to discuss the general behaviors of various methods. This evaluation was briefly mentioned in Chen et al. (2020a) but without experimental support. We discuss the correlation between the two further in supplementary section E. Empirical validation. The first takeaway from figure 1 is that the transition VICReg \u2192 VICReg-exp via the addition of the LogSumExp did not alter overall performance or behavior. While small performance differences are visible between the two when using light projectors, especially at low embedding dimension, as soon as we use a larger projector these differences disappear with them achieving 68.13% and 68.00% respectively. Focusing on the transition VICReg-exp \u2192 VICReg-ctr, we can see that there is no noticeable gap in performance in a setting where we were able to isolate the sample-contrastive and dimension-contrastive nature of the methods. This validates empirically our theoretical findings on the equivalence of sample-contrastive and dimension-contrastive methods. When comparing VICReg-ctr to our reproduction of SimCLR, \n\u221a N \u221a N N/M\nusing the original hyperparameters, we can see that VICReg-ctr performs significantly better than SimCLR, achieving 67.92% top-1 accuracy compared to 66.33%. This is surprising since the main difference between the two is that VICReg-ctr uses fewer negative pairs, which should not improve performance. As such we will focus on showing that the previously known performance of SimCLR is suboptimal and then fix it. In supplementary section F we further validate our results with k-nn classification accuracy and also show that features correlate extremely well between methods.\nImproving SimCLR's performance. To the best of our knowledge, the highest top-1 accuracies reported on ImageNet with SimCLR in 100 epochs are around 66.8% (Chen et al., 2021a). While much higher than the 64.7% originally reported, this is still significantly lower than VICReg. Motivated by the performance of VICReg-ctr, we used the same projector as VICReg and heavily tuned hyperparameters, allowing us to find that a temperature of 0.15 and base learning rate of 0.5 can lead to a top-1 accuracy of 68.6%, matching VICReg's performance in Bardes et al. (2021). This reinforces our theoretical insights and highlights the contribution of precise engineering 1 in recent self-supervised advances. As it stands, SimCLR can still serve as a strong baseline.\nA larger projector increases performance. From figure 1 we can see that for every studied method, going from a projector with architecture 2048 \u2212 d to 8192 \u2212 8192 \u2212 d yielded a significant boost in performance, especially for VICReg and VICReg-ctr, both gaining 3.5 \u2212 4 points. The projector d \u2212 d \u2212 d is in between the two depending on the embedding dimension but also shows a similar trend, the performance increases with the number of parameters for every method. While out of the scope of this work, the study of the importance of the projector's capacity is an exciting line of work that should help gain a deeper understanding of its role in self-supervised learning. We provide a preliminary discussion in the supplementary section I.\nClearing up misconceptions. While contrastive methods are often thought of as sample inefficient, thus requiring large batch sizes, and non-contrastive methods as dimension inefficient, thus requiring projectors with large output dimensions, we argue that both of these assumptions are misleading and that all of these apparent issues can be alleviated with some care. Most notably, the need for large batch sizes of contrastive methods has been studied in Yeh et al. (2021) and Zhang et al. (2022) where the main conclusions are that with more tuning of the InfoNCE parameters, the robustness of SimCLR and MoCo to small batches can be improved. Regarding the robustness of non-contrastive methods to embedding dimension, our experiments show that with a more adequate projector architecture and with careful hyperparameter tuning, the drop in performance at low embedding dimension is not as present as initially reported (Zbontar et al., 2021;Bardes et al., 2021).\nWith 256-dimensional embeddings, we were able to achieve 61.36% top-1 accuracy by tuning VI-CReg's hyperparameters, compared to the 55.9% that were initially reported in Bardes et al. (2021). This can be further improved to 65.01% with a bigger projector. While a drop is still present, we are able to reach peak performance at 1024 dimensions, which is lower than the representation's dimension of 2048 and shows that a large embedding dimension is not a deciding factor in downstream performance.", "publication_ref": ["b6", "b7", "b1", "b40", "b43", "b42", "b1", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "INFLUENCE OF THE NORMALIZATION STRATEGY", "text": "While we have shown that the performance gap between sample-contrastive and dimensioncontrastive methods can be closed with careful hyperparameter tuning, in the studied settings not  all details are equal. This is especially true regarding the normalization strategies that are used, and we illustrate the different ones in table 1. In order to show that these differences do not impact performance, we will introduce two variations of SimCLR. First, we will look at SimCLR with the centering of the dimensions, and then at SimCLR with the centering of the dimensions as well as a normalization along the dimensions instead of the embeddings. This last strategy is in essence a standardization of the dimensions and is the same scheme used by VICReg. More precisely the dimension standardization can be written as :\n\u2200i \u2208 [1, . . . , M ] K \u2022,i =K \u2022,i \u2225K \u2022,i \u2225 2 \u00d7 N M withK \u2022,i = K \u2022,i \u2212 1 N N j=1 K j,i .(14)\nThese variations will allow us to compare VICReg and SimCLR when both adopt the same normalization strategy, resulting in a comparison that will more closely fit our theoretical framework.\nAs we can see in figure 2, the centering and dimension standardization do not impact performance at all and we are able to achieve the same peak performance as before. The performance is slightly lower with a shallow projector 2048 \u2212 d, but in all the other scenarios we retrieve the same performance as the original SimCLR. This performance is on par with VICReg and its variations which reinforces our theoretical result in practice. This was further confirmed in a 1000 epoch run, where SimCLR with dimension standardization was able to reach 72.6% top-1 accuracy, compared to 73.3% for VICReg. While a small difference persists, hyperparameter tuning is very expensive in this setting and is most likely the cause of this gap.\nFrom these results, we can conclude that while the normalization strategy can be theoretically motivated or can ease the optimization process, it is not a deciding factor in the performance of selfsupervised methods and that the normalization strategy that should be used is the one that is the easiest to work with for a given method.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "CONCLUSION", "text": "Through an analysis of their criteria, we were able to show that sample-contrastive and dimensioncontrastive methods have learning objectives that are closely related, as they are effectively minimizing criteria that are equivalent up to row and column normalization of the embedding matrix. This suggests a certain duality in the behavior of such methods, which we studied empirically. Through the lens of variations of VICReg, we were able to study popular design choices in self-supervised loss functions and show their lack of impact on performance, significantly improving the robustness to embedding dimension of VICReg along the way. Motivated by our theoretical findings, we performed ample hyperparameter tuning on SimCLR and were able to close its performance gap with VICReg. We also showed that the normalization strategy does not play an important role in performance. This further reinforces the similarities between methods as predicted by our theoretical results. We expect that our results will help extend theoretical works in self-supervised learning to a wider family of methods, as well as help analyses by deriving criteria that are easier to work with. We also expect that our findings will help alleviate preconceived ideas on contrastive and non-contrastive learning. If one thing must be remembered from this work, it is that dimensioncontrastive and sample-contrastive methods are two sides of the same coin. Finally, perhaps the most important message of this work is to show that different SOTA SSL methods can be unified.\nPinpointing the source of the advancements is an important direction to consolidate our understanding.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors wish to thank Randall Balestrierio, Li Jing, Gr\u00e9goire Mialon, Nicolas Ballas, Surya Ganguli, and Pascal Vincent, in no particular order, for insightful discussions. We also thank Florian Bordes for the efficient implementations that were used for our experiments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "REPRODUCIBILITY STATEMENT", "text": "While our pretrainings are very costly, each taking around a day with 8 V100 GPUs, we provide complete hyperparameter values in table S6. They are compatible with official implementations of the losses, and for VICReg-ctr and VICReg-exp we also provide PyTorch pseudocode in supplementary section L. In order to reproduce our main figure, we also give the numerical performance in table S5. All of this should make our results reproducible, and, more importantly, should make it so that practitioners can benefit from the improved performance that we introduce.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "A BACKGROUND", "text": "In this section, we will recall the loss functions of all the methods we are considering throughout our theoretical analysis.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "DCL:", "text": "We first take a look at DCL's criterion. We consider that K is l2 normalized column-wise, i.e. embeddings are normalized. We have\nL DCL = N i=1 \u2212 log e K T \u2022,i K \u2032 \u2022,i /\u03c4 j\u0338 =i e K T \u2022,i K\u2022,j /\u03c4 = N i=1 \u2212 K T \u2022,i K \u2032 \u2022,i \u03c4 + log \uf8eb \uf8ed j\u0338 =i e K T \u2022,i K\u2022,j /\u03c4 \uf8f6 \uf8f8 .(15)\nSimCLR: We now take a look at SimCLR's criterion. We consider that K is l2 normalized columnwise, i.e. embeddings are normalized. We have\nL SimCLR = N i=1 \u2212 log e K T \u2022,i K \u2032 \u2022,i /\u03c4 e K T \u2022,i K \u2032 \u2022,i /\u03c4 + j\u0338 =i e K T \u2022,i K\u2022,j /\u03c4 (16) = N i=1 \u2212 K T \u2022,i K \u2032 \u2022,i \u03c4 + log \uf8eb \uf8ed e K T \u2022,i K \u2032 \u2022,i /\u03c4 + j\u0338 =i e K T \u2022,i K\u2022,j /\u03c4 \uf8f6 \uf8f8 .(17)\nSpectral Constrastive Loss: Spectral Contastive Loss is defined as\nL SCL = \u22122 N i=1 K T \u2022,i K \u2032 \u2022,i + j\u0338 =i K T \u2022,i K \u2022,j 2 = \u22122 N i=1 K T \u2022,i K \u2032 \u2022,i + \u2225K T K \u2212 diag(K T K)\u2225 2 F . (18)\nThe normalization that is employed is to project all embeddings on a ball of radius \u00b5. This means that if their norm is lower than \u00b5, nothing will happen to them.\nBarlow Twins: We consider that K is l2 normalized row-wise, i.e. dimensions are normalized. This gives us:\nL BT = M j=1 1 \u2212 (KK \u2032T ) j,j 2 +\u03bb M i,j,i\u0338 =j (KK \u2032T ) 2 j,i = M j=1 1 \u2212 (KK \u2032T ) j,j 2 +\u03bb\u2225KK \u2032T \u2212diag(KK \u2032T )\u2225 2 F .(19)\nVICReg: VICReg's criterion is defined as\nL V ICReg = \u03bb N i=1 \u2225K \u2022,i \u2212 K \u2032 \u2022,i \u2225 2 2 + \u00b5 (v(K) + v(K \u2032 )) + \u03bd (c(K) + c(K \u2032 )) .(20)\nWith c a criterion that penalizes the off-diagonal terms of the covariance matrix as\nc(K) = i\u0338 =j Cov(K) 2 i,j = \u2225KK T \u2212 diag(KK T )\u2225 2 F = L nc ,(21)\nand v a criterion that aims at normalizing dimensions, i.e. rows of K.\nTCR: TCR's cost function is defined as\nL T CR = \u2212 1 2 log det (I + \u03b1Cov(K)) = \u2212 1 2 log det I + \u03b1KK T = \u2212 1 2 i log 1 + \u03b1\u03c3 2 i ,(22)\nwhere \u03c3 i is the i-th singular value of K.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B LINKS BETWEEN METHODS AND OUR CRITERIA", "text": "While we focus on methods for which the regularization is obtained through the criterion, several other methods can be linked informally to our results. The difficulty in linking them to L c or L nc can also come from choices that are motivated by practical limitations, such as the use of a memory bank, and which do not change methods fundamentally.\nOne of the most surprising lines of works, BYOL (Grill et al., 2020) and SimSiam (Chen & He, 2020), showed that using stop-gradient on one side of the encoder and using a predictor network to create asymmetry was enough to avoid collapse and learn good representations. Even though they do not avoid collapse explicitly through their criteria, recent works such as Halvagal et al. (2022) or Theorem 3 from Tian et al. (2021) have shown links between the training dynamics of SimSiam and variance and covariance regularization, akin to what L nc would lead to. While these analyses require assumptions such as the linearity of the encoder, they still help shine a light on SimSiam and BYOL's behavior and enable us to see how they can be related to our results.\nDue to the popularity of sample-contrastive methods, several variants have emerged to improve their sample efficiency or their performance in general. One such modification is illustrated in MoCo Chen et al., 2020b; where a memory bank of sample is combined with an exponential moving average (EMA) of the encoder to provide better negative pairs and thus improve training. While this makes it hard to relate MoCo to our framework, it still relies on an InfoNCE criterion like SimCLR and thus leads to similar representations. SimCLR and MoCo become especially close near convergence since the online network and the EMA one will be very similar and thus the two methods also become more alike.\nClustering methods such as DeepCluster (Caron et al., 2018), SwAV (Caron et al., 2020) or DINO Caron et al. (2021) can also be related informally to sample-contrastive approaches. Similarly to MoCo, the main difference lies in the construction of the negative pairs, which are constructed using cluster centers here. The embeddings are then contrasted with these clustering prototypes using losses akin to InfoNCE. In DINO, the clustering aspect is more subtle as it is done online, thanks to the last linear layer of the projector which can be thought of as the bank of cluster prototypes, and the embeddings are then the outputs of the penultimate layer. Its projector can thus be decomposed into two parts, the first being the classical projector which is followed by L2 normalization, and the last layer which acts as a clustering layer thanks to the softmax that follows it. As such, while clustering methods cannot be clearly linked to our framework, a link to sample-contrastive methods is still present, even if only informally.\nOverall, while not all methods can fit clearly into our results, we are still able to relate most of them to sample-contrastive or dimension-contrastive methods, even if it is with less rigor. This further reinforces the similarity between methods.", "publication_ref": ["b15", "b8", "b17", "b35", "b9", "b3", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "C PROOFS", "text": "Lemma C.1. Let X, Y \u223c \u03c3 D\u22121 two i.i.d\n. random variables corresponding to vectors uniformly distributed on S D\u22121 . Their dot product follows the following distribution\nX T Y + 1 2 \u223c Beta D \u2212 1 2 , D \u2212 1 2 .\nProof. A similar result was proved in Fernandez et al. (2022), though we go one step further and derive the distribution of\nX T Y +1 2 .\nWe follow a more geometrical argument and invite the reader to confer Fernandez et al. (2022) for an alternative approach.\nBy the symmetry of the hypersphere, the distribution of X T Y is the same as the one of X T (1, 0 . . . , 0), which corresponds to rotating the reference frame. The cumulative distribution function then corresponds to the surface of the hyperspherical cap of angle cos \u22121 (X 1 ). Using the formulas for the area of a spherical cap on S D derived in Li (2011), as well as the fact than sin 2 (cos \u22121 (x)) = 1 \u2212 x 2 we directly obtain that for X T Y > 0 (i.e. cos \u22121 (X 1 ) \u2264 \u03c0\n2 ), we have\n1 \u2212 (X T Y ) 2 \u223c Beta D\u22121 2 , 1 2 .\nSince the density of the Beta distribution has reflectional symmetry, we see that (X\nT Y ) 2 \u223c Beta 1 2 , D\u22121 2 . By substituting in u = X T Y +1 2 if follows directly that u \u223c Beta D \u2212 1 2 , D \u2212 1 2 ,(23)\nconcluding the proof.\nProposition C.2. Considering an infinite amount of available negative samples, SimCLR and DCL's criteria lead to embeddings where for negative pairs (x, x \u2212 ) \u2208 R M we have\nE x T x \u2212 = 0 and Var x T x \u2212 = 1 M . (24\n)\nProof. The proof hinges on Theorem 1 from Wang & Isola (2020), which states that as the number of negative samples goes to infinity, optimizing the repulsive force of the InfoNCE criterion leads to uniformly distributed embeddings on the M -hypersphere.\nThis uniform distribution allows us to leverage Lemma C.1 in saying that as the number of negative samples goes to infinity, for any pair of random embeddings X, Y , we have\nX T Y +1 2 \u223c Beta M \u22121 2 , M \u22121 2 .", "publication_ref": ["b13", "b13", "b29", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "We can directly obtain the two following properties", "text": "E X T Y + 1 2 = M \u22121 2 M \u22121 2 + M \u22121 2 = 1 2 \u21d2 E X T Y = 0,(25)\nVar\nX T Y + 1 2 = M \u22121 2 \u00d7 M \u22121 2 M \u22121 2 + M \u22121 2 2 M \u22121 2 + M \u22121 2 + 1 = 1 4M \u21d2 Var X T Y = 1 M ,(26)\nconcluding the proof.\nProposition C.3. SimCLR-abs/sq, DCL-sq/abs, as well as Spectral Contrastive Loss are samplecontrastive methods. Barlow Twins, VICReg, and TCR are dimension-contrastive methods.\nProof. DCL-sq/abs: We first take a look at DCL-sq/abs's criteria. We consider that K is l2 normalized column-wise, i.e. embeddings are normalized. Let f : R \u2192 R + be either defined as f (x) = x 2 for DCL-sq or as f (x) = |x| for DCL-abs. We have\nL DCL = N i=1 \u2212 log \uf8eb \uf8ed e f (K T \u2022,i K \u2032 \u2022,i )/\u03c4 j\u0338 =i e f (K T \u2022,i K\u2022,j )/\u03c4 \uf8f6 \uf8f8 = N i=1 \u2212 f K T \u2022,i K \u2032 \u2022,i \u03c4 + log \uf8eb \uf8ed j\u0338 =i e f (K T \u2022,i K\u2022,j )/\u03c4 \uf8f6 \uf8f8 .\n(27) The first part of this criterion is the invariance criterion and the second part is the LogSumExp(LSE) of embeddings' similarity. We know that this is a smooth approximation of the max operator with the following bounds:\nmax {\u2200j \u0338 = i, f K T \u2022,i K \u2022,j } \u2264 \u03c4 log \uf8eb \uf8ed j\u0338 =i e f (K T \u2022,i K\u2022,j )/\u03c4 \uf8f6 \uf8f8 \u2264 max {\u2200j \u0338 = i, f K T \u2022,i K \u2022,j } +\u03c4 log(N \u22121).\n(28) We can thus say that using either\nN i=1 log \uf8eb \uf8ed j\u0338 =i e f (K T \u2022,i K\u2022,j )/\u03c4 \uf8f6 \uf8f8 or N i=1 max j\u0338 =i f K T \u2022,i K \u2022,j ,(29)\nas repulsive force will lead to the same result, a diagonal Gram matrix. Since this is the same goal as for our sample-contrastive criterion, DCL-sq and DCL-abs are sample-contrastive methods.\nThe link to L c is more visible with the right term, which corresponds to only penalizing one value per row/column of the Gram matrix. While this is less effective than penalizing all of them at once, given sufficient training iterations it will converge to the same solution.\nSimCLR-sq/abs: We now take a look at SimCLR-abs/sq's criteria. We consider that K is l2 normalized column-wise, i.e. embeddings are normalized. Let f : R \u2192 R + be either defined as f (x) = x 2 for SimCLR-sq or as f (x) = |x| for SimCLR-abs. We have\nL SimCLR = N i=1 \u2212 log \uf8eb \uf8ed e f (K T \u2022,i K \u2032 \u2022,i )/\u03c4 e f (K T \u2022,i K \u2032 \u2022,i )/\u03c4 + j\u0338 =i e f (K T \u2022,i K\u2022,j )/\u03c4 \uf8f6 \uf8f8 (30) = N i=1 \u2212 f K T \u2022,i K \u2032 \u2022,i \u03c4 + log \uf8eb \uf8ed e f (K T \u2022,i K \u2032 \u2022,i )/\u03c4 + j\u0338 =i e f (K T \u2022,i K\u2022,j )/\u03c4 \uf8f6 \uf8f8 . (31\n)\nDue to the presence of the positive pair in the repulsive force (right term), we cannot use the same reasoning with the max operator as for DCL-sq/abs which gave a clear intuition.\nNonetheless one can clearly see that to minimize this criterion, all the similarities between the negative pairs, i.e. \u2200i, \u2200j \u0338 = i, f K T \u2022,i K \u2022,j , need to be minimized. As this will result in a diagonal Gram matrix, we can say that minimizing this criterion will also minimize our sample-contrastive one. We can thus conclude that SimCLR-sq and SimCLR-abs are sample-contrastive methods.\nSpectral Constrastive Loss: We will now consider Spectral Constrastive Learning's criterion. We have\nL SCL = \u22122 N i=1 K T \u2022,i K \u2032 \u2022,i + j\u0338 =i K T \u2022,i K \u2022,j 2 = \u22122 N i=1 K T \u2022,i K \u2032 \u2022,i + \u2225K T K \u2212 diag(K T K)\u2225 2 F . (32\n)\nThis means that Spectral Contrastive Loss also falls in the sample-contrastive category.\nBarlow Twins: Looking at Barlow Twin's criterion we have\nL BT = M j=1 1 \u2212 (KK \u2032T ) j,j 2 +\u03bb M i,j,i\u0338 =j (KK \u2032T ) 2 j,i = M j=1 1 \u2212 (KK \u2032T ) j,j 2 +\u03bb\u2225KK \u2032T \u2212diag(KK \u2032T )\u2225 2 F .(33)\nSince the distribution of augmentations is the same for both views of the images, and the backbone is shared, taking a negative pair from K or K \u2032 is the same. Barlow Twins' criterion can then be rewritten as\nL BT = M j=1 1 \u2212 (KK \u2032T ) j,j 2 + \u03bb\u2225KK T \u2212 diag(KK T )\u2225 2 F . (34\n)\nAs such the right part of Barlow Twins' criterion is indeed the dimension-contrastive criterion, making Barlow Twins a dimension-contrastive method.\nVICReg: VICReg's criterion is defined as\nL V ICReg = \u03bb N i=1 \u2225K \u2022,i \u2212 K \u2032 \u2022,i \u2225 2 2 + \u00b5 (v(K) + v(K \u2032 )) + \u03bd (c(K) + c(K \u2032 )) . (35\n)\nRecall that c is a criterion that penalizes the off-diagonal terms of the covariance matrix as follows:\nc(K) = i\u0338 =j Cov(K) 2 i,j = \u2225KK T \u2212 diag(KK T )\u2225 2 F = L nc .(36)\nThis means that VICReg is a dimension-contrastive method.\nTCR: TCR's cost function is defined as\nL T CR = \u2212 1 2 log det (I + \u03b1Cov(K)) = \u2212 1 2 log det I + \u03b1KK T = \u2212 1 2 i log 1 + \u03b1\u03c3 2 i ,(37)\nwhere \u03c3 i is the i-th singular value of K. As discussed in Li et al. (2022b), this criterion leads to a diagonal covariance matrix, similarly to the non-contrastive criterion. We can thus say using either\n\u2212 1 2 i log 1 + \u03b1\u03c3 2 i or \u2225KK T \u2212 diag(KK T )\u2225 2 F (38\n)\nwill lead to diagonal covariance matrices, or similarly, null off-diagonal terms in the Covariance matrix. This means that TCR also falls in the category of dimension-contrastive methods.\nTheorem C.4. The sample-contrastive and dimension-contrastive criteria L c and L nc are equivalent up to row and column normalization of the embedding matrix K. Consider a batch size of N and an embedding dimension of M . We have:\nL nc + M j=1 \u2225K j,\u2022 \u2225 4 2 = L c + N i=1 \u2225K \u2022,i \u2225 4 2 .(39)\nProof. This proof is heavily inspired by the proof of Lemma 3.2 from Le et al. (2011) which provides a similar result for doubly stochastic matrices. We have\nL nc = \u2225KK T \u2212 diag(KK T )\u2225 2 F (40) = tr (KK T \u2212 diag(KK T )) T (KK T \u2212 diag(KK T ))(41)\n= tr(KK T KK T ) \u2212 2tr(KK T diag(KK T )) + tr(diag(KK T ) diag(KK T ))(42)\n= tr(KK T KK T ) \u2212 tr(KK T diag(KK T ))\n= tr(K T KK T K) \u2212 tr(KK T diag(KK T )).\nSimilarly for L c , we obtain\nL c = \u2225K T K \u2212 diag(K T K)\u2225 2 F (45) = tr(K T KK T K) \u2212 tr(K T K diag(K T K)).(46)\nSince\nK T K i,i = \u2225K \u2022,i \u2225 2 2 we deduce that tr(K T K diag(K T K)) = N i=1 \u2225K \u2022,i \u2225 4 2 . Similarly, we obtain that tr(KK T diag(KK T )) = M j=1 \u2225K j,\u2022 \u2225 4\n2 . Plugging this back in, we finally deduce that\nL nc = L c + N i=1 \u2225K \u2022,i \u2225 4 2 \u2212 M j=1 \u2225K j,\u2022 \u2225 4 2 ,(47)\nconcluding the proof.\nLemma C.5. If embeddings are normalized such that \u2200i, \u2225K \u2022,i \u2225 2 = a we have\nN 2 M a 4 \u2264 M j=1 \u2225K j,\u2022 \u2225 4 2 \u2264 N 2 a 4 .(48)\nConversely, if dimensions are normalized such that \u2200j, \u2225K j,\u2022 \u2225 2 = a we have\nM 2 N a 4 \u2264 N i=1 \u2225K \u2022,i \u2225 4 2 \u2264 M 2 a 4 .(49)\n. We also use a momentum of 0.9 and weight decay of 10 \u22126 . The learning rate follows a cosine annealing schedule after a 10-epoch linear warmup. We train for 100 epochs in all of our experiments. For data augmentation, we follow the protocol of BYOL (Grill et al., 2020) which is as follows Table S1: Image augmentation parameters, taken from (Grill et al., 2020). Each experiment was run on 8 Nvidia V100 GPUs, with 32GB of memory each, and took around 24 hours to complete.\nWhile this was our base experimental protocol, it was adapted for each method, mostly by changing method-specific hyperparameters as well as the learning rate, confer supplementary section K for the exact hyperparameters used for each experiment. The Pytorch pseudocode for VICReg-exp and VICReg-ctr is also available in supplementary section L.", "publication_ref": ["b30", "b25", "b15", "b15"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "E ONLINE LINEAR PROBE", "text": "As previously discussed, to evaluate our experiments, we relied on the use of a linear classifier that is trained jointly with our main network. This means that it is trained on suboptimal representations and stronger augmentations compared to what is typically done for linear evaluation. Even though these two approaches seem closely related, we are interested in finding how well they are correlated.\nTo do so, we trained a linear evaluation on VICReg and VICreg-exp with a projector architecture of 8192 \u2212 8192 \u2212 d, d \u2208 [256, 512, 1024, 2048, 8192] using the following protocol. We train the linear classifier on frozen representations for 100 epochs with a batch size of 1024 using the SGD optimizer with a base learning rate 0.25 (for VICReg) or 1.4 (for VICReg-exp), momentum 0.9, weight decay 10 \u22126 and using a cosine annealing learning rate scheduler. We compute the learning rate as lr = base lr \u00d7 batch size", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "256", "text": ". For augmentations, we follow standard procedure and use random cropping with a scale between 0.08 and 1 with an image size of 224 \u00d7 224 and horizontal flip with a probability 0.5 during training. For evaluation, we do a center crop. As we can see in table S2 and S3, the performance achieved by the offline classifier is extremely close to the performance of the online classifier. While the online classifier cost in compute is negligible, the linear evaluation is almost as long as the pretraining due to data loading bottlenecks and it requires a significant amount of learning rate tuning. This makes this online classifier a very appealing alternative since it demonstrates very correlated performances for a fraction of the computing cost.\nTraining a linear regression on those two sets of evaluations gives a model with a slope of 0.97, an intercept of 2.1, and an R 2 of 1.0. It is worth noting that since most values are close to 68, the fitting of linear regression on this data is sensitive to noise. Nonetheless, the low intercept, as well as the closeness of the slope to 1, confirm the negligible gap between the two evaluation methods that we previously intuited.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F ADDITIONAL EVIDENCE OF THE SIMILARITY OF LEARNED", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "REPRESENTATIONS", "text": "The goal of this section is to provide additional empirical evidence of the similar properties of representations learned by sample-contrastive and dimension-contrastive methods. To this effect, we will evaluate representations with a k-nn classifier and compare their similarities with CKA (Kornblith et al., 2019).\nk-nn evaluation. In order to see if our previous results only validated similar performance in a linear classification setting, we will look at performance with k-nn classifiers which evaluate how well a metric is preserved instead of linear separability. We rely on the protocol of Bardes et al. (2021), and use values of k in [1, 5, 10, 20, 50, 200], with temperatures in [0.05, 0.07, 0.1, 0.2, 0.5, 1] for the weighting of the classifiers. We then look at the best performance achieved by all methods to give a comparison that is as fair as possible.\nAs we can see in figure S1, we are able to retrieve behaviors similar to figure 1, although results appear less stable for VICReg-exp and VICReg-ctr. Nonetheless, looking at the transition VICReg-exp \u2192 VICReg-ctr we can see that the peak performance is still preserved, further validation our results for these methods were the dimension-contrastive and sample-contrastive natures are isolated. Similarly as for linear evaluation, the original implementation of SimCLR performs significantly worse than other methods, but our tuned SimCLR can recover the performance of VICReg with a 5.5 point  1 with a k-nn classifier. We notice the same pattern as previously, where going from dimension-contrastive to sample-contrastive does not lead to a significant drop in performance.\nincrease in performance. This highlights how the practical implications of our results extend beyond linear classification, while further validating our theory.\nCKA. CKA (Centered Kernel Alignment) (Kornblith et al., 2019) is a powerful tool to study the similarities between representations, which relies on HSIC (Hilbert-Schmidt Independance Criterion) (Gretton et al., 2005) with a given kernel. We will use a linear kernel for simplicity. For each method, we will study three different experiments that reached the same level of performance to measure both intra-and inter-method correlation between representtions. We also consider a random network to give a lower bound of what we can expect. SimCLR-8192 Random 1 0.93 0.93 0.9 0.91 0.91 0.9 0.89 0.9 0.89 0.89 0.89 0.14 0.93 1 0.93 0.9 0.91 0.91 0.9 0.9 0.9 0.89 0.89 0.89 0.15 0.93 0.93 1 0.9 0.91 0.91 0.9 0.9 0.9 0.89 0.89 0.89 0.15 0.9 0.9 0.9 1 0.92 0.92 0.9 0.9 0.9 0.87 0.87 0.87 0.13 0.91 0.91 0.91 0.92 1 0.92 0.9 0.9 0.9 0.88 0.88 0.88 0.13 0.91 0.91 0.91 0.92 0.92 1 0.9 0.89 0.9 0.88 0.88 0.88 0.13 0.9 0.9 0.9 0.9 0.9 0.9 1 0.94 0.94 0.89 0.89 0.89 0.14 0.89 0.9 0.9 0.9 0.9 0.89 0.94 1 0.94 0.89 0.89 0.89 0.14 0.9 0.9 0.9 0.9 0.9 0.9 0.94 0. As we can see in figure S2, all of the learned representations are highly correlated, where intraand inter-methods CKA are very similar. This both shows that different self-supervised methods, whether dimension-contrastive or sample-contrastive, provide consistent representations over different runs and also all learn similar representations. These results contrast with the findings in Figure 2 from Gwilliam & Shrivastava (2022) where they found that different methods lead to representations with low CKA. We believe that their findings can be explained by different training setups between methods since the models used were trained with different projectors and data augmentation.\nBoth of these analyses help cement our results, where we can now say that through the lens of linear classification, k-nn classification, and CKA, all studied self-supervised methods produce extremely similar representations.", "publication_ref": ["b24", "b24", "b14", "b16"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "G IMPACT OF THE SIMILARITY MEASURE ON SIMCLR", "text": "While SimCLR uses cosine similarity to push away negative pairs, we will look at what happens when we use the square or absolute value of cosine similarities, as in SimCLR-sq or SimCLR-abs.  As we can see in figure S3, the use of the squared or absolute values of the similarities did not impact the performance on image classification, it even improved slightly with a large projector when using the absolute values, achieving 68.7% top-1 accuracy. As we can see in figure S4, for all three methods we obtain a distribution of cosine similarities that is centered at 0, but they all have very different standard deviations. The main culprit of this difference is dimensional collapse, as studied extensively in Jing et al. (2022). We study this behavior in figure S5, where we see that the three methods show different levels of collapse. While SimCLRabs appears to have an almost full rank embedding matrix, we can see some collapse at around 256 dimensions for SimCLR, and 64 for SimCLR-sq. Per proposition 3.1, we know that with a perfect optimization of SimCLR's criterion, we should observe a variance of 1/D for the cosine similarities, if we have D-dimensional embeddings. However this is not the ambient dimension but the embeddings' dimension, and so when combining this result with the dimensional collapse, we clearly see that SimCLR-abs should have less variance as it has the least amount of collapse, and SimCLR-sq the highest variance as it has the most amount of collapse. Since this is what we observe in practice, these results are coherent with the three methods producing similar cosine similarities distributions, albeit with different standard deviations depending on the amount of dimensional collapse.  ", "publication_ref": ["b23"], "figure_ref": ["fig_6", "fig_7", "fig_9"], "table_ref": []}, {"heading": "H ROW AND COLUMN NORMS INTERPLAY", "text": "While we provided bounds that apply to any matrix in lemma 3.4, in practice embedding matrices have a particular structure and one can wonder where the norms are in between the relatively distant bounds.\nTo study this we took images from ImageNet, computed the corresponding embedding matrices, and then l2-normalized the rows or columns. As we can see in table S4, for every method, in any expansion or projection scenario, we are always close to the lower bound, deviating by a factor of 3 at most. This is significantly smaller than the factors N or M in lemma 3.4 which are tight when making no assumptions on the embedding matrix K. As previously discussed these extreme cases consist respectively of a constant matrix and one with only one non-zero element per row/column. It is logical that the embedding matrices that we have in practice are closer to a constant matrix, with a uniform spread of information, even though they still present some sparsity.\nAs such, for all practical concerns, the bounds are much closer in practice than they theoretically are. This means that the sample-contrastive and dimension-contrastive criteria will also be closer in practice.\nFigure S6: Online performance on ImageNet for VICReg, VICReg-exp, VICReg-ctr, and SimCLR with respect to embedding dimensions when changing the projector's architecture.\nAs discussed in section 5, the design of the projector plays a significant role in downstream performance. In figure S6, we also overlay the results for a projector with architecture 2048 \u2212 2048 \u2212 d on top of the previously discussed ones. Such a projector offers similar behavior as an 8192 \u2212 8192 \u2212 d one, but with a bit lower performance. The drop in performance is especially noticeable in dimension-contrastive methods.  As we can see in figure S7, if we take a look at the performance with respect to the number of parameters of the projector we can see a clear trend that indicates that performance is increased when increasing the number of parameters of the projector. This conclusion holds for all methods though there are some scenarios that are clear outliers. For example, for VICReg and VICReg exp we can see that with a 2048 \u2212 256 projector, the performance is significantly lower than expected.\nWhile it would be interesting to see if this increase in performance saturates at some point, our largest projectors already have 151 million parameters. Increasing it further quickly starts to become impractical due to memory constraints during training, and as such, we leave this study to future work.\nAnother aspect worth mentioning is that the increase in performance when increasing the number of parameters is not automatic. For example for VICReg, the scenario 2048 \u2212 2048 \u2212 1024 achieves 66.68% top-1 for 10 million parameters, but the scenario 8192 \u2212 8192 \u2212 256 only achieves 65.01% even though it has 86 million parameters. This drastic difference suggests that some care must be taken when designing the projector and that even though the number of parameters is important, the architecture in itself also is.", "publication_ref": [], "figure_ref": ["fig_11"], "table_ref": ["tab_4"]}, {"heading": "J INFLUENCE OF LOSS FUNCTION DESIGN ON OPTIMIZATION QUALITY", "text": "As previously discussed, the introduction of VICReg-exp allows us to study the influence of the use of the LogSumExp operator in the repulsive force, and VICReg-ctr to study the difference between sample-contrastive and dimension-contrastive methods when comparing it to VICReg-exp. This enables us to quantify the impact of these design choices on the quality of the optimization process.\nWhile a perfect optimization of the aforementioned criteria would lead to embeddings with similar properties for the covariance and Gram matrix, one can wonder how well they are optimized in practice and whether design choices have a significant impact. To this effect we will look at the Gram and Covariance matrices after optimization, both on the embeddings to study the quality of the optimization process and on the representations to study the transferability of this process to the representations since they are used for downstream tasks. For the embeddings, we use the same normalization process as is used during training, and we center the representations to alleviate the fact that the last ReLU layer constrains them to the positive orthant. This centering on the representations is only done to make the visualization more interpretable.\nAs we can see in figure S8, while VICReg penalizes the off-diagonal terms of the covariance matrix and not the Gram matrix, both matrices have off-diagonal terms that are significantly smaller than their diagonal counterparts. Similarly for VICReg-exp, we can see that both the Gram and covariance matrices are dominated by their diagonal in the embedding space, though there is noise in the off-diagonal terms. This is due to the use of the LogSumExp, which as a smooth approximation of the max operator, will mostly penalize the largest values. On the other hand, using squared values will make them penalized by their absolute and not relative value. We also observe the same behavior for VICReg-ctr and SimCLR, leading to Gram and covariance matrices that are dominated by their diagonal but are overall noisier than for VICReg and VICRegexp. This suggests that the main culprit of this noise is indeed the LogSumExp but that the samplecontrastive nature of VICReg-ctr and SimCLR also played a role in creating it.\nLooking at the representations, the differences between the methods start to fade. They all still produce Gram and covariance matrices that are dominated by their diagonal, but with some offdiagonal noise. Even though we could see a clear difference in the quality of the optimization in the embedding space, the similarity in the representation space makes it harder to interpret for practical scenarios. Indeed, we saw that all methods can be made to perform the same when evaluating the representations. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Covariance matrix", "text": "Gram matrix ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Contrastive and non-contrastive self-supervised learning recover global and local spectral embedding methods", "journal": "", "year": "2022", "authors": "Randall Balestriero; Yann Lecun"}, {"ref_id": "b1", "title": "Vicreg: Variance-invariance-covariance regularization for self-supervised learning", "journal": "", "year": "2021", "authors": "Adrien Bardes; Jean Ponce; Yann Lecun"}, {"ref_id": "b2", "title": "Signature verification using a \"siamese\" time delay neural network. In NeurIPS", "journal": "", "year": "1994", "authors": "Jane Bromley; Isabelle Guyon; Yann Lecun; Eduard Sackinger; Roopak Shah"}, {"ref_id": "b3", "title": "Deep clustering for unsupervised learning", "journal": "", "year": "2018", "authors": "Mathilde Caron; Piotr Bojanowski; Armand Joulin; Matthijs Douze"}, {"ref_id": "b4", "title": "Unsupervised learning of visual features by contrasting cluster assignments", "journal": "", "year": "2020", "authors": "Mathilde Caron; Ishan Misra; Julien Mairal; Priya Goyal; Piotr Bojanowski; Armand Joulin"}, {"ref_id": "b5", "title": "Emerging properties in self-supervised vision transformers", "journal": "", "year": "", "authors": "Mathilde Caron; Hugo Touvron; Ishan Misra; Herve Jegou; Julien Mairal Piotr Bojanowski Armand Joulin"}, {"ref_id": "b6", "title": "A simple framework for contrastive learning of visual representations", "journal": "PMLR", "year": "2020", "authors": "Ting Chen; Simon Kornblith; Mohammad Norouzi; Geoffrey Hinton"}, {"ref_id": "b7", "title": "Intriguing properties of contrastive losses", "journal": "Advances in Neural Information Processing Systems", "year": "2021", "authors": "Ting Chen; Calvin Luo; Lala Li"}, {"ref_id": "b8", "title": "Exploring simple siamese representation learning", "journal": "", "year": "2020", "authors": "Xinlei Chen; Kaiming He"}, {"ref_id": "b9", "title": "Improved baselines with momentum contrastive learning", "journal": "", "year": "2020", "authors": "Xinlei Chen; Haoqi Fan; Ross Girshick; Kaiming He"}, {"ref_id": "b10", "title": "An empirical study of training self-supervised vision transformers", "journal": "", "year": "2021", "authors": "Xinlei Chen; Saining Xie; Kaiming He"}, {"ref_id": "b11", "title": "Imagenet: A large-scale hierarchical image database", "journal": "", "year": "2009", "authors": "Jia Deng; Wei Dong; Richard Socher; Li-Jia Li; Kai Li; Li Fei-Fei"}, {"ref_id": "b12", "title": "Whitening for selfsupervised representation learning", "journal": "", "year": "2021", "authors": "Aleksandr Ermolov; Aliaksandr Siarohin; Enver Sangineto; Nicu Sebe"}, {"ref_id": "b13", "title": "Watermarking images in self-supervised latent spaces", "journal": "IEEE", "year": "2022", "authors": "Pierre Fernandez; Alexandre Sablayrolles; Teddy Furon; Herv\u00e9 J\u00e9gou; Matthijs Douze"}, {"ref_id": "b14", "title": "Measuring statistical dependence with hilbert-schmidt norms", "journal": "Springer", "year": "2005", "authors": "Arthur Gretton; Olivier Bousquet; Alex Smola; Bernhard Sch\u00f6lkopf"}, {"ref_id": "b15", "title": "Bootstrap your own latent: A new approach to self-supervised learning", "journal": "", "year": "2020", "authors": "Jean-Bastien Grill; Florian Strub; Florent Altch\u00e9; Corentin Tallec; Pierre H Richemond; Elena Buchatskaya; Carl Doersch; Bernardo Avila Pires; Zhaohan Daniel Guo; Mohammad Gheshlaghi Azar; Bilal Piot; Koray Kavukcuoglu; R\u00e9mi Munos; Michal Valko"}, {"ref_id": "b16", "title": "Beyond supervised vs. unsupervised: Representative benchmarking and analysis of image representation learning", "journal": "", "year": "2022", "authors": "Matthew Gwilliam; Abhinav Shrivastava"}, {"ref_id": "b17", "title": "Predictor networks and stop-grads provide implicit variance regularization in byol/simsiam", "journal": "", "year": "2022", "authors": "Axel Manu Srinath Halvagal; Friedemann Laborieux;  Zenke"}, {"ref_id": "b18", "title": "Provable guarantees for self-supervised deep learning with spectral contrastive loss", "journal": "NeurIPS", "year": "", "authors": "Colin Jeff Z Haochen; Adrien Wei; Tengyu Gaidon;  Ma"}, {"ref_id": "b19", "title": "Beyond separability: Analyzing the linear transferability of contrastive representations to related subpopulations", "journal": "", "year": "2022", "authors": "Colin Jeff Z Haochen; Ananya Wei; Tengyu Kumar;  Ma"}, {"ref_id": "b20", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b21", "title": "Momentum contrast for unsupervised visual representation learning", "journal": "", "year": "2020", "authors": "Kaiming He; Haoqi Fan; Yuxin Wu; Saining Xie; Ross Girshick"}, {"ref_id": "b22", "title": "Towards the generalization of contrastive selfsupervised learning", "journal": "", "year": "2021", "authors": "Weiran Huang; Mingyang Yi; Xuyang Zhao"}, {"ref_id": "b23", "title": "Understanding dimensional collapse in contrastive self-supervised learning", "journal": "", "year": "2022", "authors": "Li Jing; Pascal Vincent; Yann Lecun; Yuandong Tian"}, {"ref_id": "b24", "title": "Similarity of neural network representations revisited", "journal": "PMLR", "year": "2019", "authors": "Simon Kornblith; Mohammad Norouzi; Honglak Lee; Geoffrey Hinton"}, {"ref_id": "b25", "title": "Ica with reconstruction cost for efficient overcomplete feature learning", "journal": "NeurIPS", "year": "2011", "authors": "Quoc Le; Alexandre Karpenko; Jiquan Ngiam; Andrew Ng"}, {"ref_id": "b26", "title": "Predicting what you already know helps: Provable self-supervised learning", "journal": "", "year": "2021", "authors": "Qi Jason D Lee; Nikunj Lei; Jiacheng Saunshi;  Zhuo"}, {"ref_id": "b27", "title": "Compressive visual representations", "journal": "", "year": "2021", "authors": "Kuang-Huei Lee; Anurag Arnab; Sergio Guadarrama; John Canny; Ian Fischer"}, {"ref_id": "b28", "title": "Efficient self-supervised vision transformers for representation learning", "journal": "", "year": "2022", "authors": "Chunyuan Li; Jianwei Yang; Pengchuan Zhang; Mei Gao; Bin Xiao; Xiyang Dai; Lu Yuan; Jianfeng Gao"}, {"ref_id": "b29", "title": "Concise formulas for the area and volume of a hyperspherical cap", "journal": "Asian Journal of Mathematics and Statistics", "year": "2011", "authors": "Shengqiao Li"}, {"ref_id": "b30", "title": "Neural manifold clustering and embedding", "journal": "", "year": "2022", "authors": "Zengyi Li; Yubei Chen; Yann Lecun; Friedrich T Sommer"}, {"ref_id": "b31", "title": "Representation learning with contrastive predictive coding", "journal": "", "year": "2018", "authors": "Aaron Van Den Oord; Yazhe Li; Oriol Vinyals"}, {"ref_id": "b32", "title": "Contrasting the landscape of contrastive and non-contrastive learning", "journal": "", "year": "2022", "authors": "Ashwini Pokle; Jinjin Tian; Yuchen Li; Andrej Risteski"}, {"ref_id": "b33", "title": "Connect, not collapse: Explaining contrastive learning for unsupervised domain adaptation", "journal": "", "year": "2022", "authors": "Kendrick Shen; Robbie Jones; Ananya Kumar; Sang Michael Xie; Jeff Z Haochen; Tengyu Ma; Percy Liang"}, {"ref_id": "b34", "title": "Exploring the equivalence of siamese self-supervised learning via a unified gradient framework", "journal": "", "year": "2021", "authors": "Chenxin Tao; Honghui Wang; Xizhou Zhu; Jiahua Dong; Shiji Song; Gao Huang; Jifeng Dai"}, {"ref_id": "b35", "title": "Understanding self-supervised learning dynamics without contrastive pairs", "journal": "", "year": "2021", "authors": "Yuandong Tian; Xinlei Chen; Surya Ganguli"}, {"ref_id": "b36", "title": "Pushing the limits of self-supervised resnets: Can we outperform supervised learning without labels on imagenet?", "journal": "", "year": "2022", "authors": "Nenad Tomasev; Ioana Bica; Brian Mcwilliams; Lars Buesing; Razvan Pascanu; Charles Blundell; Jovana Mitrovic"}, {"ref_id": "b37", "title": "A note on connecting barlow twins with negative-sample-free contrastive learning", "journal": "", "year": "2021", "authors": "Yao-Hung Hubert Tsai; Shaojie Bai; Louis-Philippe Morency; Ruslan Salakhutdinov"}, {"ref_id": "b38", "title": "Understanding contrastive representation learning through alignment and uniformity on the hypersphere", "journal": "PMLR", "year": "2020", "authors": "Tongzhou Wang; Phillip Isola"}, {"ref_id": "b39", "title": "Toward understanding the feature learning process of self-supervised contrastive learning", "journal": "PMLR", "year": "2021", "authors": "Zixin Wen; Yuanzhi Li"}, {"ref_id": "b40", "title": "", "journal": "", "year": "2021", "authors": "Chun-Hsiao Yeh; Cheng-Yao Hong; Yen-Chi Hsu; Tyng-Luh Liu; Yubei Chen; Yann Lecun"}, {"ref_id": "b41", "title": "Large batch training of convolutional networks", "journal": "", "year": "2017", "authors": "Yang You; Igor Gitman; Boris Ginsburg"}, {"ref_id": "b42", "title": "Barlow twins: Self-supervised learning via redundancy reduction", "journal": "PMLR", "year": "2021", "authors": "Jure Zbontar; Li Jing; Ishan Misra; Yann Lecun; St\u00e9phane Deny"}, {"ref_id": "b43", "title": "Dual temperature helps contrastive learning without many negative samples: Towards understanding and simplifying moco", "journal": "", "year": "2022", "authors": "Chaoning Zhang; Kang Zhang; X Trung; Axi Pham; Zhinan Niu;  Qiao; D Chang; In So Yoo;  Kweon"}, {"ref_id": "b44", "title": "ibot: Image bert pre-training with online tokenizer", "journal": "", "year": "2022", "authors": "Jinghao Zhou; Chen Wei; Huiyu Wang; Wei Shen; Cihang Xie; Alan Yuille; Tao Kong"}, {"ref_id": "b45", "title": "Mugs: A multi-granular self-supervised learning framework", "journal": "", "year": "2022", "authors": "Pan Zhou; Yichen Zhou; Chenyang Si; Weihao Yu; Teck Khim Ng; Shuicheng Yan"}, {"ref_id": "b46", "title": "K COMPLETE PERFORMANCE AND HYPERPARAMETER TABLES", "journal": "", "year": "", "authors": ""}, {"ref_id": "b47", "title": "Table S5: Top-1 accuracy on ImageNet using the online linear classifier, including all performances for figures 1 and S6", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure2: The performance of SimCLR is unchanged when introducing centering or dimension standardization, highlighting the lack of importance of normalization on peak performance.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "FigureS1: Reproduction of figure1with a k-nn classifier. We notice the same pattern as previously, where going from dimension-contrastive to sample-contrastive does not lead to a significant drop in performance.", "figure_data": ""}, {"figure_label": "S2", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure S2 :S2Figure S2: CKA of the learned representations on all considered methods. For a given method, we use experiments with different output dimensions (1024,2048,8192) that achieved equivalent performance.", "figure_data": ""}, {"figure_label": "S3", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure S3 :S3Figure S3: Influence of using squared or absolute values of the cosine similarities for VICReg, with different projector architectures.", "figure_data": ""}, {"figure_label": "S4", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure S4 :S4Figure S4: Histogram of cosine similarities for negative pairs in SimCLR-abs, SimCLR-sq and SimCLR.", "figure_data": ""}, {"figure_label": "S5", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure S5 :S5Figure S5: Singular value distribution of the embeddings and representations computed on the training set of ImageNet for SimCLR, SimCLR-abs and SimCLR-sq. All methods use 512 dimensional embeddings.", "figure_data": ""}, {"figure_label": "S7", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure S7 :S7Figure S7: Online performance on ImageNet for VICReg, VICReg-exp, VICReg-ctr, and SimCLR with respect to the number of parameters in their projector.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Normalisation strategy used by different methods. Scenarios A and B for SimCLR enable a fairer comparison to VICReg-ctr and VICReg respectively.", "figure_data": "MethodVICReg VICReg-exp VICReg-ctrSimCLRClassical ABDimension centering\u2713\u2713\u2713\u2717\u2713\u2713Embedding norm111Dimension norm"}, {"figure_label": "S2S3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Relationship in performance between the online linear probe and the offline linear classifier. We used VICReg and an expander with architecture 8192 \u2212 8192 \u2212 d. Relationship in performance between the online linear probe and the offline linear classifier. We used VICReg-exp and an expander with architecture 8192 \u2212 8192 \u2212 d.", "figure_data": "Embedding dimension2565121024 2048 8192Online top-165.01 66.72 68.06 68.06 68.13Offline top-165.11 66.64 67.96 68.00 68.02Embedding dimension2565121024 2048 8192Online top-165.24 66.71 67.86 68.00 67.93Offline top-165.30 66.58 67.83 67.89 68.18"}, {"figure_label": "S4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "The empirical interplay between embedding matrix norms under row-or column-wise l2-normalization for different methods and projector architectures. We abbreviate thousands with k and millions with M. The experiment \"Random\" indicates a randomly initialized network.", "figure_data": "ExperimentProjectorColum normalizationRow normalizationN 2 M\u2225K j,\u2022 \u2225 4 2N 2 M 2 N\u2225K \u2022,i \u2225 4 2M 2VICReg8192 \u2212 8192 \u2212 8192 128128.191M 65k83k67MVICReg-exp 8192 \u2212 8192 \u2212 8192 128128.261M 65k95k67MVICReg-ctr8192 \u2212 8192 \u2212 512204820781M 256287262kSimCLR8192 \u2212 8192 \u2212 512 8192 \u2212 8192 \u2212 8192 128 20482061 129.431M 256 1M 65k433.54 113k262k 67MRandom8192 \u2212 8192 \u2212 8192 128361.341M 65k75k67M"}, {"figure_label": "S6", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Hyperparameters used for the results in table S5. Sim., Var. and Cov. indicate the weights of the criteria in VICReg and its variations. \u03c4 indicates the temperature used for LogSumExp-based methods. The hyperparameters for VICReg and SimCLR are usable with the official implementations. For VICReg-exp and VICReg-ctr, the hyperparameters are compatible with the pseudocode in section L. VICReg-exp PyTorch pseudocode.", "figure_data": "L VICREG VARIATIONS PSEUDOCODEAlgorithm 1: # f: encoder network,p: projector network, lambda, mu, nu:coefficients of the invariance, variance and covariancelosses, N: batch size, D: dimension of therepresentations, tau: temperature# mse_loss: Mean square error loss function, relu: ReLUactivation function, cut_out_diag: remove the diagonal ofa matrix,for x in loader: # load a batch with N samples# two randomly augmented versions of xx_a, x_b = augment(x)# compute embeddingsk_a = p(f(x_a)) # N x Dk_b = p(f(x_b)) # N x D# invariance losssim_loss = mse_loss(k_a, k_b)# variance lossstd_k_a = torch.sqrt(k_a.var(dim=0) + 1e-04)std_k_b = torch.sqrt(k_b.var(dim=0) + 1e-04)std_loss = torch.mean(relu(1 -std_k_a))/2 + torch.mean(relu(1 -std_k_b))/2# covariance lossk_a = k_a -k_a.mean(dim=0)k_b = k_b -k_b.mean(dim=0)cov_k_a = (k_a.T @ k_a) / (N -1)cov_k_b = (k_b.T @ k_b) / (N -1)cov_loss = torch.logsumexp(cut_out_diag(cov_k_a/tau),1).mean()/2 +Experiment torch.logsumexp(cut_out_diag(cov_k_b/tau),1). Projector Batch size base lr VICReg \u03c4 Sim. Var. Cov. mean()/2# lossd = 25610240.325254d = 512 loss = lambda * sim_loss + mu * std_loss + nu * cov_loss 1024 0.3 25 25 2VICRegd = 102410240.325252d = 2048 # optimization step10240.325252d = 8192 loss.backward()10240.325250.5d = 8192 d \u0338 = 8192 optimizer.step() VICReg-exp1024 10240.8 0.51 11 12 20.1 0.1VICReg-ctrAll10240.61110.15SimCLR-Tuned All20480.50.15"}], "formulas": [{"formula_id": "formula_0", "formula_text": "p \u2032 \u03b8 \u2032 (f \u2032 \u03b8 \u2032 (x \u2032 i )).", "formula_coordinates": [3.0, 108.0, 266.06, 51.29, 12.55]}, {"formula_id": "formula_1", "formula_text": "f \u03b8 = f \u2032 \u03b8 \u2032 and p \u03b8 = p \u2032 \u03b8 \u2032 . While most self-supervised learning approaches use positive pairs (x i , x \u2032 i ) and negative pairs {\u2200j, j \u0338 = i, (x i , x j )} {\u2200j, j \u0338 = i, (x i , x \u2032 j )", "formula_coordinates": [3.0, 108.0, 300.2, 396.0, 23.28]}, {"formula_id": "formula_2", "formula_text": "diag (A) i,j = A i,i , if i = j 0, otherwise. (1", "formula_coordinates": [3.0, 238.21, 436.83, 261.92, 22.11]}, {"formula_id": "formula_3", "formula_text": ")", "formula_coordinates": [3.0, 500.13, 443.59, 3.87, 8.64]}, {"formula_id": "formula_4", "formula_text": "L c = \u2225K T K\u2212diag(K T K)\u2225 2 F .", "formula_coordinates": [3.0, 108.0, 474.13, 119.38, 12.47]}, {"formula_id": "formula_5", "formula_text": "E x T x \u2212 = 0 and Var x T x \u2212 = 1 M . (2", "formula_coordinates": [3.0, 220.53, 681.79, 279.6, 22.31]}, {"formula_id": "formula_6", "formula_text": ")", "formula_coordinates": [3.0, 500.13, 688.84, 3.87, 8.64]}, {"formula_id": "formula_7", "formula_text": "L nc + M j=1 \u2225K j,\u2022 \u2225 4 2 = L c + N i=1 \u2225K \u2022,i \u2225 4 2 . (3", "formula_coordinates": [4.0, 226.15, 412.14, 273.98, 30.32]}, {"formula_id": "formula_8", "formula_text": ")", "formula_coordinates": [4.0, 500.13, 422.87, 3.87, 8.64]}, {"formula_id": "formula_9", "formula_text": "L nc = L c + N \u2212 M .", "formula_coordinates": [4.0, 108.0, 584.15, 91.62, 9.65]}, {"formula_id": "formula_10", "formula_text": "N 2 M a 4 \u2264 M j=1 \u2225K j,\u2022 \u2225 4 2 \u2264 N 2 a 4 .(4)", "formula_coordinates": [5.0, 244.75, 174.17, 259.25, 30.32]}, {"formula_id": "formula_11", "formula_text": "M 2 N a 4 \u2264 N i=1 \u2225K \u2022,i \u2225 4 2 \u2264 M 2 a 4 .(5)", "formula_coordinates": [5.0, 243.32, 225.35, 260.69, 30.32]}, {"formula_id": "formula_12", "formula_text": "L nc \u2212 N + N 2 M \u2264 L c \u2264 L nc \u2212 N + N 2 .(6)", "formula_coordinates": [5.0, 221.95, 368.69, 282.05, 23.89]}, {"formula_id": "formula_13", "formula_text": "L c \u2212 M + M 2 N \u2264 L nc \u2264 L c \u2212 M + M 2 . (7", "formula_coordinates": [5.0, 221.09, 407.86, 279.04, 23.89]}, {"formula_id": "formula_14", "formula_text": ")", "formula_coordinates": [5.0, 500.13, 416.49, 3.87, 8.64]}, {"formula_id": "formula_15", "formula_text": "c(K T ) = \u2225K T K T T \u2212 diag K T K T T \u2225 2 F = \u2225K T K \u2212 diag(K T K)\u2225 2 F = L c (K). (8)", "formula_coordinates": [5.0, 134.2, 601.99, 369.8, 15.0]}, {"formula_id": "formula_16", "formula_text": "L reg (K) = N i=1 \u2225K \u2022,i \u2225 4 2 \u2212 M j=1 \u2225K j,\u2022 \u2225 4 2 ,(9)", "formula_coordinates": [5.0, 228.24, 643.8, 275.76, 30.32]}, {"formula_id": "formula_17", "formula_text": "L V ICReg = \u03bb N i=1 \u2225K \u2022,i \u2212K \u2032 \u2022,i \u2225 2 2 +\u00b5 (v(K) + v(K \u2032 ))+\u03bd (L c (K) + L reg (K) + L c (K \u2032 ) + L reg (K \u2032 )) .(10)", "formula_coordinates": [5.0, 108.0, 692.1, 398.85, 39.91]}, {"formula_id": "formula_18", "formula_text": "VICReg LogSumExp \u2212 \u2212\u2212\u2212\u2212\u2212\u2192 VICReg-exp Contrastive \u2212 \u2212\u2212\u2212\u2212\u2212 \u2192 VICReg-ctr Neg. pair sampling \u2212 \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 \u2192 SimCLR", "formula_coordinates": [6.0, 144.82, 390.07, 322.36, 12.96]}, {"formula_id": "formula_19", "formula_text": "c exp (K) = 1 d i log \uf8eb \uf8ed j\u0338 =i e C(K)i,j /\u03c4 \uf8f6 \uf8f8 ,(11)", "formula_coordinates": [6.0, 223.71, 452.45, 280.29, 33.76]}, {"formula_id": "formula_20", "formula_text": "L V ICReg\u2212exp = \u03bb N i=1 \u2225K \u2022,i \u2212 K \u2032 \u2022,i \u2225 2 2 + \u00b5 (v(K) + v(K \u2032 )) + \u03bd (c exp (K) + c exp (K \u2032 )) .(12)", "formula_coordinates": [6.0, 124.11, 531.74, 379.89, 30.32]}, {"formula_id": "formula_21", "formula_text": "L V ICReg\u2212ctr = \u03bb N i=1 \u2225K \u2022,i \u2212 K \u2032 \u2022,i \u2225 2 2 + \u00b5 v(K T ) + v(K \u2032T ) + \u03bd c exp (K T ) + c exp (K \u2032T ) . (13", "formula_coordinates": [6.0, 112.98, 648.97, 386.87, 30.32]}, {"formula_id": "formula_22", "formula_text": ")", "formula_coordinates": [6.0, 499.85, 659.7, 4.15, 8.64]}, {"formula_id": "formula_23", "formula_text": "\u221a N \u221a N N/M", "formula_coordinates": [8.0, 226.6, 163.75, 258.31, 17.17]}, {"formula_id": "formula_24", "formula_text": "\u2200i \u2208 [1, . . . , M ] K \u2022,i =K \u2022,i \u2225K \u2022,i \u2225 2 \u00d7 N M withK \u2022,i = K \u2022,i \u2212 1 N N j=1 K j,i .(14)", "formula_coordinates": [9.0, 148.97, 294.18, 355.03, 30.32]}, {"formula_id": "formula_25", "formula_text": "L DCL = N i=1 \u2212 log e K T \u2022,i K \u2032 \u2022,i /\u03c4 j\u0338 =i e K T \u2022,i K\u2022,j /\u03c4 = N i=1 \u2212 K T \u2022,i K \u2032 \u2022,i \u03c4 + log \uf8eb \uf8ed j\u0338 =i e K T \u2022,i K\u2022,j /\u03c4 \uf8f6 \uf8f8 .(15)", "formula_coordinates": [13.0, 129.12, 178.12, 374.88, 33.79]}, {"formula_id": "formula_26", "formula_text": "L SimCLR = N i=1 \u2212 log e K T \u2022,i K \u2032 \u2022,i /\u03c4 e K T \u2022,i K \u2032 \u2022,i /\u03c4 + j\u0338 =i e K T \u2022,i K\u2022,j /\u03c4 (16) = N i=1 \u2212 K T \u2022,i K \u2032 \u2022,i \u03c4 + log \uf8eb \uf8ed e K T \u2022,i K \u2032 \u2022,i /\u03c4 + j\u0338 =i e K T \u2022,i K\u2022,j /\u03c4 \uf8f6 \uf8f8 .(17)", "formula_coordinates": [13.0, 174.55, 275.4, 329.45, 68.33]}, {"formula_id": "formula_27", "formula_text": "L SCL = \u22122 N i=1 K T \u2022,i K \u2032 \u2022,i + j\u0338 =i K T \u2022,i K \u2022,j 2 = \u22122 N i=1 K T \u2022,i K \u2032 \u2022,i + \u2225K T K \u2212 diag(K T K)\u2225 2 F . (18)", "formula_coordinates": [13.0, 112.98, 383.15, 391.02, 30.55]}, {"formula_id": "formula_28", "formula_text": "L BT = M j=1 1 \u2212 (KK \u2032T ) j,j 2 +\u03bb M i,j,i\u0338 =j (KK \u2032T ) 2 j,i = M j=1 1 \u2212 (KK \u2032T ) j,j 2 +\u03bb\u2225KK \u2032T \u2212diag(KK \u2032T )\u2225 2 F .(19)", "formula_coordinates": [13.0, 108.0, 484.83, 416.02, 41.5]}, {"formula_id": "formula_29", "formula_text": "L V ICReg = \u03bb N i=1 \u2225K \u2022,i \u2212 K \u2032 \u2022,i \u2225 2 2 + \u00b5 (v(K) + v(K \u2032 )) + \u03bd (c(K) + c(K \u2032 )) .(20)", "formula_coordinates": [13.0, 154.63, 554.92, 349.37, 30.32]}, {"formula_id": "formula_30", "formula_text": "c(K) = i\u0338 =j Cov(K) 2 i,j = \u2225KK T \u2212 diag(KK T )\u2225 2 F = L nc ,(21)", "formula_coordinates": [13.0, 189.65, 616.74, 314.35, 22.21]}, {"formula_id": "formula_31", "formula_text": "L T CR = \u2212 1 2 log det (I + \u03b1Cov(K)) = \u2212 1 2 log det I + \u03b1KK T = \u2212 1 2 i log 1 + \u03b1\u03c3 2 i ,(22)", "formula_coordinates": [13.0, 120.04, 684.82, 383.96, 36.24]}, {"formula_id": "formula_32", "formula_text": "Lemma C.1. Let X, Y \u223c \u03c3 D\u22121 two i.i.d", "formula_coordinates": [14.0, 108.0, 545.41, 171.95, 10.53]}, {"formula_id": "formula_33", "formula_text": "X T Y + 1 2 \u223c Beta D \u2212 1 2 , D \u2212 1 2 .", "formula_coordinates": [14.0, 229.33, 574.96, 154.54, 23.89]}, {"formula_id": "formula_34", "formula_text": "X T Y +1 2 .", "formula_coordinates": [14.0, 210.75, 623.72, 32.76, 15.12]}, {"formula_id": "formula_35", "formula_text": "1 \u2212 (X T Y ) 2 \u223c Beta D\u22121 2 , 1 2 .", "formula_coordinates": [14.0, 128.95, 721.18, 127.62, 13.47]}, {"formula_id": "formula_36", "formula_text": "T Y ) 2 \u223c Beta 1 2 , D\u22121 2 . By substituting in u = X T Y +1 2 if follows directly that u \u223c Beta D \u2212 1 2 , D \u2212 1 2 ,(23)", "formula_coordinates": [15.0, 108.0, 83.45, 396.0, 76.14]}, {"formula_id": "formula_37", "formula_text": "E x T x \u2212 = 0 and Var x T x \u2212 = 1 M . (24", "formula_coordinates": [15.0, 220.53, 214.92, 279.32, 22.31]}, {"formula_id": "formula_38", "formula_text": ")", "formula_coordinates": [15.0, 499.85, 221.98, 4.15, 8.64]}, {"formula_id": "formula_39", "formula_text": "X T Y +1 2 \u223c Beta M \u22121 2 , M \u22121 2 .", "formula_coordinates": [15.0, 108.0, 309.59, 396.0, 28.24]}, {"formula_id": "formula_40", "formula_text": "E X T Y + 1 2 = M \u22121 2 M \u22121 2 + M \u22121 2 = 1 2 \u21d2 E X T Y = 0,(25)", "formula_coordinates": [15.0, 123.43, 354.3, 380.57, 28.39]}, {"formula_id": "formula_41", "formula_text": "X T Y + 1 2 = M \u22121 2 \u00d7 M \u22121 2 M \u22121 2 + M \u22121 2 2 M \u22121 2 + M \u22121 2 + 1 = 1 4M \u21d2 Var X T Y = 1 M ,(26)", "formula_coordinates": [15.0, 138.19, 385.32, 365.81, 30.42]}, {"formula_id": "formula_42", "formula_text": "L DCL = N i=1 \u2212 log \uf8eb \uf8ed e f (K T \u2022,i K \u2032 \u2022,i )/\u03c4 j\u0338 =i e f (K T \u2022,i K\u2022,j )/\u03c4 \uf8f6 \uf8f8 = N i=1 \u2212 f K T \u2022,i K \u2032 \u2022,i \u03c4 + log \uf8eb \uf8ed j\u0338 =i e f (K T \u2022,i K\u2022,j )/\u03c4 \uf8f6 \uf8f8 .", "formula_coordinates": [15.0, 115.8, 547.23, 380.41, 34.71]}, {"formula_id": "formula_43", "formula_text": "max {\u2200j \u0338 = i, f K T \u2022,i K \u2022,j } \u2264 \u03c4 log \uf8eb \uf8ed j\u0338 =i e f (K T \u2022,i K\u2022,j )/\u03c4 \uf8f6 \uf8f8 \u2264 max {\u2200j \u0338 = i, f K T \u2022,i K \u2022,j } +\u03c4 log(N \u22121).", "formula_coordinates": [15.0, 108.0, 634.06, 440.74, 33.76]}, {"formula_id": "formula_44", "formula_text": "N i=1 log \uf8eb \uf8ed j\u0338 =i e f (K T \u2022,i K\u2022,j )/\u03c4 \uf8f6 \uf8f8 or N i=1 max j\u0338 =i f K T \u2022,i K \u2022,j ,(29)", "formula_coordinates": [15.0, 187.0, 698.98, 317.0, 33.76]}, {"formula_id": "formula_45", "formula_text": "L SimCLR = N i=1 \u2212 log \uf8eb \uf8ed e f (K T \u2022,i K \u2032 \u2022,i )/\u03c4 e f (K T \u2022,i K \u2032 \u2022,i )/\u03c4 + j\u0338 =i e f (K T \u2022,i K\u2022,j )/\u03c4 \uf8f6 \uf8f8 (30) = N i=1 \u2212 f K T \u2022,i K \u2032 \u2022,i \u03c4 + log \uf8eb \uf8ed e f (K T \u2022,i K \u2032 \u2022,i )/\u03c4 + j\u0338 =i e f (K T \u2022,i K\u2022,j )/\u03c4 \uf8f6 \uf8f8 . (31", "formula_coordinates": [16.0, 153.77, 203.63, 350.23, 73.61]}, {"formula_id": "formula_46", "formula_text": ")", "formula_coordinates": [16.0, 499.85, 257.42, 4.15, 8.64]}, {"formula_id": "formula_47", "formula_text": "L SCL = \u22122 N i=1 K T \u2022,i K \u2032 \u2022,i + j\u0338 =i K T \u2022,i K \u2022,j 2 = \u22122 N i=1 K T \u2022,i K \u2032 \u2022,i + \u2225K T K \u2212 diag(K T K)\u2225 2 F . (32", "formula_coordinates": [16.0, 112.98, 388.35, 386.87, 30.55]}, {"formula_id": "formula_48", "formula_text": ")", "formula_coordinates": [16.0, 499.85, 399.09, 4.15, 8.64]}, {"formula_id": "formula_49", "formula_text": "L BT = M j=1 1 \u2212 (KK \u2032T ) j,j 2 +\u03bb M i,j,i\u0338 =j (KK \u2032T ) 2 j,i = M j=1 1 \u2212 (KK \u2032T ) j,j 2 +\u03bb\u2225KK \u2032T \u2212diag(KK \u2032T )\u2225 2 F .(33)", "formula_coordinates": [16.0, 108.0, 470.16, 416.02, 41.5]}, {"formula_id": "formula_50", "formula_text": "L BT = M j=1 1 \u2212 (KK \u2032T ) j,j 2 + \u03bb\u2225KK T \u2212 diag(KK T )\u2225 2 F . (34", "formula_coordinates": [16.0, 186.43, 550.49, 313.42, 30.32]}, {"formula_id": "formula_51", "formula_text": ")", "formula_coordinates": [16.0, 499.85, 561.22, 4.15, 8.64]}, {"formula_id": "formula_52", "formula_text": "L V ICReg = \u03bb N i=1 \u2225K \u2022,i \u2212 K \u2032 \u2022,i \u2225 2 2 + \u00b5 (v(K) + v(K \u2032 )) + \u03bd (c(K) + c(K \u2032 )) . (35", "formula_coordinates": [16.0, 154.63, 629.78, 345.22, 30.32]}, {"formula_id": "formula_53", "formula_text": ")", "formula_coordinates": [16.0, 499.85, 640.51, 4.15, 8.64]}, {"formula_id": "formula_54", "formula_text": "c(K) = i\u0338 =j Cov(K) 2 i,j = \u2225KK T \u2212 diag(KK T )\u2225 2 F = L nc .(36)", "formula_coordinates": [16.0, 189.65, 682.69, 314.35, 22.21]}, {"formula_id": "formula_55", "formula_text": "L T CR = \u2212 1 2 log det (I + \u03b1Cov(K)) = \u2212 1 2 log det I + \u03b1KK T = \u2212 1 2 i log 1 + \u03b1\u03c3 2 i ,(37)", "formula_coordinates": [17.0, 120.04, 100.25, 383.96, 36.24]}, {"formula_id": "formula_56", "formula_text": "\u2212 1 2 i log 1 + \u03b1\u03c3 2 i or \u2225KK T \u2212 diag(KK T )\u2225 2 F (38", "formula_coordinates": [17.0, 197.19, 166.65, 302.66, 26.65]}, {"formula_id": "formula_57", "formula_text": ")", "formula_coordinates": [17.0, 499.85, 173.71, 4.15, 8.64]}, {"formula_id": "formula_58", "formula_text": "L nc + M j=1 \u2225K j,\u2022 \u2225 4 2 = L c + N i=1 \u2225K \u2022,i \u2225 4 2 .(39)", "formula_coordinates": [17.0, 226.15, 273.1, 277.85, 30.32]}, {"formula_id": "formula_59", "formula_text": "L nc = \u2225KK T \u2212 diag(KK T )\u2225 2 F (40) = tr (KK T \u2212 diag(KK T )) T (KK T \u2212 diag(KK T ))(41)", "formula_coordinates": [17.0, 148.38, 358.95, 355.62, 27.18]}, {"formula_id": "formula_60", "formula_text": "= tr(KK T KK T ) \u2212 2tr(KK T diag(KK T )) + tr(diag(KK T ) diag(KK T ))(42)", "formula_coordinates": [17.0, 166.91, 391.45, 337.09, 11.03]}, {"formula_id": "formula_63", "formula_text": "L c = \u2225K T K \u2212 diag(K T K)\u2225 2 F (45) = tr(K T KK T K) \u2212 tr(K T K diag(K T K)).(46)", "formula_coordinates": [17.0, 213.58, 462.01, 290.42, 27.18]}, {"formula_id": "formula_64", "formula_text": "K T K i,i = \u2225K \u2022,i \u2225 2 2 we deduce that tr(K T K diag(K T K)) = N i=1 \u2225K \u2022,i \u2225 4 2 . Similarly, we obtain that tr(KK T diag(KK T )) = M j=1 \u2225K j,\u2022 \u2225 4", "formula_coordinates": [17.0, 108.0, 498.95, 396.0, 30.23]}, {"formula_id": "formula_65", "formula_text": "L nc = L c + N i=1 \u2225K \u2022,i \u2225 4 2 \u2212 M j=1 \u2225K j,\u2022 \u2225 4 2 ,(47)", "formula_coordinates": [17.0, 226.15, 548.75, 277.85, 30.32]}, {"formula_id": "formula_66", "formula_text": "N 2 M a 4 \u2264 M j=1 \u2225K j,\u2022 \u2225 4 2 \u2264 N 2 a 4 .(48)", "formula_coordinates": [17.0, 244.75, 644.98, 259.25, 30.32]}, {"formula_id": "formula_67", "formula_text": "M 2 N a 4 \u2264 N i=1 \u2225K \u2022,i \u2225 4 2 \u2264 M 2 a 4 .(49)", "formula_coordinates": [17.0, 243.32, 704.88, 260.68, 30.32]}], "doi": ""}