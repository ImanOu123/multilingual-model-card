{"title": "Dual-Shutter Optical Vibration Sensing", "authors": "Mark Sheinin; Dorian Chan; Matthew O'toole; Srinivasa G Narasimhan", "pub_date": "", "abstract": "Visual vibrometry is a highly useful tool for remote capture of audio, as well as the physical properties of materials, human heart rate, and more. While visually-observable vibrations can be captured directly with a high-speed camera, minute imperceptible object vibrations can be optically amplified by imaging the displacement of a speckle pattern, created by shining a laser beam on the vibrating surface. In this paper, we propose a novel method for sensing vibrations at high speeds (up to 63kHz), for multiple scene sources at once, using sensors rated for only 130Hz operation. Our method relies on simultaneously capturing the scene with two cameras equipped with rolling and global shutter sensors, respectively. The rolling shutter camera captures distorted speckle images that encode the highspeed object vibrations. The global shutter camera captures undistorted reference images of the speckle pattern, helping to decode the source vibrations. We demonstrate our method by capturing vibration caused by audio sources (e.g. speakers, human voice, and musical instruments) and analyzing the vibration modes of a tuning fork.", "sections": [{"heading": "Introduction", "text": "Vibrations are all around us, caused by sources ranging from heartbeats to engines, to music, speech, and ultrasonics. These vibrations exhibit various amplitudes (microns to meters) and frequencies (a few Hz to MHz). As such, measuring vibrations is an essential tool in many engineering and scientific fields. However, optically sensing vibrations, especially the low-amplitude high-frequency kind, is challenging. To make matters worse, indirect damped vibrations caused by remote sources (e.g. a speaker vibrating an object [13]) can be even more subtle. Additionally, these challenges are harder to overcome when the vibrating surface is far from the imaging system or is itself moving (e.g. the natural movements of a musician playing guitar).\nThat said, much progress has been made recently on visual vibrometry. Passive capture and estimation of small motions [15, 18, 21, 35-37, 39, 45]  Optically measuring vibrations allows remote capture of speech, music, and the mechanical vibrations of various objects, including engines, bridges, and more. We propose a new method for sensing 2D object vibrations at high speeds, using a dual-shutter sensor system consisting of two low-speed cameras.\nOur system samples vibration with speeds up to 63KHz, for multiple objects at once and can handle non-static objects. We test our system by capturing and replaying audio source vibrations (e.g. speakers, human voice), analyzing the vibrational modes of a tuning fork, and capturing the vibrations of musical instruments.\nhave been used to extract (a) heart rate and sound from video [13,39] and (b) the physical properties of materials [6,9,12,16] and structures (e.g. buildings and bridges) [10]. However, low-amplitude high-frequency vibrations require a high-speed video camera with a zoom-lens and bright lighting to compensate for the short camera exposures.\nIn contrast to passive approaches, active speckle-based approaches illuminate a vibrating surface with coherent light (e.g. laser) and image the resulting speckle [5,[40][41][42]. The speckle is imaged by focusing in between the surface and the sensor. A small tilt of the vibrating surface results in a shift of the speckle, a phenomenon called the memory effect [3,42]. 1 This approach optically magnifies small-amplitude vibrations and has been used to demonstrate long-distance audio capture [5,40,42]. But, as in the passive case, high-frequency vibrations would require expensive 2D high-speed cameras, whose available bandwidth limits either the maximum sampling frequency or the captured video's spatial resolution. In response, some works [5,20,40,41] use fast 1D sensors to capture high frequencies, but they can only reconstruct vibrations along one dimension.\nWe present a novel imaging system that exploits the speed of a 1D sensor but still estimates 2D speckle-based vibrations at high frequencies in a bandwidth-efficient manner. We are inspired by many works [2,4,11,23,26,29,38] that use a rolling-shutter sensor as a 1D sensor to achieve imaging at high speeds. With this observation, we make two important changes to conventional speckle-based vibration imaging. First, we add a cylindrical lens to spread the speckle image to cover the entire vertical field of view of the rolling-shutter sensor. This allows us to extend specklebased methods to sample multiple vibration locations simultaneously, using a low-speed camera, for the first time.\nHowever, the captured speckle is distorted by the rolling shutter with unknown shifts at each image row. This distortion makes it hard to recover the 2D vibrations other than in very specialized situations (specific object motion, texture, and camera viewpoint resulting in horizontal vibration) [12]. Thus, we propose using a second co-located low-speed 2D global-shutter camera that serves two purposes: tracking the appearance of the undistorted speckle pattern at low frequencies and providing a reference for recovering the high-frequency 2D shifts of the rolling-shutter sensor's rows. We present an algorithm to recover the unknown high-speed shifts. This algorithm provides both the speckle pattern's macro-motion or drift, along with the high-frequency vibrations.\nOur dual-shutter system can recover vibrations with a range of amplitudes and frequencies (up to 63 kHz) using two 'slow' cameras (60 and 134 Hz). 2 We evaluate the system by (a) estimating the different known vibration modes of a tuning fork and (b) recovering high-quality audio by observing the membrane of a speaker. Since the system can simultaneously capture multiple vibrating surfaces, we demonstrate the separation of audio signals from multiple sources (e.g. musical instruments). The system can also capture subtle, indirect vibrations of a surface reacting to a nearby sound source in better quality than passive approaches. To measure speckle reliably with low-power lasers, we attach retroreflective markers on the vibrating surfaces. Our approach works without markers when the vibrating surfaces are near and not dark. We believe our system makes visual vibrometry of complex high-frequency vibrations more efficient and practical for many applications. 2 Zhong et al. [46] use a similar rolling-global shutter pair to generate ground truth frames for rectifying and deblurring rolling-shutter video. (b) Schematic of our dual-shutter system. We add a cylindrical lens to spread the speckle into an image-plane column, which is then relayed onto two cameras having rolling and global shutters.", "publication_ref": ["b12", "b12", "b38", "b5", "b8", "b11", "b15", "b9", "b4", "b39", "b40", "b41", "b2", "b41", "b0", "b4", "b39", "b41", "b4", "b19", "b39", "b40", "b1", "b3", "b10", "b22", "b25", "b28", "b37", "b11", "b1", "b1", "b45"], "figure_ref": [], "table_ref": []}, {"heading": "Dual-shutter speckle image formation", "text": "A beam of coherent light from a laser creates a small spot on the surface of a diffusive object. The illuminated spot is imaged by a camera whose focus plane is located some distance away from the object surface (see Fig. 2(a)). At each focus-plane point, the electric field is the sum of contributions from all the illuminated object surface points. The surface's microscopic \"roughness\" adds a nearly random phase to each contributing surface point, yielding constructive or destructive interference. This creates a random spatial interference pattern called speckle, whose squared amplitude at the focus plane is imaged by the camera.\nIt has been shown that small motions of the vibrating surface cause the speckle pattern to shift in the focus plane [17,42]. In specific scenarios, these speckle shifts are related to the tilts of the vibrating surface [42]. Hence, this approach is called speckle-based vibrometry.\nWe make two changes to the standard speckle imaging described above as is illustrated in Fig. 2(b). First, the speckle focus-plane image is split using relay optics and imaged by two separate sensors having a rolling and a global shutter, respectively. Second, a cylindrical lens is placed in front of the primary objective lens. This lens spreads the speckle pattern along the vertical direction, yielding a 'speckle column' that reaches all the rolling-shutter sensor rows, while occupying only a fraction of the sensor's image columns (e.g. 150 pixels). Therefore, unlike defocusing a 'conventional' spherical lens or using a bare sensor, our optical design can sample multiple surface points at once. Each point yields a separate speckle column that is sampled using all rolling-shutter rows (see Fig. 1).\nSuppose that the rolling-and global-shutter sensors have identical resolutions and exposures in the two sensor planes (blue rectangles in Fig. 2(b)). Then, since both sensors share the optical path, an identical image is formed on both sensors. Let I(x, t) be the image intensity in both sensors, where x \u2261 (x, y) is the pixel coordinates and t is the image trigger time. For brevity, our equations below use both vector x, and the explicit row coordinates y of x. Note that I(x, t) is a continuous function of time, yielding the image, in [grayscale] units, that would form at trigger time t. Let I GS (x, t), and I RS (x, t) denote the global-and rolling-shutter video frames captured at time t, respectively. In the global-shutter camera, all sensor pixels collect scene light simultaneously during the exposure duration, hence:\nI GS (x, t) = I(x, t).\n(1)\nHereafter, we refer to the global-shutter frames as the reference frames. In a rolling-shutter sensor, the individual image rows are exposed one after another with a constant delay D. Thus, the rolling-shutter frame at time t is:\nI RS (x, t) = I(x, t + yD).(2)\nEqs. ( 1) and ( 2) describe the spatio-temporal relationship between the rolling-shutter and global-shutter videos:\nI RS (x, t) = I GS (x, t + yD), y \u2208 {0, 1, .., H \u2212 1}, (3\n)\nwhere H is the number of rows in the rolling-shutter frame. Now suppose that both cameras simultaneously start video capture at their individual frame rates. Let t gs k denote the time stamps of K global-shutter reference frames, where k=0, 1, .., K \u22121 is the frame index. Similarly, let t rs n denote the time stamps of N rolling-shutter frames, where n=0, 1, .., N \u22121 is the frame index (see Fig. 3).\nAs discussed above, for small tilts and shifts of the illuminated surface, the imaged speckle pattern remains approximately constant, up to a 2D image-domain shift\nu(t) \u2261 (u dx (t), u dy (t)) ,(4)\nwhere u dx (t) and u dy (t) are the x-and y-axis speckle pattern shifts in pixels, respectively. Without loss of generality, we set u(t gs 0 ) = (0, 0). Thus, any two reference frames with indices k 1 and k 2 are related by image translation: 3\nI GS (x, t gs k1 ) = I GS x + u(t gs k1 ) \u2212 u(t gs k2 ), t gs k2 . (5)\nObserve that the absolute shift u(t gs k ) of any individual reference frame can be recovered by integrating all the relative image translations u(t gs k ) \u2212 u(t gs k\u22121 ): Combining Eqs. ( 3)-( 5) we get:\nu(t gs k ) = k i=1 u(t gs i ) \u2212 u(t gs i\u22121 ) , \u2200k > 0 (6)\nI RS (x, t rs n ) = I GS (x, t rs n + yD) = = I GS (x + u(t rs n + yD) \u2212 u(t gs k ), t gs k ) .(7)\nLet\n\u03b4u nk (y) \u2261 u(t rs n + yD) \u2212 u(t gs k )(8)\ndenote the relative shift of every rolling-shutter row y in I RS (x, t rs n ) with respect to the same row in I GS (x, t gs k ). In Eq. ( 8) the term u(t gs k ) is constant since all global-shutter frame rows are shifted together at time t gs k . Rearranging Eq. (8) yields a formula for the speckle image shifts starting at time t rs n and ending at time t rs n + HD:\nu(t rs n + yD) = \u03b4u nk (y) + u(t gs k ).(9)\nEq. ( 9) yields an important observation: given any pair of rolling-and global-shutter frames, we can compute H samples of the global speckle shifts with a fine temporal resolution of D. All we require to recover the samples using Eq. ( 9) are two pieces of information: the shift u(t gs k ), and \u03b4u nk (y). The shift u(t gs k ) can be computed using Eq. (6). In the next section, we discuss how to compute \u03b4u nk (y).", "publication_ref": ["b16", "b41", "b41"], "figure_ref": ["fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "Recovering the 2D speckle translation", "text": "In this section, we discuss how to recover speckle shifts u(t) using Eq. (9). Eq. (9) suggests that a single rollingshutter frame, having index n, can yield H temporal measurements of u(t) at a sampling rate of 1/D. Therefore, given no delay between two consecutive rolling-shutter frames, capturing N consecutive frames yields a recording of NHD seconds duration.\nRecovering u(t rs n + yD) requires selecting an appropriate reference frame k. Note that in principle, any reference frame k should suffice. However, object macro motion may yield little to no spatial overlap between the speckle patterns of I RS (x, t rs n ) and I GS (x, t gs k ), causing the estimation of \u03b4u nk (y) to fail. Therefore, one must select a reference frame whose timestamp t gs k is close to t rs n .\nFirst, I RS and I GS are cropped to the speckle column belonging to the object point we wish to recover (see Fig. 4(a) and (b)). Let\u012a RS (x, t rs n ) and\u012a GS (x, t gs k ) denote the resulting cropped videos. For brevity, let\u00fb nk (y) denote the recovered shifts resulting from using reference frame k:\nu nk (y) \u2261 \u03b4\u00fb nk (y) +\u00fb(t gs k ).(10)\nWe use phase correlation to compute the shifts between every pair of consecutive reference frames\u012a GS (x, t gs k ), and use Eq. (6) to yield\u00fb(t gs k ) \u2200k [14]. Borrowing notation from [28], let V ={v m } M\u22121 m=0 denote a discrete set of M possible 2D row shifts, having some sub-pixel resolution and maximum span. 4 Define the set of all row shifts for frame n as U = {\u03b4u nk (y)} \u2200y , where \u03b4u nk (y) \u2208 V. Then we recover U by minimizing the loss:\nE(U) = y [1\u2212Sy (\u03b4u nk (y))]+\u03bb y,y \u2032 V y,y \u2032 (\u03b4u nk (y), \u03b4u nk (y \u2032 )),(11)\nwhere, the data term S y (\u03b4u nk (y)) \u2264 1 quantifies the similarity of row y in\u012a RS to all M possible shifts of row y in\u012a GS . The term V y,y \u2032 (\u03b4u nk (y), \u03b4u nk (y \u2032 )) enforces smoothness by providing a penalty when neighboring rows y, y \u2032 have differing shifts [7]. We set V y,y\n\u2032 = \u2225\u03b4u nk (y) \u2212 \u03b4u nk (y \u2032 )\u2225 2 2 .\nWe compute S y (v m ) using the zero-normalized cross-correlation operator ZNCC(., .) [8]:\nS y (v m ) = ZNCC \u012a RS (x, t rs n ),\u012a GS (x + v m , t gs k ) . (12\n)\nFinally\u00db is recovered usin\u011d\nU = argmin U (E(U)) .(13)\nBut solving Eqs. ( 11)-( 13) directly for large M is computationally expensive, as it requires computing correlations with a large 'dictionary' of possible shifts. Thus, we additionally implement and use an efficient coarse-to-fine approach for solving Eq. (13) which computes correlations in the Fourier domain (see supplementary for details).", "publication_ref": ["b13", "b27", "b3", "b6", "b7"], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Merging multiple reference frames", "text": "In the above description, shift recovery for frame n relied on a single reference frame. However, as illustrated in Fig. 5, due to large amplitude vibrations or large object motions, a single reference may not be enough to recover the relative translations for all H rows, yielding partial recovery of u(t rs n + yD). Therefore, we use P \u2265 1 reference frames to estimate\u00fb(t rs n + yD), as described below. Let R n = {k 0 , k 1 , .., k P \u22121 } denote the set of indices of reference frames chosen to recover frame n. For scenes having large low-frequency motions (e.g. hand-held instruments), R n consists of the P temporally closest frames to 4 For example, choosing a 0.   t rs n . For mostly static scenes, we construct R n using frames close to t rs n , that 'cover' the largest 2D speckle domain; see supplementary for details. First, shifts\u00fb nk (y) are computed for every reference frame k \u2208 R n . Then, the shifts from all reference frames are merged using a weighted average:\nu(t rs n + yD) = k\u2208Rn W nk (y)\u00fb nk (y). (14\n)\nEach reference frame's per-row weights W nk (y) are computed using the similarity measures of the recovered shifts:\nS nk (y) \u2261 S k y (\u00fb nk (y)),(15)\nwhere we added the superscript k to S y to denote the similarity function computed for reference frame k. We set:\nW nk (y) = exp \u03b3\u015c nk (y) / k\u2208Rn exp \u03b3\u015c nk (y) , (16\n)\nwhere we set \u03b3 = 50. Eqs. ( 14) -( 16) ensure that each row takes its recovered shift from the reference frames that exhibited good similarity. When most reference frames contribute good recoveries, Eq. ( 14) has the additional benefit of reducing the noise of the recovered signal by averaging.", "publication_ref": ["b3"], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "Prototype and implementation details", "text": "Fig. 6 shows our prototype. The system consists of a rolling-and a global-shutter camera which image the same scene through a set of relay, objective and cylindrical lenses. The scene is illuminated by a 532nm 4.5mW laser in a coaxial configuration using a beam-splitter seen in the left of Fig. 6. The utilized laser has relatively low power -that of standard laser pointers widely used in classrooms. Therefore, unless stated otherwise, we boost the signal in all experiments by attaching a small patch of retro-reflective tape on the surfaces we seek to measure. To capture multiple points, we spread the laser into dots by placing a diffraction grating in the cage between the laser and beam-splitter. Please see the supplementary material for a full parts list.\nIn Section 2, we assumed that both cameras have identical resolutions and are optically aligned. In practice, the captured images differ due to both geometric (e.g. homography, horizontal flip) and radiometric distortions. In fact, both cameras need not have the same sensor resolution. In our prototype, the rolling-and global-shutter images have resolutions of 1280x944 and 2056x1542, operating at 64.7 FPS and 134 FPS, respectively. Therefore, we need to accurately calibrate the mapping between both sensors.\nCalibration includes capturing a static speckle scene, detecting and matching feature points in both frames, and computing the parameters of the desired mapping model. We used a 3rd-degree smooth bi-variate spline interpolation to compute the mapping. The mapping was computed locally per each cropped laser-point speckle column. Please see the supplementary material for more details.\nWe set an 18us exposure time in both cameras (unless stated differently). We reduce the rolling-shutter camera's region-of-interest (ROI) by 40 pixels on the top and bottom of the frame so that the horizontal field-of-view of the reference camera is slightly larger than that of the rolling-shutter one. This prevents the first and last rows from shifting outside the field of view captured by the reference camera. Finally, we set hyperparameters P = 15 reference frames and \u03bb = (1000,100) for the coarse and fine levels in Eq. ( 11), which yields a run time of 6 sec per frame.", "publication_ref": [], "figure_ref": ["fig_7", "fig_7"], "table_ref": []}, {"heading": "Experimental evaluation", "text": "We demonstrate our method by capturing and replaying vibration caused by audio sources (e.g. speakers, human voice), analyzing the vibrational modes of a tuning fork, and capturing the vibrations of musical instruments. For reference, we use a high-fidelity microphone to simultaneously record the resulting sound in most experiments. Please examine supplementary material to hear the recordings [1].", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Capturing audio signals", "text": "In Fig. 7 we point the camera at the membranes of two speakers. First, (Fig. 7 top row), we record one speaker playing a series of tones using a single un-split laser point. Examining the microphone and our system's recording, one might notice that the microphone could not 'pick up' the low frequencies (65Hz and 33Hz). This is because a typical microphones' frequency response is less sensitive at the lower frequencies [19]. Fig. 7(d) shows the Lissajous curves for three of the eight played tones. Observe that the speaker membrane vibrates differently between the three tones, suggesting that the three frequencies create different membrane vibration modes. In the bottom row of Fig. 7, we split the laser using a diffraction grating and measure a point on both speaker membranes simultaneously. Here, the left and right speakers are playing reversed chirp signals (up-chirp and down-chirp). While the microphone records a mixed signal (Fig. 7(g)), our system measures each speaker separately, yielding unmixed recordings (Fig. 7(h-i)).\nIn Fig. 8(a) we recreate an experiment similar to Davis et al. [13]. We point the system at a chips bag (with a retroreflective patch) and play the audio file used by Davis et al. Despite the different setups, which makes a direct and fair comparison difficult, it is evident both auditorily and from the spectrograms that our system recovered the original audio with higher fidelity. The intelligibility [33] and PESQ [24]  spectively, where ours is the second score (higher is better). In Fig. 8(b), we compare to a 1D speckle sensing system using the method of Wu et al. [40] for signal recovery. The 1D system consists of a high-end line sensor [34], mounted with the same laser and objective lens used in our prototype. 5 Using both systems, we simultaneously capture a speaker mounted on a rotating stage and playing a 523Hz tone. The played tone's Lissajous curve (green curve in Fig. 7(d)) suggests that the speckle motion is predominantly along a single direction. Thus, rotating the speaker allows us to test the robustness of signal recovery with respect to different speckle shifts directions. Fig. 8(b) shows that 1D speckle sensing performance strongly depends on the direction of the 2D speckle shifts. Fig. 8(b)'s left subplot shows that 1D sensing works well when the speckle motion is mostly aligned with the line sensor's direction (x-axis). However, when a nonnegligible y-axis motion component exists, the recovery breaks (Fig. 8(b) middle plot). Conversely, for the same speaker orientation, our method recovers motion in both axes correctly (Fig. 8(b) right plot).", "publication_ref": ["b18", "b12", "b32", "b23", "b39", "b33", "b4"], "figure_ref": ["fig_8", "fig_8", "fig_8", "fig_8", "fig_8", "fig_8", "fig_8"], "table_ref": []}, {"heading": "Remote recording of musical instruments", "text": "We use our system to record musical instruments remotely. In Fig. 9(top), we record a violin played by a musician. We recover the speckle pattern's macro-motion spanning thousands of pixels and the audio vibrations spanning only several pixels. Note that our approach could handle the natural motions of the instrument as it is being played. In Fig. 9(bottom), we record two musicians simultaneously playing different scales on acoustic guitars. The laser beam is split to illuminate both guitars. The microphone records both guitars yielding an unpleasant dissonant recording, while our system records each guitar separately.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Analyzing vibrational modes of a tuning fork", "text": "We analyzed the vibrational modes of a 426Hz tuning fork. As shown in Fig. 10, we strike the fork with a mallet and measure its 2D vibrations at three points placed along the fork's arm. The generated vibrational modes depend on the strike's strength, striking position along the fork, mallet tip material, and how the fork is held [32]. While a microphone can detect various acoustic frequencies produced by the various modes, it does not provide any information on the modes' type of motion. Conversely, by plotting the simultaneous vibration of multiple points, we can determine the kind of motion generating the mode. The measured frequencies are verified using a microphone (Fig. 10(d)).\nIn Fig. 10(a), the fork is struck near its head (near point 2) with a rubber-tipped mallet, mainly exciting its 'fundamental' mode. The fork arm's motion can be observed by the vibration along the x-axis, whose amplitude increases from point 0 to point 2. Striking the fork with a metal bar (Fig. 10(b)), additionally induces the 'clang' mode, which is about 6.26\u00d7 higher than the fundamental [25]. The clang mode vibrations are visible in the x-axis as a high-frequency modulating the fundamental mode. The clang mode induces an opposite phase between points 0 and 2 since these surface points tilt in opposite directions, while point 1 is approximately stationary. Hitting the fork harder (Fig. 10(c)) input signal Davis et. al. [13] our ", "publication_ref": ["b31", "b24", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Sensitivity study", "text": "We examined our system's sensitivity to tilts and transversal motion by shining a laser spot at an optomechanical stage, capable of precision tilts and transversal shifts. We measured a linear relationship between small tilts and transversal motions to the shifts of the imaged speckle pattern. The measured sensitivity to tilts was 950 and 1475 pixels/degree for the x-and y-axis, respectively, while the sensitively to the transversal motion was 43 and 61 pixels/mm for the x-and y-axis, respectively. The 1.5\u00d7 factor difference in sensitivity between the axes stems from the higher optical magnification resulting from the cylindrical lens. See supplementary for plots.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations and societal impact", "text": "Light efficiency. The performance of our method depends on the amount of light reflected from each surface point. Our prototype used a low-power laser and we enhanced light efficiency using retro-reflective tape. The supplementary material shows additional experiments that test our sys- tem without the retro-reflective markers. A higher-power laser [5,42] can make the tape unnecessary, however, eye safety issues may arise for applications involving humans. Dense vs. sparse. Our method trades off spatial resolution for temporal resolution by sampling each time measurement using a single sensor row instead of the full sensor. While this makes our method more bandwidth efficient, a disadvantage is the inability to recover dense motion fields [13]. Number of simultaneous measurements: As seen in Fig. 1, each measured point occupies a horizontal fraction of the sensor plane. Thus, the number of simultaneous points is limited by the sensor's width. As analyzed in the supplementary material, narrowing the speckle column per point (e.g. by changing the camera focus) increases the number of possible points. But, as the columns get narrower, the number of pixels per-point per-row decreases, potentially reducing correlation accuracy. Moreover, the vertical speckle makes some sensing arrangements infeasible (e.g. two points on the same image column). Handling object macro-motions: We demonstrated two types of applications: recording audio (e.g. guitar, chips bag, speaker) and sensing vibrational modes (tuning fork). For the first type, our system is robust to macro motions as long as at least one laser point hits the vibrating surface. For example, for a typical guitar, one laser point allows a lateral movement range of about 13cm (half its narrowest dimension). The second type requires sensing particular points of interest, and here, object motion is unsupported by our prototype. Future methods could use a galvo-mirror to track and maintain the laser spot on the particular object point. Our method also assumes that the reference camera is fast enough to capture the speckle's macro-motion. But fast and large object motions may yield no speckle overlay between consecutive reference frames, degrading performance. Rolling-shutter dead-time: Rolling-shutter sensors typically exhibit a slight delay between the last row of a frame and the first row of the subsequent frame. This 'dead-time' has an insignificant effect when sensing low-frequency vibrations but may introduce noise at higher frequencies with wavelength comparable to the dead-time's duration (0.5ms in our camera). Using our method with a line-sensor instead of a rolling-shutter can prevent the dead-time [5,27]. Societal impact: Optically detecting vibrations can be useful in many scientific and engineering fields but could potentially create privacy concerns. For example, laser microphones were used to eavesdrop on distant conversations.", "publication_ref": ["b4", "b41", "b12", "b4", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We present a new bandwidth-efficient approach for high-speed visual vibration sensing using two low-speed cameras. Our system can handle non-static objects and sense multiple simultaneous points (e.g., multiple musical instruments). Future works may improve signal quality using learning-based signal recovery that imposes learned priors on speech and music signals. We envision possible future applications like remotely and individually recording all instruments of an orchestra or monitoring many factoryfloor machinery using just a single static camera.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgements: This work was supported in parts by NSF Grants IIS-1900821 and CCF-1730147. We thank A. Sankaranarayanan for advice on building the optical system, J. Smerd for playing the guitar and violin, and T. Zhang for help with the 1D speckle sensing experiments.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Dual-shutter vibration sensing: Project webpage", "journal": "", "year": "", "authors": ""}, {"ref_id": "b1", "title": "Epipolar time-of-flight imaging", "journal": "ACM Trans. Graph", "year": "2002", "authors": "Supreeth Achar; Joseph R Bartels; William L Whittaker; Kiriakos N Kutulakos; Srinivasa G Narasimhan"}, {"ref_id": "b2", "title": "Imaging with local speckle intensity correlations: theory and practice", "journal": "ACM Transactions on Graphics (TOG)", "year": "2021", "authors": "Marina Alterman; Chen Bar; Ioannis Gkioulekas; Anat Levin"}, {"ref_id": "b3", "title": "Video from stills: Lensless imaging with rolling shutter", "journal": "", "year": "2019", "authors": "Nick Antipa; Patrick Oare; Emrah Bostan; Ren Ng; Laura Waller"}, {"ref_id": "b4", "title": "Long-range detection of acoustic vibrations by speckle tracking", "journal": "Applied optics", "year": "2008", "authors": "S Bianchi;  Giacomozzi"}, {"ref_id": "b5", "title": "Estimating the material properties of fabric from video", "journal": "", "year": "1984", "authors": "Bei Katherine L Bouman; Peter Xiao; William T Battaglia;  Freeman"}, {"ref_id": "b6", "title": "Fast approximate energy minimization via graph cuts", "journal": "IEEE Transactions", "year": "2001", "authors": "Yuri Boykov; Olga Veksler; Ramin Zabih"}, {"ref_id": "b7", "title": "Template matching using fast normalized cross correlation", "journal": "", "year": "2001", "authors": "Kai Briechle; D Uwe;  Hanebeck"}, {"ref_id": "b8", "title": "Smaller than the eye can see: Vibration analysis with video cameras", "journal": "", "year": "2016", "authors": "Oral Buyukozturk; Justin G Chen; Neal Wadhwa; Abe Davis; Fr\u00e9do Durand; William T Freeman"}, {"ref_id": "b9", "title": "Video camerabased vibration measurement for civil infrastructure applications", "journal": "Journal of Infrastructure Systems", "year": "2017", "authors": "G Justin; Abe Chen; Neal Davis; Fr\u00e9do Wadhwa;  Durand; T William; Oral Freeman;  B\u00fcy\u00fck\u00f6zt\u00fcrk"}, {"ref_id": "b10", "title": "Rolling shutter camera relative pose: Generalized epipolar geometry", "journal": "", "year": "2016", "authors": "Yuchao Dai; Hongdong Li; Laurent Kneip"}, {"ref_id": "b11", "title": "Visual vibrometry: Estimating material properties from small motion in video", "journal": "", "year": "2015", "authors": "Abe Davis; Katherine L Bouman; Justin G Chen; Michael Rubinstein; Fredo Durand; William T Freeman"}, {"ref_id": "b12", "title": "The visual microphone: Passive recovery of sound from video", "journal": "", "year": "2008", "authors": "Abe Davis; Michael Rubinstein; Neal Wadhwa; J Gautham; Fredo Mysore; William T Durand;  Freeman"}, {"ref_id": "b13", "title": "Registration of translated and rotated images using finite fourier transforms. IEEE Transactions on pattern analysis and machine intelligence", "journal": "", "year": "1987", "authors": "De Castro; Cjit Morandi"}, {"ref_id": "b14", "title": "Video magnification in presence of large motions", "journal": "", "year": "2015", "authors": "Mohamed Elgharib; Mohamed Hefeeda; Fredo Durand; William T Freeman"}, {"ref_id": "b15", "title": "Visual vibration tomography: Estimating interior material properties from monocular video", "journal": "", "year": "2021", "authors": "T Berthy; Alexander C Feng; Chiara Ogren; Katherine L Daraio;  Bouman"}, {"ref_id": "b16", "title": "Spedo: 6 dof ego-motion sensor using speckle defocus imaging", "journal": "", "year": "2015", "authors": "Kensei Jo; Mohit Gupta; Shree K Nayar"}, {"ref_id": "b17", "title": "Motion magnification", "journal": "ACM transactions on graphics (TOG)", "year": "2005", "authors": "Ce Liu; Antonio Torralba; T William; Fr\u00e9do Freeman; Edward H Durand;  Adelson"}, {"ref_id": "b18", "title": "How to choose a microphone: Dynamics, condensers, ribbons and more", "journal": "", "year": "", "authors": ""}, {"ref_id": "b19", "title": "Lamphone: Real-time passive sound recovery from light bulb vibrations", "journal": "Cryptology ePrint Archive", "year": "", "authors": "Ben Nassi; Yaron Pirutin; Adi Shamir; Yuval Elovici; Boris Zadov"}, {"ref_id": "b20", "title": "Learning-based video motion magnification", "journal": "", "year": "2018", "authors": "Tae-Hyun Oh; Ronnachai Jaroensri; Changil Kim; Mohamed Elgharib; ' Fr;  Durand; T William; Wojciech Freeman;  Matusik"}, {"ref_id": "b21", "title": "Speckleeye: gestural interaction for embedded electronics in ubiquitous computing", "journal": "", "year": "2012", "authors": "Alex Olwal; Andrew Bardagjy; Jan Zizka; Ramesh Raskar"}, {"ref_id": "b22", "title": "Homogeneous codes for energyefficient illumination and imaging", "journal": "ACM Transactions on Graphics (ToG)", "year": "2015", "authors": "Supreeth Matthew O'toole;  Achar; G Srinivasa; Kiriakos N Narasimhan;  Kutulakos"}, {"ref_id": "b23", "title": "Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs", "journal": "IEEE", "year": "2001", "authors": "W Antony; John G Rix;  Beerends; P Michael; Andries P Hollier;  Hekstra"}, {"ref_id": "b24", "title": "Vibrational modes of a tuning fork", "journal": "", "year": "", "authors": "Daniel Russell; A "}, {"ref_id": "b25", "title": "Rolling shutter stereo", "journal": "", "year": "2013", "authors": "Olivier Saurer; Kevin Koser; Jean-Yves Bouguet; Marc Pollefeys"}, {"ref_id": "b26", "title": "Diffraction line imaging", "journal": "Springer", "year": "2020", "authors": "Mark Sheinin; N Dinesh;  Reddy; O' Matthew;  Toole; G Srinivasa;  Narasimhan"}, {"ref_id": "b27", "title": "Depth from texture integration", "journal": "IEEE", "year": "2019", "authors": "Mark Sheinin; Y Yoav;  Schechner"}, {"ref_id": "b28", "title": "Rolling shutter imaging on the electric grid", "journal": "", "year": "2018", "authors": "Mark Sheinin; Y Yoav; Kiriakos N Schechner;  Kutulakos"}, {"ref_id": "b29", "title": "Colux: Multi-object 3d micro-motion analysis using speckle imaging", "journal": "ACM Transactions on Graphics (TOG)", "year": "2017", "authors": "M Brandon; Pratham Smith; Vishal Desai; Mohit Agarwal;  Gupta"}, {"ref_id": "b30", "title": "Tracking multiple objects outside the line of sight using speckle imaging", "journal": "", "year": "2018", "authors": "M Brandon;  Smith; O' Matthew; Mohit Toole;  Gupta"}, {"ref_id": "b31", "title": "The otologist's tuning fork examination-are you striking it correctly?", "journal": "Otolaryngology-Head and Neck Surgery", "year": "2015", "authors": "R Jayne; Travis J Stevens;  Pfannenstiel"}, {"ref_id": "b32", "title": "An algorithm for intelligibility prediction of time-frequency weighted noisy speech", "journal": "IEEE Transactions on Audio, Speech, and Language Processing", "year": "2011", "authors": "H Cees;  Taal; C Richard; Richard Hendriks; Jesper Heusdens;  Jensen"}, {"ref_id": "b33", "title": "Fact sheet xposure:camera", "journal": "", "year": "", "authors": "Petra Thanner"}, {"ref_id": "b34", "title": "Phase-based video motion processing", "journal": "ACM Transactions on Graphics (TOG)", "year": "2013", "authors": "Neal Wadhwa; Michael Rubinstein; Fr\u00e9do Durand; William T Freeman"}, {"ref_id": "b35", "title": "Riesz pyramids for fast phase-based video magnification", "journal": "IEEE", "year": "2014", "authors": "Neal Wadhwa; Michael Rubinstein; Fr\u00e9do Durand; William T Freeman"}, {"ref_id": "b36", "title": "Eulerian video magnification and analysis", "journal": "Communications of the ACM", "year": "2016", "authors": "Neal Wadhwa; Hao-Yu Wu; Abe Davis; Michael Rubinstein; Eugene Shih; J Gautham; Justin G Mysore; Oral Chen;  Buyukozturk; V John; William T Guttag;  Freeman"}, {"ref_id": "b37", "title": "000 frames-per-second compressive imaging with a conventional rolling-shutter camera by random point-spread-function engineering", "journal": "Optics Express", "year": "2020", "authors": "Gil Weinberg; Ori Katz"}, {"ref_id": "b38", "title": "Eulerian video magnification for revealing subtle changes in the world", "journal": "ACM transactions on graphics (TOG)", "year": "2012", "authors": "Yu Hao; Michael Wu; Eugene Rubinstein; John Shih; Fr\u00e9do Guttag; William Durand;  Freeman"}, {"ref_id": "b39", "title": "Fast motion estimation of one-dimensional laser speckle image and its application on real-time audio signal acquisition", "journal": "", "year": "2006", "authors": "Nan Wu; Shinichiro Haruyama"}, {"ref_id": "b40", "title": "The 20k samples-persecond real time detection of acoustic vibration based on displacement estimation of one-dimensional laser speckle images", "journal": "Sensors", "year": "2007", "authors": "Nan Wu; Shinichiro Haruyama"}, {"ref_id": "b41", "title": "Simultaneous remote extraction of multiple speech sources and heart beats from secondary speckles pattern", "journal": "Optics express", "year": "2008", "authors": "Zeev Zalevsky; Yevgeny Beiderman; Israel Margalit; Shimshon Gingold; Mina Teicher; Vicente Mico; Javier Garcia"}, {"ref_id": "b42", "title": "Vibrosight: Long-range vibrometry for smart environment sensing", "journal": "", "year": "2018", "authors": "Yang Zhang; Gierad Laput; Chris Harrison"}, {"ref_id": "b43", "title": "Vibrosight++: City-scale sensing using existing retroreflective signs and markers", "journal": "", "year": "2021", "authors": "Yang Zhang; Sven Mayer; Jesse T Gonzalez; Chris Harrison"}, {"ref_id": "b44", "title": "Video acceleration magnification", "journal": "", "year": "2017", "authors": "Yichao Zhang; L Silvia; Jan C Van Pintea;  Gemert"}, {"ref_id": "b45", "title": "Towards rolling shutter correction and deblurring in dynamic scenes", "journal": "", "year": "2021", "authors": "Zhihang Zhong; Yinqiang Zheng; Imari Sato"}, {"ref_id": "b46", "title": "Specklesense: fast, precise, low-cost and compact motion sensing using laser speckle", "journal": "", "year": "2011", "authors": "Jan Zizka; Alex Olwal; Ramesh Raskar"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure1. Optically measuring vibrations allows remote capture of speech, music, and the mechanical vibrations of various objects, including engines, bridges, and more. We propose a new method for sensing 2D object vibrations at high speeds, using a dual-shutter sensor system consisting of two low-speed cameras.Our system samples vibration with speeds up to 63KHz, for multiple objects at once and can handle non-static objects. We test our system by capturing and replaying audio source vibrations (e.g. speakers, human voice), analyzing the vibrational modes of a tuning fork, and capturing the vibrations of musical instruments.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 .2Figure 2. (a) Schematic of standard defocused speckle imaging.(b) Schematic of our dual-shutter system. We add a cylindrical lens to spread the speckle into an image-plane column, which is then relayed onto two cameras having rolling and global shutters.", "figure_data": ""}, {"figure_label": "63", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "6 Figure 3 .63Figure 3. Dual-shutter camera timing. Both cameras capture video streams simultaneously. The rolling-shutter camera samples the scene row by row with a high-frequency of 1/D, while the globalshutter camera samples the entire scene at once.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "1 pixel resolution with a \u00b140 span yields the set V = {(\u221240, \u221240), (\u221240, \u221239.9), (\u221239.9, \u221240), .., (40, 40)}.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "(a) input rolling-shutter distorted speckle column (b) global-shutter reference (representative) frame (c) recovered row shifts in both x and y axes", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 .4Figure 4. 2D speckle motion recovery. (a) Cropped rolling-shutter speckle column belonging to a single scene point, rotated by 90 degrees. (b) Single global-shutter reference frame, captured at a time instance closest to the frame in (a). (c) Recovered 2D shifts of each row in (a) using 15 reference frames (including frame (b)).", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 5 .5Figure5. Using multiple reference frames for recovery. (a) In this illustration, the speckle pattern's 2D image motion follows the white arrow. During this motion, our system captures three global-shutter frames (dashed rectangles) and a single rollingshutter frame (green rectangles are individual rows). None of the reference frames contains overlap with all the rows of the rollingshutter frame, and therefore no single frame can be used to recover the shifts for all rows. (b) Shows the x-axis shift recovered separately using each of the reference frames. Observe that rolling-shutter rows that contain little-to-no overlap with the reference frames yield noisy recoveries. (c) Our method merges wellrecovered signal portions from the multiple recoveries.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 6 .6Figure 6. Dual-shutter camera prototype.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 7 .7Figure 7. Speaker membrane experiments. Top row: A single speaker is playing eight octaves of the note C (C1 to C8). The speaker's vibrations are simultaneously recorded using our system (using a single laser point) and a microphone. (a) A spectrogram of the input signal sent to the speaker. (b) Microphone spectrogram. (c) A spectrogram of the x-axis speckle shifts recovered by our system. Observe the difference in frequency response between (b) and (c): namely, the microphone is tuned for high-frequency sensitivity, while our system has a mostly flat response. (d) Lissajous curves of three different notes show that the speaker membrane has different physical vibrations at different frequencies. Bottom row: (e-f) The left and right speakers are playing down-chirp and up-chirp signals, respectively. (g) The microphone records a mixture of both speaker signals and is unable to tell them apart. (h-i) Our system records each speaker separately (x-axis shifts shown here) and thus is able replay each channel individually.", "figure_data": ""}, {"figure_label": "a89", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "( a )Figure 8 .Figure 9 .a89Figure 8. Comparisons with related methods. (a) We attempt to reproduce the chips bag experiment of Davis et al.[13]. We vibrate a chips bag using a nearby speaker and recover the speaker's input audio. A small retro-reflective marker is attached to the chips bag for better laser reflectivity. We play the \"Marry had a little lamb...\" audio file used by Davis et al., and compare the resulting spectrograms between our method and the recovered audio obtained by Davis et al. Observe that our method recovers the audio with significantly improved fidelity (see supplementary material for audio comparison). Note that Davis et al. used a high-speed camera with a zoom lens and illuminated the chips bag with a strong light source. In comparison, we use a low-power laser, a marker and our low-speed dual-shutter system. (b) Comparison to 1D speckle sensing for a speaker tone[41]. Left: 1D sensing works well for x-axis only motion. Middle: But, for shifts having a non-negligible y-axis component, the reconstruction breaks. Right: We recover shifts in 2D, yielding correct waveforms. experiment setup microphone recovered y-axis shifts", "figure_data": ""}, {"figure_label": "a10", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "( a )Figure 10 .a10Figure 10. Analyzing the vibrational modes of a tuning fork. A 426Hz fork's vibrations are recorded using three laser points as shown in the top-left image. Each sub-figure shows a representative sample of x-and y-axis motions along with spectrograms of each axis. (a) Striking the fork with a rubber-tipped mallet mainly excites the fundamental mode, seen in the x-axis as in-phase vibrations of all three points. (b) Striking the fork with a metal rod additionally excites the clang mode. (c) An even stronger strike excites an additional higher in-plane vibration mode. (d) Spectrogram of the microphone, which measures a sum of all mode frequencies.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "I GS (x, t) = I(x, t).", "formula_coordinates": [3.0, 128.09, 238.79, 80.3, 9.65]}, {"formula_id": "formula_1", "formula_text": "I RS (x, t) = I(x, t + yD).(2)", "formula_coordinates": [3.0, 115.31, 314.99, 171.05, 9.65]}, {"formula_id": "formula_2", "formula_text": "I RS (x, t) = I GS (x, t + yD), y \u2208 {0, 1, .., H \u2212 1}, (3", "formula_coordinates": [3.0, 58.45, 367.28, 224.04, 9.65]}, {"formula_id": "formula_3", "formula_text": ")", "formula_coordinates": [3.0, 282.49, 367.6, 3.87, 8.64]}, {"formula_id": "formula_4", "formula_text": "u(t) \u2261 (u dx (t), u dy (t)) ,(4)", "formula_coordinates": [3.0, 118.34, 515.18, 168.02, 9.68]}, {"formula_id": "formula_5", "formula_text": "I GS (x, t gs k1 ) = I GS x + u(t gs k1 ) \u2212 u(t gs k2 ), t gs k2 . (5)", "formula_coordinates": [3.0, 73.47, 588.66, 212.9, 13.91]}, {"formula_id": "formula_6", "formula_text": "u(t gs k ) = k i=1 u(t gs i ) \u2212 u(t gs i\u22121 ) , \u2200k > 0 (6)", "formula_coordinates": [3.0, 83.03, 656.41, 203.34, 30.32]}, {"formula_id": "formula_7", "formula_text": "I RS (x, t rs n ) = I GS (x, t rs n + yD) = = I GS (x + u(t rs n + yD) \u2212 u(t gs k ), t gs k ) .(7)", "formula_coordinates": [3.0, 337.02, 253.71, 208.09, 28.18]}, {"formula_id": "formula_8", "formula_text": "\u03b4u nk (y) \u2261 u(t rs n + yD) \u2212 u(t gs k )(8)", "formula_coordinates": [3.0, 359.38, 304.54, 185.73, 13.91]}, {"formula_id": "formula_9", "formula_text": "u(t rs n + yD) = \u03b4u nk (y) + u(t gs k ).(9)", "formula_coordinates": [3.0, 358.0, 406.41, 187.11, 13.91]}, {"formula_id": "formula_10", "formula_text": "u nk (y) \u2261 \u03b4\u00fb nk (y) +\u00fb(t gs k ).(10)", "formula_coordinates": [4.0, 109.87, 141.08, 176.49, 13.91]}, {"formula_id": "formula_11", "formula_text": "E(U) = y [1\u2212Sy (\u03b4u nk (y))]+\u03bb y,y \u2032 V y,y \u2032 (\u03b4u nk (y), \u03b4u nk (y \u2032 )),(11)", "formula_coordinates": [4.0, 50.11, 264.38, 240.76, 30.5]}, {"formula_id": "formula_12", "formula_text": "\u2032 = \u2225\u03b4u nk (y) \u2212 \u03b4u nk (y \u2032 )\u2225 2 2 .", "formula_coordinates": [4.0, 50.11, 345.92, 236.25, 22.58]}, {"formula_id": "formula_13", "formula_text": "S y (v m ) = ZNCC \u012a RS (x, t rs n ),\u012a GS (x + v m , t gs k ) . (12", "formula_coordinates": [4.0, 56.2, 387.93, 226.01, 13.91]}, {"formula_id": "formula_14", "formula_text": ")", "formula_coordinates": [4.0, 282.21, 390.99, 4.15, 8.64]}, {"formula_id": "formula_15", "formula_text": "U = argmin U (E(U)) .(13)", "formula_coordinates": [4.0, 124.38, 434.33, 161.98, 16.52]}, {"formula_id": "formula_16", "formula_text": "u(t rs n + yD) = k\u2208Rn W nk (y)\u00fb nk (y). (14", "formula_coordinates": [4.0, 351.47, 425.18, 189.5, 22.21]}, {"formula_id": "formula_17", "formula_text": ")", "formula_coordinates": [4.0, 540.96, 427.57, 4.15, 8.64]}, {"formula_id": "formula_18", "formula_text": "S nk (y) \u2261 S k y (\u00fb nk (y)),(15)", "formula_coordinates": [4.0, 380.24, 493.15, 164.87, 12.69]}, {"formula_id": "formula_19", "formula_text": "W nk (y) = exp \u03b3\u015c nk (y) / k\u2208Rn exp \u03b3\u015c nk (y) , (16", "formula_coordinates": [4.0, 314.57, 555.36, 226.39, 20.14]}, {"formula_id": "formula_20", "formula_text": ")", "formula_coordinates": [4.0, 540.96, 555.68, 4.15, 8.64]}], "doi": ""}