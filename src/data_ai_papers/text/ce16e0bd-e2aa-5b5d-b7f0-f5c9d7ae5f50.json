{"title": "Exploring How Generative Adversarial Networks Learn Phonological Representations", "authors": "Jingyi Chen; Micha Elsner", "pub_date": "", "abstract": "This paper explores how Generative Adversarial Networks (GANs) learn representations of phonological phenomena. We analyze how GANs encode contrastive and non-contrastive nasality in French and English vowels by applying the ciwGAN architecture (Begu\u0161, 2021a). Begu\u0161 claims that ciwGAN encodes linguistically meaningful representations with categorical variables in its latent space and manipulating the latent variables shows an almost one to one corresponding control of the phonological features in ciwGAN's generated outputs. However, our results show an interactive effect of latent variables on the features in the generated outputs, which suggests the learned representations in neural networks are different from the phonological representations proposed by linguists. On the other hand, ci-wGAN is able to distinguish contrastive and noncontrastive features in English and French by encoding them differently. Comparing the performance of GANs learning from different languages results in a better understanding of what language specific features contribute to developing language specific phonological representations. We also discuss the role of training data frequencies in phonological feature learning.", "sections": [{"heading": "Introduction", "text": "Recent studies in natural language processing (NLP) have demonstrated two generic trends: neural networks dominate language-specific machine learning models; the common practice of model training (pre-training and fine-tuning) outperforms many traditional training methods and is particularly suitable for the development of language models used for various downstream tasks. These language models, however, are of black-box nature. The interpretability of these models is limited that the language representation they learned might not align to human language. How, then, to understand the opaque and complex learned representation of language models is an important question in recent studies. Phonology, the study of the sound system of human languages, plays an important role in understanding models' inherent biases and their ability to make human-like generalizations.\nThe sound systems of human languages are not organized arbitrarily, but contain structural generalizations and interdependence. Thus, learning a sound system involves not only learning to acoustically realize or recognize segments (phonetics), but also mapping them to an inventory characterized by distinctive features, and learning distributional constraints on segment sequences (phonology). Just as computational psycholinguists have investigated the degree to which neural network language models learn linguistically motivated features like syntax (Linzen et al., 2016;Lau et al., 2017;Gulordava et al., 2018;Marvin and Linzen, 2018;Futrell et al., 2019), they have also investigated the degree to which phonological organization emerges from neural models trained on acoustics (Gelderloos and Chrupa\u0142a, 2016;Chrupa\u0142a et al., 2017).\nThe degree to which these models learn phonological features is still debatable. Recently, a neural network autoencoder seems to successfully learn phoneme-like representations without explicit labels (R\u00e4s\u00e4nen et al., 2016;Shain and Elsner, 2019). While autoencoders seem to acquire some phonological generalizations, their representations of the kind of phonological features used by linguists are both incomplete and distributed across the latent space, requiring probing classifiers to detect. Because of this limited success and lack of transparency, it is difficult to tell whether higher-order phonotactic dependencies between different segments are acquired. Generative Adversarial Networks (GANs) (Goodfellow et al., 2014(Goodfellow et al., , 2020Begu\u0161, 2020b), on the other hand, are claimed to model language acquisition naturally because GANs can model phonetic and phonological computation as an almost one to one mapping from random space to generated data of a GAN instance trained on raw speech data (Begus and Zhou, 2022). The learned internal representations of GANs is claimed to resemble phonological learning in human speech acquisition: GANs learn to build their internal latent space via unsupervised phonetic learning from raw acoustic data, which is similar to human constructs underlying phonological representation by listening to the speech sounds in a language. Begu\u0161 (2021a) proposed ciwGAN (Categorical InfoGAN) which is based on WaveGAN architecture but with an extra Q-network that motivates the Generator to produce linguistically categorical and meaningful sounds. Begus and Zhou (2022) shows that ciwGAN can encode allophonic distribution: word-initial pre-vocalic aspiration of voiceless stops ([p h It] v.s. [spIt]). In English, the aspiration of stop consonant T occurs initially before a vowel (#T h V, h refers to the aspiration) while a period of stop closure occurs between the aspiration and the period frication noise of [s] (#sTV). CiwGAN successfully learned and generated this allophonic distribution in that the generated outputs obey this phonological constraint. Moreover, changing a single variable in the latent space is capable of changing generated tokens from sTV to T h V, suggesting an almost one-to-one correspondence between latent variables and phonological features. This finding is claimed to prove that GANs can model unsupervised phonological representation learning from raw speech data.\nIn this study, we explore the robustness of ciw-GAN as a phonological feature learner by testing ciwGAN on learning the feature of nasality, which is distinct in French and English. Nasality is a contrastive feature for French vowels; nasal vowels can appear independently of nasal consonants (Cohn, 1993). In English, however, vowel nasality is allophonic, like voiceless stop aspiration -nasal vowels appear only preceding nasal consonants. Linguists traditionally analyze this relationship as reflecting a single nasal feature on the consonant, without an independent feature controlling vowel nasality (Kager, 1999;McMahon, 2002;Hayes, 2011;Ogden, 2017;Zsiga, 2012). Thus, our experiment provides a more rigorously controlled test of the claims of Begus and Zhou (2022). Ciw-GAN networks are trained on English and French datasets respectively to learn the distinct nasal features of the two languages. Analysis of the result ciwGAN networks is development to answer the following research questions: (1) What features of the data contribute to learning the nasal representations in English vs. French? (2) How does the training data's distribution affect the learned feature system in waveGAN network?\nResults show interactive effects between latent variables in controlling the phonetic and phonological features: multiple to one corresponding mapping is found between latent variables and the phonetic and phonological features, suggesting that the claimed advantage of GANs over autoencoders is not as great as was originally claimed. ciwGAN do react differently in encoding the different nasal representations in English and French to indicate whether a feature is or is not contrastive, highlighting their potential as phonological learners. Moreover, we found that training data's distribution affects the learned feature system in ciwGAN; to the extent that GANs can be considered cognitively plausible models of human learning, this may lead to predictions about how changes in phonetic distribution can become phonologized into almost-categorical rules.", "publication_ref": ["b29", "b23", "b30", "b18", "b20", "b9", "b37", "b38", "b22", "b21", "b3", "b6", "b3", "b11", "b26", "b32", "b24", "b33", "b43", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Related Works", "text": "We review two areas of recent literature. Largescale unsupervised models of speech learn words and in some cases phoneme categories, but the degree to which they acquire phonological feature systems is not clear. Some smaller-scale models have been specifically analyzed in phonological terms. One recent and successful pre-trained model (wav2vec 2.0) is shown to encode audio sequences with its intermediate representation vectors, which demonstrates superiority in downstream fine-tuning such as automatic speech recognition (ASR) tasks, speaker verification tasks, and keyword spotting tasks (Baevski et al., 2020b).\nSimilar to wav2vec, Hu-BERT (Hsu et al., 2021), a pretrain language model that leverages selfsupervised learning for speech, directly processes audio waveform information from raw speech to predict clustering categories for the speech segments. Both wav2vec 2.0 and Hu-BERT have been successful in capturing acoustic information from raw speech and improve the state-of-the-art performance in speech recognition and translation. van den Oord et al. (2016) introduces a dilated causal convolutional network WaveNet which attempts to discover phone units from audios; how-ever, because of the lack of lexical knowledge, WaveNet cannot emit explicit phonemes (van den Oord et al., 2016). Moreover, the submissions for the ZeroSpeech Challenges (Dunbar et al., 2017(Dunbar et al., , 2019(Dunbar et al., , 2020(Dunbar et al., , 2021 utilizes generative models like GANs (Begu\u0161, 2021a;Yamamoto et al., 2020) and autoencoders (Chung et al., 2016;Baevski et al., 2020a) to learn the lexical or phone-level presentation from raw speech data. However, the learning of phonology features of language from raw speech data is not particularly implemented or evaluated in the above studies. Although these models have shown impressive results in speech representation learning that capture phonetic/acoustic content, the degree to which they acquire phonological feature systems is still not clear. Some studies have been focused on developing language models that learn phonological representations. In Shain and Elsner (2019), an autoencoder neural network is trained on pre-segmented acoustic data and output values that correlates to phonological features. Nevertheless, the architecture of autoencoder brings a problem in learning phonological representation: because autoencoders are trained to reproduce their inputs faithfully, their latent representations may contain too much information which is extraneous to phonological categorization, such as speaker-specific information. GANs are not trained to strictly reproduce the training data and therefore might not be subject to this issue.\nRecently, Donahue et al. ( 2019)'s study applies the GAN architecture based on the DCGAN architecture (Radford et al., 2015) to learn language features from continuous speech signals (WaveGAN). GAN networks as generative model, is firstly applied in learning allophonic distribution from raw acoustic data in Begu\u0161 (2020a,b) which also proposes a probing technique to interpret the internal representation of GAN networks. The internal language representation is probed and claimed to be interpretable in Begu\u0161 (2021b); Begus and Zhou (2022) which firstly shows that GAN networks can learn reduplication and conditional allophonic distribution of voice onset time (VOT) duration from the raw speech audio, respectively. Begu\u0161 (2021a) proposes ciwGAN (Categorical InfoWaveGAN) and fiwGAN, two GAN networks for unsupervised lexical learning from raw acoustic inputs; the two GAN networks combine WaveGAN with InfoGAN, an extension to GAN architecture, that includes an additional \"Q-network\" which encourages the model's productions to group into discrete categories (Chen et al., 2016). In these earlier papers, the discrete representational elements in these GAN architectures were proposed and interpreted with respect to lexical category learning. In our work, this interpretation does not apply, since our data consists of syllables rather than whole words. While top-down lexical information appears critical to learning many phonological contrasts, the rules governing the distribution of vowel nasality we are studying here are local phonotactic phenomena which can be learned purely by capturing the distribution of vowels and coda consonants.", "publication_ref": ["b1", "b34", "b34", "b15", "b13", "b16", "b14", "b6", "b41", "b10", "b0", "b38", "b36", "b3", "b6", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Model", "text": "In this paper, we use ciwGAN to model phonetic and phonological learning for vowel nasalization in English and French. The GAN architecture involves two deep convolutional neural networks: the Generator network and the Discriminator network (Goodfellow et al., 2014(Goodfellow et al., , 2020. They are trained against each other to boost their performance. The Generator network is trained to generate data from a set of latent variables and maximize the error rate of the Discriminator network. The Discriminator takes the training data and output of the Generator network as input and attempts to determine whether its input comes from the training dataset (actual data) or generator output (fake data). The competition of the two networks against each other makes the Generator generate data that is similar to the actual data. The architecture of ciwGAN is shown in Figure 1. The Generator takes categorical binary latent variables \u03d5 (size is 3 in Figure 1) and continuous latent variable z that are uniformly distributed in the interval (-1, 1) as input and outputs a continuous time-series data as audio signal (x). The Q-network, extra component in ciwGAN than WaveGAN, also takes audio signals as input, but gives a categorical estimation\u03c6 on the audio signal. It is trained to minimize the difference between the categorical estimation\u03c6 and the actual latent categorical variables \u03d5 in the Generator's latent space. With the Q-network, the Generator is motivated to generate audio signals that are categorically distinguishable for the Q-network.\nTo interpret the learned phonological features in the generated output, Begus and Zhou (2022) uses regression analysis. They manually label each gen-Figure 1: ciwGAN architecture: three convolutional neural networks are presented by green boxes and inputs to these neural networks are presented by purple boxes. This figure is from (Begus and Zhou, 2022). erated audio snippet with its phonological features, then measures the strength of correlation between the latent variables (z) and the phonological feature of interest. We also use this technique in our experiments to find the latent variables that correspond to the nasal feature in English and French. Begu\u0161 (2020) uses regression analysis from the latent variables to the phonetic and phonological features in the generated outputs to reveal the correspondence relations between latent variables and the phonetic and phonological features.\nHowever, to avoid expensive manual labeling, we develop a supervised nasal detector (nasalDNN), a deep neural network model adapted from Yurt et al. (2021), to determine whether a generated output carries nasality or not. The nasalDNN is a 1D CNN that takes speech segments as inputs, and calculates the posterior probabilities for the sample at the center point of the segment belongs to nasal phoneme classes [n, m, ng].\nFor French, we trained the convolutional nasalDNN on the SIWIS dataset, which has ground truth labels for both nasal consonants and nasal vowels. We used these labels to learn a four-way classifier, which we applied to the sample at the center point of each segment. In English, since TIMIT has no ground truth labeling of nasal vowels, we used a different procedure: we learned independent classifiers for vowels and nasal sounds (using consonants as the gold examples of nasals) and detected nasal vowels by intersecting the predictions.", "publication_ref": ["b22", "b21", "b3", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "Data", "text": "To learn vowel and nasality features in Engish and French, two ciwGAN instances are trained separately on TIMIT Speech Corpus (Garofolo et al., 1993) and the SIWIS French Speech Synthesis Database (Yamagishi et al., 2017). The TIMIT Speech Corpus includes English raw speech sentences (at 16 kHz sampling rate) and their corresponding time-aligned phonetic labels. In the TIMIT corpus, there are 6300 sentences recorded by 630 speakers from eight dialect regions of the United States. We used the entire TIMIT dataset to extract training data for the English experiment. The SIWIS French Speech Synthesis Database consists of high-quality French speech recordings and associated text files. There are 9750 utterances uttered by French speakers. This French database includes more than ten hours of speech data.", "publication_ref": ["b19", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "Data Preprocessing", "text": "For English dataset, we first excluded SA sentences in TIMIT, which are read by all the speakers, to avoid a possible bias and then extracted sliced sequences of the structure VT and VN from the rest of the sentences 1 . 6255 tokens are extracted from the monosyllabic words and 2474 are extracted from the multi-syllabic words' last syllable . Thus, altogether 8729 tokens from TIMIT were used for training, 5570 tokens of the structure VT, 3159 tokens of the structure VN.\nAs the SIWIS French Speech Synthesis Database does not provide time-aligned phonetic labels for their recordings, we use the Montreal Forced Aligner (McAuliffe et al., 2017), a forced alignment system with acoustic models using Kaldi speech recognition toolkit (Povey et al., 2011) to time-align a transcript corresponding to a audio file at the phone and word levels. Based on the time-aligned phonetic labels, we extracted sliced sequences of the structure VT, VN,VT,VN 2 . As French has contrastive nasal vowels and oral vowels, we usedV to indicate nasal vowels 3 and used V to show oral vowels 4 . We extracted 4686 tokens 1 T refers to voiced and voiceless stop consonants as well as the stop closures [t, d, p, b, k, g, tcl, dcl, pcl, bcl, kcl, gcl], N refers to three nasal consonants in English [n, m, ng], and V includes vowels and approximants [aa, ae, ah, ao, ax, ax-h, axr, ay, aw, eh, el, er, ey, ih, ix, iy, ow, oy, uh, uw, ux, r, l, w] 2 The T class is [t, d, p, b, k, g, tcl, dcl, pcl, bcl, kcl, gcl ", "publication_ref": ["b31", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "To explore our first research question: What features of the data contribute to learning the nasal representations in English vs. French, we implement English and French experiments. The results suggest different learned phonetic/phonological representations in ciwGAN may be caused by different typology of English and French syllable types for nasal vowels and nasal consonants.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "English Experiment", "text": "After the ciwGAN instance is trained for 649 epochs, it learns to generate 3840 speech-like sequences (VT and VN) that are similar to the training data. As described above, we label these outputs with a supervised classifier to determine which ones are nasal, then apply linear regression analysis to identify latent variables that correlate to nasal features. The results of linear regression are shown in Figure 7 in Appendix. Among the 100 latent variables in latent space, we identify 7 latent variables that have the highest chi-square scores, which indicates a strongly correlation to nasality. Figure 7 also illustrates a considerable difference between the highest seven latent variables and the rest of the variables indicating that ciwGAN may encodes nasal feature mainly with these seven latent variables and use other latent variables to increase variance.\nWe also apply another investigative technique from Begu\u0161 (2020), in which selected latent variables are set to values outside their training range. As in that study, we examine the audio generated from representations with manipulated variables, which contain exaggerated acoustic cues indicating sponding ipa symbols [a, i, OI, @, o, e, u, oe, \u00f8, E] which phonetic qualities the variables control. We sample 100 random latent vectors, and for each one, manipulate the target variable to values between -5 and 5 in increments of 1.\nAlthough seven latent variables are identified as closely corresponding to the presence of consonants' nasal feature via linear regression, only two latent variables z13 and z90 show a strong control of the nasality in consonants. Figure 6 , in Appendix, illustrates the manipulation effects of z13 and z90 on nasal consonant. The spectrograms show a relatively high F1 (around 650 Hz) initially which corresponds to the vowel and a lower amplitude (F1 at around 250 Hz) at the end of the sound which represents the nasal consonant [n]. The nasality in the consonant gradually decreases as the values of z13 and z90 increase separately. Seven latent variables are also found to be relative to nasal vowels via linear regression; however, manipulating these seven latent variables, vowels' nasality do not show a regular change pattern in the generated audios, which indicates that these seven latent variables do not have one to one corresponding control of the nasality in vowels.\nAs both latent variables z13 and z90 are able to control the nasality in consonants, we further explore the interactive effects of these two latent variables by manipulating them simultaneously to test all combinations of the two variables in range [-5,5] and increment of 1. However, no clear interactive correlation are found regarding to the nasality between the two latent variables. Although z13 and z90 show effects on the nasal feature in consonants when they are manipulated separately, z90 show a primary control on consonants' nasality. As illustrated in Figure 2a, when z90 >0, the Generator tends to produce nasal consonants while the value of z13 does not show a clear effect on generated sound features. We also found that vowels' nasality tends to covary with the presence of nasal codas. In Figure 2a, whenever a nasal vowel is detected in the generated outputs, they also have a nasal consonant detected in the outputs.\nWe also evaluate if the two latent variables (z4 and z37), with the highest chi-square value for nasal vowels, have effects on producing English nasal vowels. However, neither z4 nor z37 show control of English nasal vowels (the left panel of Figure 2bb); instead, as seen in the right panel, their primary effect is on consonant nasality. These results suggest that ciwGAN encodes En-\n(a) English -z90 & z13 (b) English -z4 & z37\nFigure 2: English experiment results: Figure 2a shows the generated audios with concurrent manipulation of latent variables z90 and z13 (x axis: z90 & y axis: z13); Figure 2b shows the generated audios with concurrent manipulation of latent variables z4 and z37 (x axis: z4 & y axis: z37); Green color heatmap (left side of Figure 2a and Figure 2b) indicates the detected English nasal vowel on generated audio; Red color heatmap (right side of Figure 2a and Figure 2b) indicates the detected English nasal consonant on the same generated audio; darkness of color refers to the proportion of detected nasal vowels and the detected nasal consonants in the manipulated audios; annotation are syllables types of the manipulated audios based on the results of nasal detectors.  glish nasal vowels as an non-contrastive phonetic feature which co-occurs with nasal consonants, a phonological feature.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "French Experiment", "text": "The networks learn to generate speech-like sequences (VT, VN,VT,VN) that are similar to training data as well as the distribution of nasalized vowels and oral vowels in French after 649 epochs' training. We perform the same analysis process as we had in English Experiment. Two latent variables (z4 and z37) are also found to be closely relative to French nasal consonants. Different from English, two latent variables (z88 and z91) show independent control of French nasal vowels. Manipulating these pairs of latent variables concurrently shows some interaction of latent variables in controlling nasal vowels and nasal consonants. In Figure 3a, although z4 show primary controls of nasal consonants, as nasal consonants tend to presence in the generated outputs when z4 is positive, some interaction effects of z4 and z37 are found near the bottom right of the right panel. In Figure 3b, z88 and z91 demonstrates interactive effects on the nasal vowels: when z88 >0 and z91<0, the Generator tends to output nasal vowels. Most importantly, the variables tested in Figure 3ba control nasal consonants while the ones in Figure 3bb control vowels-unlike the English results, in which one set of variables controlled both. These results indicate that both French nasal vowels and nasal consonants are encoded as independent phonological features in ciwGAN and ciwGAN seems to apply some interactions between latent variables to control the presence of phonological features.", "publication_ref": [], "figure_ref": ["fig_1", "fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "Balanced Training Dataset Experiments", "text": "In previous two experiments,we found that ciw-GAN can capture the contrastiveness of the phonological phenomenon in English and French with different learned representation. We are also interested to evaluate how would the frequencies of different syllable types in the training data affect the learned representations of ciwGAN. We conduct experiments on two artificially balanced datasets. For our English-like experiment, we have 5570 tokens of the VT, 5570 tokens of VN. For French-like experiment, as most French nasal vowels extracted from SIWIS tend to be /\u0151/, we mitigate this bias by only include tokens with vowel /o/ for all syllable types in the training dataset: 1031 tokens of the oT, 1031 tokens of oN, 1031 tokens of\u0151T, 1031 tokens of\u0151N.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "English-like Experiment", "text": "In contrast to the natural English ciwGAN, where no latent variables are found to control nasal vowels, the Generator seems to encode vowels' nasality with latent variables (z60, z71), even though latent variable z60 is found to controls the both nasal consonants and nasal vowels. By manipulating z60 to [-5, 5], we can decrease the proportion of nasality in both vowels and consonants and have nasal vowels and nasal consonants completely disappear in the generated data.\nInteractive effects are found between z60 and z68 and between z60 and z71 in controlling nasal consonants and nasal vowels respectively, which is similar to the interactive correlations of latent variables we found in French experiment. As illustrated in Figure 4a and Figure 4b, the ciwGAN tends to generate nasal consonants except when the values of z60 and z68 are both set to negative and ciwGAN will generate nasal vowels when z60 and z71 are non-negative. Despite the dependency between nasal vowels and nasal consonants is also found in English ciwGAN with balanced dataset: the Generator tends to produce nasal vowels following nasal consonants, ciwGANs can generate independent nasal vowels in some generated audio: there are some tokens carryVT in the generated audios.\nFrench-like Experiment With balanced dataset, we can still find latent variables that only control nasal consonants. As shown in Figure 4a nasal consonants can be produced independently when z60 <0 and z71 >0. Interactive effects of latent variables are also found on both nasal vowels and nasal codas. ciwGAN tend to generate nasal vowels when z16>0 and z88 <0, as in Figure 4b. However, different from the model trained on natural French dataset, we cannot find latent variables that only control French nasal vowels. When z16 is set to a positive value and z88 is set to be negative, the generated audios on the top right of the Figure 4b, are detected to have both nasal vowels and nasal consonants.\nThe phenomenon that interactive effects occurs in ciwGAN with balanced English dataset matches with the finding in French experiment and Frenchlike experiment, which suggests that ciwGAN develops similar learned representations between the   4a shows the generated audios with concurrent manipulation of latent variables z60 and z68 (x axis: z60 & y axis: z68); Figure 4b shows the generated audios with concurrent manipulation of latent variables z71 and z60 (x axis: z71 & y axis: z60) Green heatmap indicates the detected English nasal vowel; Red heatmap indicates the detected English nasal consonant.  two languages with balanced datasets. Besides, no latent variables can only control French nasal vowels in French-like experiment, which is similar to the results in English-like experiments, but different from French experiment.", "publication_ref": [], "figure_ref": ["fig_3", "fig_3", "fig_3", "fig_3", "fig_3", "fig_3", "fig_3"], "table_ref": []}, {"heading": "Conclusion", "text": "Our results qualify Begu\u0161 (2020a)'s claim that GANs can learn clearly interpretable representational systems in which single latent variables correspond to identifiable phonological features.\nWhile we do find this in the English experiment, we do not find it in the French experiment, Englishlike experiment and French-like experiment. This suggests that both the frequencies with which different syllable types in the data occur, and the contrastiveness of the phonological phenomenon, may affect whether the learned representation is simple or distributed across many variables. Moreover, as the learned representations in ciwGANs involve featural conjunction, this counters Begu\u0161 (2020a)'s claim of ciwGANs having an independent dimension for every phonological feature. In future work, understanding more complicated feature interactions, we plan to use eigendecomposition or other methods which can more easily represent higher-order interactions between features. However, our current methods are still informative about the learned representations, since the regression analyses show that only a few of the learned features are critical to representing nasality. On the other hand, we do find that GANs clearly distinguish between the contrastive and noncontrastive status of vowel nasality in English and French. This supports Begu\u0161 (2020a)'s higher-level claim that GANs are good phonological learners by testing it in a more controlled setting in which the same feature is compared across languages.\nWhile artificially balancing the frequencies of syllable types in the training data does not erase the difference between English and French, we do observe that the learned representations are more similar between the two, and that the GANs learning from English data begins to be able to generate someVT syllables, although with low frequency. This aligns with a widespread theory for the origin of contrastive nasality in languages like French. Changing the patterns' frequency will change the feature systems in languages.\nOur results highlight the difficulty of learning featural phonological representations from acoustic data, as well as the interpretational difficulties of detecting such representations once learned. We believe that the question of which architectures successfully acquire these systems is still openmore work needs to be done on larger pretrained models to determine which, if any, of these generalizations they encode. More careful comparisons between smaller-scale systems can also shed light on how well they distinguish between completely predictable (allophonic) distributional properties of segments due to phonotactic constraints, and statistical regularities due to the lexicon or morphology.\nOn the other hand, the observed difficulty of learning these generalizations lends support to theories of phonological change in which mistakes in acquisition lead to the expansion or restructuring of a feature inventory (Foulkes and Vihman, 2013). By looking at historical corpus of old French, we can observe how the lexicon evolves over time changing the frequency of different vowel-consonant combinations. The fact that changes in frequency result in this kind of change for our model is evidence that this mechanism is plausible, and offers a route to testing its explanatory power for specific historical hypotheses in the future.\nAlthough the long-term goal of this research is understanding how phonological representation learning works for a variety of models and phenomena, we believe it is necessary to start small, with the treatment of one particular phenomenon. In text linguistics, there are now established benchmarks for understanding linguistic representation in language models, for example, The Benchmark of Linguistic Minimal Pairs (BLiMP) (Warstadt et al., 2020), but in speech linguistics, we are lagging behind. Even doing studies of an individual phenomenon requires identifying a phonological phenomenon, extracting and labeling a corpus and conducting a study of the model's learning behavior. A diverse and comprehensive benchmark dataset for studying phonological learning (beyond phoneme segmentation and categorization) would be an exciting goal for future work.", "publication_ref": ["b17", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "D Linear Regression Analysis", "text": "In section 5, we have linear regression analysis to identify latent variables that correlate to nasal features. The values of 100 latent variables in ciw-GAN's latent space is analyzed and 7 latent variables that have the highest chi-square scores are considered to have a strongly correlation to nasality.\nFigure 7: Linear regression analysis of the nasality and the corresponding latent variables z. Y axis is Chisquare scores for 97 latent variable z and X axis is latent variables z. B2. Did you discuss the license or terms for use and / or distribution of any artifacts? Not applicable. I use TIMIT dataset and SIWIS French Speech Synthesis Database. The licenses for these two dataset are unknown B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Section 5 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. Left blank.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Section 4 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? No response.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We thank the Phonies group at OSU Linguistics Department for helpful discussion, especially Dr. Cynthia Clopper and Dr. Becca Morley. We also thank Dr. Ga\u0161per Begu\u0161 for sharing the training dataset used in (Begus and Zhou, 2022) ", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "The study of language model in their alignment to linguistic theories are interdisciplinary and hence usually hard to find explicit connection between language model and theories. In this paper we claim that a generative model, ciwGAN, can model both phonetic and phonology features. However, the two features are learned by two ciwGAN instances from disjoint training data sets. Our finding couldn't support or deny the following statements that are of researchers' concern:\n1. Generic GAN model can learn phonology features like ciwGAN.\n2. CiwGAN can model phonetic and phonology features simultaneously from a single dataset.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Manipulation Effects on Nasal Consonant", "text": "Figure 6 illustrates the manipulation effects of z13 and z90 on nasal consonant. ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations", "journal": "", "year": "2020", "authors": "Alexei Baevski; Steffen Schneider; Michael Auli"}, {"ref_id": "b1", "title": "Wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations", "journal": "", "year": "2020", "authors": "Alexei Baevski; Henry Zhou; Abdelrahman Mohamed; Michael Auli"}, {"ref_id": "b2", "title": "Generative adversarial phonology: Modeling unsupervised phonetic and phonological learning with neural networks", "journal": "Frontiers in artificial intelligence", "year": "2020", "authors": "Ga\u0161per Begu\u0161"}, {"ref_id": "b3", "title": "Interpreting Intermediate Convolutional Layers of Generative CNNs Trained on Waveforms", "journal": "", "year": "2022", "authors": "Gasper Begus; Alan Zhou"}, {"ref_id": "b4", "title": "Generative Adversarial Phonology: Modeling Unsupervised Phonetic and Phonological Learning With Neural Networks", "journal": "", "year": "2020", "authors": "Ga\u0161per Begu\u0161"}, {"ref_id": "b5", "title": "Ga\u0161per Begu\u0161. 2020b. Modeling unsupervised phonetic and phonological learning in Generative Adversarial Phonology", "journal": "", "year": "", "authors": ""}, {"ref_id": "b6", "title": "Ciwgan and fiwgan: Encoding information in acoustic data to model lexical learning with generative adversarial networks", "journal": "", "year": "2021", "authors": "Ga\u0161per Begu\u0161"}, {"ref_id": "b7", "title": "Ga\u0161per Begu\u0161. 2021b. Identity-based patterns in deep convolutional networks: Generative adversarial phonology and reduplication", "journal": "", "year": "", "authors": ""}, {"ref_id": "b8", "title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "journal": "", "year": "2016", "authors": "Xi Chen; Yan Duan; Rein Houthooft; John Schulman; Ilya Sutskever; Pieter Abbeel"}, {"ref_id": "b9", "title": "Representations of language in a model of visually grounded speech signal", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Grzegorz Chrupa\u0142a; Lieke Gelderloos; Afra Alishahi"}, {"ref_id": "b10", "title": "Audio Word2Vec: Unsupervised Learning of Audio Segment Representations using Sequence-to-sequence Autoencoder", "journal": "", "year": "2016", "authors": "Yu-An Chung; Chao-Chung Wu; Chia-Hao Shen; Hung-Yi Lee; Lin-Shan Lee"}, {"ref_id": "b11", "title": "Nasalisation in english: phonology or phonetics", "journal": "Phonology", "year": "1993", "authors": "C Abigail;  Cohn"}, {"ref_id": "b12", "title": "Impact of hydration status on electromyography and ratings of perceived exertion during the vertical jump", "journal": "International Journal of Kinesiology and Sports Science", "year": "2019", "authors": "T Paul;  Donahue; J Samuel;  Wilson; C Charles; Melinda Williams; John C Valliant;  Garner"}, {"ref_id": "b13", "title": "The Zero Resource Speech Challenge", "journal": "TTS without T", "year": "2019", "authors": "Ewan Dunbar; Robin Algayres; Julien Karadayi; Mathieu Bernard; Juan Benjumea; Xuan-Nga Cao; Lucie Miskic; Charlotte Dugrain; Lucas Ondel; Alan W Black; Laurent Besacier; Sakriani Sakti; Emmanuel Dupoux"}, {"ref_id": "b14", "title": "The Zero Resource Speech Challenge 2021: Spoken language modelling", "journal": "", "year": "2021", "authors": "Ewan Dunbar; Mathieu Bernard; Nicolas Hamilakis; Tu Anh Nguyen; Maureen De Seyssel; Patricia Roz\u00e9; Morgane Rivi\u00e8re; Eugene Kharitonov; Emmanuel Dupoux"}, {"ref_id": "b15", "title": "The Zero Resource Speech Challenge", "journal": "", "year": "2017", "authors": "Ewan Dunbar; Xuan Nga Cao; Juan Benjumea; Julien Karadayi; Mathieu Bernard; Laurent Besacier; Xavier Anguera; Emmanuel Dupoux"}, {"ref_id": "b16", "title": "The Zero Resource Speech Challenge 2020: Discovering discrete subword and word units", "journal": "", "year": "2020", "authors": "Ewan Dunbar; Julien Karadayi; Mathieu Bernard; Xuan-Nga Cao; Robin Algayres; Lucas Ondel; Laurent Besacier; Sakriani Sakti; Emmanuel Dupoux"}, {"ref_id": "b17", "title": "First language acquisition and phonological change", "journal": "", "year": "2013", "authors": "Paul Foulkes; Marilyn "}, {"ref_id": "b18", "title": "Neural language models as psycholinguistic subjects: Representations of syntactic state", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Richard Futrell; Ethan Wilcox; Takashi Morita; Peng Qian; Miguel Ballesteros; Roger Levy"}, {"ref_id": "b19", "title": "Darpa timit acoustic-phonetic continous speech corpus cdrom. nist speech disc 1-1.1. NASA STI/Recon technical report n", "journal": "", "year": "1993", "authors": "Lori F John S Garofolo;  Lamel; M William; Jonathan G Fisher; David S Fiscus;  Pallett"}, {"ref_id": "b20", "title": "From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning", "journal": "", "year": "2016", "authors": "Lieke Gelderloos; Grzegorz Chrupa\u0142a"}, {"ref_id": "b21", "title": "", "journal": "Generative adversarial networks", "year": "2020", "authors": "Ian Goodfellow; Jean Pouget-Abadie; Mehdi Mirza; Bing Xu; David Warde-Farley; Sherjil Ozair; Aaron Courville; Yoshua Bengio"}, {"ref_id": "b22", "title": "", "journal": "", "year": "2014", "authors": "Ian J Goodfellow; Jean Pouget-Abadie; Mehdi Mirza; Bing Xu; David Warde-Farley; Sherjil Ozair; Aaron Courville; Yoshua Bengio"}, {"ref_id": "b23", "title": "Colorless green recurrent networks dream hierarchically", "journal": "Long Papers", "year": "2018", "authors": "Kristina Gulordava; Piotr Bojanowski; Edouard Grave; Tal Linzen; Marco Baroni"}, {"ref_id": "b24", "title": "Introductory phonology", "journal": "John Wiley & Sons", "year": "2011", "authors": "Bruce Hayes"}, {"ref_id": "b25", "title": "Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021. HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units", "journal": "", "year": "", "authors": "Wei-Ning Hsu; Benjamin Bolte; Yao-Hung Hubert Tsai"}, {"ref_id": "b26", "title": "Optimality theory. Cambridge university press", "journal": "", "year": "1999", "authors": "Ren\u00e9 Kager"}, {"ref_id": "b27", "title": "", "journal": "", "year": "", "authors": "Alexander Jey Han Lau; Shalom Clark;  Lappin"}, {"ref_id": "b28", "title": "A probabilistic view of linguistic knowledge. Cognitive Science", "journal": "", "year": "", "authors": "Acceptability Grammaticality;  Probability"}, {"ref_id": "b29", "title": "Assessing the ability of lstms to learn syntaxsensitive dependencies", "journal": "Transactions of the Association for Computational Linguistics", "year": "2016", "authors": "Tal Linzen; Emmanuel Dupoux; Yoav Goldberg"}, {"ref_id": "b30", "title": "Targeted syntactic evaluation of language models", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Rebecca Marvin; Tal Linzen"}, {"ref_id": "b31", "title": "Montreal forced aligner: Trainable text-speech alignment using kaldi", "journal": "", "year": "2017", "authors": "Michael Mcauliffe; Michaela Socolof; Sarah Mihuc; Michael Wagner; Morgan Sonderegger"}, {"ref_id": "b32", "title": "An introduction to English phonology", "journal": "Edinburgh University Press", "year": "2002", "authors": ""}, {"ref_id": "b33", "title": "Introduction to English phonetics", "journal": "Edinburgh university press", "year": "2017", "authors": "Richard Ogden"}, {"ref_id": "b34", "title": "WaveNet: A Generative Model for Raw Audio", "journal": "", "year": "2016", "authors": "Aaron Van Den Oord; Sander Dieleman; Heiga Zen; Karen Simonyan; Oriol Vinyals; Alex Graves; Nal Kalchbrenner; Andrew Senior; Koray Kavukcuoglu"}, {"ref_id": "b35", "title": "The kaldi speech recognition toolkit", "journal": "", "year": "2011", "authors": "Daniel Povey; Arnab Ghoshal; Gilles Boulianne; Lukas Burget; Ondrej Glembek; Nagendra Goel; Mirko Hannemann; Petr Motlicek; Yanmin Qian; Petr Schwarz"}, {"ref_id": "b36", "title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "journal": "", "year": "2015", "authors": "Alec Radford; Luke Metz; Soumith Chintala"}, {"ref_id": "b37", "title": "Analyzing distributional learning of phonemic categories in unsupervised deep neural networks. In CogSci", "journal": "NIH Public Access", "year": "2016", "authors": "Tasha Okko R\u00e4s\u00e4nen; Nima Nagamine;  Mesgarani"}, {"ref_id": "b38", "title": "Measuring the perceptual availability of phonological features during language acquisition using unsupervised binary stochastic autoencoders", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Cory Shain; Micha Elsner"}, {"ref_id": "b39", "title": "Blimp: The benchmark of linguistic minimal pairs for english", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "Alex Warstadt; Alicia Parrish; Haokun Liu; Anhad Mohananey; Wei Peng; Sheng-Fu Wang; Samuel R Bowman"}, {"ref_id": "b40", "title": "The siwis french speech synthesis database", "journal": "", "year": "2017", "authors": "Junichi Yamagishi; Pierre-Edouard Honnet; Philip Garner; Alexandros Lazaridis"}, {"ref_id": "b41", "title": "Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram", "journal": "", "year": "2020", "authors": "Ryuichi Yamamoto; Eunwoo Song; Jae-Min Kim"}, {"ref_id": "b42", "title": "Fricative phoneme detection using deep neural networks and its comparison to traditional methods", "journal": "", "year": "2021", "authors": "Metehan Yurt; Pavan Kantharaju; Sascha Disch; Andreas Niedermeier; Alberto N Escalante; -B Veniamin I Morgenshtern"}, {"ref_id": "b43", "title": "The sounds of language: An introduction to phonetics and phonology", "journal": "John Wiley & Sons", "year": "2012", "authors": "C Elizabeth;  Zsiga"}, {"ref_id": "b44", "title": "Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? No response", "journal": "", "year": "", "authors": ""}, {"ref_id": "b45", "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run", "journal": "", "year": "", "authors": ""}, {"ref_id": "b46", "title": "for preprocessing, for normalization, or for evaluation", "journal": "", "year": "", "authors": ""}, {"ref_id": "b47", "title": "crowdworkers) or research with human participants? Left blank", "journal": "", "year": "", "authors": ""}, {"ref_id": "b48", "title": "Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators", "journal": "", "year": "", "authors": " D1"}, {"ref_id": "b49", "title": "crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic", "journal": "", "year": "", "authors": ""}, {"ref_id": "b50", "title": "Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?", "journal": "", "year": "", "authors": " D3"}, {"ref_id": "b51", "title": "Was the data collection protocol approved (or determined exempt) by an ethics review board? No response", "journal": "", "year": "", "authors": " D4"}, {"ref_id": "b52", "title": "Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response", "journal": "", "year": "", "authors": " D5"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "(a) French -z4 & z37 (b) French -z88 & z91", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: French experiment results: Figure3ashows the generated audios with concurrent manipulation of latent variables z4 and z37 (x axis: z4 & y axis: z37); Figure3bshows the generated audios with concurrent manipulation of latent variables z88 and z91 (x axis: z88 & y axis: z91). Green heatmap indicates the detected French nasal vowel; Red heatmap indicates the detected French nasal consonant.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "(a) English like -z60 & z68 (b) English like -z60 & z71", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure4: English-like experiment results: Figure4ashows the generated audios with concurrent manipulation of latent variables z60 and z68 (x axis: z60 & y axis: z68); Figure4bshows the generated audios with concurrent manipulation of latent variables z71 and z60 (x axis: z71 & y axis: z60) Green heatmap indicates the detected English nasal vowel; Red heatmap indicates the detected English nasal consonant.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "(a) French-like -z60 & z71 (b) French-like -z88 & z16", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure5: French-like experiment results: Figure5ashows the generated audios with concurrent manipulation of latent variables z60 and z71 (x axis: z60 & y axis: z71); Figure5bshows the generated audios with concurrent manipulation of latent variables z88 and z16 (x axis: z88 & y axis: z16) Green heatmap indicates the detected French nasal vowel; Red heatmap indicates the detected French nasal consonant.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "you describe the limitations of your work? Section 7 A2. Did you discuss any potential risks of your work? This paper does not include any risks listed in the checklist. A3. Do the abstract and introduction summarize the paper's main claims? Left blank. A4. Have you used AI writing assistants when working on this paper? Left blank. B Did you use or create scientific artifacts? Section 3 and 4 B1. Did you cite the creators of artifacts you used? Section 3 and 4", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Training Dataset for CiwGAN to Learn Vowel and Nasality Features in English and French where 2681 tokens are extracted from monosyllabic words and 2005 tokens are from the multisyllabic words' last syllable. We have 1031VT tokens, 2577 VT tokens, 47VN tokens, and 1031 VN tokens as French training dataset. Example lexical items of English and French are shown in the appendix.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "(a) English -z90 & z13 (b) English -z4 & z37", "formula_coordinates": [6.0, 134.39, 298.71, 324.21, 10.81]}], "doi": "10.48550/arXiv.1910.05453"}