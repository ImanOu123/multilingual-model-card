{"title": "SESAM at SemEval-2020 Task 8: Investigating the relationship between image and text in sentiment analysis of memes", "authors": "Lisa Bonheme; Marek Grzes", "pub_date": "", "abstract": "This paper presents our submission to task 8 (memotion analysis) of the SemEval 2020 competition. We explain the algorithms that were used to learn our models along with the process of tuning the algorithms and selecting the best model. Since meme analysis is a challenging task with two distinct modalities, we studied the impact of different multimodal representation strategies. The results of several approaches to dealing with multimodal data are therefore discussed in the paper. We found that alignment-based strategies did not perform well on memes. Our quantitative results also showed that images and text were uncorrelated. Fusion-based strategies did not show significant improvements and using one modality only (text or image) tends to lead to better results when applied with the predictive models that we used in our research.", "sections": [{"heading": "Introduction", "text": "SemEval 2020 task 8 (Sharma et al., 2020) is a sentiment analysis task targeted at memes 1 divided into three sub-tasks of increasing complexity: Sub-task A is predicting the sentiment polarity of a meme, Sub-task B is a multi-label binary classification task which aims to predict whether a meme is humorous, offensive, sarcastic and/or motivational (it can also have neither of these attributes), Sub-task C is a multi-output ordinal classification task which aims to predict the degree of humour, offence, sarcasm and motivation of a meme.\nThe dataset used for this task contains memes images whose text has been extracted by optical character recognition (OCR) and manually corrected when needed. Each meme is annotated on different aspects: sentiment polarity for sub-task A and the degree of humour, sarcasm, offence and motivation for sub-tasks B and C.\nMemes sentiment analysis is a challenging task as memes are multi-modal, rely heavily on implicit knowledge, and often use humour and sarcasm. While this topic is of growing interest for NLP community, the way image and text interact in memes has barely been explored, leading to sub-optimal representation learning.\nIn an attempt to shed some light on the role of both modalities, we investigate their correlation and their impact on each sub-task prediction. Our code is available at https://github.com/bonheml/SESAM.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related work", "text": "Sentiment analysis of text is a very active research area which still faces multiple challenges such as irony and humour detection (Hern\u00e1ndez Farias and Rosso, 2017) and low inter-annotator agreement caused by the high subjectivity of the content .\nResearch has been extended to multimodal sentiment analysis during the last years (Soleymani et al., 2017), but the focus was mostly on video and text or speech and text. The specific multi-modality of memes in sentiment analysis has only been addressed recently by French (2017), who investigated their correlation with other comments in online discussions.\nThe growing usage of memes as an alternative medium of communication on social media has also recently drawn the attention of the online abuse research community. Zannettou et al. (2018) studied the propagation of memes posted by fringe web communities 2 , and their influence and transmission between different social media. Sabat et al. (2019) performed hate speech detection on memes and showed that images were more important than text for the prediction.\nHowever, as pointed out by Vidgen et al. (2019), memes completely make sense only if one takes both text and image content into account. These modalities can also lead to totally different perceived sentiment when recombined. For example, a meme whose image is a grumpy cat and the text is \"happy birthday\" will have a very different sentiment from a meme with the same text but with an image of a happy puppy.\nWe argue that having a better understanding of both modalities interaction will contribute to more informed joint representations and is a crucial topic to explore.\nThus, we investigate the impact of multiple embeddings applied to both modalities on several models with different types of decision boundaries and verify the consistency of our findings assessing the embeddings across the three different sub-tasks. As our main focus is to study the impact of representations used, we chose simple classification models from Scikit-learn (Pedregosa et al., 2011) such as K nearest neighbours or Gaussian Na\u00efve Bayes over more complex ones such as the deep learning architecture composed of three bidirectional gated recurrent units networks with contextual intermodal attention proposed by Akhtar et al. (2019). We did not perform any hyper-parameter tuning.\nIn multi-view representation learning, different techniques can be used to represent the views, depending on the nature of the relationship between them (Guo et al., 2019). When they share latent traits, one can use alignment to project their embeddings into a common space given a constraint (e.g., distance, correlation) (Baltru\u0161aitis et al., 2019). On the other hand, if they are complementary, fusion techniques will be more useful as they will group the meaningful latent variables of each view into a compact representation (Li et al., 2019).\nIn section 3.1 we assess the usefulness of aligned representation for memes by investigating the possible correlations between images and memes. Then, in section 3.2, we study the added value of voter-based fusion techniques such as the one proposed by Gaspar and Alexandre (2019) in their work on multimodal sentiment analysis.", "publication_ref": ["b13", "b28", "b10", "b35", "b25", "b33", "b22", "b2", "b12", "b4", "b16", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "How correlated are images and text?", "text": "Exploring the possible correlations between images and text can provide valuable insights into the efficiency of the aligned representation for memes. Canonical correlation analysis (CCA) (Hotelling, 1936) has proven to be very efficient for correlation-based multimodal representation learning alignment (Wang et al., 2015), and has been successfully used for cross-modal multimedia retrieval (Rasiwasia et al., 2010). In order to provide a broad analysis of correlation, we analyse both linear and non-linear relationships between image and text embeddings.\nLinear CCA Introduced by Hotelling (1936), CCA aims to find the linear projections of two views which are maximally correlated.\nMore formally, let X \u2208 R n\u00d7m and Y \u2208 R n\u00d7p be two zero-mean matrices of n observations with m and p features respectively. We aim to find the K orthogonal linear projections\nA = [a 1 , . . . , a k ], B = [b 1 , . . . , b k ] such that: (a * , b * ) = argmax a,b corr(a T X, b T Y ) = argmax a,b a T \u03a3 XY b a T \u03a3 XX a b T \u03a3 Y Y b\nwhere \u03a3 XX and \u03a3 Y Y are the covariance matrices of X and Y respectively and \u03a3 XY is their cross covariance matrix (Uurtio et al., 2017 Fyfe, 2000;Melzer et al., 2001;Van Gestel et al., 2001;Akaho, 2001) have been developed to discover non-linear associations. However, KCCA does not scale well to large datasets. Using the better scaling capacity of deep neural networks, Andrew et al. (2013) proposed deep CCA (DCCA), a version of CCA which stacks layers of non-linear transformations for both views and optimises the correlation between their transformed representations. Given the size of the dataset and the high dimensionality of the features used in this study, we chose to use DCCA over KCCA.\nApplication to the tasks Both CCA and DCCA are applied to the training dataset. CCA results are evaluated using the first canonical correlation scores and the assessment of their statistical significance.\nAs DCCA provides only aligned embeddings, it cannot be evaluated using the same techniques. Instead, we trained DCCA on the training dataset, predicted the aligned embeddings of the dev and testing dataset and compared the results of our different models, discussed in section 2.2, with aligned and non-aligned embeddings. We also investigated the intra-class correlation by performing CCA on each class of sub-task A and each label of sub-task B.", "publication_ref": ["b14", "b34", "b23", "b14", "b30", "b15", "b19", "b32", "b1", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "How image and text contribute to the predictions?", "text": "Fusion methods Over the years, various fusion techniques for predictive models have been developed. Some rely on a neural network to perform the fusion (Tanti et al., 2017), or just concatenate the modalities into one vector and treat it as a unimodal problem (Baltru\u0161aitis et al., 2019). However, it is difficult to uncover the contribution of each modality with these techniques. In contrast, a voter-based fusion technique (Morvant et al., 2014;Gaspar and Alexandre, 2019) can be easily interpreted and will thus be used here. This technique is referred to as late fusion as the fusion is performed after the learning phase whereas techniques such as embedding concatenation, where the fusion occurs before the learning phase, are referred to as early fusion.\nAs voter fusion is model-agnostic, it also allows us to test it on different models and tasks to verify the generalisation of our findings. While late fusion has been shown to often provide better results in multimedia fusion (Snoek et al., 2005), early fusion tends to perform better when one of the modalities contribute more than the other to the predictions (Morvant et al., 2014).\nTo handle this possibility, our voter, illustrated in figure 1, is composed of three identical models which are trained on image, text and a concatenation of both embeddings respectively. Thus, we perform hybrid fusion, using the information provided by both late and early fusion. As we are only interested in exploring the impact of the different modalities, unlike in Gaspar and Alexandre (2019) where classifier decisions were weighted according to their quality, we gave all classifiers the same weights. To assess whether different modalities contribute to a different type of prediction, we also run each model independently and compare their results. Thus, if a modality is only helpful in some cases (e.g., only for negative polarity detection), the voter should provide better results than independent models.   Models The different predictive models used are logistic regression (LR), K nearest neighbours (KNN), Gaussian Na\u00efve Bayes (GNB), Random forest (RF), and multi-layer perceptron (MLP). We chose them so that we can study the impact of different embeddings on several decision boundaries. To suit the different sub-tasks objectives, these models are wrapped in meta classifiers as described in table 1. We used implementation from Scikit-learn (Pedregosa et al., 2011) for the multi-output and multi-label classifiers and a custom implementation of Frank and Hall (2001) made compatible with Scikit-learn models for the ordinal classifier.", "publication_ref": ["b29", "b4", "b21", "b11", "b27", "b21", "b11", "b22", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental setup", "text": "Data cleaning and preprocessing We manually added the text values of seven memes which had neither OCR nor corrected text values in the training dataset and removed URLs corresponding to meme sources from transcribed texts. A number of websites used for meme generation add their URL to the final meme, and this was often caught and transcribed by the OCR extraction. Following Camacho-Collados and Pilehvar (2018) study on text preprocessing, we did not perform any lemmatisation or lowercasing. To obtain one text embedding per meme, the text of each meme was vectorised using a pre-trained Universal Sentence Encoder (USE) (Cer et al., 2018) retrieved from Tensorflow hub (Abadi et al., 2015). The images were processed with Xception (Chollet, 2017), pre-trained on ImageNet (Russakovsky et al., 2015), and the penultimate layer was used as embedding.\nDataset analysis As shown in figures 2 and 3, the training dataset is highly skewed towards positive memes which are mostly funny, motivational, slightly sarcastic and offensive. Occurrences of \"extreme\" memes such as hateful offensive are very rare (e.g., there are less than 500 hateful offensive memes). The word count distribution is equivalent over each label, and we did not find words specifically attached to a given label. No obvious cluster of memes was shown by the t-SNE (van der Maaten and Hinton, 2008) or UMAP (McInnes et al., 2018) projections of sentence and image embeddings.", "publication_ref": ["b5", "b6", "b0", "b7", "b24", "b31", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Models training", "text": "The same model types and embeddings combinations are used for the three sub-tasks, and we only varied the meta classifiers as listed in table 1 to adapt the models' output to the task at hand.   The score from the model selected during the evaluation phase to be submitted to the competition is marked with *\nThe training phase where a training and a dev dataset are provided. During this phase, each architecture 3 is trained on the training dataset and evaluated on the dev dataset using the macro F1 score for sub-task A, and averaged macro F1 scores for sub-tasks B and C 4 . No hyper-parameter tuning is performed and the dev dataset is only used to filter non-informative models which will not be submitted during the evaluation phase.\nThe evaluation phase where an unlabelled testing dataset is provided. During this phase, the predictions are done using the architectures previously selected, without retraining, and uploaded to Codalab. Similarly to the training phase, the results are evaluated using a macro F1 score for sub-task A and an averaged macro F1 scores for sub-tasks B and C. The combination of model type and embedding providing the best results on the testing dataset over the three sub-tasks is selected for the final ranking.\nThe ranking phase where the model selected during the evaluation phase is submitted for ranking. The final ranking is done using the testing dataset and the same metrics as in the previous phases.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "This section provides an analysis of our results at each step of our experiment. First, we investigate the results on the dev dataset which guided our model selection for the evaluation phase. The results retrieved from Codalab during the evaluation phase for the selected models are then analysed, and we finally conclude with the analysis of the scores provided during the final ranking.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation on dev dataset", "text": "Alignment approach (correlation-based) No statistically significant correlation between image and text was found with linear CCA, either over the entire training dataset or intra-class. Non-linear Deep CCA (DCCA) does not provide significant improvements compared to non-aligned concatenated embeddings or text modality only, and even often worsened the results. It thus seems that image and text are more complementary than correlated in the case of sentiment analysis of memes. This finding is consistent with the fact that memes often make sense when the combination of image and text is considered and changing one or the other can change the associated sentiments (Vidgen et al., 2019). Given these empirical results, we conclude that alignment-based approaches may not be suitable for meme analysis.\nFusion approach (voter-based) As shown in tables 2, 3 and 4, voter-based fusion technique did not lead to better results than one modality alone and consistently worsened the results. Interestingly, all the models tested, except KNN, performed better with one modality alone over all the sub-tasks.\nMost of the models were not able to find very discriminative features in image embeddings and often ended up predicting every class as belonging to the most frequent class. This problem was also reflected in concatenated embeddings whose results were most of the time worse than the ones provided by the most informative modality. While early and hybrid fusion approaches (i.e., concatenated embeddings and voters) provide better scores than image-only, text-only generally gives the best results, especially for GNB, RF and MLP.\nWhile it intuitively makes sense to consider images in memes, it seems that image representations such as the one we used may not be suitable for the task at hand and thus underperformed. Indeed, these representations are accurate enough to perform image captioning, but they lack the higher-level information we use to interpret memes. For example, a surprised cat and a grumpy cat will just be represented as cats when the sentiment attached to a meme \"Me when I look at my grades\" can drastically change depending on the type of cat used. Thus, using embeddings extracted from image sentiment classifiers could be more suitable to sentiment analysis of memes.\nBecause of the reuse of the same image with a different text leading to different sentiments, using image embeddings only can also introduce noise to the data with one image linked to contradictory outputs. Thus, it may be more efficient to merge both embeddings early on. However, early fusion did not show consistent improvements, indicating that more complex, model-dependent fusion techniques such as neural networks may be needed.\nExcept for KNN that obtained marginally better results with fusion techniques, most models seem to perform best with text, contrary to what was reported by Sabat et al. (2019) for hate speech detection. These apparently contradictory results may be due to the usage of different discriminative features on each task. This could be an interesting avenue to explore for assessing the potential of transfer learning with memes embeddings. Indeed, the more different discriminative features used for sentiment analysis and hate speech detection of memes are, the less efficient the usage of generic meme embeddings will be.\nFinally, as memes often reflect the shared culture of the communities which create them (Lin et al., 2014), having some contextual knowledge would probably also be greatly beneficial for meme analysis.\nModel selection Given that the results of linear regression are very low and do not provide much information on the impact of each modality, it is removed from the pool of models that will be used for the evaluation phase.", "publication_ref": ["b33", "b25", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation on the testing dataset", "text": "During the evaluation phase, we used the results provided on Codalab, which are referenced in tables 5, 7 and 9 to select the model to submit for competition ranking and assess the generalisability of the conclusion made from the empirical results during the training phase. After the release of the final ranking,   the task organisers have notified us that the scores displayed on Codalab during the competition were incorrect and have released corrected scores which we discuss in section 4.3. In this section, we discuss the incorrect Codalab evaluation scores because this is what was available to us during the competition, and we used these to select the final model that we submitted.\nAlignment approach (correlation-based) Similarly to the results observed during the training phase, no statistically significant correlation between image and text was found with linear CCA. The scores obtained with DCCA were also lower than the ones obtained during the training phase. Thus, we did not use the architecture with aligned embeddings.\nFusion approach (voter-based) Surprisingly, the results in the evaluation phase were very different from those obtained during the training phase. RF and MLP, which were both performing very well on text modality over all three sub-tasks had consistently lower scores with almost equivalent results over all the embeddings tested. KNN which was previously performing well on fusion-based embeddings also provided lower scores which were almost equivalent over all the embeddings tested, with a marginal improvement with image embeddings.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model selection", "text": "The model best performing on the testing dataset of the evaluation phase, KNN with image embedding, was the one submitted for the final ranking. When this analysis was performed, we did not know that the results in tables 5, 7, 9 were incorrect.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Final results", "text": "In this section, we present the correct evaluation results which became available after the competition. Fusion approach (voter-based) As shown in tables 6, 8 and 10, the corrected results are very close to the original evaluation results for sub-task A, but vary importantly for sub-tasks B and C. Similarly to the results obtained during the training phase, GNB still favours text-only modality for all the tasks, but other models now show similar results for text, image, concat and voter. Interestingly, text embeddings provide much lower results than during the training phase, especially for RF and MLP. Various factors such as different label distributions between dev and testing dataset, more similar vocabulary between dev and training dataset, or memes with less informative text in the testing dataset could have influenced these results. We argue that an in-depth analysis of these possible factors could lead to new insights regarding the embedding features used by the models during the learning process. Thus, we will investigate it once the annotated testing dataset has been published.\nModel selection Given the corrected scores, GNB with text embeddings would have been a better model to submit for final ranking, especially for sub-task B. Unfortunately, the correct evaluation scores were not available during the competition.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We have provided an overview of the impact of different representations on meme sentiment analysis. We tested alignment-based and fusion-based techniques on a range of models on each sub-task. While none of them seemed to be beneficial for the different sub-tasks, we found that 1) alignment-based techniques were not suitable for meme analysis as image and text of memes are not correlated 2) using only one modality (text or image) tends to perform better than a combination of both when we use standard (i.e. non-deep learning) predictive models. However, these conclusions should be taken with caution as the scores obtained on the dev and testing datasets vary greatly and other factors, such as the label distribution of the dataset can also have influenced these results. Finally, we argue that a more adapted image representation, possibly enriched with contextual knowledge, as well as more complex fusion techniques, may be promising to explore.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgement We thank the SemEval-2020 organisers for their time to prepare the data and run the competition, and the reviewers for their insightful comments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "A Hyper-parameters of the models In addition to the code provided at https://github.com/bonheml/SESAM, the language, packages used and their version, and the hyper-parameters of the models are detailed in this appendix for reproducibility.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Language and library used", "text": "The experiment was implemented in Python 3.6 and the packages listed in table 11 ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "journal": "", "year": "2015", "authors": "Mart\u00edn Abadi; Ashish Agarwal; Paul Barham; Eugene Brevdo; Zhifeng Chen; Craig Citro; Greg S Corrado; Andy Davis; Jeffrey Dean; Matthieu Devin; Sanjay Ghemawat; Ian Goodfellow; Andrew Harp; Geoffrey Irving; Michael Isard; Yangqing Jia; Rafal Jozefowicz; Lukasz Kaiser; Manjunath Kudlur; Josh Levenberg"}, {"ref_id": "b1", "title": "A kernel method for canonical correlation analysis", "journal": "Springer-Verlag", "year": "2001", "authors": "Shotaro Akaho"}, {"ref_id": "b2", "title": "Multi-task Learning for Multi-modal Emotion Recognition and Sentiment Analysis", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Dushyant Md Shad Akhtar; Deepanway Chauhan; Soujanya Ghosal;  Poria"}, {"ref_id": "b3", "title": "Deep Canonical Correlation Analysis", "journal": "", "year": "2013", "authors": "Galen Andrew; Raman Arora; Jeff Bilmes; Karen Livescu"}, {"ref_id": "b4", "title": "Multimodal Machine Learning: A Survey and Taxonomy", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2019-02", "authors": "Tadas Baltru\u0161aitis; Chaitanya Ahuja; Louis-Philippe Morency"}, {"ref_id": "b5", "title": "On the Role of Text Preprocessing in Neural Network Architectures: An Evaluation Study on Text Categorization and Sentiment Analysis", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Jose Camacho; -Collados ; Mohammad Taher Pilehvar"}, {"ref_id": "b6", "title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations", "journal": "Association for Computational Linguistics", "year": "2018-11", "authors": "Daniel Cer; Yinfei Yang; Sheng-Yi Kong; Nan Hua; Nicole Limtiaco; Rhomni St; Noah John; Mario Constant; Steve Guajardo-Cespedes; Chris Yuan; Brian Tar; Ray Strope;  Kurzweil"}, {"ref_id": "b7", "title": "Xception: Deep Learning With Depthwise Separable Convolutions", "journal": "", "year": "2017-07", "authors": "Francois Chollet"}, {"ref_id": "b8", "title": "The language of internet memes", "journal": "New York University Press", "year": "2012", "authors": "Patrick Davidson"}, {"ref_id": "b9", "title": "A Simple Approach to Ordinal Classification", "journal": "Springer", "year": "2001", "authors": "Eibe Frank; Mark Hall"}, {"ref_id": "b10", "title": "Image-based memes as sentiment predictors", "journal": "", "year": "2017-07", "authors": "Jean H French"}, {"ref_id": "b11", "title": "A Multimodal Approach to Image Sentiment Analysis", "journal": "Springer International Publishing", "year": "2019", "authors": "Ant\u00f3nio Gaspar; Lu\u00eds A Alexandre"}, {"ref_id": "b12", "title": "Deep Multimodal Representation Learning: A Survey", "journal": "IEEE Access", "year": "2019", "authors": "Wenzhong Guo; Jianwen Wang; Shiping Wang"}, {"ref_id": "b13", "title": "Chapter 7 -Irony, Sarcasm, and Sentiment Analysis", "journal": "Morgan Kaufmann", "year": "2017", "authors": "Delia I Hern\u00e1ndez Farias; Paulo Rosso"}, {"ref_id": "b14", "title": "Relations Between Two Sets of Variates", "journal": "Biometrika", "year": "1936-12", "authors": "Harold Hotelling"}, {"ref_id": "b15", "title": "Kernel and nonlinear canonical correlation analysis", "journal": "International Journal of Neural Systems", "year": "2000", "authors": "Pei Ling ; Lai ; Colin Fyfe"}, {"ref_id": "b16", "title": "A Survey of Multi-View Representation Learning", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2019-10", "authors": "Yingming Li; Ming Yang; Zhongfei (mark) Zhang"}, {"ref_id": "b17", "title": "Crowdsourced explanations for humorous internet memes based on linguistic theories", "journal": "AAAI", "year": "2014-11-02", "authors": "Chi-Chin Lin; Yi-Ching Huang; Jane Yung-Jen Hsu"}, {"ref_id": "b18", "title": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction", "journal": "", "year": "2018-02", "authors": "Leland Mcinnes; John Healy; James Melville"}, {"ref_id": "b19", "title": "Nonlinear Feature Extraction Using Generalized Canonical Correlation Analysis", "journal": "Springer", "year": "2001", "authors": "Thomas Melzer; Michael Reiter; Horst Bischof"}, {"ref_id": "b20", "title": "Challenges in Sentiment Analysis", "journal": "Springer International Publishing", "year": "2017", "authors": "M Saif;  Mohammad"}, {"ref_id": "b21", "title": "Majority Vote of Diverse Classifiers for Late Fusion", "journal": "Springer", "year": "2014", "authors": "Emilie Morvant; Amaury Habrard; St\u00e9phane Ayache"}, {"ref_id": "b22", "title": "Scikit-learn: Machine learning in Python", "journal": "Journal of Machine Learning Research", "year": "2011", "authors": "Fabian Pedregosa; Ga\u00ebl Varoquaux; Alexandre Gramfort; Vincent Michel; Bertrand Thirion; Olivier Grisel; Mathieu Blondel; Peter Prettenhofer; Ron Weiss; Vincent Dubourg; Jake Vanderplas; Alexandre Passos; David Cournapeau; Matthieu Brucher; Matthieu Perrot; Duchesnay And\u00e9douard"}, {"ref_id": "b23", "title": "A new approach to cross-modal multimedia retrieval", "journal": "Association for Computing Machinery", "year": "2010", "authors": "Nikhil Rasiwasia; Jose Costa Pereira; Emanuele Coviello; Gabriel Doyle; Gert R G Lanckriet; Roger Levy; Nuno Vasconcelos"}, {"ref_id": "b24", "title": "ImageNet Large Scale Visual Recognition Challenge", "journal": "International Journal of Computer Vision (IJCV)", "year": "2015", "authors": "Olga Russakovsky; Jia Deng; Hao Su; Jonathan Krause; Sanjeev Satheesh; Sean Ma; Zhiheng Huang; Andrej Karpathy; Aditya Khosla; Michael Bernstein; Alexander C Berg; Li Fei-Fei"}, {"ref_id": "b25", "title": "Hate Speech in Pixels: Detection of Offensive Memes towards Automatic Moderation", "journal": "", "year": "2019", "authors": " Benet Oriol; Cristian Canton Sabat; Xavier Ferrer;  Giro-I-Nieto"}, {"ref_id": "b26", "title": "Viswanath Pulabaigari, and Bj\u00f6rn Gamb\u00e4ck. 2020. SemEval-2020 Task 8: Memotion Analysis-The Visuo-Lingual Metaphor! In Proceedings of the 14th International Workshop on Semantic Evaluation (SemEval-2020)", "journal": "Association for Computational Linguistics", "year": "", "authors": "Chhavi Sharma; Deepesh Bhageria; William Paka;  Scott; P Y K L Srinivas; Amitava Das; Tanmoy Chakraborty"}, {"ref_id": "b27", "title": "Early versus late fusion in semantic video analysis", "journal": "Association for Computing Machinery", "year": "2005", "authors": "G M Cees; Marcel Snoek; Arnold W M Worring;  Smeulders"}, {"ref_id": "b28", "title": "A survey of multimodal sentiment analysis", "journal": "Image and Vision Computing", "year": "2017", "authors": "Mohammad Soleymani; David Garcia; Brendan Jou; Bj\u00f6rn Schuller; Shih-Fu Chang; Maja Pantic"}, {"ref_id": "b29", "title": "What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator?", "journal": "Association for Computational Linguistics", "year": "2017-09", "authors": "Marc Tanti; Albert Gatt; Kenneth Camilleri"}, {"ref_id": "b30", "title": "", "journal": "A Tutorial on Canonical Correlation Methods. ACM Computing Surveys", "year": "2017-11", "authors": "Viivi Uurtio; M Jo\u00e3o; Jaz Monteiro; John Kandola; Delmiro Shawe-Taylor; Juho Fernandez-Reyes;  Rousu"}, {"ref_id": "b31", "title": "Visualizing data using t-SNE", "journal": "Journal of Machine Learning Research", "year": "2008-11", "authors": "Laurens Van Der Maaten; Geoffrey Hinton"}, {"ref_id": "b32", "title": "Kernel Canonical Correlation Analysis and Least Squares Support Vector Machines", "journal": "Springer", "year": "2001", "authors": "Tony Van Gestel; Johan A K Suykens; Jos De Brabanter; Bart De Moor; Joos Vandewalle"}, {"ref_id": "b33", "title": "Challenges and frontiers in abusive content detection", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Bertie Vidgen; Alex Harris; Dong Nguyen; Rebekah Tromble; Scott Hale; Helen Margetts"}, {"ref_id": "b34", "title": "On deep multi-view representation learning", "journal": "", "year": "2015-07", "authors": "Weiran Wang; Raman Arora; Karen Livescu; Jeff Bilmes"}, {"ref_id": "b35", "title": "Logistic regression: Stratified K-fold (5 folds), random seed of 0, Saga solver, maximum of 10000 iterations, 6 CPU jobs, other parameters are default values from Scikit-learn", "journal": "ACM", "year": "2018", "authors": "Savvas Zannettou; Tristan Caulfield; Jeremy Blackburn; Emiliano De Cristofaro; Michael Sirivianos; Gianluca Stringhini; Guillermo Suarez-Tangil"}, {"ref_id": "b36", "title": "K nearest neighbours: 6 CPU jobs, other parameters are default values from Scikit-learn", "journal": "", "year": "", "authors": ""}, {"ref_id": "b37", "title": "Random forest: Random seed of 0, generalization accuracy estimated with out-of-bag samples, 6 CPU jobs, other parameters are default values from Scikit-learn", "journal": "", "year": "", "authors": "Bayes Gaussian Na\u00efve"}, {"ref_id": "b38", "title": "Multi-layer perceptron: Maximum of 1000 iterations, other parameters are default values from Scikit-learn", "journal": "", "year": "", "authors": ""}, {"ref_id": "b39", "title": "DCCA: The model is composed of 3 densely-connected layers of 1000 units with sigmoid activation and an output layer of 100 units with linear activation. It is trained using a batch size of 800 during 100 epochs using all singular values. The model is optimised with RMSprop using a learning rate of 1e-3 and an L2 penalty of 1e-5. The complete implementation is", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "3", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 3 :3Figure 3: Labels distribution of sub-task C (training dataset)", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": ").", "figure_data": "ImageTextText embeddingImage embeddingConcatenated embeddingClassifierClassifierClassifierVoterFigure 1: Overview of the voter methodDeep CCA As standard CCA is only able to discover linear relationships, other techniques such asKernel CCA (KCCA) (Lai and"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Meta classifiers used for the different sub-tasks", "figure_data": "overall sentimentpositive neutral negative0 500 1000 1500 2000 2500 3000 3500 4000 CountLabels combination (Humour, Sarcasm, Offense)funny,general,slight not_funny,not_sarcastic,not_offensive very_funny,general,slight funny,general,not_offensive very_funny,general,not_offensive funny,not_sarcastic,not_offensive very_funny,twisted_meaning,very_offensive not_funny,general,slight very_funny,general,very_offensive funny,twisted_meaning,slight very_funny,not_sarcastic,not_offensive very_funny,twisted_meaning,slight not_funny,general,not_offensive funny,general,very_offensive funny,twisted_meaning,very_offensive not_funny,very_twisted,very_offensive hilarious,general,slight funny,twisted_meaning,not_offensive funny,not_sarcastic,slight very_funny,twisted_meaning,not_offensive0100200300400 Count500600700800Figure 2: Polarity distribution of sub-task A (training dataset)"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Experiment Model training and evaluation was performed in three phases:", "figure_data": "Score per type of embeddingScore per type of embeddingText Image Concat Voter DCCATextImage Concat Voter DCCALR 0.25 0.250.250.250.25LR0.620.620.620.620.62KNN 0.45 0.32*0.470.430.46KNN 0.720.65* 0.730.710.73GNB 0.39 0.200.200.210.33GNB 0.670.380.380.380.58RF0.88 0.330.320.520.32RF0.930.640.660.780.66MLP 0.86 0.330.730.770.25MLP 0.920.640.840.880.62Table 2: Macro F1 scores for sub-task A on thedev dataset. The score from the model selectedduring the evaluation phase to be submitted to thecompetition is marked with *"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ": Averaged macro F1 scores for sub-task Bon the dev dataset. The score from the model se-lected during the evaluation phase to be submittedto the competition is marked with *Score per type of embeddingTextImage Concat Voter DCCALR0.280.210.290.220.28KNN0.500.29* 0.570.410.55GNB0.450.250.350.280.40RF0.940.310.540.570.59MLP0.930.300.820.780.45"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": "Score per type of embeddingTextImage Concat Voter DCCAKNN 0.490.49* 0.490.460.49GNB 0.520.370.370.370.48RF0.430.430.420.410.44MLP 0.510.490.500.500.41Baseline0.51: Averaged macro F1 scores originally pro-vided for sub-task B on the testing dataset of theevaluation phase. The score from the model sub-mitted to the competition is marked with *"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "", "figure_data": ": Corrected averaged macro F1 scores pro-vided after the competition for sub-task B on thetesting dataset of the evaluation phase. The scorefrom the model submitted to the competition ismarked with *"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Similarly to what was observed during the training phase, DCCA did not provide any significant improvements compared to other embeddings and have worsened the results most of the time. Given the consistency of the results, we can conclude that correlation-based alignment techniques are not useful for sentiment analysis of memes as, unlike image captions, memes show no evidence of correlation between image and text.", "figure_data": "Score per type of embeddingScore per type of embeddingText Image Concat Voter DCCAText Image Concat Voter DCCAKNN 0.26 0.26* 0.260.230.25KNN 0.31 0.31* 0.300.300.30GNB 0.29 0.260.260.270.23GNB 0.32 0.230.230.250.28RF 0.18 0.180.170.160.19RF 0.27 0.270.170.270.27MLP 0.27 0.260.280.250.12MLP 0.31 0.310.310.310.22Baseline0.25Baseline0.25Table 9: Averaged macro F1 scores originally pro-Table 10: Corrected averaged macro F1 scoresvided for sub-task C on the testing dataset of theprovided after the competition for sub-task C onevaluation phase. The score from the model sub-the testing dataset of the evaluation phase. Themitted to the competition is marked with *score from the model submitted to the competitionis marked with *Alignment approach (correlation-based)"}], "formulas": [{"formula_id": "formula_0", "formula_text": "A = [a 1 , . . . , a k ], B = [b 1 , . . . , b k ] such that: (a * , b * ) = argmax a,b corr(a T X, b T Y ) = argmax a,b a T \u03a3 XY b a T \u03a3 XX a b T \u03a3 Y Y b", "formula_coordinates": [2.0, 72.0, 629.67, 454.91, 69.26]}], "doi": ""}