{"title": "XRAND: Differentially Private Defense against Explanation-Guided Attacks", "authors": "Truc Nguyen; Phung Lai; Nhathai Phan; My T Thai", "pub_date": "", "abstract": "Recent development in the field of explainable artificial intelligence (XAI) has helped improve trust in Machine-Learningas-a-Service (MLaaS) systems, in which an explanation is provided together with the model prediction in response to each query. However, XAI also opens a door for adversaries to gain insights into the black-box models in MLaaS, thereby making the models more vulnerable to several attacks. For example, feature-based explanations (e.g., SHAP) could expose the top important features that a black-box model focuses on. Such disclosure has been exploited to craft effective backdoor triggers against malware classifiers. To address this trade-off, we introduce a new concept of achieving local differential privacy (LDP) in the explanations, and from that we establish a defense, called XRAND, against such attacks. We show that our mechanism restricts the information that the adversary can learn about the top important features, while maintaining the faithfulness of the explanations.", "sections": [{"heading": "Introduction", "text": "Over decades, successes in machine learning (ML) have promoted a strong wave of AI applications that deliver vast benefits to a diverse range of fields. Unfortunately, due to their complexity, ML models suffer from opacity in terms of explainability, which reduces the trust in and the verifiability of the decisions made by the models. To meet the necessity of transparent decision making, model-agnostic explainers have been developed to help create effective, more humanunderstandable AI systems, such as LIME (Ribeiro, Singh, and Guestrin 2016) and SHAP (Lundberg and Lee 2017), among many others (Sundararajan, Taly, and Yan 2017;Selvaraju et al. 2017;Vu and Thai 2020;Vu, Nguyen, and Thai 2022;Ying et al. 2019;Shrikumar, Greenside, and Kundaje 2017;Vu et al. 2021).\nIn MLaaS systems, a customer can build an ML model by uploading their data or crowdsourcing data, and executing an ML training algorithm. Then, the model is deployed in the cloud where users can receive the model predictions for input queries. MLaaS assumes black-box models as the end-users have no knowledge about the algorithm or internal information about the underlying ML models. Several proposals advocate for deploying model explanations in the cloud, such that a predicted label and an explanation are returned for each query to provide transparency for endusers. In practice, such an explainable MLaaS system model has been developed by many cloud providers (Azure 2021;Bluemix 2021), with applications in both industries (Community 2018) and academic research (Shokri, Strobel, and Zick 2021;Milli et al. 2019).\nDespite the great potential of those explainers to improve the transparency and understanding of ML models in MLaaS, they open a trade-off in terms of security. Specifically, they allow adversaries to gain insights into black-box models, essentially uncovering certain aspects of the models that make them more vulnerable. Such an attack vector has recently been exploited by the research community to conduct several explanation-guided attacks (Shokri, Strobel, and Zick 2021;Milli et al. 2019;Miura, Hasegawa, and Shibahara 2021;Zhao et al. 2021;Severi et al. 2021). It was shown that an explainer may expose the top important features on which a black-box model is focusing, by aggregating over the explanations of multiple samples. An example of utilizing such information is the recent highly effective explanation-guided backdoor attack (XBA) against malware classifiers investigated by (Severi et al. 2021). The authors suggest that SHAP can be used to extract the topk goodware-oriented features. The attacker then selects a combination of those features and their values for crafting a trigger; and injects trigger-embedded goodware samples into the training dataset of a malware classifier, with an aim of changing the prediction of malware samples embedded with the same trigger at inference time. Defense Challenges. To prevent an adversary from exploiting the explanations, we need to control the information leaked through them, especially the top-k features. Since the explanation on each queried sample is returned to the end users, a viable defense is to randomize the explanation such that it is difficult for attackers to distinguish top-k features while maintaining valuable explainability for the decisionmaking process. A well-applied technique to achieve this is preserving local differential privacy (LDP) (Erlingsson, Pihur, and Korolova 2014;Wang et al. 2017) on the explanations. However, existing LDP-based approaches (Erlings-arXiv:2212.04454v3 [cs.LG] 14 Dec 2022 son, Pihur, and Korolova 2014; Xiong et al. 2020;Sun, Qian, and Chen 2021;Zhao et al. 2020) are not designed to protect the top-k features aggregated over the returned explanations on queried data samples. Therefore, optimizing the tradeoff between defenses against explanation-guided attacks and model explainability is an open problem.", "publication_ref": ["b28", "b21", "b37", "b29", "b39", "b40", "b48", "b33", "b3", "b6", "b31", "b23", "b31", "b23", "b24", "b50", "b30", "b30", "b13", "b44", "b47", "b36", "b51"], "figure_ref": [], "table_ref": []}, {"heading": "Contributions.", "text": "(1) We introduce a new concept of achieving LDP in model explanations that simultaneously protects the top-k features from being exploited by attackers while maintaining the faithfulness of the explanations. Based on this principle, we propose a defense against explanationguided attacks on MLaaS, called XRAND, by devising a novel two-step LDP-preserving mechanism. First, at the aggregated explanation, we incorporate the explanation loss into the randomized probabilities in LDP to make top-k features indistinguishable to the attackers. Second, at the sample-level explanation, guided by the first step, we minimize the explanation loss on each sample while keeping the features at the aggregated explanation intact. ( 2) Then, we theoretically analyze the robustness of our defense against the XBA in MLaaS by establishing two certified robustness bounds in both training time and inference time. (3) Finally, we evaluate the effectiveness of XRAND in mitigating the XBA on cloud-hosted malware classifiers.\nOrganization. The remainder of the paper is structured in the following manner. Section 2 presents some background knowledge for our paper. Section 3 discusses the explanation-guided attacks against MLaaS and establishes the security model for our defense. Our defense, XRAND, is introduced in Section 4 where its certified robustness bounds are presented in Section 5. Experimental evaluation of our solution is given in Section 6. Section 7 discusses related work and Section 8 provides some concluding remarks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "Local explainers. The goal of model explanations is to capture the importance of each feature value of a given point of interest with respect to the decision made by the classifier and which class it is pushing that decision toward. Given a sample x \u2208 R d where x j denotes the j th feature of the sample, let f be a model function in which f (x) is the probability that x belongs to a certain class. An explanation of the model's output f (x) takes the form of an explanation vector w x \u2208 R d where the j th element of w x denotes the degree to which the feature x j influences the model's decision. In general, higher values of w xj imply a higher impact.\nPerturbation-based explainers, such as SHAP (Lundberg and Lee 2017), obtain an explanation vector w x for x via training a surrogate model of the form g(x) = w x0 + d j=1 w xj x j by minimizing a loss function L(f, g) that measures how unfaithful g is in approximating f .\n\u2022 Sample-level explanation. In the context of this paper, we refer to w x as a sample-level explanation. \u2022 Aggregated explanation. We denote an aggregated explanation w as the sum of explanation vectors across samples in a certain set X , i.e., w X = x\u2208X w x . When X is clear from the context, we shall use a shorter notation w.\nLocal Differential Privacy (LDP). LDP is one of the state-of-the-arts and provable approaches to achieve individual data privacy. LDP-preserving mechanisms (Erlingsson, Pihur, and Korolova 2014;Wang et al. 2017;Bassily and Smith 2015;Duchi, Jordan, and Wainwright 2018;Acharya, Sun, and Zhang 2019) generally build on the ideas of randomized response (RR) (Warner 1965). Definition 1. \u03b5-LDP. A randomized algorithm A satisfies \u03b5-LDP, if for any two inputs x and x , and for all possible outputs O \u2208 Range(A), we have:\nP r[A(x) = O] \u2264 e \u03b5 P r[A(x ) = O]\n, where \u03b5 is a privacy budget and Range(A) denotes every possible output of A.\nThe privacy budget \u03b5 controls the amount by which the distributions induced by inputs x and x may differ. A smaller \u03b5 enforces a stronger privacy guarantee.", "publication_ref": ["b13", "b44", "b5", "b12", "b0", "b46"], "figure_ref": [], "table_ref": []}, {"heading": "XAI-guided Attack Against MLaaS", "text": "We discuss how XAI can be used to gain insights into MLaaS models, and establish the threat model for our work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Exposing MLaaS via XAI", "text": "From a security viewpoint, releasing additional information about a model's mechanism is a perilous prospect. As a function of the model that is trained on a private dataset, an explanation may unintentionally disclose critical information about the training set, more than what is needed to offer a useful interpretation. Moreover, the explanations may also expose the internal mechanism of the black-box models. For example, first, the behavior of explanations varies based on whether the query sample was a member of the training dataset, making the model vulnerable to membership inference attacks (Shokri, Strobel, and Zick 2021). Second, the explanations can be coupled with the predictions to improve the performance of generative models which, in turn, strengthens some model inversion attacks (Zhao et al. 2021). Furthermore, releasing the explanations exposes how the black-box model acts upon an input sample, essentially giving up more information about its inner workings for each query, hence, model extractions attacks can be carried out with far fewer queries, as discussed in (Milli et al. 2019;Miura, Hasegawa, and Shibahara 2021).\nFinally, (Severi et al. 2021) argues that the explanations allow an adversary to gain insight into a model's decision boundary in a generic, model-agnostic way. The SHAP values can be considered as an approximation of the confidence of the decision boundary along each feature dimension. Hence, features with SHAP values that are near zero infer low-confidence areas of the decision boundary. On the other hand, features with positive SHAP values imply that they strongly contribute to the decision made by the model. As a result, it provides us with an indication of the overall orientation for each feature, thereby exposing how the model rates the importance of each feature.", "publication_ref": ["b31", "b50", "b23", "b24", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "XAI-guided Backdoor Attack against MLaaS", "text": "The XBA on malware classifiers (Severi et al. 2021) suggests that explanations make the model vulnerable to backdoor attacks, as they reveal the top important features. Thus, it is natural to mount this XBA against a black-box model in MLaaS where an explanation is returned for each user query.\nSystem Model. Fig. 1 illustrates the system model for our work. We consider an MLaaS system where a malware classifier is deployed on a cloud-based platform. For training, the system crowdsources threat samples via user-submitted binaries to assemble a set of outsourced data. This set of outsourced data is then combined with a set of proprietary data to construct the training data to train the malware classifier.\nWe denote D = {(x n , y n )} N n=1 as the set of proprietary training data. The dataset contains sample x n \u2208 R d and its ground-truth label y n \u2208 {0, 1}, where y n = 0 denotes a goodware sample, and y n = 1 denotes a malware sample. On input x, the model f : R d \u2192 R outputs the score f (x) \u2208 [0, 1]. This score is then compared with a threshold of 0.5 to obtain the predicted label for x.\nDuring inference time, given a query containing a binary sample x, the system returns the predicted label with a SHAP explanation w x for the decision (w x \u2208 R d ). We consider an adversary who plays the role of a user in this system and can send queries at his discretion. The adversary exploits the returned explanations to craft backdoor triggers that will be injected to the system via the crowdsourcing process, thereby poisoning the outsourced data.\nThreat Model. The attacker's goal is to alter the training procedure by injecting poisoned samples into the training data, generating poisoned training data such that the resulting backdoored classifier differs from a clean classifier. An ideal backdoored classifier has the same response to a clean input as the clean classifier, but it gives an attack-chosen prediction when the input is embedded with the trigger.\nOur defense assumes a strong adversary such that he can tamper with the training dataset at his discretion without major constraints. To prevent the adversary from setting arbitrary values for the features in the trigger, the set of values that can be used is limited to the ones that exist in the dataset. This threat model promotes a defense under worst-case scenarios from the perspective of the defender.\nCrafting backdoor triggers. To craft a backdoor trigger in XBA, the adversary tries to obtain the top goodware-oriented feature by querying classifier f with samples {x} x\u2208A from their dataset A and obtaining the SHAP explanation w x for each of them. The sum of the SHAP values across all queried samples w A =\nx\u2208A w x approximately indicates the importance of each feature, and whether it is goodware-or malware-oriented. From that, the attacker greedily selects a combination of the most goodwareoriented features to create the trigger (Severi et al. 2021).", "publication_ref": ["b30", "b30"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "XRAND -Local DP Defense", "text": "This section describes our defense, XRAND, a novel twostep explanation-guided randomized response (RR) mechanism. Our idea is to incorporate the model explainability into the randomization probabilities in XRAND to guide the aggregated explanation while minimizing the difference between the perturbed explanation's surrogate model g (x) and the model f (x) at the sample-level explanation. We call the difference between g (x) and f (x) an explanation loss L, quantified as follows:\nL = z\u2208N (x) (g (z) \u2212 f (z)) 2 exp \u2212 z \u2212 x 2 \u03c3 2 (1)\nwhere x is an input sample, its neighborhood N (x) is generated by the explainer's sampling method. This function captures the difference between the modified explainer's linear surrogate model g (x) and the model f (x), essentially measuring how unfaithful g is in approximating f . To defend against explanation-guided attacks that utilize the top-k features of the aggregated explanation (e.g., XBA exploits the top-k goodware-oriented features), our idea is to randomly disorder some top-k features under LDP guarantees, thereby protecting the privacy of those features. This raises the following question: What top-k features and which data samples should be randomized to optimize the explainability of data samples while guaranteeing that the attackers cannot infer the top-k features? Algorithm Overview. To answer this question, we first integrate the explanation loss caused by potential changes of features in the aggregated explanation into the randomized probabilities to adaptively randomize each feature in the topk. Then, we minimize the explanation loss on each sample while ensuring the order of the features at the aggregated explanation follows the results of the first step. By doing so, we are able to optimize the trade-off between the model explainability and the privacy budget \u03b5 used in XRAND, as verified both theoretically (Section 4.2) and experimentally (Section 6). The pseudo-code of XRAND is shown in Alg. 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "LDP-preserving Explanations", "text": "Step 1 (Alg. 1, lines 1-10). We first compute the aggregated explanation w over the samples of the proprietary dataset w = x\u2208D w x . Then we sort w in descending order and retain a mapping v : N \u2192 N from the sorted indices to the original indices. Given that \u03c4 is a predefined threshold to control the range of out-of-top-k features that some of top-k features can swap with, and \u03b2 is a parameter bounded in Theorem 1 under a privacy budget \u03b5, XRAND defines the Algorithm 1: XRAND: Explanation-guided RR mechanism Input: model f , dataset D, aggregated explanation w, \u03b5, k, \u03c4 , test sample x Output: S, \u03b5-LDP w x 1:\nStep 1 -At aggregated explanation:\n2: for x n \u2208 D do 3:\nCompute L(x n ) # using Eq. 1 4:\nfor i \u2208 [1, k], j \u2208 [k + 1, k + \u03c4 ] do 5:\nCompute L(x n )(i, j) # using Eq. 1 6:\nCompute \u2206 L (i, j) # using Eq. 3 7:\nend for 8:\nend for 9:\nRandomizing w: w \u2190 XRAND(w, \u03b5, k, \u03c4, \u2206 L (i, j)) # using Eq. 2 10:\nReturn S 11:\nStep 2 -At sample-level explanation:\n12: w x \u2190 SHAP explanation for x 13: w x \u2190 Solve the optimization problem in (5) 14:\nReturn w x probability of flipping a top-k feature i to an out-of-top-k feature j as follows:\n\u2200i \u2208 [1, k], j \u2208 [k + 1, k + \u03c4 ], \u03c4 \u2265 k : i = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 i, with probability p i = exp(\u03b2) exp(\u03b2) + \u03c4 \u2212 1 , j, with probability q i,j = \u03c4 \u2212 1 exp(\u03b2) + \u03c4 \u2212 1 q j (2\n)\nwhere q j = exp(\u2212\u2206 L (i,j)) t\u2208[k+1,k+\u03c4 ] exp(\u2212\u2206 L (i,t)) and \u2206 L (i, j) is the aggregated changes of L (Eq. 1) when flipping features i and j, which is calculated as follows:\n\u2206 L (i, j) = 1 N N n=1 (|L(x n ) \u2212 L(x n )(i, j)|)(3)\nwhere L(x n ) is the original loss L of a sample x n \u2208 D and L(x n )(i, j) is the loss L of the sample x n after flipping features i and j (Alg. 1, lines 3,5). After randomizing the aggregated explanation, we obtain the set S of features that need to be flipped in the aggregated explanation, as follows:\nS = {(i, j)|i and j are flipped, i \u2208 [1, k], j \u2208 [k + 1, k + \u03c4 ]} (4)\nStep 2 (Alg. 1, lines 11-14). For each input test sample x, we proceed with sample-level explanation for finding the noisy explanation w x . First, we generate a set of constraints\nQ = {(i, j)|w xi \u2264 w xj } that is sufficient for S.\nIn particular, for each pair (i, j) \u2208 S, we add the following pairs to Q:\n(v(i+1), v(j)); (v(j), v(i\u22121)); (v(i), v(j\u22121)); (v(j+1), v(i))\nGiven w x as the SHAP explanation of x, we aim to find \u03c6 \u2208 R d such that w x = w x + \u03c6 satisfies the constraints in Q while minimizing the loss L. To obtain \u03c6, we solve the following optimization problem:\nmin \u03c6 z\u2208N (x) (wx + \u03c6) T z \u2212 f (z) 2 exp \u2212 z \u2212 x 2 \u03c3 2 + \u03bb \u03c6 (5) s.t. wx i + \u03c6i \u2264 wx j + \u03c6j, \u2200(i, j) \u2208 Q \u03c6i = 0 \u2200i / \u2208 Q\nwhere \u03bb is a regularization constant.\nThe resulting noisy explanation will be w x = w x +\u03c6. This problem is convex and can be solved by convex optimization solvers (Kingma and Ba 2014;Diamond and Boyd 2016).", "publication_ref": ["b17", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Privacy Guarantees of XRAND", "text": "To bound privacy loss of XRAND, we need to bound \u03b2 in Eq. 2 such that the top-k features in the explanation w preserves LDP, as follows: Theorem 1. Given two distinct explanations w and w and a privacy budget i , XRAND satisfies \u03b5 i -LDP in randomizing each feature i in top-k features of w, i.e., P (XRAND(wi)=z|w)\nP (XRAND( wi)=z| w) \u2264 exp(\u03b5 i ), if: \u03b2 \u2264 \u03b5 i + ln(\u03c4 \u2212 1) + ln(min exp(\u2212\u2206 L (i, j)) k+\u03c4 t=k+1 exp(\u2212\u2206 L (i, t))\n)\nwhere z \u2208 Range(XRAND). Proof: See Appx. A.\nBased on Theorem 1, the total privacy budget \u03b5 to randomize all top-k features is the sum of all the privacy budget\ni , i.e., \u03b5 = k i=1 \u03b5 i , since each feature i is randomized in- dependently.\nFrom Theorem 1 and Eq. 2, it can be seen that as the privacy budget \u03b5 increases, \u03b2 can increase and the flipping probability q i,j decreases. As a result, we switch fewer features out of top-k.\nPrivacy and Explainability Trade-off. To understand the privacy and explainability trade-off, we analyze the data utility of XRAND mechanism through the sum square error (SSE) of the original explanation w and the one resulting from XRAND w . The smaller the SSE is, the better data utility the randomization mechanism achieves. Theorem 2. Utility of\nXRAND: SSE = x\u2208D d i=1 (w xi \u2212 w xi ) 2 = x\u2208D k+\u03c4 i=1 (w xi \u2212 w xi ) 2\n, where d is the number of features in the explanation.\nProof. It is easy to see that we only consider the probability of flipping the top-k features to be out-of-the-top-k up to the feature k+\u03c4 . Thus, all features after k+\u03c4 , i.e., from k+\u03c4 +1 to d are not changed. Hence the theorem follows.\nFrom the theorem, at the same \u03b5, the smaller the \u03c4 , the higher the data utility that XRAND achieves. Intuitively, if \u03c4 is large, it is more flexible for the top-k features to be flipped out, but it will also impair the model explainability since the original top-k features are more likely to be moved far away from the top k. With high values of \u03b5, we can obtain a smaller SSE, thus, achieving better data utility. The effect of \u03b5 and \u03c4 on the SSE value is illustrated in Fig. 6 (Appendix).\nOur proposed XRAND can be used as a defense against the XBA since it protects the top-k important features. We further establish the connection between XRAND with certified robustness against XBA. Given a data sample x: 1) In the training time, we guarantee that up to a portion of poisoning samples in the outsourced training data, XBA fails to change the model predictions; and 2) In the inference time, we guarantee that up to a certain backdoor trigger size, XBA fails to change the model predictions. A primer on certified robustness is given in Appx. B.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Training-time Certified Robustness", "text": "We consider the original training data D as the proprietary data, and the explanation-guided backdoor samples D o as the outsourced data inserted into the proprietary data. The outsourced data D o alone may not be sufficient to train a good classifier. In addition, the outsourced data inserted into propriety data can lessen the certified robustness bound of the propriety data. Therefore, we cannot quantify the certified poisoning training size of the outsourced data D o directly by applying a bagging technique (Jia, Cao, and Gong 2021). To address this problem, we quantify the certified poisoning training size r of D o against XBA by uncovering its correlation with the poisoned training data\nD = D \u222a D o .\nGiven a model prediction on a data sample x using D, denoted as f (D, x), we ask a simple question: \"What is the minimum number poisoning data samples, i.e. ", "publication_ref": ["b16"], "figure_ref": [], "table_ref": []}, {"heading": "Inference-time Certified Robustness", "text": "It is not straightforward to adapt existing certified robustness bounds at the inference-time into XRAND, since there is a gap between model training as in existing approaches (Jia, Cao, and Gong 2021;Lecuyer et al. 2019;Phan et al. 2020) and the model training with explanation-guided poisoned data as in our system. Existing approaches can randomize the data input x and then derive certified robustness bounds given the varying output. This typical process does not consider the explanation-guided poisoned data that can potentially affect certified robustness bounds in our system. To address this gap, note that we can always find a mechanism to inject random noise into the data samples x such that the samples achieve the same level of DP guarantee as the explanations. Based on this, we can generalize existing certified bounds against XBA at the inference time in XRAND.\nWhen explanation-guided backdoor samples are inserted into the training data, upon bounding the sensitivity that the backdoor samples change the output of f , there always exists a noise \u03b1 that can be injected into a benign sample x, i.e., x + \u03b1, to achieve an equivalent \u03b5-LDP protection. Given the explanation w x of x, we focus on achieving a robustness condition to L p (\u00b5)-norm attacks, where \u00b5 is the radius of the norm ball, formulated as follows:\n\u2200\u03b1 \u2208 L p (\u00b5) : f l (x + \u03b1|w x ) > f \u00acl (x + \u03b1|w x ) (7)\nwhere l \u2208 {0, 1} is the true label of x and \u00acl is the NOT operation of l in a binary classification problem.\nThere should exist a correlation among \u03b1 and w x that needs to be uncovered in order to bound the robustness condition in Eq. 7. However, it is challenging to find a direct mapping function M : w x \u2192 \u03b1 so that when we randomize w x , the change of \u03b1 is quantified. We address this challenge by quantifying the sensitivity of \u03b1 given the average change of the explanation of multiple samples x \u2208 X , as follows:\n\u2206 \u03b1|w = 1 |X |d x\u2208X |w x \u2212 w x | 1 (8)\nwhere |X | is the size of X . \u2206 \u03b1|w can be considered as a bounded sensitivity of XRAND given the input x since: (1) We can achieve the same DP guarantee by injecting Laplace or Gaussian noise into the input x using the sensitivity \u2206 \u03b1|w ; and (2) The explanation perturbation happens only once and is permanent, that is, there is no other bounded sensitivity associated with the one-time explanation perturbation. The sensitivity \u2206 \u03b1|w establishes a new connection between explanation perturbation and the model sensitivity given the input sample x. That enables us to derive robustness bounds using different techniques, i.e., PixelDP (Lecuyer et al. 2019;Phan et al. 2019) and (boosting) randomized smoothing (RS) (Horv\u00e1th et al. 2022;Cohen, Rosenfeld, and Kolter 2019), since we consider the sensitivity \u2206 \u03b1|w as a part of randomized smoothing to derive and enhance certified robustness bounds.\nThe rest of this section only discusses the bound using PixelDP, we refer the readers to Appx. D for the bound using boosing RS. Given a randomized prediction f (x) satisfying (\u03b5, \u03b4)-PixelDP w.r.t. a L p (\u00b5)-norm metric, we have:\n\u2200l \u2208 {0, 1}, \u2200\u03b1 \u2208 L p (\u00b5) : Ef l (x) \u2264 e \u03b5 Ef l (x + \u03b1) + \u03b4 (9)\nwhere Ef l (x) is the expected value of f l (x), \u03b5 is a predefined privacy budget, and \u03b4 is a broken probability. When we use a Laplace noise, \u03b4 = 0.\nWe then apply PixelDP with the sensitivity \u2206 \u03b1|w and a noise standard deviation \u03c3 = max \u00b5\u2208R + \u00b5 such that the generalized robustness condition (Eq. 9) holds, the prediction on x using XRAND is robust against the XBA up to \u00b5 max . As a result, we have a robustness certificate of \u00b5 max for x.", "publication_ref": ["b16", "b19", "b26", "b19", "b27", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "To evaluate the performance of our defense, we conduct the XBA proposed by (Severi et al. 2021) against the explanations returned by XRAND to create backdoors on cloudhosted malware classifiers. Our experiments aim to shed light on understanding (1) the effectiveness of the defense in mitigating the XBA, and (2) the faithfulness of the explanations returned by XRAND. The experiments are conducted using LightGBM (Anderson and Roth 2018) and EmberNN (Severi et al. 2021) classification models that are trained on the EMBER (Anderson and Roth 2018) dataset. A detailed description of the experimental settings can be found in Appx. E. We quantify XRAND via the following metrics:\nAttack success rate. This metric is defined as the portion of trigger-embedded malware samples that are classified as goodware by the backdoored model. Note that we only embed the trigger into malware samples that were classified correctly by the clean model. The primary goal of our defense is to reduce this value.\nLog-odds. To evaluate the faithfulness of our XRAND explanation, we compare the log-odds score of the XRAND explanations with that of the ones originally returned by SHAP. Based on the explanation of a sample, the log-odds score is computed by identifying the top 20% important features that, if erased, can change the predicted label of the sample (Shrikumar, Greenside, and Kundaje 2017). Then we obtain the change in log-odd of the prediction score of the original sample and the sample with those features erased. The higher the log-odds score, the better the explanation in terms of identifying important features. To maintain a faithful explainability, the XRAND explanations should have comparable log-odds scores as the original SHAP explanations.\nAttack Mitigation. We observe the attack success rate of XBA when our XRAND explanations are used to craft backdoor triggers. We set k to be equal to the trigger size of the attack, and fix the predefined threshold \u03c4 = 50. Fig. 2 highlights the correlation between the attack success rate and the privacy budget \u03b5 in our defense. Intuitively, the lower  the \u03b5, the more obfuscating the top goodware-oriented features become. Hence, Figs. 2a and 2b show that the attack success rate is greatly diminished as we tighten the privacy budget, since the attacker has less access to the desired features. Moreover, in a typical backdoor attack, injecting more poisoned samples into the training data makes the attack more effective. Such behavior is exhibited in both Light-GBM (Fig. 2a) and EmberNN (Fig. 2b), though EmberNN is less susceptible to the increase of poisoned data. However, in practice, the attacker wishes to keep the number of poisoning samples relatively low to remain stealthy. At a 1% poison rate, our defense manages to reduce the attack success rate from 77.8% to 10.2% with \u03b5 = 1.0 for LightGBM. It performs better with EmberNN where the attack success rate is reduced to 5.3% at \u03b5 = 10.0. Additionally, we examine the effect of the trigger sizes on our defense. The trigger size denotes the number of features that the attacker modifies to craft poisoned samples. We vary the trigger size consistently with previous work (Severi et al. 2021). In backdoor attacks, a larger trigger makes the backdoored model more prone to misclassification, thus improving the attack success rate. Fig. 3 shows that the attack works better with large triggers on both models, however, as aforementioned, the attacker would prefer small trigger sizes for the sake of stealthiness. This experiment shows that, for the trigger sizes that we tested, our proposed defense can successfully maintain a low attack success rate given a wide range of the privacy budget \u03b5 \u2208 [0.1, 10] (Fig. 2a, 2b). We refer the readers to Appx. E for more experimental results on additional malware datasets and the evaluation of our robustness bounds. In short, regarding the training-time bound, we observe that a smaller privacy budget \u03b5 \u2208 [0.1, 10] results in a more robust model against the XBA. As for the inference-time bound, we obtain high certified accuracy (Eq. 29) under rigorous privacy budgets, i.e., 89.17% and 90.42% at \u03b5 \u2208 [0.1, 1.0]. We notice that the PixelDP-based bound attains a stronger performance than using boosting RS. This is because boosting RS is not designed for models trained on backdoored data like XRAND.\nFaithful Explainability. From the previous results, we observe that a wide range of the privacy budget \u03b5 \u2208 [0.1, 10] provides a good defense against the XBA. The question remains whether the explanations resulting from these values of \u03b5 are still faithful. Fig. 4 shows the log-odds score of the original explanations returned by SHAP and of the ones after applying our XRAND mechanism. The XRAND explanations at \u03b5 = 1.0, 10.0 have comparable log-odds scores to those of SHAP. This is because our defense works with small values of k (e.g., k = 10). Therefore, XRAND only randomizes the SHAP values within a small set of top goodwareoriented features. As a result, XRAND can still capture the important features in its explanations.\nFurthermore, we visualize the explanation of a test sample before and after applying XRAND in Figs. 5a and 5b, respectively. As can be seen, the SHAP values of the two explanations largely resemble one another, except for minor differences in less than 10 features (out of 2,351 features). Importantly, the XRAND explanation evidently manifests similar sets of important malware and goodware features as the original explanation by SHAP, which also explains the comparable log-odds score in Fig. 4. More visualizations on XRAND can be found in Appx. F.", "publication_ref": ["b30", "b1", "b30", "b33", "b30"], "figure_ref": ["fig_2", "fig_2", "fig_2", "fig_2", "fig_3", "fig_2", "fig_4", "fig_5", "fig_4"], "table_ref": []}, {"heading": "Related Work", "text": "Vulnerability of Explanations and Backdoor Defenses. There are two main lines of research in studying the vulnerability of model explanations. First, model explainers are prone to adversarial manipulations, thereby making the explanations unreliable and untrustworthy (Heo, Joo, and Moon 2019;Slack et al. 2020;Zhang et al. 2020;Ghorbani, Abid, and Zou 2019). Second, the information leaked through the explanations can be exploited to conduct several attacks against the black-box models (Shokri, Strobel, and Zick 2021;Milli et al. 2019;Miura, Hasegawa, and Shibahara 2021;Zhao et al. 2021;Severi et al. 2021).\nOur work focuses on the second direction in which we show that MLaaS is particularly susceptible to this type of attack. Although such vulnerability has been studied before, the discussion on its defense mechanism is still limited. To that extent, we propose a new concept of LDP explanation such that we can obfuscate the explanations in a way that limits the knowledge that can be leveraged by the attackers while maintaining the faithfulness of the explanations.\nMoreover, several defenses have been proposed to tackle the backdoor attacks (Liu, Dolan-Gavitt, and Garg 2018;Tran, Li, and Madry 2018;Wang et al. 2019a). As evidenced by (Severi et al. 2021) that the backdoors created by the XBA remain stealthy against these defenses. Thus, our XRAND is the first solution that can tackle XBA in MLaaS.\nLDP Mechanisms. Existing LDP mechanisms can be classified into four lines. 1) Direct encoding approaches (Warner 1965;Kairouz, Bonawitz, and Ramage 2016) are to directly apply RR mechanisms to a binary response to output the true and false responses with different probabilities. 2) Generalized RR approaches (Duchi and Rogers 2019;Wang et al. 2019b;Zhao et al. 2020) directly perturb the numerical (real) value of data into a finite set or continuous range of output values. 3) Hash-based approaches (Erlingsson, Pihur, and Korolova 2014;Wang et al. 2017;Bassily and Smith 2015;Acharya, Sun, and Zhang 2019;Wang et al. 2019c). 4) Binary encoding-based approaches (Arachchige et al. 2019;Lyu et al. 2020) converts input x into a binary encoding vector of 0 and 1, then independently applies RR mechanisms on these bits. Despite the effectiveness of these RR mechanisms, existing approaches have not been designed for XBA, in which the explanations expose the training data background.", "publication_ref": ["b15", "b34", "b49", "b14", "b31", "b23", "b24", "b50", "b30", "b20", "b38", "b42", "b30", "b46", "b16", "b11", "b43", "b51", "b13", "b44", "b5", "b0", "b45", "b2", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we have shown that, although explanations help improve the understanding and interpretability of black-box models, they also leak essential information about the inner workings of the models. Therefore, the black-box models become more vulnerable to attacks, especially in the context of MLaaS where the prediction and its explanation are returned for each query. With a novel two-step LDP-preserving mechanism, we have proposed XRAND to protect the model explanations from being exploited by adversaries via obfuscating the top important features, while maintaining the faithfulness of explanations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Proof of Theorem 1", "text": "Proof. Let us denote w i as the SHAP score of feature i in the aggregated explanation. Given the two explanations w and w that can be different at any feature and any possible output z \u2208 Range(XRAND), where Range(XRAND) denotes every possible output of XRAND, we have:\nP (XRAND(w i ) = z) P (XRAND( w i ) = z) \u2264 max P (XRAND(w i ) = z) min P (XRAND( w i ) = z) = exp(\u03b2) exp(\u03b2)+\u03c4 \u22121 min( exp(\u2212\u2206 L (i,j)) t\u2208[k+1,k+\u03c4 ] exp(\u2212\u2206 L (i,t)) \u03c4 \u22121 exp(\u03b2)+\u03c4 \u22121 ) = exp(\u03b2) (\u03c4 \u2212 1) min( exp(\u2212\u2206 L (i,j)) t\u2208[k+1,k+\u03c4 ] exp(\u2212\u2206 L (i,t)) ) \u2264 exp(\u03b5 i )(10)\nTaking a natural logarithm of Eq. 10, we obtain:\nln( exp(\u03b2) (\u03c4 \u2212 1) min( exp(\u2212\u2206 L 2 (i,j)) k+\u03c4 t=k+1 exp(\u2212\u2206 L 2 (i,t)) ) ) \u2264 ln(exp(\u03b5 i )) \u21d4 \u03b2 \u2264 \u03b5 i + ln(\u03c4 \u2212 1) + ln(min exp(\u2212\u2206 L (i, j)) k+\u03c4 t=k+1 exp(\u2212\u2206 L (i, t))\n)\nConsequently, Theorem 1 holds.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B A Primer on Certified Robustness", "text": "The ultimate goal of certified robustness is to guarantee consistency on the model performance under data perturbation.\nIn specific, it has to ensure that a small perturbation in the input does not change the predicted label. Given a benign example x, the robustness condition to l p (\u00b5)-norm attacks can be stated as follows:\n\u2200\u03b1 \u2208 l p (\u00b5) : f i (x + \u03b1) > max j:j =i f j (x + \u03b1) (11\n)\nwhere i is the ground-truth label of the sample x. The condition essentially indicates that a small perturbation \u03b1 in the input does not change the predicted label i.\nPixelDP (Lecuyer et al. 2019). To achieve the robustness condition in Eq. 11, Lecuyer et al. (Lecuyer et al. 2019) introduce an algorithm, called PixelDP. By considering an input x as a database in DP parlance, and individual features as tuples, PixelDP shows that randomizing the function f (x) to enforce DP on a small number of features in the input sample guarantees the robustness of predictions. To randomize f (x), random noise \u03c3 r is injected into either input x or an arbitrary hidden layer, resulting in the following ( r , \u03b4 r )-PixelDP condition: Lemma 1. ( r , \u03b4 r )-PixelDP (Lecuyer et al. 2019). Given a randomized scoring function f (x) satisfying ( r , \u03b4 r )-PixelDP w.r.t. a l p -norm metric, we have:\n\u2200i, \u2200\u03b1 \u2208 l p (1) : Ef i (x) \u2264 e r Ef i (x + \u03b1) + \u03b4 r (12\n)\nwhere Ef i (x) is the expected value of f i (x), r is a predefined budget, \u03b4 r is a broken probability.\nAt the prediction time, a certified robustness check is implemented for each prediction, as follows:\nE lb f i (x) > e 2 r max j:j =i\u00ca ub f j (x) + (1 + e r )\u03b4 r (13)\nwhere\u00ca lb and\u00ca ub are the lower and upper bounds of the expected value\u00caf (x) = 1 n n f (x) n , derived from the Monte Carlo estimation with an \u03b7-confidence, given n is the number of invocations of f (x) with independent draws in the noise \u03c3 r . Passing the check for a given input guarantees that no perturbation up to l p (1)-norm (where 1 is the radius of the l p -norm ball) can change the model's prediction.", "publication_ref": ["b19", "b19", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Boosting Randomized Smoothing (Horv\u00e1th et al. 2022).", "text": "Another state-of-the-art approach to certifying robustness is boosting randomized smoothing (RS). In this scheme, an ensemble modelf of M classifiers trained on the same dataset with different random seeds (same structures and settings). We denote p 1 as the success probability that the ground-truth label l is correctly predicted. Let c i and c j,j =i be the expected values of the clean model over the randomness of the training process, such as c i = E i (f i (x)) and c j,j =i = E j (f j (x)). Let t i = ci\u2212cj \u03c3i(M ) and z i be the probability distribution of the classification margin over class i, we have the following robustness condition\np 1 := P f (x + \u03b1) = i \u2265 1 \u2212 P |z i \u2212 (c i \u2212 c j )| \u2265 t i \u03c3 i (M ) \u2265 1 \u2212 j,j =i \u03c3 2 j (M ) (c i \u2212 c j ) 2(14)\nwhere \u03c3 i (M ) is the variance of the classification margin z i . It is shown in (Horv\u00e1th et al. 2022) that when \u03c3(M ) decreases (i.e., a better certified robustness bound), the number of ensemble models M increases, and the lower bound of the success probability p 1 approaches the ground-truth label l.\nGiven the success probability p 1 , confidence \u03b3, number of samples N , and perturbation variance \u03c3 2 \u03b1 (up to an incorrect prediction), the probability distribution over the certified radius (R) is defined as follows:\nP R = \u03c3 \u03b1 \u03a6 \u22121 p 1 (N 1 , N, \u03b1) = B(N 1 , N, p 1 ) (15)\nwhere \u03a6 \u22121 denotes the inverse Gaussian CDF, B(N 1 , N, p 1 ) is the probability of drawing N 1 successes in N trials from a Binomial distribution with the success probability p 1 and p 1 is the lower bound to the success probability of a Bernoulli experiment given N 1 success in N trials with confidence \u03b1 according to the Clopper-Pearson interval (Clopper and Pearson 1934).\nThe certified robustness bound is R * = arg max R such that the condition in Eq. 14 is satisfied.\nBagging ensemble learning. For the certified robustness at the training-time against XBA, we leverage the bagging ensemble learning method-based certified robustness bounds, which have been demonstrated to be effective in defending against backdoor data poisoning attacks (Jia, Cao, and Gong 2021). The ensemble learning approach trains a set of base models and leverages a majority vote to quantify the difference between the lower bound of the class with the highest probability and the upper bound of the class with the second highest probability. Base upon that, one can identify the minimum number of poisoning data samples, called certified poisoning training size, that can change the majority vote. We call s(D) as a random subsample data that are sampled from D with replacement uniformly at random. We denote p l as the label probability, in which p l = Pr[f (s(D), x) = l)] is the probability that the learned base model predicts label l for x. The ensemble classifier h predicts the label with the largest label probability for x, as:\nh(D, x) = arg max l\u2208{0,1} p l (18\n)\nTo prove the certified robustness of the mechanism h(D, x) against XBA, we need to find certified poisoning training size, which is the minimum number of poisoning training data such that the ensemble classifier changes the prediction for x. Formally, we find the minimum r D such that the following inequality is satisfied for \u2200D + \u2208 B(D, r): h(D + , x) = l \u21d4 p l < p \u00acl (19) where l \u2208 {0, 1} and \u00acl is the NOT operation of l in the binary classification.\nFinding exact values of p l and p \u00acl is difficult. Instead of that, we find the lower bound of p l and upper bound of p \u00acl (l is the true label of x), we construct regions in the space \u2126, which is the joint space of X = s(D) and Y = s(D + ), satisfying the conditions of the Neyman-Pearson Lemma (Neyman and Pearson 1933). This enables us to derive the lower and upper bounds in these regions. Suppose we have a lower bound p l of the largest label probability p l and an upper bound p \u00acl of the second largest label probability p \u00acl when the classifier is trained on the clean training dataset. Formally, p l and p \u00acl satisfy:\np l \u2265 p l \u2265 p \u00acl \u2265 p \u00acl (20)\nFollowing Theorem 1 in (Jia, Cao, and Gong 2021), we can have the following certified poisoning training size r D as follows:\nCertified poisoning training size r * D . Given a training dataset D, a model f (\u2022), and a testing sample x, the ensemble classifier h is defined in Eq. 18. Suppose l is the label with the largest probability predicted by h for x and \u00acl is the NOT operation of l in the binary classification. We also have p l and p \u00acl satisfy Eq. 20. The h does not predict the label l for x when the certified poisoning training size r D is bounded by r * D , i.e., we have:\nh(D + , x) = l, \u2200D + \u2208 B(D, r * D ),(21)\nwhere r * D is the solution to the following optimization problem:\nr * D = arg max r D r D (22) s.t. max |D|\u2212r D \u2264|D+|\u2264|D|+r D ( |D + | |D| ) k \u2212 2 max(|D, |D + |) \u2212 r D |D| k + 1 \u2212 (p l \u2212 p \u00acl \u2212 \u03c3 l \u2212 \u03c3 \u00acl ) < 0 (23\n)\nwhere \u03c3 l = p l \u2212 ( p l n k )/n k and \u03c3 \u00acl = p \u00acl n k /n k \u2212 p \u00acl .\nSolving the problem in Eqs. 22 and in Eq. 23 (Jia, Cao, and Gong 2021), we obtain the following results:\nr * D = |D| k 1 + (p l \u2212 p \u00acl \u2212 \u03c3 l \u2212 \u03c3 \u00acl ) \u2212 1 (24)\nCertified poisoning training size r * D . Similar to find the certified poisoning training size r * D , we obtain:\nr * D = |D | k 1 + (p l \u2212 p \u00acl \u2212 \u03c3 l \u2212 \u03c3 \u00acl ) \u2212 1 (25\n)\nwhere \u03c3 l = p l \u2212 ( p l n k )/n k and \u03c3 \u00acl = p \u00acl n k /n k \u2212 p \u00acl . Certified poisoning training size r. Intuitively, if D o does not consist of poisoning data examples, then r D is expected to be relatively the same with r D . Otherwise, r D can be significantly smaller than r D indicating that D o is heavily poisoned with at least r = r D \u2212 r D number of poisoning data samples towards opening backdoors on x. Therefore, after obtaining r * D and r * D , we have:\nr = r * D \u2212 r * D (26)\nTightness of the certified poisoning training size r. The bound in Theorem 3 is tight and there is no existing smaller value of r for the XBA to be successfully carried out. In our bound for the poisoning training size, the propriety data D cannot be changed by XBA; hence, r * D is fixed. In addition, r * D is the minimum value that the prediction can be changed, so it is considered as a fixed value when the outsourced data D remains the same, and it can be changed if the outsourced data is changed. For example, if one more poisoning sample is added into D o , so r * D becomes r * D +1, then the minimum value is also changed to be r * D + 1. As a result, the certified robustness bound derived in Eq. 6 is tight. D Certified Robustness Bound using boosting randomized smoothing.\nDirectly applying the boosting RS (Appx. B) to XRAND by only using the noise \u03b1 that is associated with \u2206 \u03b1|w may not be effective. In the boosting RS, different base models (i.e., same structures and settings, just different random seeds when training), are trained on clean data; meanwhile, in XRAND, the training data is backdoored data. As a result, the backdoored (and noisy) data in XRAND along with different base models in the boosting RS may increase the variance of the outputs and then narrows down the certified robustness bound, which reduces the size of the bound. This results in a gap between the boosting RS and XRAND. To close this gap, we add a smaller amount of noise, namely \u03b1 1 , into the input in addition to the noise \u03b1 associated with \u2206 \u03b1|w , as follows:\np 1 := P f (x + \u03b1 + \u03b1 1 ) = l \u2265 1 \u2212 \u03c3 2 new (M ) (c l \u2212 c \u00acl ) 2 (27) Similarly, \u03c3 2\nnew is the variance of the classification margin, which is computed as in (Horv\u00e1th et al. 2022) associated with the new noise (\u03b1 + \u03b1 1 ). Given the success probability p 1 in Eq. 27 and the perturbation variance \u03c3 2 \u03b1+\u03b11 (up to an incorrect prediction), the probability distribution over the certifiable radius R is computed as follows:\nP R = \u03c3 \u03b1+\u03b11 \u03a6 \u22121 p 1 (N 1 , N ) = B(N 1 , N, p 1 ) (28)\nwhere B(N 1 , N, p 1 ) is the probability of drawing N 1 successes in N trials from a Binomial distribution with success probability p 1 and p 1 (N 1 , N ) is the lower bound of the success probability of a Bernoulli experiment given N 1 successes in N trials. The certified robustness bound is R * = arg max R such that the robustness condition in Eq. 27 is satisfied.", "publication_ref": ["b7", "b16", "b25", "b16", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "E Experimental settings and results", "text": "Platform. Our experiments in this paper are implemented using Python 3.8 and conducted on a single GPU-assisted compute node that is installed with a Linux 64-bit operating system. The allocated resources include 8 CPU cores (AMD EPYC 7742 model) with 2 threads per core, and 100GB of RAM. The node is also equipped with 8 GPUs (NVIDIA DGX A100 SuperPod model), with 80GB of memory per GPU. As mentioned in our threat model, we do not restrict the set of features that can be used by the attacker as backdoor triggers, so that our defense can be assessed against the strongest adversary. Experimental results on Contagio and Drebin. We conduct additional experiments on the Contagio PDF (Smutz and Stavrou 2012) and Drebin (Kumar et al. 2018) datasets. The Contagio PDF dataset contains 10,000 PDF files equally split between goodware and malware, each sample is represented by a 135-dimensional feature vector. The Drebin  dataset consists of 5,560 malware and 123,453 goodware Android apps. Each sample contains 545,333 features extracted from the applications. We use a Random Forest classifier for Contagio and a Linear Support Vector Machine for Drebin, and fix the trigger size to 30 features. The classifiers are released by (Severi et al, 2021) and we keep the same experimental settings. Figure 7 shows the attack success rate of XBA using the explanations returned by XRAND.\nOn the Contagio and Drebin datasets, we observe similar behavior to our experiments with the EMBER dataset. The attack success rate decreases as we tighten the privacy budget, since the attacker has less access to the desired features. At 1% poison rate and \u03b5 = 10.0, XRAND can maintain a low success rate of 9.6% in the Contagio dataset (Fig. 7a) and 7% in the Drebin dataset (Fig. 7b).\nCertified Robustness. To evaluate the certified robustness mechanism, as in (Lecuyer et al. 2019;Jia, Cao, and Gong 2021) we use the following metric:\ncertified accuracy = |test| n=1 isCorrect(Xn) & isRobust(Xn) |test| (29\n)\nwhere |test| is the number of testing samples, isCorrect(\u2022) returns 1 if the model makes a correct prediction (else, returns 0), and isRobust(\u2022) returns 1 if the robustness size is larger than a given attack size L (else, returns 0). When running with PixelDP and Boosting RS, we adopt the hyperparameter settings from the papers (Lecuyer et al. 2019) and(Horv\u00e1th et al. 2022), respectively. To verify the certified robustness at the training time, we conduct experiments with a wide range of \u03b5 \u2208 [0.1, 100.0]. In this setting, we create 6, 000 poisoned samples by adding the trigger that follows the two-step LDP-preserving mechanism. From Fig. 8, we observe that the smaller privacy budget \u03b5, the higher certified accuracy. In fact, the smaller \u03b5 imposes more noise which provides a better LDP guarantee and the model trained with noisier data is more robust against the backdoor attacks; hence, resulting in higher certified accuracy. It is obvious that the certified accuracy decreases when the threshold number of poisoning training samples r tr increases, since the term isRobust(\u2022) decreases. For the inference-time bound, with the boosting RS certified robustness, we also conduct experiments with a wide range of privacy budget \u03b5 \u2208 [0.1, 50.0], the noise level \u03c3 \u03b1+\u03b11 \u2208 [0.25, 1.0], and the radius r \u2208 [0.25, 2.0]. We consider M = 5 base classifiers in the ensemble model of the boosting RS. With PixelDP, we conduct experiments given tight privacy budgets, i.e., \u03b5 \u2208 [0.1, 1.0]. We obtain high certified accuracy for these tight privacy budgets, i.e., 89.17% and 90.42% for \u03b5 \u2208 [0.1, 1.0], compared with 50% obtained with the boosting RS. Here, we are dealing with a binary classification problem; therefore, the variance of the model output is smaller than a multi-class classification problem. That potentially affects the effectiveness of the boosting RS.", "publication_ref": ["b35", "b18", "b30", "b19", "b16", "b19"], "figure_ref": ["fig_8", "fig_8", "fig_8", "fig_9"], "table_ref": []}, {"heading": "F Visualizing XRAND", "text": "Figures 9, 10, and 11 visualize the explanations of some test samples before and after applying XRAND. For the most part, the explanations from SHAP and XRAND largely resemble one another. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "This material is based upon work supported by the National Science Foundation under grants IIS-2123809, CNS-1935928, and CNS-1935923.   ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Hadamard response: Estimating distributions privately, efficiently, and with little communication", "journal": "", "year": "2019", "authors": "J Acharya; Z Sun; H Zhang"}, {"ref_id": "b1", "title": "EMBER: An Open Dataset for Training Static PE Malware Machine Learning Models", "journal": "", "year": "2018", "authors": "H S Anderson; P Roth"}, {"ref_id": "b2", "title": "Local differential privacy for deep learning", "journal": "IEEE Internet of Things Journal", "year": "2019", "authors": "P C M Arachchige; P Bertok; I Khalil; D Liu; S Camtepe; M Atiquzzaman"}, {"ref_id": "b3", "title": "Model Interpretability (preview) -azure machine learning", "journal": "", "year": "2021", "authors": " Azure"}, {"ref_id": "b4", "title": "", "journal": "", "year": "", "authors": " //Aka.Ms/Azuremlmodelinterpretability"}, {"ref_id": "b5", "title": "Local, private, efficient protocols for succinct histograms", "journal": "", "year": "2015", "authors": "R Bassily; A Smith"}, {"ref_id": "b6", "title": "Ai explainability 360", "journal": "", "year": "2021", "authors": " Bluemix"}, {"ref_id": "b7", "title": "The use of confidence or fiducial limits illustrated in the case of the binomial", "journal": "Biometrika", "year": "1934", "authors": "C Clopper; E Pearson"}, {"ref_id": "b8", "title": "Certified adversarial robustness via randomized smoothing", "journal": "", "year": "2019", "authors": "J Cohen; E Rosenfeld; Z Kolter"}, {"ref_id": "b9", "title": "Explainable Machine Learning Challenge", "journal": "", "year": "2018", "authors": "F Community"}, {"ref_id": "b10", "title": "CVXPY: A Pythonembedded modeling language for convex optimization", "journal": "Journal of Machine Learning Research", "year": "2016", "authors": "S Diamond; S Boyd"}, {"ref_id": "b11", "title": "Lower bounds for locally private estimation via communication complexity", "journal": "", "year": "2019", "authors": "J Duchi; R Rogers"}, {"ref_id": "b12", "title": "Minimax optimal procedures for locally private estimation", "journal": "Journal of the American Statistical Association", "year": "2018", "authors": "J C Duchi; M I Jordan; M J Wainwright"}, {"ref_id": "b13", "title": "Rappor: Randomized aggregatable privacy-preserving ordinal response", "journal": "", "year": "2014", "authors": "U Erlingsson; V Pihur; A Korolova"}, {"ref_id": "b14", "title": "Interpretation of neural networks is fragile", "journal": "", "year": "2019", "authors": "A Ghorbani; A Abid; J Zou"}, {"ref_id": "b15", "title": "Fooling neural network interpretations via adversarial model manipulation", "journal": "", "year": "2019", "authors": "J Heo; S Joo; T Moon"}, {"ref_id": "b16", "title": "Intrinsic certified robustness of bagging against data poisoning attacks", "journal": "", "year": "2016", "authors": "J Jia; X Cao; N Z Gong;  Aaai; P Kairouz; K Bonawitz; D Ramage"}, {"ref_id": "b17", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "D P Kingma; J Ba"}, {"ref_id": "b18", "title": "Effective and explainable detection of android malware based on machine learning algorithms", "journal": "", "year": "2018", "authors": "R Kumar; Z Xiaosong; R U Khan; J Kumar; I Ahad"}, {"ref_id": "b19", "title": "Certified robustness to adversarial examples with differential privacy", "journal": "", "year": "2019", "authors": "M Lecuyer; V Atlidakis; R Geambasu; D Hsu; Jana ; S "}, {"ref_id": "b20", "title": "Fine-pruning: Defending against backdooring attacks on deep neural networks", "journal": "Springer", "year": "2018", "authors": "K Liu; B Dolan-Gavitt; S Garg"}, {"ref_id": "b21", "title": "A unified approach to interpreting model predictions", "journal": "", "year": "2017", "authors": "S M Lundberg; S.-I Lee"}, {"ref_id": "b22", "title": "Towards differentially private text representations", "journal": "", "year": "2020", "authors": "L Lyu; Y Li; X He; T Xiao"}, {"ref_id": "b23", "title": "Model reconstruction from model explanations", "journal": "", "year": "2019", "authors": "S Milli; L Schmidt; A D Dragan; M Hardt"}, {"ref_id": "b24", "title": "MEGEX: Data-Free Model Extraction Attack against Gradient-Based Explainable AI", "journal": "", "year": "2021", "authors": "T Miura; S Hasegawa; T Shibahara"}, {"ref_id": "b25", "title": "On the problem of the most efficient tests of statistical hypotheses", "journal": "Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character", "year": "1933", "authors": "J Neyman; E Pearson"}, {"ref_id": "b26", "title": "Scalable differential privacy with certified robustness in adversarial learning", "journal": "", "year": "2020", "authors": "H Phan; M Thai; H Hu; R Jin; T Sun; D Dou"}, {"ref_id": "b27", "title": "Heterogeneous Gaussian mechanism: preserving differential privacy in deep learning with provable robustness", "journal": "", "year": "2019", "authors": "N Phan; M N Vu; Y Liu; R Jin; D Dou; X Wu; M T Thai"}, {"ref_id": "b28", "title": "Explaining the predictions of any classifier", "journal": "", "year": "2016", "authors": "M T Ribeiro; S Singh; C Guestrin"}, {"ref_id": "b29", "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization", "journal": "", "year": "2017", "authors": "R R Selvaraju; M Cogswell; A Das; R Vedantam; D Parikh; D Batra"}, {"ref_id": "b30", "title": "Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers", "journal": "", "year": "2021", "authors": "G Severi; J Meyer; S Coull; A Oprea"}, {"ref_id": "b31", "title": "On the privacy risks of model explanations", "journal": "", "year": "2021", "authors": "R Shokri; M Strobel; Y Zick"}, {"ref_id": "b32", "title": "AAAI/ACM Conference on AI, Ethics, and Society", "journal": "", "year": "", "authors": ""}, {"ref_id": "b33", "title": "Learning important features through propagating activation differences", "journal": "PMLR", "year": "2017", "authors": "A Shrikumar; P Greenside; A Kundaje"}, {"ref_id": "b34", "title": "Fooling lime and shap: Adversarial attacks on post hoc explanation methods", "journal": "", "year": "2020", "authors": "D Slack; S Hilgard; E Jia; S Singh; H Lakkaraju"}, {"ref_id": "b35", "title": "Malicious PDF detection using metadata and structural features", "journal": "", "year": "2012", "authors": "C Smutz; A Stavrou"}, {"ref_id": "b36", "title": "LDP-FL: Practical private aggregation in federated learning with local differential privacy", "journal": "IJCAI", "year": "2021", "authors": "L Sun; J Qian; X Chen"}, {"ref_id": "b37", "title": "Axiomatic attribution for deep networks", "journal": "PMLR", "year": "2017", "authors": "M Sundararajan; A Taly; Q Yan"}, {"ref_id": "b38", "title": "Spectral signatures in backdoor attacks", "journal": "", "year": "2018", "authors": "B Tran; J Li; A Madry"}, {"ref_id": "b39", "title": "PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks", "journal": "", "year": "2020", "authors": "M Vu; M Thai"}, {"ref_id": "b40", "title": "NeuCEPT: Learn Neural Networks's Mechanism via Critical Neurons with Precision Guarantee", "journal": "IEEE", "year": "2022", "authors": "M N Vu; T Nguyen; M T Thai"}, {"ref_id": "b41", "title": "2021. c-Eval: A unified metric to evaluate featurebased explanations via perturbation", "journal": "IEEE", "year": "", "authors": "M N Vu; T D Nguyen; N Phan; R Gera; M T Thai"}, {"ref_id": "b42", "title": "Neural cleanse: Identifying and mitigating backdoor attacks in neural networks", "journal": "IEEE", "year": "2019", "authors": "B Wang; Y Yao; S Shan; H Li; B Viswanath; H Zheng; B Y Zhao"}, {"ref_id": "b43", "title": "Collecting and analyzing multidimensional data with local differential privacy", "journal": "", "year": "2019", "authors": "N Wang; X Xiao; Y Yang; J Zhao; S C Hui; H Shin; J Shin; G Yu"}, {"ref_id": "b44", "title": "Locally differentially private protocols for frequency estimation", "journal": "", "year": "2017", "authors": "T Wang; J Blocki; N Li; S Jha"}, {"ref_id": "b45", "title": "MURS: practical and robust privacy amplification with multi-party differential privacy", "journal": "", "year": "2019", "authors": "T Wang; B Ding; M Xu"}, {"ref_id": "b46", "title": "Randomized response: A survey technique for eliminating evasive answer bias", "journal": "Journal of the American Statistical Association", "year": "1965", "authors": "S L Warner"}, {"ref_id": "b47", "title": "A comprehensive survey on local differential privacy", "journal": "Security and Communication Networks", "year": "2020", "authors": "X Xiong; S Liu; D Li; Z Cai; X Niu"}, {"ref_id": "b48", "title": "Gnnexplainer: Generating explanations for graph neural networks", "journal": "", "year": "2019", "authors": "R Ying; D Bourgeois; J You; M Zitnik; J Leskovec"}, {"ref_id": "b49", "title": "Interpretable deep learning under fire", "journal": "", "year": "2020", "authors": "X Zhang; N Wang; H Shen; S Ji; X Luo; T Wang"}, {"ref_id": "b50", "title": "Exploiting Explanations for Model Inversion Attacks", "journal": "", "year": "2021", "authors": "X Zhao; W Zhang; X Xiao; B Lim"}, {"ref_id": "b51", "title": "Local differential privacy based federated learning for Internet of Things", "journal": "IEEE Internet of Things Journal", "year": "2020", "authors": "Y Zhao; J Zhao; M Yang; T Wang; N Wang; L Lyu; D Niyato; K Y Lam"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: System model of a cloud-hosted malware classifier that leverages crowdsourced data for model training.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": ", certified poisoning training size r D , added into D to change the model prediction on x: f (D, x) = f (D + , x)?\" After adding D o into D, we ask the same question: \"What is the minimum number poisoning data samples, i.e., certified poisoning training size r D , added into D = D \u222a D o to change the model prediction on x: f (D , x) = f (D + , x)?\" The difference between r D and r D provides us a certified poisoning training size on D o . Intuitively, if D o does not consist of poisoning data examples, then r D is expected to be relatively the same with r D . Otherwise, r D can be significantly smaller than r D indicating that D o is heavily poisoned with at least r = r D \u2212 r D number of poisoning data samples towards opening backdoors on x. Theorem 3. Given two certified poisoning training sizes r * D = arg min r D r D and r * D = arg min r D r D , the certified poisoning training size r of the outsourced data D o is: r = r * D \u2212 r * D (6) Proof: Refer to Appx. C for the proof and its tightness.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "\u2206Figure 2 :2Figure 2: Attack success rate as a function of privacy budget \u03b5 and the portion of poisoned samples on LightGBM and EmberNN. The trigger size is fixed at 10 and 16 for Light-GBM and EmberNN, respectively.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure3: Attack success rate as a function of trigger size and privacy budget \u03b5. We vary the trigger size and fix the portion of poisoned samples at 0.25% and 1% for LightGBM and EmberNN, respectively.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 4 :4Figure 4: Log-odds score of the explanations of 20,000 goodware and malware samples. The leftmost box represents the original explanations by SHAP, the remaining boxes illustrate the explanations after applying XRAND at \u03b5 = 1.0, 10.0, 20.0, respectively.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure 5: Visualizing the SHAP explanation and our XRAND explanation of a test sample. The plot shows the SHAP value of each feature. The red vertical lines represent positive SHAP values that indicate malware-oriented features, while the blue vertical lines are negative SHAP values indicating goodware-oriented features.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "CProof of Theorem 3 Proof. Recall that D, D o , and D = D \u222a D o are the proprietary data, the outsourced data, and the poisoned training data, respectively. Given the model prediction on a data sample x using D, denoted as f (D, x), we ask a simple question: \"What is the minimum number poisoning data samples, i.e., certified poisoning training size r D , added into D to change the model prediction on x: f (D, x) = f (D + , x)?\" After adding D o into D, we ask the same question: \"What is the minimum number poisoning data samples, i.e., certified poisoning training size r D , added into D = D \u222a D o to change the model prediction on x: f (D , x) = f (D + , x)?\" The difference between r D and r D provides us a certified poisoning training size on D o . Intuitively, if D o does not contain poisoning data examples, then r D is expected to be relatively the same with r D . Otherwise, r D can be significantly smaller than r D indicating that D o is heavily poisoned with at least r = r D \u2212 r D number of poisoning data samples towards opening backdoors on x. Now, our goal is to find r D , r D , and their connection to r. Let us denote two sets of poisoned training datasets with at most r D poisoned training samples in D and at most r D poisoned training samples in D, namely B(D , r D ) and B(D, r D ), respectively, as follows: B(D, r D ) = {D + s.t. |D + | \u2212 |D| \u2264 r D } (16) B(D , r D ) = {D + s.t. |D + | \u2212 |D | \u2264 r D } (17)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Models.For the EMBER dataset, we train two classifiers: LightGBM and EmberNN. LightGBM is a gradient boosting model released together with the EMBER dataset. It achieves good performance for malware binary classification with 98.61% accuracy. Following Anderson et al. (Anderson and Roth 2018), we use default parameters for training LightGBM (100 trees and 31 leaves per tree). EmberNN composes of four fully connected layers, in which the first three layers use ReLU activation functions, and the last layer uses a sigmoid activation function(Severi et al. 2021). Em-berNN attains 99.14% accuracy.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 7 :7Figure 7: Attack success rate as a function of privacy budget \u03b5 and the portion of poisoned samples on the Contagio PDF and Drebin datasets. The trigger size is fixed at 30.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 8 :8Figure 8: Certified robustness at the training time. The smaller privacy budget \u03b5, the higher certified accuracy.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 9 :9Figure 9: Visualizing the SHAP explanation and our XRAND explanation of test sample 1. The plot shows the SHAP value of each feature. The red vertical lines represent positive SHAP values that indicate malware-oriented features, while the blue vertical lines are negative SHAP values indicating goodware-oriented features.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Figure 6: SSE values as a function of and \u03c4 Dataset. We conduct the XBA on malware classifiers against XRAND explanations on three malware datasets. EMBER (Anderson and Roth 2018) is a representative benchmark dataset containing malicious and benign samples used for training malware classifiers. It consists of 2,351dimensional feature vectors extracted from 1.1M Portable Executable (PE) files. The dataset contains 600, 000 training samples equally split between goodware and malware, and 200, 000 test samples, with the same class balance.We also test with malicious PDF files using the Contagio PDF dataset(Smutz and Stavrou 2012) which contains 10,000 PDF files equally split between goodware and malware, each sample is represented by a 135-dimensional feature vector. Finally, we evaluate our work on the Drebin dataset(Kumar et al. 2018) consisting of 5,560 malware and 123,453 goodware Android apps. Each sample contains 545,333 features extracted from the applications.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "P r[A(x) = O] \u2264 e \u03b5 P r[A(x ) = O]", "formula_coordinates": [2.0, 319.5, 159.25, 238.5, 19.7]}, {"formula_id": "formula_1", "formula_text": "L = z\u2208N (x) (g (z) \u2212 f (z)) 2 exp \u2212 z \u2212 x 2 \u03c3 2 (1)", "formula_coordinates": [3.0, 336.1, 275.34, 221.9, 28.84]}, {"formula_id": "formula_2", "formula_text": "2: for x n \u2208 D do 3:", "formula_coordinates": [4.0, 58.98, 116.0, 83.64, 19.77]}, {"formula_id": "formula_3", "formula_text": "for i \u2208 [1, k], j \u2208 [k + 1, k + \u03c4 ] do 5:", "formula_coordinates": [4.0, 58.98, 137.91, 173.52, 19.77]}, {"formula_id": "formula_4", "formula_text": "\u2200i \u2208 [1, k], j \u2208 [k + 1, k + \u03c4 ], \u03c4 \u2265 k : i = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 i, with probability p i = exp(\u03b2) exp(\u03b2) + \u03c4 \u2212 1 , j, with probability q i,j = \u03c4 \u2212 1 exp(\u03b2) + \u03c4 \u2212 1 q j (2", "formula_coordinates": [4.0, 64.28, 326.36, 224.35, 63.53]}, {"formula_id": "formula_5", "formula_text": ")", "formula_coordinates": [4.0, 288.63, 361.45, 3.87, 8.64]}, {"formula_id": "formula_6", "formula_text": "\u2206 L (i, j) = 1 N N n=1 (|L(x n ) \u2212 L(x n )(i, j)|)(3)", "formula_coordinates": [4.0, 86.62, 447.27, 205.88, 30.2]}, {"formula_id": "formula_7", "formula_text": "S = {(i, j)|i and j are flipped, i \u2208 [1, k], j \u2208 [k + 1, k + \u03c4 ]} (4)", "formula_coordinates": [4.0, 54.0, 561.23, 238.5, 19.92]}, {"formula_id": "formula_8", "formula_text": "Q = {(i, j)|w xi \u2264 w xj } that is sufficient for S.", "formula_coordinates": [4.0, 54.0, 623.65, 185.87, 10.62]}, {"formula_id": "formula_9", "formula_text": "(v(i+1), v(j)); (v(j), v(i\u22121)); (v(i), v(j\u22121)); (v(j+1), v(i))", "formula_coordinates": [4.0, 54.0, 655.58, 248.81, 8.74]}, {"formula_id": "formula_10", "formula_text": "min \u03c6 z\u2208N (x) (wx + \u03c6) T z \u2212 f (z) 2 exp \u2212 z \u2212 x 2 \u03c3 2 + \u03bb \u03c6 (5) s.t. wx i + \u03c6i \u2264 wx j + \u03c6j, \u2200(i, j) \u2208 Q \u03c6i = 0 \u2200i / \u2208 Q", "formula_coordinates": [4.0, 319.5, 73.97, 238.5, 65.44]}, {"formula_id": "formula_11", "formula_text": "P (XRAND( wi)=z| w) \u2264 exp(\u03b5 i ), if: \u03b2 \u2264 \u03b5 i + ln(\u03c4 \u2212 1) + ln(min exp(\u2212\u2206 L (i, j)) k+\u03c4 t=k+1 exp(\u2212\u2206 L (i, t))", "formula_coordinates": [4.0, 320.7, 299.58, 226.83, 47.37]}, {"formula_id": "formula_12", "formula_text": "i , i.e., \u03b5 = k i=1 \u03b5 i , since each feature i is randomized in- dependently.", "formula_coordinates": [4.0, 319.5, 391.23, 238.5, 22.88]}, {"formula_id": "formula_13", "formula_text": "XRAND: SSE = x\u2208D d i=1 (w xi \u2212 w xi ) 2 = x\u2208D k+\u03c4 i=1 (w xi \u2212 w xi ) 2", "formula_coordinates": [4.0, 330.02, 524.99, 227.98, 24.05]}, {"formula_id": "formula_14", "formula_text": "D = D \u222a D o .", "formula_coordinates": [5.0, 236.18, 324.69, 56.32, 9.65]}, {"formula_id": "formula_15", "formula_text": "\u2200\u03b1 \u2208 L p (\u00b5) : f l (x + \u03b1|w x ) > f \u00acl (x + \u03b1|w x ) (7)", "formula_coordinates": [5.0, 345.76, 217.41, 212.24, 9.65]}, {"formula_id": "formula_16", "formula_text": "\u2206 \u03b1|w = 1 |X |d x\u2208X |w x \u2212 w x | 1 (8)", "formula_coordinates": [5.0, 376.91, 339.26, 181.09, 26.8]}, {"formula_id": "formula_17", "formula_text": "\u2200l \u2208 {0, 1}, \u2200\u03b1 \u2208 L p (\u00b5) : Ef l (x) \u2264 e \u03b5 Ef l (x + \u03b1) + \u03b4 (9)", "formula_coordinates": [5.0, 324.48, 598.65, 233.52, 11.72]}, {"formula_id": "formula_18", "formula_text": "P (XRAND(w i ) = z) P (XRAND( w i ) = z) \u2264 max P (XRAND(w i ) = z) min P (XRAND( w i ) = z) = exp(\u03b2) exp(\u03b2)+\u03c4 \u22121 min( exp(\u2212\u2206 L (i,j)) t\u2208[k+1,k+\u03c4 ] exp(\u2212\u2206 L (i,t)) \u03c4 \u22121 exp(\u03b2)+\u03c4 \u22121 ) = exp(\u03b2) (\u03c4 \u2212 1) min( exp(\u2212\u2206 L (i,j)) t\u2208[k+1,k+\u03c4 ] exp(\u2212\u2206 L (i,t)) ) \u2264 exp(\u03b5 i )(10)", "formula_coordinates": [10.0, 65.0, 133.3, 227.5, 106.77]}, {"formula_id": "formula_19", "formula_text": "ln( exp(\u03b2) (\u03c4 \u2212 1) min( exp(\u2212\u2206 L 2 (i,j)) k+\u03c4 t=k+1 exp(\u2212\u2206 L 2 (i,t)) ) ) \u2264 ln(exp(\u03b5 i )) \u21d4 \u03b2 \u2264 \u03b5 i + ln(\u03c4 \u2212 1) + ln(min exp(\u2212\u2206 L (i, j)) k+\u03c4 t=k+1 exp(\u2212\u2206 L (i, t))", "formula_coordinates": [10.0, 55.66, 267.54, 236.46, 62.6]}, {"formula_id": "formula_20", "formula_text": "\u2200\u03b1 \u2208 l p (\u00b5) : f i (x + \u03b1) > max j:j =i f j (x + \u03b1) (11", "formula_coordinates": [10.0, 88.94, 448.73, 199.41, 14.66]}, {"formula_id": "formula_21", "formula_text": ")", "formula_coordinates": [10.0, 288.35, 449.04, 4.15, 8.64]}, {"formula_id": "formula_22", "formula_text": "\u2200i, \u2200\u03b1 \u2208 l p (1) : Ef i (x) \u2264 e r Ef i (x + \u03b1) + \u03b4 r (12", "formula_coordinates": [10.0, 70.93, 666.0, 217.42, 9.65]}, {"formula_id": "formula_23", "formula_text": ")", "formula_coordinates": [10.0, 288.35, 666.32, 4.15, 8.64]}, {"formula_id": "formula_24", "formula_text": "E lb f i (x) > e 2 r max j:j =i\u00ca ub f j (x) + (1 + e r )\u03b4 r (13)", "formula_coordinates": [10.0, 339.08, 85.57, 218.92, 16.73]}, {"formula_id": "formula_25", "formula_text": "p 1 := P f (x + \u03b1) = i \u2265 1 \u2212 P |z i \u2212 (c i \u2212 c j )| \u2265 t i \u03c3 i (M ) \u2265 1 \u2212 j,j =i \u03c3 2 j (M ) (c i \u2212 c j ) 2(14)", "formula_coordinates": [10.0, 335.64, 337.89, 222.36, 46.19]}, {"formula_id": "formula_26", "formula_text": "P R = \u03c3 \u03b1 \u03a6 \u22121 p 1 (N 1 , N, \u03b1) = B(N 1 , N, p 1 ) (15)", "formula_coordinates": [10.0, 330.58, 500.32, 227.42, 11.72]}, {"formula_id": "formula_27", "formula_text": "h(D, x) = arg max l\u2208{0,1} p l (18", "formula_coordinates": [11.0, 124.92, 513.51, 163.43, 15.05]}, {"formula_id": "formula_28", "formula_text": ")", "formula_coordinates": [11.0, 288.35, 513.83, 4.15, 8.64]}, {"formula_id": "formula_29", "formula_text": "p l \u2265 p l \u2265 p \u00acl \u2265 p \u00acl (20)", "formula_coordinates": [11.0, 397.32, 134.46, 160.68, 12.58]}, {"formula_id": "formula_30", "formula_text": "h(D + , x) = l, \u2200D + \u2208 B(D, r * D ),(21)", "formula_coordinates": [11.0, 372.16, 286.65, 185.84, 12.69]}, {"formula_id": "formula_31", "formula_text": "r * D = arg max r D r D (22) s.t. max |D|\u2212r D \u2264|D+|\u2264|D|+r D ( |D + | |D| ) k \u2212 2 max(|D, |D + |) \u2212 r D |D| k + 1 \u2212 (p l \u2212 p \u00acl \u2212 \u03c3 l \u2212 \u03c3 \u00acl ) < 0 (23", "formula_coordinates": [11.0, 319.5, 331.64, 240.28, 82.19]}, {"formula_id": "formula_32", "formula_text": ")", "formula_coordinates": [11.0, 553.85, 401.56, 4.15, 8.64]}, {"formula_id": "formula_33", "formula_text": "r * D = |D| k 1 + (p l \u2212 p \u00acl \u2212 \u03c3 l \u2212 \u03c3 \u00acl ) \u2212 1 (24)", "formula_coordinates": [11.0, 333.97, 481.02, 224.03, 14.66]}, {"formula_id": "formula_34", "formula_text": "r * D = |D | k 1 + (p l \u2212 p \u00acl \u2212 \u03c3 l \u2212 \u03c3 \u00acl ) \u2212 1 (25", "formula_coordinates": [11.0, 331.17, 550.35, 222.68, 14.66]}, {"formula_id": "formula_35", "formula_text": ")", "formula_coordinates": [11.0, 553.85, 552.74, 4.15, 8.64]}, {"formula_id": "formula_36", "formula_text": "r = r * D \u2212 r * D (26)", "formula_coordinates": [11.0, 410.88, 693.12, 147.12, 12.69]}, {"formula_id": "formula_37", "formula_text": "p 1 := P f (x + \u03b1 + \u03b1 1 ) = l \u2265 1 \u2212 \u03c3 2 new (M ) (c l \u2212 c \u00acl ) 2 (27) Similarly, \u03c3 2", "formula_coordinates": [12.0, 63.96, 395.31, 228.54, 39.28]}, {"formula_id": "formula_38", "formula_text": "P R = \u03c3 \u03b1+\u03b11 \u03a6 \u22121 p 1 (N 1 , N ) = B(N 1 , N, p 1 ) (28)", "formula_coordinates": [12.0, 62.64, 496.56, 229.86, 11.72]}, {"formula_id": "formula_39", "formula_text": "certified accuracy = |test| n=1 isCorrect(Xn) & isRobust(Xn) |test| (29", "formula_coordinates": [13.0, 62.5, 623.47, 226.27, 36.39]}, {"formula_id": "formula_40", "formula_text": ")", "formula_coordinates": [13.0, 288.77, 652.08, 3.73, 7.77]}], "doi": ""}