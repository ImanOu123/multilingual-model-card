{"title": "GAIA: A Fine-grained Multimedia Knowledge Extraction System", "authors": "Manling Li; Alireza Zareian; Ying Lin; Xiaoman Pan; Spencer Whitehead; Brian Chen; Bo Wu; Heng Ji; Shih-Fu Chang; Clare Voss; Daniel Napierski; Marjorie Freedman", "pub_date": "", "abstract": "We present the first comprehensive, open source multimedia knowledge extraction system that takes a massive stream of unstructured, heterogeneous multimedia data from various sources and languages as input, and creates a coherent, structured knowledge base, indexing entities, relations, and events, following a rich, fine-grained ontology. Our system, GAIA 1 , enables seamless search of complex graph queries, and retrieves multimedia evidence including text, images and videos. GAIA achieves top performance at the recent NIST TAC SM-KBP2019 evaluation 2 . The system is publicly available at GitHub 3 and DockerHub 4 , with complete documentation 5 .", "sections": [{"heading": "Introduction", "text": "Knowledge Extraction (KE) aims to find entities, relations and events involving those entities from unstructured data, and link them to existing knowledge bases. Open source KE tools are useful for many real-world applications including disaster monitoring (Zhang et al., 2018a), intelligence analysis (Li et al., 2019a) and scientific knowledge mining (Luan et al., 2017;Wang et al., 2019). Recent years have witnessed the great success and wide usage of open source Natural Language Processing tools (Manning et al., 2014;Fader et al., 2011;Daniel Khashabi, 2018;Honnibal and Montani, 2017), but there is no comprehensive open source system for KE. We release a new comprehensive KE system, GAIA, that advances the state of the art in two aspects: (1) it extracts and integrates knowledge across multiple languages and modalities, and (2) it classifies knowledge elements into fine-grained types, as shown in Table 1. We also release the pretrained models 6 and provide a script to retrain it for any ontology.\nGAIA has been inherently designed for multimedia, which is rapidly replacing text-only data in many domains. We extract complementary knowledge from text as well as related images or video frames, and integrate the knowledge across modalities. Taking Figure 1 as an example, the text entity extraction system extracts the nominal mention troops, but is unable to link or relate that due to a vague textual context. From the image, the entity linking system recognizes the flag as Ukrainian and represents it as a NationalityCitizen relation in the knowledge base. It can be deduced, although not for sure, that the detected people are Ukrainian. Meanwhile, our cross-media fusion system grounds the troops to the people detected in the image. This establishes a connection between the knowledge Figure 2: User-facing views of knowledge networks constructed with events automatically extracted from multimedia multilingual news reports. We display the event arguments, type, summary, similar events, as well as visual knowledge extracted from the corresponding image and video. extracted from the two modalities, allowing to infer that the troops are Ukrainian, and They refers to the Ukrainian government.\nCompared to coarse-grained event types of previous work (Li et al., 2019a), we follow a richer ontology to extract fine-grained types, which are crucial to scenario understanding and event prediction. For example, an event of type Movement.TransportPerson involving an entity of type PER.Politician.HeadOfGovernment differs in implications from the same event type involving a PER.Combatant.Sniper entity (i.e., a political trip versus a military deployment). Similarly, it is far more likely that an event of type Conflict.Attack.Invade will lead to a Contact.Negotiate.Meet event, while a Conflict.Attack.Hanging event is more likely to be followed by an event of type Contact.FuneralVigil.Meet.  The knowledge base extracted by GAIA can support various applications, such as multimedia news event understanding and recommendation. We use Russia-Ukraine conflicts of 2014-2015 as a case study, and develop a knowledge exploration interface that recommends events related to the user's ongoing search based on previously-selected attribute values and dimensions of events being viewed 7 , as shown in Figure 2. Thus, this system automatically provides the user with a more comprehensive exposure to collected events, their importance, and their interconnections. Extensions of this system to real-time applications would be particularly useful for tracking current events, providing alerts, and predicting possible changes, as well as topics related to ongoing incidents.", "publication_ref": ["b50", "b20", "b26", "b47", "b27", "b9", "b13", "b20"], "figure_ref": ["fig_0"], "table_ref": ["tab_1"]}, {"heading": "Overview", "text": "The architecture of our multimedia knowledge extraction system is illustrated in Figure 3. The system pipeline consists of a Text Knowledge Extraction (TKE) branch and a Visual Knowledge Extraction (VKE) branch (Sections 3 and 4 respectively). Each branch takes the same set of documents as input, and initially creates a separate knowledge base (KB) that encodes the information from its respec-  tive modality. Both output knowledge bases make use of the same types from the DARPA AIDA ontology 8 , as referred to in Table 1. Therefore, while the branches both encode their modality-specific extractions into their KBs, they do so with types defined in the same semantic space. This shared space allows us to fuse the two KBs into a single, coherent multimedia KB through the Cross-Media Knowledge Fusion module (Section 5). Our userfacing system demo accesses one such resulting KB, where attack events have been extracted from multi-media documents related to the 2014-2015 Russia-Ukraine conflict scenario. In response to user queries, the system recommends information around a primary event and its connected events from the knowledge graph (screenshot in Figure 2).", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": ["tab_1"]}, {"heading": "Textual Relation Extraction Textual Mention Extraction", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Textual Event Extraction", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Text Knowledge Extraction", "text": "As shown in Figure 3, the Text Knowledge Extraction (TKE) system extracts entities, relations, and events from input documents. Then it clusters identical entities through entity linking and coreference, and clusters identical events using event coreference.\n8 https://tac.nist.gov/tracks/SM-KBP/2019/ ontologies/LDCOntology", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Text Entity Extraction and Coreference", "text": "Coarse-grained Mention Extraction We extract coarse-grained named and nominal entity mentions using a LSTM-CRF  model. We use pretrained ELMo  word embeddings as input features for English, and pretrain Word2Vec (Le and Mikolov, 2014) models on Wikipedia data to generate Russian and Ukrainian word embeddings. Entity Linking and Coreference We seek to link the entity mentions to pre-existing entities in the background KBs (Pan et al., 2015), including Freebase (LDC2015E42) and GeoNames (LDC2019E43). For mentions that are linkable to the same Freebase entity, coreference information is added. For name mentions that cannot be linked to the KB, we apply heuristic rules (Li et al., 2019b) to same-named mentions within each document to form NIL clusters. A NIL cluster is a cluster of entity mentions referring to the same entity but do not have corresponding KB entries (Ji et al., 2014). Fine-grained Entity Typing We develop an attentive fine-grained type classification model with latent type representation . It takes as input a mention with its context sentence and predicts the most likely fine-grained type. We obtain the YAGO (Suchanek et al., 2008) fine-grained types from the results of Freebase entity linking, and map these types to the DARPA AIDA ontology. For mentions with identified, coarse-grained GPE and LOC types, we further determine their fine-grained types using GeoNames attributes feature class and feature code from the GeoNames entity linking result. Given that most nominal mentions are descriptions and thus do not link to entries in Freebase or GeoNames, we develop a nominal keyword list (Li et al., 2019b) for each type to incorporate these mentions into the entity analyses. Entity Salience Ranking To better distill the information, we assign each entity a salience score in each document. We rank the entities in terms of the weighted sum of all mentions, with higher weights for name mentions. If one entity appears only in nominal and pronoun mentions, we reduce its salience score so that it is ranked below other entities with name mentions. The salience score is normalized over all entities in each document.", "publication_ref": ["b19", "b31", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Text Relation Extraction", "text": "For fine-grained relation extraction, we first apply a language-independent CNN based model (Shi et al., 2018) to extract coarse-grained relations from English, Russian and Ukrainian documents. Then we apply entity type constraints and dependency patterns to these detected relations and re-categorize them into fine-grained types (Li et al., 2019b). To extract dependency paths for these relations in the three languages, we run the corresponding language's Universal Dependency parser (Nivre et al., 2016). For types without coarse-grained type training data in ACE/ERE, we design dependency pathbased patterns instead and implement a rule-based system to detect their fine-grained relations directly from the text (Li et al., 2019b).", "publication_ref": ["b42", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Text Event Extraction and Coreference", "text": "We start by extracting coarse-grained events and arguments using a Bi-LSTM CRF model and a CNNbased model (Zhang et al., 2018b) for three languages, and then detect the fine-grained event types by applying verb-based rules, context-based rules, and argument-based rules (Li et al., 2019b). We also extract FrameNet frames (Chen et al., 2010) in English corpora to enrich the fine-grained events.\nWe apply a graph-based algorithm (Al-Badrashiny et al., 2017) for our languageindependent event coreference resolution. For each event type, we cast the event mentions as nodes in a graph, so that the undirected, weighted edges be-tween these nodes represent coreference confidence scores between their corresponding events. We then apply hierarchical clustering to obtain event clusters and train a Maximum Entropy binary classifier on the cluster features (Li et al., 2019b).", "publication_ref": ["b52", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Visual Knowledge Extraction", "text": "The Visual Knowledge Extraction (VKE) branch of GAIA takes images and video key frames as input and creates a single, coherent (visual) knowledge base, relying on the same ontology as GAIA's Text Knowledge Extraction (TKE) branch. Similar to TKE, the VKE consists of entity extraction, linking, and coreference modules. Our VKE system also extracts some events and relations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Visual Entity Extraction", "text": "We use an ensemble of visual object detection and concept localization models to extract entities and some events from a given image. To detect generic objects such as person and vehicle, we employ two off-the-shelf Faster R-CNN models (Ren et al., 2015) trained on the Microsoft Common Objects in COntext (MS COCO) (Lin et al., 2014) and Open Images (Kuznetsova et al., 2018) datasets. To detect scenario-specific entities and events, we train a Class Activation Map (CAM) model (Zhou et al., 2016) in a weakly supervised manner using a combination of Open Images with image-level labels and Google image search.\nGiven an image, each R-CNN model produces a set of labeled bounding boxes, and the CAM model produces a set of labeled heat maps which are then thresholded to produce bounding boxes. The union of all bounding boxes is then post-processed by a set of heuristic rules to remove duplicates and ensure quality. We separately apply a face detector, MTCNN (Zhang et al., 2016), and add the results to the pool of detected objects as additional person entities. Finally, we represent each detected bounding box as an entity in the visual knowledge base. Since the CAM model includes some event types, we create event entries (instead of entity entries) for bounding boxes classified as events.", "publication_ref": ["b38", "b23", "b18", "b53", "b51"], "figure_ref": [], "table_ref": []}, {"heading": "Visual Entity Linking", "text": "Once entities are added into the (visual) knowledge base, we try to link each entity to the real-world entities from a curated background knowledge base. Due to the complexity of this task, we develop distinct models for each coarse-grained entity type. For the type person, we train a FaceNet model (Schroff et al., 2015) that takes each cropped human face (detected by the MTCNN model as mentioned in Section 4.1) and classifies it in one or none of the predetermined identities. We compile a list of recognizable and scenario-relevant identities by automatically searching for each person name in the background KB via Google Image Search, collecting top retrieved results that contain a face, training a binary classifier on half of the results, and evaluating on the other half. If the accuracy is higher than a threshold, we include that person name in our list of recognizable identities. For example, the visual entity in Figure 4 (a) is linked to the Wikipedia entry Rudy Giuliani 9 .\nTo recognize location, facility, and organization entities, we use a DELF model (Noh et al., 2017) pre-trained on Google Landmarks, to match each image with detected buildings against a predetermined list. We use a similar approach as mentioned above to create a list of recognizable, scenariorelevant landmarks, such as buildings and other types of structure that identify a specific location, facility, or organization. For example, the visual entity in Figure 4 (b) is linked to the Wikipedia entry Maidan Square 10 Finally, to recognize geopolitical entities, we train a CNN to classify flags into a predetermined list of entities, such as all the nations in the world, for detection in our system. Take Figure 4 (c) as an example. The flags of Ukraine, US and Russia are linked to the Wikipedia entries of corresponding countries. Once a flag in an image is recognized, we apply a set of heuristic rules to create a nationality affiliation relationship in the knowledge base between some entities in the scene and the detected country. For instance, a person who is holding a Ukrainian flag would be affiliated with the country 9 https://en.wikipedia.org/wiki/Rudy_ Giuliani 10 https://en.wikipedia.org/wiki/Maidan_ Nezalezhnosti Ukraine.", "publication_ref": ["b41", "b30"], "figure_ref": ["fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "Visual Entity Coreference", "text": "While we cast each detected bounding box as an entity node in the output knowledge base, we resolve potential coreferential links between them, since one unique real-world entity can be detected multiple times. Cross-image coreference resolution aims to identify the same entity appearing in multiple images, where the entities are in different poses from different angles. Take Figure 5 as an example. The red bounding boxes in these two images refer to the same person, so they are coreferential and are put into the same NIL cluster. Within-image coreference resolution requires the detection of duplicates, such as the duplicates in an collage image. To resolve entity coreference, we train an instancematching CNN on the Youtube-BB dataset (Real et al., 2017), where we ask the model to match an object bounding box to the same object in a different video frame, rather than to a different object. We use this model to extract features for each detected bounding box and run the DBSCAN (Ester et al., 1996) clustering algorithm on the box features across all images. The entities in the same cluster are coreferential, and are represented using a NIL cluster in the output (visual) KB. Similarly, we use a pretrained FaceNet (Schroff et al., 2015) model followed by DBSCAN to cluster face features.\nFigure 5: The two green bounding boxes are coreferential since they are both linked to \"Kirstjen Nielsen\", and two red bounding boxes are coreferential based on face features. The yellow bounding boxes are unlinkable and also not coreferential to other bounding boxes.\nWe also define heuristic rules to complement the aforementioned procedure in special cases. For example, if in the entity linking process (Section 4.2), some entities are linked to the same real-world entity based on entity linking result, we consider them coreferential. Besides, since we have both face detection and person detection which result in two entities for each person instance, we use their bounding box intersection to merge them into the same entity.", "publication_ref": ["b37", "b8", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Cross-Media Knowledge Fusion", "text": "Given a set of multimedia documents which consist of textual data, such as written articles and transcribed speech, as well as visual data, such as images and video key frames, the TKE and VKE branches of the system take their respective modality data as input, extract knowledge elements, and create separate knowledge bases. These textual and visual knowledge bases rely on the same ontology, but contain complementary information. Some knowledge elements in a document may not be explicitly mentioned in the text, but will appear visually, such as the Ukrainian flag in Figure 1. Even coreferential knowledge elements that exist in both knowledge bases are not completely redundant, since each modality has its own unique granularity. For example, the word troops in text could be considered coreferential to the individuals with military uniform detected in the image, but the uniforms being worn may provide additional visual features useful in identifying the military ranks, organizations and nationalities of the individuals.\nTo exploit the complementary nature of the two modalities, we combine the two modality-specific knowledge bases into a single, coherent, multimedia knowledge base, where each knowledge element could be grounded in either or both modalities. To fuse the two bases, we develop a state-of-the-art visual grounding system (Akbari et al., 2019) to resolve entity coreference across modalities. More specifically, for each entity mention extracted from text, we feed its text along with the whole sentence into an ELMo model  that extracts contextualized features for the entity mention, and then we compare that with CNN feature maps of surrounding images. This leads to a relevance score for each image, as well as a granular relevance map (heatmap) within each image. For images that are relevant enough, we threshold the heatmap to obtain a bounding box, compare that box content with known visual entities, and assign it to the entity with the most overlapping match. If no overlapping entity is found, we create a new visual entity with the heatmap bounding box. Then we link the matching textual and visual entities using a NIL cluster. Additionally, with visual linking (Section 4.2), we corefer cross-modal entities that are linked to the same background KB node.   Meulder, 2003), ACE (Walker et al., 2006), ERE (Song et al., 2015), AIDA (LDC2018E01:AIDA Seedling Corpus V2.0), MSCOCO (Lin et al., 2014), FDDB (Jain and Learned-Miller, 2010), LFW (Huang et al., 2008), Oxf105k (Philbin et al., 2007), YoutubeBB (Real et al., 2017), andFlickr30k (Plummer et al., 2015).", "publication_ref": ["b0", "b40", "b46", "b43", "b23", "b16", "b34", "b37"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Evaluation", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Quantitative Performance", "text": "The performance of each component is shown in Table 2. To evaluate the end-to-end performance, we participated with our system in the TAC SM-KBP 2019 evaluation 11 . The input corpus contains 1999 documents (756 English, 537 Russian, 703 Ukrainian), 6194 images, and 322 videos. We populated a multimedia, multilingual knowledge base with 457,348 entities, 67,577 relations, 38,517 events. The system performance was evaluated based on its responses to class queries and graph queries 12 , and GAIA was awarded first place. Class queries evaluated cross-lingual, crossmodal, fine-grained entity extraction and coreference, where the query is an entity type, such as FAC.Building.GovernmentBuilding, and the result is a ranked list of entities of the given type. Our entity ranking is generated by the entity salience score in Section 3.1. The evaluation metric was Average Precision (AP), where AP-B was the AP score where ties are broken by ranking all Right responses above all Wrong responses, AP-W was the AP score where ties are broken by ranking all Wrong responses above all Right responses, and AP-T was the AP score where ties are broken as in TREC Eval 13 .  Graph queries evaluated cross-lingual, crossmodal, fine-grained relation extraction, event extraction and coreference, where the query is an argument role type of event (e.g., Victim of Life.Die.DeathCausedByViolentEvents) or relation (e.g., Parent of PartWhole.Subsidiary) and the result is a list of entities with that role. The evaluation metrics were Precision, Recall and F 1 .", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Qualitative Analysis", "text": "To demonstrate the system, we have selected Ukraine-Russia Relations in 2014-2015 for a case study to visualize attack events, as extracted from the topic-related corpus released by LDC 14 . The system displays recommended events related to the user's ongoing search based on their previouslyselected attribute values and dimensions of events being viewed, such as the fine-grained type, place, time, attacker, target, and instrument. The demo is publicly available 15 with a user interface as shown in Figure 2, displaying extracted text entities and events across languages, visual entities, visual entity linking and coreference results from face, landmark and flag recognition, and the results of grounding text entities to visual entities.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Existing knowledge extraction systems mainly focus on text (Manning et al., 2014;Fader et al., 2011;Daniel Khashabi, 2018;Honnibal and Montani, 2017;Li et al., 2019a), and do not readily support fine-grained knowledge extraction. Visual knowledge extraction is typically limited to atomic concepts that have distinctive visual features of daily life (Ren et al., 2015;Schroff et al., 2015;Fern\u00e1ndez et al., 2017;Gu et al., 2018;Lin et al., 2014), and so lacks more complex concepts, making extracted elements challenging to integrate with text. Existing multimedia systems overlook the connections and distinctions between modalities (Yazici et al., 2018). Our system makes use of a multi-modal ontology with concepts from real-world, newsworthy topics, resulting in a rich cross-modal, as well as intra-modal connectivity.", "publication_ref": ["b27", "b9", "b13", "b20", "b38", "b41", "b10", "b12", "b23", "b49"], "figure_ref": [], "table_ref": []}, {"heading": "Ethical Considerations 16", "text": "Innovations in technology often face the ethical dilemma of dual use: the same advance may offer potential benefits and harms (Ehni, 2008;Hovy and Spruit, 2016;Brundage et al., 2018). We first discuss dual use, 17 as it relates to this demo in particular and then discuss two other considerations for applying this technology, data bias and privacy.\nFor our demo, the distinction between beneficial use and harmful use depends, in part, on the data. Proper use of the technology requires that input documents/images are legally and ethically obtained. Regulation and standards (e.g. GDPR 18 ) provide a legal framework for ensuring that such data is properly used and that any individual whose data is used has the right to request its removal. In the absence of such regulation, society relies on those who apply technology to ensure that data is used in an ethical way.\nEven if the data itself is obtained legally and ethically, the technology when use for unintended purposes can result in harmful outcomes. This demo organizes multimedia information to aid in navigating and understanding international events described in multiple sources. We have also applied the underlying technology on data that would aid natural disaster relief efforts (Zhang et al., 2018a) 19 and we are currently exploring the application of the models (with different data) to scientific literature and drug discovery (Wang et al., 2020) 20 .\nOne potential for harm could come if the technology were used for surveillance, especially in the context of targeting private citizens. Advances in technology require increased care when balancing potential benefits that come from preventing harmful activities (e.g. preventing human trafficking, preventing terrorism) against the potential for harm, such as when surveillance is applied too broadly (e.g. limiting speech, targeting vulnerable groups) or when system error could lead to false accusations. An additional potential harm could come from the output of the system being used in ways that magnify the system errors or bias in its training data. Our demo is intended for human interpretation. Incorporating the system's output into an automatic decision-making system (forecasting, profiling, etc.) could be harmful.\nTraining and assessment data is often biased in ways that limit system accuracy on less well represented populations and in new domains, for example causing disparity of performance for different subpopulations based on ethnic, racial, gender, and other attributes (Buolamwini and Gebru, 2018;Rudinger et al., 2018). Furthermore, trained systems degrade when used on new data that is distant from their training data. The performance of our system components as reported in Table 2 is based on the specific benchmark datasets, which could be affected by such data biases. Thus questions concerning generalizability and fairness should be carefully considered. In our system, the linking of an entity to an external source (entity linking and facial recognition) is limited to entities in Wikipedia and the publicly available background knowledge bases (KBs) provided by LDC (LDC2015E42 and LDC2019E43). These sources introduce their own form of bias, which limits the demo's applicability in both the original and new contexts. Within the research community, addressing data bias requires a combination of new data sources, research that mitigates the impact of bias, and, as done in (Mitchell et al., 2019), auditing data and models. Sections 3-5 cite data sources used for training to support future auditing.\nTo understand, organize, and recommend information, our system aggregates information about people as reported in its input sources. For example, in addition to external KB linking, the system performs coreference on named people and uses text-visual grounding to link images to the surrounding context. Privacy concerns thus merit at-tention (Tucker, 2019). The demo relies on publicly available, online sources released by the LDC 21 . When applying our system to other sources, care should be paid to privacy with respect to the intended application and the data that it uses. More generally, end-to-end algorithmic auditing should be conducted before the deployment of our software (Raji et al., 2020).\nA general approach to ensure proper, rather than malicious, application of dual-use technology should: incorporate ethics considerations as the first-order principles in every step of the system design, maintain a high degree of transparency and interpretability of data, algorithms, models, and functionality throughout the system, make software available as open source for public verification and auditing, and explore countermeasures to protect vulnerable groups.", "publication_ref": ["b7", "b14", "b3", "b50", "b4", "b39", "b28", "b45", "b1", "b36"], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Conclusion", "text": "We demonstrate a state-of-the-art multimedia multilingual knowledge extraction and event recommendation system. This system enables the user to readily search a knowledge network of extracted, linked, and summarized complex events from multimedia, multilingual sources (e.g., text, images, videos, speech and OCR).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgement", "text": "This research is based upon work supported in part by U.S. DARPA AIDA Program No. FA8750-18-2-0014 and KAIROS Program No. FA8750-19-2-1004. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Multi-level multimodal common semantic space for image-phrase grounding", "journal": "", "year": "2019", "authors": "Hassan Akbari; Svebor Karaman; Surabhi Bhargava; Brian Chen; Carl Vondrick; Shih-Fu Chang"}, {"ref_id": "b1", "title": "LDC2018E64, available to participants in NIST's TAC SM-KBP evaluation", "journal": "", "year": "", "authors": ""}, {"ref_id": "b2", "title": "Tinkerbell: Cross-lingual cold-start knowledge base construction", "journal": "", "year": "2017", "authors": "Mohamed Al-Badrashiny; Jason Bolton; Arun Tejasvi Chaganty; Kevin Clark; Craig Harman; Lifu Huang; Matthew Lamm; Jinhao Lei; Di Lu; Xiaoman Pan"}, {"ref_id": "b3", "title": "Hyrum Anderson", "journal": "", "year": "2018", "authors": "Miles Brundage; Shahar Avin; Jack Clark; Helen Toner; Peter Eckersley; Ben Garfinkel; Allan Dafoe; Paul Scharre; Thomas Zeitzoff; Bobby Filar"}, {"ref_id": "b4", "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification", "journal": "", "year": "2018", "authors": "Joy Buolamwini; Timnit Gebru"}, {"ref_id": "b5", "title": "Semafor: Frame argument resolution with log-linear models", "journal": "Association for Computational Linguistics", "year": "2010", "authors": "Desai Chen; Nathan Schneider; Dipanjan Das; Noah A Smith"}, {"ref_id": "b6", "title": "Cogcompnlp: Your swiss army knife for nlp", "journal": "", "year": "2018", "authors": ""}, {"ref_id": "b7", "title": "Dual use and the ethical responsibility of scientists", "journal": "Archivum immunologiae et therapiae experimentalis", "year": "2008", "authors": "Hans-J\u00f6rg Ehni"}, {"ref_id": "b8", "title": "A density-based algorithm for discovering clusters in large spatial databases with noise", "journal": "", "year": "1996", "authors": "Martin Ester; Hans-Peter Kriegel; J\u00f6rg Sander; Xiaowei Xu"}, {"ref_id": "b9", "title": "Identifying relations for open information extraction", "journal": "", "year": "2011", "authors": "Anthony Fader; Stephen Soderland; Oren Etzioni"}, {"ref_id": "b10", "title": "Vits: video tagging system from massive web multimedia collections", "journal": "", "year": "2017", "authors": "Delia Fern\u00e1ndez; David Varas; Joan Espadaler; Issey Masuda; Jordi Ferreira; Alejandro Woodward; David Rodr\u00edguez; Xavier Gir\u00f3-I Nieto; Juan Carlos Riveiro; Elisenda Bou"}, {"ref_id": "b11", "title": "Allennlp: A deep semantic natural language processing platform", "journal": "", "year": "2018", "authors": "Matt Gardner; Joel Grus; Mark Neumann; Oyvind Tafjord; Pradeep Dasigi; Nelson Liu; Matthew Peters; Michael Schmitz; Luke Zettlemoyer"}, {"ref_id": "b12", "title": "Ava: A video dataset of spatio-temporally localized atomic visual actions", "journal": "", "year": "2018", "authors": "Chunhui Gu; Chen Sun; A David; Carl Ross; Caroline Vondrick; Yeqing Pantofaru; Sudheendra Li; George Vijayanarasimhan; Susanna Toderici; Rahul Ricco;  Sukthankar"}, {"ref_id": "b13", "title": "spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing", "journal": "", "year": "2017", "authors": "Matthew Honnibal; Ines Montani"}, {"ref_id": "b14", "title": "The social impact of natural language processing", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Dirk Hovy; Shannon L Spruit"}, {"ref_id": "b15", "title": "Labeled faces in the wild: A database forstudying face recognition in unconstrained environments", "journal": "", "year": "2008", "authors": "B Gary; Marwan Huang; Tamara Mattar; Eric Berg;  Learned-Miller"}, {"ref_id": "b16", "title": "Fddb: A benchmark for face detection in unconstrained settings", "journal": "UMass Amherst technical report", "year": "2010", "authors": "Vidit Jain; Erik Learned-Miller"}, {"ref_id": "b17", "title": "Overview of tac-kbp2014 entity discovery and linking tasks", "journal": "", "year": "2014", "authors": "Heng Ji; Joel Nothman; Ben Hachey"}, {"ref_id": "b18", "title": "The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale", "journal": "", "year": "2018", "authors": "Alina Kuznetsova; Hassan Rom; Neil Alldrin; Jasper Uijlings; Ivan Krasin; Jordi Pont-Tuset; Shahab Kamali; Stefan Popov; Matteo Malloci; Tom Duerig"}, {"ref_id": "b19", "title": "Distributed representations of sentences and documents", "journal": "", "year": "2014", "authors": "Quoc Le; Tomas Mikolov"}, {"ref_id": "b20", "title": "Multilingual entity, relation, event and human value extraction", "journal": "", "year": "2019", "authors": "Manling Li; Ying Lin; Joseph Hoover; Spencer Whitehead; Clare Voss; Morteza Dehghani; Heng Ji"}, {"ref_id": "b21", "title": "", "journal": "Alireza", "year": "", "authors": "Manling Li; Ying Lin; Ananya Subburathinam; Spencer Whitehead; Xiaoman Pan; Di Lu; Qingyun Wang; Tongtao Zhang; Lifu Huang; Ji Heng"}, {"ref_id": "b22", "title": "Gaia at sm-kbp 2019 -a multi-media multi-lingual knowledge extraction and hypothesis generation system", "journal": "", "year": "2019", "authors": "Hassan Zareian; Brian Akbari; Bo Chen; Emily Wu; Shih-Fu Allaway; Kathleen Chang; Yixiang Mckeown; Jennifer Yao; Eric Chen; Kexuan Berquist; Xujun Sun; Ryan Peng; Marjorie Gabbard; Pedro Freedman; T K Szekely;  Kumar"}, {"ref_id": "b23", "title": "Microsoft coco: Common objects in context", "journal": "Springer", "year": "2014", "authors": "Tsung-Yi Lin; Michael Maire; Serge Belongie; James Hays; Pietro Perona; Deva Ramanan; Piotr Doll\u00e1r; C Lawrence Zitnick"}, {"ref_id": "b24", "title": "An attentive fine-grained entity typing model with latent type representation", "journal": "", "year": "2019", "authors": "Ying Lin; Heng Ji"}, {"ref_id": "b25", "title": "Reliability-aware dynamic feature composition for name tagging", "journal": "", "year": "2019", "authors": "Ying Lin; Liyuan Liu; Heng Ji; Dong Yu; Jiawei Han"}, {"ref_id": "b26", "title": "Scientific information extraction with semi-supervised neural tagging", "journal": "", "year": "2017", "authors": "Yi Luan; Mari Ostendorf; Hannaneh Hajishirzi"}, {"ref_id": "b27", "title": "The stanford corenlp natural language processing toolkit", "journal": "", "year": "2014", "authors": "D Christopher; Mihai Manning; John Surdeanu; Jenny Rose Bauer; Steven Finkel; David Bethard;  Mc-Closky"}, {"ref_id": "b28", "title": "Model cards for model reporting", "journal": "", "year": "2019", "authors": "Margaret Mitchell; Simone Wu; Andrew Zaldivar; Parker Barnes; Lucy Vasserman; Ben Hutchinson; Elena Spitzer; Deborah Inioluwa; Timnit Raji;  Gebru"}, {"ref_id": "b29", "title": "Universal dependencies v1: A multilingual treebank collection", "journal": "", "year": "2016", "authors": "Joakim Nivre; Marie-Catherine De Marneffe; Filip Ginter; Yoav Goldberg; Jan Hajic; D Christopher; Ryan Manning; Slav Mcdonald; Sampo Petrov; Natalia Pyysalo;  Silveira"}, {"ref_id": "b30", "title": "Large-scale image retrieval with attentive deep local features", "journal": "", "year": "2017", "authors": "Hyeonwoo Noh; Andre Araujo; Jack Sim; Tobias Weyand; Bohyung Han"}, {"ref_id": "b31", "title": "Unsupervised entity linking with abstract meaning representation", "journal": "", "year": "2015", "authors": "Xiaoman Pan; Taylor Cassidy; Ulf Hermjakob; Ji Heng; Kevin Knight"}, {"ref_id": "b32", "title": "Crosslingual name tagging and linking for 282 languages", "journal": "", "year": "2017", "authors": "Xiaoman Pan; Boliang Zhang; Jonathan May; Joel Nothman; Kevin Knight; Heng Ji"}, {"ref_id": "b33", "title": "Deep contextualized word representations", "journal": "", "year": "2018", "authors": "E Matthew; Mark Peters; Mohit Neumann; Matt Iyyer; Christopher Gardner; Kenton Clark; Luke Lee;  Zettlemoyer"}, {"ref_id": "b34", "title": "Object retrieval with large vocabularies and fast spatial matching", "journal": "IEEE", "year": "2007", "authors": "James Philbin; Ondrej Chum; Michael Isard; Josef Sivic; Andrew Zisserman"}, {"ref_id": "b35", "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models", "journal": "", "year": "2015", "authors": "A Bryan; Liwei Plummer; Chris M Wang; Juan C Cervantes; Julia Caicedo; Svetlana Hockenmaier;  Lazebnik"}, {"ref_id": "b36", "title": "Closing the ai accountability gap: Defining an end-to-end framework for internal algorithmic auditing", "journal": "", "year": "2020", "authors": "Andrew Inioluwa Deborah Raji; Rebecca N Smart; Margaret White; Timnit Mitchell; Ben Gebru; Jamila Hutchinson; Daniel Smith-Loud; Parker Theron;  Barnes"}, {"ref_id": "b37", "title": "Youtubeboundingboxes: A large high-precision humanannotated data set for object detection in video", "journal": "", "year": "2017", "authors": "Esteban Real; Jonathon Shlens; Stefano Mazzocchi; Xin Pan; Vincent Vanhoucke"}, {"ref_id": "b38", "title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "journal": "", "year": "2015", "authors": "Kaiming Shaoqing Ren; Ross He; Jian Girshick;  Sun"}, {"ref_id": "b39", "title": "Gender bias in coreference resolution", "journal": "", "year": "2018", "authors": "Rachel Rudinger; Jason Naradowsky; Brian Leonard; Benjamin Van Durme"}, {"ref_id": "b40", "title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition", "journal": "", "year": "2003", "authors": "F Erik; Fien Sang;  De Meulder"}, {"ref_id": "b41", "title": "Facenet: A unified embedding for face recognition and clustering", "journal": "", "year": "2015", "authors": "Florian Schroff; Dmitry Kalenichenko; James Philbin"}, {"ref_id": "b42", "title": "Genre separation network with adversarial training for cross-genre relation extraction", "journal": "", "year": "2018", "authors": "Ge Shi; Chong Feng; Lifu Huang; Boliang Zhang; Heng Ji; Lejian Liao; Heyan Huang"}, {"ref_id": "b43", "title": "From light to rich ere: annotation of entities, relations, and events", "journal": "", "year": "2015", "authors": "Zhiyi Song; Ann Bies; Stephanie Strassel; Tom Riese; Justin Mott; Joe Ellis; Jonathan Wright; Seth Kulick; Neville Ryant; Xiaoyi Ma"}, {"ref_id": "b44", "title": "Yago: A large ontology from wikipedia and wordnet", "journal": "Journal of Web Semantics", "year": "2008", "authors": "M Fabian; Gjergji Suchanek; Gerhard Kasneci;  Weikum"}, {"ref_id": "b45", "title": "Privacy, algorithms, and artificial intelligence", "journal": "", "year": "2019", "authors": "Catherine Tucker"}, {"ref_id": "b46", "title": "Ace 2005 multilingual training corpus. Linguistic Data Consortium", "journal": "", "year": "2006", "authors": "Christopher Walker; Stephanie Strassel; Julie Medero; Kazuaki Maeda"}, {"ref_id": "b47", "title": "Paperrobot: Incremental draft generation of scientific ideas", "journal": "", "year": "2019", "authors": "Qingyun Wang; Lifu Huang; Zhiying Jiang; Kevin Knight; Heng Ji; Mohit Bansal; Yi Luan"}, {"ref_id": "b48", "title": "and Boyan Onyshkevych. 2020. Covid-19 literature knowledge graph construction and drug repurposing report generation", "journal": "Ahmed Elsayed", "year": "", "authors": "Qingyun Wang; Manling Li; Xuan Wang; Nikolaus Parulian; Guangxing Han; Jiawei Ma; Jingxuan Tu; Ying Lin; Haoran Zhang; Weili Liu; Aabhas Chauhan; Yingjun Guan; Bangzheng Li; Ruisong Li; Xiangchen Song; Heng Ji; Jiawei Han; Shih-Fu Chang; James Pustejovsky; David Liem"}, {"ref_id": "b49", "title": "An intelligent multimedia information system for multimodal content extraction and querying", "journal": "", "year": "2018", "authors": "Adnan Yazici; Murat Koyuncu; Turgay Yilmaz; Saeid Sattari; Mustafa Sert; Elvan Gulen"}, {"ref_id": "b50", "title": "Elisa-edl: A cross-lingual entity extraction, linking and localization system", "journal": "", "year": "2018", "authors": "Boliang Zhang; Ying Lin; Xiaoman Pan; Di Lu; Jonathan May; Kevin Knight; Heng Ji"}, {"ref_id": "b51", "title": "Joint face detection and alignment using multitask cascaded convolutional networks", "journal": "IEEE Signal Processing Letters", "year": "2016", "authors": "Kaipeng Zhang; Zhanpeng Zhang; Zhifeng Li; Yu Qiao"}, {"ref_id": "b52", "title": "Gaia -a multi-media multi-lingual knowledge extraction and hypothesis generation system", "journal": "", "year": "2018", "authors": "Tongtao Zhang; Ananya Subburathinam; Ge Shi; Lifu Huang; Di Lu; Xiaoman Pan; Manling Li; Boliang Zhang; Qingyun Wang; Spencer Whitehead; Heng Ji; Alireza Zareian; Hassan Akbari; Brian Chen; Ruiqi Zhong; Steven Shao; Emily Allaway; Shih-Fu Chang; Kathleen Mckeown; Dongyu Li; Xin Huang; Kexuan Sun; Xujun Peng; Ryan Gabbard; Marjorie Freedman; Mayank Kejriwal; Ram Nevatia; Pedro Szekely; T K Kumar; Ali Sadeghian; Giacomo Bergami"}, {"ref_id": "b53", "title": "Learning deep features for discriminative localization", "journal": "", "year": "2016", "authors": "Bolei Zhou; Aditya Khosla; Agata Lapedriza; Aude Oliva; Antonio Torralba"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: An example of cross-media knowledge fusion and a look inside the visual knowledge extraction.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: The architecture of GAIA multimedia knowledge extraction.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Examples of visual entity linking, based on face recognition, landmark recognition and flag recognition.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": "ELMo-LSTM CRF Entity Extractor Attentive Fine-Grained Entity Typing Bi-LSTM CRFs Trigger Extractor FrameNet & Dependency based Fine-Grained Event TypingTextual Entity Coreference Collective Entity Linking and NIL Clustering Contextual Nominal Coreference CNN Argument Extractor Rule based ClassActivation Map Model Fine-Grained FaceNet Dependency based Fine-Grained Relation Typing Assembled CNN Extractor Graph based Coreference Resolution Flag Recognition Fusion and MTCNN Face Landmark Matching Pruning DetectorGeneric Features Visual KB Face Features Cross-Media Fusion Visual Grounding Multimedia KB Cross-modal Entity Linking DBSCAN Heuristics Clustering Rules"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ": Performance of each component. Thebenchmarks references are: CoNLL-2003 (Sang andDe"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": ": GAIA achieves top performance on Task 1 atthe recent NIST TAC SM-KBP2019 evaluation."}], "formulas": [], "doi": "10.18653/v1/P16-2096"}