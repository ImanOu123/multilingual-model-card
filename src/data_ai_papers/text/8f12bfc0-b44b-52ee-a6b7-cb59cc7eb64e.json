{"title": "Entropy Eestimation via Normalizing Flow", "authors": "Ziqiao Ao; Jinglai Li", "pub_date": "", "abstract": "Entropy estimation is an important problem in information theory and statistical science. Many popular entropy estimators suffer from fast growing estimation bias with respect to dimensionality, rendering them unsuitable for high-dimensional problems. In this work we propose a transform-based method for high-dimensional entropy estimation, which consists of the following two main ingredients. First by modifying the k-NN based entropy estimator, we propose a new estimator which enjoys small estimation bias for samples that are close to a uniform distribution. Second we design a normalizing flow based mapping that pushes samples toward a uniform distribution, and the relation between the entropy of the original samples and the transformed ones is also derived. As a result the entropy of a given set of samples is estimated by first transforming them toward a uniform distribution and then applying the proposed estimator to the transformed samples. Numerical experiments demonstrate the effectiveness of the method for high-dimensional entropy estimation problems.", "sections": [{"heading": "Introduction", "text": "Entropy is one of the most fundamental concepts in information theory, and has also found vast applications in other disciplines such as physics, statistics and machine learning. For example, in the data science contexts, various applications rely critically on the estimation of entropy, including goodness-of-fit testing (Vasicek 1976;Goria et al. 2005), sensitivity analysis (Azzi, Sudret, and Wiart 2020), parameter estimation (Ranneby 1984;Wolsztynski, Thierry, and Pronzato 2005), and Bayesian experimental design (Sebastiani and Wynn 2000;Ao and Li 2020).\nAs a concept of the average surprisal in a variable's possible outcomes, entropy provides a natural answer to measuring the uncertainty of probability distribution of interest. In this work we focus on the continuous version of entropy that takes the form,\nH(X) = \u2212 log[p x (x)]p x (x)dx,(1)\nwhere p x (x) is probability density function of a random variable X. Despite the rather simple definition, entropy only admits an analytical expression for a limited family of distributions and needs to be evaluated numerically in general.\nWhen the distribution of interest is analytically available, in principle its entropy can be estimated by numerical integration schemes such as the Monte Carlo method. However, in many real-world applications, the distribution of interest is not analytically available, and one has to estimate the entropy from the i.i.d. realizations drawn from the target distribution, which makes exact computation of the entropy difficult or even impossible. Entropy estimation has attracted considerable attention from various communities in the last a few decades, and a large number of methods have been developed to directly evaluate entropy from realizations. In this work we only consider non-parametric approaches which do not assume any parametric model of the target distribution, and those methods can be broadly classified into two categories. The first line of methods, known as the plug-in estimators, are to estimate the unknown probability density, and then compute the integral in Eq. (1) using numerical integration or Monte Carlo (see (Beirlant et al. 1997) for a detailed description). Some examples of density estimation approaches that have been studied for plug-in methods are kernel density estimator (Joe 1989;Hall and Morton 1993), histogram estimator (Gy\u00f6rfi and Van der Meulen 1987;Hall and Morton 1993) and field-theoretic approach (Chen, Tareen, and Kinney 2018). A major limitation of this type of methods is that they rely on an effective density estimation, which is a difficult problem in its own right, especially when the dimensionality of the problem is high. A different strategy is to directly estimate the entropy from the independent samples of the random variable. Methods following this line include sample-spacing (Miller 2003) and k-nearest neighbors (k-NN) (Kozachenko and Leonenko 1987;Kraskov, St\u00f6gbauer, and Grassberger 2004) based estimators. The latter is particularly appealing among the existing estimation methods for its theoretical and computational advantages and has been widely used in practical problems. More recent variants and extensions of the k-NN methods include (Gao, Ver Steeg, and Galstyan 2015;Berrett et al. 2019).\nEntropy estimation becomes increasingly more difficult as the dimensionality grows, and such difficulty is mainly due to the estimation bias, which decays very slowly with respect to sample size for high-dimensional problems. For example in many popular approaches including the k-NN method (Kozachenko and Leonenko 1987), the estimation bias decays at the rate of O(N \u2212\u03b3/d ) where N is the sample size, d is the dimensionality, and \u03b3 is a positive constant (Krishnamurthy et al. 2014;Kandasamy et al. 2015;Gao, Oh, and Viswanath 2018;Sricharan, Wei, and Hero 2013). As a result very few if not none of the existing entropy estimation methods can effectively handle high-dimensional problems without strong assumptions on the smoothness of the underlying distribution ( (Kandasamy et al. 2015)).\nThe main goal of this work is to provide an effective entropy estimation approach which can achieve faster bias decaying rate under mild smoothness assumption, and thus can apply to high-dimensional problems (i.e., ones of 20 dimensions or higher 1 ). The method presented here consists of two main ingredients. We propose two truncated k-NN estimators based on those by (Kozachenko and Leonenko 1987) and (Kraskov, St\u00f6gbauer, and Grassberger 2004) respectively, and also provide the bounds of the estimation bias in these estimators. Remarkably our theoretical results suggest that the estimators achieve zero bias for uniform distributions, while there is no such a result for any existing k-NN based estimators, according to the bias analysis that are available to date (Gao, Oh, and Viswanath 2018;Singh and P\u00f3czos 2016;Biau and Devroye 2015). This property offers the possibility to significantly improve the performance of entropy estimation by mapping the data points toward a uniform distribution. Therefore the second main ingredient of the method is the normalizing flow (NF) technique (Rezende and Mohamed 2015;Papamakarios et al. 2021) which constructs a sequence of invertible and differentiable mappings that transform a simple base distribution such as standard Gaussian into a more complicated distribution whose density function may not be available. Specifically we use the Masked Autoregressive Flow (Papamakarios, Pavlakou, and Murray 2017), a NF algorithm originally developed for density estimation, combined with the probability integral transform, to push the original data points towards the uniform distribution. We then estimate the entropy of the resulting near-uniform data points with the proposed truncated k-NN estimators, and derive that of the original ones accordingly (by adding an entropic correction term due to the transformation). Therefore, by combining the truncated k-NN estimators and the normalizing flow model, we are able to decode a complex high-dimensional distribution represented by realizations, and obtain an accurate estimation of its entropy. Finally, we provide several complex high-dimensional distributions to demonstrate the performance of the proposed scheme and apply it to Bayesian experimental design problems. 1 We note that in many statistical applications, problems of 20 dimensions are not regarded as \"high-dimensional\". However, the well-known minimax bias results (e.g., (Han et al. 2020;Birg\u00e9 and Massart 1995)) indicate that without the strong smoothness assumption ( (Kandasamy et al. 2015)), the curse of dimensionality is inevitable, and as a result problems of 20 dimensions or higher are deemed \"high-dimensional\" in entropy estimation.", "publication_ref": ["b35", "b10", "b2", "b25", "b36", "b29", "b0", "b3", "b15", "b12", "b11", "b12", "b7", "b21", "b17", "b18", "b8", "b4", "b17", "b19", "b16", "b9", "b33", "b16", "b17", "b18", "b9", "b32", "b5", "b26", "b23", "b24", "b13", "b6", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "k-NN Based Entropy Estimation", "text": "We provide a brief introduction to two traditional k-NN based entropy estimators in this section. We start with the original k-NN entropy estimator proposed by (Kozachenko and Leonenko 1987), where the k-th nearest neighbor is contained in the smallest possible closed ball. Next, we introduce a popular variant of the k-NN estimator proposed in (Kraskov, St\u00f6gbauer, and Grassberger 2004), and this method uses the smallest possible hyper-rectangle to cover at least k points. We finally discuss some theoretical analysis of estimation errors in the estimators.", "publication_ref": ["b17", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Kozachenko-Leonenko Estimator", "text": "Recall the definition of entropy in Eq. (1). Given a density estimator p x (x) for p x (x) and a set of N i.i.d. samples S = {x (i) } N i=1 drawn from p x (x), the entropy of the random variable X can be estimated as follows:\nH(X) = \u2212N \u22121 N i=1 log p x (x (i) ).\n(2)\nThe Kozachenko-Leonenko (KL) estimator depends on a local uniformity assumption to obtain the estimate p x (x). For each x (i) , one first identifies the k-nearest neighbors (in terms of the p-norm distance) of it, and defines the smallest closed ball covering all these k neighbors as:\nB(x (i) , i /2) = {x \u2208 R d x \u2212 x (i) p \u2264 i /2},\nwhere i be twice the distance between x (i) and its k-th nearest neighbor among the set S. We shall refer to the closed ball B(x (i) , i /2) as a cell centered at x (i) , and let q i be the mass of the cell B(x (i) , i /2) , i.e.,\nq i ( i ) = x\u2208B(x (i) , i /2) p x (x)dx.\nIt can be derived that the expectation value of log q i over i is given by\nE(log q i ) = \u03c8(k) \u2212 \u03c8(N ),(3)\nwhere \u03c8(x) = \u0393 (x) \u0393(x) with \u0393(x) being the Gamma function (Kraskov, St\u00f6gbauer, and Grassberger 2004). KL estimator then assumes that the density is constant in B(x (i) , i ), which gives\nq i ( i ) \u2248 c d d i p x (x (i) ),(4)\nwhere d is the dimension of X and\nc d = \u0393(1 + 1 p ) d /\u0393(1 + d p ),\nis the volume of the d-dimensional unit ball with respect to p-norm. Combining (3) and ( 4) one can get an estimate of the log-density at each sample point,\nlog p x (x (i) ) = \u03c8(k) \u2212 \u03c8(N ) \u2212 log c d \u2212 d log i .(5)\nPlugging the above estimates for i = 1, ..., N into (2) yields the KL estimator:\nH KL (X) = \u2212\u03c8(k) + \u03c8(N ) + log c d + d N N i=1 log i . (6)", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "KSG Estimator", "text": "As is mentioned earlier, the Kraskov-St\u00f6gbauer-Grassberger (KSG) estimator is an important variant of\u0124 KL . Unlike KL estimator that is based on closed balls, KSG estimator uses hyper-rectangles to form the cells at each data point. Namely one chooses the \u221e-norm as the distance metric (i.e p = \u221e), and as a result the cell B(x (i) , i /2) becomes a hyper-cube with side length i . Next, we allow the hyper-cube to become a hyper-rectangle: i.e., the cells admit different side lengths along different dimensions. Specifically, for j = 1, ..., d, we define i,j to be twice of the distance between x (i) and its kth nearest neighbor along dimension j, and the cell centered at x (i) covering its k-nearest neighbors becomes\nB(x (i) , i,1:d /2) = {x = (x 1 , ..., x d ) | |x j \u2212 x (i) j | \u2264 i,j /2, for j = 1, ..., d},(7)\nwhere i,1:d = ( i,1 , ..., i,d ). This change leads to a different formula for computing the mass of the cell B(x\n(i) , i,1:d /2), E(log q i ) \u2248 \u03c8(k) \u2212 d \u2212 1 k \u2212 \u03c8(N ).(8)\nIt is worth noting that the equality in Eq. ( 3) is replaced by approximate equality in Eq. ( 8), because a uniform density within the rectangle has to be assumed to obtain Eq. ( 8) (see Lemma 2 in the supplementary information (SI) for details). Using a similar local assumption as Eq. ( 4), the KSG estimator is derived as,\nH KSG (X) = \u2212\u03c8(k) + \u03c8(N ) + d \u2212 1 k + 1 N N i=1 d j=1 log i,j .\n(9) We note that the KSG method was actually developed in the context of estimating mutual information (Kraskov, St\u00f6gbauer, and Grassberger 2004), and has been reported to outperform the KL estimator in a wide range of problems (Gao, Oh, and Viswanath 2018). As has been shown above, it is straightforward to extend it to entropy estimation, and our numerical experiments also suggest that it has competitive performance as an entropy estimator, which will be demonstrated in the numerical experiments.", "publication_ref": ["b18", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Convergence Analysis", "text": "Another important issue is to analyze the estimation errors in these entropy estimators and especially how they behave as the sample size increases. In most of the k-NN based estimators including the two mentioned above, the variance is generally well controlled, decaying at a rate of O(N \u22121 ) with N being the sample size, while the main issue lies on the estimation bias. In fact, the bias of estimator H KL has been well studied, but that of H KSG receives very little attention. Previous results related to the former are listed as follows. The original (Kozachenko and Leonenko 1987) paper established the asymptotic unbiasedness for k = 1 while (Singh et al. 2003) obtained the same result for general k. For distributions with unbounded support, (Tsybakov and It should be noted that the bias bounds given by previous studies typically depend on some properties of target densities, such as smoothness parameter and Hessian matrix, providing insights that these estimators perform well on certain distributions that satisfy certain conditions. This motivates the idea that one can transform the given data points toward a desired distribution for a more accurate entropy estimation, which is detailed in next section.", "publication_ref": ["b17", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Uniformizing Mapping Based Entropy Estimation", "text": "In this section, we shall present an entropy estimation approach that is based on normalizing flow. As is mentioned earlier, it consists of two main ingredients: a truncated version of the k-NN entropy estimators, and a transformation that can map data points toward a uniform distribution.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Truncated KL/KSG Estimators", "text": "For compactly supported distributions, a significant source of bias comes from the boundary of the support, where the k-NN cells are constructed including areas outside of the support of the distribution density (Singh and P\u00f3czos 2016). Intuitively speaking, incorrectly including such areas results in an underestimate of the densities, leading to bias in the estimator. We thus propose a method to reduce the estimation bias by excluding the areas outside of the distribution support, and remarkably the resulting estimator enjoy certain convergence properties which enable us to design the NF based estimation approach. The only additional requirement for using these estimators is that the bound of support of density should be specified. Without loss of generality, we suppose the target density is supported on the unit cube Q := [0, 1] d in R d . The procedure of our method is as follows: we first determine all the cells using either KL or KSG, then examine whether each k-NN cell covers area out of the distribution support, and if so, truncate the cell at the boundary to exclude such area. Mathematically the truncated KL (tKL) estimator (with \u221e-norm), is given by\nH tKL (X) = \u2212\u03c8(k) + \u03c8(N ) + 1 N N i=1 d j=1 log \u03be i,j , (10\n)\nwhere\n\u03be i,j = min{x (i) j + i /2, 1} \u2212 max{x (i) j \u2212 i /2, 0}\n; and the truncated KSG (tKSG) esitmator is given by\nH tKSG (X) = \u2212\u03c8(k) + \u03c8(N ) + (d \u2212 1)/k + 1 N N i=1 d j=1 log \u03b6 i,j ,(11)\nwhere\n\u03b6 i,j = min{x (i) j + i,j /2, 1} \u2212 max{x (i) j \u2212 i,j /2, 0}.\nTo numerically demonstrate the improvement of the truncated estimators over the conventional version, we compare their performances on multidimensional Beta distributions with various shape parameters, and the results are reported in the SI.\nNext we shall theoretically analyze the bias of the truncated estimators. Our analysis relies on some assumptions on the density function p x , which are summarized as below: Assumption 1. The distribution p x satisfies: (c) The gradient of p x is uniformly bounded on Q o , i.e., C 2 = sup\nx\u2208Q o || p x (x)|| 1 < \u221e.\nFirst we consider the bias of estimator H tKL and the following theorem states that, the bias in H tKL is bounded and vanishes at the rate of O(N \u2212 1 d ). Theorem 1. Under Assumption 1 and for any finite k and d, the bias of the truncated KL estimator is bounded by\nE[ H tKL (X)] \u2212 H(X) \u2264 C 2 C 1+1/d 1 k N 1 d .", "publication_ref": ["b32"], "figure_ref": [], "table_ref": []}, {"heading": "The variance of the truncated KL estimator is bounded by", "text": "Var\n[ H tKL (X)] \u2264 C 1 N ,\nfor some C > 0.\nProof. See the SI.\nNote that C 2 = 0 when p x is uniform on Q, and the following corollary follows directly: Corollary 1. Under the assumption in Theorem 1, if X is uniformly distributed on Q, then the truncated KL estimator is unbiased.\nThis corollary is the theoretical foundation of the proposed method, as it suggests that if one can transform the data points into a uniform distribution, the tKL method can yield an unbiased estimate. In reality, it is usually impossible to map the data point exactly into a uniform distribution to achieve the unbiased estimate. To this end, Theorem 1 suggests that, as long as the transformed samples are close to a uniform distribution in the sense that C 2 is small, the transformation can still significantly reduce the bias. Since the main contribution of the mean-square estimation error comes from the bias (as the variance decays at the rate of O(N \u22121 )), reducing the bias therefore leads much more accurate estimation of the entropy.\nWe next consider the bias of the tKSG estimator. The second theorem shows that the expectation of H tKSG has the same limiting behavior up to a polylogarithmic factor in N . Theorem 2. Under Assumption 1 and for any finite k and d, the bias of the truncated KSG estimator is bounded by\nE[ H tKSG (X)] \u2212 H(X) \u2264 C (log N ) k+2 C k+1 1 N 1 d for some C > 0. The variance of the truncated KSG estimator is bounded by Var[ H tKSG (X)] \u2264 C (log N ) k+2 N ,\nfor some C > 0.\nProof. See the SI.\nAs one can see from Theorem 2, while the uniform distribution leads to zero bias for H tKL , we can not obtain the same result for H tKSG , which means no theoretical justification for mapping the data points toward a uniform distribution for this estimator. That said, the tKSG estimator and Theorem 2 are still useful, and the reason for that is two-fold. First as is mentioned earlier, no existing result on the bound of bias is available for the KSG estimator to the best of our knowledge, and to this end our analysis on tKSG is the first known bias bound for this type of estimators, and may provide useful information for understanding the convergence property of them. More importantly, our numerical experiments demonstrate that mapping the data points toward a uniform distribution does significantly improve the performance of tKSG as well. In fact, we have found that tKSG can achieve the same or slightly better results than tKL on the transformed samples in our test cases.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Estimating Entropy via Transformation", "text": "As is mentioned earlier, based on the interesting convergence properties of the truncated estimators in particularly tKL, we want to estimate the entropy of a given set of samples by mapping them toward a uniform distribution. To implement this idea, an essential question to ask is that, how the entropy of the transformed samples relates to that of the original ones. Proposition 1 provides an answer to this question.\nProposition 1 ((Ihara 1993)). Let f be a mapping: R d \u2192 R d , X be random variable defined on R d following distribution p x , and Z = f (X). If f is bijective and differentiable, we have\nH(X) = H(Z) + p z (z) log det \u2202f \u22121 (z) \u2202z dz, (12\n)\nwhere p z (z) is the distribution of Z.\nTherefore given a data set S = {x (i) )} N i=1 and a mapping Z = f (X), from Eq. ( 12) we can construct an entropy estimator of X as,\nH(X) = H(Z) + 1 n n i=1 log det \u2202f \u22121 (z (i) ) \u2202z ,(13)\nwhere H(Z) is an entropy estimator of Z (either tKL or tKSG) based on the transformed samples\nS Z = {z (i) = f (x (i) )} n i=1 .\nWe refer to such a mapping f (\u2022) as a uniformizing mapping (UM) and the resulting method as a UM based entropy estimator. A central question in implementation of the algorithm is obviously how to construct a UM which can push the samples toward a uniform distribution, which is discussed in next section.\nThe bias of the UM based estimator relies on the property of the UM (or equivalently the NF), on which we make the following assumption: Based on Theorem 1 and Theorem 2, we can obtain a bound of the bias of the UM based estimator.\nAssumption 2. Let S = {x (i) } N i=1\nCorollary 2. Suppose that the density function of the original distribution is differentiable and the UM satisfies Assumption 2. The bias of UM-tKL estimator is bounded by\nE[ H UM\u2212tKL (X)] \u2212 H(X) \u2264 C N UM\u2212tKL k N 1 d , (14)\nwhere lim N \u2192\u221e C N UM\u2212tKL = 0, and the bias of UM-tKSG estimator is bounded by\nE[ H UM\u2212tKSG (X)] \u2212 H(X) \u2264 C U M \u2212tKSG (log N ) k+2 N 1 d , (15\n)\nwhere C U M \u2212tKSG = C (1+C) (1+C) d +1 (1\u2212C) k+1\nand C is a positive constant.\nProof. See the SI.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Constructing UM via Normalizing Flow", "text": "We discuss in this section how to construct a UM via the NF method. First since the image of f is [0, 1] d , we assume that f is in the form of f = \u03a6 \u2022 g where g : R d \u2192 R d and \u03a6 : R d \u2192 [0, 1] d is prescribed. Recall that p z is the distribution of Z = f (X) with X following p x , and we want the function g by minimize the Kullback-Leibler divergence (KLD) between p z and the uniform distribution p u :\nmin g\u2208\u2126 D(p z |p u ) := p z (z) log p z (z) p u (z) dz,(16)\nwhere\nz = \u03a6 \u2022 g(x)\nand \u2126 is a suitable function space. Solving Eq. ( 16) directly poses some computational difficulty as the calculation involves the function \u03a6, the choice of which may affect the computational efficiency. To simplify the computation, we recall the following proposition: Proposition 2 ( (Papamakarios et al. 2021)). Let T : Y \u2192 Z be a bijective and differentiable transformation, and p z (z) be the distribution obtained by passing p y (y) through T . Then the equality\nD(\u03c0 y (y)||p y (y)) = D(\u03c0 z (z)||p z (z))(17)\nholds.\nWe now construct the mapping \u03a6 with the cumulative distribution function of the standard normal distribution, a technique known as the probability integral transform, yielding, for a given y \u2208 R d ,\n\u03a6(y) = (\u03c6 1 (y 1 ), ..., \u03c6 d (y d )), \u03c6 i (y i ) = 1 2 (1 + erf( y \u221a 2 )),\nwhere erf(\u2022) is the error function. It should be clear that if y follows a standard normal distribution, z = \u03a6(y) follows a uniform distribution in [0, 1] d , and vice versa. Now applying Proposition 2, we can show that Eq. ( 16) is equivalent to\nmin g\u2208\u2126 D(p y (y)|q(y)),(18)\nwhere y = g(x) follows distribution p y (\u2022) and q(\u2022) is the standard normal distribution. Now assume that g(\u2022) is invertible and let its inverse be h = g \u22121 . We also assume that both g and h are differentiable. Applying Proposition 2 to Eq. (18) with T = h, we find that Eq. ( 18) is equivalent to\nmin h\u2208\u2126 \u22121 D(p x (x)|q h (x)),(19)\nwhere \u2126 \u22121 = {g \u22121 |g \u2208 \u2126} and q h is the distribution obtained by passing q through the mapping h:\nq h (x) = q h \u22121 (x) det \u2202h \u22121 \u2202x .(20)\nEq. ( 19) essentially says that we want to push a standard normal distribution q toward a target distribution p x , and therefore solving Eq. ( 19) falls naturally into the framework of NF, the details of which are provided in SI. Once the mapping h(\u2022) (or equivalently g \u22121 (\u2022)) is obtained, it can be inserted directly into Algorithm 1 to estimate the sought entropy. In practice, the samples are split into two sets, where one of them is used to construct the UM and the other is used to estimate the entropy. ", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "Numerical Experiments Multivariate Normal Distribution", "text": "To validate the idea of UM based entropy estimator, a natural question to ask is that how it works with a perfect NF transformation, that yields exactly normally distributed samples. To answer this question, we first conduct the numerical tests with the standard multivariate normal distribution, corresponding to the situation that one has done a perfect NF. Specifically we test the four methods: KL, KSG, UM-tKL and UM-tKSG, and we conduct two sets of tests: in the first one we fix the sample size to be 1000 and vary the dimensionality, while in the second one we fix the dimensionality to be 40 and vary the sample size. All the tests are repeated 100 times and the Root-mean-square-error (RMSE) of the estimates are calculated. In Fig. 2 (left), we plot the RMSE (on a logarithmic scale) as a function of the dimensionality. One can see from this figure that, as the dimensionality increases, the estimation error in KL and KSG grows significantly faster than that in the two UM based ones, with the error in KL being particularly large. Next in Fig. 2 (right) we plot the RMSE against the sample size N (note that the plot is on a log-log scale) for d = 40, which shows that for this high-dimensional case, the two UM based estimators yield much lower and faster-decaying RMSE than those two estimators on the original samples. Overall these results support the theoretical findings that the estimation error can be significantly reduced by mapping the target samples toward a uniform distribution.", "publication_ref": [], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Multivariate Rosenbrock Distribution", "text": "In this example we shall see how the proposed method performs when NF is included. Specifically our example is the Rosenbrock type of distributions -the standard Rosenbrock distribution is 2-D and widely used as a testing example for various of statistical methods. Here we consider two high-dimensional extensions of the 2-D Rosenbrock (Pagani, Wiegand, and Nadarajah 2019): the hybrid Rosenbrock (HR) and the even Rosenbrock (ER) distributions. The details of the two distributions including their density functions are provided in SI. The Rosenbrock distribution is strongly non-Gaussian, and that can be demonstrated by Fig. 3 (left) which shows the samples drawn from 2-D Rosenbrock. As a comparison, Fig. 3 (right) shows the samples that have been transformed toward a uniform distribution and used in entropy estimation.\nIn this example we compare the performance of seven estimators: in addition to the four used in the previous example, we include an estimator only using NF (details in SI) as well as two state-of-the-art entropy estimators: CADEE (Ariel and Louzoun 2020) and the von-Mises based estimator (Kandasamy et al. 2015). First we test how the estimators scale with respect to dimensionality, where the sample size is taken to be N = 500d. With each method, the experiment is repeated 20 times and the RMSE is calculated. The RMSE against the dimensionality d for both test distributions is plotted in Figs. 4 (a) and (b). One can observe here that in most cases, the UM based methods (especially UM-tKSG) offer the best performance. An exception is that CADEE performs better in low dimensional cases for HR, but its RMSE grows much higher than that of the UM methods in the highdimensional regime (d > 15). Our second experiment is to fix the dimensionality at d = 10 and vary the sample size, where the RMSE is plotted against the sample size for both HR and ER in Figs. 4 (c) and (d). The figures show clearly that the RMSE of the UM based estimators decays faster than other methods in both examples, with the only exception being CADEE in the small sample (\u2264 10 4 ) regime of ER. It is also worth noting that, though it is not justified theoretically, UM-tKSG seems to perform slightly better than UM-tKL in all the cases.", "publication_ref": ["b22", "b16"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Application to Optimal Experimental Design", "text": "In this section, we apply entropy estimation to an optimal experimental design (OED) problem. Simply put, the goal of OED is to determine the optimal experimental conditions (e.g., locations of sensors) that maximize certain utility function associated with the experiments. Mathematically let \u03bb \u2208 D be design parameters representing experimental conditions, \u03b8 be the parameter of interest, and Y be the observed data. An often used utility function is the entropy of the data Y , resulting in the so-called maximum entropy sampling method (MES) (Sebastiani and Wynn 2000):\nmax \u03bb\u2208D U (\u03bb) := H(Y |\u03bb),(21)\nand therefore evaluating U (\u03bb) becomes an entropy estimation problem. This utility function is equivalent to the mutual entropy criterion under certain conditions (Shewry and Wynn 1987). This formulation is particularly useful for problems with expensive or intractable likelihoods, as the likelihoods are not needed if the utility function is computed via entropy estimation. A common application of OED is to determine   Figure 5: Top: some sample data paths of (x, y); Bottom: the optimal observation times obtained by the eight methods.\nthe observation times for stochastic processes so that one can accurately estimate the model parameters and here we provide such an example, arising from the field of population dynamics. Specifically we consider the Lotka-Volterra (LV) predatorprey model (Lotka 1925;Volterra 1927). Let x and y be the populations of prey and predator respectively, and the LV model is given b\u1e8f\nx = ax \u2212 xy,\u1e8f = bxy \u2212 y, where a and b are respectively the growth rates of the prey and the predator. In practice, often the parameters a and b are not known and need to be estimated from the population data. In a Bayesian framework, one can assign a prior distribution on a and b, and infer them from measurements made on the population (x, y). Here we assume that the prior for both a and b is a uniform distribution U [0.5, 4]. In particular we assume that the pair (x+ x , y+ y ), where x , y \u223c N (0, 0.01) are independent observation noises, is measured at d = 5 time points located within the interval [0, 10], and the goal is to determine the observation times for the experiments. As is mentioned earlier, we shall determine the observation times using the MES method. Namely, the design parameter in this example is \u03bb = (t 1 , ..., t d ), the data Y is the pair (x + x , y + y ) measured at t 1 , ..., t d , and we want to find \u03bb that maximizes the entropy H(Y |\u03bb).\nA common practice in such problems is not to optimize the observation times directly and instead parametrize them using the percentiles of a prescribed distribution to reduce the optimization dimensionality (Ryan et al. 2014). Here we use a Beta distribution, resulting in two distribution parameters to be optimized (see (Ryan et al. 2014) and SI for further details). We solve the resulting optimization problem with a grid search where the entropy is evaluated by the seven aforementioned estimators each with 10,000 samples. We plot in Fig. 5 the optimal observation time placements computed with the seven aforementioned estimators, as well as the equidistant placement for a comparison purpose. Also shown in the figure are some sample paths of the population (x, y) where we can see that the population samples are generally subject to larger variations near the two ends and relative smaller ones in the middle. Regarding the optimization results, we see that the optimal time placements obtained by the two UM based estimators and CADEE are the same, while they are different from the results of other methods. To validate the optimization results, we compute a reference entropy value for the optimal placement obtained by each method, using Nested Monte Carlo (NMC) (see (Ryan 2003) and SI for details) with a large sample size (10 5 \u00d7 10 5 ), and show the results in Table 1. Note that though the NMC can produce a rather accurate entropy estimate, it is too expensive to use directly in this OED problem. Using the reference values as the ground truth, we can further compute the RMSE of these estimates (over 20 repetitions), which are also reported in Table 1. From the table one observes that the placement of observation times computed by the two UM methods and CADEE yields the largest entropy values, which indicates that these three methods clearly outperform all the other estimators in this OED problem. Moreover, from the RMSE results we can see that the UM based methods (especially UM-tKSG) yield smaller RMSE than CADEE, suggesting that they are more statistically reliable than CADEE.", "publication_ref": ["b29", "b30", "b20", "b36", "b27", "b27", "b28"], "figure_ref": [], "table_ref": ["tab_1", "tab_1"]}, {"heading": "Conclusion", "text": "In summary we have presented a NF based entropy estimator, which is supported by both theoretical analysis and numerical experiments. We believe that the method can be useful in a wide range of real-world applications involving entropy estimation, for instance, experiment design, and we plan to explore such applications in future studies.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "This work was partially supported by the China Scholarship Council (CSC). The authors would also like to thank Dr. Alexander Kraskov for discussion about the KSG estimator.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "An approximate KLD based experimental design for models with intractable likelihoods", "journal": "PMLR", "year": "2020", "authors": "Z Ao; J Li"}, {"ref_id": "b1", "title": "Estimating differential entropy using recursive copula splitting", "journal": "Entropy", "year": "2020", "authors": "G Ariel; Y Louzoun"}, {"ref_id": "b2", "title": "Sensitivity analysis for stochastic simulators using differential entropy", "journal": "International Journal for Uncertainty Quantification", "year": "2020", "authors": "S Azzi; B Sudret; J Wiart"}, {"ref_id": "b3", "title": "Nonparametric entropy estimation: An overview", "journal": "International Journal of Mathematical and Statistical Sciences", "year": "1997", "authors": "J Beirlant; E J Dudewicz; L Gy\u00f6rfi; E C Van Der Meulen"}, {"ref_id": "b4", "title": "Efficient multivariate entropy estimation via k-nearest neighbour distances", "journal": "Annals of Statistics", "year": "2019", "authors": "T B Berrett; R J Samworth; M Yuan"}, {"ref_id": "b5", "title": "Lectures on the nearest neighbor method", "journal": "Springer", "year": "2015", "authors": "G Biau; L Devroye"}, {"ref_id": "b6", "title": "Estimation of integral functionals of a density", "journal": "The Annals of Statistics", "year": "1995", "authors": "L Birg\u00e9; P Massart"}, {"ref_id": "b7", "title": "Density estimation on small data sets", "journal": "Physical review letters", "year": "2018", "authors": "W.-C Chen; A Tareen; J B Kinney"}, {"ref_id": "b8", "title": "Efficient estimation of mutual information for strongly dependent variables", "journal": "", "year": "2015", "authors": "S Gao; G Ver Steeg; A Galstyan"}, {"ref_id": "b9", "title": "Demystifying Fixed k-Nearest Neighbor Information Estimators", "journal": "IEEE Transactions on Information Theory", "year": "2018", "authors": "W Gao; S Oh; P Viswanath"}, {"ref_id": "b10", "title": "A new class of random vector entropy estimators and its applications in testing statistical hypotheses", "journal": "Journal of Nonparametric Statistics", "year": "2005", "authors": "M N Goria; N N Leonenko; V V Mergel; P L Inverardi"}, {"ref_id": "b11", "title": "Density-free convergence properties of various estimators of entropy", "journal": "Computational Statistics & Data Analysis", "year": "1987", "authors": "L Gy\u00f6rfi; E C Van Der Meulen"}, {"ref_id": "b12", "title": "On the estimation of entropy", "journal": "Annals of the Institute of Statistical Mathematics", "year": "1993", "authors": "P Hall; S C Morton"}, {"ref_id": "b13", "title": "Optimal rates of entropy estimation over Lipschitz balls", "journal": "The Annals of Statistics", "year": "2020", "authors": "Y Han; J Jiao; T Weissman; Y Wu"}, {"ref_id": "b14", "title": "Information theory for continuous systems", "journal": "World Scientific", "year": "1993", "authors": "S Ihara"}, {"ref_id": "b15", "title": "Estimation of entropy and other functionals of a multivariate density", "journal": "Annals of the Institute of Statistical Mathematics", "year": "1989", "authors": "H Joe"}, {"ref_id": "b16", "title": "Nonparametric von Mises Estimators for Entropies, Divergences and Mutual Informations", "journal": "", "year": "2015", "authors": "K Kandasamy; A Krishnamurthy; B Poczos; L A Wasserman; J M Robins"}, {"ref_id": "b17", "title": "Sample estimate of the entropy of a random vector", "journal": "Problemy Peredachi Informatsii", "year": "1987", "authors": "L Kozachenko; N N Leonenko"}, {"ref_id": "b18", "title": "Estimating mutual information", "journal": "Physical review E", "year": "2004", "authors": "A Kraskov; H St\u00f6gbauer; P Grassberger"}, {"ref_id": "b19", "title": "Nonparametric estimation of renyi divergence and friends", "journal": "PMLR", "year": "2014", "authors": "A Krishnamurthy; K Kandasamy; B Poczos; L Wasserman"}, {"ref_id": "b20", "title": "Elements of physical biology", "journal": "Williams & Wilkins", "year": "1925", "authors": "A J Lotka"}, {"ref_id": "b21", "title": "A new class of entropy estimators for multi-dimensional densities", "journal": "IEEE", "year": "2003", "authors": "E G Miller"}, {"ref_id": "b22", "title": "An ndimensional Rosenbrock Distribution for MCMC Testing", "journal": "", "year": "2019", "authors": "F Pagani; M Wiegand; S Nadarajah"}, {"ref_id": "b23", "title": "Normalizing flows for probabilistic modeling and inference", "journal": "Journal of Machine Learning Research", "year": "2021", "authors": "G Papamakarios; E Nalisnick; D J Rezende; S Mohamed; B Lakshminarayanan"}, {"ref_id": "b24", "title": "Masked autoregressive flow for density estimation", "journal": "", "year": "2017", "authors": "G Papamakarios; T Pavlakou; I Murray"}, {"ref_id": "b25", "title": "The maximum spacing method. An estimation method related to the maximum likelihood method", "journal": "Scandinavian Journal of Statistics", "year": "1984", "authors": "B Ranneby"}, {"ref_id": "b26", "title": "Variational inference with normalizing flows", "journal": "PMLR", "year": "2015", "authors": "D Rezende; Mohamed ; S "}, {"ref_id": "b27", "title": "Towards Bayesian experimental design for nonlinear models that require a large number of sampling times", "journal": "Computational Statistics & Data Analysis", "year": "2014", "authors": "E G Ryan; C C Drovandi; M H Thompson; A N Pettitt"}, {"ref_id": "b28", "title": "Estimating expected information gains for experimental designs with application to the random fatiguelimit model", "journal": "Journal of Computational and Graphical Statistics", "year": "2003", "authors": "K J Ryan"}, {"ref_id": "b29", "title": "Maximum entropy sampling and optimal Bayesian experimental design", "journal": "Journal of the Royal Statistical Society: Series B (Statistical Methodology)", "year": "2000", "authors": "P Sebastiani; H P Wynn"}, {"ref_id": "b30", "title": "Maximum entropy sampling", "journal": "Journal of applied statistics", "year": "1987", "authors": "M C Shewry; H P Wynn"}, {"ref_id": "b31", "title": "Nearest neighbor estimates of entropy. American journal of mathematical and management sciences", "journal": "", "year": "2003", "authors": "H Singh; N Misra; V Hnizdo; A Fedorowicz; E Demchuk"}, {"ref_id": "b32", "title": "Finite-sample analysis of fixed-k nearest neighbor density functional estimators", "journal": "", "year": "2016", "authors": "S Singh; B P\u00f3czos"}, {"ref_id": "b33", "title": "Ensemble estimators for multivariate entropy estimation", "journal": "", "year": "2013", "authors": "K Sricharan; D Wei; A O Hero"}, {"ref_id": "b34", "title": "Root-n consistent estimators of entropy for densities with unbounded support", "journal": "Scandinavian Journal of Statistics", "year": "1996", "authors": "A B Tsybakov; E Van Der Meulen"}, {"ref_id": "b35", "title": "A test for normality based on sample entropy", "journal": "Journal of the Royal Statistical Society: Series B (Methodological)", "year": "1976", "authors": "O Vasicek"}, {"ref_id": "b36", "title": "Variazioni e fluttuazioni del numero d'individui in specie animali conviventi", "journal": "Signal Processing", "year": "1927", "authors": "V ; C Volterra;  Ferrari; E Wolsztynski; E Thierry; L Pronzato"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: The schematic illustration of the truncated estimator. The shaded area is that removed from the k-NN cell.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "(a) p x is continuous and supported on Q; (b) p x is bounded away from 0, i.e., C 1 = inf x\u2208Q p x (x) > 0;", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "be the set of i.i.d samples used to construct the UM and p S z be the resulting density of Z in Eq. (13). Denote C N 2 There exist a positive integer M and a positive real numberC < 1 such that: \u2200N > M, C N 2 \u2264C, a.s.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: Left: RMSE plotted against the dimensionality d. Right: RMSE (on a logarithmic scale) plotted against the sample size N .", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: Left: the original samples drawn from a 2-D Rosenbrock distribution; Right: the UM-transformed samples used in the entropy estimation.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 :4Figure 4: From left to right: RMSE vs. dimensionality for HR (a) and ER (b); RMSE vs. sample size for HR (c) and ER (d).", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "The reference entropy values of the observation time placements obtained by using all the methods. The smallest (best) entropy value is shown in bold.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "H(X) = \u2212 log[p x (x)]p x (x)dx,(1)", "formula_coordinates": [1.0, 104.64, 630.21, 187.86, 9.65]}, {"formula_id": "formula_1", "formula_text": "H(X) = \u2212N \u22121 N i=1 log p x (x (i) ).", "formula_coordinates": [2.0, 371.78, 252.14, 133.94, 30.32]}, {"formula_id": "formula_2", "formula_text": "B(x (i) , i /2) = {x \u2208 R d x \u2212 x (i) p \u2264 i /2},", "formula_coordinates": [2.0, 342.07, 349.35, 193.35, 11.72]}, {"formula_id": "formula_3", "formula_text": "q i ( i ) = x\u2208B(x (i) , i /2) p x (x)dx.", "formula_coordinates": [2.0, 372.98, 427.76, 131.53, 17.84]}, {"formula_id": "formula_4", "formula_text": "E(log q i ) = \u03c8(k) \u2212 \u03c8(N ),(3)", "formula_coordinates": [2.0, 384.4, 473.44, 173.6, 9.65]}, {"formula_id": "formula_5", "formula_text": "q i ( i ) \u2248 c d d i p x (x (i) ),(4)", "formula_coordinates": [2.0, 394.34, 537.65, 163.66, 12.69]}, {"formula_id": "formula_6", "formula_text": "c d = \u0393(1 + 1 p ) d /\u0393(1 + d p ),", "formula_coordinates": [2.0, 382.81, 567.02, 111.87, 22.31]}, {"formula_id": "formula_7", "formula_text": "log p x (x (i) ) = \u03c8(k) \u2212 \u03c8(N ) \u2212 log c d \u2212 d log i .(5)", "formula_coordinates": [2.0, 335.26, 633.46, 222.74, 11.72]}, {"formula_id": "formula_8", "formula_text": "H KL (X) = \u2212\u03c8(k) + \u03c8(N ) + log c d + d N N i=1 log i . (6)", "formula_coordinates": [2.0, 325.31, 677.01, 232.69, 30.32]}, {"formula_id": "formula_9", "formula_text": "B(x (i) , i,1:d /2) = {x = (x 1 , ..., x d ) | |x j \u2212 x (i) j | \u2264 i,j /2, for j = 1, ..., d},(7)", "formula_coordinates": [3.0, 54.62, 213.17, 237.88, 39.84]}, {"formula_id": "formula_10", "formula_text": "(i) , i,1:d /2), E(log q i ) \u2248 \u03c8(k) \u2212 d \u2212 1 k \u2212 \u03c8(N ).(8)", "formula_coordinates": [3.0, 100.45, 274.06, 193.3, 41.79]}, {"formula_id": "formula_11", "formula_text": "H KSG (X) = \u2212\u03c8(k) + \u03c8(N ) + d \u2212 1 k + 1 N N i=1 d j=1 log i,j .", "formula_coordinates": [3.0, 54.0, 394.25, 238.49, 30.32]}, {"formula_id": "formula_12", "formula_text": "H tKL (X) = \u2212\u03c8(k) + \u03c8(N ) + 1 N N i=1 d j=1 log \u03be i,j , (10", "formula_coordinates": [4.0, 62.38, 151.54, 225.97, 30.32]}, {"formula_id": "formula_13", "formula_text": ")", "formula_coordinates": [4.0, 288.35, 162.27, 4.15, 8.64]}, {"formula_id": "formula_14", "formula_text": "\u03be i,j = min{x (i) j + i /2, 1} \u2212 max{x (i) j \u2212 i /2, 0}", "formula_coordinates": [4.0, 71.02, 204.9, 200.21, 14.07]}, {"formula_id": "formula_15", "formula_text": "H tKSG (X) = \u2212\u03c8(k) + \u03c8(N ) + (d \u2212 1)/k + 1 N N i=1 d j=1 log \u03b6 i,j ,(11)", "formula_coordinates": [4.0, 59.56, 245.12, 232.94, 45.48]}, {"formula_id": "formula_16", "formula_text": "\u03b6 i,j = min{x (i) j + i,j /2, 1} \u2212 max{x (i) j \u2212 i,j /2, 0}.", "formula_coordinates": [4.0, 64.95, 313.64, 216.58, 14.07]}, {"formula_id": "formula_17", "formula_text": "x\u2208Q o || p x (x)|| 1 < \u221e.", "formula_coordinates": [4.0, 66.95, 479.44, 92.92, 16.52]}, {"formula_id": "formula_18", "formula_text": "E[ H tKL (X)] \u2212 H(X) \u2264 C 2 C 1+1/d 1 k N 1 d .", "formula_coordinates": [4.0, 90.16, 571.25, 169.49, 26.86]}, {"formula_id": "formula_19", "formula_text": "[ H tKL (X)] \u2264 C 1 N ,", "formula_coordinates": [4.0, 138.9, 620.43, 82.41, 22.31]}, {"formula_id": "formula_20", "formula_text": "E[ H tKSG (X)] \u2212 H(X) \u2264 C (log N ) k+2 C k+1 1 N 1 d for some C > 0. The variance of the truncated KSG estimator is bounded by Var[ H tKSG (X)] \u2264 C (log N ) k+2 N ,", "formula_coordinates": [4.0, 319.5, 296.94, 238.51, 81.01]}, {"formula_id": "formula_21", "formula_text": "H(X) = H(Z) + p z (z) log det \u2202f \u22121 (z) \u2202z dz, (12", "formula_coordinates": [5.0, 65.97, 106.11, 222.38, 23.89]}, {"formula_id": "formula_22", "formula_text": ")", "formula_coordinates": [5.0, 288.35, 114.75, 4.15, 8.64]}, {"formula_id": "formula_23", "formula_text": "H(X) = H(Z) + 1 n n i=1 log det \u2202f \u22121 (z (i) ) \u2202z ,(13)", "formula_coordinates": [5.0, 70.22, 197.38, 222.28, 30.32]}, {"formula_id": "formula_24", "formula_text": "S Z = {z (i) = f (x (i) )} n i=1 .", "formula_coordinates": [5.0, 54.0, 250.43, 238.49, 24.66]}, {"formula_id": "formula_25", "formula_text": "Assumption 2. Let S = {x (i) } N i=1", "formula_coordinates": [5.0, 53.64, 380.29, 137.98, 12.33]}, {"formula_id": "formula_26", "formula_text": "E[ H UM\u2212tKL (X)] \u2212 H(X) \u2264 C N UM\u2212tKL k N 1 d , (14)", "formula_coordinates": [5.0, 68.22, 551.01, 224.28, 22.31]}, {"formula_id": "formula_27", "formula_text": "E[ H UM\u2212tKSG (X)] \u2212 H(X) \u2264 C U M \u2212tKSG (log N ) k+2 N 1 d , (15", "formula_coordinates": [5.0, 58.41, 615.91, 232.99, 33.29]}, {"formula_id": "formula_28", "formula_text": ")", "formula_coordinates": [5.0, 288.35, 640.56, 4.15, 8.64]}, {"formula_id": "formula_29", "formula_text": "where C U M \u2212tKSG = C (1+C) (1+C) d +1 (1\u2212C) k+1", "formula_coordinates": [5.0, 54.0, 651.88, 164.87, 18.21]}, {"formula_id": "formula_30", "formula_text": "min g\u2208\u2126 D(p z |p u ) := p z (z) log p z (z) p u (z) dz,(16)", "formula_coordinates": [5.0, 353.55, 152.31, 204.45, 23.23]}, {"formula_id": "formula_31", "formula_text": "z = \u03a6 \u2022 g(x)", "formula_coordinates": [5.0, 345.54, 182.15, 53.53, 8.74]}, {"formula_id": "formula_32", "formula_text": "D(\u03c0 y (y)||p y (y)) = D(\u03c0 z (z)||p z (z))(17)", "formula_coordinates": [5.0, 365.59, 289.22, 192.41, 9.65]}, {"formula_id": "formula_33", "formula_text": "\u03a6(y) = (\u03c6 1 (y 1 ), ..., \u03c6 d (y d )), \u03c6 i (y i ) = 1 2 (1 + erf( y \u221a 2 )),", "formula_coordinates": [5.0, 322.88, 367.19, 231.73, 23.42]}, {"formula_id": "formula_34", "formula_text": "min g\u2208\u2126 D(p y (y)|q(y)),(18)", "formula_coordinates": [5.0, 398.4, 445.85, 159.6, 14.58]}, {"formula_id": "formula_35", "formula_text": "min h\u2208\u2126 \u22121 D(p x (x)|q h (x)),(19)", "formula_coordinates": [5.0, 391.99, 528.23, 166.02, 15.13]}, {"formula_id": "formula_36", "formula_text": "q h (x) = q h \u22121 (x) det \u2202h \u22121 \u2202x .(20)", "formula_coordinates": [5.0, 363.41, 576.82, 194.6, 23.89]}, {"formula_id": "formula_37", "formula_text": "max \u03bb\u2208D U (\u03bb) := H(Y |\u03bb),(21)", "formula_coordinates": [6.0, 391.81, 605.38, 166.19, 14.66]}], "doi": ""}