{"title": "Solving Imperfect-Information Games via Discounted Regret Minimization", "authors": "Noam Brown; Tuomas Sandholm", "pub_date": "", "abstract": "Counterfactual regret minimization (CFR) is a family of iterative algorithms that are the most popular and, in practice, fastest approach to approximately solving large imperfectinformation games. In this paper we introduce novel CFR variants that 1) discount regrets from earlier iterations in various ways (in some cases differently for positive and negative regrets), 2) reweight iterations in various ways to obtain the output strategies, 3) use a non-standard regret minimizer and/or 4) leverage \"optimistic regret matching\". They lead to dramatically improved performance in many settings. For one, we introduce a variant that outperforms CFR+, the prior state-of-the-art algorithm, in every game tested, including large-scale realistic settings. CFR+ is a formidable benchmark: no other algorithm has been able to outperform it. Finally, we show that, unlike CFR+, many of the important new variants are compatible with modern imperfect-informationgame pruning techniques and one is also compatible with sampling in the game tree.", "sections": [{"heading": "Introduction", "text": "Imperfect-information games model strategic interactions between players that have hidden information, such as in negotiations, cybersecurity, and auctions. A common benchmark for progress in this class of games is poker. The typical goal is to find an (approximate) equilibrium in which no player can improve by deviating from the equilibrium.\nFor extremely large imperfect-information games that cannot fit in a linear program of manageable size, typically iterative algorithms are used to approximate an equilibrium.\nA number of such iterative algorithms exist (Nesterov 2005;Hoda et al. 2010;Pays 2014;Kroer et al. 2015;Heinrich, Lanctot, and Silver 2015). The most popular ones are variants of counterfactual regret minimization (CFR) (Zinkevich et al. 2007;Lanctot et al. 2009;Gibson et al. 2012). In particular, the development of CFR+ was a key breakthrough that in many cases is at least an order of magnitude faster than vanilla CFR (Tammelin 2014;. CFR+ was used to essentially solve headsup limit Texas hold'em poker  and was used to approximately solve heads-up no-limit Texas hold'em (HUNL) endgames in Libratus, which defeated Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nHUNL top professionals (Brown and Sandholm 2017c;Brown and Sandholm 2017b). A blend of CFR and CFR+ was used by DeepStack to defeat poker professionals in HUNL (Morav\u010d\u00edk et al. 2017).\nThe best known theoretical bound on the number of iterations needed for CFR and CFR+ to converge to an -equilibrium (defined formally in the next section) is O( 12 ) (Zinkevich et al. 2007;. This is asymptotically slower than first-order methods that converge at rate O( 1 ) (Hoda et al. 2010;Kroer et al. 2015). However, in practice CFR+ converges much faster than its theoretical bound, and even faster than O( 1 ) in many games.\nNevertheless, we show in this paper that one can design new variants of CFR that significantly outperform CFR+. We show that CFR+ does relatively poorly in games where some actions are very costly mistakes (that is, they cause high regret in that iteration) and provide an intuitive example and explanation for this. To address this weakness, we introduce variants of CFR that do not assign uniform weight to each iteration. Instead, earlier iterations are discounted. As we show, this high-level idea can be instantiated in many different ways. Furthermore, some combinations of our ideas perform significantly better than CFR+ while others perform worse than it. In particular, one variant outperforms CFR+ in every game tested.", "publication_ref": ["b16", "b11", "b17", "b13", "b10", "b13", "b7", "b19", "b1", "b1", "b15", "b11", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Notation and Background", "text": "We focus on sequential games as the most interesting and challenging application of this work, but our techniques also apply to non-sequential games. In an imperfect-information extensive-form (that is, tree-form) game there is a finite set of players, P. \"Nature\" is also considered a player (representing chance) and chooses actions with a fixed known probability distribution. A state h is defined by all information of the current situation, including private knowledge known to only a subset of players. A(h) is the actions available in a node and P (h) is the unique player who acts at that node. If action a \u2208 A(h) leads from h to h , then we write h \u2022 a = h . H is the set of all states in the game tree. Z \u2286 H are terminal states for which no actions are available. For each player i \u2208 P, there is a payoff function u i : Z \u2192 R. We denote the range of payoffs in the game by \u2206. Formally,\n\u2206 i = max z\u2208Z u i (z) \u2212 min z\u2208Z u i (z) and \u2206 = max i\u2208P \u2206 i .\nImperfect information is represented by information sets (infosets) for each player i \u2208 P. For any infoset I belonging to player i, all states h, h \u2208 I are indistinguishable to player i. Every non-terminal state h \u2208 H belongs to exactly one infoset for each player i. The set of actions that may be chosen in I is represented as A(I). We represent the set of all infosets belonging to player i where i acts by I i .\nA strategy \u03c3 i (I) is a probability vector over actions for player i in infoset I. The probability of a particular action a is denoted by \u03c3 i (I, a). Since all states in an infoset belonging to player i are indistinguishable, the strategies in each of them are identical. Therefore, for any h \u2208 I we define \u03c3 i (h, a) = \u03c3 i (I, a) where i = P (h). We define \u03c3 i to be a strategy for player i in every infoset in the game where player i acts. A strategy profile \u03c3 is a tuple of strategies, one per player. The strategy of every player other than i is represented as \u03c3 \u2212i . u i (\u03c3 i , \u03c3 \u2212i ) is the expected payoff for player i if all players play according to strategy profile \u03c3 i , \u03c3 \u2212i .\n\u03c0\n\u03c3 (h) = \u03a0 h \u2022a h \u03c3 P (h ) (h , a)\nis the joint probability of reaching h if all players play according to \u03c3. \u03c0 \u03c3 i (h) is the contribution of player i to this probability (that is, the probability of reaching h if all players other than i, and chance, always chose actions leading to h). \u03c0 \u03c3 \u2212i (h) is the contribution of chance and all players other than i.\nA best response to \u03c3 i is a strategy BR(\u03c3 i ) such that u i \u03c3 i , BR(\u03c3 i ) = max \u03c3 \u2212i u i (\u03c3 i , \u03c3 \u2212i ). A Nash equilibrium \u03c3 * is a strategy profile where everyone plays a best response: 1950). The exploitability e(\u03c3 i ) of a strategy \u03c3 i in a two-player zero-sum game is how much worse it does versus a best response compared to a Nash equilibrium strategy. Formally, e(\n\u2200i, u i (\u03c3 * i , \u03c3 * \u2212i ) = max \u03c3 i u i (\u03c3 i , \u03c3 * \u2212i ) (Nash\n\u03c3 i ) = u i \u03c3 * i , BR(\u03c3 * i ) \u2212 u i \u03c3 i , BR(\u03c3 i ) .\nIn an -Nash equilibrium, no player has exploitability higher than .\nIn CFR, the strategy vector for each infoset is determined according to a regret-minimization algorithm. Typically, regret matching (RM) is used as that algorithm within CFR due to RM's simplicity and lack of parameters.\nThe expected value (or simply value) to player i at state h given that all players play according to strategy profile \u03c3 from that point on is defined as v \u03c3 i (h). The value to i at infoset I where i acts is the weighted average of the value of each state in the infoset, where the weight is proportional to i's belief that they are in that state conditional on knowing they are in I. Formally, v\n\u03c3 (I) = h\u2208I \u03c0 \u03c3 \u2212i (h|I)v \u03c3 i (h) and v \u03c3 (I, a) = h\u2208I \u03c0 \u03c3 \u2212i (h|I)v \u03c3 i (h \u2022 a) where \u03c0 \u03c3 \u2212i (h|I) = \u03c0 \u03c3 \u2212i (h) \u03c0 \u03c3 \u2212i (I)\n. Let \u03c3 t be the strategy on iteration t. The instantaneous regret for action a in infoset I on iteration t is r t (I, a) = v \u03c3 t (I, a) \u2212 v \u03c3 t (I) and the regret on iteration T is\nR T (I, a) = T t=1 r T (I, a)(1)\nAdditionally, R T + (I, a) = max{R T (I, a), 0} and R T (I) = max a {R T + (I, a)}. Regret for player i in the entire game is\nR T i = max \u03c3 i T t=1 u i (\u03c3 i , \u03c3 t \u2212i ) \u2212 u i (\u03c3 t i , \u03c3 t \u2212i )(2)\nIn RM, a player picks a distribution over actions in an infoset in proportion to the positive regret on those actions. Formally, on each iteration T + 1, player i selects actions a \u2208 A(I) according to probabilities  Lugosi 2006). If a player plays according to CFR on every iteration, then\nR T i \u2264 I\u2208Ii R T (I)(4)\nSo, as T \u2192 \u221e,\nR T i T \u2192 0. The average strategy\u03c3 T i (I) for an infoset I is \u03c3 T i (I) = T t=1 \u03c0 \u03c3 t i (I)\u03c3 t i (I) T t=1 \u03c0 \u03c3 t i (I)(5)\nCFR minimizes external regret (Zinkevich et al. 2007), so it converges to a coarse correlated equilibrium (Hart and Mas-Colell 2000). In two-player zero-sum games, this is also a Nash equilibrium. In two-player zero-sum games, if both players' average regret satisfies 2009). Thus, CFR is an anytime algorithm for finding an -Nash equilibrium in two-player zero-sum games.\nR T i T \u2264 , then their av- erage strategies \u03c3 T 1 ,\u03c3 T 2 are a 2 -Nash equilibrium (Waugh\nAlthough CFR theory calls for both players to simultaneously update their regrets on each iteration, in practice far better performance is achieved by alternating which player updates their regrets on each iteration. However, this complicates the theory for convergence (Farina, Kroer, and Sandholm ;Burch, Moravcik, and Schmid 2018).\nCFR+ is like CFR but with the following small changes. First, after each iteration any action with negative regret is set to zero regret. Formally, CFR+ chooses its strategy on iteration T + 1 according to Regret Matching+ (RM+), which is identical to Equation (3) but uses the regret-like value Q T (I, a) = max{0, Q T \u22121 (I, a) + r t (I, a)} rather than R T + (I, a). Second, CFR+ uses a weighted average strategy where iteration T is weighted by T rather than using a uniformly-weighted average strategy as in CFR. The best known convergence bound for CFR+ is higher (that is, worse in exploitability) than CFR by a constant factor of 2. Despite that, CFR+ typically converges much faster than CFR and usually even faster than O( 1 ).\nHowever, in some games CFR+ converges slower than 1 T . We now provide a two-player zero-sum game with this property. Consider the payoff matrix 1 0.9 \u22120.7 1 (where P 1 chooses a row and P 2 simultaneously chooses a column; the chosen entry in the matrix is the payoff for P 1 while P 2 receives the opposite). We now proceed to introducing our improvements to the CFR family.\nWeighted Averaging Schemes for CFR+ As described in the previous section, CFR+ traditionally uses \"linear\" averaging, in which iteration t's contribution to the average strategy is proportional to t. In this section we prove a bound for any sequence of non-decreasing weights when calculating the average strategy. However, the bound on convergence is never lower than that of vanilla CFR (that is, uniformly equal weight on the iterations). Theorem 1. Suppose T iterations of RM+ are played in a two-player zero-sum game. Then the weighted average strategy profile, where iteration t is weighed proportional to w t > 0 and w i \u2264 w j for all i < j, is a\nw T T t=1 wt \u2206|I| |A| \u221a T -Nash equilibrium.\nThe proof is in the appendix. It largely follows the proof for linear averaging in CFR+ .\nEmpirically we observed that CFR+ converges faster when assigning iteration t a weight of t 2 rather than a weight of t when calculating the average strategy. We therefore use this weight for CFR+ and its variants throughout this paper when calculating the average strategy.", "publication_ref": ["b8", "b5", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Regret Discounting for CFR and Its Variants", "text": "In all past variants of CFR, each iteration's contribution to the regrets is assigned equal weight. In this section we discuss discounting iterations in CFR when determining regrets-in particular, assigning less weight to earlier iterations. This is very different from, and orthogonal to, the idea of discounting iterations when computing the average strategy, described in the previous section.\nTo motivate discounting, consider the simple case of an agent deciding between three actions. The payoffs for the actions are 0, 1, and -1,000,000, respectively. From (3) we see that CFR and CFR+ assign equal probability to each action on the first iteration. This results in regrets of 333,333, 333,334, and 0, respectively. If we continue to run CFR or CFR+, the next iteration will choose the first and second action with roughly 50% probability each, and the regrets will be updated to be roughly 333,332.5 and 333,334.5, respectively. It will take 471,407 iterations for the agent to choose the second action-that is, the best action-with 100% probability. Discounting the first iteration over time would dramatically speed convergence in this case. While this might seem like a contrived example, many games include highly suboptimal actions. In this simple example the bad action was chosen on the first iteration, but in general bad actions may be chosen throughout a run, and discounting may be useful far beyond the first few iterations.\nDiscounting prior iterations has received relatively little attention in the equilibrium-finding community. \"Optimistic\" regret minimizing variants exist that assign a higher weight to recent iterations, but this extra weight is temporary and typically only applies to a short window of recent iterations; for example, counting the most recent iterate twice (Syrgkanis et al. 2015). We investigate optimistic regret minimizers as part of CFR later in this paper. CFR+ discounts prior iterations' contribution to the average strategy, but not the regrets. Discounting prior iterations has also been used in CFR for situations where the game structure changes, for example due to interleaved abstraction and equilibrium finding (Brown and Sandholm 2014;Brown and Sandholm 2015b). There has also been some work on applying discounting to perfect-information game solving in Monte Carlo Tree Search (Hashimoto et al. 2011).\nOutside of equilibrium finding, prior research has analyzed the theory for discounted regret minimization (Cesa-Bianchi and Lugosi 2006). That work investigates applying RM (and other regret minimizers) to a sequence of iterations in which iteration t has weight w t (assuming w t \u2264 1 and the final iteration has weight 1). For RM, it proves that if \u221e t=1 w t = \u221e then weighted average regret, defined as R w,T\ni = max a\u2208A T t=1 (wtr t (a)) T t=1 w t is bounded by R w,T i \u2264 \u2206 |A| T t=1 w 2 t T t=1 w t (6)\nPrior work has shown that, in two-player zero-sum games, if weighted average regret is , then the weighted average strategy, defined as \u03c3 w,T i (I) = t\u2208T wt\u03c0 \u03c3 t i (I)\u03c3 t i (I)\nt\u2208T (wt\u03c0 \u03c3 t i (I))\nfor infoset I, is a 2 -Nash equilibrium (Brown and Sandholm 2014).\nWhile there are a limitless number of discounting schemes that converge in theory, not all of them perform well in practice. This paper introduces a number of variants that perform particularly well also in practice. The first algorithm, which we refer to as linear CFR (LCFR), is identical to CFR, except on iteration t the updates to the regrets and average strategies are given weight t. That is, the iterates are weighed linearly. (Equivalently, one could multiply the accumulated regret by t t+1 on each iteration. We do this in our experiments to reduce the risk of numerical instability.) This means that after T iterations of LCFR, the first iteration only has a weight of 2 T 2 +T on the regrets rather than a weight of 1 T , which would be the case in CFR and CFR+. In the motivating example introduced at the beginning of this section, LCFR chooses the second action with 100% probability after only 970 iterations while CFR+ requires 471,407 iterations. Furthermore, from (6), the theoretical bound on the convergence of regret is only greater than vanilla CFR by a factor of 2 \u221a 3 . One could more generally use any polynomial weighting of t.\nSince the changes from CFR that lead to LCFR and CFR+ do not conflict, it is natural to attempt to combine them into a single algorithm that weighs each iteration t proportional to t and also has a floor on regret at zero like CFR+. However, we empirically observe that this algorithm, which we refer to as LCFR+, actually leads to performance that is worse than LCFR and CFR+ in the games we tested, even though its theoretical bound on convergence is the same as for LCFR.\nNevertheless, we find that using a less-aggressive discounting scheme leads to consistently strong performance. We can consider a family of algorithms called Discounted CFR with parameters \u03b1 \u03b2, and \u03b3 (DCFR \u03b1,\u03b2,\u03b3 ), defined by multiplying accumulated positive regrets by t \u03b1 t \u03b1 +1 , negative regrets by t \u03b2 t \u03b2 +1 , and contributions to the average strategy by ( t t+1 ) \u03b3 on each iteration t. In this case, LCFR is equivalent to DCFR 1,1,1 , because multiplying iteration t's regret and contribution to the average strategy by t t +1 on every iteration t \u2264 t < T is equivalent to weighing iteration t by t T . CFR+ (where iteration t's contribution to the average strategy is proportional to t 2 ) is equivalent to DCFR \u221e,\u2212\u221e,2 .\nIn preliminary experiments we found the optimal choice of \u03b1, \u03b2, and \u03b3 varied depending on the specific game. However, we found that setting \u03b1 = 3/2, \u03b2 = 0, and \u03b3 = 2 led to performance that was consistently stronger than CFR+. Thus, when we refer to DCFR with no parameters listed, we assume this set of parameters are used.\nTheorem 2 shows that DCFR has a convergence bound that differs from CFR only by a constant factor. Theorem 2. Assume that T iterations of DCFR are conducted in a two-player zero-sum game. Then the weighted average strategy profile is a 6\u2206|I|(|\n|A|+ 1 \u221a T )/ \u221a T -Nash equilibrium.\nWe provide the proof in the appendix. It combines elements of the proof for CFR+  and the proof that discounting in regret minimization is sound (Cesa-Bianchi and Lugosi 2006).\nOne of the drawbacks of setting \u03b2 \u2264 0 is that suboptimal actions (that is, actions that have an expected value lower than some other action in every equilibrium) no longer have regrets that approach \u2212\u221e over time. Instead, for \u03b2 = 0 they will approach some constant value and for \u03b2 < 0 they will approach 0. This makes the algorithm less compatible with improvements that prune negative-regret actions (Brown and Sandholm 2015a;Brown and Sandholm 2017a). Such pruning algorithms can lead to more than an order of magnitude reduction in computational and space requirements for some games. Setting \u03b2 > 0 better facilitates this pruning. For this reason in our experiments we also show results for \u03b2 = 0.5.", "publication_ref": ["b18", "b0", "b0", "b9", "b3", "b0", "b3", "b0", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental setup", "text": "We now introduce the games used in our experiments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Description of heads-up no-limit Texas hold'em", "text": "We conduct experiments on subgames of HUNL poker, a primary benchmark for imperfect-information game solving. In the version of HUNL we use, and which is standard in the Annual Computer Poker Competition, the two players (P 1 and P 2 ) start each hand with $20,000. The players alternate positions after each hand. On each of the four rounds of betting, each player can choose to either fold, call, or raise. Folding results in the player losing and the money in the pot being awarded to the other player. Calling means the player places a number of chips in the pot equal to the opponent's share. Raising means the player adds more chips to the pot than the opponent's share. A round ends when a player calls (if both players have acted). Players cannot raise beyond the $20,000 they start with. All raises must be at least $100 and at least as larger as any previous raise on that round.\nAt the start of each hand of HUNL, both players are dealt two private cards from a standard 52-card deck. P 1 places $100 in the pot and P 2 places $50 in the pot. A round of betting then occurs. Next, three community cards are dealt face up. Another round of betting occurs, starting with P 1 .\nAfter the round is over, another community card is dealt face up, and another round of betting starts with P 1 acting first. Finally, one more community card is revealed and a final betting round occurs starting with P 1 . Unless a player has folded, the player with the best five-card poker hand, constructed from their two private cards and the five community cards, wins the pot. In the case of a tie, the pot is split evenly.\nAlthough the HUNL game tree is too large to traverse completely without sampling, state-of-the-art agents for HUNL solve subgames of the full game in real time during play (Brown and Sandholm 2017b;Morav\u010d\u00edk et al. 2017;Brown and Sandholm 2017c;Brown, Sandholm, and Amos 2018) using a small number of the available bet sizes. For example, Libratus solved in real time the remainder of HUNL starting on the third betting round. We conduct our HUNL experiments on four subgames generated by Libratus 1 . The subgames were selected prior to testing. Although the inputs to the subgame are publicly available (the beliefs of both players at the start of the subgame about what state they are in, the number of chips in the pot, and the revealed cards), the exact bet sizes that Libratus considered have not been publicly revealed. We therefore use the bet sizes of 0.5x and 1x the size of the pot, as well as an all-in bet (betting all remaining chips) for the first bet of each round. For subsequent bets in a round, we consider 1x the pot and all-in.\nSubgame 1 begins at the start of the third betting round and continues to the end of the game. There are $500 in the pot at the start of the round. This is the most common situation to be in upon reaching the third betting round, and is also the hardest for AIs to solve because the remaining game tree is the largest. Since there is only $500 in the pot but up to $20,000 could be lost, this subgames contains a number of high-penalty mistake actions. Subgame 2 begins at the start of the third betting round and has $4,780 in the pot at the start of the round. Subgame 3 begins at the start of the fourth (and final) betting round with $500 in the pot, which is a common situation. Subgame 4 begins at the start of the fourth betting round with $3,750 in the pot. Exploitability is measured in terms of milli big blinds per game (mbb/g), a standard measurement in the field, which represents the number of big blinds (P 1 's original contribution to the pot) lost per hand of poker multiplied by 1,000.", "publication_ref": ["b1", "b15", "b1", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Description of Goofspiel", "text": "In addition to HUNL subgames, we also consider a version of the game of Goofspiel (limited to just five cards per player). In this version of Goofspiel, each player has five hidden cards in their hand (A, 2, 3, 4, and 5), with A being valued as 1. A deck of five cards (also of rank A, 2, 3, 4, and 5), is placed between the two players. In the variant we consider, both players know the order of revealed cards in the center will be A, 2, 3, 4, 5. On each round, the top card of the deck is flipped and is considered the prize card. Each player then simultaneously plays a card from their hand. The player who played the higher-ranked card wins the prize card. If the players played the same rank, then they split the prize's value. The cards that were bid are discarded. At the end of the game, players add up the ranks of their prize cards. A player's payoff is the difference between his total value and the total value of his opponent.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments on Regret Discounting and Weighted Averaging", "text": "Our experiments are run for 32,768 iterations for HUNL subgames and 8,192 iterations for Goofspiel. Since all the algorithms tested only converge to an -equilibrium rather than calculating an exact equilibrium, it is up to the user to decide when a solution is sufficiently converged to terminate a run. In practice, this is usually after 100 -1,000 iterations (Brown and Sandholm 2017c;Morav\u010d\u00edk et al. 2017).\nFor example, an exploitability of 1 mbb/g is considered sufficiently converged so as to be essentially solved . Thus, the performance of the presented algorithms between 100 and 1,000 iterations is arguably more important than the performance beyond 10,000 iterations. Nevertheless, we show performance over a long time horizon to display the long-term behavior of the algorithms. All our experiments use the alternating-updates form of CFR. We measure the average exploitability of the two players.\nOur experiments show that LCFR can dramatically improve performance over CFR+ over reasonable time horizons in certain games. However, asymptotically, LCFR appears to do worse in practice than CFR+. LCFR does particularly well in subgame 1 and 3, which (due to the small size of the pot relative to the amount of money each player can bet) have more severe mistake actions compared to subgames 2 and 4. It also does poorly in Goofspiel, which also likely does not have severely suboptimal actions. This suggests that LCFR is particularly well suited for games with the potential for large mistakes.\nOur experiments also show that DCFR 3 2 ,0,2 matches or outperforms CFR+ across the board. The improvement is usually a factor of 2 or 3. In Goofspiel, DCFR 3 2 ,0,2 results in essentially identical performance as CFR+.\nDCFR 3 2 ,\u2212\u221e,2 , which sets negative regrets to zero rather than multiplying them by 1 2 each iteration, generally also leads to equally strong performance, but in rare cases (such as in Figure 2), can produce a spike in exploitability that takes many iterations to recover from. Thus, we generally recommend using DCFR 3 2 ,0,2 over DCFR 3 2 ,\u2212\u221e,2 . DCFR 3 2 , 1 2 ,2 multiplies negative regrets by \u221a t \u221a t+1 on iteration t, which allows suboptimal actions to decrease in regret to \u2212\u221e and thereby facilitates algorithms that temporarily prune negative-regret sequences. In the HUNL subgames, DCFR 3 2 , 1 2 ,2 performed very similarly to DCFR 3 2 ,0,2 . However, in Goofspiel it does noticeably worse. This suggests that DCFR 3 2 , 1 2 ,2 may be preferable to DCFR 3 2 ,0,2 in games with large mistakes when a pruning algorithm may be used, but that DCFR 3 2 ,0,2 should be used otherwise.", "publication_ref": ["b1", "b15"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "NormalHedge for CFR Variants", "text": "CFR is a framework for applying regret minimization independently at each infoset in the game. Typically RM is   used as the regret minimizer primarily due to its lack of parameters and its simple implementation. However, any regret minimizer can be applied. Previous research investigated using Hedge (Littlestone and Warmuth 1994;Freund and Schapire 1997) in CFR rather than RM (Brown, Kroer, and Sandholm 2017). This led to better performance in small games, but worse performance in large games. In this section we investigate instead using NormalHedge (NH) (Chaudhuri, Freund, and Hsu 2009) as the regret minimizer in CFR.\nIn NH, on each iteration T + 1 a player i selects actions  = e. If a player plays according to NH in infoset I, then cumulative regret for that infoset is at most O(\u2206 T ln(|A|) + \u2206 ln 2 (|A|)).\nNH shares two desirable properties with RM: it does not have any parameters and it assigns zero probability to actions with negative regret (which means it can be easily used in CFR+ with a floor on regret at zero). However, the NH operation is more computationally expensive than RM because it involves exponentiation and a line search for c t .\nIn our experiments we investigate using NH in place of RM for DCFR 3 2 ,0,2 and in place of RM for LCFR. We found that NH did worse in all HUNL subgames compared to RM in LCFR, so we omit those results. Figure 6 and Figure 8 shows that NH outperforms RM in HUNL subgames when combined with DCFR 3 2 ,0,2 . However, it does worse than RM in Figure 7 and Figure 9. The two subgames it does better in have the largest \"mistake\" actions, which suggest NH may do better in games that have large mistake actions.\nIn these experiments the performance of NH is measured in terms of exploitability as a function of number of iterations. However, in our implementation, each iteration takes five times longer due to the exponentiation and line search operations involved in NH. Thus, using NH actually slows convergence in practice. Nevertheless, NH may be preferable in certain situations where the cost of the exponentiation and line search operations are insignificant, such as when an algorithm is bottlenecked by memory access rather than computational speed.", "publication_ref": ["b14", "b6", "b1", "b4"], "figure_ref": ["fig_6", "fig_8", "fig_7", "fig_9"], "table_ref": []}, {"heading": "Optimistic CFR Variants", "text": "Optimistic Hedge (Syrgkanis et al. 2015) is a regret minimization algorithm similar to Hedge in which the last iteration is counted twice when determining the strategy for the next iteration. This can lead to substantially faster convergence, including in some cases an improvement over the O( 12 ) bound on regret of typical regret minimizers. We investigate counting the last iteration twice when calculating the strategy for the next iteration . Formally, when applying Equation (3) to determine the strategy for the next iteration, we use a modified regret R T mod (I, a) =\nT \u22121 t=1 r t (I, a) + 2r T (I, a) in the equation in place of R T (I, a). We refer to this as Optimistic RM, and any CFR variant that uses it as Optimistic. We found that Optimistic DCFR 3 2 ,0,2 did worse than DCFR 3 2 ,0,2 in all HUNL subgames, so we omit those results. Figure 6 and Figure 8 shows that Optimistic LCFR outperforms LCFR in two HUNL subgames. However, it does worse than LCFR in Figure 7 and Figure 9. Just as in the case of NH, the two subgames that Optimistic LCFR does better in have the largest \"mistake\" actions, which suggests that Optimistic LCFR may do better than LCFR in games that have large mistake actions. These are the same situations that LCFR normally excels in, so this suggests that in a situation where LCFR is preferable, one may wish to use Optimistic LCFR. ", "publication_ref": ["b18"], "figure_ref": ["fig_6", "fig_8", "fig_7", "fig_9"], "table_ref": []}, {"heading": "Discounted Monte Carlo CFR", "text": "Monte Carlo CFR (MCCFR) is a variant of CFR in which certain player actions or chance outcomes are sampled (Lanctot et al. 2009;Gibson et al. 2012). MCCFR combined with abstraction has produced state-of-the-art HUNL poker AIs (Brown and Sandholm 2017c). It is also particularly useful in games that do not have a special structure that can be exploited to implement a fast vector-based implementation of CFR (Lanctot et al. 2009; Johanson et al.   2011). There are many forms of MCCFR with different sampling schemes. The most popular is external-sampling MC-CFR, in which opponent and chance actions are sampled according to their probabilities, but all actions belonging to the player updating his regret are traversed. Other MCCFR variants exist that achieve superior performance (Jackson 2017), but external-sampling MCCFR is simple and widely used, which makes it useful as a benchmark for our experiments.  Although CFR+ provides a massive improvement over CFR in the unsampled case, the changes present in CFR+ (a floor on regret at zero and linear averaging), do not lead to superior performance when applied to MCCFR . In contrast, in this section we show that the changes present in LCFR do lead to superior performance when applied to MCCFR. Specifically, we divide the MCCFR run into periods of 10 7 nodes touched. Nodes touched is an implementation-independent and hardware-independent proxy for time that counts the number of nodes traversed (including terminal nodes). After each period n ends, we multiply all accumulated regrets and contributions to the average strategies by n n+1 . Figure 10 and Figure 11 demonstrate that this leads to superior performance in HUNL compared to vanilla MCCFR. The improvement is particularly noticeable in subgame 3, which features the largest mistake actions. We also show performance if one simply multiplies the accumulated regrets and contributions to the average strategy by 1 10 after the first period ends, and thereafter runs vanilla MC-CFR (the \"Initial Discount MCCFR\" variant). The displayed results are the average of 100 different runs.", "publication_ref": ["b13", "b7", "b1", "b12"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Conclusions", "text": "We introduced variants of CFR that discount prior iterations, leading to stronger performance than the prior stateof-the-art CFR+, particularly in settings that involve large mistakes. In particular, the DCFR 3 2 ,0,2 variant matched or outperformed CFR+ in all settings.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "This material is based on work supported by the National Science Foundation under grants IIS-1718457, IIS-1617590, and CCF-1733556, and the ARO under award W911NF-17-1-0082. Noam is also sponsored by an Open Philanthropy Project AI Fellowship and a Tencent AI Lab Fellowship.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix Proof of Theorem 1", "text": "Consider the weighted sequence of iterates \u03c3 1 , ..., \u03c3 T in which \u03c3 t is identical to \u03c3 t , but weighed by w t . The regret of action a in infoset I on iteration t of this new sequence is R t (I, a).\nFrom Lemma 2 we know that R t (I, a) \u2264 \u2206 |A| \u221a T for player i for action a in infoset I. Since w a,t is a non-decreasing sequence, so we can apply Lemma 1 using weight w t for iteration t with B = \u2206 |A| \u221a T and C = 0. From Lemma 1, this means that R t (I, a) \u2264 w T \u2206 |A| \u221a T . Applying (4), we get weighted regret is at most w T \u2206|I i | |A| \u221a\nT for player i. Thus, weighted average regret is at most\nProof Theorem 2\nProof. Since the lowest amount of instantaneous regret on any iteration is \u2212\u2206 and DCFR multiplies negative regrets by 1 2 each iteration, so regret for any action at any point is greater than \u22122\u2206.\nConsider the weighted sequence of iterates \u03c3 1 , ..., \u03c3 T in which \u03c3 t is identical to \u03c3 t , but weighed by\nThe regret of action a in infoset I on iteration t of this new sequence is R t (I, a).\nFrom Lemma 4 we know that R t (I, a) \u2264 2\u2206 |A| \u221a T for player i for action a in infoset I. Since w a,t is an increasing sequence, so we can apply Lemma 1 using weight w a,t for iteration t with B = 2\u2206 |A| \u221a T and C = \u22122\u2206. From Lemma 1, this means that R t (I, a) \u2264\nSince the weights sum to one, this is also weighted average regret. Since |I\nFor any BC-plausible sequence and any sequence of non-decreasing weights w t \u2265 0,\nProof. The lemma closely resembles Lemma 3 from  and the proof shares some elements.\nWe construct a BC-plausible sequence x * 1 , ..., x * T that maximizes the weighted sum. That is, T t=1 w t x t = max x 1 ,...,x T T t=1 w t x t . We show that x * 1 = C, x * t = 0 for 1 < t < T , and x * T = (B \u2212 C).\nConsider x * T . Clearly in order to maximize the weighted sum, x * T = B \u2212 T \u22121 t=1 (w t x * t ). Next, consider x * t for t < T and assume x * t = C \u2212 t t=1 (w t x * t ) for t < t < T and assume x * T = B \u2212 T \u22121 t=1 (w t x * t ). Since w t \u2264 w T and w t \u2264 w t , so\n. By induction, this means x * 1 = C, x * t = 0 for 1 < t < T , and\n. Since x * is a maximizing sequence, so for any sequence x we have that\nLemma 2. Given a sequence of strategies \u03c3 1 , ..., \u03c3 T , each defining a probability distribution over a set of actions A, consider any definition for Q t (a) satisfying the following conditions:\nThe regret-like value Q t (a) is then an upper bound on the regret R t (a) and Q t (a) \u2212 Q t\u22121 (a) \u2265 r t (a) = R t (a) \u2212 R t\u22121 (a).\nProof. The lemma and proof closely resemble Lemma 1 in . For any t \u2265 1 we have\nSince Q 0 (a) = 0 and R 0 (a) = 0, so Q t (a) \u2265 R t (a).\nLemma 3. Given a set of actions A and any sequence of rewards v t such that |v t (a) \u2212 v t (b)| \u2264 \u2206 for all t and all a, b \u2208 A, after playing a sequence of strategies determined by regret matching but using the regret-like value Q t (a) in place of R t (a), Q T (a) \u2264 \u2206 |A|T for all a \u2208 A.\nProof. The proof is identical to that of Lemma 2 in .\nLemma 4. Assume that player i conducts T iterations of DCFR. Then weighted regret for the player is at most \u2206|I i | |A| \u221a T and weighted average regret for the player is at\nProof. The weight of iteration t < T is w t = \u03a0 T \u22121 i=t i 3/2 i 3/2 +1 and w T = 1. Thus, w t \u2264 1 for all t and therefore\nT for t < T and w T = 1. Thus, T t=1 w t \u2265 T (T + 1)/(2T ) > T /2. Applying (6) and Lemma 3, we see that Q w,T\nCorrectness of DCFR(3/2, 1/2, 2) Theorem 3. Assume that T iterations of DCFR 3 2 , 1 2 ,2 are conducted in a two-player zero-sum game. Then the weighted average strategy profile is a 9\u2206|I|| |A|/ \u221a T -Nash equilibrium.\nProof. From Lemma 5, we know that regret in DCFR 3 2 , 1 2 ,2 for any infoset I and action a cannot be below \u2212\u2206 \u221a T . Consider the weighted sequence of iterates \u03c3 1 , ..., \u03c3 T in which \u03c3 t is identical to \u03c3 t , but weighed by w a,t = \u03a0\nThe regret of action a in infoset I on iteration t of this new sequence is R t (I, a).\nFrom Lemma 4 we know that R t (I, a) \u2264 2\u2206 |A| \u221a T for player i for action a in infoset I. Since w a,t is an increasing sequence, so we can apply Lemma 1 using weight w a,t for iteration t with B = 2\u2206 |A| \u221a T and C = \u2212\u2206 |A| \u221a T . From Lemma 1, this means that R t (I, a) \u2264 Proof. We prove this inductively. On the first iteration, the lowest regret could be after multiplying by \u221a 1 \u2212\u2206 \u221a T + 1. Thus, R T +1 (I, a) \u2265 \u2212\u2206 \u221a T + 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Simultaneous abstraction and equilibrium finding in games", "journal": "", "year": "1972", "authors": "M Bowling; N Burch; M Johanson; O Tammelin; N Brown; T Sandholm; N Brown; T Sandholm; N Brown; T Sandholm"}, {"ref_id": "b1", "title": "Superhuman AI for heads-up no-limit poker: Libratus beats top professionals. Science eaao1733", "journal": "", "year": "2017", "authors": "N Brown; T Sandholm; N Brown; T Sandholm; N Brown; T Sandholm; N Brown; C Kroer; T Sandholm; N Brown; T Sandholm; Amos ; B "}, {"ref_id": "b2", "title": "Time and Space: Why Imperfect Information Games are Hard", "journal": "", "year": "2017", "authors": "Moravcik Burch;  Schmid; N Burch; M Moravcik; M Schmid"}, {"ref_id": "b3", "title": "Prediction, learning, and games", "journal": "Cambridge University Press", "year": "2006", "authors": "- Bianchi; Lugosi ; Cesa- Bianchi; N Lugosi; G "}, {"ref_id": "b4", "title": "A parameter-free hedging algorithm", "journal": "", "year": "2009", "authors": "Freund Chaudhuri; K Hsu ; Chaudhuri; Y Freund; D J Hsu"}, {"ref_id": "b5", "title": "Online convex optimization for sequential decision processes and extensive-form games", "journal": "", "year": "", "authors": "Kroer Farina; G Sandholm ] Farina; C Kroer; T Sandholm"}, {"ref_id": "b6", "title": "A decision-theoretic generalization of on-line learning and an application to boosting", "journal": "Journal of Computer and System Sciences", "year": "1997", "authors": "Y Freund; R Schapire"}, {"ref_id": "b7", "title": "Generalized sampling and variance in counterfactual regret minimization", "journal": "", "year": "2012", "authors": " Gibson"}, {"ref_id": "b8", "title": "A simple adaptive procedure leading to correlated equilibrium", "journal": "Econometrica", "year": "2000", "authors": "Mas-Colell ; Hart; S Mas-Colell; A "}, {"ref_id": "b9", "title": "Accelerated UCT and its application to two-player games", "journal": "Springer", "year": "2011", "authors": "[ Hashimoto"}, {"ref_id": "b10", "title": "Fictitious self-play in extensiveform games", "journal": "", "year": "2015", "authors": "Lanctot Heinrich;  Silver; J Heinrich; M Lanctot; D Silver"}, {"ref_id": "b11", "title": "Smoothing techniques for computing Nash equilibria of sequential games", "journal": "Mathematics of Operations Research", "year": "2010", "authors": " Hoda"}, {"ref_id": "b12", "title": "Accelerating best response calculation in large extensive games", "journal": "", "year": "2011", "authors": "E Jackson; M Johanson; K Waugh; M Bowling; M Zinkevich"}, {"ref_id": "b13", "title": "Monte Carlo sampling for regret minimization in extensive games", "journal": "ACM", "year": "2009", "authors": ""}, {"ref_id": "b14", "title": "The weighted majority algorithm. Information and Computation", "journal": "", "year": "1994", "authors": "N Littlestone; M K Warmuth"}, {"ref_id": "b15", "title": "Deepstack: Expert-level artificial intelligence in heads-up no-limit poker", "journal": "Proceedings of the National Academy of Sciences", "year": "1950", "authors": "M Morav\u010d\u00edk; M Schmid; N Burch; V Lis\u00fd; D Morrill; N Bard; T Davis; K Waugh; M Johanson; M Bowling; J Nash"}, {"ref_id": "b16", "title": "Excessive gap technique in nonsmooth convex minimization", "journal": "SIAM Journal of Optimization", "year": "2005", "authors": "Y Nesterov"}, {"ref_id": "b17", "title": "An interior point approach to large games of incomplete information", "journal": "", "year": "2014", "authors": "F Pays"}, {"ref_id": "b18", "title": "Fast convergence of regularized learning in games", "journal": "", "year": "2015", "authors": "V Syrgkanis; A Agarwal; H Luo; R E Schapire"}, {"ref_id": "b19", "title": "Regret minimization in games with incomplete information", "journal": "", "year": "2007", "authors": " Tammelin"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "plays according to regret matching in infoset I on every iteration, then on iteration T , R T (I) \u2264 \u2206 |A(I)| \u221a T (Cesa-Bianchi and", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Convergence in HUNL Subgame1.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Convergence in HUNL Subgame2.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Convergence in HUNL Subgame 3.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 4 :4Figure 4: Convergence in HUNL Subgame 4.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure 5: Convergence in 5-card Goofspiel variant.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 :6Figure 6: Convergence in HUNL Subgame 1.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 7 :7Figure 7: Convergence in HUNL Subgame 2.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 8 :8Figure 8: Convergence in HUNL Subgame 3.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 9 :9Figure 9: Convergence in HUNL Subgame 4.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 10 :10Figure 10: Convergence of MCCFR in HUNL Subgame 3.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 11 :11Figure 11: Convergence of MCCFR in HUNL Subgame 4.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u2206 i = max z\u2208Z u i (z) \u2212 min z\u2208Z u i (z) and \u2206 = max i\u2208P \u2206 i .", "formula_coordinates": [1.0, 319.5, 683.91, 238.5, 9.65]}, {"formula_id": "formula_1", "formula_text": "\u03c3 (h) = \u03a0 h \u2022a h \u03c3 P (h ) (h , a)", "formula_coordinates": [2.0, 70.0, 241.89, 118.64, 11.53]}, {"formula_id": "formula_2", "formula_text": "\u2200i, u i (\u03c3 * i , \u03c3 * \u2212i ) = max \u03c3 i u i (\u03c3 i , \u03c3 * \u2212i ) (Nash", "formula_coordinates": [2.0, 86.82, 344.3, 177.01, 13.36]}, {"formula_id": "formula_3", "formula_text": "\u03c3 i ) = u i \u03c3 * i , BR(\u03c3 * i ) \u2212 u i \u03c3 i , BR(\u03c3 i ) .", "formula_coordinates": [2.0, 63.47, 389.66, 174.26, 12.32]}, {"formula_id": "formula_4", "formula_text": "\u03c3 (I) = h\u2208I \u03c0 \u03c3 \u2212i (h|I)v \u03c3 i (h) and v \u03c3 (I, a) = h\u2208I \u03c0 \u03c3 \u2212i (h|I)v \u03c3 i (h \u2022 a) where \u03c0 \u03c3 \u2212i (h|I) = \u03c0 \u03c3 \u2212i (h) \u03c0 \u03c3 \u2212i (I)", "formula_coordinates": [2.0, 64.52, 532.12, 227.99, 30.71]}, {"formula_id": "formula_5", "formula_text": "R T (I, a) = T t=1 r T (I, a)(1)", "formula_coordinates": [2.0, 123.5, 606.79, 169.0, 30.2]}, {"formula_id": "formula_6", "formula_text": "R T i = max \u03c3 i T t=1 u i (\u03c3 i , \u03c3 t \u2212i ) \u2212 u i (\u03c3 t i , \u03c3 t \u2212i )(2)", "formula_coordinates": [2.0, 85.15, 677.13, 207.35, 30.2]}, {"formula_id": "formula_7", "formula_text": "R T i \u2264 I\u2208Ii R T (I)(4)", "formula_coordinates": [2.0, 402.35, 212.47, 155.65, 22.14]}, {"formula_id": "formula_8", "formula_text": "R T i T \u2192 0. The average strategy\u03c3 T i (I) for an infoset I is \u03c3 T i (I) = T t=1 \u03c0 \u03c3 t i (I)\u03c3 t i (I) T t=1 \u03c0 \u03c3 t i (I)(5)", "formula_coordinates": [2.0, 329.46, 244.86, 300.08, 67.5]}, {"formula_id": "formula_9", "formula_text": "R T i T \u2264 , then their av- erage strategies \u03c3 T 1 ,\u03c3 T 2 are a 2 -Nash equilibrium (Waugh", "formula_coordinates": [2.0, 319.5, 364.28, 238.5, 28.13]}, {"formula_id": "formula_10", "formula_text": "w T T t=1 wt \u2206|I| |A| \u221a T -Nash equilibrium.", "formula_coordinates": [3.0, 63.57, 186.75, 157.19, 22.73]}, {"formula_id": "formula_11", "formula_text": "i = max a\u2208A T t=1 (wtr t (a)) T t=1 w t is bounded by R w,T i \u2264 \u2206 |A| T t=1 w 2 t T t=1 w t (6)", "formula_coordinates": [3.0, 327.07, 188.5, 230.94, 58.95]}, {"formula_id": "formula_12", "formula_text": "t\u2208T (wt\u03c0 \u03c3 t i (I))", "formula_coordinates": [3.0, 449.76, 285.26, 51.57, 10.51]}, {"formula_id": "formula_13", "formula_text": "|A|+ 1 \u221a T )/ \u221a T -Nash equilibrium.", "formula_coordinates": [4.0, 54.0, 222.86, 238.5, 30.4]}], "doi": ""}