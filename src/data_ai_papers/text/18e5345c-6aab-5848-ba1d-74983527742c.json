{"title": "Low-Rank Tensors for Scoring Dependency Structures", "authors": "Tao Lei; Yu Xin; Yuan Zhang; Regina Barzilay; Tommi Jaakkola", "pub_date": "", "abstract": "Accurate scoring of syntactic structures such as head-modifier arcs in dependency parsing typically requires rich, highdimensional feature representations. A small subset of such features is often selected manually. This is problematic when features lack clear linguistic meaning as in embeddings or when the information is blended across features. In this paper, we use tensors to map high-dimensional feature vectors into low dimensional representations. We explicitly maintain the parameters as a low-rank tensor to obtain low dimensional representations of words in their syntactic roles, and to leverage modularity in the tensor for easy training with online algorithms. Our parser consistently outperforms the Turbo and MST parsers across 14 different languages. We also obtain the best published UAS results on 5 languages. 1   ", "sections": [{"heading": "Introduction", "text": "Finding an expressive representation of input sentences is crucial for accurate parsing. Syntactic relations manifest themselves in a broad range of surface indicators, ranging from morphological to lexical, including positional and part-of-speech (POS) tagging features. Traditionally, parsing research has focused on modeling the direct connection between the features and the predicted syntactic relations such as head-modifier (arc) relations in dependency parsing. Even in the case of firstorder parsers, this results in a high-dimensional vector representation of each arc. Discrete features, and their cross products, can be further complemented with auxiliary information about words participating in an arc, such as continuous vector representations of words. The exploding dimensionality of rich feature vectors must then be balanced with the difficulty of effectively learning the associated parameters from limited training data.\nA predominant way to counter the high dimensionality of features is to manually design or select a meaningful set of feature templates, which are used to generate different types of features (Mc-Donald et al., 2005a;Martins et al., 2013). Direct manual selection may be problematic for two reasons. First, features may lack clear linguistic interpretation as in distributional features or continuous vector embeddings of words. Second, designing a small subset of templates (and features) is challenging when the relevant linguistic information is distributed across the features. For instance, morphological properties are closely tied to part-of-speech tags, which in turn relate to positional features. These features are not redundant. Therefore, we may suffer a performance loss if we select only a small subset of the features. On the other hand, by including all the rich features, we face over-fitting problems.\nWe depart from this view and leverage highdimensional feature vectors by mapping them into low dimensional representations. We begin by representing high-dimensional feature vectors as multi-way cross-products of smaller feature vectors that represent words and their syntactic relations (arcs). The associated parameters are viewed as a tensor (multi-way array) of low rank, and optimized for parsing performance. By explicitly representing the tensor in a low-rank form, we have direct control over the effective dimensionality of the set of parameters. We obtain role-dependent low-dimensional representations for words (head, modifier) that are specifically tailored for parsing accuracy, and use standard online algorithms for optimizing the low-rank tensor components.\nThe overall approach has clear linguistic and computational advantages:\n\u2022 Our low dimensional embeddings are tailored to the syntactic context of words (head, modifier). This low dimensional syntactic abstraction can be thought of as a proxy to manually constructed POS tags.\n\u2022 By automatically selecting a small number of dimensions useful for parsing, we can leverage a wide array of (correlated) features. Unlike parsers such as MST, we can easily benefit from auxiliary information (e.g., word vectors) appended as features.\nWe implement the low-rank factorization model in the context of first-and third-order dependency parsing. The model was evaluated on 14 languages, using dependency data from CoNLL 2008 and CoNLL 2006. We compare our results against the MST (McDonald et al., 2005a) and Turbo (Martins et al., 2013) parsers. The low-rank parser achieves average performance of 89.08% across 14 languages, compared to 88.73% for the Turbo parser, and 87.19% for MST. The power of the low-rank model becomes evident in the absence of any part-of-speech tags. For instance, on the English dataset, the low-rank model trained without POS tags achieves 90.49% on first-order parsing, while the baseline gets 86.70% if trained under the same conditions, and 90.58% if trained with 12 core POS tags. Finally, we demonstrate that the model can successfully leverage word vector representations, in contrast to the baselines.", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Selecting Features for Dependency Parsing A great deal of parsing research has been dedicated to feature engineering (Lazaridou et al., 2013;Marton et al., 2010;Marton et al., 2011). While in most state-of-the-art parsers, features are selected manually (McDonald et al., 2005a;McDonald et al., 2005b;Martins et al., 2013;Zhang and McDonald, 2012a;Rush and Petrov, 2012a), automatic feature selection methods are gaining popularity (Martins et al., 2011b;Ballesteros and Nivre, 2012;Nilsson and Nugues, 2010;Ballesteros, 2013). Following standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature selection methods are particularly promising in parsing scenarios where the optimal feature set is likely to be a small subset of the original set of candidate features. Our technique, in contrast, is suitable for cases where the relevant information is distributed across a larger set of related features.\nEmbedding for Dependency Parsing A lot of recent work has been done on mapping words into vector spaces (Collobert and Weston, 2008;Turian et al., 2010;Dhillon et al., 2011;Mikolov et al., 2013). Traditionally, these vector representations have been derived primarily from co-occurrences of words within sentences, ignoring syntactic roles of the co-occurring words. Nevertheless, any such word-level representation can be used to offset inherent sparsity problems associated with full lexicalization (Cirik and \u015e ensoy, 2013). In this sense they perform a role similar to POS tags.\nWord-level vector space embeddings have so far had limited impact on parsing performance. From a computational perspective, adding nonsparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g., dependency arc). Because of this issue, Cirik and \u015e ensoy (2013) used word vectors only as unigram features (without combinations) as part of a shift reduce parser (Nivre et al., 2007). The improvement on the overall parsing performance was marginal. Another application of word vectors is compositional vector grammar (Socher et al., 2013). While this method learns to map word combinations into vectors, it builds on existing word-level vector representations. In contrast, we represent words as vectors in a manner that is directly optimized for parsing. This framework enables us to learn new syntactically guided embeddings while also leveraging separately estimated word vectors as starting features, leading to improved parsing performance.\nDimensionality Reduction Many machine learning problems can be cast as matrix problems where the matrix represents a set of co-varying parameters. Such problems include, for example, multi-task learning and collaborative filtering. Rather than assuming that each parameter can be set independently of others, it is helpful to assume that the parameters vary in a low dimensional subspace that has to be estimated together with the parameters. In terms of the parameter matrix, this corresponds to a low-rank assumption. Low-rank constraints are commonly used for improving generalization (Lee and Seung, 1999;Srebro et al., 2003;Srebro et al., 2004;Evgeniou and Pontil, 2007) A strict low-rank assumption can be restrictive. Indeed, recent approaches to matrix problems decompose the parameter matrix as a sum of lowrank and sparse matrices (Tao and Yuan, 2011;Zhou and Tao, 2011). The sparse matrix is used to highlight a small number of parameters that should vary independently even if most of them lie on a low-dimensional subspace (Waters et al., 2011;Chandrasekaran et al., 2011). We follow this decomposition while extending the parameter matrix into a tensor.\nTensors are multi-way generalizations of matrices and possess an analogous notion of rank. Tensors are increasingly used as tools in spectral estimation (Hsu and Kakade, 2013), including in parsing (Cohen et al., 2012) and other NLP problems (de Cruys et al., 2013), where the goal is to avoid local optima in maximum likelihood estimation. In contrast, we expand features for parsing into a multi-way tensor, and operate with an explicit low-rank representation of the associated parameter tensor. The explicit representation sidesteps inherent complexity problems associated with the tensor rank (Hillar and Lim, 2009). Our parameters are divided into a sparse set corresponding to manually chosen MST or Turbo parser features and a larger set governed by a low-rank tensor.", "publication_ref": ["b17", "b24", "b25", "b26", "b27", "b23", "b42", "b33", "b22", "b0", "b30", "b1", "b7", "b40", "b10", "b29", "b4", "b4", "b32", "b35", "b18", "b36", "b37", "b11", "b39", "b46", "b41", "b3", "b14", "b5", "b9", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Problem Formulation", "text": "We will commence here by casting first-order dependency parsing as a tensor estimation problem. We will start by introducing the notation used in the paper, followed by a more formal description of our dependency parsing task.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Basic Notations", "text": "Let A \u2208 R n\u00d7n\u00d7d be a 3-dimensional tensor (a 3way array). We denote each element of the tensor as\nA i,j,k where i \u2208 [n], j \u2208 [n], k \u2208 [d] and [n]\nis a shorthand for the set of integers {1, 2, \u2022 \u2022 \u2022 , n}. Similarly, we use M i,j and u i to represent the elements of matrix M and vector u, respectively.\nWe define the inner product of two tensors (or matrices) as A, B = vec(A) T vec(B), where vec(\u2022) concatenates the tensor (or matrix) elements into a column vector. The squared norm of a tensor/matrix is denoted by A 2 = A, A .\nThe Kronecker product of three vectors is denoted by u \u2297 v \u2297 w and forms a rank-1 tensor such that\n(u \u2297 v \u2297 w) i,j,k = u i v j w k .\nNote that the vectors u, v, and w may be column or row vectors. Their orientation is defined based on usage. For example, u \u2297 v is a rank-1 matrix uv T when u and v are column vectors (u T v if they are row vectors). We say that tensor A is in Kruskal form if\nA = r i=1 U (i, :) \u2297 V (i, :) \u2297 W (i, :)(1)\nwhere U, V \u2208 R r\u00d7n , W \u2208 R r\u00d7d and U (i, :) is the i th row of matrix U . We will directly learn a lowrank tensor A (because r is small) in this form as one of our model parameters.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dependency Parsing", "text": "Let x be a sentence and Y(x) the set of possible dependency trees over the words in x. We assume that the score S(x, y) of each candidate dependency tree y \u2208 Y(x) decomposes into a sum of \"local\" scores for arcs. Specifically:\nS(x, y) = h\u2192m \u2208 y s(h \u2192 m) \u2200y \u2208 Y(x)\nwhere h \u2192 m is the head-modifier dependency arc in the tree y. Each y is understood as a collection of arcs h \u2192 m where h and m index words in x. 2 For example, x(h) is the word corresponding to h. We suppress the dependence on x whenever it is clear from context. For example, s(h \u2192 m) can depend on x in complicated ways as discussed below. The predicted parse is obtained as\u0177 = arg max y\u2208Y(x) S(x, y).\nA key problem is how we parameterize the arc scores s(h \u2192 m). Following the MST parser (McDonald et al., 2005a) we can define rich features characterizing each head-modifier arc, compiled into a sparse binary vector \u03c6 h\u2192m \u2208 R L that depends on the sentence x as well as the chosen arc h \u2192 m (again, we suppress the dependence on x). Based on this feature representation, we define the score of each arc as s \u03b8 (h \u2192 m) = Unigram features: form form-p form-n lemma lemma-p lemma-n pos pos-p pos-n morph bias Bigram features: pos-p, pos pos, pos-n pos, lemma morph, lemma Trigram features: pos-p, pos, pos-n Table 1: Word feature templates used by our model. pos, form, lemma and morph stand for the fine POS tag, word form, word lemma and the morphology feature (provided in CoNLL format file) of the current word. There is a bias term that is always active for any word. The suffixes -p and -n refer to the left and right of the current word respectively. For example, pos-p means the POS tag to the left of the current word in the sentence. \u03b8, \u03c6 h\u2192m where \u03b8 \u2208 R L represent adjustable parameters to be learned, and L is the number of parameters (and possible features in \u03c6 h\u2192m ).\nWe can alternatively specify arc features in terms of rank-1 tensors by taking the Kronecker product of simpler feature vectors associated with the head (vector \u03c6 h \u2208 R n ), and modifier (vector \u03c6 m \u2208 R n ), as well as the arc itself (vector \u03c6 h,m \u2208 R d ). Here \u03c6 h,m is much lower dimensional than the MST arc feature vector \u03c6 h\u2192m discussed earlier. For example, \u03c6 h,m may be composed of only indicators for binned arc lengths 3 . \u03c6 h and \u03c6 m , on the other hand, are built from features shown in Table 1. By taking the cross-product of all these component feature vectors, we obtain the full feature representation for arc h \u2192 m as a rank-1 tensor\n\u03c6 h \u2297 \u03c6 m \u2297 \u03c6 h,m \u2208 R n\u00d7n\u00d7d\nNote that elements of this rank-1 tensor include feature combinations that are not part of the feature crossings in \u03c6 h\u2192m . In this sense, the rank-1 tensor represents a substantial feature expansion. The arc score s tensor (h \u2192 m) associated with the tensor representation is defined analogously as\ns tensor (h \u2192 m) = A, \u03c6 h \u2297 \u03c6 m \u2297 \u03c6 h,m\nwhere the adjustable parameters A also form a tensor. Given the typical dimensions of the component feature vectors, \u03c6 h , \u03c6 m , \u03c6 h,m , it is not even possible to store all the parameters in A. Indeed, in the full English training set of CoNLL-2008, the tensor involves around 8 \u00d7 10 11 entries while the MST feature vector has approximately 1.5 \u00d7 10 7 features. To counter this feature explosion, we restrict the parameters A to have low rank.", "publication_ref": ["b26"], "figure_ref": [], "table_ref": []}, {"heading": "Low-Rank Dependency Scoring", "text": "We can represent a rank-r tensor A explicitly in terms of parameter matrices U , V , and W as shown in Eq. 1. As a result, the arc score for the tensor reduces to evaluating U \u03c6 h , V \u03c6 m , and W \u03c6 h,m which are all r dimensional vectors and can be computed efficiently based on any sparse vectors \u03c6 h , \u03c6 m , and \u03c6 h,m . The resulting arc score\ns tensor (h \u2192 m) is then r i=1 [U \u03c6 h ] i [V \u03c6 m ] i [W \u03c6 h,m ] i (2)\nBy learning parameters U , V , and W that function well in dependency parsing, we also learn contextdependent embeddings for words and arcs. Specifically, U \u03c6 h (for a given sentence, suppressed) is an r dimensional vector representation of the word corresponding to h as a head word. Similarly, V \u03c6 m provides an analogous representation for a modifier m. Finally, W \u03c6 h,m is a vector embedding of the supplemental arc-dependent information. The resulting embedding is therefore tied to the syntactic roles of the words (and arcs), and learned in order to perform well in parsing.\nWe expect a dependency parsing model to benefit from several aspects of the low-rank tensor scoring. For example, we can easily incorporate additional useful features in the feature vectors \u03c6 h , \u03c6 m and \u03c6 h,m , since the low-rank assumption (for small enough r) effectively counters the otherwise uncontrolled feature expansion. Moreover, by controlling the amount of information we can extract from each of the component feature vectors (via rank r), the statistical estimation problem does not scale dramatically with the dimensions of \u03c6 h , \u03c6 m and \u03c6 h,m . In particular, the low-rank constraint can help generalize to unseen arcs. Consider a feature \u03b4(\nx(h) = a) \u2022 \u03b4(x(m) = b) \u2022 \u03b4(dis(x, h, m) = c\n) which is non-zero only for an arc a \u2192 b with distance c in sentence x. If the arc has not been seen in the available training data, it does not contribute to the traditional arc score s \u03b8 (\u2022). In contrast, with the low-rank constraint, the arc score in Eq. 2 would typically be non-zero.\nCombined Scoring Our parsing model aims to combine the strengths of both traditional features from the MST/Turbo parser as well as the new low-rank tensor features. In this way, our model is able to capture a wide range of information including the auxiliary features without having uncontrolled feature explosion, while still having the full accessibility to the manually engineered features that are proven useful. Specifically, we define the arc score s \u03b3 (h \u2192 m) as the combination\n(1 \u2212 \u03b3)s tensor (h \u2192 m) + \u03b3s \u03b8 (h \u2192 m) = (1 \u2212 \u03b3) r i=1 [U \u03c6 h ] i [V \u03c6 m ] i [W \u03c6 h,m ] i + \u03b3 \u03b8, \u03c6 h\u2192m (3\n)\nwhere \u03b8 \u2208 R L , U \u2208 R r\u00d7n , V \u2208 R r\u00d7n , and W \u2208 R r\u00d7d are the model parameters to be learned. The rank r and \u03b3 \u2208 [0, 1] (balancing the two scores) represent hyper-parameters in our model.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning", "text": "The training set D = {(x i ,\u0177 i )} N i=1 consists of N pairs, where each pair consists of a sentence x i and the corresponding gold (target) parse y i . The goal is to learn values for the parameters \u03b8, U , V and W that optimize the combined scoring function S \u03b3 (x, y) = h\u2192m\u2208y s \u03b3 (h \u2192 m), defined in Eq. 3, for parsing performance. We adopt a maximum soft-margin framework for this learning problem. Specifically, we find parameters \u03b8, U , V , W , and {\u03be i } that minimize\nC i \u03be i + \u03b8 2 + U 2 + V 2 + W 2 s.t. S \u03b3 (x i ,\u0177 i ) \u2265 S \u03b3 (x i , y i ) + \u0177 i \u2212 y i 1 \u2212 \u03be i \u2200y i \u2208 Y(x i ), \u2200i. (4\n)\nwhere \u0177 i \u2212y i 1 is the number of mismatched arcs between the two trees, and \u03be i is a non-negative slack variable. The constraints serve to separate the gold tree from other alternatives in Y(x i ) with a margin that increases with distance.\nThe objective as stated is not jointly convex with respect to U , V and W due to our explicit representation of the low-rank tensor. However, if we fix any two sets of parameters, for example, if we fix V and W , then the combined score S \u03b3 (x, y) will be a linear function of both \u03b8 and U . As a result, the objective will be jointly convex with respect to \u03b8 and U and could be optimized using standard tools. However, to accelerate learning, we adopt an online learning setup. Specifically, we use the passive-aggressive learning algorithm (Crammer et al., 2006) tailored to our setting, updating pairs of parameter sets, (\u03b8, U ), (\u03b8, V ) and (\u03b8, W ) in an alternating manner. This method is described below.\nOnline Learning In an online learning setup, we update parameters successively based on each sentence. In order to apply the passive-aggressive algorithm, we fix two of U , V and W (say, for example, V and W ) in an alternating manner, and apply a closed-form update to the remaining parameters (here U and \u03b8). This is possible since the objective function with respect to (\u03b8, U ) has a similar form as in the original passive-aggressive algorithm. To illustrate this, consider a training sentence x i . The update involves finding first the best competing tree,\ny i = arg max y i \u2208Y(x i ) S \u03b3 (x i , y i ) + \u0177 i \u2212 y i 1 (5)\nwhich is the tree that violates the constraint in Eq. 4 most (i.e. maximizes the loss \u03be i ). We then obtain parameter increments \u2206\u03b8 and \u2206U by solving\nmin \u2206\u03b8, \u2206U, \u03be\u22650 1 2 \u2206\u03b8 2 + 1 2 \u2206U 2 + C\u03be s.t. S \u03b3 (x i ,\u0177 i ) \u2265 S \u03b3 (x i ,\u1ef9 i ) + \u0177 i \u2212\u1ef9 i 1 \u2212 \u03be\nIn this way, the optimization problem attempts to keep the parameter change as small as possible, while forcing it to achieve mostly zero loss on this single instance. This problem has a closed form solution\n\u2206\u03b8 = min C, loss \u03b3 2 d\u03b8 2 + (1 \u2212 \u03b3) 2 du 2 \u03b3d\u03b8 \u2206U = min C, loss \u03b3 2 d\u03b8 2 + (1 \u2212 \u03b3) 2 du 2 (1 \u2212 \u03b3)du where loss = S \u03b3 (x i ,\u1ef9 i ) + \u0177 i \u2212\u1ef9 i 1 \u2212 S \u03b3 (x i ,\u0177 i ) d\u03b8 = h\u2192m \u2208\u0177 i \u03c6 h\u2192m \u2212 h\u2192m \u2208\u1ef9 i \u03c6 h\u2192m du = h\u2192m \u2208\u0177 i [(V \u03c6 m ) (W \u03c6 h,m )] \u2297 \u03c6 h \u2212 h\u2192m \u2208\u1ef9 i [(V \u03c6 m ) (W \u03c6 h,m )] \u2297 \u03c6 h\nwhere (u v) i = u i v i is the Hadamard (elementwise) product. The magnitude of change of \u03b8 and U is controlled by the parameter C. By varying C, we can determine an appropriate step size for the online updates. The updates also illustrate how \u03b3 balances the effect of the MST component of the score relative to the low-rank tensor score. When \u03b3 = 0, the arc scores are entirely based on the lowrank tensor and \u2206\u03b8 = 0. Note that \u03c6 h , \u03c6 m , \u03c6 h,m , and \u03c6 h\u2192m are typically very sparse for each word or arc. Therefore du and d\u03b8 are also sparse and can be computed efficiently.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Initialization", "text": "The alternating online algorithm relies on how we initialize U , V , and W since each update is carried out in the context of the other two. A random initialization of these parameters is unlikely to work well, both due to the dimensions involved, and the nature of the alternating updates. We consider here instead a reasonable deterministic \"guess\" as the initialization method.\nWe begin by training our model without any low-rank parameters, and obtain parameters \u03b8. The majority of features in this MST component can be expressed as elements of the feature tensor, i.e., as [\u03c6 h \u2297 \u03c6 m \u2297 \u03c6 h,m ] i,j,k . We can therefore create a tensor representation of \u03b8 such that B i,j,k equals the corresponding parameter value in \u03b8. We use a low-rank version of B as the initialization. Specifically, we unfold the tensor B into a matrix B (h) of dimensions n and nd, where\nn = dim(\u03c6 h ) = dim(\u03c6 m ) and d = dim(\u03c6 h,m ).\nFor instance, a rank-1 tensor can be unfolded as u \u2297 v \u2297 w = u \u2297 vec(v \u2297 w). We compute the top-r SVD of the resulting unfolded matrix such that B (h) = P T SQ. U is initialized as P . Each right singular vector S i Q(i, :) is also a matrix in R n\u00d7d . The leading left and right singular vectors of this matrix are assigned to V (i, :) and W (i, :) respectively. In our implementation, we run one epoch of our model without low-rank parameters and initialize the tensor A.\nParameter Averaging The passive-aggressive algorithm regularizes the increments (e.g. \u2206\u03b8 and \u2206U ) during each update but does not include any overall regularization. In other words, keeping updating the model may lead to large parameter values and over-fitting. To counter this effect, we use parameter averaging as used in the MST and Turbo parsers. The final parameters are those averaged across all the iterations (cf. (Collins, 2002)). For simplicity, in our algorithm we average U , V , W and \u03b8 separately, which works well empirically.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "Datasets We test our dependency model on 14 languages, including the English dataset from CoNLL 2008 shared tasks and all 13 datasets from CoNLL 2006 shared tasks (Buchholz and Marsi, 2006;Surdeanu et al., 2008). These datasets include manually annotated dependency trees, POS tags and morphological information. Following standard practices, we encode this information as features.\nMethods We compare our model to MST and Turbo parsers on non-projective dependency parsing. For our parser, we train both a first-order parsing model (as described in Section 3 and 4) as well as a third-order model. The third order parser simply adds high-order features, those typically used in MST and Turbo parsers, into our s \u03b8 (x, y) = \u03b8, \u03c6(x, y) scoring component. The decoding algorithm for the third-order parsing is based on (Zhang et al., 2014). For the Turbo parser, we directly compare with the recent published results in (Martins et al., 2013). For the MST parser, we train and test using the most recent version of the code. 4 In addition, we implemented two additional baselines, NT-1st (first order) and NT-3rd (third order), corresponding to our model without the tensor component.\nFeatures For the arc feature vector \u03c6 h\u2192m , we use the same set of feature templates as MST v0.5.1. For head/modifier vector \u03c6 h and \u03c6 m , we show the complete set of feature templates used by our model in Table 1. Finally, we use a similar set of feature templates as Turbo v2.1 for 3rd order parsing.\nTo add auxiliary word vector representations, we use the publicly available word vectors (Cirik First-order 2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013).\nand \u015e ensoy, 2013), learned from raw data (Globerson et al., 2007;Maron et al., 2010). Three languages in our dataset -English, German and Swedish -have corresponding word vectors in this collection. 5 The dimensionality of this representation varies by language: English has 50 dimensional word vectors, while German and Swedish have 25 dimensional word vectors. Each entry of the word vector is added as a feature value into feature vectors \u03c6 h and \u03c6 m . For each word in the sentence, we add its own word vector as well as the vectors of its left and right words. We should note that since our model parameter A is represented and learned in the low-rank form, we only have to store and maintain the low-rank projections U \u03c6 h , V \u03c6 m and W \u03c6 h,m rather than explicitly calculate the feature tensor \u03c6 h \u2297\u03c6 m \u2297\u03c6 h,m . Therefore updating parameters and decoding a sentence is still efficient, i.e., linear in the number of values of the feature vector. In contrast, assume we take the cross-product of the auxiliary word vector values, POS tags and lexical items of a word and its context, and add the crossed values into a normal model (in \u03c6 h\u2192m ). The number of features for each arc would be at least quadratic, growing into thousands, and would be a significant impediment to parsing efficiency.\nEvaluation Following standard practices, we train our full model and the baselines for 10 5 https://github.com/wolet/sprml13-word-embeddings epochs. As the evaluation measure, we use unlabeled attachment scores (UAS) excluding punctuation. In all the reported experiments, the hyperparameters are set as follows: r = 50 (rank of the tensor), C = 1 for first-order model and C = 0.01 for third-order model.", "publication_ref": ["b2", "b38", "b45", "b23", "b34", "b43", "b44", "b12", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Overall Performance Table 2 shows the performance of our model and the baselines on 14 CoNLL datasets. Our model outperforms Turbo parser, MST parser, as well as its own variants without the tensor component. The improvements of our low-rank model are consistent across languages: results for the first order parser are better on 11 out of 14 languages. By comparing NT-1st and NT-3rd (models without low-rank) with our full model (with low-rank), we obtain 0.7% absolute improvement on first-order parsing, and 0.3% improvement on third-order parsing. Our model also achieves the best UAS on 5 languages.\nWe next focus on the first-order model and gauge the impact of the tensor component. First, we test our model by varying the hyper-parameter \u03b3 which balances the tensor score and the traditional MST/Turbo score components. Figure 1 shows the average UAS on CoNLL test datasets after each training epoch. We can see that the improvement of adding the low-rank tensor is consistent across various choices of hyper parame-  To assess the ability of our model to incorporate a range of features, we add unsupervised word vectors to our model. As described in previous section, we do so by appending the values of different coordinates in the word vector into \u03c6 h and \u03c6 m . As Table 3 shows, adding this information increases the parsing performance for all the three languages. For instance, we obtain more than 0.5% absolute improvement on Swedish.\nSyntactic Abstraction without POS Since our model learns a compressed representation of feature vectors, we are interested to measure its performance when part-of-speech tags are not provided (See the performance of traditional parsers drops when tags are not provided. For example, the performance gap is 10% on German. Our experiments show that low-rank parser operates effectively in the absence of tags. In fact, it nearly reaches the performance of the original parser that used the tags on English.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1", "tab_3"]}, {"heading": "Examples of Derived Projections", "text": "We manually analyze low-dimensional projections to assess whether they capture syntactic abstraction. For this purpose, we train a model with only a tensor component (such that it has to learn an accurate tensor) on the English dataset and obtain low dimensional embeddings U \u03c6 w and V \u03c6 w for each word. The two r-dimension vectors are concatenated as an \"averaged\" vector. We use this vector to calculate the cosine similarity between words. Table 5 shows examples of five closest neighbors of queried words. While these lists include some noise, we can clearly see that the neighbors exhibit similar syntactic behavior. For example, \"on\" is close to other prepositions. More interestingly, we can consider the impact of syntactic context on the derived projections. The bottom part of Table 5 shows that the neighbors change substantially depending on the syntactic role of the word. For example, the closest words to the word \"increase\" are verbs in the context phrase \"will increase again\", while the closest words become nouns given a different phrase \"an increase of\".   Based on these results, estimating a rank-50 tensor together with MST parameters only increases the running time by a factor of 1.7.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_6", "tab_6"]}, {"heading": "Running Time", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions", "text": "Accurate scoring of syntactic structures such as head-modifier arcs in dependency parsing typically requires rich, high-dimensional feature representations. We introduce a low-rank factorization method that enables to map high dimensional feature vectors into low dimensional representations. Our method maintains the parameters as a low-rank tensor to obtain low dimensional representations of words in their syntactic roles, and to leverage modularity in the tensor for easy training with online algorithms. We implement the approach on first-order to third-order dependency parsing. Our parser outperforms the Turbo and MST parsers across 14 languages. Future work involves extending the tensor component to capture higher-order structures. In particular, we would consider second-order structures such as grandparent-head-modifier by increasing the dimensionality of the tensor. This tensor will accordingly be a four or five-way array. The online update algorithm remains applicable since each dimension is optimized in an alternating fashion.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "The authors acknowledge the support of the MURI program (W911NF-10-1-0533) and the DARPA BOLT program. This research is developed in collaboration with the Arabic Language Technoligies (ALT) group at Qatar Computing Research Institute (QCRI) within the LYAS project. We thank Volkan Cirik for sharing the unsupervised word vector data. Thanks to Amir Globerson, Andreea Gane, the members of the MIT NLP group and the ACL reviewers for their suggestions and comments. Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Mal-tOptimizer: An optimization tool for MaltParser", "journal": "", "year": "2012", "authors": "Miguel Ballesteros; Joakim Nivre"}, {"ref_id": "b1", "title": "Effective morphological feature selection with MaltOptimizer at the SPMRL 2013 shared task", "journal": "Association for Computational Linguistics", "year": "2013", "authors": "Miguel Ballesteros"}, {"ref_id": "b2", "title": "CoNLL-X shared task on multilingual dependency parsing", "journal": "Association for Computational Linguistics", "year": "2006", "authors": "Sabine Buchholz; Erwin Marsi"}, {"ref_id": "b3", "title": "Rank-sparsity incoherence for matrix decomposition", "journal": "SIAM Journal on Optimization", "year": "2011", "authors": "Venkat Chandrasekaran; Sujay Sanghavi; Pablo A Parrilo; Alan S Willsky"}, {"ref_id": "b4", "title": "The AI-KU system at the SPMRL 2013 shared task : Unsupervised features for dependency parsing", "journal": "Association for Computational Linguistics", "year": "2013", "authors": "Volkan Cirik; \u015e H\u00fcsn\u00fc;  Ensoy"}, {"ref_id": "b5", "title": "Spectral learning of latent-variable PCFGs", "journal": "", "year": "2012", "authors": "Karl Shay B Cohen; Michael Stratos;  Collins; P Dean; Lyle Foster;  Ungar"}, {"ref_id": "b6", "title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "journal": "", "year": "2002", "authors": "Michael Collins"}, {"ref_id": "b7", "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "journal": "", "year": "2008", "authors": "R Collobert; J Weston"}, {"ref_id": "b8", "title": "Online passive-aggressive algorithms", "journal": "The Journal of Machine Learning Research", "year": "2006", "authors": "Koby Crammer; Ofer Dekel; Joseph Keshet; Shai Shalev-Shwartz; Yoram Singer"}, {"ref_id": "b9", "title": "A tensor-based factorization model of semantic compositionality", "journal": "", "year": "2013", "authors": "Tim Van De Cruys; Thierry Poibeau; Anna Korhonen"}, {"ref_id": "b10", "title": "Multiview learning of word embeddings via CCA", "journal": "", "year": "2011", "authors": "S Paramveer; Dean Dhillon; Lyle Foster;  Ungar"}, {"ref_id": "b11", "title": "Multitask feature learning", "journal": "The MIT Press", "year": "2007", "authors": "A Evgeniou; Massimiliano Pontil"}, {"ref_id": "b12", "title": "Euclidean embedding of cooccurrence data", "journal": "Journal of Machine Learning Research", "year": "2007", "authors": "Gal Amir Globerson; Fernando Chechik; Naftali Pereira;  Tishby"}, {"ref_id": "b13", "title": "Most tensor problems are NP-hard", "journal": "", "year": "2009", "authors": "Christopher Hillar; Lek-Heng Lim"}, {"ref_id": "b14", "title": "Learning mixtures of spherical gaussians: moment methods and spectral decompositions", "journal": "ACM", "year": "2013", "authors": "Daniel Hsu; M Sham;  Kakade"}, {"ref_id": "b15", "title": "Efficient thirdorder dependency parsers", "journal": "", "year": "2010", "authors": "Terry Koo; Michael Collins"}, {"ref_id": "b16", "title": "Dual decomposition for parsing with non-projective head automata", "journal": "Association for Computational Linguistics", "year": "2010", "authors": "Terry Koo; M Alexander; Michael Rush; Tommi Collins; David Jaakkola;  Sontag"}, {"ref_id": "b17", "title": "Fish transporters and miracle homes: How compositional distributional semantics can help NP parsing", "journal": "Association for Computational Linguistics", "year": "2013", "authors": "Angeliki Lazaridou; Eva Maria Vecchi; Marco Baroni"}, {"ref_id": "b18", "title": "Learning the parts of objects by non-negative matrix factorization", "journal": "Nature", "year": "1999", "authors": "D Daniel; H Lee;  Sebastian Seung"}, {"ref_id": "b19", "title": "Sphere embedding: An application to partof-speech induction", "journal": "", "year": "2010", "authors": "Yariv Maron; Michael Lamar; Elie Bienenstock"}, {"ref_id": "b20", "title": "Turbo parsers: Dependency parsing by approximate variational inference", "journal": "Association for Computational Linguistics", "year": "2010", "authors": "F T Andr\u00e9; Noah A Martins; Eric P Smith;  Xing; M Q Pedro;  Aguiar;  M\u00e1rio;  Figueiredo"}, {"ref_id": "b21", "title": "Dual decomposition with many overlapping components", "journal": "Association for Computational Linguistics", "year": "2011", "authors": "F T Andr\u00e9; Noah A Martins; Pedro M Q Smith;  Aguiar; A T M\u00e1rio;  Figueiredo"}, {"ref_id": "b22", "title": "Structured sparsity in structured prediction", "journal": "Association for Computational Linguistics", "year": "2011", "authors": "F T Andr\u00e9; Noah A Martins;  Smith; M Q Pedro;  Aguiar;  M\u00e1rio;  Figueiredo"}, {"ref_id": "b23", "title": "Turning on the turbo: Fast third-order non-projective turbo parsers", "journal": "Association for Computational Linguistics", "year": "2013", "authors": "F T Andr\u00e9; Miguel B Martins; Noah A Almeida;  Smith"}, {"ref_id": "b24", "title": "Improving arabic dependency parsing with lexical and inflectional morphological features", "journal": "", "year": "2010", "authors": "Yuval Marton; Nizar Habash; Owen Rambow"}, {"ref_id": "b25", "title": "Improving arabic dependency parsing with form-based and functional morphological features", "journal": "", "year": "2011", "authors": "Yuval Marton; Nizar Habash; Owen Rambow"}, {"ref_id": "b26", "title": "Online large-margin training of dependency parsers", "journal": "", "year": "2005", "authors": "Ryan Mcdonald; Koby Crammer; Fernando Pereira"}, {"ref_id": "b27", "title": "Non-projective dependency parsing using spanning tree algorithms", "journal": "Association for Computational Linguistics", "year": "2005", "authors": "Ryan Mcdonald; Fernando Pereira; Kiril Ribarov"}, {"ref_id": "b28", "title": "Multilingual dependency analysis with a two-stage discriminative parser", "journal": "Association for Computational Linguistics", "year": "2006", "authors": "Ryan Mcdonald; Kevin Lerman; Fernando Pereira"}, {"ref_id": "b29", "title": "Efficient estimation of word representations in vector space", "journal": "CoRR", "year": "2013", "authors": "Tomas Mikolov; Kai Chen; Greg Corrado; Jeffrey Dean"}, {"ref_id": "b30", "title": "Automatic discovery of feature sets for dependency parsing", "journal": "", "year": "2010", "authors": "Peter Nilsson; Pierre Nugues"}, {"ref_id": "b31", "title": "Labeled pseudoprojective dependency parsing with support vector machines", "journal": "", "year": "2006", "authors": "Joakim Nivre; Johan Hall; Jens Nilsson; G\u00fcl\u015fen Eryiit; Svetoslav Marinov"}, {"ref_id": "b32", "title": "MaltParser: A language-independent system for data-driven dependency parsing", "journal": "Natural Language Engineering", "year": "2007", "authors": "Joakim Nivre; Johan Hall; Jens Nilsson; Atanas Chanev; G\u00fclsen Eryigit; Sandra K\u00fcbler; Svetoslav Marinov; Erwin Marsi"}, {"ref_id": "b33", "title": "Vine pruning for efficient multi-pass dependency parsing", "journal": "", "year": "2012", "authors": "Alexander Rush; Slav Petrov"}, {"ref_id": "b34", "title": "Vine pruning for efficient multi-pass dependency parsing", "journal": "Association for Computational Linguistics", "year": "2012", "authors": "M Alexander; Slav Rush;  Petrov"}, {"ref_id": "b35", "title": "Parsing with compositional vector grammars", "journal": "", "year": "2013", "authors": "Richard Socher; John Bauer; Christopher D Manning; Andrew Y Ng"}, {"ref_id": "b36", "title": "Weighted low-rank approximations", "journal": "", "year": "2003", "authors": "Nathan Srebro; Tommi Jaakkola"}, {"ref_id": "b37", "title": "Maximum-margin matrix factorization", "journal": "", "year": "2004", "authors": "Nathan Srebro; Jason Rennie; Tommi S Jaakkola"}, {"ref_id": "b38", "title": "The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies", "journal": "Association for Computational Linguistics", "year": "2008", "authors": "Mihai Surdeanu; Richard Johansson; Adam Meyers; Llu\u00eds M\u00e0rquez; Joakim Nivre"}, {"ref_id": "b39", "title": "Recovering lowrank and sparse components of matrices from incomplete and noisy observations", "journal": "SIAM Journal on Optimization", "year": "2011", "authors": "Min Tao; Xiaoming Yuan"}, {"ref_id": "b40", "title": "Word representations: A simple and general method for semi-supervised learning", "journal": "", "year": "2010", "authors": "Joseph Turian; Lev Ratinov; Yoshua Bengio"}, {"ref_id": "b41", "title": "SpaRCS: Recovering lowrank and sparse matrices from compressive measurements", "journal": "", "year": "2011", "authors": "E Andrew;  Waters; C Aswin; Richard Sankaranarayanan;  Baraniuk"}, {"ref_id": "b42", "title": "Generalized higher-order dependency parsing with cube pruning", "journal": "", "year": "2012", "authors": "Hao Zhang; Ryan Mcdonald"}, {"ref_id": "b43", "title": "Generalized higher-order dependency parsing with cube pruning", "journal": "Association for Computational Linguistics", "year": "2012", "authors": "Hao Zhang; Ryan Mcdonald"}, {"ref_id": "b44", "title": "Online learning for inexact hypergraph search", "journal": "", "year": "2013", "authors": "Hao Zhang; Liang Huang; Kai Zhao; Ryan Mcdonald"}, {"ref_id": "b45", "title": "Steps to excellence: Simple inference with refined scoring of dependency trees", "journal": "", "year": "2014", "authors": "Yuan Zhang; Tao Lei; Regina Barzilay; Tommi Jaakkola; Amir Globerson"}, {"ref_id": "b46", "title": "Godec: Randomized low-rank & sparse matrix decomposition in noisy case", "journal": "", "year": "2011", "authors": "Tianyi Zhou; Dacheng Tao"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "For our model, the experiments are ran with rank r = 50 and hyperparameter \u03b3 = 0.3. To remove the tensor in our model, we ran experiments with \u03b3 = 1, corresponding to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013), Koo et al. (", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ": First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and theEnglish dataset of CoNLL-2008."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ": Results of adding unsupervised word vec-tors to the tensor. Adding this information yieldsconsistent improvement for all languages.ter \u03b3. When training with the tensor componentalone (\u03b3 = 0), the model converges more slowly.Learning of the tensor is harder because the scor-ing function is not linear (nor convex) with respectto parameters U , V and W . However, the tensorscoring component achieves better generalizationon the test data, resulting in better UAS than NT-1st after 8 training epochs."}, {"figure_label": "44", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "). The rationale is that given all other features, the model would induce representations that play a similar role to POS tags. Note that POS +wv. -POS +POS English 88.89 90.49 86.70 90.58 German 82.63 85.80 78.71 88.50 Swedish 81.84 85.90 79.65 88.75 The first three columns show parsing results when models are trained without POS tags. The last column gives the upper-bound, i.e. the performance of a parser trained with 12 Core POS tags. The low-rank model outperforms NT-1st by a large margin. Adding word vector features further improves performance.", "figure_data": "Our modelNT-1st-"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Table6illustrates the impact of estimating low-rank tensor parameters on the running time of the algorithm. For comparison, we also show the NT-1st times across three typical languages. The Arabic dataset has the longest average sentence length, while the Chinese dataset", "figure_data": "greatlyprofitsaysonwhenactivelyearningsaddswithwhereopenlyfranchisees predictsintowhatsignificantly sharesnotedatwhyoutrightrevenuewroteduring whichsubstantially memberscontends overwhoincreasewill increase again an increase ofrisearguinggainadvancebepricescontestchargingpaymenthaltgonemembersExchequer makingsubsidiaryhitattacks hit thehardest hit issheddistributesmonopoliesralliedstayedpillstriggeredsangsophisticationappearedremovedventuresunderstateeasedfactors"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Five closest neighbors of the queried words (shown in bold). The upper part shows our learned embeddings group words with similar syntactic behavior. The two bottom parts of the table demonstrate that how the projections change depending on the syntactic context of the word.", "figure_data": "#Tok. Len.Train. Time (hour) NT-1st OursArabic42K320.130.22Chinese 337K60.370.65English 958K241.882.83"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Comparison of training times across three typical datasets. The second column is the number of tokens in each data set. The third column shows the average sentence length. Both first-order models are implemented in Java and run as a single process. has the shortest sentence length in CoNLL 2006.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "A i,j,k where i \u2208 [n], j \u2208 [n], k \u2208 [d] and [n]", "formula_coordinates": [3.0, 84.73, 645.8, 205.54, 11.95]}, {"formula_id": "formula_1", "formula_text": "(u \u2297 v \u2297 w) i,j,k = u i v j w k .", "formula_coordinates": [3.0, 355.86, 106.92, 121.1, 11.95]}, {"formula_id": "formula_2", "formula_text": "A = r i=1 U (i, :) \u2297 V (i, :) \u2297 W (i, :)(1)", "formula_coordinates": [3.0, 338.12, 217.47, 187.42, 34.69]}, {"formula_id": "formula_3", "formula_text": "S(x, y) = h\u2192m \u2208 y s(h \u2192 m) \u2200y \u2208 Y(x)", "formula_coordinates": [3.0, 315.57, 427.65, 201.69, 23.53]}, {"formula_id": "formula_4", "formula_text": "\u03c6 h \u2297 \u03c6 m \u2297 \u03c6 h,m \u2208 R n\u00d7n\u00d7d", "formula_coordinates": [4.0, 116.27, 611.63, 129.23, 14.25]}, {"formula_id": "formula_5", "formula_text": "s tensor (h \u2192 m) = A, \u03c6 h \u2297 \u03c6 m \u2297 \u03c6 h,m", "formula_coordinates": [4.0, 325.44, 90.07, 177.19, 11.95]}, {"formula_id": "formula_6", "formula_text": "s tensor (h \u2192 m) is then r i=1 [U \u03c6 h ] i [V \u03c6 m ] i [W \u03c6 h,m ] i (2)", "formula_coordinates": [4.0, 307.28, 338.71, 218.27, 67.03]}, {"formula_id": "formula_7", "formula_text": "x(h) = a) \u2022 \u03b4(x(m) = b) \u2022 \u03b4(dis(x, h, m) = c", "formula_coordinates": [4.0, 425.49, 754.78, 100.05, 10.91]}, {"formula_id": "formula_8", "formula_text": "(1 \u2212 \u03b3)s tensor (h \u2192 m) + \u03b3s \u03b8 (h \u2192 m) = (1 \u2212 \u03b3) r i=1 [U \u03c6 h ] i [V \u03c6 m ] i [W \u03c6 h,m ] i + \u03b3 \u03b8, \u03c6 h\u2192m (3", "formula_coordinates": [5.0, 93.46, 316.02, 192.57, 65.81]}, {"formula_id": "formula_9", "formula_text": ")", "formula_coordinates": [5.0, 286.03, 369.88, 4.24, 10.91]}, {"formula_id": "formula_10", "formula_text": "C i \u03be i + \u03b8 2 + U 2 + V 2 + W 2 s.t. S \u03b3 (x i ,\u0177 i ) \u2265 S \u03b3 (x i , y i ) + \u0177 i \u2212 y i 1 \u2212 \u03be i \u2200y i \u2208 Y(x i ), \u2200i. (4", "formula_coordinates": [5.0, 77.11, 628.77, 208.92, 58.3]}, {"formula_id": "formula_11", "formula_text": ")", "formula_coordinates": [5.0, 286.03, 675.25, 4.24, 10.91]}, {"formula_id": "formula_12", "formula_text": "y i = arg max y i \u2208Y(x i ) S \u03b3 (x i , y i ) + \u0177 i \u2212 y i 1 (5)", "formula_coordinates": [5.0, 333.4, 463.55, 192.14, 20.81]}, {"formula_id": "formula_13", "formula_text": "min \u2206\u03b8, \u2206U, \u03be\u22650 1 2 \u2206\u03b8 2 + 1 2 \u2206U 2 + C\u03be s.t. S \u03b3 (x i ,\u0177 i ) \u2265 S \u03b3 (x i ,\u1ef9 i ) + \u0177 i \u2212\u1ef9 i 1 \u2212 \u03be", "formula_coordinates": [5.0, 314.43, 570.76, 203.46, 47.64]}, {"formula_id": "formula_14", "formula_text": "\u2206\u03b8 = min C, loss \u03b3 2 d\u03b8 2 + (1 \u2212 \u03b3) 2 du 2 \u03b3d\u03b8 \u2206U = min C, loss \u03b3 2 d\u03b8 2 + (1 \u2212 \u03b3) 2 du 2 (1 \u2212 \u03b3)du where loss = S \u03b3 (x i ,\u1ef9 i ) + \u0177 i \u2212\u1ef9 i 1 \u2212 S \u03b3 (x i ,\u0177 i ) d\u03b8 = h\u2192m \u2208\u0177 i \u03c6 h\u2192m \u2212 h\u2192m \u2208\u1ef9 i \u03c6 h\u2192m du = h\u2192m \u2208\u0177 i [(V \u03c6 m ) (W \u03c6 h,m )] \u2297 \u03c6 h \u2212 h\u2192m \u2208\u1ef9 i [(V \u03c6 m ) (W \u03c6 h,m )] \u2297 \u03c6 h", "formula_coordinates": [5.0, 311.24, 716.42, 210.34, 46.35]}, {"formula_id": "formula_15", "formula_text": "n = dim(\u03c6 h ) = dim(\u03c6 m ) and d = dim(\u03c6 h,m ).", "formula_coordinates": [6.0, 72.0, 619.29, 218.27, 11.95]}], "doi": ""}