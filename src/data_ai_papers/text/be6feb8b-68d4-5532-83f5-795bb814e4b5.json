{"title": "It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners", "authors": "Timo Schick; Hinrich Sch\u00fctze", "pub_date": "", "abstract": "When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020)  achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much \"greener\" in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models. 1", "sections": [{"heading": "Introduction", "text": "Pretraining ever-larger language models (LMs) on massive corpora has led to large improvements in NLP (Radford et al., 2018;Devlin et al., 2019;Raffel et al., 2020, i.a.). A standard approach is to replace the pretrained model's output layer with a task-specific head and finetune the entire model on a set of labeled training data. However, language modeling is not only a powerful pretraining objective, but many tasks can be reformulated as cloze questions (e.g., by appending phrases such as \"the correct answer is __\"), allowing pretrained LMs to solve them without any or with only very few labeled examples (Radford et al., 2019;Schick and Sch\u00fctze, 2021).\nRecently, Brown et al. (2020) introduced GPT-3, a pretrained LM with an enormous 175 billion parameters, and showed that it has amazing few-shot abilities: By reformulating tasks as LM problems, ALBERT with PET/iPET outperforms GPT-3 although it is much \"greener\" in that it has three orders of magnitude fewer parameters.\nGPT-3 achieves near state-of-the-art results for some SuperGLUE  tasks given just 32 labeled examples. This is achieved through priming: GPT-3 is given a few demonstrations of inputs and corresponding outputs as context for its predictions, but no gradient updates are performed. While being straightforward to use, this method has two major drawbacks:\n\u2022 It requires a gigantic LM to work well, making it unusable in many real-world scenarios and resulting in a large carbon footprint (Strubell et al., 2019).\n\u2022 It does not scale to more than a few examples as the context window of most LMs is limited to a few hundred tokens. 2 An alternative to priming is pattern-exploiting training (PET) (Schick and Sch\u00fctze, 2021), which combines the idea of reformulating tasks as cloze questions with regular gradient-based finetuning. While PET additionally requires unlabeled data, unlabeled data is much easier to obtain than labeled examples for many real-world applications. Crucially, PET only works when the answers to be predicted by the LM correspond to a single token in its vocabulary; this is a severe limitation as many tasks cannot easily be worded that way.\nIn this work, we adapt PET for tasks that require predicting multiple tokens. We then show that in combination with ALBERT (Lan et al., 2020), PET and its iterative variant (iPET) both outperform GPT-3 on SuperGLUE with 32 training examples, while requiring only 0.1% of its parameters (Figure 1). Moreover, training with PET can be performed in several hours on a single GPU without requiring expensive hyperparameter optimization. Finally, we show that similar performance can also be achieved without unlabeled data and provide a detailed analysis of the factors contributing to PET's strong performance: its ability to combine multiple task formulations, its resilience to wordings that are hard to understand, its usage of labeled data, and characteristics of the underlying LM. Given PET's \"green\" properties, we see our work as an important contribution to an environmentally sound NLP.", "publication_ref": ["b35", "b9", "b40", "b23"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Related Work", "text": "Enabling LMs to perform zero-shot learning by providing task descriptions was proposed by Radford et al. (2019) and has been applied to text classification (Puri and Catanzaro, 2019), commonsense knowledge mining (Davison et al., 2019) and argumentative relation classification (Opitz, 2019). It is also commonly used for probing the knowledge contained within LMs (Trinh and Le, 2018;Petroni et al., 2019;Talmor et al., 2020;Schick and Sch\u00fctze, 2020;Ettinger, 2020, i.a.).\nAs finding ways to reformulate tasks as cloze questions that are understood well by LMs is difficult , Schick and Sch\u00fctze (2021) propose PET, a method that uses knowledge distillation (Hinton et al., 2015) and self-training (e.g., Scudder, 1965;Yarowsky, 1995;Brin, 1999;Mc-Closky et al., 2006) to easily combine several reformulations. Our modified version of PET uses masked language models (Devlin et al., 2019) to assign probabilities to sequences of text; this is similar to using them in a generative fashion (Wang and Cho, 2019) and has previously been investigated by Salazar et al. (2020) and Ghazvininejad et al. (2019). In contrast to PET, which uses gradient-based optimization, Radford et al. (2019) ", "publication_ref": ["b34", "b7", "b30", "b42", "b32", "b18", "b38", "b50", "b2", "b9", "b44", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "P (x)", "text": "Oil prices rise ? __ , Oil prices fall back .\nx 2\nx 1", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Yes", "text": "No\nentailment not_entailment y v(y) q p (y | x)\nFigure 2: Application of a PVP p = (P, v) for recognizing textual entailment: An input x = (x 1 , x 2 ) is converted into a cloze question P (x); q p (y | x) for each y is derived from the probability of v(y) being a plausible choice for the masked position.\nand Brown et al. (2020) investigate priming, where examples are given as context but no parameter updates are performed. Finally, our focus on reducing the amount of compute required for few-shot learning is closely related to other efforts in Green AI (Schwartz et al., 2020a) that aim to improve model efficiency, including techniques for knowledge distillation (e.g., Hinton et al., 2015;Sanh et al., 2019;Jiao et al., 2020;Mao et al., 2020;Anderson and G\u00f3mez-Rodr\u00edguez, 2020), pruning (Han et al., 2015(Han et al., , 2016Sanh et al., 2020) and quantization (Gong et al., 2014;Zafrir et al., 2019;Stock et al., 2021) as well as early exit strategies for inference Schwartz et al., 2020b;.", "publication_ref": ["b18", "b20", "b28", "b0", "b17", "b16", "b47", "b14", "b51", "b39", "b37"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Pattern-Exploiting Training", "text": "Let M be a masked language model (MLM), T its vocabulary and __ \u2208 T the mask token; we denote the set of all token sequences as T * . For some z \u2208 T * containing at least k masks and t \u2208 T , we denote with q k M (t | z) the probability that M assigns to t at the kth masked position in z; the model's logits before applying softmax are denoted with s k M (t | z). We consider the task of mapping inputs x \u2208 X to outputs y \u2208 Y , for which PET requires a set of pattern-verbalizer pairs (PVPs). Each PVP p = (P, v) consists of \u2022 a pattern P : X \u2192 T * that maps inputs to cloze questions containing a single mask;\n\u2022 a verbalizer v : Y \u2192 T that maps each output to a single token representing its task-specific meaning in the pattern.\nAs illustrated in Figure 2, the core idea of PET is to derive the probability of y being the correct output for x from the probability of v(y) being the \"correct\" token at the masked position in P (x). Based on this intuition, a conditional probability distribution q p of y given x is defined as\nq p (y | x) = exp s p (y | x) y \u2208Y exp s p (y | x)(1)\nwhere\ns p (y | x) = s 1 M (v(y) | P (x))\nis the raw score of v(y) at the masked position in P (x).\nFor a given task, identifying PVPs that perform well is challenging in the absence of a large development set. Therefore, PET enables a combination of multiple PVPs P = {p 1 , . . . , p n } as follows: \nq P (y | x) \u221d exp p\u2208P w p \u2022 s p (y | x) (2)\nsimilar to Eq. 1 where w p is a weighting term that is proportional to the accuracy achieved with p on the training set before training.\n3. The resulting soft-labeled dataset is used to train a regular sequence classifier by minimizing cross entropy between its output and q P .\nAs steps (2) and (3) above closely resemble knowledge distillation (Hinton et al., 2015), we also refer to them simply as distillation. Importantly, this process does not require holding the entire ensemble of MLMs in memory at the same time as each model's predictions can be computed sequentially; therefore, it is not more memory expensive than using a single model. To give MLMs trained on different patterns further opportunity to learn from one another, Schick and Sch\u00fctze (2021) also propose iPET, an iterative variant of PET in which several generations of models are trained on datasets of increasing size that are labeled by previous generations. This is achieved as follows: First, an ensemble of MLMs is trained as in regular PET. For each model M i , a random subset of other models is used to generate a new training set T i by assigning labels to those unlabeled examples for which the selected subset of models is most confident in its prediction. Each M i is then retrained on T i ; this process is repeated several times, each time increasing the number of examples in T i by a constant factor. For further details, we refer to Schick and Sch\u00fctze (2021).\nP 2 (x) Awful pizza! It was __ __ . x q 1 M (terri | z) < < < q 2 M ( \u2022 ble | z) (a) z = Awful pizza! It was __ \u2022 ble . x q 1 M (terri | z ) (b) z =", "publication_ref": ["b18"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "PET with Multiple Masks", "text": "An important limitation of PET is that the verbalizer v must map each output to a single token, which is impossible for many tasks. We thus generalize verbalizers to functions v : Y \u2192 T * ; this requires some modifications to inference and training. 3 We further generalize PET in that we do not assume the output space to be identical for each input: for each x \u2208 X, we denote with Y x \u2286 Y the set of possible outputs given x as input. Given a PVP p = (P, v), we define l(x) = max y\u2208Yx |v(y)| to be the maximum number of tokens required to express any output in Y x and P k (x) to be P (x) with the mask token replaced by k masks. As a running example, we consider the task of binary sentiment classification for restaurant reviews with labels Y = {+1, \u22121}. We use the pattern P (x) = x. It was __ . and a verbalizer v that maps +1 to the single token great and \u22121 to the sequence terri \u2022 ble, i.e., we assume that the MLM's tokenizer splits the word \"terrible\" into the two tokens terri and \u2022 ble. For this example, l(x) = 2 for all x; P 2 (x) is illustrated in Figure 3 (a).\n3 While PET can easily be adapted to generative MLMs (e.g., Lewis et al., 2020;Raffel et al., 2020), we stick with regular MLMs as they are more lightweight and performed better on simple cloze tasks in preliminary experiments.\nInference For x \u2208 X, y \u2208 Y x and |v(y)| = k, we redefine q p (y | x) in an autoregressive fashion: Starting from P k (x), we perform k consecutive predictions, where we always select the next token to predict based on the MLM's confidence. That is, we set q p (y | x) = q(v(y\n) | P k (x)) where q(t 1 ... t k |z) = 1 if k = 0 q j M (t j |z) \u2022 q(t |z ) if k \u2265 1 (3) with j = arg max k i=1 q i M (t i | z), z is z except z j = t j and t = t 1 ... t j\u22121 t j+1 ... t k .\nNote that unlike in original PET (Eq. 1), q p is not a probability distribution as its values do not sum to one. For our sentiment classification example, Fig-\nure 3 illustrates how q p (\u22121 | x) is computed: As |v(y)| = |{terri, \u2022 ble}| = 2, we first use z = P 2 (x)\nto compute the probability of each token in v(y) (Figure 3a). We then choose the token with the highest probability, put it in place of the corresponding mask token, and use the resulting cloze question z to compute the probability of the remaining token (Figure 3b). The overall score for y = \u22121 is then computed as\nq p (\u22121 | x) = q 2 M ( \u2022 ble | z) \u2022 q 1 M (terri | z )\nTraining Computing q p (y | x) as in Eq. 3 for each training example (x, y) would be prohibitively expensive. To enable computation of all required probabilities in a single forward pass, we approximate q p (y | x) by (i) always inserting the maximum number of mask tokens required to express any output and (ii) for each y \u2208 Y x , predicting all tokens in v(y ) = t 1 . . . t k in parallel, where we simply ignore the model's predictions for all l(x) \u2212 k superfluous mask tokens:\nq p (y | x) = k i=1 q i M (t i | P l(x) (x))(4)\nFor our running example, this means we approximate the scores q p (y | x) by computing\nq p (+1 | x) = q 1 M (great | z) q p (\u22121 | x) = q 1 M (terri | z) \u2022 q 2 M ( \u2022 ble | z)\nwhich can be done in a single forward pass as it only requires processing the cloze question z = P 2 (x) shown in Figure 3 (a) once. Asq p is not a probability distribution over Y x , cross entropy is not an ideal training objective as it can also be minimized by reducing the probability assigned to sequences z / \u2208 v(Y x ) that are not part of the output space, despite this having no effect on the model's prediction. We instead opt for multiclass hinge loss (Weston and Watkins, 1999;Dogan et al., 2016) and minimize:\ny \u2208Yx max 0; 1\u2212 logq p (y|x)+ logq p (y |x) (5)\nThat is, we require the difference between the log probability of y and the log probability of any output y \u2208 Y x \\ {y} to be at least 1.", "publication_ref": ["b25", "b46", "b11"], "figure_ref": ["fig_1", "fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "Experiments", "text": "We compare PET and GPT-3 on SuperGLUE , a natural language understanding benchmark consisting of eight challenging tasks. We cannot evaluate PET using the exact same training data as GPT-3 because for most tasks, GPT-3 uses a different set of training examples for each test example and for the other tasks, training sets were not available upon request; however, the exact choice of examples has little impact on GPT-3's performance. 4 We thus create new training sets by randomly selecting 32 examples for each task using a fixed random seed.\nWe additionally create sets of up to 20,000 unlabeled examples for each task; this is done by removing all labels from the original training sets. We refer to the resulting sets of training examples and unlabeled examples as FewGLUE. 5", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Tasks", "text": "Below, we describe each of the SuperGLUE tasks and our corresponding PVPs. We use a vertical bar (|) to mark boundaries between text segments. Of the eight tasks considered, only COPA, WSC and ReCoRD require the use of PET with multiple masks as introduced in Section 3.1.\nBoolQ (Clark et al., 2019) is a QA task where each example consists of a passage p and a yes/no question q. We use the following patterns:\n\u2022 p. Question: q? Answer: __.\n\u2022 p. Based on the previous passage, q? __.\n\u2022 Based on the following passage, q? __. p We define two verbalizers mapping questions containing a true statement to yes/true and others to no/false, respectively, for a total of 6 PVPs. and a verbalizer that maps entailment to yes, disagreement to no and neutral to maybe.\nGiven a premise p, the task in COPA (Gordon et al., 2012) is to determine the cause or effect of the premise given two options c 1 and c 2 . For determining the effect, we use the following patterns:\n\"c 1 \" or \"c 2 \"? p, so __. , c 1 or c 2 ? p, so __.\nFor determining the cause, we use the same patterns but replace so with because. The verbalizer for c 1 and c 2 is the identity function.\nFor WiC (Pilehvar and Camacho-Collados, 2019), given a word w and two sentences s 1 and s 2 in which it occurs, the task is to decide if w is used with the same sense in both sentences. We use:\n\u2022 \"s 1 \" / \"s 2 \". Similar sense of \"w\"? __. For the first two patterns, we use yes as verbalization for words used in the same sense and no for other words; for the third pattern, we use b and 2.\nFor WSC (Levesque et al., 2011), each example consists of a sentence s with a marked pronoun p and noun n, and the task is to determine whether p refers to n. We follow (Raffel et al., 2020;Brown et al., 2020) and treat WSC as a generative task. We highlight p in s by putting it in asterisks and use the following patterns:\n\u2022 s The pronoun ' * p * ' refers to __.\n\u2022 s In the previous sentence, the pronoun ' * p * ' refers to __.\n\u2022 s In the passage above, what does the pronoun ' * p * ' refer to? Answer: __.\nWe use the identity function as verbalizer for n. Note that WSC is different from other tasks in that it requires free-form completion. This in turn requires some modifications during training and inference that are discussed in Appendix A.\nMultiRC (Khashabi et al., 2018) is a QA task. Given a passage p, a question q and an answer candidate a, the task is to decide whether a is a correct answer for q. We use the same verbalizer as for BoolQ and similar patterns:\n\u2022 p. Question: q? Is it a? __.\n\u2022 p. Question: q? Is the correct answer \"a\"? __.\n\u2022 p. Based on the previous passage, q? Is \"a\" a correct answer? __.\nFor ReCoRD (Zhang et al., 2018), given a passage p and a cloze question q, the task is to decide which of a given set of answer candidates is the correct replacement for the placeholder in the cloze question.\nAs this task is already presented in the form of a cloze question, there is little room for designing PVPs, so we only use a trivial one: the concatenation of p and q as pattern and the identity function as verbalizer. With only one PVP, there is no need to perform knowledge distillation so we directly use the resulting model as our final classifier.", "publication_ref": ["b5", "b15", "b21", "b52"], "figure_ref": [], "table_ref": []}, {"heading": "Setup", "text": "As underlying LM for PET we choose ALBERTxxlarge-v2 (Lan et al., 2020), the best-performing MLM on SuperGLUE when training is performed on the regular, full size training sets. We use the same model, supplemented by a sequence classification head, as our final classifier. We run PET on the FewGLUE training sets for all SuperGLUE tasks. We do not use any development set to optimize hyperparameters; instead we use the exact same setup and hyperparameters as Schick and Sch\u00fctze (2021). For COPA, WSC and ReCoRD, we use our proposed modification of PET to support verbalizers mapping labels to multiple tokens; for all other tasks, we use regular PET. We train iPET on all tasks except COPA and WSC, as their unlabeled sets contain well below 1,000 examples, as well as ReCoRD, for which iPET makes no sense as we only use a single PVP. For these three tasks, we simply reuse the results of regular PET.", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Our main results are shown in  of 785. On average, PET performs 18 points better compared to GPT-3 Med, a model of similar size. iPET brings further improvements for 3 out of the 5 tasks that we use iPET for, most notably for CB, but results in a slight performance drop for MultiRC. Despite PET's strong performance, it still clearly performs worse than a state-of-the-art model trained on the regular, full size SuperGLUE training set.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Analysis", "text": "We investigate the importance of several factors for few-shot performance: the choice of patterns and verbalizers, the usage of both unlabeled and labeled data, and properties of the underlying language model. We also look into our proposed modification for PET to work with multiple masks and compare it to various baselines. Finally, we measure how choosing different sets of training examples affects performance. Our analysis focuses on PET as GPT-3 is not publicly available. 6", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Patterns", "text": "The way in which tasks are reformulated as cloze questions can have a huge impact on performance Schick and Sch\u00fctze, 2021). These reformulations can be arbitrarily complex; for example, the pattern used by GPT-3 for WSC contains an introductory section of almost 30 words; it is unclear if and how this formulation has been optimized. 7 To investigate the importance of patterns and verbalizers, we compare three sets of PVPs: our initial set as defined in Section 4.1 (denoted p ours ), the single PVP used by GPT-3 (p GPT-3 ), and the combination of both (p comb ). We train ALBERT using PET with all three sets of patterns; results for selected SuperGLUE tasks are shown in Table 2 (top). As can be seen, the PVP used by GPT-3 outperforms our PVPs on RTE whereas our initial set of patterns performs much better on MultiRC. These large differences in performance highlight the importance of finding good ways to express tasks as cloze questions. As it is difficult to ascertain which patterns perform well without trying them on a large set of examples, a key challenge for few-shot approaches is to compensate for PVPs that the LM fails to understand well. As seen in the performance of the model trained with p comb , PET is able to do so: not only does combining all PVPs compensate for the worse performance of p ours on RTE and of p GPT-3 on MultiRC, it even further improves average performance across the three tasks compared to the best-performing set of patterns. This clearly demonstrates the potential of carefully engineering a set of suitable patterns as opposed to just choosing a single formulation without means of evaluating its effectiveness.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Unlabeled Data Usage", "text": "Unlike GPT-3, PET requires unlabeled data to distill the knowledge of all models based on individual PVPs into a single classifier; for iPET, unlabeled data is additionally used to generate training sets for future generations. The underlying assumption   is that unlabeled data can easily be obtained, which may not always be the case in real-world settings.\nWe thus investigate the importance of unlabeled data for regular PET. To this end, we compare the performance of the final classifier in PET to that of directly using the ensemble of models corresponding to individual PVPs. While using this ensemble entirely removes the need for unlabeled data, the ensemble for k PVPs is larger than the distilled model by a factor of 3 \u2022 k as we follow the default setting of PET and train three models per PVP. However, even for a large number of PVPs the ensemble is smaller than GPT-3 by two orders of magnitude.\nResults without distillation can be seen in Table 2 (bottom). Averaged across the three tasks, the ensemble performs even better than the distilled classifier. This shows that if the goal is only to achieve good performance, then unlabeled data is not necessary; however, it is required to obtain a single, lightweight model as final classifier.\nFigure 4 illustrates the benefit of training multiple generations with iPET. For all tasks except MultiRC, there are substantial improvements from   (Dodge et al., 2020).\nOf course, there are further ways to leverage unlabeled data such as keeping an auxiliary language modeling objective during finetuning (Chronopoulou et al., 2019). While we leave investigating the impact of additionally using such methods to future work, we note that they can easily be applied to PET while there is no straightforward way to combine them with priming.", "publication_ref": ["b4"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Labeled Data Usage", "text": "We next investigate the effect of how labeled data is used, which is one of the key differences between priming and PET. We first compare PET with regular supervised training (i.e., without using any patterns), and with a fully unsupervised model (i.e., an ensemble using all PVPs but no labeled training examples). Given 32 examples, PET clearly outperforms both baselines (Table 3).\nWe next compare PET directly to priming. However, we cannot do so using ALBERT as it is only able to process sequences of up to 512 tokens, which is not enough for a set of 32 examples; we instead use XLNet (Yang et al., 2019) for this comparison. As shown in Table 3, XLNet in general performs worse than ALBERT. More importantly, XLNet with PET performs much better than priming. We were not able to obtain results with priming on MultiRC because the 32 examples in FewGLUE would require more than 10,000 tokens, so processing them with a standard Transformer (Vaswani  2017) is infeasible due to the quadratic complexity of self-attention. This highlights another important issue with priming: It does not scale well to more than a few examples; even GPT-3 is only able to process sequences of up to 2,048 tokens. While there are some Transformer variants that can deal with much longer contexts (e.g., Kitaev et al., 2020;Beltagy et al., 2020), it has yet to be investigated to what extent such models make good use of priming examples over long context spans. We further investigate the effectiveness of priming by looking at results obtained with GPT-3 more closely. To this end, Figure 5 shows the performance difference between priming GPT-3 with 32 examples and priming it with just a single example for each task and model size. 8 As can be seen, priming with 32 examples only slightly improves performance for most tasks and model sizes. For some tasks, adding more examples even leads to worse performance, especially for smaller models. For ReCoRD, even the largest model's performance slightly drops when adding more examples.\nB o o lQ C B A c c C B F 1 C O P A R T E W iC W S C M u lt iR C E M M u lt iR C F 1 a R e C o\nThe bottom row of Figure 5 shows the performance difference between ALBERT trained with PET (without distillation) and a fully unsupervised ALBERT model on all tasks. While results are not directly comparable due to different underlying models and PVPs, PET results in much stronger performance improvements compared to priming and does not worsen results for any task.  ", "publication_ref": ["b49", "b22", "b1"], "figure_ref": ["fig_4", "fig_4"], "table_ref": ["tab_7", "tab_7"]}, {"heading": "Model Type", "text": "We next look into the impact of the underlying LM on PET by comparing ALBERT with RoBERTa large  and GPT-2 medium (Radford et al., 2019). As GPT-2 is a unidirectional model similar to GPT-3, it can only process patterns where the mask token is the very last token. We therefore use p GPT-3 for CB and RTE; for MultiRC, we stick with our original set of patterns as they already fulfill this requirement. We also do not perform distillation and instead report the ensemble's performance as there is no established way of equipping GPT-2 with a sequence classification head.\nResults for training all three LMs with PET in Table 4 show that using ALBERT as underlying LM is crucial for PET's strong performance; exchanging ALBERT with RoBERTa results in an average performance drop of 8 points. However, RoBERTa still clearly outperforms GPT-3 13B, which is larger by two orders of magnitude. Importantly, PET with GPT-2 performs much worse than with the two other models. As anticipated by Brown et al. (2020), a reason for this drop in performance may be that like GPT-3, GPT-2 is unidirectional, making tasks that require comparing two sequences a challenge. However, it is important to note that there are also other substantial differences between GPT-2 and the other two models, most notably the pretraining dataset. Regardless of whether unidirectionality is the reason for GPT-2's bad performance, bidirectionality of the underlying LM is important for PET as it removes the need for the mask token to be at the very end and thus allows for more flexibility in the creation of patterns.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_9"]}, {"heading": "PET with Multiple Masks", "text": "We modified PET to work for outputs that require more than a single token. To investigate the impact of this modification, we look at the three tasks for which this is required: COPA, WSC and ReCoRD. We compare our decoding strategy of predicting to- Table 5: Results on selected tasks for our proposed variant of PET as well as other decoding strategies and for untrained ALBERT kens in order of the probability assigned to them, to which we refer as max-first, with two alternatives: decoding left-to-right (ltr) as is common for many autoregressive language models, and decoding all tokens simultaneously (parallel) as is done during training. Additionally, we compare PET with untrained ALBERT to measure the effectiveness of our proposed training loss.\nResults are shown in Table 5. PET clearly outperforms untrained ALBERT for the three tasks. Not performing distillation hurts performance for COPA, but leads to slight improvements on WSC; for ReCoRD, we did not perform distillation in the first place as we only use a single PVP. Our decoding strategy is clearly superior to parallel decoding except for WSC, for which most predictions consist only of one or two tokens, and performs slightly better than left-to-right decoding.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Training Examples", "text": "Recall that we conduct our experiments with training examples from FewGLUE, a randomly selected subset of the original SuperGLUE training examples. We used a fixed random seed s 0 to generate FewGLUE. Let \u03a3 i be the randomly selected subset of SuperGLUE for random seed s i , so \u03a3 0 = FewGLUE. In this subsection, we create two additional subsets of SuperGLUE, \u03a3 1 and \u03a3 2 , based on different seeds. This allows us to investigate how different sets of training examples affect performance. To this end, we run PET for CB, RTE and MultiRC using the three \u03a3 i . To measure only the effect of varying the training set while ignoring unlabeled examples, we do not use distillation.\nTable 6 shows that for all tasks, changing the set of training examples can result in large performance differences for PET. This highlights the importance of using the same set of examples when comparing different few-shot approaches, which is why we make the particular set of examples in FewGLUE publicly available. However, we note Table 6: Results on selected tasks for GPT-3 and for PET using training sets \u03a3 0 , \u03a3 1 , \u03a3 2 that the average performance of PET is similar to that of GPT-3 for all seeds.\nWhile our results may seem contrary to the insight that for GPT-3, the exact choice of examples does not play a major role, we suspect this to be due to the fact that priming benefits much less from training examples than PET (cf. Section 5.3); accordingly, the influence of the exact set of training examples on the model's performance is smaller.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We have proposed a simple yet effective modification of PET, enabling us to use it for tasks that require predicting multiple tokens. In extensive experiments, we have identified several factors responsible for the strong performance of PET combined with ALBERT: the possibility to concurrently use multiple patterns for transforming examples into cloze questions, the ability to compensate for patterns that are difficult to understand, the usage of labeled data to perform parameter updates, and the underlying LM itself.\nWe have shown that using PET, it is possible to achieve few-shot text classification performance similar to GPT-3 on SuperGLUE with LMs that have three orders of magnitude fewer parameters. This not only lowers financial cost, but above all reduces environmental impact immensely and leads to a much smaller carbon footprint. We see this as an important contribution to achieving the goal of an environmentally more friendly NLP. To enable comparisons with our work, we make our code, models and datasets publicly available.\nFor future work, it would be interesting to see whether PET also works for generative tasks when combined with generative LMs and whether further improvements are possible in multi-task settings.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Training Details", "text": "Our implementation can be found in the supplementary material. It extends the original implementation of PET by Schick and Sch\u00fctze (2021) which, in turn, is based on the Transformers library (Wolf et al., 2020) and PyTorch (Paszke et al., 2017). All dependencies are listed in requirements.txt. Detailed instructions on how our results can be reproduced using this implementation can be found in README.md.\nUnless explicitly stated differently, we use the exact same set of hyperparameters as Schick and Sch\u00fctze (2021) (Table 7) with the only difference that for iPET, we only train 3 generations of models to speed up training. All of our experiments were conducted using a single GPU with 11GB RAM (NVIDIA GeForce GTX 1080 Ti). With this GPU, training a single PET model for 250 steps took approximately 45 minutes. Depending on the task, labeling unlabeled examples took 0.2-1.5 hours per model. Training the final classifier for 5,000 steps on the soft-labeled dataset took 2.5 hours on average. Below, we list task-specific implementation details for all tasks in SuperGLUE.\nCOPA For COPA, we randomly switch the two options c 1 and c 2 during training with a probability of 50% to make the input more diverse; for inference, we always keep the original order. For distilling the final PET model, we obtain logits for unlabeled examples x from individual PVPs p as s p (y | x) = log q p (y | x); we use the input format proposed by .\nWiC Similar to COPA, we randomly switch the input sentences s 1 and s 2 during training. Given a word w and two sentences s 1 and s 2 , we use the sequence w: s 1 | s 2 as input for the final sequence classification model, where | marks the boundary between two text segments.\nWSC Unlike other SuperGLUE tasks, the WSC formulation of Raffel et al. (2020) and Brown et al. (2020) requires free-form completion, meaning that for each sentence s and pronoun p, we only have a single correct choice n that the model needs to predict, but we do not provide any alternatives.\nDuring training, we thus use regular cross entropy loss between n andq p (n | s, p) as defined in Eq. 4. However, in many cases this would allow the LM to easily identify the correct target based on the number of masks provided, so we modify each target by randomly adding up to three additional mask tokens, for which we require the model to predict a special <pad> token. For inference, we always just add a single mask token to ensure consistent results across multiple evaluations and perform greedy decoding as described in Section 3. We then follow Raffel et al. (2020) to map the output produced by the LM to a label y \u2208 {true, false}. For distillation, given an unlabeled example x we set s p (y | x) = 1 if the model's output for x was mapped to y and s p (y | x) = 0 otherwise. We provide inputs to the final PET model in the format s | n where | is the boundary between two text segments and mark p in s with asterisks.\nMultiRC Deviating from the hyperparameters used by Schick and Sch\u00fctze (2021), we use a maximum sequence length of 512 tokens for MultiRC both during training and inference because we found many passages to be much longer than 256 tokens. Input for the final sequence classification model is of the form p | q | a where p is the passage, q is the question, a is the answer candidate and we use | to mark boundaries between text segments.\nReCoRD For ReCoRD, we again use a maximum sequence length of 512 because many passages require more than 256 tokens. For some questions q, the ReCoRD training set contains a huge number of answer candidates. To facilitate training, we split each example into multiple examples as follows: let C be the set of answer candidates", "publication_ref": ["b47", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgments This work was funded by the European Research Council (ERC #740516). We thank the anonymous reviewers for their helpful comments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": " ", "text": "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019\n. Language models are unsupervised multitask learners. Technical report.    8.\nPreprocessing We do not perform any preprocessing, except shortening all examples to the maximum sequence length. This is done using the longest first strategy implemented in the Transformers library. All input sequences are truncated before applying patterns.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Distilling neural networks for greener and faster dependency parsing", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Mark Anderson; Carlos G\u00f3mez-Rodr\u00edguez"}, {"ref_id": "b1", "title": "Longformer: The long-document transformer", "journal": "", "year": "2020", "authors": "Iz Beltagy; Matthew E Peters; Arman Cohan"}, {"ref_id": "b2", "title": "Extracting patterns and relations from the world wide web", "journal": "Springer", "year": "1999", "authors": "Sergey Brin"}, {"ref_id": "b3", "title": "Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners", "journal": "Curran Associates, Inc", "year": "", "authors": "Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel Ziegler; Jeffrey Wu; Clemens Winter; Chris Hesse; Mark Chen; Eric Sigler; Mateusz Litwin"}, {"ref_id": "b4", "title": "An embarrassingly simple approach for transfer learning from pretrained language models", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Alexandra Chronopoulou; Christos Baziotis; Alexandros Potamianos"}, {"ref_id": "b5", "title": "BoolQ: Exploring the surprising difficulty of natural yes/no questions", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Christopher Clark; Kenton Lee; Ming-Wei Chang; Tom Kwiatkowski; Michael Collins; Kristina Toutanova"}, {"ref_id": "b6", "title": "The PASCAL recognising textual entailment challenge", "journal": "Springer", "year": "2006", "authors": "Oren Ido Dagan; Bernardo Glickman;  Magnini"}, {"ref_id": "b7", "title": "Commonsense knowledge mining from pretrained models", "journal": "", "year": "2019", "authors": "Joe Davison; Joshua Feldman; Alexander Rush"}, {"ref_id": "b8", "title": "The CommitmentBank: Investigating projection in naturally occurring discourse", "journal": "", "year": "2019", "authors": "Marie-Catherine De Marneffe; Mandy Simons; Judith Tonhauser"}, {"ref_id": "b9", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b10", "title": "2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping", "journal": "Computing Research Repository", "year": "", "authors": "Jesse Dodge; Gabriel Ilharco; Roy Schwartz; Ali Farhadi; Hannaneh Hajishirzi; Noah Smith"}, {"ref_id": "b11", "title": "A unified view on multi-class support vector classification", "journal": "J. Mach. Learn. Res", "year": "2016", "authors": "\u00dcr\u00fcn Dogan; Tobias Glasmachers; Christian Igel"}, {"ref_id": "b12", "title": "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "Allyson Ettinger"}, {"ref_id": "b13", "title": "Mask-predict: Parallel decoding of conditional masked language models", "journal": "", "year": "2019", "authors": "Marjan Ghazvininejad; Omer Levy; Yinhan Liu; Luke Zettlemoyer"}, {"ref_id": "b14", "title": "Compressing deep convolutional networks using vector quantization", "journal": "", "year": "2014", "authors": "Yunchao Gong; Liu Liu; Ming Yang; Lubomir Bourdev"}, {"ref_id": "b15", "title": "SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning", "journal": "Association for Computational Linguistics", "year": "2012", "authors": "Andrew Gordon; Zornitsa Kozareva; Melissa Roemmele"}, {"ref_id": "b16", "title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "journal": "", "year": "2016", "authors": "Song Han; Huizi Mao; William J Dally"}, {"ref_id": "b17", "title": "Learning both weights and connections for efficient neural network", "journal": "Curran Associates, Inc", "year": "2015", "authors": "Song Han; Jeff Pool; John Tran; William Dally"}, {"ref_id": "b18", "title": "Distilling the knowledge in a neural network", "journal": "", "year": "2015", "authors": "Geoffrey Hinton; Oriol Vinyals; Jeff Dean"}, {"ref_id": "b19", "title": "How can we know what language models know?", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020-06", "authors": "Zhengbao Jiang; Frank F Xu"}, {"ref_id": "b20", "title": "TinyBERT: Distilling BERT for natural language understanding", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Xiaoqi Jiao; Yichun Yin; Lifeng Shang; Xin Jiang; Xiao Chen; Linlin Li; Fang Wang; Qun Liu"}, {"ref_id": "b21", "title": "Looking beyond the surface: A challenge set for reading comprehension over multiple sentences", "journal": "", "year": "2018", "authors": "Daniel Khashabi; Snigdha Chaturvedi; Michael Roth; Shyam Upadhyay; Dan Roth"}, {"ref_id": "b22", "title": "Reformer: The efficient transformer", "journal": "", "year": "2020", "authors": "Nikita Kitaev; Lukasz Kaiser; Anselm Levskaya"}, {"ref_id": "b23", "title": "ALBERT: A lite BERT for self-supervised learning of language representations", "journal": "", "year": "2020", "authors": "Zhenzhong Lan; Mingda Chen; Sebastian Goodman; Kevin Gimpel; Piyush Sharma; Radu Soricut"}, {"ref_id": "b24", "title": "The Winograd schema challenge", "journal": "", "year": "2011", "authors": "J Hector; Ernest Levesque; Leora Davis;  Morgenstern"}, {"ref_id": "b25", "title": "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension", "journal": "", "year": "2020", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal ; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"ref_id": "b26", "title": "FastBERT: a selfdistilling BERT with adaptive inference time", "journal": "", "year": "2020", "authors": "Weijie Liu; Peng Zhou; Zhiruo Wang; Zhe Zhao; Haotang Deng; Qi Ju"}, {"ref_id": "b27", "title": "RoBERTa: A robustly optimized BERT pretraining approach", "journal": "Computing Research Repository", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b28", "title": "LadaBERT: Lightweight adaptation of BERT through hybrid model compression", "journal": "", "year": "2020", "authors": "Yihuan Mao; Yujing Wang; Chufan Wu; Chen Zhang; Yang Wang; Quanlu Zhang; Yaming Yang; Yunhai Tong; Jing Bai"}, {"ref_id": "b29", "title": "Effective self-training for parsing", "journal": "", "year": "2006", "authors": "David Mcclosky; Eugene Charniak; Mark Johnson"}, {"ref_id": "b30", "title": "Argumentative relation classification as plausibility ranking", "journal": "German Society for Com", "year": "2019", "authors": "Juri Opitz"}, {"ref_id": "b31", "title": "Automatic differentiation in PyTorch", "journal": "", "year": "2017", "authors": "Adam Paszke; Sam Gross; Soumith Chintala; Gregory Chanan; Edward Yang; Zachary Devito; Zeming Lin; Alban Desmaison; Luca Antiga; Adam Lerer"}, {"ref_id": "b32", "title": "Language models as knowledge bases?", "journal": "EMNLP-IJCNLP", "year": "2019", "authors": "Fabio Petroni; Tim Rockt\u00e4schel; Sebastian Riedel; Patrick Lewis; Anton Bakhtin; Yuxiang Wu; Alexander Miller"}, {"ref_id": "b33", "title": "WiC: the word-in-context dataset for evaluating context-sensitive meaning representations", "journal": "Long and Short Papers", "year": "2019", "authors": "Mohammad Taher Pilehvar; Jose Camacho-Collados"}, {"ref_id": "b34", "title": "Zero-shot text classification with generative language models", "journal": "Computing Research Repository", "year": "2019", "authors": "Raul Puri; Bryan Catanzaro"}, {"ref_id": "b35", "title": "Improving language understanding by generative pre-training", "journal": "", "year": "2018", "authors": "Alec Radford; Karthik Narasimhan"}, {"ref_id": "b36", "title": "", "journal": "Commun. ACM", "year": "", "authors": ""}, {"ref_id": "b37", "title": "The right tool for the job: Matching model and instance complexities", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Roy Schwartz; Gabriel Stanovsky; Swabha Swayamdipta; Jesse Dodge; Noah A Smith"}, {"ref_id": "b38", "title": "Probability of error of some adaptive pattern-recognition machines", "journal": "IEEE Transactions on Information Theory", "year": "1965", "authors": "H Scudder"}, {"ref_id": "b39", "title": "Training with quantization noise for extreme model compression", "journal": "", "year": "2021", "authors": "Pierre Stock; Angela Fan; Benjamin Graham; Edouard Grave; R\u00e9mi Gribonval; Herve Jegou; Armand Joulin"}, {"ref_id": "b40", "title": "Energy and policy considerations for deep learning in NLP", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Emma Strubell; Ananya Ganesh; Andrew Mccallum"}, {"ref_id": "b41", "title": "2020. oLMpics -on what language model pre-training captures", "journal": "Transactions of the Association for Computational Linguistics", "year": "", "authors": "Alon Talmor; Yanai Elazar; Yoav Goldberg; Jonathan Berant"}, {"ref_id": "b42", "title": "A simple method for commonsense reasoning", "journal": "Computing Research Repository", "year": "2018", "authors": "H Trieu; Quoc V Trinh;  Le"}, {"ref_id": "b43", "title": "Attention is all you need", "journal": "Curran Associates, Inc", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b44", "title": "BERT has a mouth, and it must speak: BERT as a Markov random field language model", "journal": "", "year": "2019", "authors": "Alex Wang; Kyunghyun Cho"}, {"ref_id": "b45", "title": "SuperGLUE: A stickier benchmark for general-purpose language understanding systems", "journal": "Curran Associates, Inc", "year": "2019", "authors": "Alex Wang; Yada Pruksachatkun; Nikita Nangia; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel Bowman"}, {"ref_id": "b46", "title": "Support vector machines for multi-class pattern recognition", "journal": "", "year": "1999", "authors": "Jason Weston; Chris Watkins"}, {"ref_id": "b47", "title": "Transformers: State-of-the-art natural language processing", "journal": "", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; Remi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger;  Drame"}, {"ref_id": "b48", "title": "Early exiting BERT for efficient document ranking", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Ji Xin; Rodrigo Nogueira; Yaoliang Yu; Jimmy Lin"}, {"ref_id": "b49", "title": "Xlnet: Generalized autoregressive pretraining for language understanding", "journal": "Curran Associates, Inc", "year": "2019", "authors": "Zhilin Yang; Zihang Dai; Yiming Yang; Jaime Carbonell; R Russ; Quoc V Salakhutdinov;  Le"}, {"ref_id": "b50", "title": "Unsupervised word sense disambiguation rivaling supervised methods", "journal": "Association for Computational Linguistics", "year": "1995", "authors": "David Yarowsky"}, {"ref_id": "b51", "title": "Q8BERT: Quantized 8bit BERT", "journal": "", "year": "2019", "authors": "Ofir Zafrir; Guy Boudoukh; Peter Izsak; Moshe Wasserblat"}, {"ref_id": "b52", "title": "ReCoRD: Bridging the gap between human and machine commonsense reading comprehension", "journal": "", "year": "2018", "authors": "Sheng Zhang; Xiaodong Liu; Jingjing Liu; Jianfeng Gao; Kevin Duh; Benjamin Van Durme"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Performance on SuperGLUE with 32 training examples.ALBERT with PET/iPET outperforms GPT-3 although it is much \"greener\" in that it has three orders of magnitude fewer parameters.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Inference for a verbalization consisting of the two tokens terri and \u2022 ble. (a) We first compute the probability of each token at its position in the cloze question P 2 (x) and identify the token with the highest probability. (b) We insert this token into the cloze question and compute the probability of the remaining token.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "\u2022 s 1 s 22Does w have the same meaning in both sentences? __ \u2022 w. Sense (1) (a) \"s 1 \" (__) \"s 2 \"", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Average performance (\u00b1 standard deviation) of all MLMs trained on individual patterns for three generations and of the distilled classifier (\"dist.\") across three individual training runs", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure5: Accuracy differences between priming with 32 examples and one-shot priming for all GPT-3 models as well as between ALBERT with PET (without distillation) and unsupervised ALBERT (bottom row)", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "CB(De Marneffe et al., 2019) andRTE (Dagan  et al., 2006)  are textual entailment tasks like MNLI, so we use PVPs similar to Schick and Sch\u00fctze (2021). For a premise p and hypothesis h, we use h? | __, p , \"h\"? | __, \"p\" , h? | __. p , \"h\"? | __. \"p\"", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Results on SuperGLUE for GPT-3 primed with 32 randomly selected examples and for PET / iPET with ALBERT-xxlarge-v2 after training on FewGLUE. State-of-the-art results when using the regular, full size training sets for all tasks (Raffel et al., 2020) are shown in italics.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "/ 59.0 74.7 39.1 / 77.7 68.3 PET (pours) \u00acdist 83.9 / 76.2 66.4 38.9 / 76.2 68.0 PET (pcomb) \u00acdist 83.9 / 76.2 72.9 39.6 / 76.6 70.4", "figure_data": "CBRTE MultiRC AvgModelAcc. / F1 Acc. EM / F1a-PET (pours)85.1 / 59.4 69.8 37.9 / 77.3 66.6PET (pGPT-3)83.3 / 58.1 71.8 25.4 / 68.3 63.1PET (pcomb)84.5"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Results on selected tasks for various sets of PVPs for regular PET and for an ensemble of PET models with no knowledge distillation (\"\u00acdist\")", "figure_data": "90Task Performance70 80 60BoolQ RTECB (Acc) MultiRC (F1a)123dist.iPET Generation"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Results on selected tasks for various ways of using the labeled examples available in FewGLUE", "figure_data": "the first to the second generation, whereas the thirdgeneration achieves only slight additional improve-ments. On average, standard deviation is reducedin later generations, illustrating that the modelslearn from each other and their predictions con-verge. The final distillation step brings further im-provements for all tasks except MultiRC and re-duces standard deviation across three training runsto almost zero, illustrating that PET and iPET areeffective means of reducing finetuning instability"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Results on selected tasks for PET without knowledge distillation combined with various LMs using p GPT-3 for CB/RTE and p ours for MultiRC", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "/ 57.2 72.9 32.5 / 74.8 65.4 PET \u00acdist (\u03a30) 83.9 / 76.2 66.4 38.9 / 76.2 68.0 PET \u00acdist (\u03a31) 82.1 / 57.4 61.4 39.2 / 77.9 63.2 PET \u00acdist (\u03a32) 87.5 / 84.0 61.4 34.7 / 76.3 67.6", "figure_data": "CBRTE MultiRC AvgModelAcc. / F1 Acc. EM / F1a-GPT-382.1"}], "formulas": [{"formula_id": "formula_0", "formula_text": "entailment not_entailment y v(y) q p (y | x)", "formula_coordinates": [2.0, 403.83, 74.42, 98.53, 96.68]}, {"formula_id": "formula_1", "formula_text": "q p (y | x) = exp s p (y | x) y \u2208Y exp s p (y | x)(1)", "formula_coordinates": [3.0, 103.72, 124.76, 185.42, 27.15]}, {"formula_id": "formula_2", "formula_text": "s p (y | x) = s 1 M (v(y) | P (x))", "formula_coordinates": [3.0, 101.24, 163.53, 139.99, 14.18]}, {"formula_id": "formula_3", "formula_text": "q P (y | x) \u221d exp p\u2208P w p \u2022 s p (y | x) (2)", "formula_coordinates": [3.0, 106.37, 412.35, 182.76, 22.28]}, {"formula_id": "formula_4", "formula_text": "P 2 (x) Awful pizza! It was __ __ . x q 1 M (terri | z) < < < q 2 M ( \u2022 ble | z) (a) z = Awful pizza! It was __ \u2022 ble . x q 1 M (terri | z ) (b) z =", "formula_coordinates": [3.0, 312.6, 83.08, 208.99, 115.28]}, {"formula_id": "formula_5", "formula_text": ") | P k (x)) where q(t 1 ... t k |z) = 1 if k = 0 q j M (t j |z) \u2022 q(t |z ) if k \u2265 1 (3) with j = arg max k i=1 q i M (t i | z), z is z except z j = t j and t = t 1 ... t j\u22121 t j+1 ... t k .", "formula_coordinates": [4.0, 70.47, 140.17, 218.66, 97.34]}, {"formula_id": "formula_6", "formula_text": "ure 3 illustrates how q p (\u22121 | x) is computed: As |v(y)| = |{terri, \u2022 ble}| = 2, we first use z = P 2 (x)", "formula_coordinates": [4.0, 70.87, 279.73, 219.54, 23.36]}, {"formula_id": "formula_7", "formula_text": "q p (\u22121 | x) = q 2 M ( \u2022 ble | z) \u2022 q 1 M (terri | z )", "formula_coordinates": [4.0, 90.58, 410.37, 178.85, 14.19]}, {"formula_id": "formula_8", "formula_text": "q p (y | x) = k i=1 q i M (t i | P l(x) (x))(4)", "formula_coordinates": [4.0, 104.8, 583.18, 184.33, 33.71]}, {"formula_id": "formula_9", "formula_text": "q p (+1 | x) = q 1 M (great | z) q p (\u22121 | x) = q 1 M (terri | z) \u2022 q 2 M ( \u2022 ble | z)", "formula_coordinates": [4.0, 91.97, 664.37, 215.96, 31.88]}, {"formula_id": "formula_10", "formula_text": "y \u2208Yx max 0; 1\u2212 logq p (y|x)+ logq p (y |x) (5)", "formula_coordinates": [4.0, 311.74, 168.22, 212.66, 22.4]}, {"formula_id": "formula_11", "formula_text": "B o o lQ C B A c c C B F 1 C O P A R T E W iC W S C M u lt iR C E M M u lt iR C F 1 a R e C o", "formula_coordinates": [8.0, 88.33, 191.38, 129.38, 28.23]}], "doi": "10.18653/v1/2020.iwpt-1.2"}