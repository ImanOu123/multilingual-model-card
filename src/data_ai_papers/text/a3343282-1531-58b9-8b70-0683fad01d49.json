{"title": "Small Data, Big Impact: Leveraging Minimal Data for Effective Machine Translation", "authors": "Jean Maillard; Cynthia Gao; Meta Ai; Elahe Kalbassi; Meta Ai Kaushik; Ram Sadagopan; Vedanuj Goswami; Philipp Koehn; Angela Fan; Francisco Guzm\u00e1n", "pub_date": "", "abstract": "For many languages, machine translation progress is hindered by the lack of reliable training data. Models are trained on whatever preexisting datasets may be available and then augmented with synthetic data, because it is often not economical to pay for the creation of largescale datasets. But for the case of low-resource languages, would the creation of a few thousand professionally translated sentence pairs give any benefit? In this paper, we show that it does. We describe a broad data collection effort involving around 6k professionally translated sentence pairs for each of 39 low-resource languages, which we make publicly available. We analyse the gains of models trained on this small but high-quality data, showing that it has significant impact even when larger but lower quality pre-existing corpora are used, or when data is augmented with millions of sentences through backtranslation.", "sections": [{"heading": "Introduction", "text": "State of the art machine translation models are able to cover hundreds of languages (Ma et al., 2021;Wang et al., 2022;NLLB Team et al., 2022) by relying on large amounts of annotated (Skadin , \u0161 et al., 2014;Lison and Tiedemann, 2016;Agi\u0107 and Vuli\u0107, 2019) and unannotated web crawled data (Schwenk et al., 2021;Heffernan et al., 2022). Translation for low-resource languages still faces significant challenges related to data availability, since many of these languages have neither large-scale parallel corpora nor a big presence on the web (Adelani et al., 2022b).\nTechniques such as self-supervised learning (Ma et al., 2021;Liu et al., 2021) and backtranslation (Sennrich et al., 2016;Edunov et al., 2018;Fan et al., 2020) can be effective tools to reduce the reliance on annotation for translation models. In some cases, these techniques can be combined or even be applied iteratively (Hoang et al., 2018), leading to a feedback loop that can generate increasingly better translations. In order to be effective, however, such methods still require a certain amount of seed parallel data, which can be used to kickstart the process.\nAs a result, researchers and communities looking to train translation systems for low-resource languages may find themselves wondering how much parallel data is required to achieve a given performance target level.\nIn this paper, we describe a data collection effort for 39 low-resource languages, involving the creation of over 6k seed sentence pairs per language by professional translators, which we make publicly available with an open license. We analyse the behaviour of bilingual translation systems trained on varying amounts of this data, with and without the addition of pre-existing publicly available parallel datasets, and find that even comparatively small amounts of professionally produced parallel sentences can have an outsized impact. We find that gains coming from high quality data are further enhanced when training multilingual models of closely related high-and low-resource languages, and even more so when augmenting the dataset via backtranslation.\nOverall, our results show that employing relatively small but high-quality, professionally translated datasets constitutes a promising and viable way towards achieving performant machine translation for low-resource languages, especially for those with high-resource relatives. This holds true even for languages for which some pre-existing data might already be publicly available, further highlighting the importance of high-quality training datasets.\nNotably, parallel datasets of the scale discussed here are compact enough that coverage for a new language could plausibly be collected by a relatively small group of volunteers in a week, making these results relevant for the usage of machine translation technologies in crisis situations (Lewis et al., 2011).\nOur main contributions are:\n1. The creation and public release of a professionally translated seed dataset for 39 lowresource languages. 2. An analysis of the impact of this high-quality data, both in isolation and also when combined with pre-existing datasets, based on hundreds of trained models.\n3. A study of how gains from high-quality parallel data compound when using multilingual training and backtranslation, showing that benefits from high-quality data do not get washed away when using stronger models or data augmentation.", "publication_ref": ["b26", "b10", "b39", "b24", "b4", "b34", "b18", "b26", "b25", "b35", "b12", "b13", "b19", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Background", "text": "Low-resource language translation Despite very successful recent advances in neural machine translation, most of the gains have only benefited a handful of so called high-resource languages, which have enough textual resources to satisfy the substantial data requirements of state-of-theart techniques. The vast majority of the world's languages are low-resource, and researchers have increasingly been focusing on evaluating performance in this challenging setting (Wenzek et al., 2021).\nBenchmarks Traditionally, one of the biggest challenges to the development of low-resource translation systems has been the lack of high quality evaluation data. Several benchmarks focus on specific sets of languages, such as the MADAR dataset for Arabic dialects (Bouamor et al., 2018), the Autshumato benchmark covering 11 South African languages (McKellar, 2017), or the TICO-19 benchmark covering 35 languages for the domain of medical information related to the COVID-19 pandemic (Anastasopoulos et al., 2020). More recently, the FLORES-101 dataset (Goyal et al., 2022) and its expansion to over 200 languages (NLLB Team et al., 2022) has enabled multilingual evaluation across tens of thousands of directions, including many low-resource languages. Its domain is composed of an even mixture of travel guides (Wikitravel), children's literature (Wikijunior), and news content (Wikinews).\nTraining corpora Much important work has gone towards the development of parallel corpora for low-resource languages, most of which focusing on individual language pairs (Tapo et al., 2021;Ali et al., 2021;Adelani et al., 2021;Azunre et al., 2021, inter alia). Adelani et al. (2022a) study the case of 15 low-resource African languages, most of which already have tens or hundreds of thousands of parallel sentences in the religious domain, and investigate how combining pre-trained models and a newly created corpus can lead to effective domain transfer.\nLow-resource training Amongst the techniques that can be used to decrease the reliance on manually annotated data, bitext mining (Schwenk et al., 2021;Ramesh et al., 2022) enables finding pairs of translations among large collections of unannotated monolingual text. Heffernan et al. (2022) show its effectiveness for low-resource languages, but point out that it can be limited for the most data scarce languages. Backtranslation (Sennrich et al., 2016;Edunov et al., 2018) can be used to create pseudoparallel data from monolingual data in a target language. It relies on an initial, potentially low-quality translation model -thereby having some requirements on annotated data -and can also be applied iteratively for improved performance (Hoang et al., 2018). Self-supervision (Siddhant et al., 2020) is a method employing monolingual text denoising as a joint training objective, and its use has been suggested as a way of kick-starting an iterative backtranslation pipeline. Finally, multilingual translation, which is often combined with one or more of the above techniques, has been shown to improve low-resource translation performance via cross-lingual transfer (Firat et al., 2016;Fan et al., 2020;Ma et al., 2021;Wang et al., 2022;NLLB Team et al., 2022) Training without parallel data Within the area of low-resource translation,  describe the development of translation systems for low-resource languages without using any parallel data at all, relying instead on crawled monolingual data and language transfer. Methods which don't require parallel data are likely complementary to the seed data approach proposed in this paper. However, the over-reliance on cross-lingual transfer from a high-resource language opens up the risk of a translation system flattening the differences between related languages, as observed by NLLB Team et al. (2022) for Arabic dialects. This is a particularly thorny issue for communities of speakers of endangered languages which are at risk of being displaced by a related higher-resource language -as is the case for several of the languages covered in this paper. In such cases, we recommend the seed data approach, which opens the door for the communities to take ownership in preserving their languages, and aligns well with their desire to preserve the distinctiveness of their language in technological applications.\nCrisis MT Low-resource machine translation has been studied in the context of crisis events, and has been proposed as a component of a rapid response infrastructure (Lewis et al., 2011). In particular, Lewis (2010) describe the creation of a system for Haitian Creole after the devastating 2010 earthquake, and Anastasopoulos et al. (2020) built a dataset to facilitate access to information related to the COVID-19 pandemic.", "publication_ref": ["b34", "b11", "b27", "b16", "b40", "b5", "b34", "b18", "b35", "b12", "b19", "b37", "b14", "b13", "b26", "b10", "b23", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Data collection", "text": "Regardless of the many modelling improvements aimed at reducing the amount of required supervision, it is likely impossible for translation models to reach acceptable levels of quality without even small amounts of parallel data. This is especially true for approaches that explicitly rely on the preexistence of parallel corpora, such as backtranslation. As a result, low-resource languages with corpora that are too small to enable the use of these techniques are cut off from the improvements they bring. With this in mind, we set up a data collection effort for a number of low-resource languages which fit this criterion, resulting in a dataset of around six thousand English sentences translated into each of 39 low-resource languages.\nLanguage selection In order to choose which languages to collect data for, we took several factors into account. First, we looked at the list of languages supported by Wikipedia. The usergenerated encyclopedia is one of the most visited websites in the world, and constitutes an important means of knowledge dissemination for many low-resource language communities. Crucially, Wikipedia has an open process towards support-ing new languages, 2 which has led to the platform supporting over 300 languages in 2022. 3 This list of languages was cross-referenced with those currently supported by machine translation benchmarks, including the large FLORES-200 dataset.\n4\nWe then focussed our attention to those languages for which not enough high quality data was currently publicly available for large-scale training, looking in particular at those languages with fewer than 100,000 parallel training sentences and prioritising those with the least amount of high quality data (as determined by automatic metrics such as language identification). Finally, we partnered with linguists and identified those languages for which professional translators would be available.\nSource sentence selection The dataset consists of English sentences translated into a number of low-resource languages. The source data was sampled from Wikimedia's List of articles every Wikipedia should have, 5 a collection of 10,000\nWikidata IDs corresponding to notable topics in different fields of knowledge and human activity. These are split into 11 categories such as People, History, Philosophy and Religion, Geography. We uniformly sampled a subset of IDs from which we would draw data, and mapped these to the corresponding English Wikipedia articles. From each of these articles we then sampled triplets of contiguous sentences, such that some amount of context would be provided, and ensured a maximum of one triplet would be sampled per article to guarantee a relatively uniform coverage of topics.\nFinding translators The parallel dataset was created through human translation. We identified translators through various specialised language service providers. Through a vetting process, we selected translators that were native speakers in the target language, with a minimum of two years of professional experience and a degree in a relevant field of studies, such as translation or linguistics. All translators were additionally required to have a high level of English fluency, and had to pass an initial test to assess their translation proficiency.\nTranslation workflow Translators were provided with a clear set of instructions for the project, which can be seen in Appendix B. In addition to these general instructions, in order to avoid issues of mismatching script, spelling system or dialect with the available evaluation benchmarks, we established a set of linguistic guidelines to match the data that was collected for the FLORES-200 dataset. Translators referenced these guidelines while working on the creation of the dataset. The source sentences were translated directly from English for most languages. The only exceptions were Acehnese and Banjar in the Arabic script and Tamasheq in the Tifinagh script, which were transliterated from their respective Latin script datasets, that had in turn first been translated from English. Following this process we conducted a linguistic quality assessment phase in which all translations were checked for conformance with the linguistic guidelines, and automatic quality control checks were performed.\nCompensation range The hourly compensation for translators averaged 25.80 US dollars, with a median of 25.60. The productivity rate generally ranged between 200-250 words per hour, with the exception of the Acehnese and Banjar transcriptions into Arabic which required less effort. Transcription of Tamashek into Tifinagh proved to be more difficult, and had a productivity rate close to that of translation. The full costs for the project also included quality assurance as well as other various expenses incurred by the language providers we partnered with.\nFinal dataset The final dataset size was chosen in order to obtain at least 6,000 parallel sentences per direction, while simultaneously maximising language coverage. Given the available budget, this resulted in a final dataset of 6,193 sentences translated into 39 languages, including three transcribed directions. The dataset is released under the open CC-BY-SA 4.0 license. A full list of the languages can be found in Appendix A.\n4 Experimental Setup", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "Data", "text": "Bilingual models Our first set of experiment focuses on bilingual machine translation, both into and out of English. Beyond our newly developed seed corpus described in Section 3, we sourced additional pre-existing parallel sentences with En-glish through the OpenSubtitles corpus (Lison and Tiedemann, 2016), the QCRI educational domain corpus (Abdelali et al., 2014), the PMIndia corpus (Haddow and Kirefu, 2020), the MultiIndicMT corpus (Nakazawa et al., 2021) as well as the GlobalVoices, Gnome, KDE, Sardware, Tatoeba, Ubuntu and Wikimedia corpora available through the OPUS repository (Tiedemann, 2012). The parallel sentences were obtained through the mtdata tool (Gowda et al., 2021). All models are evaluated on the devtest split of the FLORES-200 benchmark.\nMultilingual", "publication_ref": ["b24", "b0", "b17", "b28", "b41", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Bilingual experiments", "text": "For the bilingual experiments, we divide our 39 focus languages into two broad groups. The larger group, which we call unresourced languages, consists of the 27 languages for which we could find little (< 1 k) or no pre-existing parallel data available through public sources. The second group, which we call barely-resourced languages, consists of those languages that had at least one thousand pre-existing publicly available parallel sentencesthese are listed in Table 1.\nIn order to study the data scaling properties, we randomly partition each seed dataset into three chunks: one consisting of 1k seed parallel sentences, one consisting of 2k, and the final one consisting of the remaining 3k sentences.\nFor each unresourced language, we consider two directions, into and out of English. For each direction, we train three models: on the first, the first two, and all three chunks of the seed data (training corpus sizes of 1k, 3k and 6k sentences respectively). This results in 162 models overall.\nFor the barely-resourced languages, we take the same basic approach, but always include the preexisting publicly available data. In addition, we also train models using the whole seed dataset only, and the publicly available data only. This results in 120 models.\nAll bilingual models use a transformer architecture (Vaswani et al., 2017) with 6 encoder layers and 6 decoder layers, 8 attention heads, 512dimensional embeddings, 0.3 dropout, an effective batch size of 130k tokens, and are trained with an inverse square root learning rate schedule with warmup. Data for each model is tokenised with a language pair specific sentencepiece model (Kudo and Richardson, 2018). Training is conducted with fairseq (Ott et al., 2019), with each model being trained on a machine with 8 NVIDIA Tesla V100 Volta 32GB GPUs for at most 12 hours.  ", "publication_ref": ["b42", "b21", "b30"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Multilingual Experiments", "text": "Low-resource languages have been shown to significantly benefit from multilingual transfer (Arivazhagan et al., 2019;NLLB Team et al., 2022), so it is reasonable to expect that any attempts at boosting low-resource transla-tion performance would also involve multilingual training. In order to evaluate the data scaling and language transfer properties in this useful setting, we design an additional set of experiments focusing on two groups of languages.\n\u2022 We train an Italic model on the low-resource Friulian, Ligurian, Lombard, Sicilian, Sardinian and Venetian, combined with the related high-resource Catalan, Italian and Spanish, plus English.\n\u2022 We train an Indo-Aryan model on the lowresource Bhojpuri, Chhattisgarhi, Kashmiri (Devanagari script) and Magahi, combined with the related high-resource Hindi and Bengali, plus English.\nEach model is trained on all available parallel data between any of its languages. We further conduct an ablation experiment for each model, by removing all seed data and training on the publicly available data only. The training setup is analogous to that of the bilingual experiments, but the architecture is scaled up to 12 layers and 8 attention heads for both encoder and decoder, 1024dimensional embeddings, 0.1 dropout, and an effective batch size of 524k tokens. Multilingual models are trained on four machines, each with 8 NVIDIA Tesla V100 Volta 32GB GPUs, for a maximum of 48 hours.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Backtranslation", "text": "Our final set of experiments involves generating backtranslation data with the multilingual models, and training new multilingual models with this additional data. As discussed in Section 2, this technique can be particularly effective for improving low-resource translation performance. The unlabelled monolingual data it relies upon is more easily obtainable than parallel sentences (Heffernan et al., 2022), making this technique particularly important to boost performance for particularly data scarce settings. We run this experiment both using pre-existing data only, as well as with the addition of all seed data.\nDespite monolingual data taking centre stage in backtranslation, the technique still depends on the existence of a seed translation model to augment the unannotated sentences with synthetic translations. We experiment with generating backtranslation data for the two multilingual models of Section 4.3, using both the full and ablated models.\nFor the Italic model, we provide backtranslations from the six low-resource languages into both eng_Latn and ita_Latn, and vice versa. For the Indo-Aryan model, we provide backtranslations from the four low-resource languages into both eng_Latn and hin_Deva, and vice versa.", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "Results and Analysis", "text": "We report all results using automatic evaluation metrics against the FLORES-200 benchmark. We rely on the chrF++ score (Popovi\u0107, 2017), which is based on character-level n-gram overlap, and is complemented by unigram and bigram features. This score overcomes the limitations inherent to the more commonly used BLEU metric (Papineni et al., 2002), which relies on the availability of tokenisation tools for all languages and fails to accurately account for highly agglutinative languages.", "publication_ref": ["b32", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Bilingual Experiments", "text": "A summary of bilingual translation performance on the unresourced languages in reported in Figures 1a  and 1b. At the lowest training data level, consisting of 1k sentences, we obtain an average chrF++ score of 12.6 eng-xxx and 13.9 xxx-eng. Moving to the 3k-sized corpus, the average increases to 19.9 eng-xxx and 20.6 xxx-eng. Training on the full seed corpus, this further increases to 22.9 eng-xxx and 23.7 xxx-eng. On the whole, models perform at a similar level on the two translation directions, with a slightly larger spread on the eng-xxx direction.\nResults on languages that already had some amount of parallel data publicly available -which we call barely-resourced -are reported separately, in Figures 1c and 1d. We find that, even though these languages already have pre-existing training data (accounting for 12k sentences per language, on average) the addition of a mere 1k parallel sentences from our high-quality dataset brings the average performance up from 12.9 to 19.0 chrF++ in the eng-xxx direction, and from 16.0 to 20.9 chrF++ in the xxx-eng direction. Notably, we see that training without the publicly available data has little effect. Indeed, the removal of all public data accounts for a mere average chrF++ drop of 0.7 eng-xxx and 1.1 xxx-eng , underlining the fundamental role that high quality annotated data can play in improving performance for data-scarce languages.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "Multilingual Experiments", "text": "Results for the multilingual experiments on the Italic and Indo-Aryan language clusters are reported in Table 2.\nFor the xxx-eng directions, which target highresource English, we see that gains from multilingual training are substantial, averaging 25.6 chrF++ for the Italic model and 20.2 chrF++ for the Indo-Aryan model when compared to their respective bilingual versions (Appendix D). The multilingual model sees a lot more English data as target, and performs better on it. Gains are still sizable but relatively smaller for the eng-xxx directions, into low-resource languages. In this case, the average performance difference is of 13.6 and 16.2 chrF++ for the Italic and Indo-Aryan models, respectively.\nFor a comparison of the effects of seed data collection, column \u2206 in Table 2 measures the performance difference of the P+6k and P multilingual models. For the eng-xxx direction the average difference is 14.0 and 12.9 chrF++ for the Italic and Indo-Aryan models respectively; in the reverse directions, the difference is 14.6 and 9.8. This confirms that the beneficial effects of cross-lingual transfer do not compensate for the gains achieved by higher quality data.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Backtranslation", "text": "Performance for the two multilingual models keeps steadily improving when adding backtranslation. By looking at column \u2206 noBT of Table 3, which compares multilingual models with and without backtranslated data, we see that all models trained with backtranslated data outperform their base counterparts for every single direction. Gains from backtranslation are generally more pronounced for the P models, which are trained without seed data. Overall, the same trend as in previous experiments holds true: as revealed by column \u2206, which compares the P+6k and P backtranslation-augmented models, the models trained with seed data achieve the best performance for every direction.  Table 2: Performance of the Italic and Indo-Aryan multilingual models (chrF++) when trained on pre-existing data only (P) and both pre-existing and seed data (P+6k). \u2206 measures the impact of adding seed data to multilingual models, measured as the difference between the P+6k and P multilingual models.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Analysis", "text": "The same trends hold throughout our experiments: even with modelling improvements that aim to reduce the amount of required supervision, such as multilingual training and backtranslation, we observe that models trained on as little as 6k high-quality seed parallel sentences always come out ahead. This is true even for languages such as mag_Deva and hne_Deva, for which tens of thousands of pre-existing parallel sentences are publicly available.\nCrucially, we see that the multilingual model with seed data (\"Multilingual, P+6k\" in the graph) outperforms in all but one case the version without seed data but with backtranslation (\"Multilin-gual+BT, P\"). In other words, even adding vast amounts of monolingual data (as much as 2M sentences for xxx-eng) cannot make up the difference that 6k high-quality parallel sentences make.  \u2206 noBT denotes the performance difference between each model and its base multilingual version trained without backtranslation; \u2206 denotes the effect of adding seed data to multilingual models using backtranslation, measured as the difference between the P+6k and P backtranslation models. #BT denotes the size of the backtranslation corpus for each direction, measured in millions of sentences. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions", "text": "In this paper, we have described a parallel data collection effort involving 6k seed parallel sentences for 39 languages, and investigated the effects of this relatively small but high-quality dataset on machine translation performance. By training hundreds of bilingual translation models, we have looked at the data scaling properties, and found that even when several thousand pre-existing sentences are already available, adding as little as a thousand high-quality parallel sentences can significantly boost performance.\nTo answer the question of whether stronger models can compensate for the lack of high-quality data, we moved beyond simple bilingual models and introduced two modelling improvements: multilingual training of closely related low-and highresource languages, and backtranslation. We found that models trained with the additional high-quality data performed consistently better. Even when augmenting the models with vast amounts of monolingual data via backtranslation, the beneficial effects of seed data were still present.\nOverall, the results show that collecting highquality parallel data, produced by native speakers and manually aligned, is a fundamentally important investment for training machine translation models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Other ways of reducing the amount of required supervision could be attempted, but we do not expect that these would change the outcomes significantly. Self-supervised learning via masking / denoising objectives, either in the form of an auxiliary task or via the use of pretrained models, is one such approach. This however generally underperforms backtranslation, which can utilise the same monolingual data to more effect (NLLB Team et al., 2022), as we see in the experiments of Appendix E. Iterative backtranslation might offer an additional boost for data-scarce settings, but is very computationally intensive, complex, and any gains would almost certainly apply to models trained with the addition of seed data too.\nThe seed datasets that we release bring about large translation performance gains for a number of low-resource languages. We note that, due to budgetary and complexity constraints, the source data we used was sourced from English Wikipedia only. This is likely to have two effects. First, translating English-original data leads to so-called translationese effects one the low-resource side (Volansky et al., 2015), leading to decreased effectiveness for directions that target low-resource languages. Second, the data is unlikely to adequately cover diverse content from multiple cultures. An interesting avenue for future research would therefore involve studying the effects of seed parallel data that is originally translated from low-resource languages.", "publication_ref": ["b43"], "figure_ref": [], "table_ref": []}, {"heading": "Provide fluent translations without deviating", "text": "too much from the source structure. Only allow necessary changes.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Do not expand or replace information com-", "text": "pared to what is present in the source documents. Do not add any explanatory or parenthetical information, definitions, etc.\n6. Do not ignore any meaningful text that was present in the source.\n7. In case of multiple possible translations, please pick the one that makes the most sense (e.g., for gender concordance, cultural fit in the target language, level of formality, etc.).\n8. Translations must be faithful to the source in terms of pragmatics such as (if applicable) level of hedging/modality, sentiment and its intensity, negation, speech effects (disfluencies), etc.\n9. For proper nouns and common abbreviations, please see the guidelines on Named Entities below.\n10. Idiomatic expressions should not be translated word for word. Use an equivalent idiom, if one exists. If no equivalent idiom exists, use an idiom of similar meaning. If no similar expressions exist in the target language, paraphrase the idiom such that the meaning is retained in the target language.\n11. When a pronoun to be translated is ambiguous (for instance, when it could be interpreted as either him/her or he/she), opt for gender neutral pronouns (such as them/they) if those exist in the target language. However, when a pronoun to be translated is clearly marked for gender, you should follow the source material and continue to mark for gender.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Machine translation", "text": "The translations you will provide are going to be used to train new Machine Translation engines. For this reason, the translations you provide should not be biased by existing Machine Translation providers. Therefore:\n1. Translators should not reference any Machine Translation engine at all when translating, to avoid being biased by it.\nhandle Named Entities. Please review the following guidelines carefully:\n1. If there is a commonly used term in the target language for the Named Entity:\n(a) If the most commonly used term is the same as in the source language, then keep it as it is. (b) If the most commonly used term is a translation or a transliteration, then use that.\n2. If there is no commonly used term:\n(a) If possible, a transliteration of the original term should be used. (b) If a transliteration would not be commonly understood in the context, and the source term would be more acceptable, you may retain it.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Experimental details", "text": "We compute ChrF++ scores using the sacrebleu implementation, 7 with the following signature: chrF2++|nrefs:1|case: mixed|eff:yes|nc:6|nw:2|space:no| version:2.1.0.\nTraining is conducted via the fairseq framework; example training configurations for both bilingual and multilingual models are made available.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Performance of bilingual models", "text": "The full results of bilingual translation experiments for unresourced and barely-resourced languages is reported in Tables 5 and 6 respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E Self-supervised learning", "text": "In order to evaluate the effectiveness of selfsupervised learning on monolingual data (SSL), we conduct a series of experiments with our two multilingual models of Sections 4.3 and 4.4.\nThe setup of these experiments follows the denoising autoencoder technique of Liu et al. (2021). One possible approach would be to pre-train on a denoising task, and subsequently fine-tune on B2. Did you discuss the license or terms for use and / or distribution of any artifacts? section 3 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Not applicable. All datasets used were intended for machine translation B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. Data collected from Wikipedia B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? section 3 -demographic information not available due to privacy regulations B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. For the created dataset: section 3 and Appendix A .\nC Did you run computational experiments? sections 4, 5 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? section 4 (we report the size of the model in terms of layers, embedding size, etc.) C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? hyperparameter details in suppl. material C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? we report both descriptive statistics and exact per-model performance C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? section 3 and appendix D Did you use human annotators (e.g., crowdworkers) or research with human participants? section 3 D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? this information is proprietary to the language service providers we relied upon D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? section 3 D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? data comes from wikipedia D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? Not applicable. Left blank.\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? such information was not available to us due to privacy regulations", "publication_ref": ["b25"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "2. All translations will be inspected, and those that are found to be too close to Machine Translation output will be returned to the translator. These will need to be revised, or the translator will be required to provide a quick explanation as to why the translation cannot be modified further without affecting its meaning.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Named entities", "text": "Named Entities are people, places, organisations, etc., that are commonly referred to using a proper noun. This section provides guidance on how to   ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "The AMARA corpus: Building parallel language resources for the educational domain", "journal": "", "year": "2014", "authors": "Ahmed Abdelali; Francisco Guzman; Hassan Sajjad; Stephan Vogel"}, {"ref_id": "b1", "title": "Ayodele Awokoya, Happy Buzaaba, Blessing Sibanda, Andiswa Bukula, and Sam Manthalu. 2022a. A few thousand translations go a long way! leveraging pre-trained models for African news translation", "journal": "Association for Computational Linguistics", "year": "", "authors": "David Adelani; Jesujoba Alabi; Angela Fan; Julia Kreutzer; Xiaoyu Shen; Machel Reid; Dana Ruiter; Dietrich Klakow; Peter Nabende; Ernie Chang; Tajuddeen Gwadabe; Freshia Sackey; F P Bonaventure; Chris Dossou; Colin Emezue; Michael Leong; Shamsuddeen Beukman; Guyo Muhammad; Oreen Jarso; Andre Niyongabo Yousuf; Gilles Rubungo; Eric Peter Hacheme; Muhammad Umair Wairagala; Benjamin Nasir; Tunde Ajibade; Yvonne Ajayi; Jade Gitau; Mohamed Abbott; Millicent Ahmed; Anuoluwapo Ochieng; Perez Aremu; Jonathan Ogayo;  Mukiibi; Godson Fatoumata Ouoba Kabore; Derguene Kalipe;  Mbaye"}, {"ref_id": "b2", "title": "Ayodele Esther Awokoya, and Cristina Espa\u00f1a-Bonet. 2021. The effect of domain and diacritics in Yoruba-English neural machine translation", "journal": "", "year": "", "authors": "David Adelani; Dana Ruiter; Jesujoba Alabi; Damilola Adebonojo; Adesina Ayeni; Mofe Adeyemi"}, {"ref_id": "b3", "title": "", "journal": "", "year": "", "authors": "Jesujoba David Ifeoluwa Adelani; Angela Oluwadara Alabi; Julia Fan; Xiaoyu Kreutzer; Machel Shen; Dana Reid; Dietrich Ruiter; Peter Klakow; Ernie Nabende; Tajuddeen Chang; Freshia Gwadabe;  Sackey; F P Bonaventure; Chris Chinenye Dossou; Colin Emezue; Michael Leong;  Beukman; Guyo Shamsuddeen Hassan Muhammad; Oreen Dub Jarso; Andre Niyongabo Yousuf; Gilles Rubungo;  Hacheme"}, {"ref_id": "b4", "title": "JW300: A widecoverage parallel corpus for low-resource languages", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "\u017deljko Agi\u0107; Ivan Vuli\u0107"}, {"ref_id": "b5", "title": "Towards a parallel corpus of portuguese and the bantu language emakhuwa of mozambique", "journal": "", "year": "2021", "authors": "D M A Felermino; Andrew Ali; Jaimito L A Caines;  Malavi"}, {"ref_id": "b6", "title": "Alp \u00d6ktem, Eric Paquin, Grace Tang, and Sylwia Tur. 2020. TICO-19: the translation initiative for COvid-19", "journal": "", "year": "", "authors": "Antonios Anastasopoulos; Alessandro Cattelan; Zi-Yi Dou; Marcello Federico; Christian Federmann; Dmitriy Genzel; Franscisco Guzm\u00e1n; Junjie Hu; Macduff Hughes; Philipp Koehn; Rosie Lazar; Will Lewis; Graham Neubig; Mengmeng Niu"}, {"ref_id": "b7", "title": "EMNLP 2020", "journal": "", "year": "", "authors": ""}, {"ref_id": "b8", "title": "Massively multilingual neural machine translation in the wild: Findings and challenges", "journal": "", "year": "2019", "authors": "Naveen Arivazhagan; Ankur Bapna; Orhan Firat; Dmitry Lepikhin; Melvin Johnson; Maxim Krikun; Mia Xu Chen; Yuan Cao; George Foster; Colin Cherry; Wolfgang Macherey; Zhifeng Chen; Yonghui Wu"}, {"ref_id": "b9", "title": "", "journal": "Anokye Acheampong Amponsah", "year": "", "authors": "Paul Azunre; Salomey Osei; Salomey Addo; Lawrence Asamoah Adu-Gyamfi; Stephen Moore; Bernard Adabankah; Bernard Opoku; Clara Asare-Nyarko; Samuel Nyarko; Cynthia Amoaba; Esther Dansoa Appiah; Felix Akwerh; Richard Nii Lante Lawson; Joel Budu; Emmanuel Debrah"}, {"ref_id": "b10", "title": "", "journal": "", "year": "2022", "authors": "Ankur Bapna; Isaac Caswell; Julia Kreutzer; Orhan Firat; Aditya Daan Van Esch; Mengmeng Siddhant; Pallavi Niu; Xavier Baljekar; Wolfgang Garcia; Theresa Macherey; Vera Breiner; Jason Axelrod; Yuan Riesa; Mia Cao; Klaus Xu Chen; Maxim Macherey; Pidong Krikun; Alexander Wang; Apurva Gutkin; Yanping Shah; Zhifeng Huang; Yonghui Chen; Macduff Wu;  Hughes"}, {"ref_id": "b11", "title": "The MADAR Arabic dialect corpus and lexicon", "journal": "", "year": "2018", "authors": "Houda Bouamor; Nizar Habash; Mohammad Salameh; Wajdi Zaghouani; Owen Rambow; Dana Abdulrahim; Ossama Obeid; Salam Khalifa; Fadhl Eryani; Alexander Erdmann; Kemal Oflazer"}, {"ref_id": "b12", "title": "Understanding back-translation at scale", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Sergey Edunov; Myle Ott; Michael Auli; David Grangier"}, {"ref_id": "b13", "title": "Beyond english-centric multilingual machine translation", "journal": "The Journal of Machine Learning Research", "year": "2020", "authors": "Angela Fan; Shruti Bhosale; Holger Schwenk; Zhiyi Ma; Ahmed El-Kishky; Siddharth Goyal; Mandeep Baines; Onur Celebi; Guillaume Wenzek; Vishrav Chaudhary; Naman Goyal; Tom Birch; Vitaliy Liptchinsky; Sergey Edunov; Edouard Grave; Michael Auli; Armand Joulin"}, {"ref_id": "b14", "title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Orhan Firat; Kyunghyun Cho; Yoshua Bengio"}, {"ref_id": "b15", "title": "Many-to-English machine translation tools, data, and pretrained models", "journal": "", "year": "2021-05", "authors": "Thamme Gowda; Zhao Zhang; Chris Mattmann; Jonathan "}, {"ref_id": "b16", "title": "The Flores-101 evaluation benchmark for low-resource and multilingual machine translation", "journal": "Transactions of the Association for Computational Linguistics", "year": "2022", "authors": "Naman Goyal; Cynthia Gao; Vishrav Chaudhary; Peng-Jen Chen; Guillaume Wenzek; Da Ju; Sanjana Krishnan; Marc'aurelio Ranzato; Francisco Guzm\u00e1n; Angela Fan"}, {"ref_id": "b17", "title": "Pmindia -a collection of parallel corpora of languages of india", "journal": "", "year": "2020", "authors": "Barry Haddow; Faheem Kirefu"}, {"ref_id": "b18", "title": "Bitext mining using distilled sentence representations for low-resource languages", "journal": "", "year": "2022", "authors": "Kevin Heffernan; Onur \u00c7elebi; Holger Schwenk"}, {"ref_id": "b19", "title": "Iterative backtranslation for neural machine translation", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Duy Vu Cong; Philipp Hoang; Gholamreza Koehn; Trevor Haffari;  Cohn"}, {"ref_id": "b20", "title": "Europarl: A parallel corpus for statistical machine translation", "journal": "", "year": "2005", "authors": "Philipp Koehn"}, {"ref_id": "b21", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Taku Kudo; John Richardson"}, {"ref_id": "b22", "title": "Haitian Creole: How to build and ship an MT engine from scratch in 4 days, 17 hours, & 30 minutes", "journal": "", "year": "2010", "authors": "William Lewis"}, {"ref_id": "b23", "title": "Crisis MT: Developing a cookbook for MT in crisis situations", "journal": "Association for Computational Linguistics", "year": "2011", "authors": "William Lewis; Robert Munro; Stephan Vogel"}, {"ref_id": "b24", "title": "OpenSub-titles2016: Extracting large parallel corpora from movie and TV subtitles", "journal": "", "year": "2016", "authors": "Pierre Lison; J\u00f6rg Tiedemann"}, {"ref_id": "b25", "title": "Continual mixed-language pre-training for extremely low-resource neural machine translation", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Zihan Liu; Pascale Genta Indra Winata;  Fung"}, {"ref_id": "b26", "title": "Deltalm: Encoder-decoder pre-training for language generation and translation by augmenting pretrained multilingual encoders", "journal": "", "year": "2021", "authors": "Shuming Ma; Li Dong; Shaohan Huang; Dongdong Zhang; Alexandre Muzio; Saksham Singhal; Xia Hany Hassan Awadalla; Furu Song;  Wei"}, {"ref_id": "b27", "title": "Autshumato machine translation evaluation set", "journal": "", "year": "2017", "authors": "Cindy A Mckellar"}, {"ref_id": "b28", "title": "Overview of the 8th workshop on Asian translation", "journal": "", "year": "2021", "authors": "Toshiaki Nakazawa; Hideki Nakayama; Chenchen Ding; Raj Dabre; Shohei Higashiyama; Hideya Mino; Isao Goto; Win Pa Pa; Anoop Kunchukuttan; Shantipriya Parida; Ond\u0159ej Bojar; Chenhui Chu; Akiko Eriguchi; Kaori Abe; Yusuke Oda; Sadao Kurohashi"}, {"ref_id": "b29", "title": "", "journal": "Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov", "year": "", "authors": "Marta R Nllb Team; James Costa-Juss\u00e0; Onur Cross; Maha \u00c7elebi; Kenneth Elbayad; Kevin Heafield; Elahe Heffernan; Janice Kalbassi; Daniel Lam; Jean Licht; Anna Maillard; Skyler Sun; Guillaume Wang; Al Wenzek; Bapi Youngblood; Loic Akula; Gabriel Barrault; Prangthip Mejia-Gonzalez; John Hansanti; Semarley Hoffman;  Jarrett"}, {"ref_id": "b30", "title": "fairseq: A fast, extensible toolkit for sequence modeling", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Myle Ott; Sergey Edunov; Alexei Baevski; Angela Fan; Sam Gross; Nathan Ng; David Grangier; Michael Auli"}, {"ref_id": "b31", "title": "Bleu: a method for automatic evaluation of machine translation", "journal": "", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"ref_id": "b32", "title": "chrf++: words helping character ngrams", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Maja Popovi\u0107"}, {"ref_id": "b33", "title": "Samanantar: The largest publicly available parallel corpora collection for 11 indic languages", "journal": "", "year": "", "authors": "Gowtham Ramesh; Sumanth Doddapaneni; Aravinth Bheemaraj; Mayank Jobanputra; A K Raghavan; Ajitesh Sharma; Sujit Sahoo; Harshita Diddee; J Mahalakshmi; Divyanshu Kakwani; Navneet Kumar; Aswin Pradeep; Srihari Nagaraj; Kumar Deepak; Vivek Raghavan"}, {"ref_id": "b34", "title": "CCMatrix: Mining billions of high-quality parallel sentences on the web", "journal": "", "year": "2021", "authors": "Holger Schwenk; Guillaume Wenzek; Sergey Edunov; \u00c9douard Grave; Armand Joulin; Angela Fan"}, {"ref_id": "b35", "title": "Improving neural machine translation models with monolingual data", "journal": "", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"ref_id": "b36", "title": "Annual Meeting of the Association for Computational Linguistics", "journal": "", "year": "", "authors": ""}, {"ref_id": "b37", "title": "Leveraging monolingual data with self-supervision for multilingual neural machine translation", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Aditya Siddhant; Ankur Bapna; Yuan Cao; Orhan Firat; Mia Chen; Sneha Kudugunta; Naveen Arivazhagan; Yonghui Wu"}, {"ref_id": "b38", "title": "Towards the next 1000 languages in multilingual machine translation: Exploring the synergy between supervised and self-supervised learning", "journal": "", "year": "2022", "authors": "Aditya Siddhant; Ankur Bapna; Orhan Firat; Yuan Cao; Mia Xu Chen; Isaac Caswell; Xavier Garcia"}, {"ref_id": "b39", "title": "Billions of parallel words for free: Building and using the EU bookshop corpus", "journal": "", "year": "2014", "authors": "Raivis Skadin; J\u00f6rg Tiedemann; Roberts Rozis; Daiga Deksne"}, {"ref_id": "b40", "title": "Domain-specific mt for low-resource languages: The case of bambara-french", "journal": "", "year": "2021", "authors": "Auguste Allahsera; Michael Tapo; Sarah Leventhal; Christopher M Luger; Marcos Homan;  Zampieri"}, {"ref_id": "b41", "title": "Parallel data, tools and interfaces in OPUS", "journal": "", "year": "2012", "authors": "J\u00f6rg Tiedemann"}, {"ref_id": "b42", "title": "Attention is all you need", "journal": "Curran Associates, Inc", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Illia Kaiser;  Polosukhin"}, {"ref_id": "b43", "title": "On the features of translationese. Digital Scholarship in the Humanities", "journal": "", "year": "2015", "authors": "Vered Volansky; Noam Ordan; Shuly Wintner"}, {"ref_id": "b44", "title": "", "journal": "", "year": "", "authors": "Hongyu Wang; Shuming Ma; Li Dong; Shaohan Huang; Dongdong Zhang"}, {"ref_id": "b45", "title": "2021. Findings of the WMT 2021 shared task on large-scale multilingual machine translation", "journal": "", "year": "", "authors": "Guillaume Wenzek; Vishrav Chaudhary; Angela Fan; Sahir Gomez; Naman Goyal; Somya Jain; Douwe Kiela; Tristan Thrush; Francisco Guzm\u00e1n"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "1", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 22Figure 2 brings together the average performance of all models trained on the Italic and Indo-Aryan language clusters -bilingual, multilingual, and multilingual with backtranslation -both when trained only on pre-existing data alone (first set of bars), and when trained with the addition of high-quality seed data (hatched bars).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 1 :1Figure1: Average bilingual translation performance (chrF++). Unresourced languages are trained on increasing amounts of seed data (1k, 3k, 6k sentences). Barely-resourced languages are trained on pre-existing data (P), plus increasing amounts of seed data (P+1k, P+3k, P+6k), and seed data alone (6k). Full results in Appendix D.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: Comparison of average performance (chrF++) on the Italic and Indo-Aryan languages for all model types trained, both with pre-existing data only (P) and with the addition of all seed data (P+6k, hatched).", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "List of the 12 barely-resourced languages, for which some data (parallel sentences) was already publicly available.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "A1. Did you describe the limitations of your work? Left blank. A2. Did you discuss any potential risks of your work? Not applicable. Left blank. A3. Do the abstract and introduction summarize the paper's main claims?", "figure_data": "ACL 2023 Responsible NLP ChecklistLanguageeng-xxxxxx-engA For every submission:1k3k6k1k3k6kace_Arab15.0 15.818.9 21.0ace_Latn21.7 25.117.6 20.0 25.2ary_Arab13.3 15.4 20.312.6 18.8 21.8arz_Arab14.4 18.3 21.217.3 21.0 24.3bam_Latn7.6 17.7 19.912.3 19.1 20.7ban_Latn18.4 25.9 29.417.3 24.0 27.8bho_Deva13.1 18.8 21.911.3 19.1 24.1Left blank.bjn_Arab bjn_Latn17.7 20.1 18.2 27.6 31.816.7 20.0 23.1 24.8 28.0bug_Latn crh_Latn A4. Have you used AI writing assistants when working on this paper? 12.0 20.7 23.7 19.4 22.716.9 18.8 21.7 20.6 23.3Left blank.dik_Latn dzo_Tibt11.0 14.9 17.8 20.4 23.516.0 16.7 19.9 17.1 19.4B Did you use or create scientific artifacts? section 3grn_Latn kas_Arab kas_Deva19.3 23.3 11.7 15.8 19.2 9.3 10.521.3 24.2 20.1 22.8 17.9 19.8knc_Arab knc_Latn B1. Did you cite the creators of artifacts you used? lmo_Latn section 413.1 13.9 14.6 11.7 15.9 18.9 6.0 20.8 23.613.6 13.6 16.9 18.4 21.5 17.7 22.9 26.7ltg_Latn25.0 29.817.1 24.9 29.3mri_Latn23.8 31.1 33.613.1 22.9 26.6scn_Latn16.3 25.4 29.616.9 24.8 29.0shn_Mymr19.4 22.111.4 21.0 24.1szl_Latn15.4 24.6 29.116.9 25.5 30.2taq_Tfng12.6 14.4 15.214.4 17.4 18.6tzm_Tfng15.7 20.5 23.219.1 22.0vec_Latn16.7 28.2 33.517.5 27.0 32.3Average13.2 19.9 22.915.6 20.6 23.7Table 5: Translation performance (chrF++) of bilingual unresourced models trained on increasing amounts of seed data.machine translation. As this was shown to hurt per-formance by NLLB Team et al. (2022), we insteadfollow their recommended multi-tasking approach.Along with the regular machine translation train-ing, target sentences in noised form are fed to theencoder, with the objective of maximising the likeli-hood of predicting the unnoised sentence. Noisingis performed by randomly masking spans of a sen-tence with a mixture of special <mask> tokens orrandomly sampled tokens from the model's vocab-ulary. The experiments are conducted in the P+6ksetting, including all pre-existing publicly availablecorpora as well as the full seed data. To be ableto directly compare the SSL and BT approaches,for these experiments we reuse the monolingual7 https://github.com/mjpost/sacrebleu 8 https://github.com/fairinternal/ fairseq-py; training configurations are at https: //github.com/facebookresearch/fairseq/ tree/nllb/examples/nllb/modeling/train/ conf/cfgcorpora of Section 4.4. As can be seen in Table 7, we find that backtrans-lation outperforms self-supervised learning with the denoising objective on every single direction evaluated. Comparing these models to the ones"}], "formulas": [{"formula_id": "formula_0", "formula_text": "Multilingual", "formula_coordinates": [4.0, 306.14, 214.15, 59.37, 14.19]}], "doi": "10.18653/v1/2022.naacl-main.223"}