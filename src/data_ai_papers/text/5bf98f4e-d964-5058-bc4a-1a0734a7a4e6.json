{"title": "NewsEdits: A News Article Revision Dataset and a Document-Level Reasoning Challenge", "authors": "Alexander Spangher; Xiang Ren; Jonathan May; Nanyun Peng", "pub_date": "", "abstract": "News article revision histories provide clues to narrative and factual evolution in news articles. To facilitate analysis of this evolution, we present the first publicly available dataset of news revision histories, NewsEdits. Our dataset is large-scale and multilingual; it contains 1.2 million articles with 4.6 million versions from over 22 English-and French-language newspaper sources based in three countries, spanning 15 years of coverage (2006)(2007)(2008)(2009)(2010)(2011)(2012)(2013)(2014)(2015)(2016)(2017)(2018)(2019)(2020)(2021). 1    We define article-level edit actions: Addition, Deletion, Edit and Refactor, and develop a highaccuracy extraction algorithm to identify these actions. To underscore the factual nature of many edit actions, we conduct analyses showing that added and deleted sentences are more likely to contain updating events, main content and quotes than unchanged sentences. Finally, to explore whether edit actions are predictable, we introduce three novel tasks aimed at predicting actions performed during version updates. We show that these tasks are possible for expert humans but are challenging for large NLP models. We hope this can spur research in narrative framing and help provide predictive tools for journalists chasing breaking news.", "sections": [{"heading": "Introduction", "text": "Revision histories gathered from various natural language domains like Wikipedia (Grundkiewicz and Junczys-Dowmunt, 2014), Wikihow (Faruqui et al., 2018) and student learner essays (Zhang and Litman, 2015) have primarily been studied to explore stylistic changes, such as grammatical error correction (Shah et al., 2020) and argumentation design (Afrin et al., 2020). However, deeper questions about content updates and narrative evolution are underexplored: Which facts are uncertain and likely to be changed? Which events are likely to We identify sentence-level operations -Edit, Addition, Deletion and Refactor -between two versions of a news article (merges, shown here, and splits are a special cases of Edits). We propose tasks aimed at predicting these operations on article versions. We characterize aspects of additions, deletions and edits. We hope NewsEdits can contribute to research on narrative and factual development patterns.", "publication_ref": ["b25", "b18", "b64", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "update? What voices and perspectives are needed to complete a narrative?", "text": "Existing edits corpora do not address these questions due to the nature of previously studied domains: as shown in Yang et al. (2017), the distribution of edits in other domains, like Wikipedia, tend to focus on syntax or style edits. In this work, we introduce a novel domain for revision histories, news article revision histories which, we show, covers the updating of events. Many edits in news either (1) incorporate new information (2) update events or (3) broaden perspectives (Section 3).\nOur dataset, NewsEdits, contains 1.2 million articles with 4.6 million versions. We develop a document-level view for studying revisions and define four edit actions to characterize changes between versions: sentence Addition, Deletion, Edit and Refactor (i.e. the sentence is moved within a document). We introduce algorithms for identifying these actions. We count over 40 million Edits, Additions, Deletions or Refactors in NewsEdits.\nWe argue that news is an important, practical medium to study questions about narrative, factual and stylistic development. This is because, we hypothesize, there are consistent patterns in the way articles update in the breaking news cycle (Usher, 2018). To prove this hypothesis, we show that updates are predictable. We design three tasks:\n(1) \"predict whether an article will be updated,\" (2) \"predict how much of an article will updated,\" (3) \"predict sentence-level edit actions.\" We show that current large language model (LLM)-based predictors provide a strong baseline above random guessing in most tasks, though expert human journalists perform significantly better. Our insights are twofold: (a) article updates are predictable and follow common patterns which humans are able to discern (b) significant modeling progress is needed to address the questions outlined above. See Section 4.6 for more details.\nFinally, we show that the NewsEdits dataset can bring value to a number of specific, ongoing research directions: event-temporal relation extraction (Ning et al., 2018;Han et al., 2019a), article link prediction (Shahaf and Guestrin, 2010), factguided updates (Shah et al., 2020), misinformation detection (Appelman and Hettinga, 2015), headline generation (Shen et al., 2017) and author attribution (Savoy, 2013), as well as numerous directions in computational journalism (Cohen et al., 2011;Spangher et al., 2020) and communications fields (Spangher et al., 2021b).\nOur contributions are the following:\n1. We introduce NewsEdits, the first public academic corpus of news revision histories.\n2. We develop a document-level view of structural edits and introduce a highly scalable sentence-matching algorithm to label sentences in our dataset as Addition, Deletion, Edit, Refactor. We use these labels to conduct analyses characterizing these operations.\n3. We introduce three novel prediction tasks to assess reasoning about whether and how an article will change. We show that current large language models perform poorly compared with expert human judgement.", "publication_ref": ["b86", "b78", "b55", "b27", "b65", "b64", "b3", "b67", "b62", "b14", "b69", "b71"], "figure_ref": [], "table_ref": []}, {"heading": "The NewsEdits Dataset", "text": "NewsEdits is a dataset of 1.2 million articles and 4.6 million versions. In Section 2.1, we discuss the sources from which we gathered our dataset. In Section 2.2, we discuss the categories of edit-actions designed to characterize changes between versions, and in Section 2.3, we discuss the algorithm we built to identify these edit-actions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data Collection", "text": "We collect a dataset of news article versions. An article is defined by a unique URL, while a version is one publication (of many) to that same URL. We combine data from two online sources that monitor news article updates: NewsSniffer 2 and Twitter accounts powered by DiffEngine. 3 These sources were chosen because, together, they tracked most major U.S., British and Canadian news outlets (Kirchhoff, 2010). Our corpus consists of article versions from 22 media outlets over a 15-year timescale (2006)(2007)(2008)(2009)(2010)(2011)(2012)(2013)(2014)(2015)(2016)(2017)(2018)(2019)(2020)(2021), including The New York Times, Washington Post and Associated Press. Although the median number of updates per article is 2, as shown in Figure 2, this varies depending on the outlet. More dataset details in Appendix E.", "publication_ref": ["b37"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Edit-Action Operations", "text": "Since we are interested in how an entire news article updates between versions, we focus on sentence edits (document-level actions), not word edits (sentence-level actions). Identifying that sentences are added and deleted (vs. updated), can help us study the degree of change an edit introduces in the article Gurevych, 2012, 2013;Fong and Biuk-Aghai, 2010).\nThus, we define the following sentence-level edit-actions, shown in Figure 1: Addition, Deletion, Edit and Refactor. Additions should contain novel information and Deletions should remove information from the article. Edits should be substantially similar except for syntactic changes, rephrased and minimally changed or updated information. Special cases of the Edit operation result in sentences that are merged or split without substantial changes. See Section 2.3 for more details.\nRefactors are intentionally moved in an article. 4 Refactors are important because, based on the inverse pyramid 5 (P\u00f6ttker, 2003) of article structure, sentences that are higher in an article are more important (Scanlan, 2003). Thus, Refactors give us insight into the changing importance of sentences in a narrative.", "publication_ref": ["b21", "b58", "b63"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Edit-Action Extraction", "text": "To extract these edit-actions, we need to be able to construct a bipartite graph linking sentences between two versions of an article (example graph shown in Figure 1). If an edge exists between a sentence in one version and a sentence in the other, the sentence is an Edit (or Unchanged). If no edge exists, the sentence is an Addition (if the sentence exists in the newer version only) or Deletion (if it exists in the older version only). We identify Refactors based on an algorithm we develop: in short, we identify a minimal set of edges in the graph which causes all observed edge-crossings. For details on this algorithm, see Appendix F. In order to construct this bipartite graph, we need a scalable, effective, sentence-similarity algorithm. There is a wide body of research in assessing sentence-similarity (Quan et al., 2019;Abujar et al., 2019;Yao et al., 2018;Chen et al., 2018). However, many of these algorithms measure symmetric sentence-similarity. As shown in Figure 1, two sentences from the old version can be merged in the new version. 6 The symmetric similarity between these three sentences would be low, leading us to label the old sentences as Deletions and the new one an Addition, even if they were minimally edited version t+1 , which is movement that is not caused by other operations. We label this as a Refactor.\n5 An inverse pyramid narrative structure is when the most crucial information, or purpose of the story, is presented first (Scanlan, 2003).\n6 E.g. \"ipsum. Lorem\" \u2192 \"ipsum; and Lorem\". Conversly, one sentence can also be split.\n(for concrete examples, see Table 14). This violates our tag definitions (Section 2.2). So, we need to measure one-way similarity between sentences, allowing us to label merged and split sentences as Edits. Our algorithm is an asymmetrical version of the maximum alignment metric described by Kajiwara and Komachi (2016):\nSim asym (x,y) = 1 x x i=1 max j \u03c6 (x i ,y j )\nwhere \u03c6 (x i ,y j ) \u2236= similarity between words x i in sentence x and y j in sentence y.\nWe test several word-similarity functions, \u03c6 . The first uses a simple lexical overlap, where \u03c6 (x i ,y j ) = 1 if lemma(x i ) = lemma(y j ) and 0 otherwise. 7 The second uses word-embeddings, where \u03c6 (x i ,y j ) = Emb(x i ) \u22c5 Emb(y j ), and Emb(x i ) is the embedding derived from a pretrained language model (Jiao et al., 2020;Liu et al., 2019).\nEach \u03c6 function assesses word-similarity; the next two methods use \u03c6 to assess sentence similarity. Maximum alignment counts the number of word-matches between two sentences, allowing many-to-many word-matches between sentences. Hungarian matching (Kuhn, 1955) is similar, except it only allows one-to-one matches. We compare these with BLEU variations (Papineni et al., 2002), which have been used previously to assess sentence similarity (Faruqui et al., 2018).", "publication_ref": ["b60", "b0", "b87", "b12", "b63", "b36", "b33", "b44", "b38", "b56", "b18"], "figure_ref": ["fig_0", "fig_0"], "table_ref": ["tab_0"]}, {"heading": "Edit-Action Extraction Quality", "text": "Although our sentence-similarity algorithm is unsupervised, we need to collect ground-truth data in order to set hyperparameters (i.e. the similarity threshold above which sentences are considered a match) and evaluate different algorithms. To    do this, we manually identify sentence matches in 280 documents. We asked two expert annotators to identify matches if sentences are nearly the same, they contain the same information but are stylistically different, or if they have substantial overlap in meaning and narrative function. See Appendix G for more details on the annotation task. We use 50% of these human-annotated labels to set hyperparameters, and 50% to evaluate match predictions, shown in Table 1. Maximum Alignment with TinyBERT-medium embeddings (Jiao et al., 2020) (Max-TB-medium) performs best. 8", "publication_ref": ["b33"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Exploratory Analysis", "text": "We extract all edit actions in our dataset using methods described in the previous section. Statistics on the total number of operations are shown in Table 2. In this section, we analyze Additions, Deletions and Edits to explore when, how and why these editactions are made and the clues this provides as to why articles are updated. We leave a descriptive analysis of Refactors to future work. Insight #1: Timing and location of additions, deletions and edits reflect patterns of breaking news and inverse pyramid article structure. How do editing operations evolve from earlier to later versions, and where do they occur in the news article?\nIn Figure 3a, we show that edit-actions in an article's early versions are primarily adding or updating information: new articles tend to have roughly 20% of their sentences edited, 10% added and few deleted. This fits a pattern of breaking news lifecy-  cles: an event occurs, reporters publish a short draft quickly, and then they update as new information is learned (Hansen et al., 1994;Lewis and Cushion, 2009). We further observe, as is demonstrated in Figure 6 in the appendix, that updates occur rapidly: outlets known for breaking news 9 have a median article-update time of < 2 hours. An article's later lifecycle, we see, is determined by churn: \u2248 5% of sentences are added and 5% are deleted every version. As seen in Figure 3b, additions and edits are more likely to occur in the beginning of an article, while deletions are more likely at the end, indicating newer information is prioritized in an inverse pyramid structural fashion. Insight #2: Additions and deletions are more likely to contain fact-patterns associated with breaking news (quotes, events, or main ideas) than unchanged sentences. In the previous section, we showed that the timing and position of edit-actions reflects breaking news scenarios. To provide further clues about the semantics of editactions, we sample Additions, Deletions and unchanged sentences and study the kinds of information contained in these sentences. We study three different fact-patterns associated with breaking news: events, quotes and main ideas (Ekstr\u00f6m et al., 2021;Usher, 2018). To measure the prevalence of these fact-patterns, we sample 200,000 documents (7 million sentences) from our corpus and run an event-extraction pipeline (Ma et al., 2021), quote-detection pipeline (Spangher et al., 2020), and news discourse model (Spangher et al., 2021a). As shown in Table 3, we find added and deleted sentences have significantly more events, quotes and Main-Idea and Cause discourse than unchanged sentences. (See Appendix B for more details.) Insight #3: Edited sentences often contain updating events. The analyses in the previous sec-9 E.g. Associated Press, New York Times and Wash. Post", "publication_ref": ["b29", "b40", "b17", "b78", "b46", "b69"], "figure_ref": ["fig_3", "fig_7", "fig_3"], "table_ref": ["tab_2", "tab_4"]}, {"heading": "130", "text": "Event Chains (attack, killed), (injured, killed), (shot, dead), (shot, killed), (attack, injured), (injured, died), (election, won), (meeting, talks), (talks, meeting), (elections, election), (war, conflict) tions have established that edit-actions both are positioned in the article in ways that resemble, and contain information that is described by, breaking news epistemologies (Ekstr\u00f6m et al., 2021). A remaining question is whether the edit-actions change fact-patterns themselves, rather than simply changing the style or other attributes of sentences.\nOne way to measure this is to explore whether edit-actions update the events in a story (Han et al., 2019b). We focus on pairs of edited sentences. We randomly sample Edits from documents in our corpus (n = 432,329 pairs) and extract events using Ma et al. (2021)'s model. We find that edited sentence pairs are more likely to contain events (43.5%) than unchanged sentences (31.4%). Further, we find that 37.1% of edited sentences with events contain different across versions. We give a sample of pairs in Table 4. This shows that many within sentence operations update events.\nTaken together, we have shown in this analysis that factual updates drive many of the edit operations that we have constructed to describe NewsEdits revision histories. Next, we will measure how predictable these update patterns are.", "publication_ref": ["b17", "b28"], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Predictive Analysis on NewsEdits", "text": "As shown in Section 3, many edit-actions show breaking news patterns, which Usher (2018) observed follow common update patterns. Now, we explore how predictable these operations are, to address whether future work on the fundamental research questions addressed in Section 1 around narrative design is feasible.\nIn this section, we outline three tasks that involve predicting the future states of articles based on the current state. These tasks, we hypothesize, outline several modeling challenges: (1) identify indicators of uncertainty used in news writing 10 (Ekstr\u00f6m et al., 2021), (2) identify informational incompleteness, like source representation (Spangher et al., 2020) and (3) identify prototypical event patterns (Wu et al., 2022). These are all strategies that ex-pert human evaluators used when performing our tasks (Section 4.6). The tasks range from easier to harder, based on the sparsity of the data available for each task and the dimensionality of the prediction. We show that they are predictable but present a challenge for current language modeling approaches: expert humans perform these tasks much more accurately than LLM-based baselines.\nIn addition to serving a model-probing and dataexplanatory purpose, these tasks are also practical: journalists told us in interviews that being able to perform these predictive tasks could help newsrooms allocate reporting resources in a breaking news scenario. 11", "publication_ref": ["b17", "b69", "b84"], "figure_ref": [], "table_ref": []}, {"heading": "Task Description and Training Data Construction", "text": "We now describe our tasks. For all three tasks, we focus on breaking news by filtering NewsEdits down to short articles (# sents \u2208 [5,15]) with low version number (<20) from select outlets. 12 Task 1: Will this document update? Given the text of an article at version v, predict if \u2203v + 1. This probes whether the model can learn a high-level notion of change, irrespective of the fact that different edit-actions have different consequences for the information presented in a news article.\nFor Task 1, y = 1 if a newer version of an article was published and 0 otherwise. We sample 100,000 short article versions from NewsEdits, balancing across length, version number, and y. Task 2: How much will it update? Given the text of an article at version v, predict in the next version how many Additions, Deletions, Edits, Refactors will occur. This moves beyond Task #1 and requires the model to learn more about how each edit-action category changes an article.\nFor Task 2, y = counts of sentence-level labels (Num. Additions, Num. Deletions, Num. Refactors, Num. Edits) described in the previous sections, aggregated per document. Each count is binned: [0,1), [0,3), [3,\u221e) and is predicted separately as a multiclass classification problem. We sample 150,000 short article versions balancing for sources, length and version number. Task 3: How will it update? For each sentence in version v, predict whether: (1) the sentence itself Architecture diagram for the model used for our tasks. Word-embeddings are averaged using Self-Attention to form sentence-vectors. A minimal transformer layer is used to contextualize these vectors (+Contextual Layer). In Tasks 1 and 2, self-attention is used to generate a document-embedding vector.\nwill change (i.e. it will be a Deletion or Edit) (2) a Refactor will occur (i.e. it will be moved either up or down in the document) or (3) an Addition will occur (i.e. either above or below the sentence). This task, which we hypothesize is the hardest task, requires the model to reason specifically about the informational components of each sentence and understand nuance about structure and form in a news article (i.e. like the inverse pyramid structure (P\u00f6ttker, 2003)).\nFor Task 3, y = individual sentence-level labels. Labels are derived for the following subtasks mentioned above: (1) Sentence Operations is a categorical label comprising: [Deletion, Edit, Unchanged], expressed as a one-hot vector. (2) Refactor is a categorical label comprising: [Up, Down, Unchanged], also expressed as a one-hot vector. (3) Addition Above and Addition Below are each binary labels expressing whether > 1 sentences was added above or below the target sentence. Because some sentences had Additions above and below, we chose to model this subtask as two separate classification tasks. We sample 100,000 short article versions, balancing for sources, length and version number.\nFor each task, the input X is a document represented as a sequence of sentences. For each evaluation set, we sample 4k documents balancing for class labels (some labels are highly imbalanced and cannot be balanced).", "publication_ref": ["b58"], "figure_ref": [], "table_ref": []}, {"heading": "Modeling", "text": "We benchmark our tasks using a RoBERTa-based architecture shown in Figure 4. Spangher et al. (2021a) showed that a RoBERTa-based architec-ture (Liu et al., 2019) with a contextualization layer outperformed other LLM-based architectures like Reimers and Gurevych (2019) for document-level understanding tasks (further insight given in Section 4.6).\nIn our model, each sentence from document d is fed into a pretrained RoBERTa Base model 13 to obtain contextualized word embeddings. The word embeddings are then averaged using self-attention, creating sentence vectors. For Task 3, these vectors are then used directly for sentence-level predictions. For Tasks 1 and 2 these vectors are condensed further, using self-attention, into a single document vector which is then used for document-level predictions. The sentence vectors are optionally contextualized to incorporate knowledge of surrounding sentences, using a small Transformer layer 14 (+Contextualized in Tables 5, 6, 7).\nWe experiment with the following variations. For Task 2, we train with less data (n = 30,000 version pairs) and more data (n = 150,000 version pairs), balanced as described in Section 4.1, to test whether a larger dataset would help the models generalize better. We also experiment, for all tasks, with freezing the bottom 6 layers of the RoBERTa architecture (+Partially Frozen) to probe whether pretrained knowledge is helpful for these tasks. Additionally, we experiment giving the version number of the older version as an additional input feature alongside the text of the document (+Version).\nFinally, for Tasks 2 and 3, we attempt to jointly model all subtasksusing separate prediction heads for each subtask but sharing all other layers. We use uniform loss weighting between the tasks. Spangher et al. (2021a) showed that various document-level understanding tasks could benefit by being modeled jointly. For our tasks, we hypothesize that decisions around one operation might affect another: i.e. if a writer deletes many sentences in one draft they might also add sentences, so we test whether jointly modeling has a positive effect.\nWe do not consider any feature engineering on the input text, like performing event extraction (Ma et al., 2021), even though results in Section 3 show that certain types of edit-actions are more likely to contain events. We wish to establish a strong baseline and test whether models can learn salient features on their own. For more discussion on mod-  Table 6: Task 3 Benchmarks: Baseline model performance for sentence-Level tasks. Addition tasks are: \"Was a sentence added below the target sentence?\", \"Was a sentence added above the target sentence?\" Sentence Operations columns are three operations that occur on the target sentence: \"Deletion\", \"Editing\", \"Unchanged\". Refactor is binned into whether the target sentence is \"Moved Up\", \"Moved Down\" or \"Unchanged\". (Scores shown are median of 1,000 bootstrap resamples of the evaluation dataset.)  eling choices and hyperparameter values, see Appendix D.", "publication_ref": ["b44", "b61", "b46"], "figure_ref": ["fig_4"], "table_ref": ["tab_7"]}, {"heading": "Human Performance", "text": "To evaluate how well human editors agree on edits, we design two human evaluation tasks and recruit 5 journalists with \u2265 1 year of editing experience at major U.S. and international media outlets. Evaluation Task 1: We show users the text of an article and ask them whether or not there will be an update. Collectively, they annotate 100 articles. After completing each round, they are shown the true labels. This evaluates Task 1. Evaluation Task 2: We show users the sentences of an article, and they are able to move sentences, mark them as deleted or edited, and add sentenceblocks above or below sentences. They are not asked to write any text, only mark the high-level actions of \"I would add a sentence,\" etc. Collectively they annotate 350 news articles. After each annotation, they see what edits actually happened.\nThe raw output evaluates Task 3 and we aggregate their actions for each article to evaluate Task 2. They are instructed to use their expert intuition and they are interviewed afterwards on the strategies used to make these predictions. (See Appendix G for task guidelines and interviews).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "As shown in Tables 5, 6, and 7, model-performance indicates that our tasks do range from easier (Task 1) to harder (Task 3). While our models show improvements above Random, and Most Popular  in almost all subtasks, a notable exception is Task 3's Addition subtasks, where the models do not clearly beat Random. We note that this was also the most difficult subtask for human evaluators.\nWe observe that +Partially Frozen increases performance on Task 2, boosting performance in all subtasks by \u2248 10 points. In contrast, it does not increase performance on Task 3, perhaps indicating that the subtasks in Task 3 are difficult for the current LLM paradigm. Although adding version embeddings (+Version) boosts performance for Task 1, it does not seem to measurably increase performance for the other tasks. Finally, performing Task 2 and 3 as multitask learning problems decreases performance for all subtasks.\nIn contrast, human evaluators beat model performance across tasks, most consistently in Task 2, with on average performance 20 F1-score points above Baseline models. On Task 3, human performance also is high relative to model performance. We observe that, despite Additions in Task 3 being the hardest task, as judged by human and model performance, humans showed a \u2248 40 point increase above model performance. Humans are also better at correctly identifying minority classes, with a wider performance gap seen for Macro F1 scores (i.e. see Sentence Operations, where the majority of sentences are unchanged).", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Error Analysis", "text": "We perform an error analysis on the Task 2 task and find that there are several categories of edits that are easier to predict than others. We run Latent Dirichlet allocation on 40,000 articles, shown in Table 8. 15 We assign documents to their highest topic and find that articles covering certain news topics (like War) update in a much more predictable pattern than others (like Business), with a spread of over 26 F1-score points. Further, we find that certain edit-patterns are easier to differentiate, like articles that grow between 1-5 sentences (Table 8). This 15 Topic words shown in Appendix C.\nshow us ways to select for subsets of our dataset that are more standard in their update patterns.\nThe class imbalance of this dataset (Table 2) results in the Most Popular scoring highly. To mitigate this, we evaluate on balanced datasets. Class imbalanced training approaches Spangher et al., 2021a) might be of further help.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_11", "tab_11", "tab_2"]}, {"heading": "Evaluator Interviews", "text": "To better understand the process involved with successful human annotation, we conducted evaluator interviews. We noticed that evaluators first identified whether the main news event was still occurring, or if it was in the past. For the former, they tried to predict when the event would update. 16 For the latter, they considered discourse components to determine if an article was narratively complete and analyzed the specificity of the quotes. 17 They determined where to add information in the story based on structural analysis, and stressed the importance of the inverse pyramid for informational uncertainty: information later in an article had more uncertainty; if confirmed, it would be moved up in later versions. 18 Finally, they considered the emotional salience of events; if a sentence described an event causing harm, it would be moved up. 19 Clearly, these tasks demand strong worldknowledge and common sense, as well and highlevel discourse, structural and narrative awareness. 20 Combining these different forms of reasoning, our results show, is challenging for current language models, which, for many subtasks, perform worse than guessing. +Multitask performance actually decreases performance for both Task 2 and Task 3, indicating that these models learn features that do not generalize across subtasks. This contrasts with what our evaluators said: their decision to delete sentences often used the same reasoning as, and were dependent on, their decisions to add.\nHowever, we see potential for improvement in these tasks. Current LLMs have been shown to identify common arcs in story-telling (Boyd et al., 2020), identify event-sequences (Han et al., 2019b) and reason about discourse structures (Spangher et al., 2021a;Li et al., 2021). Further, for the ROCStories challenge, which presents four sentences and tasks the model with predicting the fifth (Mostafazadeh et al., 2017(Mostafazadeh et al., , 2016, LLMs have been shown to perform scene reconstruction (Tian et al., 2020b), story planning , and structural common sense reasoning . These are all aspects of reasoning that our evaluators told us they relied on. Narrative arcs in journalism are often standard and structured (Neiger and Tenenboim-Weinblatt, 2016), so we see potential for improvement.", "publication_ref": ["b6", "b28", "b41", "b52", "b51", "b76", "b53"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "A significant contribution of this work, we feel, is the introduction of a large corpus of news edits into revision-history research and the framing of questions around sentence-level edit-actions. Despite the centrality of news writing in NLP (Marcus et al., 1993;Carlson et al., 2003;Pustejovsky et al., 2003;Walker et al., 2006), we know of no academic corpus of news revision histories. Two works that analyze news edits to predict article quality  do not release their datasets. 21 WikiNews 22 articles and editorannotations have been used for document summarization (Bravo-Marquez and Manriquez, 2012), timeline synthesis (Zhang and Wan, 2017; Minard et al., 2016), word-identification (Yimam et al., 2017 and entity salience . However, we are not aware of any work using WikiNews revision histories. We did not include WikiNews because its collaborative community edits differ from professional news edits.\nSince at least 2006, internet activists have tracked changes made to major digital news articles (Herrmann, 2006). NewsDiffs.org, NewsSniffer and DiffEngine are platforms which researchers have used to study instances of gender and racial bias in article drafts, 23 (Brisbane, 2012;Burke, 2016;Jones and Neubert, 2017;Fass and Main, 2014) shifting portrayals of social events, (Johnson et al., 2016) and lack of media transparency (Gourarie, 2015). These tools collect article versions from RSS feeds and the Internet Archive. Major newspa-pers 24 and thousands of government websites 25 are being analyzed. We use DiffEngine and NewsSniffer to construct NewsEdits.\nWikihow (Anthonio et al., 2020;Bhat et al., 2020) and Source Code Diffs (Tan and Bockisch, 2019;Shen et al., 2019;Tsantalis et al., 2018;Silva and Valente, 2017;Marrese-Taylor et al., 2020;Xu et al., 2019) use revision histories from domains and for purposes different than ours. Many tasks have benefited from studying Wikipedia Revisions, like text simplification (Yatskar et al., 2010), textual entailment (Zanzotto and Pennacchiotti, 2010), discourse learning (Daxenberger and Gurevych, 2013) and grammatical error correction (Faruqui et al., 2018). However, most tasks focus on word-level edit operations to explore sentence-level changes. Ours focuses on sentence-level operations to explore document-level changes. Research in Student Learner Essays focuses on editing revisions made during essay-writing (Leacock et al., 2010;Zhang, 2020;Zhang and Litman, 2015). Researchers categorize the intention and effects of each edit (Zhang et al., 2017;Afrin et al., 2020), but do not try to predict edits.", "publication_ref": ["b47", "b10", "b59", "b80", "b7", "b50", "b31", "b8", "b9", "b35", "b19", "b34", "b24", "b4", "b4", "b74", "b66", "b77", "b68", "b48", "b85", "b89", "b16", "b18", "b39", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this work, we have introduced the first largescale dataset of news edits, extracted edit-actions, and shown that many were fact-based. We showed that edit-actions are predictable by experts but challenging for current LM-backed classifierss. Going forward, we will develop a schema describing the types of edits. We are inspired by the Wikipedia Intentions schema developed by Yang et al. (2017), and are working in collaboration with journalists to further clarify the differences. This development will help to clarify the nature of these edits as well as focus further directions of inquiry.", "publication_ref": ["b86"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We are grateful to Amanda Stent, Sz-Rung Shiang, Gabriel Kahn, Casey Williams, Meg Robbins, I-Hung Hsu, Mozhdeh Gheini, Jiao Sun and our anonymous reviewers for invaluable feedback. Spangher is grateful for Bloomberg for supporting this research with a PhD fellowship. May is supported by DARPA Contract FA8750-19-2-0500.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethical Considerations", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dataset", "text": "We received permission from the original owners of the datasets, NewsSniffer and DiffEngine. Both sources are shared under strong sharing licenses. NewsSniffer is released under an AGPL-3.0 License, 26 which is a strong \"CopyLeft\" license.\nDiffEngine is released under an Attribution-NoDerivatives 4.0 International license. 27 Our use is within the bounds of intended use given in writing by the original dataset creators, and is within the scope of their licensing.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Privacy", "text": "We believe that there are no adverse privacy implications in this dataset. The dataset comprises news articles that were already published in the public domain with the expectation of widespread distribution. We did not engage in any concerted effort to assess whether information within the dataset was libelious, slanderous or otherwise unprotected speech. We instructed annotators to be aware that this was a possibility and to report to us if they saw anything, but we did not receive any reports. We discuss this more below.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations and Risks", "text": "The primary theoretical limitation in our work is that we did not include a robust non-Western language source; indeed, our only two languages were English and French. We tried to obtain sources in non-Western newspapers and reached out to a number of activists that use the DiffEngine platform to collect news outside of the Western world, including activists from Russia and Brazil. Unfortunately, we were not able to get a responses.\nThus, this work should be viewed with that important caveat. We cannot assume a priori that all cultures necessarily follow this approach to breaking news and indeed all of the theoretical works that we cite in justifying our directions also focus on English-language newspapers. We provide documentation in the Appendix about the language, source, timeline and size of each media outlet that we use in this dataset.\nOne possible risk is that some of the information contained in earlier versions of news articles was updated or removed for the express purpose that it 26 https://opensource.org/licenses/AGPL-3.0 27 https://creativecommons.org/licenses/by-nd/4.0/ was potentially unprotected speech: libel, slander, etc. We discussed this with the original authors of NewsSniffer and DiffEngine. During their years of operation, neither author has received any requests to take versions down. Furthermore, instances of First Amendment lawsuits where the plaintiff was successful in challenging content are rare in the U.S. We are not as familiar with the guidelines of protected speech in other countries.\nAnother risk we see is the misuse of this work on edits for the purpose of disparaging and denigrating media outlets. Many of these news tracker websites have been used for noble purposes (e.g. holding newspapers accountable for when they make stylistic edits or try to update without giving notice). But we live in a political environment that is often hostile to the core democracy-preserving role of the media. We focus on fact-based updates and hope that this resource is not used to unnecessarily find fault with media outlets.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Computational Resources", "text": "The experiments in our paper require computational resources. All our models run on a single 30GB NVIDIA V100 GPU, along with storage and CPU capabilities provided by AWS. While our experiments do not need to leverage model or data parallelism, we still recognize that not all researchers have access to this resource level.\nWe use Huggingface RoBERTa-base models for our predictive tasks, and release the code of all the custom architectures that we construct at https: //github.com/isi-nlp/NewsEdits.git. Our models do not exceed 300 million parameters.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Annotators", "text": "We recruited annotators from professional journalism networks like the NICAR listserve. 28 All the annotators consented to annotate as part of the experiment, and were paid $1 per task, above the highest minimum wage in the U.S. Of our five annotators, three are based in large U.S. cities, one lives in a small U.S. city and one lives in a large Brazilian city. Four annotators identify as white and one identifies as Latinx. Four annotators identify as male and one identifies as female. This data collection process is covered under a university IRB. We do not publish personal details about the annotations, and their interviews were given with consent and full awareness that they would be published in full. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Dataset: Broader Scope", "text": "We expect that NewsEditswill be useful for a range of existing tasks for revision corpora, such as edit language modeling (Yin et al., 2018) and grammatical error correction (Grundkiewicz and Junczys-Dowmunt, 2014). We also think NewsEdits can impact other areas of NLP research and computational journalism, including: 1. Resource Allocation in Newsrooms Newsrooms are often tasked with covering multiple breaking news stories that are unfolding simultanesouly (Usher, 2018). When multiple stories are being published to cover breaking news, or multiple news events are breaking at the same time, newsrooms are often forced to make decisions on which journalists to assign to continue reporting stories. This becomes especially pronounced in an era of budget cuts and localjournalism shortages (Nielsen, 2015). We interviewed 3 journalists with over 20 years of experience at major breaking news outlets. They agreed that a predictive system that performed the tasks explored in Section 4 would be very helpful for allowing editors track which stories are most likely to change the most, allowing them to keep resources on these stories. 2. Event-temporal relation extraction (Ning et al., 2018) and Fact-guided updates (Shah et al., 2020). As shown in Tables 3 and 4, added and edited sentences are both more likely to contain events, and event updates. We see potential for using these sentences to train revise-and-edit (Hashimoto et al., 2018) models.\n3. Misinformation: Journalists often issue formal Corrections when they discover errors in their reporting (Appelman and Hettinga, 2015). 29 We found 14,301 corrections in added sentences across the same sample with a custom lexicon. 30 This might be used to help compare malicious campaigns with honest errors (Ferrara, 2017). 4. Headline Generation (Shen et al., 2017). Across a sample of 2 million version pairs, we count 376,944, or 17% that have a headline update. Headlines have been used to predict emotional salience (Gupta and Yang, 2019). Modeling edits that result in headline changes can help differentiate salient from non-salient edits.\n5. Authorship Attribution is the task of predicting which authors were involved in writing an article. We found 2,747 Contributor Lines 31 added to articles. This can provide a temporal extension to author-attribution models such as Savoy (2013). 6. Identifying Informational Needs: Source inclusion (Spangher et al., 2020) and discourse structures (Choubey et al., 2020;Spangher et al., 2021a) of static articles have been studied. We see this corpus as being useful for studying when these narrative elements are added. Directions that we have not explored, but possibly interesting include: style transfer (Fu et al., 2018), detecting bias in news articles (Mehrabi et al., 2020), cross-cultural sensitivity (Tian et al., 2020a), insertion-based article generation (Lu and Peng, 2021), and framing changes in response to an unfolding story (Spangher et al., 2021b).", "publication_ref": ["b91", "b25", "b78", "b54", "b55", "b64", "b3", "b67", "b26", "b69", "b13", "b22", "b49", "b45", "b71"], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "B Exploratory Analysis Details", "text": "Insight #2 in Section 3 was based on several experiments that we ran. Here we provide more details about the experiments we ran. Events: We sample of 200,000 documents (7 million sentences) from our corpus 32 and use Eventplus (Ma et al., 2021) to extract all events. We find added/deleted sentences have significantly more events than unchanged sentences. Quotes: Using a quote extraction pipeline (Spangher et al., 2020), we extract explicit and implicit quotes from the sample of documents used above. The pipeline identifies patterns associated with quotes (e.g. double quotation marks) to distantly supervise training an algorithm to extract a wide variety of implicit and explicit quotes with high accuracy (.8 F1-score). We find added/deleted sentences contain significantly more quotes than unchanged sentences. News Discourse: We train a model to identify three coarse-grained discourse categories in news text: Main (i.e. main story) Cause (i.e. immediate context), and Distant (i.e. history, analysis, etc.) We use a news discourse schema (Van Dijk, 1983) and a labeled dataset which contains 800 news articles labeled on the sentence-level (Choubey et al., 2020). We train a model on this dataset to score news articles in our dataset. 33 Then, we filter to Addition, Deletion, etc. sentences. We show that added and deleted sentences are significantly more likely than unchanged sentences to be Main or Cause sentences, while unchanged sentences are significantly more likely to be Distant.", "publication_ref": ["b46", "b69", "b79", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "C Error Analysis: Continued", "text": "As discussed in Section 4.5, we perform Latent Dirichlet Allocation (Blei et al., 2003) to softcluster documents. In Table 9, we show the top k = 10 words for each topic i (i.e. \u03b2 i 1,...k where\n\u03b2 i 1 > \u03b2 i 2 > ... > \u03b2 i k ).", "publication_ref": ["b5"], "figure_ref": [], "table_ref": ["tab_14"]}, {"heading": "D Experiment Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.1 Modeling Decisions", "text": "For Task 1, we sample documents in our training dataset, balancing across versions and y and exclude articles with more than 6,000 characters. However, because of the imbalanced nature of the dataset, we could not fully balance.\nAs is seen in Table 2, +Version, the version number of the old version had a large effect on the performance of the model, boosting performance by over 10 points. We believe that this is permissible, because the version number of the old article is available at prediction time. Interestingly, the effect is actually the opposite of what we would expect. As can be seen in Figure 5, the more versions an article has, the more likely it is to contain another version. This is perhaps because articles with many versions are breaking news articles, and they behave differently than articles with fewer versions. To more properly test a model's ability to judge breaking news specifically, we can create a validation set where all versions of a set of articles are included; thus the model is forced to identify at early versions whether an article is a breaking news story or not.\nFor Task 2, we first experiment with different regression modeling heads before reframing the task as a classification task. We test with Linear Regression and Poisson Regression, seeking to learn the raw counts. However, we found that we were not able to improve above random in any subtask and reframed the problem as a binned classification problem. ", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": ["tab_2"]}, {"heading": "D.2 Hyperparameters and Training", "text": "For all tasks, we used pretrained RoBERTa Base from Wolf et al. (2020). We used reasonable defaults for learning rate, dropout and other hyperparameters explored in Spangher et al. (2021a), which we describe now. For all tasks, we used AdamW as an optimizer, with values \u03b2 1 = .9, \u03b2 2 = .99, \u03b5 = 1e\u22128.\nWe used batch-size = 1 but experimented with different gradient accumulations (i.e. effective batch size) \u2208 [10,20,100]. We did not find much impact to varying this parameter. We used a learning rate of 1e-6 as in Spangher et al. (2021a). Early in experimentation, we trained for 10 epochs, but did not observe any improvement past the 3rd epoch, so we limited training to 5 epochs. We used a dropout probability of .1, 0 warmup steps and 0 weight decay. The embedding dimensionality for the pretrained RoBERTa Base we used is 768, and for all other layers, we used a hidden-dimension of 512.\nFor deriving sentence embeddings, we tested several different methods. We tested both using the <sep> token from RoBERTa and averaging the word-embeddings of each word-piece, as in Spangher et al. (2021a), but found that a third method-using self-attention over the word embeddings, or a learned, weighted average-performed the best. We concatenated a sentence-level positional embedding vector, as in Spangher et al. (2021a), with a max cutoff of 40 positional embeddings (i.e. every sentence with an index greater than 40 was assigned the same vector.)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E Dataset Details", "text": "Here, we give additional details on the dataset, starting with relevant analyses and ending with technical details that should guide the user on how to access our dataset.    ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.1 Additional Analysis", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.1.1 Amount of time between Versions", "text": "The amount of time between republication of an article varies widely across news outlets, and has a large role in determining what kinds of stories are being republished. As can be seen in Figure 6, we group sources into 4 categories: (1) Figure 6a, those that update articles over weeks (tabloids and magazines), (2) Figure 6b, those that update articles on a daily basis, on median, (3) Figure 6c, those that update 2-3 times a day, and (4) Figure 6d, those that update hourly, or breaking news outlets.\nWe are especially interested in rapid updates, because, by limits imposed by this timescale on how  much information can be gathered by journalists, these updates are more likely to contain single units of information, updates and quotes. Thus, in our experiments, we focus on The New York Times, Independent, Associated Press, Washington Post, and BBC. We also include Guardian and Reuters because they typically compete directly with the previously mentioned outlets in terms of content and style, even if they do not publish as frequently.", "publication_ref": [], "figure_ref": ["fig_7", "fig_7", "fig_7", "fig_7", "fig_7"], "table_ref": []}, {"heading": "E.1.2 Discourse Across Time", "text": "We are interested in the dynamics of articles over time. Although this analysis is still ongoing, we seek to understand how, as the article grows through time, the types of information included in it changes.We show in Figure 7a and 7b that in  later versions and longer articles 34 sentences are dominated by Distant discourse. Interestingly, later versions are also more likely to have Main and Cause discourse added. Based on our annotator interviews, we surmise that this is because, for breaking news, a journalist is frequently trying to assess the causes behind the story. In early drafts, we also see Main sentences being removed. This is due to, as the story is updating in early versions, the Main event is most likely to be changing.", "publication_ref": [], "figure_ref": ["fig_9"], "table_ref": []}, {"heading": "E.1.3 Top Words Top Words:", "text": "We characterize added and deleted sentences by their word usage in Table 10. Words indicating present-tense, recent updates are more likely: day-names like \"Monday\" or \"Tuesday\" and the present-tense verb \"says\" (compared with the past-tense \"said\" in unchanged sentences).", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "E.1.4 Collection of Corrections, Authorship", "text": "To identify instances of Corrections in added sentences, we used the following lexicon: \"was corrected\", \"revised\", \"clarification\", \"earlier error\", \"version\", \"article\"\nHere are some examples of corrections:\n\u2022 CORRECTION: An earlier version of this story ascribed to Nato spokesman Brig Gen Carsten Jacobsen comments suggesting that after Saturday\u015b shooting, people would have to be \"looking over their shoulders\" in Afghan ministries. \u2022 CORRECTION 19 November 2012:An earlier version of this story incorrectly referred to \"gargoyles\", not \"spires\". \u2022 Correction 7 March 2012: An earlier version of this story mistakenly said Rushbrook's car had been travelling at 140mph at the time of the crash. To identify instances of Contributor Lines, we use the following lexicon: \"reporting by\", \"additional reporting\", \"contributed reporting\", \"editing by\"\n34 Version Number has spearman's correlation r = .335 with article length. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.2 Dataset Tables and Fields", "text": "Our dataset is released in a set of 5 SQLite tables. Three of them are primary data tables, and two are summary-statistic tables. Our primary data tables are: articles, sentence_diffs, word_diffs; the first two of which are shown in Tables 12a and 12b (word_diffs shares a similar structure with sentence_diffs). We compile two summary statistics tables to cache statistics from sentence_diffs and word_diffs; they calculate metrics such as NUM_SENTENCES_ADDED and NUM_SENTENCES_REMOVED per article. 35 The sentence_diffs data table's schema is shown in Table 12 and some column-abbreviated sample rows are shown in Table 14. As can be seen, the diffs are calculated and organized on a sentencelevel. Each row shows a comparison of sentences between two adjacent versions of the same article. 36 Every row in sentence_diffs contains index columns: SOURCE, A_ID, VERSION_OLD, and VERSION_NEW. These columns can be used to uniquely map each row in sentence_diffs to two rows in article. 37\n35 These summary statistic tables make it convenient to, say, filter sentence_diffs in order train a model on all articles that have one sentence added; or all articles that have no sentences removed.\n36 So, for instance, article A, with versions 1, 2 where each version has sentences i, ii, iii, would have 3 rows (assuming sentences were similar): A.1-2.i, A.1-2.ii, A.1-2.iii. 37 One mapping for sentence_diffs.VERSION_OLD = article.VERSION_ID and one mapping for sentence_diffs.VERSION_NEW = article.VERSION_ID.  ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0", "tab_0", "tab_0"]}, {"heading": "E.3 TAG columns in sentence_diffs", "text": "The columns TAG_OLD and TAG_NEW in sentence_diffs have specific meaning: how to transform from version to its adjacent version. In other words, TAG_OLD conveys where to find SENT_OLD in VERSION_NEW and whether to change it, whereas TAG_NEW does the same for SENT_NEW in VERSION_OLD.\nMore concretely, consider the examples in Table 14b, 14a and 14c. As can be seen, each tag is 3-part and has the following components. Component 1 can be either M, A, or R. M means that the sentence in the current version was Matched with a sentence in the adjacent version, A means that a sentence was Added to the new version and R means the sentence was Removed from the old version. 38 Component 2 is only present for Matched sentences, and refers to the index or indices of the sentence(s) in the adjacent version. 39 Additionally, Component 3 is also only present if the sentence is Matched. It can be either C or U. C refers to whether the matched sentence was Changed and U to whether it was Unchanged.\nAlthough not shown or described in detail, all M sentences have corresponding entry-matches in 38 i.e. an Added row is not present in the old version and a Removed row is not present in the new version. They have essentially the same meaning and we could have condensed notation, but we felt this was more intuitive.\n39 I.e. in TAG_OLD, the index refers to the SENTENCE_ID of SENT_NEW word_diffs table, which has a similar schema and tagging aim.\nA user might use these tags in the following ways:\n1. To compare only atomic edits, as in Faruqui et al. (2018), a user could filter sentence_diffs to sentences where M..C is in TAG_OLD (or equivalently, TAG_NEW). Then, they would join TAG_OLD.Component_2 with SENTENCE_ID. Finally, they would select SENT_OLD, SENT_NEW. 40 2. To view only refactorings, or when a sentence is moved from one location in the article to another, a user could filter sentence_diffs to only sentences containing M..U and follow a similar join process as in use-case 1. 3. To model which sentences might be added, i.e. p(sentence i \u2208 article t+1 sentence i \u2209 article t ), a user would select all sentences in SENT_OLD, and all sentences in SENT_NEW where A is in TAG_NEW. 4. To model the inverse of use-case 3, i.e. which sentences would be removed, or p(sentence i \u2209 article t+1 sentence i \u2208 article t ), a user would select all sentences in SENT_NEW, and all sentences in SENT_OLD where R is in TAG_OLD.  ", "publication_ref": ["b18"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "E.4 Comparison With Other Edits Corpora", "text": "Here, we give a tabular comparison with other edits corpora, showing our", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F Algorithm Details", "text": "In this section, we give further examples further justify our asymmetrical sentence-matching algorithm. The examples shown in Tables 14b, 14a and 14c illustrate our requirements. The first example, shown in Table 14b, occurs when a sentence is edited syntactically, but its meaning does not change. 42 So, we need our sentence-matching algorithm to use a sentence-similarity measure that considers semantic changes and does not consider surface-level changes. The second example, shown in Table 14a, occurs when a sentence is split (or inversely, two sentences are merged.) Thus, we need our sentence matching algorithm to consider manyto-one matchings for sentences. The third example, shown in Table 14c, occurs when sentence-order is rearranged, arbitrarily, throughout a piece. Finally, we need our sentence-matching algorithm to perform all pairwise comparisons of sentences.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0", "tab_0", "tab_0", "tab_0"]}, {"heading": "F.1 Refactors", "text": "To identify which sentences were intentionally moved rather than moved as a consequence of other document-level changes, we develop an iterative algorithm based on the idea that a refactor is an intentional sentence movement that creates an edgecrossing. Algorithm 2 givens our algorithm. In English, our algorithm represents sentence 42 Syntactic changes: synonyms are used, or phrasing is condensed, but substantially new information is not added\ninput :Article versions v old , v new , Match Threshold T output :maps m old\u2192new , m old\u2190new initialize; m old\u2192new , m old\u2190new = {}, {}; // match v old \u2192 v new for (i,s i ) \u2208 v old do d = max s j \u2208vnew Sim asym (s i ,s j ) j = argmax s j \u2208vnew Sim asym (s i ,s j ) m old\u2192new [i] = j \u00d7 1[d > T ] end // match v old \u2190 v new for ( j,s j ) \u2208 v new do d = max s i \u2208v old Sim asym (s j ,s i ) i = argmax s i \u2208v old Sim asym (s j ,s i ) m old\u2190new [ j] = i \u00d7 1[d > T ]\nend Algorithm 1:\nAsymmetrical sentencematching algorithm. Input v old , v new are lists of sentences, and output is an index mapper. If a sentence maps to 0 (i.e. d < T ), there is no match. Sim asym is described in text. matches between two article versions as a bipartite graph. We use a Binary Tree to recursively find all edge crossings in that graph. This idea is based off of the solution for an SPOJ challenge problem: https://www.spoj.com/problems/ MSE06H/. 43 We extend this problem to return the set of all edge crossings, not just the crossing number.\nThen, we filter edge crossings to a candidate  set, applying the following conditions in order and stopping when there is only one edge crossing left:\n(1) edges that have the most number of crossings (2) edges that extend the most distance or (3) edges that move upwards. In most cases, we only apply the first and then the second conditions. In very rare cases, we apply all three. In rarer cases, we apply all three and still have multiple candidate edges. In those cases, we just choose the first edge in the candidate set. We continue removing edges until we have no more crossings.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G Annotation-Task Descriptions", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G.1 Task: Sentence Matching", "text": "We give our annotators the following instructions:\nThe goal of this exercise is to help us identify sentences in an article-rewrite that contain substantially new information. To do this, you will identiy which sentences match between two versions of an article.\nTwo sentences match if:\n1. They are nearly the same, word-forword.\n2. They convey the same information but are stylistically different.\n3. They have slightly different information but have substantial overlap in meaning and narrative function.\nExamples of Option 3 include (please see the \"Examples\" section for real examples):\n1. Updating events.\n\u2022 (Ex) The man was presumed missing. \u2192 The man was found in his home. \u2022 (Ex) The death count was at 23. \u2192 50 were found dead. \u2022 (Ex) The senators are still negotiating the details. \u2192 The senators have reached a deal. 2. An improved analysis.\n\u2022 (Ex) The president is likely seeking improved relations. \u2192 The president is likely hoping that hardliners will give way to moderates, improving relations. \u2022 (Ex) The storm, a Category IV, is expected to hit Texas. \u2192 The storm, downgraded to Category III, is projected to stay mainly in the Gulf. \u2022 (Ex) Analysts widely think the shock will be temporary. \u2192 The shock, caused by widespread shipping delays, might last into December, but will ultimately subside. 3. A quote that is very similar or serves the same purpose.\n\u2022 (Ex) \"We knew we had to get it done.\" said Senator Murphy. \u2192 \"At the end of the day, no one could leave until we had a deal\" said Senator Harris. \u2022 (Ex) \"It was gripping.\" said the by-  stander. \u2192 \"I couldn't stop watching.\" said a moviegoer.\nTwo sentences do not match if:\n1. They contain substantially different information.\n2. They serve different narrative functions.\n3. There is a much better match for one sentence somewhere else in the document.\nThings to keep in mind:\n\u2022 Two sentences might match even if they are in different parts of the document.\n\u2022 One sentence can match with multiple other sentences, because that sentence might be split up into multiple sentences, each with similar information as parts of the original. \u2022 Sentences don't have to match.\n-Substantially new information, perspectives or narrative tools might be added in a new version. -Substantially old information, perspectives or narrative tools might be removed from an old version.\nAnnotators completed the task by drawing lines between sentences in different versions of an article. An example is shown in Figure 8. We use highlighting to show when non overlapping sequences in the inbox, using simple lexical overlap. If the user mouses over a text block, they can see which words do no match between all textblocks on the other side. Although this might bias them towards our lexical matching algorithms, we do not see them beaking TB-medium. This was very helpful for reducing the cognitive overload of the task.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G.2 Task: Edit Actions", "text": "In this task, workers were instructed to perform edit operations to an article version in anticipation of what the next version would look like. We recruited 5 workers: journalists who collectively had over a decade of experience working for outlets like The New York Times, Huffington Post, Vice, a local outlet in Maine, and freelancing.\nWe gave our workers the following instructions.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G.3 Annotator Analysis", "text": "We seek here to characterize the performance of different expert annotators. We see in Table 15 that there are three workers which do over 30 tasks each. We characterize the per-task accuracy by counting the number of edit-operations per document, and The sandbox on the right is where annotators actually perform the task. The first sentence has been Edited, two sentences have been Added, the third has been Deleted and the fourth has been Refactored downwards.  seeing if they got the same number as the true number of edits (each expressed as a binned count i.e. low: [0,1) operations, medium: [1,3) operations, high: [3,\u221e) operations).\nWe show that there is a wide variety of performances, in Table 16, with some workers getting over 75% of the operations correct and others getting \u2248 30% correct. Interestingly, we see that there is a learning process occurring. In Figure 10, we see that workers get better over time as they do more tasks. This indicates that the training procedure of letting them see the edits that actually happened is successful at teaching them the style and patterns the edits will take.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": ["tab_0", "tab_0"]}, {"heading": "G.4 Annotator Interview 1", "text": "This annotator was involved in the Editing task. They edited 50 stories.\n1. As I was going through, I almost always decided to edit the lede, and was almost always correct with that. Most leads, I thought, could be more efficient, they could incorporate more details from further down in the story into the lede. Also, as stories unfolded, the actor responsible for the event becomes clear, that information will get added to the lede. For example, a building collapses in Manhattan -> faulty beam causes the building collapse. This detail often only becomes apparent afterwards.\nWhat I realized doing this was that there are different genres of breaking news article, and genre matters a lot for how it gets updated. These are the following categories:\n(a) Stories where the future is contingent, and you're making predictions in realtime. ex) A sailor went missing off the isle of Mann. This story is fundamentally about an unknown -will he be discovered or not? This is one of the harder ones to figure out how to update. How it plays out determines how it will be updated. If the search goes on for a long time, you'll have more details, you'll have quotes from his family, conditions on the water. If he's found, this stuff becomes irrelevant. You'll have information about how he gets found, then you'll have information about how many people get updated. ex) A story was about \"Trump is about to make a speech\". \"Trump expected to speak\". I updated it as if event didn't happen yet. But the real update actually contained him speaking. Stories about when multiple futures can happen, without knowing the timescale of the update, are difficult to predict. I determined whether an event was unfolding by looking for several clues. I looked for certain words: \"expected\", \"scheduled\", etc. Usually this signals an event-update. I looked for stories where there's a ton of uncertainty.\nAnother clue was that the only sources are official statements (ex. \"Officials in Yemen say something happened\".) The space of possible change increases. You're going to get conflicting reports, eye-witnesses contradicting official statements. Some articles included direct appeals to readers-\"don't use the A4 if you're traveling between London, etc.\" For crime articles: \"if you have any information, please contact agency.\" This kind of direct appeal is not relevant in the next version. (b) Past stories when the event is totally in the past.\nFor these stories, I looked for vagueness of the original article to determine what would be updated. If it's more specific, for example, with exact death toll numbers, information about specific actors and victims, the less it's going to be updated. For these stories, my tendency was to add at least 1-2 sentences of context towards the end of every story. If you're writing for Reuters, you might not need that.\nIn general, I wanted to see some background, people involved.\nThe quotes you're getting, are they press releases or are they directly from people? If they more official statements and press releases, then you'll see more updates in the form of specific victim quotes. One general note: most breaking stories were about bad things. Disasters, crashes, missing people, etc. For a bombing, there's a pretty predictable pattern of expansion. Death toll will get added, more eyewitness accounts. It has an expansionary trajectory. 2. How did you determine if a sentence needed to be added? I decided to add anywhere I saw vagueness. I added a lot towards the beginning, right after the nut graf is where I added the most sentences. If I saw a sentence taken from a press release, I added after that, assuming that the journalist would get a more fleshed-out quote from someone.\nOften I added [sentences] at the end to add context. I never added something before the lead.\nMaybe a story has two ideas, then I'd add sentences to the second half to flesh out a second idea.\nSometimes I thought about different categories of information-quotes, analysis, etc.-and it was obvious if some of that was missing.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "How did you determine if a sentence needed to be deleted?", "text": "I very rarely thought things needed to be deleted One of the challenges of the experiment was that it was hard to indicate how to combine sentences. I got around this by hitting \"edit\" for sentences that needed to be combined. Then I'd delete ones below, assuming that the edited sentence would include a clause from the sentence below it.\nStructural sentences and cues got deleted often. Sentences like \"More follows\", etc. Nothing integral to the substance of the story.\nI noticed that almost always, [informational content of sentences that had been deleted] had been reincorporated. 4. How did you determine if a sentence needed to be moved up/down? I did this by feel, what seemed important. One example: A building collapse in Morocco. A sentence way towards the end had a report about weak foundations, that needed to be brought up. This indicated that the journalist became more confident about something\nThe inverted pyramid so widely used, in a breaking news it's fairly easy to weight the importance of different elements. Thus, I rarely felt the need to move items upwards.\nSometimes I saw examples of when what was initially a small quote from official was expanded in a later version. Then, it was brought up because the quote became more important. But usually, my instinct would not be to move quotes from officials up.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Did it help to see what actually happened after you finished the task?", "text": "Usually there was 1-2 things that we had done that were basically the same.\nA couple of times, [I] was satisfied to see that the updated story made the same decision to switch sentences around.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Any general closing thoughts?", "text": "Most interesting thing was to see how formally constrained journalists and editors are, and how much these forms and genres shape your thought and your work.\nThere are assumptions get baked into the genres about who's credible, what kinds of things carry weight, sorts of outcomes deserve special attention, a whole epistemic framework.\nEven though there's a lot of variation, there's a fair amount of consistency.\nI was disappointed that, especially for rapidly expanding stories, the edits were mainly causes and main events. I saw very few structural, causal analyses added to breaking stories. There was some analysis that got added to one story about bombings in the Middle East, but still, not a whole lot about how the specific conflict originated.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G.5 Annotator Interview 2", "text": "This annotator was involved in both the editing task and the version-prediction task. They annotated over 100 examples of the first task, and 50 of the second.\n1. What was your general thought process while doing the edits task?\nFirst, before starting, I made the assumption that every story would need edits, because I think everything could always use more work. In reality, if the article wasn't updated the way it was, I was representing one option. My process was:\n(a) Read the whole story, don't make any changes at first. (b) Then, I would think about what I thought was the most important sentence. (c) I would often pull that high up into the lede, and then I'd add a sentence before or after. The factors that determined the most important part of the article were:\n(a) Some indication of harm done or the most recent development. I always took \"harm done\" as the most important part of a story. For example: Death count-20 people were killed in some explosion vs. a bomb went off here. Moved the \"20 people killed\" higher because that was a harm ex. Officials are investigating whether so-and-so doctored documents. (b) Then, I would add/delete and edit based on these. So, I would create a new sentence and edit the next sentence to give more context. 2. How did you determine if a sentence needed to be added?\nSo, after identifying the lede that I described previously, I went through and looked through what parts I felt needed more context or a quote. Getting quotes was very important. Often I identified events that I thought warranted a reaction, acknowledgment, information from a source. If these weren't there, I added a sentence. I didn't keep a checklist of these elements (i.e. \"quote\", \"context\", etc.) It was more a gut feeling about what it needed. If I were going back and doing it again, I would write out a checklist.\nOften, especially when the news was unpredictable, I would often add a sentence in the beginning saying \"I don't know what this sentence is going to be, but it's going to be something\". In other words, I was adding context to what the unknown would be. I was able to do this pretty successfully, to predict what context would happen around the unpredictable event.\nWhere I tried to add more information to flesh out certain unknowns:\n(a) If an official said something that needed to be followed up on, I would delete these and add new sentences (b) I had hoped that the reporter would get that information themselves through eyewitnesses, court documents, etc. (c) Sometimes an official would give filler quotes like: \"we'll have more information later this afternoon\". These would be replaced with the actual update. (d) Context: I would add historical context. How often has something been occurring in this area, etc. Many of real updates did have these contextual sentences. 3. How did you decide whether a sentence needed to be edited?\nAfter I decided what would be moved up, I looked at details (dates, people, etc.). Sentences with details were the ones that were most likely to be edited. 4. How did you determine if a sentence needed to be deleted? I deleted sentences that were redundant. I identified filler quotes (e.g. officials saying they'll get more information soon.). These would be deleted when, presumably, more information did come in. Sometimes a quote was redundant to a sentence that was already there. One of the challenges was deciding when to delete or edit a sentence. 5. How did you determine if a sentence needed to be moved up/down?\nI almost always moved sentences upwards, to the top. As we discussed previously, the top then needs to have room for an update. Again, as we discussed previously, I used harm and recent developments as a metric to decide where to move. The context was also moved around based on when the events took place.\nI also tried to focus on recent developments. For example: \"Officials are investigating whether so-and-so doctored documents\". I would move that to the top. I pulled up the active part of the article to express what was actually happening. 6. What things did you get wrong?\nI was really bad at predicting stories that were \"delete all\", \"replace all\". I struggled more with stories that were about political leaders speaking at an event or speaking at a conference, because these ended up going different ways. Sometimes they made a big announcement that would make headlines, but it was hard to known beforehand what that announcement would be.\nFor crime, or spot news, it was clearer that an event was unfolding and would have specific updates. By \"spot news\", I mean stories about crimes, fires, rescues, weather events/disasters, etc. -something unexpected as opposed to articles about events that have been planned, like the example of a political figure speaking at a conference. It was these unexpected events that actually follow more predictable paths when they unfold.\nI saw a lot of discrepancies between sentences I chose to edit, and then the actual result was that they got deleted. For example, the death toll was in a sentence, and I'd edit that sentence, but they chose to add a sentence with the same information. The sentence matching algorithm didn't do a good job with informational units that were not at the sentence level. 7. How did you assess uncertainty in an article?\nOften it was topic-based. I can't think of key indicators that I used to assess uncertainty.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Was really helpful after I made the edits to see what actually happened?", "text": "I tried to balanced this with what my natural instincts were. I did get better over time. I did feel more confident over time. The changes would be more in my decisions to edit vs. add/delete. In my head, I had the same end result in mind, but they edited it and I added a new sentence. I never felt I was widely off 9. Did you see a lot of analytical pieces? Or mainly breaking news?\nI saw a mix of stuff that was analytical vs. factual. There were certainly more breaking news events, events that were going to happen and change on the same day. However, I did see some day 2 stories. Sometimes, they were updates that were part of an ongoing investigation. The breaking stories and spot news, crime, were the easiest to do. Those ones seem much more formulaic. 10. What was your general thought process while doing the versioning task? How did you identify versions that updated?\nThis one was trickier because I would assume that everything would be updated, everything would be improved. The mindset change that I made was \"Will this story itself be edited, or will they write a followup with more information?\" Once I made this separation this became easier 11. What patterns did you observe in this task?\nThe timing of when I thought an update would occur ended up mattering a lot. I paid closer attention to stories that would have updates within the same day or a short period of time. The longer the time-periods between updates, the more likely a new piece would be published instead of an update.\nAgain, crime and spot news it was clear -the person was on the scene at this minute, they'd get more information.\nThe other giveaways were \"so and so is expected to deliver remarks later this afternoon.\" It wasn't quite a preview of the event but it would clearly be updated\nThe other thing that made me choose to mark a story as \"would be updated\" is if there was a key perspective missing or if there was no quotes at all. By \"key perspective\", I mean, a key quote from a participant that is usually present in this type of story. For a crime, for example, this included: Law enforcement perspective, witness, family. In general, it means that both sides are represented. 12.\nWere there examples that you thought would update that didn't? There were some with stock figures, quarterly earnings, that I initially thought would be updated, but I had seen the examples that were filled out, but I'd be more accepting that this was a final report and that it's not going to have any quotes. I became better at identifying which types of pieces wouldn't have context or quotes. 13. Anything I may have missed?\nI tried to flag a couple of articles that transferred over inaccurately. Sometimes there were cases of where one article published to the same URL was something completely different. Sometimes there were calls for subscribing to newsletters or related story links. I deleted ones that were repetitive. This might have influenced results on some articles. These structural updates were annoying. 14. Could you see solving this kind of prediction task as being useful in a newsroom?\nI could see it being used as a people management tool. Newsrooms are desperate for any kind of methodology to guide the decisions they make. Deciding who should attack a new story, and who should stay put working on their old piece would help a lot!", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Algorithm 2: Identifying Refactors. We define refactors as the minimal set of edge crossings in a bipartite graph which, when removed, remove all edge crossings.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "input :Sentence matches, i.e. edges e between doc i and doc j, as a list of tuples: e i = (s i1 ,s i2 ),e j = (s j1 ,s j2 ).... output :Minimal set of edges r that, when removed, eliminate all crossings. // Subroutine identifies all edge crossings in e \u2032 and returns mapping c = {e i \u2192 [e j ,e k ...],e j \u2192 ...} from each edge to all its crossings. c = getEdgeCrossings(e) while c > 0 do // Find candidate set: all edges with maximum crossings.\ni where c[e \u2032 i ] = m if e \u2032 > 1 then // Filter candidate set: all edges \u2208 e \u2032 that extend the maximum distance.\ni where ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Sentence similarity estimation for text summarization using deep learning", "journal": "Springer", "year": "2019", "authors": "Mahmudul Sheikh Abujar; Syed Hasan;  Akhter Hossain"}, {"ref_id": "b1", "title": "Annotation and classification of evidence and reasoning revisions in argumentative writing", "journal": "", "year": "2020", "authors": "Tazin Afrin; Elaine Lin Wang; Diane Litman; Lindsay Clare Matsumura; Richard Correnti"}, {"ref_id": "b2", "title": "2020. wikiHowToImprove: A resource and analyses on edits in instructional texts", "journal": "European Language Resources Association", "year": "", "authors": "Talita Anthonio; Irshad Bhat; Michael Roth"}, {"ref_id": "b3", "title": "Do news corrections affect credibility? not necessarily", "journal": "Newspaper Research Journal", "year": "2015", "authors": "Alyssa Appelman; Kirstie Hettinga"}, {"ref_id": "b4", "title": "Towards modeling revision requirements in wikiHow instructions", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Irshad Bhat; Talita Anthonio; Michael Roth"}, {"ref_id": "b5", "title": "Latent dirichlet allocation", "journal": "J. Mach. Learn. Res", "year": "2003", "authors": "David M Blei; Andrew Y Ng; Michael I Jordan"}, {"ref_id": "b6", "title": "The narrative arc: Revealing core narrative structures through text analysis", "journal": "Science advances", "year": "2020", "authors": "L Ryan; Kate G Boyd; James W Blackburn;  Pennebaker"}, {"ref_id": "b7", "title": "A zipf-like distant supervision approach for multidocument summarization using wikinews articles", "journal": "Springer", "year": "2012", "authors": "Felipe Bravo; - Marquez; Manuel Manriquez"}, {"ref_id": "b8", "title": "Insider's view of changes, from outside. The New York Times", "journal": "", "year": "2012", "authors": "Arthur S Brisbane"}, {"ref_id": "b9", "title": "Newsdiffs: A tool for tracking changes to online news articles -vr research -public records research: Opposition research", "journal": "", "year": "2016", "authors": "Austin Burke"}, {"ref_id": "b10", "title": "Building a discourse-tagged corpus in the framework of rhetorical structure theory", "journal": "Springer", "year": "2003", "authors": "Lynn Carlson; Daniel Marcu; Mary Ellen Okurowski"}, {"ref_id": "b11", "title": "Incorporating structured commonsense knowledge in story completion", "journal": "", "year": "2019", "authors": "Jiaao Chen; Jianshu Chen; Zhou Yu"}, {"ref_id": "b12", "title": "Sentence similarity measures revisited: ranking sentences in pubmed documents", "journal": "", "year": "2018", "authors": "Qingyu Chen; Sun Kim; John Wilbur; Zhiyong Lu"}, {"ref_id": "b13", "title": "Discourse as a function of event: Profiling discourse structure in news articles around the main event", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Prafulla Kumar Choubey; Aaron Lee; Ruihong Huang; Lu Wang"}, {"ref_id": "b14", "title": "Computational journalism", "journal": "Communications of the ACM", "year": "2011", "authors": "Sarah Cohen; T James; Fred Hamilton;  Turner"}, {"ref_id": "b15", "title": "A corpus-based study of edit categories in featured and non-featured Wikipedia articles", "journal": "", "year": "2012", "authors": "Johannes Daxenberger; Iryna Gurevych"}, {"ref_id": "b16", "title": "Automatically classifying edit categories in wikipedia revisions", "journal": "", "year": "2013", "authors": "Johannes Daxenberger; Iryna Gurevych"}, {"ref_id": "b17", "title": "The epistemologies of breaking news", "journal": "Journalism Studies", "year": "2021", "authors": "Mats Ekstr\u00f6m; Amanda Rams\u00e4lv; Oscar Westlund"}, {"ref_id": "b18", "title": "WikiAtomicEdits: A multilingual corpus of Wikipedia edits for modeling language and discourse", "journal": "", "year": "2018", "authors": "Manaal Faruqui; Ellie Pavlick; Ian Tenney; Dipanjan Das"}, {"ref_id": "b19", "title": "Revealing the news: How online news changes without you noticing", "journal": "Digital Journalism", "year": "2014", "authors": "John Fass; Angus Main"}, {"ref_id": "b20", "title": "Disinformation and social bot operations in the run up to the 2017 french presidential election", "journal": "", "year": "2017", "authors": ""}, {"ref_id": "b21", "title": "What did they do? deriving high-level edit histories in wikis", "journal": "", "year": "2010", "authors": "Peter Kin-Fong Fong; Robert P Biuk-Aghai "}, {"ref_id": "b22", "title": "Style transfer in text: Exploration and evaluation", "journal": "", "year": "2018", "authors": "Zhenxin Fu; Xiaoye Tan; Nanyun Peng; Dongyan Zhao; Rui Yan"}, {"ref_id": "b23", "title": "The Associate Press Rules Regulations and General Orders", "journal": "", "year": "1953", "authors": "Norm Goldstein"}, {"ref_id": "b24", "title": "Why 'diffing' could make news organizations more transparent", "journal": "Columbia Journalism Review", "year": "2015", "authors": "Chava Gourarie"}, {"ref_id": "b25", "title": "The wiked error corpus: A corpus of corrective wikipedia edits and its application to grammatical error correction", "journal": "Springer", "year": "2014", "authors": "Roman Grundkiewicz; Marcin Junczys-Dowmunt"}, {"ref_id": "b26", "title": "Predicting and understanding news social popularity with emotional salience features", "journal": "", "year": "2019", "authors": "Raj Kumar Gupta; Yinping Yang"}, {"ref_id": "b27", "title": "Deep structured neural network for event temporal relation extraction", "journal": "", "year": "2019", "authors": "Rujun Han; I-Hung Hsu; Mu Yang; Aram Galstyan; Ralph Weischedel; Nanyun Peng"}, {"ref_id": "b28", "title": "Joint event and temporal relation extraction with shared representations and structured prediction", "journal": "", "year": "2019", "authors": "Rujun Han; Qiang Ning; Nanyun Peng"}, {"ref_id": "b29", "title": "Local breaking news: Sources, technology, and news routines", "journal": "Journalism Quarterly", "year": "1994", "authors": "A Kathleen; Jean Hansen; Joan L Ward; Mark Conners;  Neuzil"}, {"ref_id": "b30", "title": "A retrieve-and-edit framework for predicting structured outputs", "journal": "Curran Associates Inc", "year": "2018", "authors": "B Tatsunori; Kelvin Hashimoto; Yonatan Guu; Percy Oren;  Liang"}, {"ref_id": "b31", "title": "Sniffing out edits. BBC", "journal": "", "year": "2006", "authors": "Steve Herrmann"}, {"ref_id": "b32", "title": "Proofread sentence generation as multi-task learning with editing operation prediction", "journal": "Short Papers", "year": "2017", "authors": "Yuta Hitomi; Hideaki Tamori; Naoaki Okazaki; Kentaro Inui"}, {"ref_id": "b33", "title": "TinyBERT: Distilling BERT for natural language understanding", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Xiaoqi Jiao; Yichun Yin; Lifeng Shang; Xin Jiang; Xiao Chen; Linlin Li; Fang Wang; Qun Liu"}, {"ref_id": "b34", "title": "The effect of new york times event coding techniques on social movement analyses of protest data", "journal": "Emerald Group Publishing Limited", "year": "2016", "authors": "Jonathan P Erik W Johnson; Jon Schreiner;  Agnone"}, {"ref_id": "b35", "title": "Using RSS to improve web harvest results for news web sites", "journal": "Journal of Western Archives", "year": "2017", "authors": "M Gina; Michael Jones;  Neubert"}, {"ref_id": "b36", "title": "Building a monolingual parallel corpus for text simplification using sentence similarity based on alignment between word embeddings", "journal": "", "year": "2016", "authors": "Tomoyuki Kajiwara; Mamoru Komachi"}, {"ref_id": "b37", "title": "US newspaper industry in transition", "journal": "DIANE Publishing", "year": "2010", "authors": "M Suzanne;  Kirchhoff"}, {"ref_id": "b38", "title": "The hungarian method for the assignment problem", "journal": "", "year": "1955", "authors": " Harold W Kuhn"}, {"ref_id": "b39", "title": "Automated grammatical error detection for language learners", "journal": "Synthesis lectures on human language technologies", "year": "2010", "authors": "Claudia Leacock; Martin Chodorow; Michael Gamon; Joel Tetreault"}, {"ref_id": "b40", "title": "The thirst to be first: An analysis of breaking news stories and their impact on the quality of 24-hour news coverage in the uk", "journal": "Journalism Practice", "year": "2009", "authors": "Justin Lewis; Stephen Cushion"}, {"ref_id": "b41", "title": "Scientific discourse tagging for evidence extraction", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Xiangci Li; Gully Burns; Nanyun Peng"}, {"ref_id": "b42", "title": "Dice loss for dataimbalanced NLP tasks", "journal": "", "year": "2020", "authors": "Xiaoya Li; Xiaofei Sun; Yuxian Meng; Junjun Liang; Fei Wu; Jiwei Li"}, {"ref_id": "b43", "title": "Annual Meeting of the Association for Computational Linguistics", "journal": "Online. Association for Computational Linguistics", "year": "", "authors": ""}, {"ref_id": "b44", "title": "Roberta: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b45", "title": "On efficient training, controllability and compositional generalization of insertion-based language generators", "journal": "", "year": "2021", "authors": "Sidi Lu; Nanyun Peng"}, {"ref_id": "b46", "title": "Eventplus: A temporal event understanding pipeline", "journal": "", "year": "2021", "authors": "Derek Mingyu; Jiao Ma; Mu Sun; Kung-Hsiang Yang; Nuan Huang; Shikhar Wen; Rujun Singh; Nanyun Han;  Peng"}, {"ref_id": "b47", "title": "Building a large annotated corpus of english: The penn treebank", "journal": "", "year": "1993", "authors": "Mitchell Marcus; Beatrice Santorini; Mary Ann Marcinkiewicz"}, {"ref_id": "b48", "title": "Learning to describe editing activities in collaborative environments: A case study on github and wikipedia", "journal": "", "year": "2020", "authors": "Edison Marrese-Taylor; Pablo Loyola; Jorge A Balazs; Yutaka Matsuo"}, {"ref_id": "b49", "title": "Man is to person as woman is to location: Measuring gender bias in named entity recognition", "journal": "", "year": "2020", "authors": "Ninareh Mehrabi; Thamme Gowda; Fred Morstatter; Nanyun Peng; Aram Galstyan"}, {"ref_id": "b50", "title": "Meantime, the newsreader multilingual event and time corpus", "journal": "", "year": "2016", "authors": "Anne-Lyse Minard; Manuela Speranza; Ruben Urizar; Begona Altuna; Marieke Van Erp; Anneleen Schoen; Chantal Van Son"}, {"ref_id": "b51", "title": "A corpus and cloze evaluation for deeper understanding of commonsense stories", "journal": "", "year": "2016", "authors": "Nasrin Mostafazadeh; Nathanael Chambers; Xiaodong He; Devi Parikh; Dhruv Batra; Lucy Vanderwende; Pushmeet Kohli; James Allen"}, {"ref_id": "b52", "title": "Lsdsem 2017 shared task: The story cloze test", "journal": "", "year": "2017", "authors": "Nasrin Mostafazadeh; Michael Roth; Annie Louis; Nathanael Chambers; James Allen"}, {"ref_id": "b53", "title": "Understanding journalism through a nuanced deconstruction of temporal layers in news narratives", "journal": "Journal of Communication", "year": "2016", "authors": "Motti Neiger; Keren Tenenboim-Weinblatt"}, {"ref_id": "b54", "title": "The uncertain future of local journalism. Pre-publication version of chapter", "journal": "", "year": "2015", "authors": " Rasmus Kleis Nielsen"}, {"ref_id": "b55", "title": "A multiaxis annotation scheme for event temporal relations", "journal": "", "year": "2018", "authors": "Qiang Ning; Hao Wu; Dan Roth"}, {"ref_id": "b56", "title": "Bleu: a method for automatic evaluation of machine translation", "journal": "", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"ref_id": "b57", "title": "Towards controllable story generation", "journal": "", "year": "2018", "authors": "Nanyun Peng; Marjan Ghazvininejad; Jonathan May; Kevin Knight"}, {"ref_id": "b58", "title": "News and its communicative quality: the inverted pyramid-when and why did it appear?", "journal": "Journalism Studies", "year": "2003", "authors": "Horst P\u00f6ttker"}, {"ref_id": "b59", "title": "The timebank corpus", "journal": "", "year": "2003", "authors": "James Pustejovsky; Patrick Hanks; Roser Sauri; Andrew See; Robert Gaizauskas; Andrea Setzer; Dragomir Radev; Beth Sundheim; David Day; Lisa Ferro"}, {"ref_id": "b60", "title": "An efficient framework for sentence similarity modeling", "journal": "", "year": "2019", "authors": "Zhe Quan; Zhi-Jie Wang; Yuquan Le; Bin Yao; Kenli Li; Jian Yin"}, {"ref_id": "b61", "title": "Sentence-bert: Sentence embeddings using siamese bert-networks", "journal": "", "year": "2019", "authors": "Nils Reimers; Iryna Gurevych"}, {"ref_id": "b62", "title": "Authorship attribution based on a probabilistic topic model. Information Processing & Management", "journal": "", "year": "2013", "authors": "Jacques Savoy"}, {"ref_id": "b63", "title": "Writing from the top down: Pros and cons of the inverted pyramid", "journal": "", "year": "2003", "authors": "Chip Scanlan"}, {"ref_id": "b64", "title": "Automatic fact-guided sentence modification", "journal": "", "year": "2020", "authors": "Darsh Shah; Tal Schuster; Regina Barzilay"}, {"ref_id": "b65", "title": "Connecting the dots between news articles", "journal": "", "year": "2010", "authors": "Dafna Shahaf; Carlos Guestrin"}, {"ref_id": "b66", "title": "Intellimerge: a refactoring-aware software merging technique", "journal": "Proceedings of the ACM on Programming Languages", "year": "2019", "authors": "Bo Shen; Wei Zhang; Haiyan Zhao; Guangtai Liang; Zhi Jin; Qianxiang Wang"}, {"ref_id": "b67", "title": "Recent advances on neural headline generation", "journal": "Journal of computer science and technology", "year": "2017", "authors": " Shi-Qi; Yan-Kai Shen; Cun-Chao Lin; Yu Tu; Zhi-Yuan Zhao; Mao-Song Liu;  Sun"}, {"ref_id": "b68", "title": "Refdiff: detecting refactorings in version histories", "journal": "IEEE", "year": "2017", "authors": "Danilo Silva; Marco Tulio Valente"}, {"ref_id": "b69", "title": "don't quote me on that\": Finding mixtures of sources in news articles", "journal": "", "year": "2020", "authors": "Alexander Spangher; Jonathan May; Emilio Ferrara; Nanyun Peng"}, {"ref_id": "b70", "title": "Multitask semi-supervised learning for class-imbalanced discourse classification", "journal": "Association for Computational Linguistics", "year": "", "authors": "Alexander Spangher; Jonathan May"}, {"ref_id": "b71", "title": "what's the diff?\": Examining news article updates and changing narratives during the uss theodore roosevelt coronavirus crisis", "journal": "", "year": "2021", "authors": "Alexander Spangher; Amberg-Lynn Scott; Ke Huang-Isherwood"}, {"ref_id": "b72", "title": "On the difference or equality of information, misinformation, and disinformation: A critical research perspective. Informing Science", "journal": "", "year": "2006", "authors": "Bernd Carsten Stahl"}, {"ref_id": "b73", "title": "Analyzing the revision logs of a Japanese newspaper for article quality assessment", "journal": "", "year": "2017", "authors": "Hideaki Tamori; Yuta Hitomi; Naoaki Okazaki; Kentaro Inui"}, {"ref_id": "b74", "title": "A survey of refactoring detection tools", "journal": "", "year": "2019", "authors": "Liang Tan; Christoph Bockisch"}, {"ref_id": "b75", "title": "Fred Morstatter, and Nanyun Peng. 2020a. Identifying cultural differences through multi-lingual wikipedia", "journal": "", "year": "", "authors": "Yufei Tian; Tuhin Chakrabarty"}, {"ref_id": "b76", "title": "Scene restoring for narrative machine reading comprehension", "journal": "", "year": "2020", "authors": "Zhixing Tian; Yuanzhe Zhang; Kang Liu; Jun Zhao; Yantao Jia; Zhicheng Sheng"}, {"ref_id": "b77", "title": "Accurate and efficient refactoring detection in commit history", "journal": "IEEE", "year": "2018", "authors": "Nikolaos Tsantalis; Matin Mansouri; Laleh Eshkevari"}, {"ref_id": "b78", "title": "Breaking news production processes in us metropolitan newspapers: Immediacy and journalistic authority", "journal": "Journalism", "year": "2018", "authors": "Nikki Usher"}, {"ref_id": "b79", "title": "Discourse analysis: Its development and application to the structure of news", "journal": "Journal of communication", "year": "1983", "authors": " Teun A Van Dijk"}, {"ref_id": "b80", "title": "ACE 2005 multilingual training corpus LDC2006T06. Linguistic Data Consortium", "journal": "", "year": "2006", "authors": "Christopher Walker; Stephanie Strassel; Julie Medero; Kazuaki Maeda"}, {"ref_id": "b81", "title": "2020. eRevis(ing): Students' revision of text evidence use in an automated writing evaluation system", "journal": "Assessing Writing", "year": "", "authors": "Elaine Lin Wang; Lindsay Clare Matsumura; Richard Correnti; Diane Litman; Haoran Zhang; Emily Howe; Ahmed Magooda; Rafael Quintana"}, {"ref_id": "b82", "title": "2020. transformers: State-of-the-art natural language processing", "journal": "Association for Computational Linguistics", "year": "", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; Remi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger;  Drame"}, {"ref_id": "b83", "title": "Wn-salience: A corpus of news articles with entity salience annotations", "journal": "", "year": "2020", "authors": "Chuan Wu; Evangelos Kanoulas; Wei Maarten De Rijke;  Lu"}, {"ref_id": "b84", "title": "Understanding multimodal procedural knowledge by sequencing multimodal instructional manuals", "journal": "", "year": "2022", "authors": "Te-Lin Wu; Alex Spangher; Pegah Alipoormolabashi; Marjorie Freedman; Ralph Weischedel; Nanyun Peng"}, {"ref_id": "b85", "title": "Commit message generation for source code changes", "journal": "", "year": "2019", "authors": "Shengbin Xu; Yuan Yao; Feng Xu; Tianxiao Gu; Hanghang Tong; Jian Lu"}, {"ref_id": "b86", "title": "Identifying semantic edit intentions from revisions in wikipedia", "journal": "", "year": "2017", "authors": "Diyi Yang; Aaron Halfaker; Robert Kraut; Eduard Hovy"}, {"ref_id": "b87", "title": "A novel sentence similarity model with word embedding based on convolutional neural network. Concurrency and Computation: Practice and Experience", "journal": "", "year": "2018", "authors": "Haipeng Yao; Huiwen Liu; Peiying Zhang"}, {"ref_id": "b88", "title": "Planand-write: Towards better automatic storytelling", "journal": "", "year": "2019", "authors": "Lili Yao; Nanyun Peng; Ralph Weischedel; Kevin Knight; Dongyan Zhao; Rui Yan"}, {"ref_id": "b89", "title": "For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia", "journal": "Association for Computational Linguistics", "year": "2010", "authors": "Mark Yatskar; Bo Pang; Cristian Danescu-Niculescu-Mizil; Lillian Lee"}, {"ref_id": "b90", "title": "Cwig3g2-complex word identification task across three text genres and two user groups", "journal": "Short Papers", "year": "2017", "authors": "Sanja Seid Muhie Yimam; Martin \u0160tajner; Chris Riedl;  Biemann"}, {"ref_id": "b91", "title": "Learning to represent edits", "journal": "", "year": "2018", "authors": "Pengcheng Yin; Graham Neubig; Miltiadis Allamanis; Marc Brockschmidt; Alexander L Gaunt"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1:We identify sentence-level operations -Edit, Addition, Deletion and Refactor -between two versions of a news article (merges, shown here, and splits are a special cases of Edits). We propose tasks aimed at predicting these operations on article versions. We characterize aspects of additions, deletions and edits. We hope NewsEdits can contribute to research on narrative and factual development patterns.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Number of versions per article, by outlet.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Dynamics of edit actions.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 4 :4Figure 4:Architecture diagram for the model used for our tasks. Word-embeddings are averaged using Self-Attention to form sentence-vectors. A minimal transformer layer is used to contextualize these vectors (+Contextual Layer). In Tasks 1 and 2, self-attention is used to generate a document-embedding vector.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure 5: Percentage of the training dataset for Task 1 which contains y = 1, or where another version of the article has been published.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "n a ti o n a lp o s t to r o n to s u n c a lg a r y h e r a ld la p r e s s e to r o n to s ta r Distribution over days per update, group 4. Median across all sources in this group is .05 days, or 1.33 hours.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 6 :6Figure6: Average time between version updates. We break sources into four primary groups with similar update distributions.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Odds of discourse element, by version. odds = p(d v) p(d !v). Odds of discourse element in added sentences. odds = p(d v, a) p(d v, !a). Odds of discourse element in deleted sentences. odds = p(d v, del) p(d v, !del).", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 7 :7Figure 7: Dynamics of news discourse composition size across time. d refers to discourse label, v refers to version and a, del refer to is_added, is_deleted", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 9 :9Figure9: Example of Editing Task. The gray boxes on the left serve as a reference for how the original article was written. The sandbox on the right is where annotators actually perform the task. The first sentence has been Edited, two sentences have been Added, the third has been Deleted and the fourth has been Refactored downwards.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "F1 scores on validation data for matching algorithms. Left-hand group shows embedding-based methods (TinyBert (TB) and RoBERTa (RB)) with Maximum or Hungarian matching. Middle group shows ngram methods. Right-hand group shows BLEU for different ngram weightings (1,2 and 1,2,3 are uniform weightings over unigrams, bigrams and trigrams).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Total Num. % of Sents.", "figure_data": "Edits26.6 mil.17.6 %Additions10.2 mil.6.8 %Deletions5.4 mil.3.6 %Refactors1.6 mil.1.1 %"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Summary Statistics for Sentence Operations", "figure_data": "% Sents in Vers.10 200Version num 20 Edits Additions Deletions(a) Edit-actions by version(% total in version)."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ": % Additions, Deletions or Unchanged sen-tences that contain Events or Quotes, or have newsdiscourse role: Main (main events), Cause (immedi-ate context) or Distant (history, analysis). F < .01, n = 7,368,634."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Selection of top event extracted from edited sentence pairs across article versions.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": "([0,1) sentences), \"medium\""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Error Analysis: LDA (first two columns): Documents belonging to some topics are easier to predict than others. By label (last column): medium-range growth is easier to predict.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Fabio Massimo Zanzotto and Marco Pennacchiotti. 2010. Expanding textual entailment corpora from wikipedia using co-training. In Proceedings of the 2nd Workshop on The People's Web Meets NLP: Collaboratively Constructed Semantic Resources, pages 28-36.", "figure_data": "Fan Zhang, Homa B Hashemi, Rebecca Hwa, and Di-ane Litman. 2017. A corpus of annotated revisionsfor studying argumentative writing. In Proceedingsof the 55th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 1568-1578.Fan Zhang and Diane Litman. 2015. Annotation andclassification of argumentative writing revisions. InProceedings of the Tenth Workshop on InnovativeUse of NLP for Building Educational Applications,pages 133-143, Denver, Colorado. Association forComputational Linguistics."}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Topic Model: Top Topics, selected on the bases of the number of documents they are most-expressed in. Labels are assigned by the researchers post-hoc. Several topics appear to be subsets of a broader Crime topic: we note the superclass Crime in parentheses. The specific Crime topic mentioned in the main body is the Violence topic(Topic 18)    ", "figure_data": ""}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "Top Words in Additions/Deletions vs. top words in unchanged sentences.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "Here are some examples of contributor lines:\u2022 Additional reporting by Simon Browning.\u2022 'The article relied heavily on reporting byReuters and the BBC, and it cited Reuters in saying that during a visit in October 1989", "figure_data": "by Pope John Paul II to South Korea, Chinahad prevented the pope's airplane from flyingthrough Chinese airspace.\u2022 The revelation comes after reporting by TheNew York Times last week showing that thehead of communications at the N.I.H.'s parentagency, the Department of Health and HumanServices, also accused federal scientists of us-ing the coronavirus to try to defeat Mr. Trump.\u2022 Additional reporting by Daniel Strauss in Rich-mond, Virginia, Richard Luscombe in WestPalm Beach, Florida, and Ed Pilkington in Es-sex Junction, Vermont."}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "A summary of the number of total number of articles and versions for different media outlets which comprise our dataset. Also shown is the original collection that they were derived from (DE for DiffEngine, and NS from NewsSniffer), and the date-ranges during which articles from each outlet were collected.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "DB schema for the article table. SOURCE, A_ID and VERSION_ID are the primary key columns. DB schema for the sentence_diffs table (word_diffs is similar). Tablecomparesversion pairs of articles. The rows in the table are on the sentence-level; V_OLD_ID refers to the index of the old version, V_NEW_ID refers to the index of the new version. TAG_OLD gives information for how to transition from the old version to the new version; TAG_NEW is the inverse.", "figure_data": "Column NameTypeColumn NameTypeColumn NameTypeSOURCEindexTITLEtextCREATEDtextA_IDindexURLtextARCHIVE_URLtextVERSION_IDindexTEXTtextNUM_VER-intSIONS(a) Column NameTypeColumn NameTypeColumn NameTypeSOURCEindexV_NEW_IDindexTAG_OLDtextA_IDindexSENTENCE_IDindexSENT_NEWtextV_OLD_IDindexSENT_OLDtextTAG_NEWtext(b)"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_21", "figure_caption": "Schemas for two databases central to our content organization scheme.", "figure_data": ""}, {"figure_label": "13", "figure_type": "table", "figure_id": "tab_23", "figure_caption": "A comparison of natural langauge revision history corpora.", "figure_data": ""}, {"figure_label": "14", "figure_type": "table", "figure_id": "tab_25", "figure_caption": "Here we show demos of three tricky edge-cases and how our tagging scheme handles them. Old Tag annotates a Old Version relative to changes in the New Version (or \"converts\" the Old Version to the New Version). New Tag is the inverse.", "figure_data": ""}, {"figure_label": "15", "figure_type": "table", "figure_id": "tab_27", "figure_caption": "Count of Tasks Completed per worker", "figure_data": "the Submit button and please check tosee what the actual edits were so you canimprove for next task!"}, {"figure_label": "16", "figure_type": "table", "figure_id": "tab_29", "figure_caption": "Accuracy", "figure_data": "across document tasks (i.e. % binscorrect across document-level subtasks: Added, Edited,Deleted, Refactored).Task Accuracy0.4 0.6 0.8A1685VEOIJIUMR A17GX84A96WF6C A2E8P5A3IKROKB A2USH7VYFMU1ME A30BGCC8EC1NW ASQL7ZBXI7WF62040 Task Index 6080 100Figure 10: Worker Accuracy over time, by task"}], "formulas": [{"formula_id": "formula_0", "formula_text": "Sim asym (x,y) = 1 x x i=1 max j \u03c6 (x i ,y j )", "formula_coordinates": [3.0, 339.69, 359.46, 151.19, 32.7]}, {"formula_id": "formula_1", "formula_text": "\u03b2 i 1 > \u03b2 i 2 > ... > \u03b2 i k ).", "formula_coordinates": [18.0, 70.86, 239.3, 77.29, 22.38]}, {"formula_id": "formula_2", "formula_text": "input :Article versions v old , v new , Match Threshold T output :maps m old\u2192new , m old\u2190new initialize; m old\u2192new , m old\u2190new = {}, {}; // match v old \u2192 v new for (i,s i ) \u2208 v old do d = max s j \u2208vnew Sim asym (s i ,s j ) j = argmax s j \u2208vnew Sim asym (s i ,s j ) m old\u2192new [i] = j \u00d7 1[d > T ] end // match v old \u2190 v new for ( j,s j ) \u2208 v new do d = max s i \u2208v old Sim asym (s j ,s i ) i = argmax s i \u2208v old Sim asym (s j ,s i ) m old\u2190new [ j] = i \u00d7 1[d > T ]", "formula_coordinates": [22.0, 316.48, 298.69, 183.16, 214.34]}], "doi": "10.18653/v1/2020.emnlp-main.675"}