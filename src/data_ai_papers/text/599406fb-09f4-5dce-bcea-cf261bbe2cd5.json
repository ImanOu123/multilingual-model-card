{"title": "Deep Learning of Graph Matching", "authors": "Andrei Zanfir; Cristian Sminchisescu", "pub_date": "", "abstract": "The problem of graph matching under node and pairwise constraints is fundamental in areas as diverse as combinatorial optimization, machine learning or computer vision, where representing both the relations between nodes and their neighborhood structure is essential. We present an end-to-end model that makes it possible to learn all parameters of the graph matching process, including the unary and pairwise node neighborhoods, represented as deep feature extraction hierarchies. The challenge is in the formulation of the different matrix computation layers of the model in a way that enables the consistent, efficient propagation of gradients in the complete pipeline from the loss function, through the combinatorial optimization layer solving the matching problem, and the feature extraction hierarchy. Our computer vision experiments and ablation studies on challenging datasets like PASCAL VOC keypoints, Sintel and CUB show that matching models refined end-to-end are superior to counterparts based on feature hierarchies trained for other problems.", "sections": [{"heading": "Introduction and Related Work", "text": "The problem of graph matching -establishing correspondences between two graphs represented in terms of both local node structure and pair-wise relationships, be them visual, geometric or topological -is important in areas like combinatorial optimization, machine learning, image analysis or computer vision, and has applications in structure-from-motion, object tracking, 2d and 3d shape matching, image classification, social network analysis, autonomous driving, and more. Our emphasis in this paper is on matching graph-based image representations but the methodology applies broadly, to any graph matching problem where the unary and pairwise structures can be represented as deep feature hierarchies with trainable parameters.\nUnlike other methods such as RANSAC [12] or iterative closest point [4], which are limited to rigid displacements, graph matching naturally encodes structural infor-mation that can be used to model complex relationships and more diverse transformations. Graph matching operates with affinity matrices that encode similarities between unary and pairwise sets of nodes (points) in the two graphs. Typically it is formulated mathematically as a quadratic integer program [25,3], subject to one-to-one mapping constraints, i.e. each point in the first set must have an unique correspondence in the second set. This is known to be NPhard so methods often solve it approximately by relaxing the constraints and finding local optima [19,38].\nLearning the parameters of the graph affinity matrix has been investigated by [7,20] or, in the context of the more general hyper-graph matching model [10], by [21]. In those cases, the number of parameters is low, often controlling geometric affinities between pairs of points rather than the image structure in the neighborhood of those points. Recently there has been a growing interest in using deep features for both geometric and semantic visual matching tasks, either by training the network to directly optimize a matching objective [8,27,16,36] or by using pre-trained, deep features [23,14] within established matching architectures, all with considerable success.\nOur objective in this paper is to marry the (shallow) graph matching to the deep learning formulations. We propose to build models where the graphs are defined over unary node neighborhoods and pair-wise structures computed based on learned feature hierarchies. We formulate a complete model to learn the feature hierarchies so that graph matching works best: the feature learning and the graph matching model are refined in a single deep architecture that is optimized jointly for consistent results. Methodologically, our contributions are associated to the construction of the different matrix layers of the computation graph, obtaining analytic derivatives all the way from the loss function down to the feature layers in the framework of matrix backpropagation, the emphasis on computational efficiency for backward passes, as well as a voting based loss function. The proposed model applies generally, not just for matching different images of a category, taken in different scenes (its primary design), but also to different images of the same scene, or from a video.", "publication_ref": ["b11", "b3", "b24", "b2", "b18", "b37", "b6", "b19", "b9", "b20", "b7", "b26", "b15", "b35", "b22", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Problem Formulation", "text": "Input. We are given two input graphs G 1 = (V 1 , E 1 ) and G 2 = (V 2 , E 2 ), with |V 1 | = n and |V 2 | = m. Our goal is to establish an assignment between the nodes of the two graphs, so that a criterion over the corresponding nodes and edges is optimized (see below).\nGraph Matching. Let v \u2208 {0, 1} nm\u00d71 be an indicator vector such that v ia = 1 if i \u2208 V 1 is matched to a \u2208 V 2 and 0 otherwise, while respecting one-to-one mapping constraints. We build a square symmetric positive matrix M \u2208 R nm\u00d7nm such that M ia;jb measures how well every pair (i, j) \u2208 E 1 matches with (a, b) \u2208 E 2 . For pairs that do not form edges, their corresponding entries in the matrix are set to 0. The diagonal entries contain node-tonode scores, whereas the off-diagonal entries contain edgeto-edge scores. The optimal assignment v * can be formulated as\nv * = argmax v v \u22a4 Mv, s.t. Cv = 1, v \u2208 {0, 1} nm\u00d71 (1)\nThe binary matrix C \u2208 R nm\u00d7nm encodes one-to-one mapping constraints: \u2200a i v ia = 1 and \u2200i a v ia = 1. This is known to be NP-hard, so we relax the problem by dropping both the binary and the mapping constraints, and solve\nv * = argmax v v \u22a4 Mv, s.t. v 2 = 1 (2)\nThe optimal v * is then given by the leading eigenvector of the matrix M. Since M has non-negative elements, by using Perron-Frobenius arguments, the elements of v * are in the interval [0, 1], and we interpret v * ia as the confidence that i matches a.\nLearning. We estimate the matrix M parameterized in terms of unary and pair-wise point features computed over input images and represented as deep feature hierarchies. We learn the feature hierarchies end-to-end in a loss function that also integrates the matching layer. Specifically, given a training set of correspondences between pairs of images, we adapt the parameters so that the matching minimizes the error, measured as a sum of distances between predicted and ground truth correspondences. In our experiments, we work with graphs constructed over points that correspond to the 2d image projections of the 3d structure of the same physical object in motion (in the context of videos), or over point configurations that correspond to the same semantic category (matching instances of visual categories, e.g. different birds). The main challenge is the propagation of derivatives of the loss function through a factorization of the affinity matrix M, followed by matching (in our formulation, this is an optimization problem, solved using eigen-decomposition) and finally the full feature extraction hierarchy used to compute the unary and pair-wise point representations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Derivation Preliminaries", "text": "In practice, we build an end-to-end deep network that integrates a feature extracting component that outputs the required descriptors F for building the matrix M. We solve the assignment problem (2) and compute a matching loss L(v * ) between the solution v * and the ground-truth. The network must be able to pass gradients w.r.t the loss function from the last to the first layer. The key gradients to compute -which we cover in \u00a73 -are \u2202L/\u2202M and \u2202L/\u2202F. This computation could be difficult in the absence of an appropriate factorization, as the computational and memory costs become prohibitive. Moreover, as some of our layers implement complex matrix functions, a matrix generalization of back-propagation is necessary [15] for systematic derivations and computational efficiency. In the sequel we cover its main intuition and refer to [15] for details.\nMatrix backpropagation. We denote A : B = Tr(A \u22a4 B) = vec(A)vec(B) \u22a4 . For matrix derivatives, if we denote by f a function that outputs f (X) = Y and by L the network loss, the basis for the derivation starts from the Taylor expansion of the matrix functions [26] at the two layers. By deriving the functional L expresssing the total variation dY in terms of dX, dY = L(dX)\nand then using that\n\u2202L \u2022 f \u2202X : dX = \u2202L \u2202Y : L(dX) = L * ( \u2202L \u2202Y ) : dX(4)\nwe obtain the equality \u2202(L \u2022 f )/\u2202X = L * (\u2202L/\u2202Y), where L * is the adjoint operator of L. This recipe for finding the partial derivatives is used across all of our network layers. The derivations of L and L * are layer-specific and are given in the following sections.", "publication_ref": ["b14", "b14", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Affinity Matrix Factorization", "text": "Zhou and De la Torre [38] introduced a novel factorization of the matrix M that is generally applicable to all stateof-the-art graph matching methods. It explicitly exposes the graph structure of the set of points and the unary and pairwise scores between nodes and edges, respectively,\nM = [vec(M p )] + (G 2 \u2297 G 1 )[vec(M e )](H 2 \u2297 H 1 ) \u22a4 (5)\nwhere [x] represents the diagonal matrix with x on the main diagonal, and \u2297 is the Kronecker product. The matrix M p \u2208 R n\u00d7m represents the unary term, measuring nodeto-node similarities, whereas M e \u2208 R p\u00d7q measures edgeto-edge similarity; p, q are the numbers of edges in each graph, respectively. The two matrices encode the first-order and second-order potentials. To describe the structure of each graph, we define, as in [38], the node-edge incidence matrices as G, H \u2208 {0, 1} n\u00d7p , where g ic = h jc = 1 if the c th edge starts from the i th node and ends at the j th node. We have two pairs, {G 1 , H 1 } \u2208 {0, 1} n\u00d7p and {G 2 , H 2 } \u2208 {0, 1} m\u00d7q , one for each image graph.\nOne simple way to build M e and M p is\nM e = X\u039bY \u22a4 , M p = U 1 U 2\u22a4(6)\nwhere X \u2208 R p\u00d72d and Y \u2208 R q\u00d72d are the per-edge feature matrices, constructed such that for any c th edge that starts from the i th node and ends at the j th node, we set the edge descriptor as the concatenation of the descriptors extracted at the two nodes\nX c = [F 1 i |F 1 j ], Y c = [F 2 i |F 2 j ](7)\nThe matrices F 1 , U 1 \u2208 R n\u00d7d and F 2 , U 2 \u2208 R m\u00d7d contain per-node feature vectors of dimension d, extracted at possibly different levels in the network, and \u039b is a 2d\u00d72d blocksymmetric parameter matrix. Superscripts 1, 2 indicate over which input image (source or target) are the features computed.", "publication_ref": ["b37", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "Deep Network Optimization for Graph Matching", "text": "In this section we describe how to integrate and learn the graph matching model end-to-end, by implementing the required components in an efficient way. This allows us to back-propagate gradients all the way from the loss function down to the feature layers. The main components of our approach are shown in Fig. 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Affinity Matrix Layer", "text": "If we define the node-to-node adjacency matrices A 1 \u2208 {0, 1} n\u00d7n , A 2 \u2208 {0, 1} m\u00d7m , with a ij = 1 if there is an edge from the i th node to the j th node, then\nA 1 = G 1 H \u22a4 1 , A 2 = G 2 H \u22a4 2 (8)\nThe Affinity Matrix layer receives as input the required hierarchy of features, and the adjacency matrices A 1 and A 2 used to reconstruct the optimal G 1 , H 1 , G 2 , H 2 matrices, which verify the equations (8). It is easier to describe the connectivity of the graphs by adjacency matrices than by node-edge incidence matrices, but we still need the latter for efficient backward passes at higher layers of the network. Next, we describe the forward and the backward passes of this layer, as parts of the trainable deep network.\nForward pass. ", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "Given", "text": "A 1 , A 2 , recover the matrices G 1 , H 1 , G 2 , H 2 , such that A 1 = G 1 H \u22a4 1 , A 2 = G 2 H \u22a4 2 2. Given F 1 , F 2 ,\n\u039b = \u039b 1 \u039b 2 \u039b 2 \u039b 1 , \u039b ij > 0, \u2200i, j(9)\nWriting the variation of the loss layer in terms of the variation of edge matrix and using the recipe (4), Out: M as given by eq. ( 5)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\u2192", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Power Iteration", "text": "In:\nM Computations: v0 \u2190 1, v k+1 = Mv k / |Mv k | Out: v * \u2192 \u2192 Bi-Stochastic\nIn: v * Computations: reshape v * to a matrix and apply eqs. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Power Iteration Layer", "text": "Computing the leading eigenvector v * of the affinity matrix M can be done using power iterations\nv k+1 = Mv k Mv k (12\n)\nWe initialize v 0 = 1. We use \u2022 as the \u2113 2 norm, \u2022 2 . Forward pass. We run the assignment from (12) for N iterations, and output the vector v * = v N . Recall that M \u2208 R nm\u00d7nm , where n, m are the number of nodes in each graph and p, q respectively the number of edges, the time complexity of the forward pass, per power iteration, is O(n 2 m 2 ), when the matrix M is dense. If we exploit the sparsity of M the cost drops to O(nnz) where nnz represents the number of non-zero elements of the matrix M, being equal to pq + nm. Backward pass. To compute gradients, we express the variation of the loss and identify the required partial derivatives\ndL = \u2202L \u2202v k+1 : dv k+1 = \u2202L \u2202v k+1 : d Mv k Mv k d Mv k Mv k = (I \u2212 v k+1 v \u22a4 k+1 ) Mv k d(Mv k ) = = (I \u2212 v k+1 v \u22a4 k+1 ) Mv k (dMv k + Mdv k ) (13)\nKnowing that y : Ax = yx \u22a4 : A = A \u22a4 y : x, and using the symmetry of M, we derive\ndL = (I \u2212 v k+1 v \u22a4 k+1 ) Mv k \u2202L \u2202v k+1 v \u22a4 k : dM+ + M (I \u2212 v k+1 v \u22a4 k+1 ) Mv k \u2202L \u2202v k+1 : dv k (14)\nNoticing that dv k is still a function of dM, the gradients \u2202L/\u2202M and \u2202L/\u2202v k are iteratively built:\n\u2202L \u2202M = k (I \u2212 v k+1 v \u22a4 k+1 ) Mv k \u2202L \u2202v k+1 v \u22a4 k (15\n)\n\u2202L \u2202v k = M (I \u2212 v k+1 v \u22a4 k+1 ) Mv k \u2202L \u2202v k+1\nwhere we receive \u2202L/\u2202v * from the upper network layers. The computational cost of ( 15) is O(m 2 n 2 ) -regardless of the sparsity of M -and the memory complexity is \u0398(m 2 n 2 ). Such costs are prohibitive in practice. By employing techniques of matrix back-propagation [15], we can exploit the special factorization (5) of matrix M, to make operations both memory and time efficient. In fact, set aside efficiency, it would not be obvious how a classic approach would be used in propagating derivatives through a complex factorization like (5). Exploiting (5) and the recipe from (4), and omitting for clarity the term M p , we obtain (16) as detailed in Fig. 2.\nNote that \u2022 n\u00d7m is the operator that reshapes an nm\u00d71 vector into an n \u00d7 m matrix. For derivations we use the property that, for any compatible matrices A, B and V, (A \u2297 B) \u22a4 vec(V) = vec(B \u22a4 VA). Consequently, by also considering the unary term M p , it follows that\n(I \u2212 v k+1 v \u22a4 k+1 ) Mv k \u2202L \u2202v k+1 v \u22a4 k : dM = (I \u2212 v k+1 v \u22a4 k+1 ) Mv k \u2202L \u2202v k+1 v \u22a4 k : (G 2 \u2297 G 1 )[vec(dM e )](H 2 \u2297 H 1 ) \u22a4 = diag (G 2 \u2297 G 1 ) \u22a4 (I \u2212 v k+1 v \u22a4 k+1 ) Mv k \u2202L \u2202v k+1 v \u22a4 k (H 2 \u2297 H 1 ) : dM e = (G 2 \u2297 G 1 ) \u22a4 (I \u2212 v k+1 v \u22a4 k+1 ) Mv k \u2202L \u2202v k+1 \u2299 (H 2 \u2297 H 1 ) \u22a4 v k : dM e = G \u22a4 1 (I \u2212 v k+1 v \u22a4 k+1 ) Mv k \u2202L \u2202v k+1 n\u00d7m G 2 \u2299 H \u22a4 1 v k n\u00d7m H 2 : dM e (16\n)\nFigure 2: Derivations expressing the variation of the loss function w.r.t the variation of the edge affinity matrix M e , given the variation of the loss function w.r.t the variation of the whole affinity matrix M, from eq. ( 14)\n\u2202L \u2202M e = k G \u22a4 1 (I \u2212 v k+1 v \u22a4 k+1 ) Mv k \u2202L \u2202v k+1 n\u00d7m G 2 \u2299 \u2299 H \u22a4 1 v k n\u00d7m H 2(17)\n\u2202L \u2202M p = k (I \u2212 v k+1 v \u22a4 k+1 ) Mv k \u2202L \u2202v k+1 \u2299 v k(18)\nWith careful ordering of operations, the complexities for the backward pass are now O(max(m 2 q, n 2 p)) and \u0398(pq).\nTaking into account the sparsity of the node-edge incidence matrices G, H, efficiency can be further improved.", "publication_ref": ["b11", "b14", "b4", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Bi-Stochastic Layer", "text": "We make the result of the Power Iteration layer doublestochastic by mapping the eigenvector v * on the L1 constraints of the matching problem (1): \u2200a, i v ia = 1 and \u2200i, a v ia = 1. This is suggested by multiple authors [13,37,19], as it was observed to significantly improve performance. We introduce a new Bi-Stochastic layer that takes as input any correspondence vector v * \u2208 R nm\u00d71 + , reshaped to a matrix of size n \u00d7 m, and outputs the double-stochastic variant, as described in [30]. Even though the original algorithm assumes only square matrices, the process can be generalized as shown in [9]. Forward pass. Given a starting matrix S 0 = v * n\u00d7m , we run the following assignments for a number of iterations\nS k+1 = S k [1 \u22a4 n S k ] \u22121 , S k+2 = [S k+1 1 m ] \u22121 S k+1 (19)\nBackward pass. Given a starting gradient \u2202L/\u2202S, we iteratively compute\n\u2202L \u2202S k+1 = [S k+2 1 m ] \u22121 \u2202L \u2202S k+2 \u2212 \u2212 diag [S k+2 1 m ] \u22122 \u2202L \u2202S k+2 S \u22a4 k+2 1 \u22a4 m (20\n)\n\u2202L \u2202S k = \u2202L \u2202S k+1 [1 \u22a4 n S k+1 ] \u22121 \u2212 \u2212 1 n diag [1 \u22a4 n S k+1 ] \u22122 S \u22a4 k+1 \u2202L \u2202S k+1 \u22a4 (21)", "publication_ref": ["b12", "b36", "b18", "b29", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Converting Confidence-maps to Displacements", "text": "We use a special layer, called Voting layer to convert from the confidence map S \u2208 R n\u00d7m -passed on by the Bi-Stochastic layer -to a displacement vector. The idea is to normalize, for each assigned point i, its corresponding candidate scores given by the ith row of S, denoted S(i, 1...m). We then use it to weight the matrix of positions P \u2208 R m\u00d72 and subtract the position of match i.\nd i = exp[\u03b1S(i, 1...m)] j exp[\u03b1S(i, j)] P \u2212 P i (22)\nwhere [] is now just a bulkier bracket. In practice we set \u03b1 to large values (e.g. 200). By softly voting over the fixed candidate locations, our solution can interpolate for more precise localization. We also set confidences to 0 for candidates that are too far away from assigned points, or those added through padding. Hence these do not contribute to the loss, given by\nL(d) = i \u03c6(d i \u2212 d gt i )(23)\nwhere \u03c6(x) = \u221a x \u22a4 x + \u01eb is a robust penalty term, and d gt is the ground-truth displacement from the source point to the correct assignment. This layer is implemented in multiple, fully differentiable steps: a) first, scale the input by \u03b1, b) use a spatial map for discarding candidate locations that are further away than a certain threshold from the starting location and use it to modify the confidence maps, c) use a softmax layer to normalize the confidence maps, d) compute the displacement map. The discard map sets the confidences to 0 for points that are further away than a certain distance, or for points that were added through padding. Such points do not contribute in the final loss, given by (23).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section we describe the models used as well as detailed experimental matching results, both quantitative (including ablation studies) and qualitative, on three challenging datasets: MPI Sintel, CUB, and PASCAL keypoints.\nDeep feature extraction network. We rely on the VGG-16 architecture from [29], that is pretrained to perform classification in the ImageNet ILSVRC [28] but we can use any other deep network architecture. We implement our deep learning framework in MatConvNet [31]. As edge features F we use the output of layer relu5_1 (and the entire hierarchy under it), and for the node features U we use the output of layer relu4_2 (with the parameters of the associated hierarchy under it). Features are all normalized to 1 through normalization layers, right before they are used to compute the affinity matrix M. We conduct experiments for geometric and semantic correspondences on MPI-Sintel [6], Caltech-UCSD Birds-200-2011 [32] and PAS-CAL VOC [11] with Berkeley annotations [5].\nMatching networks. GMNwVGG is our proposed Graph Matching Network based on a VGG feature extractor. The suffix -U means that default (initial) weights are used; -T means trained end-to-end; the GMNwVGG-T w/o V variant does not use, at testing, the Voting layer in order to compute the displacements, but directly assigns indices of maximum value across the rows of the confidence map S, as correspondences. NNwVGG gives nearest-neighbour matching based on deep node descriptors U. MPI-Sintel. Besides the main datasets CUB and PAS-CAL, typically employed in validating matching methods, we also use Sintel in order to demonstrate the generality and flexibility of the formulation. The Sintel input images are consecutive frames in a movie and exhibit large displacements, large appearance changes, occlusion, non-rigid movements and complex atmospheric effects (only included in the final set of images). The Sintel training set consists of 23 video sequences (organized as folders) and 1064 frames. In order to make sure that we are fairly training and evaluating, as images from the same video depict instances of the same objects, we split the data into 18 folders (i.e. 796 image pairs) for training and 5 folders (i.e. 245 image pairs) for testing. To be able to set a high number of correspondences under the constraints of memory usage and available computational resolutions of our descriptors, we partition the input images in 4 \u00d7 4 blocks padded for maximum displacement, which we match one by one. Note that for this particular experiment we do not use the Bi-Stochastic layer, as the one-to-one mapping constraints do not apply to the nature of this problem (the assignment can be manyto-one e.g. in scaling). Notice that our model is designed to establish correspondences between two images containing similar structures, generally from different scenes, not tailored explicitly for optical flow, where additional smoothness constraints can be exploited. A main point of our Sintel experiments is to show the scalability of our method which operates with affinity matrices of size 10 6 \u00d7 10 6 . A complete forward and backward pass runs in roughly 2 seconds on a 3.2 Ghz Intel Xeon machine, with Titan X Pascal GPU. 1 We show quantitative results in table 1. We use the Percentage of Correct Keypoints metric to test our matching performance, with a threshold of 10 pixels. We compare against other state-of-the-art matching methods, following the protocol in [8]. Notice that even untrained, our net-Method PCK@10 pixels SIFT flow [22] 89.0 DaisyFF [34] 87.3 DSP [17] 85.3 DM [33] 89.  We show quantitative results in table 2. The \"PCK@\u03b1\" metric [35] represents the percentage of predicted correspondences that are closer than \u03b1 \u221a w 2 + h 2 from groundtruth locations, where w, h are image dimensions. Qualitative results are shown in fig. 5.\nOur end-to-end fully trainable matching models significantly outperform nearest neighbor approaches (EPE error is halved) based on deep features or similar graph matching formulations based on deep features not refined jointly with the assignment model. Our model GMNwVGG-T w/o V which does not use, at testing, the Voting layer in order to compute the displacements is inferior to the soft-voting mechanism of our compete method GMNwVGG-T. We also achieve state-of-the-art results compared to UCN [8], WarpNet [16], SIFT [24] and DSP [17], cf. fig. 3. We train one matching network for all classes, hence our model is agnostic to the semantic category. This dataset is considerably more challenging than CUB: bounding-boxes can range from very large to extremely small (e.g. 30 \u00d7 30), the training and testing pairs cover combinations of two images from each class (meaning that we cannot expect them to contain objects in the same pose) and the appearance variation is more extreme. We rely on the same protocol for evaluation and setting meta-parameters as in the CUB experiment. Results are shown in Fig. 6, and comparisons in table 3.", "publication_ref": ["b28", "b27", "b30", "b5", "b31", "b10", "b4", "b0", "b7", "b21", "b33", "b16", "b32", "b34", "b7", "b15", "b23", "b16"], "figure_ref": ["fig_3", "fig_1", "fig_4"], "table_ref": ["tab_5"]}, {"heading": "Method", "text": "PCK@0.1 (Class average) conv4 flow [23] 24.9 SIFT flow [22] 24.  We test following the same protocol as for CUB-200-2011 and obtain improvements over state-of-the-art. Notice that results of UCN [8] differ from the paper, as we use the straight average for the 20 semantic classes. Their paper reports the weighted average based on class frequency -under that metric UCN obtains PCK@0.1 = 44 and we obtain 45.3, so the improvement is preserved compared to the direct averaging case.", "publication_ref": ["b22", "b21", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions", "text": "We have presented an end-to-end learning framework for graph matching with general applicability to models containing deep feature extraction hierarchies and combinatorial optimization layers. We formulate the problem as a quadratic assignment under unary and pair-wise node relations represented using deep parametric feature hierarchies. All model parameters are trainable and the graph matching optimization is included within the learning formulation. As such, the main challenges are the calculation of backpropagated derivatives through complex matrix layers and the implementation of the entire framework (factorization of the affinity matrix, bi-stochastic layers) in a computationally efficient manner. Our experiments and ablation studies on diverse datasets like PASCAL VOC keypoints, Sintel and CUB show that fully learned graph matching models   surpass nearest neighbor counterparts, or approaches that use deep feature hierarchies that were not refined jointly with (and constrained by) the quadratic assignment problem.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Optnet: Differentiable optimization as a layer in neural networks", "journal": "CoRR", "year": "2017", "authors": "B Amos; J Z Kolter"}, {"ref_id": "b1", "title": "The fast bilateral solver", "journal": "", "year": "2015", "authors": "J T Barron; B Poole"}, {"ref_id": "b2", "title": "Shape matching and object recognition using low distortion correspondences", "journal": "IEEE", "year": "2005", "authors": "A C Berg; T L Berg; J Malik"}, {"ref_id": "b3", "title": "Method for registration of 3-d shapes", "journal": "", "year": "1992", "authors": "P J Besl; N D Mckay"}, {"ref_id": "b4", "title": "Poselets: Body part detectors trained using 3d human pose annotations", "journal": "IEEE", "year": "2009", "authors": "L Bourdev; J Malik"}, {"ref_id": "b5", "title": "A naturalistic open source movie for optical flow evaluation", "journal": "Springer", "year": "2012", "authors": "D J Butler; J Wulff; G B Stanley; M J Black"}, {"ref_id": "b6", "title": "Learning graph matching", "journal": "", "year": "2009", "authors": "T S Caetano; J J Mcauley; L Cheng; Q V Le; A J Smola"}, {"ref_id": "b7", "title": "Universal correspondence network", "journal": "", "year": "2016", "authors": "C B Choy; J Gwak; S Savarese; M Chandraker"}, {"ref_id": "b8", "title": "Balanced graph matching", "journal": "", "year": "2006", "authors": "T Cour; P Srinivasan; J Shi"}, {"ref_id": "b9", "title": "A tensor-based algorithm for high-order graph matching", "journal": "", "year": "2011", "authors": "O Duchenne; F Bach; I.-S Kweon; J Ponce"}, {"ref_id": "b10", "title": "The pascal visual object classes challenge 2011 (voc2011) development kit", "journal": "", "year": "2011", "authors": "M Everingham; J Winn"}, {"ref_id": "b11", "title": "Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography", "journal": "Communications of the ACM", "year": "1981", "authors": "M A Fischler; R C Bolles"}, {"ref_id": "b12", "title": "A graduated assignment algorithm for graph matching", "journal": "", "year": "1996", "authors": "S Gold; A Rangarajan"}, {"ref_id": "b13", "title": "Proposal flow", "journal": "", "year": "2016", "authors": "B Ham; M Cho; C Schmid; J Ponce"}, {"ref_id": "b14", "title": "Training deep networks with structured layers by matrix backpropagation", "journal": "", "year": "2015", "authors": "C Ionescu; O Vantzos; C Sminchisescu"}, {"ref_id": "b15", "title": "Warpnet: Weakly supervised matching for single-view reconstruction", "journal": "", "year": "2016", "authors": "A Kanazawa; D W Jacobs; M Chandraker"}, {"ref_id": "b16", "title": "Deformable spatial pyramid matching for fast dense correspondences", "journal": "", "year": "2013", "authors": "J Kim; C Liu; F Sha; K Grauman"}, {"ref_id": "b17", "title": "Fine-grained recognition without part annotations", "journal": "", "year": "2015", "authors": "J Krause; H Jin; J Yang; L Fei-Fei"}, {"ref_id": "b18", "title": "A spectral technique for correspondence problems using pairwise constraints", "journal": "IEEE", "year": "2005", "authors": "M Leordeanu; M Hebert"}, {"ref_id": "b19", "title": "Unsupervised learning for graph matching", "journal": "International journal of computer vision", "year": "2012", "authors": "M Leordeanu; R Sukthankar; M Hebert"}, {"ref_id": "b20", "title": "Semisupervised learning and optimization for hypergraph matching", "journal": "IEEE", "year": "2011", "authors": "M Leordeanu; A Zanfir; C Sminchisescu"}, {"ref_id": "b21", "title": "Sift flow: Dense correspondence across scenes and its applications", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2011", "authors": "C Liu; J Yuen; A Torralba"}, {"ref_id": "b22", "title": "Do convnets learn correspondence?", "journal": "", "year": "2014", "authors": "J L Long; N Zhang; T Darrell"}, {"ref_id": "b23", "title": "Distinctive image features from scaleinvariant keypoints", "journal": "International journal of computer vision", "year": "2004", "authors": "D G Lowe"}, {"ref_id": "b24", "title": "A global solution to sparse correspondence problems", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2003", "authors": "J Maciel; J P Costeira"}, {"ref_id": "b25", "title": "Matrix differential calculus with applications in statistics and econometrics", "journal": "", "year": "1995", "authors": "J R Magnus; H Neudecker"}, {"ref_id": "b26", "title": "Convolutional neural network architecture for geometric matching", "journal": "", "year": "2017", "authors": "I Rocco; R Arandjelovi\u0107; J Sivic"}, {"ref_id": "b27", "title": "Imagenet large scale visual recognition challenge", "journal": "International Journal of Computer Vision", "year": "2015", "authors": "O Russakovsky; J Deng; H Su; J Krause; S Satheesh; S Ma; Z Huang; A Karpathy; A Khosla; M Bernstein"}, {"ref_id": "b28", "title": "Very deep convolutional networks for large-scale image recognition", "journal": "", "year": "2014", "authors": "K Simonyan; A Zisserman"}, {"ref_id": "b29", "title": "Concerning nonnegative matrices and doubly stochastic matrices", "journal": "Pacific Journal of Mathematics", "year": "1967", "authors": "R Sinkhorn; P Knopp"}, {"ref_id": "b30", "title": "Matconvnet -convolutional neural networks for matlab", "journal": "", "year": "2015", "authors": "A Vedaldi; K Lenc"}, {"ref_id": "b31", "title": "The Caltech-UCSD Birds-200-2011 Dataset", "journal": "", "year": "2011", "authors": "C Wah; S Branson; P Welinder; P Perona; S Belongie"}, {"ref_id": "b32", "title": "Deepflow: Large displacement optical flow with deep matching", "journal": "IEEE", "year": "2013", "authors": "P Weinzaepfel; J Revaud; Z Harchaoui; C Schmid"}, {"ref_id": "b33", "title": "Daisy filter flow: A generalized approach to discrete dense correspondences", "journal": "CVPR", "year": "2014", "authors": "H Yang; W Lin; J Lu"}, {"ref_id": "b34", "title": "Articulated human detection with flexible mixtures of parts", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2013", "authors": "Y Yang; D Ramanan"}, {"ref_id": "b35", "title": "Lift: Learned invariant feature transform", "journal": "", "year": "2016", "authors": "K M Yi; E Trulls; V Lepetit; P Fua"}, {"ref_id": "b36", "title": "Probabilistic graph and hypergraph matching", "journal": "IEEE", "year": "2008", "authors": "R Zass; A Shashua"}, {"ref_id": "b37", "title": "Factorized graph matching", "journal": "IEEE", "year": "2012", "authors": "F Zhou; F De La; Torre "}], "figures": [{"figure_label": "191", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "( 19 )Figure 1 :191Figure 1: Computational pipeline of our fully trainable graph matching model. In training, gradients w.r.t. the loss function are passed through a deep feature extraction hierarchy controlling the unary and pair-wise terms associated to the nodes and edges of the two graphs, the factorization of the resulting affinity matrix, the eigen-decomposition solution of the matching problem, and the voting-based assignment layer. For each module we show the inputs, outputs and the key variables involved. Detailed derivations for the associated computations are given in the corresponding paper sections.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Our methods and other state-of-the-art techniques on CUB.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Four visual examples on the MPI-Sintel test partition, which exhibit large motions and occlusion areas. From top to bottom: the source images with the initial grid of points overlaid and the target images with the corresponding matches as found by our fully trained network. Colors are unique and they encode correspondences. Even for fast moving objects, points tend to track the surface correctly, without sliding -see e.g. the dragon's wing, claw, and the flying monster.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: Four qualitative examples of our best performing network GMNwVGG-T, on the CUB-200-2011 test-set. Images with a black contour represent the source, whereas images with a red contour represent targets. Color-coded correspondences are found by our method. The green framed images show ground-truth correspondences. The colors of the drawn circular markers uniquely identify 15 semantic keypoints.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure 6: Twelve qualitative examples of our best performing network GMNwVGG-T on the PASCAL VOC test-set. For every pair of examples, the left shows the source image and the right the target. Colors identify the computed assignments between points. The method finds matches even under extreme appearance and pose changes.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "build X, Y according to (7) 3. Build M e = X\u039bY \u22a4 4. Given U 1 , U 2 , build M p = U 1 U 2\u22a4 5. Build M according to (5) and make G 1 , H 1 , G 2 , H 2 available for the upper layers Backward pass. Assuming the network provides \u2202L/\u2202M e and \u2202L/\u2202M p , this layer must return \u2202L/\u2202F 1 , \u2202L/\u2202F 2 , \u2202L/\u2202U 1 and \u2202L/\u2202U 2 ; it must also compute \u2202L/\u2202\u039b in order to update the matrix \u039b. We assume \u2202L/\u2202M e and \u2202L/\u2202M p as input, and not \u2202L/\u2202M, because the subsequent layer can take advantage of this special factorization for efficient computation. We note that matrix \u039b must have the following form in order for M to be symmetric with positive elements", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "To compute the partial derivatives \u2202L/\u2202F 1 and \u2202L/\u2202F 2 , we identify and sum up the corresponding 1 \u00d7 d sub-blocks in the matrices \u2202L/\u2202X and \u2202L/\u2202Y. The partial derivative \u2202L/\u2202\u039b is used to compute the derivatives \u2202L/\u2202\u039b 1 and \u2202L/\u2202\u039b 2 . Note that in implementing the positivity condition from (9), one can use a ReLU unit. I1, I2 Out: features matrices F 1 , F 2 and U 1 , U 2 as computed by any CNN, at certain levels of the hierarchy, e.g. VGG-16 [29] \u2192 Affinity Matrix In: F 1 , F 2 , U 1 , U 2 Build graph structure: G1, G2, H1, H2 Computations: build Me and Mp", "figure_data": "Deep Feature ExtractorIn:dL =\u2202L \u2202M e: dM e =\u2202L \u2202M e: d(X\u039bY \u22a4 )=\u2202L \u2202M Y : d\u039b++\u2202L \u2202M e\u22a4X\u039b : dY(10)We identify the terms\u2202L \u2202X=\u2202L \u2202M eY\u039b \u22a4\u2202L \u2202\u039b= X \u22a4 \u2202L \u2202M eY\u2202L \u2202Y=\u2202L \u2202M eX\u039b(11)"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "To train our model, we sample random pairs of images from the training set of CUB-200-2011, which are not present in the test set. The number of points in the two graphs are maximum n = 15 and m = 256 -as sampled from a 16\u00d716 grid -with a Delaunay triangulation on the first graph, and fully connected on the second. In the Voting layer, we no longer discard candidate locations that are too far away from the source points. A complete forward and backward pass runs in 0.3 seconds.", "figure_data": "2 91.5 85.9 87.01 88.03 92.6 Table 1: Comparative matching results for Sintel. UCN [8] NNwVGG-U NNwVGG-T GMNwVGG-U GMNwVGG-T work remains competitive, as the graph structure acts as a regularizer. After training, the network has significantly in-creased accuracy. Visual examples are given in fig. 4. CUB. This dataset contains 11,788 images of 200 bird categories, with bounding box object localization and 15 annotated parts. We use the test set built by [16] which con-sists of 5,000 image pairs that are within 3 nearest neighbors apart on the pose graph of [18], so we can expect that birds are in somewhat similar poses in each pair to be tested. But across all images, there is a significant variation in pose, ar-ticulation and appearance. Method EPE (in pixels) PCK@0.05 GMNwVGG-U 41.63 0.63 NNwVGG-U 59.05 0.46 GMNwVGG-T w/o V 18.22 0.83 GMNwVGG-T 17.02 0.86"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Our models (with ablations) on CUB.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Our models as well as other state-of-the art approaches on PASCAL VOC.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "v * = argmax v v \u22a4 Mv, s.t. Cv = 1, v \u2208 {0, 1} nm\u00d71 (1)", "formula_coordinates": [2.0, 55.8, 329.15, 230.57, 18.52]}, {"formula_id": "formula_1", "formula_text": "v * = argmax v v \u22a4 Mv, s.t. v 2 = 1 (2)", "formula_coordinates": [2.0, 93.83, 411.56, 192.54, 16.91]}, {"formula_id": "formula_3", "formula_text": "\u2202L \u2022 f \u2202X : dX = \u2202L \u2202Y : L(dX) = L * ( \u2202L \u2202Y ) : dX(4)", "formula_coordinates": [2.0, 326.59, 473.85, 218.53, 23.78]}, {"formula_id": "formula_4", "formula_text": "M = [vec(M p )] + (G 2 \u2297 G 1 )[vec(M e )](H 2 \u2297 H 1 ) \u22a4 (5)", "formula_coordinates": [2.0, 316.34, 657.2, 228.77, 23.08]}, {"formula_id": "formula_5", "formula_text": "M e = X\u039bY \u22a4 , M p = U 1 U 2\u22a4(6)", "formula_coordinates": [3.0, 104.85, 213.6, 181.52, 11.87]}, {"formula_id": "formula_6", "formula_text": "X c = [F 1 i |F 1 j ], Y c = [F 2 i |F 2 j ](7)", "formula_coordinates": [3.0, 108.36, 304.79, 178.0, 18.31]}, {"formula_id": "formula_7", "formula_text": "A 1 = G 1 H \u22a4 1 , A 2 = G 2 H \u22a4 2 (8)", "formula_coordinates": [3.0, 110.44, 585.37, 175.92, 12.77]}, {"formula_id": "formula_8", "formula_text": "A 1 , A 2 , recover the matrices G 1 , H 1 , G 2 , H 2 , such that A 1 = G 1 H \u22a4 1 , A 2 = G 2 H \u22a4 2 2. Given F 1 , F 2 ,", "formula_coordinates": [3.0, 316.33, 94.92, 228.78, 41.32]}, {"formula_id": "formula_9", "formula_text": "\u039b = \u039b 1 \u039b 2 \u039b 2 \u039b 1 , \u039b ij > 0, \u2200i, j(9)", "formula_coordinates": [3.0, 355.6, 359.87, 189.52, 22.6]}, {"formula_id": "formula_10", "formula_text": "M Computations: v0 \u2190 1, v k+1 = Mv k / |Mv k | Out: v * \u2192 \u2192 Bi-Stochastic", "formula_coordinates": [4.0, 76.59, 99.32, 455.77, 105.72]}, {"formula_id": "formula_11", "formula_text": "v k+1 = Mv k Mv k (12", "formula_coordinates": [4.0, 133.99, 386.74, 148.22, 23.44]}, {"formula_id": "formula_12", "formula_text": ")", "formula_coordinates": [4.0, 282.21, 393.7, 4.15, 8.91]}, {"formula_id": "formula_13", "formula_text": "dL = \u2202L \u2202v k+1 : dv k+1 = \u2202L \u2202v k+1 : d Mv k Mv k d Mv k Mv k = (I \u2212 v k+1 v \u22a4 k+1 ) Mv k d(Mv k ) = = (I \u2212 v k+1 v \u22a4 k+1 ) Mv k (dMv k + Mdv k ) (13)", "formula_coordinates": [4.0, 62.28, 606.01, 224.08, 82.57]}, {"formula_id": "formula_14", "formula_text": "dL = (I \u2212 v k+1 v \u22a4 k+1 ) Mv k \u2202L \u2202v k+1 v \u22a4 k : dM+ + M (I \u2212 v k+1 v \u22a4 k+1 ) Mv k \u2202L \u2202v k+1 : dv k (14)", "formula_coordinates": [4.0, 342.84, 350.15, 202.27, 55.05]}, {"formula_id": "formula_15", "formula_text": "\u2202L \u2202M = k (I \u2212 v k+1 v \u22a4 k+1 ) Mv k \u2202L \u2202v k+1 v \u22a4 k (15", "formula_coordinates": [4.0, 348.53, 444.69, 192.43, 29.4]}, {"formula_id": "formula_16", "formula_text": ")", "formula_coordinates": [4.0, 540.96, 453.95, 4.15, 8.91]}, {"formula_id": "formula_17", "formula_text": "\u2202L \u2202v k = M (I \u2212 v k+1 v \u22a4 k+1 ) Mv k \u2202L \u2202v k+1", "formula_coordinates": [4.0, 348.46, 477.65, 138.45, 25.74]}, {"formula_id": "formula_18", "formula_text": "(I \u2212 v k+1 v \u22a4 k+1 ) Mv k \u2202L \u2202v k+1 v \u22a4 k : dM = (I \u2212 v k+1 v \u22a4 k+1 ) Mv k \u2202L \u2202v k+1 v \u22a4 k : (G 2 \u2297 G 1 )[vec(dM e )](H 2 \u2297 H 1 ) \u22a4 = diag (G 2 \u2297 G 1 ) \u22a4 (I \u2212 v k+1 v \u22a4 k+1 ) Mv k \u2202L \u2202v k+1 v \u22a4 k (H 2 \u2297 H 1 ) : dM e = (G 2 \u2297 G 1 ) \u22a4 (I \u2212 v k+1 v \u22a4 k+1 ) Mv k \u2202L \u2202v k+1 \u2299 (H 2 \u2297 H 1 ) \u22a4 v k : dM e = G \u22a4 1 (I \u2212 v k+1 v \u22a4 k+1 ) Mv k \u2202L \u2202v k+1 n\u00d7m G 2 \u2299 H \u22a4 1 v k n\u00d7m H 2 : dM e (16", "formula_coordinates": [5.0, 87.08, 82.96, 453.88, 115.43]}, {"formula_id": "formula_19", "formula_text": ")", "formula_coordinates": [5.0, 540.96, 180.27, 4.15, 8.91]}, {"formula_id": "formula_20", "formula_text": "\u2202L \u2202M e = k G \u22a4 1 (I \u2212 v k+1 v \u22a4 k+1 ) Mv k \u2202L \u2202v k+1 n\u00d7m G 2 \u2299 \u2299 H \u22a4 1 v k n\u00d7m H 2(17)", "formula_coordinates": [5.0, 59.45, 295.32, 226.91, 51.48]}, {"formula_id": "formula_21", "formula_text": "\u2202L \u2202M p = k (I \u2212 v k+1 v \u22a4 k+1 ) Mv k \u2202L \u2202v k+1 \u2299 v k(18)", "formula_coordinates": [5.0, 69.64, 346.03, 216.72, 29.4]}, {"formula_id": "formula_22", "formula_text": "S k+1 = S k [1 \u22a4 n S k ] \u22121 , S k+2 = [S k+1 1 m ] \u22121 S k+1 (19)", "formula_coordinates": [5.0, 61.36, 702.05, 225.0, 12.84]}, {"formula_id": "formula_23", "formula_text": "\u2202L \u2202S k+1 = [S k+2 1 m ] \u22121 \u2202L \u2202S k+2 \u2212 \u2212 diag [S k+2 1 m ] \u22122 \u2202L \u2202S k+2 S \u22a4 k+2 1 \u22a4 m (20", "formula_coordinates": [5.0, 323.66, 297.24, 217.3, 51.55]}, {"formula_id": "formula_24", "formula_text": ")", "formula_coordinates": [5.0, 540.96, 332.3, 4.15, 8.91]}, {"formula_id": "formula_25", "formula_text": "\u2202L \u2202S k = \u2202L \u2202S k+1 [1 \u22a4 n S k+1 ] \u22121 \u2212 \u2212 1 n diag [1 \u22a4 n S k+1 ] \u22122 S \u22a4 k+1 \u2202L \u2202S k+1 \u22a4 (21)", "formula_coordinates": [5.0, 333.75, 351.93, 211.36, 53.93]}, {"formula_id": "formula_26", "formula_text": "d i = exp[\u03b1S(i, 1...m)] j exp[\u03b1S(i, j)] P \u2212 P i (22)", "formula_coordinates": [5.0, 363.15, 526.85, 181.97, 25.46]}, {"formula_id": "formula_27", "formula_text": "L(d) = i \u03c6(d i \u2212 d gt i )(23)", "formula_coordinates": [5.0, 377.1, 656.48, 168.01, 22.73]}], "doi": ""}