{"title": "DropMessage: Unifying Random Dropping for Graph Neural Networks", "authors": "Taoran Fang; Zhiqing Xiao; Chunping Wang; Jiarong Xu; Xuan Yang; Yang Yang", "pub_date": "", "abstract": "Graph Neural Networks (GNNs) are powerful tools for graph representation learning. Despite their rapid development, GNNs also face some challenges, such as over-fitting, oversmoothing, and non-robustness. Previous works indicate that these problems can be alleviated by random dropping methods, which integrate augmented data into models by randomly masking parts of the input. However, some open problems of random dropping on GNNs remain to be solved. First, it is challenging to find a universal method that are suitable for all cases considering the divergence of different datasets and models. Second, augmented data introduced to GNNs causes the incomplete coverage of parameters and unstable training process. Third, there is no theoretical analysis on the effectiveness of random dropping methods on GNNs. In this paper, we propose a novel random dropping method called DropMessage, which performs dropping operations directly on the propagated messages during the message-passing process. More importantly, we find that DropMessage provides a unified framework for most existing random dropping methods, based on which we give theoretical analysis of their effectiveness. Furthermore, we elaborate the superiority of DropMessage: it stabilizes the training process by reducing sample variance; it keeps information diversity from the perspective of information theory, enabling it become a theoretical upper bound of other methods. To evaluate our proposed method, we conduct experiments that aims for multiple tasks on five public datasets and two industrial datasets with various backbone models. The experimental results show that DropMessage has the advantages of both effectiveness and generalization, and can significantly alleviate the problems mentioned above. Our code is available at: https://github.com/zjunet/DropMessage.", "sections": [{"heading": "Introduction", "text": "Graphs, ubiquitous in the real world, are used to present complex relationships among various objects in numerous domains such as social media (social networks), finance (trading networks), and biology (biological networks). As powerful tools for representation learning on graphs, graph neural networks (GNNs) have attracted considerable attention recently (Defferrard, Bresson, and Vandergheynst 2016;Kipf and Welling 2017;Velickovic et al. 2018;Ding, Tang, and Zhang 2018). In particular, GNNs adopt a message-passing schema (Gilmer et al. 2017), in which each node aggregates information from its neighbors in each convolutional layer, and have been widely applied in various downstream tasks such as node classification (Kipf and Welling 2017), link prediction Welling 2016), vertex clustering (Ramaswamy, Gedik, andLiu 2005), and recommendation systems (Ying et al. 2018).\nYet, despite their rapid development, training GNNs on large-scale graphs is facing several challenges such as overfitting, over-smoothing, and non-robustness. Indeed, compared to other data forms, gathering labels for graph data is expensive and inherently biased, which limits the generalization ability of GNNs due to over-fitting. Besides, representations of different nodes in a GNN tend to become indistinguishable as a result of aggregating information from neighbors recursively. This phenomenon of over-smoothing prevents GNNs from effectively modeling the higher-order dependencies from multi-hop neighbors (Li, Han, and Wu 2018;Xu et al. 2018;Chen et al. 2020;Zhao and Akoglu 2020;Oono andSuzuki 2020, 2019). Recursively aggregating schema makes GNNs vulnerable to the quality of input graphs (Zhu et al. 2019;Z\u00fcgner, Akbarnejad, and G\u00fcnnemann 2018). In other words, noisy graphs or adversarial attacks can easily influence a GNN's performance.\nThe aforementioned problems can be helped by random dropping methods (Hinton et al. 2012;Rong et al. 2019;Feng et al. 2020), which integrate augmented data into models by randomly masking parts of the input. These methods (Maaten et al. 2013;Matsuoka 1992;Bishop 1995;Cohen, Rosenfeld, and Kolter 2019) focus on randomly dropping or sampling existing information, and can also be considered as a data augmentation technique. Benefiting from the advantages of being unbiased, adaptive, and free of parameters, random dropping methods have greatly contributed to improving the performance of most GNNs.\nHowever, some open questions related to random dropping methods on GNNs still exist. First, a general and critical issue of existing random dropping methods is that augmented data introduced to GNNs make parameters difficult to converge and the training process unstable. Moreover, it is challenging to find an optimal dropping method suitable to all graphs and models, because different graphs and models are equipped with their own properties and the model performance can be Considering the messages propagated by the center node (i.e., Node 1), DropMessage allows to propagate distinct messages to different neighbor nodes, and its induced message matrix can be arbitrary. The induced message matrices of other methods obey some explicit constraints and can be regarded as special forms of DropMessage. influenced greatly by employing various dropping strategies. Furthermore, the answer to how to choose a proper dropping rate when applying these methods is still unclear, and so far no theoretical guarantee has been provided to explain why random dropping methods can improve the performance of a GNN.\nIn this paper, we propose a novel random dropping method called DropMessage, which can be applied to all messagepassing GNNs. As Figure 1 suggests, existing random dropping methods perform dropping on either the node feature matrix (Hinton et al. 2012;Feng et al. 2020) or the adjacency matrix (Rong et al. 2019), while our DropMessage performs dropping operations on the propagated messages, which allows the same node to propagate different messages to its different neighbors. Besides, we unify existing dropping methods into our framework and demonstrate theoretically that conducting random dropping methods on GNNs is equivalent to introducing additional regularization terms to their loss functions, which makes the models more robust. Furthermore, we also elaborate the superiority of our DropMessage whose sample variance is much smaller and training process is more stable. From the perspective of information theory, DropMessage keeps the property of information diversity, and is theoretically regarded as an upper bound of other random dropping methods. To sum up, the contributions of this paper are as follows:\n\u2022 We propose a novel random dropping method, called DropMessage, for all message-passing GNNs. Existing random dropping methods on GNNs can be unified into our framework via performing masking in accordance with the certain rule on the message matrix. In other words, these methods can be regarded as one special form of DropMessage. ", "publication_ref": ["b9", "b21", "b37", "b13", "b16", "b21", "b30", "b48", "b23", "b45", "b5", "b51", "b28", "b28", "b53", "b54", "b17", "b32", "b15", "b25", "b26", "b1", "b7", "b17", "b15", "b32"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Related Work", "text": "To sort out the key logic of our work, we first review some related work about random dropping methods with a particular focus on GNNs.\nIn general, random dropping can be regarded as a form of feature-noising schema that alleviates over-fitting by artificially corrupting the training data. As a representative work, Dropout is first introduced by Hinton et al. (Hinton et al. 2012) and has been proved to be effective in many scenarios (Abu-Mostafa 1990;Burges and Sch\u00f6lkopf 1996;Simard et al. 1998;Rifai et al. 2011;Maaten et al. 2013). Besides, Bishop (Bishop 1995) demonstrates the equivalence of corrupted features and L 2 -type regularization. Wager et al. (Wager, Wang, and Liang 2013) show that the dropout regularizer is first-order equivalent to an L 2 regularizer that being applied after scaling the features by an estimate of the inverse diagonal Fisher information matrix.\nWith the rapid development of GNNs, random dropping has also been generalized to the graph field, thus leading to three most common methods: Dropout (Hinton et al. 2012), DropEdge (Rong et al. 2019) and DropNode (Feng et al. 2020). Dropout performs random dropping operation on the node feature matrix, while DropEdge and DropNode, as the name implies, respectively act on the adjacency matrix (edges) and nodes. These random dropping methods can also be regarded as special forms of data augmentation (Shorten and Khoshgoftaar 2019;Frid-Adar et al. 2018;Buslaev et al. 2020;Ding et al. 2022b;Velickovic et al. 2019b), with the advantage of not requiring parameter estimation (Papp et al. 2021;Luo et al. 2021;Chen, Ma, and Xiao 2018;Zeng et al. 2020) and easy to apply. All the methods mentioned above can be used to alleviate over-fitting and over-smoothing on GNNs. However, they can achieve effective performance only on some specific datasets and GNNs. The question of how to find an optimal dropping method that suitable for most cases still remains to be explored. Moreover, there is no theoretical explanation about the effectiveness of random dropping methods on GNNs, which adds some ambiguity to the function of these methods.", "publication_ref": ["b17", "b0", "b2", "b36", "b31", "b25", "b1", "b40", "b17", "b32", "b15", "b35", "b15", "b3", "b11", "b39", "b29", "b24", "b6", "b49"], "figure_ref": [], "table_ref": []}, {"heading": "Notations and Preliminaries", "text": "Notations. Let G = (V, E) represent the graph, where V = {v 1 , . . . , v n } denotes the set of n nodes, and E \u2286 V \u00d7 V is the set of edges between nodes. The node features can be denoted as a matrix X = {x 1 , . . . , x n } \u2208 R n\u00d7c , where x i is the feature vector of the node v i , and c is the dimensionality of node features. The edges describe the relations between nodes and can be represented as an adjacent matrix A = {a 1 , . . . , a n } \u2208 R n\u00d7n , where a i denotes the i-th row of the adjacency matrix, and A (i, j) denotes the relation between nodes v i and v j . Also, the node degrees are given by d = {d 1 , . . . , d n }, where d i computes the sum of edge weights connected to node v i . Meanwhile, the degree of the whole graph is calculated by d(G) = n i d i . When we apply message-passing GNNs on G, the message matrix can be represented as M = {m 1 , . . . , m k } \u2208 R k\u00d7c , where m i is a message propagated between nodes, k is the total number of messages propagated on the graph, and c is the dimension number of the messages. Message-passing GNNs. Most of the existing GNN models adopt the message-passing framework, where each node sends messages to its neighbors and simultaneously receives messages from its neighbors. In the process of the propagation, node representations are updated based on node feature information and messages from neighbors, which can be formulated as\nh (l+1) i = \u03b3 (l) (h (l) i , AGG j\u2208N (i) (\u03c6 (l) (h (l) i , h (l) j , e j,i ))) (1)\nwhere h (l) i denotes the hidden representation of node v i in the l-th layer, and N (i) is a set of nodes adjacent to node v i ; e j,i represents the edge from node j to node i; \u03c6 (l) and \u03b3 (l) are differentiable functions; and AGG represents the aggregation operation. From the perspective of the messagepassing schema, we can gather all the propagated messages into a message matrix M \u2208 R k\u00d7c . Specifically, each row of the message matrix M corresponds to a message propagated on a directed edge, which can be expressed as below:\nM (l) (i,j) = \u03c6 (l) (h (l) i , h (l) j , e (l) j,i )\nwhere \u03c6 denotes the mapping that generates the messages, c is the dimension number of the messages, and the row number k of the message matrix M is equal to the directed edge number in the graph.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Our Approach", "text": "In this section, we introduce our proposed DropMessage, which can be applied to all message-passing GNNs. We first describe the details of our approach, and further prove that the most common existing random dropping methods, i.e., Dropout, DropEdge and DropNode, can be unified into our framework. Based on that, we give a theoretical explanation of the effectiveness of these methods. After that, we theoretically analyze the superiority of DropMessage in terms of stabilizing the training process and keeping information diversity. Finally, we derive a theoretical upper bound to guide the selection of dropping rate \u03b4.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "DropMessage", "text": "Algorithm description. Different from existing random dropping methods, DropMessage performs directly on the message matrix M instead of the feature matrix or the adjacency matrix. More specifically, DropMessage conducts dropping on the message matrix with the dropping rate \u03b4, which means that \u03b4|M| elements of the message matrix will be masked in expectation. Formally, this operation can be regarded as a sampling process. For each element M i,j in the message matrix, we generate an independent mask i,j to determine whether it will be preserved or not, according to a Bernoulli distribution i,j \u223c Bernoulli(1 \u2212 \u03b4). Then, we obtain the perturbed message matrix M by multiplying each element with its mask. Finally, we scale M with the factor of 1 1\u2212\u03b4 to guarantee that the perturbed message matrix is equal to the original message matrix in expectation. Thus, the whole process can be expressed as\nM i,j = 1 1\u2212\u03b4 i,j M i,j , where i,j \u223c Bernoulli(1 \u2212 \u03b4)\n. The applied GNN model then propagates information via the perturbed message matrix M instead of the original message matrix. It should be moted that DropMessage only affects on the training process. Practical implementation. In practice, we do NOT need to generate the complete message matrix explicitly, because each row in the message matrix represents a distinct directed edge in the graph, and our proposed DropMessage can be applied to every directed edge independently. This property allows DropMessage to be easily parallelized, e.g., edge-wise, node-wise, or batch-wise, and to be applied to the messagepassing backbone model without increasing time or space complexity. Unifying random dropping methods. As we have mentioned above, DropMessage differs from existing methods by directly performing on messages instead of graphs. However, in intuition, the dropping of features, edges, nodes or messages will all eventually act on the message matrix. It inspires us to explore the theoretical connection between different dropping methods. As a start, we demonstrate that Dropout, DropEdge, DropNode, and DropMessage can all be formulated as Bernoulli sampling processes in Table 1. More importantly, we find that existing random dropping methods are actually special cases of DropMessage, and thus can be expressed in a uniform framework. Lemma 1. Dropout, DropEdge, DropNode, and DropMessage perform random masking on the message matrices in accordance with certain rules.\nWe provide the equivalent operation on the message matrix of each method below. \nX i,j = X i,j DropEdge A i,j = A i,j DropNode X i = X i DropMessage M i,j = M i,j s.t. \u223c Bernoulli(1 \u2212 \u03b4)\nDropout. Dropping the elements X drop = {X i,j | i,j = 0} in the feature matrix X is equivalent to masking elements M drop = {M i,j |source(M i,j ) \u2208 X drop } in the message matrix M, where source(M i,j ) indicates which element in the feature matrix that M i,j corresponds to. DropEdge. Dropping the elements E drop = {E i,j |A i,j = 1 and i,j = 0} in the adjacency matrix A is equivalent to masking elements\nM drop = {M i |edge(M i ) \u2208 E drop } in the message matrix M, where edge(M i ) indicates which edge that M i corresponds to. DropNode. Dropping the elements V drop = {X i | i = 0} in the feature matrix X is equivalent to masking elements M drop = {M i |node(M i ) \u2208 V drop } in the message matrix M,\nwhere node(M i ) indicates which row in the feature matrix that M i corresponds to.\nDropMessage. This method directly performs random masking on the message matrix M.\nAccording to above descriptions, we find DropMessage conduct finest-grained masking on the message matrix, which makes it the most flexible dropping method, and other methods can be regarded as a special form of DropMessage. Theoretical explanation of effectiveness. Previous studies have explored and explained why random dropping works in the filed of computer vision (Wager, Wang, and Liang 2013;Wan et al. 2013). However, to the best of our knowledge, the effectiveness of random dropping on GNNs has not been studied yet. To fill this gap, based on the unified framework of existing methods, we next provide a theoretical analysis. Theorem 1. Unbiased random dropping on GNNs methods introduce an additional regularization term into the objective functions, which makes the models more robust.\nProof. For analytical simplicity, we assume that the downstream task is a binary classification and we apply a single layer GCN (Kipf and Welling 2017) as the backbone model, which can be formulated as H = BMW, where M denotes the message matrix, W denotes the transformation matrix, B \u2208 R n\u00d7k indicates which messages should be aggregated by each node and B is its normalized form. Also, we adopt sigmoid as non-linear function and present the result as Z = sigmoid(H). When we use cross-entropy as loss function, the objective function can be expressed as follows:\nL CE = j,yj =1 log(1 + e \u2212hj ) + k,y k =0 log(1 + e h k ) (2)\nWhen performing random dropping on graphs, we use the perturbed message matrix M instead of the original message matrix M. Thus, the objective function in expectation can be expressed as follows:\nE( L CE ) = L CE + i 1 2 z i (1 \u2212 z i )V ar(h i ) (3)\nMore details of the derivation can be found in Appendix. As shown in Equation 3, random dropping methods on graphs introduce an extra regularization to the objective function. For binary classification tasks, this regularization enforces the classification probability approach to 0 or 1, thus a clearer judgment can be obtained. By reducing the variance ofh i , random dropping methods motivate the model to extract more essential high-level representations. Therefore, the robustness of the models is enhanced. It is noted that Equation 3 can be well generalized to multi-classification tasks by extending dimension of the model output. Formally, when dealing with the multi-classification task, the final objective function can be expressed as E( L CE ) =\nL CE + i 1 2 z ci i (1 \u2212 z ci i )V ar(h ci i )\n, where c i is the label of node v i , and the superscript indicates which dimension of the vector is selected.", "publication_ref": ["b40"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Advantages of DropMessage", "text": "We give two additional analysis to demonstrate the advantages of DropMessage on two aspects: stabilizing the training process and keeping diverse information. Reducing sample variance. All random dropping methods are challenged by the problem of unstable training process. As existing works suggest, it is caused by the random noises introduced into each training epoch. These noises then add the difficulty of parameter coverage and the unstability of training process. Generally, sample variance can be used to measure the degree of stability. According to Table 1, the input of each training epoch can be regarded as a random sample of the whole graph, and the sample variance is calculated by the average difference of every two independent samples. Compared with other random dropping methods, DropMessage effectively alleviates the aforementioned problem by reducing the sample variance.\nTheorem 2. DropMessage presents the smallest sample variance among existing random dropping methods on messagepassing GNNs with the same dropping rate \u03b4.\nWe leave the proof in Appendix. Intuitively, DropMessage independently determines whether an element in the message matrix is masked or not, which is exactly the smallest Bernoulli trail for random dropping on the message matrix. By reducing the sample variance, DropMessage diminishes the difference of message matrices among distinct training epochs, which stabilizes the training process and expedites the convergence. The reason why DropMessage has the minimum sample variance is that it is the finest-grained random dropping method for GNN models. When applying DropMessage, each element M i,j will be independently judged that whether it should be masked.\nKeeping diverse information. In the following, we compare different random dropping methods with their degree of losing information diversity, from the perspective of information theory. Definition 1. The information diversity consists of feature diversity and topology diversity. We define feature diversity as FD G = card({ M SN (vi),l 0 \u2265 1}), where v i \u2208 V, l \u2208 [0, c), SN (v i ) indicates the slice of the row numbers corresponding to the edges sourced from v i ; topology diversity is defined as TD G = card({ M j 0 \u2265 1}), where j \u2208 [0, k). M \u2208 R k\u00d7c represents the message matrix, \u2022 0 calculates the zero norm of the input vector, and card(\u2022) counts the number of elements in the set.\nIn other words, feature diversity is defined as the total number of preserved feature dimensions from distinct source nodes; topology diversity is defined as the total number of directed edges propagating at least one dimension message. With the above definition, we claim that a method possesses the ability of keeping information diversity only under the condition where neither the feature diversity nor the topology diversity decreases after random dropping. Lemma 2. None of Dropout, DropEdge, and DropNode is able to keep information diversity.\nAccording to Definition 1, when we drop an element of the feature matrix X, all corresponding elements in the message matrix are masked and the feature diversity is decreased by 1. When we drop an edge in adjacency matrix, the corresponding two rows for undirected graphs in the message matrix are masked and the topology diversity is decreased by 2. Similarly, when we drop a node, i.e., a row in the feature matrix, elements in the corresponding rows of the message matrix are all masked. Both the feature diversity and the topology diversity are therefore decreased. Thus, for all of these methods, their feature and topology information cannot be completely recovered by propagated messages, leading to the loss of information diversity. Theorem 3. DropMessage can keep information diversity in expectation when\n\u03b4 i \u2264 1 \u2212 min( 1 di , 1 c )\n, where \u03b4 i is the dropping rate for node v i , d i is the out-degree of v i , and c is the feature dimension.\nProof. DropMessage conducts random dropping directly on message matrix M. To keep the diversity of the topology information, we expect that at least one element of each row in message matrix M can be preserved in expectation:\nE(|M f |) \u2265 1 \u21d2 (1 \u2212 \u03b4)c \u2265 1 \u21d2 \u03b4 \u2264 1 \u2212 1 c (4)\nTo keep the diversity of the feature information, we expect that for every element in the feature matrix X, at least one of its corresponding elements in the message matrix M is preserved in expectation:\nE(|M e |) \u2265 1 \u21d2 (1 \u2212 \u03b4 i )d i \u2265 1 \u21d2 \u03b4 i \u2264 1 \u2212 1 d i (5)\nTherefore, to keep the information diversity, the dropping rate \u03b4 i should satisfy both Equation 4 and Equation 5as\n\u03b4 i \u2264 1 \u2212 min( 1 d i , 1 c )(6)\nFrom the perspective of information theory, a random dropping method with the capability of keeping information diversity can preserve more information and theoretically perform better than those without such capability. Thus, it can explain why our method performs better than those existing dropping methods. Actually, we may only set one dropping rate \u03b4 for the whole graph rather than for each node in practice. Consequently, both DropMessage and other methods may lose some information. However, DropMessage still preserves more information than other methods with the same dropping rate even under this circumstance. It is demonstrated that DropMessage remains its advantage in real-world scenarios.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "We empirically validate the effectiveness and adaptability of our proposed DropMessage in this section. In particular, we explore the following questions: 1) Does DropMessage outperform other random dropping methods on GNNs? 2) Could DropMessage further improve the robustness and training efficiency of GNNs? 3) Does information diversity (described in Definition 1) matter in GNNs? Datasets. We employ 7 graph datasets in our experiments, including 5 public datasets Cora, CiteSeer, PubMed, ogbnarxiv, Flickr and 2 industrial datasets FinV, Telecom.\n\u2022 Cora, CiteSeer, PubMed, ogbn-arxiv: These 4 different citation networks are widely used as graph benchmarks (Sen et al. 2008;Hu et al. 2020). We conduct node classification tasks on each dataset to determine the research area of papers/researchers. We also consider link prediction on the first three graphs to predict whether one paper cites another. \u2022 Flickr: It is provided by Flickr, the largest photo-sharing website (Zeng et al. 2020). One node in the graph represents one image uploaded to Flickr. If two images share some common properties (e.g., same geographic location, same gallery, or comments by the same user), an edge between the nodes of these two images will appear. We conduct the node classification task that aims to categorize these images into 7 classes determined by their tags. \u2022 FinV, Telecom: These are two real-world mobile communication networks provided by FinVolution Group (Yang et al. 2019) and China Telecom (Yang et al. 2021), respectively. In the two datasets, nodes represent users, and edges indicate the situation where two users have communicated with each other at a certain frequency. The task is to identify whether a user is a default borrower or a telecom fraudster.\nBaseline methods. We compare our proposed DropMessage with other existing random dropping methods, including Dropout (Hinton et al. 2012), DropEdge (Rong et al. 2019), and DropNode (Feng et al. 2020). We adopt these dropping methods on various GNNs as the backbone model, and compare their performances on different datasets. Backbone models. In this paper, we mainly consider three mainstream GNNs as our backbone models: GCN (Kipf and Welling 2017), GAT (Velickovic et al. 2018), and APPNP (Klicpera, Bojchevski, and G\u00fcnnemann 2019). We take the official practice of these methods while make some minor modifications. All these backbone models have random dropping modules for different steps in their model implementation. For instance, GAT models perform random dropping after self-attention calculation, while APPNP models perform random dropping at the beginning of each iteration. For a fair comparison, we unify the implementation of random dropping modules in the same step for different backbone models. We fix Dropout, DropEdge, and DropNode on the initial input and fix DropMessage at the start point of the message propagation process.", "publication_ref": ["b33", "b18", "b49", "b47", "b46", "b17", "b32", "b15", "b37", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Comparison Results", "text": "Table 2 summarizes the overall results. For the node classification task, the performance is measured by accuracy on four public datasets (Cora, CiteSeer, PubMed, ogbn-arxiv).\nAs for Flickr and two imbalanced industrial datasets, we employ F1 scores. When it comes to the link prediction task, we calculate the AUC values for comparisons. Considering the space limitation, the std values of the experimental results are presented in the Appendix.\nEffect of random dropping methods. It is observed that random dropping methods consistently outperform GNNs without random dropping in both node classification and link prediction. Besides, we see that the effects of random dropping methods vary over different datasets, backbone models, and downstream tasks. For example, random dropping methods on APPAP obtain an average accuracy improvement of 1.4% on CiteSeer, while 0.1% on PubMed. Meanwhile, random dropping methods achieve 2.1% accuracy improvement for GCN on Cora, while only 0.8% for GAT.\nComparison of different dropping methods. Our pro-posed DropMessage works well in all settings, exhibiting its strong adaptability to various scenarios. Overall, we have 21 settings under the node classification task, each of which is a combination of different backbone models and datasets (e.g., GCN-Cora). It is showed that DropMessage achieves the optimal results in 15 settings, and gets sub-optimal results in the rest. As to 9 setttings under the link prediction task, DropMessage achieves the optimal results in 5 settings, and sub-optimal results in the rest. Moreover, the stable performance of DropMessage over all datasets compared to other methods is clearly presented. Taking DropEdge as the counterexample, it appears strong performance on industrial datasets but demonstrates a clear drop on public ones. A reasonable explanation is that the message matrix patterns reserved by distinct mask methods vary from each other as presented in Table 1. With the favor of its finest-grained dropping strategy, DropMessage obtains smaller inductive bias. Thus, compared with other methods, DropMessage is more applicable in most scenarios.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2", "tab_1"]}, {"heading": "Additional Results", "text": "Robustness analysis. We study the robustness of dropping methods through measuring their ability of handling perturbed graphs. To guarantee that the initial data is comparatively clean, we conduct experiments on three citation networks: Cora, CiteSeer, and PubMed. We randomly add a certain ratio of edges into these datasets and perform the node classification. We find that all the random dropping methods have positive effects when the perturbation rate increases from 0% to 30%. The average improvement in the case of 30% perturbation reached 37% compared to that without perturbation, which indicates that the random dropping methods strengthen the robustness of GNN models. Besides, our proposed DropMessage shows its versatility and outperforms other dropping methods in noisy situations. Detailed results are exhibited in Appendix. Over-smoothing analysis. Over-smoothing is a common issue on GNNs (Li, Han, and Wu 2018), which implies that the node representations become indistinguishable as the network depth increases. In this part, we evaluate the effects that various random dropping methods exert on this issue, and measure the degree of over-smoothing by MADGap (Chen et al. 2020). It should be noted that here a smaller value indicates the more indistinguishable node representations and vice versa. Experiments are conducted on Cora with GCNs serving as backbone models. Figure 2a and Figure 2b show the relative increase of MADGap values and test accuracies of the final node representations compared to the original model without any random dropping techniques. The results indicate that all these random dropping methods can alleviate over-smoothing by increasing the MADGap values and test accuracies when the depth of the model increases. Among all random dropping methods, our proposed DropMessage exhibits a superiority of consistency. It obtains an average improvement of 3.3% on MADGap values and an average improvement of 4.9% on test accuracies compared to other random dropping methods when the layer number l \u2265 3. This result can be explained by the fact that DropMessage can generate more various messages than other methods, which prevents the nodes from converging to the same representations to some extent. A more detailed theoretical explanation can be found in Appendix.\nTraining process analysis. We conduct experiments to analyze the loss during the training process when employing different random dropping methods. Figure 2c shows the change of loss in GCN training processes when employing different random dropping methods on Cora. Furthermore, the similar training loss curves can be drawn under other experimental settings. The experimental results suggest that DropMessage presents the smallest sample variance among all methods, thus achieving the fastest convergence and the most stable performance. This is consistent with the theoretical results in Section 4.2.\nInformation diversity analysis. We conduct experiments to evaluate the importance of information diversity for messagepassing GNN models. We set Cora as our experimental dataset, which contains 2708 nodes and 5429 edges. The average node degree of Cora is close to 4. According to Equation 6, the upper bound of dropping rate is calculated from the node degree and the feature dimension. The feature dimension number of Cora is 1433, which is much larger than the number of node degree. Therefore, the upper bound is only determined by the degree of the node. In (backbone)nodewise settings, we set the dropping rate to be equal to its upper bound \u03b4 i = 1 \u2212 1 di for each node. In (backbone)average settings, we set the dropping rate \u03b4 i = 0.75 + i , where i \u223c U nif orm(\u22120.15, 0.15). Both of these settings employ DropMessage. The average random dropping rate of all nodes is almost identical under these two settings, but only the former one can keep the information diversity in expectation. Table 3 presents the results. The (backbone)-nodewise settings outperform (backbone)-average settings regardless of which backbone model is selected.", "publication_ref": ["b23", "b5"], "figure_ref": ["fig_1", "fig_1", "fig_1"], "table_ref": ["tab_3"]}, {"heading": "Conclusion", "text": "In this paper, we propose DropMessage, a general random dropping method for message-passing GNN models. We first unify all random dropping methods to our framework via performing dropping on the message matrix and analyzing their effects. Then we illustrate the superiority of DropMessage theoretically in stabilizing the training process and keeping information diversity. Due to its fine-grained dropping operations on the message matrix, DropMessage shows greater applicability in most cases. By conducting experiments for multiple tasks on five public datasets and two industrial datasets, we demonstrate the effectiveness and generalization of our proposed method.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Derivation Details", "text": "Detailed proof of Theorem 1. Theorem 1 Unbiased random dropping methods introduce an extra regularization term into the objective functions, which make the models more robust.\nWe give more derivation details of Theorem 1. When we use cross-entropy as the loss function, the objective function can be expressed as follows:\nL CE = \u2212 j,yj =1 log(z j ) \u2212 k,y k =0 log(1 \u2212 z k ) = j,yj =1 log(1 + e \u2212hj ) + k,y k =0 log(1 + e h k )\nAccording to above equation, the initial objective function is L CE = j,yj =1 log(1 + e \u2212hj ) + k,y k =0 log(1 + e h k ). When we perturb the message matrix, the objective function can be regarded as a process of adding a bias to the original function, expressed as follows:\nE( L CE ) = j,yj =1 [log(1 + e \u2212hj ) + E(f (h j , h j ))] + k,y k =0 [log(1 + e h k ) + E(g(h k , h k ))]\nwhere f (h j , h j ) = log(1 + e \u2212hj ) \u2212 log(1 + e \u2212hj ), and g(h k , h k ) = log(1 + eh k ) \u2212 log(1 + e h k ). We can approximate it with the second-order Taylor expansion of f (.) and g(.) around h i . Thus, the objective function in expectation can be expressed as bellow:\nE( L CE ) = L CE + E( j,yj =1 [(\u22121 + z j )(h j \u2212 h j ) + 1 2 z j (1 \u2212 z j )(h j \u2212 h j ) 2 ]) + E( k,y k =0 [z k (h k \u2212 h k ) + 1 2 z k (1 \u2212 z k )(h k \u2212 h k ) 2 ]) = L CE + i 1 2 z i (1 \u2212 z i )V ar(h i )\nProof of Theorem 2. Theorem 2 DropMessage presents the smallest sample variance among all existing random dropping methods on messagepassing GNNs with the same dropping rate \u03b4.\nProof. As stated in Lemma 1, all random dropping methods on graphs can be converted to masking operations on the message matrix M. We can measure the difference of message matrices in different epochs by the way of comparing the sample variance of random dropping methods, which can be measured via the norm variance of the message matrix |M| F . Without loss of generality, we assume the original message matrix M is 1 n\u00d7n , i.e., every element is 1. Thus, we can calculate its sample variance via the 1-norm of the message matrix.\nWe consider that the message-passing GNNs do not possess the node-sampler or the edge-sampler, which means every directed edge corresponds to a row vector in the message matrix M. For analytical simplicity, we assume that the graph is undirected and the degree of each node is d. In this case, k = 2|E| = nd rows of the message matrix counts in total. All random dropping methods can be considered as multiple independent Bernoulli samplings. The whole process conforms to a binomial distribution, and so we can calculate the variance of |M|. DropMessage. Perform ncd times of Bernoulli sampling. Dropping an element in the message matrix leads to masking 1 elements in the message matrix. Its variance can be calculated by V ar dm (|M|) = (1 \u2212 \u03b4)\u03b4ncd.\nTherefore, the variances of the random dropping methods are sorted as follows:\nV ar dm (|M|) \u2264 V ar do (|M|) \u2264 V ar dn (|M|)\nV ar dm (|M|) \u2264 V ar de (|M|)\nOur DropMessage has the smallest sample variance among all existing random dropping methods.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiment Details", "text": "Hardware spcification and environment. We run our experiments on the machine with Intel Xeon Gold CPUs (6240@2.6Ghz), ten NVIDIA GeForce 2080ti GPUs (11GB). The code is written in Python 3.8, Pytorch 1.8, and Pytorch-Geometric 1.7. Dataset statistics. Table 4 shows the statistics of datasets. Implementation details. We conduct 20 independent experiments for each setting and obtain the average results. On the five public datasets, we continue to employ the same hyper-parameter settings as previous works have proposed. And on the two real-world datasets, we obtain the best parameters through careful tuning. As for public datasets Cora, CiteSeer, PubMed, and Flickr, we apply two-layer models. However, when it comes to the public dataset ogbn-arxiv and two industrial datasets, Telecom and FinV, we employ three-layer models with two batch normalization layers between the network layers. These experimental settings are identical for node classification tasks and link prediction tasks. The number of hidden units on GCNs is 16 for Cora, CiteSeer, PubMed, and is 64 for others. For GATs, we apply eight-head models with 8 hidden units for Cora, CiteSeer, PubMed, and use single-head models with 128 hidden units for the other datasets. As for APPNPs, we use the teleport probability \u03b1 = 0.1 and K = 10 power iteration steps. The number of hidden units on APPNPs is always 64 for all datasets. In all cases, we use Adam optimizers with learning rate of 0.005 and L 2 regularization of 5 \u00d7 10 \u22124 , and train each model 200 epochs. We adjust the dropping rate from 0.05 to 0.95 in steps of 0.05 and select the optimal one for each setting. Table 5 and 6 present the optimal selections of dropping rates \u03b4.  Results of robustness analysis. We conduct experiments for robustness analysis on three citation networks: Cora, CiteSeer, and PubMed. Specifically, we randomly add a certain ratio of edges (0%, 10%, 20%, 30%) into these datasets and perform the node classification. Table 8 summarizes the classification accuracy of robustness analysis.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4", "tab_5", "tab_8"]}, {"heading": "Related Works About Data Augmentations", "text": "In this section, we introduce some previous works about data augmentation techniques that are related to the random dropping methods discussed in our paper. GNN's effectiveness tends to be weakened due to the noise and low-resource problems in real-world graph data (Dai, Aggarwal, and Wang 2021) (Ding et al. 2022a)(Sun, Lin, and Zhu 2020). Data augmentation has attracted a lot of research interest as it is an effective tool to improve model performance in noisy settings (Zhao et al. 2021). However, apart from i.i.d. data, graph data, which is defined on non-Euclidean space with multi-modality, is hard to be handled by conventional data augmentation methods (Ding et al. 2022c). To address this problem, an increasing number of graph data augmentation methods have been proposed, which include feature-wise (Velickovic et al. 2019a), structure-wise (Cai, Wang, and Wang 2021) (Jin et al. 2021), and label-wise augmentations (Zhang et al. 2017) (Verma et al. 2019).", "publication_ref": ["b10", "b52", "b12", "b38", "b19", "b50", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "Additional Experiments", "text": "Results on SOTA models. We also compare the performance of different random dropping methods and our proposed DropMessage on two SOTA backbone models: DAGNN (Liu, Gao, and Ji 2020) and GCNII (Ming Chen, Zengfeng Huang, and Li 2020). We employ these two models to perform node classification tasks on five datasets. Table 9 presents the experimental results. The results indicate that DropMessage consistently outperforms other random dropping methods. Comparison to Random Augmentation Methods. Random dropping methods are similar to random augmentation techniques used in graph contrastive learning. We compare the performance of our proposed DropMessage with some widely-used augmentation techniques (Ding et al. 2022b), and their brief descriptions are listed as below.\nNode Dropping: it randomly discards a certain portion of vertices along with their connections. Edge Perturbation: it perturbs the connectivities in graph through randomly adding or dropping a certain ratio of edges. Subgraph: it samples a subgraph using random walk. Table 10 summaries the experimental results on five datasets (Cora, CiteSeer, PubMed, ogbn-arxiv and Flickr) for node classification tasks. The results indicate that our proposed DropMessage consistently outperforms the random data augmentation methods. Graph Property Prediction. We perform the graph property prediction task on ogbg-molhiv and ogbg-molpcba (Hu et al. 2020). They are two molecular property prediction datasets adopted from the MoleculeNet (Wu et al. 2017). Table 11 shows the results of ROC-AUC scores with GCN as the backbone model. Graph Rewiring. We conduct experiments on graph rewiring to evaluate the robustness of random dropping methods. Specifically, we first remove a certain ratio of edges, and then randomly add an equal number of edges. We perform the experiments on three citation datasets with GCN as the backbone model. Table 12 presents the results.  ", "publication_ref": ["b24", "b11", "b18", "b43"], "figure_ref": [], "table_ref": ["tab_1", "tab_1", "tab_1"]}, {"heading": "Acknowledgments", "text": "This work was partially supported by NSFC (62176233), the National Key Research and Development Project of China (2018AAA0101900), and the Fundamental Research Funds for the Central Universities.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Adversarial Attacks. We also conduct experiments to evaluate the effectiveness of random dropping methods against adversarial attacks. We apply PGD attacks (Xu et al. 2019) to perturb the graph structures on Cora and CiteSeer, using GCN as the backbone model. Figure 3 presents the results.", "publication_ref": ["b44"], "figure_ref": [], "table_ref": []}, {"heading": "Theoretical Analysis Towards Over-smoothing", "text": "The over-smoothing issue extensively exists in graph neural networks (Chen et al. 2020;Elinas and Bonilla 2022). As the number of model layers increases, node representations become nearly indistinguishable, which leads to a significant decrease on model performance. However, random dropping methods can alleviate this problem, and our proposed DropMessage achieves the best effect compared with other random dropping methods. Now, we give a theoretical demonstration from the perspective of the information theory by measuring the Shannon entropy (Shannon 2001) of propagated messages. The Shannon entropy quantifies \"the amount of information\", and measures the degree of confusion and diversity of the given variable, which can be expressed as:  \nwhere X denotes all possible values of x. Now, we measure the degree of over-smoothing of the model by calculating the Shannon entropy of the propagated messages. Intuitively, the larger the Shannon entropy, the more diverse the propagated messages are, and the less likely the aggregated representations will converge.\nWe assume there are k types of d-dimension messages propagated in the GNN model. For the i-th type of the propagated messages, they are delivered for t i times by n i nodes. Then, we can calculate the Shannon entropy for the initial messages:\nThen, we consider the Shannon entropy after random dropping with dropping rate \u03b4. For DropEdge and DropNode, they generate blank messages at the ratio \u03b4. So the Shannon entropy can be expressed as:\nWhen it comes to Dropout, the Shannon entropy can be calculated as:\nAs for our proposed DropMessage, the Shannon entropy can be expressed as:\nBesides, we have t i \u2265 n i . So, with proper dropping rate \u03b4, we can obtain: From above derivations, we prove the effectiveness of random dropping methods in alleviating over-smoothing issue, and our proposed DropMessage achieves the best effect.", "publication_ref": ["b5", "b14", "b34"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Learning from hints in neural networks", "journal": "Journal of complexity", "year": "1990", "authors": "Y S Abu-Mostafa"}, {"ref_id": "b1", "title": "Training with Noise is Equivalent to Tikhonov Regularization", "journal": "Neural Computation", "year": "1995", "authors": "C M Bishop"}, {"ref_id": "b2", "title": "Improving the Accuracy and Speed of Support Vector Machines", "journal": "", "year": "1996", "authors": "C Burges; B Sch\u00f6lkopf"}, {"ref_id": "b3", "title": "Albumentations: fast and flexible image augmentations", "journal": "ArXiv", "year": "2020", "authors": "A V Buslaev; A Parinov; E Khvedchenya; V I Iglovikov; A A Kalinin"}, {"ref_id": "b4", "title": "Graph coarsening with neural networks", "journal": "", "year": "2021", "authors": "C Cai; D Wang; Y Wang"}, {"ref_id": "b5", "title": "Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View", "journal": "", "year": "2020", "authors": "D Chen; Y Lin; W Li; P Li; J Zhou; X Sun"}, {"ref_id": "b6", "title": "FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling", "journal": "", "year": "2018", "authors": "J Chen; T Ma; C Xiao"}, {"ref_id": "b7", "title": "Certified Adversarial Robustness via Randomized Smoothing", "journal": "", "year": "2019", "authors": "J M Cohen; E Rosenfeld; J Z Kolter"}, {"ref_id": "b8", "title": "Nrgnn: Learning a label noise resistant graph neural network on sparsely and noisily labeled graphs", "journal": "", "year": "2021", "authors": "E Dai; C Aggarwal; S Wang"}, {"ref_id": "b9", "title": "Convolutional neural networks on graphs with fast localized spectral filtering", "journal": "", "year": "2016", "authors": "M Defferrard; X Bresson; P Vandergheynst"}, {"ref_id": "b10", "title": "Meta propagation networks for graph few-shot semi-supervised learning", "journal": "AAAI", "year": "2022", "authors": "K Ding; J Wang; J Caverlee; H Liu"}, {"ref_id": "b11", "title": "Data Augmentation for Deep Graph Learning: A Survey", "journal": "ArXiv", "year": "2022", "authors": "K Ding; Z Xu; H Tong; H Liu"}, {"ref_id": "b12", "title": "Data augmentation for deep graph learning: A survey", "journal": "", "year": "2022", "authors": "K Ding; Z Xu; H Tong; H Liu"}, {"ref_id": "b13", "title": "Semi-supervised learning on graphs with generative adversarial nets", "journal": "", "year": "2018", "authors": "M Ding; J Tang; J Zhang"}, {"ref_id": "b14", "title": "Addressing Over-Smoothing in Graph Neural Networks via Deep Supervision", "journal": "ArXiv", "year": "2022", "authors": "P Elinas; E V Bonilla"}, {"ref_id": "b15", "title": "GAN-based Synthetic Medical Image Augmentation for increased CNN Performance in Liver Lesion Classification", "journal": "Neurocomputing", "year": "2018", "authors": "W Feng; J Zhang; Y Dong; Y Han; H Luan; Q Xu; Q Yang; E Kharlamov; J Tang; M Frid-Adar; I Diamant; E Klang; M M Amitai; J Goldberger; H Greenspan"}, {"ref_id": "b16", "title": "Neural message passing for quantum chemistry", "journal": "", "year": "2017", "authors": "J Gilmer; S S Schoenholz; P F Riley; O Vinyals; G E Dahl"}, {"ref_id": "b17", "title": "Improving neural networks by preventing co-adaptation of feature detectors", "journal": "ArXiv", "year": "2012", "authors": "G E Hinton; N Srivastava; A Krizhevsky; I Sutskever; R Salakhutdinov"}, {"ref_id": "b18", "title": "Open Graph Benchmark: Datasets for Machine Learning on Graphs", "journal": "", "year": "2020", "authors": "W Hu; M Fey; M Zitnik; Y Dong; H Ren; B Liu; M Catasta; J Leskovec"}, {"ref_id": "b19", "title": "Graph condensation for graph neural networks", "journal": "", "year": "2021", "authors": "W Jin; L Zhao; S Zhang; Y Liu; J Tang; N Shah"}, {"ref_id": "b20", "title": "Variational graph autoencoders", "journal": "", "year": "2016", "authors": "T N Kipf; M Welling"}, {"ref_id": "b21", "title": "Semi-supervised classification with graph convolutional networks", "journal": "", "year": "2017", "authors": "T N Kipf; M Welling"}, {"ref_id": "b22", "title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "journal": "", "year": "2019", "authors": "J Klicpera; A Bojchevski; S G\u00fcnnemann"}, {"ref_id": "b23", "title": "Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning", "journal": "", "year": "2018", "authors": "Q Li; Z Han; X.-M Wu"}, {"ref_id": "b24", "title": "Learning to Drop: Robust Graph Neural Network via Topological Denoising", "journal": "", "year": "2020", "authors": "M Liu; H Gao; S Ji;  Acm; D Luo; W Cheng; W Yu; B Zong; J Ni; H Chen; X Zhang"}, {"ref_id": "b25", "title": "Learning with Marginalized Corrupted Features", "journal": "", "year": "2013", "authors": "L V D Maaten; M Chen; S Tyree; K Q Weinberger"}, {"ref_id": "b26", "title": "Noise injection into inputs in backpropagation learning", "journal": "IEEE Trans. Syst. Man Cybern", "year": "1992", "authors": "K Matsuoka"}, {"ref_id": "b27", "title": "Simple and Deep Graph Convolutional Networks", "journal": "", "year": "2020", "authors": "Ming Chen; Z W Huang; B D Li; Y "}, {"ref_id": "b28", "title": "On Asymptotic Behaviors of Graph CNNs from Dynamical Systems Perspective. ArXiv, abs", "journal": "", "year": "1905", "authors": "K Oono; T Suzuki; K Oono; T Suzuki"}, {"ref_id": "b29", "title": "DropGNN: Random Dropouts Increase the Expressiveness of Graph Neural Networks", "journal": "", "year": "2021", "authors": "P A Papp; K Martinkus; L Faber; R Wattenhofer"}, {"ref_id": "b30", "title": "A distributed approach to node clustering in decentralized peer-to-peer networks", "journal": "", "year": "2005", "authors": "L Ramaswamy; B Gedik; L Liu"}, {"ref_id": "b31", "title": "The Manifold Tangent Classifier", "journal": "", "year": "2011", "authors": "S Rifai; Y Dauphin; P Vincent; Y Bengio; X Muller"}, {"ref_id": "b32", "title": "Dropedge: Towards deep graph convolutional networks on node classification", "journal": "", "year": "2019", "authors": "Y Rong; W Huang; T Xu; J Huang"}, {"ref_id": "b33", "title": "Collective classification in network data", "journal": "", "year": "2008", "authors": "P Sen; G Namata; M Bilgic; L Getoor; B Galligher; T Eliassi-Rad"}, {"ref_id": "b34", "title": "A mathematical theory of communication", "journal": "Bell Syst. Tech. J", "year": "2001", "authors": "C E Shannon"}, {"ref_id": "b35", "title": "A survey on Image Data Augmentation for Deep Learning", "journal": "Journal of Big Data", "year": "2019", "authors": "C Shorten; T M Khoshgoftaar"}, {"ref_id": "b36", "title": "Multi-stage selfsupervised learning for graph convolutional networks on graphs with few labeled nodes", "journal": "", "year": "1998", "authors": "P Y Simard; Y A Lecun; J S Denker; B Victorri;  Springer; K Sun; Z Lin; Z Zhu"}, {"ref_id": "b37", "title": "Graph Attention Networks", "journal": "", "year": "2018", "authors": "P Velickovic; G Cucurull; A Casanova; A Romero; ' Lio; P Bengio; Y "}, {"ref_id": "b38", "title": "", "journal": "Deep Graph Infomax. ICLR (Poster)", "year": "2019", "authors": "P Velickovic; W Fedus; W L Hamilton; P Li\u00f2; Y Bengio; R D Hjelm"}, {"ref_id": "b39", "title": "", "journal": "Deep Graph Infomax. ICLR", "year": "2019", "authors": "P Velickovic; W Fedus; W L Hamilton; ' Lio; P Bengio; Y Hjelm; R D "}, {"ref_id": "b40", "title": "Manifold mixup: Better representations by interpolating hidden states", "journal": "", "year": "2013", "authors": "V Verma; A Lamb; C Beckham; A Najafi; I Mitliagkas; D Lopez-Paz; Y Bengio;  Pmlr; S Wager; S I Wang; P Liang"}, {"ref_id": "b41", "title": "", "journal": "", "year": "", "authors": "L Wan; M D Zeiler; S Zhang; Y Lecun; R Fergus"}, {"ref_id": "b42", "title": "Regularization of Neural Networks using DropConnect", "journal": "", "year": "", "authors": ""}, {"ref_id": "b43", "title": "Molecu-leNet: A Benchmark for Molecular Machine Learning", "journal": "", "year": "2017", "authors": "Z Wu; B Ramsundar; E N Feinberg; J Gomes; C Geniesse; A S Pappu; K Leswing; V S Pande"}, {"ref_id": "b44", "title": "Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective", "journal": "", "year": "2019", "authors": "K Xu; H Chen; S Liu; P.-Y Chen; T.-W Weng; M Hong; X Lin"}, {"ref_id": "b45", "title": "Representation Learning on Graphs with Jumping Knowledge Networks", "journal": "", "year": "2018", "authors": "K Xu; C Li; Y Tian; T Sonobe; K Kawarabayashi; S Jegelka"}, {"ref_id": "b46", "title": "Mining Fraudsters and Fraudulent Strategies in Large-Scale Mobile Social Networks", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2021", "authors": "Y Yang; Y Xu; Y Sun; Y Dong; F Wu; Y Zhuang"}, {"ref_id": "b47", "title": "Understanding Default Behavior in Online Lending", "journal": "", "year": "2019", "authors": "Y Yang; Y Xu; C Wang; Y Sun; F Wu; Y Zhuang; M Gu"}, {"ref_id": "b48", "title": "Graph convolutional neural networks for web-scale recommender systems", "journal": "", "year": "2018", "authors": "R Ying; R He; K Chen; P Eksombatchai; W L Hamilton; J Leskovec"}, {"ref_id": "b49", "title": "GraphSAINT: Graph Sampling Based Inductive Learning Method", "journal": "ArXiv", "year": "2020", "authors": "H Zeng; H Zhou; A Srivastava; R Kannan; V Prasanna"}, {"ref_id": "b50", "title": "mixup: Beyond empirical risk minimization", "journal": "", "year": "2017", "authors": "H Zhang; M Cisse; Y N Dauphin; D Lopez-Paz"}, {"ref_id": "b51", "title": "PairNorm: Tackling Oversmoothing in GNNs. ArXiv, abs", "journal": "", "year": "1909", "authors": "L Zhao; L Akoglu"}, {"ref_id": "b52", "title": "Data augmentation for graph neural networks", "journal": "", "year": "2021", "authors": "T Zhao; Y Liu; L Neves; O Woodford; M Jiang; N Shah"}, {"ref_id": "b53", "title": "Robust graph convolutional networks against adversarial attacks", "journal": "", "year": "2019", "authors": "D Zhu; Z Zhang; P Cui; W Zhu"}, {"ref_id": "b54", "title": "Adversarial Attacks on Neural Networks for Graph Data. Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining", "journal": "", "year": "2018", "authors": "D Z\u00fcgner; A Akbarnejad; S G\u00fcnnemann"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: Illustrations of DropMessage and other existing random dropping methods. Considering the messages propagated by the center node (i.e., Node 1), DropMessage allows to propagate distinct messages to different neighbor nodes, and its induced message matrix can be arbitrary. The induced message matrices of other methods obey some explicit constraints and can be regarded as special forms of DropMessage.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Over-Smoothing and Training Process Analysis.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Dropout. Perform nc times of Bernoulli sampling. Dropping an element in the feature matrix leads to masking d elements in the message matrix. Its variance can be calculated by V ar do (|M|) = (1 \u2212 \u03b4)\u03b4ncd 2 . DropEdge. Perform nd 2 times of Bernoulli sampling. Dropping an element in the adjacency matrix leads to masking 2c elements in the message matrix. Its variance can be calculated by V ar de (|M|) = 2(1 \u2212 \u03b4)\u03b4nc 2 d. DropNode. Perform n times of Bernoulli sampling. Dropping an element in the node set leads to masking cd elements in the message matrix. Its variance can be calculated by V ar dn (|M|) = (1 \u2212 \u03b4)\u03b4nc 2 d 2 .", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "We theoretically demonstrate the effectiveness of the random dropping methods, filling the gap in this field.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Overview of different random dropping methods in a view of Bernoulli sampling process.", "figure_data": "MethodFormulaDropout"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Comparison results of different random dropping methods. The best results are in bold, while the second-best ones are underlined.", "figure_data": "Task & DatasetNode classificationLink predictionModelCora CiteSeer PubMed ogbn-arxivFlickr TelecomFinVCoraCiteSeer PubMedGCN80.6870.8378.9770.080.51880.60800.42200.91980.89590.9712GCN-Dropout83.1671.4879.1371.160.52220.66010.45260.92780.91070.9766GCN-DropEdge81.6971.4379.0670.880.52140.66500.47290.92950.90670.9762GCN-DropNode83.0472.1279.0070.980.52130.62430.45710.92380.90520.9748GCN-DropMessage83.3371.8379.2071.270.52230.67100.48760.93050.90710.9772GAT81.3570.1477.2070.320.49880.70500.44670.91180.88950.9464GAT-Dropout82.4171.3178.3171.280.49980.73820.45390.91820.90550.9536GAT-DropEdge81.8271.1777.7070.670.50040.75680.48960.92060.90370.9493GAT-DropNode82.0871.4477.9870.960.49920.72140.46470.92240.91040.9566GAT-DropMessage82.2071.4878.1471.130.50130.75740.48610.92160.90760.9553APPNP81.4570.6279.7969.110.50470.62170.39520.90580.88440.9531APPNP-Dropout82.2371.9379.9269.360.50550.65780.40230.91190.90710.9611APPNP-DropEdge82.7572.1079.8369.150.50610.65910.41490.91390.91310.9626APPNP-DropNode81.7971.5079.8169.270.50530.64120.41820.90680.89790.9561APPNP-DropMessage82.3772.6580.0469.720.50720.66190.43780.91650.91410.9634"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ": Classification accuracy (%) for information diver-sity analysis (where AVG denotes average, and NW denotesnodewise).ModelGCNGATAPPNPAVGNWAVGNWAVGNWAccuracy (%) 81.62 82.67 80.81 81.61 80.71 81.56"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Dataset Statistics.", "figure_data": "DatasetNodesEdgesFeature ClassesTrain/Val/TestCora2708542914337140 / 500 / 1000CiteSeer3327473237036120 / 500 / 1000PubMed1971744338500360 / 500 / 1000ogbn-arxiv 169343 11662431284090941 / 29799 / 48603Flickr89250899756500750% / 25% / 25%FinV340751 1575498261260% / 20% / 20%Telecom509304 80999621260% / 20% / 20%"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Statistics of Optimal Dropping Rates \u03b4 for Link Predictions.Additional data for comparison results. For the node classification task, the performance is measured by accuracy on four public datasets (Cora, CiteSeer, PubMed, ogbn-arxiv). As for Flickr and two imbalanced industrial datasets, we employ F1 scores. When it comes to the link prediction task, we calculate the AUC values for comparisons. Table7present the std values of comparison results.", "figure_data": "DatasetDropout DropEdge DropNode DropMessageGCN-Cora0.200.450.400.40GAT-Cora0.200.400.150.45APPNP-Cora0.500.350.050.35GCN-CiteSeer0.250.300.250.50GAT-CiteSeer0.250.500.150.50APPNP-CiteSeer0.500.500.100.40GCN-PubMed0.250.250.500.25GAT-PubMed0.500.150.250.50APPNP-PubMed0.200.250.050.20"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Statistics of Optimal Dropping Rates \u03b4 for Node Classifications.", "figure_data": "DatasetDropout DropEdge DropNode DropMessageGCN-Cora0.900.400.900.90GAT-Cora0.900.400.250.90APPNP-Cora0.850.700.150.80GCN-CiteSeer0.800.400.850.90GAT-CiteSeer0.350.900.450.90APPNP-CiteSeer0.850.450.350.80GCN-PubMed0.200.100.100.15GAT-PubMed0.600.700.350.85APPNP-PubMed0.800.500.100.75GCN-ogbn-arxiv0.200.200.150.25GAT-ogbn-arxiv0.200.200.150.20APPNP-ogbn-arxiv0.200.200.200.25GCN-Flickr0.150.200.150.20GAT-Flickr0.350.150.050.40APPNP-Flickr0.350.350.050.35"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "The Std Values of Comparison Results.", "figure_data": "Task & DatasetNode classificationLink predictionModelCora CiteSeer PubMed ogbn-arxivFlickr TelecomFinVCoraCiteSeer PubMedGCN0.370.550.670.410.00440.00430.00610.00220.00530.0043GCN-Dropout0.680.590.770.520.00500.00670.00690.00440.00730.0055GCN-DropEdge0.910.720.810.540.00610.00550.00770.00450.00770.0080GCN-DropNode1.060.950.880.750.00640.00790.00890.00640.00910.0094GCN-DropMessage0.590.590.530.560.00420.00270.00550.00440.00640.0051GAT0.590.620.530.370.00420.00330.00430.00420.00470.0049GAT-Dropout0.770.710.670.490.00630.00460.00670.00540.00620.0066GAT-DropEdge0.821.020.790.540.00770.00520.00550.00790.00820.0087GAT-DropNode0.890.860.880.620.00790.00770.00910.00930.00920.0085GAT-DropMessage0.690.670.470.540.00520.00550.00560.00370.00320.0077APPNP0.330.340.510.330.00240.00320.00530.00380.00170.0069APPNP-Dropout0.430.540.430.290.00320.00590.00720.00630.00460.0042APPNP-DropEdge0.720.710.860.550.00390.00320.00880.00870.00580.0098APPNP-DropNode0.490.420.660.730.00540.00650.00770.00740.00350.0103APPNP-DropMessage0.520.240.370.380.00440.00390.00440.00650.00410.0032"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Classification Accuracy (%) for Robustness Analysis.", "figure_data": "DatasetCoraCiteSeerPubMedModel0%10%20%30%0%10%20%30%0%10%20%30%"}], "formulas": [{"formula_id": "formula_0", "formula_text": "h (l+1) i = \u03b3 (l) (h (l) i , AGG j\u2208N (i) (\u03c6 (l) (h (l) i , h (l) j , e j,i ))) (1)", "formula_coordinates": [3.0, 61.95, 445.97, 230.55, 14.07]}, {"formula_id": "formula_1", "formula_text": "M (l) (i,j) = \u03c6 (l) (h (l) i , h (l) j , e (l) j,i )", "formula_coordinates": [3.0, 115.47, 580.78, 115.57, 14.68]}, {"formula_id": "formula_2", "formula_text": "M i,j = 1 1\u2212\u03b4 i,j M i,j , where i,j \u223c Bernoulli(1 \u2212 \u03b4)", "formula_coordinates": [3.0, 319.14, 344.35, 240.1, 24.27]}, {"formula_id": "formula_3", "formula_text": "X i,j = X i,j DropEdge A i,j = A i,j DropNode X i = X i DropMessage M i,j = M i,j s.t. \u223c Bernoulli(1 \u2212 \u03b4)", "formula_coordinates": [4.0, 88.92, 108.33, 167.76, 61.41]}, {"formula_id": "formula_4", "formula_text": "M drop = {M i |edge(M i ) \u2208 E drop } in the message matrix M, where edge(M i ) indicates which edge that M i corresponds to. DropNode. Dropping the elements V drop = {X i | i = 0} in the feature matrix X is equivalent to masking elements M drop = {M i |node(M i ) \u2208 V drop } in the message matrix M,", "formula_coordinates": [4.0, 53.64, 274.6, 239.1, 78.09]}, {"formula_id": "formula_5", "formula_text": "L CE = j,yj =1 log(1 + e \u2212hj ) + k,y k =0 log(1 + e h k ) (2)", "formula_coordinates": [4.0, 63.86, 678.41, 228.64, 22.88]}, {"formula_id": "formula_6", "formula_text": "E( L CE ) = L CE + i 1 2 z i (1 \u2212 z i )V ar(h i ) (3)", "formula_coordinates": [4.0, 348.92, 109.7, 209.08, 26.65]}, {"formula_id": "formula_7", "formula_text": "L CE + i 1 2 z ci i (1 \u2212 z ci i )V ar(h ci i )", "formula_coordinates": [4.0, 319.5, 290.59, 145.09, 13.98]}, {"formula_id": "formula_8", "formula_text": "\u03b4 i \u2264 1 \u2212 min( 1 di , 1 c )", "formula_coordinates": [5.0, 139.09, 475.86, 86.69, 13.47]}, {"formula_id": "formula_9", "formula_text": "E(|M f |) \u2265 1 \u21d2 (1 \u2212 \u03b4)c \u2265 1 \u21d2 \u03b4 \u2264 1 \u2212 1 c (4)", "formula_coordinates": [5.0, 82.54, 564.44, 209.96, 22.31]}, {"formula_id": "formula_10", "formula_text": "E(|M e |) \u2265 1 \u21d2 (1 \u2212 \u03b4 i )d i \u2265 1 \u21d2 \u03b4 i \u2264 1 \u2212 1 d i (5)", "formula_coordinates": [5.0, 70.38, 634.99, 222.12, 23.23]}, {"formula_id": "formula_11", "formula_text": "\u03b4 i \u2264 1 \u2212 min( 1 d i , 1 c )(6)", "formula_coordinates": [5.0, 129.86, 685.11, 162.64, 23.23]}, {"formula_id": "formula_12", "formula_text": "L CE = \u2212 j,yj =1 log(z j ) \u2212 k,y k =0 log(1 \u2212 z k ) = j,yj =1 log(1 + e \u2212hj ) + k,y k =0 log(1 + e h k )", "formula_coordinates": [10.0, 202.42, 156.59, 207.17, 49.97]}, {"formula_id": "formula_13", "formula_text": "E( L CE ) = j,yj =1 [log(1 + e \u2212hj ) + E(f (h j , h j ))] + k,y k =0 [log(1 + e h k ) + E(g(h k , h k ))]", "formula_coordinates": [10.0, 205.34, 257.58, 201.31, 52.04]}, {"formula_id": "formula_14", "formula_text": "E( L CE ) = L CE + E( j,yj =1 [(\u22121 + z j )(h j \u2212 h j ) + 1 2 z j (1 \u2212 z j )(h j \u2212 h j ) 2 ]) + E( k,y k =0 [z k (h k \u2212 h k ) + 1 2 z k (1 \u2212 z k )(h k \u2212 h k ) 2 ]) = L CE + i 1 2 z i (1 \u2212 z i )V ar(h i )", "formula_coordinates": [10.0, 183.73, 359.57, 244.55, 103.69]}], "doi": ""}