{"title": "JPDAF Based HMM for Real-Time Contour Tracking", "authors": "Yunqiang Chen; Yong Rui; Thomas S Huang", "pub_date": "", "abstract": "Tracking objects using multiple cues yields more robust results. The well-known hidden Markov model (HMM) provides a powerful framework to incorporate multiple cues by expanding its observation. However, a plain HMM does not capture the inter-correlation between measurements of neighboring states when computing the transition probabilities. This can seriously damage the tracking performance. To overcome this difficulty, in this paper, we propose a new HMM framework targeted at contour-based object tracking. A joint probability data association filter (JPDAF) is used to compute the HMM's transition probabilities, taking into account the intercorrelated neighboring measurements. To ensure real-time performance, we have further developed an efficient method to calculate the data association probability via dynamic programming, which allows the proposed JPDAF-HMM to run comfortably at 30 frames/sec. This new tracking framework not only can easily incorporate various image cues (e.g., edge intensity, foreground region color and background region color), but also offers an on-line learning process to adapt to changes in the scene. To evaluate its tracking performance, we have applied the proposed JPDAF-HMM in various realworld video sequences. We report promising tracking results in complex environments.", "sections": [{"heading": "Introduction", "text": "Many real-world applications, e.g., video surveillance [13] and video conferencing [14], require robust visual object tracking. Unfortunately, robust and efficient visual tracking in complex environments is still an open problem even after years of research.\nTo discriminate targets from clutter, various image cues have been proposed. For example, object contour is used in [9], face template is used in [4], and color distributions are used in [5,24]. Because each of the above features may not be robust enough individually, more and more researchers are resorting to multiple visual cues [2,18,10,20]. The main difficulty for multi-feature based tracking is, however, how to probabilistically integrate and adapt the multiple cues when objects and background gradually changetheir appearance.\nHidden Markov model (HMM) [17] provides a powerful and efficient way to incorporate multiple features by expanding the observation vectors. However, extending the HMM structure from handling 1D time series to 2D imagery data is challenging. Pseudo-2D-HMM (embedded HMM) has been proposed for character recognition [12], face recognition [15] and template matching [19]. A two-level HMM is defined, which consists of a set of super states, along with a set of embedded states. The super states model the horizontal dimension while the embedded states model the vertical dimension. The difficulty of applying this approach to real-time object tracking is the large number of parameters to be trained.\nIn this paper, we propose a new type of HMM targeted at real-time object tracking. We use parametric shape to model object contours. Observations are collected along the normal lines of the contour (see Figure 1) as in [9]. We define the HMM states to be the contour point location along each normal line. If we have pixels on a normal line, we have states in the HMM. This representation allows us to convert the contour in 2D image plane into an easier-to-solve 1D problem. Furthermore, this new HMM formulation allows us to develop a powerful object tracking framework in the following way: \u00a1 Multiple tracking cues, e.g., edge intensity, foreground /background region properties, can easily be integrated into the tracking process probabilistically.", "publication_ref": ["b12", "b13", "b8", "b3", "b4", "b23", "b1", "b17", "b9", "b19", "b16", "b11", "b14", "b18", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "\u00a1", "text": "In addition to using the standard contour smoothness constraint to compute the state transition probabilities, we further develop a joint probability data association filter (JPDAF) to encode richer inter-relationships between neighboring measurements, thus leading to more accurate transition probabilities. The resultant new HMM is termed JPDAF-HMM.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\u00a1", "text": "A robust on-line training process similar to the training of traditional HMM (Baum Algorithm) is also proposed to adapt the observation model through time.\nThe rest of the paper is organized as follows. In Section 2, we present a 1D HMM framework that can easily incorporate multiple visual cues. Specifically, we will discuss the 1D contour representation, a multi-cue observation model, and a transition probability model based on the contour smoothness constraint. The transition probability is one of the most important components in a HMM. More accurate transition probabilities lead to more accurate and robust tracking results. In Section 3, in addition to the standard contour smoothness constraint as used in Section 2, we develop a JPDAF to encode richer interrelationships between neighboring measurements, thus leading to more accurate transition probabilities. An efficient algorithm for calculating JPDAF is also presented for real-time performance. In Section 4, We present an adaptive learning process to update the observation models through time to handle dynamic environments. The overview of the tracking diagram is also presented. To evaluate the performance of the proposed approach, we have implemented a real-time tracking system, and applied it on various real-world video sequences. We report promising tracking results in Section 5. Concluding remarks are given in Section 6.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Contour tracking using HMM", "text": "For tracking non-rigid objects in complex environments, the active contour model has been proved to be a powerful tool [11,21,16]. For real-time tracking, it is imperative to have an efficient optimization method to find the best contour. A common difficulty of the active contour model is its deficiency in recursively refining the contours in the 2D image plane [1,6]. In fact, because of the aperture problem, only the deformations along the normal lines of the contours can be detected. So, we can restrict the contour searching to a set of normal lines to the predicted contour position (see Figure 1). Let , be the index of pixels along a normal line. Furthermore, let . This is shown in Figure 2.", "publication_ref": ["b10", "b20", "b15", "b0", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Figure 2. Graphic model of contour tracking", "text": "In Section 2.1., we will give detailed description on how to incorporate multiple cues into the observation model. A simplified state transition model is then discussed in Section 2.2 based on the contour smoothness constraint. Given the observation model and the state transition probabilities, in Section 2.3, we present how to find the optimal contour efficiently via the Viterbi algorithm [17].\nDifferent from other contour models [7] that also resort to the Dynamic Programming to obtain the global optimal contour, HMM offers an elegant way to integrate multiple visual cues and a probabilistic model adaptation formula (shown in Section 4) to adapt itself to the dynamic environments, which is very important for a robust tracking system.  (3) where is the prior probability of hypothesis g h . In addition to the edge likelihood model, other cues about the region properties of the foreground and background, e.g., mixture color models, can easily be integrated into the HMM framework. Let \" A n %", "publication_ref": ["b16", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Observation likelihood of multiple cues", "text": "g i h \u00a3 j p B k l m \u00a3 0 n p o h q \u00a3 g \u00a5 \u00a7 h \u00a9 \u00a9 \u00a9 \u00a7 r v g l s \u00a3 j p 4 k l m \u00a3 t m \u00a7 6 k s u \u00a3 n p o 7 v X \u00a3 d \u00a5 7 \u00a7 \u00a9 \u00a9 \u00a9 \u00a7 \u00a7 v x w \u00a3 y q ! v(\nand \" A % represent the color distribution for the foreground (FG) and background (BG), respectively. The posterior probabilities X \" % and X \"\nA n % c an be derived: X\n(Assume \" A n % ( \u00a3 Q \" % ) X \" % \u00a3 \" 2 % \" A % U \" 2 n % (4) X \" F n % ( \u00a3 \" 2 n % \" 2 % U \" A n %(5)\n\" $ y s r B % \u00a3 V \" F er 4 7 % g b # X \" A n \u00a3 ' # \" A % 6 % b t X \" X \u00a3 \u00a4 & \" A b % @ %(6)\nOther cues can also be integrated in a similar way. As we will show in Section 4, our proposed HMM framework also allows us to update the foreground/background color model in a probabilistic way through time.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Computing transition probabilities", "text": "In addition to the observation model discussed in the previous sub-section, another important component in HMM is the transition probability. It determines how a state transits to another state. In this section, we will use the standard contour smoothness constraint to derive the transition probability. We will defer a much more sophisticated smoothness constraint based on region properties to Section 3.\nHere we follow the philosophy in traditional snake model [1], which use an internal energy term to penalize the roughness of the contour. But instead of using an internal energy in an optimization framework, we encode this constraint in transition probabilities. To achieve this, the smoothness constraint has to be represented in a causal form. In Figure 1, we can see when the normal lines are dense (e.g., 30 in our experiments), the true contour points on adjacent normal lines tend to have the same displacement from the predicted contour position (indexed as 0 on each normal line). This correlation is causal and can be captured by transition probabilities \" F r E r 4 7 t%\n: \" F r 9 r 7 t % ( \u00a3 D h k b @ h 8 2 \u00a1 e \u00a2 e \u00a3 s \u00a1 \u00a4 (7\n)\nwhere D is a normalization constant and | is a predefined constant that regulates the smoothness of the contour. This transition probability will penalize sudden changes of the contour points between adjacent lines, hence resulting in a smooth contour. The best contour can be obtained by the Viterbi algorithm described in the following section. ", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Best contour searching by Viterbi algorithm", "text": "i\u00aa \u00a3 \u00ab \u00ac 6 \u00ae X \u00ab \u00b0 X \" i w % \u00a3 \u00ab \u00ac 6 ( \u00ae \u00ab \u00b0 X \" i \u00a7 w % (8) Let's define \u00b1 \" F \u00a2 \u00a7 e # % ( \u00a3 \u00b2 f \u00a7 8 1 @ h 8 X \" F y \u00a7 e r 7 t s \u00a7 e r \u00a3 S # % (9)\nUsing the Markov conditional independence assumptions, it can be recursively computed as follows: \n\u00b1 \" $ \u00a2 \u00a7 e # % ( \u00a3 X \" F y r 4 \u00a3 S # % b \u00ae X \u00ab l X \" $ r B i \u00a3 0 r B 7 t \u00a3 \u00b3 q ' % \u00b1 \" q 7 \u00a7 \u00a2 \u00a5 B % (10) q \u00aa \" $ \u00a2 \u00a7 e # % ( \u00a3 X \" F y r 4 \u00a3 S # % G \u00ab \u00ac 6 ( \u00ae \u00ab l X \" $ r \u00a3 0 r 7 t m \u00a3 V q ' % \u00b1 \" q 7 \u00a7 \u00a2 \u00a5 B %(\n\u00bc \u00aa \u00a3 d \" A \u00bb \u00c5 \u00bb % t \u00bb \u00c5 \u00bd (13\n)\nThe above ellipse representation\n\u00bc \u00a3 \u00b5 P \u00a7 # \u00a7\u00b8 \u00a7 e D s \u00a7 \u00b9 \u00a7 6 k h R 2 \u00c5\nis convenient mathematically. But there is no clear physical interpretations of the five parameters. In tracking, a different 5element ellipse representation is normally used:\n\u00c8 \u00a3 g P1 \u00c9 \u00a7 6 5 # \u00a7 e \u00ca ( \u00a7 6 \u00cb \u00a7 \u00cc ( R\nwhere ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Improving transition probabilities:", "text": "The JPDAF-HMM Transition probability is one of the most important parameters in an HMM. It directly affects the best state sequence estimation. In Section 2.2, we have derived a simplified way of computing transition probabilities based on the contour smoothness constraint. Even though simple, it does not take into account all the information available to us. Specifically, the contour smoothness constraint only considers the contour points themselves. All the other pixels on the measurement lines are ignored. Unfortunately, considering the contour points only can be dangerous. This is especially true when noise is presented in the measurement, which is almost always the case in reality. In this section, we develop a JPDAFbased method to encode not only the contour smoothness constraint, but also the region smoothness constraint observed on all the pixels along the normal lines. To ensure real-time performance, we further develop an efficient JPDAF algorithm by using dynamic programming. The resulted JPDAF-HMM runs comfortably in real-time.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Encoding region smoothness constraint using JPDAF", "text": "Under normal condition, pixel intensity values of human body parts (e.g., face or head) change smoothly inside their regions. It is therefore a reasonable assumption that in human tracking, the foreground and background have smooth region properties so that the measurements in Equation (1) on two adjacent lines are similar. Let   \nThe region smoothness concept can be illustrated by a synthesized image in Figure 3. There are two regions where the lighter region represents the object and the darker region represents the background clutter. There are two adjacent normal lines shown in the figure, i.e., line 1 and line 2. Points ' \u00a7 ' and '\u00b8' are detected edge points on line 1. Similarly, points 'D ' and ' \u00b9 ' are detected edge points on line 2. Our goal is to find where are the contour points on these two lines. The measurements on these two lines are shown in Figure 4. They are similar to each other except for some distortions. Based on the contour smoothness constraint only, the contour from ' \u00a7 ' to 'D ' and the contour from '\u00b8' to 'D ' have almost the same transition probabilities because \u00a7 y D \u00c9 \u00d7 \u00a6 D\n. However, if we consider the region smoothness assumption as well, the possible contour can only be ' \u00a7 \u00b9 ' or '\u00b8D ', but never be ' \u00a7 To verify this region smoothness concept, we have applied it on the synthesized image shown in Figure 5. When we use the region smoothness constraint, it has a large penalty for the contour to jump from\n\u00d8 \u00a7 8 \u00d8 t o \u00d8 D \u00d8\n, and the result is shown in Fig- Different from the uniform statistic region model in [3], our assumption here is more relaxed. The object can have various color regions (e.g. front or side view of human head with hair as shown in experiments) each of which is homogeneous locally. To illustrate this, another test is shown in Figure 6 which has different intensity regions in the foreground. We can see the difference between Figure 5 and Figure 6: the observations on line 2 are not the same. There is no segment 'D \u00b9 '. No matter we match ' \u00a7 ' to 'D ' or '\u00b8' to 'D ', the segment ' \u00a7\u00b8' does not have a matching part. Therefore, the region smoothness constraint penalties are the same for matching ' \u00a7 ' to 'D ' or '\u00b8' to 'D '. The algorithm favors the result in Figure 6 (b) because it is smoother.", "publication_ref": ["b2"], "figure_ref": ["fig_3", "fig_6", "fig_6", "fig_6"], "table_ref": []}, {"heading": "Efficient association by dynamic programming", "text": "In the previous sub-section, we have shown that much more accurate transition probabilities between adjacent lines can be obtained by using the region smoothness constraint, which in turn results in better results (see Equation ( 14)). To ensure real-time tracking performance, in this sub-section we will explore an efficient algorithm for implementing the JPDAF.\nThe association probabilities between all the possible pairs of states (\"\nF T \u00d9 U \u00a5 B %\n) should be calculated. Because we can not assume observations on two adjacent normal lines are exactly the same, correlation of the measurements on two lines can not be used. Windowed correlation technique could be used to estimate \" F r s r 4 7 t% [22]. However, it has the following limitations: 1). it is only an approximation to the true \" $ r # r 7 t %\n; 2). there is no principled way to determine the right window size; and 3). the computation is not efficient: it   transition probabilities, and there is no window size to be determined. Given measurement lines 1 and 2, the calculation of the matching distance can be explained in the following recursive equation (see also Figure 7): \n\u00ce \u00cf \" A e \u00a7 \u00d1 q ' % \u00a3 \u00a4 \u00b2 f $ \u00dd \u00de \u00df \u00e0 \u00df \u00e1 \u00ce \u00cf \" A \u00c9 \u00a5 \u00a7 $ q ' % U \u00b3 \u00b9 \" A t \" 2 b % \u00a7 @ \" q ' % 6 % \u00ce \u00cf m \" A e \u00a7 \u00d1 q \u00a5 B % U \u00b3 \u00b9 \" A t\" 2 b % \u00a7 @ ' \" q ' % 6 % \u00ce \u00cf m \" A \u00c9 \u00a5 \u00a7 $ q y \u00a5 4 % U V \u00b9 \" 2 t\" 2 b % G \u00a7 6 8 7 \" q ' % @ %(15)\n\u00ce \u00d2 \u00ba \" A \u00a7 % \u00a9 \u00a3 \u00ce \u00d2 s \" \u00a7 \u00d1 q ' % \u00a3 \u00dc W to \u00ce \u00d2 \u00ba \" @ \u00a7 %\n. After obtaining all the matching distances, the state transition probabilities can be computed and contour tracking can be accomplished by the Viterbi algorithm described in Section 2.3.", "publication_ref": ["b21"], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "Extending JPDAF-HMM to temporal domain", "text": "The previous two sections have described the JPDAF-HMM model for each individual frame. In this section, we will extend it to handle tracking in video sequences. Specifically, we will discuss how to probabilistically train the parameters for frame C based on the tracking results at frame C \u00a5 . We will then give a complete algorithm for the JPDAF-HMM tracking framework.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "On-line learning of observation model", "text": "In dynamic environment, both object and background may gradually change their appearances. An on-line training is therefore necessary to adapt the observation likelihood models dynamically. A naive way is to completely trust the contour returned by the Viterbi algorithm at frame C \u00ba p \u00a5 , and average all the pixels inside and outside the contour to obtain the new foreground/background color model at frame C . However, if error occurs at frame C \u00a9 \u00b6 \u00a5\n, this procedure may adapt the model in the wrong way. Fortunately, HMM allows us to train the observation models in a probabilistic way.\nInstead of completely trusting the contour obtained at frame C c Q \u00a5\n, we can make a soft decision of how to update the observation models by using the forward-backward algorithm. The \"forward probability distribution\" is defined as follow:  This probability gives us a robust way to weigh different pixels during the observation models adaptation. The more confidently classified pixels will contribute more to the color model while the less confidently classified pixels will contribute less:\n\" 2 % ( \u00a3 \u00e9 9 X \" $ r H % r 8 \" F y \" $ r 4 % ( \u00a3 \u00a3 \u00a4 % \u00e9 # X \" F r H % \" 2 n % ( \u00a3 \u00e9 # X \" F r H \u00e3 n % r 8 \" F y \" $ r 4 % ( \u00a3 \u00a3 \u00a4 % \u00e9 9 X \" F r H f n % (24)\nThe new adapted models will reflect the changing color distributions during the tracking. It is then plugged back into Equation ( 6) during the contour searching in the next frame. To reduce the number of parameters, we are not training the transition probabilities, which is related to the density of the observation lines and remains relative constant during the tracking process.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Tracking diagram", "text": "The complete JDAPF-HMM tracking procedure is summarized as follows (also see Figure 8):\n1. Prediction: Predict where the object will be in the current frame", "publication_ref": [], "figure_ref": ["fig_8"], "table_ref": []}, {"heading": "C", "text": "based on the tracking results in previous frame C \u00a5 and the object's dynamics. Observations are collected along a set of normal lines of the predicted contour. We adopt the Langevin process to model the human movement dynamics [23]: To begin this tracking procedure, a separate initialization module is needed. This can be done either manually or by change detection [20]. We currently hand initialize on the first frame in each sequence.\n\u00ec \u00c8 B \u00ed \u00ee \u00c8\u00ed x \u00ef \u00a3 \u00ec \u00a5 \u2022 \u00f0 W \u00a7 \u00ef \u00ec \u00c8 h \u00ed \u00f1 \u00ee \u00c8\u00ed \u00f1 \u00ef U \u00ec W\u00ef \u00b2 \u00d0 \u00f2(", "publication_ref": ["b22", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "To validate the efficacy and robustness of the proposed JPDAF-HMM approach, we have applied it to various realworld test sequences. In the experiments, the human head is modeled by an ellipse ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\u00a3 g \u00a5 h W", "text": ". We use the Langevin process to model the human movement, as discussed in Section 4. The JPDAF-HMM tracking algorithm is implemented in C++ on Windows 2000 platform. No attempt is made on code optimization and the current system runs at 30 frames/sec comfortably on a standard Dell PIII 1G machine. /zoom camera. The sequences simulate various tracking conditions, including appearance changes, quick movement, out of plane rotation, shape deformation, camera zoom in and out, and partial occlusion. Sequence A, shown in Figure 9, is in a cluttered office environment with 400 frames (30 frames/second). Note that the background clutter (sharp edges and similar color) impose great challenges to any contour based visual tracking algorithms. The color of the door is very similar to that of human face and causes great difficulty to color-based tracking algorithms. For fine-level comparing purpose, we display the contour results returned from the Viterbi algorithm instead of the final ellipse. The comparison is shown in Figure 9. In the plain HMM, tracking is distracted by the strong edges on the background when the foreground boundary does not have high contrast and the error is gradually enlarged. The JPDAF-HMM ensures that the contour is not distracted by the sharp edges on the background because of its sophisticated transition probabilities (see Section 3).\nFigure 10 shows another comparison on Sequence B, a 200frame sequence captured at 30 frames/sec. It is a very cluttered environment with multiple people's presence. There are both appearance and lighting changes of the person's head. The proposed JPDAF-HMM still successfully tracks the target through out the sequence while the plain HMM fails. To test JPDAF-HMM's ability to handle partial occlusion, we have conducted further experiments on Sequence C, which . We obtain promising tracking results in all the tested sequences.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we have proposed a new HMM framework targeted at real-time contour-based object tracking. A joint probability data association filter (JPDAF) is used to compute the HMM's transition probability, taking into account the inter-correlated neighboring measurements. To ensure real-time performance, we have further developed an efficient method to calculate the data association probability via dynamic programming, which allows the proposed JPDAF-HMM to run comfortably at 30 frames/sec. This new tracking framework not only can easily incorporate various image cues (e.g., edge intensity, foreground color and background color), but also offers an on-line learning process to adapt to changes in the scene (see Section 4.1). We have reported promising tracking results on various real-world video sequences.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This work was supported in part by National Science Foundation Grant CDA 96-24396, EIA-99-75019 and in part by Gift Grant from Microsoft Research. Thanks Stan Birchfield for allowing us to use the test sequences.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Using dynamic programming for solving variational problems in vision", "journal": "IEEE Trans. on Pattern Analysis & Machine Intell", "year": "1990-09", "authors": "A Amini; T Weymouth; R Jain"}, {"ref_id": "b1", "title": "Elliptical head tracking using intensity gradients and color histograms", "journal": "", "year": "1998", "authors": "S Birchfield"}, {"ref_id": "b2", "title": "Statistical region snake-based segmentation adapted to different physical noise models", "journal": "IEEE Trans. on Pattern Analysis & Machine Intell", "year": "1999-11", "authors": "C Chesnaud; P Refregier; V Boulet"}, {"ref_id": "b3", "title": "Face detection with informationbased maximum discrimination", "journal": "", "year": "1997", "authors": "A Colmenzrez; T Huang"}, {"ref_id": "b4", "title": "Real-time tracking of non-rigid objects using mean shift", "journal": "", "year": "2000", "authors": "D Comaniciu; V Ramesh; P Meer"}, {"ref_id": "b5", "title": "Active-rays: Polar-transformed active contours for real-time contour tracking", "journal": "Real-Time Imaging", "year": "1999-06", "authors": "J Denzler; H Niemann"}, {"ref_id": "b6", "title": "Dynamicprogramming for detecting, tracking, and matching deformable contours", "journal": "IEEE Trans. on Pattern Analysis & Machine Intell", "year": "1996-05", "authors": "D Geiger; A Gupta; L Costa; J Vlontzos"}, {"ref_id": "b7", "title": "Computer and Robot Vision", "journal": "", "year": "", "authors": "R Haralick"}, {"ref_id": "b8", "title": "Condensation -conditional density propagation for visual tracking", "journal": "Int. J. Computer Vision", "year": "1998", "authors": "M Isard; A Blake"}, {"ref_id": "b9", "title": "Icondensation: Unifying low-level and high-level tracking in a stochastic framework", "journal": "", "year": "1998", "authors": "M Isard; A Blake"}, {"ref_id": "b10", "title": "Snakes: Active contour models", "journal": "Int. J. Computer Vision", "year": "1988", "authors": "M Kass; A Witkin; D Terzopoulos"}, {"ref_id": "b11", "title": "Keyword spotting in poorly printed documents using pseudo-2d hidden markov-models", "journal": "IEEE Trans. on Pattern Analysis & Machine Intell", "year": "1994-08", "authors": "S Kuo; O Agazzi"}, {"ref_id": "b12", "title": "Simultaneous tracking and verification via sequential posterior", "journal": "", "year": "2000", "authors": "B Li; R Chellappa"}, {"ref_id": "b13", "title": "Automating camera management in a lecture room environment", "journal": "", "year": "2000", "authors": "Q Liu; Y Rui; A Gupta; J Cadiz"}, {"ref_id": "b14", "title": "Maximum likelihood training of the embedded hmm for face detection and recognition", "journal": "", "year": "2000", "authors": "A Nefian; M Hayes; Iii "}, {"ref_id": "b15", "title": "Robust tracking of position and velocity with kalman snakes", "journal": "IEEE Trans. on Pattern Analysis & Machine Intell", "year": "1999-06", "authors": "N Peterfreund"}, {"ref_id": "b16", "title": "An introduction to hidden Markov models", "journal": "IEEE ASSP Mag", "year": "1986-01", "authors": "L R Rabiner; B H Juang"}, {"ref_id": "b17", "title": "Joint probabilistic techniques for tracking multi-part objects", "journal": "", "year": "1998", "authors": "C Rasmussen; G Hager"}, {"ref_id": "b18", "title": "Robust person tracking with non-stationary background using a combined pseudo-2d-hmm and kalman-filter approach", "journal": "", "year": "1999", "authors": "G Rigoll; S Muller; B Winterstein"}, {"ref_id": "b19", "title": "Dynamic layer representation and its applications to tracking", "journal": "", "year": "2000-06", "authors": "H Tao; H S Sawhney; R Kumar"}, {"ref_id": "b20", "title": "Tracking with kalman snakes", "journal": "", "year": "1992", "authors": "D Terzopoulos; R Szeliski"}, {"ref_id": "b21", "title": "Keeping your eye on the ball: Tracking occluding contours of unfamiliar objects without distraction", "journal": "", "year": "1995", "authors": "K Toyama; G Hager"}, {"ref_id": "b22", "title": "Nonlinear filtering for speaker tracking in noisy and reverberant environments", "journal": "", "year": "2000", "authors": "J Vermaak; A Blake"}, {"ref_id": "b23", "title": "Color tracking by transductive learning", "journal": "", "year": "2000", "authors": "Y Wu; T S Huang"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "the background. Combining the edge likelihood model and the color posterior probabilities, we have the following multi-cue observation likelihood function:", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "center of the ellipse, \u00ca and \u00cb are the lengths of the major and minor axes of the ellipse, and \u00cc is the orientation of the ellipse. Because \u00bc and \u00c8 are two representations of the same ellipse [8], we use them interchangeably in the paper.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "respectively. These two contour points segment the two lines into foreground segments and background segments. Based on the region smoothness", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 .3Figure 3. An illustration of the region smoothness constraint: both '\u00b8D ' and ' \u00a7", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "contour candidates ' \u00a7 \u00b9 ' and '\u00b8D ' will be further discriminated by HMM based on all the observation lines.)", "figure_data": ""}, {"figure_label": "425", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 . Observations on line 1 and line 2 Figure 5 .425Figure 4. Observations on line 1 and line 2", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 .6Figure 6. Foreground object with multiple regions: (a) The synthesized image. (b) Contour tracking with regions smoothness constraint. (c) Observation on line 1 and line 2.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 7 .7Figure 7. Dynamic programming:", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 8 .8Figure 8. The diagram of the tracking", "figure_data": ""}, {"figure_label": "910", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 9 .Figure 10 .910Figure 9. Sequence A: comparison of plain HMM with JPDAF-HMM: Top row is the results of plain HMM without JPDAF. Bottom row is the results of JPDAF-HMM.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 11 .11Figure 11. Tracking with partial occlusions", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "the predicted contour and indexed as W . If the prediction were always accurate, the detected contour points on all normal lines should be exactly at the center, i.e.,", "figure_data": "D E \" F \u00a2 9 % X \u00a3 Y W  \u00a7 b a c \u00a2 0 H d P \u00a5  \u00a7 e S R . In reality, however, we need to find the true contour point D E \" $ \u00a2 9 % based on the measurements. Note that instead of representingthe contour by a 2D image coordinate, we can now representthe contour by a 1D function To detect the contour points accurately, different cues (edge D \" F \u00a2 9 % G  \u00a7 \u00a2 f \u00a3 g \u00a5  \u00a7 h \u00a9 \u00a9 \u00a9 .  \u00a7intensity, color model of the foreground and background) andprior constraints (e.g. contour smoothness constraint) can beintegrated by using HMM. The hidden states of the HMMare the true contour point on each normal line, (denoted asi w \u00a3 q p s r s t 4  \u00a7 \u00a9 \u00a9 \u00a9  \u00a7 \u00a3 x p s y t s  \u00a7 \u00a9 \u00a9 \u00a9 line \u00a2 . A HMM is specified by the number of states (in our r  \u00a7 h \u00a9 \u00a9 \u00a9  \u00a7 r 4 u I v ) . The observations of the HMM,  \u00a7 e y  \u00a7 \u00a9 \u00a9 \u00a9  \u00a7 e y u , are collected along each normal v case, T U \u00a5 ) , the observation model X \" $ y c r , and the transi-% tion probability \" F r 9 r 7 t % . Given current state , the current r observation y is independent of previous state r E and pre-7 t vious observation y . In addition, because of the Marko-7 t vian property, we have \" F r s rt \u00a7 e r B E  \u00a7 h \u00a9 \u00a9 \u00a9  \u00a7 r 4 7 t% \u00a3 \" $ r B r 4 7 t%! # \" $ & % (1) i s the corresponding image coordinate of the denote the image intensity at pixel on line . That is, \u00a2 ' # \" $ & % ( \u00a3 0 ) # \" 2 1 # 3 4 #  \u00a7 6 5 7 3 4 8 % where \" 2 1 9 3 4 #  \u00a7 @ 5 7 3 4 ' % pixel on the \u00a2 t h normal line. ) & \" A 1 9 3 4 9  \u00a7 6 5 7 3 4 i s the image inten-' % sity at \" 2 1 9 3 4 9  \u00a7 @ 5 8 3 B ' % .Figure 1. Illustration of the 1D contour model:At frame on the tracking results on previous frames. The dashed , the solid curve is the predicted contour based Ccurve is the true contour that we want to find. A set ofmeasurements are collected along thenormal linesof the predicted contour. on the t h normal line. The true contour can be found if D E \" $ \u00a2 is the true contour point # % \u00a2 we can detect all the D E \" F \u00a2 9 % G  \u00a7 \u00a2 I H Q P \u00a5  \u00a7 S R .Each normal line hasT V U \u00a5"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "therefore means that none of the edges is associated with the true contour.With the assumption that the clutter is a Poisson process along the line with spatial density", "figure_data": "where contour, kl\u00a3 p t means the kl \u00a3 z n otherwise. Hypothesis q t h edge is associated with the true g hand the true target mea-{ surement is normally distributed with standard deviation | , 9 } we can obtain the edge likelihood model as follows:\" Ac r\u00a3 S% j \u00a5 U\u00a5 T E |} 4 {ft\" b\" F e# E |} T%%2)"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "To increase the stability of tracking, we reduce the degree of freedom of the contour by fitting it to a parametric shape.", "figure_data": "which can be representation in matrix form: \u00bb 0 \u00a3 \u00be \u00bf \u00bf \u00bf \u00bf \u00c0 1 t 5 t 1 9 t G 5 8 t 1 9 t \u00a9 \u00a9 \u00a9 1 u 5 1u 5u 1u u and \u00bd d \u00a3 \u00c4 P \u00a5  \u00a7 h \u00a5  \u00a7 h \u00a9 \u00a9 \u00a9  \u00a7 h \u00a5 R AE \u00c5 . The parameters of the best-fit ellipse \u00bb \u00bc s \u00a3 , where \u00bd 5 8 t \u00a9 \u00c1 \u00c2 \u00c2 \u00c2 \u00c2 \u00a9 \u00a9 5u \u00c3 \u00bc \u00aa \u00a3 \u00c7 P \u00a7 #  \u00a7\u00b8 \u00a7 6 D s  \u00a7 \u00b9  \u00a7 e k R can be obtained by the least mean square AE \u00c5 (LMS) solution:\u00b111)with the initialization where the initial state probabilities \" b \u00a5 7  \u00a7 e # % \u00a3 \u00ae X \u00ab X \" $ r E t % j \u00a3 X \" F y P  \u00a7 R . The term q \u00aa \" F \u00a2  \u00a7 & % records the \"best previous state\" tr s t % @ X \" F r s t % t e t  \u00a7 r 4 t \u00b5 H , from state at line \u00a2 . We therefore obtain at the end of the sequence \u00ae X \u00ab \u00b0 X \" w  \u00a7 \u00b1 i % \u00a3 0 \u00ae X \u00ab 3 \" F  \u00b6  \u00a7 e # % . The optimal state sequence i s \u00aa can be obtained by back tracking q \u00aa , starting from r \u00aa u \u00a3 j \u00ab \u00ac 6 \u00ae X \u00ab 3 \u00b1 \" $  \u00b6  \u00a7 e # % , with r \u00aa 7 t \u00a3 q \u00aa \" $ r \u00aa  \u00a7 \u00a2 9 % . The com-putation cost of the Viterbi algorithm is y \" $ \u2022 c \" F T U \u00a5 B % . 6 % Unlike traditional active contour model [1, 6], this methodcan give us the optimal contour without recursively search-ing the 2D image plane. Given the best state sequence p s r \u00aa t  \u00a7 \u00a9 \u00a9 \u00a9  \u00a7 r \u00aa u , we denote the corresponding image coordinate i \u00e4 \u00a3 v of the best contour point r \u00aa on line \u00a2 by P 1  \u00a7 @ 5 R .:\u00a7 8 1 Uy 5UD G 15U \u00b3 \u00b9 1U k B 5y \u00a5 \u00ba \u00a3 0 W(12)"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Evaluate the state transition probabilities based on JPDAF as shown in Equation(14). Efficient dynamic programming solution to JPDAF is explained in Section 3.2.", "figure_data": "(c) Best contour: With previously computed observa-tion likelihood and the transition probability matrix,best contour w.r.t the given observations can be findby the Viterbi Algorithm described in Section 2.3.(d) Contour fitting: Based on the detected contour, fitthe best ellipse using Equation (13).. 3. Model adaptation: Using forward-backward algorithmto estimate a soft classification of each pixel (to fore-ground and background) on the normal lines and updatethe color model of foreground and background based onEquation (24). Update the velocity of the objects (e.g.,translation, rotation and scaling). Go to step 1 when newframe arrives.25)where \" b \u00a9 \u00cb \u00c8 \u00a3 \u00c4 P 1  \u00a7 @ 5 9  \u00a7 6 \u00ca 9 \u00f3 \u00f0 ! % ,\u00b8\u00a3 \u00f5 \u00f4  \u00a7 @ \u00cb (  \u00a7 \u00cc ( R \u00a5 \u00a9  \u00a7 i s the parametric ellipse, . \u00cb # \u00f3 is the rate constant, a thermal excitation process drawn from Gaussian distri- \u00a7 \u00b3 \u00a3 is \u00b2bution the steady-state root-mean-square velocity. \" A W !  \u00a7 e \u00f6 % , \u00f0 is the discretization time step andis \u00f42. Contour tracking:(a) Observation likelihood: Evaluate the observationlikelihood for every pixel on each normal line\u00a2 :\" $ y sr 4\u00a3 S &%  \u00a7 e # X H Q P\u00a7R \u00d1  \u00a7 e \u00a2 I H \u00eb P \u00a5  \u00a7S R"}], "formulas": [{"formula_id": "formula_0", "formula_text": "g i h \u00a3 j p B k l m \u00a3 0 n p o h q \u00a3 g \u00a5 \u00a7 h \u00a9 \u00a9 \u00a9 \u00a7 r v g l s \u00a3 j p 4 k l m \u00a3 t m \u00a7 6 k s u \u00a3 n p o 7 v X \u00a3 d \u00a5 7 \u00a7 \u00a9 \u00a9 \u00a9 \u00a7 \u00a7 v x w \u00a3 y q ! v(", "formula_coordinates": [2.0, 341.9, 750.35, 206.58, 42.1]}, {"formula_id": "formula_1", "formula_text": "(Assume \" A n % ( \u00a3 Q \" % ) X \" % \u00a3 \" 2 % \" A % U \" 2 n % (4) X \" F n % ( \u00a3 \" 2 n % \" 2 % U \" A n %(5)", "formula_coordinates": [3.0, 93.3, 323.42, 193.49, 91.33]}, {"formula_id": "formula_2", "formula_text": "\" $ y s r B % \u00a3 V \" F er 4 7 % g b # X \" A n \u00a3 ' # \" A % 6 % b t X \" X \u00a3 \u00a4 & \" A b % @ %(6)", "formula_coordinates": [3.0, 75.4, 489.55, 211.39, 81.6]}, {"formula_id": "formula_3", "formula_text": ": \" F r 9 r 7 t % ( \u00a3 D h k b @ h 8 2 \u00a1 e \u00a2 e \u00a3 s \u00a1 \u00a4 (7", "formula_coordinates": [3.0, 370.3, 151.94, 182.09, 55.51]}, {"formula_id": "formula_4", "formula_text": ")", "formula_coordinates": [3.0, 552.39, 176.06, 3.92, 8.97]}, {"formula_id": "formula_5", "formula_text": "i\u00aa \u00a3 \u00ab \u00ac 6 \u00ae X \u00ab \u00b0 X \" i w % \u00a3 \u00ab \u00ac 6 ( \u00ae \u00ab \u00b0 X \" i \u00a7 w % (8) Let's define \u00b1 \" F \u00a2 \u00a7 e # % ( \u00a3 \u00b2 f \u00a7 8 1 @ h 8 X \" F y \u00a7 e r 7 t s \u00a7 e r \u00a3 S # % (9)", "formula_coordinates": [3.0, 309.12, 355.25, 247.19, 88.9]}, {"formula_id": "formula_6", "formula_text": "\u00b1 \" $ \u00a2 \u00a7 e # % ( \u00a3 X \" F y r 4 \u00a3 S # % b \u00ae X \u00ab l X \" $ r B i \u00a3 0 r B 7 t \u00a3 \u00b3 q ' % \u00b1 \" q 7 \u00a7 \u00a2 \u00a5 B % (10) q \u00aa \" $ \u00a2 \u00a7 e # % ( \u00a3 X \" F y r 4 \u00a3 S # % G \u00ab \u00ac 6 ( \u00ae \u00ab l X \" $ r \u00a3 0 r 7 t m \u00a3 V q ' % \u00b1 \" q 7 \u00a7 \u00a2 \u00a5 B %(", "formula_coordinates": [3.0, 317.1, 442.45, 239.32, 104.9]}, {"formula_id": "formula_7", "formula_text": "\u00bc \u00aa \u00a3 d \" A \u00bb \u00c5 \u00bb % t \u00bb \u00c5 \u00bd (13", "formula_coordinates": [4.0, 122.3, 239.35, 160.28, 25.7]}, {"formula_id": "formula_8", "formula_text": ")", "formula_coordinates": [4.0, 282.58, 243.14, 4.19, 8.97]}, {"formula_id": "formula_9", "formula_text": "\u00bc \u00a3 \u00b5 P \u00a7 # \u00a7\u00b8 \u00a7 e D s \u00a7 \u00b9 \u00a7 6 k h R 2 \u00c5", "formula_coordinates": [4.0, 184.5, 255.65, 69.3, 27.1]}, {"formula_id": "formula_10", "formula_text": "\u00c8 \u00a3 g P1 \u00c9 \u00a7 6 5 # \u00a7 e \u00ca ( \u00a7 6 \u00cb \u00a7 \u00cc ( R", "formula_coordinates": [4.0, 127.3, 312.75, 76.0, 27.3]}, {"formula_id": "formula_12", "formula_text": "\u00d8 \u00a7 8 \u00d8 t o \u00d8 D \u00d8", "formula_coordinates": [4.0, 397.5, 769.1, 35.0, 21.96]}, {"formula_id": "formula_13", "formula_text": "F T \u00d9 U \u00a5 B %", "formula_coordinates": [5.0, 80.8, 680.35, 42.9, 27.1]}, {"formula_id": "formula_14", "formula_text": "\u00ce \u00cf \" A e \u00a7 \u00d1 q ' % \u00a3 \u00a4 \u00b2 f $ \u00dd \u00de \u00df \u00e0 \u00df \u00e1 \u00ce \u00cf \" A \u00c9 \u00a5 \u00a7 $ q ' % U \u00b3 \u00b9 \" A t \" 2 b % \u00a7 @ \" q ' % 6 % \u00ce \u00cf m \" A e \u00a7 \u00d1 q \u00a5 B % U \u00b3 \u00b9 \" A t\" 2 b % \u00a7 @ ' \" q ' % 6 % \u00ce \u00cf m \" A \u00c9 \u00a5 \u00a7 $ q y \u00a5 4 % U V \u00b9 \" 2 t\" 2 b % G \u00a7 6 8 7 \" q ' % @ %(15)", "formula_coordinates": [5.0, 317.6, 592.65, 238.82, 58.2]}, {"formula_id": "formula_15", "formula_text": "\u00ce \u00d2 \u00ba \" A \u00a7 % \u00a9 \u00a3 \u00ce \u00d2 s \" \u00a7 \u00d1 q ' % \u00a3 \u00dc W to \u00ce \u00d2 \u00ba \" @ \u00a7 %", "formula_coordinates": [5.0, 309.4, 726.55, 246.7, 32.1]}, {"formula_id": "formula_16", "formula_text": "\" 2 % ( \u00a3 \u00e9 9 X \" $ r H % r 8 \" F y \" $ r 4 % ( \u00a3 \u00a3 \u00a4 % \u00e9 # X \" F r H % \" 2 n % ( \u00a3 \u00e9 # X \" F r H \u00e3 n % r 8 \" F y \" $ r 4 % ( \u00a3 \u00a3 \u00a4 % \u00e9 9 X \" F r H f n % (24)", "formula_coordinates": [6.0, 328.9, 290.05, 227.52, 79.4]}, {"formula_id": "formula_17", "formula_text": "\u00ec \u00c8 B \u00ed \u00ee \u00c8\u00ed x \u00ef \u00a3 \u00ec \u00a5 \u2022 \u00f0 W \u00a7 \u00ef \u00ec \u00c8 h \u00ed \u00f1 \u00ee \u00c8\u00ed \u00f1 \u00ef U \u00ec W\u00ef \u00b2 \u00d0 \u00f2(", "formula_coordinates": [6.0, 348.6, 578.35, 195.23, 40.9]}], "doi": ""}