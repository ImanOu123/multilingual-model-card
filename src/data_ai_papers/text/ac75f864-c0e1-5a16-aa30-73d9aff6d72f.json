{"title": "Visual, Spatial, Geometric-Preserved Place Recognition for Cross-View and Cross-Modal Collaborative Perception", "authors": "Peng Gao; Jing Liang; Yu Shen; Sanghyun Son; Ming C Lin", "pub_date": "", "abstract": "Place recognition plays an important role in multirobot collaborative perception, such as aerial-ground search and rescue, in order to identify the same place they have visited. Recently, approaches based on semantics showed the promising performance to address cross-view and cross-modal challenges in place recognition, which can be further categorized as graphbased and geometric-based methods. However, both methods have shortcomings, including ignoring geometric cues and affecting by large non-overlapped regions between observations. In this paper, we introduce a novel approach that integrates semantic graph matching and distance fields (DF) matching for cross-view and cross-modal place recognition. Our method uses a graph representation to encode visual-spatial cues of semantics and uses a set of class-wise DFs to encode geometric cues of a scene. Then, we formulate place recognition as a two-step matching problem. We first perform semantic graph matching to identify the correspondence of semantic objects. Then, we estimate the overlapped regions based on the identified correspondences and further align these regions to compute their geometricbased DF similarity. Finally, we integrate graph-based similarity and geometry-based DF similarity to match places. We evaluate our approach over two public benchmark datasets, including KITTI and AirSim. Compared with the previous methods, our approach achieves around 10% improvement in ground-ground place recognition in KITTI and 35% improvement in aerialground place recognition in AirSim. CONFIDENTIAL. Limited circulation. For review only.", "sections": [{"heading": "I. INTRODUCTION", "text": "Multi-robot systems have been widely studied over the past decades due to their scalability [1], parallelism [2] and reliability to failures [3], [4]. To enable efficient multi-robot collaboration, collaborative perception is an essential component to build a shared situational awareness of the surrounding environments by integrating individual perceptions. Collaborative perception has various real-world applications, such as collaborative multi-simultaneous localization and mapping (CSLAM) [5], [6], connected autonomous driving [7], [8], multi-robot delivery [9] and collaboratively search and rescue [10], [11].\nPlace recognition is a fundamental capability in multi-robot collaborative perception, with the goal of deciding if two robots are observing the same place. As shown in Figure 1, when unmanned ground vehicles (UGVs) and unmanned aerial vehicles (UAVs) collaboratively search an area, they need to recognize if they are observing the same place given their own observations before performing further operations, such as merging local maps, collaborative tracking and reasoning. However, place recognition in multi-robot systems is very challenging, as the multi-robot observations can be Peng Gao, Jing Liang, Yu Shen, Sanghyun Son, and Ming C. Lin are with the Department of Computer Science, University of Maryland, College Park, MD, USA. Email: {gaopeng, jingl, yushen, shh1295, lin}@umd.edu. Fig. 1. An example scenario for place recognition in a robot team consisting of ground and aerial robots with different visual sensing capabilities. When three robots collaboratively search an area, before they merge their local maps (graph-based maps), they need to recognize the same place given their observations acquired by different sensors.\nacquired by different sensors and the observations can appear quite different due to large perspective changes.\nGiven the importance of place recognition, a variety of studies have been developed. Traditional methods typically learn representations based on various sensing information, such as RGB images [12], [13] or LiDAR points [14], [15]. However, when a pair of observations have large perspective changes or are acquired from different sensors, these methods will lose effect. Recently, semantic-based approaches demonstrate promising performance to deal with perspective changes [16], [17] and sensing modality changes [18], [19]. They can be further divided into two groups, including graph-based methods based on the topology of semantic objects [16], [18], [17] and geometric-based methods based on fine geometry of semantics (e.g., shape, contour, density) [19], [20]. However, these methods still face several shortcomings. First, graphbased approaches simply abstract semantic objects as graph nodes, which ignore the important geometric cues, such as shape. Second, geometric-based approaches can not well address large non-overlapped regions due to perspective and scale changes.\nIn this work, we represent an observation as a semantic graph and a set of class-wise distance fields (DFs), thus encoding visual, spatial, and geometric cues of the observation. In the semantic graph, nodes denote objects with semantic attributes and edges denote the spatial distance between a pair of objects. In a DF of a specific class, each element is labeled with the distance to the closest pixel/point of that class. Given the graph representations, we perform a novel deep semantic graph matching approach based on the geometric transformer to identify the correspondences of objects. The identified correspondences are further used to estimate the overlapped regions between a pair of DFs. Given the estimated overlapped region, we align two DFs by estimating their relative rotation (yaw angle) and compute the geometric similarity based on the aligned DFs. The final place recognition is performed by integrating graph-based similarity and geometric-based DF similarity.\nThe key contribution of this work is the introduction of visual, spatial, and geometric preserved place recognition for both ground-ground and aerial-ground multi-robot systems with different sensing capabilities. Specifically,\n\u2022 We propose a novel representation that consistently represents an observation as a semantic graph and a set of class-wise DFs, which encodes visual, spatial, and geometric cues to improve expressiveness for place recognition. Our representations can be used in a multi-robot team with the same sensing modality or with different modalities.\n\u2022 We propose an effective place recognition approach that integrates semantic graph matching and DF matching in a unified way. Our approach is able to not only perform ground-ground place recognition but also aerial-ground place recognition with large perspective changes. The remainder of the paper is organized as follows. In Section II, we review existing methods for place recognition. In Section III, we introduce the proposed visual, spatial and geometric preserved place recognition approach. In Section IV, we present and discuss our experimental results in the scenarios of ground-ground and aerial-ground cases in KITTI and AirSim. Finally, we conclude the paper in Section V.", "publication_ref": ["b0", "b1", "b2", "b3", "b4", "b5", "b6", "b7", "b8", "b9", "b10", "b11", "b12", "b13", "b14", "b15", "b16", "b17", "b18", "b15", "b17", "b16", "b18", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "II. RELATED WORK", "text": "Traditional methods for place recognition can be divided into two groups, including keypoint-based and region-based methods. The first group of methods focuses on using local features of key points in observations to perform place recognition, such as SIFT features [21], visual-spatial features [13], and super-point features of point clouds [22], [23]. The second group of methods focuses on using region-based holistic features to represent a scene, such as landmark-based graph [24], VLAD descriptor [25], HOG [26], GIST [27] and multi-modal VLAD features [28]. However, these methods can not work well in scenarios with large perspective changes, in which visual appearances of the same scene look quite different [29]. In addition, these methods can not work when a pair of observations are acquired by different sensors mounted on two robots.\nTo deal with the large-perspective challenge, some methods are proposed to learn view-invariant features based on Siamese network architectures [30], [31], [32], which are able to perform aerial-ground place recognition in geo-localization between ground-view observations and satellite maps. However, these methods can only work for specific scenarios with aerial-ground views, and they are not suitable to be deployed directly for ground-ground views [33].\nRecently, several methods aim to study semantic representations and matching for cross-view and cross-modal place recognition. We further divide these methods into two categories, including graph-based and geometric-based methods. First, graph-based methods perform place recognition based on semantic graph representations, such as using semantic graph matching [16], [34], semantic histogram [17], bag of words [35], maximum clique [33], and semantic random walk [18]. Second, geometric-based methods focus on using fine geometric information of semantic objects for place recognition, such as shape, density, and contour of semantics. The existing methods include using truncated distance field (TDF) matching with manually defined scale factors to perform cross-view localization between RGB and LiDAR observations [19], learning view-invariant semantic scan representations [20], [36] and registering road shapes [37].\nEven though semantic-based methods achieve promising performance, there are several shortcomings that have not been well addressed yet. First, graph-based representations are constructed by abstracting an object as a graph node, which ignores important geometric cues of semantics. Second, even though geometric-based approaches can well preserve geometric cues, they can not deal with large nonoverlapped regions caused by large perspective and scale changes, which leads to strict limitations, such as requiring close viewpoints [20], manually selecting scale factors [19], or traveling a long distance to generate a unique road pattern [37] or collect enough number of static vehicles [33]. Our approach that integrates graph-based and geometric-based matching in a unified way can address these shortcomings for place recognition in multi-robot collaborative perception.", "publication_ref": ["b20", "b12", "b21", "b22", "b23", "b24", "b25", "b26", "b27", "b28", "b29", "b30", "b31", "b32", "b15", "b33", "b16", "b34", "b32", "b17", "b18", "b19", "b35", "b36", "b19", "b18", "b36", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "III. APPROACH", "text": "Notation. Matrices are denoted as boldface capital letters, e.g., M = {M i,j } \u2208 R n\u00d7m . M i,j denotes the element in the i-th row and the j-th column of M. M i:j denotes all the elements from the i-th column to the j-th column of M. Vectors are denoted as boldface lowercase letters v \u2208 R n , and scalars are denoted as lowercase letters.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Problem Formulation", "text": "We consider three kinds of observations in this paper, including RGB images acquired by UAVs, RGB-D images, or LiDAR points acquired by UGVs, which are common sensor configurations in multi-robot teams. Our approach represents each observation consistently with a semantic graph representation and a set of class-wise distance field (DF) representations, as shown in Figure 2.\nSpecifically, given an observation with semantic labels obtained via semantic segmentation algorithms [38], [39], we first represent a place with a semantic graph G = (P, E, S) to encode the visual and spatial cues of the place. The node set P = {p i , i = 1, . . . , n} represents the centroid locations of all semantic objects, with p i encoding the centroid location of the i-th semantic object. We also define a semantic set S = {s i , i = 1, . . . , n} where s i \u2208 R m is the one-hot feature vector to encode the visual semantics of objects in P and m denotes the number of semantic classes. The edge set E = {e i,j , i, j = 1, 2, . . . , n, i \u0338 = j} represents the connection between a pair of nodes, where e i,j = 1 represents the connection between the i-th node p i \u2208 P and the j-th node p j \u2208 P.\nTo encode geometric cues of observations, we further represent a place with a class-wise DF set F = {F i } m , i = 1, 2, . . . , m}, where F i = {f j,k } l\u00d7w denotes a DF belonging to the i-th class. Specifically, we first project all the observations to a top-down view and convert them from the Cartesian coordinate to the polar coordinate, which is computed as follows:\nr = x 2 + y 2 (1) \u03c1 = arctan( y x )(2)\nwhere [x, y] denotes the Cartesian coordinates and [r, \u03c1] denotes the polar coordinates. Given the polarized observations, we compute DF as follows:\nf j,k = arg min j \u2032 ,k \u2032 ||j \u2212 j \u2032 , k \u2212 k \u2032 || 2 if \u03d5(I j \u2032 ,k \u2032 ) = i (3)\nwhere I j \u2032 ,k \u2032 denotes the pixel/point at coordinate j, k in the the top-down projected views. \u03d5(I j \u2032 ,k \u2032 ) denotes the semantic label of the pixel/point and f j,k denotes the distance from the coordinate j, k to the closest point of the i-th class.\nIn place recognition, observations observed by a pair of robots can be represented as M = {G, F} and M \u2032 = {G \u2032 , F \u2032 } respectively. We formulate place recognition as a two-step matching problem, including the semantic graph matching with graphs G and G \u2032 , as well as the DF matching with F and F \u2032 . The objective is to compute a similarity score to determine whether these observations are recorded at the same place.", "publication_ref": ["b37", "b38"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "B. Deep Semantic Graph Matching", "text": "We first formulate place recognition as a graph matching problem with graphs G and G \u2032 . Given a graph representation G, we encode visual and spatial cues of each object as H = {h i } = \u03c8(G), where h i is the embedding vector of the i-th object and \u03c8 is the geometric transformer network [40]. h i explicitly encodes not only the i-th object's visual semantic cue but also the spatial cues, including distance and angle information. Formally, \u03c8 is defined as :\nq l i = W l q h l i , k l i = W l k h l i , v l i = W l v h l i (4)\nwhere q l i , k l i and v l i denote query, key and value at the l-th layer, W l q , W l k , W l v denote their associating trainable weights. h l i denotes the visual semantic embedding vector of the i-th object, where h 0 i = s 0 i . The spatial information of objects is encoded as\nr l i,j = d l i,j W l d + max k {a l i,j,k W l a } (5\n)\nwhere r l i,j denotes the spatial embedding of the i-th object with respect to its j-th neighbor object, d l i,j denotes the distance between them, a l i,j,k denotes the angle of vertex i in the triangle constructed by the i-th, j-th and k-th objects. Given different observations with semantic labels (first row), including an RGB image, an RGBD image pair, and a LiDAR point cloud, our approach represents them consistently with a semantic graph (second row) and a set of class-wise DFs (fourth row). DFs are generated by projecting observations to a top-down view and transforming them to polar coordinates (third row). As ground RGBD observations' field of view (FOV) is 60 \u2022 , which is smaller than aerial RGB observations 360 \u2022 and ground Lidar observations 360 \u2022 , thus its polarized regions and DFs are much smaller than the other two's.\nIn the self-attention mechanism, we encode visual semantic and spatial features of objects given the self attention, which is computed as follows:\n\u03b1 l i,j = SoftMax (q l i ) \u22a4 (k l j + W l r r i,j ) \u221a c l (6)\nwhere \u03b1 l i,j is the self attention from object j to object i at layer l. To encode spatial relationships of objects, we add the spatial embedding r i,j into the learning process, where W l r denotes its learnable parameter matrix. c l is the dimensions of r i,j . This attention weight is obtained by comparing the query with its neighborhood keys and spatial attributes. The final attention is normalized by the SoftMax function. The object embedding vector weighted by self-attention is computed as h l+1 i = ei,j =1 \u03b1 l i,j (v l j ). In the cross-attention mechanism, we further encode visualspatial features of potentially matched objects in the other observation. The cross attention is computed as follows:\n\u03b2 l i,j = SoftMax (q l i ) \u22a4 (k \u2032 l j ) \u221a c l (7)\nwhere \u03b2 l i,j is the cross attention from the i-th object in graph G to the j-th object in graph G \u2032 . The object embedding vector weighted by cross attentions is computed as\nh l+1 i = ei,j =1 \u03b2 l i,j (v \u2032l j )\n. The final object embedding vector is obtained via alternating self-attention and cross-attention multiple times on the visual-spatial attributes of objects.\nGiven the object embedding vectors, we compute the similarity between pairs of nodes in the graph G and the graph G \u2032 as follows: where exp() denotes the exponential operator and S \u2208 R n\u00d7n \u2032 represents the similarity between the two graphs with n and n \u2032 objects respectively. As there are large perceptual noises and outliers existed in the observations, it is extremely hard to find one-to-one correspondences between G and G \u2032 . Thus, we relax the one-to-one constraint to one-to-many constraint.\nS i,j = exp(\u2212||h i \u2212 h \u2032 j || 2 )(8)\nFormally, we identify correspondences via selecting top M similarities in {Y i,j } n\u00d7n \u2032 = topM(S), where Y denotes the correspondence matrix with Y i,j = 1 denoting the correspondence between the i-th object in G and the j-th object in G \u2032 , otherwise Y i,j = 0. We use the circle loss to train our network [41], which is defined as:\nL G \u2032 \u2192G = p \u2032 i \u2208P \u2032 log[1 + pj \u2208P p exp(\u03b3(D i,j \u2212 \u03b4 p ) 2 ) (9) p k \u2208P n exp(\u03b3(\u03b4 n \u2212 D i,k ) 2 )](10)\nwhere P = {P p , P n }G denotes the node set in graph G. P p denotes the positive nodes that have corresponding nodes in graph G \u2032 . Similarly, P n denotes the negative nodes that have no corresponding nodes in graph G \u2032 . D = {D i,j } n\u00d7n \u2032 denotes the distance matrix with D i,j = ||h i \u2212 h j || 2 denoting the distance between a pair of feature vectors. \u03b4 p = 0.2 and \u03b4 n = 1.4 are two hyperparameters, which denote the positive and negative margins separately. \u03b3 = 40 denotes the scale factor. L G \u2032 \u2192G describes the loss given node set P \u2032 and P = {P p , P n }. Similarly, we can compute the loss L G\u2192G \u2032 given node set P and P \u2032 = {P \u2032p , P \u2032n }. The overall loss is defined as L = (L\nG\u2192G \u2032 + L G \u2032 \u2192G )/2.", "publication_ref": ["b39", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "C. DF Matching", "text": "We further perform DF matching to encode geometric cues (e.g., shape, density) for place recognition. One traditional way is to directly calculate the distance between a pair of DF features [19]. Due to the large perspective changes in observations acquired by different robots, especially aerialground scenarios, the existence of non-overlapped regions will heavily affect the performance of place recognition.", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "1) Addressing Non-Overlapped Regions:", "text": "To address the problem of non-overlapped regions, we estimate the overlapped regions by calculating the convex hull of all the nodes that have correspondences identified by semantic graph matching. Formally, the overlapped regions in a pair of observations are denoted as A and A \u2032 separately, which are computed as A = covexhull({p i } M ) and A \u2032 = covexhull({p \u2032 j } M ), where covexhull denotes the function to compute convex hull given a list of points. The nodes have correspondences are denoted as {p i } M \u2208 P and {p \u2032 j } M \u2208 P \u2032 where M is the number of correspondences encoded in the constraint Y i,j = 1. We simplify the convex hull as a rectangle in this paper.\n2) Addressing Rotation Changes: Given the estimated overlapped regions, we estimate the yaw angle (the motion direction of robots) between two robots' observations to align their DFs. Specifically, given a pair of polarized DFs F and F \u2032 , the estimation of the yaw angle between them is defined as follows:\nf \u03b8 = vec([F \u03b8: , F 0:\u03b8 ])(11)\nwhere [F \u03b8: , F 0:\u03b8 ] denotes the shift operation on the polarized DFs. As F and F \u2032 are all in polar coordinates, rotating their observations in the yaw direction is equivalent to shifting their polarized DFs with the yaw angle \u03b8. vec denotes the vectorization operation that converts a matrix to a vector by concatenating its rows. Finally, given a pair of DF features f \u03b8 and f \u2032 \u03b8 , we estimate the optimal shift as follows:\n\u03b8 * = arg max \u03b8 ( m i f \u03b8 f \u2032 \u03b8 |f \u03b8 ||f \u2032 \u03b8 | )(12)\nwhere \u03b8 * denotes the optimal shift between a pair of DFs, which is computed by maximizing the overall DF similarity, where m denotes the number of semantic classes. The DF matching process is illustrated in Figure 3. Not only can our proposed DF matching deal with the inputs consisting of an RGB image and a point cloud, but it is also applicable to pairs of RGB images or pairs of point clouds. ", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "D. Place Recognition", "text": "Given the semantic graph matching and DF matching, the place recognition score is computed as follows:\nscore = \u03bb K topK(S i,j ) + (1 \u2212 \u03bb) m m i f \u03b8 * f \u2032\u03b8 * |f \u03b8 * ||f \u2032\u03b8 * | (13\n)\nwhere \u03bb is a hyperparameter that controls the weights of the graph-level similarity and the geometric-based DF similarity. The graph-level similarity is computed as the average of top K graph node similarities. The geometric DF similarity is computed as the average of m class-wise DF similarities. If one of f \u03b8 * and f \u2032\u03b8 * is not existed, then f \u03b8 * f \u2032\u03b8 * |f \u03b8 * ||f \u2032\u03b8 * | = 0. If both of them are not existed, then f \u03b8 * f \u2032\u03b8 * |f \u03b8 * ||f \u2032\u03b8 * | = 1. Given the final similarity score that considers visual (e.g., semantics), spatial (e.g., object topology) and geometric cues (e.g., shape, density) of semantics, we perform robust place recognition by thresholding the similarity.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "IV. EXPERIMENTS", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Experimental Setup", "text": "We employ two place recognition datasets, including a large-scale real-world dataset (KITTI) [42] and a simulated dataset (AirSim) [18] to benchmark our approach. Our experiments cover scenarios including ground-ground robots with LiDAR sensors and aerial-ground robots with RGB and RGBD sensors. Information on the benchmark datasets are presented in Table I.\nIn the KITTI dataset, we generate over 200, 000 data instances. Each data instance contains a pair of point clouds. Following the recent method [16], we use RangeNet++ [39] to perform semantic segmentation on raw point clouds to detect these semantic objects. A total of 12 classes of objects is used to construct semantic graphs, including cars, trucks, other vehicles, sidewalks, other ground, buildings, fences, vegetation, truck, terrain, pole, and traffic signs. We use 3D positions of objects to generate the node set and the nearest neighbor search to generate the edge set. We use fences and vegetation to construct DFs. The ground-truth loop closure is obtained based on the ground-truth poses provided by the KITTI odometry dataset. We decide if two point clouds are positive or negative based on the Euclidean distance between them. If the distance is less than 10 m, then they are positive. If the distance is over 20 m, then they are negative. The ground-truth correspondences of objects are identified based on the unique ID of vehicles provided by the semantic KITTI dataset [43] and the ground-truth poses provided by the KITTI odometry dataset [42].\nIn the AirSim dataset, we generate over 10, 000 data instances. Each data instance contains one RGBD image pair acquired from a UGV and one RGB image acquired from a UAV. Following the recent method [18], we construct semantic graphs with 5 semantic objects, including buildings, fences, hedges, vegetation, and vehicles. In addition, we select fences, hedges, vegetation, and vehicles to construct DFs. For graph representations, we use 3D positions of objects and the nearest neighbor search to construct semantic graphs. In particular, 3D positions of objects observed by ground robots can be obtained directly from the depth images. For the aerial observations, we assume that the depth values of objects are the same (ignore the height of objects), which is the flight height of the UAV. For DF representations, we construct them using top-down projection of RGBD observations acquired from the ground view, and the RGB observations acquired from the aerial view directly. A pair of observations are decided to be positive when there are at least 10 correspondences between them. If the number of correspondences is 0, then the pair of observations is decided to be negative. The groundtruth correspondences are identified based on their groundtruth poses.\nIn the implementation of our network \u03c8, we set the number of network layers to be L = 6 with 3 self-attention layers and 3 cross-attention layers alternatively. Each attention layer has m, 64, and 32 as their input, hidden, and output channels separately, where m is the number of semantic classes. We set M = 10 for topM as defined in Eq. (8) and set K = 5 for topK as defined in Eq. (13). In addition, we set \u03bb = 0.8 as defined in Eq. (8). In all the experiments, we use ADMM as the optimization method with the learning rate setting to 0.0001 and weight decay setting to 0.00005.\nFor comparison, we first implement two baseline methods, including (Ours-gm) that only uses semantic graph matching and (Ours-df) that only uses DF matching. We also evaluate our full approach (Ours). In addition, we compare our methods with three previous methods, including one traditional approach, one graph-based approach and one geometric-based approach for place recognition.\n\u2022 Point cloud vector of locally aggregated descriptors (PointVlad) [44] that is the traditional LiDAR point-CONFIDENTIAL. Limited circulation. For review only. based place recognition approach. As this method can not deal with RGB images, we just evaluate it in the KITTI dataset. \u2022 Semantic graph matching (SG) [16] that recognizes places based on the similarity between a pair of semantic graphs.\n\u2022 Cross-view geometric-based matching (CGM) [19] that uses top-down projected LiDAR points acquired by ground robots and aerial-view RGB image patches cropped from reference maps to perform learning-free TDF-based matching. As we treat place recognition as a data retrieval process, we use the following metrics to evaluate place recognition performance.\n\u2022 Precision-recall curve is used as the evaluation metric, which is a standard metric used in the place recognition literature [16]. Precision is defined as the ratio of the retrieved correct places over all the retrieved places.\nRecall is defined as the ratio of the retrieved correct places over the ground-truth correct places. \u2022 Area under the curve (AUC) is a single-value evaluation metric to evaluate the overall performance of place recognition methods, which takes values in [0, 1] with a greater value indicating a better performance, and a value 1 indicating the perfect performance.", "publication_ref": ["b41", "b17", "b15", "b38", "b42", "b41", "b17", "b7", "b43", "b15", "b18", "b15"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "B. Results on the KITTI Dataset", "text": "The KITTI dataset totally contains 11 sequences obtained by a 64-ring LiDAR, as shown in Figure 4(a). We use sequences 00, 01, 03, and 05 for training, sequences 04, 06, and 07 for validation, and sequences 02 and 08 with loop closures for testing. As sequence 02 has the largest number of instances among all the sequences with loop closures, and sequence 08 has reverse loops, they are the most challenging sequences in the evaluation of place recognition.\nQuantitative results are presented in Figure 5(a) and Figure 5(b) based on the precision-recall curve. We can see that Ours-gm outperforms graph-based method SG by explicitly considering the spatial relationships of objects in the learning process. Ours-df outperforms geometric-based method CGM, which indicates the importance of addressing nonoverlapping regions between pairs of observations and estimating the rotation between them. Finally, our full approach outperforms the baseline method due to its capability of integrating visual, spatial, and geometric cues, as well as addressing non-overlapped regions for place recognition. In addition, we observe that the performance of PointVlad and CGM drops quickly in sequence 08 compared with it in sequence 02. It is because they can not deal with totally opposite-direction cases in sequence 08. SG performs much better as semantic graph matching is invariant to perspective changes. However, it still can not address non-overlapped regions when two observations are recorded far from each other. By addressing non-overlapped regions, our approach performs the best.\nWe also use a single-value evaluation metric AUC to quantitatively evaluate our approach and comparisons, as shown in Table II. It is observed that our approach obtains the score of 0.9357 and 0.8767 in sequences 02 and 08 separately, which significantly outperforms the second-best method [16]. The improvements are around 15% and 8% separately.", "publication_ref": ["b15"], "figure_ref": ["fig_2"], "table_ref": ["tab_0"]}, {"heading": "C. Results on the Aerial-Ground AirSim Dataset", "text": "The AirSim dataset contains observation pairs acquired from a UAV and a UGV. The whole trajectory is 1km. In this dataset, we use RGBD images acquired by a UGV as the ground-view observations. We also use RGB images acquired by a UAV and the UAV flight height as the aerial-view observations, as shown in Figure 4(b). This dataset is very challenging due to the large perspective changes and sensing modality changes in aerial-ground observations.\nThe quantitative results obtained by our method and comparisons are demonstrated in Figure 5(c). As the sensing modalities of observations are different, the traditional point feature-based approach PointVlad can not be used in this case. In addition, we can see that our baseline method Oursgm significantly outperforms SG and Ours-df significantly outperforms CGM, which indicates the importance of explicitly learning visual-spatial cues in semantic graph matching and estimating overlapping regions to integrate geometric cues for place recognition. As shown in Table II. Our full approach achieves 35% improvements compared with the previous methods [16], [19] on AUC in the aerial-ground scenarios, which indicates the importance of integrating graph matching and geometric-based DF matching for place recognition, especially in the aerial-ground scenarios.\nThe qualitative results obtained by our full approach on the AirSim dataset are illustrated in Figure 6. The results show identified correspondences of objects, estimated overlapped regions, and matched places between aerial-ground observations. We observe that our approach can well identify the correspondences of objects in positive cases. Based on the correctly identified correspondences, our approach can significantly reduce the non-overlapped regions, as shown in Figure 6(a). For negative cases, the mismatched correspondences will generate two regions with large visual differences, thus significantly decreasing the matching score of negative cases, as shown in Figure 6(b). By correctly identifying correspondences, estimating overlapped regions, and integrating geometric cues into place recognition, our approach can perform place recognition well in both cross-view (aerial-ground) and cross-modality (RGBD-RGB) scenarios.\nCONFIDENTIAL. Limited circulation. For review only.  We run our approach on a Linux machine with an i7 16core CPU, 16G memory, and an RTX 2080 GPU. The average execution speed of our graph matching approach is 75Hz. Our full approach achieves 15Hz execution speed on KITTI and 6Hz on AirSim datasets. 1) Influence of Semantic Segmentation: The influence of semantic segmentation on our approach is shown in Figure 7(a). We compare the performance of our approach based on the semantic labels provided by RangeNet++ [39] (denoted as rn) and the ground-truth labels provided by Semantic KITTI (denoted as sk). We use sequences 02 and 08 to evaluate the influence. In sequence 02, it is observed that our approach achieves similar AUC scores based on RangeNet-provided labels or ground-truth labels, which are 0.9356 and 0.9420 separately. In sequence 08, the performance of our approach decreases from 0.9613 to 0.8767 when we change groundtruth labels with RangeNet-provided labels.", "publication_ref": ["b15", "b18", "b38"], "figure_ref": ["fig_2", "fig_4"], "table_ref": ["tab_0"]}, {"heading": "D. Discussion", "text": "2) Hyperparameter: The analysis of hyperparameter \u03bb as defined in Eq. ( 13) is shown in Figure 7(b). The hyperparameter \u03bb is used to control the trade-off between graph matching similarity and DF similarity. We observe that our approach achieves the best performance when \u03bb \u2208 [0.5, 0.8].", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "V. CONCLUSION", "text": "We propose a novel approach that integrates visual, spatial, and geometric cues to perform cross-view and cross-modal place recognition. Our approach consistently represents multimodal observations, including RGB image, RGBD image pair, and LiDAR point cloud, as a semantic graph and a set of class-wise DFs. Given the cross-modal representations, our approach integrates semantic graph matching and DF matching in a unified way to perform place recognition, which can explicitly address non-overlapped regions between observations. Experimental results on two public benchmark datasets have shown that our approach obtains promising place recognition performance in both ground-ground and aerial-ground multi-robot systems.\nOur approach has some limitations, offering possible future directions. First, the execution speed of our approach is affected by the size of the observations, especially in the generation of DFs. Downsampling techniques can be developed to reduce this size and further improve the runtime performance. Second, currently our approach assumes singlemodal observations as input and can be extended to take multimodal observations as inputs, such as UGVs with IMU and LiDAR, and UAVs with visual odometry and RGB camera, to improve robustness of place recognition.\nCONFIDENTIAL. Limited circulation. For review only. ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A survey on aerial swarm robotics", "journal": "IEEE Transactions on Robotics", "year": "2018", "authors": "S.-J Chung; A A Paranjape; P Dames; S Shen; V Kumar"}, {"ref_id": "b1", "title": "Task partitioning in swarms of robots: An adaptive method for strategy selection", "journal": "Swarm Intelligence", "year": "2011", "authors": "G Pini; A Brutschy; M Frison; A Roli; M Dorigo; M Birattari"}, {"ref_id": "b2", "title": "Regularized graph matching for correspondence identification under uncertainty in collaborative perception", "journal": "", "year": "2021", "authors": "P Gao; R Guo; H Lu; H Z Zhang"}, {"ref_id": "b3", "title": "On fault tolerance and scalability of swarm robotic systems", "journal": "", "year": "2013", "authors": "J D Bjerknes; A F Winfield"}, {"ref_id": "b4", "title": "CCM-SLAM: Robust and efficient centralized collaborative monocular simultaneous localization and mapping for robotic teams", "journal": "Journal of Field Robotics", "year": "2019", "authors": "P Schmuck; M Chli"}, {"ref_id": "b5", "title": "Multi-UAV collaborative monocular SLAM", "journal": "", "year": "2017", "authors": ""}, {"ref_id": "b6", "title": "Bayesian deep graph matching for correspondence identification in collaborative perception", "journal": "", "year": "2021", "authors": "P Gao; H Zhang"}, {"ref_id": "b7", "title": "Survey of connected automated vehicle perception mode: from autonomy to interaction", "journal": "Intelligent Transportation Systems", "year": "2018", "authors": "S Wei; D Yu; C L Guo; L Dan; W W Shu"}, {"ref_id": "b8", "title": "General Place Recognition Survey: Towards the real-world autonomy age", "journal": "ArXiv", "year": "2022", "authors": "P Yin; S Zhao; I Cisneros; A Abuduweili; G Huang; M Milford; C Liu; H Choset; S Scherer"}, {"ref_id": "b9", "title": "Collaborative multi-robot search and rescue: Planning, coordination, perception, and active vision", "journal": "IEEE Access", "year": "2020", "authors": "J P Queralta; J Taipalmaa; B C Pullinen; V K Sarker; T N Gia; H Tenhunen; M Gabbouj; J Raitoharju; T Westerlund"}, {"ref_id": "b10", "title": "Balancing mission and comprehensibility in multi-robot systems for disaster response", "journal": "", "year": "2021", "authors": "B Reily; J G Rogers; C Reardon"}, {"ref_id": "b11", "title": "Orb-slam: a versatile and accurate monocular slam system", "journal": "IEEE transactions on robotics", "year": "2015", "authors": "R Mur-Artal; J M M Montiel; J D Tardos"}, {"ref_id": "b12", "title": "Long-term loop closure detection through visualspatial information preserving multi-order graph matching", "journal": "", "year": "2020", "authors": "P Gao; H Zhang"}, {"ref_id": "b13", "title": "Voxel-based representation learning for place recognition based on 3d point clouds", "journal": "", "year": "2020", "authors": "S Siva; Z Nahman; H Zhang"}, {"ref_id": "b14", "title": "LOAM: Lidar odometry and mapping in realtime", "journal": "", "year": "2014", "authors": "J Zhang; S Singh"}, {"ref_id": "b15", "title": "Semantic graph based place recognition for 3D point clouds", "journal": "", "year": "2020", "authors": "X Kong; X Yang; G Zhai; X Zhao; X Zeng; M Wang; Y Liu; W Li; F Wen"}, {"ref_id": "b16", "title": "Semantic histogrambased graph matching for real-time multi-robot global localization in large scale environment", "journal": "IEEE Robotics and Automation Letters", "year": "2021", "authors": "X Guo; J Hu; J Chen; F Deng; T L Lam"}, {"ref_id": "b17", "title": "X-View: Graph-based semantic multi-view localization", "journal": "IEEE Robotics and Automation Letters", "year": "2018", "authors": "A Gawel; C Don; R Siegwart; J Nieto; C Cadena"}, {"ref_id": "b18", "title": "Any way you look at it: Semantic crossview localization and mapping with lidar", "journal": "IEEE Robotics and Automation Letters", "year": "2021", "authors": "I D Miller; A Cowley; R Konkimalla; S S Shivakumar; T Nguyen; T Smith; C J Taylor; V Kumar"}, {"ref_id": "b19", "title": "SSC: Semantic scan context for large-scale place recognition", "journal": "", "year": "2021", "authors": "L Li; X Kong; X Zhao; T Huang; W Li; F Wen; H Zhang; Y Liu"}, {"ref_id": "b20", "title": "Superglue: Learning feature matching with graph neural networks", "journal": "", "year": "2020", "authors": "P.-E Sarlin; D Detone; T Malisiewicz; A Rabinovich"}, {"ref_id": "b21", "title": "Kpconv: Flexible and deformable convolution for point clouds", "journal": "", "year": "2019", "authors": "H Thomas; C R Qi; J.-E Deschaud; B Marcotegui; F Goulette; L J Guibas"}, {"ref_id": "b22", "title": "Feature pyramid networks for object detection", "journal": "", "year": "2017", "authors": "T.-Y Lin; P Doll\u00e1r; R Girshick; K He; B Hariharan; S Belongie"}, {"ref_id": "b23", "title": "Long-term place recognition through worstcase graph matching to integrate landmark appearances and spatial relationships", "journal": "", "year": "2020", "authors": "P Gao; H Zhang"}, {"ref_id": "b24", "title": "NetVLAD: CNN architecture for weakly supervised place recognition", "journal": "", "year": "2016", "authors": "R Arandjelovic; P Gronat; A Torii; T Pajdla; J Sivic"}, {"ref_id": "b25", "title": "Robust visual robot localization across seasons using network flows", "journal": "", "year": "2014", "authors": "T Naseer; L Spinello; W Burgard; C Stachniss"}, {"ref_id": "b26", "title": "An online sparsitycognizant loop-closure algorithm for visual navigation", "journal": "", "year": "2014", "authors": "Y Latif; G Huang; J J Leonard; J Neira"}, {"ref_id": "b27", "title": "Fusionvlad: A multi-view deep fusion networks for viewpoint-free 3d place recognition", "journal": "IEEE Robotics and Automation Letters", "year": "2021", "authors": "P Yin; L Xu; J Zhang; H Choset"}, {"ref_id": "b28", "title": "Are state-of-the-art visual place recognition techniques any good for aerial robotics", "journal": "", "year": "2019", "authors": "M Zaffar; A Khaliq; S Ehsan; M Milford; K Alexis; K Mcdonald-Maier"}, {"ref_id": "b29", "title": "Cross-view image sequence geo-localization", "journal": "", "year": "2023", "authors": "X Zhang; W Sultani; S Wshah"}, {"ref_id": "b30", "title": "Are these from the same place? seeing the unseen in cross-view image geo-localization", "journal": "", "year": "2021", "authors": "R Rodrigues; M Tani"}, {"ref_id": "b31", "title": "Spatial-aware feature aggregation for image based cross-view geo-localization", "journal": "", "year": "2019", "authors": "Y Shi; L Liu; X Yu; H Li"}, {"ref_id": "b32", "title": "View-invariant localization using semantic objects in changing environments", "journal": "", "year": "2022", "authors": "J Ankenbauer; K Fathian; J P How"}, {"ref_id": "b33", "title": "SemanticLoop: Loop closure with 3d semantic graph matching", "journal": "IEEE Robotics and Automation Letters", "year": "2022", "authors": "J Yu; S Shen"}, {"ref_id": "b34", "title": "SVG-Loop: Semanticvisual-geometric information-based loop closure detection", "journal": "Remote Sensing", "year": "2021", "authors": "Z Yuan; K Xu; X Zhou; B Deng; Y Ma"}, {"ref_id": "b35", "title": "RINet: Efficient 3d lidar-based place recognition using rotation invariant neural network", "journal": "IEEE Robotics and Automation Letters", "year": "2022", "authors": "L Li; X Kong; X Zhao; T Huang; W Li; F Wen; H Zhang; Y Liu"}, {"ref_id": "b36", "title": "Crossview mapping with graph-based geolocalization on city-scale street maps", "journal": "", "year": "2022", "authors": "Z Ye; C Bao; X Liu; H Bao; Z Cui; G Zhang"}, {"ref_id": "b37", "title": "Understanding convolution for semantic segmentation", "journal": "", "year": "2018", "authors": "P Wang; P Chen; Y Yuan; D Liu; Z Huang; X Hou; G Cottrell"}, {"ref_id": "b38", "title": "Rangenet++: Fast and accurate lidar semantic segmentation", "journal": "", "year": "2019", "authors": "A Milioto; I Vizzo; J Behley; C Stachniss"}, {"ref_id": "b39", "title": "Geometric transformer for fast and robust point cloud registration", "journal": "", "year": "2022", "authors": "Z Qin; H Yu; C Wang; Y Guo; Y Peng; K Xu"}, {"ref_id": "b40", "title": "Circle loss: A unified perspective of pair similarity optimization", "journal": "", "year": "2020", "authors": "Y Sun; C Cheng; Y Zhang; C Zhang; L Zheng; Z Wang; Y Wei"}, {"ref_id": "b41", "title": "Vision meets robotics: The KITTI dataset", "journal": "The International Journal of Robotics Research", "year": "2013", "authors": "A Geiger; P Lenz; C Stiller; R Urtasun"}, {"ref_id": "b42", "title": "SemanticKITTI: A dataset for semantic scene understanding of lidar sequences", "journal": "", "year": "2019", "authors": "J Behley; M Garbade; A Milioto; J Quenzel; S Behnke; C Stachniss; J Gall"}, {"ref_id": "b43", "title": "Pointnetvlad: Deep point cloud based retrieval for large-scale place recognition", "journal": "", "year": "2018", "authors": "M A Uy; G H Lee"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig. 2 .2Fig. 2.Given different observations with semantic labels (first row), including an RGB image, an RGBD image pair, and a LiDAR point cloud, our approach represents them consistently with a semantic graph (second row) and a set of class-wise DFs (fourth row). DFs are generated by projecting observations to a top-down view and transforming them to polar coordinates (third row). As ground RGBD observations' field of view (FOV) is 60 \u2022 , which is smaller than aerial RGB observations 360 \u2022 and ground Lidar observations 360 \u2022 , thus its polarized regions and DFs are much smaller than the other two's.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Fig. 3 .3Fig. 3. Overview of our DF matching approach that consists of estimating overlapped regions based on identified correspondences, transforming RGB images or point clouds to polar coordinates, estimating rotation (yaw angle) between a pair of polarized observations, and computing the DF similarity based on the aligned observations. [The figures are best viewed in color].", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Fig. 4 .4Fig. 4. Illustrations of the visual observations obtained by a pair of robots in KITTI and AirSim. KITTI covers the scenario of ground-ground robots with LiDAR sensors. AirSim covers the scenario of aerial-ground robots with RGB and RGBD cameras.", "figure_data": ""}, {"figure_label": "56", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Fig. 5 .Fig. 6 .56Fig. 5. Quantitative results on the KITTI dataset and AirSim dataset based on the precision-recall curve. Our methods are illustrated with solid curves and the others are shown in dash curves. Our method consistently achieves higher precision over the state-of-the-art methods on 3 datasets. [Best viewed in color].", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Fig. 7 .7Fig. 7. Characteristics of our approach: (a) the influence caused by semantic segmentation (SS) and (b) analysis of hyperparameter \u03bb.", "figure_data": ""}, {"figure_label": "I", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "OF THE REAL-WORLD KITTI AND SIMULATED AIRSIM DATASETS FOR PLACE RECOGNITION.", "figure_data": "DatasetKITTIAirSim# Training Cases91,82610,148# Validation Cases40,5311,000# Testing Cases91,6742,000Robot TypeGround vs GroundAerial vs GroundSensorsLiDAR vs LiDARRGB vs RGBD# Semantics125"}, {"figure_label": "II", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "RESULTS OF OUR APPROACH AND COMPARISONS WITH THREE PREVIOUS METHODS BASED ON AUC SCORE. OURS ACHIEVES THE HIGHEST SCORES ON ALL 3 DATASETS.", "figure_data": "MethodKITTI-02KITTI-08AirSimPointVlad [44]0.75860.076-CGM [19]0.50510.10140.2216SG [16]0.78070.79750.5054Ours-df0.56070.32210.2759Ours-gm0.91590.84640.6618Ours-full0.93570.87670.8598"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Manuscript 789 submitted to 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Received March 2, 2023.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "r = x 2 + y 2 (1) \u03c1 = arctan( y x )(2)", "formula_coordinates": [3.0, 144.67, 204.66, 154.13, 34.01]}, {"formula_id": "formula_1", "formula_text": "f j,k = arg min j \u2032 ,k \u2032 ||j \u2212 j \u2032 , k \u2212 k \u2032 || 2 if \u03d5(I j \u2032 ,k \u2032 ) = i (3)", "formula_coordinates": [3.0, 78.06, 284.78, 220.74, 18.67]}, {"formula_id": "formula_2", "formula_text": "q l i = W l q h l i , k l i = W l k h l i , v l i = W l v h l i (4)", "formula_coordinates": [3.0, 88.73, 577.18, 210.07, 12.69]}, {"formula_id": "formula_3", "formula_text": "r l i,j = d l i,j W l d + max k {a l i,j,k W l a } (5", "formula_coordinates": [3.0, 107.27, 663.13, 187.66, 16.73]}, {"formula_id": "formula_4", "formula_text": ")", "formula_coordinates": [3.0, 294.93, 665.52, 3.87, 8.64]}, {"formula_id": "formula_5", "formula_text": "\u03b1 l i,j = SoftMax (q l i ) \u22a4 (k l j + W l r r i,j ) \u221a c l (6)", "formula_coordinates": [3.0, 351.55, 384.63, 206.45, 26.74]}, {"formula_id": "formula_6", "formula_text": "\u03b2 l i,j = SoftMax (q l i ) \u22a4 (k \u2032 l j ) \u221a c l (7)", "formula_coordinates": [3.0, 372.41, 573.44, 185.59, 28.38]}, {"formula_id": "formula_7", "formula_text": "h l+1 i = ei,j =1 \u03b2 l i,j (v \u2032l j )", "formula_coordinates": [3.0, 313.2, 648.02, 102.04, 13.37]}, {"formula_id": "formula_8", "formula_text": "S i,j = exp(\u2212||h i \u2212 h \u2032 j || 2 )(8)", "formula_coordinates": [3.0, 381.75, 721.0, 176.26, 14.34]}, {"formula_id": "formula_9", "formula_text": "L G \u2032 \u2192G = p \u2032 i \u2208P \u2032 log[1 + pj \u2208P p exp(\u03b3(D i,j \u2212 \u03b4 p ) 2 ) (9) p k \u2208P n exp(\u03b3(\u03b4 n \u2212 D i,k ) 2 )](10)", "formula_coordinates": [4.0, 68.94, 415.97, 229.86, 51.83]}, {"formula_id": "formula_10", "formula_text": "G\u2192G \u2032 + L G \u2032 \u2192G )/2.", "formula_coordinates": [4.0, 127.33, 610.32, 78.34, 9.65]}, {"formula_id": "formula_11", "formula_text": "f \u03b8 = vec([F \u03b8: , F 0:\u03b8 ])(11)", "formula_coordinates": [4.0, 392.53, 500.15, 165.47, 11.72]}, {"formula_id": "formula_12", "formula_text": "\u03b8 * = arg max \u03b8 ( m i f \u03b8 f \u2032 \u03b8 |f \u03b8 ||f \u2032 \u03b8 | )(12)", "formula_coordinates": [4.0, 378.02, 613.31, 179.98, 30.32]}, {"formula_id": "formula_13", "formula_text": "score = \u03bb K topK(S i,j ) + (1 \u2212 \u03bb) m m i f \u03b8 * f \u2032\u03b8 * |f \u03b8 * ||f \u2032\u03b8 * | (13", "formula_coordinates": [5.0, 67.27, 261.36, 227.38, 30.32]}, {"formula_id": "formula_14", "formula_text": ")", "formula_coordinates": [5.0, 294.65, 272.09, 4.15, 8.64]}], "doi": ""}