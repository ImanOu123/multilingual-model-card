{"title": "Non-projective Dependency Parsing using Spanning Tree Algorithms", "authors": "Ryan Mcdonald; Fernando Pereira; Kiril Ribarov; Jan Haji\u010d", "pub_date": "", "abstract": "We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n 3 ) time. More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965;Edmonds, 1967) MST algorithm, yielding an O(n 2 ) parsing algorithm. We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques McDonald et al., 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies.", "sections": [{"heading": "Introduction", "text": "Dependency parsing has seen a surge of interest lately for applications such as relation extraction (Culotta and Sorensen, 2004), machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), and lexical resource augmentation (Snow et al., 2004). The primary reasons for using dependency structures instead of more informative lexicalized phrase structures is that they are more efficient to learn and parse while still encoding much of the predicate-argument information needed in applications. Dependency representations, which link words to their arguments, have a long history (Hudson, 1984). Figure 1 shows a dependency tree for the sentence John hit the ball with the bat. We restrict ourselves to dependency tree analyses, in which each word depends on exactly one parent, either another word or a dummy root symbol as shown in the figure. The tree in Figure 1 is projective, meaning that if we put the words in their linear order, preceded by the root, the edges can be drawn above the words without crossings, or, equivalently, a word and its descendants form a contiguous substring of the sentence.\nIn English, projective trees are sufficient to analyze most sentence types. In fact, the largest source of English dependency trees is automatically generated from the Penn Treebank (Marcus et al., 1993) and is by convention exclusively projective. However, there are certain examples in which a nonprojective tree is preferable. Consider the sentence John saw a dog yesterday which was a Yorkshire Terrier. Here the relative clause which was a Yorkshire Terrier and the object it modifies (the dog) are separated by an adverb. There is no way to draw the dependency tree for this sentence in the plane with no crossing edges, as illustrated in Figure 2. In languages with more flexible word order than English, such as German, Dutch and Czech, non-projective dependencies are more frequent. Rich inflection systems reduce reliance on word order to express  grammatical relations, allowing non-projective dependencies that we need to represent and parse efficiently. A non-projective example from the Czech Prague Dependency Treebank (Haji\u010d et al., 2001) is also shown in Figure 2.\nMost previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al. (1999), Yamada and Matsumoto (2003), Scholz (2004), andMcDonald et al. (2005). These systems have shown that accurate projective dependency parsers can be automatically learned from parsed data. However, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for English. In particular, Wang and Harper (2004) describe a broad coverage non-projective parser for English based on a hand-constructed constraint dependency grammar rich in lexical and syntactic information. Nivre and Nilsson (2005) presented a parsing model that allows for the introduction of non-projective edges into dependency trees through learned edge transformations within their memory-based parser. They test this system on Czech and show improved accuracy relative to a projective parser. Our approach differs from those earlier efforts in searching optimally and efficiently the full space of non-projective trees.\nThe main idea of our method is that dependency parsing can be formalized as the search for a maximum spanning tree in a directed graph. This formalization generalizes standard projective parsing models based on the Eisner algorithm (Eisner, 1996) to yield efficient O(n 2 ) exact parsing methods for nonprojective languages like Czech. Using this spanning tree representation, we extend the work of Mc-Donald et al. (2005) on online large-margin discrim-inative training methods to non-projective dependencies.\nThe present work is related to that of Hirakawa (2001) who, like us, reduces the problem of dependency parsing to spanning tree search. However, his parsing method uses a branch and bound algorithm that is exponential in the worst case, even though it appears to perform reasonably in limited experiments. Furthermore, his work does not adequately address learning or measure parsing accuracy on held-out data.\nSection 2 describes an edge-based factorization of dependency trees and uses it to equate dependency parsing to the problem of finding maximum spanning trees in directed graphs. Section 3 outlines the online large-margin learning framework used to train our dependency parsers. Finally, in Section 4 we present parsing results for Czech. The trees in Figure 1 and Figure 2 are untyped, that is, edges are not partitioned into types representing additional syntactic information such as grammatical function. We study untyped dependency trees mainly, but edge types can be added with simple extensions to the methods discussed here.", "publication_ref": ["b7", "b8", "b22", "b23", "b16", "b18", "b11", "b10", "b1", "b28", "b21", "b19", "b27", "b20", "b10", "b13"], "figure_ref": ["fig_0", "fig_0", "fig_1", "fig_1", "fig_0", "fig_1"], "table_ref": []}, {"heading": "Dependency Parsing and Spanning Trees", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Edge Based Factorization", "text": "In what follows, x = x 1 \u2022 \u2022 \u2022 x n represents a generic input sentence, and y represents a generic dependency tree for sentence x. Seeing y as the set of tree edges, we write (i, j) \u2208 y if there is a dependency in y from word x i to word x j .\nIn this paper we follow a common method of factoring the score of a dependency tree as the sum of the scores of all edges in the tree. In particular, we define the score of an edge to be the dot product be-tween a high dimensional feature representation of the edge and a weight vector, s(i, j) = w \u2022 f(i, j)\nThus the score of a dependency tree y for sentence x is,\ns(x, y) = (i,j)\u2208y s(i, j) = (i,j)\u2208y w \u2022 f(i, j)\nAssuming an appropriate feature representation as well as a weight vector w, dependency parsing is the task of finding the dependency tree y with highest score for a given sentence x.\nFor the rest of this section we assume that the weight vector w is known and thus we know the score s(i, j) of each possible edge. In Section 3 we present a method for learning the weight vector.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Maximum Spanning Trees", "text": "We represent the generic directed graph G = (V, E) by its vertex set\nV = {v 1 , . . . , v n } and set E \u2286 [1 : n] \u00d7 [1 : n] of pairs (i, j) of directed edges v i \u2192 v j .\nEach such edge has a score s(i, j). Since G is directed, s(i, j) does not necessarily equal s(j, i). A maximum spanning tree (MST) of G is a tree y \u2286 E that maximizes the value (i,j)\u2208y s(i, j) such that every vertex in V appears in y. The maximum projective spanning tree of G is constructed similarly except that it can only contain projective edges relative to some total order on the vertices of G. The MST problem for directed graphs is also known as the maximum arborescence problem.\nFor each sentence x we define the directed graph\nG x = (V x , E x ) given by V x = {x 0 = root, x 1 , . . . , x n } E x = {(i, j) : i = j, (i, j) \u2208 [0 : n] \u00d7 [1 : n]}\nThat is, G x is a graph with the sentence words and the dummy root symbol as vertices and a directed edge between every pair of distinct words and from the root symbol to every word. It is clear that dependency trees for x and spanning trees for G x coincide, since both kinds of trees are required to be rooted at the dummy root and reach all the words in the sentence. Hence, finding a (projective) dependency tree with highest score is equivalent to finding a maximum (projective) spanning tree in G x .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Chu-Liu-Edmonds(G, s)", "text": "Graph\nG = (V, E) Edge weight function s : E \u2192 R 1. Let M = {(x * , x) : x \u2208 V, x * = arg max x s(x , x)} 2. Let GM = (V, M ) 3. If GM has no cycles, then it is an MST: return GM 4. Otherwise, find a cycle C in GM 5. Let GC = contract(G, C, s) 6. Let y = Chu-Liu-Edmonds(GC, s) 7. Find a vertex x \u2208 C s. t. (x , x) \u2208 y, (x , x) \u2208 C 8. return y \u222a C \u2212 {(x , x)} contract(G = (V, E), C, s) 1. Let GC be the subgraph of G excluding nodes in C 2. Add a node c to GC representing cycle C 3. For x \u2208 V \u2212 C : \u2203 x \u2208C (x , x) \u2208 E Add edge (c, x) to GC with s(c, x) = max x \u2208C s(x , x) 4. For x \u2208 V \u2212 C : \u2203 x \u2208C (x, x ) \u2208 E Add edge (x, c) to GC with s(x, c) = max x \u2208C [s(x, x ) \u2212 s(a(x ), x ) + s(C)]\nwhere a(v) is the predecessor of v in C and s(C) = P v\u2208C s(a(v), v) 5. return GC ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Non-projective Trees", "text": "To find the highest scoring non-projective tree we simply search the entire space of spanning trees with no restrictions. Well-known algorithms exist for the less general case of finding spanning trees in undirected graphs (Cormen et al., 1990).\nEfficient algorithms for the directed case are less well known, but they exist. We will use here the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965;Edmonds, 1967), sketched in Figure 3 following Leonidas (2003). Informally, the algorithm has each vertex in the graph greedily select the incoming edge with highest weight. If a tree results, it must be the maximum spanning tree. If not, there must be a cycle. The procedure identifies a cycle and contracts it into a single vertex and recalculates edge weights going into and out of the cycle. It can be shown that a maximum spanning tree on the contracted graph is equivalent to a maximum spanning tree in the original graph (Leonidas, 2003). Hence the algorithm can recursively call itself on the new graph. Naively, this algorithm runs in O(n 3 ) time since each recursive call takes O(n 2 ) to find the highest incoming edge for each word and to contract the graph. There are at most O(n) recursive calls since we cannot contract the graph more then n times. However, Tarjan (1977) gives an efficient implementation of the algorithm with O(n 2 ) time complexity for dense graphs, which is what we need here.\nTo find the highest scoring non-projective tree for a sentence, x, we simply construct the graph G x and run it through the Chu-Liu-Edmonds algorithm. The resulting spanning tree is the best non-projective dependency tree. We illustrate here the application of the Chu-Liu-Edmonds algorithm to dependency parsing on the simple example x = John saw Mary, with directed graph representation G x , If the result were a tree, it would have to be the maximum spanning tree. However, in this case we have a cycle, so we will contract it into a single node and recalculate edge weights according to Figure 3. The new vertex w js represents the contraction of vertices John and saw. The edge from w js to Mary is 30 since that is the highest scoring edge from any vertex in w js . The edge from root into w js is set to 40 since this represents the score of the best spanning tree originating from root and including only the vertices in w js . The same leads to the edge from Mary to w js . The fundamental property of the Chu-Liu-Edmonds algorithm is that an MST in this graph can be transformed into an MST in the original graph (Leonidas, 2003). Thus, we recursively call the algorithm on this graph. Note that we need to keep track of the real endpoints of the edges into and out of w js for reconstruction later. Running the algorithm, we must find the best incoming edge to all words This is a tree and thus the MST of this graph. We now need to go up a level and reconstruct the graph.\nThe edge from w js to Mary originally was from the word saw, so we include that edge. Furthermore, the edge from root to w js represented a tree from root to saw to John, so we include all those edges to get the final (and correct) MST, A possible concern with searching the entire space of spanning trees is that we have not used any syntactic constraints to guide the search. Many languages that allow non-projectivity are still primarily projective. By searching all possible non-projective trees, we run the risk of finding extremely bad trees. We address this concern in Section 4.", "publication_ref": ["b3", "b0", "b9", "b17", "b17", "b24", "b17"], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "Projective Trees", "text": "It is well known that projective dependency parsing using edge based factorization can be handled with the Eisner algorithm (Eisner, 1996). This algorithm has a runtime of O(n 3 ) and has been employed successfully in both generative and discriminative parsing models (Eisner, 1996;McDonald et al., 2005). Furthermore, it is trivial to show that the Eisner algorithm solves the maximum projective spanning tree problem.\nThe Eisner algorithm differs significantly from the Chu-Liu-Edmonds algorithm. First of all, it is a bottom-up dynamic programming algorithm as opposed to a greedy recursive one. A bottom-up algorithm is necessary for the projective case since it must maintain the nested structural constraint, which is unnecessary for the non-projective case.", "publication_ref": ["b10", "b10", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Dependency Trees as MSTs: Summary", "text": "In the preceding discussion, we have shown that natural language dependency parsing can be reduced to finding maximum spanning trees in directed graphs. This reduction results from edge-based factorization and can be applied to projective languages with the Eisner parsing algorithm and non-projective languages with the Chu-Liu-Edmonds maximum spanning tree algorithm. The only remaining problem is how to learn the weight vector w.\nA major advantage of our approach over other dependency parsing models is its uniformity and simplicity. By viewing dependency structures as spanning trees, we have provided a general framework for parsing trees for both projective and nonprojective languages. Furthermore, the resulting parsing algorithms are more efficient than lexicalized phrase structure approaches to dependency parsing, allowing us to search the entire space without any pruning. In particular the non-projective parsing algorithm based on the Chu-Liu-Edmonds MST algorithm provides true non-projective parsing. This is in contrast to other non-projective methods, such as that of Nivre and Nilsson (2005), who implement non-projectivity in a pseudo-projective parser with edge transformations. This formulation also dispels the notion that non-projective parsing is \"harder\" than projective parsing. In fact, it is easier since non-projective parsing does not need to enforce the non-crossing constraint of projective trees. As a result, non-projective parsing complexity is just O(n 2 ), against the O(n 3 ) complexity of the Eisner dynamic programming algorithm, which by construction enforces the non-crossing constraint.", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "Online Large Margin Learning", "text": "In this section, we review the work of McDonald et al. (2005) for online large-margin dependency parsing. As usual for supervised learning, we assume a training set T = {(x t , y t )} T t=1 , consisting of pairs of a sentence x t and its correct dependency tree y t . In what follows, dt(x) denotes the set of possible dependency trees for sentence x.\nThe basic idea is to extend the Margin Infused Relaxed Algorithm (MIRA)  to learning with structured outputs, in the present case dependency trees. Figure 4 gives pseudo-code for the MIRA algorithm as presented by McDonald et al. (2005). An online learning algorithm considers a single training instance at each update to w. The auxiliary vector v accumulates the successive values of w, so that the final weight vector is the average of the weight vec-\nTraining data: T = {(xt, yt)} T t=1 1. w0 = 0; v = 0; i = 0 2. for n : 1..N 3. for t : 1..T 4. min \u201a \u201a \u201aw (i+1) \u2212 w (i) \u201a \u201a \u201a s.t. s(xt, yt) \u2212 s(xt, y ) \u2265 L(yt, y ), \u2200y \u2208 dt(xt) 5. v = v + w (i+1) 6. i = i + 1 7. w = v/(N * T )\nFigure 4: MIRA learning algorithm.\ntors after each iteration. This averaging effect has been shown to help overfitting (Collins, 2002).\nOn each update, MIRA attempts to keep the new weight vector as close as possible to the old weight vector, subject to correctly classifying the instance under consideration with a margin given by the loss of the incorrect classifications. For dependency trees, the loss of a tree is defined to be the number of words with incorrect parents relative to the correct tree. This is closely related to the Hamming loss that is often used for sequences (Taskar et al., 2003).\nFor arbitrary inputs, there are typically exponentially many possible parses and thus exponentially many margin constraints in line 4 of Figure 4.", "publication_ref": ["b19", "b19", "b2", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Single-best MIRA", "text": "One solution for the exponential blow-up in number of trees is to relax the optimization by using only the single margin constraint for the tree with the highest score, s(x, y). The resulting online update (to be inserted in Figure 4, line 4) would then be: 2005) used a similar update with k constraints for the k highest-scoring trees, and showed that small values of k are sufficient to achieve the best accuracy for these methods. However, here we stay with a single best tree because kbest extensions to the Chu-Liu-Edmonds algorithm are too inefficient (Hou, 1996).\nmin w (i+1) \u2212 w (i) s.t. s(x t , y t ) \u2212 s(x t , y ) \u2265 L(y t , y ) where y = arg max y s(x t , y ) McDonald et al. (\nThis model is related to the averaged perceptron algorithm of Collins (2002). In that algorithm, the single highest scoring tree (or structure) is used to update the weight vector. However, MIRA aggressively updates w to maximize the margin between the correct tree and the highest scoring tree, which has been shown to lead to increased accuracy.", "publication_ref": ["b15", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Factored MIRA", "text": "It is also possible to exploit the structure of the output space and factor the exponential number of margin constraints into a polynomial number of local constraints (Taskar et al., 2003;Taskar et al., 2004). For the directed maximum spanning tree problem, we can factor the output by edges to obtain the following constraints:\nmin w (i+1) \u2212 w (i) s.t. s(l, j) \u2212 s(k, j) \u2265 1 \u2200(l, j) \u2208 y t , (k, j) / \u2208 y t\nThis states that the weight of the correct incoming edge to the word x j and the weight of all other incoming edges must be separated by a margin of 1.\nIt is easy to show that when all these constraints are satisfied, the correct spanning tree and all incorrect spanning trees are separated by a score at least as large as the number of incorrect incoming edges. This is because the scores for all the correct arcs cancel out, leaving only the scores for the errors causing the difference in overall score. Since each single error results in a score increase of at least 1, the entire score difference must be at least the number of errors. For sequences, this form of factorization has been called local lattice preference (Crammer et al., 2004). Let n be the number of nodes in graph G x .\nThen the number of constraints is O(n 2 ), since for each node we must maintain n \u2212 1 constraints. The factored constraints are in general more restrictive than the original constraints, so they may rule out the optimal solution to the original problem. McDonald et al. (2005) examines briefly factored MIRA for projective English dependency parsing, but for that application, k-best MIRA performs as well or better, and is much faster to train.", "publication_ref": ["b25", "b26", "b6", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We performed experiments on the Czech Prague Dependency Treebank (PDT) (Haji\u010d, 1998;Haji\u010d et al., 2001). We used the predefined training, development and testing split of this data set. Furthermore, we used the automatically generated POS tags that are provided with the data. Czech POS tags are very complex, consisting of a series of slots that may or may not be filled with some value. These slots represent lexical and grammatical properties such as standard POS, case, gender, and tense. The result is that Czech POS tags are rich in information, but quite sparse when viewed as a whole. To reduce sparseness, our features rely only on the reduced POS tag set from Collins et al. (1999). The number of features extracted from the PDT training set was 13, 450, 672, using the feature set outlined by McDonald et al. (2005).\nCzech has more flexible word order than English and as a result the PDT contains non-projective dependencies. On average, 23% of the sentences in the training, development and test sets have at least one non-projective dependency. However, less than 2% of total edges are actually non-projective. Therefore, handling non-projective edges correctly has a relatively small effect on overall accuracy. To show the effect more clearly, we created two Czech data sets. The first, Czech-A, consists of the entire PDT. The second, Czech-B, includes only the 23% of sentences with at least one non-projective dependency. This second set will allow us to analyze the effectiveness of the algorithms on non-projective material. We compared the following systems:\n1. COLL1999: The projective lexicalized phrase-structure parser of Collins et al. (1999).", "publication_ref": ["b12", "b11", "b1", "b19", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "N&N2005:", "text": "The pseudo-projective parser of Nivre and Nilsson (2005).", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "McD2005:", "text": "The projective parser of McDonald et al. (2005) that uses the Eisner algorithm for both training and testing. This system uses k-best MIRA with k=5.\n4. Single-best MIRA: In this system we use the Chu-Liu-Edmonds algorithm to find the best dependency tree for Single-best MIRA training and testing.\n5. Factored MIRA: Uses the quadratic set of constraints based on edge factorization as described in Section 3.2. We use the Chu-Liu-Edmonds algorithm to find the best tree for the test data.", "publication_ref": ["b19"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Results are shown in Table 1. There are two main metrics. The first and most widely recognized is Accuracy, which measures the number of words that correctly identified their parent in the tree. Complete measures the number of sentences in which the resulting tree was completely correct. Clearly, there is an advantage in using the Chu-Liu-Edmonds algorithm for Czech dependency pars-   Collins et al. (1999) and Zeman (2004) that use expensive O(n 5 ) parsing algorithms. We should note that the results in Collins et al. (1999) are different then reported here due to different training and testing data sets. One concern raised in Section 2.2.1 is that searching the entire space of non-projective trees could cause problems for languages that are primarily projective. However, as we can see, this is not a problem. This is because the model sets its weights with respect to the parsing algorithm and will disfavor features over unlikely non-projective edges.\nSince the space of projective trees is a subset of the space of non-projective trees, it is natural to wonder how the Chu-Liu-Edmonds parsing algorithm performs on projective data since it is asymptotically better than the Eisner algorithm. Table 2 shows the results for English projective dependency trees extracted from the Penn Treebank (Marcus et al., 1993) using the rules of Yamada and Matsumoto (2003).  This shows that for projective data sets, training and testing with the Chu-Liu-Edmonds algorithm is worse than using the Eisner algorithm. This is not surprising since the Eisner algorithm uses the a priori knowledge that all trees are projective.", "publication_ref": ["b1", "b29", "b1", "b18", "b28"], "figure_ref": [], "table_ref": ["tab_2", "tab_4"]}, {"heading": "Discussion", "text": "We presented a general framework for parsing dependency trees based on an equivalence to maximum spanning trees in directed graphs. This framework provides natural and efficient mechanisms for parsing both projective and non-projective languages through the use of the Eisner and Chu-Liu-Edmonds algorithms. To learn these structures we used online large-margin learning (McDonald et al., 2005) that empirically provides state-of-the-art performance for Czech.\nA major advantage of our models is the ability to naturally model non-projective parses. Nonprojective parsing is commonly considered more difficult than projective parsing. However, under our framework, we show that the opposite is actually true that non-projective parsing has a lower asymptotic complexity. Using this framework, we presented results showing that the non-projective model outperforms the projective model on the Prague Dependency Treebank, which contains a small number of non-projective edges.\nOur method requires a tree score that decomposes according to the edges of the dependency tree. One might hope that the method would generalize to include features of larger substructures. Unfortunately, that would make the search for the best tree intractable (H\u00f6ffgen, 1993).", "publication_ref": ["b19", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We thank Lillian Lee for bringing an important missed connection to our attention, and Koby Crammer for his help with learning algorithms. This work has been supported by NSF ITR grants 0205448 and 0428193.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "On the shortest arborescence of a directed graph", "journal": "Science Sinica", "year": "1965", "authors": "Y J Chu; T H Liu"}, {"ref_id": "b1", "title": "A statistical parser for Czech", "journal": "", "year": "1999", "authors": "M Collins; J Haji\u010d; L Ramshaw; C Tillmann"}, {"ref_id": "b2", "title": "Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms", "journal": "", "year": "2002", "authors": "M Collins"}, {"ref_id": "b3", "title": "Introduction to Algorithms", "journal": "MIT Press/McGraw-Hill", "year": "1990", "authors": "T H Cormen; C E Leiserson; R L Rivest"}, {"ref_id": "b4", "title": "Ultraconservative online algorithms for multiclass problems", "journal": "JMLR", "year": "2003", "authors": "K Crammer; Y Singer"}, {"ref_id": "b5", "title": "Online passive aggressive algorithms", "journal": "", "year": "2003", "authors": "K Crammer; O Dekel; S Shalev-Shwartz; Y Singer"}, {"ref_id": "b6", "title": "New large margin algorithms for structured prediction", "journal": "", "year": "2004", "authors": "K Crammer; R Mcdonald; F Pereira"}, {"ref_id": "b7", "title": "Dependency tree kernels for relation extraction", "journal": "", "year": "2004", "authors": "A Culotta; J Sorensen"}, {"ref_id": "b8", "title": "Machine translation using probabilistic synchronous dependency insertion grammars", "journal": "", "year": "2005", "authors": "Y Ding; M Palmer"}, {"ref_id": "b9", "title": "Journal of Research of the National Bureau of Standards", "journal": "", "year": "1967", "authors": "J Edmonds"}, {"ref_id": "b10", "title": "Three new probabilistic models for dependency parsing: An exploration", "journal": "", "year": "1996", "authors": "J Eisner"}, {"ref_id": "b11", "title": "The Prague Dependency Treebank 1.0 CDROM. Linguistics Data Consortium Cat", "journal": "", "year": "2001", "authors": "J Haji\u010d; E Hajicova; P Pajas; J Panevova; P Sgall; B Vidova Hladka"}, {"ref_id": "b12", "title": "Building a syntactically annotated corpus: The Prague dependency treebank. Issues of Valency and Meaning", "journal": "", "year": "1998", "authors": "J Haji\u010d"}, {"ref_id": "b13", "title": "Semantic dependency analysis method for Japanese based on optimum tree search algorithm", "journal": "", "year": "2001", "authors": "H Hirakawa"}, {"ref_id": "b14", "title": "Learning and robust learning of product distributions", "journal": "", "year": "1993", "authors": "-U Klaus;  H\u00f6ffgen"}, {"ref_id": "b15", "title": "Algorithm for finding the first k shortest arborescences of a digraph", "journal": "Mathematica Applicata", "year": "1996", "authors": "W Hou"}, {"ref_id": "b16", "title": "Word Grammar", "journal": "", "year": "1984", "authors": "R Hudson"}, {"ref_id": "b17", "title": "Arborescence optimization problems solvable by Edmonds' algorithm", "journal": "Theoretical Computer Science", "year": "2003", "authors": "G Leonidas"}, {"ref_id": "b18", "title": "Building a large annotated corpus of English: the Penn Treebank", "journal": "Computational Linguistics", "year": "1993", "authors": "M Marcus; B Santorini; M Marcinkiewicz"}, {"ref_id": "b19", "title": "Online large-margin training of dependency parsers", "journal": "", "year": "2005", "authors": "R Mcdonald; K Crammer; F Pereira"}, {"ref_id": "b20", "title": "Pseudo-projective dependency parsing", "journal": "", "year": "2005", "authors": "J Nivre; J Nilsson"}, {"ref_id": "b21", "title": "Deterministic dependency parsing of english text", "journal": "", "year": "2004", "authors": "J Nivre; M Scholz"}, {"ref_id": "b22", "title": "Automatic paraphrase acquisition from news articles", "journal": "", "year": "2002", "authors": "Y Shinyama; S Sekine; K Sudo; R Grishman"}, {"ref_id": "b23", "title": "Learning syntactic patterns for automatic hypernym discovery", "journal": "", "year": "2004", "authors": "R Snow; D Jurafsky; A Y Ng"}, {"ref_id": "b24", "title": "Finding optimum branchings. Networks", "journal": "", "year": "1977", "authors": "R E Tarjan"}, {"ref_id": "b25", "title": "Max-margin Markov networks", "journal": "", "year": "2003", "authors": "B Taskar; C Guestrin; D Koller"}, {"ref_id": "b26", "title": "Max-margin parsing", "journal": "", "year": "2004", "authors": "B Taskar; D Klein; M Collins; D Koller; C Manning"}, {"ref_id": "b27", "title": "A statistical constraint dependency grammar (CDG) parser", "journal": "", "year": "2004", "authors": "W Wang; M P Harper"}, {"ref_id": "b28", "title": "Statistical dependency analysis with support vector machines", "journal": "", "year": "2003", "authors": "H Yamada; Y Matsumoto"}, {"ref_id": "b29", "title": "Parsing with a Statistical Dependency Model", "journal": "", "year": "2004", "authors": "D Zeman"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: An example dependency tree.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Non-projective dependency trees in English and Czech.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Chu-Liu-Edmonds algorithm for finding maximum spanning trees in directed graphs.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "step of the algorithm is to find, for each word, the highest scoring incoming edge", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Dependency parsing results for Czech. Czech-B is the subset of Czech-A containing only sentences with at least one non-projective dependency.", "figure_data": "ing. Even though less than 2% of all dependenciesare non-projective, we still see an absolute improve-ment of up to 1.1% in overall accuracy over theprojective model. Furthermore, when we focus onthe subset of data that only contains sentences withat least one non-projective dependency, the effectis amplified. Another major improvement here isthat the Chu-Liu-Edmonds non-projective MST al-gorithm has a parsing complexity of O(n 2 ), versusthe O(n 3 ) complexity of the projective Eisner algo-rithm, which in practice leads to improvements inparsing time. The results also show that in termsof Accuracy, factored MIRA performs better thansingle-best MIRA. However, for the factored model,we do have O(n 2 ) margin constraints, which re-sults in a significant increase in training time oversingle-best MIRA. Furthermore, we can also see thatthe MST parsers perform favorably compared to themore powerful lexicalized phrase-structure parsers,such as those presented by"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Dependency parsing results for English using spanning tree algorithms.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "s(x, y) = (i,j)\u2208y s(i, j) = (i,j)\u2208y w \u2022 f(i, j)", "formula_coordinates": [3.0, 91.68, 154.81, 187.11, 23.97]}, {"formula_id": "formula_1", "formula_text": "V = {v 1 , . . . , v n } and set E \u2286 [1 : n] \u00d7 [1 : n] of pairs (i, j) of directed edges v i \u2192 v j .", "formula_coordinates": [3.0, 72.0, 337.93, 226.79, 25.05]}, {"formula_id": "formula_2", "formula_text": "G x = (V x , E x ) given by V x = {x 0 = root, x 1 , . . . , x n } E x = {(i, j) : i = j, (i, j) \u2208 [0 : n] \u00d7 [1 : n]}", "formula_coordinates": [3.0, 72.0, 514.09, 220.83, 47.95]}, {"formula_id": "formula_3", "formula_text": "G = (V, E) Edge weight function s : E \u2192 R 1. Let M = {(x * , x) : x \u2208 V, x * = arg max x s(x , x)} 2. Let GM = (V, M ) 3. If GM has no cycles, then it is an MST: return GM 4. Otherwise, find a cycle C in GM 5. Let GC = contract(G, C, s) 6. Let y = Chu-Liu-Edmonds(GC, s) 7. Find a vertex x \u2208 C s. t. (x , x) \u2208 y, (x , x) \u2208 C 8. return y \u222a C \u2212 {(x , x)} contract(G = (V, E), C, s) 1. Let GC be the subgraph of G excluding nodes in C 2. Add a node c to GC representing cycle C 3. For x \u2208 V \u2212 C : \u2203 x \u2208C (x , x) \u2208 E Add edge (c, x) to GC with s(c, x) = max x \u2208C s(x , x) 4. For x \u2208 V \u2212 C : \u2203 x \u2208C (x, x ) \u2208 E Add edge (x, c) to GC with s(x, c) = max x \u2208C [s(x, x ) \u2212 s(a(x ), x ) + s(C)]", "formula_coordinates": [3.0, 319.2, 65.19, 228.81, 191.78]}, {"formula_id": "formula_4", "formula_text": "Training data: T = {(xt, yt)} T t=1 1. w0 = 0; v = 0; i = 0 2. for n : 1..N 3. for t : 1..T 4. min \u201a \u201a \u201aw (i+1) \u2212 w (i) \u201a \u201a \u201a s.t. s(xt, yt) \u2212 s(xt, y ) \u2265 L(yt, y ), \u2200y \u2208 dt(xt) 5. v = v + w (i+1) 6. i = i + 1 7. w = v/(N * T )", "formula_coordinates": [5.0, 313.2, 54.86, 218.28, 118.65]}, {"formula_id": "formula_5", "formula_text": "min w (i+1) \u2212 w (i) s.t. s(x t , y t ) \u2212 s(x t , y ) \u2265 L(y t , y ) where y = arg max y s(x t , y ) McDonald et al. (", "formula_coordinates": [5.0, 313.2, 496.97, 188.39, 59.34]}, {"formula_id": "formula_6", "formula_text": "min w (i+1) \u2212 w (i) s.t. s(l, j) \u2212 s(k, j) \u2265 1 \u2200(l, j) \u2208 y t , (k, j) / \u2208 y t", "formula_coordinates": [6.0, 132.36, 216.53, 105.75, 40.36]}], "doi": ""}