{"title": "Resolving Indirect Referring Expressions for Entity Selection", "authors": "Mohammad Javad Hosseini; Filip Radlinski; Silvia Pareti; Annie Louis", "pub_date": "", "abstract": "Recent advances in language modeling have enabled new conversational systems. In particular, it is often desirable for people to make choices among specified options when using such systems. We address this problem of reference resolution, when people use natural expressions to choose between the entities. For example, given the choice 'Should we make a Simnel cake or a Pandan cake?' a natural response from a dialog participant may be indirect: 'let's make the green one'. Such natural expressions have been little studied for reference resolution. We argue that robustly understanding such language has large potential for improving naturalness in dialog, recommendation, and search systems. We create AltEntities 1 (Alternative Entities), a new public dataset of 42K entity pairs and expressions (referring to one entity in the pair), and develop models for the disambiguation problem. Consisting of indirect referring expressions across three domains, our corpus enables for the first time the study of how language models can be adapted to this task. We find they achieve 82%-87% accuracy in realistic settings, which while reasonable also invites further advances.", "sections": [{"heading": "Introduction", "text": "Natural dialog often requires resolving referring expressions (REs), not only within and across texts, but also for grounding natural language expressions to specific entities or images. We focus on a specific conversational setting where a speaker's utterance intends to disambiguate between known named entities. While many aspects of RE resolution have been studied extensively, past work has focused on pragmatic reasoning (Dale and Reiter, 1995;Frank and Goodman, 2012), influence of discourse (Orita et al., 2015), and multimodal (e.g., image) context (Zhang et al., 2018).\nDid you mean a Simnel or Pandan cake? It looks surprisingly green in color Without any frosting or fruit It is made from some leaf Comes from Indonesia Isn't the Easter one In the specific case of dialog, when people make choices, the natural REs are not always item names, spatial locations or attributes present in the question. For instance when the choice is among items with similar names (perhaps disambiguating automatic speech recognition errors), or items with difficult to pronounce names, or where the user does not even recall which name is correct but instead recalls some higher level attribute, the user may choose an indirect expression (Table 1). Most related to our work, Celikyilmaz et al. (2014) previously studied REs in response to a set of related items (e.g., Harry Potter movies) shown in a user interface. Their work both contains direct (using entity name), indirect, as well as locational (entity's position on the screen) expressions. Predating recent advances in language models (LMs), their best model is a decision tree classifier consuming knowledge graph metadata.\nIn this work, we created the AltEntities corpus by a multi-step process, soliciting crowdworkers to provide diverse yet realistic natural expressions for selecting entities in three domains: BOOKS, RECIPES, and MUSIC. To obtain natural and casual dialogic language, we introduce a novel cartoon-based annotation approach (Figure 1). AltEntities consists of 6,247 alternative questions (presenting two entities) along with 42,529 REs. In this context, REs are typically definite noun phrases with a pronominal head and a restrictive relative phrase or one of its reduced variants.\nOur experiments are based on fine-tuned BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) LMs. We assess the representation of entity names as well as other sources of entity information. We find that the results depend significantly on the type of entity information provided to the models alongside the REs: If a LM only has access to the entity names but no other information, a case that might happen especially for long tail entities, accuracy is around 60%. On the other hand, if a LM is (unrealistically) given entity information that is identical to that shown to annotators producing the REs, accuracy is very high (up to 95%). However, if the model (more realistically) only has access to generic information that may or may not overlap with annotators' knowledge (Section 5), accuracy of our models is only 82%-87%, leaving significant room for methodological improvements.", "publication_ref": ["b7", "b13", "b18", "b34", "b4", "b9", "b22"], "figure_ref": ["fig_0"], "table_ref": ["tab_0"]}, {"heading": "Related Work", "text": "Our work adds to recent efforts to allow users to speak more naturally to conversational systems.\nHere, we present the most related studies focusing on the properties of REs as well as their resolution.\nAlternative Questions. Our questions belong to the class of alternative questions (e.g. 'Are you staying or leaving?'). Several studies have focused on the form and semantics of such questions, and differences from yes/no questions particularly on the basis of prosody (Beck and Kim, 2006;Biezma and Rawlins, 2012;Pruitt and Roelofsen, 2013).\nThis paper focuses on the deep understanding of answers to such alternative questions when they are posed for selecting between two entities.\nSpeaker-Listener Cooperation. The research in this space follow the Rational Speech Act Theory (Frank and Goodman, 2012), where the way speakers and listeners reason about each others' intentions and beliefs explains which attributes speakers pick to describe an entity, and how listeners disambiguate the entity. Vogel et al. (2013); Monroe et al. (2017) focus on the pragmatic reasoning involved during the conversation which helps in reaching a common understanding of the topic. Wilkes-Gibbs and Clark (1992) study how REs change as the conversation proceeds. In an experiment, they show that participants start from long and indefinite descriptions of images, but end up with short and definite references. Jordan and Walker ( 2005) study the subproblem of content and attribute selection for generating object descriptions.\nIn our data collection, we assume a conversation between two humans in three dialog turns, where the first two turns prime the RE produced in the last turn (Section 3). Common Ground. In addition to the interlocutors' intentions, their prior or shared knowledge also plays an important role in how they understand each other's utterances. Sometimes the common knowledge arises from a shared situation, e.g., in navigation dialog (Engonopoulos et al., 2013;Misu et al., 2014;Fang et al., 2014) or the presence of a visual space (Yu et al., 2018;Bernardi and Pezzelle, 2021). In the latter, the common ground is given, i.e., it is assumed the image is what all participants in the interaction see in the same way. In many other situations, e.g., in a dialog between two friends about a movie or a book, the common ground is hidden and we can only make assumptions of what information participants share.\nIn this work, during data collection, we assume that annotators have access to rich common ground involving multiple modalities such as text, image, and video (Section 3.3). During model training inference, we explore performance with varying levels of background information (Sectoin 5.2).\nImplicature Understanding. This paper advances the broad area of understanding implicature in dialog. For example, a few recent papers developed datasets and models for indirect boolean responses (without saying 'yes' or 'no') (Pragst and Ultes, 2018;Louis et al., 2020;Takayama et al., 2021;Damgaard et al., 2021). Interestingly, Ruis et al. (2022) shows that LLMs cannot solve such implicatures in a zero-shot setting.\nRE resolution. There are few prior studies around the data and models for resolution tasks such as ours. Stoyanchev et al. (2021) built a method where references to items from prior context in a dialog are resolved by detecting state updates. Unlike our work, their REs focus on attributes (e.g., Italian in the Italian restaurant) discussed in prior dialog. Celikyilmaz et al. (2014) collect REs to a target item among others shown on a screen (e.g., a set of Harry Potter movies). Their expressions contain both direct (reference to entity name) and indirect references, where the latter comprise about 25% of the data (\u2248 6K REs). To aid the resolution of indirect ones, they include features which capture the overlap between an expression and knowledge graph attributes for each item.\nOur work creates a large scale corpus (42K REs) exclusively for indirect REs, and explores how LMs encode the knowledge for disambiguation.", "publication_ref": ["b0", "b2", "b21", "b13", "b30", "b17", "b31", "b10", "b16", "b12", "b33", "b1", "b20", "b15", "b29", "b8", "b26", "b28", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Collecting Rich Referring Expressions", "text": "To maximize generalizability, we collect data in three domains: BOOKS, RECIPES, and MUSIC. These were selected to cover a diverse variety of entity types with different kinds of available information -e.g. plot summaries for books, images for recipes, and lyrics and videos for songs. We performed careful and detailed annotations, and explain the annotation steps in this section.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Cartoon-driven Annotation Setup", "text": "Previous work in question-answering and dialog typically asks annotators to complete text-based input boxes (Rajpurkar et al., 2016;Choi et al., 2018;Rajpurkar et al., 2018;Reddy et al., 2019;Eric et al., 2020). We employ a novel cartoon-bubble completion method, aiming to immerse annotators in the dialog setting to obtain more natural and informal REs. We start with a brief overview of the setup, and then explain the steps in detail.\nFigure 1 shows the first (of our two) annotation screens. Annotators are shown a cartoon with two characters (Bob and Alice) in a fictional conversation, and asked (as Bob) to complete the last speech bubble. This pictorial depiction, and the casting of the dialog as a casual chat between friends encourage the annotators to produce friendly, short, and dialogic responses. However, annotators are generally unlikely to know details about entities sampled from a collection. Therefore, we also provide background information on the entities (bottom of Figure 1), corresponding to common knowledge that the two characters could share on the topic.\nAfter annotators are shown this information, they proceed to a second screen (Figure 2). It indicates one of the entities (books in this example). They are asked to describe that entity (indirectly) with 3 to 5 responses: We found eliciting more entries encourages diversity and depth in the responses. Our data consists of the entity pairs, their descriptions, the target entity, and annotator expressions.\nFrom Figure 2, note that once on the response screen, annotators cannot re-read descriptions. This encourages recall from memory. The reasoning behind this, and many other aspects of this design, are explained in the next sections.", "publication_ref": ["b24", "b6", "b23", "b25", "b11"], "figure_ref": ["fig_0", "fig_0", "fig_1", "fig_1"], "table_ref": []}, {"heading": "The Conversational Cartoon", "text": "The cartoon has three cells as shown in Figure 1. The first is a domain-specific utterance intended to set context. For example, 'Remember that book we saw at the store?' sets up the dialog as one recalling a specific book. These utterances are from a set of five manually written expressions for each domain, with one selected at random for each conversation. Examples in the RECIPES and MUSIC domains are 'That recipe on today's Masterchef was too good!' and 'You sang that song really well yesterday.' Appendix A shows all these utterances.\nThe alternative question is presented in the second cell. This question follows a fixed template: Do you mean 'A' or 'B'? where 'A' and 'B' are the names of two related entities. Our entities are sampled from Wikipedia page titles, with any disambiguation parentheses removed. When the names are identical, we retain the Wikipedia disambiguation: For instance, one such question is Do you mean 'The Gladiator (Turtledove novel)' or 'The Gladiator (Scarrow novel)'?.\nThe third cell is completed by the crowdworkers, assuming the role of Bob to enter text that refers to the target entity. They enter those expressions as shown in Figure 2. Further screenshots of our interface for all domains are provided in Appendix B.", "publication_ref": [], "figure_ref": ["fig_0", "fig_1"], "table_ref": []}, {"heading": "Entity Background", "text": "In real dialogs, when people differentiate between options, they draw on partial knowledge about entities that they recall. We aimed to foster a similar situation in our corpus, while doing so in a controlled manner without requiring domain-expert annotators. As such, when selected entities are shown to annotators, they are also presented with background information (bottom of Figure 1). We draw the background also from Wikipedia, biasing towards sections relevant to each domain. For BOOKS, these are the main (first) and plot summary sections. For RECIPES, we used the main, preparation, and ingredients sections. For each entity, up to 750 characters of one of these sections are shown on the interface. For RECIPES, the food's image 2 is also always shown to help the annotators quickly realize what it looks like (Figure 3).\nFor MUSIC, however, we found Wikipedia text to be less useful: Pages contain details and trivia (e.g., 5th single on the album or sold 4 million copies), which we judged unlikely to be included  in natural background knowledge about a song. On the other hand, song lyrics and music are very relevant in this domain, but are not usually found in Wikipedia. Consequently, we presented a Google search link for the song in the background section, and asked the annotators to listen to at least some of each song, and read about them before writing expressions. The search query contained the song's title and its artist, e.g., Hello (by Adele). Since information about the song comes from search, we also biased our candidates towards popular songs, which have more detailed results (Section 3.4).", "publication_ref": [], "figure_ref": ["fig_0", "fig_2"], "table_ref": []}, {"heading": "Generating Alternative Questions", "text": "The alternative questions (Do you mean 'A' or 'B'?) are generated automatically: (i) Candidate entities are extracted from English Wikipedia for each do- main (Section 3.4.1), then (ii) we substitute 'A' and 'B' by sampling entity pairs (Section 3.4.2).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Selecting Candidate Entities", "text": "For each domain, we collect English Wikipedia articles by checking the presence of certain Wikipedia templates (infoboxes 3 ), and the presence of particular sections: For RECIPES, we additionally included articles with an ingredients section.\nThis set was then filtered to exclude very short articles, or those ambiguous between domains. For MUSIC, we use article length (number of sections/subsections) as a proxy for popularity, and choose the top \u2248 1000 articles. To remove any sensitive or offensive content, we also filter articles whose content matches a list of sensitive words. Appendix C contains the details of the above filters.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sampling Entity Pairs", "text": "Much linguistic work on alternative questions has focused on the semantics and pragmatics of these utterances (Biezma and Rawlins, 2012), but we also need to make decisions about which entity pairs could make for a challenging disambiguation problem. Entity pairs sampled uniformly at random are less likely to be interesting, since they may not share many properties, making disambiguation easier. In this work, we develop entity pair sampling techniques at different similarity levels, as a proxy for disambiguation difficulty.\nUniform sampling. Entity pairs are sampled uniformly at random from the domain.\nSame name. These entities have the same name in Wikipedia followed by a disambiguation phrase within parentheses. An example is Dawn (McLaughlin novel) and Dawn (Andrews novel).\nSimilar title. These entities have a similar title in terms of character edit distance (distance \u2264 3), where the title could optionally consists of a disambiguation phrase within parentheses.\nSimilar description. This method looks for deeper similarity within the text of Wikipedia articles: We sample a first entity uniformly, then select the second with the highest similarity using a Universal Sentence Encoder (Cer et al., 2018). The input to the encoder is the Wikipedia section shown as the background knowledge to annotators.\nSimilar infobox attributes. Here we take entities that share important domain-specific properties, e.g., recipe origin, or the song genre. We match entities (except BOOKS) using the 'attributes' listed in the Wikipedia infobox: {type} and {type, country} for RECIPES, and {genre}, {artist}, and {genre, artist} for MUSIC.\nWe applied the same name method only to BOOKS, and the similar title method only to BOOKS and RECIPES. The other domains did not contain enough such examples. We applied the similar description method to all domains. We applied the similar infobox attributes method to RECIPES and MUSIC, but not the BOOKS domain;  Vary the phrasing: the book about, I meant the, was thinking of, the one about, I wasn't referring to, etc. Don't Mention the book by name or position (e.g., the second one).\nUse too detailed information that Alice may not recall (eg. 1992 or in the 90s are better choices than Sep 9 1992).\nCopy whole sentences from the description. however, some pairs with identical attributes were already covered by the other methods for BOOKS.\nTable 3 shows the number of sampled entity pairs for each domain and sampling method.", "publication_ref": ["b2", "b5"], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Annotator Instructions and Pilot Runs", "text": "To maximize RE naturalness, we also provided annotators different domain-specific examples. Figure 2 shows those for the book The sympathizer.\nThe REs are about topic (about Vietnam war), timeline (set in the 70s), and contrasts (Not the one about slavery, and The one published earlier).\nThey also emphasize use of general statements instead of overly specific and unrealistic ones, e.g., set in the 70s instead of 1975. Table 4 shows a detailed note on desirable expressions. We performed pilot studies to understand how annotators responded to our instructions, and used these to refine the instructions. A first study (for BOOKS) examined how annotators should use the background text, comparing designs where annotators could, or could not, go back-and-forth between the description screen (Figure 1), and the data collection screen (Figure 2). With back-and-forth possible, the responses contained excessive details, e.g., reiterating large portions of background text (The book that was last of three juvenile novels that Wollheim wrote for Winston). With back-andforth removed, annotators produced shorter REs (7.99 vs 9.61 words), with fewer proper nouns and numbers per RE (0.43 vs 0.88) as they are harder to remember. They also used more contrastives, e.g., starting with 'not the' (21.8% vs 2.2%) which involve drawing on information about both books. Thus, we adopted the memory recall setting. 4 After the first pilot study, we performed one pilot per domain for relatively small instruction refinements.", "publication_ref": [], "figure_ref": ["fig_1", "fig_0", "fig_1"], "table_ref": ["tab_5"]}, {"heading": "The AltEntities Corpus", "text": "Our annotations were carried out using a pool of around 60 in-house crowdworkers. 5 They were all native English speakers recruited from U.S., U.K., Canada, and Australia so as to obtain a diverse set of perspectives. 6 Each question was shown to two workers to get multiple inputs per question. Around 2K entity pairs were annotated for each domain resulting in around 42K expressions in total. Table 5 shows the final corpus statistics, and Table 6 shows example expressions for the three domains. We release the dataset under the CC-BY SA 3.0 License as per the Wikipedia License.\nThe REs for BOOKS were on average a word longer than for other domains. They also contained more named entities per expression. Each domain contains some repeated REs (e.g., the pop song), that are often high-level responses, e.g., a song's genre. The BOOKS domain contains the most unique responses. The number of contrastives, estimated as REs starting with \"not the\", are from 8% in MUSIC up to 20% in BOOKS. 7 For MUSIC and RECIPES, we manually checked 200 random REs for references to modalities other than text. Around 10% multi-modal REs were present in the RECIPES domain (mostly color), and 20% in the MUSIC domain (mostly beat, speed, and mood).\nWe estimated the RE error rate by manually inspecting 40 question samples (around 250 to 300 expressions) per domain. The error rate is between 4.5% to 6.8% for the three domains. 78% of these errors were due to the RE applying to both items, not just the target entity. The remaining errors were mostly due to confusing the two entities. We also 4 Note that the MUSIC entities are provided with search links which open in a new page, making back-and-forth possible, although it was discouraged in the guidelines.\n5 Paid contractors who work with our institution on such tasks. 6 The average number of questions per annotator is 217. The minimum number of annotations was 10, and the maximum was 2015 questions, followed by 610 questions. Around 80% of annotators annotated around 100-600 questions each. We did not observe any obvious correlation between dataset artifacts and specific annotators. 7 This estimate gives a lower bound as there are other types of contrastives expressions such as the newer song.  ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7", "tab_8"]}, {"heading": "RECIPES comes from Azerbaijan", "text": "The Japanese steamed cake The ones eaten at Christmas cornmeal is the main ingredient Not the one with dried peaches. note that the rate of exact string match between REs and Wikipedia text is < 1%.\nThe annotators were inspired by the provided stylistic cues in the instructions (e.g., starting with the one or I meant the), but followed our guidelines to vary their responses as well. We observed that the content of REs (e.g., timeline, lyrics, singer or band information, instrument) included both the categories covered by the provided examples (e.g., timeline for books and songs) and novel categories (e.g., background information on books and songs such as The one inspired by a Rolling Stones song).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Task and Models", "text": "Indirect reference resolution can be defined as follows: Given an alternative question with K choices 8 C = {c 1 , . . . , c K }, and a RE r, models should disambiguate the choice c * \u2208 C intended by r. We assume r does not directly mention c * by its name or position, but does uniquely refer to c * .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Information Available to Models", "text": "At a minimum, all models require the RE r and the names of the choices C = {c 1 , . . . , c K }. In addition, models may use textual descriptions {s 1 , . . . , s K } to aid disambiguation. We define choice text s i (1 \u2264 i \u2264 K) as: (a) The entity name c i , or (b) the concatenation of c i and the textual description s i , separated by a delimiter. 9 We consider the following four experimental setups.\nNAME: The entity name without further description of the entities. We use this setting as a baseline.\nFor the remaining models, we add the following description to the name (truncated to 512 tokens):\nINFOBOX: The concatenation of all infobox key-value pairs (e.g., 'genre: pop').\nUNSHOWN BACKGROUND: The INFOBOX text, concatenated with all the Wikipedia sections of the entity, excluding the section shown to the annotators as background. Since annotators were shown a search link and not a specific Wikipedia section for the MUSIC domain, we do not remove any Wikipedia section for the MUSIC entities. We note that the UNSHOWN BACKGROUND might have some overlap with the information shown to crowdworkers, but the text is not directly given to them. Hence, it is a fair setup to evaluate models in a practical system where the models might not have all the background information.\nORACLE: The same background text that was shown to the annotators (Section 3.3). Note that this only exists for BOOKS and RECIPES, as for MUSIC, annotators were only shown a search link.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Models", "text": "We evaluated 5 different models. For each, we score match to each entity choices and select c * with the highest score value.\nUniversal Sentence Encoder: We calculate the cosine similarity between the universal sentence encoder (USE; Cer et al.2018) embeddings for the RE r and each choice's text s i .\nEntailment: Using a textual entailment classifier, we classify whether a choice's text s i entails the RE r. We use the confidence of the 'entailment' label as the score. We use a BERT model trained on the MNLI dataset (Williams et al., 2018) as our classifier. For all models based on BERT, we use BERT large uncased.\nBERT. We turn our task into binary classification: We make one example per choice (c i , r) with label 1 if r refers to c i ; otherwise, label 0. We finetune BERT with a binary classification layer (with two units) on top of its [CLS] token embeddings. The LM input is the sequence [CLS]s i [SEP]r. Dur-ing inference, for each choice c i , we compute the probability of label 1 as its score.\nBERT Joint. In contrast to the above binary setup, we encode all the K sequences [CLS]s i [SEP]r with BERT. We apply a linear layer (with one unit) on top of the [CLS] token embeddings from each sequence. We normalize the scores using softmax. Finally, we minimize a categorical cross entropy loss given the K scores. During inference, we directly use each choice's score.\nT5. We turn our task into binary classification, as with the BERT binary model. We fine-tune a T5 XL model (3B parameters) with input sequence \"expression: r entity: c i description: s i \" and output sequence 1 or 0. For the NAME input type, the input sequence omits the \"description\" part.", "publication_ref": ["b32"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We split the questions in the AltEntities corpus in each domain into training (70%), development (15%), and test (15%) sets. To avoid information leaking between the sets, we allow each target item to be in only one of the sets. For the USE and entailment models, we do not tune any hyperparameters. For supervised models, we tune the learning rate, batch size, and number of epochs using a grid search on the development data (96 configurations for BERT and 24 configurations for T5). We report the hyper-parameter details in Appendix D.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Reference Resolution Accuracy", "text": "We compute the accuracy of each (alternative question, RE) pair, i.e. whether the correct choice is scored highest. As K=2 in our experiments, a random baseline has accuracy 50%.\nWe show the test set results in Table 7 for all domains and input types. 10 For each model, we also show the average results of all input types. Among the models, USE performs worst (61.03%), followed by the entailment model (66.91%). BERT Joint (73.56%) is on average 1.61% better than BERT (71.52%), confirming that modeling the choices jointly is effective. T5 has the highest average results (77.43%), as expected given that we experimented with T5 XL with 3B parameters compared to BERT large with 360M.\nIn the ORACLE setting for BOOKS and RECIPES, accuracy is understandably high (up to 95.10% for BOOKS and 92.60% for RECIPES). We note that    these results are an over-estimate of the model capabilities. On the other hand, in the NAME setting, in most cases the results are slightly above 50%, with the best result being 61.97% for the MUSIC domain with the T5 model. Here the LMs rely on their memorized entity knowledge (Petroni et al., 2019), suggesting that BERT and T5 embeddings are not sufficient to resolve arbitrary entity references.\nWith the INFOBOX input, the T5 model accuracy is 78.30%, 83.33% and 74.28% for BOOKS, RECIPES, and MUSIC, respectively. It increases to 83.40%, 86.76%, and 82.27%, respectively, with the UNSHOWN BACKGROUND input where we add unstructured text data to the structured infobox data. This shows the text is helpful when resolving REs. In practical settings, models should work with relevant, but not necessary the same background knowledge as users because (1) it is not possible to have access to users' actual knowledge, and (2) models always have some limitation in the amount of text they can input. We thus rely on the UNSHOWN BACKGROUND setting as a realistic setting for measuring the capabilities of the different models.", "publication_ref": ["b19"], "figure_ref": [], "table_ref": ["tab_10"]}, {"heading": "Cross-Domain Experiments", "text": "Reference resolution is a semantic task, and ideally models would learn general task aspects rather than domain details. We test generalization by finetuning our models on one domain and testing on another. We used the UNSHOWN BACKGROUND setting for these experiments as the most realistic.\nTable 8 shows the T5 model results. 11 We do not observe much difference when models are tested out of domain, supporting the hypothesis that our models are indeed generalizable. This observation is rather important since our models could be used without separate training for new choice domains.\nWe also create a mixed training (and development) set that combines the data of the three domains. The mixed training set gives better results on average, taking advantage of larger training set and cues from all the domains. However, since the dataset in each domain is relatively large, the mixed training does not increase the results substantially.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_11"]}, {"heading": "Results and Entity Similarity", "text": "Section 3.4.1 explained how we selected entity pairs to have different levels of similarity. We now examine how this affects performance. Table 9 shows the results for the T5 model with the UN-SHOWN BACKGROUND input. We compute accuracy per test example subset, where each originated from a specific similarity sampling method.\nAs expected, when the two entities are randomly selected, disambiguation is easiest since they have little in common. The task becomes harder as entities become more similar, with entities with similar infobox attributes having the lowest performance.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_12"]}, {"heading": "Error Analysis", "text": "We analyzed the errors from the T5 model in the UNSHOWN BACKGROUND setting, to understand   10.\nIn most cases, there is no textual overlap between the RE and the background. This is because either the relevant text is removed (by design) since it is shown to the raters, or the Wikipedia text does not contain the information at all (e.g., music lyrics). Future research could evaluate how to adapt LMs to improve their entity knowledge to reason beyond the input textual evidence. In addition, retrieval augmented LMs could be applied to retrieve relevant information before performing the prediction (Borgeaud et al., 2022;Shi et al., 2023).\nIn other cases, the model suffers from poor reasoning, e.g., that clam is seafood, or a vegetarian dish does not contain seafood. In addition, the model often misclassifies examples when entity attributes are compared (e.g., the newer one). Multimodality covers around 25% of the errors in the RECIPES and MUSIC domains, e.g., annotators referenced visual aspects from music videos or recipes (e.g., looks like shells), or an acoustic aspect from a song (e.g., with the piano intro or more upbeat).\nThe remaining errors are because of wrong annotations, usually with the REs appling to both items. This wrong annotation rate (23%-30%) is much higher than the error rate in the whole dataset (less than 7% as discussed in Section 4) since the model has learned the task to a good extent.\nWe also analyzed correctly classified examples (for the MUSIC domain) to understand what types of REs are classified correctly. The results are shown in Appendix F.", "publication_ref": ["b3", "b27"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Conclusion", "text": "We have revisited RE resolution with a new focus on indirect expressions, introducing AltEntities, a new large dataset for this task -covering BOOKS, RECIPES, and MUSIC examples. The dataset was collected using a novel cartoon completion approach to encourage conversational and causal expressions while avoiding name or position expressions. The experimental results show that in a realistic setting, LMs adapted for this task achieve 82%-87% accuracy. While an improvement on existing approaches, this also encourages further research on this important problem. Moreover, we showed that the models' performance does not drop when trained and tested on different domains, suggesting that models can learn the semantic task well and generalize to new domains.\nIt is notable that in practice, many entities do not have textual descriptions or rich meta-data. Future research could study resolving REs with minimal information, e.g., when we only have access to their names or limited meta-data. Future research could also use multi-modal input for training and inference. Further, to handle more complex REs such as the newer one, or the happy song, one could decompose a RE into simpler expressions and then perform the comparison. Similar data collection methodologies could be applied to collect a dataset with more number of choices and also cases where neither or multiple choices match the RE.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "As with any natural language understanding task, there are practical limitations and related ethical aspects that must be considered before deploying a system. In particular, our corpus and modeling approach assume that the user-provided REs always refer to one of the two options. If this is not the case, or if the RE is particularly contrived, undesirable or unexpected behavior may occur: For any expression, including for instance one made with arbitrary derisive language, the model would attempt to resolve this to one of the alternative entities. One approach system designers may consider could be to pre-classify any user-provided REs to avoid interpreting those that are off topic or phrased in a negative manner.\nA second consideration is that of corpus representativeness. In our case, as this is a first corpus for this task, we have limited ourselves to English Wikipedia, native English speaking annotators, and particular item sampling strategies for practical reasons. However, if used for training a deployed system, the examples present may bias any model to understand specific types of references but not others. Similarly, the items in our corpus are sufficiently popular to have a relatively long Wikipedia entry, whereas items not present in Wikipedia, or with only minimal information, may exhibit different characteristics.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "The data collection protocol was reviewed by an ethics panel to remove potential ethical concerns. A few ethical concerns were mentioned by the panel which were then judged to be handled well. These included ensuring that the entities, texts and REs were free from biased and sensitive language. We address this by filtering using a list of sensitive words (see Section 3.4.1 and Table 12). The panel also recommended a diverse representation of entities and domains. Thus our data comes from diverse domains and the entities are sampled from a large set of Wikipedia articles.\nStill, we note that the limitations mentioned in Section 8 need to be considered and addressed carefully when using our dataset or models for evaluation or training of a deployed system. In addition, a biased corpus may lead to an evaluation that is unaware of RE language forms used in other cultures and languages, or that refer to other types of items. We expect this consideration to be important in practical settings.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "A Opening Utterances", "text": "The first annotation screen (Figure 1) starts with a manually written opening utterance. Table 11 shows all these utterances for the three domains..", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": ["tab_0"]}, {"heading": "B Annotation Guidelines", "text": "In this section, we provide the domain-specific guidelines that were shown to the annotators prior to the start of their annotation. The guidelines for each domain includes three instruction screens. The second and third instruction screens are then repeated for each alternative question as their first and second annotation screens, respectively (the two screen discussed in Section 4).\nIn the first instruction screen, a summary of the task based on a cartoon completion setup is shown to the annotators. Figure 4 shows the first instruction screen for the BOOKS domain. We do not show the first instruction screen for the other two domains as they are very similar to the BOOKS domain except that the text is slightly different to reflect the domain, and that the examples are from those domains.\nThe second instruction screen provides further information about the task and describes where the annotators should acquire the knowledge to perform the annotations. Figures 5, and 7, and 9 show the second instruction screens for the BOOKS, RECIPES, and MUSIC domains, respectively.\nThe third instruction screen shows which item should be referred to, and lists five examples of appropriate REs. The REs cover different aspects of the items to encourage the annotators to cover a variety of the item aspects. It also lists a number of actions that the annotators should or should not do. Figures 6, 8, and 10 show the third instruction screen for the BOOKS, RECIPES, and MUSIC domains, respectively.", "publication_ref": [], "figure_ref": ["fig_3", "fig_4"], "table_ref": []}, {"heading": "C Filtering Wikipedia Articles", "text": "Table 12 shows a number of filters we applied to narrow down the extracted articles.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "D Hyper-parameters Details and Computing Infrastructure", "text": "We tune the hyper-parameters using a grid search based on the accuracy of the indirect reference resolution task on the development set of each domain. For BERT and BERT multiple choice models, we select the base learning rate from {1e\u22124, 5e\u22125, 3e\u22125, 1e\u22125, 5e\u22126, 3e\u22126, 1e\u22126, 5e\u22127}, the training batch size from {16, 32, 64}, and the number of epochs from {1, 3, 5, 10}.\nFor T5, we select the base learning rate from {5e\u22127, 1e\u22127, 3e\u22126, 5e\u22126, 1e\u22125, 3e\u22125, 5e\u22125, 1e\u22124} and the training batch size from {16, 32, 64}.\nWe train the T5 models for 50K steps (batches).\nTable 13 shows the selected hyper-parameters for each model, domain, and input type.\nWe used Cloud TPU v2 accelerators for both training and inference. In our experiments, each training epoch took on average around 4 minutes for BERT, 6 minutes for BERT Multiple Choice, and 15 to 25 minutes for T5 models.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "E Development Set Results", "text": "We reported the test set results in multiple settings in Section 6. In this section, we report all those results on the development sets.\nTable 14 shows the development set results of different models for all domains and input types. We note that the general trends are very similar to that of the test sets. On average, the results of different models are slightly higher for the development set compared to the test set (up to 2.35%). This is expected as we have tuned the hyper-parameters on the development sets.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "F Analyzing Correctly Classified Examples", "text": "We analyzed 100 correctly classified examples in the MUSIC domain and assigned one or more categories (e.g., date or genre) to each example. We used the predictions of our T5 model with the UN-SHOWN BACKGROUND input. Table 15 shows the results which cover a wide range of categories.         Items should focus on a single topic. For example, we do not accept a movie that has a recorded song for the MUSIC domain. Items with a selected section length \u2264 250 characters 12 Items have enough information in the section selected to show as background knowledge to the annotators. Books or music items that do not have genres in their infobox Items contain important attributes for the domain     B2. Did you discuss the license or terms for use and / or distribution of any artifacts?", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "4", "text": "B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? 4 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Section 3.4.1: \"To remove any sensitive or offensive content, we also filter articles whose content matches a list of sensitive words.\" In addition, we did not ask the raters for any Personally Identifiable Information.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Sections 3 and 4.\nB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. Sections 4 and 6.\nC Did you run computational experiments?", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "6", "text": "C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? Section 6 and Appendix D.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Intervention effects in alternative questions", "journal": "The Journal of Comparative Germanic Linguistics", "year": "2006", "authors": "Sigrid Beck; Shin-Sook Kim"}, {"ref_id": "b1", "title": "Linguistic issues behind visual question answering", "journal": "Language and Linguistics Compass", "year": "2021", "authors": "Raffaella Bernardi; Sandro Pezzelle"}, {"ref_id": "b2", "title": "Responding to alternative and polar questions", "journal": "Linguistics and Philosophy", "year": "2012", "authors": "Mar\u00eda Biezma; Kyle Rawlins"}, {"ref_id": "b3", "title": "Improving language models by retrieving from trillions of tokens", "journal": "PMLR", "year": "2022", "authors": "Sebastian Borgeaud; Arthur Mensch; Jordan Hoffmann; Trevor Cai; Eliza Rutherford; Katie Millican; George Bm Van Den Driessche; Jean-Baptiste Lespiau; Bogdan Damoc; Aidan Clark"}, {"ref_id": "b4", "title": "Association for Computational Linguistics", "journal": "", "year": "2014", "authors": "Asli Celikyilmaz; Zhaleh Feizollahi; Dilek Hakkani-Tur; Ruhi Sarikaya"}, {"ref_id": "b5", "title": "Universal sentence encoder", "journal": "", "year": "2018", "authors": "Daniel Cer; Yinfei Yang; Sheng-Yi Kong; Nan Hua; Nicole Limtiaco; Rhomni St John; Noah Constant; Mario Guajardo-Cespedes; Steve Yuan; Chris Tar"}, {"ref_id": "b6", "title": "Quac: Question answering in context", "journal": "", "year": "2018", "authors": "Eunsol Choi; He He; Mohit Iyyer; Mark Yatskar; Wentau Yih; Yejin Choi; Percy Liang; Luke Zettlemoyer"}, {"ref_id": "b7", "title": "Computational interpretations of the gricean maxims in the generation of referring expressions", "journal": "Cognitive Science", "year": "1995", "authors": "Robert Dale; Ehud Reiter"}, {"ref_id": "b8", "title": "I'll be there for you\": The one with understanding indirect answers", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Cathrine Damgaard; Paulina Toborek; Trine Eriksen; Barbara Plank"}, {"ref_id": "b9", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b10", "title": "Predicting the resolution of referring expressions from user behavior", "journal": "", "year": "2013", "authors": "Nikos Engonopoulos; Martin Villalba; Ivan Titov; Alexander Koller"}, {"ref_id": "b11", "title": "MultiWOZ 2.1: A consolidated multi-domain dialogue dataset with state corrections and state tracking baselines", "journal": "", "year": "2020", "authors": "Mihail Eric; Rahul Goel; Shachi Paul; Abhishek Sethi; Sanchit Agarwal; Shuyang Gao; Adarsh Kumar; Anuj Goyal; Peter Ku; Dilek Hakkani-Tur"}, {"ref_id": "b12", "title": "Collaborative models for referring expression generation in situated dialogue", "journal": "", "year": "2014", "authors": "Rui Fang; Malcolm Doering; Joyce Chai"}, {"ref_id": "b13", "title": "Predicting pragmatic reasoning in language games", "journal": "Science", "year": "2012", "authors": "C Michael; Noah D Frank;  Goodman"}, {"ref_id": "b14", "title": "Learning content selection rules for generating object descriptions in dialogue", "journal": "Journal of Artificial Intelligence Research", "year": "2005", "authors": "W Pamela; Marilyn A Jordan;  Walker"}, {"ref_id": "b15", "title": "I'd rather just go to bed\": Understanding indirect answers", "journal": "", "year": "2020", "authors": "Annie Louis; Dan Roth; Filip Radlinski"}, {"ref_id": "b16", "title": "Situated language understanding at 25 miles per hour", "journal": "", "year": "2014", "authors": "Teruhisa Misu; Antoine Raux; Rakesh Gupta; Ian Lane"}, {"ref_id": "b17", "title": "Colors in Context: A Pragmatic Neural Model for Grounded Language Understanding", "journal": "Transactions of the Association for Computational Linguistics", "year": "2017", "authors": "Will Monroe; X D Robert; Noah D Hawkins; Christopher Goodman;  Potts"}, {"ref_id": "b18", "title": "Why discourse affects speakers' choice of referring expressions", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Naho Orita; Eliana Vornov; Naomi Feldman; Hal Daum\u00e9; Iii "}, {"ref_id": "b19", "title": "Language models as knowledge bases?", "journal": "", "year": "2019", "authors": "Fabio Petroni; Tim Rockt\u00e4schel; Sebastian Riedel; Patrick Lewis; Anton Bakhtin; Yuxiang Wu; Alexander Miller"}, {"ref_id": "b20", "title": "Changing the level of directness in dialogue using dialogue vector models and recurrent neural networks", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Louisa Pragst; Stefan Ultes"}, {"ref_id": "b21", "title": "The interpretation of prosody in disjunctive questions", "journal": "Linguistic inquiry", "year": "2013", "authors": "Kathryn Pruitt; Floris Roelofsen"}, {"ref_id": "b22", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "The Journal of Machine Learning Research", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b23", "title": "Know what you don't know: Unanswerable questions for squad", "journal": "Short Papers", "year": "2018", "authors": "Pranav Rajpurkar; Robin Jia; Percy Liang"}, {"ref_id": "b24", "title": "Squad: 100, 000+ questions for machine comprehension of text", "journal": "", "year": "2016", "authors": "Pranav Rajpurkar; Jian Zhang; Konstantin Lopyrev; Percy Liang"}, {"ref_id": "b25", "title": "Coqa: A conversational question answering challenge", "journal": "Transactions of the Association for Computational Linguistics", "year": "2019", "authors": "Siva Reddy; Danqi Chen; Christopher D Manning"}, {"ref_id": "b26", "title": "Large language models are not zero-shot communicators", "journal": "", "year": "2022", "authors": "Laura Ruis; Akbir Khan; Stella Biderman; Sara Hooker; Tim Rockt\u00e4schel; Edward Grefenstette"}, {"ref_id": "b27", "title": "Replug: Retrievalaugmented black-box language models", "journal": "", "year": "2023", "authors": "Weijia Shi; Sewon Min; Michihiro Yasunaga; Minjoon Seo; Rich James; Mike Lewis; Luke Zettlemoyer; Wen-Tau Yih"}, {"ref_id": "b28", "title": "Action state update approach to dialogue management", "journal": "IEEE", "year": "2021", "authors": "Svetlana Stoyanchev; Simon Keizer; Rama Doddipatla"}, {"ref_id": "b29", "title": "Direct: Direct and indirect responses in conversational text corpus", "journal": "", "year": "2021", "authors": "Junya Takayama; Tomoyuki Kajiwara; Yuki Arase"}, {"ref_id": "b30", "title": "Implicatures and nested beliefs in approximate decentralized-POMDPs", "journal": "Short Papers", "year": "2013", "authors": "Adam Vogel; Christopher Potts; Dan Jurafsky"}, {"ref_id": "b31", "title": "Coordinating beliefs in conversation", "journal": "", "year": "1992", "authors": "Deanna Wilkes; - Gibbs; H Herbert;  Clark"}, {"ref_id": "b32", "title": "A broad-coverage challenge corpus for sentence understanding through inference", "journal": "Long Papers", "year": "2018", "authors": "Adina Williams; Nikita Nangia; Samuel Bowman"}, {"ref_id": "b33", "title": "Mattnet: Modular attention network for referring expression comprehension", "journal": "", "year": "2018", "authors": "Licheng Yu; Zhe Lin; Xiaohui Shen; Jimei Yang; Xin Lu; Mohit Bansal; Tamara L Berg"}, {"ref_id": "b34", "title": "Grounding referring expressions in images by variational context", "journal": "", "year": "2018", "authors": "Hanwang Zhang; Yulei Niu; Shih-Fu Chang"}, {"ref_id": "b35", "title": "Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? Section 6 and Appendix D", "journal": "", "year": "", "authors": ""}, {"ref_id": "b36", "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? We did not observe meaningful differences when running the experiments multiple times in the preliminary experiments", "journal": "", "year": "", "authors": ""}, {"ref_id": "b37", "title": "for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc", "journal": "", "year": "", "authors": ""}, {"ref_id": "b38", "title": "crowdworkers) or research with human participants? Sections 3 and 4", "journal": "", "year": "", "authors": ""}, {"ref_id": "b39", "title": "Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators", "journal": "", "year": "", "authors": " D1"}, {"ref_id": "b40", "title": "This work was carried out by participants who are paid contractors. Those contractors received a standard contracted wage, which complies with living wage laws in their country of employment. Due to global privacy concerns, we cannot include more details about our participants", "journal": "", "year": "", "authors": "S ; U K Canada; Australia "}, {"ref_id": "b41", "title": "Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?", "journal": "", "year": "", "authors": " D3"}, {"ref_id": "b42", "title": "Was the data collection protocol approved (or determined exempt) by an ethics review", "journal": "", "year": "", "authors": " D4"}, {"ref_id": "b43", "title": "Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?", "journal": "", "year": "", "authors": " D5"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Annotators were shown a cartoon in which they were asked to complete the final step of a conversation.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Annotation screen for entering expressions.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Background descriptions for two recipes.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: The second instruction screen shown for the BOOKS domain. It provides further information about the task and describes where the annotators should acquire the knowledge to perform the annotations.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure 6: The third instruction screen shown for the BOOKS domain. It shows which item should be referred to, and lists five examples of appropriate REs. It also lists a number of actions that the annotators should or should not do.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 7 :7Figure 7: The second instruction screen shown for the RECIPES domain. It provides further information about the task and describes where the annotators should acquire the knowledge to perform the annotations.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 8 :8Figure 8: The third instruction screen shown for the RECIPES domain. It shows which item should be referred to, and lists five examples of appropriate REs. It also lists a number of actions that the annotators should or should not do.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 9 :9Figure 9: The second instruction screen shown for the MUSIC domain. It provides further information about the task and describes where the annotators should acquire the knowledge to perform the annotations.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 10 :10Figure 10: The third instruction screen shown for the MUSIC domain. It shows which item should be referred to, and lists five examples of appropriate REs. It also lists a number of actions that the annotators should or should not do.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Responses to the question which intend to choose Pandan cake over the alternative.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "shows the number of candidate entities.", "figure_data": "Main Plot Summary Preparation Ingredients TotalBOOKS RECIPES MUSIC 22,763 2,822 1,032 5,858 ---343 --147 -28,621 3,312 1,032"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Number of extracted candidate items for each domain and background section.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Number of sampled entity pairs (questions) for each domain and sampling method. Keep it casual and conversational.Varied, interesting, and creative expressions. Use alternative words, e.g., award instead of prize.", "figure_data": "Do"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "The AltEntities corpus statistics", "figure_data": "BOOKS The one that is set in the 1880s It's by a famous detective writer The fictional one not the one with the 12 year old boy It's the book that has rock and politics in itMUSIC The one without words It is the song sung by an Australian. It has synthesizer sounds in it Came out in mid of 2000. Based on life experienced in Sheffield."}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Random REs from crowd annotators.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "BOOKS RECIPES MUSIC ORAC NAME INBO UNBA ORAC NAME INBO UNBA NAME INBO UNBA AVG USE 67.25 54.35 56.65 60.40 69.28 55.73 63.75 65.00 57.83 61.05 60.08 61.03 Entailment 84.95 52.15 63.65 68.80 79.98 54.08 67.14 74.41 54.52 64.49 71.84 66.91 BERT 93.30 50.55. 74.35 79.80 87.87 53.32 77.84 81.01 53.93 61.60 73.13 71.52 BERT Joint 94.05 59.80 75.35 81.50 88.94 54.12 75.21 80.87 56.59 67.48 75.24 73.56 T5 95.10 55.65 78.30 83.40 92.60 61.97 83.33 86.76 58.11 74.28 82.27 77.43", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Indirect reference resolution results for different models on all domains and input types: ORACLE (ORAC), NAME, INFOBOX ( INBO), UNSHOWN BACKGROUND (UNBA). The best result of each column is boldfaced. When the difference between the best result and another result is not statistically significant (paired t-test with p-value < 0.05), the other result is made both bold and italic (only 4 cases).", "figure_data": "Training DomainBOOKS RECIPES MUSIC MIXEDTest Domain BOOKS RECIPES MUSIC 83.40 83.55 82.54 81.60 86.76 82.96 82.05 84.80 82.27 83.90 87.47 83.28"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "", "figure_data": ": T5 results for the UNSHOWN BACKGROUND setup, when trained on one domain and tested on an-other domain.Uniform Same Name Similar Title Similar Desc Similar Attrs AllBOOKS RECIPES MUSIC 90.30 92.54 88.58 85.02 --83.86 86.29 -74.70 82.24 80.39 -81.55 77.12 83.40 86.76 82.27"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Best Song Ever is a song recorded by English-Irish... These Days is a song by British pop group... It has to do something with dancing all night. Boerewors..., a type of sausage which originated in South Africa.", "figure_data": "Error TypeTarget ItemNon-Target ItemAnnotator UtteranceNo Textual Overlap47%(B) 27%(R) 42%(M) Poor reasoning 25%(B) 18%(R) 13%(M) Multi-modality 0%(B) 25%(R) 22%(M) Wrong Annotation 28%(B) 30%(R) 23%(M)Clams casino is a clam \"on the halfshell\" dish... Dark Age... release_date: July 30, 2019... It's Not Over is the debut single by American rock... Pandoro appeared in re-mote times, the product of... My Story (Gillard book) is a political memoir of Julia Gillard... Tight Connection to My Heart (by Bob Dylan)...White pudding is a meat dish popular in Ireland, North-ern Ireland... Buddha's delight ... is a veg-etarian dish... Iron Gold... release_date: January 16, 2018... Love Child is a 1968 song re-leased by the Motown... Pandebono... It is said that an Italian baker who lived... My Story (Das book) is an autobiographical book writ-ten by Indian author... Like a Rolling Stone (by Bob Dylan)...It can be stewed. The one with seafood in sauce. It is the most recent one. Has a marriage proposal in the music video Brownish-yellow in its colour. I mean the book that is technically an auto-biography. this song is by an Ameri-can singer."}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Error analysis results. Under each error type, we report the percentage of examples from the BOOKS (B), RECIPES (R), and MUSIC (M) domains. We also show two example for each error type.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Figure 4: The first instruction screen shown for the BOOKS domain. It summarizes the task based on a cartoon completion setup.", "figure_data": "BOOKS \"Remember that book we saw at the store?\" \"Hey, about that book I lent you last month...\" \"Can you get me that book on the first shelf?\" \"I really liked that book from the reading club...\" \"That book I got was super interesting!\"MUSIC \"So that song I keep singing...\" \"One of those cool songs that Bob sang last night...\" \"You sang that song really well yesterday...\" \"Could you play that song from your playlist?\" \"I'll now play my favorite song.\"RECIPES \"Remember that fabulous stuff from Tom's party?\" \"That recipe on today's Masterchef was too good!\" \"Going to make that dish from Mary's potluck.\" \"Our favorite food blogger had a cool episode this week!\" \"Does mom's cookbook have that recipe?\""}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "The manual utterances which are used to populate the first cell of the cartoon.", "figure_data": ""}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "List of filters applied to select candidate items from those extracted from Wikipedia articles. For each filter, we show the rationale behind it.", "figure_data": "BERT BERT Multiple Choice T5BOOKS ORAC NAME INBO UNBA ORAC NAME INBO UNBA NAME INBO UNBA RECIPES MUSIC 3e-5 1e-5 5e-6 1e-5 5e-6 5e-7 1e-5 3e-5 1e-5 3e-6 5e-6 16 16 32 16 16 16 32 64 64 64 32 epochs 5 lr bsz 10 3 3 3 1 3 1 1 3 3 lr 3e-5 5e-6 3e-5 3e-5 3e-5 1e-6 3e-5 3e-5 5e-6 1e-5 5e-6 bsz 64 32 32 64 64 32 64 64 64 32 32 epochs 3 3 1 1 1 1 1 1 1 1 3 lr 5e-6 3e-5 3e-6 3e-6 3e-6 3e-6 3e-6 3e-6 3e-6 3e-6 3e-6 bsz 64 32 64 64 32 32 16 64 64 64 32"}, {"figure_label": "13", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "Selected hyper-parameters for the supervised models for each domain and input type. We list selected values for base learning rate (lr), Training batch size (bsz), Num training epochs (epochs). RECIPES MUSIC ORAC NAME INBO UNBA ORAC NAME INBO UNBA NAME INBO UNBA AVG USE 66.06 55.15 59.12 58.41 70.77 52.48 64.98 66.36 57.53 60.71 60.57 61.10 Entailment 85.00 50.91 63.16 70.54 81.31 56.73 69.41 75.58 52.68 62.42 74.32 67.46 BERT 94.34 59.58 78.27 81.91 88.87 53.99 76.15 81.07 60.57 63.35 74.50 73.87 BERT Joint 95.00 61.85 77.31 82.47 89.58 56.60 76.86 81.21 59.79 68.07 76.17 74.99 T5 95.91 61.04 78.98 84.13 93.22 56.69 82.80 85.77 59.14 72.33 82.97 77.54", "figure_data": "BOOKS"}, {"figure_label": "14", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "Indirect reference resolution development set results for different models on all domains and input types: ORACLE (ORAC), NAME, INFOBOX ( INBO), UNSHOWN BACKGROUND (UNBA). The best result of each column is boldfaced.", "figure_data": "Category Date Content Singer or band Genre Further song info Comparison Negation Instrument or sound It is a midtempo R&B ballad Example 1 was released in 2012 Singer compared his new life and the old. Not the sad song Example 2 the song that's only a few years old The one by a male singer song is by an Irish rock band It is the song that is R&B. it's that baroque pop ballad track Was remixed in the late 80s The one sampled from Shirly Bassey The newer one Released later Not the song about greed No not the one with Rap not the one with the piano intro Album One from their second album The one from the album WordshakerPercentage 25% 24% 19% 13% 10% 10% 10% 7% 5%"}, {"figure_label": "15", "figure_type": "table", "figure_id": "tab_21", "figure_caption": "Categories of correctly classified REs in the MUSIC domain. The results are based on the T5 model with the UNSHOWN BACKGROUND input. 12333 ACL 2023 Responsible NLP Checklist A For every submission: A1. Did you describe the limitations of your work? 8 A2. Did you discuss any potential risks of your work? Section 9, as part of the Ethics Statement. A3. Do the abstract and introduction summarize the paper's main claims? Abstract and Section 1.A4. Have you used AI writing assistants when working on this paper?", "figure_data": "Left blank.B Did you use or create scientific artifacts?3, 4, 5, and 6B1. Did you cite the creators of artifacts you used?3, 4, 5, and 6"}], "formulas": [], "doi": "10.3115/v1/D14-1223"}