{"title": "Deformable Template As Active Basis", "authors": "Ying Nian Wu; Zhangzhang Si; Chuck Fleming; Song-Chun Zhu", "pub_date": "", "abstract": "This article proposes an active basis model and a shared pursuit algorithm for learning deformable templates from image patches of various object categories. In our generative model, a deformable template is in the form of an active basis, which consists of a small number of Gabor wavelet elements at different locations and orientations. These elements are allowed to slightly perturb their locations and orientations before they are linearly combined to generate each individual training or testing example. The active basis model can be learned from training image patches by the shared pursuit algorithm. The algorithm selects the elements of the active basis sequentially from a dictionary of Gabor wavelets. When an element is selected at each step, the element is shared by all the training examples, in the sense that a perturbed version of this element is added to improve the encoding of each example. Our model and algorithm are developed within a probabilistic framework that naturally embraces wavelet sparse coding and random field.", "sections": [{"heading": "Introduction", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model, algorithm and theory", "text": "The concept of deformable templates [10] is an important element in object recognition. In this article, we present a generative model and a model-based algorithm for learning deformable templates from image patches of various object categories. The machinery we adopt is the wavelet sparse coding model [7] and the matching pursuit algorithm [5]. Our method is a very simple modification of this machinery, with the aim of coding specific ensembles of image patches of various object categories.\nWe call our model the active basis model, which represents a deformable template in the form of an active basis. An active basis consists of a small number of Gabor wavelet elements at different locations and orientations, and these elements are allowed to slightly perturb their locations and orientations before they are linearly combined to generate Figure (1) illustrates the basic idea. It displays 7 image patches of cars at the same scale and in the same pose. These image patches are defined on a common image lattice, which is the bounding box of the cars. These image patches are represented by an active basis consisting of 60 Gabor wavelet elements at different locations and orientation, as displayed in the first plot of figure (1). Each wavelet element is represented symbolically by a bar at the same location and with the same length and orientation. The length of each element is about 1/10 of the length of the image patch. These elements are automatically selected from a dictionary of Gabor wavelet elements at a dense collection of locations and orientations. The selected elements do not have much overlap and are well connected. They form a template of the training image patches.\nThe 60 elements of the active basis in the first plot are allowed to locally perturb their locations and orientations when they are linearly combined to encode each training or testing example, as illustrated by the remaining 7 pairs of plots of figure (1). For each pair, the left plot displays the observed car image, and the right plot displays the 60 Gabor wavelet elements that are actually used for encoding the corresponding observed image. These 60 elements are perturbed versions of the 60 elements of the active basis displayed in the first plot, so these elements form a deformed template. The deformation of the template is encoded by the local perturbations of the elements of the active basis.\nThe active basis can be learned from training image patches by a shared pursuit algorithm. The algorithm selects the elements of the active basis sequentially from the dictionary of Gabor wavelets. When an element is selected at each step, the element is shared by all the training examples in the sense that a perturbed version of this element is added to improve the encoding of each example. It is worth noting that for the last two examples in figure (1), the strong edges in the background are not encoded, because these edges are not shared by other examples. Therefore they are ignored by the shared pursuit algorithm.\nOur model and algorithm are developed within a theoretical framework that naturally embraces sparse coding and random fields. Specifically, we rewrite the sparse coding model so that the probability distribution of the image intensities can be rigorously defined in terms of tilting a stationary random field by a probability ratio term involving the sparse coding variables.", "publication_ref": ["b9", "b6", "b4", "b0", "b0", "b0", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Contributions and past work", "text": "The contributions of this paper are: (1) An active basis model for representing deformable templates. (2) A shared pursuit algorithm for learning deformable templates. (3) A theoretical framework that integrates sparse coding and random fields.\nTo credit past work, the active basis model is inspired by the biologically motivated schemes of Riesenhuber and Poggio [8] and Mutch and Lowe [6]. The difference is that we keep track of the deformation of the active basis and maintain the linear additive representation. The shared pursuit algorithm is inspired by the adaboost method of Viola and Jones [9]. The difference is that we work within the framework of generative model. The name \"active basis\" is clearly derived from \"active contours\" [4] and \"active appearance model.\" [1] The difference is that our method does not involve control points. Or more precisely, the elements of the active basis play the double role of both control points and linear basis vectors. Lastly, our work is a revision of the texton model [11].", "publication_ref": ["b7", "b5", "b8", "b3", "b0", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Active basis representation 2.1. A dictionary of Gabor wavelets", "text": "A Gabor function is of the form: G(x, y) \u221d exp{\u2212[(x/\u03c3 x ) 2 + (y/\u03c3 y ) 2 ]/2}e ix . We can translate, rotate, and dilate G(x, y) to obtain a general form of Gabor wavelets:\nB x,y,s,\u03b1 (x , y ) = G(x/s,\u1ef9/s)/s 2 , wherex = (x \u2212 x) cos \u03b1 \u2212 (y \u2212 y) sin \u03b1,\u1ef9 = (x \u2212 x) sin \u03b1 + (y \u2212 y)\ncos \u03b1. s is the scale parameter, and \u03b1 is the orientation. The central frequency of B x,y,s,\u03b1 is \u03c9 = 1/s.\nWe normalize the Gabor sine and cosine wavelets to have zero mean and unit l 2 norm. For an image I, the projection coefficient of I onto B x,y,s,\u03b1 or the filter response is I, B x,y,s,\u03b1 = x ,y I(x , y )B x,y,s,\u03b1 (x , y ).\nLet {(I m (x, y), (x, y) \u2208 D), m = 1, ..., M } be a sample of training image patches defined on a domain D of rectangular lattice, and D is the bounding box of the objects of the same category and in the same pose. Our method is scale specific. We fix s so that the length of B x,y,s,\u03b1 (e.g., 17 pixels) is about 1/10 of the length of D.\nThe dictionary of Gabor wavelet elements is \u2126 = {B x,y,s,\u03b1 , \u2200(x, y, s, \u03b1)}, where (x, y, s, \u03b1) are densely discretized: (x, y) \u2208 D with a fine sub-sampling rate (e.g., every 2 pixels), and \u03b1 \u2208 {k\u03c0/K, k = 0, ..., K \u2212 1} (e.g., K = 15).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Active basis", "text": "The backbone of the active basis model is\nI m = n i=1 c m,i B m,i + m , (1\n)\nB m,i \u2248 B i , i = 1, ..., n. (2\n)\nwhere B i \u2208 \u2126, B m,i \u2208 \u2126, and (c m,i , i = 1, ..., n) are coefficients. To define B m,i \u2248 B i , suppose\nB i = B xi,yi,s,\u03b1i , (3) B m,i = B xm,i,ym,i,s,\u03b1m,i ,(4)\nthen B m,i \u2248 B i if and only if there exists (d m,i , \u03b4 m,i ) such that\nx m,i = x i + d m,i sin \u03b1 i ,(5)\ny m,i = y i + d m,i cos \u03b1 i , (6) \u03b1 m,i = \u03b1 i + \u03b4 m,i , (7) d m,i \u2208 [\u2212b 1 , b 1 ], \u03b4 m,i \u2208 [\u2212b 2 , b 2 ].(8)\nThat is, we allow B i to shift its location along its normal direction, and we also allow B i to shift its orientation. b 1 and b 2 are the bounds for the allowed displacement in location and turn in orientation (e.g., b 1 = 6 pixels, and b 2 = \u03c0/15). In the above notation, the deformable template is the active basis B = (B i , i = 1, ..., n). The deformed template or the activated basis is B m = (B m,i , i = 1, ..., n) \u2248 B. See figure (1) for illustration. (1) For each putative candidate B i \u2208 \u2126, do the following:", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Shared matching pursuit for least squares", "text": "For m = 1, ..., M , choose the optimal B m,i that max-\nimizes | m , B m,i | 2 among all possible B m,i \u2248 B i .\nThen choose that particular candidate B i with the maximum corresponding\nm | m , B m,i | 2 .\n(2\n) For m = 1, ..., M , let c m,i \u2190 m , B m,i , and let m \u2190 m \u2212 c m,i B m,i . (3) Stop if i = n.\nOtherwise let i \u2190 i + 1, and go to (1).\nIn this article, we choose to adopt the more general probabilistic formulation, where the least squares criterion is a special case of the log-likelihood.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Probabilistic formulation", "text": "With the active basis representation ( 1) and ( 2) as the backbone, we can put probability distributions on the variables in the representation in order to construct a generative model. With such a model, learning can be based on likelihood.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Rewriting sparse coding model", "text": "Given template B = (B i , i = 1, ..., n), we assume that (d m,i , \u03b4 m,i ) \u223c uniform(\u2206 = [\u2212b 1 , b 1 ] \u00d7 [\u2212b 2 , b 2 ]) in or- der to generate the deformed template B m = (B m,i , i = 1, ..., n) according to (3)-(8).\nGiven deformed template B m = (B m,i , i = 1, ..., n), we need to specify the distribution of the foreground coefficients c m = (c m,i , i = 1, ..., n), and the distribution of the background residual m , in order to generate I m according to (1). The commonly assumed model is\n(c m,1 , ..., c m,n ) \u223c g(c m,1 , ..., c m,n ),(9)\nm (x, y) \u223c N(0, \u03c3 2 ) independently,(10)\n(c m,1 , ..., c m,n ) is independent of m . (11\n)\nThere are two problems with the above specification. (1) A white noise model does not capture the texture properties of the background. (2) The foreground distribution g cannot be estimated in closed form because we must deconvolve the additive noise m . The following observation helps solve these two problems.\nTheorem 1 For the representation (1), given B m = (B m,i , i = 1, ..., n), under the assumptions ( 9), ( 10) and (11), the distribution of I m is\np(I m | B m ) = q(I m ) p(r m,1 , ..., r m,n ) q(r m,1 , ..., r m,n ) , (12\n)\nwhere r m,i = I m , B m,i , i = 1, ..., n. q(I m ) is the density of white noise model, i.e., I m (x, y) \u223c N(0, \u03c3 2 ) independently. q(r m,1 , ..., r m,n ) is the density of (r m,1 , ..., r m,n ) under q(I m ). p(r m,1 , ..., r m,n ) is the den- sity of (r m,1 , ..., r m,n ) under p(I m | B m ).\nThe proof is given in the appendix. The basic idea of the proof is very simple. By adding i c m,i B m,i to the white noise background m , we only change the dimensions of m within the subspace spanned by (B m,i , i = 1, ..., n), without disturbing the rest of the dimensions. This can be accomplished by multiplying q(I m ) by the probability ratio p(r m,1 , ..., r m,n )/q(r m,1 , ..., r m,n ), which changes the distribution of (r m,1 , ..., r m,n ) from q(r m,1 , ..., r m,n ) to p(r m,1 , ..., r m,n ), without changing the distribution of the remaining dimensions.\nWe may use the compact matrix notation. Let I m be the As M \u2192 \u221e, the log-likelihood per observation\n|D| \u00d7 1 vector,\n1 M M m=1 log p(r m ) q(r m ) \u2192 KL(p(r m )|q(r m )),(14)\nwhich is the Kullback-Leibler divergence from p(r m ) to q(r m ).\nEquivalence to least squares. Random field tilting. Equation ( 12) is actually more general than is defined in Theorem 1: (1) The background q(I m ) can be any random field. (2) The sparse coding variables (r m,i , i = 1, ..., n) can be any deterministic transformations of I m . In this more general context, ( 12) is a random field tilting scheme, which consists of ( 1 , r m,n ). The remaining |D|\u2212n dimensions are implicit. This is a generalized version of projection pursuit [3]. The following are some perspectives to view this scheme:\nUnder white noise q(I m ), r m = B m I m \u223c N(0, B m B m \u03c3 2 ). If we as- sume p(r m ) is such that r m \u223c N(0, B m B m \u03c3 2 0 ) with \u03c3 2 0 > \u03c3 2 , then log[p(r m )/q(r m )] is positively linear in r m (B m B m ) \u22121 r m = I m B m (B m B m ) \u22121 B m I m ,\n(1) Hypothesis testing. q(I m ) can be considered the null hypothesis. p(r m,1 , ..., r m,n ) can be considered the test statistics to reject q(I m ). The above scheme modifies the null hypothesis to an alternative hypothesis.\n(2) Classification. q(I m ) can be considered the ensemble of negative examples. p(I m ) is the ensemble of positive examples. The sparse coding variables (r m,i , i = 1, ..., n) are the features that distinguish the two ensembles.\n(3) Coding. Instead of coding (r m,i , i = 1, ..., n) by q, we code them by p. The gain in coding length is the KLdivergence (14).", "publication_ref": ["b0", "b0", "b10", "b1", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Model specification", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sparse coding variables. Given", "text": "B m = {B m,i , i = 1, ..., n}, with B m B m \u2248 1, we choose to use r m,i = h m (| I m , B m,i | 2 ), i = 1, ..., n, as sparse coding vari- ables. | I m , B m,i | 2 is\nthe local energy, which is the sum of squares of the responses from the pair of Gabor cosine and sine wavelets. We ignore the local phase information, which is unimportant for shapes. h m () is a monotone normalization transformation that is independent of object categories.\nTo specify the model, we need to (1) specify the background q(I m ) and derive h m () and q(r m,1 , ..., r m,n ). (2) specify the foreground p(r m,1 , ..., r m,n ). We can pool these images to estimate p(r m,1 , ..., r m,n ), as illustrated by the vertical arrows at specific locations. p(r m,1 , ..., r m,n ) is to be contrasted against the background q(r m,1 , ..., r m,n ), which is not location specific, as illus-  \nI m , B m,i | 2 \u223c \u03c3 2 m \u03c7 2 2 \u223c 2\u03c3 2 m exp(1), i.e.,", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "the exponential distribution, and", "text": "| I m , B m,i | 2 /2\u03c3 2 m \u223c exp(1). If B m B m = 1, then | I m , B i | 2 /2\u03c3\n| I m , B i | 2 /\u03c3 2 m,s are independent for i = 1, ..., n if B m B m = 1. \u03c3 2 s,m can be estimated b\u0177 \u03c3 2 m,s = 1 |D|K x,y\u2208D \u03b1 | I m , B x,y,s,\u03b1 | 2 , (15\n)\nwhere K is the total number of orientations. The tail of the distribution is Pr m,s > r) be the tail of this marginal distribution, then F (r) exp(\u2212r) for large r, because there are strong edges in this ensemble. The transformation that equates the tails\n(| I m , B i | 2 /\u03c3 2 m,s > r) = exp(\u2212r), which is short. (3\nF (r) = exp(\u2212r 0 ) is r 0 = \u2212 log F (r), so \u2212 log F (| I m , B m,i | 2 /\u03c3 2 m,s ) \u223c exp(1)\n. \u2212 log F is a non-linear whitening transformation. Therefore, we have\nr m,i = h m (| I m , B m,i | 2 ) = \u2212 log F (| I m , B m,i | 2 /\u03c3 2 m,s ).\nWe assume that the generic ensemble inherits from Gaussian process the property that (r m,i , i = 1, ..., n) are independent under B m B m = 1. So q(r m,1 , ..., r m,n ) = exp{\u2212 n i=1 r m,i }, i.e., r m,i \u223c exp(1) independently for i = 1, ..., n.\nOne can learn F (r) by the tail proportions in the marginal histogram of natural images. In our current implementation, we use a crude but simple approximation. Because \u2212 log F (r) r for large r, we assume a saturation threshold \u03be > 0, and approximate \u2212 log F (r) \u2248 min(r, \u03be) (e.g., \u03be = 16).\nForeground model p(r m,1 , ..., r m,n ). We assume the simplest model for p(r m,1 , ..., r m,n ): r m,i \u223c exp(\u03bb i ) independently for i = 1, ..., n, with \u03bb i < 1. The density of r m,i is p(r) = \u03bb i exp(\u2212\u03bb i r). This is the maximum entropy model under the constraint E p (r \nm,i ) = 1/\u03bb i . Log-likelihood is log[p(I m | B m )/q(I m )] = n i=1 log p(r m,i ) q(r m,i ) = n i=1 [(1 \u2212 \u03bb i )r m,i + log \u03bb i ].(16\nB = (B i , i = 1, ..., n) by maximizingr i subject t\u014d r i \u2212 1 \u2212 logr i > log |\u2206|, (17\n)\nand the elements in B m = (B m,i , i = 1, ..., n) are approximately non-overlapping.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Shared pursuit for maximum likelihood", "text": "We use the notation \u2202B m,i to denote all the B \u2208 \u2126, such that B, B m,i > \u03b6, i.e., those elements that overlap with B m,i . (3) Stop if i = n. Otherwise let i \u2190 i + 1, and go to (1).\nThe stopping criterion can also be based on (17).\nFind and sketch. We can used the learned model, in particular, B = (B i , i = 1, ..., n), and \u03bb = (\u03bb i , i = 1, ..., n), to find the object in a new testing image I m , m / \u2208 {1, ..., M }. Suppose I m is defined on domain D m , which can be much larger than the bounding box D. We\nslide D over D m . Let D x,y \u2282 D m be the bounding box centered at (x, y) \u2208 D m . Within each D x,y , for i = 1, ..., n, choose the optimal B m,i \u2248 B i that maximizes r m,i = [I m , B m,i ]. Then compute the log-likelihood score l m (x, y) = n i=1 [(1 \u2212 \u03bb i )r m,i + log \u03bb i ]. Choose (x, y)\nwith maximum log-likelihood score l m (x, y). The corresponding B m = (B m,i , i = 1, ..., n) is the sketch of the object. If the size of the object in the testing I m is different than the size of objects in the training images, we can scale I m to obtain a sequence of zoomed versions of I m . Then we can choose the optimal scale based on the maximum log-likelihood scores obtained over multiple scales.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Active mean vector and active correlation", "text": "The\ndeformable template B = (B i , i = 1, ..., n) in the above section is parametrized by \u03bb = (\u03bb i , i = 1, ..., n). The log-likelihood score is n i=1 [(1 \u2212 \u03bb i )r m,i + log \u03bb i ],\nwhich is non-linear in \u03bb. This motivates us to introduce a simpler linear score without explicit probabilistic assumptions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Linear scoring", "text": "We parametrizes the deformable template B = (B i , i = 1, ..., n) by \u03b8 = (\u03b8 i , i = 1, ..., n), where \u03b8 is a unit vector with \u03b8 2 = 1. We replace the log-likelihood score ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Shared pursuit for maximum correlation", "text": "(0) The same as maximum likelihood. (2) The same as maximum likelihood.\n(3) If i = n, normalize \u03b8 so that \u03b8 2 = 1, then stop. Otherwise let i \u2190 i + 1, and go to (1).\nWe can also use the active correlation score for find-andsketch.  One issue that concerns us is normalization. In this experiment, we normalize within the whole image instead of normalizing within the sliding bounding box. We also tried the latter normalization scheme. Active correlation still selects the correct scale. However, for log-likelihood, the correct scale is near a local maximum instead of the global maximum. Another issue revealed by more experiments is that the maximum likelihood position is not always the correct position. We shall investigate these issues in future work.   tively. We also built an adaboost classifier [9]  We then test on a separate data set with 88 positives and 474 negatives. Figure (7) displays the three ROC curves for active basis models learned by log-likelihood and active correlation, and the adaboost. The AUC (area under curve) for adaboost is .936. The AUC for log-likelihood scoring is .941. The AUC for active correlation scoring is .971. We did not implement cascade for adaboost. This example shows that our method is comparable to adaboost.   ", "publication_ref": ["b0", "b8", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgement", "text": "We thank the area chair and the three reviewers for their criticisms that help improve the presentation of the paper. We are grateful to Zhuowen Tu for helpful discussions. The work is supported by NSF-DMS 0707055, NSF-IIS 0713652, and ONR N00014-05-01-0543.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "the first image. We then fit the model on this single image, and use it to find the cats in the other images. Then we re-learn the model, and re-find the cats using the re-learned model. Figure (9) shows the results after 3 iterations, where the first plot is B = (B i , i = 1, ..., n), n = 40.\nReproducibility: Data and source codes can be downloaded from the webpage listed on the title page.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Discussion", "text": "Residual image and inhibitive features. After activating basis (B m,i , i = 1, ..., n) and modeling (r m,i , i = 1, ..., n), we can also pool the texture statistics on the residual image that is not covered by (B m,i , i = 1, ..., n), and tilt q(I m ) on residual image. Along the same theme, we can also introduce inhibitive features on the residual image.\nCenter versus boundary. Even though one may view model ( 12) from a classification perspective, the model is trained by maximizing likelihood, which targets the center of the data, instead of the classification boundary. The advantage of targeting the center is that it is more efficient for small training samples, and more convenient for unsupervised learning.\nMaximum entropy or minimum divergence. Model ( 12) is a special computable case of maximum entropy or minimum divergence principle [2], which tilts q(I m ) to p(I m ) = exp{ \u03bb, H(I m )}q(I m )/Z(\u03bb), for some statistics H(), where Z(\u03bb) is normalizing constant. If H is the histogram of (r m,i , i = 1, ..., n), we get model (12) for shapes. If H consists of spatially pooled histograms, we get the Markov random field model [12] for textures. ", "publication_ref": ["b1", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Appendix", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Active appearance models", "journal": "PAMI", "year": "2001", "authors": "G J Tf Cootes; C J Edwards;  Taylor"}, {"ref_id": "b1", "title": "Inducing features of random fields", "journal": "IEEE PAMI", "year": "1997", "authors": "Della S Della Pietra; J Pietra;  Lafferty"}, {"ref_id": "b2", "title": "Exploratory projection pursuit", "journal": "Journal of the American Statistical Association", "year": "1987", "authors": " Jh Friedman"}, {"ref_id": "b3", "title": "Snakes: Active contour models", "journal": "", "year": "1987", "authors": "M Kass; D Witkin;  Terzopoulos"}, {"ref_id": "b4", "title": "Matching pursuit in a time-frequency dictionary", "journal": "IEEE Signal Processing", "year": "1993", "authors": "S Mallat;  Zhang"}, {"ref_id": "b5", "title": "Multiclass object recognition with sparse, localized features", "journal": "", "year": "2006", "authors": "J Mutch;  Lowe"}, {"ref_id": "b6", "title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "journal": "Nature", "year": "1996", "authors": " Ba Olshausen;  Field"}, {"ref_id": "b7", "title": "Hierarchical models of object recognition in cortex", "journal": "Nature Neuroscience", "year": "1999", "authors": "M Riesenhuber;  Poggio"}, {"ref_id": "b8", "title": "Robust real-time face detection", "journal": "IJCV", "year": "2004", "authors": "P A Viola;  Jones"}, {"ref_id": "b9", "title": "Feature extraction from faces using deformable templates", "journal": "IJCV", "year": "1992", "authors": " Al Yuille; D S Pw Hallinan;  Cohen"}, {"ref_id": "b10", "title": "What are textons", "journal": "IJCV", "year": "2005", "authors": " Sc Zhu;  Guo; Z J Wang;  Xu"}, {"ref_id": "b11", "title": "Minimax entropy principle and its applications in texture modeling", "journal": "Neural Computation", "year": "1997", "authors": " Sc Zhu; D Wu;  Mumford"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. Active basis formed by 60 Gabor wavelet elements. The first plot displays the 60 elements, where each element is represented by a bar. For each of the other 7 pairs, the left plot is the observed image, and the right plot displays the 60 Gabor wavelet elements resulting from locally shifting the 60 elements in the first plot to fit the corresponding observed image.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Given the examples {I m , m = 1, ..., M }, we can learn the template B and its deformed versions {B m \u2248 B, m = 1, ..., M }. We may use the least squares criterion M m=1 I m \u2212 n i=1 c m,i B m,i 2 to drive the following shared matching pursuit algorithm. (0) For m = 1, ..., M , let m \u2190 I m . Let i \u2190 1.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "where |D| is the number of pixels in domain D. Let B m = (B m,1 , ..., B m,n ) be the |D| \u00d7 n matrix, where each column is a vectorized version of B m,i . Let r m = (r m,1 , ..., r m,n ) be the n \u00d7 1 vector of sparse coding variables. Then r m = B m I m . The foreground p(r m ) can be estimated directly by pooling the sample {r m = B m I m , m = 1, ..., M }, which are responses of Gabor wavelets at fixed locations (subject to local perturbations B m \u2248 B), so we do not need to estimate g, which involves unnecessary deconvolution of the additive noise m . Under the white noise model q(I m ) where I m (x, y) \u223c N(0, \u03c3 2 ) independently, we have r m \u223c N(0, B m B m \u03c3 2 ), so q(r m ) is in closed form. Log-likelihood and KL-divergence. We can estimate the template B and its deformed versions {B m \u2248 B, m = 1, ..., M } by maximizing the log-likelihood M m=1 log[p(I m | B m )/q(I m )] = M m=1 log p(r m ) q(r m ) . (13)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "which is the squared norm of the projection of I m onto the subspace spanned by B m , which equals to I m 2 \u2212 min cm I m \u2212 B m c m 2 , where c m = (c m,1 , ..., c m,n ) . So maximizing the log-likelihood (13) with such p(r m ) is equivalent to the least squares criterion. Orthogonality. The norm I m B m (B m B m ) \u22121 B m I m naturally favors the selection of orthogonal B m .In this article, we enforce that B m B m \u2248 1, m = 1, ..., M, for simplicity, where \"1\" denotes the identity matrix. That is, we enforce that the elements in the deformed template B m = (B m,i , i = 1, ..., n) are approximately orthogonal to each other, or do not have much overlap. The precise definition of B m B m \u2248 1 is: B m,i , B m,j < \u03b6 for i = j, where \u03b6 is a small threshold (e.g., \u03b6 = .1).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": ") Replacing background q(r m,1 , ..., r m,n ) by foreground p(r m,1 , ..., r m,n ). (2) Retaining the conditional distribution of the remaining |D| \u2212 n dimensions of I m given (r m,1 , ...", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure ( 22) illustrates the idea. The shaded rectangles are training images.", "figure_data": ""}, {"figure_label": "21", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 2 . 1 )21Figure 2. p(rm,1, ..., rm,n) is pooled over training images (shaded rectangles) at specific locations. q(rm,1, ..., rm,n) is derived from stationary background q(Im).", "figure_data": ""}, {"figure_label": "22", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "2 m 2 )22are independent for i = 1, ..., n. (Stationary isotropic Gaussian q(I m ). Let s be the common scale of B m = (B m,i , i = 1, ..., n). B m,i can sense I m only within a limited frequency band around 1/s. Let \u03c3 2 m,s = E[| I m , B x,y,s,\u03b1 | 2 ], and assume that the spectrum of the Gaussian process is locally flat within the abovementioned frequency band, then as far as B m can sense, q(I m ) is no different than white noise N(0, \u03c3 2 m,s /2). Therefore, | I m , B i | 2 /\u03c3 2 m,s \u223c exp(1), and this is a whitening transformation.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": ")Generic ensemble of natural image patches. If we pool the marginal distribution of | I m , B m,i | 2 /\u03c3 2 m,s over the ensemble of natural image patches, and let F (r) = Pr(| I m , B m,i | 2 /\u03c3 2", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": ") Given B, the prior distribution of B m is uniform: p(B m | B) = 1/|\u2206| n , where \u2206 = [\u2212b 1 , b 1 ] \u00d7 [\u2212b 2 , b 2 ] is the allowed range of shifting in location and orientation for each B i , and |\u2206| is the size of \u2206. So the posterior distribution p(B m | I m , B) \u221d p(I m | B m )/q(I m ). Thus, B m,i can be estimated by maximizing r m,i or | I m , B m,i | 2 among all possible B m,i \u2248 B i , subject to that (B m,i , i = 1, ..., n) are approximately non-overlapping. Given {B m , m = 1, ..., M }, \u03bb i can be estimated by pooling {r m = B m I m , m = 1, ..., M }. The maximum likelihood estimate is\u03bb i = 1/r i , wherer i = M m=1 r m,i /M is the average response. Replacing \u03bb i b\u0177 \u03bb i , the log-likelihood or coding gain per image 1 M M m=1 log[p(I m , B m | B)/q(I m )] = n i=1 (r i \u2212 1 \u2212 logr i ) \u2212 n log |\u2206|, where p(I m , B m | B) = p(I m | B m )p(B m | B). log |\u2206| is the cost for coding the shifting from B i to B m,i . We can sequentially introduce the elements of", "figure_data": ""}, {"figure_label": "012", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "( 0 )( 1 )( 2 )012For m = 1, ..., M , and for each B \u2208 \u2126, compute [I m , B] = \u2212 log F (| I m , B | 2 /\u03c3 2 m,s ) with \u03c3 2 m,s estimated by (15). Set i \u2190 1. For each putative candidate B i \u2208 \u2126, do the following: For m = 1, ..., M , choose the optimal B m,i that maximizes [I m , B m,i ] among all possible B m,i \u2248 B i . Then choose that particular candidate B i with the maximum corresponding m [I m , B m,i ]. Set \u03bb i = M/ m [I m , B m,i ]. For m = 1, ..., M , for each B \u2208 \u2202B m,i , set [I m , B] \u2190 0, to enforce approximate non-overlapping constraint.", "figure_data": ""}, {"figure_label": "32", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 3 . 2 mm32Figure 3. The first plot is B = {Bi, i = 1, ..., n}, n = 48, where each Bi is represented by a bar. For the rest M = 37 examples, the left is Im, and the right is Bm = (Bm,i, i = 1, ..., n). The M examples are listed in the descending order of log-likelihood.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "( 1 )1For each putative candidate B i \u2208 \u2126, do the following: For m = 1, ..., M , choose the optimal B m,i that maximizes [I m , B m,i ] among all possible B m,i \u2248 B i . Then choose that particular candidate B i with the maximum corresponding m [I m , B m,i ] 1/2 . Set \u03b8 i = m [I m , B m,i ] 1/2 /M .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Parameter values. Size of Gabor wavelets = 17 \u00d7 17. (x, y) is sub-sampled every 2 pixels. The orientation \u03b1 takes K = 15 equally spaced angles in [0, \u03c0]. The saturation threshold in approximation \u2212 log F (r) \u2248 min(r, \u03be) is \u03be = 16. The shift along the normal direction d m,i \u2208 [\u2212b 1 , b 1 ] = {\u22126, \u22124, \u22122, 0, 2, 4, 6} pixels. The shift of orientation \u03b4 m,i \u2208 [\u2212b 2 , b 2 ] = {\u22121, 0, 1} angles out of K = 15 angles. So |\u2206| = 21. The orthogonality tolerance is \u03b6 = .1. Experiment 1: Learning active basis. We apply the shared pursuit algorithm to a training set of M = 37 car images. The car images are 82 \u00d7 164.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Figure ( 33) displays the results from the algorithm. The algorithm returns n = 48 elements using the stopping criterion (17). The first plot displays the learned active basis B = {B i , i = 1, ..., n} where each B i is represented symbolically by a bar at the same location with the same length and orientation as B i . The intensity of the bar B i is the averager i . For the remaining M pairs of plots, the left plot shows I m , and the right plot shows B m = (B m,i , i = 1, ..., n). The intensity of each B m,i is r 1/2 m,i . These M examples are arranged in descending order by their log-likelihood scores (16). All the examples with non-typical poses are in the lower end. We obtained similar result using active correlation. The examples displayed in figure (1) are produced after we force the algorithm to select 60 elements.Experiment 2: Find and sketch. Using the learned model in experiment 1, we can find the car in the testing image shown in figure(4). The upper left plot is the testing image. The upper right plot displays the sketch of the car at the maximum likelihood scale and location. The lower left plot displays the maximum log-likelihood score over scale. The lower right plot displays the map of the log-likelihood at the optimal scale. We obtained similar result based on active correlation.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "Experiment 3 :3ROC comparison. Figure (5) displays 12 of the 43 training examples paired with their B m = (B m,i , i = 1, ..., n), n = 40, obtained by maximum likelihood. Figure (6.a) and (b) display the active bases B = (B i , i = 1, ..., n), n = 40, selected by the shared pursuit, using log-likelihood and active correlation scoring respec-", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "Figure 4 .4Figure 4. find and sketch. Lower left: the maximum log-likelihood score over scale. Lower right: the map of log-likelihood score at the optimal scale.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_17", "figure_caption": "Figure 5 .5Figure 5. Some training examples and the corresponding Bm.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_18", "figure_caption": "Figure 6 .6Figure 6. (a) B = (Bi, i = 1, ..., 40) selected by active correlation. (b) B selected by log-likelihood. (c) 80 weak classifiers (from the same dictionary of Gabor wavelets) selected by adaboost. The red ones are the weak classifiers whose responses are larger than thresholds, while the blue ones are otherwise.", "figure_data": ""}, {"figure_label": "74", "figure_type": "figure", "figure_id": "fig_19", "figure_caption": "Figure 7 .Experiment 4 :74Figure 7. ROC curves for active basis models learned by active correlation and log-likelihood respectively, and adaboost. AUC means area under ROC curve. Experiment 4: Mixture and EM. Suppose there are two categories in the training examples. We may assume a mixture model p(r m,1 , ..., r m,n )= \u03c1p (1) (r m,1 , ..., r m,n ) + (1 \u2212 \u03c1)p (0) (r m,1 , ..., r m,n ), where p (k) (r m,1 , ..., r m,n ) = n i=1 \u03bb (k) i exp{\u2212\u03bb (k) i r m,i }, k = 0, 1.We can fit the model by the EM algorithm. Then we classify the examples into the two categories based on posterior probabilities produced by the last iteration of the E-step. After that, we re-learn the active basis model for each category separately.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_20", "figure_caption": "Figure 8 .8Figure 8. Top row: clustering result by EM. Bottom row: relearned templates for the two clusters.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_21", "figure_caption": "Figure ( 8 )8Figure (8) displays the 37 training examples. We first learn a common B = (B i , i = 1, ..., n) with n = 80. Then we fit the mixture model on the coefficients of the 80 elements. The EM algorithm separates the examples into two clusters, as shown in figure (8), where there are 2 mistakes. Then we re-learn active basis models on the two clusters separately, with n = 60. The bottom row of figure (8) displays the learned templates. We can also re-learn the active basis models within the M-step in each iteration. Experiment 5: Find and learn. By combining the codes in the first two experiments, our method has the potential to handle training images that are not aligned, as suggested by the following preliminary experiment. There are five images of cats that are of the same size but at different locations. The only supervision is to give the bounding box for", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "B x,y,s,\u03b1 (x , y ) = G(x/s,\u1ef9/s)/s 2 , wherex = (x \u2212 x) cos \u03b1 \u2212 (y \u2212 y) sin \u03b1,\u1ef9 = (x \u2212 x) sin \u03b1 + (y \u2212 y)", "formula_coordinates": [2.0, 110.57, 24.51, 434.54, 639.75]}, {"formula_id": "formula_1", "formula_text": "I m = n i=1 c m,i B m,i + m , (1", "formula_coordinates": [2.0, 383.53, 287.47, 157.7, 31.17]}, {"formula_id": "formula_2", "formula_text": ")", "formula_coordinates": [2.0, 541.24, 298.56, 3.87, 8.76]}, {"formula_id": "formula_3", "formula_text": "B m,i \u2248 B i , i = 1, ..., n. (2", "formula_coordinates": [2.0, 383.54, 321.22, 157.7, 10.71]}, {"formula_id": "formula_4", "formula_text": ")", "formula_coordinates": [2.0, 541.24, 322.1, 3.87, 8.76]}, {"formula_id": "formula_5", "formula_text": "B i = B xi,yi,s,\u03b1i , (3) B m,i = B xm,i,ym,i,s,\u03b1m,i ,(4)", "formula_coordinates": [2.0, 385.11, 378.15, 160.0, 25.2]}, {"formula_id": "formula_6", "formula_text": "x m,i = x i + d m,i sin \u03b1 i ,(5)", "formula_coordinates": [2.0, 367.03, 449.56, 178.08, 10.26]}, {"formula_id": "formula_7", "formula_text": "y m,i = y i + d m,i cos \u03b1 i , (6) \u03b1 m,i = \u03b1 i + \u03b4 m,i , (7) d m,i \u2208 [\u2212b 1 , b 1 ], \u03b4 m,i \u2208 [\u2212b 2 , b 2 ].(8)", "formula_coordinates": [2.0, 367.03, 464.51, 178.08, 40.15]}, {"formula_id": "formula_8", "formula_text": "imizes | m , B m,i | 2 among all possible B m,i \u2248 B i .", "formula_coordinates": [3.0, 70.04, 108.98, 216.32, 12.08]}, {"formula_id": "formula_9", "formula_text": "m | m , B m,i | 2 .", "formula_coordinates": [3.0, 165.78, 132.88, 65.82, 13.58]}, {"formula_id": "formula_10", "formula_text": ") For m = 1, ..., M , let c m,i \u2190 m , B m,i , and let m \u2190 m \u2212 c m,i B m,i . (3) Stop if i = n.", "formula_coordinates": [3.0, 53.44, 153.14, 232.92, 40.47]}, {"formula_id": "formula_11", "formula_text": "Given template B = (B i , i = 1, ..., n), we assume that (d m,i , \u03b4 m,i ) \u223c uniform(\u2206 = [\u2212b 1 , b 1 ] \u00d7 [\u2212b 2 , b 2 ]) in or- der to generate the deformed template B m = (B m,i , i = 1, ..., n) according to (3)-(8).", "formula_coordinates": [3.0, 50.11, 357.12, 236.25, 45.5]}, {"formula_id": "formula_12", "formula_text": "(c m,1 , ..., c m,n ) \u223c g(c m,1 , ..., c m,n ),(9)", "formula_coordinates": [3.0, 102.38, 472.06, 183.98, 10.71]}, {"formula_id": "formula_13", "formula_text": "m (x, y) \u223c N(0, \u03c3 2 ) independently,(10)", "formula_coordinates": [3.0, 106.42, 485.13, 179.94, 12.58]}, {"formula_id": "formula_14", "formula_text": "(c m,1 , ..., c m,n ) is independent of m . (11", "formula_coordinates": [3.0, 102.38, 502.39, 179.83, 10.26]}, {"formula_id": "formula_15", "formula_text": ")", "formula_coordinates": [3.0, 282.21, 502.82, 4.15, 8.76]}, {"formula_id": "formula_16", "formula_text": "p(I m | B m ) = q(I m ) p(r m,1 , ..., r m,n ) q(r m,1 , ..., r m,n ) , (12", "formula_coordinates": [3.0, 78.63, 642.56, 203.58, 23.83]}, {"formula_id": "formula_17", "formula_text": ")", "formula_coordinates": [3.0, 282.21, 649.72, 4.15, 8.76]}, {"formula_id": "formula_18", "formula_text": "where r m,i = I m , B m,i , i = 1, ..., n. q(I m ) is the density of white noise model, i.e., I m (x, y) \u223c N(0, \u03c3 2 ) independently. q(r m,1 , ..., r m,n ) is the density of (r m,1 , ..., r m,n ) under q(I m ). p(r m,1 , ..., r m,n ) is the den- sity of (r m,1 , ..., r m,n ) under p(I m | B m ).", "formula_coordinates": [3.0, 308.86, 24.51, 236.26, 58.53]}, {"formula_id": "formula_19", "formula_text": "|D| \u00d7 1 vector,", "formula_coordinates": [3.0, 308.86, 227.07, 58.61, 9.96]}, {"formula_id": "formula_20", "formula_text": "1 M M m=1 log p(r m ) q(r m ) \u2192 KL(p(r m )|q(r m )),(14)", "formula_coordinates": [3.0, 334.44, 503.89, 210.67, 31.07]}, {"formula_id": "formula_21", "formula_text": "Under white noise q(I m ), r m = B m I m \u223c N(0, B m B m \u03c3 2 ). If we as- sume p(r m ) is such that r m \u223c N(0, B m B m \u03c3 2 0 ) with \u03c3 2 0 > \u03c3 2 , then log[p(r m )/q(r m )] is positively linear in r m (B m B m ) \u22121 r m = I m B m (B m B m ) \u22121 B m I m ,", "formula_coordinates": [3.0, 308.86, 570.74, 236.26, 58.62]}, {"formula_id": "formula_22", "formula_text": "B m = {B m,i , i = 1, ..., n}, with B m B m \u2248 1, we choose to use r m,i = h m (| I m , B m,i | 2 ), i = 1, ..., n, as sparse coding vari- ables. | I m , B m,i | 2 is", "formula_coordinates": [4.0, 50.11, 462.03, 236.25, 46.57]}, {"formula_id": "formula_23", "formula_text": "I m , B m,i | 2 \u223c \u03c3 2 m \u03c7 2 2 \u223c 2\u03c3 2 m exp(1), i.e.,", "formula_coordinates": [4.0, 308.86, 330.71, 236.25, 25.02]}, {"formula_id": "formula_24", "formula_text": "| I m , B m,i | 2 /2\u03c3 2 m \u223c exp(1). If B m B m = 1, then | I m , B i | 2 /2\u03c3", "formula_coordinates": [4.0, 308.86, 354.62, 236.25, 24.04]}, {"formula_id": "formula_25", "formula_text": "| I m , B i | 2 /\u03c3 2 m,s are independent for i = 1, ..., n if B m B m = 1. \u03c3 2 s,m can be estimated b\u0177 \u03c3 2 m,s = 1 |D|K x,y\u2208D \u03b1 | I m , B x,y,s,\u03b1 | 2 , (15", "formula_coordinates": [4.0, 308.86, 474.17, 377.05, 61.63]}, {"formula_id": "formula_26", "formula_text": ")", "formula_coordinates": [4.0, 540.96, 515.56, 4.15, 8.76]}, {"formula_id": "formula_27", "formula_text": "(| I m , B i | 2 /\u03c3 2 m,s > r) = exp(\u2212r), which is short. (3", "formula_coordinates": [4.0, 308.86, 556.53, 236.24, 34.92]}, {"formula_id": "formula_28", "formula_text": "F (r) = exp(\u2212r 0 ) is r 0 = \u2212 log F (r), so \u2212 log F (| I m , B m,i | 2 /\u03c3 2 m,s ) \u223c exp(1)", "formula_coordinates": [4.0, 391.63, 653.55, 153.49, 10.71]}, {"formula_id": "formula_29", "formula_text": "r m,i = h m (| I m , B m,i | 2 ) = \u2212 log F (| I m , B m,i | 2 /\u03c3 2 m,s ).", "formula_coordinates": [5.0, 50.11, 54.71, 240.29, 13.54]}, {"formula_id": "formula_30", "formula_text": "m,i ) = 1/\u03bb i . Log-likelihood is log[p(I m | B m )/q(I m )] = n i=1 log p(r m,i ) q(r m,i ) = n i=1 [(1 \u2212 \u03bb i )r m,i + log \u03bb i ].(16", "formula_coordinates": [5.0, 62.06, 256.46, 220.15, 93.32]}, {"formula_id": "formula_31", "formula_text": "B = (B i , i = 1, ..., n) by maximizingr i subject t\u014d r i \u2212 1 \u2212 logr i > log |\u2206|, (17", "formula_coordinates": [5.0, 50.11, 621.48, 236.25, 42.78]}, {"formula_id": "formula_32", "formula_text": ")", "formula_coordinates": [5.0, 282.21, 654.42, 4.15, 8.76]}, {"formula_id": "formula_33", "formula_text": "slide D over D m . Let D x,y \u2282 D m be the bounding box centered at (x, y) \u2208 D m . Within each D x,y , for i = 1, ..., n, choose the optimal B m,i \u2248 B i that maximizes r m,i = [I m , B m,i ]. Then compute the log-likelihood score l m (x, y) = n i=1 [(1 \u2212 \u03bb i )r m,i + log \u03bb i ]. Choose (x, y)", "formula_coordinates": [5.0, 308.86, 366.01, 236.25, 60.03]}, {"formula_id": "formula_34", "formula_text": "deformable template B = (B i , i = 1, ..., n) in the above section is parametrized by \u03bb = (\u03bb i , i = 1, ..., n). The log-likelihood score is n i=1 [(1 \u2212 \u03bb i )r m,i + log \u03bb i ],", "formula_coordinates": [5.0, 308.86, 541.66, 236.25, 36.12]}], "doi": ""}