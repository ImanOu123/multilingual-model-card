{"title": "Query Chains: Learning to Rank from Implicit Feedback", "authors": "Filip Radlinski; Thorsten Joachims", "pub_date": "2006-05-08", "abstract": "This paper presents a novel approach for using clickthrough data to learn ranked retrieval functions for web search results. We observe that users searching the web often perform a sequence, or chain, of queries with a similar information need. Using query chains, we generate new types of preference judgments from search engine logs, thus taking advantage of user intelligence in reformulating queries. To validate our method we perform a controlled user study comparing generated preference judgments to explicit relevance judgments. We also implemented a real-world search engine to test our approach, using a modified ranking SVM to learn an improved ranking function from preference data. Our results demonstrate significant improvements in the ranking given by the search engine. The learned rankings outperform both a static ranking function, as well as one trained without considering query chains.", "sections": [{"heading": "INTRODUCTION", "text": "Designing effective ranking functions for free text retrieval has proved notoriously difficult. Retrieval functions designed for one collection and application often do not work well on other collections without additional time consuming modifications. This has led to interest in using machine learning methods for automatically learning ranked retrieval functions.\nFor this learning task, training data can be collected in two ways. One approach relies on actively soliciting training data by recording user queries and then asking users to explicitly provide relevance judgments on retrieved documents (such as [7,13,22]). Few users are willing to do this, making significant amounts of such data difficult to obtain. An alternative approach is to extract implicit relevance feedback from search engine log files (such as in [6,15]). This allows virtually unlimited data to be collected at very low cost, although interpretation is more complex.\nIrrespective of the approach, to the best of our knowledge all previous research in learning retrieval functions has considered each query independently. We will show that this ignores valuable information that is hidden in the sequence of queries and clicks in a search session. For instance, if we repeatedly observe the query \"special collections\" followed by another for \"rare books\" on a library search system, we may deduce that web pages relevant to the second query may also be relevant to the first. Additionally, this log information can also allow us to learn to correct spelling mistakes in a similar way. For example, we observed that users searching for the \"Lexis Nexis\" repository often first search for \"Lexis Nexus\" by mistake.\nAs users search, it is well documented that they often reformulate their queries [3,8,18,20]. Previous work has attempted to predict query reformulations, but to the best of our knowledge these reformulations have never been used to learn better retrieval functions. In this paper, we refer to a sequence of reformulated queries as a query chain. When queries are considered independently, log files only provide implicit feedback on a few results at the top of the result set for each query because users very rarely look further down the list. The advantage of using query chains is that we can also deduce relevance judgments on the many more documents seen during an entire search session.\nThe key contribution of this work is recognizing that we can successfully use evidence of query chains that is present in search engine log files to learn better retrieval functions. We demonstrate a simple method for automatically detecting query chains in query and clickthrough logs. Using this data, we show how to infer preference judgments as to the relative relevance of documents both within individual query results, and between documents returned by different queries within the same query chain. The method used to generate the preference judgments is validated using a controlled user study. We then adapt a ranking SVM to learn a ranked retrieval function from the preference judgments. In doing so, we propose a general retrieval model that can learn to associate individual documents with specific query words, even if the words do not occur in the documents. This differs from previous learned ranked retrieval functions in that our method can learn a much more general class of functions.\nWe demonstrate the effectiveness of our approach on a real-world web search system, the Cornell University library 1 web search. We name our implementation the Osmot search engine, and it is available for download to the research community. The name is derived from the word osmosis, as learning from implicit feedback is, in our opinion, almost as good as learning from users by osmosis.", "publication_ref": ["b6", "b12", "b21", "b5", "b14", "b2", "b7", "b17", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORK", "text": "When learning to rank, the method by which training data is collected offers an important way to distinguish between different approaches. This data usually consists of a set of statements as to the relevance of a document, or set of documents, to a given query. Such relevance judgments are either collected explicitly by asking users, or implicitly by observing user behavior and drawing conclusions. Moreover, the statements can be absolute or relative. Absolute feedback involves statements that a particular document is, or is not, relevant to a query. Relative feedback involves statements that a particular document is more relevant to a query than some other document.\nMost previous work in learning to rank has assumed absolute relevance judgments. On the one hand, a number of methods in ordinal regression use explicit feedback to learn to rank, such as work by Crammer and Singer [7], Rajaram et al. [22] and Herbrich et al. [13]. However, explicit feedback is expensive to collect, with few users willing to spend the additional time to provide it in a real-world setting. This makes typical labeled data sets small and difficult to work with. A number of researchers have collected absolute relevance judgments implicitly from clickthrough logs, such as [4,6,19,25]. They postulate that documents clicked on in search results are highly likely to be relevant. For example, Kemp et al. [19] present a learning search engine using document transformation. They assume results clicked on are relevant to the query and append the query to these documents. However, implicit clickthrough data has been shown to be biased as it is relative to the retrieval function quality and ordering [15,17]. This makes its interpretation as absolute feedback of questionable accuracy.\nCohen et al. [6] and Freund et al. [9] propose using log data to generate relative preference feedback. Both approaches consider learning a ranking function from these preference judgments, along similar lines as this work. However, in contrast to our method their learned function is limited to a combination of rankings given by a fixed set of manually constructed \"experts\". This approach of learning a combination of functions is also used by most other work in this area [1,2,4,15,21].\nJoachims [15] refined the interpretation of clickthrough log data as relative feedback. He suggests that given a ranking and a clicked-on document d, any document ranked above d but not clicked on is likely less relevant than d. In this paper, we evaluate the validity of this construction, and extend it to query chains. We also use a more general ranking function and extend the learning algorithm to query chains.  An important innovation in this paper is that we learn a more general ranking function than previous work by associating query words with specific documents. This approach has been used previously to learn to generate abstracts [23], and in document transformation [19], but not to learn ranking functions. Prior approaches cannot learn to associate \"new\" documents with a given query because they combine or re-order results obtained from one or more static ranking functions. In particular, given a query q, they cannot learn to retrieve any document not originally returned by q. Coming closest to solving this limitation previously, the method presented by Kemp et al. [19] could be extended with query chains. However, they assume implicit absolute feedback, making their approach more likely to be susceptible to bias and noise.", "publication_ref": ["b6", "b21", "b12", "b3", "b5", "b18", "b24", "b18", "b14", "b16", "b5", "b8", "b0", "b1", "b3", "b14", "b20", "b14", "b22", "b18", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "ANALYSIS OF USER BEHAVIOR", "text": "In order to infer implicit preference judgments from log files, we need to understand how users assess search results. Clearly we can only derive valid feedback for results that the user actually looked at and assessed. In this section we explore this question.\nAn eye tracking study was performed to observe how users formulate queries, assess the results returned by the search engine and select the links they click on [11,12]. Thirty six undergraduate student volunteers were instructed to search for the answers to five navigational and five informational queries [5]. The former involved finding a specific web page while the latter involved finding some specific information. The subjects were asked to start from the Google search page and find the answers. There were no restrictions on what queries they may choose, how and when to reformulate queries, or which links to follow. Users were told that the goal of the study was to observe how people search the  Web, but were not told of the specific interest in their behavior on the results page of Google. All clicks, the results returned by Google, and the pages connected to the results were recorded by an HTTP proxy. Movement of the eyes was recorded using an ASL 504 commercial eye tracker (Applied Science Technologies, Bedford, MA). More details on the experimental setup are provided in [12]. Figure 1 shows the fraction of the time users looked at, and clicked on, each of the top 10 search results for a query. It tells us that users usually look at least at the top two result abstracts. Interestingly, note that despite the top two documents receiving almost equal attention, users were much more likely to click on the first result. Figure 2 (adapted from Figure 2 in [12]) shows the number of abstracts viewed above and below any result that was clicked on. This figure tells us that users usually scan the results in order from top to bottom. We also see that users usually look at one abstract below any they click on. Further analysis showed that this is usually the abstract immediately below the one clicked on [17]. We conclude that users typically look at most of the results from the first to the one below the last one clicked on.\nPrevious work studying web search behavior [20,24] observed that users rarely run only a single query and immediately find suitable results. Rather, they tend to perform a sequence of queries for any given question. Such query chains are also observed in the eye tracking study. The mean query chain length was 2.2 queries, although the particular questions asked and the laboratory environment would be expected to have an influence on this value. A number of papers (e.g. [3,10,18]) successfully learn to predict query reformulations. Their success on this task suggests that the problem of detecting query chains, which we will have to address, is feasible.", "publication_ref": ["b10", "b11", "b4", "b11", "b11", "b16", "b19", "b23", "b2", "b9", "b17"], "figure_ref": ["fig_0", "fig_1", "fig_1"], "table_ref": []}, {"heading": "FROM LOG FILES TO FEEDBACK", "text": "This section details our approach for generating relative preference feedback from query and clickthrough logs as implemented in the Osmot search engine. We then present an evaluation of this approach using results from the eye tracking study.\nConsider the queries shown in Figure 3 as examples we use to demonstrate the value of query chains. The first shows the results presented to a user running the query \"NDLF\"  We either consider a single query, q, or a query q that has been preceded by a query q \u2032 . Given a query, a dot represents a result document and an x indicates the result was clicked on. We generate a constraint for each arrow shown, with respect to the query marked.\non the Cornell University library search page. The user is searching for the National Digital Library Foundation website, but has retrieved only meeting notes that reference people working for the NDLF. The desired page is not in these results, most probably because it does not contain the word \"NDLF\". The second query is a search performed in Google by a participant in the eye tracking study in attempting to find the name of the house that Ezra Cornell built for himself. We get many results, but in fact none of the top 10 contain any relevant information. In both cases, single query feedback will not be informative because no relevant documents were retrieved. In the former case, the results simply do not contain any documents relevant to the query. In the latter, if there is a relevant document it is unlikely the user will look far enough in the results to see it.\nOn the other hand, after both of these queries, we observed that the user continued running other queries. Often, such later queries are more successful. If a user finds a relevant document with a later query, it is reasonable to assume that the user would have preferred to have seen the relevant document over the results actually returned earlier. Recognizing the information necessary to make these deductions is present in search engine log files, we now propose specific strategies for generating such preference feedback from query chains. We defer a discussion of how to group queries into query chains to Section 6.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Implicit Feedback Strategies", "text": "We generate preference feedback using six strategies. These strategies are illustrated in Figure 4. The first two strategies show preferences that can be inferred without query chains. q1 q2 d1 d4 x d2 x d5 d3 d6 d2 >q1 d1 d4 >q2 d5 d4 >q1 d5 d4 >q1 d1 d4 >q1 d3\nFigure 5: Sample query chain and the feedback that would be generated using all six feedback strategies. Two queries were run, and each returned three documents. One document in each query was clicked on. di >q dj means that di is preferred over dj with respect to the query q.\nThe first one, \"Click >q Skip Above\" was proposed in [6,15]. This strategy proposes that given a clicked-on document (marked x in the figure), any higher ranked document that was not clicked on is likely less relevant. The preference is indicated by an arrow labeled with the query, to show that the preference is with respect to that query. We expect this to be valid because the eye tracking study showed that users view results in order, and a user is unlikely to click on a document she considers less relevant than another document she observed. Note that these preferences are not stating that the clicked-on document is relevant, rather that it is more likely to be relevant than the ones not clicked on above. The second strategy, \"Click First >q No-Click Second\" makes use of the fact that users typically view both of the top two results before clicking. It states that if the first document is clicked on, but the second is not, the first is likely more relevant than the second. It seems reasonable to assume that having considered two options, the user is likely to click on the more relevant one. The next two strategies are identical to the first two except that they generate feedback with respect to the previous query. The intuition behind this is that since the two queries belong to the same query chain, the user is looking for the same information with both. Had the user been presented with the new results for the earlier query, she would have preferred the clicked-on document over those skipped above.\nThe last two strategies make the most use of query chains. The strategy \"Click > q \u2032 Skip Earlier Query\" states that a clicked-on document is preferred over any result not clicked on in an earlier query q \u2032 (within the same query chain). This judgment is made with respect to the earlier query, 2 q \u2032 . Since the eye tracking study revealed that users usually look one document past the last one clicked on, we also generate a preference for this document. In the event that no documents were clicked on in the earlier query, we use the fact that users usually look at the top two results. This is exploited in the feedback strategy \"Click > q \u2032 Top Two Earlier Query\" by generating preferences for the top two results. In the unusual case where there are not enough results to the earlier query to use these strategies, we select a random document as if it had been at the end of the results.\nUltimately, given some query chain, we make use of all six strategies to generate the preference feedback. Figure 5 Strategy  gives a sample query chain and the feedback that would be generated in this case.", "publication_ref": ["b5", "b14"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Accuracy of Feedback Strategies", "text": "While the feedback strategies proposed above are intuitively appealing, a quantitative evaluation is necessary to establish their degree of validity. To determine the accuracy of each individual strategy, we conducted a controlled experiment following the setup of the eye-tracking study described in Section 3 for an additional 16 subjects. For these subjects, we evaluated in how far the preferences derived from the feedback strategies agree with explicit relevance judgments made by independent judges.\nFor these 16 subjects, we collected all results and their associated web pages returned by Google from the HTTPproxy cache that recorded their sessions. We grouped the results by query chain and subject and collected explicit relevance judgments using five judges. The judges were asked to weakly order all results encountered during each query chain according to their relevance to the question. To avoid biasing the judges, the order in which results were presented to the judges was randomized and the judges were not given the abstracts Google used when presenting the results. Some of the query chains were assessed by two judges for inter-judge agreement verification. The agreement between judges is reasonably high. Whenever two judges expressed a strict preference between two pages, they agree in the direction of preference in 86.4% of the cases.\nWe now evaluate the extent to which the preferences generated from clicks agree with the explicit judgments. Table 1 summarizes the results. The table shows the percentage of times the preferences generated from clicks using the above strategies agree with the direction of a strict preference of a relevance judge. The first two lines in the table show the accuracy of the strategies that do not exploit query chains. The \"Click >q Skip Above\" strategy is 78.2% accurate, which is substantially and significantly better than the random baseline of 50%. Furthermore, it is reasonably close in accuracy to the average agreement of 86.4% between the explicit judgments from different judges, which can serve as an upper bound for the accuracy one could ideally expect even from explicit user feedback. The second within-query strategy, \"Click First >q No-Click Second\", appears less accurate. However, since it produces fewer preferences (i.e. only on queries where the user clicked exclusively on the first link), the confidence intervals are large. Independent of the accuracy, the preferences from this strategy are probably less informative, since they only confirm the current ranking and never suggest a reordering. Lines 3 and 4 in Table 1 show the accuracy of the two strategies that exploit query chains. Both \"Click > q \u2032 Skip Earlier Query\" and \"Click > q \u2032 Top Two Earlier Query\" are significantly more accurate than random. In particular, the accuracy of \"Click > q \u2032 Top Two Earlier Query\" is very close to the average agreement between judges. Note that this strategy produces particularly informative preferences, since it associates documents with query words that may not occur in the document.\nA possible explanation for the difference in accuracy between the two query-chain strategies is that they apply to different types of query chains. While \"Click > q \u2032 Skip Earlier Query\" is applied when the previous query received a click, the strategy \"Click > q \u2032 Top Two Earlier Query\" is applied precisely in the opposite case. To investigate the effect of this difference, we also evaluated a variant of \"Click > q \u2032 Top Two Earlier Query\". This variant generates preferences analogous to \"Click > q \u2032 Top Two Earlier Query\", but in chains where the previous query did receive a click (but excluding the clicked results). The accuracy of this strategy is 67.7% \u00b1 9.4, indicating that the absence of a click followed by another query with a click is particularly strong evidence regarding the relevance of the results of the earlier query.\nOverall, we conclude that the preferences generated from the clickthrough logs are reasonably accurate and that they convey information regarding the user's preferences.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "EVALUATION ENVIRONMENT", "text": "While the previous section showed that the preferences generated from logs files are accurate, can they be used to learn an improved retrieval system?\nTo address this question, we constructed a publicly accessible real-world search engine. The search engine implements a full-text search of web pages maintained by the Cornell University library 1 (CUL). This collection includes over 13,500 web pages. We used the Nutch search engine 3 as a starting point, with the Osmot search engine effectively being a wrapper around Nutch that implements logging, log analysis, learning, reranking and evaluation functionality. Osmot is designed to allow any number of different ranking functions to be plugged into it. In the experiments in this paper, we chose Nutch's built-in retrieval function as the baseline to compare against and build upon. The Nutch retrieval function is based on the cosine distance and incorporates several modifications to make it more suitable for web search including special cases for phrase matches and HTML fields.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "DETECTING QUERY CHAINS", "text": "In order to use query chains, we must first have a method to identify them. In this section we propose such a heuristic and demonstrate its effectiveness.\nAs a basis for our evaluation, we created a dataset using search logs from the CUL search engine. We manually labeled query chains in the logs for a period of 5 weeks. The search logs recorded the query, date, IP address, results returned, number of clicks on the results and a session id uniquely assigned to each user. We extracted the list of queries, grouped them by IP address and sorted them chronologically. Queries from an IP address with no other queries within 24 hours were automatically marked as not Table 2: Features used to learn to classify query chains. q1 and q2 are two queries at times t1 and t2, with t1 < t2. r1 and r2 are the respective result sets, with r1 \u2032 and r2 \u2032 being the top 10 results.\nbelonging to a query chain. This resulted in 1285 queries. Two judges (the authors of this paper) then individually grouped the queries into query chains manually, using search engines to resolve uncertainties (such as a query for a person followed by one for the department where the person is a faculty member). Finally, the judges combined their identified query chains, resolving the small number of disagreements between themselves through further investigation.\nFor each pair of queries from the same IP address within half an hour, we generated a training example by constructing a feature vector. The training example was labeled using the query chains identified manually. If the two queries belonged to the same query chain the example was labeled as positive. Otherwise it was labeled as negative. This led to 3418 training examples of which 3096 were labeled as positive. The feature vector generated given two queries q1 and q2 consisted of the 16 features shown in Table 2.\nUsing this data, we trained a number of SVM classifiers with various parameters. The classifiers learned tended to label almost all examples as positive. Among our best performing models was an SVM with an RBF kernel with C = 100 and \u03b3 = 1. Evaluating using five-fold cross validation, it gave an average accuracy of 94.3% and precision of 96.5%. This compares to a accuracy and precision of 91.6% for a simple non-learning strategy where we assume all pairs of queries from the same IP address within half an hour of each other are in the same query chain. As this difference is relatively small, and computing this feature vector for every query pair is relatively expensive (in particular since it depends on the abstracts retrieved), we decided to rely simply on our heuristic measure. We judged that a precision of over 90% is sufficient for our present purposes. We considered extending the half-hour window on our training data in order to increase the recall, but decided that we were recognizing a sufficient number of query chains without doing so.\nHowever, to gain some insight into the properties of query chains we trained a linear SVM using the same data and computed the total weight on each feature. The features with largest positive weight were CosineDistance(q1, q2), which measures the cosine distance between q1 and q2, and CosineDistance(doc ids of r1', doc ids of r2'), which measures the overlap between the documents in the top 10 results. This indicates that if two queries are similar, or if they retrieve many of the same documents, then they are more likely to be in the same query chain. The feature with largest negative weight measures the minimum number of results returned by either query normalized between 0 and 1, NormalizedMin(|r1|, |r2|). This indicates that if one of the queries returns few results, the queries are more likely to be in a query chain. Our interpretation is that if q1 returns no results, the user is more likely to run a second query.\nWe conclude that it is possible to segment log files into query chains with reasonable accuracy.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "LEARNING RANKING FUNCTIONS", "text": "Given log files recording user behavior on a web search engine, we have shown how to transform the log records into preference judgments in Section 4 after identifying query chains using the method from Section 6. Next, we present an algorithm to learn from these preferences, which we then evaluate using the Osmot search engine described earlier.\nWe assume as input preference judgments over documents di and dj for a given query q to be of the following form.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "di >q dj", "text": "(1)\nSuch a preference judgment indicates that di is preferred over dj given q. As our retrieval model, we chose a linear retrieval function:\nrel(di, q) = w \u2022 \u03a6(di, q)(2)\nwhere \u03a6(di, q) (which we define later) is a function that maps documents and queries to a feature vector. Intuitively, it can be thought of as a feature vector describing the quality of the match between a document di and the query q. w is a weight vector that assigns weights to each of the features in \u03a6, thus giving us a real valued retrieval function where a higher score indicates a document di is estimated to be more relevant to the query q. The task of learning a ranking function becomes one of learning an optimal w.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ranking SVMs", "text": "We used a modified ranking SVM to learn w in Equation 2. Here, we briefly introduce ranking SVMs [15], which generalize ordinal regression SVMs [13]. We start by rewriting Equation 1 as:\nw \u2022 \u03a6(di, q) > w \u2022 \u03a6(dj , q)\nWe then add a margin, and non-negative slack variables to allow some of the preference constraints to be violated, as is done with classification SVMs. This yields a preference constraint over w.\nw \u2022 \u03a6(di, q) \u2265 w \u2022 \u03a6(dj , q) + 1 \u2212 \u03beij\nAlthough we cannot efficiently find a w that minimizes the number of violated constraints, we can minimize an upper bound on the number of violated constraints, \u03beij. Simultaneously maximizing the margin leads to the following convex quadratic optimization problem:\nmin w,\u03be ij 1 2 w \u2022 w + C ij \u03beij subject to \u2200(q, i, j) : w \u2022 \u03a6(di, q) \u2265 w \u2022 \u03a6(dj, q) + 1 \u2212 \u03beij \u2200i, j : \u03beij \u2265 0 (3)\nWe will later add more constraints to the optimization problem taking advantage of prior knowledge in the learning to rank setting.", "publication_ref": ["b14", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Retrieval Function Model", "text": "Next we must specify the mapping \u03a6(di, q). This definition is key in determining what class of ranking functions we can learn, and is therefore particularly important in determining the usefulness of this method. We define two types of features: rank features \u03c6 f rank (d, q) and term/document features \u03c6terms(d, q). Rank features serve to exploit the existing retrieval functions rel f 0 , while term/document features allow us to learn more fine-grained relationships between particular query terms and specific documents.\nFirst we need a few definitions. Let T := {t1, . . . , tN } be all the terms (words) in our dictionary. A query q is a set of terms q := {t \u2032 1 , . . . , t \u2032 n } where t \u2032 i \u2208 T . Let D := {d1, . . . , dM } be the set of all documents in our collection. We assume the original search engine has a number of available retrieval functions rel f 0 (d, q) with f \u2208 F . We define r f 0 (q) as the ordered set of results as ranked by rel f 0 for query q. In the experiments in this paper, F consists of a single ranking function as provided by Nutch for the sake of simplicity. Now,\n\u03a6(d, q) = \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 \u03c6 f 1 rank (d, q) . . . \u03c6 f F rank (d, q) \u03c6terms(d, q) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb \u03c6 f rank (d, q) = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 1(Rank(d in r f 0 (d, q)) \u2264 1) . . . 1(Rank(d in r f 0 (q)) \u2264 10) 1(Rank(d in r f 0 (q)) \u2264 15) . . . 1(Rank(d in r f 0 (q)) \u2264 100) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb \u03c6terms(d, q) = \uf8ee \uf8ef \uf8f0 1(d = d1 \u2227 t1 \u2208 q) . . . 1(d = dM \u2227 tN \u2208 q) \uf8f9 \uf8fa \uf8fb\nwhere 1 is the indicator function.\nBefore looking at the term features \u03c6terms(d, q), let's explore the rank features \u03c6 f i rank (d, q). For each retrieval function rel f i 0 we have 28 rank features (for ranks 1,2,..,10,15, 20,..,100). Each of these is set to 1 if the rank of the document in r f i 0 is at or above the specified rank. The rank features allow us to learn weights for the rankings of the original search results. This allows the learned ranking function to combine different retrieval functions with different weights, as is done in prior work described earlier.\nWe do not consider the specific scores assigned by rel f 0 in order to account for potentially different magnitudes of the scores from different retrieval functions. This also ensures that our method could generalize to settings where we do not have access to the scores assigned to documents but only the document ranks. As an example, if some document d is at rank 4 given query q and using retrieval function f1 then \u03c6 f 1 rank (d, q) = [0, 0, 0, 1, . . . , 1] T . If a document is not ranked in the top 100 by the retrieval function f1, then all the features of \u03c6 f 1 rank are 0. This means that documents not ranked in the top 100 results by a retrieval function rel f i 0 are indistinguishable using the \u03c6 f i rank features (although we could increase the maximum rank considered arbitrarily). We chose this cutoff as it is extremely rare for users to look beyond the top 100 results. We also have N M term/document features. For convenience, let \u03c6 i,j term (d, q) correspond to the term with di and tj in \u03c6terms(d, q). There is one for every (term, document) pair in T \u00d7 D. The term/document features allow the ranking function to learn associations between specific query words and documents by assigning a non-zero value to the appropriate weight. This is usually an extremely large number of features, although most never appear in our training data and can thus be ignored. Furthermore, the feature vector \u03c6terms(d, q) is very sparse. For any particular document d \u2208 D, given a query with |q| terms, only |q| of the \u03c6 i,j term (d, q) features are set to 1. Specifically, only the terms for one i value (where d = di) and with tj \u2208 q are non-zero. The sparsity makes this problem well suited for solving using support vector machines. A positive value of the weight w i,j term , associated with the feature \u03c6 i,j term , indicates that di is more likely to be relevant to queries containing the term tj , while a negative value means the opposite.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Adding Prior Knowledge", "text": "When learning to rank, we have additional prior knowledge that should be incorporated into this problem. Absent any other information, documents with a higher rank in the original ranking should be ranked higher in the learned ranking system. This is intuitive because on average we would expect the document relevance to be a decreasing function of the original rank of the documents, unless the original ranking function is particularly poor. We define such additional constraints in this section.\nIt is also of practical importance to add these constraints: In our training data almost all of the relevance judgments generated state that a lower ranked document is preferred to a higher ranked document. Without additional constraints, a trivial and undesirable solution to the optimization problem in Equation 3 would be one that reverses the original ranking by assigning a negative value to each of the weights corresponding to rank features in \u03a6. To see this, consider again Figure 4. The \"Click > q(q \u2032 ) Skip Above\" preferences would be satisfied if the rankings were reversed. These preferences are much more common than \"Click First > q(q \u2032 ) No-Click Second\" preferences. In the last two preferences classes, the preferred document is also presumably somewhere much lower in the results for q \u2032 (if it is not in the results, we can think of it as being at the bottom of the results), and hence the preferences would also be satisfied if the entire ranking were reversed.\nWe add additional hard constraints to the optimization problem specified in Equation 3. These constraints require that weights for each of the rank features must be greater than a constant positive value wmin:\n\u2200i \u2208 [1, 28|F |]. w i \u2265 wmin(4)\nIntuitively, wmin limits how quickly the original ranking is changed by training data. To see this, briefly consider a setting where we have a single ranking function f and a query q = t \u2032 that returns at least 100 results. Let di be the\nranking r d1 d2 d3 d4 ranking r \u2032 d2 d5 d1 d6 \u21d2 combined(r, r \u2032 ) d1 d2 d5 d3 d4 f6\nFigure 6: Two example rankings with four results each, and the combined outputs we would generate by starting with the top ranked document from ranking r.\ndocument ranked at position i in r f 0 (q). In this case,\n\u03c6 f rank (d100, q) = [0, . . . , 0, 0, 1] T \u03c6 f rank (d95, q) = [0, . . . , 0, 1, 1] T \u2022 \u2022 \u2022 \u03c6 f rank (d1, q) = [1, . . . , 1, 1, 1]\nT Calling the part of w that corresponds to rank features w rank , from Equation 4 we then get\nw rank \u2022 \u03c6 f rank (d100, q) \u2265 wmin w rank \u2022 \u03c6 f rank (d95, q) \u2265 2wmin \u2022 \u2022 \u2022 w rank \u2022 \u03c6 f rank (d1, q) \u2265 28wmin\nNow say we have a document d that is preferred over d1 but is not in the original results. d would be ranked highest if rel(d, q) > rel(d1, q). We know from Section 7.2 that only \u03c6 t \u2032 ,d term (d, q) is non-zero in \u03c6terms(d, q). Expanding and simplifying, this would imply:\nwterms \u2022 \u03c6terms(d, q) \u2265 28wmin + wterms \u2022 \u03c6terms(d1, q) w d,q term \u2265 28wmin + w d 1 ,q terms where w \u03b1,\u03b2\nterm corresponds to \u03c6 \u03b1,\u03b2 term (d, q). The larger wmin, the larger in magnitude w d,q term and w d 1 ,q term must be before this happens. A ranking SVM minimizes over w \u2022 w + C \u03beij , so the terms will only become large if there is sufficient training data to support a reordering.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Evaluation Methodology", "text": "In order to evaluate our results, we need an unbiased method for comparing two ranked retrieval functions. For this purpose we use the method detailed in [16]. This method was shown to give an accurate assessment of retrieval quality under reasonable assumptions. Given two ranking functions, we present users with a combination of the results from both. We know that users scan results from top to bottom, so we intertwine the results such that there is no presentation bias favoring either ranking function. This evaluation method is built into the Osmot search engine. Figure 6 shows two example rankings, r and r \u2032 , from two different retrieval functions as well as a combination of them, combined(r, r \u2032 ). Let seen(n, r) and seen(n, r \u2032 ) be the number of results the user has seen from rankings r and r \u2032 respectively after looking at the top n results from the combined ranking. seen(n, r) and seen(n, r \u2032 ) are defined as the smallest number of results that we have to combine from r and r \u2032 to produce the top n results of the combined ranking. We generate the combined ranking such that for any n, seen(n, r) \u2265 seen(n, r \u2032 ) \u2265 seen(n, r) \u2212 1. In our example, if the user looks at the top three results in the combined ranking, this is satisfied because seen(3, r) = 2 and seen(3, r \u2032 ) = 2. If the user looks at the top five results, seen(5, r) = 4 and seen(5, r \u2032 ) = 3. To compensate for a bias toward the results of r (seen(n, r) is sometimes one bigger than seen(n, r \u2032 )), we randomly switch r and r \u2032 half the time. This means that in expectation seen(n, r) = seen(n, r \u2032 ). The property is proved rigorously in [16].\nOnce we have presented the user with a combined ranking, we need to evaluate which of the two rankings is preferred. We first determine which results the user looked at by taking the lowest ranked clicked-on document as where the user stopped scanning the results (a conservative estimate). If the two rankings are equally good, we would expect the user to click on just as many results from each given that she has seen an equal number from each (in expectation). We measure clicks(r), the number of documents clicked on that are in the top seen(n, r) results of r, and similarly clicks(r \u2032 ). For example, in Figure 6, say the user clicked on d1 and d5. We would infer the user looked at the top 3 results. From before, we have seen(3, r) = seen(3, r \u2032 ) = 2. Therefore, clicks(r) = 1 (d1) and clicks(r \u2032 ) = 1 (d5).\nIf in expectation clicks(r) > clicks(r \u2032 ), we can conclude that the user prefers the ranking r over r \u2032 . When evaluating ranking functions, we count how often clicks(r) > clicks(r \u2032 ), and clicks(r) < clicks(r \u2032 ). We then use a binomial sign test to verify if the difference in counts of clicks(r) > clicks(r \u2032 ) and clicks(r) < clicks(r \u2032 ) is statistically significant. If so, we can say one ranking is preferred over the other.", "publication_ref": ["b15", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Training the Ranking SVM", "text": "We collected training data from the CUL search engine using the original ranking function between June and December 2004. During this time, we recorded user queries and clicks, observing 9,949 queries and 7,429 clicks. While we were collecting this data, the users saw results as ranked by the built-in Nutch retrieval function, which we denote as rel0. This gave 120,134 preferences constraints by applying all six strategies introduced above. We call these preferences PQC . Of these, 45,610 preferences were generated without using the query chain strategies. We call this subset of the preferences PNC .\nAfter adding the hard constraints as described above, we trained a ranking SVM for each of the two sets of preferences with a linear kernel and a default value of C using SV M light [14]. We set wmin = 1. Using the preferences PQC we learned a retrieval function relQC and using the preferences PNC we learned relNC . The former model has 41,354 support vectors, while the latter has 18,034.\nThe ranking model learned using query chains, relQC, instantiated 18,748 features. The number of features instantiated can be expected to grow almost linearly in the size of the document collection, and sub-linearly in the amount of training data collected (depending on overall user search behavior). However, this did not pose a problem from the SVM solver because all the preference judgments were sparse.", "publication_ref": ["b13"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "User Prefers Mode Chains Other Indifferent relQC vs. rel0 392 (32%) 239 (20%) 579 (47%) relQC vs. relNC 211 (17%) 160 (13%) 855 (70%) Table 3: Results on Cornell Library search engine. rel0 is the original retrieval function, relQC is that trained using query chains, and relNC is that trained without using query chains.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results and Discussion", "text": "We evaluated the ranking functions on the CUL search from 10 December 2004 through 18 February 2005 using the evaluation method described in Section 7.4. When a user connected to the search engine, we randomly selected an \"evaluation mode\" for that user. The user either saw a ranking combining rel0 and relQC or a ranking combining relQC and relNC. For consistency, we kept the same combination for the duration of each user's session (otherwise, if the user immediately re-ran the same query he or she may confusingly get different results).\nDuring the evaluation, we collected about 1200 queries in each evaluation mode. The results for both evaluation modes are shown in Table 3. These results show a number of interesting properties. Firstly, 53% of the time relQC, the ranking function trained using query chains, performs differently to the original ranking function, rel0. 30% of the time the two trained ranking functions perform differently. In particular, the first of these values indicates that our method often makes a difference in search engine performance. Given that the original ranking function is reasonable, it would be surprising if these values were much higher. As long as our method does not cause relevant documents that are ranked highly by rel0 to be lowered in rank, we would see identical performance in the cases when rel0 performs well.\nSecondly, from Table 3 we see that relQC outperforms rel0 more often than we would expect at random were the two ranking functions equally good. Using a binomial sign test, and the null hypothesis that the two ranking functions are equally effective, we are able to reject the null hypothesis with over 99% confidence. This establishes that our learned ranking function is an improvement over the original one. Of course, given the new ranking function, we are collecting new training data and can re-run the whole learning process. We expect this to produce continued improvement in ranking performance.\nFinally, the model trained using query chains outperforms the model trained without using query chains with over 99% confidence, using the same test. This demonstrates that by exploiting the information about query chains present in log files, we are able to see a measurable additional improvement in search engine performance over what we would see without using this extra information.\nOne may wonder if it makes sense to learn associations between specific query words and documents. Given our initial 9,949 training queries, Table 4 shows the top ten words that appear most frequently in queries. We see that queries tend to be repetitive. Ignoring the three stopwords in the top ten words, we found that at least one of the remaining seven words appears in 12% of all queries. At least one of the top 100 words (removing stopwords) appears in 38% of all  In order to understand where the improvements are coming from, it is useful to look at the word/document features with largest positive and negative weights. The top and bottom five features are given in Table 5. First we consider the top five features, which for the most part describe very sensible associations. The feature for \"lexus\" is associated with the main homepage of the Lexis-Nexis library resource. This is clearly a spelling correction, with a search for \"lexus\" originally returning no results. The same search now places the correct document at the top of the results. The feature for \"ebook\" returns the main ebooks web page. A search for ebook previously returned seven results, none of which were particularly useful. The top one, titled \"Answers to Frequent Job Searching Research Questions\", happened to mention access to ebooks from off campus. The feature for \"reuleaux\" is associated with an FAQ page page about the CUL digital collections. The web page provides a clear link to a site that describes models designed by Professor Reuleaux. This contrasts with the original top result being a broken link, and the second result being a newsletter with only passing reference to the model collection. The feature for \"and\" is of little practical interest (we did not remove stopwords). Finally, the fifth word \"oed\" is an acronym for the \"Oxford English Dictionary\". The associated document clearly links to it, in contrast with the original top result which was an information bulletin showing a set of screen shots how to get to the OED among other things.\nThe five features with the most negative weights in Table 5 are equally interesting. Four of them relate to meeting notes mentioning the National Digital Library Foundation. Using the original ranking function, this search generated just 6 results with only such meeting notes. With the learned system, a search for \"ndlf\" now returns similar results to a search for \"National Digital Library Foundation\". These results appear slightly more useful from the short abstracts that are presented. However, we discovered that in fact the search engine had not indexed the main NDLF web page. We see here that the search system has recognized users running chains of queries looking for the NDLF website, although none have been successful in finding it. Despite this, some of the worst results for this query have indeed been pushed down the results list. The fifth feature is harder to interpret, but from log files it appears that users looking for the Department of Learning and Instruction saw this result and repeatedly skipped it. This document used to appear as the top result given the query \"instruction\".", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4", "tab_5", "tab_5"]}, {"heading": "CONCLUSIONS AND FUTURE WORK", "text": "In this paper, we have demonstrated that query chains can be used to extract useful information from search engine log files. After presenting an algorithm to infer preference judgments from log files, we showed that the preferences judgments are valid, independent of the learning method. We then presented a method to identify query chains, and an algorithm that uses the preference judgments to learn an improved ranking function. The model used for the ranking function is more general than in previous work. In particular, it allows the algorithm to learn to include new documents originally not present in initial search results in the learned rankings. The evaluation shows our approach to be effective, and that it can learn highly flexible modifications to the original search results. The Osmot search engine is available to the research community 4 .\nA natural question that arises in this setting is the tolerance of this method to noise in the training data, particularly should users click in malicious ways. While we used noisy real-world data, we plan to explicitly study the effect of noise, words with two meanings, and click-spam on our approach.\nAlso, the strategies presented in Section 4.1 give equal weight to each pair of queries within a query chain. However, we suspect that there is additional information present in the position of a query within a chain, and of a click within the sequence of all clicks for each chain. In particular, it is possible that the last query and last clicks may be more informative than earlier ones.\nThirdly, exploiting the fact that it is possible to collect virtually unlimited amounts of search engine log data, we believe that the methods presented in this paper can be extended to learn personalized ranking functions. We are currently refining the Osmot search engine and will use it on the arXiv.org e-Print archive 5 in order to conduct such experiments.\nFinally, from a practical perspective our approach pushes the limit of problems that current SVM implementations can solve in reasonable time due to the number of constraints we generate. We believe there is room for improvement in learning methods to efficiently deal with such large numbers of constraints, for example using an incremental optimization approach. Perhaps there are also alternative learning methods, rather than SVMs, that can be used to optimize over preference constraints while being able to learn sufficiently general ranking functions.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "ACKNOWLEDGMENTS", "text": "We would first like to thank three undergraduate and Masters students, Charitha Tillekeratne, Alex Cheng and Atit Patel for working on an initial implementation of the CUL search engine. We also thank our colleagues Laura Granka, Bing Pang, Helene Hembrooke and Geri Gay for their collaboration in the eye tracking study. We thank Paul Houle for his help and support regarding the full text search engine for the Cornell University Library web pages, and Simeon Warner and Paul Ginsparg for interesting discussions on this topic. We also thank the subjects of the eye tracking study and the five relevance judges. This work was funded under NSF CAREER Award IIS-0237381 and the KD-D grant.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Learning to retrieve information", "journal": "", "year": "1995", "authors": "B Bartell; G W Cottrell"}, {"ref_id": "b1", "title": "Automatic combination of multiple ranked retrieval systems", "journal": "", "year": "1994", "authors": "B T Bartell; G W Cottrell; R K Belew"}, {"ref_id": "b2", "title": "Agglomerative clustering of a search engine query log", "journal": "", "year": "2000", "authors": "D Beeferman; A Berger"}, {"ref_id": "b3", "title": "A machine learning architecture for optimizing web search engines", "journal": "", "year": "1996-08", "authors": "J Boyan; D Freitag; T Joachims"}, {"ref_id": "b4", "title": "A taxonomy of web search", "journal": "SIGIR Forum", "year": "2002", "authors": "A Broder"}, {"ref_id": "b5", "title": "Learning to order things", "journal": "Journal of Artificial Intelligence Research", "year": "1999", "authors": "W W Cohen; R E Shapire; Y Singer"}, {"ref_id": "b6", "title": "Pranking with ranking", "journal": "", "year": "2001", "authors": "K Crammer; Y Singer"}, {"ref_id": "b7", "title": "Spelling correction as an iterative process that exploits the collective knowledge of web users", "journal": "", "year": "2004", "authors": "S Cucerzan; E Brill"}, {"ref_id": "b8", "title": "An efficient boosting algorithm for combining preferences", "journal": "", "year": "1998", "authors": "Y Freund; R Iyer; R E Schapire; Y Singer"}, {"ref_id": "b9", "title": "Experience with an adaptive indexing scheme", "journal": "", "year": "1985", "authors": "G W Furnas"}, {"ref_id": "b10", "title": "Eye tracking analysis of user behaviors in online search", "journal": "", "year": "2004", "authors": "L Granka"}, {"ref_id": "b11", "title": "Eye-tracking analysis of user behavior in www search", "journal": "", "year": "2004", "authors": "L Granka; T Joachims; G Gay"}, {"ref_id": "b12", "title": "Large margin rank boundaries for ordinal regression", "journal": "", "year": "2000", "authors": "R Herbrich; T Graepel; K Obermayer"}, {"ref_id": "b13", "title": "Making large-scale SVM learning practical", "journal": "MIT Press", "year": "1999", "authors": "T Joachims"}, {"ref_id": "b14", "title": "Optimizing search engines using clickthrough data", "journal": "ACM", "year": "2002", "authors": "T Joachims"}, {"ref_id": "b15", "title": "Evaluating retrieval performance using clickthrough data", "journal": "Physica/Springer Verlag", "year": "2003", "authors": "T Joachims"}, {"ref_id": "b16", "title": "Accurately interpreting clickthrough data as implicit feedback", "journal": "", "year": "2005", "authors": "T Joachims; L Granka; B Pang; H Hembrooke; G Gay"}, {"ref_id": "b17", "title": "Query word deletion prediction", "journal": "", "year": "2003", "authors": "R Jones; D C Fain"}, {"ref_id": "b18", "title": "Long-term learning for web search engines", "journal": "", "year": "2003", "authors": "C Kemp; K Ramamohanarao"}, {"ref_id": "b19", "title": "Patterns of search: Analyzing and modelling web query refinement", "journal": "", "year": "1999", "authors": "T Lau; E Horvitz"}, {"ref_id": "b20", "title": "Expert agreement and content based reranking in a meta search environment using Mearf", "journal": "", "year": "2002", "authors": "B U Oztekin; G Karypis; V Kumar"}, {"ref_id": "b21", "title": "Classification approach towards ranking and sorting problems", "journal": "", "year": "2003-09", "authors": "S Rajaram; A Garg; Z S Zhou; T S Huang"}, {"ref_id": "b22", "title": "Query association for effective retrieval", "journal": "", "year": "2002", "authors": "F Scholer; H E Williams"}, {"ref_id": "b23", "title": "Analysis of a very large AltaVista query log", "journal": "Digital SRC", "year": "1998", "authors": "C Silverstein; M Henzinger; H Marais; M Moricz"}, {"ref_id": "b24", "title": "Applying co-training to clickthrough data for search engine adaptation", "journal": "", "year": "2004", "authors": "Q Tan; X Chai; W Ng; D.-L Lee"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Percentage of time an abstract was viewed/clicked on depending on the rank of the result.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Mean number of abstracts viewed above and below a clicked link depending on its rank.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Two example queries and result sets.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure4: Feedback strategies. We either consider a single query, q, or a query q that has been preceded by a query q \u2032 . Given a query, a dot represents a result document and an x indicates the result was clicked on. We generate a constraint for each arrow shown, with respect to the query marked.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "3http://www.nutch.org/ CosineDistance(q1, q2) CosineDistance(doc ids of r1', doc ids of r2') CosineDistance(abstracts of r1', abstracts of r2') TrigramMatch(q1, q2) ShareOneWord(q1, q2) ShareTwoWords(q1, q2) SharePhraseOfTwoWords(q1, q2) NumberOfDifferentWords(q1, q2) t2 \u2212 t1 \u2264 {5, 10, 30, 100} seconds t2 \u2212 t1 > 100 seconds NormalizedNumberOfClicks(r1) NormalizedMin(|r1|, |r2|) NormalizedMax(|r1|, |r2|)", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Query 1: NDLF 1. http://.../staffweb/SMG/SMG970319.html 2. http://.../staffweb/SMG/SMG970226.html 3. http://.../staffweb/SMG/SMG960417.html 4. http://.../staffweb/SMG/SMG960403.html 5. http://.../staffweb/SMG/SMG960828.html", "figure_data": "Query 2: \"Ezra Cornell\" residence1. Dear Uncle Ezra -Questions for Tuesday, May. . .2. Dear Uncle Ezra -Questions for Thursday,. . .3. Ezra Cornell had close Albion ties4. October 1904 -Albion 100 Years Age5. Cornell competes with Off-Housing market. . ."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Accuracy Click >q Skip Above 78.2 \u00b1 5.6 Click First >q No-Click Second 63.4 \u00b1 16.5 Click >q Skip Earlier Query 68.0 \u00b1 8.4 Click >q Top Two Earlier Query 84.5 \u00b1 6.1 Inter-Judge Agreement 86.4", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Accuracy of the strategies for generating pairwise preferences from clicks. The base of comparison are the explicit page judgments. Note that the first two cases cover two preferences strategies each.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "The most common words to appear in queries in the training data, and the fraction of queries in which they occur.", "figure_data": "WordFraction of queriesof3.56 %library2.75 %bibliography2.60 %and2.55 %annotated2.42 %reserve2.32 %citation1.99 %web1.48 %the1.41 %course1.33 %WordDocumentWeightlexusLexis-Nexis Academic Universe22.8ebookCUL eContent Collection22.5reuleauxCUL Digital Collections21.8andPrintable News and Notes 07/0319.6oedDictionaries and Encyclopedias19.5ndlfManagement meeting notes 03/97-21.0ndlfManagement meeting notes 02/97-20.6ndlfManagement meeting notes 04/96-19.5ndlfManagement meeting notes 04/96-18.6instruction Library Research Workshops-18.3"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Five most positive and most negative feature weights in the ranking function learned using query chains on the Cornell University Library (CUL) search engine queries. Moreover, for many popular queries, there appear to be only a few documents that are truly relevant to the query. Hence it is not surprising that by learning individual query word/document associations we can see significant improvements in ranking results.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "rel(di, q) = w \u2022 \u03a6(di, q)(2)", "formula_coordinates": [6.0, 127.68, 328.31, 165.23, 8.97]}, {"formula_id": "formula_1", "formula_text": "w \u2022 \u03a6(di, q) > w \u2022 \u03a6(dj , q)", "formula_coordinates": [6.0, 123.0, 505.68, 100.67, 8.97]}, {"formula_id": "formula_2", "formula_text": "w \u2022 \u03a6(di, q) \u2265 w \u2022 \u03a6(dj , q) + 1 \u2212 \u03beij", "formula_coordinates": [6.0, 104.16, 568.56, 137.58, 8.97]}, {"formula_id": "formula_3", "formula_text": "min w,\u03be ij 1 2 w \u2022 w + C ij \u03beij subject to \u2200(q, i, j) : w \u2022 \u03a6(di, q) \u2265 w \u2022 \u03a6(dj, q) + 1 \u2212 \u03beij \u2200i, j : \u03beij \u2265 0 (3)", "formula_coordinates": [6.0, 70.68, 640.4, 222.23, 42.48]}, {"formula_id": "formula_4", "formula_text": "\u03a6(d, q) = \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 \u03c6 f 1 rank (d, q) . . . \u03c6 f F rank (d, q) \u03c6terms(d, q) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb \u03c6 f rank (d, q) = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 1(Rank(d in r f 0 (d, q)) \u2264 1) . . . 1(Rank(d in r f 0 (q)) \u2264 10) 1(Rank(d in r f 0 (q)) \u2264 15) . . . 1(Rank(d in r f 0 (q)) \u2264 100) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb \u03c6terms(d, q) = \uf8ee \uf8ef \uf8f0 1(d = d1 \u2227 t1 \u2208 q) . . . 1(d = dM \u2227 tN \u2208 q) \uf8f9 \uf8fa \uf8fb", "formula_coordinates": [6.0, 340.92, 285.11, 190.78, 192.91]}, {"formula_id": "formula_5", "formula_text": "\u2200i \u2208 [1, 28|F |]. w i \u2265 wmin(4)", "formula_coordinates": [7.0, 118.44, 645.83, 174.47, 10.77]}, {"formula_id": "formula_6", "formula_text": "ranking r d1 d2 d3 d4 ranking r \u2032 d2 d5 d1 d6 \u21d2 combined(r, r \u2032 ) d1 d2 d5 d3 d4 f6", "formula_coordinates": [7.0, 368.04, 61.72, 136.67, 113.86]}, {"formula_id": "formula_7", "formula_text": "\u03c6 f rank (d100, q) = [0, . . . , 0, 0, 1] T \u03c6 f rank (d95, q) = [0, . . . , 0, 1, 1] T \u2022 \u2022 \u2022 \u03c6 f rank (d1, q) = [1, . . . , 1, 1, 1]", "formula_coordinates": [7.0, 357.36, 269.15, 156.41, 54.15]}, {"formula_id": "formula_8", "formula_text": "w rank \u2022 \u03c6 f rank (d100, q) \u2265 wmin w rank \u2022 \u03c6 f rank (d95, q) \u2265 2wmin \u2022 \u2022 \u2022 w rank \u2022 \u03c6 f rank (d1, q) \u2265 28wmin", "formula_coordinates": [7.0, 363.24, 353.99, 145.61, 54.15]}, {"formula_id": "formula_9", "formula_text": "wterms \u2022 \u03c6terms(d, q) \u2265 28wmin + wterms \u2022 \u03c6terms(d1, q) w d,q term \u2265 28wmin + w d 1 ,q terms where w \u03b1,\u03b2", "formula_coordinates": [7.0, 319.92, 474.24, 232.07, 39.93]}], "doi": ""}