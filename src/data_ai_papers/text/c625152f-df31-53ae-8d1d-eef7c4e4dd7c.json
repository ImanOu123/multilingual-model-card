{"title": "Learning from eXtreme Bandit Feedback", "authors": "Romain Lopez; Inderjit S Dhillon; Michael I Jordan", "pub_date": "2021-02-22", "abstract": "We study the problem of batch learning from bandit feedback in the setting of extremely large action spaces. Learning from extreme bandit feedback is ubiquitous in recommendation systems, in which billions of decisions are made over sets consisting of millions of choices in a single day, yielding massive observational data. In these large-scale realworld applications, supervised learning frameworks such as eXtreme Multi-label Classification (XMC) are widely used despite the fact that they incur significant biases due to the mismatch between bandit feedback and supervised labels. Such biases can be mitigated by importance sampling techniques, but these techniques suffer from impractical variance when dealing with a large number of actions. In this paper, we introduce a selective importance sampling estimator (sIS) that operates in a significantly more favorable bias-variance regime. The sIS estimator is obtained by performing importance sampling on the conditional expectation of the reward with respect to a small subset of actions for each instance (a form of Rao-Blackwellization). We employ this estimator in a novel algorithmic procedure-named Policy Optimization for eXtreme Models (POXM)-for learning from bandit feedback on XMC tasks. In POXM, the selected actions for the sIS estimator are the top-p actions of the logging policy, where p is adjusted from the data and is significantly smaller than the size of the action space. We use a supervised-tobandit conversion on three XMC datasets to benchmark our POXM method against three competing methods: BanditNet, a previously applied partial matching pruning strategy, and a supervised learning baseline. Whereas BanditNet sometimes improves marginally over the logging policy, our experiments show that POXM systematically and significantly improves over all baselines.", "sections": [{"heading": "Introduction", "text": "In the classical supervised learning paradigm, it is assumed that every data point is accompanied by a label. Such labels provide a very strong notion of feedback, where the learner is able to assess not only the loss associated with the action that they have chosen but can also assess losses of actions that they did not choose. A useful weakening of this paradigm involves considering so-called \"bandit feedback,\" where the training data simply provides evaluations of selected actions without delineating the correct action. Bandit Copyright \u00a9 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nfeedback is often viewed as the province of reinforcement learning, but it is also possible to combine bandit feedback with supervised learning by considering a batch setting in which each data point is accompanied by an evaluation and there is no temporal component. This is the Batch Learning from Bandit Feedback (BLBF) problem (Swaminathan and Joachims 2015a).\nOf particular interest is the off-policy setting where the training data is provided by a logging policy, which differs from the learner's policy and differs from the optimal policy. Such problems arise in many real-world problems, including supply chains, online markets, and recommendation systems (Rahul, Dahiya, and Singh 2019), where abundant data is available in a logged format but not in a classical supervised learning format.\nAnother difficulty with the classical notion of a \"label\" is that real-world problems often involve huge action spaces. This is the case, for example, in real-world recommendation systems where there may be billions of products and hundreds of millions of consumers. Not only is the cardinality of the action space challenging both from a computational point of view and a statistical point of view, but even the semantics of the labels can become obscure-it can be difficult to place an item conceptually in one and only category. Such challenges have motivated the development of eXtreme multi-label classification (XMC) and eXtreme Regression (XR) (Bhatia et al. 2016) methods, which focus on computational scalability issues and target settings involving millions of labels. These methods have had real-world applications in domains such as e-commerce (Agrawal et al. 2013) and dynamic search advertising (Prabhu et al. 2018(Prabhu et al. , 2020.\nWe assert that the issues of bandit feedback and extremescale action spaces are related. Indeed, it is when action spaces are large that it is particularly likely that feedback will only be partial. Moreover, large action spaces tend to support multiple tasks and grow in size and scope over time, making it likely that available data will be in the form of a logging policy and not a single target input-output mapping.\nWe also note that the standard methodology for accommodating the difference between the logging policy and an optimal policy needs to be considered carefully in the setting of large action spaces. Indeed, the standard methodology is some form of importance sampling (Swaminathan and Joachims 2015a), and importance sampling estimators can run aground when their variance is too high (see, e.g., Lefortier et al. (2016)). Such variance is likely to be particularly virulent in large action spaces. Some examples of the XMC framework do treat labels as subject to random variation (Jain, Prabhu, and Varma 2016), but only with the goal of improving the prediction of rare labels; they do not tackle the broader problem of learning from logging policies in extreme-scale action spaces. It is precisely this broader problem that is our focus in the current paper.\nThe literature on offline policy learning in Reinforcement Learning (RL) has also been concerned with correcting for implicit feedback bias (see, e.g., Degris, White, and Sutton (2012)). This line of work differs from ours, however, in that the focus in RL is not on extremely-large action spaces, and RL is often based on simulators rather than logging policies (Chen et al. 2019c;Bai, Guan, and Wang 2019). Closest to our work is the work of Chen et al. (2019b), who propose to use offline policy gradients on a large action space (millions of items). Their method relies, however, on a proprietary action embedding, unavailable to us.\nAfter a brief overview of BLBF and XMC , we present a new form of BLBF that blends bandit feedback with multilabel classification. We introduce a novel assumption, specific to the XMC setting, in which most actions are irrelevant (i.e., incur a null reward) for a particular instance. This motivates a Rao-Blackwellized (Casella and Robert 1996) estimator of the policy value for which only a small set of relevant actions per instance are considered. We refer to this approach as selective importance sampling (sIPS). We provide a theoretical analysis of the bias-variance tradeoff of the sIPS estimator compared to naive importance sampling. In practice, the selected actions for the sIPS estimator are the top-p actions from the logging policy, where p can be adjusted from the data. We derive a novel learning method based on the sIPS estimator, which we refer to as Policy Optimizer for eXtreme Models (POXM). Finally, we propose a modification of a state-of-the-art neural XMC method AttentionXML (You et al. 2019b) to learn from bandit feedback. Using a supervised-learning-to-bandit conversion (Dudik, Langford, and Li 2011), we benchmark POXM against BanditNet (Joachims, Swaminathan, and de Rijke 2018), a partial matching scheme from Wang et al. (2016) and a supervised learning baseline on three XMC datasets (EUR-Lex, Wiki10-31K and Amazon-670K) (Bhatia et al. 2016). We show that naive application of the state-of-theart method BanditNet (Joachims, Swaminathan, and de Rijke 2018) sometimes improves over the logging policy, but only marginally. Conversely, POXM provides substantial improvement over the logging policy as well as supervised learning baselines.", "publication_ref": ["b39", "b7", "b1", "b35", "b36", "b39", "b27", "b18", "b14", "b13", "b6", "b12", "b9", "b50", "b15", "b20", "b44", "b7", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Background eXtreme Multi-label Classification (XMC)", "text": "Multi-label classification aims at assigning a relevant subset Y \u2282 [L] to an instance x, where [L] \u2236= {1, . . . , L} denotes the set of L possible labels. XMC is a specific case of multi-label classification in which we further assume that all Y are small subsets of a massive collection (i.e., generally Y L < 0.01). Naive one-versus-all approaches to multi-label classification usually do not scale to such a large number of labels and adhoc methods are often employed. Furthermore, the marginal distribution of labels across all instances exhibits a long tail, which causes additional statistical challenges.\nAlgorithmic approaches to XMC include optimized oneversus-all methods Sch\u00f6lkopf 2017, 2019;Yen et al. 2017Yen et al. , 2016, embedding-based methods (Bhatia et al. 2015;Tagami 2017;Guo et al. 2019), probabilistic label tree-based (Prabhu et al. 2018;Jasinska et al. 2016;Khandagale, Xiao, and Babbar 2020;Wydmuch et al. 2018) and deep learning-based methods (You et al. 2019b;Liu et al. 2017;You et al. 2019a;Chang et al. 2019). Each algorithm usually proposes a specific approach to model the text as well as deal with tail labels. For example, Babbar and Sch\u00f6lkopf (2019) uses a robust SVM approach on TF-IDF features. PfastreXML (Jain, Prabhu, and Varma 2016) assumes a particular noise model for the observed labels and proposes to weight the importance of tail labels. Atten-tionXML (You et al. 2019b) uses a bidirectional-LSTM to embed the raw text as well as a multi-label attention mechanism to help capture the most relevant part of the input text for each label. For datasets with large L, AttentionXML trains one model per layer of a shallow and wide probabilistic latent tree using a small set of candidate labels.", "publication_ref": ["b47", "b48", "b8", "b42", "b17", "b35", "b19", "b23", "b46", "b50", "b30", "b49", "b10", "b5", "b18", "b50"], "figure_ref": [], "table_ref": []}, {"heading": "Batch Learning from Bandit Feedback (BLBF)", "text": "We assume that the instance x is sampled from a distribution P(x). The action for this particular instance is a unique label y \u2208 [L], sampled from the logging policy \u03c1(y x) and a feedback value r \u2208 R is observed. Repeating this data collection process yields the dataset [(x i , y i , r i )] n i=1 . The BLBF problem consists in maximizing the expected reward V (\u03c0) of a policy \u03c0. We use importance sampling (IS) to estimate V (\u03c0) from data based on the logging policy as follows:\nV IS (\u03c0) = 1 n n i=1 \u03c0(y i x i ) \u03c1(y i x i ) r i .(1)\nClassically, identifying the optimal policy via this estimator is infeasible without a thorough exploration of the action space by the logging policy (Langford, Strehl, and Wortman 2008). More specifically, the IS estimatorV IS (\u03c0) requires the following basic assumption for there to be any hope of asymptotic optimality: Assumption 1. There exists a scalar > 0 such that for all x \u2208 R d and y \u2208 [L], \u03c1(y x) > .\nThe IS estimator has high variance when \u03c0 assigns actions that are infrequent in \u03c1; hence a variety of regularization schemes have been developed, based on riskupper-bound minimization, to control variance. Examples of upper bounds include empirical Bernstein concentration bounds (Swaminathan and Joachims 2015a) and various divergence-based bounds (Atan, Zame, and Mihaela Van Der Schaar 2018;Wu and Wang 2018;Johansson, Shalit, and Sontag 2016;Lopez et al. 2020). Another common strategy for reducing the variance is to propose a model of the reward function, using as a baseline a doubly robust estimator (Dudik, Langford, and Li 2011;Su et al. 2019).\nA recurrent issue with BLBF is that the policy may avoid actions in the training set when the rewards are not scaled properly; this is the phenomenon of propensity overfitting. Swaminathan and Joachims (2015b) tackled this problem via the self-normalized importance sampling estimator (SNIS), in which IS estimates are normalized by the average importance weight. SNIS is invariant to translation of the rewards and may be used as a safeguard against propensity overfitting. BanditNet (Joachims, Swaminathan, and de Rijke 2018) made this approach amenable to stochastic optimization by translating the reward distribution:\nV BN (\u03c0) = 1 n n i=1 \u03c0(y i x i ) \u03c1(y i x i ) [r i \u2212 \u03bb] ,(2)\nand selecting \u03bb over a small grid based on the SNIS estimate of the policy value.\nLearning an XMC algorithm from bandit feedback requires offline learning from slates Y , where each element of the slate comes from a large action space. Swaminathan et al. (2017) proposes a pseudo-inverse estimator for offline learning from combinatorial bandits. However, such an approach is intractable for large action spaces as it requires inverting a matrix whose size is linear in the number of actions. Another line of work focuses on offline evaluation and learning of semi-bandits for ranking (Li et al. 2018;Joachims, Swaminathan, and Schnabel 2017) but only with a small number of actions. In real-world data, a partial matching strategy between instances and relevant actions is applied in applications to internet marketing for policy evaluation (Wang et al. 2016;Li, Kim, and Zitouni 2015). More recently, Chen et al. (2019b) proposed a top-k off-policy correction method for a real-world recommender system. Their approach deals with millions of actions although it treats label embeddings as given, whereas this problem is in general a hard problem for XMC.", "publication_ref": ["b26", "b39", "b45", "b22", "b31", "b15", "b38", "b40", "b20", "b41", "b29", "b21", "b44", "b28", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Bandit Feedback and Multi-label Classification", "text": "We consider a setting in which the algorithm (e.g., a policy for a recommendation system) observes side information x \u2208 R d and is allowed to output a subset Y \u2286 [L] of the L possible labels. Side information is independent at each round and sampled from a distribution P(x). We assume that the subset Y has fixed size Y = , which allows us to adopt the slate notation Y = (y 1 , . . . , y ). The algorithm observes noisy feedback for each label, R = (r 1 , . . . , r ), and we further assume that the joint distribution over R decomposes as P (R x, Y ) = \u220f j=1 P (r j x, y j ) . We will denote the conditional reward distribution as a function:\n\u03b4(x, y) = E[r x, y].\nIn the case of multi-label classification, this feedback can be formed with random variables indicating whether each individual label is inside the true set of labels for each datapoint (Gentile and Orabona 2014). More concretely, feedback may be formed from sale or click information (Chen et al. 2019b,c).\nWe are interested in optimizing a policy \u03c0(Y x) from offline data. Accessible data is sampled according to an existing algorithm, the logging policy \u03c1(Y x). We assume that both joint distributions over the slate decompose into an auto-regressive process. For example, for \u03c0 we assume:\n\u03c0(Y x) = j=1 \u03c0(y j x, y 1\u2236j\u22121 ).\n(3)\nIntroducing this decomposition does not result in any loss of generality, as long as the action order is identifiable (otherwise, one would need to consider all possible orderings (Kool, van Hoof, and Welling 2020b)). This is a reasonable hypothesis because the order of the actions may also be logged as supplementary information. We now define the value of a policy \u03c0 as:\nV (\u03c0) = E P(x) E \u03c0(Y x) E 1 \u22ba R x, Y .(4)\nIn our setting, the reward decomposes as a sum of independent contributions of each individual action. The reward may in principle be generalized to be rank dependent, or to consider interactions between items (Gentile and Orabona 2014), but this is beyond the scope of this work.\nA general approach for offline policy learning is to estimate V (\u03c0) from logged data using importance sampling (Swaminathan and Joachims 2015a). As emphasized in Swaminathan et al. (2017), the combinatorial size of the action space \u2126(L ) may yield an impractical variance for importance sampling. This is particularly the case for XMC, where typical values of L are minimally in the thousands. A natural strategy to improve over the IS estimator on the slate Y is to exploit the additive reward decomposition in Eq. (4). Along with the factorization of the policy in Eq. (3), we may reformulate the policy value as:\nV (\u03c0) = E P(x)\nj=1 E \u03c0(y1\u2236j x) \u03b4(x, y j ).\n(\n)5\nThe benefit of this new decomposition is that instead of performing importance sampling on Y , we can now use IS estimators, each with a better bias-variance tradeoff. Unbiased estimation of V (\u03c0) in Eq. (5) via importance sampling still requires Assumption 1. The logging policy must therefore explore a large action space. However, most actions are unrelated to a given context and deploying an online logging policy that satisfies Assumption 1 may yield a poor customer experience.", "publication_ref": ["b16", "b25", "b39", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Learning from eXtreme Bandit Feedback", "text": "We now explore alternative assumptions for the logging policy that may be more suitable to the setting of very large action spaces. We formalize the notion that most actions are irrelevant using the following assumption: We refer to the function \u03a8 as an action selector, as it maps a context to a set of relevant actions. Throughout the manuscript, we use the notation \u039b 0 to refer to the pointwise set complement of any action selector \u039b. Intuitively, we are interested in the case where \u03a8(x) \u226a L for all x. Assumption 2 is implicitly used in online marketing applications of offline policy evaluation, formulated as a partial matching between actions and instances (Wang et al. 2016;Li, Kim, and Zitouni 2015). Notably, this assumption can be assimilated to a mixed-bandit feedback setting, where we observe feedback for all of \u03a8 0 (x) but only one selected action inside of \u03a8(x). Under Assumption 2, the IS estimator will be unbiased for all logging policies that satisfy the following relaxed assumption: Assumption 3. (\u03a8-overlap condition). There exists a scalar > 0 such that for all x \u2208 R d and y \u2208 \u03a8(x), \u03c1(y x) > .\nBatch learning from bandit feedback may be possible under this assumption, as long as the logging policy explores a set of actions large enough to cover the actions from \u03a8 but small enough to avoid exploring too many suboptimal actions. Furthermore, Assumption 2 also reveals the existence of \u03a8 0 (x), a sufficient statistic for estimating the reward on the irrelevant actions. Making appeal to Rao-Blackwellization (Casella and Robert 1996), we can incorporate this information to estimate each of the terms of Eq. (5) (e.g., in the case = 1 and \u2207 = 0):\nV (\u03c0) = E P(x) \u03c0 (\u03a8(x) x) \u22c5 E \u03c0(y x) [\u03b4(x, y) y \u2208 \u03a8(x)] .\n(6) The decomposition in Eq. ( 6) suggests that when the action selector \u03a8 is known, one can estimate V (\u03c0) via importance sampling for the conditional expectation of the rewards with respect to the event {y \u2208 \u03a8(x)}. Intuitively, this means that one can modify the importance sampling scheme to only include a relevant subset of labels and ignore all the others. Without loss of generality, we assume that \u2207 = 0 in the remainder of this manuscript.\nIn practice, the oracle action selector \u03a8 is unknown and needs to be estimated from the data. It may be hard to infer the smallest \u03a8 such that Assumption 2 is satisfied. Conversely, a trivial action selector including all actions is valid (it does include all relevant actions) but is ultimately unpractical. As a flexible compromise, we will replace \u03a8 in Eq. (6) by any action selector \u03a6 and study the bias-variance tradeoff of the resulting plugin estimator.\nLet \u03c1 be a logging policy with a large enough support to satisfy Assumption 3. Let \u03a6 be an action selector such that \u03a6(x) \u2282 supp \u03c1(\u22c5 x) almost surely in x, where supp denotes the support of a probability distribution. The role of \u03a6 is to prune out actions to maintain an optimal bias-variance tradeoff. In the case = 1, the \u03a6-selective importance sampling (sIS) estimatorV \u03a6 sIS (\u03c0) for action selection \u03a6 can be written as:\nV \u03a6 sIS (\u03c0) = 1 n n i=1 \u03c0(y i x i , y \u2208 \u03a6(x)) \u03c1(y i x i ) r i .(7)\nIts bias and variance depends on how different the policy \u03c0 is from the logging policy \u03c1 (as in classical BLBF) but also on the degree of overlap of \u03a6 with \u03a8:\nTheorem 1 (Bias-variance tradeoff of selective importance sampling). Let R and \u03c1 satisfy Assumptions 2 and 3. Let \u03a6 be an action selector such that \u03a6(x) \u2282 supp \u03c1(\u22c5 x) almost surely in x. The bias of the sIS estimator is:\nEV \u03a6 sIS (\u03c0) \u2212 V (\u03c0) \u2264 \u2206\u03ba(\u03c0, \u03a8, \u03a6),(8)\nwhere \u03ba(\u03c0, \u03a8, \u03a6) = E P(x) \u03c0 \u03a8(x) \u2229 \u03a6 0 (x) x quantifies the overlap between the oracle action selector \u03a8 and the proposed action selector \u03a6, weighted by the policy \u03c0. The performance of the two estimators can be compared as follows:\nMSE V \u03a6 sIS (\u03c0) \u2264 MSE V IS (\u03c0) + 2\u2206 2 \u03ba(\u03c0, \u03a8, \u03a6) (9) \u2212 \u03c3 2 n E P(x) \u03c0 2 \u03a6 0 (x) x \u03c1 (\u03a6 0 (x) x) ,(10)\nwhere\n\u03c3 2 = inf x,y\u2208R d \u00d7[K] E[r 2 x, y].\nWe provide the complete proof of this theorem in the appendix. As expected by Rao-Blackellization, we see that if \u03a6 completely covers \u03a8 (i.e., for all\nx \u2208 R d , \u03a8(x) \u2282 \u03a6(x)), thenV \u03a6\nsIS (\u03c0) is unbiased and has more favorable performance thanV IS (\u03c0). Admittedly, Eq. (10) shows that both estimators have similar mean-square error when \u03c0 puts no mass on potentially irrelevant actions y \u2208 \u03a6 0 (x). However, during the process of learning the optimal policy or in the event of propensity overfitting, we expect \u03c0 to put a non-zero mass on potentially irrelevant actions y \u2208 \u03a6 0 (x), with positive probability in x. For these reasons, we expectV \u03a6 sIS (\u03c0) to provide significant improvement overV IS (\u03c0) for policy learning.\nEven though Eq. (10) provides insight into the performance of sIS, unfortunately it cannot be used directly in selecting \u03a6. We instead propose a greedy heuristic to select a small number of action selectors. For example, \u03a6 p (x) corresponds to the top-p labels for instance x according to the logging policy. With this approach, the bias of theV \u03a6 sIS (\u03c0) estimator is a decreasing function of p, as the overlap with \u03a8 increases. Furthermore, the variance increases with p as long as the added actions are irrelevant. In practice, we use a small grid search for p \u2208 {10, 20, 50, 100} and choose the optimal p with the SNIS estimator, as in BanditNet. We believe this is a reasonable approach whenever the logging policy ranks the relevant items sufficiently high but can be improved (e.g., top-p for p in 5 to 100).", "publication_ref": ["b44", "b28", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Policy Optimization for eXtreme Models", "text": "We apply sIS for each of the terms of the policy value from Eq. (5) in order to estimate V (\u03c0) from bandit feedback,\n(x i , Y i , R i ) n\ni=1 , and learn an optimal policy. As an additional step to reduce the variance, we prune the importance sampling weights of earlier slate actions, following Achiam et al. (2017):\nV \u03a6 sIS (\u03c0) = 1 n n i=1 j=1 \u03c0 \u03a6 (y i,j x i , y 1\u2236j\u22121 ) \u03c1(y i,j x i , y 1\u2236j\u22121 ) r i,j ,(11)\nwhere \u03c0 \u03a6 designates the distribution \u03c0 restricted to the set \u03a6(x) for every x. Because \u03c0(Y x) is a copula, one can derive all joint distributions starting from the corresponding one-dimensional marginals (Sklar 1959). In this work, we focus on the case of ordered sampling without replacement to respect an important design restriction: the slate Y must not have redundant actions. For the j-th slate component, the relevant conditional probability is formed from the base marginal probabilities \u03c0(y x) as follows:\n\u03c0 \u03a6 (y j x, y 1\u2236j\u22121 ) = \u03c0(y j x) y \u2032 \u2208\u03a6(x) \u03c0(y \u2032 x) \u2212 k<j \u03c0(y k x) . (12\n)\nFrom a computational perspective, the action selector also diminishes the computational burden, leading to efficient computations of the probabilities when the marginals are parameterized by a softmax distribution. Indeed, Eq. ( 12) depends only on the logits for the actions inside of the set \u03a6. This helps our approach to scale to large XMC datasets.\nAs mentioned in the background section, directly maximizing the importance sampling estimate of the policy value in Eq. (11) may be pathological due to propensity overfitting. The BanditNet approach may be adapted to the slate case using a different loss translation scheme for each element:\nV \u03a6 sIS (\u03c0) = 1 n n i=1 j=1 \u03c0 \u03a6 (y i,j x i , y 1\u2236j\u22121 ) \u03c1(y i,j x i , y 1\u2236j\u22121 ) [r i,j \u2212 \u03bb j ], (13\n)\nand with (\u03bb 1 , . . . , \u03bb ) selected out of a small grid based on the self-normalized importance sampling estimate of the policy value from the training data (Joachims, Swaminathan, and de Rijke 2018). For computational reasons, we only search for a unique \u03bb and, following Joachims, Swaminathan, and de Rijke ( 2018), we focus on the grid {0.7, 0.8, 0.9, 1.0}. We refer to this approach as Policy Optimization for eXtreme Models (POXM), named after the seminal algorithm from Swaminathan and Joachims (2015a).", "publication_ref": ["b0", "b37", "b20", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We evaluate our approach on real-world datasets with a supervised learning to bandit feedback conversion (Dudik, Langford, and Li 2011;Gentile and Orabona 2014). We report results on three datasets from the Extreme Classification Repository (Bhatia et al. 2016), with L ranging from several thousand to half a million (Table 1). EUR-Lex (Mencia and F\u00fcrnkranz 2008) has a relatively small label set and each instance has a sparse label set. Wiki10-31K (Zubiaga 2012) has a larger label set as well as more abundant annotations. ", "publication_ref": ["b15", "b16", "b7", "b33", "b51"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Simulating Bandit Feedback from XMC datasets", "text": "An XMC dataset is a collection of observations\n(x i , Y * i ) n i=1\nfor which each instance x i is associated with an optimal set of labels Y * i . To form a logging policy \u03c1, we train Atten-tionXML on a small fraction \u03b1 of the dataset to get estimates of the marginal probability for each label (values are provided in the appendix). These probabilities must be normalized in order to sum to one, as expected in the multi-label setting (Wydmuch et al. 2018). The ground-truth labels may be used to investigate whether \u03a6 p (the top p actions from \u03c1) approximately satisfies the \u03a8-covering condition. On the EUR-LeX dataset the obtained logging policy on its top 20 action covers around 75% of the rewards (Figure 1). Using more actions may be suboptimal as these may add variance with only a marginal benefit on the bias, as captured by Theorem 1. Finally, we form bandit feedback for slates of size by sampling without replacement from \u03c1. The reward is a binary variable depending on whether the chosen action belongs to the reference set Y * . We fix = 5 in all experiments.", "publication_ref": ["b46"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Evaluation metrics", "text": "P@k (Precision at k), nDCG@k (normalized Discounted Cumulative Gain at k) as well as PSP@k (Propensity Scored Precision at k) are widely used metrics for evaluating XMC methods (Jain, Prabhu, and Varma 2016;Bhatia et al. 2016). We adapt these metrics to the evaluation of stochastic policies by taking expectations of the relevant statistics over Table 2: Performance comparisons of POXM and other competing methods over the three medium-scale datasets. All experiments are conducted with bandit feedback. In italic are the results from the AttentionXML manuscript, for the full-information feedback on all the training data (the supervised learning skyline). Methods R@3 R@5 nDCR@3 nDCR@5 PSR@3 PSR@5 slates of size k (with distinct items). For example, R@k (Reward at k) is defined as:\nR@k = E \u03c0(y1,...,y k ) 1 k k l=1 1{y l \u2208 Y * }.(14)\nSimilarly, we define nDCR@k and PSR@k (analogous to nDCG@k and PSP@k). We estimate those metrics using sampling without replacement.", "publication_ref": ["b18", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Competing methods and experimental settings", "text": "We compare POXM to other offline policy learning methods in the specific context of AttentionXML (You et al. 2019b). Furthermore, hyperparameters that are specific to Atten-tionXML are fixed across all experiments (Table 2 of You et al. (2019b)) so that our results are not confounded by those choices. To reduce training time and focus on how well each method deals with large action spaces, we use the LSTM weights from AttentionXML and treat those as fixed for all experiments. Finally, we noticed that the scale of gradients of the objective function for IS-based methods was different from supervised learning methods (similarly reported in Joachims, Swaminathan, and de Rijke (2018)). Consequently, we lowered the learning rate for these algorithms from 1e\u22124 to 5e\u22125.\nWe compare POXM to several baselines. First, we report results for the Direct Method (DM), a supervised learning baseline where AttentionXML is trained with a partial classification loss, using only the feedback from actions for each instance. The deterministic policy picks the top-k actions from the predicted value, akin to Prabhu et al. (2020). Second, we use BanditNet as a baseline. For this, we train AttentionXML using gradients of Eq. (11), but without conditioning on action set \u03a6. Instead, we use the full softmax (akin to Joachims, Swaminathan, and de Rijke ( 2018)) or we approximate it with negative sampling (Mikolov et al. 2013) at training time (only for the Amazon-670K dataset). Finally, we also investigate the effect of the partial matching strategy of Wang et al. (2016); Li, Kim, and Zitouni (2015) while training with BanditNet (referred to as BanditNet-PM). In this baseline we ignore feedback from actions that are not in \u03a6 p .", "publication_ref": ["b50", "b50", "b20", "b36", "b34", "b44", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Table 2 shows the performance results of POXM and other competing methods. POXM consistently outperformed the logging policy and always significantly improved over the competing methods. As expected, the performance is lower than AttentionXML learned on the full training set. The direct method also improved over the logging policy but only marginally, which is attributable to the bias from the logging policy. BanditNet and its partial matching variant did not improve over the logging policy on both EUR-Lex and Amazon-670K. We believe this is due to the sparsity of the rewards. Indeed, BanditNet outperforms the logging policy as well as the Direct Method baseline on Wiki10-31K that has many more labels per instance. Furthermore, we see that partial matching has a positive effect on BanditNet for EUR- LeX but not for the other datasets. For all choices of logging policy in Figure 1, the optimal value of p selected by POXM is the smallest possible (p = 10). Therefore, we investigated how the algorithm behaved with more stochastic policies on the EUR-LeX dataset. For this, we injected Gumbel noise into the label probabilities (details in the appendix) and analyzed the performance of POXM for logging policies with p \u2208 {10, 20, 50}. We provide summary statistics for the three logging policies and report the results of POXM in Figure 2. We see that each logging policy has a best performing value of p (middle) that is aligned with the summary statistics of the logging policy (left) as well as the normalized importance sampling (SNIS) policy value estimate (right). This shows that POXM keeps improving over the logging policy for more stochastic policies and that SNIS is a reasonable procedure for selecting the parameter p.", "publication_ref": [], "figure_ref": ["fig_1", "fig_0"], "table_ref": []}, {"heading": "Discussion", "text": "We have presented POXM: a scalable algorithmic framework for learning XMC classifiers from bandit feedback. On real-world datasets, we have shown that POXM is systematically able to improve over the logging policy. This is not the case for the current state-of-the-art method, BanditNet. The latter does not always improve over the logging policy, which may be attributable to propensity overfitting.\nAll public datasets for eXtreme multi-label classification present the problem of imbalanced label distribution. Indeed, certain important labels (commonly referred to as tail labels), with more descriptive power, might be rarely used because of biases inherent to the data collection process. Although we do not provide a specific treatment of tail labels in this manuscript, we proposed in the appendix a simple extension of POXM (named wPOXM) based on Jain, Prabhu, and Varma (2016) to address this problem. Briefly, we extended the traditional data generating process for BLBF to treat the labels as noisy, and assumed that our observation scheme is biased towards the head labels. This leads to a slight modification of the sIS estimator and the POXM procedure to include the label propensity scores. wPOXM significantly improved over POXM for all propensity-weighted metrics, with 4.77% improvement of the PSR@3 metric. We leave more refined analyses for future work.\nAn important point in the XMC literature is computational efficiency. In this study, we used a machine with 8 GPUs Tesla K80 to run our experiments. This is mainly because our implementation relies on AttentionXML, itself implemented in PyTorch. The runtime of POXM on each dataset ranges from less than one hour for EUR-LeX to less than three hours for Amazon-670K. An important aspect of POXM's implementation is the reduced softmax computation. We verified this on the Amazon-670K dataset in which we tracked the runtime for growing size of the parameter p. For less than p \u2264 100 actions, POXM took around 3s to backpropagate through 1,000 samples. However, this runtime was multiplied by ten for p=10,000 (25s) and we could not run POXM for p \u2265 20,000 because of an out-of-memory error. An interesting research direction would be to apply this framework to other XMC algorithms.\nA performance gap remains between POXM and the skyline performance from the supervised method Atten-tionXML. It is possible that alternative parameterizations of the policy \u03c0 may further improve performance; for example, using a probabilistic latent tree for policy gradients as in Chen et al. (2019a) or using the Gumbel-Top-k trick (Kool, van Hoof, and Welling 2020a). Furthermore, doubly robust estimators (Dudik, Langford, and Li 2011;Su et al. 2019;Wang et al. 2019) may further help in incorporating prior knowledge about the reward function.", "publication_ref": ["b18", "b11", "b24", "b15", "b38", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "Proofs", "text": "Theorem 1 (Bias-variance tradeoff of selective importance sampling). Let R and \u03c1 satisfy Assumptions 2 and 3. Let \u03a6 be an action selector such that \u03a6(x) \u2282 supp \u03c1(\u22c5 x) almost surely in x. The bias of the sIS estimator is:\nEV \u03a6 sIS (\u03c0) \u2212 V (\u03c0) \u2264 \u2206\u03ba(\u03c0, \u03a8, \u03a6),(8)\nwhere \u03ba(\u03c0, \u03a8, \u03a6) = E P(x) \u03c0 \u03a8(x) \u2229 \u03a6 0 (x) x quantifies the overlap between the oracle action selector \u03a8 and the proposed action selector \u03a6, weighted by the policy \u03c0. The performance of the two estimators can be compared as follows:\nMSE V \u03a6 sIS (\u03c0) \u2264 MSE V IS (\u03c0) + 2\u2206 2 \u03ba(\u03c0, \u03a8, \u03a6) (9) \u2212 \u03c3 2 n E P(x) \u03c0 2 \u03a6 0 (x) x \u03c1 (\u03a6 0 (x) x) ,(10)\nwhere \u03c3 2 = inf x,y\u2208R d \u00d7[K] E[r 2 x, y].\nProof. We first derive an expression for the bias of the sIPS estimator under Assumption 3 for \u03c1 and Assumption 2 for \u03b4: \nEV \u03a6 sIS(\n= E P(x)\u03c0(y x) 1 {y\u2208\u03a6 0 (x)} \u03b4(x, y)\n= E P(x)\u03c0(y x) 1 {y\u2208\u03a8(x)} 1 {y\u2208\u03a6 0 (x)} \u03b4(x, y) ,\nwhere we exploited the fact that importance sampling is unbiased for each x on \u03a6(x) and that \u03b4(x, y) is zero for y \u2208 \u03a8 0 (x). By taking absolute value on both sides, we can bound the bias as follows:\nEV \u03a6 sIS (\u03c0) \u2212 V (\u03c0) \u2264 \u2206E P(x) \u03c0 \u03a8(x) \u2229 \u03a6 0 (x) x(19)\nWe now relate the mean-square error of the IPS and sIPS estimators: \nMSE V IS (\u03c0) = MSE V \u03a6 sIS (\u03c0) + E V IS (\u03c0) 2 \u2212V \u03a6 sIS (\u03c0) 2 \u2212 2V (\u03c0) V IS (\u03c0) \u2212V \u03a6 sIS (\u03c0) = MSE V \u03a6 sIS (\u03c0) + EV IS (\u03c0) 2 \u2212 EV \u03a6 sIS (\u03c0)\n\u2265 \u03c3 2 n E P(x) \u23a1 \u23a2 \u23a2 \u23a2 \u23a3 \u03c0 2 \u03a6 0 (x) x \u03c1 (\u03a6 0 (x) x) \u2206 \u03c7 2 (\u03c0(y x) \u03c1(y x)) \u23a4 \u23a5 \u23a5 \u23a5 \u23a6 ,(23)\nwhere \u2206 \u03c7 2 denotes the chi-square divergence and\u03c0 (resp.\u03c1) denote the normalized probability distribution \u03c0 (resp. \u03c1) on the set \u03a6 0 (x) for each x. The form of chi-square divergence in Equation ( 24) is greater or equal to 1. Therefore, we have that:\nEV IS (\u03c0) 2 \u2265 EV \u03a6 sIS (\u03c0) 2 + \u03c3 2 n E P(x) \u03c0 2 \u03a6 0 (x) x \u03c1 (\u03a6 0 (x) x) .(25)\nThen, using the fact that theV \u03a6 sIS (\u03c0) always underestimates V (\u03c0) and the upper bound in Equation ( 19), we get: EV \u03a6 sIS (\u03c0) \u2212 V (\u03c0) \u2265 \u2212\u2206E P(x) \u03c0 y \u2208 \u03a8(x) \u2229 \u03a6 0 (x) x (26) Put together, we recover the bound: 27) \nMSE V IS (\u03c0) \u2265 MSE V \u03a6 sIS (\u03c0) + \u03c3 2 n E P(x) \u03c0 2 \u03a6 0 (x) x \u03c1 (\u03a6 0 (x) x) \u2212 2\u2206 2 E P(x) \u03c0 \u03a8(x) \u2229 \u03a6 0 (x) x(", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We acknowledge Kush Batia, Lerrel Pinto, Adam Stooke, Sujay Sanghavi, Hsiang-Fu Yu, Arya Mazumdar and Rajat Sen for helpful conversations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data splitting and logging policy construction", "text": "We split the datasets into train / test according to the AttentionXML manuscript (also in Table 1 of our manuscript). All results are reported on the test dataset. We first run AttentionXML on a fraction \u03b1 of the training dataset. The resulting probabilistic latent tree provides us with non-normalized marginal probabilitiesp(y x) for each label y and instance x. For all experiments, we keep only those probabilities for the top 100 actions for each instance. Then, we may control the randomness over the logging policy in two complementary ways. First, we may add an iid centered Gumbel random variable g (with scaling parameter \u03b2) in order to perturb the rank of the actions: E(x, y) = logp(y x) + g. (28) We use this step only for the robustness analysis. Second, we may scale this energy by a temperature parameter T to get the re-normalized probability distribution:\nwith the convention that E(x, y) = \u2212\u221e if y is not in the top 100 action for x with respect top(y x). We report all the values of \u03b1, \u03b2 and T in Table 3.\nOnce the logging policy constructed, we train all the bandit feedback algorithms on all of the training set instances x, for which we attribute actions y sampled without replacement from \u03c1 instead of the optimal actions y * . ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Treatment of tail labels in POXM", "text": "In this section, we explain how to adapt the weighing strategy from (Jain, Prabhu, and Varma 2016) to the case of POXM. We refer to this modified algorithm as wPOXM.\nFor developing the intuition of this algorithm, we simply detail the case = 1. In this setting, the value of a policy \u03c0 can be defined as:\n) In the classification setting, r is a binary random variable indicating whether the label y is relevant for context x. Now, we extend this setting by assuming that the label set we observe is incomplete (the main assumption of (Jain, Prabhu, and Varma 2016)) and that there exists an unobserved random variable y * that denotes the complete label set.\nIn this setting, we would like to maximize the reward over the complete label set distribution:\nHowever, we only observe the data on the logging policy, so we must reweigh the samples accordingly:\nwhere p y are the propensity weights estimated from (Jain, Prabhu, and Varma 2016).\nWe therefore implemented a simple extension of POXM, named wPOXM, that follows this reweighing scheme. We benchmarked this approach against POXM in the EUR-LeX dataset and report the results in Table 4. R@3 R@5 nDCR@3 nDCR@5 PSR@3 PSR@5 PSnDCR@3 PSnDCR@5 -5.38% -5.00% -4.53% -4.38% +4.77% +4.28% +5.73% +6.51%\nAs one can see from this table, weighing by the inverse propensity score is effective, as all the weighted metrics are significantly improved. We note a lower performance for the other rewards metrics, explain by the fact that head labels get less often chosen by the policy. All in all, those results show that POXM's paradigm is flexible and can be extended to take into account the tail labels, an important aspect of eXtreme multi-label classification.", "publication_ref": ["b18", "b18", "b18"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Constrained Policy Optimization", "journal": "", "year": "2017", "authors": "J Achiam; D Held; A Tamar; P Abbeel"}, {"ref_id": "b1", "title": "Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages", "journal": "", "year": "2013", "authors": "R Agrawal; A Gupta; Y Prabhu; M Varma"}, {"ref_id": "b2", "title": "", "journal": "", "year": "", "authors": "O Atan; W R Zame; Mihaela Van Der;  Schaar"}, {"ref_id": "b3", "title": "Counterfactual Policy Optimization Using Domain-Adversarial Neural Networks", "journal": "", "year": "", "authors": ""}, {"ref_id": "b4", "title": "Dismec: Distributed sparse machines for extreme multi-label classification", "journal": "", "year": "2017", "authors": "R Babbar; B Sch\u00f6lkopf"}, {"ref_id": "b5", "title": "Data scarcity, robustness and extreme multi-label classification", "journal": "", "year": "2019", "authors": "R Babbar; B Sch\u00f6lkopf"}, {"ref_id": "b6", "title": "A Model-Based Reinforcement Learning with Adversarial Training for Online Recommendation", "journal": "", "year": "2019", "authors": "X Bai; J Guan; H Wang"}, {"ref_id": "b7", "title": "The extreme classification repository: Multi-label datasets and code", "journal": "", "year": "2016", "authors": "K Bhatia; K Dahiya; H Jain; A Mittal; Y Prabhu; M Varma"}, {"ref_id": "b8", "title": "Sparse local embeddings for extreme multi-label classification", "journal": "", "year": "2015", "authors": "K Bhatia; H Jain; P Kar; M Varma; P Jain"}, {"ref_id": "b9", "title": "Rao-Blackwellisation of sampling schemes", "journal": "Biometrika", "year": "1996", "authors": "G Casella; C P Robert"}, {"ref_id": "b10", "title": "X-BERT: eXtreme Multi-label Text Classification with using Bidirectional Encoder Representations from Transformers. arXiv", "journal": "", "year": "2019", "authors": "W.-C Chang; H.-F Yu; K Zhong; Y Yang; I Dhillon"}, {"ref_id": "b11", "title": "Large-scale interactive recommendation with tree-structured policy gradient", "journal": "", "year": "2019", "authors": "H Chen; X Dai; H Cai; W Zhang; X Wang; R Tang; Y Zhang; Y Yu"}, {"ref_id": "b12", "title": "Top-k off-policy correction for a REIN-FORCE recommender system", "journal": "", "year": "2019", "authors": "M Chen; A Beutel; P Covington; S Jain; F Belletti; E H Chi"}, {"ref_id": "b13", "title": "Generative Adversarial User Model for Reinforcement Learning Based Recommendation System", "journal": "", "year": "2019", "authors": "X Chen; S Li; H Li; S Jiang; Y Qi; L Song"}, {"ref_id": "b14", "title": "Off-policy actor-critic", "journal": "", "year": "2012", "authors": "T Degris; M White; R S Sutton"}, {"ref_id": "b15", "title": "Doubly Robust Policy Evaluation and Learning", "journal": "", "year": "2011", "authors": "M Dudik; J Langford; L Li"}, {"ref_id": "b16", "title": "On multilabel classification and ranking with bandit feedback", "journal": "The Journal of Machine Learning Research", "year": "2014", "authors": "C Gentile; F Orabona"}, {"ref_id": "b17", "title": "Breaking the Glass Ceiling for Embedding-Based Classifiers for Large Output Spaces", "journal": "", "year": "2019", "authors": "C Guo; A Mousavi; X Wu; D Holtmann-Rice; S Kale; S Reddi; S Kumar"}, {"ref_id": "b18", "title": "Extreme multilabel loss functions for recommendation, tagging, ranking & other missing label applications", "journal": "", "year": "2016", "authors": "H Jain; Y Prabhu; M Varma"}, {"ref_id": "b19", "title": "Extreme F-measure maximization using sparse probability estimates", "journal": "", "year": "2016", "authors": "K Jasinska; K Dembczynski; R Busa-Fekete; K Pfannschmidt; T Klerx; E Hullermeier"}, {"ref_id": "b20", "title": "Deep Learning with Logged Bandit Feedback", "journal": "", "year": "2018", "authors": "T Joachims; A Swaminathan; M De Rijke"}, {"ref_id": "b21", "title": "Unbiased learning-to-rank with biased feedback", "journal": "", "year": "2017", "authors": "T Joachims; A Swaminathan; T Schnabel"}, {"ref_id": "b22", "title": "Learning Representations for Counterfactual Inference", "journal": "", "year": "2016", "authors": "F Johansson; U Shalit; D Sontag"}, {"ref_id": "b23", "title": "Bonsaidiverse and shallow trees for extreme multi-label classification", "journal": "Machine Learning", "year": "2020", "authors": "S Khandagale; H Xiao; R Babbar"}, {"ref_id": "b24", "title": "Ancestral Gumbel-Top-k Sampling for Sampling Without Replacement", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "W Kool; H Van Hoof; M Welling"}, {"ref_id": "b25", "title": "Estimating gradients for discrete random variables by sampling without replacement", "journal": "", "year": "2020", "authors": "W Kool; H Van Hoof; M Welling"}, {"ref_id": "b26", "title": "Exploration scavenging", "journal": "", "year": "2008", "authors": "J Langford; A Strehl; J Wortman"}, {"ref_id": "b27", "title": "Large-scale Validation of Counterfactual Learning Methods: A Test-Bed", "journal": "", "year": "2016", "authors": "D Lefortier; A Swaminathan; X Gu; T Joachims; M De Rijke"}, {"ref_id": "b28", "title": "Toward predicting the outcome of an A/B experiment for search relevance", "journal": "", "year": "2015", "authors": "L Li; J Y Kim; I Zitouni"}, {"ref_id": "b29", "title": "Offline evaluation of ranking policies with click models", "journal": "", "year": "2018", "authors": "S Li; Y Abbasi-Yadkori; B Kveton; S Muthukrishnan; V Vinay; Z Wen"}, {"ref_id": "b30", "title": "Deep learning for extreme multi-label text classification", "journal": "", "year": "2017", "authors": "J Liu; W.-C Chang; Y Wu; Yang ; Y "}, {"ref_id": "b31", "title": "Cost-Effective Incentive Allocation via Structured Counterfactual Inference", "journal": "", "year": "2020", "authors": "R Lopez; C Li; X Yan; J Xiong; M Jordan; Y Qi; L Song"}, {"ref_id": "b32", "title": "Hidden factors and hidden topics: understanding rating dimensions with review text", "journal": "", "year": "2013", "authors": "J Mcauley; J Leskovec"}, {"ref_id": "b33", "title": "Efficient pairwise multilabel classification for large-scale problems in the legal domain", "journal": "", "year": "2008", "authors": "E L Mencia; J F\u00fcrnkranz"}, {"ref_id": "b34", "title": "Distributed representations of words and phrases and their compositionality", "journal": "", "year": "2013", "authors": "T Mikolov; I Sutskever; K Chen; G S Corrado; J Dean"}, {"ref_id": "b35", "title": "Parabel: Partitioned label trees for extreme classification with application to dynamic search advertising", "journal": "", "year": "2018", "authors": "Y Prabhu; A Kag; S Harsola; R Agrawal; M Varma"}, {"ref_id": "b36", "title": "A Review of Trends and Techniques in Recommender Systems", "journal": "", "year": "2019", "authors": "Y Prabhu; A Kusupati; N Gupta; M ; H Varma; D Singh"}, {"ref_id": "b37", "title": "Fonctions de repartition a n-dimensions et leurs marges", "journal": "", "year": "1959", "authors": "A Sklar"}, {"ref_id": "b38", "title": "CAB: Continuous adaptive blending for policy evaluation and learning", "journal": "", "year": "2019", "authors": "Y Su; L Wang; M Santacatterina; T Joachims"}, {"ref_id": "b39", "title": "Counterfactual Risk Minimization: Learning from Logged Bandit Feedback", "journal": "", "year": "2015", "authors": "A Swaminathan; T Joachims"}, {"ref_id": "b40", "title": "The Self-Normalized Estimator for Counterfactual Learning", "journal": "", "year": "2015", "authors": "A Swaminathan; T Joachims"}, {"ref_id": "b41", "title": "Off-policy evaluation for slate recommendation", "journal": "", "year": "2017", "authors": "A Swaminathan; A Krishnamurthy; A Agarwal; M Dudik; J Langford; D Jose; I Zitouni"}, {"ref_id": "b42", "title": "AnnexML: Approximate nearest neighbor search for extreme multi-label classification", "journal": "", "year": "2017", "authors": "Y Tagami"}, {"ref_id": "b43", "title": "Batch Learning from Bandit Feedback through Bias Corrected Reward Imputation", "journal": "", "year": "2019", "authors": "L Wang; Y Bai; A Bhalla; T Joachims"}, {"ref_id": "b44", "title": "Beyond ranking: Optimizing whole-page presentation", "journal": "", "year": "2016", "authors": "Y Wang; D Yin; L Jie; P Wang; M Yamada; Y Chang; Q Mei"}, {"ref_id": "b45", "title": "Variance Regularized Counterfactual Risk Minimization via Variational Divergence Minimization", "journal": "", "year": "2018", "authors": "H Wu; M Wang"}, {"ref_id": "b46", "title": "A no-regret generalization of hierarchical softmax to extreme multi-label classification", "journal": "", "year": "2018", "authors": "M Wydmuch; K Jasinska; M Kuznetsov; R Busa-Fekete; K Dembczynski"}, {"ref_id": "b47", "title": "Ppdsparse: A parallel primal-dual sparse method for extreme classification", "journal": "", "year": "2017", "authors": "I E Yen; .-H Huang; X Dai; W Ravikumar; P Dhillon; I Xing; E "}, {"ref_id": "b48", "title": "Pd-sparse: A primal and dual sparse approach to extreme multiclass and multilabel classification", "journal": "", "year": "2016", "authors": "I E Yen; .-H Huang; X Ravikumar; P Zhong; K Dhillon; I "}, {"ref_id": "b49", "title": "HAXMLNet: Hierarchical Attention Network for Extreme Multi-Label Text Classification. arXiv", "journal": "", "year": "2019", "authors": "R You; Z Zhang; S Dai; S Zhu"}, {"ref_id": "b50", "title": "AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification", "journal": "", "year": "2019", "authors": "R You; Z Zhang; Z Wang; S Dai; H Mamitsuka; S Zhu"}, {"ref_id": "b51", "title": "Enhancing navigation on Wikipedia with social tags. arXiv", "journal": "", "year": "2012", "authors": "A Zubiaga"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Assumption 2 .2(Sparse feedback condition). The individual feedback random variable r takes values in the bounded interval [\u2207, \u2206]. For all x \u2208 R d , the label set [L] can be partitioned as [L] = \u03a8(x) \u2210 \u03a8 0 (x) such that for all actions y of \u03a8 0 (x), the expected reward is minimal: \u03b4(x, y) = \u2207.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure1: Expected R@5 and CDF of the logging policy for the top-k action for each XMC dataset. Exploration is limited to a subset of relevant actions.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure2: Data-driven selection of p on the EUR-LeX dataset. Left: logging policy statistics for three randomization scenarios (A, B, C, described in appendix). Middle: R@5 performance for each POXM variant and each logging policy. Right: SNIS estimates used for selection of p in POXM.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "\u03c0) \u2212 V (\u03c0) = E P(x) E \u03c1(y x) E P(r x,y) 1 {y\u2208\u03a6(x)} \u03c0(y x) \u03c1(y x) r \u2212 E \u03c0(y x) \u03b4(x, y)(15)= E P(x) E \u03c1(y x) 1 {y\u2208\u03a6(x)} \u03c0(y x) \u03c1(y x) \u03b4(x, y) \u2212 E \u03c0(y x) \u03b4(x, y)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "v(x, y) = E[r x, y] and \u03c3 2 = inf x,y\u2208R d \u00d7[K] v(x, y). Let us focus for now on the second order moment difference in Equation (19), which we can decompose as:EV IS (\u03c0) 2 \u2212 EV \u03a6 sIS (\u03c0) 2 = 1 n E P(x)\u03c1(y x)P(r x,y) 1 {y\u2208\u03a6 0 (x)} r 2 \u03c0 2 (y x) \u03c1 2 (y x) x)\u03c1(y x) 1 {y\u2208\u03a6 0 (x)} v(x, y)\u03c0 2 (y x) \u03c1 2 (y x) x) E \u03c1(y x) 1 {y\u2208\u03a6 0 (x)} \u03c0 2 (y x) \u03c1 2 (y x)", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "XMC datasets used for semi-simulation of eXtreme bandit feedback.", "figure_data": "DatasetNtrainNtestDLLLEUR-Lex15,4493,865 186,1043,9565.30 20.79Wiki10-31K14,1466,616 101,93830,938 18.648.52Amazon-670K 490,449 153,025 135,909 670,0915.453.99N"}], "formulas": [{"formula_id": "formula_0", "formula_text": "V IS (\u03c0) = 1 n n i=1 \u03c0(y i x i ) \u03c1(y i x i ) r i .(1)", "formula_coordinates": [2.0, 382.6, 468.85, 175.4, 26.83]}, {"formula_id": "formula_1", "formula_text": "V BN (\u03c0) = 1 n n i=1 \u03c0(y i x i ) \u03c1(y i x i ) [r i \u2212 \u03bb] ,(2)", "formula_coordinates": [3.0, 102.14, 207.57, 190.36, 26.83]}, {"formula_id": "formula_2", "formula_text": "\u03b4(x, y) = E[r x, y].", "formula_coordinates": [3.0, 54.0, 639.97, 93.62, 11.79]}, {"formula_id": "formula_3", "formula_text": "\u03c0(Y x) = j=1 \u03c0(y j x, y 1\u2236j\u22121 ).", "formula_coordinates": [3.0, 374.84, 127.38, 127.82, 18.18]}, {"formula_id": "formula_4", "formula_text": "V (\u03c0) = E P(x) E \u03c0(Y x) E 1 \u22ba R x, Y .(4)", "formula_coordinates": [3.0, 360.55, 236.57, 197.45, 14.99]}, {"formula_id": "formula_5", "formula_text": "V (\u03c0) = E P(x)", "formula_coordinates": [3.0, 365.76, 449.58, 57.51, 11.79]}, {"formula_id": "formula_6", "formula_text": ")5", "formula_coordinates": [3.0, 550.26, 450.32, 7.74, 8.64]}, {"formula_id": "formula_7", "formula_text": "V (\u03c0) = E P(x) \u03c0 (\u03a8(x) x) \u22c5 E \u03c0(y x) [\u03b4(x, y) y \u2208 \u03a8(x)] .", "formula_coordinates": [4.0, 54.0, 351.97, 240.75, 11.79]}, {"formula_id": "formula_8", "formula_text": "V \u03a6 sIS (\u03c0) = 1 n n i=1 \u03c0(y i x i , y \u2208 \u03a6(x)) \u03c1(y i x i ) r i .(7)", "formula_coordinates": [4.0, 89.5, 632.12, 203.0, 37.45]}, {"formula_id": "formula_9", "formula_text": "EV \u03a6 sIS (\u03c0) \u2212 V (\u03c0) \u2264 \u2206\u03ba(\u03c0, \u03a8, \u03a6),(8)", "formula_coordinates": [4.0, 371.41, 107.02, 186.59, 13.62]}, {"formula_id": "formula_10", "formula_text": "MSE V \u03a6 sIS (\u03c0) \u2264 MSE V IS (\u03c0) + 2\u2206 2 \u03ba(\u03c0, \u03a8, \u03a6) (9) \u2212 \u03c3 2 n E P(x) \u03c0 2 \u03a6 0 (x) x \u03c1 (\u03a6 0 (x) x) ,(10)", "formula_coordinates": [4.0, 334.65, 186.06, 223.35, 45.6]}, {"formula_id": "formula_11", "formula_text": "\u03c3 2 = inf x,y\u2208R d \u00d7[K] E[r 2 x, y].", "formula_coordinates": [4.0, 345.97, 237.03, 124.91, 13.13]}, {"formula_id": "formula_12", "formula_text": "x \u2208 R d , \u03a8(x) \u2282 \u03a6(x)), thenV \u03a6", "formula_coordinates": [4.0, 319.5, 273.81, 238.5, 21.68]}, {"formula_id": "formula_13", "formula_text": "(x i , Y i , R i ) n", "formula_coordinates": [4.0, 343.12, 595.53, 53.32, 13.13]}, {"formula_id": "formula_14", "formula_text": "V \u03a6 sIS (\u03c0) = 1 n n i=1 j=1 \u03c0 \u03a6 (y i,j x i , y 1\u2236j\u22121 ) \u03c1(y i,j x i , y 1\u2236j\u22121 ) r i,j ,(11)", "formula_coordinates": [4.0, 343.32, 647.22, 214.68, 27.2]}, {"formula_id": "formula_15", "formula_text": "\u03c0 \u03a6 (y j x, y 1\u2236j\u22121 ) = \u03c0(y j x) y \u2032 \u2208\u03a6(x) \u03c0(y \u2032 x) \u2212 k<j \u03c0(y k x) . (12", "formula_coordinates": [5.0, 59.11, 268.29, 229.24, 33.16]}, {"formula_id": "formula_16", "formula_text": ")", "formula_coordinates": [5.0, 288.35, 275.57, 4.15, 8.64]}, {"formula_id": "formula_17", "formula_text": "V \u03a6 sIS (\u03c0) = 1 n n i=1 j=1 \u03c0 \u03a6 (y i,j x i , y 1\u2236j\u22121 ) \u03c1(y i,j x i , y 1\u2236j\u22121 ) [r i,j \u2212 \u03bb j ], (13", "formula_coordinates": [5.0, 63.98, 446.65, 224.37, 27.2]}, {"formula_id": "formula_18", "formula_text": ")", "formula_coordinates": [5.0, 288.35, 455.7, 4.15, 8.64]}, {"formula_id": "formula_19", "formula_text": "(x i , Y * i ) n i=1", "formula_coordinates": [5.0, 510.31, 402.33, 47.19, 14.49]}, {"formula_id": "formula_20", "formula_text": "R@k = E \u03c0(y1,...,y k ) 1 k k l=1 1{y l \u2208 Y * }.(14)", "formula_coordinates": [6.0, 98.06, 409.3, 194.44, 26.35]}, {"formula_id": "formula_21", "formula_text": "EV \u03a6 sIS (\u03c0) \u2212 V (\u03c0) \u2264 \u2206\u03ba(\u03c0, \u03a8, \u03a6),(8)", "formula_coordinates": [10.0, 238.66, 94.75, 319.34, 13.63]}, {"formula_id": "formula_22", "formula_text": "MSE V \u03a6 sIS (\u03c0) \u2264 MSE V IS (\u03c0) + 2\u2206 2 \u03ba(\u03c0, \u03a8, \u03a6) (9) \u2212 \u03c3 2 n E P(x) \u03c0 2 \u03a6 0 (x) x \u03c1 (\u03a6 0 (x) x) ,(10)", "formula_coordinates": [10.0, 207.71, 134.7, 350.29, 45.6]}, {"formula_id": "formula_23", "formula_text": "EV \u03a6 sIS(", "formula_coordinates": [10.0, 141.68, 220.13, 26.83, 13.63]}, {"formula_id": "formula_27", "formula_text": "EV \u03a6 sIS (\u03c0) \u2212 V (\u03c0) \u2264 \u2206E P(x) \u03c0 \u03a8(x) \u2229 \u03a6 0 (x) x(19)", "formula_coordinates": [10.0, 205.51, 331.71, 352.49, 13.62]}, {"formula_id": "formula_28", "formula_text": "MSE V IS (\u03c0) = MSE V \u03a6 sIS (\u03c0) + E V IS (\u03c0) 2 \u2212V \u03a6 sIS (\u03c0) 2 \u2212 2V (\u03c0) V IS (\u03c0) \u2212V \u03a6 sIS (\u03c0) = MSE V \u03a6 sIS (\u03c0) + EV IS (\u03c0) 2 \u2212 EV \u03a6 sIS (\u03c0)", "formula_coordinates": [10.0, 131.78, 361.49, 337.8, 33.87]}, {"formula_id": "formula_29", "formula_text": "\u2265 \u03c3 2 n E P(x) \u23a1 \u23a2 \u23a2 \u23a2 \u23a3 \u03c0 2 \u03a6 0 (x) x \u03c1 (\u03a6 0 (x) x) \u2206 \u03c7 2 (\u03c0(y x) \u03c1(y x)) \u23a4 \u23a5 \u23a5 \u23a5 \u23a6 ,(23)", "formula_coordinates": [10.0, 239.68, 512.4, 318.32, 51.76]}, {"formula_id": "formula_31", "formula_text": "EV IS (\u03c0) 2 \u2265 EV \u03a6 sIS (\u03c0) 2 + \u03c3 2 n E P(x) \u03c0 2 \u03a6 0 (x) x \u03c1 (\u03a6 0 (x) x) .(25)", "formula_coordinates": [10.0, 204.47, 589.57, 353.53, 28.16]}, {"formula_id": "formula_32", "formula_text": "MSE V IS (\u03c0) \u2265 MSE V \u03a6 sIS (\u03c0) + \u03c3 2 n E P(x) \u03c0 2 \u03a6 0 (x) x \u03c1 (\u03a6 0 (x) x) \u2212 2\u2206 2 E P(x) \u03c0 \u03a8(x) \u2229 \u03a6 0 (x) x(", "formula_coordinates": [10.0, 116.78, 665.34, 428.77, 28.16]}], "doi": ""}