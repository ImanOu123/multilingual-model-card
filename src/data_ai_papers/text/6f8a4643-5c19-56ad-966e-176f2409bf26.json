{"title": "UnNatural Language Inference", "authors": "Koustuv Sinha; Prasanna Parthasarathi; Joelle Pineau; Adina Williams", "pub_date": "", "abstract": "Recent investigations into the inner-workings of state-of-the-art large-scale pre-trained Transformer-based Natural Language Understanding (NLU) models indicate that they appear to know humanlike syntax, at least to some extent. We provide novel evidence that complicates this claim: we find that state-of-the-art Natural Language Inference (NLI) models assign the same labels to permuted examples as they do to the original, i.e. they are largely invariant to random wordorder permutations. This behavior notably differs from that of humans; we struggle with ungrammatical sentences. To measure the severity of this issue, we propose a suite of metrics and investigate which properties of particular permutations lead models to be word-order invariant. In the MNLI dataset, for example, we find almost all (98.7%) examples contain at least one permutation which elicits the gold label. Models are sometimes even able to assign gold labels to permutations that they originally failed to predict correctly. We provide a comprehensive empirical evaluation of this phenomenon, and further show that this issue exists for both Transformers and pre-Transformer RNN / ConvNet based encoders, as well as across multiple languages (English and Mandarin Chinese). Our code and data are available at https://github.com/facebookresearch/unlu.", "sections": [{"heading": "Introduction", "text": "Of late, large scale pre-trained Transformer-based (Vaswani et al., 2017) models-such as RoBERTa (Liu et al., 2019), BART (Lewis et al., 2020), and GPT-2 and -3 (Radford et al., 2019;Brown et al., 2020)-have exceeded recurrent neural networks' performance on many NLU tasks (Wang et al., 2018(Wang et al., , 2019. Several papers have even suggested that Transformers pretrained on a language modeling (LM) objective can capture syntactic informa- He and his associates were operating at the level of the metaphor.\nC his at and metaphor the of were He operating associates n't level .\nhis the and metaphor level the were He at associates operating of .", "publication_ref": ["b86", "b49", "b44", "b68", "b89", "b88"], "figure_ref": [], "table_ref": []}, {"heading": "C", "text": "Table 1: Examples from the MNLI Matched development set. Both the original example and the permuted one elicit the same classification label (entailment and contradiction respectively) from RoBERTa (large). A simple demo is provided in an associated Google Colab notebook.\ntion (Hewitt and Manning, 2019;Jawahar et al., 2019;Wu et al., 2020), with their self-attention layers being capable of surprisingly effective learning (Rogers et al., 2020). In this work, we question such claims that current models \"know syntax\". Since there are many ways to investigate \"syntax\", we must be clear on what we mean by the term. Knowing the syntax of a sentence means being sensitive to the order of the words in that sentence (among other things). Humans are sensitive to word order, so clearly, \"language is not merely a bag of words\" (Harris, 1954, p.156). Moreover, it is easier for us to identify or recall words presented in canonical orders than in disordered, ungrammatical sentences; this phenomenon is called the \"sentence superiority effect\" (Cattell 1886;Scheerer 1981;Toyota 2001;Baddeley et al. 2009;Grainger 2017, 2019;Wen et al. 2019, i.a.). In our estimation then, if one wants to claim that a model \"knows syntax\", then they should minimally show that the model is sensitive to word order (at least for e.g. English or Mandarin Chinese).\nGenerally, knowing the syntax of a sentence is taken to be a prerequisite for understanding what that sentence means (Heim and Kratzer, 1998). Models should have to know the syntax first then, if performing any particular NLU task that genuinely requires a humanlike understanding of meaning (cf. Bender and Koller 2020). Thus, if our models are as good at NLU as our current evaluation methods suggest, we should expect them to be sensitive to word order (see Table 1). We find, based on a suite of permutation metrics, that they are not.\nWe focus here on textual entailment, one of the hallmark tasks used to measure how well models understand language (Condoravdi et al., 2003;Dagan et al., 2005). This task, often also called Natural Language Inference (NLI; Bowman et al. 2015, i.a.), typically consists of two sentences: a premise and a hypothesis. The objective is to predict whether the premise entails the hypothesis, contradicts it, or is neutral with respect to it. We find rampant word order insensitivity in purportedly high performing NLI models. For nearly all premise-hypothesis pairs, there are many permuted examples that fool the models into providing the correct prediction. In case of MNLI, for example, the current state-of-the-art of 90.5% can be increased to 98.7% merely by permuting the word order of test set examples. We even find drastically increased cross-dataset generalization when we reorder words. This is not just a matter of chance-we show that the model output probabilities are significantly different from uniform.\nWe verify our findings with three popular English NLI datasets-SNLI (Bowman et al., 2015), MultiNLI (Williams et al., 2018b) and ANLI (Nie et al., 2020))-and one Chinese one, OCNLI (Hu et al., 2020a). It is thus less likely that our findings result from some quirk of English or a particular tokenization strategy. We also observe the effect for various transformer architectures pre-trained on language modeling (BERT, RoBERTa, DistilBERT), and non-transformers, including a ConvNet, an In-ferSent model, and a BiLSTM.\nOur contributions are as follows: (i) we propose a suite of metrics (Permutation Acceptance) for measuring model insensitivity to word order ( \u00a73), (ii) we construct multiple permuted test datasets for measuring NLI model performance at a large scale ( \u00a75), (iii) we show that NLI models focus on words more than word order, but can partially reconstruct syntactic information from words alone ( \u00a76), (iv) we show the problem persists on out-ofdomain data, (v) we show that humans struggle with UnNatural Language Inference, underscoring the non-humanlikeness of SOTA models ( \u00a77), (vi) finally, we explore a simple maximum entropybased method ( \u00a78) to encourage models not to accept permuted examples.", "publication_ref": ["b35", "b39", "b102", "b72", "b10", "b75", "b83", "b3", "b34", "b4", "b17", "b20", "b7", "b98", "b61", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Researchers in NLP have realized the importance of syntactic structure in neural networks going back to Tabor (1994). An early hand annotation effort on PASCAL RTE (Dagan et al., 2006) suggested that \"syntactic information alone was sufficient to make a judgment\" for roughly one third of examples (Vanderwende and Dolan, 2005). Anecdotally, large generative language models like GPT-2 or -3 exhibit a seemingly humanlike ability to generate fluent and grammatical text (Goldberg, 2019;Wolf, 2019). However, the jury is still out as to whether transformers genuinely acquire syntax.\nModels appear to have acquired syntax. When researchers have peeked inside Transformer LM's pretrained representations, familiar syntactic structure (Hewitt and Manning, 2019;Jawahar et al., 2019;Lin et al., 2019;Wu et al., 2020), or a familiar order of linguistic operations (Jawahar et al., 2019;Tenney et al., 2019), has appeared. There is also evidence, notably from agreement attraction phenomena (Linzen et al., 2016) that transformerbased models pretrained on LM do acquire some knowledge of natural language syntax (Gulordava et al., 2018;Chrupa\u0142a and Alishahi, 2019;Jawahar et al., 2019;Lin et al., 2019;Manning et al., 2020;Hawkins et al., 2020;Linzen and Baroni, 2021). Results from other phenomena ) such as NPI licensing (Warstadt et al., 2019a) lend additional support. The claim that LMs acquire some syntactic knowledge has been made not only for transformers, but also for convolutional neural nets (Bernardy and Lappin, 2017), and RNNs (Gulordava et al., 2018;van Schijndel and Linzen, 2018;Wilcox et al., 2018;Zhang and Bowman, 2018;Prasad et al., 2019;Ravfogel et al., 2019)-although there are many caveats (e.g., Ravfogel et al. 2018;White et al. 2018;Davis and van Schijndel 2020;Chaves 2020;Da Costa and Chaves 2020;Kodner and Gupta 2020).\nModels appear to struggle with syntax. Several works have cast doubt on the extent to which NLI models in particular know syntax (although each work adopts a slightly different idea of what \"knowing syntax\" entails). For example, McCoy et al. (2019) argued that the knowledge acquired by models trained on NLI (for at least some popular datasets) is actually not as syntactically sophisticated as it might have initially seemed; some transformer models rely mainly on simpler, nonhumanlike heuristics. In general, transformer LM performance has been found to be patchy and variable across linguistic phenomena (Dasgupta et al., 2018;Naik et al., 2018;Jeretic et al., 2020). This is especially true for syntactic phenomena (Marvin and Linzen, 2018;Hu et al., 2020b;Gauthier et al., 2020;McCoy et al., 2020;, where transformers are, for some phenomena and settings, worse than RNNs . From another angle, many have explored architectural approaches for increasing a network's sensitivity to syntactic structure (Chen et al., 2017;Li et al., 2020). Williams et al. (2018a) showed that learning jointly to perform NLI and to parse resulted in parse trees that match no popular syntactic formalisms. Furthermore, models trained explicitly to differentiate acceptable sentences from unacceptable ones (i.e., one of the most common syntactic tests used by linguists) have, to date, come nowhere near human performance (Warstadt et al., 2019b).\nInsensitivity to Perturbation. Most relatedly, several concurrent works (Pham et al., 2020;Alleman et al., 2021;Gupta et al., 2021;Parthasarathi et al., 2021) investigated the effect of word order permutations on transformer NNs. Pham et al. ( 2020) is very nearly a proper subset of our work except for investigating additional tasks (i.e. from the GLUE benchmark of Wang et al. 2018) and performing a by-layer-analysis. Gupta et al. (2021) also relies on the GLUE benchmark, but additionally investigates other types of \"destructive\" perturbations. Our contribution differs from these works in that we additionally include the following: we (i) outline theoretically-informed predictions for how models should be expected to react to permuted input (we outline a few options), (ii) show that permuting can \"flip\" an incorrect prediction to a correct one, (iii) show that the problem isn't specific to Transformers, (iv) show that the problem persists on out of domain data, (v) offer a suite of flexible metrics, and (vi) analyze why models might be accepting permutations (BLEU and POS-tag neighborhood analysis). Finally, we replicate our findings in another language. While our work (and Pham et al.; Gupta et al.) only permutes data during fine-tuning and/or evaluation, recently Sinha et al. explored the sensitivity during pre-training, and found that models trained on n-gram permuted sentences perform remarkably close to regular MLM pre-training. In the context of generation, Parthasarathi et al. (2021) crafted linguistically relevant perturbations (on the basis of part-of-speech tagging and dependency parsing) to evaluate whether permutation hinders automatic machine translation models. Relatedly, but not for translation, Alleman et al. (2021) investigated a smaller inventory of perturbations with emphasis on phrasal boundaries and the effects of n-gram perturbations on different layers in the network.\nNLI Models are very sensitive to words. NLI models often over-attend to particular words to predict the correct answer (Gururangan et al., 2018;Clark et al., 2019). Wallace et al. (2019) show that some short sequences of non-human-readable text can fool many NLU models, including NLI models trained on SNLI, into predicting a specific label. In fact, Ettinger (2020) observed that for one of three test sets, BERT loses some accuracy in wordperturbed sentences, but that there exists a subset of examples for which BERT's accuracy remains intact. If performance isn't affected (or if permutation helps, as we find it does in some cases), it suggests that these state-of-the-art models actually perform somewhat similarly to bag-of-words models (Blei et al., 2003;Mikolov et al., 2013).", "publication_ref": ["b81", "b85", "b28", "b100", "b35", "b39", "b46", "b102", "b39", "b82", "b48", "b29", "b14", "b39", "b46", "b51", "b33", "b47", "b91", "b5", "b29", "b76", "b96", "b103", "b67", "b69", "b70", "b95", "b23", "b11", "b19", "b43", "b54", "b22", "b59", "b40", "b52", "b38", "b27", "b53", "b12", "b45", "b97", "b93", "b64", "b1", "b63", "b89", "b63", "b1", "b31", "b15", "b87", "b24", "b6", "b55"], "figure_ref": [], "table_ref": []}, {"heading": "Our Approach", "text": "As we mentioned, linguists generally take syntactic structure to be necessary for humans to know what sentences mean. Many also find the NLI task to a very promising approximation of human natural language understanding, in part because it is rooted in the tradition of logical entailment. In the spirit of propositional logic, sentence meaning is taken to be truth-conditional (Frege, 1948;Montague, 1970;Chierchia and McConnell-Ginet, 1990;Heim and Kratzer, 1998). That is to say that understanding a sentence is equivalent to knowing the actual conditions of the world under which the sentences would be (judged) true (Wittgenstein, 1922). If grammatical sentences are required for sentential inference, as per a truth conditional approach (Montague, 1970), then permuted sentences should be meaningless. Put another way, the meanings of highly permuted sentences (if they exist) are not propositions, and thus those sentences don't have truth conditions. Only from their truth conditions of sentences can we tell if a sentence entails another. In short, the textual entailment task is technically undefined in our \"unnatural\" setting.\nSince existing definitions don't immediately extend to UnNatural Language Inference (UNLI), we outline several hypothetical systematic ways that a model might perform, had it been sensitive to word order. We hypothesize two models that operate on the first principles of NLI, and one that doesn't. In the first case, Model A deems permuted sentences meaningless (devoid of truth values), as formal semantic theories of human language would predict. Thus, it assigns \"neutral\" to every permuted example. Next, Model B does not deem permuted sentences meaningless, and attempts to understand them. Humans find understanding permuted sentences difficult (see our human evaluations in \u00a77). Model B could also similarly struggle to decipher the meaning, and just equally sample labels for each example (i.e., assigns equal probability mass to the outcome of each label). Finally, we hypothesize a non-systematic model, Model C, which attempts to treat permuted sentences as though they weren't permuted at all. This model could operate similarly as bag-of-words (BOW), and thus always assign the same label to the permuted examples as it would to the un-permuted examples. If the model failed to assign the original gold label to the original unpermuted examples, it will also fail to assign the original gold label to its permutations; it will never get higher accuracy on permuted examples than on unpermuted ones. We find in our experiments that the state-of-theart Transformer-based NLI models (as well as pre-Transformer class of models) do not perform like any of the above hypothetical models. They perform closest to Model C, but are, in some cases, actually able to achieve higher accuracy on permuted examples. To better quantitatively describe this behaviour, we introduce our suite of Permutation Acceptance metrics that enable us to quantify how accepting models are of permuted sentences.", "publication_ref": ["b25", "b57", "b13", "b34", "b99", "b57"], "figure_ref": [], "table_ref": []}, {"heading": "Methods", "text": "Constructing the permuted dataset. For a given dataset D having splits D train and D test , we first train an NLI model M on D train to achieve comparable accuracy to what was reported in the original papers. We then construct a randomized version of D test , which we term asD test such that: for each example (p i , h i , y i ) \u2208 D test (where p i and h i are the premise and hypothesis sentences of the example respectively and y i is the gold label), we use a permutation operator F that returns a list (P i ,\u0124 i ) of q permuted sentences (p i and\u0125 i ), where q is a hyperparameter. F essentially permutes all positions of the words in a given sentence (i.e., either in premise or hypothesis) with the restriction that no words maintain their original position. In our initial setting, we do not explicitly control the placement of the words relative to their original neighbors, but we analyze clumping effects in \u00a75. D test now consists of |D test |\u00d7q examples, with q different permutations of hypothesis and premise for each original test example pair. If a sentence S (e.g., h i ) contains w words, then the total number of available permutations of S are (w \u2212 1)!, thus making the output of F a list of (w\u22121)! q permutations in this case. For us, the space of possible outputs is larger, since we permute p i and h i separately (and ignore examples for which any |S|\u2264 5).\nDefining Permutation Acceptance. The choice of q naturally allows us to analyze a statistical view of the predictability of a model on the permuted sentences. To that end, we define the following notational conventions. Let A be the original accuracy of a given model M on a dataset D, and c be the number of examples in a dataset which are marked as correct according to the standard formulation of accuracy for the original dataset (i.e., they are assigned the ground truth label). Typically A is given by c |Dtest| or c |D dev | . Let Pr M (P i ,\u0124 i ) cor then be the percentage of q permutations of an example (p i , h i ) assigned the ground truth label y i by M :\nPr M (P i ,\u0124 i ) cor = 1 q (p j \u2208P i ,\u0125 j \u2208\u0124 i ) ((M (p j ,\u0125 j ) = y i ) \u2192 1)(1)\nTo get an overall summary score, we let \u2126 x be the percentage of examples (p i , h i ) \u2208 D test for which Pr M (P i ,\u0124 i ) cor exceeds a predetermined threshold 0 < x < 1. Concretely, a given example will count as correct according to \u2126 x if more than x percent of its permutations (P i and\u0124 i ) are assigned y i by the model M . Mathematically,\n\u2126 x = 1 | D test | (p i ,h i )\u2208Dtest ((Pr M (P i ,\u0124 i ) cor > x) \u2192 1).\n(2) There are two specific cases of \u2126 x that we are most interested in. First, we define \u2126 max or the Maximum Accuracy, where x = 1/|D test |. In short, \u2126 max gives the percentage of examples (p i , h i ) \u2208 D test for which there is at least one permutation (p j ,\u0125 j ) that model M assigns the gold label y i 1 . Second, we define \u2126 rand , or Random Baseline Accuracy, where x = 1/m or chance probability (for balanced m-way classification, where m = 3 in NLI). This metric is less stringent than \u2126 max , as it counts an example if at least one third of its permutations are assigned the gold label (hence provides a lower-bound relaxation). See Figure 1 for a graphical representation of \u2126 x .\nWe also define D f to be the list of examples originally marked incorrect according to A, but are now deemed correct according \u2126 max . D c is the list of examples originally marked correct according to A. Thus, we should expect D f < D c for models that have high accuracy. Additionally, we define P c and P f , as the dataset average percentage of permutations which predicted the gold label, when the 1 Theoretically, \u2126max \u2192 1 if the number of permutations q is large. Thus, in our experiments we set q = 100.\nexamples were originally correct (D c ) and when the examples were originally incorrect (D f ) as per A (hence, flipped) respectively.\nP c = 1 |D c | |D c | i=0 M (P i ,\u0124 i ) cor\n(3) P f is defined similarly by replacing D c by D f . Note that for a classic BOW model, P c = 100 and P f = 0, because it would rely on the words alone (not their order) to make its classification decision. Since permuting removes no words, BOW models should come to the same decisions for permuted examples as for the originals.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Results", "text": "We present results for two types of models:\n(a) Transformer-based models and (b) Non-Transformer Models. In (a), we investigate the state-of-the-art pre-trained models such as RoBERTa-Large (Liu et al., 2019), BART-Large (Lewis et al., 2020) and DistilBERT . For (b) we consider several recurrent and convolution based neural networks, such as In-ferSent (Conneau et al., 2017), Bidirectional LSTM (Collobert and Weston, 2008) and ConvNet (Zhao et al., 2015). We train all models on MNLI, and evaluate on in-distribution (SNLI and MNLI) and out-of-distribution datasets (ANLI). We independently verify results of (a) using both our fine-tuned model using HuggingFace Transformers      (Hu et al., 2020a). Bold marks the highest value per metric (red shows the model is insensitive to permutation). for almost all examples in D test such that model M predicts the gold label. We also observe high \u2126 rand at 79.4%, showing that there are many examples for which the models outperform even a random baseline in accepting permuted sentences (see Appendix E for more \u2126 values.) Evaluating out-of-domain generalization with ANLI dataset splits resulted in an \u2126 max value that is notably higher than A (89.7% \u2126 max for RoBERTa compared to 45.6% A). As a consequence, we encounter many flips, i.e., examples where the model is unable to predict the gold label, but at least one permutation of that example is able to. However, recall this analysis expects us to know the gold label upfront, so this test can be thought of as running a word-order probe test on the model until the model predicts the gold label (or give up by exhausting our set of q permutations). For out-of-domain generalization, \u2126 rand decreases considerably (36.4% \u2126 rand on A1), which means fewer permutations are accepted by the model. Next, recall that a classic bag-of-words model would have P c = 100 and P f = 0. No model performs strictly like a classic bag of words although they do perform somewhat BOW-like (P c >> P f for all test splits, Figure 5). We find this BOW-likeness to be higher for certain non-Transformer models, (InferSent) as they exhibit higher P c (84.2% for InferSent compared to 70.7% for RoBERTa on MNLI).\nModels are very confident. The phenomenon we observe would be of less concern if the correct label prediction was just an outcome of chance, which could occur when the entropy of the log probabilities of the model output is high (suggesting uniform probabilities on entailment, neutral and contradiction labels, recall Model B from \u00a73). We first investigate the model probabilities for the Transformer-based models on the permutations that lead to the correct answer in Figure 2. We find overwhelming evidence that model confidences on in-distribution datasets (MNLI, SNLI) are highly skewed, resulting in low entropy, and it varies among different model types. BART proves to be the most skewed Transformer-based model. This skewness is not a property of model capacity, as we observe DistilBERT log probabilities to have similar skewness as RoBERTa (large) model, while exhibiting lower A, \u2126 max , and \u2126 rand .\nFor non-Transformers whose accuracy A is lower, the \u2126 max achieved by these models are also predictably lower. We observe roughly the same relative performance in the terms of \u2126 max (Figure 5 and Appendix Table 2) and Average entropy (Figure 2). However, while comparing the averaged entropy of the model predictions, it is clear that there is some benefit to being a worse model-non-Transformer models are not as overconfident on randomized sentences as Transformers are. High confidence of Transformer models can be attributed to the overthinking phenomenon commonly observed in deep neural networks (Kaya et al., 2019) and BERT-based models (Zhou et al., 2020).\nSimilar artifacts in Chinese NLU. We extended the experiments to the Original Chinese NLI dataset (Hu et al., 2020a, OCNLI), and reused the pre-trained RoBERTa-Large and InferSent (non-Transformer) models on OCNLI. Our findings are similar to the English results (Table 3), thereby suggesting that the phenomenon is not just an artifact of English text or tokenization.\nOther Results. We investigated the effect of sentence length (which correlates with number of possible permutations; Appendix A), and hypothesisonly randomization (models exhibit similar phenomenon even when only hypothesis is permuted; Appendix C).", "publication_ref": ["b49", "b44", "b18", "b16", "b104", "b37", "b42", "b105"], "figure_ref": ["fig_2", "fig_2"], "table_ref": ["tab_1", "tab_2"]}, {"heading": "Analyzing Syntactic Structure Associated with Tokens", "text": "A natural question to ask following our findings: what is it about particular permutations that leads models to accept them? Since the permutation oper- ation is drastic and only rarely preserves local word relations, we first investigate whether there exists a relationship between Permutation Acceptance scores and local word order preservation. Concretely, we compare bi-gram word overlap (BLEU-2) with the percentage of permutations that are deemed correct (Figure 3). 3 Although the probability of a permuted sentence to be predicted correctly does appear to track BLEU-2 score (Figure 3), the percentage of examples which were assigned the gold label by the Transformer-based models is still higher than we would expect from permutations with lower BLEU-2 (66% for the lowest BLEU-2 range of 0 \u2212 0.15), suggesting preserved relative word order alone cannot explain the high permutation acceptance rates. Thus, we find that local order preservation does correlate with Permutation Acceptance, but it doesn't fully explain the high Permutation Acceptance scores. We now further ask whether \u2126 is related to a more abstract measure of local word relations, i.e., part-of-speech (POS) neighborhood.\nMany syntactic formalisms, like Lexical Functional Grammar (Kaplan and Bresnan, 1995;Bresnan et al., 2015, LFG), Head-drive Phrase Structure Grammar (Pollard and Sag, 1994, HPSG) or Lexicalized Tree Adjoining Grammar (Schabes et al., 1988;Abeille, 1990, LTAG), are \"lexicalized\", i.e., individual words or morphemes bear syntactic features telling us which other words they can combine with. For example, \"buy\" could be associated with (at least) two lexicalized syntactic structures, one containing two noun phrases (as in Kim bought cheese), and another with three (as in Lee bought Logan cheese). We speculate that our NLI models might accept permuted examples at high rates, because they are (perhaps noisily) reconstructing the original sentence from abstract, word-anchored information about common neighbors.\nTo test this, we POS-tagged D train using 17 Universal Part-of-Speech tags (using spaCy, Honnibal et al. 2020). For each w i \u2208 S i , we compute the occurrence probability of POS tags on tokens in the neighborhood of w i . The neighborhood is specified by the radius r (a symmetrical window r tokens from w i \u2208 S i to the left and right). We denote this sentence level probability of neighbor POS tags for a word w i as \u03c8 r {w i ,S i } \u2208 R 17 (see an example in Figure 7 in the Appendix). Sentence-level word POS neighbor scores can be averaged across D train to get a type level score \u03c8 r {w i ,D train } \u2208 R 17 , \u2200w i \u2208 D train . Then, for a sentence S i \u2208 D test , for each word w i \u2208 S i , we compute a POS mini-tree overlap score:\n\u03b2 k {w i ,S i } = 1 k | argmax k \u03c8 r {w i ,D train } \u2229 argmax k \u03c8 r {w i ,S i } | (4)\nConcretely, \u03b2 k {w i ,S i } computes the overlap of topk POS tags in the neighborhood of a word w i in S with that of the train statistic. If a word has the same mini-tree in a given sentence as it has in the training set, then the overlap would be 1. For a given sentence S i , the aggregate \u03b2 k {S i } is defined by the average of the overlap scores of all its words:\n\u03b2 k {S i } = 1 |S i | w i \u2208S i \u03b2 k {w i ,S i }\n, and we call it a POS minitree signature. We can also compute the POS minitree signature of a permuted sentence\u015c i to have \u03b2 k {\u015c i } . If the permuted sentence POS signature comes close to that of the true sentence, then their ratio (i.e., \u03b2 k {\u015c i } /\u03b2 k {S i } ) will be close to 1. Also, since POS signature is computed with respect to the train distribution, a ratio of > 1 indicates that the permuted sentence is closer to the overall train statistic than to the original unpermuted sentence in terms of POS signature. If high overlap with the training distribution correlates with percentage of permutations deemed correct, then our models treat words as if they project syntactic minitrees. We investigate the relationship with percentage of permuted sentences accepted with\n\u03b2 k {\u015c i } /\u03b2 k {S i }\nin Figure 4. We observe that the POS Tag Minitree hypothesis holds for Transformer-based models, RoBERTa, BART and DistilBERT, where the percentage of accepted pairs increase as the sentences have higher overlap with the un-permuted sentence in terms of POS signature. For non-Transformer models such as InferSent, ConvNet, and BiLSTM models, the POS signature ratio to percentage of correct permutation remains the same or decreases, suggesting that the reasoning process employed by these models does not preserve local abstract syntax structure (i.e., POS neighbor relations).", "publication_ref": ["b41", "b74"], "figure_ref": ["fig_3", "fig_3", "fig_4"], "table_ref": []}, {"heading": "Human Evaluation", "text": "We expect humans to struggle with UNLI, given our intuitions and the sentence superiority findings (but see Mollica et al. 2020). To test this, we presented two experts in NLI (one a linguist) with permuted sentence pairs to label. 4 Concretely, we draw equal number of examples from MNLI Matched dev set (100 examples where RoBERTa predicts the gold label, D c and 100 examples where it fails to do so, D f ), and then permute these examples using F. The experts were given no additional information (recall that it is common knowledge that NLI is a roughly balanced 3-way classification task). Unbeknownst to the experts, all permuted sentences in the sample were actually accepted by the RoBERTa (large) model (trained on MNLI dataset). We observe that the experts performed  much worse than RoBERTa (Table 4), although their accuracy was a bit higher than random. We also find that for both experts, accuracy on permutations from D c was higher than on D f , which verifies findings that showed high word overlap can give hints about the ground truth label (Dasgupta et al., 2018;Poliak et al., 2018;Gururangan et al., 2018;.", "publication_ref": ["b56", "b22", "b65", "b31"], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Training by Maximizing Entropy", "text": "We propose an initial attempt to mitigate the effect of correct prediction on permuted examples. As we observe in \u00a75, model entropy on permuted examples is significantly lower than expected. Neural networks tend to output higher confidence than random for even unknown inputs (Gandhi and Lake, 2020), which might be an underlying cause of the high Permutation Acceptance.\nAn ideal model would be ambivalent about randomized ungrammatical sentences. Thus, we train NLI models baking in the principle of mutual exclusivity (Gandhi and Lake, 2020) by maximizing model entropy. Concretely, we fine-tune RoBERTa on MNLI while maximizing the entropy (H) on a subset of n randomized examples ((p i ,r i ), for each example (p, h) in MNLI. We modify the loss function as follows:\n(5)\nL = argmin \u03b8 ((p,h),y) y log(p(y|(p, h); \u03b8)) + n i=1 H y|(p i ,\u0125 i ); \u03b8\nUsing this maximum entropy method (n = 1), we find that the model improves considerably with respect to its robustness to randomized sentences, all while taking no hit to accuracy (Table 5). We observe that no model reaches a \u2126 max score close to 0, suggesting further room to explore other methods for decreasing models' Permutation Acceptance. Similar approaches have also proven useful (Gupta et al., 2021) for other tasks as well.  ", "publication_ref": ["b26"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "Future Work & Conclusion", "text": "We show that state-of-the-art models do not rely on sentence structure the way we think they should: NLI models (Transformer-based models, RNNs, and ConvNets) are largely insensitive to permutations of word order that corrupt the original syntax. We also show that reordering words can cause models to flip classification labels. We do find that models seem to have learned some syntactic information as is evidenced by a correlation between preservation of abstract POS neighborhood information and rate of acceptance by models, but these results do not discount the high rates of Permutation Acceptance, and require further verification.\nCoupled with the finding that humans cannot perform UNLI at all well, the high rate of permutation acceptance that we observe leads us to conclude that current models do not yet \"know syntax\" in the fully systematic and humanlike way we would like them to. A few years ago,  encouraged NLP to consider \"the details of human language, how it is learned, processed, and how it changes, rather than just chasing state-of-the-art numbers on a benchmark task.\" We expand upon this view, and suggest one particular future direction: we should train models not only to do well on clean test data, but also to not to overgeneralize to corrupted input.\nFigure 5: Comparison of \u2126 max ,\u2126 rand ,P c and P f with the model accuracy A on multiple datasets, where all models are trained on the MNLI corpus (Williams et al., 2018b).", "publication_ref": ["b98"], "figure_ref": [], "table_ref": []}, {"heading": "A Effect of Length on Permutation Acceptance", "text": "We investigate the effect of length on Permutation Acceptance in Figure 6. We observe that shorter sentences in general have a somewhat higher probability of acceptance for examples which was originally predicted correctly-since shorter sentences have fewer unique permutations. However, for the examples which were originally incorrect, the trend is not present. ", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "B Example of POS Minitree", "text": "In \u00a76, we developed a POS signature for each word in at least one example in a test set, then compare that signature to the distribution of the same word in the training set. Figure 7 provides a snapshot a word \"river\" from the test set and shows how the POS signature distribution of the word in a particular example match with that of aggregated training statistic. In practice, we select the top k POS tags for the word in the test signature as well as the train, and calculate their overlap. When comparing the model performance with permuted sentences, we compute a ratio between the original test overlap score and an overlap score calculated instead from the permuted test. In the Figure 7, 'river' would have a POS tag minitree score of 0.75. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Effect of Hypothesis-Only Randomization", "text": "In recent years, the impact of the hypothesis sentence (Gururangan et al., 2018;Tsuchiya, 2018;Poliak et al., 2018) on NLI classification has been a topic of much interest. As we define in \u00a73, logical entailment can only be defined for pairs of propositions. We investigated one effect where we randomize only the hypothesis sentences while keeping the premise intact. Figure 9(a) shows that the \u2126 max value is almost the same for the two schemes; randomizing the hypothesis alone also leads the model to accept many permutations.", "publication_ref": ["b31", "b84", "b65"], "figure_ref": [], "table_ref": []}, {"heading": "D Effect of clumped words in random permutations", "text": "Since our original permuted dataset consists of extremely randomized words, we observe very low BLEU-3 (< 0.2) and BLEU-4 scores (< 0.1). To study the effect of overlap across a wider range of permutations, we devised an experiment where we clump certain words together before performing random permutations. Concretely, we clump 25%, 50% and 75% of the words in a sentence and then permute the remaining words and the clumped word as a whole. This type of clumped-permutation allows us to study the full range of BLEU-2/3/4 scores, which we present in Figure 10. As expected, the acceptability of permuted sentences increase linearly with BLEU score overlap. E Effect of the threshold of \u2126 x in various test splits\nWe defined two variations of \u2126 x , \u2126 max and \u2126 rand , but theoretically it is possible to define any arbitrary threshold percentage x to evaluate the unnatural language inference mechanisms of different models. In Figure 8   that models does not benefit from data augmentation). We also ensure that we use the same hyperparameters while training as with the standard setup. Concretely,D train consists of n hypothesispremise pairs from MNLI training data, where each example is a permuted output of the original pair.\nWe present the results of such training in Table 6, and compare the accuracy (\u00c2) with that of the standard setup (A). Note, during inference for all the models we use the un-permuted examples. As we can see, models perform surprisingly close to the original accuracy A even when trained with ungrammatical sentences. This adds further proof to the BOW nature of NLU models. Figure 9: Comparing the effect between randomizing both premise and hypothesis and only hypothesis on two Transformer-based models, RoBERTa and BART (For more comparisons please refer to Appendix). In 9(a), we observe the difference of \u2126 max is marginal in in-distribution datasets (SNLI, MNLI), while hypothesis-only randomization is worse for out-of-distribution datasets (ANLI). In 9(b), we compare the mean number of permutations which elicited correct response, and naturally the hypothesis-only randomization causes more percentage of randomizations to be correct.  ", "publication_ref": [], "figure_ref": ["fig_1", "fig_6"], "table_ref": []}, {"heading": "G Reproducibility Checklist", "text": "As per the prescribed Reproducibility Checklist, we provide the information of the following:\n\u2022 A clear description of the mathematical setting, algorithm and/or model: We provide details of models used in \u00a75\n\u2022 Description of the computing infrastructure used: We used 8 NVIDIA V100 32GB GPUs to train the models and perform all necessary inferences. We didn't run hyperparameter tuning for Transformer-based models as we used the published hyperparameters from the original models.\n\u2022 Average runtime for each approach: On an average, each model inference experiment consistine of 100 permutations for each example takes roughly 1 hour to complete.\n\u2022 Relevant statistics of the datasets used: We provide the statistics of the datasets used in Table 7.\n\u2022 Explanation of any data that were excluded, and all pre-processing steps:\nWe exclude examples where either the hypothesis and premise consists of less than 6 tokens. This way, we ensure that we have 100 unique permutations for each example.\n\u2022 Link to downloadable version of data and code:\nWe provide downloadable version of our data and code at https://github.com/facebookresearch/unlu.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_11"]}, {"heading": "Acknowledgments", "text": "Thanks to Omar Agha, Dzmitry Bahdanau, Sam Bowman, Hagen Blix, Ryan Cotterell, Emily Dinan, Michal Drozdal, Charlie Lovering, Nikita Nangia, Alicia Parrish, Grusha Prasad, Roy Schwartz, Shagun Sodhani, Anna Szabolsci, Alex Warstadt, Jackie Chi-kit Cheung, Timothy O'Donnell and members of McGill MCQLL lab for many invaluable comments and feedback on early drafts.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Lexical and syntactic rules in a Tree Adjoining Grammar", "journal": "", "year": "1990", "authors": "Anne Abeille"}, {"ref_id": "b1", "title": "Syntactic perturbations reveal representational correlates of hierarchical phrase structure in pretrained language models", "journal": "", "year": "2021", "authors": "Matteo Alleman; Jonathan Mamou; Miguel A Del Rio; Hanlin Tang; Yoon Kim; Sueyeon Chung"}, {"ref_id": "b2", "title": "Representation of constituents in neural language models: Coordination phrase as a case study", "journal": "", "year": "2019", "authors": "Aixiu An; Ethan Peng Qian; Roger Wilcox;  Levy"}, {"ref_id": "b3", "title": "Working memory and binding in sentence recall", "journal": "Journal of Memory and Language", "year": "2009", "authors": "Alan D Baddeley; J Graham; Richard J Hitch;  Allen"}, {"ref_id": "b4", "title": "Climbing towards NLU: On meaning, form, and understanding in the age of data", "journal": "", "year": "2020", "authors": "Emily M Bender; Alexander Koller"}, {"ref_id": "b5", "title": "Using deep neural networks to learn syntactic agreement", "journal": "CSLI Publications", "year": "2017", "authors": "Jean- ; Phillipe Bernardy; Shalom Lappin"}, {"ref_id": "b6", "title": "", "journal": "", "year": "2003", "authors": "David M Blei; Andrew Y Ng; Michael I Jordan"}, {"ref_id": "b7", "title": "A large annotated corpus for learning natural language inference", "journal": "", "year": "2015", "authors": "R Samuel; Gabor Bowman; Christopher Angeli; Christopher D Potts;  Manning"}, {"ref_id": "b8", "title": "Lexical-functional syntax", "journal": "John Wiley & Sons", "year": "2015", "authors": "Joan Bresnan; Ash Asudeh; Ida Toivonen; Stephen Wechsler"}, {"ref_id": "b9", "title": "Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners", "journal": "Curran Associates, Inc", "year": "", "authors": "Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel Ziegler; Jeffrey Wu; Clemens Winter; Chris Hesse; Mark Chen; Eric Sigler; Mateusz Litwin"}, {"ref_id": "b10", "title": "The time it takes to see and name objects. Mind, os-XI(41", "journal": "", "year": "1886", "authors": "James Mckeen; Cattell "}, {"ref_id": "b11", "title": "What don't RNN language models learn about filler-gap dependencies?", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Rui Chaves"}, {"ref_id": "b12", "title": "Enhanced LSTM for natural language inference", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Qian Chen; Xiaodan Zhu; Zhen-Hua Ling; Si Wei; Hui Jiang; Diana Inkpen"}, {"ref_id": "b13", "title": "Meaning and grammar: An Introduction to Semantics", "journal": "MIT Press", "year": "1990", "authors": "Gennaro Chierchia; Sally Mcconnell-Ginet"}, {"ref_id": "b14", "title": "Correlating neural and symbolic representations of language", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Grzegorz Chrupa\u0142a; Afra Alishahi"}, {"ref_id": "b15", "title": "What does BERT look at? an analysis of BERT's attention", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Kevin Clark; Urvashi Khandelwal; Omer Levy; Christopher D Manning"}, {"ref_id": "b16", "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "journal": "", "year": "2008", "authors": "Ronan Collobert; Jason Weston"}, {"ref_id": "b17", "title": "Entailment, intensionality and text understanding", "journal": "", "year": "2003", "authors": "Cleo Condoravdi; Dick Crouch; Reinhard Valeria De Paiva; Daniel G Stolle;  Bobrow"}, {"ref_id": "b18", "title": "Supervised learning of universal sentence representations from natural language inference data", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Alexis Conneau; Douwe Kiela; Holger Schwenk; Lo\u00efc Barrault; Antoine Bordes"}, {"ref_id": "b19", "title": "Assessing the ability of transformer-based neural models to represent structurally unbounded dependencies", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Jillian Da; Costa ; Rui Chaves"}, {"ref_id": "b20", "title": "The pascal recognising textual entailment challenge", "journal": "Springer", "year": "2005", "authors": "Oren Ido Dagan; Bernardo Glickman;  Magnini"}, {"ref_id": "b21", "title": "The PASCAL recognising textual entailment challenge", "journal": "Springer", "year": "2006", "authors": "Oren Ido Dagan; Bernardo Glickman;  Magnini"}, {"ref_id": "b22", "title": "Evaluating compositionality in sentence embeddings", "journal": "", "year": "2018", "authors": "Ishita Dasgupta; Demi Guo; Andreas Stuhlm\u00fcller; J Samuel; Noah D Gershman;  Goodman"}, {"ref_id": "b23", "title": "Recurrent neural network language models always learn English-like relative clause attachment", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Forrest Davis; Marten Van Schijndel"}, {"ref_id": "b24", "title": "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "Allyson Ettinger"}, {"ref_id": "b25", "title": "Sense and reference. The philosophical review", "journal": "", "year": "1948", "authors": "Gottlob Frege"}, {"ref_id": "b26", "title": "Mutual exclusivity as a challenge for deep neural networks", "journal": "Curran Associates, Inc", "year": "2020", "authors": "Kanishk Gandhi; M Brenden;  Lake"}, {"ref_id": "b27", "title": "SyntaxGym: An online platform for targeted evaluation of language models", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Jon Gauthier; Jennifer Hu; Ethan Wilcox; Peng Qian; Roger Levy"}, {"ref_id": "b28", "title": "Assessing BERT's syntactic abilities", "journal": "", "year": "2019", "authors": "Yoav Goldberg"}, {"ref_id": "b29", "title": "Colorless green recurrent networks dream hierarchically", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Kristina Gulordava; Piotr Bojanowski; Edouard Grave; Tal Linzen; Marco Baroni"}, {"ref_id": "b30", "title": "2021. BERT & family eat word salad: Experiments with text understanding", "journal": "AAAI", "year": "", "authors": "Ashim Gupta; Giorgi Kvernadze; Vivek Srikumar"}, {"ref_id": "b31", "title": "Annotation artifacts in natural language inference data", "journal": "", "year": "2018", "authors": "Swabha Suchin Gururangan; Omer Swayamdipta; Roy Levy; Samuel Schwartz; Noah A Bowman;  Smith"}, {"ref_id": "b32", "title": "Distributional structure. Word", "journal": "", "year": "1954", "authors": "S Zellig;  Harris"}, {"ref_id": "b33", "title": "Investigating representations of verb bias in neural language models", "journal": "", "year": "2020", "authors": "Robert Hawkins; Takateru Yamakoshi; Thomas Griffiths; Adele Goldberg"}, {"ref_id": "b34", "title": "Semantics in generative grammar", "journal": "Blackwell Oxford", "year": "1998", "authors": "Irene Heim; Angelika Kratzer"}, {"ref_id": "b35", "title": "A structural probe for finding syntax in word representations", "journal": "Long and Short Papers", "year": "2019", "authors": "John Hewitt; Christopher D Manning"}, {"ref_id": "b36", "title": "spaCy: Industrial-strength Natural Language Processing in Python", "journal": "", "year": "", "authors": ""}, {"ref_id": "b37", "title": "OCNLI: Original Chinese Natural Language Inference", "journal": "", "year": "2020", "authors": "Hai Hu; Kyle Richardson; Liang Xu; Lu Li; Sandra K\u00fcbler; Lawrence Moss"}, {"ref_id": "b38", "title": "A systematic assessment of syntactic generalization in neural language models", "journal": "", "year": "2020", "authors": "Jennifer Hu; Jon Gauthier; Peng Qian; Ethan Wilcox; Roger Levy"}, {"ref_id": "b39", "title": "What does BERT learn about the structure of language", "journal": "", "year": "2019", "authors": "Ganesh Jawahar; Beno\u00eet Sagot; Djam\u00e9 Seddah"}, {"ref_id": "b40", "title": "Are natural language inference models IMPPRESsive? Learning IMPlicature and PRESupposition", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Paloma Jeretic; Alex Warstadt; Suvrat Bhooshan; Adina Williams"}, {"ref_id": "b41", "title": "Formal system for grammatical representation. Formal Issues in Lexical-Functional Grammar", "journal": "", "year": "1995", "authors": "M Ronald; Joan Kaplan;  Bresnan"}, {"ref_id": "b42", "title": "Shallow-Deep Networks: Understanding and mitigating network overthinking", "journal": "", "year": "2019", "authors": "Yigitcan Kaya; Sanghyun Hong; Tudor Dumitras"}, {"ref_id": "b43", "title": "Overestimation of syntactic representation in neural language models", "journal": "", "year": "2020", "authors": "Jordan Kodner; Nitish Gupta"}, {"ref_id": "b44", "title": "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension", "journal": "", "year": "2020", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal ; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"ref_id": "b45", "title": "SA-NLI: A supervised attention based framework for natural language inference", "journal": "Neurocomputing", "year": "2020", "authors": "Peiguang Li; Hongfeng Yu; Wenkai Zhang; Guangluan Xu; Xian Sun"}, {"ref_id": "b46", "title": "Open sesame: Getting inside BERT's linguistic knowledge", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Yongjie Lin; Yi Chern Tan; Robert Frank"}, {"ref_id": "b47", "title": "Syntactic structure from deep learning. Annual Review of Linguistics", "journal": "", "year": "2021", "authors": "Tal Linzen; Marco Baroni"}, {"ref_id": "b48", "title": "Assessing the ability of LSTMs to learn syntax-sensitive dependencies", "journal": "Transactions of the Association for Computational Linguistics", "year": "2016", "authors": "Tal Linzen; Emmanuel Dupoux; Yoav Goldberg"}, {"ref_id": "b49", "title": "Roberta: A robustly optimized BERT pretraining approach. arXiv", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b50", "title": "Computational linguistics and deep learning", "journal": "Computational Linguistics", "year": "2015", "authors": "D Christopher;  Manning"}, {"ref_id": "b51", "title": "Emergent linguistic structure in artificial neural networks trained by self-supervision", "journal": "", "year": "2020", "authors": "Christopher D Manning; Kevin Clark; John Hewitt; Urvashi Khandelwal; Omer Levy"}, {"ref_id": "b52", "title": "Targeted syntactic evaluation of language models", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Rebecca Marvin; Tal Linzen"}, {"ref_id": "b53", "title": "BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance", "journal": "", "year": "2020", "authors": "R ; Thomas Mccoy; Junghyun Min; Tal Linzen"}, {"ref_id": "b54", "title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference", "journal": "", "year": "2019", "authors": "Tom Mccoy; Ellie Pavlick; Tal Linzen"}, {"ref_id": "b55", "title": "Efficient estimation of word representations in vector space", "journal": "", "year": "2013-05-02", "authors": "Tom\u00e1s Mikolov; Kai Chen; Greg Corrado; Jeffrey Dean"}, {"ref_id": "b56", "title": "Composition is the core driver of the language-selective network", "journal": "Neurobiology of Language", "year": "2020", "authors": "Francis Mollica; Matthew Siegelman; Evgeniia Diachek; T Steven; Zachary Piantadosi; Richard Mineroff; Hope Futrell; Peng Kean; Evelina Qian;  Fedorenko"}, {"ref_id": "b57", "title": "Universal grammar", "journal": "Theoria", "year": "1970", "authors": "Richard Montague"}, {"ref_id": "b58", "title": "Exploring numeracy in word embeddings", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Aakanksha Naik; Abhilasha Ravichander; Carolyn Rose; Eduard Hovy"}, {"ref_id": "b59", "title": "Stress test evaluation for natural language inference", "journal": "", "year": "2018", "authors": "Aakanksha Naik; Abhilasha Ravichander; Norman Sadeh; Carolyn Rose; Graham Neubig"}, {"ref_id": "b60", "title": "Human vs. muppet: A conservative estimate of human performance on the GLUE benchmark", "journal": "", "year": "2019", "authors": "Nikita Nangia; Samuel R Bowman"}, {"ref_id": "b61", "title": "Adversarial NLI: A new benchmark for natural language understanding", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Yixin Nie; Adina Williams; Emily Dinan; Mohit Bansal; Jason Weston; Douwe Kiela"}, {"ref_id": "b62", "title": "fairseq: A fast, extensible toolkit for sequence modeling", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Myle Ott; Sergey Edunov; Alexei Baevski; Angela Fan; Sam Gross; Nathan Ng; David Grangier; Michael Auli"}, {"ref_id": "b63", "title": "Sometimes we want translationese", "journal": "", "year": "2021", "authors": "Prasanna Parthasarathi; Koustuv Sinha; Joelle Pineau; Adina Williams"}, {"ref_id": "b64", "title": "Out of order: How important is the sequential order of words in a sentence in natural language understanding tasks?", "journal": "", "year": "2020", "authors": "M Thang; Trung Pham; Long Bui; Anh Mai;  Nguyen"}, {"ref_id": "b65", "title": "Hypothesis only baselines in natural language inference", "journal": "", "year": "2018", "authors": "Adam Poliak; Jason Naradowsky; Aparajita Haldar; Rachel Rudinger; Benjamin Van Durme"}, {"ref_id": "b66", "title": "Head-driven phrase structure grammar", "journal": "University of Chicago Press", "year": "1994", "authors": "Carl Pollard; A Ivan;  Sag"}, {"ref_id": "b67", "title": "Using priming to uncover the organization of syntactic representations in neural language models", "journal": "", "year": "2019", "authors": "Grusha Prasad; Marten Van Schijndel; Tal Linzen"}, {"ref_id": "b68", "title": "Language models are unsupervised multitask learners", "journal": "", "year": "2019", "authors": "Alec Radford; Jeffrey Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"ref_id": "b69", "title": "Studying the inductive biases of RNNs with synthetic variations of natural languages", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Shauli Ravfogel; Yoav Goldberg; Tal Linzen"}, {"ref_id": "b70", "title": "Can LSTM learn to capture agreement? the case of Basque", "journal": "", "year": "2018", "authors": "Shauli Ravfogel; Yoav Goldberg; Francis Tyers"}, {"ref_id": "b71", "title": "EQUATE: A benchmark evaluation framework for quantitative reasoning in natural language inference", "journal": "", "year": "2019", "authors": "Abhilasha Ravichander; Aakanksha Naik; Carolyn Rose; Eduard Hovy"}, {"ref_id": "b72", "title": "A primer in BERTology: What we know about how BERT works", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "Anna Rogers; Olga Kovaleva; Anna Rumshisky"}, {"ref_id": "b73", "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter", "journal": "", "year": "2020", "authors": "Victor Sanh; Lysandre Debut; Julien Chaumond; Thomas Wolf"}, {"ref_id": "b74", "title": "Parsing strategies with 'lexicalized' grammars: Application to Tree Adjoining Grammars", "journal": "", "year": "1988", "authors": "Yves Schabes; Anne Abeille; Aravind K Joshi"}, {"ref_id": "b75", "title": "Early german approaches to experimental reading research: The contributions of wilhelm wundt and ernst meumann", "journal": "Psychological Research", "year": "1981", "authors": "Eckart Scheerer"}, {"ref_id": "b76", "title": "A neural model of adaptation in reading", "journal": "", "year": "2018", "authors": "Marten Van Schijndel; Tal Linzen"}, {"ref_id": "b77", "title": "Quantity doesn't buy quality syntax with neural language models", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Aaron Marten Van Schijndel; Tal Mueller;  Linzen"}, {"ref_id": "b78", "title": "Masked language modeling and the distributional hypothesis: Order word matters pre-training for little", "journal": "", "year": "2021", "authors": "Koustuv Sinha; Robin Jia; Dieuwke Hupkes; Joelle Pineau; Adina Williams; Douwe Kiela"}, {"ref_id": "b79", "title": "The sentence superiority effect revisited", "journal": "Cognition", "year": "2017", "authors": "Joshua Snell; Jonathan Grainger"}, {"ref_id": "b80", "title": "Word position coding in reading is noisy. Psychonomic bulletin & review", "journal": "", "year": "2019", "authors": "Joshua Snell; Jonathan Grainger"}, {"ref_id": "b81", "title": "Syntactic innovation: A connectionist model", "journal": "", "year": "1994", "authors": "Whitney Tabor"}, {"ref_id": "b82", "title": "BERT rediscovers the classical NLP pipeline", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Ian Tenney; Dipanjan Das; Ellie Pavlick"}, {"ref_id": "b83", "title": "Changes in the constraints of semantic and syntactic congruity on memory across three age groups", "journal": "Perceptual and Motor Skills", "year": "2001", "authors": "Hiroshi Toyota"}, {"ref_id": "b84", "title": "Performance impact caused by hidden bias of training data for recognizing textual entailment", "journal": "", "year": "2018", "authors": "Masatoshi Tsuchiya"}, {"ref_id": "b85", "title": "What syntax can contribute in the entailment task", "journal": "Springer", "year": "2005", "authors": "Lucy Vanderwende;  William B Dolan"}, {"ref_id": "b86", "title": "Attention is all you need", "journal": "Curran Associates, Inc", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Illia Kaiser;  Polosukhin"}, {"ref_id": "b87", "title": "Universal adversarial triggers for attacking and analyzing NLP", "journal": "", "year": "2019", "authors": "Eric Wallace; Shi Feng; Nikhil Kandpal; Matt Gardner; Sameer Singh"}, {"ref_id": "b88", "title": "Superglue: A stickier benchmark for general-purpose language understanding systems", "journal": "", "year": "2019", "authors": "Alex Wang; Yada Pruksachatkun; Nikita Nangia; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel R Bowman"}, {"ref_id": "b89", "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Alex Wang; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel Bowman"}, {"ref_id": "b90", "title": "Can neural networks acquire a structural bias from raw linguistic data?", "journal": "", "year": "2020", "authors": "Alex Warstadt;  Samuel R Bowman"}, {"ref_id": "b91", "title": "Investigating BERT's knowledge of language: Five analysis methods with NPIs", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Alex Warstadt; Yu Cao; Ioana Grosu; Wei Peng; Hagen Blix; Yining Nie; Anna Alsop; Shikha Bordia; Haokun Liu; Alicia Parrish; Sheng-Fu Wang; Jason Phang; Anhad Mohananey; Paloma Phu Mon Htut; Samuel R Jeretic;  Bowman"}, {"ref_id": "b92", "title": "BLiMP: The benchmark of linguistic minimal pairs for English", "journal": "", "year": "2020", "authors": "Alex Warstadt; Alicia Parrish; Haokun Liu; Anhad Mohananey; Wei Peng; Sheng-Fu Wang; Samuel R Bowman"}, {"ref_id": "b93", "title": "Bowman", "journal": "", "year": "2019", "authors": "Alex Warstadt; Amanpreet Singh; Samuel R "}, {"ref_id": "b94", "title": "Parallel, cascaded, interactive processing of words during sentence reading", "journal": "Cognition", "year": "2019", "authors": "Yun Wen; Joshua Snell; Jonathan Grainger"}, {"ref_id": "b95", "title": "Lexicosyntactic inference in neural models", "journal": "", "year": "2018", "authors": "Aaron Steven White; Rachel Rudinger; Kyle Rawlins; Benjamin Van Durme"}, {"ref_id": "b96", "title": "What do RNN language models learn about filler-gap dependencies?", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Ethan Wilcox; Roger Levy; Takashi Morita; Richard Futrell"}, {"ref_id": "b97", "title": "Do latent tree learning models identify meaningful structure in sentences? Transactions of the Association for", "journal": "Computational Linguistics", "year": "2018", "authors": "Adina Williams; Andrew Drozdov; Samuel R Bowman"}, {"ref_id": "b98", "title": "A broad-coverage challenge corpus for sentence understanding through inference", "journal": "Long Papers", "year": "2018", "authors": "Adina Williams; Nikita Nangia; Samuel Bowman"}, {"ref_id": "b99", "title": "Tractatus Logico-Philosophicus", "journal": "Brace & Company, Inc", "year": "1922", "authors": "Ludwig Wittgenstein"}, {"ref_id": "b100", "title": "Some additional experiments extending the tech report\" assessing berts syntactic abilities\" by yoav goldberg", "journal": "", "year": "2019", "authors": "Thomas Wolf"}, {"ref_id": "b101", "title": "Transformers: State-of-theart natural language processing", "journal": "", "year": "2020", "authors": "Thomas Wolf; Julien Chaumond; Lysandre Debut; Victor Sanh; Clement Delangue; Anthony Moi; Pierric Cistac; Morgan Funtowicz; Joe Davison; Sam Shleifer"}, {"ref_id": "b102", "title": "Perturbed masking: Parameter-free probing for analyzing and interpreting BERT", "journal": "", "year": "2020", "authors": "Zhiyong Wu; Yun Chen; Ben Kao; Qun Liu"}, {"ref_id": "b103", "title": "Language modeling teaches you more than translation does: Lessons learned through auxiliary syntactic task analysis", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Kelly Zhang; Samuel Bowman"}, {"ref_id": "b104", "title": "Self-adaptive hierarchical sentence model", "journal": "", "year": "2015", "authors": "Han Zhao; Zhengdong Lu; Pascal Poupart"}, {"ref_id": "b105", "title": "Bert loses patience: Fast and robust inference with early exit", "journal": "Curran Associates, Inc", "year": "2020", "authors": "Wangchunshu Zhou; Canwen Xu; Tao Ge; Julian Mcauley; Ke Xu; Furu Wei"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "associates weren't operating at the level of metaphor.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Graphical representation of the Permutation Acceptance class of metrics. Given a sample test set D test with six examples, three of which originally predicted correctly (model predicts gold label), three incorrectly (model fails to predict gold label), with n = 6 permutations, \u2126 max ,\u2126 rand , \u2126 1.0 , P c and P f are provided. Green boxes indicate permutations accepted by the model. Blue boxes mark examples that crossed each threshold and were used to compute the corresponding metric.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Average entropy of model confidences on permutations that yielded the correct results for Transformer-based models (top) and Non-Transformerbased models (bottom). Results are shown for D c (orange) and D f (blue). The boxes show the quartiles of the entropy distributions.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: BLEU-2 score versus acceptability of permuted sentences across all test datasets. RoBERTa and BART performance is similar but differs considerably from the performance of non-Transformer-based models, such as InferSent and ConvNet.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 4 :4Figure 4: POS Tag Mini Tree overlap score and percentage of permutations which the models assigned the gold-label.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 6 :6Figure 6: Length and Permutation Acceptanceby Transformer-based models.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 8 :8Figure 8: \u2126 x threshold for all datasets with varying x and computing the percentage of examples that fall within the threshold. The top row consists of in-distribution datasets (MNLI, SNLI) and the bottom row contains out-ofdistribution datasets (ANLI)", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Models accept many permuted examples. We find \u2126 max is very high for models trained and evaluated on MNLI (in-domain generalization), reaching 98.7% on MNLI dev. and test sets (in RoBERTa, compared to A of 90.6% (Table2). Recall, human accuracy is approximately 92% on MNLI dev.", "figure_data": "ModelEval. DatasetA \u2126 maxP cP f \u2126 randMNLI m dev0.906 0.987 0.707 0.383 0.794MNLI mm dev 0.901 0.987 0.707 0.387 0.790SNLI dev0.879 0.988 0.768 0.393 0.826RoBERTa-LargeSNLI test0.883 0.988 0.760 0.407 0.828A1*0.456 0.897 0.392 0.286 0.364A2*0.271 0.889 0.465 0.292 0.359A3*0.268 0.902 0.480 0.308 0.397Mean0.652 0.948 0.611 0.351 0.623MNLI m dev0.902 0.989 0.689 0.393 0.784MNLI mm dev 0.900 0.986 0.695 0.399 0.788SNLI dev0.886 0.991 0.762 0.363 0.834BART-LargeSNLI test0.888 0.990 0.762 0.370 0.836A1*0.455 0.894 0.379 0.295 0.374A2*0.316 0.887 0.428 0.303 0.397A3*0.327 0.931 0.428 0.333 0.424Mean0.668 0.953 0.592 0.351 0.634MNLI m dev0.800 0.968 0.775 0.343 0.779MNLI mm dev 0.811 0.968 0.775 0.346 0.786SNLI dev0.732 0.956 0.767 0.307 0.731DistilBERTSNLI test0.738 0.950 0.770 0.312 0.725A1*0.251 0.750 0.511 0.267 0.300A2*0.300 0.760 0.619 0.265 0.343A3*0.312 0.830 0.559 0.259 0.363Mean0.564 0.883 0.682 0.300 0.575MNLI m dev0.658 0.904 0.842 0.359 0.712MNLI mm dev 0.669 0.905 0.844 0.368 0.723SNLI dev0.556 0.820 0.821 0.323 0.587InferSentSNLI test0.560 0.826 0.824 0.321 0.600A1*0.316 0.669 0.425 0.395 0.313A2*0.310 0.662 0.689 0.249 0.330A3*0.300 0.677 0.675 0.236 0.332Mean0.481 0.780 0.731 0.322 0.514MNLI m dev0.631 0.926 0.773 0.340 0.684MNLI mm dev 0.640 0.926 0.782 0.343 0.694SNLI dev0.506 0.819 0.813 0.339 0.597ConvNetSNLI test0.501 0.821 0.809 0.341 0.596A1*0.271 0.708 0.648 0.218 0.316A2*0.307 0.725 0.703 0.224 0.356A3*0.306 0.798 0.688 0.234 0.388Mean0.452 0.817 0.745 0.291 0.519MNLI m dev0.662 0.925 0.800 0.351 0.711MNLI mm dev 0.681 0.924 0.809 0.344 0.724SNLI dev0.547 0.860 0.762 0.351 0.598BiLSTMSNLI test0.552 0.862 0.771 0.363 0.607A1*0.262 0.671 0.648 0.271 0.340A2*0.297 0.728 0.672 0.209 0.328A3*0.304 0.731 0.656 0.219 0.331Mean0.472 0.814 0.731 0.301 0.520"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ": Statistics for Transformer-based modelstrained on MNLI corpus (Williams et al., 2018b). Thehighest values are bolded (red indicates the model mostinsensitive to permutation) per metric and per modelclass (Transformers and non-Transformers). A1*, A2*and A3* refer to the ANLI dev. sets (Nie et al., 2020).ModelA\u2126maxP cP f\u2126randRoBERTa-Large 0.784 0.988 0.726 0.339 0.773InferSent0.573 0.931 0.771 0.265 0.615ConvNet0.407 0.752 0.808 0.199 0.426BiLSTM0.566 0.963 0.701 0.271 0.611"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "\u00b10.068 0.454 0.649 \u00b10.102 0.515 \u00b10.089 Y 0.378 \u00b10.064 0.378 0.411 \u00b10.098 0.349 \u00b10.087", "figure_data": "Evaluator AccuracyMacro F1 Acc on D cAcc on D fX0.581"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Human (expert) evaluation on 200 permuted examples from the MNLI matched development set. Half of the permuted pairs contained shorter sentences and the other, longer ones. All permuted examples were assigned the gold label by RoBERTa-Large.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": ": NLI Accuracy (A) and Permutation Accep-tance metrics (\u2126 max ) of RoBERTa when trained onMNLI dataset using vanilla (V) and Maximum Ran-dom Entropy (ME) method."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "we show the effect of different thresholds, including \u2126 max where x = 1/|D test| and \u2126 rand where x = 0.34. To test this, we train Transformer-based models on top ofD train , which is computed by applying F on each example of D train for q = 1 times. This ensures a control case where we keep the same amount of training data as the standard setup (such", "figure_data": "We observe for in-distribution datasets (top row, MNLI and SNLI splits), in the extreme setting when x = 1.0, there are more than 10% of examples available, and more than 25% in case of InferSent and DistilBERT. For out-of-distribution datasets (bottom row, ANLI splits) we observe a much lower trend, suggesting generalization itself is the bottleneck in permuted sentence understanding. F Training with permuted examples In this section, we hypothesize that if the NLU models are mostly insensitive to word order, then training using permuted examples should also yield the same or comparable accuracy as training us-ing grammatically correct data (i.e., the standard setup). Eval Data MNLI Matched MNLI Mismatched 0.901 0.878 0.900 0.869 0.811 0.769 RoBERTa BART DistilBERT A\u00c2 A\u00c2 A\u00c2 0.906 0.877 0.902 0.862 0.800 0.760 SNLI Dev 0.879 0.870 0.886 0.854 0.732 0.719 SNLI Test 0.883 0.873 0.888 0.859 0.738 0.719 ANLI R1 (Dev) 0.456 0.367 0.455 0.336 0.251 0.250 ANLI R2 (Dev) 0.271 0.279 0.316 0.293 0.300 0.290 ANLI R3 (Dev) 0.268 0.271 0.327 0.309 0.312 0.312"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Statistics for Transformer-based models when trained on permuted MNLI corpus. We compare the accuracy for both models trained on unpermuted data (A) and the permuted data (\u00c2). We use original test sets during inference.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Dataset statistics used in this paper for inference. 'Used Examples' provides the number of premise-hypothesis pairs for the dataset which we selected for inference (i.e., examples where at least 100 unique permutations were possible).", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "Pr M (P i ,\u0124 i ) cor = 1 q (p j \u2208P i ,\u0125 j \u2208\u0124 i ) ((M (p j ,\u0125 j ) = y i ) \u2192 1)(1)", "formula_coordinates": [4.0, 317.98, 657.6, 207.56, 46.59]}, {"formula_id": "formula_1", "formula_text": "\u2126 x = 1 | D test | (p i ,h i )\u2208Dtest ((Pr M (P i ,\u0124 i ) cor > x) \u2192 1).", "formula_coordinates": [5.0, 83.61, 389.3, 205.01, 31.0]}, {"formula_id": "formula_2", "formula_text": "P c = 1 |D c | |D c | i=0 M (P i ,\u0124 i ) cor", "formula_coordinates": [5.0, 352.83, 113.35, 126.66, 36.19]}, {"formula_id": "formula_3", "formula_text": "\u03b2 k {w i ,S i } = 1 k | argmax k \u03c8 r {w i ,D train } \u2229 argmax k \u03c8 r {w i ,S i } | (4)", "formula_coordinates": [8.0, 82.57, 426.47, 207.7, 41.38]}, {"formula_id": "formula_4", "formula_text": "\u03b2 k {S i } = 1 |S i | w i \u2208S i \u03b2 k {w i ,S i }", "formula_coordinates": [8.0, 72.0, 572.66, 126.79, 16.02]}, {"formula_id": "formula_5", "formula_text": "\u03b2 k {\u015c i } /\u03b2 k {S i }", "formula_coordinates": [8.0, 473.19, 289.38, 51.85, 17.11]}, {"formula_id": "formula_6", "formula_text": "L = argmin \u03b8 ((p,h),y) y log(p(y|(p, h); \u03b8)) + n i=1 H y|(p i ,\u0125 i ); \u03b8", "formula_coordinates": [9.0, 88.77, 576.82, 178.82, 60.11]}], "doi": "10.3115/981823.981860"}