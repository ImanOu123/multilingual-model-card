{"title": "Learning inverse folding from millions of predicted structures", "authors": "Chloe Hsu; Robert Verkuil; Jason Liu; Zeming Lin; Brian Hie; Tom Sercu; Adam Lerer; Alexander Rives", "pub_date": "", "abstract": "We consider the problem of predicting a protein sequence from its backbone atom coordinates. Machine learning approaches to this problem to date have been limited by the number of available experimentally determined protein structures. We augment training data by nearly three orders of magnitude by predicting structures for 12M protein sequences using AlphaFold2. Trained with this additional data, a sequence-to-sequence transformer with invariant geometric input processing layers achieves 51% native sequence recovery on structurally held-out backbones with 72% recovery for buried residues, an overall improvement", "sections": [{"heading": "Introduction", "text": "Designing novel amino acid sequences that encode proteins with desired properties, known as de novo protein design, is a central challenge in bioengineering (Huang et al., 2016). The most well-established approaches to this problem use an energy function which directly models the physical basis of a protein's folded state (Alford et al., 2017).\nRecently a new class of deep learning based approaches has been proposed, using generative models to predict sequences for structures (Ingraham et al., 2019;Strokach et al., 2020;Anand-Achim et al., 2021;Jing et al., 2021b), generate backbone structures (Anand & Huang, 2018;Eguchi et al., 2020), jointly generate structures and sequences (Anishchenko et al., 2021;Wang et al., 2021), or model sequences directly (Rives et al., 2021;Madani et al., 2021;Shin et al., 2021;Gligorijevic et al., 2021;Bryant et   Augmenting inverse folding with predicted structures. To evaluate the potential for training protein design models with predicted structures, we predict structures for 12 million UniRef50 protein sequences using AlphaFold2 (Jumper et al., 2021). An autoregressive inverse folding model is trained to perform fixed-backbone protein sequence design. Train and test sets are partitioned at the topology level, so that the model is evaluated on structurally held-out backbones. We compare transformer models having invariant geometric input processing layers, with fully geometric models used in prior work. Span masking and noise is applied to the input coordinates.\n2021; Dallago et al., 2021). The potential to learn the rules of protein design directly from data makes deep generative models a promising alternative to current physics-based energy functions.\nHowever, the relatively small number of experimentally determined protein structures places a limit on deep learning approaches. Experimentally determined structures cover less than 0.1% of the known space of protein sequences.\nWhile the UniRef sequence database (Suzek et al., 2015) has over 50 million clusters at 50% sequence identity; as of January 2022, the Protein Data Bank (PDB) (Berman et al., 2000) contains structures for fewer than 53,000 unique sequences clustered at the same level of identity.\nHere we explore whether predicted structures can be used to overcome the limitation of experimental data. With progress in protein structure prediction (Jumper et al., 2021;Evans et al., 2022;Baek et al., 2021), it is now possible to consider learning from predicted structures at scale. Predicting structures for the sequences in large databases can expand the structural coverage of protein sequences by orders of magnitude. To train an inverse model for protein design, we predict structures for 12 million sequences in UniRef50 using AlphaFold2.\nWe focus on the problem of predicting sequences from backbone structures, known as inverse folding or fixed backbone design. We approach inverse folding as a sequenceto-sequence problem (Ingraham et al., 2019), using an autoregressive encoder-decoder architecture, where the model is tasked with recovering the native sequence of a protein from the coordinates of its backbone atoms.\nWe make use of the large number of sequences with unknown structures by adding them as additional training data, conditioning the model on predicted structures when the experimental structures are unknown (Figure 1). This approach parallels back-translation (Sennrich et al., 2015;Edunov et al., 2018) in machine translation, where predicted translations in one direction are used to improve a model in the opposite direction. Back-translation has been found to effectively learn from extra target data (i.e. sequences) even when the predicted inputs (i.e. structures) are of low quality.\nWe find that existing approaches have been limited by data.\nWhile current state-of-the-art inverse folding models degrade when training is augmented with predicted structures, much larger models and different model architectures can effectively learn from the additional data, leading to an improvement of nearly 10 percentage points in the recovery of sequences for structurally held out native backbones.\nWe evaluate models on fixed backbone design benchmarks from prior work, and assess the generalization capabilities across a series of tasks including design of complexes and binding sites, partially masked backbones, and multiple conformations. We further consider the use of the models for zero-shot prediction of mutational effects on protein function and stability, complex stability, and binding affinity.", "publication_ref": ["b25", "b0", "b27", "b63", "b2", "b31", "b17", "b4", "b73", "b52", "b41", "b57", "b20", "b10", "b33", "b13", "b64", "b7", "b33", "b19", "b5", "b27", "b56", "b16"], "figure_ref": ["fig_8"], "table_ref": []}, {"heading": "Learning inverse folding from predicted structures", "text": "The goal of inverse folding is to design sequences that fold to a desired structure. In this work, we focus on the backbone structure without considering side chains. While each of the 20 amino acid has a specific side chain, they share a common set of atoms that make up the amino acid backbone. Among the backbone atoms, we choose the N, C\u03b1 (alpha Carbon), and C atom coordinates to represent the backbone.\nUsing the structures of naturally existing proteins we can train a model for this task by supervising it to predict the protein's native sequence from the coordinates of its backbone atoms in three-dimensional space. Formally we represent this problem as one of learning the conditional distribution p(Y |X), where for a protein of length n, given a sequence X of spatial coordinates (x 1 , . . . , x i , . . . , x 3n ) for each of the backbone atoms N, C\u03b1, C in the structure, the objective is to predict Y the native sequence (y 1 , . . . , y i , . . . , y n ) of amino acids. This density is modeled autoregressively through a sequence-to-sequence encoder-decoder:\np(Y |X) = n i=1 p(y i |y i\u22121 , . . . , y 1 ; X)(1)\nWe train a model by minimizing the negative log likelihood of the data. We can design sequences by sampling, or by finding sequences that maximize the conditional probability given the desired structure.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data", "text": "Predicted structures We generate 12 million structures for sequences in UniRef50 to explore how predicted structures can improve inverse folding models. To select se-quences for structure prediction we first use MSA Transformer (Rao et al., 2021) to predict distograms for MSAs of all UniRef50 sequences. We rank the sequences by distogram LDDT scores (Senior et al., 2020) as a proxy for the quality of the predictions. We take the top 12 million sequences not longer than five hundred amino acids and forward fold them using the AlphaFold2 model with a final Amber (Hornak et al., 2006) relaxation. This results in a predicted dataset approximately 750 times the size of the training set of experimental structures (Appendix A.1).\nTraining and evaluation data We evaluate models on a structurally held-out subset of CATH (Orengo et al., 1997).\nWe partition CATH at the topology level with an 80/10/10 split resulting in 16153 structures assigned to the training set, 1457 to the validation set, and 1797 to the test set. Particular care is required to prevent leakage of information in the test set via the predicted structures. We use Gene3D topology classification (Lees et al., 2012) to filter both the sequences used for supervision in training, as well as the MSAs used as inputs for AlphaFold2 predictions (Appendix A.1). We also perform evaluations on a smaller subset of the CATH test set that has been additionally filtered by TMscore using Foldseek (van Kempen et al., 2022) to exclude any structures with similarity to those in the training set (Appendix B).", "publication_ref": ["b52", "b55", "b23", "b46", "b37", "b68"], "figure_ref": [], "table_ref": []}, {"heading": "Model architectures", "text": "We study model architectures using Geometric Vector Perceptron (GVP) layers (Jing et al., 2021b) that learn rotationequivariant transformations of vector features and rotationinvariant transformations of scalar features.\nWe present results for three model architectures: (1) GVP-GNN from Jing et al. (2021b) which is currently state-ofthe-art on inverse folding; (2) a GVP-GNN with increased width and depth (GVP-GNN-large); and (3) a hybrid model consisting of a GVP-GNN structural encoder followed by a generic transformer (GVP-Transformer). All models used in evaluations are trained to convergence, with detailed hyperparameters listed in Table A.1.\nIn inverse folding, the predicted sequence should be independent of the reference frame of the structural coordinates.\nFor any rotation and translation T of the input coordinates, we would like for the model's output to be invariant under these transformations, i.e., p(Y |X) = p(Y |T X). Both the GVP-GNN and GVP-Transformer inverse folding models studied in this work are invariant (Appendix A.3).", "publication_ref": ["b31", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "GVP-GNN", "text": "We start with the GVP-GNN architecture with 3 encoder layers and 3 decoder layers as described in (Jing et al., 2021b), with the vector gates described in (Jing et al., 2021a) (GVP-GNN, 1M parameters). As inputs to GVP-GNN, protein structures are represented as proximity graphs where each amino acid corresponds to a node in the graph.\nThe node features are a combination of scalar node features derived from dihedral angles and vector node features derived from the relative positions of the backbone atoms, while the edge features capture the relative positions of nearby amino acids.\nWhen trained on predicted structures, we find a deeper and wider version of GVP-GNN with 8 encoder layers and 8 decoder layers (GVP-GNN-large, 21M parameters) performs better. Scaling GVP-GNN further did not improve model performance in preliminary experiments (Figure 6c).", "publication_ref": ["b31", "b30"], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "GVP-Transformer", "text": "We use GVP-GNN encoder layers to extract geometric features, followed by a generic autoregressive encoder-decoder Transformer (Vaswani et al., 2017). In GVP-GNN, the input features are translation-invariant and each layer is rotation-equivariant. We perform a change of basis on the vector features from GVP-GNN into local reference frames defined for each amino acid to derive rotationinvariant features (Appendix A.3). In ablation studies increasing the number of GVP-GNN encoder layers improves the overall model performance (Figure C.1), indicating that the geometric reasoning capability in GVP-GNN is complementary to the Transformer layers. Scaling improves performance up to a 142M-parameter GVP-Transformer model with 4 GVP-GNN encoder layers, 8 generic Transformer encoder layers, and 8 generic Transformer decoder layers (Figure 6c).", "publication_ref": ["b70"], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "Training", "text": "Combining data ratio. For larger models, a high ratio of predicted data during training helps prevent overfitting on the smaller experimental train set (Figure 6b). While adding predicted data improves performance, training only on predicted data leads to substantially worse performance (Table C.2).\nThe loss is equally weighted for each amino acid in target sequences. We mask out predicted input coordinates with AlphaFold2 confidence score (pLDDT) below 90, around 25% of the predicted coordinates. See Figure 3 for visualization of the pLDDT confidence score. Most often these low confidence regions are at the start and the end of sequences and may correspond to disordered regions. We prepend one token at the beginning of each sequence to indicate whether the structure is experimental or predicted. For each residue we provide the pLDDT confidence score from AlphaFold2 as a feature encoded by Gaussian radial basis functions.\nAdding Gaussian noise at the scale of 0.1 angstroms to the predicted structures during training slightly improves performance (Table C.1). This finding is consistent with Edunov et al. (2018), who observe that backtranslation with sampled or noisy synthetic data provides a stronger training signal than maximum a posteriori (MAP) predictions.\nSpan masking To enable sequence design for partially masked backbones, we introduce backbone masking during training. We experiment with both independent random masking and span masking. In natural language processing, span masking improves performance over random masking (Joshi et al., 2020). We randomly select continuous spans of up to 30 amino acids until 15% of input backbone coordinates are masked. The communication patterns in the geometric layers are adapted to account for masking with details in Appendix A.2. Span masking improves the performance of GVP-Transformer both on unmasked backbones (Table C.1) and on masked regions (Figure 4).  The GVP-GNN architecture degrades to the perplexity of the background distribution for masked regions of more than a few tokens, while GVP-Transformer maintains moderate accuracy on long masked spans, especially when trained on masked spans.", "publication_ref": ["b16", "b32"], "figure_ref": ["fig_6", "fig_2", "fig_4"], "table_ref": []}, {"heading": "Results", "text": "We evaluate models across a variety of benchmarks in two overall settings: fixed backbone sequence design and zeroshot prediction of mutation effects. For fixed backbone design, we start with evaluation in the standard setting (Ingraham et al., 2019;Jing et al., 2021b) of sequence design given all backbone coordinates. Then, we make the sequence design task more challenging along three dimensions: (1) introducing masking on coordinates; (2) generalization to protein complexes; and (3) conditioning on multiple conformations. Additionally, we show that inverse folding models are effective zero-shot predictors for protein complex stability, binding affinity, and insertion effects.", "publication_ref": ["b27", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Fixed backbone protein design", "text": "We begin with the task of predicting the native protein sequence given its backbone atom (N, C\u03b1, C) coordinates. Perplexity and sequence recovery on held-out native sequences are two commonly used metrics for this task. Perplexity measures the inverse likelihood of native sequences in the predicted sequence distribution (low perplexity for high likelihood). Sequence recovery (accuracy) measures how often sampled sequences match the native sequence at each position. To maximize sequence recovery, the predicted sequences are sampled with low temperature T = 1e\u22126 from the model. While the model is calibrated (Figure C.5), a lower temperature results in sequences with higher likelihoods (and hence typically higher sequence recovery) and lower diversity. Empirically at temperature as low as 1e\u22126 the sampling is almost deterministic. Table 1 compares models using the perplexity and sequence recovery metrics on the structurally held-out backbones.\nWe observe that current state-of-the-art inverse folding models are limited by the CATH training set. Scaling the current 1M parameter model (GVP-GNN) to 21M parameters (GVP-GNN-large) on the CATH dataset results in overfitting with a degradation of sequence recovery from 42.2% to 39.2% (Table 1). On the other hand, the current model at the 1M parameter scale cannot make use of the predicted structures: training GVP-GNN with predicted structures results in a degradation to 38.6% sequence recovery (Table 1), with performance worsening with increasing numbers of predicted structures in training (Figure 6a).\nLarger models benefit from training on the AlphaFold2predicted UniRef50 structures. Training with predicted structures increases sequence recovery from 39.2% to 50.8% for GVP-GNN-large and from 38.3% to 51.6% for GVP-Transformer over training only on the experimentally derived structures. The improvements are also reflected in perplexity. Similar improvements are observed on the test subset filtered by TM-score (Table B.1). The best model trained with UniRef50 predicted stuctures, GVP-Transformer, improves sequence recovery by 9.4 percentage points over the best model, GVP-GNN, trained on CATH alone.\nAs there are many sequences that can fold to approximately the same structure, even an ideal protein design model will not have 100% native sequence recovery. We observe that the GVP-GNN-large and GVP-Transformer models are wellcalibrated (Figure C.5). The substitution matrix between native sequences and model-designed sequences resembles the BLOSUM62 substitution matrix (Figure C.4), albeit noticeably sparser for the amino acid Proline.\nWhen we break down performance on core residues and surface residues, as expected, core residues are more constrained and have a high native sequence recovery rate of Each box shows the distribution of perplexities for the core or surface residues across different sequences. Bottom: Perplexity and sequence recovery as a function of solvent accessible surface area. Increased sequence recovery for buried residues suggests the model learns dense hydrophobic packing constraints in the core. 72%, while surface residues are not as constrained and have a lower sequence recovery of 39% (Figure 5; top). Generally perplexity increases with the solvent accessible surface area (Figure 5; bottom). Despite the lower sequence recovery on the surface, sampled sequences do tend not to have hydrophobic residues on the surface (Figure C.6).\nAs an example of inverse folding of a structurally-remote protein, we re-design the receptor binding domain (RBD) sequence of the SARS-CoV-2 spike protein (PDB: 6XRA and 6VXX; illustrated in Figure C.3) with the two models. The SARS-CoV-2 spike protein has no match to the training data with TM-score above 0.5. Both GVP-GNN and GVP-Transformer achieve high sequence recovery (49.7% and 53.6%) for the native RBD sequence (Table C.4). See Table C.8 for a list of randomly sampled sequence designs.\nWhile perplexity and sequence recovery are informative metrics, low perplexity and high sequence recovery do not necessarily guarantee structural similarity. One empirically observed failure mode in sampled sequences is repetition of the same amino acid, e.g. EEEEEEE. It would be interesting to further identify more failure modes by studying the experimental or predicted structures of sampled sequences. Table 2. Sequence design performance on complexes in the CATH topology test split when given the backbone coordinates of only a chain (\"Chain\" column) and when given all backbone coordinates of the complex (\"Complex\" column). The perplexity is evaluated on the same chain in the complex for both columns.\nPartially-masked backbones We evaluate the models on partial backbones. While masking during training does not significantly change test performance on unmasked backbones (Table C.1), masking does enable models to non-trivially predict sequences for mask regions. Although GVP-GNN-large has low perplexity on short-length masks, its performance quickly degrades to the perplexity of the background distribution on masks longer than 5 amino acids (Figure 4). By contrast, the GVP-Transformer model maintains moderate performance even on longer masked regions, with less degradation if trained with span masking instead of independent random masking (Figure 4).\nProtein complexes Although the training data only consists of single chains, we find that models generalize to multi-chain protein complexes. We represent complexes by concatenating the chains together with 10 mask tokens between chains, and place the target chain for sequence design at the beginning during concatenation. We include all complexes in the CATH 4.3 test set up to 1000 amino acids in length. For chains that are part of a protein complex, there is a substantial improvement in perplexity of both models when given the full complex coordinates as input, versus only the single chain (Table 2 and Figure  that both GVP-GNN and GVP-Transformer can make use of inter-chain information from amino acids that are close in 3D structure but far apart in sequence.\nMultiple conformations Multi-state design is of interest for engineering enzymes and biosensors (Langan et al., 2019;Quijano-Rubio et al., 2021). Some proteins exist in multiple distinct folded forms in equilibrium, while other proteins may exhibit distinct conformations when binding to partner molecules.\nFor a backbone X, the inverse folding model predicts a conditional distribution p(Y |X) over possible sequences Y for the backbone. To design a protein sequence compatible with two states A and B, we would like find sequences with high likelihoods in the conditional distributions p(Y |A) and p(Y |B) for each state. We use the geometric average of the two conditional likelihoods as a proxy for the desired Table 3. Zero-shot performance on binding affinity prediction for the receptor binding domain (RBD) of SARS-CoV-2 Spike, evaluated on ACE2-RBD mutational scan data (Starr et al., 2020). The zero-shot predictions are based on the sequence log-likelihood for the receptor binding motif (RBM), which is the portion of the RBD in direct contact with ACE2 (Lan et al., 2020). We evaluate in four settings: 1) Given sequence data alone (\"No coords\"); 2) Given backbone coordinates for both ACE2 and the RBD but excluding the RBM and without sequence (\"No RBM coords\"); 3) Given the full backbone for the RBD but no information for ACE2 (\"No ACE2 coords\"); and 4) Given all coordinates for the RBD and ACE2.\ndistribution p(Y |A, B) conditioned on the sequence being compatible with both states.\nWe compare single-state and multi-state sequence design performance on 87 test split proteins with multiple conformations in the PDBFlex dataset (Hrabe et al., 2016). On locally flexible residues, multi-state design results in lower sequence perplexity than single-state design (Figure 7). See Appendix C for more details on the PDBFlex data.", "publication_ref": ["b36", "b50", "b60", "b35", "b24"], "figure_ref": ["fig_6", "fig_5", "fig_5", "fig_4", "fig_4", "fig_7"], "table_ref": []}, {"heading": "Zero-shot predictions", "text": "We next show that inverse folding models are effective zeroshot predictors of mutational effects across practical design applications, including prediction of complex stability, binding affinity, and insertion effects. To score the effect of a mutation on a particular sequence, we use the ratio between likelihoods of the mutated and wildtype sequences according to the inverse folding model, given the experimentally determined wildtype structure. Exact likelihood evaluations are possible from both GVP-GNN and GVP-Transformer as they are both based on autoregressive decoders. We then compare these likelihood ratio scores to experimentally-determined fitness values measured on the same set of sequences.\nDe novo mini-proteins Rocklin et al. (2017) performed deep mutational scans across a set of de novo designed miniproteins with 10 different folds measuring the stability in response to point mutations. The likelihoods of inverse folding models have been shown to correlate with experimentally measured stability using this dataset (Ingraham et al., 2019;Jing et al., 2021b). We evaluate the GVP-Transformer and GVP-GNN-large models on the same mutational scans, and observe improvements in stability predictions from using predicted structures as training data for 8 out of 10 folds in the dataset (Table C.3). Further details are in Appendix C.", "publication_ref": ["b54", "b27", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Complex stability", "text": "We evaluate models on zero-shot prediction of mutational effects on protein complex interfaces, using the Atom3D benchmark (Townshend et al., 2020) which incorporates binding free energy changes in the SKEMPI database (Jankauskait\u0117 et al., 2019) as a binary classification task. We find that sequence log-likelihoods from GVP-GNN are effective zero-shot predictors of stability changes of protein complexes even without predicted structures as training data (Table C.5), performing comparably to the best supervised method which uses transfer learning. While we observe a substantial improvement in perplexity when predicted structures are added to training (Table 2), this does not further improve complex stability prediction for the single-point mutations in SKEMPI (Table C.5), indicating potential limitations of evaluating models only on single-point mutations.\nBinding affinity While the SKEMPI dataset features one mutation entry per protein, we also want to evaluate whether inverse folding models can rank different mutations on the same protein, potentially enabling binding-affinity optimization, which is an important task in therapeutic design. We assess whether inverse folding models can predict mutational effects on binding by leveraging a dataset generated by Starr et al. (2020) in which all single amino acid substitutions to the SARS-CoV-2 receptor binding domain (RBD) were experimentally measured for binding affinity to human ACE2. Given potential applications to interface optimization or design, we focus on mutations within the receptor binding motif (RBM), the portion of the RBD in direct contact with ACE2 (Lan et al., 2020). When given all RBD and ACE2 coordinates, the best inverse folding model produces RBD-sequence log-likelihoods that have a Spearman correlation of 0.69 with experimental binding affinity measurements (Table 3). We observe weaker correlations when not providing the model with ACE2 coordinates, indicating that inverse folding models take advantage of structural information in the binding partner. When masking RBM coordinates (69 of 195 residues, a longer span than masked during model training), we no longer observe correlation between RBD log-likelihood and binding affinity, indicating that the model relies on structural information at the interface to identify interface designs that preserve binding. Zero-shot prediction via inverse folding outperforms methods for sequence-based variant effect prediction, which use the likelihood ratio between the mutant and wildtype amino acids at each position to predict the impact of a mutation on binding affinity. These likelihoods are inferred by masked language models, ESM-1b, ESM-1v, and ESM-MSA-1b, as described by Meier et al. ( 2021) (Table 3); additional details are given in Appendix C.\nSequence insertions Using masked coordinate tokens at insertion regions, inverse folding models can also predict insertion effects. On adeno-associated virus (AAV) capsid variants, we show that relative differences in sequence log-likelihoods correlate with the experimentally measured insertion effects from Bryant et al. (2021). As shown in Table C.6, both GVP-GNN and GVP-Transformer outperform the sequence-only zero-shot prediction baseline ESM-1v (Meier et al., 2021). When evaluating on subsets of sequences increasingly further away from the wildtype (\u2265 2, \u2265 3, and \u2265 8 mutations), the GVP-GNN-large and GVP-Transformer models trained with predicted structures have increasing advantages compared to GVP-GNN trained without predicted structures.", "publication_ref": ["b65", "b28", "b60", "b35", "b10", "b52"], "figure_ref": [], "table_ref": []}, {"heading": "Related work", "text": "Structure-based protein sequence design Early work on design of protein sequences studied the packing of amino acid side chains to fill the interior space of predetermined backbone structures, either for a fixed backbone conformation (Street & Mayo, 1999;Dahiyat & Mayo, 1997;De-Grado et al., 1991), or with flexibility in the backbone conformation (Harbury et al., 1998). Since then, the Rosetta energy function (Alford et al., 2017) has become an established approach for structure-based sequence design. An alternative non-parametric approach involves decomposing the library of known structures into common sequencestructure motifs (Zhou et al., 2020).\nEarly machine learning approaches in structure-based protein sequence design used fragment-based and energy-based global features derived from structures (Li et al., 2014;O'Connell et al., 2018). More recently, convolution-based deep learning methods have also been applied to predict amino acid propensities given the surrounding local structural environments (Anand-Achim et al., 2021;Boomsma & Frellsen, 2017;Shroff et al., 2020;Li et al., 2020;Qi & Zhang, 2020;Zhang et al., 2020;Chen et al., 2019;Wang et al., 2018). Another recent machine learning approach is to leverage structure prediction networks for sequence design. Anishchenko et al. (2021) carried out Monte Carlo sampling in the sequence space to invert the trRosetta (Yang et al., 2020) structure prediction network for sequence design, while Norn et al. (2021) backpropagated gradients through the trRosetta network. Language models A large body of work has focused on modeling the sequences in individual protein families. Shin et al. (2021) show that protein-specific autoregressive sequence models trained on related proteins can predict point mutation and indel effects and design functional nanobodies. Trinquier et al. (2021) also studied protein-specific autoregressive models for sequence generation.\nRecently language models have been proposed for modeling large scale databases of protein sequences rather than families of related sequences. Examples include (Bepler & Berger, 2019;Alley et al., 2019;Heinzinger et al., 2019;Rao et al., 2019;Madani et al., 2020;Elnaggar et al., 2021;Rives et al., 2021;Rao et al., 2021). Meier et al. (2021) found that the log-likelihoods of large protein language models predict mutational effects. Madani et al. (2021) study an autoregressive sequence model conditioned on functional annotations and show it can generate functional proteins.\nStructure-agnostic protein sequence design We point the reader to Wu et al. (2021) for a review of the many machine learning-based sequence design approaches that do not explicitly model protein structures. Additionally, as an alternative to sequence generation models, model-guided algorithms design sequences based on predictive models as oracles (Yang et al., 2019;Angermueller et al., 2019;Brookes et al., 2019;Sinai et al., 2020).\nBack-translation For machine translation (MT) in NLP, Sennrich et al. (2015) studied how to leverage large amounts of monolingual data in the target language, a setting that parallels the situation we consider with protein sequences (the target language in our case). Sennrich et al. found it most effective to generate synthetic source sentences by performing the backwards translation from the target sentence, i.e. back-translation. This parallels the approach we take of predicting structures for sequence targets that have unknown structures. Edunov et al. (2018) further investigated back-translation for large-scale language models.", "publication_ref": ["b62", "b12", "b21", "b0", "b79", "b39", "b45", "b2", "b8", "b58", "b38", "b49", "b78", "b11", "b72", "b4", "b75", "b4", "b57", "b66", "b6", "b22", "b51", "b40", "b18", "b52", "b52", "b52", "b41", "b74", "b76", "b3", "b9", "b59", "b56", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions", "text": "While there are billions of protein sequences in the largest sequence databases, the number of available experimentally determined structures is on the order of hundreds of thousands, imposing a limit on generative methods that learn from protein structure data. In this work, we explored whether predicted structures from recent deep learning methods can be used in tandem with experimental structures to train models for protein design.\nTo this end, we generated structures for 12 million UniRef50 sequences using AlphaFold2. As a result of training with this data we observe improvements in perplexity and sequence recovery by substantial margins, and demonstrate generalization to longer protein complexes, to proteins in multiple conformations, and to zero-shot prediction for mutation effects on binding affinity and AAV packaging. These results highlight that in addition to the geometric inductive biases which have been the major focus for work on inversefolding to date, finding ways to leverage more sources of training data is an equally important path to improved modeling capabilities.\nContemporary with our work, the AlphaFold Protein Structure Database (Varadi et al., 2021) is rapidly growing, featuring 1 million predicted structures as of June 2022. Structurebased protein design models will likely continue to benefit from this new data source as the coverage expands to encompass more of the structural universe. Additionally, with the recent progress in structure prediction for multi-chain protein complexes (Evans et al., 2022;Humphreys et al., 2021), predicted complex structures could be another valuable source of data for learning protein-protein interactions.\nWe also take initial steps toward more general structureconditional protein design tasks. By integrating backbone span masking into the inverse folding task and using a sequence-to-sequence transformer, reasonable sequence predictions can be achieved for short masked spans.\nIf ways can be found to continue to leverage predicted structures for generative models of proteins, it may be possible to create models that learn to design proteins from an expanded universe of the billions of natural sequences whose structures are currently unknown. ", "publication_ref": ["b69", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "A. Additional details on datasets, training procedures, and model architectures", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1. Details on dataset of predicted structures", "text": "We used training data from two sources: 1) experimental protein structures from the CATH 40% non-redundant chain set, and 2) AlphaFold2-predicted structures from UniRef50 sequences. To evaluate the generalization performance across different protein folds, we split the train, validation, and test data based on the CATH hierarchical classification of protein structures (Orengo et al., 1997) for both data sources. To achieve that a rigorous structural hold-out, we additionally use foldseek (van Kempen et al., 2022) for pairwise TMalign between the test set the train set.\nCATH topology split. Following the structural split methodology in previous work (Ingraham et al., 2019;Jing et al., 2021b;Strokach et al., 2020), we randomly split the CATH v4.3 (latest version) topology classification codes into train, validation, and test sets at a 80/10/10 ratio. The CATH (Orengo et al., 1997) structural hierarchy, classifies domains in four levels: Class (C), Architecture (A), Topology/fold (T), and Homologous superfamily (H). The topology/fold (T) level roughly corresponds to the SCOP fold classification.\nExperimental structures. We collected full chains up to length 500 for all domains in the CATH v4.3 40% sequence identity non-redundant set. The experimental structure data contained only stand-alone chains and no multichain complexes.\nAs each chain may be classified with more than one topology codes, we further removed chains with topology codes spanning different splits, so that there is no overlap in topology codes between train, validation, and test. This results in 16,153 chains in the train split, 1457 chains in the validation split, and 1797 chains in the test split.\nPredicted structures. We curated a new data set of AlphaFold2 (Jumper et al., 2021)-predicted structures for a selective subset of UniRef50 ( 202001) sequences. To prevent information leakage about the test set from the predicted structures, we proceeded in the following steps.\nFirst, we annotated UniRef50 sequences with CATH classification according to the Gene3D (Lees et al., 2012) database, also used by Strokach (Strokach et al., 2020) for data curation. Gene3D represents each CATH classification code as a library of representative profile HMMs. We searched all HMMs associated with the validation and test splits against the UniRef50 sequences using default parameters in hmmsearch (Potter et al., 2018) and excluded all hits.\nAdditionally, as AlphaFold2 predictions use multiple sequence alignments (MSAs) as inputs, we also took precaution to avoid information leakage from sequences in the MSAs. We created a filtered version of UniRef100 by searching all the validation-split and test-split Gene3D HMMs against UniRef100 (202001) and excluding all hits. Then, we constructed our MSAs using hhblits (Steinegger et al., 2019) on this filtered version of UniRef100. While this filtering step was out of precaution, in retrospect it was perhaps unlikely for the MSA inputs to AlphaFold2 to leak information as the MSAs themselves were not seen during training. The filtering step may have negatively impacted the quality of the resulting predicted structures, although empirically only a very small percentage of MSA sequences were filtered out.\nAs AlphaFold2 predictions are computationally costly, our budget only allowed for predicting structures for a subset of the UniRef50 sequences. We ranked UniRef50 sequences based on the distogram lDDT score, based on distogram predictions from MSATransformer (Rao et al., 2021), as a proxy for the quality of predicted structures. While the original distogram lDDT score (Supplementary Equation 6 in (Senior et al., 2020)) is based on pairwise distances from native protein structures, in the absence of native structures we use the argmax of pairwise distances instead, effectively measuring the \"sharpness\" of distograms and prioritizing sharper predictions. In this order, using AlphaFold2 Model 1 on the filtered UniRef100 MSAs described above, we obtained predicted structures for the top 12 million UniRef50 sequences under length 500, roughly 750 times the CATH train set size.\nWe used the publicly released model weights from AlphaFold2 Model 1 for CASP14 as a single model, as opposed the 5-model ensemble in (Jumper et al., 2021), to cover more sequences with the same amount of computing resources. We curated the input MSAs from UniRef100 with hhblits, with an additional filtering step as described above. To reduce computational costs, compared to the standard AlphaFold2 protocol, we did not include the UniRef90 jackhmmer MSAs, or the MGnify and BFD metagenomics MSAs, nor the pdb70 templates. Other than a reduced inputs, we followed the default settings in AlphaFold2 open source code, using 3 recycling iterations and the default Amber relaxation protocol. Despite the reduced inputs, the resulting 12 million predicted structures still have high pLDDT scores from AlphaFold, with 75% of residues having pLDDT above 90 (highly confident). We found that increasing the predicted data size to up to 1 million structures (75 times the CATH experimental data size) substantially improves model performance. Beyond 1 million structures, models still benefit from more data but with diminished marginal returns (Figure 6a).\nNoise on AlphaFold2-predicted backbone coordinates. Even after Amber relaxation, the backbone coordinates predicted by AlphaFold2 contain artifacts in the sub-Angstrom scale that may give away amino acid identities. Without adding noise on predicted structures, there is a substantial gap between held-out set performance on predicted structures and on experimental structures. To prevent the model from learning non-generalizable AlphaFold2-specific rules, we added Gaussian noise at the 0.1A scale on predicted backbone coordinates. The Gaussian noise improves the invariant Transformer performance but not the GVP-GNN performance (Supplementary Figure C.1).", "publication_ref": ["b46", "b68", "b27", "b31", "b63", "b46", "b33", "b37", "b63", "b48", "b61", "b52", "b55", "b33"], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "A.2. Details on span masking", "text": "We add a binary feature indicating whether each coordinate is masked or not. In GVP-Transformer, we exclude the masked nodes in the GVP-GNN encoder layers, and then impute zeros when passing the GVP-GNN outputs into the main Transformer. Imputing zeros for missing vector features ensure the rotation-and translation-invariance of the model. In GVP-GNN, we impute zeros for the input vector features, and in the input graph connect the masked nodes to the k sequence nearest-neighbors (k = 30) in lieu of the k nearest nodes by spatial distance.\nFor span masking, we randomly select continuous spans of up to 30 amino acids until 15% of input backbone coordinates are masked. Such a span masking scheme has shown to improve performance on natural language processing benchmarks (Joshi et al., 2020). The span lengths are sampled from a geometric distribution Geo(p) where p = 0.05 (corresponding to an average span length of 1/p = 20). The starting points for the spans are uniformly randomly sampled. Compared to independent random masking, span masking is better for GVP-Transformer but not for GVP-GNN (Table C.1).\nFor the amino acids with masked coordinates, we exclude the corresponding nodes from the input graph to the pre-processing GVP message passing layers, and then impute zeros for the geometric features when passing the GVP outputs into the main Transformer. Imputing zeros for missing vector features ensure the rotation-and translation-invariance of the model.", "publication_ref": ["b32"], "figure_ref": [], "table_ref": []}, {"heading": "A.3. Details on model architectures", "text": "Autoregressive modeling. GVP-GNN and GVP-Transformer both have encoder-decoder architectures. The encoder only receives the structural features. The decoder receives the encoder output along with the one-hot encoding of the amino acids.\nIn the autoregresive decoder, sequence information only propagate from amino acid i to j for i < j. The last decoder layer produces a 20-way scalar output per position and softmax activation to predict the probabilities for the amino acid identity at the next position in the sequence.\nInvariance to rotation and translation. The input features for both GVP-GNN and GVP-Transformer are translationinvariant, making the overall models also invariant to translations.\nEach GVP-GNN layer is rotation-equivariant, that is, for a vector feature x and any arbitrary rotation T , T f (x) = f (T x).\nWith equivariant intermediate layers and an invariant output projection layer, GVP-GNN is overall invariant to rotations, since the composition of an equivariant function f with an invariant function g produces an invariant function g(f (x)).\nThe GVP-Transformer architecture is also invariant to rotations and translations. The initial GVP-GNN layers in GVP-Transformer output rotation-invariant scalar features and rotation-equivariant vector features for each amino acid. To make the overall GVP-Transformer invariant, we perform a change of basis on GVP-GNN vector outputs to produce rotation-invariant features for the Transformer. More specifically, for each amino acid, we define a local reference frame based on the N, CA, and C atom positions in the amino acid, following Algorithm 21 in AlphaFold2 (Jumper et al., 2021). We then perform a change of basis according to this local reference frame, rotating the vector features in GVP-GNN outputs into the local reference frames of each amino acid. (If GVP-GNN outputs are used directly as Transformer inputs without this change of basis, the GVP-Transformer model would not be rotation-invariant.) We concatenate this rotated \"local version\" of vector features together with the scalar features as inputs to the Transformer. The concatenated features are invariant to both translations and rotations on the input backbone coordinates, forming a L \u00d7 E matrix where L is the number of amino acids in the protein backbone and E is the feature dimension. For amino acids with masked or missing coordinates, the features are imputed as zeros.\nTransformer. We closely followed the original autoregressive encoder-decoder Transformer architecture (Vaswani et al., 2017) except for using learned positional embeddings instead of sinusoidal positional embeddings, attention dropout, and layer normalization inside the residual blocks (\"pre-layernorm\"). For model scaling experiments, we followed the model sizes in (Turc et al., 2019), and chose the 142-million-parameter model with 8 encoder layers, 8 decoder layers, 8 attention heads, and embedding dimension 512 based on the best validation set performance (Figure 6c shows test set ablation).\nThe GVP-GNN, GVP-GNN-large, and GVP-Transformer models used in the evaluations in this manuscript are all trained to convergence, with detailed hyperparameters listed in Table A.1.  Distribution of the highest TM-score from each test example to the train set. For example, 54% of the CATH topology split test set has at least one match in the train set with TM-score above 0.5, and 27% of the topology split test set has at least one match in the train set with TM-score above 0.6. ", "publication_ref": ["b33", "b70", "b67"], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "B. TM-score-based test set", "text": "In addition to the CATH topology-based test set following previous work (Ingraham et al., 2019;Jing et al., 2021b) We found that the conclusions about model performance overall remains the same on this TM-score-based test set as on the CATH topology split test set. For consistency with prior work, we report metrics on the CATH topology test set in the main manuscript, while showing metrics on the smaller TM-score-based test set in Table B.1.", "publication_ref": ["b27", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "C. Additional results and details", "text": "Ablation on noise and masking during training. We found that GVP-Transformer models trained with Gaussian noise during training perform slightly better at test time than those trained without (Table C.1). When given full backbone coordinates at test time, training with span masking only very slightly improves model performance compared to no masking or to random masking, even though there is a much larger performance gap between random masking and span masking on regions with masked backbone coordinates (Figure 4).\nDual-state design test set from PDBFlex. We test design performance on multiple conformations by finding test split proteins with distinct conformations in the PDBFlex database. From PDBFlex, we looks for experimental structures of protein sequences in the CATH topology split test set (95% sequence identity or above), and take all paired instances that are at least 5 angstroms apart in overall RMSD between conformations. We report perplexity on locally flexible residues (defined as local RMSD above 1 angstrom). To be more conservative in our evaluation, we show the better of the two conformations to represent single-state perplexity in Figure 7.\nAblation on the number of GVP-GNN encoder layers in GVP-Transformer. Increasing the number of GVP-GNN encoder layers improves the overall model performance (Figure C.1), indicating that the geometric reasoning capability in GVP-GNN is complementary to the Transformer layers.   (Rocklin et al., 2017), with highest correlation bolded.\nModel performance when trained only on predicted structures. When trained on the 12 million predicted structures without including any of the experimental structures from CATH in training data, the model performance of GVP-GNN, GVP-GNN-large, and GVP-Transformer is across the board substantially worse than when trained only on the CATH structures (Table C.2). This gap is especially pronounced for the larger GVP-GNN-large and GVP-Transformer models.\nStability prediction on de novo small proteins. We predict protein stability on an experimentally measured stability dataset for de novo small proteins (Rocklin et al., 2017). We use the relative difference in sequence conditional loglikelihoods as a predictor for stability and compute Pearson correlation with the mutation effect following (Ingraham et al., 2019), assuming that more stable sequences should score higher in log-likelihoods. For each fold, Rocklin et al. (2017) starts with a reference protein and generates sequence variants with single amino acid substitutions. We calculate the Pearson correlation between sequence conditional log-likelihood scores and experimental stability measurements for all designed sequences in each fold. With predicted structures as additional training data, the GVP-Transformer model improves the pearson correlation on 8 out of the 10 folds.\nPerplexity and sequence recovery of SARS-CoV-2 RBD. We show perplexity and sequence recovery on the SARS-CoV-2 protein receptor binding domain (RBD) as an example for inverse folding. The RBD can exist in a closed-state with the RBD down or in an open-state with the RBD up (Walls et al., 2020) 2017) with hhblits (Steinegger et al., 2019) (using two iterations and an E-value cutoff of 0.001) based on the RBD wildtype sequence as the query.\nPredicting complex stability changes upon mutations. SKEMPI (Jankauskait\u0117 et al., 2019) is a database of binding free energy changes upon single point mutations within protein complex interfaces. This database is used as a task in the Atom3D benchmark suite (Townshend et al., 2020) for comparing supervised stability prediction methods. The task is to classify whether the stability of the complex increases as a result of the mutation. We compare zero-shot predictions using inverse folding models to supervised and transfer learning methods (Townshend et al., 2020;Jing et al., 2021a) on the Atom3D test set. We find that sequence log-likelihoods from GVP-GNN, GVP-GNN-large, and GVP-Transformer models are all effective zero-shot predictors of stability changes of protein complexes (Table C.5), performing comparably to the best supervised method which uses transfer learning.\nPredicting insertion effects on AAV. Using masked coordinate tokens at insertion regions, inverse folding models can also predict the effects of sequence insertions. Adeno-associated virus (AAV) capsids are a promising gene delivery vehicle, approved by the US Food and Drug Administration for use as gene delivery vectors in humans. Focusing on mutating a 28-amino acid segment, Bryant et al. (2021) generated more than 200,000 variants of AAV sequences with 12-29 mutations across this region, and measured their ability to package of a DNA payload. This dataset is unique compared to many other mutagenesis datasets in that most sequences feature random insertions in the 28-amino acid segment, as opposed to only random substitutions.\nWe use inverse folding models to predict insertion and substitution effects as follows: For each sequence, we input the full backbone coordinates of the wild-type (PDB: 1LP3), and insert one masked token into the input backbone coordinates for each insertion. Then we compare the conditional sequence log-likelihood on this input with masks to the conditional sequence log-likelihood of the wild-type sequence on the wild-type backbone. The difference in these two conditional log-likelihoods are used as the score for predicting packaging ability.\nWe report the zero-shot performance on each of the 7 data subsets evaluated in the FLIP (Dallago et al., 2021) benchmark suite. For amino acid insertions (marked as lowercase letters in the FLIP data), the corresponding backbone coordinates for those amino acids are marked as unknown in the input structure. As shown in Table C.6, GVP-Transformer trained with predicted structures outperforms the sequence-only zero-shot prediction baseline ESM-1v on 6 out of the 7 data subsets.\nThe reported standard deviations are calculated by sampling different subsets of 10,000 variants from the evaluation data.\nFor ESM-1v, we scored variant sequences based on the independent marginals formula as described in Equation 1from Meier et al. (2021), scoring mutations using the log odds ratio at the mutated position, assuming an additive model when a set of multiple mutations T exist in the same sequence:\nt\u2208T log p(x t = x mt t |x ins \\T ) \u2212 log p(x t = x wt t |x \\T )(2)\nwhere x ins \\T in the first term is the wild-type sequence with mask tokens at insertion positions and x \\T in the second term is the wild-type sequence without insertions.\nConfusion matrix. We calculated the substitution scores between native sequences and sampled sequences (sampled with temperature T = 1) by using the same log odds ratio formula as in the BLOSUM62 substition matrix. For two amino acids x and y, the substitution score s(x, y) is s(x, y) = log p(x, y) q(x)q(y) ,\nwhere p(x, y) is the jointly likelihood that native amino acid x is substituted by sampled amino acid y, q(x) is the marginal likelihood in the native distribution, and q(y) is the marginal likelihood in the sampled distribution.\nCalibration. Calibration curves examines how well the probabilistic predictions of a classifier are calibrated, plotting the true frequency of the label against its predicted probability. When computing the calibration curve, for each amino acid, we bin the predicted probabilities into 10 bins and then compare with the true probability.\nPlacement of hydrophobic residues. We define the amino acids IVLFCMA as hydrophobic residues, and inspect the distribution of solvent accessible surface area for both hydrophobic residues and polar (non-hydrophobic) residues. Solvent accessible surface area calculated with the Shrake-Rupley (\"rolling probe\") algorithm from the biotite package (Kunzmann & Hamacher, 2018) and summed over all atoms in each amino acid. All models have similar distributions of accessible surface area for hydrophobic residues, also similar to the distribution in native sequences (Figure C.6).\nSampling speed. We profile the sampling speed with PyTorch Profiler, averaging over the sampling time for 30 sequences in each sequence length bucket on a Quadro RTX 8000 GPU with 48GB memory. For the generic Transformer decoder, we use the incremental causal decoding implementation in fairseq (Ott et al., 2019). For GVP-GNN, we use the implementation from the gvp-pytorch GitHub repository.  \nL A G V S E R T I D P K Q N F Y MHWC L A G V S E R T I D P K Q N F Y M H W C Confusion matrix L A G V S E R T I D P K Q N F Y MHWC L A G V S E R T I D P K Q N F Y M H W C BLOSUM62 (reference)", "publication_ref": ["b54", "b54", "b27", "b54", "b71", "b61", "b28", "b65", "b65", "b30", "b10", "b13", "b52", "b34", "b47"], "figure_ref": ["fig_4", "fig_7"], "table_ref": []}, {"heading": "", "text": "Acknowledgements We thank Halil Akin, Sal Candido, David Ding, Ori Kabeli, Joshua Meier, Sergey Ovchinnikov, Prajit Ramachandran, Ammar Rizvi, Kathy Wei, Kevin Yang, Zhongkai Zhu, and the anonymous reviewers for feedback on the manuscript and insightful conversations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": " ", "text": "(Ott et al., 2019)\n.", "publication_ref": ["b47"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "The rosetta allatom energy function for macromolecular modeling and design", "journal": "Journal of chemical theory and computation", "year": "2017", "authors": "R F Alford; A Leaver-Fay; J R Jeliazkov; M J O'meara; F P Dimaio; H Park; M V Shapovalov; P D Renfrew; V K Mulligan; K Kappel"}, {"ref_id": "b1", "title": "Unified rational protein engineering", "journal": "", "year": "", "authors": "E C Alley; G Khimulya; S Biswas; M Alquraishi; G M Church"}, {"ref_id": "b2", "title": "Protein sequence design with a learned potential", "journal": "Biorxiv", "year": "2021", "authors": "N Anand-Achim; R R Eguchi; I I Mathews; C P Perez; A Derry; R B Altman; P.-S Huang"}, {"ref_id": "b3", "title": "Model-based reinforcement learning for biological sequence design", "journal": "", "year": "2019", "authors": "C Angermueller; D Dohan; D Belanger; R Deshpande; K Murphy; L Colwell"}, {"ref_id": "b4", "title": "De novo protein design by deep network hallucination", "journal": "Nature", "year": "2021", "authors": "I Anishchenko; S J Pellock; T M Chidyausiku; T A Ramelot; S Ovchinnikov; J Hao; K Bafna; C Norn; A Kang; A K Bera"}, {"ref_id": "b5", "title": "Accurate prediction of protein structures and interactions using a three-track neural network", "journal": "Science", "year": "2021", "authors": "M Baek; F Dimaio; I Anishchenko; J Dauparas; S Ovchinnikov; G R Lee; J Wang; Q Cong; L N Kinch; R D Schaeffer; C Mill\u00e1n; H Park; C Adams; C R Glassman; A Degiovanni; J H Pereira; A V Rodrigues; A A Van Dijk; A C Ebrecht; D J Opperman; T Sagmeister; C Buhlheller; T Pavkov-Keller; M K Rathinaswamy; U Dalwadi; C K Yip; J E Burke; K C Garcia; N V Grishin; P D Adams; R J Read; D Baker"}, {"ref_id": "b6", "title": "Learning protein sequence embeddings using information from structure", "journal": "", "year": "2019", "authors": "T Bepler; B Berger"}, {"ref_id": "b7", "title": "The protein data bank", "journal": "Nucleic acids research", "year": "2000", "authors": "H M Berman; J Westbrook; Z Feng; G Gilliland; T N Bhat; H Weissig; I N Shindyalov; P E Bourne"}, {"ref_id": "b8", "title": "Spherical convolutions and their application in molecular modelling", "journal": "Curran Associates, Inc", "year": "2017", "authors": "W Boomsma; J ; Frellsen; U V Luxburg; S Bengio; H Wallach; R Fergus; S Vishwanathan; Garnett "}, {"ref_id": "b9", "title": "Conditioning by adaptive sampling for robust design", "journal": "PMLR", "year": "2019", "authors": "D Brookes; H Park; J Listgarten"}, {"ref_id": "b10", "title": "Deep diversification of an aav capsid protein by machine learning", "journal": "Nature Biotechnology", "year": "2021", "authors": "D H Bryant; A Bashir; S Sinai; N K Jain; P J Ogden; P F Riley; G M Church; L J Colwell; E D Kelsic"}, {"ref_id": "b11", "title": "To improve protein sequence profile prediction through image captioning on pairwise residue distance map", "journal": "Journal of chemical information and modeling", "year": "2019", "authors": "S Chen; Z Sun; L Lin; Z Liu; X Liu; Y Chong; Y Lu; H Zhao; Yang ; Y "}, {"ref_id": "b12", "title": "Probing the role of packing specificity in protein design", "journal": "Proceedings of the National Academy of Sciences", "year": "1997", "authors": "B I Dahiyat; S L Mayo"}, {"ref_id": "b13", "title": "Benchmark tasks in fitness landscape inference for proteins. bioRxiv", "journal": "", "year": "2021", "authors": "C Dallago; J Mou; K E Johnston; B J Wittmann; N Bhattacharya; S Goldman; A Madani; Yang ; K K Flip"}, {"ref_id": "b14", "title": "Robust deep learning based protein sequence design using proteinmpnn", "journal": "bioRxiv", "year": "2022", "authors": "J Dauparas; I Anishchenko; N Bennett; H Bai; R J Ragotte; L F Milles; B I M Wicky; A Courbet; R J De Haas; N Bethel; P J Y Leung; T F Huddy; S Pellock; D Tischer; F Chan; B Koepnick; H Nguyen; A Kang; B Sankaran; A Bera; N P King; D Baker"}, {"ref_id": "b15", "title": "De novo protein design: what are we learning?", "journal": "Current Opinion in Structural Biology", "year": "1991", "authors": "W F Degrado; D P Raleigh; T Handel"}, {"ref_id": "b16", "title": "", "journal": "", "year": "2018", "authors": "S Edunov; M Ott; M Auli; D Grangier"}, {"ref_id": "b17", "title": "Ig-vae: generative modeling of immunoglobulin proteins by direct 3d coordinate generation", "journal": "bioRxiv", "year": "2020", "authors": "R R Eguchi; N Anand; C A Choe; P.-S Huang"}, {"ref_id": "b18", "title": "Prottrans: Towards cracking the language of lifes code through selfsupervised deep learning and high performance computing", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2021", "authors": "A Elnaggar; M Heinzinger; C Dallago; G Rehawi; W Yu; L Jones; T Gibbs; T Feher; C Angerer; M Steinegger; D Bhowmik; B Rost"}, {"ref_id": "b19", "title": "Protein complex prediction with alphafold-multimer", "journal": "bioRxiv", "year": "2022", "authors": "R Evans; M O'neill; A Pritzel; N Antropova; A Senior; T Green; A \u017d\u00eddek; R Bates; S Blackwell; J Yim; O Ronneberger; S Bodenstein; M Zielinski; A Bridgland; A Potapenko; A Cowie; K Tunyasuvunakool; R Jain; E Clancy; P Kohli; J Jumper; D Hassabis"}, {"ref_id": "b20", "title": "Function-guided protein design by deep manifold sampling", "journal": "bioRxiv", "year": "2021", "authors": "V Gligorijevic; D Berenberg; S Ra; A Watkins; S Kelow; K Cho; R Bonneau"}, {"ref_id": "b21", "title": "High-resolution protein design with backbone freedom", "journal": "Science", "year": "1998", "authors": "P B Harbury; J J Plecs; B Tidor; T Alber; P S Kim"}, {"ref_id": "b22", "title": "Modeling aspects of the language of life through transfer-learning protein sequences", "journal": "BMC bioinformatics", "year": "2019", "authors": "M Heinzinger; A Elnaggar; Y Wang; C Dallago; D Nechaev; F Matthes; B Rost"}, {"ref_id": "b23", "title": "Comparison of multiple amber force fields and development of improved protein backbone parameters", "journal": "Proteins: Structure, Function, and Bioinformatics", "year": "2006", "authors": "V Hornak; R Abel; A Okur; B Strockbine; A Roitberg; C Simmerling"}, {"ref_id": "b24", "title": "Pdbflex: exploring flexibility in protein structures", "journal": "Nucleic acids research", "year": "2016", "authors": "T Hrabe; Z Li; M Sedova; P Rotkiewicz; L Jaroszewski; A Godzik"}, {"ref_id": "b25", "title": "The coming of age of de novo protein design", "journal": "Nature", "year": "2016", "authors": "P.-S Huang; S E Boyken; D Baker"}, {"ref_id": "b26", "title": "Computed structures of core eukaryotic protein complexes", "journal": "Science", "year": "", "authors": "I R Humphreys; J Pei; M Baek; A Krishnakumar; I Anishchenko; S Ovchinnikov; J Zhang; T J Ness; S Banjade; S R Bagde"}, {"ref_id": "b27", "title": "Generative models for graph-based protein design", "journal": "", "year": "2019-12-08", "authors": "J Ingraham; V K Garg; R Barzilay; T S. ; H M Jaakkola; H Larochelle; A Beygelzimer; F Buc"}, {"ref_id": "b28", "title": "Skempi 2.0: an updated benchmark of changes in protein-protein binding energy, kinetics and thermodynamics upon mutation", "journal": "Bioinformatics", "year": "2019", "authors": "J Jankauskait\u0117; B Jim\u00e9nez-Garc\u00eda; J Dapk\u016bnas; J Fern\u00e1ndez-Recio; I H Moal"}, {"ref_id": "b29", "title": "Iterative refinement graph neural network for antibody sequence-structure co-design", "journal": "", "year": "2021", "authors": "W Jin; J Wohlwend; R Barzilay; T Jaakkola"}, {"ref_id": "b30", "title": "Equivariant graph neural networks for 3d macromolecular structure", "journal": "", "year": "2021", "authors": "B Jing; S Eismann; P N Soni; R O Dror"}, {"ref_id": "b31", "title": "Learning from protein structure with geometric vector perceptrons", "journal": "", "year": "2021", "authors": "B Jing; S Eismann; P Suriana; R J L Townshend; R O Dror"}, {"ref_id": "b32", "title": "Spanbert: Improving pre-training by representing and predicting spans", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "M Joshi; D Chen; Y Liu; D S Weld; L Zettlemoyer; O Levy"}, {"ref_id": "b33", "title": "Highly accurate protein structure prediction with alphafold", "journal": "Nature", "year": "2021", "authors": "J Jumper; R Evans; A Pritzel; T Green; M Figurnov; O Ronneberger; K Tunyasuvunakool; R Bates; A \u017d\u00eddek; A Potapenko"}, {"ref_id": "b34", "title": "Biotite: a unifying open source computational biology framework in python", "journal": "BMC bioinformatics", "year": "2018", "authors": "P Kunzmann; K Hamacher"}, {"ref_id": "b35", "title": "Structure of the sars-cov-2 spike receptor-binding domain bound to the ace2 receptor", "journal": "Nature", "year": "2020", "authors": "J Lan; J Ge; J Yu; S Shan; H Zhou; S Fan; Q Zhang; X Shi; Q Wang; L Zhang"}, {"ref_id": "b36", "title": "De novo design of bioactive protein switches", "journal": "Nature", "year": "2019", "authors": "R A Langan; S E Boyken; A H Ng; J A Samson; G Dods; A M Westbrook; T H Nguyen; M J Lajoie; Z Chen; S Berger"}, {"ref_id": "b37", "title": "Gene3d: a domain-based resource for comparative genomics, functional annotation and protein network analysis", "journal": "Nucleic acids research", "year": "2012", "authors": "J Lees; C Yeats; J Perkins; I Sillitoe; R Rentzsch; B H Dessailly; C Orengo"}, {"ref_id": "b38", "title": "Predicting changes in protein thermodynamic stability upon point mutation with deep 3d convolutional neural networks", "journal": "PLoS computational biology", "year": "2020", "authors": "B Li; Y T Yang; J A Capra; M B Gerstein"}, {"ref_id": "b39", "title": "Direct prediction of profiles of sequences compatible with a protein structure by neural networks with fragment-based local and energy-based nonlocal profiles", "journal": "Proteins: Structure, Function, and Bioinformatics", "year": "2014", "authors": "Z Li; Y Yang; E Faraggi; J Zhan; Y Zhou"}, {"ref_id": "b40", "title": "Language modeling for protein generation", "journal": "", "year": "2020", "authors": "A Madani; B Mccann; N Naik; N S Keskar; N Anand; R R Eguchi; P.-S Huang; R Socher;  Progen"}, {"ref_id": "b41", "title": "Deep neural language modeling enables functional protein generation across families", "journal": "bioRxiv", "year": "2021", "authors": "A Madani; B Krause; E R Greene; S Subramanian; B P Mohr; J M Holton; J L Olmos; C Xiong; Z Z Sun; R Socher"}, {"ref_id": "b42", "title": "Language models enable zero-shot prediction of the effects of mutations on protein function", "journal": "Advances in Neural Information Processing Systems", "year": "", "authors": "J Meier; R Rao; R Verkuil; J Liu; T Sercu; A Rives"}, {"ref_id": "b43", "title": "Uniclust databases of clustered and deeply annotated protein sequences and alignments", "journal": "Nucleic acids research", "year": "2017", "authors": "M Mirdita; L Von Den Driesch; C Galiez; M J Martin; J S\u00f6ding; M Steinegger"}, {"ref_id": "b44", "title": "Protein sequence design by conformational landscape optimization", "journal": "Proceedings of the National Academy of Sciences", "year": "", "authors": "C Norn; B I Wicky; D Juergens; S Liu; D Kim; D Tischer; B Koepnick; I Anishchenko; D Baker; S Ovchinnikov"}, {"ref_id": "b45", "title": "Predicting sequence profiles from protein structures using deep neural networks", "journal": "Proteins: Structure, Function, and Bioinformatics", "year": "2018", "authors": "J O'connell; Z Li; J Hanson; R Heffernan; J Lyons; K Paliwal; A Dehzangi; Y Yang; Y Zhou;  Spin2"}, {"ref_id": "b46", "title": "Cath-a hierarchic classification of protein domain structures", "journal": "Structure", "year": "1997", "authors": "C A Orengo; A D Michie; S Jones; D T Jones; M B Swindells; J M Thornton"}, {"ref_id": "b47", "title": "A fast, extensible toolkit for sequence modeling", "journal": "", "year": "2019", "authors": "M Ott; S Edunov; A Baevski; A Fan; S Gross; N Ng; D Grangier; M Auli;  Fairseq"}, {"ref_id": "b48", "title": "Hmmer web server: 2018 update", "journal": "Nucleic acids research", "year": "2018", "authors": "S C Potter; A Luciani; S R Eddy; Y Park; R Lopez; R D Finn"}, {"ref_id": "b49", "title": "Densecpd: improving the accuracy of neural-network-based computational protein sequence design with densenet", "journal": "Journal of Chemical Information and Modeling", "year": "2020", "authors": "Y Qi; J Z Zhang"}, {"ref_id": "b50", "title": "De novo design of modular and tunable protein biosensors", "journal": "Nature", "year": "2021", "authors": "A Quijano-Rubio; H.-W Yeh; J Park; H Lee; R A Langan; S E Boyken; M J Lajoie; L Cao; C M Chow; M C Miranda"}, {"ref_id": "b51", "title": "Evaluating protein transfer learning with tape. Advances in neural information processing systems", "journal": "", "year": "2019", "authors": "R Rao; N Bhattacharya; N Thomas; Y Duan; P Chen; J Canny; P Abbeel; Y Song"}, {"ref_id": "b52", "title": "", "journal": "bioRxiv", "year": "2021", "authors": "R Rao; J Liu; R Verkuil; J Meier; J F Canny; P Abbeel; T Sercu; A Rives;  Msa;  Transformer"}, {"ref_id": "b53", "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences", "journal": "Proceedings of the National Academy of Sciences", "year": "", "authors": "A Rives; J Meier; T Sercu; S Goyal; Z Lin; J Liu; D Guo; M Ott; C L Zitnick; J Ma"}, {"ref_id": "b54", "title": "Global analysis of protein folding using massively parallel design, synthesis, and testing", "journal": "Science", "year": "2017", "authors": "G J Rocklin; T M Chidyausiku; I Goreshnik; A Ford; S Houliston; A Lemak; L Carter; R Ravichandran; V K Mulligan; A Chevalier"}, {"ref_id": "b55", "title": "Improved protein structure prediction using potentials from deep learning", "journal": "Nature", "year": "2020", "authors": "A W Senior; R Evans; J Jumper; J Kirkpatrick; L Sifre; T Green; C Qin; A \u017d\u00eddek; A W Nelson; A Bridgland"}, {"ref_id": "b56", "title": "Improving neural machine translation models with monolingual data", "journal": "", "year": "2015", "authors": "R Sennrich; B Haddow; A Birch"}, {"ref_id": "b57", "title": "Protein design and variant prediction using autoregressive generative models", "journal": "Nature communications", "year": "2021", "authors": "J.-E Shin; A J Riesselman; A W Kollasch; C Mcmahon; E Simon; C Sander; A Manglik; A C Kruse; D S Marks"}, {"ref_id": "b58", "title": "Discovery of novel gain-of-function mutations guided by structure-based deep learning", "journal": "ACS synthetic biology", "year": "2020", "authors": "R Shroff; A W Cole; D J Diaz; B R Morrow; I Donnell; A Annapareddy; J Gollihar; A D Ellington; R Thyer"}, {"ref_id": "b59", "title": "Adalead: A simple and robust adaptive greedy search algorithm for sequence design", "journal": "", "year": "2020", "authors": "S Sinai; R Wang; A Whatley; S Slocum; E Locane; E D Kelsic"}, {"ref_id": "b60", "title": "Deep mutational scanning of sars-cov-2 receptor binding domain reveals constraints on folding and ace2 binding", "journal": "Cell", "year": "2020", "authors": "T N Starr; A J Greaney; S K Hilton; D Ellis; K H Crawford; A S Dingens; M J Navarro; J E Bowen; M A Tortorici; A C Walls"}, {"ref_id": "b61", "title": "Hh-suite3 for fast remote homology detection and deep protein annotation", "journal": "BMC bioinformatics", "year": "2019", "authors": "M Steinegger; M Meier; M Mirdita; H V\u00f6hringer; S J Haunsberger; J S\u00f6ding"}, {"ref_id": "b62", "title": "", "journal": "Computational protein design. Structure", "year": "1999", "authors": "A G Street; S L Mayo"}, {"ref_id": "b63", "title": "Fast and flexible protein design using deep graph neural networks", "journal": "Cell Systems", "year": "2020", "authors": "A Strokach; D Becerra; C Corbi-Verge; A Perez-Riba; P M Kim"}, {"ref_id": "b64", "title": "Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches", "journal": "Bioinformatics", "year": "2015", "authors": "B E Suzek; Y Wang; H Huang; P B Mcgarvey; C H Wu; U Consortium"}, {"ref_id": "b65", "title": "ATOM3D: tasks on molecules in three dimensions", "journal": "", "year": "2012", "authors": "R J L Townshend; M V\u00f6gele; P Suriana; A Derry; A Powers; Y Laloudakis; S Balachandar; B M Anderson; S Eismann; R Kondor; R B Altman; R O Dror"}, {"ref_id": "b66", "title": "Efficient generative modeling of protein sequences using simple autoregressive models", "journal": "", "year": "2021", "authors": "J Trinquier; G Uguzzoni; A Pagnani; F Zamponi; M Weigt"}, {"ref_id": "b67", "title": "Well-read students learn better: On the importance of pre-training compact models", "journal": "", "year": "2019", "authors": "I Turc; M.-W Chang; K Lee; K Toutanova"}, {"ref_id": "b68", "title": "Foldseek: fast and accurate protein structure search", "journal": "bioRxiv", "year": "2022", "authors": "M Van Kempen; S Kim; C Tumescheit; M Mirdita; J S\u00f6ding; M Steinegger"}, {"ref_id": "b69", "title": "AlphaFold Protein Structure Database: massively expanding the structural coverage of protein-sequence space with high-accuracy models", "journal": "Nucleic Acids Research", "year": "2021", "authors": "M Varadi; S Anyango; M Deshpande; S Nair; C Natassia; G Yordanova; D Yuan; O Stroe; G Wood; A Laydon; A \u017d\u00eddek; T Green; K Tunyasuvunakool; S Petersen; J Jumper; E Clancy; R Green; A Vora; M Lutfi; M Figurnov; A Cowie; N Hobbs; P Kohli; G Kleywegt; E Birney; D Hassabis; S Velankar"}, {"ref_id": "b70", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "A Vaswani; N Shazeer; N Parmar; J Uszkoreit; L Jones; A N Gomez; \u0141 Kaiser; I Polosukhin"}, {"ref_id": "b71", "title": "Structure, function, and antigenicity of the sars-cov-2 spike glycoprotein", "journal": "Cell", "year": "2020", "authors": "A C Walls; Y.-J Park; M A Tortorici; A Wall; A T Mcguire; D Veesler"}, {"ref_id": "b72", "title": "Computational protein design with deep learning neural networks", "journal": "Scientific reports", "year": "2018", "authors": "J Wang; H Cao; J Z Zhang; Y Qi"}, {"ref_id": "b73", "title": "Deep learning methods for designing proteins scaffolding functional sites", "journal": "bioRxiv", "year": "2021", "authors": "J Wang; S Lisanza; D Juergens; D Tischer; I Anishchenko; M Baek; J L Watson; J H Chun; L F Milles; J Dauparas"}, {"ref_id": "b74", "title": "Protein sequence design with deep generative models", "journal": "Current Opinion in Chemical Biology", "year": "2021", "authors": "Z Wu; K E Johnston; F H Arnold; Yang ; K K "}, {"ref_id": "b75", "title": "Improved protein structure prediction using predicted interresidue orientations", "journal": "Proceedings of the National Academy of Sciences", "year": "2020", "authors": "J Yang; I Anishchenko; H Park; Z Peng; S Ovchinnikov; D Baker"}, {"ref_id": "b76", "title": "Machine-learningguided directed evolution for protein engineering", "journal": "Nature methods", "year": "2019", "authors": "K K Yang; Z Wu; Arnold ; F H "}, {"ref_id": "b77", "title": "Masked inverse folding with sequence transfer for protein representation learning", "journal": "bioRxiv", "year": "2022", "authors": "K K Yang; N Zanichelli; H Yeh"}, {"ref_id": "b78", "title": "Prodconn: Protein design using a convolutional neural network", "journal": "Proteins: Structure, Function, and Bioinformatics", "year": "2020", "authors": "Y Zhang; Y Chen; C Wang; C.-C Lo; X Liu; W Wu; J Zhang"}, {"ref_id": "b79", "title": "A generalpurpose protein design framework based on mining sequence-structure relationships in known protein structures", "journal": "Proceedings of the National Academy of Sciences", "year": "2020", "authors": "J Zhou; A E Panaitiu; G Grigoryan"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure1. Augmenting inverse folding with predicted structures. To evaluate the potential for training protein design models with predicted structures, we predict structures for 12 million UniRef50 protein sequences using AlphaFold2(Jumper et al., 2021). An autoregressive inverse folding model is trained to perform fixed-backbone protein sequence design. Train and test sets are partitioned at the topology level, so that the model is evaluated on structurally held-out backbones. We compare transformer models having invariant geometric input processing layers, with fully geometric models used in prior work. Span masking and noise is applied to the input coordinates.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 .2Figure 2. Illustration of the protein design tasks considered.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 .3Figure3. Example AlphaFold prediction compared with experimental structure for a UniRef50 sequence (UniRef50: P07260; PDB: 1AP8). The experimental structure is shown as pink with transparency. The prediction is coloured by the pLDDT confidence score, with blue in high-confidence regions.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "scheme during training) GVP-GNN-large (random mask) GVP-GNN-large (span mask) GVP-Transformer (random mask) GVP-Transformer (span mask) Random sequence", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 4 .4Figure4. Perplexity on regions of masked coordinates of different lengths. The GVP-GNN architecture degrades to the perplexity of the background distribution for masked regions of more than a few tokens, while GVP-Transformer maintains moderate accuracy on long masked spans, especially when trained on masked spans.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 .5Figure 5. Comparison of perplexity and sequence recovery by structural context according to two different measures: number of neighbors (top) and solvent accessible surface area (bottom). Top: Breakdown for core and surface residues. Residues are categorized by density of neighboring C\u03b1 atoms within 10A of the central residue C\u03b1 atom (core: \u2265 24 neighbors; surface: < 16 neighbors).Each box shows the distribution of perplexities for the core or surface residues across different sequences. Bottom: Perplexity and sequence recovery as a function of solvent accessible surface area. Increased sequence recovery for buried residues suggests the model learns dense hydrophobic packing constraints in the core.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 .6Figure 6. Ablation studies on training data. (a) Effect of increasing the number of predicted structures. The original GVP-GNN degrades with training on additional data, but GVP-GNN-large and GVP-Transformer improve with increasing numbers of predicted structures. (b) Effect of increasing the mixing ratio during training between predicted and experimental structures. A higher ratio of predicted structures improves performance for both GVP-GNN-large and GVP-Transformer. (c) GVP-GNN and GVP-Transformer model size.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 7 .7Figure7. Dual-state design. GVP-Transformer conditioned on two conformations results in lower sequence perplexity at locally flexible residues than single-conformation conditioning for structurally held-out proteins in PDBFlex (see Appendix C for details).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure B. 1 .1Figure B.1. An illustrative example of structural overlap between CATH topology splits. The jack bean canavalin (PDB code 1DGW; chain Y; red) and the soybean \u03b2-Conglycinin (PDB code 1UIJ; chain B; blue) are assigned different topology codes in CATH (1.10.10 and 2.60.120), but they align with TM-score 0.94 and CA RMSD 0.7A on a segment of 90 residues. The difference in topology classifications likely resulted from CATH annotating only a 37-residue mainly helical segment of the jack bean canavalin as a domain while annotating a longer 176-residue mainly beta sheet segment of the soybean \u03b2-Conglycinin as a domain.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "FigureFigure B.2. Distribution of the highest TM-score from each test example to the train set. For example, 54% of the CATH topology split test set has at least one match in the train set with TM-score above 0.5, and 27% of the topology split test set has at least one match in the train set with TM-score above 0.6.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": ", we also create an even more stringent test set based on pairwise TM-score comparison between train and test examples. The CATH topology split does not completely prevent high TM-score matches between train and test structures. We illustrate such an example in Figure B.1, and show overall TM-score statistics Figure B.2. We constructed a TM-score-based test set of 223 proteins with no TMalign matches (TM-score \u2265 0.5) from the train set, using the foldseek (van Kempen et al., 2022) TMalign tool with default parameters for the pairwise search.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure C. 1 .1Figure C.1. Effects of varying the number of GVP-GNN pre-processing layers in the GVP-Transformer model, as measured by perplexity on CATH topology split test set.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure C.2. Fixed backbone sequence design perplexity for protein complexes. The model is evaluated on 796 structurally held-out protein complexes. Comparison of conditioning on the backbone coordinates of individual chains (x-axis) with conditioning on backbone coordinates of the entire complex (y-axis). Note that for both values perplexity is evaluated on the same chain in the complex. The shift to the lower right indicates improved perplexity when the model is given the complete structure of the complex.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Figure C. 4 .Figure4Figure C.4. Confusion matrix between native sequence and sampled sequences from the model, compared to BLOSUM62 as reference.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "al.,    ", "figure_data": "Predicted structures (12 million)UniRef50 sequences (12 million)CATH structures (~16,000)CATH sequences (~16,000)"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Generative models of proteins The literature on structure-based generative models of protein sequences is the closest to our work.Ingraham et al. (2019) introduced the formulation of fixed-backbone design as a conditional sequence generation problem, using invariant features with graph neural networks, modeling each amino acid as a node in the graph with edges connecting spatially adjacent amino acids.Jing et al. (2021b;a)  further improved graph neu to perform infilling, although conditioning on both coordinates and amino acid identities instead of considering the inverse folding task. Also contemporary to this work,Jin et al. (2021) develop a conditional generation model for jointly generating sequences and structures for antibody complementarity determining regions (CDRs), conditioned on framework region structures.Huang (2018)  who model fixed-length protein backbones with generative adversarial networks (GANs) via pairwise distance matrices, andEguchi et al. (2020) who generate antibody structures with variational autoencoders (VAEs).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Fixed backbone sequence design performance on the more stringent structurally held-out test set from CATH v4.3 chains (and its short and single-chain subsets) in terms of per-residue perplexity (lower is better) and recovery (higher is better).", "figure_data": "PerplexityRecovery %ModelDataShort Single-chain AllShort Single-chainAllStructured GNNCATH10.087.047.0627.8%35.1%35.4%GVP-GNNCATH + AlphaFold2 9.87 8.135.76 6.615.86 6.5031.5% 26.3%41.1% 36.3%.40.4% 36.8%GVP-GNN-largeCATH + AlphaFold2 7.08 8.876.62 4.466.68 4.3931.0% 34.1%37.2% 48.2%37.4% 48.7%GVP-TransformerCATH + AlphaFold2 6.99 8.806.78 4.366.97 4.3428.5% 33.0%36.7% 48.9%36.3% 49.5%Table B.1."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Table C.1. Effects of adding Gaussian noise to predicted structures and effects of span masking during training, as measured by perplexity on CATH topology split test set.", "figure_data": "PerplexitySpan maskingGaussian noise4.10GVP-Transformer (142M params, mixing ratio 1:40)Span masking Independent random masking Gaussian noise No noise4.32 4.30No maskingGaussian noise4.20"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Table C.2. Model performance when trained only using the 12 million predicted structures without CATH training data, as measured by perplexity on CATH topology split test set.", "figure_data": "Pearson correlationFoldStructured GNNGVP-GNNGVP-GNN-large+AF2 GVP-Transformer+AF2(Ingraham et al., 2019) (Jing et al., 2021a)\u03b2\u03b2\u03b1\u03b2\u03b2 370.470.530.620.70\u03b2\u03b2\u03b1\u03b2\u03b2 14980.450.390.370.33\u03b2\u03b2\u03b1\u03b2\u03b2 17020.120.260.240.22\u03b2\u03b2\u03b1\u03b2\u03b2 17160.470.570.600.58\u03b1\u03b2\u03b2\u03b1 7790.570.480.620.64\u03b1\u03b2\u03b2\u03b1 2230.360.470.570.55\u03b1\u03b2\u03b2\u03b1 7260.210.190.240.26\u03b1\u03b2\u03b2\u03b1 8720.230.390.380.42\u03b1\u03b1\u03b1 1340.360.440.460.50\u03b1\u03b1\u03b1 1380.410.440.550.58Average0.370.420.470.48"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Table C.8. 60 randomly sampled RBD dual-state sequence designs from the GVP-Transformer model with sampling temperature 1.0 and conditioned on both the open and closed states.", "figure_data": "RBD native sequence:FPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDIYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKDesigned sequences:YPDLTNGCPLESVFQRSEYASVVDWSKEEIHDCVLDFTELRSSSDWDTIQCYGVTPQELNELCFTKVQAFSFVIQGFKVKEVAPGKKGVLADKTFRLDDDFTGCVVAKSARDKYATLMPNRNYYYRTNSSVKLLPYQRTIKDPLVSFGFTRTNSLPNQPYRVVVLRFDENYHFVQVCGPRGFPRISTSCPTKKIFNTAQFAQVHSYSSKKIENCVLNMDEIVSSAEWDTVECYGVSKEKLNTLCFSQVEALSFVIEGRSTAEIKPGATGWIADYDFRLPDDFTGCVYAKSAVNNLAQQEPNHGYWFREIAQQHIPPYEKSTSFPLKYYAFNPSASEDLQPYQVVVFKFDTHLTFASVCGPVSYPENENVCNTDEIFDKKKYAQVYKYANEKISNCVLDFSEVFESAKWETKTCFGVNKSDLNQICFTKVNAHSFIIRGDKVSQVAPGMEGVLADDTFKLPNDFVGCVVAKTSANKYAAEKANNNYWYLKVDLKAIAPYEQKDESPLSYNSFKPTNGVALQPFKVVVYEFDMSLKKASVCGPLDFPENSRRCPLDRVFKRKHFHSSDNWGEEQLENCVLDFTDVYTNQRFDSVKCFGVKEEELEEFCAVDVRAHRFVIKGALRDEVAPGMHGPFADYDFRLDSDFTGCVVARSAKNSIAKEPVNNSFWYVHTRAIKIRPYERINQKNDSYYGFRETAAQDHQPFEVVVFEFIFKNAFADVCGPVENRDNSNTCPLHAIFKIESYNTVHDWGAFDISDCVLDFSIVVENTRWDTIECYGDAKEDLNSLCFTSVKAHSFVIESKDVKQIAPGMSGMIADFTFKLPDDFVGCVISKTAVEKYAKKPENTSYYYRVKDNSKILPYQRLTPTPLYYWGFKGTNEYEKQPYEVVVFDFDEEIKTCSVCGPQGYPLIQNSCPLNQVFDVHQFASIDNYSTTKLENCVWNFSVTYNSNSWDEIVCYGEAKNELNDRCYVKVRAHQFVIEGKDVKQIAPGRTGVYADNTFRLPDNYTGCVISKSTRNRIAKEDGNRDYLYTTSVDSEIAPFENKISSPLQSYTFSPTAPYNCEPYEVTVFSFDTTYADAKVCGPKNNPSIENVCDLNADFELEHFHPVNNFGNSTISNCILDMTKVINNNAWDTVECFGVSKYLLEDMCFTAVFAHRFVIEGKLVKEIAPGKTGSIADVTFRLPNNFVGCVVSRSTVNDDAKVEGNTSYHFRRTASEKIQPFEKRLESPCEHFSFSPTDSEDLQPYQVSVYDFDTNYNFVSVCGPRGYPDISKACPTDAVFMKDHYASSVNYSQIKIQDCVMDFSWVDESTKWDTKHCYGVHNNWLDEMAFKEVNAHCFVIKGHKYKEIAPGRHGVIADYTFKLPDNFVGCVVSRSSRDYIARDGFNTSYLFRTFDRAVIEPYEIQELQPLKKYNFTPYSPDNSQPYHVVVFEMDTKNDFKNVCGPQKRPPIFESCPLKDIFEKSSFASVSNWSKEVIRDCVLDMSAVTQSSKWDTFRCYGVEKDELNHQCYTSVYAHSFVVRGKDVVEIAPGKKGEIANATFRLPDSFVGCVVAKSERNNYAEVPPNLNFLFRRERDEKLAPYEVLTSSGLEMYGFTPSEAAGKQPYEVVIFAFDEGKNASNVCGPQAYPLVSESCNFDAIFNADHYDSVHSWGKKKISNCVLNFDEIITSDKFHSKTCYGVKVHELNHICYTEVQAHSFVIRRADVKEVAPGKNGILADKTFRLPDDFVGCVVSHTAKNKISKDKADFGYYFRRLSRFSIAPYSNKTHSGLEYFNFEPSSGYNEQPYRVVVFELNLQRQACDVCGPITYPTINRQCPLDEVFDKEMFSSVDAYSVSVISDCVLDMSSTTDSPRWDCIRCYGDFKYGLNDDCYTSVEAHNFRIRNSEIVEIAPGKHGYLADDTFRLPDNFVGCVVCRSARNKYAKLNWNTDYYYVNLTPEKILPFENKSCHPLKFYNFASTNGTEEQPFDVVVFAFWKQDRARNVCGPISYPDNDKTCPTRDVFEVHQFDSVYDFSKKKLKDCVLNMSEITNSREWSTRYCYGVTPNDLNSLCFTEVYAHSFVIEGSKVKEIAPGRTGDLADFTFRLPNDFKGIVVARTSRYTVAREFGNERYKYRNNLSKEINPFENLEINPISHYSFSPNNRSDMQPYEVVVYDFDENYDFCDVCGPQHYPPITKRCPLDTVFNTSTYATVDNWSKTSIHNCILDTSIVYNSTTFDTIQCYGVSRYQLNSDSFITVFAHTFIIKSELVSEVAPGKTGTIADYTFKLPDTFTGCVVDRSARFSLYEPTANDKYFYRKNASSKIAPYKRNFENPLKFMNFSPNNTRDNRPREVVVFAFNTHFAKVDVYGPVKNPTITENCPLATVFKTREFSSVVNFGAKDITDCVLDWSKVTSSKRYDTIKCFGVGKSRLETMCFSNVTAHSFIMRGKDYAEVAPGRRGEYADTTFRLPDNFVGCVVARNAKDVLAKLAANRGYKYRLQLKSTINPYERIESEPLQYYNFSPSNSLPDQPYAVVVYEFNQELILSNVCGPVAHPLNQNICPFDKIYATDRLSSVYNYSRDIIQDCVLNFNLIYLSSAWDTIKCFGVSRFDLNERCYSKVSAHSFIIRGDRVSEIAPGKHGIIADATFRLPDNFTGCVISKSLRNRYAINPPNIFYYYRRQLDAKINPFERVDTNPVKKWNFRPTNLADDQPYAVNVFEMDDNYVFAHVCGPQNNNAITKDCPLREVFSRKEYASVCNWSVTNISYCVMNMSIIHESSEWDTIDCYGVKKTTLNNLCFTKVKALSFVIKGSEVNEIAPGKQGYIADLCFRLPDTFEGCVVARNAVDTYAQCPRNRMYYYRKNSSKKLAPFEKLNNSPLKYHDFNNTDSFDEQPYKVVVFQLDKKRSAQSICGPQKYPLNTNHCPLDEVFDTETYAMVHDWSRTILSNCRLDMSQIYSSTSFTTKRCFGINTDDLNDMCFTKVEAHNFVIRGDEVKQIAPGMTGRLADHTFKLDDTFTGCVVARSTRNTIAKTGANDAYLYVQQAQKKIAPYQQKKSDPQCTFEFSPVCSRDLAPFEVVVFDFDTTHDFKDVDGPRSRPRLLNKCPLSTIFEKDTFEEISDWDETAIKDCIFDFSTIVNNTKWNSKSCFGVKANELNDLCFTYVSALSFVIQGKDVSQVAPGKSGKIADLTFKLPDDFVGCVVAKSYVAKEAWFGSNRTFYYRQDSSANVKPFEKNNNTPCHAYPFQATQAEHEQAFGVVVFRFDREWSEGTVHGPQTYPRITTTCPTNYVFRRNEYASVDRFSAENITNCVLDMSTITNSADFNTIRCYGVKANDLNNLCFVKVQAHSFVIKGHDVNEVAPGKSGVIADYTFKLSNNFSGCVIAKSAKDRYAKPPANNNYYYLNRSTGHIQPFEVLHGSPLLHYGFSPSKSYDNQPFEVIVLDFNTELRSSSVCGPQNRPNNTGTCSTDKIFDQRYYASVDAFSSATIKNCVLDFRKVTNASVNDIQKCYGVQQDGLDSLCFVEIKAHNFVIRGDLTVEVAPGRKGYVADATFRLPDSFTGCVVSRSARNHLAARGENLDFRYRPHRDQKLKPYERSNANPNASYGFNPGATYPNQPYEVVVYSFDLREREADVCGPISYPAITNTCPTSDIFRRSRYSSVYKWSQGKFENCILNLDNVHTSTDWDEVKCYGTLASKLNDLCFTKVFGHSFIIRGSEVSQIAPGEHGSLADYTLRLDDDFTGCVFAKDYTSTYAKEPINNHYWYRAVANAPINPFERREDRPLQFYNFSPSSHKNDEPYEVVVFSLDIRHDFCRVYGPEGYPLVTTKCPFDQILKRDTYASVDNFASQLLSECVLDFTEIYNSRRWDQIRCYGVASKHLNELCFVKVRAHSFVIRGDMVSQIAPGQNGVIADDTFRLPDDFTGCVVSRDAKNEYATDGRNNGYMYRYIAAKDIAPWQRHDDKPDHSYGFRPSNPEERAPYRVVVFEFDLAKKKSSVCGDITRPNILNRCPTSKIFDRQTFASVHKWDKVDITSCVLDTSGIVNSGKYAMYKCFGIRPEDLNSLCFTDVKAYSFVIQGKDVKEIAPGMTGELADLTFRLSDDFAGCVIARSAVNKYSKDDANKSFFWRKVRMQKVKPFQRNLFSNLEYRSFKKSNEKDNQPYEVVVFDFNLEKEACSVCGPILYPTNSNSCPFESVFLQENYASIYNWESTLIENCILDMTVINEGMQWDTKSCFGVRKSDLNNMCFTKVVAHSLVIEGSKASQIKPGEHGILADRTFTLDSNFVGCVVSKSAVHKFAKEGYNDSYYYVECVVGRQKPFQNKEQTPLRYYNFQPTHLQSQQPYQVVIFEMDESDHRCSVCGPQSYPNVTTKCPTEDIFDRQTFAQEVLWSKDLIDNCVLDFSYITDSEKWNTFQCYGDNKHDLNDRCYTNVRALNFVVEGKKTLEIAPGRHGLLADVTFRLPDNFIGCVVARSESNRIAKDGFNHNFLYRFNAVRDILPFERTSDSPLAYYGFRPTNLTNLQPYKVVVFVFDENYRERSVCGPIQRPDIVNTCPLANVLRRAGYSSVRNWAITRISNCVFDFAIIQNSLLADTLYCFGVNAQDLNSRCFTEVNAHSFVVQGKDIVEIAPGKRGSLADNTFKLDSDFVGCVVSRSALNYEAKTEGNFEYFYRTSADRDISPYERRIRTPLRYYEFKPDMADHEQPFKVVVFAFDEREGFSTVCGPQDYADIKTRCDFRKIFDRKEYASSYNYSMKHISDCVFDASIVRQSTKWDTIECYGVHKDDLNDMCFVDIKAHNFVIEGQSVSQIAPGRHGQIADNTFRLPDDFVGCVVAKNLRNKYASKDPNRNYLYTISRDGDIEPYERRIVDPLREFHFNRQDPYGWQPFAINVFDFDLREQRTDVCGPLNNPETSNSCPFSSVFSRSHYSSVDNFGADEIKDCVLDFDSIVKSNKFDTYKCFGVAESDLDTKCFTKVVAHSFIIRGDMVKQIAPGKTGTIANNTFNLPNDFNGCVVAKSSRYEIAGRRYNANYRERLIHVSRIEPYQNPVASPEIAWDFRPTDPTDKQPYDVVVYEFDTSWNACRVCGPVNYPSINNVCSVTDVFNTDKYASVVNFSNTLIKDCVLDFRIVNSSKEFSVKSCYGVAKDNLNTLCFTSVTANRFVIRGKDVTEISPGKSGHIADYTFRLEDNFTGCVIAKSDLNKLAKTTPNTKYFYRSKRPSAITPFEKLNENPLKFWNFKANNSQGNQPFEVTVFRFDLTEERRTVCGPQKFPPVTTRCPTDNVFRKENYAAVNTFSETLIKDCVFDFTNIRKSKEWDTFECYGVDPKDLNYLCFTKVWAYRFVIRGDLVRQIAPGERGVVADSTFTLPDDFVGCVVARSQRDDLARRPDNNAYLYRKGDGLFLQPFERYNSNPLEFYGFSPNNEYANQPYEVVVLAMDERYDFTKVHGPVIRPKNKEVCPTSLVFQSNEFAPVSKWSKSEISNCVLDMSETVNDQDWATVRSYGITKNKLNEECFTEVHAHSMIIRGDMVKEVAPGQRGEIADASFRLPDDFVGVVVARSAVSHYAKGGANDKYYYTVLNTYKIKPYEKMTFNPLKFYQFESDNEAPFNPYKIVVFDFDLNDDFRNVCGPRDYPNIKETCPFDNVLEKSAYSSVDNWSSTEVKDCVADFSVVVNSAKYDTVTCYGVNKFDLNAICMYKVIAHSFIIEGHRVKEIAPGKHGILADDTFKLPDDFVGCVISRSARNMLAKDRPNDAYYYTKNNGYTRKPFENNKSSPLRYYAFKPTNEEKDQPYEVVIFAFATDYRQADVCGPVKYPEIQTQCPLKNILDRESYASVKQFSVSKISNCVLDFRETNDSKRWDMIHCYGVDKNSLNSRSYTEVYAHSFVVRGDLVREIAPGQSGILADKTFKLSDDYVGCVVARSAKDKLATSEPDTTYLYRNHSASSIKPFENALRTPLFAYGFNPRNNFQDQPFEVVVFEFDFNLDRASVSGPQAYPANTTKCPYSAIFDTAEYRSVHEYSSSKISNCVLNFSDVNESTQWDTKLCYGMKPESLDDECFKMVTAHNFVIRGEFVRQIAPGTTGLLADNSFKLPDDFIGCVIAKSGVEVYANSTLSNNYCYISNSSNPIKPFEVGDTSPLQSWPFTPENSISYQPYLVTVLEFNRDFAFYDVCGPIDFPDINRVCPLDDIFYRTNYNQSTNAAETNISSCVFDFSDIILDRKFNSVQCYGVDKHDLNTRCFTQVNAHSFVIEGEYRTQIAPGKHGIIADDTFKLHNSFTGCVVSRDAGDYIASGRGNERYWYRITQSRDIAPYQNQSPSPLRFYNFTPSGSVPHRPFEVVVFDFDYTYDFTNVCGPLLRPEITAECPFDSVFERTEYAPVFKWSSETLADCILDFRTVYQSSRYDTRVCYGVQKRDLDSLCAVKVEAHSFIIRGDKVSQIAPGKHGVLADNTFKLPNDFVGCVVARSSKNLYAKPHGNENYFYAEGRGSDIDPFENLCINPLKFYNFTPSNMKDNQPYEVIVFAMDYRMRKCSVCGPVEYPDINNRCPTENIFYRRSFSEVCHFSNDRIENCVLDMRLVISDKKYDTFTPYGVERDELNFQSFVEVRALSFVIRGDKVHQIAPGTTGEIADYTFRLDNNFTGCVVCRDAQNEIAQIGPNMDYTWRCNDRSTIKPYERLTNQPLRRYMWSKTKGFDNQPYKVVVYEFDESFSETDVCGPRNYPDVLNECPLDQIFDRDHYSSVNAFDMKDIKNCVLDFSSISNSSKWSIKKCFGVALNNLNRLCFKKIYAHRMIINNDRVREIAPGQHGTIAVFDFRLPDDFRGCVIARSSIHRDAKNNCNNAYLWRSGASERILPYQRNYNSPLKYYNFSNDNNDANQPYKVVIFSYNEDYDFCSVCGPQRYPRIVQHCPTKEIFDTFDYSSVYNYSMEKIHNCVLNFSSVVQSKRWSTVRCYGVNAEDLDDRCYTEVYAHRFVIMGQHVKQIAPGRSGVLADYTFKLPDDFIGCVVARNGNDSLATENYNNKYWWRQSSESSIDPFERRNSSPLFYYNFKPTNERPNQPYMVVVFEMDTKYDFTNVCGPQEYPQVTTTCPLSEVFDKKKFASVHDFSMTRIEDCVLDFNRIVESSVWKTFKCYGVQPRDLNHICAVRVDAHMFVIRGDWVKQIAPGEHGSIADYTFRLKDDFEGCVVARSKRFELANESINDSYWYRDNWMRDIEPYERLQSSDLKFYGFKRSNSQNDQPFEVVVFELFPEMSFSNVCGPVSNPPDENECPFDAIFNTEEFASVSKYSNKTITDCVADFSRITNSQEFDSIHCYGVDKHDLNDLAFTNVYAHRFVIQGKDVNEIAPGKTGNLANATFRLDDSFDGCVVSRSGKNKIATPFGNKQYFFRTDRLAAINPYERIVSTSLLTYCFHPEDDYALQPYDTVILELNESFQFRSVCGDENRPPNTTECPIDKIFNRSTYASVVNFSSSQLQNCVFDFDIITKSKKWDKIICYGVEKEDLNSLTFSKVMAHSFVIAHDLVGEVAPGKVGEIADVDFRLPDDFVGCVVSQSGVNFLSQAPGNKSYYYRLYANETIKPYEKVNENPLEFYNFTPTNPEDQQPYDVVIFDMDTTDAFRTVCGPVGRPNITQKCKFQNIIDTNSYASVHNWSESVLENCVFDFSKVVNDQKWESVKCFGISKYDLNDLCYSKVRALSFVIRGDQADEIAPGKHGTIADDTFRLPDKFIGCVVWRSSKNKISESVEDRRFCYIGKSQQWINPYEVKKVTSVQFFNFSPINPEEQQPYEVVVFDLDETHSFASVCGPELLPDNTVKCPLSEVFDTKAYSSAVIWSETKIQSCVMDWSIVTKSRSFTTVTCYGVDSTQLNDLSFVEVRAMSFVIKGSLISQIAPGQTGIIADEDFRLPNDFIGCVVARDANNTLARTTINSDYKYRDLDIQTIEPFEVRIRSPMRCYNFEPNNVADKQPYDVVVFSLDKSMNSAHVNGPVNLPEKTTTCPFSEIFDRRKYASIHKYSNQVISDCVLDFRIVVESSTYDSTDCYGISKDELNKLCFTKVYAHSFVIEGKNVAQIAPGKTGKLADNTFRLPDNFTGCVVHRSAKSRIASTGGNQTYIYRDVATSSLAPYERRYDTPCKTYDFKPEDEANKEPFDVVVFEMNVVHDFCSVCGPVDNPPINKVCPTSEIFFREAYASVNLWNNFLIRNCVLDFSEILKSKRYDVFQCYGVSKSSLNFLCAQQINAHNFIIRGERVEEIAPGKRGVLADKTFKLPDNFVGCVVAHDMKNRYSLYPSNNSYFYQLLRRKTIQPFENWDNTPLQEYGFSRGAVQNNQPFQVVVLEFDLKYIINDVCGPQNRPQINNICPLRDIFEKKTYASVHNYSKMEIKDCVLDFSTVYSSSKFDTITCYGVKADDLNKRCFVNVYAHNFVIRGKDVSQIKPGKWGEIADNDFKLSDDFTGCVISRSAANYIAQNPPNTKYIYIKARSSKLEPYQNFQSSPVKFYRFTPSNADNEQPYEIVVFSFNETLRFSSVCGPQGVPLISETCPASEIFDIESYSSVNAYSRKIISDCLLDWNEITVSTDWSMIHCYGETPEDLNSLCYTRVTAHSFVIQGQNIAEIAPGKSGTIANKSFKLPDDFQGCVISKDMRKKIARYKPNKTYWYRVERNTSLKPFEVLNSTPLKYFEFNILQRRDNRPYAVVVFSFDVQNNFRTVCGPMDYPANTHHCPFNEIFETKHYSSVNDFSMHNIHNCVADMSNIVNSTVWDIKNCYGVYCEDLNELSFTRVNAHSFVIRGDKVAEIAPGRHGGIADVTFRLPDSFVGCVITRDSRNFIAQGSSDDGYFYSECRQSVIAPFERSFNNQTRYYCFSPFNGPARQPFDVVVFEFNLKRDFSSVCGPQLFPPITKTCDFNSVFNKKNFASVHKWAMDDISNCTANWDTITKSQSWNMFKCYGEKKNTLNIRSFTNVYALSFVIKNDYVKQIRPGENGILADYTFRLDTDFVGCVVARTAKDKYAKPYGDNTYYYVSKYSRKIKPFEINDASPLYHFVFRREDGNENQPYRIVVFQLDTQYSKPDVCGPREYPSITRKCPTDAVFNREHFNTVENWSSRELKDCILDFTTINSSKKWDSKHCFGVKSDDLNNLCFTKVFAHSFVIEGDEVKEIAPGKTGSLANTTFRLPDNFTGCVVSRSQRYYSATNPANVDYFYRRSAGDTIDPYERIKLDPLKSYGFTKADTYANQPHDVVVFKLDLSYKAAHVCGPTKYPDDTNACDLTSVFKRENFSSVHNYSRSLLKDCVLNMSHIYKSNAYDTIKCFGVSKDSLNKLCFTRVEAHNFVVKKENVKEIAPGKHGVLADNTFRLPDSFTGCVVSSSAKGIVAKSNNENSYYYTKYEEKEVSPYENSAENPCQSFGFSPHASEGEAPFNVTVFSLDMTKKKVSVCGPQAYPLITNECPFNDVFNKVSFSPVSEFAADKIMNCILDMDDTTQSQVYDTKKCYGVKGSDLNTLCFTKVTAHSFVIKGSSVSEIAPGKHGIIADYTFKLPDNFVGCVVARSAQNYYARAGYNDGYWYVVHTDDYIKPFQNLKRNPLKFFGFEPGNKSGDQPYEVIIFDFDTHYEKMDVCGPEDRPTIYTTCDTASIFERFSYASVDDYSSSELKNCVLNTYNIYKSNKWDTKICYGTDVKDLNQKCFTKVTAHSFVIRSDLESEIAPGRTGDLADTSFKLADNFEGCVYAFSSRNKVANQNGDNSFWYTVTSEDKLMPFEKNEISSDRRYNFITENGPRNQPYNVVVLHFDQHHAFESVCGPLTHPEDTRQCPFDLVFKRMSYASVNRFDRKLLQNCVADMHNTLYSTKWDTFTCYGVNARDLNDECFREVQAHSFVIQGSYTKEIAPGKRGRIADYTFKLSDDFVGCVISRSANMTLAKQYSNFNYHFRQVKGSKILPYEVLSVEPLRYYNFNKDNEREKQPYEIVVFKFDLSQDRASVCGPVKYPNNQTKCPLDKIFDAQRYASVSKFSKDVVTNCILDLREVVNSLKWDTIKCYGVKKDELNNMCFSKVIAHSFIIKGADVKQVAPGKHGELANNSFKLSNNFTGCVISKSAVHAIAKDGPNDSYKLRKTVKSTIKPFENLKPSPLDRFEFNYKNSYDEQPYIVHVFEFDRSIREVDVCGPAMYPDIKTTCPLDSIFQCKEYSQVVDFGSVLVKDCVLHMNSLVESSEYDSKICYGIDIKELNNFCAVDVKAHMFIIDGKDVKEIAPGQSGSIADRTFKLPDDFTGCVVSRSSRNYYAKPGANTSYYYRDYRGEYIAPYERHTKTPLCYFLFDDTSTRNQQPYSVVVFEMNLNRTFRDVCGPLKDRPITNTCPTSEVFQTDRLSSVFSWDSFNLSTCVMDMEKVRNSKLWNTRKCYGVNISDLNAYCWSSVYAHQFVIRGDLTNQIRPGKSGTIANNTFNLNSSFIGCVISRSSVNRLASDVVNNRFMWRIVSFGNIQPFERHRSNPLVYYNFSPNNSENNQPYDVTVFEFNQKFERRDVCGPLRYPQITNKCPLDQVFDRKKFASVYNYSTENLENCILDMSGVTGSTEYKTRKCFGVSQTELNKLCFTSVSAHSFVIKGYDVKQIAPGKTGSLADYTFRLPDTFRGCVVARSLRHQTSHDLPEKEYFYRIDYNYDVNPYENLTRTPLFTYCFHDVNSKKHQPYAVVILDFDNGSDFADVCGPLPNPPITTNCPLSKIFDRLEYSSSVDYTKHDIRNCRLDTSALYLSANYTSKTCYGVLKDRLNELSYVSVSAYTFVIRGVDVAQVAPGNTGIIADNTFTLPNAFVGCVVSQDATNTIAVDNCNNDYWFRRRHNNIIDPFENKFTTPLRAFDFVRENSVGNQPFEVVVFQLDKNYRKMTVCGPVD"}], "formulas": [{"formula_id": "formula_0", "formula_text": "p(Y |X) = n i=1 p(y i |y i\u22121 , . . . , y 1 ; X)(1)", "formula_coordinates": [2.0, 350.69, 563.55, 190.75, 30.32]}, {"formula_id": "formula_1", "formula_text": "t\u2208T log p(x t = x mt t |x ins \\T ) \u2212 log p(x t = x wt t |x \\T )(2)", "formula_coordinates": [22.0, 199.32, 277.03, 342.13, 22.13]}, {"formula_id": "formula_3", "formula_text": "L A G V S E R T I D P K Q N F Y MHWC L A G V S E R T I D P K Q N F Y M H W C Confusion matrix L A G V S E R T I D P K Q N F Y MHWC L A G V S E R T I D P K Q N F Y M H W C BLOSUM62 (reference)", "formula_coordinates": [23.0, 125.25, 79.32, 311.43, 159.94]}], "doi": "10.1126/science.abj8754"}