{"title": "Reducing the Sampling Complexity of Topic Models", "authors": "Aaron Q Li; Amr Ahmed; Sujith Ravi; Alexander J Smola; Cmu Mld; Google St; Pittsburgh Pa", "pub_date": "", "abstract": "Inference in topic models typically involves a sampling step to associate latent variables with observations. Unfortunately the generative model loses sparsity as the amount of data increases, requiring O(k) operations per word for k topics. In this paper we propose an algorithm which scales linearly with the number of actually instantiated topics k d in the document. For large document collections and in structured hierarchical models k d k. This yields an order of magnitude speedup. Our method applies to a wide variety of statistical models such as PDP [16,4] and HDP [19]. At its core is the idea that dense, slowly changing distributions can be approximated efficiently by the combination of a Metropolis-Hastings step, use of sparsity, and amortized constant time sampling via Walker's alias method.", "sections": [{"heading": "INTRODUCTION", "text": "Topic models are some of the most versatile tools for modeling statistical dependencies. Given a set of observations xi \u2208 X , such as documents, logs of user activity, or communications patterns, we want to infer the hidden causes motivating this behavior. A key property in topic models is that they model p(x) via a discrete hidden factor, z via p(x|z) and p(z). For instance, z may be the cluster of a document. In this case it leads to Gaussian and Dirichlet mixture models [14]. When z is a vector of topics associated with individual words, this leads to Latent Dirichlet Allocation [3]. Likewise, whenever z indicates a term in a hierarchy, it leads to structured and mixed-content annotations [19,2,4,12]. with the data. A substantial improvement in this context was provided by [22] who exploited sparsity to decompose the collapsed sampler [9] for Latent Dirichlet Allocation. As a result the sampling cost can be reduced from O(k), the total number of topics to O(k d + kw), i.e. the number kw of topics occurring for a particular word w and k d for a particular document d. This insight led to an order of magnitude improvement for sampling topic models, thus making their implementation feasible at a large scale. In fact, the strategy is sufficiently robust that it can be extended to settings where the topic smoother depends on the words [15].\nFor small datasets the assumption k d +kw k is well satisfied. Unfortunately, as the number of documents grows, so does the number of topics in which a particular word occurs. In particular kw \u2192 k, since the probability of observing any particular topic for a given word is rarely nonzero: Assume that the probability of occurrence for a given topic for a word is bounded from below by \u03b4. Then the probability of the topic occurring at least once in a collection of n documents is given by 1 \u2212 (1 \u2212 \u03b4) n \u2265 1 \u2212 e \u2212n\u03b4 \u2192 1 for n \u2192 \u221e.\nFrom this it follows that kw = O(k) for n = O(\u03b4 \u22121 log k). In other words, for large numbers documents the efficiencies discussed in [22] vanish. This is troubling, since in many industrial settings n can be in the order of billions to trillions. Consequently, with increasing amounts of data, the time to process individual documents increases due to loss of sparsity, thus leading to a superlinear increase in runtime.\nOn the other hand, the topic-sparsity for a given document essentially remains unchanged, regardless of the total number of related documents that are available. This is due to the fact that the number of tokens per document is typically less than O(k). For instance, microblogs contain only dozens of words, yet admit to thousands of topics. 1 This situation is exacerbated when it comes to hierarchical and structured topic models, since there the number of (sub)topics can grow considerably more rapidly. Hence the use of sparsity is crucial in designing efficient samplers.", "publication_ref": ["b13", "b2", "b18", "b1", "b3", "b11", "b21", "b8", "b14", "b21", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Metropolis-Hastings-Walker Sampling", "text": "The present paper proposes a new decomposition of the collapsed conditional probability, in conjunction with a Metropolis-Hastings [7] scheme and the use of the alias method, introduced by Walker [20,13], to amortize dense updates for random variables. This method is highly versatile. It defers corrections to the model and avoids renormalization. This allows us to apply it to both flat and hierarchical models. Experimental evaluation demonstrates the efficacy of our approach, yielding orders of magnitude acceleration and a simplified algorithm.\nWhile we introduce our algorithm in the context of topic models, it is entirely general and applies to a much richer class of models. At its heart lies the insight that in many inference problems the model parameters only change relatively slowly during sampling. For instance, the location of cluster centers, the definition of topics, or the shape of autoregressive functions, only change relatively slowly. Hence, if we could draw from a distribution over k outcomes k times, Walker's alias method would allow us to generate samples in amortized constant time. At the same time, the Metropolis Hastings algorithm allows us to use approximations of the correct probability distribution, provided that we compute ratios between successive states correctly. Our approach is to draw from the stale distribution in constant time and to accept the transition based on the ratio between successive states. This step takes constant time. Moreover, the proposal is independent of the current state. Once k samples have been drawn, we simply update the alias table. In honor of the constitutent algorithms we refer to our technique as the Metropolis Hastings Walker (MHW) sampler.", "publication_ref": ["b6", "b19", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "TOPIC MODELS", "text": "We begin with a brief introduction to topic models and the associated inference problems. This includes a short motivation of sampling schemes in the context collapsed samplers [9,18] and of stochastic variational models [21]. It is followed by a description of extensions to hierarchical models.", "publication_ref": ["b8", "b17", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Latent Dirichlet Allocation", "text": "In LDA [3] one assumes that documents are mixture distributions of language models associated with individual topics. That is, the documents are generated following the graphical model below:\nfor all i for all d for all k \u03b1 \u03b8 d z di w di \u03c8 k \u03b2\nFor each document d draw a topic distribution \u03b8 d from a Dirichlet distribution with concentration parameter \u03b1\n\u03b8 d \u223c Dir(\u03b1).(1)\nFor each topic t draw a word distribution from a Dirichlet distribution with concentration parameter \u03b2 \u03c8t \u223c Dir(\u03b2).\n(2)\nFor each word i \u2208 {1 . . . n d } in document d draw a topic from the multinomial \u03b8 d via\nz di \u223c Discrete(\u03b8 d ). (3\n)\nDraw a word from the multinomial \u03c8z di via\nw di \u223c Discrete(\u03c8z di ). (4\n)\nThe beauty of the Dirichlet-multinomial design is that the distributions are conjugate. This means that the multinomial distributions \u03b8 d and \u03c8 k can be integrated out, thus allowing one to express p(w, z|\u03b1, \u03b2, n d ) in closed-form [9]. This yields a Gibbs sampler to draw p(z di |rest) efficiently.\nThe conditional probability is given by\np(z di |rest) \u221d (n \u2212di td + \u03b1t)(n \u2212di tw + \u03b2w) n \u2212di t +\u03b2 .(5)\nHere the count variables n td , ntw and nt denote the number of occurrences of a particular (topic,document) and (topic,word) pair, or of a particular topic respectively. Moreover, the superscript \u2022 \u2212di denotes said count when ignoring the pair (z di , w di ). For instance, n \u2212di tw is obtained when ignoring the (topic,word) combination at position (d, i). Finally, \u03b2 := w \u03b2w denotes the joint normalization.\nAt first glance, sampling from (5) appears to cost O(k) time since we have k nonzero terms in a sum that needs to be normalized. [22] devised an ingenious strategy for exploiting sparsity by decomposing terms into\np(z di |rest) \u221d \u03b2w \u03b1t n \u2212di t +\u03b2 + n \u2212di td \u03b2w n \u2212di t +\u03b2 + n \u2212di tw n \u2212di td + \u03b1t n \u2212di t +\u03b2\nAs can be seen, for small collections of documents only the first term is dense, and more specifically, t \u03b1t/(n \u2212di t +\u03b2) can be computed from t \u03b1t/(nt +\u03b2) in O(1) time. That is, whenever both n td and ntw are sparse, sampling from p(z di |rest) can be accomplished efficiently. The use of packed index variables and a clever reordering of (topic,count) pairs further improve efficient sampling to O(kw + k d ).\nStochastic variational inference [11] requires an analogous sampling step. The main difference being that rather than using n tw +\u03b2w n t +\u03b2 to capture p(w|t) one uses a natural parameter \u03b7tw associated with the conjugate variational distribution. Unfortunately this renders the model dense, unless rather careful precautions are undertaken [11] to separate residual dense and sparse components.\nInstead, we devise a sampler to draw from p(z di |rest) in amortized O(k d ) time. We accomplish this by using\np(z di |rest) \u221d n \u2212di td n \u2212di tw + \u03b2w n \u2212di t +\u03b2 + \u03b1t(n \u2212di tw + \u03b2w) n \u2212di t +\u03b2 (6)\nHere the first term is sparse in k d and we can draw from it in O(k d ) time. The second term is dense, regardless of the number of documents (this holds true for stochastic variational samplers, too). However, the 'language model' p(w|t) does not change too drastically whenever we resample a single word. The number of words is huge, hence the amount of change per word is concomitantly small. This insight forms the basis for applying Metropolis-Hastings-Walker sampling.", "publication_ref": ["b2", "b8", "b21", "b10", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Poisson Dirichlet Process", "text": "To illustrate the fact that the MHW sampler also works with models containing a dense generative part, we describe its application to the Poisson Dirichlet Process [4,16]. The model is given by the following variant of the LDA model:\nfor all i for all d for all k \u03b1 \u03b8 d z di w di \u03c8t \u03c80 \u03b2\nIn a conventional topic model the language model is simply given by a multinomial draw from a Dirichlet distribution. This fails to exploit distribution information between topics, such as the fact that all topics have the same common underlying language. A means for addressing this problem is to add a level of hierarchy to model the distribution over \u03c8t via t p(\u03c8t|\u03c80)p(\u03c80|\u03b2) rather than t p(\u03c8t|\u03b2). Such a model is depicted above.\nThe ingredients for a refined language model are a Pitman-Yor Topic Model (PYTM) [17] that is more appropriate to deal with natural languages. This is then combined with the Poisson Dirichlet Process (PDP) [16,4] to capture the fact that the number of occurences of a word in a natural language corpus follows power-law. Within a corpus, the frequency of a word is approximately inversely proportional to its ranking in number of occurences. Each draw from a Poisson Dirichlet Process PDP(b, a, \u03c80) is a probability distribution. The base distribution \u03c80 defines the common underlying distribution shared across the generated distributions. Under the settings of Pitman-Yor Topic Model, each topic defines a distribution over words, and the base distribution defines the common underlying common language model shared by the topics. The concentration parameter b controls how likely a word is to occur again while being sampled from the generated distribution. The discount parameter a prevents a word to be sampled too often by imposing a penalty on its probability based on its frequency. The combined model described explicityly in [5]:\n\u03b8 d \u223c Dir(\u03b1) \u03c80 \u223c Dir(\u03b2) z di \u223c Discrete(\u03b8 d ) \u03c8t \u223c PDP(b, a, \u03c80) w di \u223c Discrete (\u03c8z di )\nAs can be seen, the document-specific part is identical to LDA whereas the language model is rather more sophisticated. Likewise, the collapsed inference scheme is analogous to a Chinese Restaurant Process [6,5]. The technical difficulty arises from the fact that we are dealing with distributions over countable domains. Hence, we need to keep track of multiplicities, i.e. whether any given token is drawn from \u03b2i or \u03b20. This will require the introduction of additional count variables in the collapsed inference algorithm.\nEach topic is equivalent to a restaurant. Each token in the document is equivalent to a customer. Each type of word corresponds each type of dish served by the restaurant. The same results in [6] can be used to derive the conditional probability by introducing axillary variables:\n\u2022 stw denotes the number of tables serving dish w in restaurant t. Here t is the equivalent of a topic. \u2022 r di indicates whether w di opens a new table in the restaurant or not (to deal with multiplicities). \u2022 mtw denotes the number of times dish w has been served in restaurant t (analogously to n wk in LDA).\nThe conditional probability is given by:\np(z di = t, r di = 0|rest) \u221d \u03b1t + n dt bt + mt mtw + 1 \u2212 stw mtw + 1 S m tw +1 s tw ,a t S m tw s tw ,a t(7)\nif no additional 'table' is opened by word w di . Otherwise\np(z di = t, r di = 1|rest) (8) \u221d(\u03b1t + n dt ) bt + atst bt + mt stw + 1 mtw + 1 \u03b3 + stw \u03b3 + st S m tw +1 s tw +1,a t S m tw s tw ,a t\nHere S N M,a is the generalized Stirling number. It is given by\nS N +1 M,a = S N M \u22121,a + (N \u2212 M a)S N M,a and S N M,a = 0\nfor M > N , and S N 0,a = \u03b4N,0. A detailed analysis is given in [4]. Moreover we have mt = w mtw, and st = t stw.\nSimilar to the conditional probability expression in LDA, these two expressions can be written as a combination of a sparse term and a dense term, simply by splitting the factor (\u03b1t + n dt ) into its sparse component n dt and its dense counterpart \u03b1t. Hence we can apply the same strategy as before when sampling topics from LDA, albeit now using a twice as large space of state variables.", "publication_ref": ["b3", "b15", "b16", "b15", "b3", "b4", "b5", "b4", "b5", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Hierarchical Dirichlet Process", "text": "To illustrate the efficacy and generality of our approach we discuss a third case where the document model itself is more sophisticated than a simple collapsed Dirichlet-multinomial. We demonstrate that there, too, inference can be performed efficiently. Consider the two-level topic model based on the Hierarchical Dirichlet Process [19] (HDP-LDA). In it, the topic distribution for each document \u03b8 d is drawn from a Dirichlet process DP(b1, \u03b80). In turn, \u03b80 is drawn from a Dirichlet process DP(b0, H(\u2022)) governing the distribution over topics. In other words, we add an extra level of hierarchy on the document side (compared to the extra hierarchy on the language model used in the PDP).\nfor all i for all d for all k H \u03b80 \u03b8 d z di w di \u03c8 k \u03b2\nMore formally, the joint distribution is as follows:\n\u03b80 \u223c DP(b0, H(\u2022)) \u03c8t \u223c Dir(\u03b2) \u03b8 d \u223c DP(b1, \u03b80) z di \u223c Discrete(\u03b8 d ) w di \u223c Discrete (\u03c8z di )\nBy construction, DP(b0, H(\u2022)) is a Dirichlet Process, equivalent to a Poisson Dirichlet Process PDP(b0, a, H(\u2022)) with the discount parameter a set to 0. The base distribution H(.) is often assumed to be a uniform distribution in most cases. At first, a base \u03b80 is drawn from DP(b0, H(\u2022)). This governs how many topics there are in general, and what their overall prevalence is. The latter is then used in the next level of the hierarchy to draw a document-specific distribution \u03b8 d that serves the same role as in LDA. The main difference is that unlike in LDA, we use \u03b80 to infer which topics are more popular than others.\nIt is also possible to extend the model to more than two levels of hierarchy, such as the infinite mixture model [19]. Similar to Poisson Dirichlet Process, an equivalent Chinese Restaurant Franchise analogy [6,19] exists for Hierarchical Dirichlet Process with multiple levels. In this analogy, each Dirichlet Process is mapped to a single Chinese Restau-rant Process, and the hierarchical structure is constructed to identify the parent and children of each restaurant.\nThe general (collapsed) structure is as follows: let Nj be the total number of customers in restaurant j and njt be the number of customers in restaurant j served with dish t. When a new customer (a token) enters restaurant j with the corresponding Dirichlet Process DP (bj, Hj(\u2022)), there are two types of seating arrangement for the customer:\n\u2022 With probability n jt b j +N j the customer is served with dish (topic) t and sits at an existing table.\n\u2022 With probability b j b j +N j the customer sits at a new table served with a new dish t drawn from Hj(\u2022).\nIn the event that the customer sits at a new table, a phantom customer is sent upstream the hierarchy to the parent restaurant of j, denoted by j , with corresponding Dirichlet Process DP (b j , H j (\u2022)). The parent restaurant then decides the seating arrangement of the phantom customer under the same rules. This process repeats, until there is no more parent restaurant or any of phantom customer decides to sit in an existing table in any parent restaurant along the path.\nWe use the block Gibbs sampler given in [6] as it allows us to extend our approach for multi-level Hierarchical Dirichlet Process, and performs better than the samplers given in [19] and the collapsed Gibbs sampler given in [4], as measured in convergence speed, running time, and topic quality.\nThe key difference of [6] relative to [19] is that rather than keeping track of relative assignments of tables to each other (and the resulting multiplicities and infrequent block moves) it simply keeps track of the level within the hierarchy of restaurants at which an individual customer opens a new table. The advantage is that this allows us to factor out the relative assignment of customers to specific tables but rather only keep track of the dishes that they consume. The obvious downside being that a small number of customers can be blocked from moves if they opened a table at a high position of the hierarchy that other customers depend upon. Improving mixing in this context is subject to future work.\nIn the setting studied above we only have a two-level HDP: that of the parent DP tying all documents together and the DP within each document, governing its topic distribution. We use z di \u2208 N to denote the topic indicator of word i at position d and u di \u2208 {0, 1} to indicate whether a new table is opened at the root level (i.e. u di = 1). Moreover, define s td to be the table counter for document d, i.e. the number of times a table serving topic t has been opened, and let st be the associated counter for the base DP, associated with tables opened at the parent level. Finally, let s := t st be the total number of tables opened at the root level.\nClearly the situation where st = 0 and u di = 0 is impossible since this would imply that we are opening a new table at document d while there is no matching table available at the root level. Hence for the collapsed sampler we only need to consider the following cases:\n\u2022 A new root topic is generated. That is, we currently have st and need to set u di = 1. \u2022 A new table is added at document d. In this case we require that st, i.e. that the topic exists at the root level. Moreover, obviously it requires that s td = 0 since we are adding the first table to serve dish t. \u2022 No additional topic is introduced but we may be introducing an additional table.\nThis amounts to the following (unnormalized) conditional probabilities. See [6] for further details.\np (z di = t, u di = u|rest)(9)\n\u221d \u03b2w + mtw \u03b2 + mt \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 b 0 b 1 b 0 +s if st = 0 and u di = 1 b 1 s 2 t (s t +1)(s+b 0 )\nif st = 0 and s td = 0\nS n dt +1 s dt +1,0 S n dt s dt ,0 s dt +1 n dk +1\nif st = 0 and s td = 0\nExpressions for the generalized form are analogous. Both forms contain a fraction with its numerator being the sum of a sparse term mtw and a dense term \u03b2w. Therefore, the conditional probability can be decomposed to a dense term multiplied by \u03b2w, and a sparse term multiplied by mtw. Applying the same methodology, the sampling complexity of a multi-level HDP can be reduced to O(kw).", "publication_ref": ["b18", "b18", "b5", "b18", "b5", "b18", "b3", "b5", "b18", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "METROPOLIS-HASTINGS-WALKER", "text": "We now introduce the key components for the MHW algorithm and how to use it in sampling topics. They consist of the alias method [20,13] and a simplified version of the Metropolis-Hastings sampler [7].", "publication_ref": ["b19", "b12", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Walker's Alias Method", "text": "Typically, when drawing from a distribution over l outcomes, it is accepted that one would need to perform O(l) work to generate a sample. In fact, this is a lower bound, since we need to inspect each probability at least once before we can construct the sampler. However, what is commonly overlooked is that there exist algorithms that allow us to draw subsequent samples from the same distribution in O(1) time. This means that drawing l samples from a distribution over l outcomes can be accomplished in O(1) amortized time per draw. We make extensive use of this fact.\nDenote by pi with i \u2208 {1 . . . l} the probabilities of a distribution over l outcomes from which we would like to sample. The algorithm works by decomposing a distribution over l events into l bins of equal probability by pairing at most two events per bin. Since it 'robs' from the probabilities pi > 1/l and adds to those with pi < 1/l it is also referred to as 'Robin Hood' method [13]. The algorithm proceeds as follows:\n1: GenerateAlias(p, l)\n2: Initialize L = H = \u2205 and A = []. 3: for i = 1 to l do 4: if pi \u2264 l \u22121 then 5: L \u2190 L \u222a {(i, pi)} 6: else 7: H \u2190 H \u222a {(i, pi)} 8:\nend if 9: end for 10: while L = \u2205 do 11:\nExtract (i, pi) from L and (h, p h ) from H 12:\nA\n\u2190 [A, (i, h, pi)] 13: if p h \u2212 pi > l \u22121 then 14: H \u2190 H \u222a {(h, p h \u2212 pi)} 15: else 16: L \u2190 L \u222a {(h, p h \u2212 pi)} 17:\nend if 18: end while 19: return A This yields an array A containing triples (i, h, p h ) with p h < l \u22121 . It runs in O(l) time since at each step one event is removed from the list. And the probabilities remain unchanged, as can be seen by induction. All we need to do now is to draw a random element from A and flip a biased coin to accept h or i with probability lp h and 1\u2212lp h respectively.\n1: SampleAlias(A, l) 2: bin = RandInt(l) 3: (i, h, p) = A[bin] 4: if lp > RandUnif(1) then 5: return h 6: else 7:\nreturn i 8: end if Note that the alias method works since we are implicitly exploiting parallelism inherent in CPUs: as long as l does not exceed 2 64 are guaranteed that even an information theoretically inefficient code will not require more than 64 bit, which can be generated in constant time.", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "Sampling with Proposal Distributions", "text": "Whenever we draw l identical samples from p it is clear that the above algorithm provides an O(1) sampler. However, if p changes, it is difficult to apply the alias sampler directly. To address this, we use rejection sampling and Metropolis-Hastings procedures. Rejection sampling proceeds as follows:\n1: Rejection(p, q, c) 2: repeat 3:\nDraw i \u223c q(i) 4: until p(i) \u2265 cq(i)RandUnif(1) 5: return i\nHere p is the distribution we would like to draw from, q is a reference distribution that makes sampling easy, and c \u2265 1 is chosen such that cq(i) \u2265 p(i) for all i. We then accept with probability p(i) cq(i) . It is well known that the expected number of samples to draw via Rejection(p, q, c) is c, provided that a good bound c exists. In this case we have the following: Lemma 1 Given l distributions pi and q over l outcomes satisfying ciq \u2265 pi, the expected amortized runtime complexity for drawing using SampleAlias(A, l) and rejecting using In many cases, unfortunately, we do not know ci, or computing ci is essentially as costly as drawing from pi itself. Moreover, in some cases ci may be unreasonably large. In this situation we resort to Metropolis Hastings sampling [7] using a stationary proposal distribution. As in rejection sampling, we use a proposal distribution q and correct the effect of sampling from the 'wrong' distribution by a subsequent acceptance step. The main difference is that Metropolis Hastings can be considerably more efficient than Rejection sampling since it only requires that the ratios of probabilities are close rather than requiring knowledge of a uniform upper bound on the ratio. The drawback is that instead of drawing iid samples from p we end up with a chain of dependent samples from p, as governed by q.\nRejection(pi, q, ci) is given by O 1 l l i=1 ci .\nFor the purpose of the current method we only need to concern ourselves with stationary distributions p and q, i.e. p(i) = p(i|j) and q(i) = q(i|j), hence we only discuss this special case below. For a more general discussion see e.g. [8].\n1: StationaryMetropolisHastings(p, q, n) 2: if no initial state exists then i \u223c q(i) 3: for l = 1 to n do 4: Draw j \u223c q(j)", "publication_ref": ["b6", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "5:", "text": "if RandUnif(1) < min 1, p(j)q(i) p(i)q(j) then 6:\ni \u2190 j 7:\nend if 8: end for 9: return i As a result, provided that p and q are sufficiently similar, the sampler accepts most of the time. This is the case, e.g. whenever we use a stale variant of p as the proposal q. Obviously, a necessary requirement is that q(i) > 0 whenever p(i) > 0, which holds, e.g. whenever we incorporate a smoother.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MHW Sampling", "text": "In combining both methods we arrive at, what we believe is a significant improvement over each component individually. It works as follows:\n1: Initialize A \u2190 GenerateAlias(p, l) 2: for i = 1 to N n do 3:\nUpdate q as needed 4:\nSample j \u223c StationaryMetropolisHastings(p, A, n) 5: end for Provided that the sampler mixes within n rounds of the MHprocedure, this generates draws from up-to-date versions of p. Note that a further improvement is possible whenever we can start with a more up-to-date draw from p, e.g. in the case of revisiting a document in a topic model. After burn-in the previous topic assignment for a given word is likely to be still pertinent for the current sampling pass.\nLemma 2 If the Metropolis Hastings sampler over N outcomes using q instead of p mixes well in n steps, the amortized cost of drawing n samples from q is O(n) per sample. This follows directly from the construction of the sampler and the fact that we can amortize generating the alias table. Note that by choosing a good starting point and after burnin we can effectively set n = 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "APPLICATIONS", "text": "We now have all components necessary for an accelerated sampler. The trick is to recycle old values for p(w di |z di ) even when they change slightly and then to correct this via a Metropolis-Hastings scheme. Since the values change only slightly, we can therefore amortize the values efficiently. We begin by discussing this for the case of 'flat' topic models and extend it to hierarchical models subsequently.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sampler for LDA", "text": "We now design a proposal distribution for (6). It involves computing the document-specific sparse term exactly and approximating the remainder with slightly stale data. Furthermore, to avoid the need to store a stale alias \nTo perform an MH-step we then draw from q(t) in O(k d ) amortized time. The step from topic s to topic t is accepted with probability min (1, \u03c0) where\n\u03c0 = n \u2212di td + \u03b1t n \u2212di sd + \u03b1s \u2022 n \u2212di tw + \u03b2w n \u2212di sw + \u03b2w \u2022 n \u2212di s +\u03b2 n \u2212di t +\u03b2 \u2022 P dw p dw (s) + Qwqw(s) P dw p dw (t) + Qwqw(t)\nNote that the last fraction effectively removes the normalization in p dw and qw respectively, that is, we take ratios of unnormalized probabilities. Complexity: To draw from q costs O(k d ) time. This is so since computing P dw has this time complexity, and so does the sampler for p dw . Moreover, drawing from qw(t) is O(1), hence it does not change the order of the algorithm. Note that repeated draws from q are only O(1) since we can use the very same alias sampler also for draws from p dw . Finally, evaluating \u03c0 costs only O(1) time. We have the following: ", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "Sampler for the Poisson Dirichlet Process", "text": "Following the same steps as above, the basic Poisson Dirichlet Process topic model can be decomposed by exploiting the sparsity of n dt . The main difference to before is that we need to account for the auxiliary variable r \u2208 {0, 1} rather than just the topic indicator t. The alias table is: As before, we use a Metropolis-Hastings sampler, although this time for the state pair (s, t) \u2192 (s , t ) and accept as before by using the ratio of current and stale probabilities (the latter given by q). As before in the context of LDA, the time complexity of this sampler is amortized O(k d ).\nqw(t, r) := \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03b1 k b t +m tw m tw \u2212s tw +1 m tw +1 S m tw +1 s tw ,a t S m tw s tw ,a t if r = 1 \u03b1 k b t", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sampler for the HDP", "text": "Due to slight differences in the nature of the sparse term and the dense term, we demonstrate the efficacy of our approach for sparse language models here. That is, we show that whenever the document model is dense but the language model sparse, our strategy still applies. In other words, this sampler works at O(kw) cost which is beneficial for infrequent words.\nFor brevity, we only discuss the derivation for the two level HDP-LDA, given that the general multi-level HDP can be easily extended from the derivation. Recall (9). Now the alias table is now given by:\nqw(t, u) :=p (z di = t, u di = u|rest) \u03b2 + mt \u03b2w Qw := t,u qw(t, u)\nand the exact term is given by p dw (t, u) :=\u03b3wp(z di = t, u di = u|rest)(\u03b3 + mt)mtw\nP dw := t,u p dw (t, u)\nAs before, we engineer the proposal distribution to be a combination of stale and fresh counts. It is given by\nq(t, u) := P dw P dw + Qw p dw (t, u) + Qw P dw + Qw qw(t, u)\nSubsequently, the state transition (t, u) \u2192 (t , u ) is accepted using straightforward Metropolis-Hastings acceptance ratios. We omitted the subscript w di = w for brevity. The same argument as above shows that the time complexity of our sampler for drawing from HDP-LDA is amortized O(kw).", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTS", "text": "To demonstrate empirically the performance of the alias method we implemented the aforementioned samplers in both their base forms that have O(k) time complexity, as well as our alias variants which have amortized O(k d ) time complexity. In addition to this, we implemented the SparseLDA [22] algorithm with the full set of features including the sorted list containing a compact encoding of ntw and n dt , as well as dynamic O(1) update of bucket values. Beyond the standard implementation provided in MalletLDA   by [22], we made two major improvements: we accelerated the sorting algorithm for the compact list of encoded values to amortized O(1); and we avoided hash maps which substantially improved the speed in general with small sacrifice of memory efficiency (we need an inverted list of the indices and an inverted list of the indices of the inverted lists).\nIn this section these implementations will be referred as [6], and AliasHDP at O(kw).\nLDA which is O(k), SparseLDA which is O(kw + k d ), AliasLDA which is O(k d ), PDP at O(k) [5], AliasPDP at O(k d ), HDP at O(k)", "publication_ref": ["b21", "b21", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Environment and Datasets", "text": "All our implementations are written in C++11 in a way that maximise runtime speed, compiled with gcc 4.8 with -O3 compiler optimisation in amd64 architecture. All our experiments are conducted on a laptop with 12GB memory and an Intel i7-740QM processor with 1.73GHz clock rate, 4\u00d7256KB L2 Cache and 6MB L3 Cache. Furthermore, we only use one single sampling thread across all experiments. Therefore, only one CPU core is active throughout and only 256KB L2 cache is available. We further disabled Turbo Boost to ensure all experiment are run at exactly 1.73GHz clock rate. Ubuntu 13.10 64bit served as runtime.\nWe use 5 datasets with a variety in sizes, vocabulary length, and document lengths for evaluation, as shown in Table 1 . RedState dataset contains American political blogs crawled from redstate.com in year 2011. GPOL contains a subset of political news articles from Reuters RCV1 collection. 2 We also included the Enron Email Dataset, 3 . NY-Times contains articles published by New York Times between year 1987 and 2007. PubMedSmall is a subset of approximately 1% of the biomedical literature abstracts from PubMed. Stopwords are removed from all datasets. Furthermore, words occurring less than 10 times are removed from NYTimes, Enron, and PubMedSmall. NYTimes, Enron, and PUBMED datasets are available at [1].", "publication_ref": ["b1", "b2", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation Metrics and Parameters", "text": "We evaluate the algorithms based on two metrics: the amount of time elapsed for one Gibbs sampling iteration and perplexity. The perplexity is evaluated after every 5 iterations, beginning with the first evaluation at the ending of the first Gibbs sampling iteration. We use the standard held-out method [10] to evaluate test perplexity, in which a small set of test documents originating from the same collection is set to query the model being trained. This produces an estimate of the document-topic mixtures\u03b8 dt for each test document d. From there the perplexity is then evaluated as:\n\u03c0(W|rest) := D d=1 N d \u22121 D d=1 log p(w d |rest) where p(w d |rest) = n d i=1 k t=1 p(wi = w|z di = t, rest)p(z di = t|rest)\nHere we obtain the estimate of p(wi = w|z di = t, rest) from the model being trained. To avoid effects due to variation in the number of topics, we hardcoded k = 1024 for all experiments except one (GPOL) where we vary k and observe the effect on speed per iteration. We use fixed values for hyperparameters in all our models, setting \u03b1 = \u03b2 = 0.1 for LDA, a = 0.1, b = 10, and \u03b3 = 0.1 for the PDP, and b0 = b1 = 10, \u03b3 = 0.1 for the HDP. For alias implementations, we fix the number of Metropolis-Hasting sampling steps at 2, as we observed a satisfactory acceptance rate (over 90%) at these settings. Only a negligible improvement in perplexity was observed by raising this value. Furthermore, we did not observe degraded topic quality even when Metropolis-Hasting sampling step was reduced to n = 1, and in all our experiments the perplexity almost perfectly converges at the same pace (i.e. along number of iterations) with the same algorithm without applying alias method (albeit with much less time per iteration).  ", "publication_ref": ["b9"], "figure_ref": [], "table_ref": []}, {"heading": "Performance Summary", "text": "Figure 6 shows the overall performance of perplexity as a function of time elapsed when comparing SparseLDA vs AliasLDA on the four larger datasets. When k is fixed to 1024, substantial performance in perplexity over running time on all problems with the exception of the Enron dataset, most likely due to its uniquely small vocabulary size. The gap in performance is increased as the datasets become larger and more realistic in size. The gain in performance is noted in particular when the average document length is smaller since our sampler scales with O(k d ) which is likely to be smaller for short documents.\nFigure 2 gives the comparison between PDP, HDP and their aliased variants on GPOL and Enron. By the time AliasPDP and AliasHDP are converged, the straightforward sampler are still at their first few iterations.", "publication_ref": [], "figure_ref": ["fig_8", "fig_3"], "table_ref": []}, {"heading": "Performance as a Function of Iterations", "text": "In the following we evaluate the performance in two separate parts: perplexity as a function of iterations and runtime vs. iterations. We first establish that the acceleration comes at no cost of degraded topic quality, as shown in Figure 6. The convergence speed and converged perplexity of AliasLDA, AliasPDP, and AliasHDP almost perfectly match the non-alias counterparts. This further shows that our choice of relatively small number of Metropolis-Hasting steps (2 per sample) is adequate.\nThe improved performance in running time of our alias implementations can be seen in all phases of sampling when compared to non-alias standard implementations (LDA, PDP, HDP). When compared to SparseLDA (Figure 3), the performance gain is salient during all phases on larger datasets (except for the early stage in Enron dataset), and the performance is very close on small datasets (0.35s per iteration on AliasLDA vs. 0.25s per iteration on SparseLDA). As the size of the data grows AliasLDA outperforms SparseLDA without degrading topic quality, reducing the amount of time for each Gibbs iteration on NYTimes corpus by around 12% to 38% overall, on Enron corpus by around 30% after 30 iterations, and on PubMedSmall corpus by 27%-60% throughout the first 50 iterations. Compared to SparseLDA, the time required for each Gibbs iteration with AliasLDA grows at a much slower rate, and the benefits of reduced sampling complexity is particularly clear when the average length of each document is small.  The gap in performance is especially large for more sophisticated language modelsl such as PDP and HDP. The running time for each Gibbs iteration is reduced by 60% to 80% for PDP, and 80% to 95% for HDP, an order of magnitude on improvement.", "publication_ref": [], "figure_ref": ["fig_8", "fig_1"], "table_ref": []}, {"heading": "Varying the number of topics", "text": "When the number of topics k increases, the running time for an iteration of AliasLDA increases at a much lower rate than SparseLDA, as seen from Figure 4 on dataset GPOL since k d is almost constant. Even though the gap between SparseLDA and AliasLDA may seem insignificant at k = 1024, it becomes very pronounced at k = 2048 (45% improvement) and at k = 4096 (over 100%) This confirms the observation above that shorter documents benefits more from AliasLDA in the sense that the average documents length L/D relative to the number of topics k becomes \"shorter\" as k increases. This yields a more sparse n dt and lower k d for a document d on average.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Varying the corpus size", "text": "Figure 5 demonstrates how the gap in running time speed scales with growing number of documents in the same domain. We measure the average runtime for the first 50 Gibbs iterations on 10%, 20%, 40%, 75%, and 100% of PubMedSmall dataset. The speedup ratio for each subset is at 31%, 34%, 37%, 41%, 43% respectively. In other words, it increases with the amount of data, which conforms our intuition that adding new documents increases the density of ntw, thus slowing down the sparse sampler much more than the alias sampler, since the latter only depends on k d rather than k d + kw.  ", "publication_ref": [], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "Perplexity vs. Runtime", "text": "GPOL", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSION", "text": "In this paper, we described an approach that effectively reduces sampling complexity of topic models from O(k) to O(k d ) in general, and from O(kw+k d ) (SparseLDA) to O(k d ) (AliasLDA) for LDA topic model. Empirically, we showed that our approach scales better than existing state-of-theart method when the number of topics and the number of documents become large. This enables many large scale applications, and many existing applications which require a scalable distributed approach. In many industrial applications where the number of tokens easily reaches billions, these properties are crucial and often desirable in designing a scalable and responsive service. We also demonstrated an order of magnitude improvement when our approach is applied to complex models such as PDP and HDP. With an order of magnitude gain in speed, PDP and HDP may become much more appealing to many applications for their superior convergence performance, and more sophisticated representation of topic distributions and language models.\nFor k = 1024 topics the number of tokens processed per second in our implementation is beyond 1 million for all datasets except one (NYTimes), of which contains substantially more lengthy documents. This is substantially faster than many known implementations when measured in number of tokens processed per computing second per core, such as YahooLDA [18], and GraphLab, given that we only utilise a single thread on a single laptop CPU core.", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "UCI machine learning repository", "journal": "", "year": "2013", "authors": "K Bache; M Lichman"}, {"ref_id": "b1", "title": "The nested chinese restaurant process and Bayesian nonparametric inference of topic hierarchies", "journal": "Journal of the ACM", "year": "2010", "authors": "D Blei; T Griffiths; M Jordan"}, {"ref_id": "b2", "title": "", "journal": "Latent Dirichlet allocation. JMLR", "year": "2003-01", "authors": "D Blei; A Ng; M Jordan"}, {"ref_id": "b3", "title": "A bayesian review of the poisson-dirichlet process", "journal": "", "year": "2010", "authors": "W Buntine; M Hutter"}, {"ref_id": "b4", "title": "Differential topic models", "journal": "", "year": "2014", "authors": "C Chen; W Buntine; N Ding; L Xie; L Du"}, {"ref_id": "b5", "title": "Sampling table configurations for the hierarchical poisson-dirichlet process", "journal": "", "year": "2011", "authors": "C Chen; L Du; W Buntine"}, {"ref_id": "b6", "title": "Bayesian estimation of state-space model using the metropolis-hastings algorithm within gibbs sampling", "journal": "Computational Statistics and Data Analysis", "year": "2001", "authors": "J Geweke; H Tanizaki"}, {"ref_id": "b7", "title": "Markov Chain Monte Carlo in Practice", "journal": "", "year": "1995", "authors": "W R Gilks; S Richardson; D J Spiegelhalter"}, {"ref_id": "b8", "title": "Finding scientific topics", "journal": "PNAS", "year": "2004", "authors": "T Griffiths; M Steyvers"}, {"ref_id": "b9", "title": "Parameter estimation for text analysis", "journal": "", "year": "2004", "authors": "G Heinrich"}, {"ref_id": "b10", "title": "Stochastic variational inference", "journal": "", "year": "2012", "authors": "M Hoffman; D M Blei; C Wang; J Paisley"}, {"ref_id": "b11", "title": "Nonparametric bayes pachinko allocation", "journal": "", "year": "2007", "authors": "W Li; D Blei; A Mccallum"}, {"ref_id": "b12", "title": "Fast generation of discrete random variables", "journal": "Journal of Statistical Software", "year": "2004", "authors": "G Marsaglia; W W Tsang; J Wang"}, {"ref_id": "b13", "title": "Markov chain sampling methods for Dirichlet process mixture models", "journal": "", "year": "1998", "authors": "R M Neal"}, {"ref_id": "b14", "title": "Word features for latent dirichlet allocation", "journal": "", "year": "2010", "authors": "J Petterson; A Smola; T Caetano; W Buntine; S Narayanamurthy"}, {"ref_id": "b15", "title": "The two-parameter poisson-dirichlet distribution derived from a stable subordinator", "journal": "A. of Probability", "year": "1997", "authors": "J Pitman; M Yor"}, {"ref_id": "b16", "title": "Topic models with power-law using Pitman-Yor process", "journal": "ACM", "year": "2010", "authors": "I Sato; H Nakagawa"}, {"ref_id": "b17", "title": "An architecture for parallel topic models", "journal": "", "year": "2010", "authors": "A J Smola; S Narayanamurthy"}, {"ref_id": "b18", "title": "Hierarchical dirichlet processes", "journal": "JASA", "year": "2006", "authors": "Y Teh; M Jordan; M Beal; D Blei"}, {"ref_id": "b19", "title": "An efficient method for generating discrete random variables with general distributions", "journal": "ACM TOMS", "year": "1977", "authors": "A J Walker"}, {"ref_id": "b20", "title": "Online variational inference for the hierarchical Dirichlet process", "journal": "", "year": "2011", "authors": "C Wang; J Paisley; D M Blei"}, {"ref_id": "b21", "title": "Efficient methods for topic model inference on streaming document collections", "journal": "", "year": "2009", "authors": "L Yao; D Mimno; A Mccallum"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Proof.Preprocessing costs amortized O(1) time. Each rejection sampler costs O(ci) work. Averaging over the draws proves the claim.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Lemma 33Drawing up to k steps in a Metropolis-Hastings proposal from p(z di |rest) can be accomplished in O(k d ) amortized time per sample and O(k) space.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 1 :1Figure 1: Runtime time comparison between LDA, HDP, PDP and their Alias sampled counterparts AliasLDA, AliasHDP and AliasPDP.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: Perplexity as a function of runtime (in seconds) for PDP, AliasPDP, HDP, and AliasHDP on GPOL (left) and Enron (right).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: Runtimes of SparseLDA and AliasLDA on PubMedSmall (left) and NyTimes (right).", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 :4Figure 4: Comparison of SparseLDA and AliasLDA on GPOL when varying the number of topics for k \u2208 {256, 1024, 2048, 4096}. Percentage of full PubMedSmall collection Seconds per iteration", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 5 :5Figure 5: Average runtime per iteration when compared on {10%, 20%, 40%, 75%, 100%} of the PubMedSmall dataset for SparseLDA and AliasLDA.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 6 :6Figure6: Perplexity as a function of runtime (left) and number of iterations (right) for LDA, SparseLDA, and LDA, PDP and HDP, both with and without using the Alias method. We see considerable acceleration at unchanged perplexity.", "figure_data": ""}, {"figure_label": "A", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "we simply draw from the distribution and keep the samples. Once this supply is exhausted we compute a new table. and the associated probability distribution. Then we perform the following steps:1. Generate the alias table A using qw. 2. Draw k samples from qw and store them in Sw. 3. Discard A and only retain Qw and the array Sw.Generating Sw and computing Qw costs O(k) time. In particular, storage of Sw requires at most O(k log 2 k) bits, thus it is much more compact than A. Note, however, that we need to store Qw and qw(t).", "figure_data": "Alias table: Denote byQw :=t\u03b1tntw + \u03b2w nt +\u03b2and qw(t) :=\u03b1t Qwntw + \u03b2w nt +\u03b2the alias normalization Metropolis Hastings proposal: Denote byP dw :=tn \u2212di tdn \u2212di tw + \u03b2w n \u2212di t +\u03b2and p dw (t) :=n \u2212di td P dwn \u2212di tw + \u03b2w n \u2212di t +\u03b2the sparse document-dependent topic contribution. Com-puting it costs O(k d ) time. This allows us to construct aproposal distributionq(t) :=P dw P dw + Qwp dw (t) +Qw P dw + Qwqw(t)"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "t +a t s tw b t +m tw", "figure_data": "Likewise, the sparse document-specific contribution isp dw (t, r) :=n dt\uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f31 b t +m tm tw \u2212s tw +1 m tw +1 s tw +1 m tw +1 \u03b2+s tw S m tw +1 s tw ,a t S m tw s tw ,a t \u03b2+s t m tw +1 S s tw +1,a t S m tw s tw ,a tif r = 1 otherwiseP dw :=p dw (t, r)r,tAs previously, computing p dw (t, r) only costs O(k d ) time,which allows a proposal distribution very similar to the casein LDA to be constructed:q(t, r) :=P dw P dw + Qwp dw (t, r) +Qw P dw + Qwqw(t, r)+a t s t b t +m ts tw +1 m tw +1\u03b2+s tw \u03b2+s tm tw +1 s tw +1,a t S S m tw s tw ,a totherwiseQw :=qw(t, r)r,t"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": "Datasets and their statistics. V: vocabularysize; L: total number of training tokens, D: numberof training documents; T: number of test documents.L/V is the average number occurrences of a word.L/D is the average document length."}], "formulas": [{"formula_id": "formula_0", "formula_text": "for all i for all d for all k \u03b1 \u03b8 d z di w di \u03c8 k \u03b2", "formula_coordinates": [2.0, 73.2, 530.01, 194.51, 29.75]}, {"formula_id": "formula_1", "formula_text": "\u03b8 d \u223c Dir(\u03b1).(1)", "formula_coordinates": [2.0, 135.68, 598.9, 157.23, 8.35]}, {"formula_id": "formula_2", "formula_text": "z di \u223c Discrete(\u03b8 d ). (3", "formula_coordinates": [2.0, 133.04, 708.05, 155.94, 8.35]}, {"formula_id": "formula_3", "formula_text": ")", "formula_coordinates": [2.0, 288.99, 708.05, 3.92, 7.86]}, {"formula_id": "formula_4", "formula_text": "w di \u223c Discrete(\u03c8z di ). (4", "formula_coordinates": [2.0, 393.74, 80.43, 158.26, 9.49]}, {"formula_id": "formula_5", "formula_text": ")", "formula_coordinates": [2.0, 552.0, 80.43, 3.92, 7.86]}, {"formula_id": "formula_6", "formula_text": "p(z di |rest) \u221d (n \u2212di td + \u03b1t)(n \u2212di tw + \u03b2w) n \u2212di t +\u03b2 .(5)", "formula_coordinates": [2.0, 360.49, 170.81, 195.43, 24.56]}, {"formula_id": "formula_7", "formula_text": "p(z di |rest) \u221d \u03b2w \u03b1t n \u2212di t +\u03b2 + n \u2212di td \u03b2w n \u2212di t +\u03b2 + n \u2212di tw n \u2212di td + \u03b1t n \u2212di t +\u03b2", "formula_coordinates": [2.0, 316.81, 327.52, 238.0, 24.56]}, {"formula_id": "formula_8", "formula_text": "p(z di |rest) \u221d n \u2212di td n \u2212di tw + \u03b2w n \u2212di t +\u03b2 + \u03b1t(n \u2212di tw + \u03b2w) n \u2212di t +\u03b2 (6)", "formula_coordinates": [2.0, 343.77, 538.33, 212.14, 24.23]}, {"formula_id": "formula_9", "formula_text": "for all i for all d for all k \u03b1 \u03b8 d z di w di \u03c8t \u03c80 \u03b2", "formula_coordinates": [3.0, 64.23, 77.21, 226.46, 29.75]}, {"formula_id": "formula_10", "formula_text": "\u03b8 d \u223c Dir(\u03b1) \u03c80 \u223c Dir(\u03b2) z di \u223c Discrete(\u03b8 d ) \u03c8t \u223c PDP(b, a, \u03c80) w di \u223c Discrete (\u03c8z di )", "formula_coordinates": [3.0, 79.96, 411.68, 186.78, 36.39]}, {"formula_id": "formula_11", "formula_text": "p(z di = t, r di = 0|rest) \u221d \u03b1t + n dt bt + mt mtw + 1 \u2212 stw mtw + 1 S m tw +1 s tw ,a t S m tw s tw ,a t(7)", "formula_coordinates": [3.0, 51.46, 694.87, 241.44, 23.25]}, {"formula_id": "formula_12", "formula_text": "p(z di = t, r di = 1|rest) (8) \u221d(\u03b1t + n dt ) bt + atst bt + mt stw + 1 mtw + 1 \u03b3 + stw \u03b3 + st S m tw +1 s tw +1,a t S m tw s tw ,a t", "formula_coordinates": [3.0, 340.84, 74.99, 215.09, 37.25]}, {"formula_id": "formula_13", "formula_text": "S N +1 M,a = S N M \u22121,a + (N \u2212 M a)S N M,a and S N M,a = 0", "formula_coordinates": [3.0, 339.41, 138.07, 193.91, 11.67]}, {"formula_id": "formula_14", "formula_text": "for all i for all d for all k H \u03b80 \u03b8 d z di w di \u03c8 k \u03b2", "formula_coordinates": [3.0, 326.03, 430.13, 227.7, 29.75]}, {"formula_id": "formula_15", "formula_text": "\u03b80 \u223c DP(b0, H(\u2022)) \u03c8t \u223c Dir(\u03b2) \u03b8 d \u223c DP(b1, \u03b80) z di \u223c Discrete(\u03b8 d ) w di \u223c Discrete (\u03c8z di )", "formula_coordinates": [3.0, 352.54, 486.11, 167.64, 49.84]}, {"formula_id": "formula_16", "formula_text": "p (z di = t, u di = u|rest)(9)", "formula_coordinates": [4.0, 338.55, 86.27, 217.37, 8.35]}, {"formula_id": "formula_17", "formula_text": "\u221d \u03b2w + mtw \u03b2 + mt \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 b 0 b 1 b 0 +s if st = 0 and u di = 1 b 1 s 2 t (s t +1)(s+b 0 )", "formula_coordinates": [4.0, 329.84, 100.26, 211.85, 57.88]}, {"formula_id": "formula_18", "formula_text": "S n dt +1 s dt +1,0 S n dt s dt ,0 s dt +1 n dk +1", "formula_coordinates": [4.0, 391.28, 140.45, 53.81, 23.48]}, {"formula_id": "formula_19", "formula_text": "2: Initialize L = H = \u2205 and A = []. 3: for i = 1 to l do 4: if pi \u2264 l \u22121 then 5: L \u2190 L \u222a {(i, pi)} 6: else 7: H \u2190 H \u222a {(i, pi)} 8:", "formula_coordinates": [4.0, 320.71, 533.35, 141.45, 70.63]}, {"formula_id": "formula_20", "formula_text": "\u2190 [A, (i, h, pi)] 13: if p h \u2212 pi > l \u22121 then 14: H \u2190 H \u222a {(h, p h \u2212 pi)} 15: else 16: L \u2190 L \u222a {(h, p h \u2212 pi)} 17:", "formula_coordinates": [4.0, 316.81, 637.96, 130.32, 60.17]}, {"formula_id": "formula_21", "formula_text": "Draw i \u223c q(i) 4: until p(i) \u2265 cq(i)RandUnif(1) 5: return i", "formula_coordinates": [5.0, 57.69, 372.0, 133.82, 28.79]}, {"formula_id": "formula_22", "formula_text": "Rejection(pi, q, ci) is given by O 1 l l i=1 ci .", "formula_coordinates": [5.0, 53.8, 515.0, 186.79, 12.55]}, {"formula_id": "formula_24", "formula_text": "\u03c0 = n \u2212di td + \u03b1t n \u2212di sd + \u03b1s \u2022 n \u2212di tw + \u03b2w n \u2212di sw + \u03b2w \u2022 n \u2212di s +\u03b2 n \u2212di t +\u03b2 \u2022 P dw p dw (s) + Qwqw(s) P dw p dw (t) + Qwqw(t)", "formula_coordinates": [6.0, 53.8, 380.62, 246.9, 25.04]}, {"formula_id": "formula_25", "formula_text": "qw(t, r) := \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03b1 k b t +m tw m tw \u2212s tw +1 m tw +1 S m tw +1 s tw ,a t S m tw s tw ,a t if r = 1 \u03b1 k b t", "formula_coordinates": [6.0, 58.38, 652.93, 220.57, 35.42]}, {"formula_id": "formula_26", "formula_text": "qw(t, u) :=p (z di = t, u di = u|rest) \u03b2 + mt \u03b2w Qw := t,u qw(t, u)", "formula_coordinates": [6.0, 343.81, 395.51, 184.45, 36.54]}, {"formula_id": "formula_27", "formula_text": "P dw := t,u p dw (t, u)", "formula_coordinates": [6.0, 355.14, 472.23, 78.75, 17.64]}, {"formula_id": "formula_28", "formula_text": "q(t, u) := P dw P dw + Qw p dw (t, u) + Qw P dw + Qw qw(t, u)", "formula_coordinates": [6.0, 337.59, 522.37, 197.54, 20.23]}, {"formula_id": "formula_29", "formula_text": "LDA which is O(k), SparseLDA which is O(kw + k d ), AliasLDA which is O(k d ), PDP at O(k) [5], AliasPDP at O(k d ), HDP at O(k)", "formula_coordinates": [7.0, 316.81, 701.81, 239.11, 8.37]}, {"formula_id": "formula_30", "formula_text": "\u03c0(W|rest) := D d=1 N d \u22121 D d=1 log p(w d |rest) where p(w d |rest) = n d i=1 k t=1 p(wi = w|z di = t, rest)p(z di = t|rest)", "formula_coordinates": [8.0, 58.94, 486.54, 228.81, 61.37]}], "doi": "10.1145/2623330.2623756"}