{"title": "Zero Shot Learning for Code Education: Rubric Sampling with Deep Learning Inference", "authors": "Mike Wu; Milan Mosse; Noah Goodman; Chris Piech", "pub_date": "", "abstract": "In modern computer science education, massive open online courses (MOOCs) log thousands of hours of data about how students solve coding challenges. Being so rich in data, these platforms have garnered the interest of the machine learning community, with many new algorithms attempting to autonomously provide feedback to help future students learn. But what about those first hundred thousand students? In most educational contexts (i.e. classrooms), assignments do not have enough historical data for supervised learning. In this paper, we introduce a human-in-the-loop \"rubric sampling\" approach to tackle the \"zero shot\" feedback challenge. We are able to provide autonomous feedback for the first students working on an introductory programming assignment with accuracy that substantially outperforms data-hungry algorithms and approaches human level fidelity. Rubric sampling requires minimal teacher effort, can associate feedback with specific parts of a student's solution and can articulate a student's misconceptions in the language of the instructor. Deep learning inference enables rubric sampling to further improve as more assignment specific student data is acquired. We demonstrate our results on a novel dataset from Code.org, the world's largest programming education platform.", "sections": [{"heading": "Introduction", "text": "The need for high quality education at scale poses a difficult challenge. The price of education per student is growing faster than economy-wide costs (Bowen 2012), limiting the resources available to support student learning. When also considering the rising need to provide adult retraining, the gap between the demand for education and our ability to provide is especially large. In recent years, massively open online courses (MOOCs) from platforms like Coursera and Code.org have made progress by scaling the delivery of content. However, MOOCs largely ignore an equally important ingredient for learning: high quality feedback. The clear societal need, alongside massive amounts of data has led to a machine learning grand challenge: learn how to provide feedback for education at scale, especially in computer science due to its apparent structure and high demand.\nScaling feedback (a.k.a. \"feedback\" challenge) has proven to be a hard machine learning problem. Despite dozens of projects to combine massive datasets with cutting edge deep learning, current approaches fall short. Three issues emerge:\n(1) for even basic computer science education, homework datasets have statistical distributions with heavy tails similar to natural language; (2) hand labeling feedback is expensive, rendering supervised solutions infeasible; (3) in real world contexts feedback is needed for assignments with small (or zero) historical records of student learning. For the billions of learners around the world, most education and assessments have at most hundreds of records. Even if students use Code.org, assignments are constantly changing, making the small-data context perennial. It is a zero-shot solution that has potential to deliver enormous social impact.\nWe build upon a simple insight that enables us to move beyond the supervised paradigm: When experts give feedback, they are asked to perform the hard task of predicting misconception (y) given program (x). When breaking down the cognitive steps that experts go through, they often solve the inference by first thinking generatively p(x, y). They imagine, \"if a student were to have a particular set of misconceptions, what sorts of programs is he or she likely to produce.\" Thinking generatively is much easier: while there are a finite set of decomposable misconceptions, they combine into exponential amounts of unique solutions.\nWe formalize this intuition into a technique we call \"rubric sampling\" to elicit samples from an expert prior of the joint distribution p(x, y) and use deep learning for inference p(y|x). With no historical examples, rubric sampling enables feedback with accuracy close to the fidelity of human teachers, outperforming data-intensive state of the art algorithms. We case study this technique on Code.org, an online programming platform that has been used by 610 million students and has provided a full curriculum to 29 million students, equivalent to 39% of the US K12 population.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Specific contributions in this paper:", "text": "1. We introduce the Zero Shot Feedback Challenge on a dataset from 8 assignments from Code.org along with an evaluation set of 800 labels.\n2. We articulate a novel solution: rubric sampling with deep learning inference which sets the new state of the art in code feedback prediction: F1 score doubled over baseline, approaching human level accuracy.\narXiv:1809.01357v2 [cs.LG] 17 Dec 2018\n3. We introduce the ability to (i) attribute feedback to specific parts of code, (ii) trace learning over time and (iii) generate synthetic datasets.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Zero Shot Feedback Challenge", "text": "The \"Zero-Shot\" Feedback Challenge is to infer the misconceptions that led to errors in a student's answer using zero historical examples of student work and zero expert annotations. Though this challenge is difficult, it is a task that humans find straightforward. Experts are especially adept at generalizing: an instructor does not need to see thousands of instances of a misunderstanding in order to understand it.\nWhy is zero-shot so important? Human annotated examples are surprisingly time consuming to acquire. In 2014, Code.org launched an initiative where hundreds of thousands of instructors were crowdsourced to provide feedback to student solutions 1 . Labeling was hard and undesirable work and the long tail of unique solutions meant that even after thousands of human hours of teacher work, the annotations were only scratching the surface of feedback. The initiative was cancelled after two years and the effort has not been reproduced since. For small classrooms and massive online platforms alike, it is infeasible to acquire the supervision required for contemporary nonlinear methods.\nWe foresee three main approaches: (1) learn to transfer information from one assignment to another, (2) learn to incorporate expert knowledge, and (3) form algorithms that can generalize from small amounts of human annotations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Education Feedback If you were to solve an assignment on Code.org today, the hints you would be given are generated from a unit test system combined with static analysis of the students solution. It has been a widely reported social-good objective to improve upon these hints (Price and Barnes 2017) especially since the state of the art is far from ideal (O' Rourke, Ballweber, and Popovi\u00ed 2014). Achieving this goal has proved to be hard. Previous research on a more basic set of Code.org challenges (the \"Hour of Code\") have scratched the surface with respect to providing feedback at scale. Original work found that latent patterns in how students solve programming assignments have signal as to how he or she should proceed (Piech et al. 2015c). Applying a neural network improved prediction of feedback (Piech et al. 2015a) but models were (1) too far from human accuracy, (2) weren't able to explain its predictions and (3) required massive amounts of data. The current state of the art combines these ideas and provides some improvements (Wang et al. 2017a). In this paper we propose a method which uses less data, approaches human accuracy and works on more complex Code.org assignments by diverging from the classic supervised framework. Research on feedback for even more complex assignments such as medical education (Geigle, Zhai, and Ferguson 2016) and natural language questions (Bulgarov and Nielsen 2018) has also relied on datahungry supervised learning and perhaps would benefit from a rubric sampling inspired approach.\n1 http://www.code.org/hints Theoretical inspiration for our expert-based generative rubric sampling derives from Brown's \"Repair Theory\" which argues that the best way to help students is to understand the generative origins of their mistakes (Brown and VanLehn 1980). Simulating student cognition has been applied to simple arithmetic problems (Koedinger et al. 2015) and recent hard coded models have been very successful in inferring why students make subtraction mistakes (Feldman et al. 2018). Researchers have argued that such expert models are infeasible for assignments as complex as coding (Paa\u00dfen et al. 2017). However, the automated hierarchical decomposition achieved by ) inspired us to develop rubric sampling, a simple expert model that works for programming assignments.\nZero Shot Learning There is a growing body of work in zero shot learning from the machine learning community, spawned by poor performance on unseen data.\nThe simplest approach is to include a human-in-the-loop. While labeling is one way human experts can \"teach\" a model, it is often not the most efficient. Instead, (Lee et al. 2017) leverages knowledge graphs build by humans to estimate a similarity score. Similarly, (Lake, Salakhutdinov, and Tenenbaum 2015) present a probabilistic knowledge graph (i.e. a Bayesian program) for generating symbols that outperform deep learning on out-of-sample alphabets. In this work, we employ a specific knowledge graph called a grammar, which we find to improve generalization.\nA more complex approach (without human involvement) focuses on unsupervised algorithms to estimate the data distribution. Wang et al. 2017b) train an adversarial autoencoder by generating synthetic examples and concurrently fitting a discriminator to classify between synthetic and empirical data. (Xian et al. 2018) propose a similar method for a CNN feature space. In this paper, we generalize this technique to a larger class of (non-differentiable) models: in lieu of a discriminator, we interpolate between synthetic and empirical data via a multimodal autoencoder.", "publication_ref": ["b7", "b8", "b7", "b10", "b2", "b1", "b0", "b3", "b1", "b7", "b5", "b4", "b11", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "The Code.org Exercises Dataset", "text": "Code.org is an online education platform for teaching beginners fundamental concepts in programming. Students build their solutions in a drag-and-drop interface that pieces together blocks of code. Its growing popularity since 2013 has captured a large audience, having been used by 610 million students worldwide. We investigate a single curriculum consisting of 8 exercises from Code.org's catalog. In this particular unit, students are learning to combine nested for loops with variables, and particularly the use of a for loop counter in calculations. The problems are all presented as drawing geometric shapes in a 2D coordinate space, requiring knowledge of angles. For instance, the curriculum begins with the task of drawing an equilateral triangle (see Figure 1).\nThe dataset is compiled from 54,488 students. Each time a student runs their code, the submission is saved. Each student is then associated with a trajectory of programs whose length depends on how many tries the student took. In total, there are 1,598,375 submissions. Since the exercises do P1 15,692 / 51,073 P2 50,190 / 48,606 P3 35,545 / 44,819 P4 12,905 / 42,995 P5 27,200 / 42,361 P6 59,693 / 41,198 P7 46,446 / 38,560 P8 59,615 / 36,727 Figure 1: The curricula for learning nested for loops in Code.org. To provide intuition on the vast domain complexity, we show the number of unique solutions and the number of students who attempted the problem for each of the 8 exercises.\nnot have a bounded solution space, students could produce arbitrarily long programs. This implies that, much like natural language, the distribution of student submissions has extremely heavy tails. Figure 2 shows how closely the submissions follow a Zipf distribution. To emphasize the difficulty, even after a million students, there is still a 15% chance that a new student generates a solution never seen before. Evaluation Metric If we only cared about accuracy, we would prioritize the handful of 5 to 10 programs that make up the majority of the dataset. But given the infinite number of possible programs, struggling students who would benefit most from feedback will almost certainly not submit any one of the \"most likely\" programs. Knowing this, we define our evaluation metrics in terms of the Zipf itself: let the head (of the Zipf) refer to the top 20 programs ordered by frequency, the tail as any program with a frequency of three or less, and the body as everything in between. Figure 2 shows the rough placement of these three splits. When evaluating models, we ignore the head: these very common programs can be manually labeled. Instead, we will report two F1 scores 2 : one for programs in the body and one for the tail.\nHuman Annotations We collected fine-grained human annotations to over 800 unique solutions (chosen randomly from P1 and P8) from 7 teaching assistants from the Stanford introduction to programming course. We chose to label programs from the easiest (P1) and hardest (P8) exercises as they are most different. Intermediate exercises (P2 to P7) share many of same structural features. The annotations are binary labels of 20 misconceptions that cover geometric concepts (e.g. doesn't understand equilateral is 60 degrees) to control flow concepts (e.g. repeats code instead of using a loop). 200 annotations were used to measure inter-rater reliability and the remaining 600 were used for evaluation. We refer to this dataset as D. Labeling took 25.9 hours (117 seconds per program). At this rate, the entire dataset would take 9987 hours of expert time, over a year of continual work.", "publication_ref": [], "figure_ref": ["fig_1", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Methods", "text": "We consider learning tasks given a dataset of n labeled examples, where each example (indexed by i) has an input string x i and a target output vector y i = [y i,1 , ..., y i,l ] composed of l independent binary labels. In this context, we assume each string represents a block-based program in Lisplike notation. Specifically, a program string is a sequence of T i tokens, each representing either an operator (functions, for loop, if statements, etc.), an operand (i.e. variable), an open parenthesis \"(\", or a close parenthesis \")\". See Listing 1 for an example program. Formally then, we describe the dataset as: D = {x i , y i } n i=1 where x i = [x i,1 , ..., x i,Ti ]. The goal is to learn a function\u0177 i = f (x i ) such that we minimize the error metric defined above, err(\u0177 i , y i ). For supervised approaches, we split D into a training (D train ) and test set (D test ) via a 80-20 ratio. For unsupervised methods, we evaluate on the entire set D. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Baselines", "text": "Majority Label As a sanity check, we can naively make predictions by voting for the majority label from D train .  To generate a synthetic example, we sequentially choose a set of non-terminal nodes, each of which will emit a set of terminal nodes. The composition of these terminal nodes make up a program whereas the composition of non-terminal nodes make up the labels/feedback. The emission and transition probabilities are specified by a human designer (or learned via evolutionary strategies).\nrecurrent neural network (RNN) to predict y i from o i . Unfortunately, this model requires x i to compile.\nFeedforward Neural Network To circumvent compilation, one can tackle the more difficult problem of predicting feedback from raw program strings (Piech et al. 2015b).\nWe train a l-dimensional classifier composed of a RNN over tokens by minimizing the binary cross entropy between predictions\u0177 i and ground truth y i vectors.\nmin l j=1 [\u2212(y i,j log\u0177 i,j ) + (1 \u2212 y i,j ) log(1 \u2212\u0177 i,j )] (1)\nThe model architecture borrows the sentence encoder (without any stochastic layers) from (Bowman et al. 2015) and concatenates a 3-layer perceptron with a softmax over l output dimensions. As we will reuse these architectures for other models, we refer to deterministic encoder as the program network and the latter MLP as the feedback network.\nTrajectory Prediction No model so far uses the fact that each student submits many programs before either stopping or reaching the correct solution. In fact, the current SOTA (Wang et al. 2017a) is to associate a trajectory of k programs (x 1 , x 2 , ..., x k ) with the label y i corresponding to the last program, x k . Then, for each program x i , we train an embedding e i = f (z|x i ), where f is the program network. This results in a sequence of embeddings (e 1 , e 2 , ..., e k ) for a single trajectory. We concurrently train a second RNN to compress the sequence to a single vector z i = RNN(e 1 , e 2 , ..., e k ). This is then provided as input to the feedback network. The hope is that structure induced by a trajectory implicitly provides labels that strengthen learning.\nDeep Generative Model Finally, we present a new baseline that is promising in the context of limited data. If we consider programs and feedback as two modalities, one approach is to capture the joint distribution p(x i , y i ). Doing so, we can make predictions by sampling from the conditional having seen the program:\u0177 i \u223c p(y i |x i ). To do this, we train a multimodal variational autoencoder, MVAE (Wu and Goodman 2018) with two channels. Succinctly, this generative model uses a product-of-experts inference network where the joint distribution factorizes into a product of distributions defined by two modalities: q(z|x, y) = q(z|x)q(z|y) where x and y are two observed modalities and z is a latent variable. We optimize the multimodal evidence lower bound (Wu and Goodman 2018; Vedantam et al. 2017), which is a sum of three lower bounds:\nE q \u03c6 h (z|x,y) [ h\u2208{x,y} \u03bb h log p \u03b8 h (h|z)] \u2212 \u03b2KL[q \u03c6 h (z|x, y), p(z)] + E q \u03c6x (z|x) [log p \u03b8x (x|z)] \u2212 \u03b2KL[q \u03c6x (z|x), p(z)] + E q \u03c6y (z|y) [log p \u03b8y y|z)] \u2212 \u03b2KL[q \u03c6y (z|y), p(z)](2)\nTo parameterize q \u03c6x and p \u03b8x , we use architectures from (Bowman et al. 2015). For q \u03c6y and p \u03b8y , we use 3-layer MLPs 3 . To the best of our knowledge, this is the first application of a deep generative model to the feedback challenge.", "publication_ref": ["b8", "b0", "b10", "b9", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Rubric Sampling", "text": "So far the models have been severely constrained by the number of labels. If we had a more efficient labeling strategy, we could better train these deep models to their full potential. For instance, imagine instead of focusing on individual programs, we ask experts to describe a student's thought process, enumerating strategies to get to a right or wrong answer. Given a detailed enough description, we can use it to label indefinitely. Granted, these labels will be noisy but the quantity should make up for any uncertainty. In fact, we can formalize such a \"description\" as a context-free grammar.\nA context-free grammar (CFG) is composed of a set of acyclic production rules describing a space of output strings. As its name suggests, each rule is applied regardless of context (meaning no conditional arguments). Formally, a production rule is made up of non-terminal and terminal symbols. Non-terminal symbols are hidden and either produce another non-terminal symbol or a terminal one. Terminal symbols are made up of tokens that appear in the final output string. For instance, consider the following CFG: S \u2192 AA; A \u2192 \u03b1; A \u2192 \u03b2. S and A are non-terminal symbols whereas \u03b1 and \u03b2 are terminal. It is easy to see that this Figure 4: (a) The F1 scores for P1 and P8. We plot two bars for each model representing the F1 score on the body (left) and on the tail (right). Rubric sampling models perform far better than baselines and grow close to human-level. The \"Zero Shot\" marking refers to rubric sampling without fine-tuning. (b) Highlighting sub-programs conditioned on 4 feedback labels. The MVAE contains a modality for highlighting masks generated using the rubric. Imagine programming education where sections of a student's code can be highlighted along with helpful diagnostics.\nCFG can only generate one of {\u03b1\u03b2, \u03b2\u03b1, \u03b1\u03b1, \u03b2\u03b2}. A probabilistic context-free grammar (PCFG) is a CFG parameterized by a vector \u03b8 where each production rule has a probability \u03b8 i attached. For example, we can make our CFG from above probabilistic: S\n1.0 \u2212 \u2212 \u2192 AA; A 0.9 \u2212 \u2212 \u2192 \u03b1; A 0.1 \u2212 \u2212 \u2192 \u03b2. Now,\nthe space of possible outputs has not changed but for instance, \u03b1\u03b2 will be more probable than \u03b2\u03b1.\nFor the feedback challenge, the non-terminal symbols are labels, y i and the terminal symbols are programs, x i . For example, a possible production rule might be:\nCorrectly identified 45 degree angle\n1.0 \u2212 \u2212 \u2192 T urn(45)\nWith a PCFG, we can generate an infinite amount of synthetically labeled programs, D syn = {x i , y i }, and use D syn to train data-hungry models. We refer to this as rubric sampling. In practice, we sample a million synthetic examples. When training supervised networks, we only include unique examples to avoid prioritizing programs in the Zipf head.\nCreating rubrics is surprisingly easy. For a novice (undergraduate) and an expert (professor), making a PCFG took 19.4 minutes. To make the process even easier, we developed a simple meta language for representing a grammar. 4", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Further Learning from Unlabeled Programs", "text": "As students use the platform, unlabeled submissions accumulate over time. We refer to the dataset as D unlabeled .\nEvolutionary Strategies We can use unlabeled data in rubric sampling to automatically learn \u03b8. This means alleviating some of the burden for a human-in-the-loop, since choosing \u03b8 is often more difficult than designing the grammar itself. Intuitively, it is easier to reason about what mistakes a student can make than how often a student will make each mistake. But since a PCFG is discrete, we cannot differentiate. However, we can hope to approximate local gradients by sampling \u03b8 values within some -neighborhood and computing finite differences along these random directions (Salimans et al. 2017). If we repeatedly take a linear combination of the \"best\" samples as measured by a fitness function, then over many iterations, we expect the PCFG to improve. The challenge is in picking the fitness function.\nA good choice is to pick \u03b8 whose generated distribution, D syn is \"closest\" to D unlabeled 5 . As both are Zipf-ian, we can properly measure \"closeness\" using a rank order metric (Havlin 1995), as rank is independent of frequency.\nRubric Sampling with MVAE Another way to service unlabeled data is to train with it: one of the features of the MVAE is that it can handle missing modalities. We can fit the MVAE with two data sources: D syn and D unlabeled .\nIn the case of missing labels, Equation 2 decomposes into the (unimodal) ELBO (Wu and Goodman 2018):\nE q \u03c6x (z|x) [log p \u03b8x (x|z)] \u2212 \u03b2KL[q \u03c6x (z|x), p(z)]\n(3) Thus, the MVAE is shown both a synthetic minibatch, (x i , y i ) \u223c D syn , which is used to compute the multimodal elbo, and an unlabeled minibatch, (x i ) \u223c D unlabeled , which computes Equation 3. We can jointly optimize the two losses by summing the gradients prior to taking an optimization step. Intuitively, this interpolates between D unlabeled and D syn , no longer completely relying on the PCFG. One can also interpret this as a soft-version of structure learning since using D unlabeled is somewhat akin to \"editing\" the PCFG.\nD unlabeled D generated log-zipf exp-zipf MVAE D unlabeled D generated\nFigure 5: log-Zipf transformation. Applying a log to frequencies of x i \u223c D is like \"tempering\". This helps mitigate the effect of a few elements dominating the distribution.\nLog-Zipf Transformation Capturing a Zipf is hard for a generative model since it is very easy to memorize the top 5 programs and very hard to capture the tail. To make it easier, we apply a log transformation, 6 e.g. if a program appears 10 times in D, it only appears once in the transformed dataset, D. Then, when generating with the MVAE, we invert the log by exponentiating the frequency of each unique program (exp-Zipf). Intuitively, log-Zipf is similar to \"tempering\" a distribution as it reduces any extreme peaks and troughs.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Recreation of Human Labels", "text": "Figure 4 reports a set of F1 scores, including human-level performance estimated from annotations. Each model is given two bar plots, one for programs in the body (left) and one in the tail (right). First, we see that baselines have lower F1 scores compared to models that take advantage of synthetic data. That being said, the new baseline MVAE we introduced already performs far better than the previous SOTA. In P1, using rubric sampling increases the F1 score by 0.31 in the body and 0.13 in the tail (we find similar gains in P8). These promising results imply that the grammar indeed is effective. We also find that combining the MVAE with rubric sampling boosts the F1 by an additional 0.2, reaching 94% accuracy in P1 and 95% in P8. With these scores, we are reasonably confident that for a new student, despite the likelihood that he/she will submit a never-beforeseen program, we will provide good feedback.\nTo put these results in terms of impact, Table 1 estimates the number of correct feedback we could have given to students in P1 to P8 based on projections from the annotated set. Over the full curriculum, our best model would have provided the correct feedback to an expected 126,000 additional programs compared to what Code.org currently uses, potentially helping thousands more students.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Tracing Knowledge Across Curricula", "text": "If we had a scalable way to provide feedback, what impact could we make to code education? We can begin to measure this impact using the full Code.org curriculum. Having  We ignore programs in the head of the Zipf as those can be manually labeled. With the best model, we could have provided 126,000 additional points of feedback.\ndemonstrated good performance on P1 and P8, we can confidently apply rubric sampling to P2 through P7. This is quite powerful as it allows us to estimate student understanding over a curricula. Critically, we can gauge the performance of both individual students and the student body as a whole. These sort of queries are valuable to teachers to be able to (1) measure a student's progress scalably and (2) judge how useful assignments and lessons have been.\nIn Figure 6, we analyze the average student's level of understanding over the 8 problems for two main concepts: loops and geometry (e.g. shapes, angles, movement). For each submission in a student's trajectory, we classify it as having either 1) no errors, 2) more loop errors, or 3) more geometry errors 7 . The figure shows the distribution of students in each of the three categories from the first 10 submissions. From looking at behavior within a problem and between problems, we can make the following inferences:\n1. Most students are successfully completing each problem. In other words, the blue area is increasing over time.\nEquivalently, the number of students still struggling by the 10th submission approaches a small number.\n2. The difficulty of problems is not uniform. P6 is much more difficult than the others as the fraction of students with correct solutions is roughly constant. In contrast, P1, P4, and P5 are easier, where students quickly cease to make mistakes. As a teacher, one could use this information to improve the educational content and better hone in on areas where more students struggle.\n3. Students are learning geometry better than looping.\nThe rate that the pink area approaches zero is consistently faster than that of the orange area. By P8, students are barely struggling with geometry but a fair proportion still find looping difficult. As the curriculum was intended to teach nested looping, one interpretation is that the drawing aspect was somewhat distracting.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Fine-grain Feedback: Code Highlighting", "text": "With most online programming education, the form of feedback is limited to pop-up tips or compiler messages. But, In general the percentage of students with no errors increases over time as more students finish the problem. Additionally, we can extrapolate that students are more effectively learning geometry than looping, as the area covered by pink decreases faster and more consistently than the area covered by orange. We can also see that P6 is somewhat of an outlier, being much more difficult for students than any other problem. (bottom row) In addition to aggregate statistics, we can also track learning for individual students. We can infer that this particular student tries several attempts with P6 before dropping out.\nwith generative models we can provide more fine-grain feedback through highlighting subsets of the program responsible for the model predicting each feedback label.\nFirst, if we consider a PCFG, the task of highlighting a program x i is equivalent to finding the most likely parsing in a probabilistic tree that would generate x i . In practice, we use the A* algorithm for fast Viterbi parsing (Klein and Manning 2003). Given the most likely parsing, we can follow the trajectory from root to leaf and record which subprograms are generated by non-terminal nodes. This has one major limitation: only programs within the support of the PCFG can be highlighted. To bypass this, we can curate a synthetic dataset with each program having a segmentation mask denoting which tokens to highlight. If we treat the mask as an additional modality, we can then learn the joint distribution over programs, labels, and masks. See (Wu and Goodman 2018) for details in defining a VAE with three modalities. In Figure 4b, we randomly sample 4 programs and show segmentation masks. The running time to compute a mask is negligible, meaning that this can be used for providing on-the-fly feedback to students. Moreover, highlighting provides a notion of interpretability (which is extremely important if we are working with students), much like Grad-Cam (Selvaraju et al. 2017) did for computer vision.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "Clustering Students by Level of Understanding", "text": "With any latent generative model, the rich latent space provides a vector representation for unstructured data. Using the MVAE, we first sample z i \u223c q(z i |x i ) for all x i \u2208 D test ; we then train use t- SNE (Maaten and Hinton 2008) to reduce to two dimensions. In Figure 7b and c, we color each embedded program from D syn by whether the true label is positive or negative. We find that the space is partitioned to group programs with similar feedback together. In Figure 7a, we see that D unlabeled is also organized into disjoint clusters. This implies that even with no data about a new student we can draw inferences from their neighbors in latent space.", "publication_ref": [], "figure_ref": ["fig_5", "fig_5"], "table_ref": []}, {"heading": "Positive", "text": "Negative Unlabeled  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discussion", "text": "We get closer to human level performance than previous SOTA. Any of the rubric sampling models beat the SOTA by at least 0.46 in P1 and 0.24 in P8, nearly tripling the F1 score in P1 and doubling in P8. In both exercises, our best model is just under 95% accuracy, which is encouraging for this to be implemented in the real world.\nWe can effectively track student growth over time.\nWith a high performing model, we can analyze students over time. For Code.org, we were able to ( 1) gauge what individual students struggle with, (2) gauge what a collective of students struggle with, (3) identify the effectiveness of a curriculum, and (4) identify the difficulty of problems.\nMaking a rubric is neither difficult nor costly. It is not the case that only experts can make a good rubric. We also asked an undergraduate (novice) to make a rubric for P1 and found that while an experts' rubric averaged 0.69 \u00b1 0.03 in F1 score, the novice's averaged 0.63 \u00b1 0.01, both of which are much higher than baselines (0.29\u00b1, 0.03). Furthermore, we measured that it took a group of teaching assistants 24.9 hours to label 800 unique programs. In comparison, it took a novice an average of 19.4 minutes to make a rubric.\nWe provide feedback to programs that do not compile. Rubic sampling and MVAE make no assumptions on program behavior, working out-of-the-box from the 1st student.\nWe do not need to handpick \u03b8 when designing a rubric.\nIn Figure 1, the PCFG uses hand-picked \u03b8. However, one can argue that it is difficult to know how often students make mistakes and yet, the choice of \u03b8 is important: performance drops if we randomly choose. For example, in P1, using hand-picked \u03b8 has a 0.26 \u00b1 0.002 increase over random \u03b8 in F1-score. In the absence of an expert, we can use evolutionary strategies to find a good minima starting from a random initialization. Over 3 runs, we found that learning \u03b8 reduces the difference to 0.007 \u00b1 0.005 in P1, even beating expert parameters by 0.005 \u00b1 0.009 in P8. With a large dataset, we only have to define the structure, not the probabilities.\nThe log-Zipf transform ensures that learning does not collapse to memorizing the most frequent programs. If we were to use the raw statistics of the student data, the MVAE would minimize loss by memorizing the most frequent programs. As evidence of this, when sampling from its generative model, we found that the MVAE only generated programs in the top 25 programs by frequency (even with 1 million samples). We propose the log-Zipf transformation as a general tool for parameter learning with Zipfdistributed data. Critically, the transform downweights the head and upweights the tail in an invertible fashion.\nThe MVAE interpolates between synthetic and empirical data. Unlike the PCFG, we can train the MVAE with multiple data sources. Effectively, the programs we show to the model is drawn from an interpolation between the synthetic distribution defined by rubric sampling and the true (unlabeled) data distribution. Intuitively, the increase in performance from the MVAE can be attributed to better capturing the true density of student programs (see Figure 9).\nWe can generate and open-source a large dataset of student programs. Datasets with student data are difficult to release to the open community. But large public datasets have been a dominant force in pushing the boundaries of research. Luckily, with generative models, we can curate our own \"Imagenet\" for education. But, we want to ensure that our dataset matches D in distribution. Intuitively, it is impossible that a PCFG can capture D since that would require production rules that span the entire tail of the Zipf. In fact, as shown in Figure 8, the PCFG is not that faithful. One remedy is to use the MVAE trained with D unlabeled as that is interpolating between distributions. Figure 8, confirms that the MVAE matches the shape of D much better. ", "publication_ref": [], "figure_ref": ["fig_1", "fig_6", "fig_6"], "table_ref": []}, {"heading": "Limitations and Future Work", "text": "The effectiveness of rubric sampling is largely determined by the complexity of the programming task. Although a block-based language like in Code.org already has an infinite number of possible programs, the set of probable student programs is much smaller than in a typical university level coding assignment. An important distinction is the introduction of variable and function names. As suggested by Figure 9, the version of rubric sampling used in this paper may have difficulty scaling to harder problems. As PCFGs are context-free, making a sufficiently expressive grammar for complex problems requires extremely large graphs with repetitive subgraphs. Future work could look to generalizing rubric sampling to arbitrary graphs with conditional branching, or investigate structural learning to improve graphs to cover more of the empirical data. The space of likely student programs in a blockbased language can be covered by a PCFG. But in a higherorder language like Python, the space is much larger.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We introduce the zero shot feedback challenge. On a widely used platform, we show rubric sampling to far surpass the SOTA. We combine this with a generative model to cluster students, highlight code, and incorporate historical data. This approach can scale feedback for real world use.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "The authors thank Lisa Wang, Baker Franke and the Code.org team for support, and guidance. MW is supported by NSF GRFP. NDG is supported by DARPA PPAML under FA8750-14-2-0006. We thank Connie Liu for edits.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "The cost disease in higher education: is technology the answer?", "journal": "", "year": "1980", "authors": "W G Bowen;  Bowman"}, {"ref_id": "b1", "title": "Proposition entailment in educational applications using deep neural networks", "journal": "ACM", "year": "2018", "authors": "F A Bulgarov; R Nielsen;  Feldman"}, {"ref_id": "b2", "title": "An exploration of automated grading of complex assignments", "journal": "ACM", "year": "2016", "authors": "Zhai Geigle; C Ferguson ; Geigle; C Zhai; D C Ferguson"}, {"ref_id": "b3", "title": "Methods for evaluating simulated learners: Examples from simstudent", "journal": "", "year": "2003", "authors": "D Klein; C D Manning; K R Koedinger; N Matsuda; C J Maclellan; E A Mclaughlin"}, {"ref_id": "b4", "title": "Humanlevel concept learning through probabilistic program induction", "journal": "Science", "year": "2015", "authors": "Salakhutdinov Lake; B M Salakhutdinov; R Tenenbaum; J B "}, {"ref_id": "b5", "title": "Multi-label zero-shot learning with structured knowledge graphs", "journal": "Journal of machine learning research", "year": "2008-11", "authors": "C.-W Lee; W Fang; C.-K Yeh; Y.-C F Wang; L Maaten; G Hinton"}, {"ref_id": "b6", "title": "Codewebs: scalable homework search for massive open online programming courses", "journal": "ACM", "year": "2014", "authors": " Nguyen"}, {"ref_id": "b7", "title": "The continuous hint factory-providing hints in vast and sparsely populated edit distance spaces", "journal": "ACM", "year": "2014", "authors": "Ballweber Rourke; ; Popovi\u00ed; E O'rourke; C Ballweber; Z Popovi\u00ed; B Paa\u00dfen; B Hammer; T W Price; T Barnes; S Gross; N Pinkwart; C Piech; J Huang; A Nguyen; M Phulsuksombati; M Sahami; L Guibas"}, {"ref_id": "b8", "title": "Position paper: Block-based programming should offer intelligent support for learners", "journal": "IEEE", "year": "2015", "authors": "C Piech; J Huang; A Nguyen; M Phulsuksombati; M Sahami; L Guibas; C Piech; M Sahami; J Huang; L Guibas; T W Price; T Barnes; T Salimans; J Ho; X Chen; S Sidor; I Sutskever; R R Selvaraju; M Cogswell; A Das; R Vedantam; D Parikh; D Batra"}, {"ref_id": "b9", "title": "Generalized zero-shot learning via synthesized examples", "journal": "", "year": "2017", "authors": " Vedantam"}, {"ref_id": "b10", "title": "Learning to represent student knowledge on programming exercises using deep learning", "journal": "", "year": "2017", "authors": "[ Wang"}, {"ref_id": "b11", "title": "Multimodal generative models for scalable weaklysupervised learning", "journal": "", "year": "2017", "authors": "China ; Wuhan;  Wang"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure2: The distribution of programs for 8 problems from Code.org follow closely to a Zipf distribution, as shown by the linear relationship between the log probability of a program and the log of its rank in frequency. 5 to 10 programs dominate while the rest are in the heavy tails.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Listing 1 :1Example x i from P1 with 51 tokens. The tokens Program and WhenRun serve the role of a start-of-sentence tokens. A model will receive each token in order. ( Program ( WhenRun ) ( Move ( F o r w a r d ) ( V a l u e ( Number ( 50 ) ) ) ) ( R e p e a t ( V a l u e ( Number ( 3 ) ) ) ( Body ( Turn ( L e f t ) ( V a l u e ( Number ( 120 ) ) ) ) ) ) )", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure3: Probabilistic grammar (PCFG) for synthetic block-based programs. To generate a synthetic example, we sequentially choose a set of non-terminal nodes, each of which will emit a set of terminal nodes. The composition of these terminal nodes make up a program whereas the composition of non-terminal nodes make up the labels/feedback. The emission and transition probabilities are specified by a human designer (or learned via evolutionary strategies).", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 6 :6Figure6: Student understanding of loops and geometry across curricula: (top row) We plot the percentage of students who are either doing perfect (cyan), struggling more with looping concepts (orange), or struggling more with geometry concepts (pink). In general the percentage of students with no errors increases over time as more students finish the problem. Additionally, we can extrapolate that students are more effectively learning geometry than looping, as the area covered by pink decreases faster and more consistently than the area covered by orange. We can also see that P6 is somewhat of an outlier, being much more difficult for students than any other problem. (bottom row) In addition to aggregate statistics, we can also track learning for individual students. We can infer that this particular student tries several attempts with P6 before dropping out.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Dsyn: Turn/Move (c) Dsyn: No Repeat", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 7 :7Figure 7: Clustering students. Using the inference network in the MVAE, we can embed a program in 2D. In D unlabeled (a), we see a handful of distinct clusters. In D syn (b,c), we find meaningful clusters that are segmented by labels.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 8 :8Figure 8: We compare D M V AE syn", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure9: The space of likely student programs in a blockbased language can be covered by a PCFG. But in a higherorder language like Python, the space is much larger.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Predicting from Output The ubiquitous way to provide feedback is via unit tests: analyze the output of executing x", "figure_data": "i) primitivesii) token emissioniii) merge tokenswhen runwhen run Terminal Nodes repeat until do repeat for doTurn TurnLeft{integer}Right \u2026For loop: correct non-terminal relation: mimic student thinking Start Token Move: wrong opMove: wrong multipledonesynthetic program: example output sampled from grammarTurn Left repeat for 3 Move forward 1000 doMove forward{string}For loop: wrong end For loop: wrong end For loop: no loop Non-Terminal NodesMove: wrong op Turn: no turnTurn: Correct For loop: Correct\u2026when run terminal relation: emission of code Turn Left repeat for do3Move forward1000part of the generation we get semantic tags as synthetic feedback:Move: wrong multiple For loop: correct Program (start token) Move: wrong op"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Amount of correct feedback over the curriculum.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "min l j=1 [\u2212(y i,j log\u0177 i,j ) + (1 \u2212 y i,j ) log(1 \u2212\u0177 i,j )] (1)", "formula_coordinates": [4.0, 66.54, 324.93, 225.96, 30.32]}, {"formula_id": "formula_1", "formula_text": "E q \u03c6 h (z|x,y) [ h\u2208{x,y} \u03bb h log p \u03b8 h (h|z)] \u2212 \u03b2KL[q \u03c6 h (z|x, y), p(z)] + E q \u03c6x (z|x) [log p \u03b8x (x|z)] \u2212 \u03b2KL[q \u03c6x (z|x), p(z)] + E q \u03c6y (z|y) [log p \u03b8y y|z)] \u2212 \u03b2KL[q \u03c6y (z|y), p(z)](2)", "formula_coordinates": [4.0, 321.16, 296.87, 240.53, 65.96]}, {"formula_id": "formula_2", "formula_text": "1.0 \u2212 \u2212 \u2192 AA; A 0.9 \u2212 \u2212 \u2192 \u03b1; A 0.1 \u2212 \u2212 \u2192 \u03b2. Now,", "formula_coordinates": [5.0, 145.87, 356.14, 146.63, 12.56]}, {"formula_id": "formula_3", "formula_text": "1.0 \u2212 \u2212 \u2192 T urn(45)", "formula_coordinates": [5.0, 215.82, 433.67, 60.8, 12.34]}, {"formula_id": "formula_4", "formula_text": "E q \u03c6x (z|x) [log p \u03b8x (x|z)] \u2212 \u03b2KL[q \u03c6x (z|x), p(z)]", "formula_coordinates": [5.0, 349.61, 574.68, 179.95, 16.66]}, {"formula_id": "formula_5", "formula_text": "D unlabeled D generated log-zipf exp-zipf MVAE D unlabeled D generated", "formula_coordinates": [6.0, 89.78, 89.23, 165.99, 64.61]}], "doi": ""}