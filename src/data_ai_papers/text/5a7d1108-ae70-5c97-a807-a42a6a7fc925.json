{"title": "NEUROLOGIC A esque Decoding: Constrained Text Generation with Lookahead Heuristics", "authors": "Ximing Lu; Sean Welleck; Peter West; Liwei Jiang; Jungo Kasai; Daniel Khashabi; Ronan Le Bras; Lianhui Qin; Youngjae Yu; Rowan Zellers; Noah A Smith; Yejin Choi", "pub_date": "", "abstract": "The dominant paradigm for neural text generation is left-to-right decoding from autoregressive language models. Constrained or controllable generation under complex lexical constraints, however, requires foresight to plan ahead for feasible future paths. Drawing inspiration from the A* search algorithm, we propose NEUROLOGIC A esque, 1 a decoding algorithm that incorporates heuristic estimates of future cost. We develop lookahead heuristics that are efficient for large-scale language models, making our method a dropin replacement for common techniques such as beam search and top-k sampling. To enable constrained generation, we build on NEU-ROLOGIC decoding (Lu et al., 2021), combining its flexibility in incorporating logical constraints with A esque estimates of future constraint satisfaction. Our approach outperforms competitive baselines on five generation tasks, and achieves new state-of-the-art performance on table-totext generation, constrained machine translation, and keyword-constrained generation. The improvements are particularly notable on tasks that require complex constraint satisfaction or in few-shot or zero-shot settings. NEU-ROLOGIC A esque illustrates the power of decoding for improving and enabling new capabilities of large-scale language models.", "sections": [{"heading": "Introduction", "text": "The dominant paradigm for neural text generation is based on left-to-right decoding from autoregressive language models such as GPT-2/3 (Radford et al., 2019;Brown et al., 2020). Under this paradigm, common decoding techniques such as beam search or top-k/p sampling (Holtzman et al., 2020)  Figure 1: NEUROLOGIC leverages lookahead heuristics to guide generations towards those that satisfy the given task-specific constraints. In this example from the COMMONGEN task, although summer is a more likely next word given the already-generated past, NEUROLOGIC looks ahead to see that selecting winter results in a generation that incorporates unsatisfied constraint snow with a higher probability later on. Thus, winter is preferred despite being lower probability than summer.\nthis lack of foresight often suffices for open-ended text generation -where any coherent text can be acceptable -for constrained text generation, planning ahead is crucial for incorporating all desired content in the generated output (Hu et al., 2017;Dathathri et al., 2019).\nClassical search algorithms such as A* search (Hart et al., 1968;Pearl, 1984;Korf, 1985) address the challenge of planning ahead by using heuristic estimation of future cost when making decisions. Drawing inspiration from A* search, we develop NEUROLOGIC A esque (shortened to NEUROLOGIC ), which combines A*-like heuristic estimates of future cost (e.g., perplexity, constraint satisfaction) with common decoding algorithms for neural text generation (e.g., beam search, top-k sampling), while preserving the efficiency demanded by large-scale neural language models.\nAs selecting the next token to generate based on the optimal future cost is NP-complete (Chen et al., 2018), we develop lookahead heuristics, which approximate cost at each decoding step based on con-tinuations of the sequence-so-far. Figure 1 shows an example, where NEUROLOGIC A esque guides generation towards a decision that would have been ignored based on the past alone, but is selected after looking ahead and incorporating the probability that constraints are satisfied in the future.\nOur approach builds on NEUROLOGIC Decoding of , a variation of beam-search for controlling generation through rich logic-based lexical constraints expressed in Conjunctive Normal Form (CNF). Our work generalizes  by ( 1) incorporating novel lookahead heuristics to estimate future contraint satisfaction, and (2) developing additional unconstrained variants that can work with an empty set of constraints. These new algorithm variants support broad applications of NEUROLOGIC , including unconstrained generation, as demonstrated in our experiments.\nOur experiments across five generation tasks demonstrate that our approach outperforms competitive baselines. We test NEUROLOGIC in conjunction with both supervised and unsupervised models and find that the performance gain is pronounced especially in zero-shot or few-shot settings. On the COMMONGEN benchmark, using NEUROLOGIC with an off-the-shelf language model outperforms a host of supervised baselines with conventional decoding algorithms, demonstrating that a strong inference-time algorithm such as NEUROLOGIC can alleviate the need for costly annotated datasets. Moreover, NEUROLOGIC achieves state-of-the-art performance in various settings, including WMT17 English-German machine translation with lexical constraints (Dinu et al., 2019) and few-shot E2ENLG table-to-text generation (Chen et al., 2020b).\nIn summary, we develop NEUROLOGIC A esque, a new decoding algorithm for effective and efficient text generation. To our knowledge this is the first A*-like algorithm for guided text generation via lookahead heuristics. Our algorithm is versatile, as it can be applied to a variety of tasks via inference-time constraints, reducing the need for costly labeled data. Extensive experiments show its effectiveness on several important generation benchmarks.", "publication_ref": ["b47", "b9", "b17", "b20", "b14", "b41", "b24", "b13", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "NEUROLOGIC A esque Decoding", "text": "We describe NEUROLOGIC A esque Decoding (shortened as NEUROLOGIC ), our decoding algorithm motivated by A * search (Hart et al., 1968), a best-first search algorithm that finds high-scoring paths using a heuristic estimate of future return. We first introduce the decoding problem, and then describe our heuristics with a novel lookahead procedure for adapting NEUROLOGIC search to unconstrained and constrained generation with largescale autoregressive models.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "Decoding With A esque Lookahead", "text": "Decoding. Sequence-to-sequence generation is the task of generating an output sequence y given an input sequence x. We consider standard leftto-right, autoregressive models, p \u03b8 (y | x) = |y| t=1 p \u03b8 (y t | y <t , x), and omit x to reduce clutter. Decoding consists of solving,\ny * = arg max y\u2208Y F (y), (1\n)\nwhere Y is the set of all sequences. In our setting, the objective F (y) takes the form s(y) + H(y), where s(y) is log p \u03b8 (y), and H(y) is either zero when no constraints are specified, or is a score for satisfying constraints on y.\nOur method takes the perspective of decoding as discrete search, in which states are partial prefixes, y <t , actions are tokens in vocabulary V (i.e., y t \u2208 V), and transitions add a token to a prefix, y <t \u2022 y t . Each step of decoding consists of (1) expanding a set of candidate next-states, (2) scoring each candidate, and (3) selecting the k best candidates:\nY t = {y <t \u2022 y t | y <t \u2208 Y t\u22121 , y t \u2208 V}, Y t = arg topk (y<t,yt)\u2208Y t {f (y <t , y t )} ,(2)\nwhere Y 0 = { bos } and f (\u2022) is a scoring function that approximates the objective F . Common decoding algorithms such as beam search score candidates without considering future tokens, e.g., f (y <t , y t ) = log p \u03b8 (y \u2264t ).\nLookahead heuristics. Our method incorporates an estimate of the future into candidate selection. Ideally, we want to select candidates that are on optimal trajectories, replacing Equation 2 with:\nY t = arg topk (y<t,yt)\u2208Y t max y>t F (y <t , y t , y >t ) , (3\n)\nwhere y >t represents future trajectories. However, computing Equation 3 presents two difficulties: 1) the objective F (y) may be unknown or difficult to compute, and 2) the space of y >t is prohibitively large.\nMotivated by A * search (Hart et al., 1968), a best-first search algorithm that finds high-scoring paths by selecting actions that maximize:\nf (a) = s(a) + h(a),\nwhere s(a) is the score-so-far and h(a) is a heuristic estimate of the future score, we approximate the objective using a lightweight heuristic h(\u2022):\nY t = arg topk y \u2264t \u2208Y t s(y \u2264t ) + max y>t h(y <t , y t , y >t ) ,(4)\nwhere s(y \u2264t ) = log p \u03b8 (y \u2264t ). To make the search tractable, we search over a set of lookahead continuations, approximating Equation 3 as,\nY t = arg topk y \u2264t \u2208Y t s(y \u2264t ) + max L (y \u2264t ) h(y \u2264t+ ) ,(5)\nwhere each element y t+1:t+ of L (y \u2264t ) is a lengthcontinuation of y \u2264t . Beam search corresponds to setting and h to 0.\nA esque decoding. Beam search, A* search, and our method fall under a general class of algorithms that differ based on (1) which candidates are expanded, (2) which candidates are pruned, (3) how candidates are scored (Meister et al., 2020). We inherit the practical advantages of beam search-style expansion and pruning, while drawing on A*-like heuristics to incorporate estimates of the future, and refer to our method as A esque decoding.\nGenerating lookaheads. We compare several methods for generating the lookaheads L (y \u2264t ).\nThe greedy lookahead produces a single sequence, L = {y t+1:t+ }, starting from y \u2264t and selecting each token according to y t = arg max y\u2208V p \u03b8 (y | y <t ).\nWe also consider a soft lookahead which interpolates between providing the greedy token and a uniform mixture of tokens as input at each step. Specifically, we adjust the model's probabilities with a temperature,p \u03b8 (y t | y <t ) = softmax(s t /\u03c4 ), where s t \u2208 R |V| is a vector of logits, and feed the expected token embedding as input at step t,\ne t = E yt\u223cp(yt|y<t) [E(y t )],(6)\nwhere E \u2208 R |V|\u00d7d is the model's token embedding matrix. The soft lookahead moves from providing the greedy token as input (\u03c4 \u2192 0) to a uniform mixture of tokens (\u03c4 \u2192 \u221e) based on the value of temperature \u03c4 . When using the soft lookahead, we usep in place of p when scoring tokens. The soft (and greedy) lookahead is efficient, but only explores a single trajectory. The beam lookahead trades off efficiency for exploration, returning a set L containing the top-k candidates obtained by running beam search for steps starting from y <t .\nFinally, the sampling lookahead explores beyond the highly-probable beam search continuations, generating each y t+1:t+ \u2208 L using, y t \u223c p \u03b8 (y | y <t ), for t from t+1 to t+k.\nNext, we move to our proposed lookahead heuristics, starting with the unconstrained setting.", "publication_ref": ["b14", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "Unconstrained Generation with NEUROLOGIC", "text": "First we consider a standard decoding setting,\narg max y\u2208Y log p \u03b8 (y | x).\nWe score candidates based on a combination of the history and estimated future, by using the likelihood of the lookahead as a heuristic. That is, at the tth step of decoding, we use Equation 5 with:\nh(y \u2264t+ ) = \u03bb log p \u03b8 (y t+1:t+ | y \u2264t , x), (7) where \u03bb controls how much we rely on the estimated future versus the history, similar to weighted A* (Pohl, 1970).", "publication_ref": ["b42"], "figure_ref": [], "table_ref": []}, {"heading": "NEUROLOGIC for Constrained Generation", "text": "Our lookahead heuristics lend themselves to decoding with lexical constraints in a way that standard beam search does not. For constrained generation, we build on and generalize NEUROLOGIC decoding algorithm of -a beambased search algorithm that supports a wide class of logical constraints for lexically constrained generation-with estimates of future constraint satisfaction.\nBackground of NEUROLOGIC. NEUROLOGIC  accepts lexical constraints in CNF:\nD 1 \u2228 D 2 \u2022 \u2022 \u2022 \u2228 D i C 1 \u2227 \u2022 \u2022 \u2022 \u2227 D i \u2228 \u2022 \u2022 \u2022 \u2228 D N C M\nwhere each D i represents a single positive or negative constraint, D(a, y) or \u00acD(a, y), enforcing the phrase a to be included in or omitted from y.  refer to each constraint D i as a literal, and each disjunction C j of literals as a clause.\nNEUROLOGIC is a beam-based approximate search for an objective which seeks fluent sequences in which all clauses are satisfied:\narg max y\u2208Y p \u03b8 (y | x) \u2212 \u03bb M j=1 (1 \u2212 C j ),\nwhere \u03bb 0 penalizes unsatisfied clauses. At each step of the search, NEUROLOGIC scores each of the k\u00d7|V| candidates (y <t , y t ) based on whether they (partially) satisfy new constraints,\nf (y \u2264t ) = log p \u03b8 (y \u2264t | x) + \u03bb 1 max D(a,y \u2264t ) |\u00e2| |a| ,(8)\nwhere the maximization is over a set of unsatisfied multi-token constraints a tracked by NEURO-LOGIC, and\u00e2 is the prefix of a in the ongoing generation. For example, for y \u2264t =\"The boy climbs an apple\" and constraint a=\"apple tree\",\u00e2 is \"apple\". Intuitively, this function rewards candidates that are in the process of satisfying a constraint.\nIn lieu of taking the top-k scoring candidates (Equation 5), NEUROLOGIC prunes candidates that contain clauses that violate constraints, groups the candidates to promote diversity, and selects highscoring candidates from each group. We use the same pruning and grouping approach, and refer the reader to  for further details.\nNEUROLOGIC decoding. Our method improves upon the NEUROLOGIC scoring function with an estimate of future constraint satisfaction. Our key addition is a lookahead heuristic that adjusts a candidate (y <t , y t )'s score proportional to the probability of satisfying additional unsatisfied constraints in the lookahead y t+1:t+ :\nh future (y \u2264t+ ) = \u03bb 2 max D(a,y \u2264t ) log p \u03b8 (D(a, y t+1:t+ ) | x, y \u2264t ), (9)\nwhere we define the probability that constraint a is satisfied using the most probable subsequence,\np \u03b8 (D(a, y t+1:t+ ) | x, y \u2264t ) = max t \u2208[t,t+ ] p \u03b8 (y t :t +|a| = a | x, y <t ), (10)\n\u03bb 2 is a scaling hyperparameter for the heuristic.\nIntuitively, this lookahead heuristic brings two benefits. When y t is a token that would satisfy a multi-token constraint, the lookahead incorporates the score of the full constraint. When y t is a token that is not part of a constraint, the lookahead allows for incorporating the score of a future constraint that would be satisfied if y t was selected.\nWe add our lookahead heuristic to the NEU-ROLOGIC scoring function (Equation 8), and call the resulting decoding procedure NEUROLOGIC A esque (or, NEUROLOGIC in short).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We first consider constrained generation benchmarks: COMMONGEN ( \u00a73.1), constrained machine translation ( \u00a73.2), table-to-text generation ( \u00a73.3), and constrained question generation ( \u00a73.4). NEUROLOGIC consistently outperforms previous approaches, especially in zero-shot and fewshot cases. These low-resource settings are particularly important, as many practical tasks face data scarcity. Finally, we find that A esque lookahead is useful even without constraints, as shown in unconstrained story generation task ( \u00a73.5).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Metrics.", "text": "As automatic metrics, we use: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015), SPICE (Anderson et al., 2016) and NIST (Lin and Hovy, 2003).", "publication_ref": ["b40", "b30", "b6", "b49", "b3", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Constrained Commonsense Generation", "text": "COMMONGEN (Lin et al., 2020) is a commonsense generation task with lexical constraints: given a set of concepts (e.g., {throw, run, javelin, track}), models need to generate a coherent sentence describing a plausible scenario using all given concepts (e.g., \"a man runs on a track and throws a javelin.\").\nApproach and Baselines. Following , we enforce that each concept c i appear in output y under some morphological inflection. We test in both supervised and zero-shot settings. In the supervised setting, we finetune GPT-2 (Radford et al., 2019) as a sequence-to-sequence model. In the zero-shot setting, we use GPT-2 off-the-shelf (no fine-tuning) and rely on constrained decoding to guide generation. We compare with previous constrained decoding algorithms CBS (Anderson et al., 2017), GBS (Hokamp and Liu, 2017), DBA (Post and Vilar, 2018a), NEUROLOGIC  and TSMH .\nMetrics. We report standard automatic metrics as well as coverage, the average percentage of concepts present in generations. Additionally, we conduct human evaluation on 100 test examples using Amazon Mechanical Turk (AMT), with 3 annotators per example (template in Appendix D). Workers rate each generation on language quality, sce-    (Hokamp and Liu, 2017), DBA (Post and Vilar, 2018a), and NEUROLOGIC .\nnario plausibility, coverage of given concepts, and an overall score on a 3-point Likert scale. 2 Results. Table 1 ", "publication_ref": ["b29", "b47", "b4", "b16", "b43", "b16", "b43"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Constrained Machine Translation", "text": "Next, we test on constrained machine translation (MT). It is often critical to have control over MT systems, such as to incorporate domain-specific terminology (Post and Vilar, 2018a;Dinu et al., 2019). To achieve this goal, recent work proposed constrained decoding algorithms Hokamp and Liu, 2017;Hasler et al., 2018;Hu et al., 2019  (2019), we report BLEU and term use rates, i.e., percentage of times given constraint terms were generated out of total number of constraint terms.\nResults.    (Chen et al., 2020b), in multiple few-shot settings with various numbers of training instances. We report standard automatic metrics, as well as information coverage, i.e., percentage of information present in the generation.\nResults. ", "publication_ref": ["b43", "b16", "b15", "b19", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Constrained Question Generation", "text": "Next, we consider constrained question generation , where models need to generate interrogative questions using given keywords. This task is zero-shot without any training data, further testing the capacity of NEUROLOGIC to guide off-the-shelf models without finetuning.\nApproach, Baselines, and Metrics. We use GPT-2 off-the-shelf and compare NEUROLOGIC with previous constrained decoding methods, including CGMH , TSMH  and NEUROLOGIC . We report standard generation metrics and keyword coverage as in \u00a73.1. We conduct human evaluation following subsection 3.1, to measure grammar, fluency, meaningfulness, and overall quality of the generated questions, using a 3-point Likert scale 3 (template in Appendix D).\nResults. Table 7 presents comparisons across different decoding methods based on off-the-shelf language models. NEUROLOGIC outperforms all previous methods with respect to both automatic and manual metrics; it enhances the generation quality while achieving perfect constraint satisfaction. The difference between NEUROLOGIC and NEUROLOGIC is particularly large compared to other tasks. We suspect that the search problem is much harder here, due to the lack of supervision and complex logical constraints involving both keywords and syntax. As a whole, the results demonstrate the effectiveness of NEUROLOGIC in tackling challenging constrained generation problems.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_12"]}, {"heading": "Unconstrained Story Generation", "text": "Finally, we demonstrate NEUROLOGIC can also improve unconstrained generation. We investigate whether A esque decoding with our unconstrained lookahead heuristic (Equation 7) can (1) improve beam search, which typically struggles in openended settings (Holtzman et al., 2020;Welleck et al., 2019b), and (2) improve sampling algorithms that are commonly used in open-ended generation.\nWe consider conditional story generation on the RocStories dataset (Mostafazadeh et al., 2016): given a first sentence x, generate the full story y.\nApproach, Baselines and Metrics. We use GPT-2, fine-tuned on the RocStories training set.\nWe apply A esque decoding to (1) beam search, the setting used so far in the experiments, and (2) top-k sampling (Fan et al., 2018), a commonly used sampling algorithm in open-ended generation. For top-k sampling, we use the heuristic to adjust the probability scores, then renormalize. We use standard automatic metrics: perplexity and BLEU for fluency, and unique n-grams as a measure of diversity. We conduct human evaluation following subsection 3.1, for story flow and overall quality on a 3-point Likert scale 4 (template in Appendix D).\nResults.   lookahead provides an estimate which better resembles a continuation from top-k sampling.", "publication_ref": ["b17", "b51", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "A* search in NLP. Many classical NLP problems (e.g., parsing, text alignment) can be seen as structured prediction subject to a set of taskspecific constraints. For many such problems, A* search has been used effectively (Och et al., 2001;Haghighi et al., 2007;Hopkins and Langmead, 2009;Meister et al., 2020). For example, Klein and Manning (2003); Zhang and Gildea (2006); Auli and Lopez (2011); Lee et al. (2016) have used it in the context of parsing. Similar approaches are used for finding high-probability alignments (Naim et al., 2013). Despite these applications, applying informed heuristic search to text generation with autoregressive language models (this work's focus) has been underexplored.\nDecoding strategies for text generation. The rise of autoregressive language models like GPT (Radford et al., 2018) has inspired work on decoding strategies (Post and Vilar, 2018a;Ippolito et al., 2019;Zheng et al., 2020;Leblond et al., 2021;. These works often focus on incorporating factors like diversity (Ippolito et al., 2019), fluency (Holtzman et al., 2020), or constraints (Anderson et al., 2017;Hokamp and Liu, 2017;Post and Vilar, 2018b;Welleck et al., 2019a;Qin et al., 2020;. Constrained beam search (Anderson et al., 2017) and grid beam search (Hokamp and Liu, 2017) extend beam search to satisfy lexical constraints during generation.  incorporate logic-based constraints into beam search, which we extend with lookahead heuristics.\nOther work addresses the mismatch between monotonic decoding and satisfying constraints that can depend on a full generation, through MCMC sampling , recursive non-monotonic generation (Welleck et al., 2019a), continuous optimization (Qin et al., 2020), or generated contexts . Unlike these past works, NEUROLOGIC A esque explicitly decodes future text to estimate the viability of different paths for satisfying constraints.", "publication_ref": ["b38", "b18", "b34", "b23", "b54", "b5", "b27", "b37", "b46", "b43", "b21", "b56", "b21", "b17", "b4", "b16", "b44", "b50", "b45", "b4", "b16", "b50", "b45"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "Inspired by the A* search algorithm, we introduce NEUROLOGIC A esque decoding, which brings A*-like heuristic estimates of the future to common left-to-right decoding algorithms for neural text generation. A esque lookahead heuristics improve over existing decoding methods (e.g., NEU-ROLOGIC, beam, greedy, sample decoding methods) in both constrained and unconstrained settings across a wide spectrum of tasks. Our work demonstrates the promise of moving beyond the current paradigm of unidirectional decoding for text generation, by taking bidirectional information from both the past and future into account to generate more globally coherent text. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Broader Impact and Ethical Implications", "text": "Our method deals with improving neural text generation, thus inheriting the potential impact and risks brought by text generation applications (e.g. dual use, see Pandya (2019); Brown et al. (2020)). Constraining generation through logical constraints offers the promise of improved control, consistency, and human-machine collaboration in highimpact applications such as translation, machineaided writing, and education. On the other hand, constrained generation methods could foreseeably be used to generate text that contains biased, offensive, and/or hateful keywords (e.g., extremist texts; McGuffie and Newhouse, 2020). For a broader discussion of these risks, and of the risks of large pretrained language models in general, refer to discussions in Brown et al. (2020); Bender et al. (2021).  A Further Experiments", "publication_ref": ["b39", "b9", "b9", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Constrained Commonsense Generation", "text": "Studying Lookahead Strategies. We further use COMMONGEN to study the lookahead strategies for NEUROLOGIC proposed in \u00a72.1 (Figure 3). With infinite lookahead length and number of lookaheads |L |, lookahead decoding exactly solves Equation 3, finding an optimal trajectory. In practice these are finite, meaning that the quality of the lookahead approximation can depend on the lookahead strategy and its hyperparameters. For practical choices of and |L |, we empirically study how varying the lookahead strategy and hyperparameters affects performance. In Figure 3, we study the greedy, soft, beam, and sampling lookahead strategies. Figure 3(a) shows the effect of increasing the lookahead length for the greedy lookahead strategy. Increasing the length improves up to one point -e.g., 5-7 steps -then decreases thereafter, likely due to the difficulty of long-horizon approximation.\nFigure 3(b) studies the temperature in the soft lookahead, showing that greedy (\u03c4 = 0.0) performs well, with slight gains if \u03c4 is carefully selected. The results suggest that one can safely bypass tuning \u03c4 using fast, greedy lookahead.\nNext, Figure 3(c) shows that with beam lookahead, increasing the beam width improves performance up to a certain point (here, 11). Similarly, increasing the number of samples with sampling ", "publication_ref": [], "figure_ref": ["fig_2", "fig_2", "fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "A.2 Unconstrained Story Generation", "text": "Fluency and Diversity Tradeoff We study the effect of A esque decoding in unconstrained generation with different decoding hyperparameters: beam size in beam search and k value in top-k sampling. Figure 4 plots the fluency (measured by likelihood) versus diversity (measured by unique 3-grams) for generations with various beam sizes or top-k values. Ideally, we want generations to be both fluent and diverse (top right). However, we observe a fluency and diversity tradeoff in practice. A esque decoding flattens this trend and results in larger area under the curve. The effect is especially strong with beam search. In summary, A esque decoding yields a more favorable balance of fluency and diversity compared to conventional decoding methods, regardless of hyperparameters.  We download off-the-shelf models, including pretrained GPT-2 and Marian MT, from HuggingFace\nTransformers (Wolf et al., 2020), which are implemented in the PyTorch deep learning framework.", "publication_ref": ["b53"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "C.2 Model Training Details", "text": "All training is performed on a single NVIDIA Quadro RTX 8000 GPU and costs about 100 GPU hours in total. Our method is implemented with PyTorch an the Huggingface Transformers library.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.2.1 COMMONGEN", "text": "For supervised setting, we finetune GPT-2 for conditional generation. We follow 's setup and use their hyperparameters for finetuning, as shown in Table 10. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "C.2.2 Constrained Machine Translation", "text": "For fair comparison, we reproduced MT model (two-layer transformer) used by Dinu et al. (2019), using the same setup and hyperparameters reported in their original paper.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.2.3 Table-to-text Generation", "text": "We finetune GPT-2 with random sampled few-shot training instances from E2ENLG dataset. We used the same hyperparameters for finetuning with Li and Liang (2021), as shown in Table 11. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "C.2.4 Unconstrained Story Generation", "text": "We finetune GPT-2 for conditional story generation on the RocStories dataset: given a first sentence x, generate the full story y. Hyperparameters for finetuning are given in Table 12. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "C.3 Generation Details", "text": "All generation is performed on a single NVIDIA Quadro RTX 8000 GPU and costs about 100 GPU hours in total.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.3.1 COMMONGEN", "text": "NEUROLOGIC hyperparameters for COMMONGEN in supervised and zero-shot setting are shown in Table 13 and Table 14 separately. We use the same NEUROLOGIC hyperparameters with , including beam size, \u03b1, \u03b2 and \u03bb 1 . We performed a hyperparameter grid search for the scaling factor \u03bb 2 over the range [0, 0.3], for the look ahead step over the the range [1,15], for the look ahead temperature over the the range [0, 1.0], for the look ahead beam width over the the range [1,10], and for the look ahead number of sample over the the range [1, 10], using a small subset of COMMONGEN development set.  ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1", "tab_1"]}, {"heading": "C.3.2 Constrained Machine Translation", "text": "NEUROLOGIC hyperparameters for constrained machine translation are shown in Table 15. We use the same beam size with Dinu et al. ( 2019) for fair comparison. We performed a hyperparameter grid search for the pruning threshold \u03b1 over the range [50,300], for the pruning threshold \u03b2 over the range [1,3], for the scaling factor \u03bb 1 over the range [0, 1.0], for the scaling factor \u03bb 2 over the range [0, 0.3], for the look ahead step over the the range [5,40], using a subset of WMT2013 IATE development set. We use the same hyperparameters for look ahead temperature, look ahead beam width, and look ahead number of sample with supervised COMMONGEN and omit the hyperparameter search due to the computational cost. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "C.3.5 Unconstrained Story Generation", "text": "A esque hyperparameters with beam search and top-k sampling for unconstrained story generation are shown in Table 18 and Table 19 separately. We performed a hyperparameter grid search for the scaling factor \u03bb 2 over the range [0, 1.0], for the look ahead step over the the range [1,15], for the look ahead temperature over the the range [0, 1.0], for the look ahead beam width over the the range [1,15], and for the look ahead number of sample over the the range [1, 15], using a small subset of RocStories development set.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1", "tab_1"]}, {"heading": "C.4 Dataset Details", "text": "Details of datasets used for downstream tasks are provided in Table 22.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "D Human Evaluation", "text": "We include screenshots of the human evaluation templates for CommonGen (Figure 5), Constrained scaling factor \u03bb 2 0.6 look ahead step 4 look ahead (greedy) temperature 0 look ahead (beam) beam width 4 look ahead (sample) number of sample 15 Question Generation (Figure 6), and RocStories (Figure 7) tasks. We ensure the annotators are paid adequately for at least $15 per hour and we inform annotators that their annotations are used for model evaluation purpose.", "publication_ref": [], "figure_ref": ["fig_5", "fig_6", "fig_7"], "table_ref": []}, {"heading": "E Qualitative Generation Examples", "text": "Qualitative examples of the constrained question generation and unconstrained story generation are shown in Table 21 and 20. F Limitations and Risks.\nLimitations. For constrained generation, NEU-ROLOGIC A esque decoding can only take the constraints that can be formulated as logical expressions as described in the paper; we leave it to future work to expand the scope of such logical constraints.\nRisks. Constrained generation methods could foreseeably be used to generate text that contains biased, offensive, and/or hateful keywords. (e.g., extremist texts; McGuffie and Newhouse, 2020). For a broader discussion of these risks, and of the risks of large pretrained language models in general, refer to discussions in Brown et al. (2020); Bender et al. (2021).   beam search I went to the store and looked at all the options. I decided to go with the PS4. I went home and waited for the PS4 to arrive. When the PS4 arrived, I was very excited.\nbeam search + A esque I went to the store and looked at all the options. I decided to go with the PS4 because it was the most expensive. When I got home, I plugged the PS4 into my TV. The PS4 turned on and I was happy.\nMy roommate is from Germany.\nbeam search She moved to the United States when she was 16. She has been in the United States for 10 years now. She is a great roommate.\nbeam search + A esque He came to the United States when he was 10 years old. He had a hard time adjusting to American culture. One day, he decided to move back to Germany. He was very happy to be back in his homeland.\nOliver was nervous about his wedding.\nbeam search He had never been to a wedding before. He decided to go to the rehearsal dinner. The rehearsal dinner went very well. Oliver had a great time at his wedding.\nbeam search + A esque He had never been married before and didn't know what to expect. When the day of the wedding came, he was very nervous. When he walked down the aisle, he knew it was going to be great. Oliver and his wife had a wonderful wedding.\nMy friends all love to go to the club to dance.\nbeam search One day I decided to go with them. We went to the club and had a great time. We danced all night and had a great time. We decided to go back next week.\nbeam search + A esque One day, I decided to go with them. We went to the club and danced all night. When we got home, I told my friends about the fun we had. They all agreed that it was the best night of their lives.   , TSMH  and NEUROLOGIC    --300 RocStories (Mostafazadeh et al., 2016) 45,496 1,871 1,871 ", "publication_ref": ["b9", "b7", "b36"], "figure_ref": [], "table_ref": ["tab_1"]}], "references": [{"ref_id": "b0", "title": "", "journal": "", "year": "2019", "authors": " Cgmh (miao"}, {"ref_id": "b1", "title": "", "journal": "", "year": "2020", "authors": " Tsmh (zhang"}, {"ref_id": "b2", "title": "", "journal": "", "year": "2021", "authors": " Neurologic (lu"}, {"ref_id": "b3", "title": "Spice: Semantic propositional image caption evaluation", "journal": "Springer", "year": "2016", "authors": "References Peter Anderson; Basura Fernando; Mark Johnson; Stephen Gould"}, {"ref_id": "b4", "title": "Guided open vocabulary image captioning with constrained beam search", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Peter Anderson; Basura Fernando; Mark Johnson; Stephen Gould"}, {"ref_id": "b5", "title": "Efficient CCG parsing: A* versus adaptive supertagging", "journal": "", "year": "2011", "authors": "Michael Auli; Adam Lopez"}, {"ref_id": "b6", "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments", "journal": "", "year": "2005", "authors": "Satanjeev Banerjee; Alon Lavie"}, {"ref_id": "b7", "title": "On the dangers of stochastic parrots: Can language models be too big?", "journal": "", "year": "2021", "authors": "Emily Bender; Timnit Gebru; Angelina Mcmillan-Major; Shmargaret Shmitchell"}, {"ref_id": "b8", "title": "Findings of the 2017 conference on machine translation (WMT17)", "journal": "", "year": "2017", "authors": "Ond\u0159ej Bojar; Rajen Chatterjee; Christian Federmann; Yvette Graham; Barry Haddow; Shujian Huang; Matthias Huck; Philipp Koehn; Qun Liu; Varvara Logacheva; Christof Monz; Matteo Negri; Matt Post; Raphael Rubino; Lucia Specia; Marco Turchi"}, {"ref_id": "b9", "title": "Language models are few-shot learners", "journal": "", "year": "2020", "authors": "T Brown; B Mann; Nick Ryder; Melanie Subbiah; J Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; G Kr\u00fcger; T Henighan; R Child; Aditya Ramesh; D Ziegler; Jeffrey Wu; Clemens Winter; Christopher Hesse; Mark Chen; E Sigler; Mateusz Litwin; Scott Gray; J Benjamin Chess; Christopher Clark; Sam Berner; A Mccandlish; Ilya Radford; Dario Sutskever;  Amodei"}, {"ref_id": "b10", "title": "Guiding neural machine translation decoding with external knowledge", "journal": "", "year": "2017", "authors": "Rajen Chatterjee; Matteo Negri; Marco Turchi; Marcello Federico; Lucia Specia; Fr\u00e9d\u00e9ric Blain"}, {"ref_id": "b11", "title": "Logical natural language generation from open-domain tables", "journal": "", "year": "2020", "authors": "Wenhu Chen; Jianshu Chen; Yu Su; Zhiyu Chen; William Yang Wang"}, {"ref_id": "b12", "title": "KGPT: Knowledge-grounded pretraining for data-to-text generation", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Wenhu Chen; Yu Su; Xifeng Yan; William Yang Wang"}, {"ref_id": "b13", "title": "Recurrent neural networks as weighted language recognizers", "journal": "Long Papers", "year": "2018", "authors": "Yining Chen; Sorcha Gilroy; Andreas Maletti; Jonathan May; Kevin Knight"}, {"ref_id": "b14", "title": "A formal basis for the heuristic determination of minimum cost paths", "journal": "IEEE Transactions on Systems Science and Cybernetics", "year": "1968", "authors": "Peter E Hart; Nils J Nilsson; Bertram Raphael"}, {"ref_id": "b15", "title": "Neural machine translation decoding with terminology constraints", "journal": "", "year": "2018", "authors": "Eva Hasler; Adri\u00e0 De Gispert; Gonzalo Iglesias; Bill Byrne"}, {"ref_id": "b16", "title": "Lexically constrained decoding for sequence generation using grid beam search", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Chris Hokamp; Qun Liu"}, {"ref_id": "b17", "title": "The curious case of neural text degeneration", "journal": "", "year": "2020", "authors": "Ari Holtzman; Jan Buys; Li Du; Maxwell Forbes; Yejin Choi"}, {"ref_id": "b18", "title": "Cube pruning as heuristic search", "journal": "", "year": "2009", "authors": "Mark Hopkins; Greg Langmead"}, {"ref_id": "b19", "title": "Improved lexically constrained decoding for translation and monolingual rewriting", "journal": "Long and Short Papers", "year": "2019", "authors": "J Edward Hu; Huda Khayrallah; Ryan Culkin; Patrick Xia; Tongfei Chen; Matt Post; Benjamin Van Durme"}, {"ref_id": "b20", "title": "Toward controlled generation of text", "journal": "PMLR", "year": "2017", "authors": "Zhiting Hu; Zichao Yang; Xiaodan Liang; Ruslan Salakhutdinov; Eric P Xing"}, {"ref_id": "b21", "title": "Comparison of diverse decoding methods from conditional language models", "journal": "", "year": "2019", "authors": "Daphne Ippolito; Reno Kriz; Jo\u00e3o Sedoc; Maria Kustikova; Chris Callison-Burch"}, {"ref_id": "b22", "title": "Marian: Fast neural machine translation in C++", "journal": "", "year": "2018", "authors": "Marcin Junczys-Dowmunt; Roman Grundkiewicz; Tomasz Dwojak; Hieu Hoang; Kenneth Heafield; Tom Neckermann; Frank Seide; Ulrich Germann; Alham Fikri Aji; Nikolay Bogoychev; F T Andr\u00e9"}, {"ref_id": "b23", "title": "A* parsing: Fast exact Viterbi parse selection", "journal": "", "year": "2003", "authors": "Dan Klein; Christopher D Manning"}, {"ref_id": "b24", "title": "Depth-first iterative-deepening: An optimal admissible tree search. Artificial intelligence", "journal": "", "year": "1985", "authors": "E Richard;  Korf"}, {"ref_id": "b25", "title": "Computing krippendorff's alpha reliability. Departmental papers (ASC)", "journal": "", "year": "2007", "authors": "Klaus Krippendorff"}, {"ref_id": "b26", "title": "Ioannis Antonoglou, Karen Simonyan, and Oriol Vinyals. 2021. Machine translation decoding beyond beam search", "journal": "", "year": "", "authors": "R\u00e9mi Leblond; Jean-Baptiste Alayrac; Laurent Sifre; Miruna Pislar; Jean-Baptiste Lespiau"}, {"ref_id": "b27", "title": "Global neural CCG parsing with optimality guarantees", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Kenton Lee; Mike Lewis; Luke Zettlemoyer"}, {"ref_id": "b28", "title": "Prefix-tuning: Optimizing continuous prompts for generation", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Lisa Xiang; Percy Li;  Liang"}, {"ref_id": "b29", "title": "Commongen: A constrained text generation challenge for generative commonsense reasoning", "journal": "", "year": "2020", "authors": "Ming Bill Yuchen Lin; Wangchunshu Shen; Pei Zhou; Chandra Zhou; Yejin Bhagavatula; Xiang Choi;  Ren"}, {"ref_id": "b30", "title": "Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out", "journal": "", "year": "2004", "authors": "Chin-Yew Lin"}, {"ref_id": "b31", "title": "Automatic evaluation of summaries using n-gram cooccurrence statistics", "journal": "", "year": "2003", "authors": "Chin-Yew Lin; Eduard Hovy"}, {"ref_id": "b32", "title": "Neuro-Logic decoding: (un)supervised neural text generation with predicate logic constraints", "journal": "", "year": "2021", "authors": "Ximing Lu; Peter West; Rowan Zellers; Le Ronan; Chandra Bras; Yejin Bhagavatula;  Choi"}, {"ref_id": "b33", "title": "The radicalization risks of gpt-3 and advanced neural language models", "journal": "", "year": "2020", "authors": "Kris Mcguffie; Alex Newhouse"}, {"ref_id": "b34", "title": "Best-first beam search", "journal": "", "year": "2020", "authors": "Clara Meister; Tim Vieira; Ryan Cotterell"}, {"ref_id": "b35", "title": "Cgmh: Constrained sentence generation by metropolis-hastings sampling", "journal": "", "year": "2019", "authors": "Ning Miao; Hao Zhou; Lili Mou; Rui Yan; Lei Li"}, {"ref_id": "b36", "title": "A corpus and cloze evaluation for deeper understanding of commonsense stories", "journal": "", "year": "2016", "authors": "Nasrin Mostafazadeh; Nathanael Chambers; Xiaodong He; Devi Parikh; Dhruv Batra; Lucy Vanderwende; Pushmeet Kohli; James Allen"}, {"ref_id": "b37", "title": "Text alignment for real-time crowd captioning", "journal": "", "year": "2013", "authors": "Iftekhar Naim; Daniel Gildea; Walter Lasecki; Jeffrey P Bigham"}, {"ref_id": "b38", "title": "An efficient a* search algorithm for statistical machine translation", "journal": "", "year": "2001", "authors": "Franz Josef Och; Nicola Ueffing; Hermann Ney"}, {"ref_id": "b39", "title": "The dual-use dilemma of artificial intelligence", "journal": "", "year": "2019", "authors": "Jayshree Pandya"}, {"ref_id": "b40", "title": "BLEU: a method for automatic evaluation of machine translation", "journal": "", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"ref_id": "b41", "title": "Heuristics -intelligent search strategies for computer problem solving", "journal": "", "year": "1984", "authors": "Judea Pearl"}, {"ref_id": "b42", "title": "First Results on the Effect of Error in Heuristic Search", "journal": "", "year": "1970", "authors": "Ira Pohl"}, {"ref_id": "b43", "title": "Fast lexically constrained decoding with dynamic beam allocation for neural machine translation", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Matt Post; David Vilar"}, {"ref_id": "b44", "title": "Fast lexically constrained decoding with dynamic beam allocation for neural machine translation", "journal": "", "year": "2018", "authors": "Matt Post; David Vilar"}, {"ref_id": "b45", "title": "Backpropagationbased decoding for unsupervised counterfactual and abductive reasoning", "journal": "", "year": "2020", "authors": "Lianhui Qin; Vered Shwartz; Peter West; Chandra Bhagavatula; Jena D Hwang; Ronan Le Bras; Antoine Bosselut; Yejin Choi"}, {"ref_id": "b46", "title": "Improving language understanding by generative pre-training", "journal": "", "year": "2018", "authors": "Alec Radford; Karthik Narasimhan"}, {"ref_id": "b47", "title": "Language models are unsupervised multitask learners", "journal": "OpenAI Blog", "year": "2019", "authors": "Alec Radford; Jeffrey Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"ref_id": "b48", "title": "Pragmatically informative text generation", "journal": "Long and Short Papers", "year": "2019", "authors": "Sheng Shen; Daniel Fried; Jacob Andreas; Dan Klein"}, {"ref_id": "b49", "title": "Cider: Consensus-based image description evaluation", "journal": "", "year": "2015", "authors": "Ramakrishna Vedantam; Lawrence Zitnick; Devi Parikh"}, {"ref_id": "b50", "title": "Non-monotonic sequential text generation", "journal": "PMLR", "year": "2019", "authors": "Sean Welleck; Kiant\u00e9 Brantley; Hal Daum\u00e9 Iii; Kyunghyun Cho"}, {"ref_id": "b51", "title": "Neural text generation with unlikelihood training", "journal": "", "year": "2019", "authors": "Sean Welleck; Ilia Kulikov; Stephen Roller; Emily Dinan; Kyunghyun Cho; Jason Weston"}, {"ref_id": "b52", "title": "Reflective decoding: Beyond unidirectional generation with off-the-shelf language models", "journal": "", "year": "2021", "authors": "Peter West; Ximing Lu; Ari Holtzman; Chandra Bhagavatula; Jena D Hwang; Yejin Choi"}, {"ref_id": "b53", "title": "Transformers: State-of-the-art natural language processing", "journal": "", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; Remi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander Lhoest;  Rush"}, {"ref_id": "b54", "title": "Efficient search for inversion transduction grammar", "journal": "", "year": "2006", "authors": "Hao Zhang; Daniel Gildea"}, {"ref_id": "b55", "title": "Language generation via combinatorial constraint satisfaction: A tree search enhanced Monte-Carlo approach", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Maosen Zhang; Nan Jiang; Lei Li; Yexiang Xue"}, {"ref_id": "b56", "title": "Opportunistic decoding with timely correction for simultaneous translation", "journal": "", "year": "2020", "authors": "Renjie Zheng; Mingbo Ma; Baigong Zheng; Kaibo Liu; Liang Huang"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "determine which token to generate next based on what happened in the past, without explicitly looking ahead into the future. While 1 Pronounced [ey stAr Esk]. summer on the road winter through the snow \u2713 \u2717 I drive my car during the Write a sentence with these concepts car drive snow p (w | p a st ) = 0 .4 A \u2605 p( w | pa st ) = 0. 2", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "AcknowledgmentThis work was supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC) (funding reference number 401233309), DARPA MCS program through NIWC Pacific (N66001-19-2-4031), Google Cloud Compute, and Allen Institute for AI, Microsoft PhD Fellowship.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Effect of varying the primary hyperparameter for each lookahead strategy ( \u00a72.1) -(a) greedy (lookahead length), (b) soft (temperature), (c) beam (number of beams), and (d) sample (number of samples). Performance is measured on the COMMONGEN validation set, using BLEU-4 and Coverage.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Likelihood (y-axis) vs. number of unique 3grams (x-axis) using supervised GPT-2 on RocStories. Figure (a) denotes decoding with beam search, with a varying amount of beam size. Figure (b) denotes decoding with top-k sampling, with a varying amount of k value. The brown and blue lines denote with and without A esque heuristics separately.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "beam) beam width 5 look ahead (sample) number of sample 4", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure 5: Human evaluation template for the Constrained Commonsense Generation task.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 :6Figure 6: Human evaluation template for the Interrogative Sentence Generation task.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 7 :7Figure 7: Human evaluation template for the RocStories task.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Performance of various decoding methods with supervised or off-the-shelf GPT-2 on the COMMONGEN test set, measured with automatic and human evaluations. We only tried NEUROLOGIC (greedy) in the unsupervised setting because of the computational cost. The best numbers are bolded and the second best ones are underlined. NEUROLOGIC A dog is scrubbing his paws with soap and water. water NEUROLOGIC A man is scrubbing a dog with soap and water.", "figure_data": "WordsMethodGenerationcut piece DBA GBSCut a piece of wood to use as a fence. Cut a piece of wood to use as a fence."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Example generations for the COMMONGEN task across supervised NEUROLOGIC and baselines, including GBS", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Agreement by ordinal Krippendorff alpha (0 \u2264 \u03b1 \u2264 1)(Krippendorff, 2007) is 0.40, 0.46, 0.36, and 0.44 (respectively) indicating fair to moderate agreement.", "figure_data": "Method UnconstrainedDinu et al. BLEU Term% 25.8 76.3Marian MT BLEU Term% 32.9 85.0train-by-app.26.092.9--train-by-rep.26.094.5--Post and Vilar (2018a)25.382.033.094.3NEUROLOGIC26.595.133.497.1NEUROLOGIC (greedy) NEUROLOGIC (sample) NEUROLOGIC (beam)26.7 26.6 26.695.8 95.4 95.833.7 33.7 33.697.2 97.2 97.2compares different constraineddecoding methods on top of the finetuned and off-the-shelf GPT-2, in supervised and zero-shot set-tings respectively. The key observations are:1. NEUROLOGICoutperforms all previousconstrained-decoding methods in both super-vised and zero-shot settings. Surprisingly, un-supervised NEUROLOGIC outperforms all su-pervised methods based on human evaluation.2. ComparedtovanillaNEUROLOGIC,NEUROLOGIC improves generation qualitywhile maintaining high constraint satisfaction.The difference is especially substantial in thezero-shot setting. Intuitively, this setting leaves"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Results on constrained MT. The left section uses the same two-layer transformer asDinu et al.  (2019), while the right one uses a stronger Marian MT EN-DE model. The highlighted methods modify training data specifically for constrained generation, and thus cannot be applied to off-the-shelf models. The best numbers are bold, second best are underlined.", "figure_data": "more room for incorporating constraint-drivensignals due to the lack of supervision.3. NEUROLOGIC reaches similar performanceusing different lookahead strategies, amongwhich beam lookahead slightly outperforms theothers based on human evaluation, and greedylookahead has the lowest runtime. We analyzelookahead strategies further in Appendix A."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Constrained MT performance broken down by the number of constraint terms (# T). All configurations use the two-layer tranformer fromDinu et al. (2019). The best numbers are bolded and the second best ones are underlined.", "figure_data": "NEUROLOGIC can be readily applied to off-the-shelf MT systems for constrained machine trans-lation. We follow Dinu et al. (2019) and evaluateon the WMT17 EN-DE test set (Bojar et al., 2017).The constraint here is to integrate given customterminologies into the translation output; constraintterms are automatically created from the IATE EUterminology database for 414 test sentences.Approach, Baselines, and Metrics. We exper-iment with two MT systems: Dinu et al. (two-layer transformer) and the off-the-shelf Marian MT(Junczys-Dowmunt et al., 2018). We compare withprevious constrained decoding algorithms, includ-ing DBA (Post and Vilar, 2018a), NEUROLOGIC(Lu et al., 2021), and also specialized training pro-posed by Dinu et al. (2019). Following Dinu et al."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Table 3 presents experimental results with Dinu et al.'s model and Marian MT. In both cases, NEUROLOGIC outperforms prior methods in BLEU and term coverage. Besides higher quality and coverage, NEUROLOGIC is plug-and-play, working with any off-the-shelf MT system, unlike previous training-based methods. Table4breaks down the performance by the number of constraint terms. We see that the improvement brought by NEUROLOGIC is especially large when given complex constraints with multiple terms. (e.g., 96.5 vs. 93.7 from NEUROLOGIC in term of coverage).", "figure_data": "Decode Method Beam SearchNIST BLEU METEOR CIDEr ROUGE Coverage 3.82 42.8 32.6 10.8 57.8 73.6CBS6.50 42.336.413.054.391.6GBS6.26 40.736.712.954.294.1NEUROLOGIC6.95 47.638.916.358.797.6NEUROLOGIC (greedy) 7.11 49.2 NEUROLOGIC (beam) 7.01 48.940.0 40.017.5 17.260.0 59.8100.0 99.9NEUROLOGIC (sample) 7.11 49.340.117.560.0100.0"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Performance of different decoding methods with few-shot GPT-2 finetuned on 0.1% E2ENLG data. The best numbers are bold, second best are underlined.", "figure_data": "Method TGen (Du\u0161ek and Jur\u010d\u00ed\u010dek, 2016)0.1% 0.5% 1% 5% 3.6 27.9 35.2 57.3Template-GPT-2 (Chen et al., 2020a) 22.547.8 53.3 59.9KGPT-Graph (Chen et al., 2020b)39.853.3 55.1 61.5KGPT-Seq (Chen et al., 2020b)40.253.0 54.1 61.1GPT-242.857.1 56.8 61.1GPT-2 + NEUROLOGIC47.656.9 58.0 62.9GPT-2 + NEUROLOGIC (greedy)49.258.0 58.4 63.4"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "", "figure_data": "compares various decodingmethods with few-shot GPT-2 finetuned on 0.1%of the data. NEUROLOGIC substantially outper-forms previous methods on all metrics, consistentlyimproving quality while achieving near-perfect con-straint satisfaction. Previous work (CBS and GBS)improves constraint satisfaction, but negatively af-fects quality, indicated by drops in BLEU and"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "", "figure_data": "presents the results of beamsearch and top-k sampling with and withoutA esque heuristics. A esque heuristics result inmore fluent, coherent and interesting stories forboth beam search and top-k sampling. For beamsearch, A esque not only enhances generationquality-e.g. improving human evaluation scoresfrom 2.32 to 2.63-but also boosts generation diver-sity, reflected by number of unique n-grams. Fortop-k sampling, A esque heuristics improve qual-ity, while maintaining comparable diversity. Wefurther analyze quality and diversity tradeoff inAppendix A. Moreover, we notice that beam looka-head works the best for beam search, and greedylookahead works the best for top-k sampling. Wesuspect that beam lookahead gives the most accu-rate estimate of future beam path, while greedy"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Performance of different unsupervised decoding algorithms on constrained question generation.", "figure_data": "Decode MethodFluency PPL BLEU-1 BLEU-2 Uniq. 2-gram Uniq. 3-gram Uniq. 4-gram DiversityHuman Eval Coherence Overallbeam search2.2433.716.520.13k34.09k41.91k2.462.32beam search + A esque (greedy)2.1134.316.720.63k34.94k43.02k2.562.57beam search + A esque (beam)2.1434.416.820.68k35.03k43.12k2.622.63beam search + A esque (sample)2.1634.416.720.78k35.41k43.64k2.592.57top-k sample4.0131.413.928.54k48.36k56.62k2.232.15top-k sample + A esque (greedy)3.6832.114.328.47k48.44k56.63k2.482.47top-k sample + A esque (beam)3.7532.214.428.53k48.27k56.36k2.392.34top-k sample + A esque (sample)3.7032.014.228.57k48.04k56.15k2.472.44"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2019. Plug and play language models: A simple approach to controlled text generation.In International Conference on Learning Representations.", "figure_data": "Georgiana Dinu, Prashant Mathur, Marcello Federico, and Yaser Al-Onaizan. 2019. Training neural ma-chine translation to apply terminology constraints. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3063-3068, Florence, Italy. Association for Compu-tational Linguistics.Ond\u0159ej Du\u0161ek and Filip Jur\u010d\u00ed\u010dek. 2016. Sequence-to-sequence generation for spoken dialogue via deep syntax trees and strings. In Proceedings of the 54th Annual Meeting of the Association for Compu-tational Linguistics (Volume 2: Short Papers), pages 45-51, Berlin, Germany. Association for Computa-tional Linguistics.Ond\u0159ej Du\u0161ek, Jekaterina Novikova, and Verena Rieser. 2018. Findings of the E2E NLG Challenge. In Proc. of the 11th International Conference on Nat-ural Language Generation, pages 322-328, Tilburg, The Netherlands. Association for Computational Linguistics.Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-erarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889-898, Melbourne, Australia. Association for Computational Linguistics.Aria Haghighi, John DeNero, and Dan Klein. 2007. Approximate factoring for A* search. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 412-419, Rochester, New York. Association for Computational Linguistics."}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "", "figure_data": ": Runtime (seconds per sentence) of different decoding algorithms with finetuned GPT2-L on the COMMONGEN datasetC Experimental DetailsC.1 Off-the-Shelf Models"}, {"figure_label": "13", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "", "figure_data": ": NEUROLOGIC COMMONGEN in supervised setting. hyperparameters for"}, {"figure_label": "18", "figure_type": "table", "figure_id": "tab_22", "figure_caption": "A esque hyperparameters with beam search for unconstrained story generation.", "figure_data": "k valueHyperparameterAssignment 5scaling factor \u03bb 20.5look ahead step3look ahead (greedy) temperature0look ahead (beam) beam width4look ahead (sample) number of sample15Table 19: A esque hyperparameters with top-k sam-pling for unconstrained story generation."}, {"figure_label": "20", "figure_type": "table", "figure_id": "tab_23", "figure_caption": "Example generations for unconstrained story generation using beam search with and without A esque.", "figure_data": "WordsMethodGenerationwaste water heatCGMH TSMH NEUROLOGIC How much water is waste heat? what waste is there, it seems now? where was the waste -water heater? Why do we waste so much water to heat NEUROLOGIC our homes?CGMHwhen would she finally turn twenty -one?Naples plague killedwhy was the plague epidemic in naples not in fact killed? NEUROLOGIC Who was killed in the plague in Naples? TSMH How many people are killed by the plague NEUROLOGIC in Naples?CGMHwhat war was ever fought after american imperialism collapsed?controversial aspect imperialismwhat are some controversial aspects of present -day american imperialism? NEUROLOGIC Whose imperialism is it, anyway? TSMH What is the most controversial aspect of NEUROLOGIC imperialism?CGMHor were they the very first steam engines efficient enough for mass -production?engines efficient steamTSMH NEUROLOGICwhy are steam engines so energy-efficient, just like fossil fuels? Why do you think steam engines are so efficient?NEUROLOGIC Why are steam engines so efficient?"}, {"figure_label": "21", "figure_type": "table", "figure_id": "tab_24", "figure_caption": "Example generations for constrained question generation with NEUROLOGIC and baselines, including CGMH", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_25", "figure_caption": ").", "figure_data": "Dataset COMMONGEN (Lin et al., 2020)train dev. test 32,651 993 1,497WMT2013/2017 IATE (Dinu et al., 2019)-581414E2ENLG (Du\u0161ek et al., 2018)4,862 547630Interrogative question"}, {"figure_label": "22", "figure_type": "table", "figure_id": "tab_26", "figure_caption": "Details of datasets in downstream tasks.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "y * = arg max y\u2208Y F (y), (1", "formula_coordinates": [2.0, 369.27, 281.65, 150.91, 21.46]}, {"formula_id": "formula_1", "formula_text": ")", "formula_coordinates": [2.0, 520.18, 281.65, 4.24, 13.15]}, {"formula_id": "formula_2", "formula_text": "Y t = {y <t \u2022 y t | y <t \u2208 Y t\u22121 , y t \u2208 V}, Y t = arg topk (y<t,yt)\u2208Y t {f (y <t , y t )} ,(2)", "formula_coordinates": [2.0, 329.99, 488.16, 194.43, 38.86]}, {"formula_id": "formula_3", "formula_text": "Y t = arg topk (y<t,yt)\u2208Y t max y>t F (y <t , y t , y >t ) , (3", "formula_coordinates": [2.0, 315.26, 680.92, 204.92, 23.95]}, {"formula_id": "formula_4", "formula_text": ")", "formula_coordinates": [2.0, 520.18, 680.92, 4.24, 13.15]}, {"formula_id": "formula_5", "formula_text": "f (a) = s(a) + h(a),", "formula_coordinates": [3.0, 134.2, 118.26, 91.58, 10.91]}, {"formula_id": "formula_6", "formula_text": "Y t = arg topk y \u2264t \u2208Y t s(y \u2264t ) + max y>t h(y <t , y t , y >t ) ,(4)", "formula_coordinates": [3.0, 70.86, 186.42, 221.59, 35.49]}, {"formula_id": "formula_7", "formula_text": "Y t = arg topk y \u2264t \u2208Y t s(y \u2264t ) + max L (y \u2264t ) h(y \u2264t+ ) ,(5)", "formula_coordinates": [3.0, 76.88, 278.15, 212.25, 35.48]}, {"formula_id": "formula_8", "formula_text": "e t = E yt\u223cp(yt|y<t) [E(y t )],(6)", "formula_coordinates": [3.0, 122.38, 674.08, 166.76, 14.66]}, {"formula_id": "formula_9", "formula_text": "arg max y\u2208Y log p \u03b8 (y | x).", "formula_coordinates": [3.0, 365.59, 324.23, 99.38, 19.83]}, {"formula_id": "formula_10", "formula_text": "D 1 \u2228 D 2 \u2022 \u2022 \u2022 \u2228 D i C 1 \u2227 \u2022 \u2022 \u2022 \u2227 D i \u2228 \u2022 \u2022 \u2022 \u2228 D N C M", "formula_coordinates": [3.0, 318.45, 675.15, 192.32, 29.66]}, {"formula_id": "formula_11", "formula_text": "arg max y\u2208Y p \u03b8 (y | x) \u2212 \u03bb M j=1 (1 \u2212 C j ),", "formula_coordinates": [4.0, 99.75, 116.81, 160.49, 34.69]}, {"formula_id": "formula_12", "formula_text": "f (y \u2264t ) = log p \u03b8 (y \u2264t | x) + \u03bb 1 max D(a,y \u2264t ) |\u00e2| |a| ,(8)", "formula_coordinates": [4.0, 77.25, 213.24, 211.89, 33.79]}, {"formula_id": "formula_13", "formula_text": "h future (y \u2264t+ ) = \u03bb 2 max D(a,y \u2264t ) log p \u03b8 (D(a, y t+1:t+ ) | x, y \u2264t ), (9)", "formula_coordinates": [4.0, 76.44, 551.7, 212.69, 39.64]}, {"formula_id": "formula_14", "formula_text": "p \u03b8 (D(a, y t+1:t+ ) | x, y \u2264t ) = max t \u2208[t,t+ ] p \u03b8 (y t :t +|a| = a | x, y <t ), (10)", "formula_coordinates": [4.0, 80.82, 625.97, 208.31, 40.4]}], "doi": "10.18653/v1/D17-1098"}