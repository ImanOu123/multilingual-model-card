{"title": "Emoji-Powered Representation Learning for Cross-Lingual Sentiment Classification", "authors": "Zhenpeng Chen; Sheng Shen; Ziniu Hu; Xuan Lu; Qiaozhu Mei; Xuanzhe Liu", "pub_date": "2019-03-25", "abstract": "Sentiment classification typically relies on a large amount of labeled data. In practice, the availability of labels is highly imbalanced among different languages, e.g., more English texts are labeled than texts in any other languages, which creates a considerable inequality in the quality of related information services received by users speaking different languages. To tackle this problem, crosslingual sentiment classification approaches aim to transfer knowledge learned from one language that has abundant labeled examples (i.e., the source language, usually English) to another language with fewer labels (i.e., the target language). The source and the target languages are usually bridged through off-the-shelf machine translation tools. Through such a channel, cross-language sentiment patterns can be successfully learned from English and transferred into the target languages. This approach, however, often fails to capture sentiment knowledge specific to the target language, and thus compromises the accuracy of the downstream classification task. In this paper, we employ emojis, which are widely available in many languages, as a new channel to learn both the cross-language and the language-specific sentiment patterns. We propose a novel representation learning method that uses emoji prediction as an instrument to learn respective sentiment-aware representations for each language. The learned representations are then integrated to facilitate cross-lingual sentiment classification. The proposed method demonstrates state-of-the-art performance on benchmark datasets, which is sustained even when sentiment labels are scarce.", "sections": [{"heading": "INTRODUCTION", "text": "Sentiment analysis has become a critical topic in various research communities, including natural language processing (NLP) [22,26], Web mining [23,38], information retrieval [14,57], ubiquitous computing [30,53], and human-computer interaction [24,59]. Due to its effectiveness in understanding user attitudes, emotions, and even latent psychological statuses from text, sentiment analysis has been widely applied to all kinds of Web content such as blogs, Tweets, user reviews, and forum discussions, and it has been a critical component in many applications such as customer review tracking [27], sales prediction [39], product ranking [41], stock market prediction [54], opinion polling [45], recommender systems [55], personalized content delivery [29], and online advertising [52].\nSimilar to many other text mining tasks, existing work on sentiment analysis mainly deals with English texts [22,26,38,57]. Although some efforts have also been made with other languages such as Japanese [51], sentiment analysis for non-English languages is far behind. This creates a considerable inequality in the quality of the aforementioned Web services received by non-English users, especially considering that 74.6% of Internet users are non-English speakers as of 2018 [9]. The cause of this inequality is quite simple: effective sentiment analysis tools are often built upon supervised learning techniques, and there are way more labeled examples in English than in other languages.\nA straightforward solution is to transfer the knowledge learned from a label-rich language (i.e., the source language, usually English) to another language that has fewer labels (i.e., the target language), an approach known as cross-lingual sentiment classification [19]. In practice, the biggest challenge of cross-lingual sentiment classification is how to fill the linguistic gap between English and the target language. Many choose to bridge the gap through standard NLP techniques, and in particular, most recent studies have been using off-the-shelf machine translation tools to generate pseudo parallel corpora and then learn bilingual representations for the downstream sentiment classification task [50,58,62]. More specifically, many of these methods enforce the aligned bilingual texts to share a unified embedding space, and sentiment analysis of the target language is conducted in that space.\nAlthough this approach looks sensible and easily executable, the performance of these machine translation-based methods often falls short. Indeed, a major obstacle of cross-lingual sentiment analysis is the so-called language discrepancy problem [19], which machine translation does not tackle well. More specifically, sentiment expressions often differ a lot across languages. Machine translation is able to retain the general expressions of sentiments that are shared across languages (e.g., \"angry\" or \"\u6012\u3063\u3066\u3044\u308b\" for negative sentiment), but it usually loses or even alters the sentiments in language-specific expressions [44]. As an example, in Japanese, the common expression \"\u6e6f\u6c34\u306e\u3088\u3046\u306b\u4f7f\u3046\" indicates a negative sentiment, describing the excessive usage or waste of a resource. However, its translation in English, \"use it like hot water, \" not only loses the negative sentiment but also sounds odd.\nThe reason behind this pitfall is easy to explain: machine translation tools are usually trained on parallel corpora that are built in the first place to capture patterns shared across languages instead of patterns specific to individual languages. In other words, the problem is due to the failure to retain language-specific sentiment knowledge when unilaterally pursuing generalization across languages. A new bridge needs to be built beyond machine translation, which not only transfers \"general sentiment knowedge\" from the source language but also captures \"private sentiment knowledge\" of the target language. That bridge can be built with emojis.\nIn this paper, we tackle the problem of cross-lingual sentiment analysis by employing emojis as an instrument. Emojis are considered an emerging ubiquitous language used worldwide [16,40]; in our approach they serve both as a proxy of sentiment labels and as a bridge between languages. Their functionality of expressing emotions [21,34] motivates us to employ emojis as complementary labels for sentiments, while their ubiquity [16,40] makes it feasible to learn emoji-sentiment representations for almost every active language. Coupled with machine translation, the cross-language patterns of emoji usage can complement the pseudo parallel corpora and narrow the language gap, and the language-specific patterns of emoji usage help address the language discrepancy problem.\nWe propose ELSA, a novel framework of Emoji-powered representation learning for cross-Lingual Sentiment Analysis. Through ELSA, language-specific representations are first derived based on modeling how emojis are used alongside words in each language. These per-language representations are then integrated and refined to predict the rich sentiment labels in the source language, through the help of machine translation. Different from the mandatorily aligned bilingual representations in existing studies, the joint representation learned through ELSA catches not only the general sentiment patterns across languages, but also the language-specific patterns. In this way, the new representation and the downstream tasks are no longer dominated by the source language.\nWe evaluate the performance of ELSA on a benchmark Amazon review dataset that has been used in various cross-lingual sentiment classification studies [50,58,62]. The benchmark dataset covers nine tasks combined from three target languages (i.e., Japanese, French, and German) and three domains (i.e., book, DVD, and music). Results indicate that ELSA outperforms existing approaches on all of these tasks in terms of classification accuracy. Experiments also show that the emoji-powered model works well even when the volume of unlabeled and labeled data are rather limited. To evaluate the generalizability of ELSA, we also apply the method to Tweets, which again demonstrates state-of-the-art performance. In summary, the major contributions of this paper are as follows:\n\u2022 To the best of our knowledge, this is the first study that leverages emojis as an instrument in cross-lingual sentiment classification. We demonstrate that emojis provide not only surrogate sentiment labels but also an effective way to address language discrepancy.\n\u2022 We propose a novel representation learning method to incorporating language-specific knowledge into cross-lingual sentiment classification, which uses an attention-based Long Short-Term Memory (LSTM) model to capture sentiments from emoji usage. \u2022 We demonstrate the effectiveness and efficiency of ELSA for cross-lingual sentiment classification using multiple large-scale datasets. ELSA significantly improves the state-of-the-art results on the benchmark datasets. 1 \u2022 The use of emojis as a bridge provides actionable insights into other Web mining applications that suffer from similar problem of inequality among languages.\nThe rest of this paper is organized as follows. Section 2 presents the related work. Section 3 formulates the problem and presents the proposed approach (ELSA) to cross-lingual representation learning. Section 4 evaluates ELSA and analyzes the effectiveness of emojis in the learning process. Section 5 discusses the scalability and generalizability of ELSA, followed by concluding remarks in Section 6.", "publication_ref": ["b17", "b21", "b18", "b33", "b9", "b52", "b25", "b48", "b19", "b54", "b22", "b34", "b36", "b49", "b40", "b50", "b24", "b47", "b17", "b21", "b33", "b52", "b46", "b4", "b14", "b45", "b53", "b57", "b14", "b39", "b11", "b35", "b16", "b29", "b11", "b35", "b45", "b53", "b57"], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORK", "text": "We start with a summary of existing literature related to our study.\nEmojis. Emojis, also known as ideograms or smileys, can be used as compact expressions of objects, topics, and emotions. Being encoded in Unicode, they have no language barriers and are diffused on the Internet rapidly [40]. The prevalence of emojis has attracted researchers from various research communities such as NLP, ubiquitous computing, human-computer interaction, multimedia, and Web mining [12,16,20,21,34,40,43]. Many efforts have been devoted to studying their usage across platforms [43], across genders [20], across languages [16], and across cultures [40]. The various non-verbal functions of emojis play an important role in their wide adoption. Emojis are used to replace content words, express situational and additional emotions, adjust tones, express intimacy, etc. [21,34]. In particular, expressing sentiment is demonstrated to be the most popular intention for using emojis [34], so that emojis can be used as effective proxies for sentiment polarities [26]. Considering the ubiquitous usage of emojis across languages and their functionality of expressing sentiments, we make the first effort to use emojis as an instrument to improve cross-lingual sentiment analysis.\nTextual Sentiment Analysis. Sentiment analysis is a classical NLP task aiming to study the emotions, opinions, evaluations, appraisals, and attitudes of people from text data [37]. Many widely used tools, such as SentiStrength [56] and LIWC [49], simply aggregate the polarity of individual words to determine the overall sentiment score of a text. Better performance of sentiment classification is often obtained through supervised machine learning [46]. Recently, with the emergence of deep learning techniques, many researchers have attempted to use advanced neural network models for sentiment analysis [60]. Supervised machine learning methods, including deep learning models, usually require a large volume of labeled data for training. In reality, however, high-quality sentiment labels are often scarce due to the labor-consuming and  error-prone human annotation process [26]. To address this limitation, researchers have used sentimental hashtags and emoticons as weak sentiment labels [22,23]. These weak labels are usually language/community-specific. In addition, figuring out the sentiment polarities of certain hashtags or emoticons can be hard. In recent years, emoticons have been gradually replaced by increasingly popular emojis [48], and emojis have started to be explored as proxies of sentiment labels [26]. We follow the same intuition and utilize emojis as surrogate labels to learn per-language representations. Instead of attempting to directly map emojis to sentiment polarities, however, we integrate these language-specific representations and feed them through downstream tasks to predict real, high quality sentiment labels (in the source language).\nCross-Lingual Text Classification. There is a significant imbalance in the availability of labeled corpora among different languages: more in English, and much fewer in other languages. Crosslingual learning is a common approach to tackling this problem in various text mining tasks such as Web page classification [36], topic categorization [61], and sentiment analysis [50,58,62]. Many researchers divide cross-lingual learning process into two stages: first encoding texts in the source and the target languages into continuous representations, and then utilizing these representations for the final classification task in the target language [50,58,62].\nTo bridge the linguistic gap between the source and the target languages, most studies introduce a translation oracle to project different languages' representations into a unified space at different (e.g., word or document) levels [18,50,58,62]. The performance of these methods thus heavily depends on the quality of the machine translation tools and the pseudo parallel corpora they generate. Unfortunately, different from topical words, emotional language patterns like sentiment (or sarcasm, humor), which present strong language-specific characteristics, cannot be easily transferred in this way. We utilize the easily accessible emoji-texts to incorporate both cross-language and language-specific knowledge into the representations of the source and the target languages. The implicit sentiment knowledge encoded in the usage of diverse emojis solves both the label imbalance and the language discrepancy problems.", "publication_ref": ["b35", "b7", "b11", "b15", "b16", "b29", "b35", "b38", "b38", "b15", "b11", "b35", "b16", "b29", "b29", "b21", "b32", "b51", "b44", "b41", "b55", "b21", "b17", "b18", "b43", "b21", "b31", "b56", "b45", "b53", "b57", "b45", "b53", "b57", "b13", "b45", "b53", "b57"], "figure_ref": [], "table_ref": []}, {"heading": "THE ELSA APPROACH", "text": "To better illustrate the workflow of ELSA, we first give a formulation of our problem. Cross-lingual sentiment classification aims to use the labeled data in a source language (i.e., English) to learn a model that can classify the sentiment of test data in a target language. In our setting, besides labeled English documents (L S ), we also have large-scale unlabeled data in English (U S ) and in the target language (U T ). Furthermore, there exist unlabeled data containing emojis, both in English (E S ) and in the target language (E T ). In practice, these unlabeled, emoji-rich data can be easily obtained from online social media such as Twitter. Our task is to build a model that can classify the sentiment polarity of document in the target language solely based on the labeled data in the source language (i.e., L S ) and the different kinds of unlabeled data (i.e., U S , U T , E S and E T ). Finally, we use a held-out set of labeled documents in the target language (L T ), which can be small, to evaluate the model. The workflow of ELSA is illustrated in Figure 1, with the following steps. In step 1 and step 2, we build sentence representation models for both the source and the target languages. Specifically, for each language, we employ a large number of Tweets to learn word embeddings (through Word2Vec [42]) in an unsupervised fashion. From these word embeddings, we learn higher-level sentence representation through predicting the emojis used in a sentence. This can be viewed as a distantly supervised learning process, where emojis serve as surrogate sentiment labels. In step 3, we translate each labeled English document into the target language, sentence by sentence, through Google Translate. Both the English sentences and their translations are fed into the representation models learned in steps 1 and 2 to obtain their per-language representations (step 4 and step 5). Then in step 6 and step 7 we aggregate these sentence representations back to form two compact representations for each training document, one in English and the other in the target language. In step 8, we use the two representations as features to predict the real sentiment label of each document and obtain the final sentiment classifier. In the test phase, for a new document in the target language, we translate it into English and then follow the previous steps to obtain its representation (step 9), based on which we predict the sentiment label using the classifier (step 10). ", "publication_ref": ["b37"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Word Embedding Layer", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "W1", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Representation Learning", "text": "Representations of documents need to be learned before we train the sentiment classifier. Intuitively, one could simply use off-theshelf word embedding techniques to create word representations and then average the word vectors to obtain document embeddings. Such embeddings, however, capture neither per-language nor cross-language sentiment patterns. Since emojis are widely used to express sentiments across languages, we learn sentimentaware representations of documents using emoji prediction as an instrument. Specifically, in a distantly supervised way, we use emojis as surrogate sentiment labels and learn sentence embeddings by predicting which emojis are used in a sentence. This representation learning process is conducted separately in the source and the target languages to capture language-specific sentiment expressions. The architecture of the representation learning model is illustrated in Figure 2. First, we pre-train low-level word embeddings using tens of millions of unlabeled Tweets (i.e., the word embedding layer). Then, we represent every single word as a unique vector and use stacked bi-directional LSTM layers and one attention layer to encode these word vectors into sentence representations. The attention layer takes the outputs of both the embedding layer and the two LSTM layers as input, through the skip-connection algorithm [31], which enables unimpeded information flow in the whole training process. Finally, the model parameters are learned by minimizing the output error of the softmax layer. The details of the architecture are elaborated below.\nWord Embedding Layer. The word embeddings are pre-trained with the skip-gram algorithm [42] based on either U S or U T , which encode every single word into a continuous vector space. Words that commonly occur in a similar context are embedded closely in the vector space, which captures word semantic information. We leave the details of this standard Word2Vec process to the readers [42].\nBi-Directional LSTM Layer. As a special type of recurrent neural network (RNN), LSTM [33] is particularly suitable for modeling the sequential property of text data. At each step (e.g., word token), LSTM combines the current input and knowledge from the previous steps to update the states of the hidden layer. To tackle the gradient vanishing problem [32] of traditional RNNs, LSTM incorporates a gating mechanism to determine when and how the states of hidden layers can be updated. Each LSTM unit contains a memory cell and three gates (i.e., an input gate, a forget gate, and an output gate) [47]. The input and output gates control the input activations into the memory cell and the output flow of cell activations into the rest of the network, respectively. The memory cells in LSTM store the sequential states of the network, and each memory cell has a self-loop whose weight is controlled by the forget gate.\nLet us denote each sentence in E S or E T as (x, e), where\nx = [d 1 , d 2 , ..., d L ]\nas a sequence of word vectors representing the plain text (by removing emojis) and e as one emoji contained in the text. At step t, LSTM computes unit states of the network as follows:\ni (t ) = \u03c3 (U i d t + W i h (t \u22121) + b i ), f (t ) = \u03c3 (U f d t + W f h (t \u22121) + b f ), o (t ) = \u03c3 (U o d t + W o h (t \u22121) + b o ), c (t ) = f t \u2299 c (t \u22121) + i (t ) \u2299 tanh(U c d t + W c h (t \u22121) + b c ), h (t ) = o (t ) \u2299 tanh(c (t ) ), where i (t ) , f (t ) , o (t ) , c (t )\n, and h (t ) denote the state of the input gate, forget gate, output gate, memory cell, and hidden layer at step t. W , U , b respectively denote the recurrent weights, input weights, and biases. \u2299 is the element-wise product. We can extract the latent vector for each step t from LSTM. In order to capture the information from the context both preceding and following a word, we use the bi-directional LSTM. We concatenate the latent vectors from both directions to construct a bi-directional encoded vector h i for every single word vector d i , which is:\n\u2192 h i = \u2212\u2192 LST M(d i ), i \u2208 [1, L], \u2190 h i = \u2190\u2212 LST M(d i ), i \u2208 [L, 1], h i = [ \u2192 h i , \u2190 h i ].\nAttention Layer. We employ a skip-connection that concatenates the outputs of the embedding layer and the two bi-directional LSTM layers as the input of the attention layer. The i-th word of the input sentence can be represented as u i :\nu i = [d i , h i1 , h i2 ],\nwhere d i , h i1 , and h i2 denote the encoded vectors of words extracted in the word embedding layer and the first and second bi-directional LSTMs, respectively. Since not all words contribute equally to predicting emojis or expressing sentiments, we employ the attention mechanism [13] to determine the importance of every single word. The attention score of the i-th word is calculated by\na i = exp(W a u i ) L j=1 exp(W a u j ) ,\nwhere W a is the weight matrix used by the attention layer. Then each sentence can be represented as the weighted sum of all words in it, using the attention scores as weights. That is,\nv = L i=1 a i u i .\nSoftmax Layer. The sentence representation is then transferred into the softmax layer, which returns a probability vector Y . Each element of this vector indicates the probability that this sentence contains a specific emoji. The i-th element of the probability vector is calculated as:\ny i = exp(v T w i + b i ) K j=1 exp(v T w j + b j )\n, where w i and b i define the weight and bias of the i-th element.\nFinally, we learn the model parameters by minimizing the cross entropy between the output probability vectors and the one-hot vectors of the emoji contained in each sentence. After learning the parameters, we can extract the output of the attention layer to represent each input sentence. Through this emoji-prediction process, words with distinctive sentiments can be identified, and the plain text surrounding the same emojis will be represented similarly.\nGiven the fact that the sentiment labels are limited, once the emojipowered sentence representations are trained, they are locked in the downstream sentiment prediction task to avoid over-fitting.", "publication_ref": ["b26", "b37", "b37", "b28", "b27", "b42", "b8"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Training the Sentiment Classifier", "text": "Based on the pre-trained, per-language sentence representations, we then learn document representations and conduct cross-lingual sentiment classification. First, for each English document D s \u2208 L S , we use the pre-trained English representation model to embed every single sentence in it. Second, we aggregate these sentence representations to derive a compact document representation. Because different parts of a document contribute differently to the overall sentiment, we once again adopt the attention mechanism here. Supposing the sentence vectors as v i , we calculate the document vector r s as:\nr s = N i=1 \u03b2 i v i , where \u03b2 i = exp(W b v i ) N j=1 exp(W b v j ) ,\nwhere W b is the weight matrix of the attention layer and \u03b2 i is the attention score of the i-th sentence in the document. Next, we use Google Translate to translate D s into the target language (D t ). We then leverage the pre-trained target-language representation model to form representations for each translated document following the same process above. Supposing the text representations of D s and D t are r s and r t respectively, we concatenate them into a joint representation r c = [r s , r t ], which contains sentiment knowledge from both English and the target language, ensuring that our model is not dominated by the labeled English documents. Finally, we input r c into an additional softmax layer to predict the real sentiment label of D s .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sentiment Classification for Target Language", "text": "When we receive an unlabeled document in L T , we first translate it into English. Based on the representation models trained above, the original document and its English translation can be represented as r t and r s . We represent this document as [r s , r t ] and input it into the classifier, which outputs a predicted sentiment polarity. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "EVALUATION", "text": "In this section, we evaluate the effectiveness and efficiency of ELSA using standard benchmark datasets for cross-lingual sentiment classification as well as a large-scale corpus of Tweets.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Dataset", "text": "The labeled data (L S for training and L T for testing) used in our work are from the Amazon review dataset [3] created by Prettenhofer and Stein [50]. This dataset is representative and used in a variety of cross-lingual sentiment classification work [50,58,62]. It covers four languages (i.e., English, Japanese, French, and German) and three domains (i.e., book, DVD, and music). For each combination of language and domain, the dataset contains 1,000 positive reviews and 1,000 negative reviews. We select English as the source language and the other three as the target languages. Therefore, we can evaluate our approach on nine tasks in total (i.e., combinations of the three domains and three target languages). For each task, we use the 2,000 labeled English reviews in the corresponding domain for training and the 2,000 labeled reviews in each target language for evaluation. The translations of the test reviews are already provided in this dataset, so we only need to translate the English reviews into target languages.\nTo achieve unlabeled data (U S and U T ), we collect a sample of English, Japanese, French, and German Tweets between September 2016 and March 2018. All collected Tweets are used to train the word embeddings. As emojis are widely used on Twitter [48], we are able to extract emoji-labeled Tweets, which are used to learn emoji-powered sentence representations. For each language, we extract Tweets containing the top 64 emojis used in this language. As many Tweets contain multiple emojis, for each Tweet, we create separate examples for each unique emoji used in it to make the emoji prediction a single-label classification task instead of more complicated multi-label classification.\nWe then conduct the following preprocessing procedures for the documents. We remove all Retweets, and Tweets that contain URLs, to ensure that words appear in their original contexts and that the meaning of the Tweets do not depend on external content. Then we tokenize all the texts (including reviews and Tweets) into words, convert them into lowercase, and shorten the words with redundant characters into their canonical forms (e.g., \"cooooool\" is converted to \"cool\"). As Japanese words are not separated by white spaces, we use a tokenization tool called MeCab [2] to segment Japanese documents. In addition, we use special tokens to replace mentions and numbers. The processed emoji-Tweets provide the E S and E T datasets, whose statistics are presented in Table 1.", "publication_ref": ["b45", "b45", "b53", "b57", "b43", "b1"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Implementation Details", "text": "We learn the initial word embeddings using the skip-gram model with the window-size of 5 on the raw Tweets. The word vectors are then fine-tuned during the sentence representation learning phase. In the representation learning phase, to regularize our model, L2 regularization with parameter 10 \u22126 is applied for embedding weights. Dropout is applied at the rate of 0.5 before the softmax layer. The hidden units of bi-directional LSTM layers are set as 1,024 (512 in each direction). We randomly split the emoji-Tweets into the training, validation, and test sets in the proportion of 7:2:1. Accordingly, we use early stopping [17] to tune hyperparameters based on the validation performance through 50 epochs, with mini-batch size of 250. We used the Adam algorithm [35] for optimization, with the two momentum parameters set to 0.9 and 0.999, respectively. The initial learning rate was set to 10 \u22123 . In the phase of training the sentiment classifier, for exhaustive parameter tuning, we randomly select 90% of the labeled data as the training set and the remaining 10% as the validation set. The whole framework is implemented with TensorFlow [11].", "publication_ref": ["b12", "b30", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Baselines and Accuracy Comparison", "text": "To evaluate the performance of ELSA, we employ three representative baseline methods for comparison:\nMT-BOW uses the bag-of-words features to learn a linear classifier on the labeled English data [50]. It uses Google Translate to translate the test data into English and applies the pre-trained classifier to predict the sentiment polarity of the translated documents.\nCL-RL is the word-aligned representation learning method proposed by Xiao and Guo [58]. It constructs a unified word representation that consists of both language-specific components and shared components, for the source and the target languages. To establish connections between the two languages, it leverages Google Translate to create a set of critical parallel word pairs, and then it forces each parallel word pair to share the same word representation. The document representation is computed by taking the average over all words in the document. Given the representation as features, it trains a linear SVM model using the labeled English data.\nBiDRL is the document-aligned representation learning method proposed by Zhou et al. [62]. It uses Google Translate to create labeled parallel documents and forces the pseudo parallel documents to share the same embedding space. It also enforces constraints to make the document vectors associated with different sentiments fall into different positions in the embedding space. Furthermore, it forces documents with large textual differences but the same sentiment to have similar representations. After this representation learning process, it concatenates the vectors of one document in both languages and trains a logistic regression sentiment classifier.\nAs the benchmark datasets have quite balanced positive and negative reviews, we follow the aforementioned studies to use accuracy as an evaluation metric. All the baseline methods have been evaluated with exactly the same training and test data sets used in previous studies [62], so we make direct comparisons with their reported results. Unfortunately, we cannot obtain the individual predictions of these methods, so we are not able to report the statistical significance (such as McNemar's test [25]) of the difference between these baselines and ELSA. To alleviate this problem and get robust results, we run ELSA 10 times with different random initiations and summarize its average accuracy and standard deviation in Table 2, as well as the reported performance of the baselines. As illustrated in Table 2, ELSA outperforms all three baseline methods on all nine tasks. Looking more closely, the performance of all methods in Japanese sentiment classification is worse than in French and German tasks. According to the language systems defined by ISO 639 [10], English, French, and German belong to the same language family (i.e., Indo-European), while Japanese belongs to the Japonic family. In other words, French and German are more in common with English, and it is expected to be easier to translate English texts into French and German and transfer the sentiment knowledge from English to them. Therefore, in fact, Japanese tasks are most difficult and none of the previous methods have been able to achieve an accuracy above 0.8. It is encouraging to find that ELSA achieves an accuracy of 0.808 on the Japanese music task and an accuracy close to 0.8 (0.791) on the Japanese DVD task. The 0.783 accuracy on the book task is also non-negligible as it improves on the best existing model by almost 7 percent. In addition, although the French and German tasks are a little easier than the Japanese ones, none of the existing approaches can achieve an accuracy over 0.85 on any of the six tasks. However, our approach can achieve a mean accuracy higher than 0.85 on all of the six tasks.\nNext, we compare the results more thoroughly and further demonstrate the advantages of our approach. As is shown, the representation learning approaches (CL-RL, BiDRL, and ELSA) all outperform the shallow method MT-BOW on most tasks. This is reasonable as representation learning approaches embed words into high-dimensional vectors in a continuous semantic space and thus overcome the feature sparsity issue of traditional bag-of-words approaches. Furthermore, we observe that the document-level representation approaches (BiDRL and ELSA) outperform the wordlevel CL-RL. This indicates that incorporating document-level information into representations is more effective than focusing on individual words. Finally, ELSA outperforms the BiDRL on all tasks. In order to narrow the linguistic gap, BiDRL leverages only pseudo parallel texts to learn the common sentiment patterns between languages. Besides the pseudo parallel texts, ELSA also learns from the emoji usage in both languages. On the one hand, as a ubiquitous emotional signal, emojis are adopted across languages to express common sentiment patterns, which can complement the pseudo parallel corpus. On the other hand, the language-specific patterns of emoji usage help incorporate the language-specific knowledge of sentiments into the representation learning, which can benefit the downstream sentiment classification in the target language. As a next step, we explore the role of emojis in the learning process with a more comprehensive investigation.  ", "publication_ref": ["b45", "b53", "b57", "b57", "b20", "b5"], "figure_ref": [], "table_ref": ["tab_2", "tab_2"]}, {"heading": "The Power of Emojis", "text": "To further evaluate the contribution of emojis in ELSA, we conduct subsequent experiments to investigate the effects of emojis from three perspectives, i.e., overall performance, effectiveness of representation learning, and text comprehension.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Overall Performance", "text": "To understand how emojis affect cross-lingual sentiment classification in general, a straightforward idea is to remove the emojiprediction phase and compare simplified versions of ELSA:\nN-ELSA removes the emoji-prediction phase of both languages and directly uses two attention layers to realize the transformation from word vectors to the final document representation. There is no emoji data used in this model.\nT-ELSA removes the emoji-based representation learning on the English side. It uses the emoji-powered representations for the target language and translates labeled English documents into the target language to train a sentiment classifier for the target language. This model only leverages emoji usage in the target language.\nS-ELSA removes the emoji-based representation learning in the target language. It uses the emoji-powered representations of English and trains a sentiment classifier based on labeled English documents. Documents in the target language are first translated into English and then classified. This model only leverages emoji usage in the source language (i.e., English).\nTest accuracy of these models is illustrated in Table 3. We find that ELSA outperforms N-ELSA on all nine tasks. N-ELSA is only a little better than uniform guess (50%) since it learns the common patterns between languages only from pseudo parallel texts and does not incorporate sentiment information effectively. An alternative conjecture is that 2,000 reviews are insufficient to train such a complex model, which may have led to the problem of over-fitting.\nTo test between the two hypotheses, we mix up the labeled reviews in English and in the target language and randomly select 2,000 examples from the mixed set for training and use the remaining samples as a new test set. All other settings of the experiment are kept the same except for the new train/test split. Trained and tested in this way, the accuracy of N-ELSA becomes acceptable, with an average accuracy of 0.777 on all tasks. This indicates that over-fitting might not have been the major reason, while language discrepancy might be. Indeed, N-ELSA can still work well if we effectively incorporate cross-language sentiment information into the training process. More specifically, the original N-ELSA is dominated by  English sentiment information learned from pseudo parallel texts and fails to generalize to the target language correctly. When we input the sentiment information (labeled documents) of both English and the target language into the model, performance improves. Unfortunately, in a cross-lingual sentiment classification setting, we can not acquire enough labels in the target language. Emojis help the model capture generalizable sentiment knowledge, even if there is no labeled example for training in the target language.\nIn addition, ELSA also consistently achieves better accuracy compared to T-ELSA and S-ELSA on all tasks (McNemar's test [25] is performed and the differences are all statistically significant at the 5% level). The superiority of ELSA shows that only extracting sentiment information from one language is not enough for the cross-lingual sentiment task and that incorporating languagespecific knowledge for both languages is critical to the model's performance. Indeed, S-ELSA fails to capture sentiment patterns in the target language; and T-ELSA falls short in extracting transferable sentiment patterns from English (indicating that emojis are still beneficial even if there are sentiment labels in a language).", "publication_ref": ["b20"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Effectiveness of Representation Learning", "text": "To better understand the sentiment information learned through the emoji usage, we then conduct an empirical experiment at the word representation level. Recall that after the word embedding phase, each individual word can be represented by a unique vector and that these word vectors are then fine-tuned in the emojiprediction phase. Next, we would like to evaluate whether sentiment information is better captured by the new word representations under the effects of emojis. We sample 50 English words with distinct sentiments from the MPQA subjectivity lexicon [1] based on their frequencies in our corpus. These words are manually labeled in terms of positive or negative polarity from MPQA, and we regard these labels as the ground-truth for further evaluation.\nWe expect that an informative representation can embed words with same sentiment polarity closely in the vector space. To measure and illustrate the similarity, we calculate the similarity score between every two words using the cosine of the corresponding embedding vectors. Based on the cosine similarity, we perform a hierarchical clustering [15] and visualize the clustering results in Figure 3. The color scale of each cell indicates the similarity between the two words. The darker the cell, the more similar the representations of the two words.\nIn Figure 3(a), we use naive embeddings learned by Word2Vec (no emoji), and words with different sentiments cannot be clearly separated. Many words with different sentiments are embedded closely, for example, \"generous\" and \"picky\" in the bottom right section. This indicates that shallow word embeddings do not effectively capture the sentiment information.\nIn contrast, in Figure 3(b), we can easily observe two clusters after the fine-tuned emoji-prediction model. The top left corner cluster contains the positive words, while the bottom right corner contains the negative words. Only one positive word, \"sure,\" is incorrectly clustered with negative words. By checking the contexts of this word in our corpus, we find it is usually co-used with both positive and negative words, making its polarity ambiguous. The correct clustering of nearly all the words indicates that emoji usage is an effective channel to capture sentiment knowledge, which is desirable for downstream sentiment classification.", "publication_ref": ["b0", "b10"], "figure_ref": ["fig_3", "fig_3", "fig_3"], "table_ref": []}, {"heading": "Text Comprehension", "text": "We then explore how the emoji-powered representations benefit text comprehension. We select a representative case that is  incorrectly classified by N-ELSA but correctly classified by ELSA. This case is selected from the Japanese test samples and we use the segment of its translated English version for illustration in Figure 4.\nAlthough the whole document expresses dissatisfaction with an album, it is not that easy to identify this intent directly from each single sentence due to the translation quality and the document's complex compositions. For example, if we consider only the third sentence without context, the author seems to express a positive attitude. However, in fact, the author expresses an obviously negative attitude in the fourth and sixth sentences.\nIn Figure 4, we present the attention distribution of words and sentences generated by N-ELSA and ELSA, which indicates how the two models comprehend this document, or the rationale behind their classification decisions. We use the color scale of the background to indicate the attention scores of words in each sentence. The darker the word, the more it is attended to. For each sentence, we list its attention score in this document. In Figure 4(b), we also list the top 3 emojis ELSA predicts for each sentence, which may indicate its sentiment polarity predicted by ELSA.\nLet us first look at Figure 4(a), which demonstrates how N-ELSA processes the sentiment information. On the word level, N-ELSA tends to focus more on neutral words like \"song\" or \"album\" instead of sentimental words. On the sentence level, an extremely high attention is placed on the fifth sentence. However, the fifth sentence describes how the album is different from the first one and it does not express the obviously negative sentiment.\nIn contrast, after incorporating of emojis, ELSA is able to work with a proper logic (see Figure 4(b)). ELSA places its attention to the emotional adjectives, such as \"interesting\" and \"not possible,\" and contrast conjunctions such as \"however. \" Thus, it manages to identify the sentiment of each sentence as expected, which can be further explained by the predicted emojis on the left. Besides the most popular in our corpus, and predicted for the fourth and sixth sentence indicate the negative sentiment of the author, while and in the third sentence indicate positive sentiment. Then on the sentence level, ELSA places less attention to the positive third sentence, while centering upon the fourth and the sixth sentences. Through this comparison, we can see that emojis bring additional knowledge to the text comprehension and make the attention mechanism more effective.", "publication_ref": [], "figure_ref": ["fig_5", "fig_5", "fig_5", "fig_5", "fig_5"], "table_ref": []}, {"heading": "DISCUSSION", "text": "So far, we have presented the performance of ELSA on benchmark datasets and demonstrated the power of emojis in our learning process. There are some issues that could potentially affect its effectiveness and efficiency, which call for further discussion.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sensitivity on Data Volume", "text": "As we learn text representations from large amount of Tweets, we want to investigate whether ELSA works well with a smaller volume of data. First, we investigate the size of unlabeled data. The English representation model, once learned, can be reused by any other English-target language pair. We only need to scale down the Tweets and emoji-Tweets in the target language and observe the changes in performance on benchmarks. In details, we use 80%, 60%, 40%, and 20% of the collected Tweets to re-train the target-language representation model and keep the final supervised training fixed. We summarize the results in Figures 5(a), 5(b), and 5(c). For the Japanese tasks, when we scale down the unlabeled data, the performance gets slightly worse. Comparing the results using 20% and 100% of the Tweets, the accuracy differences in three domains are 0.021, 0.014, and 0.018, respectively. For French and German, the performance fluctuates less than 0.01. Most importantly, ELSA can outperform the existing methods on all nine tasks even with the 20% unlabeled data. This indicates that even though a target language is not as actively used on Twitter, our approach still works.\nFurthermore, although there are more labeled examples in English than other languages, in general, labels are still scarce. Hence, if a model can rely on even fewer labeled English documents, it is very desirable. To test this, we scale down the labeled data by 80%, 60%, 40%, and 20%. As shown in Figures 5(d), 5(e), and 5(f), the performance of ELSA slightly declines with the decrease of labels, but even with 20% labels (i.e., 400 labeled English samples), ELSA outperforms the existing methods using all 2,000 labeled samples on almost all tasks. This shows that with the help of large-scale emoji-Tweets, the model is less dependent on sentiment labels.", "publication_ref": [], "figure_ref": ["fig_7", "fig_7"], "table_ref": []}, {"heading": "Generalizability", "text": "Most previous cross-lingual sentiment studies [50,58,62] used the Amazon review dataset for evaluation. To compare with them, we also adopt this dataset in the main experiment of this paper. Sentiment classification in other domains such as social media is also important. Can ELSA still work well in a new domain? To evaluate the generalizability of our approach, we apply ELSA to a representative type of social media data -Tweets. As Tweets are short and informal, sentiment classification for them is considered to be a big challenge [28].\nAs cross-lingual studies on Tweets are very limited, we take only one recent cross-lingual method (MT-CNN) proposed by Deriu et al. [23] for comparison. It also relies on large-scale unlabeled Tweets and a translation tool. It first trains a sentiment classifier for English and then applies it to the translations of text documents in the target language. The training process for English Tweets contains three phases. First, it uses raw Tweets to create word embeddings just like our method. Second, it leverages \":)\" and \":(\" as weak labels and applies a multi-layer CNN model to adapt the word embeddings. Finally, it trains the model on labeled English Tweets. This work and our work both have coverage of French and German Tweets, so we use the two as the target languages for comparison.\nAs the sentiment-labeled Tweets used by [23] are released in forms of Twitter IDs and some of them are no longer available now, we cannot directly compare our model to the reported results in [23]. For fair comparison, we reproduce their method on the labeled Tweets that can still be collected. Based on the pre-trained representation models of MT-CNN [5] and ELSA, we use the same labeled English Tweets to train and validate the two classifiers and then test them on the same data (i.e., labeled French and German Tweets that can be collected). We list the sizes of the labeled English, French, and German Tweets we use in Table 4. From the distribution, a naive baseline using uniform guess would achieve an accuracy of 0.451 for French and 0.628 for German.\nResults are summarized in Table 5. The two approaches both outperform uniform guess, and ELSA outperforms the MT-CNN by 0.161 on French task and 0.155 on German task. Although we use the same training, validation, and test set for both approaches, we are still concerned about whether the pre-trained representation models have introduced unfairness. Specifically, if we have used more unlabeled Tweets for representation learning than MT-CNN, our outstanding performance may simply attribute to the size of data. To answer this question, we refer to [23] about their data size. We find that they uses 300M raw Tweets and 60M Tweets containing \":)\" and \":(\" for representation learning. In contrast, we only used 81M raw Tweets and 13.7M emoji-Tweets in three languages combined. Considering that emoticons are significantly less used than emojis on Twitter [48], although they use about 4.4 times more weak-labeled Tweets, these Tweets had to be collected from much more than 4.4 times of raw Tweets than ours. It is clear ELSA outperforms MT-CNN and relies less on data size.", "publication_ref": ["b45", "b53", "b57", "b23", "b18", "b18", "b18", "b18", "b43"], "figure_ref": [], "table_ref": ["tab_4", "tab_5"]}, {"heading": "CONCLUSION", "text": "As a ubiquitous emotional signal, emojis are widely adopted across languages to express sentiments. We leverage this characteristic of emojis, both using them as surrogate sentiment labels and using emoji prediction an instrument to address the language discrepancy in cross-lingual sentiment classification. We have presented ELSA, a novel emoji-powered representation learning framework, to capture both general and language-specific sentiment knowledge in the source and the target languages for cross-lingual sentiment classification. The representations learned by ELSA capture not only sentiment knowledge that generalizes across languages, but also language-specific patterns. We evaluate ELSA with comprehensive experiments on various benchmark datasets, which outperforms the state-of-the-art cross-lingual sentiment classification methods even when the size of labeled and unlabeled data decreases. The promising results indicate that emojis may be used as an a general instrument for text mining tasks that suffer from the scarcity of labeled examples, especially in situations where an inequality among different languages presents.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "MPQA opinion corpus", "journal": "", "year": "2005-10-22", "authors": ""}, {"ref_id": "b1", "title": "MeCab: Yet Another Part-of-Speech and Morphological Analyzer", "journal": "", "year": "2006", "authors": ""}, {"ref_id": "b2", "title": "DEFT 2015: Test Corpus", "journal": "", "year": "2015-04-28", "authors": ""}, {"ref_id": "b3", "title": "download-the-full-training-data-for-semeval-2017-task-4", "journal": "", "year": "2017-04-28", "authors": ""}, {"ref_id": "b4", "title": "Internet world users by language", "journal": "", "year": "2018-10-22", "authors": ""}, {"ref_id": "b5", "title": "Retrieved on October 22", "journal": "", "year": "2018", "authors": ""}, {"ref_id": "b6", "title": "Tensorflow: a system for large-scale machine learning", "journal": "", "year": "2016", "authors": "Mart\u00edn Abadi; Paul Barham; Jianmin Chen; Zhifeng Chen; Andy Davis; Jeffrey Dean; Matthieu Devin; Sanjay Ghemawat; Geoffrey Irving; Michael Isard"}, {"ref_id": "b7", "title": "Untangling emoji popularity through semantic embeddings", "journal": "", "year": "2017", "authors": "Wei Ai; Xuan Lu; Xuanzhe Liu; Ning Wang; Gang Huang; Qiaozhu Mei"}, {"ref_id": "b8", "title": "Neural machine translation by jointly learning to align and translate", "journal": "", "year": "2014", "authors": "Dzmitry Bahdanau; Kyunghyun Cho; Yoshua Bengio"}, {"ref_id": "b9", "title": "Multitask learning for fine-grained Twitter sentiment analysis", "journal": "", "year": "2017", "authors": "Georgios Balikas; Simon Moura; Massih-Reza Amini"}, {"ref_id": "b10", "title": "Fast optimal leaf ordering for hierarchical clustering", "journal": "", "year": "2001", "authors": "Ziv Bar; - Joseph; David K Gifford; Tommi S Jaakkola"}, {"ref_id": "b11", "title": "How cosmopolitan are emojis?: Exploring emojis usage and meaning over different languages with distributional semantics", "journal": "", "year": "2016", "authors": "Francesco Barbieri; Germ\u00e1n Kruszewski; Francesco Ronzano"}, {"ref_id": "b12", "title": "Overfitting in neural nets: backpropagation, conjugate gradient, and early stopping", "journal": "", "year": "2000", "authors": "Rich Caruana; Steve Lawrence; C. Lee Giles"}, {"ref_id": "b13", "title": "An autoencoder approach to learning bilingual word representations", "journal": "", "year": "2014", "authors": "A P Sarath Chandar; Stanislas Lauly; Hugo Larochelle; M Mitesh; Balaraman Khapra;  Ravindran; C Vikas; Amrita Raykar;  Saha"}, {"ref_id": "b14", "title": "Modeling language discrepancy for cross-lingual sentiment analysis", "journal": "", "year": "2017", "authors": "Qiang Chen; Chenliang Li; Wenjie Li"}, {"ref_id": "b15", "title": "Through a gender lens: learning usage patterns of emojis from large-scale Android users", "journal": "WWW", "year": "2018", "authors": "Zhenpeng Chen; Xuan Lu; Wei Ai; Huoran Li; Qiaozhu Mei; Xuanzhe Liu"}, {"ref_id": "b16", "title": "Sender-intended functions of emojis in US messaging", "journal": "", "year": "2016", "authors": "Henriette Cramer; Paloma De Juan; Joel R Tetreault"}, {"ref_id": "b17", "title": "Enhanced sentiment learning using Twitter hashtags and smileys", "journal": "", "year": "2010", "authors": "Dmitry Davidov; Oren Tsur; Ari Rappoport"}, {"ref_id": "b18", "title": "Leveraging large amounts of weakly supervised data for multi-language sentiment classification", "journal": "", "year": "2017", "authors": "Jan Deriu; Aur\u00e9lien Lucchi; Valeria De Luca; Aliaksei Severyn; Simon M\u00fcller; Mark Cieliebak; Thomas Hofmann; Martin Jaggi"}, {"ref_id": "b19", "title": "Characterizing debate performance via aggregated Twitter sentiment", "journal": "", "year": "2010", "authors": "Nicholas Diakopoulos; David A Shamma"}, {"ref_id": "b20", "title": "Approximate statistical tests for comparing supervised classification learning algorithms", "journal": "Neural computation", "year": "1998", "authors": "G Thomas;  Dietterich"}, {"ref_id": "b21", "title": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm", "journal": "", "year": "2017", "authors": "Bjarke Felbo; Alan Mislove; Anders S\u00f8gaard; Iyad Rahwan; Sune Lehmann"}, {"ref_id": "b22", "title": "Sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis", "journal": "", "year": "2004", "authors": "Michael Gamon"}, {"ref_id": "b23", "title": "Like it or not: a survey of Twitter sentiment analysis methods", "journal": "Comput. Surveys", "year": "2016", "authors": "Anastasia Giachanou; Fabio Crestani"}, {"ref_id": "b24", "title": "Sentiment-aware personalized Tweet recommendation through multimodal FFM", "journal": "Multimedia Tools Appl", "year": "2018", "authors": "Ryosuke Harakawa; Daichi Takehara; Takahiro Ogawa; Miki Haseyama"}, {"ref_id": "b25", "title": "Reactions: Twitter based mobile application for awareness of friends' emotions", "journal": "", "year": "2012", "authors": "Herdem Kiraz Candan"}, {"ref_id": "b26", "title": "Training and analysing deep recurrent neural networks", "journal": "", "year": "2013", "authors": "M Hermans; B Schrauwen"}, {"ref_id": "b27", "title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions", "journal": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems", "year": "1998", "authors": "Sepp Hochreiter"}, {"ref_id": "b28", "title": "Long short-term memory", "journal": "Neural Computation", "year": "1997", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"ref_id": "b29", "title": "Spice up your chat: the intentions and sentiment effects of using emojis", "journal": "", "year": "2017", "authors": "Tianran Hu; Han Guo; Hao Sun; Thuy - ; Thi Nguyen; Jiebo Luo"}, {"ref_id": "b30", "title": "Adam: a method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b31", "title": "Can Chinese Web pages be classified with English data source", "journal": "WWW", "year": "2008", "authors": "Xiao Ling; Gui-Rong Xue; Wenyuan Dai; Yun Jiang; Qiang Yang; Yong Yu"}, {"ref_id": "b32", "title": "Sentiment analysis and opinion mining", "journal": "Morgan & Claypool Publishers", "year": "2012", "authors": "Bing Liu"}, {"ref_id": "b33", "title": "Content attention model for aspect based sentiment analysis", "journal": "WWW", "year": "2018", "authors": "Qiao Liu; Haibin Zhang; Yifu Zeng; Ziqi Huang; Zufeng Wu"}, {"ref_id": "b34", "title": "ARSA: a sentimentaware model for predicting sales performance using blogs", "journal": "", "year": "2007", "authors": "Yang Liu; Xiangji Huang; Aijun An; Xiaohui Yu"}, {"ref_id": "b35", "title": "Learning from the ubiquitous language: an empirical analysis of emoji usage of smartphone users", "journal": "", "year": "2016", "authors": "Xuan Lu; Wei Ai; Xuanzhe Liu; Qian Li; Ning Wang; Gang Huang; Qiaozhu Mei"}, {"ref_id": "b36", "title": "Star quality: aggregating reviews to rank products and merchants", "journal": "", "year": "2010", "authors": "Mary Mcglohon; Natalie S Glance; Zach Reiter"}, {"ref_id": "b37", "title": "Efficient estimation of word representations in vector space", "journal": "Computer Science", "year": "2013", "authors": "Tomas Mikolov; Kai Chen; Greg Corrado; Jeffrey Dean"}, {"ref_id": "b38", "title": "Blissfully happy\" or \"ready to fight\": varying interpretations of emoji", "journal": "", "year": "2016", "authors": "Hannah Jean Miller; Jacob Thebault-Spieker; Shuo Chang; Isaac L Johnson; Loren G Terveen; Brent J Hecht"}, {"ref_id": "b39", "title": "How translation alters sentiment", "journal": "J. Artif. Intell. Res", "year": "2016", "authors": "M Saif; Mohammad Mohammad; Svetlana Salameh;  Kiritchenko"}, {"ref_id": "b40", "title": "From Tweets to polls: linking text sentiment to public opinion time series", "journal": "", "year": "2010", "authors": "O' Brendan; Ramnath Connor; Bryan R Balasubramanyan; Noah A Routledge;  Smith"}, {"ref_id": "b41", "title": "Thumbs up?: Sentiment classification using machine learning techniques", "journal": "", "year": "2002", "authors": "Bo Pang; Lillian Lee; Shivakumar Vaithyanathan"}, {"ref_id": "b42", "title": "On the difficulty of training recurrent neural networks", "journal": "", "year": "2013", "authors": "Razvan Pascanu; Tomas Mikolov; Yoshua Bengio"}, {"ref_id": "b43", "title": "Emoticons vs. emojis on Twitter", "journal": "", "year": "2015", "authors": "Umashanthi Pavalanathan; Jacob Eisenstein"}, {"ref_id": "b44", "title": "Linguistic inquiry and word count: LIWC", "journal": "Lawrence Erlbaum Associates", "year": "1999", "authors": "J W Pennebaker; L E Francis; R J Booth"}, {"ref_id": "b45", "title": "Cross-language text classification using structural correspondence learning", "journal": "", "year": "2010", "authors": "Peter Prettenhofer; Benno Stein"}, {"ref_id": "b46", "title": "Automatically annotating a five-billion-word corpus of Japanese blogs for affect and sentiment analysis", "journal": "", "year": "2012", "authors": "Michal Ptaszynski; Rafal Rzepka; Kenji Araki; Yoshio Momouchi"}, {"ref_id": "b47", "title": "DASA: dissatisfaction-oriented advertising based on sentiment analysis", "journal": "Expert Systems with Applications", "year": "2010", "authors": "Guang Qiu; Xiaofei He; Feng Zhang; Yuan Shi; Jiajun Bu; Chun Chen"}, {"ref_id": "b48", "title": "Inferring mood instability on social media by leveraging ecological momentary assessments", "journal": "", "year": "2017", "authors": "Koustuv Saha; Larry Chan; Gregory D Kaya De Barbaro; Munmun De Abowd;  Choudhury"}, {"ref_id": "b49", "title": "Exploiting topic based Twitter sentiment for stock prediction", "journal": "", "year": "2013", "authors": "Jianfeng Si; Arjun Mukherjee; Bing Liu; Qing Li; Huayi Li; Xiaotie Deng"}, {"ref_id": "b50", "title": "Applying uncertainty theory into the restaurant recommender system based on sentiment analysis of online Chinese reviews", "journal": "World Wide Web", "year": "2019", "authors": "Lihua Sun; Junpeng Guo; Yanlin Zhu"}, {"ref_id": "b51", "title": "Sentiment in short strength detection informal text", "journal": "JASIST", "year": "2010", "authors": "Mike Thelwall; Kevan Buckley; Georgios Paltoglou; Di Cai; Arvid Kappas"}, {"ref_id": "b52", "title": "Sentence-level sentiment classification with weak supervision", "journal": "", "year": "2017-06", "authors": "Fangzhao Wu; Jia Zhang; Zhigang Yuan; Sixing Wu"}, {"ref_id": "b53", "title": "Semi-supervised representation learning for cross-lingual text classification", "journal": "", "year": "2013", "authors": "Min Xiao; Yuhong Guo"}, {"ref_id": "b54", "title": "The way I talk to you: sentiment expression in an organizational context", "journal": "", "year": "2012", "authors": "Jiang Yang; A Lada; Mark S Adamic; Zhen Ackerman; Ching-Yung Wen;  Lin"}, {"ref_id": "b55", "title": "Deep learning for sentiment analysis: a survey", "journal": "Wiley Interdiscip. Rev. Data Min. Knowl. Discov", "year": "2018", "authors": "Lei Zhang; Shuai Wang; Bing Liu"}, {"ref_id": "b56", "title": "Transfer learning for cross-language text categorization through active correspondences construction", "journal": "", "year": "2016", "authors": "Joey Tianyi Zhou; Ivor W Sinno Jialin Pan; Shen-Shyang Tsang;  Ho"}, {"ref_id": "b57", "title": "Cross-lingual sentiment classification with bilingual document representation learning", "journal": "", "year": "2016", "authors": "Xinjie Zhou; Xiaojun Wan; Jianguo Xiao"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: The workflow of ELSA.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Network architecture for representation learning through emoji prediction.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "(a) Clusters of selected words with Word2Vec representations.(b) Clusters of selected words with emoji-powered representations.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Comparison of word representations with and without emoji prediction.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "(a) Word and sentence attention distribution generated by N-ELSA.(b) Word and sentence attention distribution generated by ELSA.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 :4Figure 4: Case study: Effect of emojis on text comprehension.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "(a) Unlabeled data: Japanese tasks (b) Unlabeled data: French tasks (c) Unlabeled data: German tasks (d) Labeled English data: Japanese tasks (e) Labeled English data: French tasks (f) Labeled English data: German tasks", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 5 :5Figure 5: Accuracy of ELSA when size of unlabeled and labeled data changes.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "The sizes of the Tweets and emoji-Tweets.", "figure_data": "LanguageEnglish Japanese French GermanRaw Tweets39.4M19.5M29.2M12.4MEmoji-Tweets6.6M2.9M4.4M2.7M"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "The accuracy of ELSA (standard deviations in parentheses) and baseline methods on the nine benchmark tasks.", "figure_data": "Language Domain MT-BOW CL-RL BiDRLELSABook0.7020.7110.7320.783 (0.003)JapaneseDVD0.7130.7310.7680.791 (0.004)Music0.7200.7440.7880.808 (0.005)Book0.8080.7830.8440.860 (0.002)FrenchDVD0.7880.7480.8360.857 (0.002)Music0.7580.7870.8250.860 (0.002)Book0.7970.7990.8410.864 (0.001)GermanDVD0.7790.7710.8410.861 (0.001)Music0.7720.7730.8470.878 (0.002)"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Performance of ELSA and its simplified versions.", "figure_data": "Language Domain N-ELSA T-ELSA S-ELSA ELSABook0.527*0.742*0.753*0.783JapaneseDVD0.507*0.756*0.766*0.791Music0.513*0.792*0.778*0.808Book0.505*0.821*0.850*0.860FrenchDVD0.507*0.816*0.843*0.857Music0.503*0.811*0.848*0.860Book0.513*0.804*0.848*0.864GermanDVD0.521*0.790*0.849*0.861Music0.513*0.818*0.863*0.878"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "The sizes of labeled Tweets collected.", "figure_data": "DatasetLanguagePositive Neutral NegativeTrainingEnglish [8]5,1013,7421,643Validation English [7]1,038987365TestFrench [4] German [6]987 1,0571,389 4,441718 1,573"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Classification accuracy on French and German Tweets. indicates the difference between ELSA and the baseline methods is statistically significant (p < 0.05) by McNemar's test.", "figure_data": "Language ELSA MT-CNN Uniform GuessFrench0.6960.535*0.451*German0.8090.654*0.628*"}], "formulas": [{"formula_id": "formula_0", "formula_text": "x = [d 1 , d 2 , ..., d L ]", "formula_coordinates": [4.0, 317.96, 185.55, 240.25, 20.96]}, {"formula_id": "formula_1", "formula_text": "i (t ) = \u03c3 (U i d t + W i h (t \u22121) + b i ), f (t ) = \u03c3 (U f d t + W f h (t \u22121) + b f ), o (t ) = \u03c3 (U o d t + W o h (t \u22121) + b o ), c (t ) = f t \u2299 c (t \u22121) + i (t ) \u2299 tanh(U c d t + W c h (t \u22121) + b c ), h (t ) = o (t ) \u2299 tanh(c (t ) ), where i (t ) , f (t ) , o (t ) , c (t )", "formula_coordinates": [4.0, 317.62, 238.36, 218.01, 82.48]}, {"formula_id": "formula_2", "formula_text": "\u2192 h i = \u2212\u2192 LST M(d i ), i \u2208 [1, L], \u2190 h i = \u2190\u2212 LST M(d i ), i \u2208 [L, 1], h i = [ \u2192 h i , \u2190 h i ].", "formula_coordinates": [4.0, 391.87, 417.45, 92.0, 50.74]}, {"formula_id": "formula_3", "formula_text": "u i = [d i , h i1 , h i2 ],", "formula_coordinates": [4.0, 405.6, 528.42, 64.42, 9.76]}, {"formula_id": "formula_4", "formula_text": "a i = exp(W a u i ) L j=1 exp(W a u j ) ,", "formula_coordinates": [4.0, 396.9, 614.82, 82.06, 25.3]}, {"formula_id": "formula_5", "formula_text": "v = L i=1 a i u i .", "formula_coordinates": [4.0, 414.05, 684.0, 47.32, 28.0]}, {"formula_id": "formula_6", "formula_text": "y i = exp(v T w i + b i ) K j=1 exp(v T w j + b j )", "formula_coordinates": [5.0, 124.44, 150.35, 95.07, 26.63]}, {"formula_id": "formula_7", "formula_text": "r s = N i=1 \u03b2 i v i , where \u03b2 i = exp(W b v i ) N j=1 exp(W b v j ) ,", "formula_coordinates": [5.0, 120.81, 437.6, 85.94, 45.02]}], "doi": "10.1145/3308558.3313600"}