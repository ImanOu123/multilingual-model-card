{"title": "Scaling Personalized Web Search", "authors": "Glen Jeh; Jennifer Widom", "pub_date": "", "abstract": "Recent web search techniques augment traditional text matching with a global notion of \"importance\" based on the linkage structure of the web, such as in Google's PageRank algorithm. For more refined searches, this global notion of importance can be specialized to create personalized views of importance-for example, importance scores can be biased according to a user-specified set of initially-interesting pages. Computing and storing all possible personalized views in advance is impractical, as is computing personalized views at query time, since the computation of each view requires an iterative computation over the web graph. We present new graph-theoretical results, and a new technique based on these results, that encode personalized views as partial vectors. Partial vectors are shared across multiple personalized views, and their computation and storage costs scale well with the number of views. Our approach enables incremental computation, so that the construction of personalized views from partial vectors is practical at query time. We present efficient dynamic programming algorithms for computing partial vectors, an algorithm for constructing personalized views from partial vectors, and experimental results demonstrating the effectiveness and scalability of our techniques.", "sections": [{"heading": "Introduction and Motivation", "text": "General web search is performed predominantly through text queries to search engines. Because of the enormous size of the web, text alone is usually not selective enough to limit the number of query results to a manageable size. The PageRank algorithm [10], among others [8], has been proposed (and implemented in Google [1]) to exploit the linkage structure of the web to compute global \"importance\" scores that can be used to influence the ranking of search results. To encompass different notions of importance for different users and queries, the basic PageRank algorithm can be modified to create \"personalized views\" of the web, redefining importance according to user preference. For example, a user may wish to specify his bookmarks as a set of preferred pages, so that any query results that are important with respect to his bookmarked pages would be ranked higher. While experimentation with the use of personalized PageRank has shown its utility and This work was supported by the National Science Foundation under grant IIS-9817799.\npromise [5,10], the size of the web makes its practical realization extremely difficult. To see why, let us review the intuition behind the PageRank algorithm and its extension for personalization.\nThe fundamental motivation underlying PageRank is the recursive notion that important pages are those linked-to by many important pages. A page with only two in-links, for example, may seem unlikely to be an important page, but it may be important if the two referencing pages are Yahoo! and Netscape, which themselves are important pages because they have numerous in-links. One way to formalize this recursive notion is to use the \"random surfer\" model introduced in [10]. Imagine that trillions of random surfers are browsing the web: if at a certain time step a surfer is looking at page p, at the next time step he looks at a random out-neighbor of p. As time goes on, the expected percentage of surfers at each page p converges (under certain conditions) to a limit r(p) that is independent of the distribution of starting points. Intuitively, this limit is the PageRank of p, and is taken to be an importance score for p, since it reflects the number of people expected to be looking at p at any one time.\nThe PageRank score r(p) reflects a \"democratic\" importance that has no preference for any particular pages. In reality, a user may have a set P of preferred pages (such as his bookmarks) which he considers more interesting. We can account for preferred pages in the random surfer model by introducing a \"teleportation\" probability c: at each step, a surfer jumps back to a random page in P with probability c, and with probability 1 \u2212 c continues forth along a hyperlink. The limit distribution of surfers in this model would favor pages in P , pages linked-to by P , pages linked-to in turn, etc. We represent this distribution as a personalized PageRank vector (PPV) personalized on the set P . Informally, a PPV is a personalized view of the importance of pages on the web. Rankings of a user's text-based query results can be biased according to a PPV instead of the global importance distribution.\nEach PPV is of length n, where n is the number of pages on the web. Computing a PPV naively using a fixed-point iteration requires multiple scans of the web graph [10], which makes it impossible to carry out online in response to a user query. On the other hand, PPV's for all preference sets, of which there are 2 n , is far too large to compute and store offline. We present a method for encoding PPV's as partially-computed, shared vectors that are practical to compute and store offline, and from which PPV's can be computed quickly at query time.\nIn our approach we restrict preference sets P to subsets of a set of hub pages H, selected as those of greater interest for personalization. In practice, we expect H to be a set of pages with high PageRank (\"important pages\"), pages in a human-constructed directory such as Yahoo! or Open Directory [2], or pages important to a particular enterprise or application. The size of H can be thought of as the available degree of personalization. We present algorithms that, unlike previous work [5,10], scale well with the size of H. Moreover, the same techniques we introduce can yield approximations on the much broader set of all PPV's, allowing at least some level of personalization on arbitrary preference sets.\nThe main contributions of this paper are as follows.\n\u2022 A method, based on new graph-theoretical results (listed next), of encoding PPV's as partial quantities, enabling an efficient, scalable computation that can be divided between precomputation time and query time, in a customized fashion according to available resources and application requirements.\n\u2022 Three main theorems: The Linearity Theorem allows every PPV to be represented as a linear combination of basis vectors, yielding a natural way to construct PPV's from shared components. The Hubs Theorem allows basis vectors to be encoded as partial vectors and a hubs skeleton, enabling basis vectors themselves to be constructed from common components.\nThe Decomposition Theorem establishes a linear relationship among basis vectors, which is exploited to minimize redundant computation.\n\u2022 Several algorithms for computing basis vectors, specializations of these algorithms for computing partial vectors and the hubs skeleton, and an algorithm for constructing PPV's from partial vectors using the hubs skeleton.\n\u2022 Experimental results on real web data demonstrating the effectiveness and scalability of our techniques.\nIn Section 2 we introduce the notation used in this paper and formalize personalized PageRank mathematically. Section 3 presents basis vectors, the first step towards encoding PPV's as shared components. The full encoding is presented in Section 4. Section 5 discusses the computation of partial quantities. Experimental results are presented in Section 6. Related work is discussed in Section 7. Section 8 summarizes the contributions of this paper. Additional material, primarily proofs of theorems, appears in a set of appendices.", "publication_ref": ["b7", "b5", "b2", "b7", "b7", "b7", "b2", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "Let G = (V, E) denote the web graph, where V is the set of all web pages and E contains a directed edge p, q iff page p links to page q. For a page p, we denote by I(p) and O(p) the set of inneighbors and out-neighbors of p, respectively. Individual in-neighbors are denoted as\nI i (p) (1 \u2264 i \u2264 |I(p)|)\n, and individual out-neighbors are denoted analogously. For convenience, pages are numbered from 1 to n, and we refer to a page p and its associated number i interchangeably. For a vector v, v(p) denotes entry p, the p-th component of v. We always typeset vectors in boldface and scalars (e.g., v(p)) in normal font. All vectors in this paper are n-dimensional and have nonnegative entries. They should be thought of as distributions rather than arrows. The magnitude of a vector v is defined to be n i=1 v(i) and is written |v|. In this paper, vector magnitudes are always in [0, 1].\nIn an implementation, a vector may be represented as a list of its nonzero entries, so another useful measure is the size of v, the number of nonzero entries in v. We generalize the preference set P discussed in Section 1 to a preference vector u, where |u| = 1 and u(p) denotes the amount of preference for page p. For example, a user who wants to personalize on his bookmarked pages P uniformly would have a u where u(p) = 1 |P | if p \u2208 P , and u(p) = 0 if p / \u2208 P . We formalize personalized PageRank scoring using matrix-vector equations. Let A be the matrix corresponding to the web graph G, where A ij = 1 |O(j)| if page j links to page i, and A ij = 0 otherwise. For simplicity of presentation, we assume that every page has at least one out-neighbor, as can be enforced by adding self-links to pages without out-links. The resulting scores can be adjusted to account for the (minor) effects of this modification, as specified in Appendix C.2.\nFor a given u, the personalized PageRank equation can be written as\nv = (1 \u2212 c)Av + cu (1)\nwhere c \u2208 (0, 1) is the \"teleportation\" constant discussed in Section 1. Typically c \u2248 0.15, and experiments have shown that small changes in c have little effect in practice [10]. A solution v to equation ( 1) is a steady-state distribution of random surfers under the model discussed in Section 1, where at each step a surfer teleports to page p with probability c\u2022u(p), or moves to a random outneighbor otherwise [10]. By a theorem of Markov Theory, a solution v with |v| = 1 always exists and is unique [9]. 1 The solution v is the personalized PageRank vector (PPV) for preference vector u. If u is the uniform distribution vector u = [1/n, . . . , 1/n], then the corresponding solution v is the global PageRank vector [10], which gives no preference to any pages. For the reader's convenience, Table 1 on the next page lists terminology that will be used extensively in the coming sections.", "publication_ref": ["b7", "b7", "b6", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Basis Vectors", "text": "We present the first step towards encoding PPV's as shared components. The motivation behind the encoding is a simple observation about the linearity 2 of PPV's, formalized by the following theorem.\nTheorem (Linearity). For any preference vectors u 1 and u 2 , if v 1 and v 2 are the two corresponding PPV's, then for any constants \u03b1 1 , \u03b1 2 \u2265 0 such that \u03b1 1 + \u03b1 2 = 1,\n\u03b1 1 v 1 + \u03b1 2 v 2 = (1 \u2212 c)A(\u03b1 1 v 1 + \u03b1 2 v 2 ) + c(\u03b1 1 u 1 + \u03b1 2 u 2 )\n(2)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Term Description Section", "text": "Hub Set H A subset of web pages. 1\nPreference Set P Set of pages on which to personalize 1 (restricted in this paper to subsets of H).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Preference Vector u", "text": "Preference set with weights. 2\nPersonalized PageRank Vector Importance distribution induced by a preference vector.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "(PPV)", "text": "Basis Vector r p (or r i ) PPV for a preference vector with a single nonzero entry 3 at p (or i).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Hub Vector r p", "text": "Basis vector for a hub page p \u2208 H.\n3 Partial Vector (r p \u2212 r H p )\nUsed with the hubs skeleton to construct a hub vector.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4.2", "text": "Hubs Skeleton S Used with partial vectors to construct a hub vector.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4.3", "text": "Web Skeleton Extension of the hubs skeleton to include pages not in H.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4.4.3", "text": "Partial Quantities Partial vectors and the hubs, web skeletons.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Intermediate Results", "text": "Maintained during iterative computations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "5.2", "text": "Table 1: Summary of terms.\nInformally, the Linearity Theorem says that the solution to a linear combination of preference vectors u 1 and u 2 is the same linear combination of the corresponding PPV's v 1 and v 2 . The proof is in Appendix A.\nLet x 1 , . . . , x n be the unit vectors in each dimension, so that for each i, x i has value 1 at entry i and 0 everywhere else. Let r i be the PPV corresponding to x i . Each basis vector r i gives the distribution of random surfers under the model that at each step, surfers teleport back to page i with probability c. It can be thought of as representing page i's view of the web, where entry j of r i is j's importance in i's view. Note that the global PageRank vector is 1 n (r 1 + \u2022 \u2022 \u2022 + r n ), the average of every page's view.\nAn arbitrary personalization vector u can be written as a weighted sum of the unit vectors\nx i : u = n i=1 \u03b1 i x i(3)\nfor some constants \u03b1 1 , . . . , \u03b1 n . By the Linearity Theorem,\nv = n i=1 \u03b1 i r i(4)\nis the corresponding PPV, expressed as a linear combination of the basis vectors r i .\nRecall from Section 1 that preference sets (now preference vectors) are restricted to subsets of a set of hub pages H. If a basis hub vector (or hereafter hub vector) for each p \u2208 H were computed and stored, then any PPV corresponding to a preference set P of size k (a preference vector with k nonzero entries) can be computed by adding up the k corresponding hub vectors r p with the appropriate weights \u03b1 p .\nEach hub vector can be computed naively using the fixed-point computation in [10]. However, each fixed-point computation is expensive, requiring multiple scans of the web graph, and the computation time (as well as storage cost) grows linearly with the number of hub vectors |H|. In the next section, we enable a more scalable computation by constructing hub vectors from shared components.", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "Decomposition of Basis Vectors", "text": "In Section 3 we represented PPV's as a linear combination of |H| hub vectors r p , one for each p \u2208 H. Any PPV based on hub pages can be constructed quickly from the set of precomputed hub vectors, but computing and storing all hub vectors is impractical. To compute a large number of hub vectors efficiently, we further decompose them into partial vectors and the hubs skeleton, components from which hub vectors can be constructed quickly at query time. The representation of hub vectors as partial vectors and the hubs skeleton saves both computation time and storage due to sharing of components among hub vectors. Note, however, that depending on available resources and application requirements, hub vectors can be constructed offline as well. Thus \"query time\" can be thought of more generally as \"construction time\". We compute one partial vector for each hub page p, which essentially encodes the part of the hub vector r p unique to p, so that components shared among hub vectors are not computed and stored redundantly. The complement to the partial vectors is the hubs skeleton, which succinctly captures the interrelationships among hub vectors. It is the \"blueprint\" by which partial vectors are assembled to form a hub vector, as we will see in Section 4.3.\nThe mathematical tools used in the formalization of this decomposition are presented next. 3", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Inverse P-distance", "text": "To formalize the relationship among hub vectors, we relate the personalized PageRank scores represented by PPV's to inverse P-distances in the web graph, a concept based on expected-f distances as introduced in [7].\nLet p, q \u2208 V . We define the inverse P-distance r p (q) from p to q as r p (q) =\nt:p q P [t]c(1 \u2212 c) l(t)(5)\nwhere the summation is taken over all tours t (paths that may contain cycles) starting at p and ending at q, possibly touching p or q multiple times. For a tour t = w 1 , . . . , w k , the length l(t) is k \u2212 1, the number of edges in t. The term P [t], which should be interpreted as \"the probability of traveling t\", is defined as\nk\u22121 i=1 1 |O(w i )| , or 1 if l(t) = 0.\nIf there is no tour from p to q, the summation is taken to be 0. 4 Note that r p (q) measures distances inversely: it is higher for nodes q \"closer\" to p. As suggested by the notation and proven in Appendix C, r p (q) = r p (q) for all p, q \u2208 V , so we will use r p (q) to denote both the inverse P-distance and the personalized PageRank score. Thus PageRank scores can be viewed as an inverse measure of distance.\nLet H \u2286 V be some nonempty set of pages. For p, q \u2208 V , we define r H p (q) as a restriction of r p (q) that considers only tours which pass through some page h \u2208 H in equation (5). That is, a page h \u2208 H must occur on t somewhere other than the endpoints. Precisely, r H p (q) is written as r H p (q) =\nt:p H q P [t]c(1 \u2212 c) l(t)(6)\nwhere the notation t : p H q reminds us that t passes through some page in H. Note that t must be of length at least 2. In this paper, H is always the set of hub pages, and p is usually a hub page (until we discuss the web skeleton in Section 4.4.3).", "publication_ref": ["b4", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Partial Vectors", "text": "Intuitively, r H p (q), defined in equation ( 6), is the influence of p on q through H. In particular, if all paths from p to q pass through a page in H, then H separates p and q, and r H p (q) = r p (q). For well-chosen sets H (discussed in Section 4.4.2), it will be true that r p (q) \u2212 r H p (q) = 0 for many pages p, q. Our strategy is to take advantage of this property by breaking r p into two components: (r p \u2212 r H p ) and r H p , using the equation\nr p = (r p \u2212 r H p ) + r H p(7)\nWe first precompute and store the partial vector (r p \u2212r H p ) instead of the full hub vector r p . Partial vectors are cheaper to compute and store than full hub vectors, assuming they are represented as a list of their nonzero entries. Moreover, the size of each partial vector decreases as |H| increases, making this approach particularly scalable. We then add r H p back at query time to compute the full hub vector. However, computing and storing r H p explicitly could be as expensive as r p itself. In the next section we show how to encode r H p so it can be computed and stored efficiently.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Hubs Skeleton", "text": "Let us briefly review where we are: In Section 3 we represented PPV's as linear combinations of hub vectors r p , one for each p \u2208 H, so that we can construct PPV's quickly at query time if we have precomputed the hub vectors, a relatively small subset of PPV's. To encode hub vectors efficiently, in Section 4.2 we said that instead of full hub vectors r p , we first compute and store only partial vectors (r p \u2212 r H p ), which intuitively account only for paths that do not pass through a page of H (i.e., the distribution is \"blocked\" by H). Computing and storing the difference vector r H p efficiently is the topic of this section. It turns out that the vector r H p can be be expressed in terms of the partial vectors (r h \u2212 r H h ), for h \u2208 H, as shown by the following theorem. Recall from Section 3 that x h has value 1 at h and 0 everywhere else.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Theorem (Hubs). For any", "text": "p \u2208 V , H \u2286 V , r H p = 1 c h\u2208H (r p (h) \u2212 cx p (h)) r h \u2212 r H h \u2212 cx h(8)\nIn terms of inverse P-distances (Section 4.1), the Hubs Theorem says roughly that the distance from page p to any page q \u2208 V through H is the distance r p (h) from p to each h \u2208 H times the distance r h (q) from h to q, correcting for the paths among hubs by r H h (q). The terms cx p (h) and cx h deal with the special cases when p or q is itself in H. The proof, which is quite involved, is in Appendix D.\nThe quantity r h \u2212 r H h appearing on the right-hand side of ( 8) is exactly the partial vectors discussed in Section 4.2. Suppose we have computed r p (H) = {(h, r p (h)) | h \u2208 H} for a hub page p. Substituting the Hubs Theorem into equation 7, we have the following Hubs Equation for constructing the hub vector r p from partial vectors:\nr p = (r p \u2212 r H p ) + 1 c h\u2208H (r p (h) \u2212 cx p (h)) r h \u2212 r H h \u2212 cx h (9)\nThis equation is central to the construction of hub vectors from partial vectors. The set r p (H) has size at most |H|, much smaller than the full hub vector r p , which can have up to n nonzero entries. Furthermore, the contribution of each entry r p (h) to the sum is no greater than r p (h) (and usually much smaller), so that small values of r p (h) can be omitted with minimal loss of precision (Section 6). The set S = {r p (H) | p \u2208 H} forms the hubs skeleton, giving the interrelationships among partial vectors.\nAn intuitive view of the encoding and construction suggested by the Hubs Equation ( 9) is shown in Figure 1. At the top, each partial vector (r h \u2212 r H h ), including (r p \u2212 r H p ), is depicted as a notched triangle labeled h at the tip. The triangle can be thought of as representing paths starting at h, although, more accurately, it represents the distribution of importance scores computed based on the paths, as discussed in Section 4.1. A notch in the triangle shows where the computation of a partial vector \"stopped\" at another hub page. At the center, a part r p (H) of the hubs skeleton is depicted as a tree so the \"assembly\" of the hub vector can be visualized. The hub vector is constructed by logically assembling the partial vectors using the corresponding weights in the hubs skeleton, as shown at the bottom.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Discussion", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Summary", "text": "In summary, hub vectors are building blocks for PPV's corresponding to preference vectors based on hub pages. Partial vectors, together with the hubs skeleton, are building blocks for hub vectors. Transitively, partial vectors and the hubs skeleton are building blocks for PPV's: they can be used to construct PPV's without first materializing hub vectors as an intermediate step (Section 5.4).\nNote that for preference vectors based on multiple hub pages, constructing the corresponding PPV from partial vectors directly can result in significant savings versus constructing from hub vectors, since partial vectors are shared across multiple hub vectors.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Choice of H", "text": "So far we have made no assumptions about the set of hub pages H. Not surprisingly, the choice of hub pages can have a significant impact on performance, depending on the location of hub pages within the overall graph structure. In particular, the size of partial vectors is smaller when pages in H have higher PageRank, since high-PageRank pages are on average close to other pages in terms of inverse P-distance (Section 4.1), and the size of the partial vectors is related to the inverse P-distance between hub pages and other pages according to the Hubs Theorem. Our intuition is that high-PageRank pages are generally more interesting for personalization anyway, but in cases where the intended hub pages do not have high PageRank, it may be beneficial to include some high-PageRank pages in H to improve performance. We ran experiments confirming that the size of partial vectors is much smaller using high-PageRank pages as hubs than using random pages.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Web Skeleton", "text": "The techniques used in the construction of hub vectors can be extended to enable at least approximate personalization on arbitrary preference vectors that are not necessarily based on H. Suppose we want to personalize on a page p / \u2208 H. The Hubs Equation can be used to construct r H p from partial vectors, given that we have computed r p (H). As discussed in Section 4.3, the cost of computing and storing r p (H) is orders of magnitude less than r p . Though r H p is only an approximation to r p , it may still capture significant personalization information for a properly-chosen hub set H, as r H p can be thought of as a \"projection\" of r p onto H. For example, if H contains pages from Open Directory, r H p can capture information about the broad topic of r p . Exploring the utility of the web skeleton W = {r p (H) | p \u2208 V } is an area of future work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Computation", "text": "In Section 4 we presented a way to construct hub vectors from partial vectors (r p \u2212 r H p ), for p \u2208 H, and the hubs skeleton S = {r p (H) | p \u2208 H}. We also discussed the web skeleton W = {r p (H) | p \u2208 V }. Computing these partial quantities naively using a fixed-point iteration [10] for each p would scale poorly with the number of hub pages. Here we present scalable algorithms that compute these quantities efficiently by using dynamic programming to leverage the interrelationships among them. We also show how PPV's can be constructed from partial vectors and the hubs skeleton at query time. All of our algorithms have the property that they can be stopped at any time (e.g., when resources are depleted), so that the current \"best results\" can be used as an approximation, or the computation can be resumed later for increased precision if resources permit.\nWe begin in Section 5.1 by presenting a theorem underlying all of the algorithms presented (as well as the connection between PageRank and inverse P-distance, as shown in Appendix C). In Section 5.2, we present three algorithms, based on this theorem, for computing general basis vectors. The algorithms in Section 5.2 are not meant to be deployed, but are used as foundations for the algorithms in Section 5.3 for computing partial quantities. Section 5.4 discusses the construction of PPV's from partial vectors and the hubs skeleton.", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "Decomposition Theorem", "text": "Recall the random surfer model of Section 1, instantiated for preference vector u = x p (for page p's view of the web). At each step, a surfer s teleports to page p with some probability c. If s is at p, then at the next step, s with probability 1 \u2212 c will be at a random out-neighbor of p. That is, a fraction (1 \u2212 c) 1 |O(p)| of the time, surfer s will be at any given out-neighbor of p one step after teleporting to p. This behavior is strikingly similar to the model instantiated for preference vector\nu = 1 |O(p)| |O(p)| i=1 x O i (p)\n, where surfers teleport directly to each O i (p) with equal probability\n1 |O(p)| .\nThe similarity is formalized by the following theorem.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Theorem (Decomposition). For any", "text": "p \u2208 V , r p = (1 \u2212 c) |O(p)| |O(p)| i=1 r O i (p) + cx p(10)\nThe Decomposition Theorem says that the basis vector r p for p is an average of the basis vectors r O i (p) for its out-neighbors, plus a compensation factor cx p . The proof is in Appendix B. The Decomposition Theorem gives another way to think about PPV's. It says that p's view of the web (r p ) is the average of the views of its out-neighbors, but with extra importance given to p itself. That is, pages important in p's view are either p itself, or pages important in the view of p's out-neighbors, which are themselves \"endorsed\" by p. In fact, this recursive intuition yields an equivalent way of formalizing personalized PageRank scoring: basis vectors can be defined as vectors satisfying the Decomposition Theorem.\nWhile the Decomposition Theorem identifies relationships among basis vectors, a division of the computation of a basis vector r p into related subproblems for dynamic programming is not inherent in the relationships. For example, it is possible to compute some basis vectors first and then to compute the rest using the former as solved subproblems. However, the presence of cycles in the graph makes this approach ineffective. Instead, our approach is to consider as a subproblem the computation of a vector to less precision. For example, having computed r O i (p) to a certain precision, we can use the Decomposition Theorem to combine the r O i (p) 's to compute r p to greater precision. This approach has the advantage that precision needs not be fixed in advance: the process can be stopped at any time for the current best answer.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithms for Computing Basis Vectors", "text": "We present three algorithms in the general context of computing full basis vectors. These algorithms are presented primarily to develop our algorithms for computing partial quantities, presented in Section 5.3. All three algorithms are iterative fixed-point computations that maintain a set of intermediate results\n(D k [ * ], E k [ * ]). For each p, D k [p]\nis a lower-approximation of r p on iteration k, i.e., D k [p](q) \u2264 r p (q) for all q \u2208 V . We build solutions D k [p] (k = 0, 1, 2, . . . ) that are successively better approximations to r p , and simultaneously compute the error components\nE k [p], where E k [p] is the \"projection\" of the vector (r p \u2212 D k [p]) onto the (actual) basis vectors.\nThat is, we maintain the invariant that for all k \u2265 0 and all p \u2208 V ,\nD k [p] + q\u2208V E k [p](q)r q = r p (11\n)\nThus, D k [p] is a lower-approximation of r p with error q\u2208V E k [p](q)r q = |E k [p]|.\nWe begin with D 0 [p] = 0 and E 0 [p] = x p , so that logically, the approximation is initially 0 and the error is r p . To store E k [p] and D k [p] efficiently, we can represent them in an implementation as a list of their nonzero entries. While all three algorithms have in common the use of these intermediate results, they differ in how they use the Decomposition Theorem to refine intermediate results on successive iterations.\nIt is important to note that the algorithms presented in this section and their derivatives in Section 5.3 compute vectors to arbitrary precision; they are not approximations. In practice, the precision desired may vary depending on the application. Our focus is on algorithms that are efficient and scalable with the number of hub vectors, regardless of the precision to which vectors are computed.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Basic Dynamic Programming Algorithm", "text": "In the basic dynamic programming algorithm, a new basis vector for each page p is computed on each iteration using the vectors computed for p's out-neighbors on the previous iteration, via the Decomposition Theorem. On iteration k, we derive\n(D k+1 [p], E k+1 [p]) from (D k [p], E k [p])\nusing the equations:\nD k+1 [p] = 1 \u2212 c |O(a)| |O(p)| i=1 D k [O i (p)] + cx p (12) E k+1 [p] = 1 \u2212 c |O(a)| |O(p)| i=1 E k [O i (p)](13)\nA proof of the algorithm's correctness is given in Appendix E, where the error |E k [p]| is shown to be reduced by a factor of 1 \u2212 c on each iteration.\nNote that although the E k [ * ] values help us to see the correctness of the algorithm, they are not used here in the computation of D k [ * ] and can be omitted in an implementation (although they will be used to compute partial quantities in Section 5.3). The sizes of D k [p] and E k [p] grow with the number of iterations, and in the limit they can be up to the size of r p , which is the number of pages reachable from p. Intermediate scores (D k [ * ], E k [ * ]) will likely be much larger than available main memory, and in an implementation\n(D k [ * ], E k [ * ]) could be read off disk and (D k+1 [ * ], E k+1 [ * ]\n) written to disk on each iteration. When the data for one iteration has been computed, data from the previous iteration may be deleted. Specific details of our implementation are discussed in Section 6.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Selective Expansion Algorithm", "text": "The selective expansion algorithm is essentially a version of the naive algorithm that can readily be modified to compute partial vectors, as we will see in Section 5.3.1.\nWe derive\n(D k+1 [p], E k+1 [p]\n) by \"distributing\" the error at each page q (that is, E k [p](q)) to its out-neighbors via the Decomposition Theorem. Precisely, we compute results on iteration-k using the equations:\nD k+1 [p] = D k [p] + q\u2208Q k (p) cE k [p](q)x q (14) E k+1 [p] = E k [p] \u2212 q\u2208Q k (p) E k [p](q)x q + q\u2208Q k (p) 1 \u2212 c |O(q)| |O(q)| i=1 E k [p](q)x O i (q) (15\n)\nfor a subset Q k (p) \u2286 V . If Q k (p) = V for all k, then the error is reduced by a factor of 1 \u2212 c on each iteration, as in the basic dynamic programming algorithm. However, it is often useful to choose a selected subset of V as Q k (p). For example, if Q k (p) contains the m pages q for which the error E k [p](q) is highest, then this top-m scheme limits the number of expansions and delays the growth in size of the intermediate results while still reducing much of the error. In Section 5.3.1, we will compute the hub vectors by choosing Q k (p) = H. The correctness of selective expansion is proven in Appendix F.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Repeated Squaring Algorithm", "text": "The repeated squaring algorithm is similar to the selective expansion algorithm, except that instead of extending (D k+1 [ * ], E k+1 [ * ]) one step using equations ( 14) and (15), we compute what are essentially iteration-2k results using the equations\nD 2k [p] = D k [p] + q\u2208Q k (p) E k [p](q)D k [q] (16\n)\nE 2k [p] = E k [p] \u2212 q\u2208Q k (p) E k [p](q)x q + q\u2208Q k (p) E k [p](q)E k [q] (17\n)\nwhere Q k (p) \u2286 V . For now we can assume that Q k (p) = V for all p; we will set Q k (p) = H to compute the hubs skeleton in Section 5.3.2. The correctness of these equations is proven in Appendix G, where it is shown that repeated squaring reduces the error much faster than the basic dynamic programming or selective expansion algorithms. If Q k (p) = V , the error is squared on each iteration, as equation ( 17) reduces to:\nE 2k [p] = q\u2208V E k [p](q)E k [q] (18\n)\nAs an alternative to taking Q k (p) = V , we can also use the top-m scheme of Section 5.2.2.\nNote that while all three algorithms presented can be used to compute the set of all basis vectors, they differ in their requirements on the computation of other vectors when computing r p : the basic dynamic programming algorithm requires the vectors of out-neighbors of p to be computed as well, repeated squaring requires results\n(D k [q], E k [q]\n) to be computed for q such that E k [p](q) > 0, and selective expansion computes r p independently.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Computing Partial Quantities", "text": "In Section 5.2 we presented iterative algorithms for computing full basis vectors to arbitrary precision. Here we present modifications to these algorithms to compute the partial quantities:\n\u2022 Partial vectors (r p \u2212 r H p ), p \u2208 H. \u2022 The hubs skeleton S = {r p (H) | p \u2208 H} (which can be computed more efficiently by itself than as part of the entire web skeleton).\n\u2022 The web skeleton\nW = {r p (H) | p \u2208 V }.\nEach partial quantity can be computed in time no greater than its size, which is far less than the size of the hub vectors.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Partial Vectors", "text": "Partial vectors can be computed using a simple specialization of the selective expansion algorithm (Section 5.2.2): we take Q 0 (p) = V and Q k (p) = V \u2212 H for k > 0, for all p \u2208 V . That is, we never \"expand\" hub pages after the first step, so tours passing through a hub page H are never considered. Under this choice of\nQ k (p), D k [p] + cE k [p] converges to (r p \u2212 r H p ) for all p \u2208 V .\nOf course, only the intermediate results\n(D k [p], E k [p]\n) for p \u2208 H should be computed. A proof is presented in Appendix H. This algorithm makes it clear why using high-PageRank pages as hub pages improves performance: from a page p we expect to reach a high-PageRank page q sooner than a random page, so the expansion from p will stop sooner and result in a shorter partial vector.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Hubs Skeleton", "text": "While the hubs skeleton is a subset of the complete web skeleton and can be computed as such using the technique to be presented in Section 5.3.3, it can be computed much faster by itself if we are not interested in the entire web skeleton, or if higher precision is desired for the hubs skeleton than can be computed for the entire web skeleton.\nWe use a specialization of the repeated squaring algorithm (Section 5.2.3) to compute the hubs skeleton, using the intermediate results from the computation of partial vectors. Suppose\n(D k [p], E k [p]\n), for k \u2265 1, have been computed by the algorithm of Section 5.3.1, so that q / \u2208H E k [p](q) < , for some error . We apply the repeated squaring algorithm on these results using Q k (p) = H for all successive iterations. As shown in Appendix I, after i iterations of repeated squaring, the total error |E i [p]| is bounded by (1 \u2212 c) 2 i + /c. Thus, by varying k and i, r p (H) can be computed to arbitrary precision.\nNotice that only the intermediate results \n(D k [h], E k [h]) for h \u2208 H", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Web Skeleton", "text": "To compute the entire web skeleton, we modify the basic dynamic programming algorithm (Section 5.2.1) to compute only the hub scores r p (H), with corresponding savings in time and memory usage. We restrict the computation by eliminating entries q / \u2208 H from the intermediate results\n(D k [p], E k [p]\n), similar to the technique used in computing the hubs skeleton.\nThe justification for this modification is that the hub score D k+1 [p](h) is affected only by the hub scores D k [ * ](h) of the previous iteration, so that D k+1 [p](h) in the modified algorithm is equal to that in the basic algorithm. Since |H| is likely to be orders of magnitude less than n, the size of the intermediate results is reduced significantly.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Construction of PPV's", "text": "Finally, let us see how a PPV for preference vector u can be constructed directly from partial vectors and the hubs skeleton using the Hubs Equation. (Construction of a single hub vector is a specialization of the algorithm outlined here.) Let u = \u03b1 1 p 1 + \u2022 \u2022 \u2022 + \u03b1 z p z be a preference vector, where\np i \u2208 H for 1 \u2264 i \u2264 z. Let Q \u2286 H, and let r u (h) = z i=1 \u03b1 i (r p i (h) \u2212 cx p i (h))(19)\nwhich can be computed from the hubs skeleton. Then the PPV v for u can be constructed as\nv = z i=1 \u03b1 i (r p i \u2212 r H p i ) + 1 c h\u2208Q ru(h)>0 r u (h) (r h \u2212 r H h ) \u2212 cx h (20)\nBoth the terms (r p i \u2212 r H p i ) and (r h \u2212 r H h ) are partial vectors, which we assume have been precomputed. The term cx h represents a simple subtraction from (r h \u2212 r H h ). If Q = H, then (20) represents a full construction of v. However, for some applications, it may suffice to use only parts of the hubs skeleton to compute v to less precision. For example, we can take Q to be the m hubs h for which r u (h) is highest. Experimentation with this scheme is discussed in Section 6.3. Alternatively, the result can be improved incrementally (e.g., as time permits) by using a small subset Q each time and accumulating the results.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We performed experiments using real web data from Stanford's WebBase [6], a crawl of the web containing 120 million pages. Since the iterative computation of PageRank is unaffected by leaf pages (i.e., those with no out-neighbors), they can be removed from the graph and added back in after the computation [10]. After removing leaf pages, the graph consisted of 80 million pages Both the web graph and the intermediate results\n(D k [ * ], E k [ * ]\n) were too large to fit in main memory, and a partitioning strategy, based on that presented in [4], was used to divide the computation into portions that can be carried out in memory. Specifically, the set of pages V was partitioned into k arbitrary sets P 1 , . . . , P k of equal size (k = 10 in our experiments). The web graph, represented as an edge-list E, is partitioned into k chunks  \nE i (1 \u2264 i \u2264 k),", "publication_ref": ["b3", "b7", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Computing Partial Vectors", "text": "For comparison, we computed both (full) hub vectors and partial vectors for various sizes of H, using the selective expansion algorithm with Q k (p) = V (full hub vectors) and Q k (p) = V \u2212 H (partial vectors). As discussed in Section 4.4.2, we found the partial vectors approach to be much more effective when H contains high-PageRank pages rather than random pages. In our experiments H ranged from the top 1000 to top 100, 000 pages with the highest PageRank. The constant c was set to 0.15.\nTo evaluate the performance and scalability of our strategy independently of implementation and platform, we focus on the size of the results rather than computation time, which is linear in the size of the results. Because of the number of trials we had to perform and limitations on resources, we computed results only up to 6 iterations, for |H| up to 100, 000. Figure 2 plots the average size of (full) hub vectors and partial vectors (recall that size is the number of nonzero entries), as computed after 6 iterations of the selective expansion algorithm, which for computing full hub vectors is equivalent to the basic dynamic programming algorithm. Note that the x-axis plots |H| in logarithmic scale.\nExperiments were run using a 1.4 gigahertz CPU on a machine with 3.5 gigabytes of memory. For |H| = 50, 000, the computation of full hub vectors took about 2.8 seconds per vector, and about 0.33 seconds for each partial vector. We were unable to compute full hub vectors for |H| = 100, 000 due to the time required, although the average vector size is expected not to vary significantly with |H| for full hub vectors. In Figure 2 we see that the reduction in size from using our technique becomes more significant as |H| increases, suggesting that our technique scales well with |H|.", "publication_ref": [], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Computing the Hubs Skeleton", "text": "We computed the hubs skeleton for |H| = 10, 000 by running the selective expansion algorithm for 6 iterations using Q k (p) = H, and then running the repeated squaring algorithm for 10 iterations (Section 5.3.2), where Q k (p) is chosen to be the top 50 entries under the top-m scheme (Section 5.2.2). The average size of the hubs skeleton is 9021 entries. Each iteration of the repeated squaring algorithm took about an hour, a cost that depends only on |H| and is constant with respect to the precision to which the partial vectors are computed.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Constructing Hub Vectors from Partial Vectors", "text": "Next we measured the construction of (full) hub vectors from partial vectors and the hubs skeleton. Note that in practice we may construct PPV's directly from partial vectors, as discussed in Section 5.4. However, performance of the construction would depend heavily on the user's preference vector. We consider hub vector computation because it better measures the performance benefits of our partial vectors approach.\nAs suggested in Section 4.3, the precision of the hub vectors constructed from partial vectors can be varied at query time according to application and performance demands. That is, instead of using the entire set r p (H) in the construction of r p , we can use only the highest m entries, for m \u2264 |H|. Figure 3 plots the average size and time required to construct a full hub vector from partial vectors in memory versus m, for |H| = 10, 000. Results are averaged over 50 randomlychosen hub vectors. Note that the x-axis is in logarithmic scale.\nRecall from Section 6.1 that the partial vectors from which the hubs vector is constructed were computed using 6 iterations, limiting the precision. Thus, the error values in Figure 3 are roughly 16% (ranging from 0.166 for m = 100 to 0.163 for m = 10, 000). Nonetheless, this error is much smaller than that of the iteration-6 full hub vectors computed in Section 6.1, which have error (1 \u2212 c) 6 = 38%. Note, however, that the size of a vector is a better indicator of precision than the magnitude, since we are usually most interested in the number of pages with nonzero entries in the distribution vector. An iteration-6 full hub vector (from Section 6.1) for page p contains nonzero entries for pages at most 6 links away from p, 93, 993 pages on average. In contrast, from Figure 3 we see that a hub vector containing 14 million nonzero entries can be constructed from partial vectors in 6 seconds.", "publication_ref": [], "figure_ref": ["fig_4", "fig_4", "fig_4"], "table_ref": []}, {"heading": "Related Work", "text": "The use of personalized PageRank to enable personalized web search was first proposed in [10], where it was suggested as a modification of the global PageRank algorithm, which computes a universal notion of importance. The computation of (personalized) PageRank scores was not addressed beyond the naive algorithm.\nIn [5], personalized PageRank scores were used to enable \"topic-sensitive\" web search. Specifically, precomputed hub vectors corresponding to broad categories in Open Directory were used to bias importance scores, where the vectors and weights were selected according to the text query. Experiments in [5] concluded that the use of personalized PageRank scores can improve web search, but the number of hub vectors used was limited to 16 due to the computational requirements, which were not addressed in that work. Scaling the number of hub pages beyond 16 for finer-grained personalization is a direct application of our work.\nAnother technique for computing web-page importance, HITS, was presented in [8]. In HITS, an iterative computation similar in spirit to PageRank is applied at query time on a subgraph consisting of pages matching a text query and those \"nearby\". Personalizing based on user-specified web pages (and their linkage structure in the web graph) is not addressed by HITS. Moreover, the number of pages in the subgraphs used by HITS (order of thousands) is much smaller than that we consider in this paper (order of millions), and the computation from scratch at query time makes the HITS approach difficult to scale.\nAnother algorithm that uses query-dependent importance scores to improve upon a global version of importance was presented in [11]. Like HITS, it first restricts the computation to a subgraph derived from text matching. (Personalizing based on user-specified web pages is not addressed.) Unlike HITS, [11] suggested that importance scores be precomputed offline for every possible text query, but the enormous number of possibilities makes this approach difficult to scale.\nThe concept of using \"hub nodes\" in a graph to enable partial computation of solutions to the shortest-path problem was used in [3] in the context of database search. That work deals with searches within databases, and on a scale far smaller than that of the web.\nSome system aspects of (global) PageRank computation were addressed in [4]. The diskbased data-partitioning strategy used in the implementation of our algorithm is adopted from that presented therein.\nFinally, the concept of inverse P-distance used in this paper is based on the concept of expectedf distance introduced in [7], where it was presented as an intuitive model for a similarity measure in graph structures.", "publication_ref": ["b7", "b2", "b2", "b5", "b8", "b8", "b0", "b1", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Summary", "text": "We have addressed the problem of scaling personalized web search:\n\u2022 We started by identifying a linear relationship that allows personalized PageRank vectors to be expressed as a linear combination of basis vectors. Personalized vectors corresponding to arbitrary preference sets drawn from a hub set H can be constructed quickly from the set of precomputed basis hub vectors, one for each hub h \u2208 H.\n\u2022 We laid the mathematical foundations for constructing hub vectors efficiently by relating personalized PageRank scores to inverse P-distances, an intuitive notion of distance in arbitrary directed graphs. We used this notion of distance to identify interrelationships among basis vectors.\n\u2022 We presented a method of encoding hub vectors as partial vectors and the hubs skeleton.\nRedundancy is minimized under this representation: each partial vector for a hub page p represents the part of p's hub vector unique to itself, while the skeleton specifies how partial vectors are assembled into full vectors.\n\u2022 We presented algorithms for computing basis vectors, and showed how they can be modified to compute partial vectors and the hubs skeleton efficiently.\n\u2022 We ran experiments on real web data showing the effectiveness of our approach. Results showed that our strategy results in significant resource reduction over full vectors, and scales well with |H|, the degree of personalization.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Using the identity", "text": "A p = A i \u2212 cU i + cU p we have:\nA p v p = 1 \u2212 c k k i=1 (A i \u2212 cU i + cU p )r i + cA p x p = 1 \u2212 c k k i=1 A i r i \u2212 1 \u2212 c k c k i=1 U i r i + 1 \u2212 c k c k i=1 U p r i + cA p x p = 1 \u2212 c k k i=1 r i \u2212 1 \u2212 c k c k i=1 x i + 1 \u2212 c k c k i=1 x p + cA p x p = 1 \u2212 c k k i=1 r i \u2212 1 \u2212 c k c k i=1 x i + (1 \u2212 c)cx p + c((1 \u2212 c)A + cU p )x p = 1 \u2212 c k k i=1 r i \u2212 1 \u2212 c k c k i=1 x i + (1 \u2212 c)cx p + (1 \u2212 c)cAx p + c 2 x p = 1 \u2212 c k k i=1 r i + (1 \u2212 c)cx p + c 2 x p + (1 \u2212 c)c Ax p \u2212 1 k k i=1 x i = 1 \u2212 c k k i=1 r i + (1 \u2212 c)cx p + c 2 x p = 1 \u2212 c k k i=1 r i + cx p = v p", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Inverse P-distance C.1 Relation to Personalized PageRank", "text": "The relationship between inverse P-distances and personalized PageRank scores is given by the following theorem.\nTheorem. For all p, q \u2208 V , r p (q) = r p (q)\nProof: Writing the Decomposition Theorem in scalar form for page p, we get a set of n equations, one for each q \u2208 V , of the form\nr p (q) = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 (1 \u2212 c) |O(p)| i=1 r O i (p) (q) (if p = q) (1 \u2212 c) |O(p)| i=1 r O i (p) (q) + c (if p = q)\nLet us now fix q, and consider the set of n equations, one for each p \u2208 V , in the above form. By a proof very similar to that given in [7], it can be shown these equations have a unique solution, so we need only show that r p (q) satisfies these equations as well.\nClearly, if there is no path from p to q, then r p (q) = r p (q) = 0, so suppose q can be reached from p. Consider the tours t starting at p and ending at q in which the first step is to the outneighbor O z (p). If p = q, there is a one-to-one correspondence between such t and tours t from O z (p) to q: for each t we may derive a corresponding t by appending the edge p, O z (p) at the beginning. Let T be the bijection that takes each t to the corresponding t. If the length of t is l, then the length of t = T (t ) is l + 1. Moreover, the probability of traveling t is\nP [t] = 1 |O(p)| P [t ].\nWe can now split the sum in (5) according to the first step of the tour t to write\nr p (q) = |O(p)| z=1 t : Oz(p) q P [T (t )]c(1 \u2212 c) l(T (t )) = 1 \u2212 c |O(p)| |O(p)| z=1 t : Oz(p) b P [t ]c(1 \u2212 c) l(t) = 1 \u2212 c |O(p)| |O(p)| i=1 r p (q)\nIf p = q, then the same correspondence holds except that there is an extra tour t from p to q = p which does not correspond to any tour t starting from an O z (p): the zero length tour t = p . The length of this tour is 0, and in this case P ", "publication_ref": ["b4", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "C.2 Loop Factor", "text": "The use of inverse P-distances yields further insight into the fairness of PageRank scoring. Since the global PageRank for a page q is just the uniform sum n p=1 r p (q)/n, we see that the PageRank of a page q is the average, over all pages p, of the inverse P-distance from p to q. The intuition is that high-PageRank pages are on average \"close\" to other pages under this distance measure. However, note that the summation in (5) is taken over tours that may touch q multiple times. The effect is that a page q can influence its own PageRank (by a factor less than 1/c) simply by changing its out-links. In particular, if a page q with PageRank PR(q) links to every page p for which there is a path to q (as are logically created for pages without out-links in [5,10]), then its PageRank would be a factor c + (1 \u2212 c)PR(q) less than if it had linked to itself and no other page. This \"loop factor\" can be quantified as r q (q): under the definition that tours t from p to q may touch q only once, r p (q) can be written as r p (q) = r q (q)\nt:p q P [t](1 \u2212 c) l(t)\nwhere the summation is independent of q's out-links. This is the expected-f distance [7] from p to q, for f (x) = (1 \u2212 c) x . Thus eliminating the loop factor (dividing by r q (q) to get the expected-f distance) may result in a fairer scoring.", "publication_ref": ["b2", "b7", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "D Proof: Hubs Theorem Theorem (Hubs). For any", "text": "p \u2208 V , H \u2286 V , a) r H p = 1 c h\u2208H (r p (h) \u2212 cx p (h)) r h \u2212 r H h \u2212 cx h b) r H p = 1 c h\u2208H r p (h) \u2212 r H p (h) \u2212 cx p (h) (r h \u2212 cx h )", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of (a):", "text": "The idea is to separate tours t going through H into two parts, everything up to the last occurrence of a page h \u2208 H, and the rest. Let \u03b2(t), for tours t : p H q, denote the beginning of t to the last occurrence of a page h \u2208 H which t passes through, so \u03b2(t) = p, . . . , h . Let \u03b3(t) be the rest, so \u03b3(t) = h, . . . , q . Let \u03c0(t) = P [t]c(1 \u2212 c) l(t) for short. Let s(t) be the set of pages that t passes through, so that r H p (q) can be written as\nr H p (q) = t:p q s(t)\u2229H =\u2205 P [t]c(1 \u2212 c) l(t)\nLet us first partition the summation in (6) according to \u03b2(t):\nr H p (q) = t 1 |t 1 =\u03b2(t) t:p H q t:p H q \u03b2(t)=t 1 P [t]c(1 \u2212 c) l(t)(22)\nFor each t, \u03b2(t) is itself a tour t : p h; conversely, each t : p h is a \u03b2(t) for some t, with the exception of the zero-length tour t = p in the special case where p \u2208 H. Thus we can group the tours t by h and \u03b2(t) ending at h to rewrite (22) as:\nr H p (q) = h\u2208H t 1 :p h l(t 1 )>0 t:p H q \u03b2(t)=t 1 P [t]c(1 \u2212 c) l(t)\nBut P [t] = P [\u03b2(t)]P [\u03b3(t)], and l(t) = l(\u03b2(t)) + l(\u03b3(t)), so r H p (q) = h\u2208H t 1 :p h l(t 1 )>0\nt:p H q \u03b2(t)=t 1 P [\u03b2(t)]P [\u03b3(t)]c(1 \u2212 c) l(\u03b2(t))+l(\u03b3(t))\n= 1 c h\u2208H t 1 :a h l(t 1 )>0 \u03c0(t 1 )\nt:p H q \u03b2(t)=t 1 \u03c0(\u03b3(t))\nThere is a canonical bijection \u03b3 t 1 between tours t : p H q with \u03b2(t) = t 1 and tours t : h q which do not pass through H (for which s(t ) \u2229 H = \u2205), with the exception of the zero-length tour q when q \u2208 H. That is, \u03b3 t 1 (t) = \u03b3(t) = t , so we can write each tour t as t = \u03b3 \u22121 t 1 (t ). Replacing \u03b3(t) in the previous equation with \u03b3(t) = \u03b3(\u03b3 \u22121 t 1 (t )) = t and accounting for the possible zerolength tour, we have\nr H p (q) = 1 c h\u2208H t 1 :p h l(t 1 )>0 \u03c0(t 1 ) \uf8eb \uf8ec \uf8ec \uf8ed t :h q s(t )\u2229H=\u2205 \u03c0(t ) \u2212 x h (q) t = q \u03c0(t ) \uf8f6 \uf8f7 \uf8f7 \uf8f8 = 1 c h\u2208H t 1 :p h l(t 1 )>0 \u03c0(t 1 ) \uf8eb \uf8ec \uf8ec \uf8ed t :h q s(t )\u2229H=\u2205 \u03c0(t ) \u2212 x h (q)c \uf8f6 \uf8f7 \uf8f7 \uf8f8\nBut the set of tours t from h to q which do not pass through H is the set of tours from h to q minus the set of tours from h to q which pass through H. Thus,\nr H p (q) = 1 c h\u2208H t 1 :p h l(t 1 )>0 \u03c0(t 1 ) t :h q \u03c0(t ) \u2212 t :h H q \u03c0(t ) \u2212 cx h (q) = 1 c h\u2208H t 1 :p h l(t 1 )>0 \u03c0(t 1 ) r h (q) \u2212 r H h (q) \u2212 cx h (q)\nFinally,\nt 1 :p h l(t 1 )>0 \u03c0(t 1 ) = r p (h) \u2212 cx p (h)\nwhere cx p (h) accounts for the possible tour t 1 = p when p = h, for which P [t 1 ]c(1 \u2212 c) l(t 1 ) = c, and we have\nr H p (q) = 1 c h\u2208H (r p (h) \u2212 cx p (h)) r h (q) \u2212 r H h (q) \u2212 cx h (q)\nThis equation written in vector form is the Hubs Theorem (a).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of (b):", "text": "The idea is to separate tours t going through H differently: everything up to the first (instead of last) occurrence of a page h \u2208 H, and the rest. Let \u03b2(t), for tours t : p H q, denote the beginning of t to the first occurrence of a page h \u2208 H which t passes through, so \u03b2(t) = p, . . . , h . Let \u03b3(t) be the rest, so \u03b3(t) = h, . . . , q .\nLet us first partition the summation in (6) \nP [t]c(1 \u2212 c) l(t)(23)\nFor each t, \u03b3(t) is itself a tour t : h q; conversely, each t : h q is a \u03b3(t) for some t, with the exception of the zero-length tour t = q in the special case where q \u2208 H. Thus we can group the tours t by h and \u03b3(t) beginning at h to rewrite (23) as:\nr H p (q) = h\u2208H t 2 :h q l(t 2 )>0 t:p H q \u03b3(t)=t 2 P [t]c(1 \u2212 c) l(t) But P [t] = P [\u03b2(t)]P [\u03b3(t)], and l(t) = l(\u03b2(t)) + l(\u03b3(t)), so r H p (q) = h\u2208H t 2 :h q l(t 2 )>0 t:p H q \u03b3(t)=t 2 P [\u03b2(t)]P [\u03b3(t)]c(1 \u2212 c) l(\u03b2(t))+l(\u03b3(t)) = 1 c h\u2208H t 2 :h q l(t 2 )>0 \u03c0(t 2 ) t:p H q \u03b3(t)=t 2 \u03c0(\u03b2(t))\nThere is a canonical bijection \u03b2 t 2 between tours t : p H q with \u03b3(t) = t 2 and tours t : a h which do not pass through H (for which s(t ) \u2229 H = \u2205), with the exception of the zero-length tour p when p \u2208 H. That is, \u03b2 t 2 (t) = \u03b2(t) = t , so we can write each tour t as t = \u03b2 \u22121 t 2 (t ). Replacing \u03b2(t) in the previous equation with \u03b2(t) = \u03b2(\u03b2 \u22121 t 1 (t )) = t and accounting for the possible zero-length tour, we have\nr H p (q) = 1 c h\u2208H t 2 :h q l(t 2 )>0 \u03c0(t 2 ) \uf8eb \uf8ec \uf8ec \uf8ed t :p h s(t )\u2229H=\u2205 \u03c0(t ) \u2212 x p (h) t = p \u03c0(t ) \uf8f6 \uf8f7 \uf8f7 \uf8f8 = 1 c h\u2208H t 2 :h q l(t 2 )>0 \u03c0(t 2 ) \uf8eb \uf8ec \uf8ec \uf8ed t :p h s(t )\u2229H=\u2205 \u03c0(t ) \u2212 x p (h)c \uf8f6 \uf8f7 \uf8f7 \uf8f8\nBut the set of tours t from p to h which do not pass through H is the set of tours from p to h minus the set of tours from p to h which pass through H. Thus,\nr H p (q) = 1 c h\u2208H t 2 :h q l(t 2 )>0 \u03c0(t 2 ) t :p h \u03c0(t ) \u2212 t :p H h \u03c0(t ) \u2212 cx p (h) = 1 c h\u2208H t 2 :h q l(t 2 )>0 \u03c0(t 2 ) r p (h) \u2212 r H p (h) \u2212 cx p (h)\nFinally,\nt 2 :h q l(t 2 )>0 \u03c0(t 2 ) = r h (q) \u2212 cx h (q)\nwhere cx h (q) accounts for the possible tour t 2 = q when q = h, for which P [t 2 ]c(1 \u2212 c) l(t 2 ) = c, and we have\nr H p (q) = 1 c h\u2208H r p (h) \u2212 r H p (h) \u2212 cx p (h) (r h (q) \u2212 cx h (q))\nThis equation written in vector form is the Hubs Theorem (b).", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "E Proof: Basic Dynamic Programming Algorithm", "text": "To prove correctness of the basic dynamic programming algorithm, we need to show that for all k \u2265 0 and p \u2208 V , D k [p] + q\u2208V E k+1 [p](q)r q = r p , and that the sequence {E k [p]} converges to 0 as k tends towards infinity, which implies that D k [p] converges to r p . In particular,\n|E k [p]| = (1 \u2212 c) k .\nThe proof is by induction on k. The case for k = 0 is obvious, so suppose the claim is true for k, for some k \u2265 0. First we show that\nD k+1 [p] + q\u2208V E k+1 [p](q)r q = r p : D k+1 [p] + q\u2208V E k+1 [p](q)r q = 1 \u2212 c |O(p)| |O(p)| i=1 D k [O i (p)] + cx p + q\u2208V 1 \u2212 c |O(p)| |O(p)| i=1 E k [O i (p)](q)r q = cx p + 1 \u2212 c |O(p)| |O(p)| i=1 D k [O i (p)] + q\u2208V E k [O i (p)](q)r q = 1 \u2212 c |O(p)| |O(p)| i=1 r O i (p) + cx p = r p\nwhere the last step is justified by the Decomposition Theorem. Now we show that\n|E k+1 [p]| = (1 \u2212 c) k+1 : |E k+1 [p]| = 1 \u2212 c |O(p)| |O(p)| i=1 E k [O i (p)] = 1 \u2212 c |O(p)| |O(p)| i=1 |E k [O i (p)]| = 1 \u2212 c |O(p)| |O(p)| i=1 (1 \u2212 c) k = 1 \u2212 c |O(p)| |O(p)|(1 \u2212 c) k = (1 \u2212 c) k+1", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F Proof: Selective Expansion Algorithm", "text": "As in the proof of the basic dynamic programming algorithm, we first show that\nD k+1 [p] + q\u2208V E k+1 [p](q)r q = r p for an arbitrary Q k (p) \u2286 V : D k+1 [p] + q\u2208V E k+1 [p](q)r q = \uf8eb \uf8ed D k [p] + q\u2208Q k (p) cE k [p](q)x q \uf8f6 \uf8f8 + q \u2208V E k [p] \u2212 q\u2208Q k (p) E k [p](q)x q + q\u2208Q k (p) 1 \u2212 c |O(q)| |O(q)| i=1 E k [p](q)x O i (q) (q )r q = D k [p] + q \u2208V E k [p](q )r q + q\u2208Q k (p) cE k [p](q)x q \u2212 q \u2208V q\u2208Q k (p) E k [p](q)x q (q )r q + q \u2208V q\u2208Q k (p) 1 \u2212 c |O(q)| |O(q)| i=1 E k [p](q)x O i (q) (q )r q\nBy the inductive hypothesis,\nD k [p] + q \u2208V E k [p](q )r q = r p\nso we need only show that the latter terms cancel. Since x q (q ) = 1 if q = q and 0 otherwise, and similarly for x O i (q) (q ), we have\nq \u2208V q\u2208Q k (p) E k [p](q)x q (q )r q = q\u2208Q k (p) E k [p](q)r q and q \u2208V q\u2208Q k (p) 1 \u2212 c |O(q)| |O(q)| i=1 E k [p](q)x O i (q) (q )r q = q\u2208Q k (p) 1 \u2212 c |O(q)| |O(q)| i=1 E k [p](q)r O i (q)\nBy the Decomposition Theorem, \ncE k [p](q)x q + 1 \u2212 c |O(q)| |O(q)| i=1 E k [p](q)r O i (q) = E k [p](q)r b for all q \u2208 Q k (p),\nE k [p](q) = max{E k [p](q) | q \u2208 V } ensures that the error tends towards 0. In particular, such is the case if Q k (p) = V or Q k (p) is the top m > 0 pages q with the highest E k [p](q).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G Proof: Repeated Squaring Algorithm", "text": "To verify the correctness of the repeated squaring algorithm, we show that\nD 2k [p] + q\u2208Q k (p) E 2k [p](q)r q = r q for an arbitrary Q k (p) \u2286 V : D 2k [p] + q\u2208V E 2k [p](q)r q = D k [p] + q\u2208Q k (p) E k [p](q)D k [q]+ q \u2208V \uf8eb \uf8ed E k [p](q ) \u2212 q\u2208Q k (p) E k [p](q)x q (q ) + q\u2208Q k (p) E k [p](q)E k [q](q ) \uf8f6 \uf8f8 r q = D k [p] + q \u2208V E k [p](q )r q + q\u2208Q k (p) E k [p](q) D k [q] \u2212 q \u2208V x q (q )r q + q \u2208V E k [q](q )r q = r p + q\u2208Q k (p) E k [p](q) D k [q] + q \u2208V E k [q](q )r q \u2212 r q = r p + q\u2208Q k (p) [0] = r p\nAs in the proof of the selective expansion algorithm, the error tends towards 0 if Q k (p) contains the top m > 0 pages q with the highest E k [p](q). If Q k (p) = V , the error is squared on each iteration, for if |E k [ * ]| = , using equation 18 we have:\n|E 2k [p]| = q\u2208V E k [p](q)E k [q] = q\u2208V E k [p](q)|E k [q]| = q\u2208V E k [p](q) = |E k [p]| = 2\nClearly, for all but the first two iterations, repeated squaring reduces error much faster than the decay factor of 1\u2212c (for both the basic dynamic programming and selective expansion algorithms) when Q k (p) = V .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "H Proof: Computation of Partial Vectors", "text": "We first show that the following hold for all k \u2265 1 and p, q \u2208 V : \nP [t](1 \u2212 c) l(t) (if q \u2208 H)(25)\nwhere s(t) is the set of pages appearing on t other than at the endpoints (i.e., pages which t passes through), and s (t) is the set of pages appearing on t other than at the beginning. Consider the case for k = 1 (recall that all pages are expanded on iteration 0). The only tours in (24) are the zero-length tours t = p when p = q (which pass through no hubs), for which P where we have replaced s(t ) in the summation with s (t ), since q / \u2208 H. We want to show that this is equal to t:p q l(t)=k+1 s(t)\u2229H=\u2205\nP [t](1 \u2212 c) l(t)(27)\nConsider the set of tours t : p q, with l(t) = k + 1 and s(t) \u2229 H = \u2205, for which the last step is from q \u2208 (V \u2212 H) \u2229 I(q) to q. There is a one-to-one correspondence between such t and tours t : p q of length k with s (t) \u2229 H = \u2205: for each t we may derive a corresponding t by appending the edge q , q at the end. Let T be the bijection that takes each t to the corresponding t. If the length of t is l, then the length of t = T (t ) is l + 1. Moreover, the probability of traveling t is P [t] = 1 |O(q  \n)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgment", "text": "The authors thank Taher Haveliwala for many useful discussions and extensive help with implementation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "APPENDIX", "text": "A Proof: Linearity Theorem Theorem (Linearity). For any preference vectors u 1 and u 2 , if v 1 and v 2 are the two corresponding PPV's, then for any constants \u03b1 1 , \u03b1 2 \u2265 0 such that \u03b1 1 + \u03b1 2 = 1,\nProof: First we rewrite equation (1) in an equivalent form. For a given preference vector u, we define the derived matrix A u as\nwhere U is the n \u00d7 n matrix with U ij = u i for all i, j. If we require that |v| = 1, we can write equation (1) as\nWithout loss of generality, let the out-neighbors of p be 1, . . . , k. Let A p be the derived matrix corresponding to x p , and let A 1 , . . . , A k be the derived matrices for u = x 1 , . . . , x k , respectively. Let U p and U 1 , . . . , U k be the corresponding U 's in equation (21).\nLet\nClearly, |v p | = 1. We need to show that A p v p = v p , in which case v p = r p , since PPV's are unique (Section 1). First we have that:", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Proximity search in databases", "journal": "", "year": "1998-08", "authors": "Roy Goldman; Narayanan Shivakumar; Suresh Venkatasubramanian; Hector Garcia-Molina"}, {"ref_id": "b1", "title": "Efficient computation of PageRank", "journal": "", "year": "1999", "authors": "H Taher;  Haveliwala"}, {"ref_id": "b2", "title": "Topic-sensitive PageRank", "journal": "", "year": "2002-05", "authors": "H Taher;  Haveliwala"}, {"ref_id": "b3", "title": "WebBase: A repository of web pages", "journal": "", "year": "2000-05", "authors": "Jun Hirai; Sriram Raghavan; Andreas Paepcke; Hector Garcia-Molina"}, {"ref_id": "b4", "title": "SimRank: A measure of structural-context similarity", "journal": "", "year": "2002-07", "authors": "Glen Jeh; Jennifer Widom"}, {"ref_id": "b5", "title": "Authoritative sources in a hyperlinked environment", "journal": "", "year": "1998-01", "authors": "Jon M Kleinberg"}, {"ref_id": "b6", "title": "Randomized Algorithms", "journal": "Cambridge University Press", "year": "1995", "authors": "Rajeev Motwani; Prabhakar Raghavan"}, {"ref_id": "b7", "title": "The PageRank citation ranking: Bringing order to the Web", "journal": "", "year": "1998", "authors": "Lawrence Page; Sergey Brin; Rajeev Motwani; Terry Winograd"}, {"ref_id": "b8", "title": "The intelligent surfer: Probabilistic combination of link and content information in PageRank", "journal": "", "year": "2002-12", "authors": "Matthew Richardson; Pedro Domingos"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Intuitive view of the construction of hub vectors from partial vectors and the hubs skeleton.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "are ever needed to update scores for D k [p], and of the former, only the entries D k [h](q), E k [h](q), for q \u2208 H, are used to compute D k [p](q). Since we are only interested in the hub scores D k [p](q), we can simply drop all non-hub entries from the intermediate results. The running time and storage would then depend only on the size of r p (H) and not on the length of the entire hub vectors r p . If the restricted intermediate results fit in main memory, it is possible to defer the computation of the hubs skeleton to query time.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "where E i contains all edges p, q for which p \u2208 P i . Intermediate results D k [p] and E k [p] were represented together as a list L k [p] = (q 1 , d 1 , e 1 ), (q 2 , d 2 , e 2 ), . . . where D k [p](q z ) = d z and E k [p](q z ) = e z , for z = 1, 2, . . . . Only pages q z for which either d z > 0 or e z > 0 were included. The set of intermediate results L k [ * ] was partitioned into k 2 chunks L i,j k [ * ], so that L i,j k [p] contains triples (q z , d z , e z ) of L k [p] for which p \u2208 P i and q z \u2208 P j . In each of the algorithms for computing partial quantities, only a", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: Average Vector Size vs. Number of Hubs", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: Construction Time and Size vs. Hubs Skeleton Portion (m)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "[t]c(1 \u2212 c) l(t) = c. Thus r p (q) = 1 \u2212 c |O(p)| |O(p)| i=1 r O i (p) (q) + c when p = q.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "which shows that the terms indeed cancel. Since |D k [p]| increases by c q\u2208Q k (p) E k [p](q) each iteration, the error decreases by c q\u2208Q k (p) E k [p](q) each iteration. Thus, any choice of Q k (p) containing a maximal page q such that", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "[t]c(1 \u2212 c) l(t) = c = D 1 [p](q). The only tours in (25) are t = p, q when q is an out-neighbor of p, for whichP [t](1 \u2212 c) l(t) = 1\u2212c |O(p)| = E k [p](q). Now suppose for induction that equations (24) and (25) hold for some k \u2265 1. By equation (14) with Q k (p) = V \u2212 H, the difference between D k+1 [p](q) and D k [p](q), for q / \u2208 H, is D k+1 [p](q) \u2212 D k [p](q) = cE 1 [p](q). By the inductive hypothesis, this difference can be written ascE k [p](q) = t:p q l(t)=k s(t)\u2229H=\u2205 P [t]c(1 \u2212 c) l(t) Since q / \u2208 H, the restriction s(t) \u2229 H = \u2205 is equivalent to s (t) \u2229 H = \u2205, so that D k+1 [p](q) = D k [p](q) + cE k [p](q) = t:p q l(t)<k s (t)\u2229H=\u2205 P [t]c(1 \u2212 c) l(t) + t:p q l(t)=k s (t)\u2229H=\u2205 P [t]c(1 \u2212 c) l(t) = t:p q l(t)<k+1 s (t)\u2229H=\u2205 P [t]c(1 \u2212 c) l(t) If q \u2208 H, D k+1 [p](q) = D k [p](q) = t:p q l(t)<k s (t)\u2229H=\u2205 P [t]c(1 \u2212 c) l(t) = t:p q l(t)<k+1 s (t)\u2229H=\u2205 P [t]c(1 \u2212 c) l(t)since there is no tour t : p q with l(t) > 0 for which s (t) \u2229 H = \u2205. Next we show thatE k+1 [p](q) = t:p q l(t)=k+1 s(t)\u2229H=\u2205 P [t](1 \u2212 c) l(t) for q / \u2208 H. By equation (15) with Q k (p) = V \u2212 H, we have E k+1 [p](q) = q \u2208(V \u2212H)\u2229I(q) 1 \u2212 c |O(q )| E k [p](q )(26) since only the expansion of the in-neighbors of q can contribute to E k+1 [p](q), and of these, only the ones not in H are expanded. Expanding E k [p](q ) using the inductive hypothesis, (26) becomes E k+1 [p](q) = q \u2208(V \u2212H)\u2229I(q) 1 \u2212 c |O(q )| t :p q l(t )=k s (t )\u2229H=\u2205 P [t ](1 \u2212 c) l(t )", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "| P [t ].Thus we can split the summation in (27) according to q to rewrite it ast:p q l(t)=k+1 s(t)\u2229H=\u2205 P [t](1 \u2212 c) l(t) = q \u2208(V \u2212H)\u2229I(q) t :p q l(t )=k s (t )\u2229H=\u2205 P [T (t )](1 \u2212 c) l(T (t )) = q \u2208(V \u2212H)\u2229I(q) 1 \u2212 c |O(q )| t :p q l(t )=k s (t )\u2229H=\u2205 P [t ](1 \u2212 c) l(t ) (28)which is what we wanted to show. Now we show thatE k+1 [p](q) = t:p q 1\u2264l(t)\u2264k+1 s(t)\u2229H=\u2205 P [t](1 \u2212 c) l(t) for q \u2208 H. By equation (15) with Q k (p) = V \u2212 H, E k+1 [p](q) = E k [p](q) + q \u2208(V \u2212H)\u2229I(q) 1 \u2212 c |O(q )| E k [p](q ) = E k [p](q) + q \u2208(V \u2212H)\u2229I(q) 1 \u2212 c |O(q )| t :p q l(t )=k s(t )\u2229H=\u2205 P [t ](1 \u2212 c) l(t )Equation (28) still applies, and we haveE k+1 [p](q) = E k [p](q) + t:p q l(t)=k+1 s(t)\u2229H=\u2205 P [t](1 \u2212 c) l(t) = t:p q 1\u2264l(t)\u2264k s(t)\u2229H=\u2205 P [t](1 \u2212 c) l(t) + t:p q l(t)=k+1 s(t)\u2229H=\u2205 P [t](1 \u2212 c) l(t) = t:p q 1\u2264l(t)\u2264k+1 s(t)\u2229H=\u2205 P [t](1 \u2212 c) l(t)which completes the proof of equations (24) and (25).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Finally, we showthat for all q \u2208 V , D k [p](q)+cE k [p](q) converges to r p (q)\u2212r H p (q) as k \u2192 \u221e. If q / \u2208 H, then E k [p](q) \u2192 0 as k \u2192 \u221e, andD k [p](q) + cE k [p](q) = D k [p](q) + t:p q l(t)<k s (t)\u2229H=\u2205 P [t]c(1 \u2212 c) l(t) \u2192 r p (q) \u2212 r H p (q) since s (t) \u2229 H = s(t) \u2229 H when q / \u2208 H. If q \u2208 H, then D k [p](q) + cE k [p](q) = t:p q l(t)<k s (t)\u2229H=\u2205 P [t]c(1 \u2212 c) l(t) + t:p q 1\u2264l(t)\u2264k s(t)\u2229H=\u2205 P [t]c(1 \u2212 c) l(t)When q \u2208 H, s (t) \u2229 H = \u2205 unless p = q and t = p . Thus,D k [p](q) + cE k [p](q) = cx p (q) + t:p q 1\u2264l(t)\u2264k s(t)\u2229H=\u2205 P [t]c(1 \u2212 c) l(t) = t:p q 0\u2264l(t)\u2264k s(t)\u2229H=\u2205 P [t]c(1 \u2212 c) l(t)which converges to r p (q) \u2212 r H p (q) as k \u2192 \u221e.I Proof: Computation of the Hubs SkeletonLet (D i [p], E i [p]) denote the results after i iterations of repeated squaring, so that the intermediate results left by selective expansion correspond to i = 0.The error initially associated with hub pages, h / \u2208H E 0 [p](h), is bounded by 1 \u2212 c because the first step of selective expansion expands all pages (Section 5.2.2). By equation (17) with Q i (p) = H, the error associated with hub pages on iteration i \u2265 1 of repeated squaring, q\u2208H E i [p](q), is bounded by (1\u2212c) 2 i . Moreover, the error associated with non-hub pages, q /\u2208H E i [p](q), increases by at most (1 \u2212 c) 2 i q / \u2208H E i\u22121 [p](q) compared to the previous iteration. Using a geometric series to bound q / \u2208H E i [p](q), the total error |E i [p]| of iteration i is bounded by (1 \u2212 c) 2 i + /c.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "I i (p) (1 \u2264 i \u2264 |I(p)|)", "formula_coordinates": [3.0, 72.0, 597.79, 468.0, 27.82]}, {"formula_id": "formula_1", "formula_text": "v = (1 \u2212 c)Av + cu (1)", "formula_coordinates": [4.0, 254.61, 293.95, 285.39, 10.8]}, {"formula_id": "formula_2", "formula_text": "\u03b1 1 v 1 + \u03b1 2 v 2 = (1 \u2212 c)A(\u03b1 1 v 1 + \u03b1 2 v 2 ) + c(\u03b1 1 u 1 + \u03b1 2 u 2 )", "formula_coordinates": [4.0, 160.8, 661.42, 290.4, 11.54]}, {"formula_id": "formula_3", "formula_text": "3 Partial Vector (r p \u2212 r H p )", "formula_coordinates": [5.0, 84.84, 223.69, 428.09, 28.35]}, {"formula_id": "formula_4", "formula_text": "x i : u = n i=1 \u03b1 i x i(3)", "formula_coordinates": [5.0, 273.11, 533.96, 266.89, 58.53]}, {"formula_id": "formula_5", "formula_text": "v = n i=1 \u03b1 i r i(4)", "formula_coordinates": [5.0, 274.35, 627.62, 265.65, 35.77]}, {"formula_id": "formula_6", "formula_text": "t:p q P [t]c(1 \u2212 c) l(t)(5)", "formula_coordinates": [7.0, 278.0, 95.71, 262.0, 25.67]}, {"formula_id": "formula_7", "formula_text": "k\u22121 i=1 1 |O(w i )| , or 1 if l(t) = 0.", "formula_coordinates": [7.0, 202.73, 178.63, 124.68, 17.75]}, {"formula_id": "formula_8", "formula_text": "t:p H q P [t]c(1 \u2212 c) l(t)(6)", "formula_coordinates": [7.0, 271.79, 324.34, 268.21, 25.94]}, {"formula_id": "formula_9", "formula_text": "r p = (r p \u2212 r H p ) + r H p(7)", "formula_coordinates": [7.0, 250.38, 547.41, 289.62, 14.91]}, {"formula_id": "formula_10", "formula_text": "p \u2208 V , H \u2286 V , r H p = 1 c h\u2208H (r p (h) \u2212 cx p (h)) r h \u2212 r H h \u2212 cx h(8)", "formula_coordinates": [8.0, 189.1, 282.36, 350.9, 54.11]}, {"formula_id": "formula_11", "formula_text": "r p = (r p \u2212 r H p ) + 1 c h\u2208H (r p (h) \u2212 cx p (h)) r h \u2212 r H h \u2212 cx h (9)", "formula_coordinates": [8.0, 151.42, 510.39, 388.58, 31.9]}, {"formula_id": "formula_12", "formula_text": "u = 1 |O(p)| |O(p)| i=1 x O i (p)", "formula_coordinates": [11.0, 72.0, 291.77, 127.58, 16.92]}, {"formula_id": "formula_13", "formula_text": "1 |O(p)| .", "formula_coordinates": [11.0, 73.2, 310.2, 26.4, 15.82]}, {"formula_id": "formula_14", "formula_text": "p \u2208 V , r p = (1 \u2212 c) |O(p)| |O(p)| i=1 r O i (p) + cx p(10)", "formula_coordinates": [11.0, 228.98, 339.49, 311.02, 62.02]}, {"formula_id": "formula_15", "formula_text": "(D k [ * ], E k [ * ]). For each p, D k [p]", "formula_coordinates": [12.0, 171.38, 151.81, 175.17, 11.54]}, {"formula_id": "formula_16", "formula_text": "E k [p], where E k [p] is the \"projection\" of the vector (r p \u2212 D k [p]) onto the (actual) basis vectors.", "formula_coordinates": [12.0, 72.0, 203.81, 468.0, 11.54]}, {"formula_id": "formula_17", "formula_text": "D k [p] + q\u2208V E k [p](q)r q = r p (11", "formula_coordinates": [12.0, 233.23, 249.48, 301.79, 23.76]}, {"formula_id": "formula_18", "formula_text": ")", "formula_coordinates": [12.0, 535.02, 249.52, 4.98, 10.76]}, {"formula_id": "formula_19", "formula_text": "Thus, D k [p] is a lower-approximation of r p with error q\u2208V E k [p](q)r q = |E k [p]|.", "formula_coordinates": [12.0, 72.0, 291.98, 417.82, 13.23]}, {"formula_id": "formula_20", "formula_text": "(D k+1 [p], E k+1 [p]) from (D k [p], E k [p])", "formula_coordinates": [12.0, 331.38, 560.08, 208.62, 11.54]}, {"formula_id": "formula_21", "formula_text": "D k+1 [p] = 1 \u2212 c |O(a)| |O(p)| i=1 D k [O i (p)] + cx p (12) E k+1 [p] = 1 \u2212 c |O(a)| |O(p)| i=1 E k [O i (p)](13)", "formula_coordinates": [12.0, 195.11, 601.93, 344.89, 78.77]}, {"formula_id": "formula_22", "formula_text": "(D k [ * ], E k [ * ]) could be read off disk and (D k+1 [ * ], E k+1 [ * ]", "formula_coordinates": [13.0, 72.0, 161.07, 468.0, 28.88]}, {"formula_id": "formula_23", "formula_text": "(D k+1 [p], E k+1 [p]", "formula_coordinates": [13.0, 142.57, 307.97, 96.73, 11.54]}, {"formula_id": "formula_24", "formula_text": "D k+1 [p] = D k [p] + q\u2208Q k (p) cE k [p](q)x q (14) E k+1 [p] = E k [p] \u2212 q\u2208Q k (p) E k [p](q)x q + q\u2208Q k (p) 1 \u2212 c |O(q)| |O(q)| i=1 E k [p](q)x O i (q) (15", "formula_coordinates": [13.0, 119.97, 371.93, 420.03, 70.08]}, {"formula_id": "formula_25", "formula_text": ")", "formula_coordinates": [13.0, 535.02, 416.73, 4.98, 10.76]}, {"formula_id": "formula_26", "formula_text": "D 2k [p] = D k [p] + q\u2208Q k (p) E k [p](q)D k [q] (16", "formula_coordinates": [14.0, 154.14, 103.43, 380.88, 25.31]}, {"formula_id": "formula_27", "formula_text": ")", "formula_coordinates": [14.0, 535.02, 103.47, 4.98, 10.76]}, {"formula_id": "formula_28", "formula_text": "E 2k [p] = E k [p] \u2212 q\u2208Q k (p) E k [p](q)x q + q\u2208Q k (p) E k [p](q)E k [q] (17", "formula_coordinates": [14.0, 155.67, 137.71, 379.35, 25.31]}, {"formula_id": "formula_29", "formula_text": ")", "formula_coordinates": [14.0, 535.02, 143.32, 4.98, 10.76]}, {"formula_id": "formula_30", "formula_text": "E 2k [p] = q\u2208V E k [p](q)E k [q] (18", "formula_coordinates": [14.0, 235.68, 275.48, 299.34, 23.76]}, {"formula_id": "formula_31", "formula_text": ")", "formula_coordinates": [14.0, 535.02, 275.52, 4.98, 10.76]}, {"formula_id": "formula_32", "formula_text": "(D k [q], E k [q]", "formula_coordinates": [14.0, 331.46, 383.25, 71.32, 11.54]}, {"formula_id": "formula_33", "formula_text": "W = {r p (H) | p \u2208 V }.", "formula_coordinates": [14.0, 177.13, 560.63, 110.35, 11.5]}, {"formula_id": "formula_34", "formula_text": "Q k (p), D k [p] + cE k [p] converges to (r p \u2212 r H p ) for all p \u2208 V .", "formula_coordinates": [14.0, 233.25, 708.84, 306.75, 14.31]}, {"formula_id": "formula_35", "formula_text": "(D k [p], E k [p]", "formula_coordinates": [15.0, 265.52, 74.4, 71.21, 11.54]}, {"formula_id": "formula_36", "formula_text": "(D k [p], E k [p]", "formula_coordinates": [15.0, 72.0, 307.97, 70.12, 11.54]}, {"formula_id": "formula_37", "formula_text": "(D k [h], E k [h]) for h \u2208 H", "formula_coordinates": [15.0, 284.55, 394.64, 129.52, 11.54]}, {"formula_id": "formula_38", "formula_text": "(D k [p], E k [p]", "formula_coordinates": [15.0, 72.0, 610.88, 70.12, 11.54]}, {"formula_id": "formula_39", "formula_text": "p i \u2208 H for 1 \u2264 i \u2264 z. Let Q \u2286 H, and let r u (h) = z i=1 \u03b1 i (r p i (h) \u2212 cx p i (h))(19)", "formula_coordinates": [16.0, 104.2, 151.84, 435.81, 58.32]}, {"formula_id": "formula_40", "formula_text": "v = z i=1 \u03b1 i (r p i \u2212 r H p i ) + 1 c h\u2208Q ru(h)>0 r u (h) (r h \u2212 r H h ) \u2212 cx h (20)", "formula_coordinates": [16.0, 159.38, 245.49, 380.62, 44.64]}, {"formula_id": "formula_41", "formula_text": "(D k [ * ], E k [ * ]", "formula_coordinates": [16.0, 325.12, 554.43, 72.81, 11.54]}, {"formula_id": "formula_42", "formula_text": "E i (1 \u2264 i \u2264 k),", "formula_coordinates": [16.0, 327.93, 623.8, 75.83, 11.5]}, {"formula_id": "formula_43", "formula_text": "A p v p = 1 \u2212 c k k i=1 (A i \u2212 cU i + cU p )r i + cA p x p = 1 \u2212 c k k i=1 A i r i \u2212 1 \u2212 c k c k i=1 U i r i + 1 \u2212 c k c k i=1 U p r i + cA p x p = 1 \u2212 c k k i=1 r i \u2212 1 \u2212 c k c k i=1 x i + 1 \u2212 c k c k i=1 x p + cA p x p = 1 \u2212 c k k i=1 r i \u2212 1 \u2212 c k c k i=1 x i + (1 \u2212 c)cx p + c((1 \u2212 c)A + cU p )x p = 1 \u2212 c k k i=1 r i \u2212 1 \u2212 c k c k i=1 x i + (1 \u2212 c)cx p + (1 \u2212 c)cAx p + c 2 x p = 1 \u2212 c k k i=1 r i + (1 \u2212 c)cx p + c 2 x p + (1 \u2212 c)c Ax p \u2212 1 k k i=1 x i = 1 \u2212 c k k i=1 r i + (1 \u2212 c)cx p + c 2 x p = 1 \u2212 c k k i=1 r i + cx p = v p", "formula_coordinates": [24.0, 119.3, 126.22, 372.9, 339.72]}, {"formula_id": "formula_44", "formula_text": "r p (q) = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 (1 \u2212 c) |O(p)| i=1 r O i (p) (q) (if p = q) (1 \u2212 c) |O(p)| i=1 r O i (p) (q) + c (if p = q)", "formula_coordinates": [24.0, 180.55, 673.9, 244.73, 60.43]}, {"formula_id": "formula_45", "formula_text": "P [t] = 1 |O(p)| P [t ].", "formula_coordinates": [25.0, 453.43, 207.53, 86.57, 15.82]}, {"formula_id": "formula_46", "formula_text": "r p (q) = |O(p)| z=1 t : Oz(p) q P [T (t )]c(1 \u2212 c) l(T (t )) = 1 \u2212 c |O(p)| |O(p)| z=1 t : Oz(p) b P [t ]c(1 \u2212 c) l(t) = 1 \u2212 c |O(p)| |O(p)| i=1 r p (q)", "formula_coordinates": [25.0, 188.44, 252.28, 234.62, 126.23]}, {"formula_id": "formula_47", "formula_text": "t:p q P [t](1 \u2212 c) l(t)", "formula_coordinates": [26.0, 293.79, 115.24, 90.97, 25.67]}, {"formula_id": "formula_48", "formula_text": "p \u2208 V , H \u2286 V , a) r H p = 1 c h\u2208H (r p (h) \u2212 cx p (h)) r h \u2212 r H h \u2212 cx h b) r H p = 1 c h\u2208H r p (h) \u2212 r H p (h) \u2212 cx p (h) (r h \u2212 cx h )", "formula_coordinates": [26.0, 85.45, 264.71, 261.47, 80.86]}, {"formula_id": "formula_49", "formula_text": "r H p (q) = t:p q s(t)\u2229H =\u2205 P [t]c(1 \u2212 c) l(t)", "formula_coordinates": [26.0, 227.98, 453.17, 155.54, 34.0]}, {"formula_id": "formula_50", "formula_text": "r H p (q) = t 1 |t 1 =\u03b2(t) t:p H q t:p H q \u03b2(t)=t 1 P [t]c(1 \u2212 c) l(t)(22)", "formula_coordinates": [26.0, 207.54, 528.85, 332.47, 35.26]}, {"formula_id": "formula_51", "formula_text": "r H p (q) = h\u2208H t 1 :p h l(t 1 )>0 t:p H q \u03b2(t)=t 1 P [t]c(1 \u2212 c) l(t)", "formula_coordinates": [26.0, 203.0, 640.53, 205.51, 35.35]}, {"formula_id": "formula_52", "formula_text": "t:p H q \u03b2(t)=t 1 \u03c0(\u03b3(t))", "formula_coordinates": [27.0, 286.92, 140.1, 76.47, 33.05]}, {"formula_id": "formula_53", "formula_text": "r H p (q) = 1 c h\u2208H t 1 :p h l(t 1 )>0 \u03c0(t 1 ) \uf8eb \uf8ec \uf8ec \uf8ed t :h q s(t )\u2229H=\u2205 \u03c0(t ) \u2212 x h (q) t = q \u03c0(t ) \uf8f6 \uf8f7 \uf8f7 \uf8f8 = 1 c h\u2208H t 1 :p h l(t 1 )>0 \u03c0(t 1 ) \uf8eb \uf8ec \uf8ec \uf8ed t :h q s(t )\u2229H=\u2205 \u03c0(t ) \u2212 x h (q)c \uf8f6 \uf8f7 \uf8f7 \uf8f8", "formula_coordinates": [27.0, 151.63, 292.96, 308.74, 119.0]}, {"formula_id": "formula_54", "formula_text": "r H p (q) = 1 c h\u2208H t 1 :p h l(t 1 )>0 \u03c0(t 1 ) t :h q \u03c0(t ) \u2212 t :h H q \u03c0(t ) \u2212 cx h (q) = 1 c h\u2208H t 1 :p h l(t 1 )>0 \u03c0(t 1 ) r h (q) \u2212 r H h (q) \u2212 cx h (q)", "formula_coordinates": [27.0, 141.9, 485.17, 318.74, 86.9]}, {"formula_id": "formula_55", "formula_text": "t 1 :p h l(t 1 )>0 \u03c0(t 1 ) = r p (h) \u2212 cx p (h)", "formula_coordinates": [27.0, 234.55, 621.87, 142.9, 33.14]}, {"formula_id": "formula_56", "formula_text": "r H p (q) = 1 c h\u2208H (r p (h) \u2212 cx p (h)) r h (q) \u2212 r H h (q) \u2212 cx h (q)", "formula_coordinates": [27.0, 163.96, 695.95, 278.61, 31.9]}, {"formula_id": "formula_57", "formula_text": "P [t]c(1 \u2212 c) l(t)(23)", "formula_coordinates": [28.0, 332.15, 187.05, 207.85, 22.59]}, {"formula_id": "formula_58", "formula_text": "r H p (q) = h\u2208H t 2 :h q l(t 2 )>0 t:p H q \u03b3(t)=t 2 P [t]c(1 \u2212 c) l(t) But P [t] = P [\u03b2(t)]P [\u03b3(t)], and l(t) = l(\u03b2(t)) + l(\u03b3(t)), so r H p (q) = h\u2208H t 2 :h q l(t 2 )>0 t:p H q \u03b3(t)=t 2 P [\u03b2(t)]P [\u03b3(t)]c(1 \u2212 c) l(\u03b2(t))+l(\u03b3(t)) = 1 c h\u2208H t 2 :h q l(t 2 )>0 \u03c0(t 2 ) t:p H q \u03b3(t)=t 2 \u03c0(\u03b2(t))", "formula_coordinates": [28.0, 72.0, 298.05, 383.57, 157.45]}, {"formula_id": "formula_59", "formula_text": "r H p (q) = 1 c h\u2208H t 2 :h q l(t 2 )>0 \u03c0(t 2 ) \uf8eb \uf8ec \uf8ec \uf8ed t :p h s(t )\u2229H=\u2205 \u03c0(t ) \u2212 x p (h) t = p \u03c0(t ) \uf8f6 \uf8f7 \uf8f7 \uf8f8 = 1 c h\u2208H t 2 :h q l(t 2 )>0 \u03c0(t 2 ) \uf8eb \uf8ec \uf8ec \uf8ed t :p h s(t )\u2229H=\u2205 \u03c0(t ) \u2212 x p (h)c \uf8f6 \uf8f7 \uf8f7 \uf8f8", "formula_coordinates": [28.0, 151.28, 586.04, 309.44, 119.0]}, {"formula_id": "formula_60", "formula_text": "r H p (q) = 1 c h\u2208H t 2 :h q l(t 2 )>0 \u03c0(t 2 ) t :p h \u03c0(t ) \u2212 t :p H h \u03c0(t ) \u2212 cx p (h) = 1 c h\u2208H t 2 :h q l(t 2 )>0 \u03c0(t 2 ) r p (h) \u2212 r H p (h) \u2212 cx p (h)", "formula_coordinates": [29.0, 141.45, 116.54, 319.63, 86.9]}, {"formula_id": "formula_61", "formula_text": "t 2 :h q l(t 2 )>0 \u03c0(t 2 ) = r h (q) \u2212 cx h (q)", "formula_coordinates": [29.0, 235.05, 258.94, 141.89, 33.14]}, {"formula_id": "formula_62", "formula_text": "r H p (q) = 1 c h\u2208H r p (h) \u2212 r H p (h) \u2212 cx p (h) (r h (q) \u2212 cx h (q))", "formula_coordinates": [29.0, 163.4, 341.16, 285.21, 31.9]}, {"formula_id": "formula_63", "formula_text": "|E k [p]| = (1 \u2212 c) k .", "formula_coordinates": [29.0, 72.0, 495.63, 468.0, 28.13]}, {"formula_id": "formula_64", "formula_text": "D k+1 [p] + q\u2208V E k+1 [p](q)r q = r p : D k+1 [p] + q\u2208V E k+1 [p](q)r q = 1 \u2212 c |O(p)| |O(p)| i=1 D k [O i (p)] + cx p + q\u2208V 1 \u2212 c |O(p)| |O(p)| i=1 E k [O i (p)](q)r q = cx p + 1 \u2212 c |O(p)| |O(p)| i=1 D k [O i (p)] + q\u2208V E k [O i (p)](q)r q = 1 \u2212 c |O(p)| |O(p)| i=1 r O i (p) + cx p = r p", "formula_coordinates": [29.0, 73.43, 530.3, 464.36, 171.31]}, {"formula_id": "formula_65", "formula_text": "|E k+1 [p]| = (1 \u2212 c) k+1 : |E k+1 [p]| = 1 \u2212 c |O(p)| |O(p)| i=1 E k [O i (p)] = 1 \u2212 c |O(p)| |O(p)| i=1 |E k [O i (p)]| = 1 \u2212 c |O(p)| |O(p)| i=1 (1 \u2212 c) k = 1 \u2212 c |O(p)| |O(p)|(1 \u2212 c) k = (1 \u2212 c) k+1", "formula_coordinates": [30.0, 72.0, 70.8, 468.0, 214.02]}, {"formula_id": "formula_66", "formula_text": "D k+1 [p] + q\u2208V E k+1 [p](q)r q = r p for an arbitrary Q k (p) \u2286 V : D k+1 [p] + q\u2208V E k+1 [p](q)r q = \uf8eb \uf8ed D k [p] + q\u2208Q k (p) cE k [p](q)x q \uf8f6 \uf8f8 + q \u2208V E k [p] \u2212 q\u2208Q k (p) E k [p](q)x q + q\u2208Q k (p) 1 \u2212 c |O(q)| |O(q)| i=1 E k [p](q)x O i (q) (q )r q = D k [p] + q \u2208V E k [p](q )r q + q\u2208Q k (p) cE k [p](q)x q \u2212 q \u2208V q\u2208Q k (p) E k [p](q)x q (q )r q + q \u2208V q\u2208Q k (p) 1 \u2212 c |O(q)| |O(q)| i=1 E k [p](q)x O i (q) (q )r q", "formula_coordinates": [30.0, 84.62, 348.52, 455.39, 247.38]}, {"formula_id": "formula_67", "formula_text": "D k [p] + q \u2208V E k [p](q )r q = r p", "formula_coordinates": [30.0, 229.33, 630.66, 152.83, 23.91]}, {"formula_id": "formula_68", "formula_text": "q \u2208V q\u2208Q k (p) E k [p](q)x q (q )r q = q\u2208Q k (p) E k [p](q)r q and q \u2208V q\u2208Q k (p) 1 \u2212 c |O(q)| |O(q)| i=1 E k [p](q)x O i (q) (q )r q = q\u2208Q k (p) 1 \u2212 c |O(q)| |O(q)| i=1 E k [p](q)r O i (q)", "formula_coordinates": [30.0, 185.26, 708.08, 240.68, 25.28]}, {"formula_id": "formula_69", "formula_text": "cE k [p](q)x q + 1 \u2212 c |O(q)| |O(q)| i=1 E k [p](q)r O i (q) = E k [p](q)r b for all q \u2208 Q k (p),", "formula_coordinates": [31.0, 72.0, 171.24, 369.16, 61.61]}, {"formula_id": "formula_70", "formula_text": "E k [p](q) = max{E k [p](q) | q \u2208 V } ensures that the error tends towards 0. In particular, such is the case if Q k (p) = V or Q k (p) is the top m > 0 pages q with the highest E k [p](q).", "formula_coordinates": [31.0, 72.0, 273.35, 468.0, 28.84]}, {"formula_id": "formula_71", "formula_text": "D 2k [p] + q\u2208Q k (p) E 2k [p](q)r q = r q for an arbitrary Q k (p) \u2286 V : D 2k [p] + q\u2208V E 2k [p](q)r q = D k [p] + q\u2208Q k (p) E k [p](q)D k [q]+ q \u2208V \uf8eb \uf8ed E k [p](q ) \u2212 q\u2208Q k (p) E k [p](q)x q (q ) + q\u2208Q k (p) E k [p](q)E k [q](q ) \uf8f6 \uf8f8 r q = D k [p] + q \u2208V E k [p](q )r q + q\u2208Q k (p) E k [p](q) D k [q] \u2212 q \u2208V x q (q )r q + q \u2208V E k [q](q )r q = r p + q\u2208Q k (p) E k [p](q) D k [q] + q \u2208V E k [q](q )r q \u2212 r q = r p + q\u2208Q k (p) [0] = r p", "formula_coordinates": [31.0, 72.0, 394.78, 448.6, 314.21]}, {"formula_id": "formula_72", "formula_text": "|E 2k [p]| = q\u2208V E k [p](q)E k [q] = q\u2208V E k [p](q)|E k [q]| = q\u2208V E k [p](q) = |E k [p]| = 2", "formula_coordinates": [32.0, 220.73, 142.33, 167.21, 127.15]}, {"formula_id": "formula_73", "formula_text": "P [t](1 \u2212 c) l(t) (if q \u2208 H)(25)", "formula_coordinates": [32.0, 290.2, 511.56, 249.8, 19.91]}, {"formula_id": "formula_74", "formula_text": "P [t](1 \u2212 c) l(t)(27)", "formula_coordinates": [33.0, 292.16, 689.29, 247.84, 12.97]}, {"formula_id": "formula_75", "formula_text": ")", "formula_coordinates": [34.0, 146.08, 164.35, 2.82, 6.99]}], "doi": ""}