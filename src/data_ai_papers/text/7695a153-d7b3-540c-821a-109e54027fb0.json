{"title": "Large Margin Training for Hidden Markov Models with Partially Observed States", "authors": "Trinh-Minh-Tri Do; Thierry Arti\u00e8res", "pub_date": "", "abstract": "HMMs with a partially labeled dataset has been extensively studied in the speech and handwriting recognition fields. Yet due to the non-convexity of the optimization problem, previous works usually rely on severe approximations so that it is still an open problem. We propose a new learning algorithm that relies on non-convex optimization and bundle methods and allows tackling the original optimization problem as is. It is proved to converge to a solution with accuracy with a rate O(1/ ). We provide experimental results gained on speech and handwriting recognition that demonstrate the potential of the method.", "sections": [{"heading": "Introduction", "text": "Hidden Markov Models (HMMs) have been widely used for automatic speech recognition and handwriting recognition . Continuous Density HMMs (CDHMMs) are particularly suited for dealing with sequences of real-valued feature vectors that one gets after typical front-end processing in signal processing tasks. CDHMMs usually exploit Gaussian mixture models to describe the variability of observations in a state. HMM parameters are learnt with the Expectation-Maximization algorithm (EM) to maximize the joint likelihood of observation and of hidden state sequences.\nTraining is performed based on a partially labeled training set, that is a set of observation sequences together with the corresponding classes (classification case) or with the corresponding unit sequences (seg-Appearing in Proceedings of the 26 th International Conference on Machine Learning, Montreal, Canada, 2009. Copyright 2009 by the author(s)/owner(s). mentation case). This is the usual setting in speech or handwriting recognition tasks, where one never gets the complete sequence of states corresponding to an observation sequence in the training stage. In test, segmentation is performed through Viterbi decoding that maps an observation sequence into a state sequence. Based on the underlying semantics of the states (e.g. passing through the three states of the left-right HMM corresponding to a particular phone means this phone has been recognized), the sequence of states translates into a sequence of labels (e.g. phones). This typical use of HMMs is very popular since it is both simple and efficient, and it scales well with large corpus. However, this learning strategy does not focus on what we are primarily concerned with, namely minimizing the classification (or the segmentation) error rate. Hence, a number of attempts have been made to develop discriminative learning methods for HMMs. First studies, in the speech recognition field, aimed at optimizing a discriminative criterion such as the Minimum Classification Error (MCE) (Juang & Katagiri, 1992) or the Maximum Mutual Information (MMI) criterion (Woodland & Povey, 2002).\nRecently, a promising direction has been explored with the development of margin-based methods for sequences (Taskar et al., 2004;Tsochantaridis et al., 2004). However these works mainly deal with fully supervised learning. There is still work to do to extend these works to the learning of CDHMMs with partially labeled data. Building on these seminal works a few approaches have been proposed for large margin learning of HMMs, especially in the speech recognition community (Sha & Saul, 2007;Jiang & Li, 2007) (see (Yu & Deng, 2007) for a review). However none of these works actually handle the whole problem of max-margin learning for HMM parameters in the standard partially labeled setting. (Jiang & Li, 2007) focuses on correctly predicted examples near the decision boundary only, while (Sha & Saul, 2007) mainly focuses on the fully supervised case, where the sequence of states corresponding to the training sequences are determined by a traditionally trained HMM system via Maximum Likelihood Estimation (MLE).\nHere we propose a new method for discriminative learning of CDHMM models in the complex unlabeled setting. The main difficulty one encounters when formulating the maximum margin learning of CDHMM with partially labeled data lies in the non-convexity of the optimization problem. While (Sha & Saul, 2007) consider a simpler and convex problem we build on non-convex optimization ideas and investigate here the direct optimization of the non-convex objective function using bundle methods (Kiwiel, 1985;Hiriart-Urruty & Lemarechal, 1993). Our contributions are:\n\u2022 A new fast optimization method that can handle non-convex, non-differentiable function, with a theoretical analysis of its convergence rate.\n\u2022 Experimental results showing the method is well adapted for large margin training of CDHMMs.\nWe first present the maximum margin training formalization in the general case of structured outputs and for HMM learning in particular. Then we show how such a learning problem may resume to a non-convex optimization problem that may be solved with a variant of bundle methods that we propose, for which we provide a detailed analysis convergence. Finally we provide experimental results on speech and on handwriting recognition that show first the performance of the classifiers and second the efficiency of the optimization method.", "publication_ref": ["b3", "b10", "b7", "b9", "b6", "b1", "b11", "b1", "b6", "b6", "b4", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Large Margin Learning and HMMs", "text": "In this section, we address the problem of learning a model for structured outputs based on partially labeled data. Although our method is quite general we mainly focus here on the special case of learning a model for sequence segmentation based on partially labeled training sequences, which fits our case study of maximum margin learning for HMMs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Large Margin Learning for Structured Outputs with Partially Labeled data", "text": "We consider a training set that consists of K inputoutput pairs (x 1 , y 1 ), ..., (x K , y K ) \u2208 X \u00d7 Y where\nx i = (x i 1 , x i 2 , ..., x i T ) \u2208 (R d )\nT is an observation sequence and y i = (y i 1 , y i 2 , ..., y i L ) \u2208 L L is the corresponding label sequence (with L \u2264 T ). We consider hidden variables (z 1 , z 2 , ..., z K ) where z i stands for the missing variables corresponding to the i th training sample. For instance if we wish to learn a HMM for speech recognition, z i might be the full state sequence corresponding to the i th speech signal x i while y i is the corresponding sequence of phones.\nWe are interested in learning a discriminant function F : X \u00d7 Y \u2192 R over input-output pairs which can be used to predict the output y for an input x:\nh(x, w) = argmax y\u2208Y F (x, y, w)(1)\nwhere w denotes the parameter vector to be learned.\nIn the case of partially labeled data, the discriminant function F (x, y, w) may take various forms such as F (x, y, w) = max z g(x, y, z, w) where g(x, y, z, w) stands for an elementary discriminant function. For instance g might be a linear function g(x, y, z, w) = \u03a6(x, y, z), w with \u03a6(x, y, z) being a feature vector.\nFollowing previous works (Tsochantaridis et al., 2004;Taskar et al., 2004), learning an optimal h may be done by solving the following soft margin problem:\nmin w,\u03be \u03bb 2 w 2 + i \u03be i s.t. F (x i ,y i ,w)\u2265F (x i ,y,w)+\u2206(y i ,y)\u2212\u03be i \u2200i\u2200y =y i \u03be i \u22650 \u2200i(2)\nwhere \u2206(y i , y) terms allow taking into account differences between labellings (Cf. (Taskar et al., 2004)).\nThe equivalent unconstrained problem is:\nminw \u03bb 2 w 2 + i maxy(F (x i ,y,w)+\u2206(y i ,y)\u2212F (x i ,y i ,w)) f (w)\n(3) where f (w) is the primal objective function. A variant consists in using a softmax instead of a max in (3): maxy(F (x i ,y,w)+\u2206(y i ,y)\u2212F (x i ,y i ,w)) \u2248max 0, log y =y i e F (x i ,y,w)+\u2206(y i ,y) \u2212F (x i ,y i ,w)\n(4)", "publication_ref": ["b9", "b7", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Application to CDHMMs", "text": "We consider standard CDHMMs with Gaussian mixture as probability density function (pdf) within each state. The pdf over observation frame x \u2208 R d within a state s is defined as a Gaussian mixture:\np(x|s) = M m=1 p s,m N s,m (x)(5)\nwhere p s,m stands for the prior probability of the m th mixture in state s, and N s,m stands for the Gaussian distribution whose mean vector is noted \u00b5 s,m and whose covariance matrix is noted \u03a3 s,m . \nwhere, using notation p s,m (x)\ndef = p s,m N s,m (x): p(x, y, s, m|w) = \u03c0 s1 p s1,m1 (x 1 ) T t=2 a st\u22121,st p st,mt (x t )\nIn practice one often uses the approximation p(x, y|w) \u2248 max s p(x, y, s|w) or even: \np(x, y|w) \u2248 max s,m p(x, y, s, m|w)(7\nor the following softmax variant: ,y,s,m,w) \uf8f6 \uf8f8 (9)\nF sof t (x, y, w) = log \uf8eb \uf8ed s\u2208S(y),m e g(x", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Solving the large margin HMM training optimization problem", "text": "In the context of our study solving the unconstrained problem Eq. (3) raises a major problem since f (w) is naturally non-convex. For instance if F (x, y, w) = max z g(x, y, z, w) and assuming g is linear, then F (x, y, w) is a convex function. Yet f (w) is not convex because of the \u2212F (x i , y i , w) terms which are concave.\nAlternatively the constrained form in Eq. (2) raises problems too since there does not exist any efficient algorithm to solve such a non-convex problem with an exponential number of constraints.\nSome previous works overcome the difficulty by considering a smaller sized problem, e.g. through a heuristic selection (based on Viterbi-like decoding and N-best lists) of a subset of training samples together with a subset of candidate labelings (see (Yu & Deng, 2007) for a review). This is a first step but further approximations are needed to solve the problem. For instance (Jiang & Li, 2007) also relies on an additional convex relaxation before solving the problem with semi definite programming. At the end the impact of successive approximations and simplifications is difficult to measure and the robustness of the method is questionable.\nA more direct solution has been proposed in (Sha, 2006) which is based on primal optimization (Eq.\n(3)) and convex relaxation ideas. These authors proposed to overcome the non-convexity of f by linearizing function g (as in Eq. ( 8)) and by simplifying concave terms \u2212F (x i , y i , w) in the following way. They remove the maximization in the computation of F (x i , y i , w) by using an oracle (A Generative System learnt with MLE) providing a guess s i gs , m i gs for argmax s,m [g(x i , y i , s, m, w)]. Hopefully, using the oracle trick the objective function becomes convex in w:\nf o (w)= \u03bb 2 w 2 + i maxy(F (x i ,y,w)+\u2206(y i ,y)\u2212g(x i ,y i ,s i gs ,m i gs ,w)) (10\n)\nAt the end, the quality of the solution is not clear since there are no guarantees that such an oracle provides relevant information for learning the discriminant system. Such a limitation has been stressed in (Sha, 2006) where the more complex HMMs topology is (i.e. the stronger the oracle approximation is) the less interesting discriminant training is.", "publication_ref": ["b11", "b1", "b5", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Non-Convex Optimization Algorithm", "text": "We consider the optimization problem below which includes the maximum margin HMM learning problem as a special case (Cf. Eq. (3), ( 8)):\nmin w f (w) = \u03bb 2 w 2 + R(w) (11)\nwhere R(w) is an upper bound of the empirical risk that we want to minimize.\nOur algorithm is inspired by a recent variant of bundle methods for minimizing convex regularized risk in machine learning problems (Teo et al., 2007;Joachims, 2006). This variant has two main advantages, the first one being its very good convergence rate, the second one being its relevant stopping criterion, namely the gap between the objective and the minimum of the approximated problem. Our algorithm inherits both advantages, but is also able to solve non-convex problems. The approach described in (Teo et al., 2007) solves the general optimization problem in Eq. ( 11) whatever R(w) provided that it is convex. We briefly describe now the method in the case where R(w) is convex. It relies on the cutting plane technique, where a cutting plane of R(w) at w is defined as: The bundle method aims at iteratively building an increasingly accurate piecewise quadratic lower bound of the objective function. Starting with an initial (e.g. random) solution w 1 , one first determines the solution w 2 minimizing the approximation problem g 1 (w) = \u03bb 2 w 2 + c w1 (w). Then a new cutting plane c w2 is built and one looks for the minimum w 3 of the more accurate approximation problem g 2 (w) = \u03bb 2 w 2 + max(c w1 (w), c w2 (w)). More generally at iteration t, one adds a new cutting plane at the current solution w t , and looks for the solution w t+1 minimizing the new approximated problem:\nw t+1 = argmin w g t (w) v t = min w g t (w) with g t (w) = \u03bb 2 w 2 + max j=1..t {c j (w)} (13)\nwhere, in the convex case, c j (w) \u2261 c wj (w) is defined as in Eq. (12). We use the notation c j (w) rather than c wj (w) to stress that c j (w) is built from solution w j in iteration j but does not necessarily coincide with c wj (w) as we will see below in the non-convex case.\nAt iteration t the minimization of the approximated problem in Eq. ( 13) can be solved by quadratic programming. Note that by construction of the quadratic lower bound g t (w) the minimum of the approximated problem v t = g t (w t+1 ) increases every iteration. It may be shown that the gap between the minimum observed value of the objective function and the minimum of the approximated problem, f (w) \u2212 g t (w t+1 ), decreases towards zero and that it requires O(1/\u03bb ) iterations to reach a gap less than (Teo et al., 2007).", "publication_ref": ["b8", "b2", "b8", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Handling non-convex risk function", "text": "Unfortunately, the approach in (Teo et al., 2007) cannot be used for non-convex problems since the minimization of the approximated problem may lead to a local maximum w. Figure 1b illustrates a situation where the method designed for convex cases yields such an improper solution. Consider we get the two cutting planes computed at w 1 and w 2 after two iterations.\nThen minimizing the approximated problem gives w 3 which is a local maximum.\nThe problem comes from the fact that a cutting plane of a non-convex function is not necessarily a lower bound, which leads to a poor approximation (see Figure 1a). Indeed the linearization error (R(w)\u2212c w (w)) of a cutting plane c w at a point w may be negative, meaning that the function is overestimated at that point. In the following we will say in such a case that there is a conflict between c w and w.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Figure 1. Cutting planes and linearization errors", "text": "Standard solution to overcome conflicts is to lower any new cutting plane c wt (w) = a wt , w + b wt that gives negative linearization errors for the solutions w j at previous iterations (Kiwiel, 1985). This may be done by tuning offset b wt . However, this approach does not guarantee any more the improvement of v t , as the change in the cutting plane parameters changes the approximated problem. This is the reason why the convergence rate of standard bundle methods is not guaranteed, and usually slow in practice.\nInstead, our algorithm handles non-convex risk while still preserving the good convergence rate O(1/\u03bb ), it is described in Algorithm 1. One key idea lies in that rather than solving all conflicts between a new cutting plane and all previous solutions, we focus on the conflict of the new cutting plane c wt w.r.t the best observed solution up to now, w * t (see line 4). Fundamentally we allow eventual overestimation at other previous solutions and focus on the area of the best observed solution by considering there is a conflict if and only if condition ( 14) is not satisfied:\nc wt (w * t ) = a wt , w * t + b wt \u2264 R(w * t )(14)\nA second main idea lies in the modification procedure of c wt in case of conflict where c wt is adjusted into c t . Finally the next solution w t+1 is found by minimization of the approximated problem Eq. (13) like in the convex case but where c j denotes now either the \"true\" cutting plane c wj computed at iteration j (Cf. Eq. ( 12)), or its modified version in case there was a conflict at iteration j.", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "Solve Conflict", "text": "In case of conflict we look for a modified cutting plane that satisfies both Eq. ( 14) and Eq. (15):\n\u03bb 2 w t 2 + c t (w t ) \u2265 f (w * t ) (15)\nwhose interest will appear later in the convergence analysis subsection. Note that c wt always satisfies ( 15) by definition of w * t , so that c t also satisfies (15) in case there is no conflict.\nAlgorithm 2 guarantees that the new cutting plane c t with parameters a t and b t satisfies condition ( 14) and ( 15). First it tries to solve the conflict by tuning b t while fixing a t = a wt . The two conditions ( 14) and ( 15) may be rewritten as: Compute w t+1 and v t according to Eq. ( 13) \nb t \u2264 R(w * t ) \u2212 a wt , w * t = U b t \u2265 f (w * t ) \u2212 \u03bb 2 w t 2 \u2212 a wt , w t = L (16\n9: gap t = f (w * t ) \u2212 v\n[a t , b t ] = [\u2212\u03bbw * t , f (w * t ) \u2212 \u03bb 2 w t 2 \u2212 a t , w t ] 2007\n). One can assume f (w) being locally convex then, which would make it converge to a local minimum.\nWe first recall a result from (Teo et al., 2007) which is needed in our proof (Lemma 3.1). Then Lemma 3.2 determines a lower bound on the decrease of the gap every iteration. Finally, Theorem 3.1 proves that Algorithm 1 converges to a solution with accuracy with a rate O(1/\u03bb ). Lemma 3.1. (Teo et al., 2007) The minimum of 1 2 qx 2 \u2212 lx with q, l > 0 and x \u2208 [0, 1] is bounded from above by \u2212 l 2 min(1, l/q).", "publication_ref": ["b8", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Lemma 3.2. Approximation gap decrease:", "text": "gap t\u22121 \u2212 gap t \u2265 min( gap t\u22121 2 , (gap t\u22121 ) 2 \u03bb 8G 2 ) (19\n)\nwhere the approximation gap is defined as gap t = f (w * t ) \u2212 v t , and where G is an upper bound on the norm of cutting planes direction parameters a i .\nProof. The approximation problem (13) at iteration t can be rewritten as follows:\nv t = min w,\u03be \u03bb 2 w 2 + \u03be s.t. a j , w + b j \u2264 \u03be j = 1..t (20\n)\nwhere \u03be is a slack variable. The solution is given by a saddle point of the Lagrangian that must be minimized wrt. parameters w, \u03be and maximized wrt. Lagrange multipliers. One gets easily the dual form:\nv t = max \u03b1\u2208R t D t (\u03b1) = \u2212 \u03bb 2 \u03b1At \u03bb 2 + \u03b1B t s.t \u03b1 i \u2265 0 \u2200i = 1..t i=1..t \u03b1 i = 1 (21\n)\nwhere A t = [a 1 ; ...; a t ] and B t = [b 1 ; ...; b t ] and \u03b1 stands for the vector of Lagrange multipliers (of length t at iteration t). Let \u03b1 t be the solution maximizing D t (\u03b1) 1 . The primal solution, which may be obtained using saddle point optimality conditions (of Lagrange duality), is: w t+1 = \u2212 \u03b1tAt \u03bb , with: v t = D t (\u03b1 t ). Unfortunately v t cannot be computed explicitly since it is the result of a quadratic program. However, we may interestingly study the dual D t for a subspace of \u03b1 values, along the segment line between \u03b1 start = [\u03b1 t\u22121 , 0] and \u03b1 end = [0, .., 0, 1]. It is easy to verify that any point along this segment\n\u03b1(\u03b7) = \u03b7\u03b1 start + (1 \u2212 \u03b7)\u03b1 end \u03b7 \u2208 [0, 1] (22) is feasible. Note also that D t (\u03b1 start ) = D t\u22121 (\u03b1 t\u22121 ) = v t\u22121 , which implies naturally that v t \u2265 v t\u22121 .\nSubstituting Eq. ( 22) into Eq. ( 21) and noting that\nA t = [A t\u22121 ; a t ], B t = [B t\u22121 ; b t ], we get a quadratic form (in \u03b7) for D t (\u03b1(\u03b7)): D t (\u03b1(\u03b7)) = \u2212 1 2\u03bb a t \u2212 \u03b1 t\u22121 A t\u22121 2 \u03b7 2 + 1 \u03bb \u03b1 t\u22121 A t\u22121 2 \u2212 \u03b1 t\u22121 B t\u22121 + 1 \u03bb a t , \u03b1 t\u22121 A t\u22121 + b t \u03b7 \u2212 1 2\u03bb \u03b1 t\u22121 A t\u22121 2 + \u03b1 t\u22121 B t\u22121 (23)\nThis expression may be simplified by using that \u03b1 t\u22121 was the solution of the dual approximation problem at iteration t \u2212 1, hence w t = \u2212 \u03b1t\u22121At\u22121 \u03bb and v t\u22121 = D t\u22121 (\u03b1 t\u22121 ). Then:\nD t (\u03b1(\u03b7)) = \u2212 1 2\u03bb a t + \u03bbw t 2 \u03b7 2 + \u03bb 2 w t 2 + a t , w t + b t \u2212 v t\u22121 \u03b7 +v t\u22121 = \u2212 1 2 q\u03b7 2 + l\u03b7 + v t\u22121 with q = 1 \u03bb a t + \u03bbw t\u22121 2 and l = \u03bb 2 w t 2 + a t , w t + b t \u2212 v t\u22121 . Note that Eq.(15) \u21d2 l \u2265 f (w * t ) \u2212 v t\u22121 .\nWe consider now two cases. If l \u2264 0, and using that\nv t\u22121 \u2264 v t , one gets f (w * t ) \u2264 v t\u22121 \u2264 v t which yields gap t \u2264 0 \u2264 gap t\u22121 \u2212 min( gapt\u22121 2 , (gapt\u22121) 2 \u03bb 8G 2\n) assuming naturally that gap t\u22121 > > 0 since convergence was not reached at iteration t \u2212 1 (otherwise algorithm would have stopped). Note that we never observed such a singular case l \u2264 0 in our experiments but its study is required to complete the proof.\nThe case where l > 0 is not as simple and we rely on Lemma 3.1 for bounding the minimum of v t\u22121 \u2212 D t (\u03b1(\u03b7)) \u2261 1 2 q\u03b7 2 \u2212 l\u03b7:\nmin \u03b7\u2208[0,1] v t\u22121 \u2212 D t (\u03b1(\u03b7)) \u2264 \u2212 l 2 min(1, l/q) 1\nWe used solvers from the STPR toolbox available at http://cmp.felk.cvut.cz/cmp/software/stprtool/ Next, since \u2200\u03b7 \u2208\n[0, 1], v t \u2265 D t (\u03b1(\u03b7)): \u2212v t \u2264 \u2212v t\u22121 \u2212 l 2 min(1, l/q) Adding f (w * t ) to both sides, using l \u2265 f (w * t ) \u2212 v t\u22121 : f (w * t ) \u2212 v t \u2264 f (w * t ) \u2212 v t\u22121 \u2212 f (w * t )\u2212vt\u22121 2 min(1, f (w * t )\u2212vt\u22121 q ) (24\n)\nNow note that x \u2212 x 2 min(1, x/q) is monotonically in- creasing \u2200q > 0. Also f (w * t ) is monotonically decreas- ing so that f (w * t ) \u2212 v t\u22121 \u2264 f (w * t\u22121 ) \u2212 v t\u22121 = gap t\u22121 .\nPutting this together:\ngap t \u2264 gap t\u22121 \u2212 gapt\u22121 2 min(1, gap t\u22121 /q) (25\n)\nFinally we show that q \u2264 4G 2 /\u03bb. Actually, q = 1 \u03bb a t + \u03bbw t\u22121 2 = 1 \u03bb a t +\u03b1 t\u22121 A t\u22121 2 where \u03b1 t\u22121 A t\u22121 \u2264 G because \u2200i a i \u2264 G and i=1..t\u22121 \u03b1 i t\u22121 = 1. Finally a t \u2212 \u03b1 t\u22121 A t\u22121\n2 \u2264 4G 2 and we get q \u2264 4G 2 /\u03bb.\nTheorem 3.1. Algorithm 1 produces an approximation gap below after T steps where :\nT \u2264 T 0 + 8G 2 /\u03bb \u2212 2 with T 0 = 2log 2 \u03bb w1+a1/\u03bb G \u2212 2 (26)\nand converges with a rate O(1/\u03bb ).\nProof. Consider the two quantities occurring in Eq. ( 19), gap t\u22121 /2 and \u03bbgap 2 t\u22121 /8G 2 . We first show that the situation where gap t\u22121 /2 > \u03bbgap 2 t\u22121 /8G 2 (i.e. gap t\u22121 > 4G 2 /\u03bb) may only happen a finite number of iterations, T 0 . Actually if gap t\u22121 > 4G 2 /\u03bb Lemma 3.2 shows that gap t \u2264 gap t\u22121 /2 and the gap is at least divided by two every iteration. Then gap t\u22121 > 4G 2 /\u03bb may arise for at most T 0 = log 2 (\u03bbgap 1 /4G 2 )+1. Since gap 1 = \u03bb 2 w 1 +a 1 /\u03bb 2 (it may be obtained analytically since the approximation function in the first iteration is quadratic),\nT 0 = 2log 2 \u03bb w1+a1/\u03bb G \u2212 2.\nHence after at most T 0 iterations the gap decrease obeys gap t \u2212 gap t\u22121 \u2264 \u2212gap 2 t\u22121 /8G 2 \u2264 0. To estimate the number of iterations required to reach gap t \u2264 , we follow an idea from (Teo et al., 2007) and introduce a function u(t) which is an upper bound of gap t . Solving differential equation u\n(t) = \u2212 \u03bb 8G 2 u 2 (t) with boundary condition u(T 0 ) = 4G 2 /\u03bb gives u(t) = \u2212 8G 2 \u03bb(t+2\u2212T0) \u2265 gap t \u2200t \u2265 T 0 . Solving u(t) \u2264 \u21d0\u21d2 t \u2265 8G 2 /\u03bb + T 0 \u2212 2,\nthe solution is reached with accuracy within T 0 + 8G 2 /\u03bb \u2212 2 iterations.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We provide experimental results on speech and on online handwritten digit recognition and analyze experimentally the convergence behavior of our method.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Automatic Speech Recognition", "text": "We performed experiments on the TIMIT database with the standard splitting into train, development and test data. The signal was preprocessed using the procedure described in (Sha & Saul, 2007). There are 3696 utterances and over 1 million frames in the training set. A left-right HMM with one to three states and Gaussian mixture probability densities was build for each of 48 phonetic classes. We followed standard conventions in mapping the 48 phonetic labels down to 39 broader phone categories and error rates were computed as the sum of substitution, deletion, and insertion error rates from the alignment process.\nWe naturally compared our algorithms with a non discriminant system (MLE) (trained with the HTK Toolkit). In addition this MLE system is used during the training of discriminant systems both for initialization and for regularization. Actually we used the regularization term \u03bb 2 w \u2212 w M LE 2 which experimentally performs slightly better than \u03bb 2 w 2 . Moreover, using w \u2212 w M LE 2 for regularization term leads to a bigger optimal value of \u03bb than using w 2 , which reduces considerably the number of training steps (and also the number of cutting planes required to approximate well the objective function). We implemented two variants of our method (non-convex optimization or NCO), one uses the hard-max (NCO-H) and the other one (NCO-S) uses the soft-max version over all possible labellings (see Eq. (4),( 9)), this latter version is implemented with a Forward-Backward procedure. Also, we used the hamming distance for \u2206(y i , y).\nExperimentally NCO-S is about 10 times slower than NCO-H which is 2 times slower than MLE training, we give hints now. Actually learning cost mainly decomposes into two terms, computing frames probabilities and dynamic programming. In exprimental settings such as in speech recognition the first term dominates and is similar for NCO and MLE methods if training sentences include many different phones. Besides, to reach a gap < 1%, NCO-H requires about two times more iteration than MLE requires to converge. Finally NCO-H training is 2 times slower than MLE.\nWe compared our methods to three competitive discriminant methods, the large margin convex formulation of (Sha & Saul, 2007) (named Oracle), and two benchmark discriminant methods, Conditional Maximum Likelihood (CML) and Minimum Classification Error (MCE). Table 1 shows phone error rates of all these methods for one-state HMM per phone. Note that Oracle, MCE and MMI results are taken from (Sha, 2006) and correspond to the same experimental setting. These results clearly show first that discrim-  (Sha, 2006) only report results for 3 states HMM with a small number of gaussians. As may be seen in these experiments the oracle method is not able to exploit the increasing complexity of the models while our method can take advantage of the number of states to reach lower error rates. We believe that this success comes from the original non-convex formulation.", "publication_ref": ["b6", "b6", "b5", "b5"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "On-line Handwriting Recognition", "text": "On-line handwriting signals are temporal sequences of the position of an electronic pen captured through a digital tablet. We used a part of the Unipen international database with a total of 15k digit samples, 5k samples for training and 10k samples for testing. We trained a five states left-right CDHMM for each digit.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Convergence", "text": "Finally, we analyze experimentally the convergence rate of our algorithms (on speech recognition experiments). In these experiments learning is performed until the approximation gap becomes less than 1% of the objective function, which is enough to reach an optimal error-rate. Figure 2a plots the evolution of the error rate on the development set and on the test set against the learning iteration number. It is seen that the error rate remains stable after about 50 iterations.\nFigure 2b shows the evolution of the approximation gap (in log scale) as a function of the iteration number, with different values of the regularization parameter \u03bb. For clarity reasons we plot a normalized gap which is computed by dividing the gap by the number of frames in the training set. Note that the convergence rate depends directly on the value of \u03bb as observed in (Teo et al., 2007), which is not the case in traditional bundle methods. This comes naturally from the use of a regularization term. More importantly, this figure shows that the convergence rate is actually closer to O(log(1/ )) than to our theoretical proven O(1/ ). ", "publication_ref": ["b8"], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Conclusion", "text": "We described a new method for maximum margin learning of CDHMMs, that allows learning with partially labeled training sets, which is still an open problem. We showed how this optimization problem may be cast as a non-convex optimization problem for which we propose a method based on bundle meth-ods and cutting planes. We provided a convergence proof and reported experimental results on speech and handwritten digit recognition showing improved results over state of the art algorithms.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Convex analysis and minimization algorithms, i and ii", "journal": "Springer-Verlag", "year": "1993", "authors": "J Hiriart-Urruty; C Lemarechal"}, {"ref_id": "b1", "title": "Incorporating training errors for large margin hmms under semi-definite programming framework", "journal": "", "year": "2007", "authors": "H Jiang; X Li"}, {"ref_id": "b2", "title": "Training linear SVMs in linear time", "journal": "", "year": "2006", "authors": "T Joachims"}, {"ref_id": "b3", "title": "Discriminative learning for minimum error classification", "journal": "IEEE Trans. Signal Processing", "year": "1992", "authors": "B Juang; S Katagiri"}, {"ref_id": "b4", "title": "Methods of descent for nondifferentiable optimization", "journal": "Springer-Verlag", "year": "1985", "authors": "K C Kiwiel"}, {"ref_id": "b5", "title": "Large margin training of acoustic models for speech recognition", "journal": "", "year": "2006", "authors": "F Sha"}, {"ref_id": "b6", "title": "Large margin hidden markov models for automatic speech recognition", "journal": "MIT Press", "year": "2007", "authors": "F Sha; L K Saul"}, {"ref_id": "b7", "title": "Maxmargin markov networks", "journal": "", "year": "2004", "authors": "B Taskar; C Guestrin; D Koller"}, {"ref_id": "b8", "title": "A scalable modular convex solver for regularized risk minimization", "journal": "", "year": "2007", "authors": "C H Teo; Q V Le; A Smola; S V Vishwanathan"}, {"ref_id": "b9", "title": "Support vector machine learning for interdependent and structured output spaces", "journal": "", "year": "2004", "authors": "I Tsochantaridis; T Hofmann; T Joachims; Y Altun"}, {"ref_id": "b10", "title": "Large scale discriminative training of hidden markov models for speech recognition", "journal": "Computer Speech and Language", "year": "2002", "authors": "P Woodland; D Povey"}, {"ref_id": "b11", "title": "Large-margin discriminative training of hidden markov models for speech recognition", "journal": "", "year": "2007", "authors": "D Yu; L Deng"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Ns,m (x) = 1 (2\u03c0) d/2 |\u03a3s,m| 1/2 e \u2212 1 A N states HMM is defined with a set of parameters w = {\u03a0, A, \u00b5, \u03a3}. Using standard notations, \u03a0 stands for the initial state probabilities, A for transition probabilities, \u00b5 for all mean vectors \u00b5 = {\u00b5 s,m |m \u2208 [1, M ], s \u2208 [1, N ]}, and \u03a3 for all covariance matrices, \u03a3 = {\u03a3 s,m |m \u2208 [1, M ], s \u2208 [1, N ]}. The joint probability p(x, y|w) of an input-output pair x = (x 1 , ..., x T ) and y = (y 1 , ..., y L ) may be computed by summation over two sets of hidden variables: the sequence of states (called segmentation) s = (s 1 , ..., s T ); and the sequence of numbers of Gaussian components (in Gaussian mixtures) responsible for the observations of x, m = (m 1 , ..., m T ). In fact s runs over the set S(y) of segmentations matching y: p(x, y|w) = s\u2208S(y) m p(x, y, s, m|w)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": ") HMM max margin training can be easily cast in the formalism of previous section by defining the following score function, and considering z = (s, m): g(x, y, s, m, w) = log p(x, y, s, m|w) F (x, y, w) = max s\u2208S(y),m g(x, y, s, m, w)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "c w (w) = a w , w + b w s.t. c w (w ) = R(w ) and \u2202 w c w (w ) \u2208 \u2202 w R(w ) (12) Here a w \u2208 \u2202 w R(w ) is a subgradient of R(w) at w and b w = R(w ) \u2212 a w , w . Such a cutting plane c w (w) is a linear lower bound of the risk R(w) and \u03bb 2 w 2 + c w (w) is a quadratic lower bound of f (w).", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 .2Figure 2. Training LMHMM with 3 states and 4 Gaussians for speech recognition", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "= argmin wj \u2208{w1,...wt} f (w j )", "figure_data": "Algorithm 1 Non-Convex Cutting Plane1: Input: w 1 , \u03bb,2: for t = 1 to \u221e do3:Define c wt according to Eq. (12)4: t 5: w  *  if condition (14) is not satisfied then6:c t = SolveConf lict(w  *  t , w t , c wt )7:else c t = c wt8:a t , w  *  t + b t = a t , w  *  t + f (w  *  t ) \u2212 \u03bb = R(w  *  t ) + a t + \u03bb 2 (w  *  t + w t ), w  *  t \u2212 w t 2 w t 2 \u2212 a t , w t(17)where we used the definition of objective functionf (w  *  t ) = \u03bb 2 w  *  t for a t (Cf. Line 5) and obtain 2 +R(w  *  t ). Then, we substitute \u2212\u03bbw  *  ta t , w  *  t + b t = R(w  *  t ) \u2212 \u03bb 2 w  *  t \u2212 w t \u2264 R(w  *  t )2(18)Convergence AnalysisOur algorithm improves in two ways over previousstandard non-convex bundle methods. First, it gen-erates a sequence w t that converges to a solution w  *where f (w  *  ) \u2264 f (w t )\u2200t, which is not guarantied withstandard methods that may generate (stationary) clus-ter points, not necessarily better than previous solu-tions. Second, we provide below an upper bound onthe convergence rate of our algorithm which convergeswith O(1/ ) rate, one cannot derive such a rate forstandard bundle methods. Experimentally, after hav-ing reached \"a moderate gap\"(which is fast) no con-flicts arise and our algorithm behaves like (Teo et al.,"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Phone error rates with single state phone HMMs (N = 1) and mixtures of M Gaussian laws per state.Table 2. Phone error rates with multi-state phone HMMs.", "figure_data": "MMLENCO-HNCO-SOracleCMLMCE144.75 31.4431.02 31.236.4 35.6239.54 29.7030.21 30.834.6 34.5436.06 29.1329.30 29.832.8 32.4834.46 28.2929.11 28.231.5 30.9NM MLE NCO-H Oracle(Sha, 2006)2 states 138.21 29.57Not Available2 states 234.14 27.99NA2 states 432.00 27.67NA2 states 831.25 27.58NA3 states 136.70 28.7037.83 states 231.92 27.9332.63 states 430.28 27.40NA3 states 829.55 27.61NAinant approaches significantly outperform MLE train-ing, and second that large margin approaches (NCOand Oracle) significantly outperform the two other dis-criminant methods. Note also that the two variantsof our method NCO-H and NCO-S perform similarly.Since NCO-H is much faster we report only NCO-Hresults in the following. Table 2 shows results witha few states per left-right phone HMM, for the twomost efficient techniques (NCO and Oracle) only. Notethat"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "reports classification error rates of three systems, namely MLE, the Oracle method and NCO-H. Again, one can see that our method reaches the best results whatever M the number of Gaussian in Gaussian mixtures. NCO-H is shown to significantly outperform the Oracle based method showing that our algorithm has been able here too to efficiently learn from partially labeled training samples.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Handwritten digit recognition error rates(N = 5)    ", "figure_data": "M MLE Oracle NCO-H1 13.502.011.532 10.501.701.3339.481.821.534 15.021.741.56"}], "formulas": [{"formula_id": "formula_0", "formula_text": "x i = (x i 1 , x i 2 , ..., x i T ) \u2208 (R d )", "formula_coordinates": [2.0, 55.44, 646.91, 128.49, 12.48]}, {"formula_id": "formula_1", "formula_text": "h(x, w) = argmax y\u2208Y F (x, y, w)(1)", "formula_coordinates": [2.0, 362.1, 157.77, 179.34, 17.19]}, {"formula_id": "formula_2", "formula_text": "min w,\u03be \u03bb 2 w 2 + i \u03be i s.t. F (x i ,y i ,w)\u2265F (x i ,y,w)+\u2206(y i ,y)\u2212\u03be i \u2200i\u2200y =y i \u03be i \u22650 \u2200i(2)", "formula_coordinates": [2.0, 319.42, 321.43, 222.02, 38.75]}, {"formula_id": "formula_3", "formula_text": "minw \u03bb 2 w 2 + i maxy(F (x i ,y,w)+\u2206(y i ,y)\u2212F (x i ,y i ,w)) f (w)", "formula_coordinates": [2.0, 314.9, 423.41, 218.58, 26.66]}, {"formula_id": "formula_4", "formula_text": "p(x|s) = M m=1 p s,m N s,m (x)(5)", "formula_coordinates": [2.0, 364.38, 621.69, 177.06, 14.11]}, {"formula_id": "formula_6", "formula_text": "def = p s,m N s,m (x): p(x, y, s, m|w) = \u03c0 s1 p s1,m1 (x 1 ) T t=2 a st\u22121,st p st,mt (x t )", "formula_coordinates": [3.0, 57.65, 275.9, 234.53, 38.34]}, {"formula_id": "formula_7", "formula_text": "p(x, y|w) \u2248 max s,m p(x, y, s, m|w)(7", "formula_coordinates": [3.0, 98.42, 357.02, 186.78, 10.32]}, {"formula_id": "formula_9", "formula_text": "F sof t (x, y, w) = log \uf8eb \uf8ed s\u2208S(y),m e g(x", "formula_coordinates": [3.0, 67.77, 475.98, 150.57, 34.15]}, {"formula_id": "formula_10", "formula_text": "f o (w)= \u03bb 2 w 2 + i maxy(F (x i ,y,w)+\u2206(y i ,y)\u2212g(x i ,y i ,s i gs ,m i gs ,w)) (10", "formula_coordinates": [3.0, 307.44, 321.76, 245.35, 25.69]}, {"formula_id": "formula_11", "formula_text": ")", "formula_coordinates": [3.0, 537.01, 337.49, 4.43, 9.96]}, {"formula_id": "formula_12", "formula_text": "min w f (w) = \u03bb 2 w 2 + R(w) (11)", "formula_coordinates": [3.0, 364.23, 517.38, 177.21, 22.87]}, {"formula_id": "formula_13", "formula_text": "w t+1 = argmin w g t (w) v t = min w g t (w) with g t (w) = \u03bb 2 w 2 + max j=1..t {c j (w)} (13)", "formula_coordinates": [4.0, 69.41, 371.25, 220.03, 36.57]}, {"formula_id": "formula_14", "formula_text": "c wt (w * t ) = a wt , w * t + b wt \u2264 R(w * t )(14)", "formula_coordinates": [4.0, 345.72, 595.16, 195.72, 12.69]}, {"formula_id": "formula_15", "formula_text": "\u03bb 2 w t 2 + c t (w t ) \u2265 f (w * t ) (15)", "formula_coordinates": [5.0, 116.73, 121.45, 172.71, 22.87]}, {"formula_id": "formula_16", "formula_text": "b t \u2264 R(w * t ) \u2212 a wt , w * t = U b t \u2265 f (w * t ) \u2212 \u03bb 2 w t 2 \u2212 a wt , w t = L (16", "formula_coordinates": [5.0, 77.53, 275.23, 207.48, 25.52]}, {"formula_id": "formula_17", "formula_text": "9: gap t = f (w * t ) \u2212 v", "formula_coordinates": [5.0, 312.23, 177.22, 97.87, 12.19]}, {"formula_id": "formula_18", "formula_text": "[a t , b t ] = [\u2212\u03bbw * t , f (w * t ) \u2212 \u03bb 2 w t 2 \u2212 a t , w t ] 2007", "formula_coordinates": [5.0, 307.44, 286.78, 220.2, 45.6]}, {"formula_id": "formula_19", "formula_text": "gap t\u22121 \u2212 gap t \u2265 min( gap t\u22121 2 , (gap t\u22121 ) 2 \u03bb 8G 2 ) (19", "formula_coordinates": [5.0, 323.93, 505.68, 213.08, 24.48]}, {"formula_id": "formula_20", "formula_text": ")", "formula_coordinates": [5.0, 537.02, 513.37, 4.43, 9.96]}, {"formula_id": "formula_21", "formula_text": "v t = min w,\u03be \u03bb 2 w 2 + \u03be s.t. a j , w + b j \u2264 \u03be j = 1..t (20", "formula_coordinates": [5.0, 329.21, 601.05, 207.81, 23.49]}, {"formula_id": "formula_22", "formula_text": ")", "formula_coordinates": [5.0, 537.01, 608.14, 4.43, 9.96]}, {"formula_id": "formula_23", "formula_text": "v t = max \u03b1\u2208R t D t (\u03b1) = \u2212 \u03bb 2 \u03b1At \u03bb 2 + \u03b1B t s.t \u03b1 i \u2265 0 \u2200i = 1..t i=1..t \u03b1 i = 1 (21", "formula_coordinates": [5.0, 319.64, 683.7, 217.37, 37.1]}, {"formula_id": "formula_24", "formula_text": ")", "formula_coordinates": [5.0, 537.01, 696.89, 4.43, 9.96]}, {"formula_id": "formula_25", "formula_text": "\u03b1(\u03b7) = \u03b7\u03b1 start + (1 \u2212 \u03b7)\u03b1 end \u03b7 \u2208 [0, 1] (22) is feasible. Note also that D t (\u03b1 start ) = D t\u22121 (\u03b1 t\u22121 ) = v t\u22121 , which implies naturally that v t \u2265 v t\u22121 .", "formula_coordinates": [6.0, 55.44, 226.81, 234.0, 44.86]}, {"formula_id": "formula_26", "formula_text": "A t = [A t\u22121 ; a t ], B t = [B t\u22121 ; b t ], we get a quadratic form (in \u03b7) for D t (\u03b1(\u03b7)): D t (\u03b1(\u03b7)) = \u2212 1 2\u03bb a t \u2212 \u03b1 t\u22121 A t\u22121 2 \u03b7 2 + 1 \u03bb \u03b1 t\u22121 A t\u22121 2 \u2212 \u03b1 t\u22121 B t\u22121 + 1 \u03bb a t , \u03b1 t\u22121 A t\u22121 + b t \u03b7 \u2212 1 2\u03bb \u03b1 t\u22121 A t\u22121 2 + \u03b1 t\u22121 B t\u22121 (23)", "formula_coordinates": [6.0, 55.44, 291.24, 234.0, 81.46]}, {"formula_id": "formula_27", "formula_text": "D t (\u03b1(\u03b7)) = \u2212 1 2\u03bb a t + \u03bbw t 2 \u03b7 2 + \u03bb 2 w t 2 + a t , w t + b t \u2212 v t\u22121 \u03b7 +v t\u22121 = \u2212 1 2 q\u03b7 2 + l\u03b7 + v t\u22121 with q = 1 \u03bb a t + \u03bbw t\u22121 2 and l = \u03bb 2 w t 2 + a t , w t + b t \u2212 v t\u22121 . Note that Eq.(15) \u21d2 l \u2265 f (w * t ) \u2212 v t\u22121 .", "formula_coordinates": [6.0, 55.44, 436.84, 234.0, 83.38]}, {"formula_id": "formula_28", "formula_text": "v t\u22121 \u2264 v t , one gets f (w * t ) \u2264 v t\u22121 \u2264 v t which yields gap t \u2264 0 \u2264 gap t\u22121 \u2212 min( gapt\u22121 2 , (gapt\u22121) 2 \u03bb 8G 2", "formula_coordinates": [6.0, 55.44, 537.92, 234.0, 27.71]}, {"formula_id": "formula_29", "formula_text": "min \u03b7\u2208[0,1] v t\u22121 \u2212 D t (\u03b1(\u03b7)) \u2264 \u2212 l 2 min(1, l/q) 1", "formula_coordinates": [6.0, 67.88, 676.2, 202.61, 26.25]}, {"formula_id": "formula_30", "formula_text": "[0, 1], v t \u2265 D t (\u03b1(\u03b7)): \u2212v t \u2264 \u2212v t\u22121 \u2212 l 2 min(1, l/q) Adding f (w * t ) to both sides, using l \u2265 f (w * t ) \u2212 v t\u22121 : f (w * t ) \u2212 v t \u2264 f (w * t ) \u2212 v t\u22121 \u2212 f (w * t )\u2212vt\u22121 2 min(1, f (w * t )\u2212vt\u22121 q ) (24", "formula_coordinates": [6.0, 307.44, 69.55, 229.72, 84.23]}, {"formula_id": "formula_31", "formula_text": ")", "formula_coordinates": [6.0, 537.01, 135.22, 4.43, 9.96]}, {"formula_id": "formula_32", "formula_text": "Now note that x \u2212 x 2 min(1, x/q) is monotonically in- creasing \u2200q > 0. Also f (w * t ) is monotonically decreas- ing so that f (w * t ) \u2212 v t\u22121 \u2264 f (w * t\u22121 ) \u2212 v t\u22121 = gap t\u22121 .", "formula_coordinates": [6.0, 307.44, 166.94, 234.0, 36.42]}, {"formula_id": "formula_33", "formula_text": "gap t \u2264 gap t\u22121 \u2212 gapt\u22121 2 min(1, gap t\u22121 /q) (25", "formula_coordinates": [6.0, 324.55, 221.07, 212.46, 14.47]}, {"formula_id": "formula_34", "formula_text": ")", "formula_coordinates": [6.0, 537.01, 223.38, 4.43, 9.96]}, {"formula_id": "formula_35", "formula_text": "Finally we show that q \u2264 4G 2 /\u03bb. Actually, q = 1 \u03bb a t + \u03bbw t\u22121 2 = 1 \u03bb a t +\u03b1 t\u22121 A t\u22121 2 where \u03b1 t\u22121 A t\u22121 \u2264 G because \u2200i a i \u2264 G and i=1..t\u22121 \u03b1 i t\u22121 = 1. Finally a t \u2212 \u03b1 t\u22121 A t\u22121", "formula_coordinates": [6.0, 307.44, 241.48, 234.0, 47.44]}, {"formula_id": "formula_36", "formula_text": "T \u2264 T 0 + 8G 2 /\u03bb \u2212 2 with T 0 = 2log 2 \u03bb w1+a1/\u03bb G \u2212 2 (26)", "formula_coordinates": [6.0, 356.71, 326.99, 184.73, 26.86]}, {"formula_id": "formula_37", "formula_text": "T 0 = 2log 2 \u03bb w1+a1/\u03bb G \u2212 2.", "formula_coordinates": [6.0, 368.68, 504.16, 114.5, 14.38]}, {"formula_id": "formula_38", "formula_text": "(t) = \u2212 \u03bb 8G 2 u 2 (t) with boundary condition u(T 0 ) = 4G 2 /\u03bb gives u(t) = \u2212 8G 2 \u03bb(t+2\u2212T0) \u2265 gap t \u2200t \u2265 T 0 . Solving u(t) \u2264 \u21d0\u21d2 t \u2265 8G 2 /\u03bb + T 0 \u2212 2,", "formula_coordinates": [6.0, 307.44, 582.78, 234.0, 51.33]}], "doi": ""}