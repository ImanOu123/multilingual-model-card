{"title": "Bridging the Gap between Training and Inference for Neural Machine Translation", "authors": "Wen Zhang; Yang Feng; Fandong Meng; Di You; Qun Liu", "pub_date": "", "abstract": "Neural Machine Translation (NMT) generates target words sequentially in the way of predicting the next word conditioned on the context words. At training time, it predicts with the ground truth words as context while at inference it has to generate the entire sequence from scratch. This discrepancy of the fed context leads to error accumulation among the way. Furthermore, word-level training requires strict matching between the generated sequence and the ground truth sequence which leads to overcorrection over different but reasonable translations. In this paper, we address these issues by sampling context words not only from the ground truth sequence but also from the predicted sequence by the model during training, where the predicted sequence is selected with a sentence-level optimum. Experiment results on Chinese\u2192English and WMT'14 English\u2192German translation tasks demonstrate that our approach can achieve significant improvements on multiple datasets.", "sections": [{"heading": "Introduction", "text": "Neural Machine Translation has shown promising results and drawn more attention recently. Most NMT models fit in the encoder-decoder framework, including the RNN-based (Sutskever et al., 2014;Bahdanau et al., 2015;Meng and Zhang, 2019), the CNN-based (Gehring et al., 2017) and the attention-based (Vaswani et al., 2017) models, which predict the next word conditioned on the previous context words, deriving a language model over target words. The scenario is at training time the ground truth words are used as context * Corresponding author.\nwhile at inference the entire sequence is generated by the resulting model on its own and hence the previous words generated by the model are fed as context. As a result, the predicted words at training and inference are drawn from different distributions, namely, from the data distribution as opposed to the model distribution. This discrepancy, called exposure bias (Ranzato et al., 2015), leads to a gap between training and inference. As the target sequence grows, the errors accumulate among the sequence and the model has to predict under the condition it has never met at training time.\nIntuitively, to address this problem, the model should be trained to predict under the same condition it will face at inference. Inspired by DATA AS DEMONSTRATOR (DAD) (Venkatraman et al., 2015), feeding as context both ground truth words and the predicted words during training can be a solution. NMT models usually optimize the cross-entropy loss which requires a strict pairwise matching at the word level between the predicted sequence and the ground truth sequence. Once the model generates a word deviating from the ground truth sequence, the cross-entropy loss will correct the error immediately and draw the remaining generation back to the ground truth sequence. However, this causes a new problem. A sentence usually has multiple reasonable translations and it cannot be said that the model makes a mistake even if it generates a word different from the ground truth word. For example, reference: We should comply with the rule. cand1:\nWe should abide with the rule. cand2:\nWe should abide by the law. cand3:\nWe should abide by the rule.\nonce the model generates \"abide\" as the third target word, the cross-entropy loss would force the model to generate \"with\" as the fourth word (as cand1) so as to produce larger sentence-level likelihood and be in line with the reference, although \"by\" is the right choice. Then, \"with\" will be fed as context to generate \"the rule\", as a result, the model is taught to generate \"abide with the rule\" which actually is wrong. The translation cand1 can be treated as overcorrection phenomenon. Another potential error is that even the model predicts the right word \"by\" following \"abide\", when generating subsequent translation, it may produce \"the law\" improperly by feeding \"by\" (as cand2). Assume the references and the training criterion let the model memorize the pattern of the phrase \"the rule\" always following the word \"with\", to help the model recover from the two kinds of errors and create the correct translation like cand3, we should feed \"with\" as context rather than \"by\" even when the previous predicted phrase is \"abide by\". We refer to this solution as Overcorrection Recovery (OR).\nIn this paper, we present a method to bridge the gap between training and inference and improve the overcorrection recovery capability of NMT. Our method first selects oracle words from its predicted words and then samples as context from the oracle words and ground truth words. Meanwhile, the oracle words are selected not only with a wordby-word greedy search but also with a sentencelevel evaluation, e.g. BLEU, which allows greater flexibility under the pairwise matching restriction of cross-entropy. At the beginning of training, the model selects as context ground truth words at a greater probability. As the model converges gradually, oracle words are chosen as context more often. In this way, the training process changes from a fully guided scheme towards a less guided scheme. Under this mechanism, the model has the chance to learn to handle the mistakes made at inference and also has the ability to recover from overcorrection over alternative translations. We verify our approach on both the RNNsearch model and the stronger Transformer model. The results show that our approach can significantly improve the performance on both models.", "publication_ref": ["b14", "b0", "b8", "b4", "b16", "b10", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "RNN-based NMT Model", "text": "Our method can be applied in a variety of NMT models. Without loss of generality, we take the RNN-based NMT (Bahdanau et al., 2015) as an example to introduce our method. Assume the source sequence and the observed translation are (Cho et al., 2014) is used to acquire two sequences of hidden states, the annotation of\nx = {x 1 , \u2022 \u2022 \u2022 , x |x| } and y * = {y * 1 , \u2022 \u2022 \u2022 , y * |y * | }. Encoder. A bidirectional Gated Recurrent Unit (GRU)\nx i is h i = [ \u2212 \u2192 h i ; \u2190 \u2212 h i ].\nNote that e x i is employed to represent the embedding vector of the word\nx i . \u2212 \u2192 h i = GRU(e x i , \u2212 \u2192 h i\u22121 ) (1) \u2190 \u2212 h i = GRU(e x i , \u2190 \u2212 h i+1 ) (2)\nAttention. The attention is designed to extract source information (called source context vector). At the j-th step, the relevance between the target word y * j and the i-th source word is evaluated and normalized over the source sequence\nr ij = v T a tanh (W a s j\u22121 + U a h i )(3)\n\u03b1 ij = exp (r ij ) |x| i =1 exp r i j (4)\nThe source context vector is the weighted sum of all source annotations and can be calculated by\nc j = |x| i=1 \u03b1 ij h i (5)\nDecoder. The decoder employs a variant of GRU to unroll the target information. At the j-th step, the target hidden state s j is given by\ns j = GRU(e y * j\u22121 , s j\u22121 , c j )(6)\nThe probability distribution P j over all the words in the target vocabulary is produced conditioned on the embedding of the previous ground truth word, the source context vector and the hidden state\nt j = g e y * j\u22121 , c j , s j (7) o j = W o t j (8) P j = softmax (o j )(9)\nwhere g stands for a linear transformation, W o is used to map t j to o j so that each target word has one corresponding dimension in o j .", "publication_ref": ["b0", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Approach", "text": "The main framework (as shown in Figure 1) of our method is to feed as context either the ground truth words or the previous predicted words, i.e. oracle GRU cell words, with a certain probability. This potentially can reduce the gap between training and inference by training the model to handle the situation which will appear during test time. We will introduce two methods to select the oracle words. One method is to select the oracle words at the word level with a greedy search algorithm, and another is to select a oracle sequence at the sentence-level optimum.\ns j\u22121 \u2022 \u2022 \u2022 y * j\u22121 Logistic regression classifier \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 Decoder y oracle j\u22121 1 \u2212 p 1 \u2212 p p p\nThe sentence-level oracle provides an option of ngram matching with the ground truth sequence and hence inherently has the ability of recovering from overcorrection for the alternative context. To predict the j-th target word y j , the following steps are involved in our approach:\n1. Select an oracle word y oracle j\u22121 (at word level or sentence level) at the {j\u22121}-th step. (Section Oracle Word Selection) 2. Sample from the ground truth word y * j\u22121 with a probability of p or from the oracle word y oracle j\u22121 with a probability of 1\u2212p. (Section Sampling with Decay) 3. Use the sampled word as y j\u22121 and replace the y * j\u22121 in Equation ( 6) and ( 7) with y j\u22121 , then perform the following prediction of the attention-based NMT.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Oracle Word Selection", "text": "Generally, at the j-th step, the NMT model needs the ground truth word y * j\u22121 as the context word to predict y j , thus, we could select an oracle word y oracle j\u22121 to simulate the context word. The oracle word should be a word similar to the ground truth or a synonym. Using different strategies will produce a different oracle word y oracle j\u22121 . One option is that word-level greedy search could be employed to output the oracle word of each step, which is called Word-level Oracle (called WO). Besides, we can further optimize the oracle by enlarging the search space with beam search and then reranking the candidate translations with a sentencelevel metric, e.g. BLEU (Papineni et al., 2002), GLEU , ROUGE (Lin, 2004), etc, the selected translation is called oracle sentence, the words in the translation are Sentence-level Oracle (denoted as SO).", "publication_ref": ["b9", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Word-Level Oracle", "text": "For the {j\u22121}-th decoding step, the direct way to select the word-level oracle is to pick the word with the highest probability from the word distribution P j\u22121 drawn by Equation ( 9), which is shown in Figure 2. The predicted score in o j\u22121 is the value before the softmax operation. In practice, we can acquire more robust word-level oracles by introducing the Gumbel-Max technique (Gumbel, 1954;Maddison et al., 2014), which provides a simple and efficient way to sample from a categorical distribution. The Gumbel noise, treated as a form of regularization, is added to o j\u22121 in Equation ( 8), as shown in Figure 3, then softmax function is performed, the word distribution of y j\u22121 is approximated by\n\u03b7 = \u2212 log (\u2212 log u) (10) o j\u22121 = (o j\u22121 + \u03b7) /\u03c4 (11\n)\nP j\u22121 = softmax (\u00f5 j\u22121 ) (12\n)\nwhere \u03b7 is the Gumbel noise calculated from a uniform random variable u \u223c U(0, 1), \u03c4 is temperature. As \u03c4 approaches 0, the softmax function is similar to the argmax operation, and it becomes uniform distribution gradually when \u03c4 \u2192 \u221e.\nSimilarly, according toP j\u22121 , the 1-best word is selected as the word-level oracle word\ny oracle j\u22121 = y WO j\u22121 = argmax P j\u22121 (13)\nNote that the Gumbel noise is just used to select the oracle and it does not affect the loss function for training.", "publication_ref": ["b5", "b7"], "figure_ref": ["fig_1", "fig_2"], "table_ref": []}, {"heading": "Sentence-Level Oracle", "text": "The sentence-level oracle is employed to allow for more flexible translation with n-gram matching required by a sentence-level metric. In this paper, we employ BLEU as the sentence-level metric. To select the sentence-level oracles, we first perform beam search for all sentences in each batch, assuming beam size is k, and get k-best candidate translations. In the process of beam search, we also could apply the Gumbel noise for each word generation. We then evaluate each translation by calculating its BLEU score with the ground truth sequence, and use the translation with the highest BLEU score as the oracle sentence. We denote it as y S = (y S 1 , ..., y S |y S | ), then at the j-th decoding step, we define the sentence-level oracle word as\n+ = Noise \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 + \u2022 \u2022 \u2022 Predicted Score y oracle j\u22121 1-best", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Logistic regression classifier", "text": "y oracle j\u22121 = y SO j\u22121 = y S j\u22121 (14)\nBut a problem comes with sentence-level oracle.\nAs the model samples from ground truth word and the sentence-level oracle word at each step, the two sequences should have the same number of words. However we can not assure this with the naive beam search decoding algorithm. Based on the above problem, we introduce force decoding to make sure the two sequences have the same length. Force Decoding. As the length of the ground truth sequence is |y * |, the goal of force decoding is to generate a sequence with |y * | words followed by a special end-of-sentence (EOS) symbol. Therefore, in beam search, once a candidate translation tends to end with EOS when it is shorter or longer than |y * |, we will force it to generate |y * | words, that is,\n\u2022 If the candidate translation gets a word distribution P j at the j-th step where j |y * | and EOS is the top first word in P j , then we select the top second word in P j as the j-th word of this candidate translation.\n\u2022 If the candidate translation gets a word distribution P |y * |+1 at the {|y * |+1}-th step where EOS is not the top first word in P |y * |+1 , then we select EOS as the {|y * |+1}-th word of this candidate translation.\nIn this way, we can make sure that all the k candidate translations have |y * | words, then re-rank the k candidates according to BLEU score and select the top first as the oracle sentence. For adding Gumbel noise into the sentence-level oracle selection, we replace the P j withP j at the j-th decoding step during force decoding.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sampling with Decay", "text": "In our method, we employ a sampling mechanism to randomly select the ground truth word y * j\u22121 or the oracle word y oracle j\u22121 as y j\u22121 . At the beginning of training, as the model is not well trained, using y oracle j\u22121 as y j\u22121 too often would lead to very slow convergence, even being trapped into local optimum. On the other hand, at the end of training, if the context y j\u22121 is still selected from the ground truth word y * j\u22121 at a large probability, the model is not fully exposed to the circumstance which it has to confront at inference and hence can not know how to act in the situation at inference. In this sense, the probability p of selecting from the ground truth word can not be fixed, but has to decrease progressively as the training advances. At the beginning, p=1, which means the model is trained entirely based on the ground truth words. As the model converges gradually, the model selects from the oracle words more often.\nBorrowing ideas from but being different from  which used a schedule to decrease p as a function of the index of mini-batch, we define p with a decay function dependent on the index of training epochs e (starting from 0)\np = \u00b5 \u00b5 + exp (e/\u00b5)(15)\nwhere \u00b5 is a hyper-parameter. The function is strictly monotone decreasing. As the training proceeds, the probability p of feeding ground truth words decreases gradually.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Training", "text": "After selecting y j\u22121 by using the above method, we can get the word distribution of y j according to Equation ( 6), ( 7), ( 8) and ( 9). We do not add the Gumbel noise to the distribution when calculating loss for training. The objective is to maximize the probability of the ground truth sequence based on maximum likelihood estimation (MLE).\nThus following loss function is minimized:\nL (\u03b8) = \u2212 N n=1 |y n | j=1 log P n j y n j (16\n)\nwhere N is the number of sentence pairs in the training data, |y n | indicates the length of the n-th ground truth sentence, P n j refers to the predicted probability distribution at the j-th step for the n-th sentence, hence P n j y n j is the probability of generating the ground truth word y n j at the j-th step. further developed the method by sampling as context from the previous ground truth word and the previous predicted word with a changing probability, not treating them equally in the whole training process. This is similar to our method, but they do not include the sentence-level oracle to relieve the overcorrection problem and neither the noise perturbations on the predicted distribution.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Another direction of attempts is the sentencelevel training with the thinking that the sentencelevel metric, e.g., BLEU, brings a certain degree of flexibility for generation and hence is more robust to mitigate the exposure bias problem. To avoid the problem of exposure bias, Ranzato et al. (2015) presented a novel algorithm Mixed Incremental Cross-Entropy Reinforce (MIXER) for sequence-level training, which directly optimized the sentence-level BLEU used at inference. Shen et al. (2016) introduced the Minimum Risk Training (MRT) into the end-to-end NMT model, which optimized model parameters by minimizing directly the expected loss with respect to arbitrary evaluation metrics, e.g., sentence-level BLEU. Shao et al. (2018) proposed to eliminate the exposure bias through a probabilistic n-gram matching objective, which trains NMT NMT under the greedy decoding strategy.", "publication_ref": ["b13", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We carry out experiments on the NIST Chinese\u2192English (Zh\u2192En) and the WMT'14 English\u2192German (En\u2192De) translation tasks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Settings", "text": "For Zh\u2192En, the training dataset consists of 1.25M sentence pairs extracted from LDC corpora 1 . We choose the NIST 2002 (MT02) dataset as the validation set, which has 878 sentences, and the NIST 2003 (MT03), NIST 2004 (MT04), NIST 2005 (MT05) and NIST 2006 (MT06) datasets as the test sets, which contain 919, 1788, 1082 and 1664 sentences respectively. For En\u2192De, we perform our experiments on the corpus provided by WMT'14, which contains 4.5M sentence pairs 2 . We use the newstest2013 as the validation set, and the newstest2014 as the test sets, which containing 3003 and 2737 sentences respectively. We measure the translation quality with BLEU scores (Papineni et al., 2002). For Zh\u2192En, caseinsensitive BLEU score is calculated by using the mteval-v11b.pl script. For En\u2192De, we tokenize the references and evaluate the performance with case-sensitive BLEU score by the multi-bleu.pl script. The metrics are exactly the same as in previous work. Besides, we make statistical significance test according to the method of Collins et al. (2005).\nIn training the NMT model, we limit the source and target vocabulary to the most frequent 30K words for both sides in the Zh\u2192En translation task, covering approximately 97.7% and 99.3% words of two corpus respectively. For the En\u2192De translation task, sentences are encoded using bytepair encoding (BPE) (Sennrich et al., 2016) with 37k merging operations for both source and target languages, which have vocabularies of 39418 and 40274 tokens respectively. We limit the length of sentences in the training datasets to 50 words for Zh\u2192En and 128 subwords for En\u2192De. For RNNSearch model, the dimension of word embedding and hidden layer is 512, and the beam size in testing is 10. All parameters are initialized by the uniform distribution over [\u22120.1, 0.1]. The mini-batch stochastic gradient descent (SGD) algorithm is employed to train the model parameters with batch size setting to 80. Moreover, the learning rate is adjusted by adadelta optimizer (Zeiler, 2012) with \u03c1=0.95 and =1e-6. Dropout is applied on the output layer with dropout rate being 0.5.  default settings (fairseq 3 ).", "publication_ref": ["b9", "b3", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Systems", "text": "The following systems are involved:\nRNNsearch: Our implementation of an improved model as described in Section 2, where the decoder employs two GRUs and an attention. Specifically, Equation 6 is substituted with:\ns j = GRU 1 (e y * j\u22121 , s j\u22121 ) (17\n)\ns j = GRU 2 (c j ,s j )(18)\nBesides, in Equation 3, s j\u22121 is replaced withs j\u22121 .\nSS-NMT: Our implementation of the scheduled sampling (SS) method  on the basis of the RNNsearch. The decay scheme is the same as Equation 15 in our approach.\nMIXER: Our implementation of the mixed incremental cross-entropy reinforce (Ranzato et al., 2015), where the sentence-level metric is BLEU and the average reward is acquired according to its offline method with a 1-layer linear regressor.\nOR-NMT: Based on the RNNsearch, we introduced the word-level oracles, sentence-level oracles and the Gumbel noises to enhance the overcorrection recovery capacity. For the sentencelevel oracle selection, we set the beam size to be 3, set \u03c4 =0.5 in Equation ( 11) and \u00b5=12 for the decay function in Equation ( 15). OR-NMT is the abbreviation of NMT with Overcorrection Recovery.\n3 https://github.com/pytorch/fairseq", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "Results on Zh\u2192En Translation", "text": "We verify our method on two baseline models with the NIST Zh\u2192En datasets in this section.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results on the RNNsearch", "text": "As shown in Table 1, Tu et al. (2016) propose to model coverage in RNN-based NMT to improve the adequacy of translations. Shen et al. (2016) propose minimum risk training (MRT) for NMT to directly optimize model parameters with respect to BLEU scores. Zhang et al. (2017) model distortion to enhance the attention model. Compared with them, our baseline system RNNsearch 1) outperforms previous shallow RNN-based NMT system equipped with the coverage model (Tu et al., 2016); and 2) achieves competitive performance with the MRT (Shen et al., 2016) and the Distortion (Zhang et al., 2017) on the same datasets. We hope that the strong shallow baseline system used in this work makes the evaluation convincing.\nWe also compare with the other two related methods that aim at solving the exposure bias problem, including the scheduled sampling    our approach further gives a significant improvements on most test sets and achieves improvement by about +1.2 BLEU points on average.", "publication_ref": ["b15", "b13", "b20", "b15", "b13", "b20"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Results on the Transformer", "text": "The methods we propose can also be adapted to the stronger Transformer model. The evaluated results are listed in Table 1. Our word-level method can improve the base model by +0.54 BLEU points on average, and the sentence-level method can further bring in +1.0 BLEU points improvement.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Factor Analysis", "text": "We propose several strategies to improve the performance of approach on relieving the overcorrection problem, including utilizing the word-level oracle, the sentence-level oracle, and incorporating the Gumbel noise for oracle selection. To investigate the influence of these factors, we conduct the experiments and list the results in Table 2.\nWhen only employing the word-level oracle, the translation performance was improved by +1.21 BLEU points, this indicates that feeding predicted words as context can mitigate exposure bias. When employing the sentence-level oracle, we can further achieve +0.62 BLEU points improvement. It shows that the sentence-level oracle performs better than the word-level oracle in terms of BLEU. We conjecture that the superiority may come from a greater flexibility for word generation which can mitigate the problem of overcorrection. By incorporating the Gumbel noise during the generation of the word-level and sentencelevel oracle words, the BLEU score are further improved by 0.56 and 0.53 respectively. This indicates Gumbel noise can help the selection of each oracle word, which is consistent with our claim that Gumbel-Max provides a efficient and robust way to sample from a categorical distribution.  ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "About Convergence", "text": "In this section, we analyze the influence of different factors for the convergence. Figure 4 gives the training loss curves of the RNNsearch, word-level oracle (WO) without noise and sentence-level oracle (SO) with noise. In training, BLEU score on the validation set is used to select the best model, a detailed comparison among the BLEU score curves under different factors is shown in Figure 5. RNNsearch converges fast and achieves the best result at the 7-th epoch, while the training loss continues to decline after the 7-th epoch until the end. Thus, the training of RNNsearch may encounter the overfitting problem. Figure 4 and 5 also reveal that, integrating the oracle sampling and the Gumbel noise leads to a little slower convergence and the training loss does not keep decreasing after the best results appear on the validation set. This is consistent with our intuition that oracle sampling and noises can avoid overfit- Figure 6 shows the BLEU scores curves on the MT03 test set under different factors 4 . When sampling oracles with noise (\u03c4 =0.5) on the sentence level, we obtain the best model. Without noise, our system converges to a lower BLEU score. This can be understood easily that using its own results repeatedly during training without any regularization will lead to overfitting and quick convergence. In this sense, our method benefits from the sentence-level sampling and Gumbel noise.", "publication_ref": [], "figure_ref": ["fig_4", "fig_5", "fig_4", "fig_6"], "table_ref": []}, {"heading": "About Length", "text": "Figure 7 shows the BLEU scores of generated translations on the MT03 test set with respect to the lengths of the source sentences. In particular, we split the translations for the MT03 test set into different bins according to the length of source sentences, then test the BLEU scores for translations in each bin separately with the results reported in Figure 7. Our approach can achieve big improvements over the baseline system in all bins, especially in the bins (10,20], (40,50] and (70,80] of the super-long sentences. The crossentropy loss requires that the predicted sequence is exactly the same as the ground truth sequence which is more difficult to achieve for long sentences, while our sentence-level oracle can help recover from this kind of overcorrection.", "publication_ref": [], "figure_ref": ["fig_7", "fig_7"], "table_ref": []}, {"heading": "Effect on Exposure Bias", "text": "To validate whether the improvements is mainly obtained by addressing the exposure bias problem, we randomly select 1K sentence pairs from the Zh\u2192En training data, and use the pre-trained RNNSearch model and proposed model to decode the source sentences. The BLEU score of RNNSearch model was 24.87, while our model produced +2.18 points. We then count the ground truth words whose probabilities in the predicted distributions produced by our model are greater than those produced by the baseline model, and mark the number as N . There are totally 28, 266 gold words in the references, and N =18, 391. The proportion is 18, 391/28, 266=65.06%, which could verify the improvements are mainly obtained by addressing the exposure bias problem.  We also evaluate our approach on the WMT'14 benchmarks on the En\u2192De translation task. From the results listed in Table 3, we conclude that the proposed method significantly outperforms the competitive baseline model as well as related approaches. Similar with results on the Zh\u2192En task, both scheduled sampling and MIXER could improve the two baseline systems. Our method im-proves the RNNSearch and Transformer baseline models by +1.59 and +1.31 BLEU points respectively. These results demonstrate that our model works well across different language pairs.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Results on En\u2192De Translation", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "The end-to-end NMT model generates a translation word by word with the ground truth words as context at training time as opposed to the previous words generated by the model as context at inference. To mitigate the discrepancy between training and inference, when predicting one word, we feed as context either the ground truth word or the previous predicted word with a sampling scheme. The predicted words, referred to as oracle words, can be generated with the wordlevel or sentence-level optimization. Compared to word-level oracle, sentence-level oracle can further equip the model with the ability of overcorrection recovery. To make the model fully exposed to the circumstance at reference, we sample the context word with decay from the ground truth words. We verified the effectiveness of our method with two strong baseline models and related works on the real translation tasks, achieved significant improvement on all the datasets. We also conclude that the sentence-level oracle show superiority over the word-level oracle.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We thank the three anonymous reviewers for their valuable suggestions. This work was supported by National Natural Science Foundation of China (NO. 61662077, NO. 61876174) and National Key R&D Program of China (NO. YS2017YFGH001428).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Neural machine translation by jointly learning to align and translate", "journal": "", "year": "2015", "authors": "Dzmitry Bahdanau; Kyunghyun Cho; Yoshua Bengio"}, {"ref_id": "b1", "title": "Scheduled sampling for sequence prediction with recurrent neural networks", "journal": "Curran Associates, Inc", "year": "2015", "authors": "Samy Bengio; Oriol Vinyals; Navdeep Jaitly; Noam Shazeer"}, {"ref_id": "b2", "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Kyunghyun Cho; Bart Van Merrienboer; Caglar Gulcehre; Dzmitry Bahdanau; Fethi Bougares; Holger Schwenk; Yoshua Bengio"}, {"ref_id": "b3", "title": "Clause restructuring for statistical machine translation", "journal": "", "year": "2005", "authors": "Michael Collins; Philipp Koehn; Ivona Kucerova"}, {"ref_id": "b4", "title": "Convolutional sequence to sequence learning", "journal": "", "year": "2017", "authors": "Jonas Gehring; Michael Auli; David Grangier; Denis Yarats; Yann N Dauphin"}, {"ref_id": "b5", "title": "Statistical theory of extreme valuse and some practical applications", "journal": "Nat. Bur. Standards Appl. Math. Ser", "year": "1954", "authors": "Emil Julius Gumbel"}, {"ref_id": "b6", "title": "Rouge: A package for automatic evaluation of summaries", "journal": "Association for Computational Linguistics", "year": "2004", "authors": "Chin-Yew Lin"}, {"ref_id": "b7", "title": "A* sampling", "journal": "Curran Associates, Inc", "year": "2014", "authors": "J Chris; Daniel Maddison; Tom Tarlow;  Minka"}, {"ref_id": "b8", "title": "Dtmt: A novel deep transition architecture for neural machine translation", "journal": "", "year": "2019", "authors": "Fandong Meng; Jinchao Zhang"}, {"ref_id": "b9", "title": "Bleu: a method for automatic evaluation of machine translation", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"ref_id": "b10", "title": "Sequence level training with recurrent neural networks", "journal": "", "year": "2015", "authors": "Aurelio Marc; Sumit Ranzato; Michael Chopra; Wojciech Auli;  Zaremba"}, {"ref_id": "b11", "title": "Neural machine translation of rare words with subword units", "journal": "Long Papers", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"ref_id": "b12", "title": "Greedy search with probabilistic n-gram matching for neural machine translation", "journal": "", "year": "2018", "authors": "Chenze Shao; Xilin Chen; Yang Feng"}, {"ref_id": "b13", "title": "Minimum risk training for neural machine translation", "journal": "", "year": "2016", "authors": "Shiqi Shen; Yong Cheng; Zhongjun He; Wei He; Hua Wu; Maosong Sun; Yang Liu"}, {"ref_id": "b14", "title": "Sequence to sequence learning with neural networks", "journal": "Curran Associates, Inc", "year": "2014", "authors": "Ilya Sutskever; Oriol Vinyals; Quoc V Le"}, {"ref_id": "b15", "title": "Modeling coverage for neural machine translation", "journal": "", "year": "2016", "authors": "Zhaopeng Tu; Zhengdong Lu; Yang Liu; Xiaohua Liu; Hang Li"}, {"ref_id": "b16", "title": "Attention is all you need", "journal": "Curran Associates, Inc", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Illia Kaiser;  Polosukhin"}, {"ref_id": "b17", "title": "Improving multi-step prediction of learned time series models", "journal": "AAAI Press", "year": "2015", "authors": "Arun Venkatraman; J Andrew Hebert;  Bagnell"}, {"ref_id": "b18", "title": "Google's neural machine translation system: Bridging the gap between human and machine translation", "journal": "", "year": "2016", "authors": "Yonghui Wu; Mike Schuster; Zhifeng Chen; V Quoc; Mohammad Le; Wolfgang Norouzi; Maxim Macherey; Yuan Krikun; Qin Cao; Klaus Gao;  Macherey"}, {"ref_id": "b19", "title": "Adadelta: an adaptive learning rate method", "journal": "", "year": "2012", "authors": "D Matthew;  Zeiler"}, {"ref_id": "b20", "title": "Incorporating word reordering knowledge into attention-based neural machine translation", "journal": "", "year": "2017", "authors": "Jinchao Zhang; Mingxuan Wang; Qun Liu; Jie Zhou"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: The architecture of our method.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Word-level oracle without noise.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Word-level oracle with Gumbel noise.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Some other researchers have noticed the problem of exposure bias in NMT and tried to solve it.Venkatraman et al. (2015) proposed DATA AS DEMONSTRATOR (DAD) which initialized the training examples as the paired two adjacent ground truth words and at each step added the predicted word paired with the next ground truth word as a new training example. ", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 4 :4Figure 4: Training loss curves on Zh\u2192En translation with different factors. The black, blue and red colors represent the RNNsearch, RNNsearch with word-level oracle and RNNsearch with sentence-level oracle systems respectively.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure 5: Trends of BLEU scores on the validation set with different factors on the Zh\u2192En translation task.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 :6Figure 6: Trends of BLEU scores on the MT03 test set with different factors on the Zh\u2192En translation task.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 7 :7Figure 7: Performance comparison on the MT03 test set with respect to the different lengths of source sentences on the Zh\u2192En translation task.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ": Case-insensitive BLEU scores (%) on Zh\u2192En translation task. \" \u2021\", \" \u2020\", \" \" and \" * \" indicate statisticallysignificant difference (p<0.01) from RNNsearch, SS-NMT, MIXER and Transformer, respectively."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Factor analysis on Zh\u2192En translation, the results are average BLEU scores on MT03\u223c06 datasets.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Case-sensitive BLEU scores (%) on En\u2192De task. The \" \u2021\" indicates the results are significantly better (p<0.01) than RNNsearch and Transformer.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "x = {x 1 , \u2022 \u2022 \u2022 , x |x| } and y * = {y * 1 , \u2022 \u2022 \u2022 , y * |y * | }. Encoder. A bidirectional Gated Recurrent Unit (GRU)", "formula_coordinates": [2.0, 307.28, 105.01, 218.27, 38.86]}, {"formula_id": "formula_1", "formula_text": "x i is h i = [ \u2212 \u2192 h i ; \u2190 \u2212 h i ].", "formula_coordinates": [2.0, 307.28, 146.88, 217.77, 26.3]}, {"formula_id": "formula_2", "formula_text": "x i . \u2212 \u2192 h i = GRU(e x i , \u2212 \u2192 h i\u22121 ) (1) \u2190 \u2212 h i = GRU(e x i , \u2190 \u2212 h i+1 ) (2)", "formula_coordinates": [2.0, 361.73, 175.37, 163.82, 56.74]}, {"formula_id": "formula_3", "formula_text": "r ij = v T a tanh (W a s j\u22121 + U a h i )(3)", "formula_coordinates": [2.0, 341.32, 319.41, 184.22, 14.19]}, {"formula_id": "formula_4", "formula_text": "\u03b1 ij = exp (r ij ) |x| i =1 exp r i j (4)", "formula_coordinates": [2.0, 364.03, 337.41, 161.51, 31.27]}, {"formula_id": "formula_5", "formula_text": "c j = |x| i=1 \u03b1 ij h i (5)", "formula_coordinates": [2.0, 376.74, 413.22, 148.8, 21.79]}, {"formula_id": "formula_6", "formula_text": "s j = GRU(e y * j\u22121 , s j\u22121 , c j )(6)", "formula_coordinates": [2.0, 354.76, 494.33, 170.78, 13.76]}, {"formula_id": "formula_7", "formula_text": "t j = g e y * j\u22121 , c j , s j (7) o j = W o t j (8) P j = softmax (o j )(9)", "formula_coordinates": [2.0, 369.07, 595.6, 156.47, 48.46]}, {"formula_id": "formula_8", "formula_text": "s j\u22121 \u2022 \u2022 \u2022 y * j\u22121 Logistic regression classifier \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 Decoder y oracle j\u22121 1 \u2212 p 1 \u2212 p p p", "formula_coordinates": [3.0, 78.59, 64.26, 206.62, 81.12]}, {"formula_id": "formula_9", "formula_text": "\u03b7 = \u2212 log (\u2212 log u) (10) o j\u22121 = (o j\u22121 + \u03b7) /\u03c4 (11", "formula_coordinates": [3.0, 368.2, 468.72, 272.36, 28.17]}, {"formula_id": "formula_10", "formula_text": ")", "formula_coordinates": [3.0, 609.11, 486.61, 33.91, 9.46]}, {"formula_id": "formula_11", "formula_text": "P j\u22121 = softmax (\u00f5 j\u22121 ) (12", "formula_coordinates": [3.0, 363.82, 503.63, 157.18, 11.36]}, {"formula_id": "formula_12", "formula_text": ")", "formula_coordinates": [3.0, 521.0, 504.71, 4.54, 9.46]}, {"formula_id": "formula_13", "formula_text": "y oracle j\u22121 = y WO j\u22121 = argmax P j\u22121 (13)", "formula_coordinates": [3.0, 331.92, 632.53, 193.62, 15.18]}, {"formula_id": "formula_14", "formula_text": "+ = Noise \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 + \u2022 \u2022 \u2022 Predicted Score y oracle j\u22121 1-best", "formula_coordinates": [4.0, 140.12, 67.17, 144.63, 51.18]}, {"formula_id": "formula_15", "formula_text": "y oracle j\u22121 = y SO j\u22121 = y S j\u22121 (14)", "formula_coordinates": [4.0, 132.09, 345.89, 158.17, 14.19]}, {"formula_id": "formula_16", "formula_text": "p = \u00b5 \u00b5 + exp (e/\u00b5)(15)", "formula_coordinates": [4.0, 373.21, 486.73, 152.34, 25.77]}, {"formula_id": "formula_17", "formula_text": "L (\u03b8) = \u2212 N n=1 |y n | j=1 log P n j y n j (16", "formula_coordinates": [4.0, 323.5, 711.15, 197.5, 25.67]}, {"formula_id": "formula_18", "formula_text": ")", "formula_coordinates": [4.0, 521.0, 719.25, 4.54, 9.46]}, {"formula_id": "formula_19", "formula_text": "s j = GRU 1 (e y * j\u22121 , s j\u22121 ) (17", "formula_coordinates": [6.0, 124.1, 420.12, 161.63, 13.76]}, {"formula_id": "formula_20", "formula_text": ")", "formula_coordinates": [6.0, 285.72, 421.2, 4.54, 9.46]}, {"formula_id": "formula_21", "formula_text": "s j = GRU 2 (c j ,s j )(18)", "formula_coordinates": [6.0, 136.49, 439.17, 153.77, 11.36]}], "doi": "10.3115/1219840.1219906"}