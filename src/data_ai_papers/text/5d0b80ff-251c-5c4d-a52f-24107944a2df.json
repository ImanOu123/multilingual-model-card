{"title": "Assessing Data Mining Results via Swap Randomization", "authors": "Aristides Gionis; Heikki Mannila; Taneli Mielik\u00e4inen; Panayiotis Tsaparas", "pub_date": "", "abstract": "The problem of assessing the significance of data mining results on high-dimensional 0-1 data sets has been studied extensively in the literature. For problems such as mining frequent sets and finding correlations, significance testing can be done by, e.g., chi-square tests, or many other methods. However, the results of such tests depend only on the specific attributes and not on the dataset as a whole. Moreover, the tests are more difficult to apply to sets of patterns or other complex results of data mining. In this paper, we consider a simple randomization technique that deals with this shortcoming. The approach consists of producing random datasets that have the same row and column margins with the given dataset, computing the results of interest on the randomized instances, and comparing them against the results on the actual data. This randomization technique can be used to assess the results of many different types of data mining algorithms, such as frequent sets, clustering, and rankings. To generate random datasets with given margins, we use variations of a Markov chain approach, which is based on a simple swap operation. We give theoretical results on the efficiency of different randomization methods, and apply the swap randomization method to several wellknown datasets. Our results indicate that for some datasets the structure discovered by the data mining algorithms is a random artifact, while for other datasets the discovered structure conveys meaningful information.", "sections": [{"heading": "INTRODUCTION", "text": "One of the most important considerations in data mining is deciding whether the discovered patterns or models are significant. While traditional statistics has long been considering the issue of significance testing, in data mining people have been less interested in the theme.\nThe framework of hypothesis testing in statistical data analysis is very well developed for assessing the significance of individual patterns or models. The methods are typically based either on analytical expressions or randomization tests. However, often they are not well-suited for assessing complex results of data mining, such as clusterings or pattern sets.\nIn this paper we consider the use of swap randomization [5] for assessing data mining results on 0-1 datasets. The basic idea of swap randomization is as follows. Given the dataset D, create random datasets with the same row and column margins D, run the data mining algorithm on those, and see if the results are significantly different on the real data than on the randomized datasets. If not, then we presume that the results are really due to the row and column margins, and not due to interesting relations in the data. The datasets with the same margins as the original one are generated by swaps, as shown in Figure 1: take two rows u and v and two columns A and B of the data table with u(A) = v(B) = 1 and u(B) = v(A) = 0, and change the rows so that u(B) = v(A) = 1 and u(A) = v(B) = 0. This operation maintains the row and column sums of the dataset, and all datasets with the same row and column sums can be reached through a series of swaps [5].\nThus swap randomization is an extension of traditional randomization methods. For instance, a chi-square test for assessing the significance of frequent itemsets is an analytical technique based on studying the distribution of datasets with given column margins, but the row margins are allowed to vary. Similarly, methods that randomize the target value in prediction tasks keep the column margins fixed, but they do not impose any constraint on the row margins. A moti-vating example of why it is important to maintain also the row margins is given in the next section.\nSwap randomization has been considered in various applications; see, e.g., the survey paper by Cobb and Chen [5]. The problem of creating 0-1 datasets with given row and column margins has theoretical interest of its own; see, e.g., [1,7]. Generating contingency tables with fixed margins is a problem that has been studied in statistics (see, e.g., [4]). Randomization methods in general form a large research area [8].\nOur contributions in this paper are twofold: (i) we describe the algorithmic aspects of swap randomization when applied to large data sets, and (ii) we show how this method can be applied in the data mining setting. In more detail, we give a description of several different ways of generating random matrices with given margins and discuss their performance. Swap randomizations can be performed efficiently and can be applied to reasonably large datasets, as our experiments show. We give extensive empirical results showing that some well-known datasets appear to have very little interesting patterns or cluster centers, while other datasets have lots of structure.\nThe rest of this paper is organized as follows. In Section 2 we present an overview of the swap randomization method, and in Section 3 we discuss the applications of the approach to specific data mining tasks. Section 4 describes how the random matrices with given margins are generated and gives results on the performance of the algorithms. In Section 5 we describe the experimental results. Section 6 discusses related work, and Section 7 gives some concluding remarks.", "publication_ref": ["b4", "b4", "b4", "b0", "b6", "b3", "b7"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "OVERVIEW OF THE APPROACH", "text": "In this section we give an overview of the method, explain the intuition behind it, describe the algorithmic challenges it poses, and show how it can be applied to testing significance of results obtained by different kinds of data mining algorithms.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The randomization approach", "text": "Let D denote a 0-1 matrix with m rows and n columns that represents our dataset. Assume that we are interested in assessing the result obtained by a particular data mining algorithm A on input D. Let A(D) denote the result of the algorithm. For simplicity, assume that it can be described by a single number. For instance, for frequent set mining algorithms, it can be the number of sets whose frequency exceeds a certain support threshold. Similarly, for a clustering algorithm, it can be the error of the clustering solution.\nIn our randomization approach we generate k datasets D1, . . . , D k , such that each Dt, t = 1, . . . , k, is an m \u00d7 n 0-1 matrix that has the same row and column sums as the original matrix D; each dataset Dt is assumed to be a uniform and independent sample from the space of all m \u00d7 n 0-1 matrices with the given margins. Then the algorithm A is executed on each sampled dataset Dt, yielding results Xt = A(Dt) for t = 1, . . . , k. Now, the significance of the result A(D) of the algorithm A on the data D is tested by comparing it against the set X = {X1, . . . , X k } of the results of A on the sampled datasets. If the result of the algorithm on the original data does not deviate significantly from the values in X, then the result A(D) is not surprising and its significance is small.\nAssuming that the sampled datasets are independent and that k is large enough so that X gives an approximation of the real distribution, then the empirical p-value of X0 = A(D) is\n1 k + 1 (min{|{t | Xt < X0}|, |{t | Xt > X0}|} + 1) ,\ni.e., the fraction of the random datasets in which we see a value more extreme than the value in the real data. Another measure for quantifying the significance of the value X0 is captured by the Z score\nZ = |X0 \u2212 b X| b \u03c3 , where b X = E[X1, . . . , X k ]\nis the empirical mean of the set X and b \u03c3 2 = Var[X1, . . . , X k ] is the empirical variance. Large values of Z indicate that X0 deviates a lot from the mean of the results obtained on the random datasets.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Why maintain row and column margins?", "text": "As mentioned in the introduction, randomization is widely used as a significance testing method. For example, in control studies in medical genetics it is customary to estimate the interestingness of discovered patterns by a permutation test. In such a test the variable describing whether a patient belongs to the case or the control group is permuted randomly, and the original data analysis is repeated. The findings on the real data are accepted only if they are stronger than on, say, 99% of the randomized datasets.\nHowever, in many data mining tasks the goal is not to predict a single variable. For example, pattern discovery and clustering look at the structure of the whole data set. One could of course think of randomizing each column of the dataset independently, but this method ignores some of the structure of the dataset.\nAs an example, consider the datasets D1 and D2 in Figure 2. In both datasets variables X and Y are positively correlated, and the itemset {X, Y } occurs more often than the independence assumption would imply. As the columns of X and Y are the same for both datasets, any measure of the importance of the association between X and Y that takes only the columns of X and Y into account will give the same results for D1 and D2. However, in dataset D1 X and Y co-occur in all types of rows, whereas in dataset D2 the co-occurrence of X and Y happens exclusively in very dense rows. Thus, in D2 the high frequency of the pair {X, Y } is not due to some specific property that binds X with Y , but rather to the fact that X and Y tend to occur on rows that have lots of 1's.\nIndeed, consider the dataset E1 containing 10 copies of D1, and E2 containing 10 copies of D2. The columns for X and Y are still same in both datasets, and in both cases the frequency of the pair is 60. When we generate 1000 random datasets with the margins of E1 the maximum and average frequencies of {X, Y } were 59 and 52.4, and the standard deviation 2.5; thus, all values were smaller than 60, yielding an empirical p-value of 0.001. For E2 the corresponding numbers are 69, 63.2, and 2.0; and in only 70 cases the frequency was 60 or less, giving an empirical p-value of 0.07. Thus, we can conclude that in E1 the pair {X, Y } is strongly overrepresented, while in E2 it occurs slightly less often than one would expect. This indicates that the context of the pair of variables can have a strong effect on the significance of the frequency of a pair. In both cases we are interested in the correlation between columns (attributes) X and Y . The significance of the correlation result might depend on the overall context of the dataset\nX Y 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 Dataset D1 X Y 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 Dataset D2\nThe above example demonstrates the basic concept underlying swap randomization: it takes the bias of row and column counts into account by randomizing over datasets with the same row and column margins as the original dataset. As a result the notion of interestingness we consider is conditional to the knowledge of the marginal sums. We are interested in assessing information in the dataset that is not conveyed by the marginal sums of the data table.\nAs an example, consider a dataset whose row sums satisfy a power law. This finding can be interesting, but it does not offer any additional information about the structure of the dataset. Using swap randomization one can assess whether a quantity of interest is not immediately implied by the powerlaw marginals, and thus common to all datasets with the same margin distributions.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Generating matrices with given margins", "text": "The technical challenge in the above approach is to generate random 0-1 datasets with given row and column sums. This problem has been studied extensively in statistics [4,5], theoretical computer science [1,7] and in application areas [11,14].\nIn this paper we use a Markov chain approach to the problem of sampling. Starting from the original dataset, we make a small random local move, which interchanges a pair of 1's with a pair of 0's and does not change the row and column sums. Such a local move is called a swap, and a sequence of swaps is performed until the data mixes sufficiently enough and a random sample is obtained.\nThis Markov chain thus consists of datasets with the given margins; two datasets are adjacent, if there is a swap that changes one to the other. The Markov chain is reversible, i.e., a swap can be undone by a single (reverse) swap. However, the chain is not regular, i.e., some datasets (states) have more neighbors than others. This implies that the stationary distribution of the chain is not the uniform distribution. Therefore, a straightforward application of swapping does not guarantee uniform sampling.\nThe problem of non-uniformity can be fixed in at least two ways: (i) by using the Metropolis-Hastings algorithm [9,13], which is a well-studied method for converting a Markov chain with stationary distribution \u03c0 to another one with stationary distribution \u03c0 , and (ii) by adding self-loops in order to guarantee that all states have the same degree.\nFor applying the Metropolis-Hastings algorithm, one needs to compute the degree of any given state of the chain, that is, the number of all valid swaps for a given 0-1 matrix. We give a simple formula for computing the degree at each state, and we show how to maintain this quantity incrementally. The complexity of incremental maintenance of the state degree is O(min{m, n}) for an m \u00d7 n matrix, making the algorithm somehow inefficient. On the other hand, adding self-loops does not require computing any additional expensive information; so while more steps are needed for convergence, the time complexity of each step is, in expectation, constant, making it a very efficient algorithm in practice.", "publication_ref": ["b3", "b4", "b0", "b6", "b10", "b13", "b8", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "USING THE FRAMEWORK", "text": "In this section we describe how the swap randomization framework can be applied to different data mining tasks, such as finding frequent sets and correlations, clustering, and spectral analysis of datasets. Our methodology allows us to investigate the significance of the patterns that exist in a given dataset, at different levels of granularity.\nFirst, we are able to characterize the significance of global aspects of the dataset. If the number of frequent sets, or the number of highly correlated pairs contained in the dataset is not significant with respect to that found in a randomly rearranged dataset, then we can conclude that the dataset does not contain any interesting global structure of frequent sets, or of highly significant correlations.\nAdditionally, we can also look at individual itemsets. In this case we are interested in identifying itemsets whose frequency is smaller or larger in the sampled datasets when compared with the original dataset. If the frequency of an itemset drops in the sampled dataset, it is implied that the frequency can not be explained by the margins of the dataset. If the frequency increases, a possible explanation is that the items in the itemset are anti-correlated in the original dataset.\nThe above observations apply also when mining simple association rules. Recall that the accuracy (confidence) of a rule (X \u21d2 B) is defined to be f (XB) f (X) , where f (XB) and f (X) are the frequencies of X \u222a {B} and {X}, respectively. Assume now that X is a singleton set. Since f (X) remains fixed, the confidence of the rule is proportional to the frequency f (XB). Therefore, the significance of the rule (X \u21d2 B) is determined by the significance of the pair {X, B}. Due to this observation and space constraints we omit further discussion on association rules.\nSwap randomization can be applied to testing the significance of clustering results. Given a clustering algorithm like k-means, and a target number of clusters k, simply compare the clustering error in the original dataset with the clustering error in the sampled datasets. If the difference is large, then one can deduce that the dataset has meaningful cluster structure. This simple approach turns out to yield very clear results on synthetic datasets with known cluster structure.\nA different notion of global structure is captured in the singular values and vectors of the data matrix. The singular vectors capture the linear trends in the dataset. The corresponding singular values capture the strength of the linear\nl j i k j l i k Figure 3: A swap in the graph representation GD.\ntrend, that is, the tendency of the rows or columns to align with the corresponding singular vectors.\nIn randomly generated data, the strongest linear trends should be determined by the degree structure of the dataset. This is usually the first singular value. The remaining dataset has no structure; thus we expect the remaining singular values to be small. If the original data contains some linear structure, then the top singular values (especially the nonprincipal ones) should be higher than those of random sets with the same margins.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SAMPLING DATASETS WITH GIVEN ROW AND COLUMN MARGINS", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Basics", "text": "We now describe the process of sampling a matrix from the space of all m \u00d7 n 0-1 matrices with given margins.\nLet D be a 0-1 dataset with m rows and n columns. We denote by ri the sum of the i-th row of D, i = 1, . . . , m, and by cj the sum of the j-th column, j = 1, . . . , n. An equivalent way to represent the input matrix D is as a bipartite graph GD = (R, C, E) with |R| = m and |C| = n. Vertex i \u2208 R corresponds to the i-th row of D, vertex j \u2208 C corresponds to the j-th column of D, and (i, j) \u2208 E if and only if D(i, j) = 1 for all i and j. The degrees of the vertices of the graph are ri for i \u2208 R, and cj for j \u2208 C.\nThe main idea is to start from the graph GD corresponding to the original data set and perform a local swap that leaves the margins unchanged. When many such swaps have been performed the resulting graph can be considered as a random dataset drawn randomly from the stationary distribution.\nIn more detail, a local swap can be defined by four vertices, i, j, k, and l of GD, such that i, k \u2208 R and j, l \u2208 C, and (i, j) \u2208 E, (k, l) \u2208 E, (i, l) \u2208 E, (k, j) \u2208 E. A new dataset is then formed by updating the edges as follows.\nE \u2190 E \\ {(i, j), (k, l)} \u222a {(i, l), (k, j)},\nthat is, we remove the current edges {(i, j), (k, l)} and we add new edges {(i, l), (k, j)}. Visually, a local swap is depicted in Figure 3 for the graph representation and in Figure 1 for the matrix representation.\nFormally, a local swap is a step on the a Markov chain M = {S, T }, where the state space S is the set of all graphs with the given degree sequences, and T is the set of transitions defined by swaps. In other words, the set T contains all pairs of graphs (G, G ) such that it is possible to obtain G from G (or vice versa) by performing a local swap.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Na\u00efve nonuniform approach", "text": "Algorithm 1 shows a straightforward implementation of this Markov approach.\nFinding the next transition (G, G ) \u2208 T from graph G, that is, performing line 3 of Algorithm Na\u00efve, is not a completely straightforward task. The simplest way is to pick a pair of edges in G, reject if the edges are not swappable, Algorithm 1 Na\u00efve Input: Graph GD, number of random walk steps kn Ouput: Graph G with the same degree sequences as GD 1: G \u2190 GD 2: while kn > 0 do 3:\nG \u2190 Find adjacent(G) 4: G \u2190 G 5:\nkn \u2190 kn \u2212 1 6: end while 7: Return G and repeat until finding a pair of swappable edges. This is shown in Algorithm 2. Alternatively, one could store all swappable pairs in a structure, and select one uniformly at random. The selection process becomes faster, but there is additional cost of updating the data structure at each step.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 2 Find adjacent", "text": "Input: Graph G Ouput: Graph G that differs from G in exactly one swap (i.e., (G, G ) \u2208 T ) repeat Select edges (i, j), (k, l) \u2208 E(G) uniformly at random until (i, l) \u2208 E(G) and (k, j) \u2208 E(G) E(G ) \u2190 E(G) \\ {(i, j), (k, l)} \u222a {(i, l), (k, j)}\nGiven graph G, the Algorithm Find adjacent generates a graph G uniformly at random among all graphs G such that (G, G ) \u2208 T . The reason is that each such graph G can be set at an one-to-one correspondence with a pair of swappable edges-the non-swappable edges can be ignored. Algorithm Find adjacent clearly samples uniformly at random from the set of swappable pairs: each swappable pair is sampled with probability proportional to 2/|E| 2 . Now, in order for the Markov chain to sample graphs uniformly at random from the set S, the following conditions have to hold:\n1. The state space S is connected under the transitions of M.\n2. M has uniform stationary distribution.\n3. Starting from GD a sufficiently large number of local swaps have to be performed until the chain mixes. We would like to know how many such swaps should be performed, i.e., the mixing time of the chain.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Connectedness:", "text": "The Markov chain is connected. One can go by any state of the chain to any other state via local swaps [5].\nUniformity: First notice that the Markov chain M is reversible. Now, for each graph (state) G \u2208 S we define d(G), the degree of the Markov chain M at G, to be the number of different graphs (states) G such that (G, G ) \u2208 T . From the theory of the Markov chains, it is well known that the stationary distribution of a reversible chain is proportional to the degree at each state in the underlying transition graph. Therefore, in order to obtain a uniform distribution, all states of the Markov chain must have the same degree. A simple construction shows that this is not true in general for the Markov chain M. Therefore, the Na\u00efve Algorithm 1 does not converge to the uniform distribution.", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "Mixing time:", "text": "The mixing time of the Markov chain we defined above has been the object of theoretical study [5], but without any conclusive results. It is estimated, that running the chain for a number of steps in the order of the number of 1's in the matrix is sufficient for convergence. We do not deal with the theoretical aspects of converge, but we study it empirically in the experimental section.", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "The self-loop method", "text": "The straightforward application of the Markov Chain approach does not produce uniform sampling. There are two ways to fix this bias and obtain uniform distribution. The first way is by adding self-loops, as it is shown in Algorithm 3. Algorithm Self loop works as Na\u00efve does. It samples pairs of edges until it finds a swappable pair. The difference with Na\u00efve, however, is that in Self loop all steps are counted and they decrease the counter, thus, non-swappable pairs of edges are counted as self-loops. The reason that Self loop leads to uniform distribution is that when selfloops are counted the degree of each G \u2208 S becomes fixed and equal to |E| 2 . Each pair of edges, swappable or nonswappable, contributes one to the degree of all states. Algorithm 3 Self loop Input: Graph GD, number of random walk steps ks Ouput: Graph G with the same degree sequences as GD 1: G \u2190 GD 2: while ks > 0 do 3:\nSelect edges (i, j), (k, l) \u2208 E(G) 4: if ((i, l) \u2208 E(G) and (k, j) \u2208 E(G)) then 5: E(G ) \u2190 E(G) \\ {(i, j), (k, l)} \u222a {(i, l), (k, j)} 6:\nend if 7:\nks \u2190 ks \u2212 1 8: end while 9: Return G", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Metropolis-Hastings approach", "text": "The second way of sampling from the uniform distribution is by using the Metropolis-Hastings algorithm [9,13], which is a standard method of converting a Markov chain with stationary distribution \u03c0 to another Markov chain with stationary distribution \u03c0 . In our case \u03c0(G) \u223c d(G) and we want \u03c0 (G) \u223c 1, so the Metropolis algorithm becomes as shown in Algorithm 4. For some swap that takes the algorithm from state (graph) G to state G if the state G has higher degree then the algorithm performs the swap with probability d (G) d(G ) . The algorithm assumes knowledge of the degree d(G) for each graph G \u2208 S. We will discuss soon how d(G) can be computed.", "publication_ref": ["b8", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 4 Metropolis-Hastings", "text": "Input: Graph GD, number of random walk steps km Ouput: Graph G with the same degree sequences as GD\n1: G \u2190 GD 2: while km > 0 do 3: G \u2190 Find adjacent(G) 4: G \u2190 G , with probability min{1, d(G) d(G ) } 5:\nkm \u2190 km \u2212 1 6: end while 7: Return G", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Running time", "text": "We now analyze the running time of the algorithms. We will prove some results on the complexity of the approaches, including a result characterizing the degree of a state in the Markov chain of the datasets. The conclusion in this section is that the Self loop algorithm is always more efficient than the Metropolis-Hastings algorithm.\nFirst, we assume that we can sample edges in constant time and we can test if a pair of edges is swappable or not in constant time. The former task can be performed by keeping all edges in an array, while the latter task can be performed by keeping in memory the data D in the matrix form, or by storing all edges in a hash table.\nThe running time of Find adjacent is a random variable and it depends on the number of swappable edges for each graph (state) G. Recall that the number of swappable pairs of graph G is d(G). Therefore, the probability of finding a swappable pair of edges is precisely\nd(G) |E| 2 , thus the expected time staying in G is |E| 2 d(G) . Without counting the self loops, the probability of visiting graph G is d(G)\n2|T | , which is precisely the stationary distribution of Algorithm Na\u00efve at G. Thus, the expected running time of Algorithm Find adjacent is\nTF = X G\u2208S |E| 2 d(G) \u2022 d(G) 2|T | = |E| 2 2 \u2022 |S| |T | . (1\n)\nNotice that |T |/|S| = O(|E| 2 ), since the degree of each graph G in S is at most |E| 2 . One the other hand, the following Lemma is immediate.\nLemma 1. For bipartite graphs G = (U, V, E) in which the maximum degree is o(|E|), we have |T |/|S| = \u2126(|E| 2 ).\nProof. Notice that the random walk leaves the degrees at each vertex unaffected in all states. Given any state (graph G) in S, consider an edge (i, j) \u2208 E(G). Any other edge (k, l) \u2208 E(G) can be swapped with (i, j) unless either l \u2208 \u0393(i) or k \u2208 \u0393(j) (or both), where \u0393(i) are the neighbors of i in the bipartite graph. Thus, the number of edges that should be excluded from swapping with (i, j) is o(|E|), yielding a total number of at least (|E| 2 \u2212 o(|E|) \u2022 |E| swappable pairs. Since each state in S has degree \u2126(|E| 2 ), the lemma follows.\n2\nCorollary 1. For bipartite graphs G = (U, V, E) whose degree distribution follows power law with \u03b1 > 2 we have\n|T |/|S| = \u2126(|E| 2 ).\nProof. For simplicity assume that |U | = |V | = n. For power laws with exponent \u03b1 > 2 we have |E| = O(n) in expectation and the maximum degree is n 1 \u03b1\u22121 = o(n) (e.g., see [15]). Thus, the conditions of Lemma 1 are satisfied. 2\nThe above results imply that for some important classes of datasets -such as graphs with bounded degrees or degrees that follow a power law distribution -the expected time TF of the Find adjacent algorithm is constant. Thus, for those classes of data, the running time of Algorithm Na\u00efve is TN = TF \u2022 kn = O(kn). Similarly, for the Self loop algorithm the overall running time is TS = O(ks). Furthermore, the expected time spent in each state for performing self-loops (before moving out to a new state) is constant.\nWe now turn to the running time of Metropolis-Hastings. This running time can be written as TM = T 0 D + km(TF + TD), where TF is the running time of Find adjacent, TD is the time need to compute d(G ) given that d(G) is already computed, and T 0 D is the time needed to compute d(G) for the first time. Next we explain how to compute d(G) and how to update the computation for d(G ). The time needed for the update is linear time with respect to min{m, n}. Theorem 1. Let G = (U, V, E) be a bipartite graph represented as a binary matrix D with m = |U | rows and n = |V | columns. Let ri be the \"left\" degree of node i \u2208 U , cj be the \"right\" degree of node j \u2208 V , and define M = DD T . Then, the number of graphs G that are yielded with one local swap from G is equal to\nd(G) = J(G) \u2212 Z(G) + 2K22(G),(2)\nwhere\nJ(G) = 1 2 |E|(|E| + 1) \u2212 X i\u2208U r 2 i \u2212 X j\u2208V c 2 j ! is the number of disjoint pairs of edges, Z(G) = X (i,j)\u2208E (ri \u2212 1)(cj \u2212 1)\nis the number of \"Z\" structures\n{(a, b), (c, d), (c, b) \u2208 E, with a, b, c, d all distinct}, and K22(G) = X i,k\u2208U i =k M (i, k) 2 ! = 1 2 X i,k\u2208U i =k M (i, k) 2 \u2212 M (i, k)\nis the number of K2,2 cliques of G.\nProof. Each disjoint pair of edges is swappable unless it is part of a \"Z\" structure. In each K2,2 there are 2 disjoint pairs of edges and 4 \"Z\"s, but there are no swappable pairs, so we should add 2 to bring the count to 0. 2 We note that the Metropolis-Hastings still needs to run the Find adjacent algorithm for finding a candidate swap pair. Although the way that the algorithm moves between states is different (neighboring states are not chosen uniformly at random), and thus it may guarantee faster convergence, we believe that this probably does not offset the additional cost incurred by the computation of the degrees, and thus we prefer to experiment with the Self loop algorithm. We note that it may be possible to maintain the number of swappable pairs in linear time, thus eliminating  ", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "EMPIRICAL RESULTS", "text": "We perform experiments with many of the well known datasets used in the data mining community. A description of the datasets we are using is as follows: Abstracts contains document-word information on a collection of project abstracts submitted for funding by NSF. Abstracts is a pruned version of Abstracts, where we keep only words of medium frequency (only words with frequency counts between 200 and 8854 are kept). Courses is a student-course dataset of courses completed by the Computer Science students of the University of Helsinki. Retail is a marketbasket dataset collected in a Belgian supermarket [2]. Kosarak is a click-stream dataset from a Hungarian news website. Finally, Paleo, the smallest dataset, contains information of species fossils found in specific paleontological sites in Europe. Exact information of the datasets, including the sizes and the density of 1's are shown in Table 1.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Convergence and performance", "text": "We have tested extensively the convergence properties of the swap-based Markov chain for various datasets. Designing diagnostics for the convergence of a Markov chain is far from easy and it is an open research question, so our tests can provide only evidence that the chain has mixed and by no means do they constitute a proof.\nAn example among the many tests we have performed is showed in Figure 4. For each of our datasets, we measure the number of frequent itemsets for a given threshold. The y-axis in Figure 4 shows the number of frequent itemsets in the sampled datasets, divided by the number of frequent itemsets in the original dataset. The x-axis shows the number of steps in the Markov chain scaled by the number of 1's in the corresponding dataset, i.e., position x = i shows a sample after iL steps, where L is the number of 1's in the corresponding dataset. We see that in almost all cases the chain mixes quite rapidly: already after L steps (4L in the case of Kosarak) the number of frequent sets has stabilized.\nSimilar kind of convergence evidence was obtained for all our measures: frequencies of specific itemsets, number of correlations above a certain threshold, clustering errors, etc. In all of our experiments shown below we have run the chain with very large steps in order to ensure convergence.\nAdditionally, the swaps can be performed quite efficiently. Table 2 shows the running time for the different datasets, using a modest Perl implementation on a 3GHz Pentium machine with 2GB of memory. The reported times is for  obtaining one sample after performing 5L swaps. In most cases much smaller steps can be used.", "publication_ref": [], "figure_ref": ["fig_3", "fig_3"], "table_ref": ["tab_2"]}, {"heading": "Frequent itemsets", "text": "In this section we describe the results of frequent itemset mining using swap randomization. Table 3 shows the number of frequent sets for the datasets described in Section 5. We compute the collections of frequent sets in the original data, in random data under swap randomization and in random data under independent permutations of columns (i.e., only column margins are maintained). The collections are denoted by F, Fs and Fp, respectively. The minimum support thresholds were chosen so that the number of frequent sets is not exceedingly large. Frequent items, i.e., frequent sets of size one, are omitted from the table since they do not change by swapping or permuting the columns. In the case of swapped and permuted datasets, we show the mean values (and standard deviations) of 500 randomized versions of the datasets.\nTable 3 clearly demonstrates the differences between the randomization methods. All datasets seem to contain many interesting frequent sets when compared to the frequent sets in the corresponding datasets with permuted columns. The sizes of the frequent set collections were always considerably smaller in the permuted data than in the original data. On the other hand, different datasets show very different behavior with respect to swapping. In Abstracts and Retail the number of frequent sets remain about the same under swap randomization, whereas in Abstracts , Courses,  and Paleo the numbers decrease significantly. Finally, there is a considerable increase in the number of frequent sets in Kosarak.\nInterpreting the results, we can conclude that the structure captured by frequent itemsets in Abstracts and Retail can be attributed mainly to the row and column margins, and thus it is preserved in random datasets where the margins are preserved. On the other hand, in the datasets Abstracts , Courses and Paleo the structure captured by frequent sets is more interesting, since it disappears under swap randomization.\nThe increase in the number of frequent sets in the case of Kosarak implies that many sets of items are anti-correlated with each other. A possible explanation for this phenomenon lies in the structure and origin of the data. The row and column margins are highly skewed in Kosarak. The anticorrelations can be interpreted by the nature of the dataset.  news portal: the link structure of the websites can cause negative correlations between groups of pages.\nAlthough the number of frequent itemsets is indicative of the structure that is contained in the data, it is not informative with respect to what are the actual itemsets contained in the collections, and how the collections relate to each other. It may well be the case that collections have about the same size yet they are completely disjoint.\nWe now describe how the collections change under swap randomization. Table 4 shows the average fraction of itemsets that are preserved or disappear, compared to the original collection. For the datasets Abstracts, Kosarak, Retail where the size of the collection remains relatively stable (or in the case of Kosarak increases), the mean fraction of preserved itemsets is around 70%, confirming our intuition that the original collection did not contain much interesting structure. This is especially pronounced in the case of the Retail data, where on average 88% of the frequent itemsets are preserved. For the remaining datasets, the mean fraction of preserved itemsets drops below 9%.\nThe swap randomization can be used also to suggest unexpected sets in the data, i.e., sets that are very frequent in the original data but very rare in the swapped data. For example, the {dissertation, doctoral, promising} is common in the Abstracts data (support 682) but rare in the corresponding swapped data (mean support 2.4). Similarly, the set {differential, equations, partial, solutions} has support 679 whereas its mean support in swapped data is less than 0.4. The most \"dull\" itemset is the set {address, result} with supports 691 and 691.6, respectively.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4", "tab_4", "tab_6"]}, {"heading": "Correlations", "text": "We now study how correlations between items change under swap randomization. Computing all pairwise correlations between the columns in the data tables is computationally expensive for most of our datasets. Instead we focus on the k columns with the highest column degree, for k = 100. The rationale is that items that appear frequently are usually more interesting and we want to study their correlations. Furthermore, this allows for an interesting comparison with the randomization technique that permutes columns independently. Since the column counts are large, our experiments give an indication as to how the row counts affect the significance of the pair.  Table 5 shows our results for different datasets. We compute the maximum and minimum correlation values, as well as the number of pairs whose correlation exceeds a certain threshold, for different thresholds. We present the values for the original data, as well as the mean value, and the z-value for both the swapped and the independently permuted data. The statistics are taken over 100 different samples.\nFrom the results we make the following observations. As expected, when randomizing the dataset, strong correlations, either positive or negative, tend to disappear for both methods of randomization. However, the way that this is done differs between the two methods. For the independent permutation model correlations concentrate very sharply and almost symmetrically around zero. For swap randomization negative correlations disappear in a much faster rate, e.g., for Retail and Abstracts they disappear almost completely. On the other hand, the number of  positive correlations remains relatively high, indicating that to some extend the correlations in the dataset (especially low correlations) can be explained by the row and column degrees. This become especially clear when one compares the mean values for the swap and the independent model in the retail and the abstracts datasets. On the other hand, for the Courses and Abstracts datasets we observe that positive correlations drop significantly faster.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "Clustering", "text": "Our results on assessing the clustering structure of datasets are showed in Table 6. We perform our clustering experiments using Matlab's k-means default function. We obtain results only for the small-and medium-size datasets-our largest datasets cannot be clustered by Matlab's k-mean function. For each dataset, we measure the clustering error for the original dataset for clustering with k = 10 and k = 20, which is showed in the third column of Table 6. Then we sample 100 sampled datasets, which we cluster with the same parameters. We compute the mean and standard deviation of the clustering error in the sampled datasets, which are shown in the fourth and fifth columns of Table 6. The sixth column (Z) reports the distance in standard deviations between the error in the original dataset and the mean error in sampled datasets. Finally, the last column records the empirical p-value as described in Section 2.1.\nThe datasets S1 and S2 are synthetically generated datasets with 1000 points, 20 dimensions, and 10 embedded clusters having 100 points each. The difference is that S1 has 10% noise (probability of fliping a bit in the matrix), while S2 has 45% noise, that is, its clustered structure has been hugely corrupted by noise. We see that our results indicate that S1 has indeed clustered structure, while this is not the case for S2. For the real datasets, we see again that all the datasets have clustered structure except from the Retail dataset.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_10", "tab_10", "tab_10"]}, {"heading": "Singular values", "text": "We compute the top-20 singular values for the randomized sets, and we compare the average value of each singular value with the corresponding one of the original dataset. We observed that in most cases, the first singular value of  the random datasets is relatively large, comparable to the first singular value of the original data. The explanation is that the first singular value captures the linear trend that is defined by the degree sequences. In contrast, the nonprincipal singular values are significantly smaller in the random datasets. Thus, we conclude that swap randomization destroys the linear trend in the data.\nSince the sum of the singular values is equal in the original and the swapped data, and since the first singular value in the original data is larger than the first singular value in the swapped data, there should be a \"crossing point\" (see Figure 5). This actually suggests an interesting heuristic for estimating the correct number of dimensions when projecting to low-dimensional spaces. The index of the first singular value that is significantly lower than the corresponding random one is probably a good indicator that the structure contained in the remaining singular vectors is no more interesting than that contained in a random matrix.\nWe observed that in many cases this crossing point has a meaningful interpretation in the data. For example, the Paleo data is conjectured to contain three clusters and the crossing point for this data is indeed at position 3. We experiment further with the above idea on synthetic data, in which we can plant a known number of clusters. Figure 5 shows that for a dataset with 10 clusters the crossing point is at position 10, for noise levels of 10% and 30%, that is, with in a large range of noise.", "publication_ref": [], "figure_ref": ["fig_5", "fig_5"], "table_ref": []}, {"heading": "RELATED WORK", "text": "Defining the significance of discovered patterns has attracted a lot of attention in data mining. In one of the first papers, Brin et al. [3] considered measuring the significance of associations via chi-square test. A lot of other measures have been proposed to capture the interestingness of patterns, e.g., see [10,12,20]; a comprehensive presentation and comparison of such methods can be found in [18].\nThe problem is also very well studied in statistics, and there is a significant amount of work for sampling from the space of contingency tables [4,5,6,17] as well as several studies that give asymptotics on the exact number of such tables, e.g., [19]. A good survey on the topic is provided by Chen et al. [4].\nIn theoretical computer science the subject has drawn attention in the context of providing bounds for the mixing of the Markov chain. Very recently Bez\u00e1kov\u00e1 et al. [1] gave a polynomial time algorithm for sampling binary 0-1 matrices with given margins. The algorithm is based on a different Markov chain than the one based on swaps.\nThe problem of generating random matrices with fixed margins has also been studied in many application areas, such as ecology [16] and biology [11], and analysis of complex networks [14].\nFinally we remark that it is possible to generate directly random datasets that do not preserve exact row and column sums but on expectation. This involves setting each entry (i, j) equal to 1 with probability ricj /L, and equal to 0 otherwise. This expectation model has the drawback that the fraction ricj /L has probability interpretation only if max{ri, cj } \u2264 L. Experimenting with this model we found that it gives similar results as the swap method, but sometimes it is slightly inaccurate. For instance, such inaccuracies were observed in the Kosarak dataset in which both the row and column sums follow power-law distribution. Additionally, the savings in running time are not significant.", "publication_ref": ["b2", "b9", "b11", "b19", "b17", "b3", "b4", "b5", "b16", "b18", "b3", "b0", "b15", "b10", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSIONS AND OPEN PROBLEMS", "text": "We have studied the algorithmic properties of swap randomization, and described how it can be used in assessing results of data mining algorithms. We gave an algorithmic treatment of the swap randomization method, showing some results on the computation of the number of immediately reachable states in the Markov chain, and we showed that the Self loop algorithm is always more efficient than the Metropolis-Hastings method for this problem. Our work shows that swap randomization is efficient in practice, and that it can be used for large datasets.\nWe have conducted extensive experiments on the use of swap randomization. The results are very interesting in that they show big differences in the amount of structure that are present in the datasets. For example, the Retail dataset apparently has very little structure apart from its very skewed degree distribution for columns and (slightly less) for rows. The number of frequent sets in the real dataset is about the same as in the randomized versions, and clustering the original or randomized version yields about the same error. On the other hand, several of the other datasets clearly have lots of second-order structure, as evidenced by the dramatic drop in the number of frequent sets and strong correlations when moving to the randomized version of the data.\nSwap randomization is a technique that maintains the first-order statistics of the data. Thus it should not be used to study the significance of discoveries that depend only on the first-order statistics of the data, i.e., the row and column margins; power laws are an example of these types of statistics. An interesting question is whether it is possible to generate, from a dataset D, random datasets having the same margins as D while keeping also some second-order statistics (e.g., the frequency of certain variable pairs) fixed.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We thank Jean-Fran\u00e7ois Boulicaut, C\u00e9line Robardet, and J\u00e9r\u00e9my Besson for interesting discussions that got us started on swaps.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Availability", "text": "Software for swap randomization can be found at http://www.cs.helsinki.fi/hiit bru/software/swaps/", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Sampling binary contingency tables with a greedy start", "journal": "", "year": "2006", "authors": "I Bez\u00e1kov\u00e1; N Bhatnagar; E Vigoda"}, {"ref_id": "b1", "title": "Using association rules for product assortment decisions: A case study", "journal": "", "year": "1999", "authors": "T Brijs; G Swinnen; K Vanhoof; G Wets"}, {"ref_id": "b2", "title": "Beyond market baskets: Generalizing association rules to correlations", "journal": "", "year": "1997", "authors": "S Brin; R Motwani; C Silverstein"}, {"ref_id": "b3", "title": "Sequential Monte Carlo methods for statistical analysis of tables", "journal": "Journal of the American Statistical Association", "year": "2005", "authors": "Y Chen; P Diaconis; S P Holmes; J S Liu"}, {"ref_id": "b4", "title": "An application of Markov chain Monte Carlo to community ecology", "journal": "American Mathematical Monthly", "year": "2003", "authors": "G W Cobb; Y.-P Chen"}, {"ref_id": "b5", "title": "Rectangular arrays with fixed margins", "journal": "", "year": "1995", "authors": "P Diaconis; A Gangolli"}, {"ref_id": "b6", "title": "Approximate counting by dynamic programming", "journal": "ACM", "year": "2003", "authors": "M Dyer"}, {"ref_id": "b7", "title": "Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses", "journal": "Springer", "year": "2000", "authors": "P Good"}, {"ref_id": "b8", "title": "Monte Carlo sampling methods using Markov chains and their applications", "journal": "Biometrika", "year": "1970", "authors": "W K Hastings"}, {"ref_id": "b9", "title": "A general measure of rule interestingness", "journal": "", "year": "2001", "authors": "S Jaroszewicz; D A Simovici"}, {"ref_id": "b10", "title": "Efficient sampling algorithm for estimating subgraph concentrations and detecting network motifs", "journal": "Bioinformatics", "year": "2004", "authors": "N Kashtan; S Itzkovitz; R Milo; Alon ; U "}, {"ref_id": "b11", "title": "Pruning and summarizing the discovered associations", "journal": "", "year": "1999", "authors": "B Liu; W Hsu; Y Ma"}, {"ref_id": "b12", "title": "Equations of state calculations by fast computing machines", "journal": "Journal of Chemical Physics", "year": "1953", "authors": "N Metropolis; A Rosenbluth; M Rosenbluth; A Teller; E Teller"}, {"ref_id": "b13", "title": "Network motifs: Simple building blocks of complex networks", "journal": "Science", "year": "2002", "authors": "R Milo; S Shen-Orr; S Itzkovirz; N Kashtan; D Chklovskii; Alon ; U "}, {"ref_id": "b14", "title": "The structure and function of complex networks", "journal": "SIAM Review", "year": "2003", "authors": "M Newman"}, {"ref_id": "b15", "title": "Testing ecological patterns", "journal": "American Scientist", "year": "2000", "authors": "J Sanderson"}, {"ref_id": "b16", "title": "Enumeration and simulation methods for 0-1 matrices with given marginals", "journal": "Psychometrika", "year": "1991", "authors": "F Snijders"}, {"ref_id": "b17", "title": "Selecting the right interestingness measure for association patterns", "journal": "", "year": "2002", "authors": "P.-N Tan; V Kumar; J Srivastava"}, {"ref_id": "b18", "title": "Precise number of (0, 1)-matrices in u(r, s)", "journal": "Discrete Mathematics", "year": "1998", "authors": "B Y Wang; F Zhang"}, {"ref_id": "b19", "title": "Exploiting a support-based upper bound of Pearson's correlation coefficient for efficiently identifying strongly correlated pairs", "journal": "", "year": "2004", "authors": "H Xiong; S Shekhar; P.-N Tan; V Kumar"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: A swap in a 0-1 matrix.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Examples of two 0-1 datasets, D1 and D2.In both cases we are interested in the correlation between columns (attributes) X and Y . The significance of the correlation result might depend on the overall context of the dataset", "figure_data": ""}, {"figure_label": "22", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "2 Corollary 2 .22Given graphs G and G such that (G, G ) \u2208 T , d(G ) can be calculated from d(G) in time O(min{m, n}). Proof. Without loss of generality assume that min{m, n} = m, and we are using the m \u00d7 m matrix M = DD T . Otherwise we can use the n \u00d7 n matrix M = D T D. Using Equation (2) we have d(G ) = d(G) \u2212 \u2206Z + 2 \u2206K22 Graphs G and G differ only by one swap. so, matrices D(G) and D(G ) differ only in four positions, and matrices M (G) and M (G ) differ only in two rows and two columns. Therefore \u2206Z can be computed in constant time and \u2206K22 can be computed in O(m) time.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Convergence: x-axis: the number of steps (\u00d7 the number of 1's in the data); y-axis: the number of frequent itemsets in the sampled datasets, divided by the number of frequent itemsets in the original dataset. Dataset time (sec) Abstracts 12m53s Abstracts 9m11s Courses 3.35s Dataset time (sec) Kosarak 8m38s Paleo 0.100s Retail 1m1.5s", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure 5: Singular values of the original dataset and of sampled datasets (mean out 100 samples) for synthetic datasets. The crossing point of the two lines at position 10 corresponds to the number of clusters planted in the data.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "The datasets the cost of the Find adjacent algorithm. However, this is non-trivial task, and it still does not guarantee that the Metropolis-Hastings algorithm would be faster. Recall also that in many cases, the cost of Find adjacent is constant in expectation.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": "Running times needed to perform swaprandomization on the different datasets. We reportthe clock time (in secs) needed to perform a numberof swaps equal to 5 times the number of the 1's inthe dataset."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "The number of frequent itemsets. |X|: the minimum cardinality of the itemset we include to the count. |F|: the number of frequent sets of cardinality at least |X| in the original data; |Fs|: the expected number of frequent sets in swapped data; |Fp|: the expected number of frequent sets in random data with the same column margins as the original data. The values in parentheses are the standard deviations. The expectations and standard deviations were computed on 500 experiments.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Changes in the collections of frequent sets. D: the dataset; F: the frequent itemset collection in the dataset; Fs: the frequent itemset collection in the swapped dataset; |F \u2229Fs| |F | : the fraction of itemsets that are preserved in the collection; |F \\Fs| |F | : the fraction of frequent itemsets that disappear from the collection. The values involving swapped data are expectations on 500 experiments. The values in the parentheses are the standard deviations.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Statistics for correlation values. A row of type max contains the value of the largest correlation, while a row of type, say \u2265 0.01, contains the number of correlation pairs with value greater than 0.01. The empirical p statistic in all the above results is 1%.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Results on Clustering. k: number of clusters used in k-means, E: clustering error in the original dataset, mean: mean clustering error in the sampled datasets, std: standard deviation of the clustering error in the sampled datasets, Z: distance of E from mean measured in standard deviations, p: empirical p-value.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "1 k + 1 (min{|{t | Xt < X0}|, |{t | Xt > X0}|} + 1) ,", "formula_coordinates": [2.0, 336.84, 43.47, 200.13, 20.85]}, {"formula_id": "formula_1", "formula_text": "Z = |X0 \u2212 b X| b \u03c3 , where b X = E[X1, . . . , X k ]", "formula_coordinates": [2.0, 316.8, 117.09, 149.88, 39.45]}, {"formula_id": "formula_2", "formula_text": "X Y 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 Dataset D1 X Y 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 Dataset D2", "formula_coordinates": [3.0, 79.92, 4.36, 183.67, 151.29]}, {"formula_id": "formula_3", "formula_text": "l j i k j l i k Figure 3: A swap in the graph representation GD.", "formula_coordinates": [4.0, 57.24, 4.42, 232.19, 43.58]}, {"formula_id": "formula_4", "formula_text": "E \u2190 E \\ {(i, j), (k, l)} \u222a {(i, l), (k, j)},", "formula_coordinates": [4.0, 95.53, 466.96, 155.38, 8.97]}, {"formula_id": "formula_5", "formula_text": "G \u2190 Find adjacent(G) 4: G \u2190 G 5:", "formula_coordinates": [4.0, 320.76, 60.28, 119.64, 29.85]}, {"formula_id": "formula_6", "formula_text": "Input: Graph G Ouput: Graph G that differs from G in exactly one swap (i.e., (G, G ) \u2208 T ) repeat Select edges (i, j), (k, l) \u2208 E(G) uniformly at random until (i, l) \u2208 E(G) and (k, j) \u2208 E(G) E(G ) \u2190 E(G) \\ {(i, j), (k, l)} \u222a {(i, l), (k, j)}", "formula_coordinates": [4.0, 316.8, 209.8, 238.92, 71.73]}, {"formula_id": "formula_7", "formula_text": "Select edges (i, j), (k, l) \u2208 E(G) 4: if ((i, l) \u2208 E(G) and (k, j) \u2208 E(G)) then 5: E(G ) \u2190 E(G) \\ {(i, j), (k, l)} \u222a {(i, l), (k, j)} 6:", "formula_coordinates": [5.0, 57.72, 301.6, 211.64, 40.29]}, {"formula_id": "formula_8", "formula_text": "1: G \u2190 GD 2: while km > 0 do 3: G \u2190 Find adjacent(G) 4: G \u2190 G , with probability min{1, d(G) d(G ) } 5:", "formula_coordinates": [5.0, 57.72, 580.6, 179.76, 53.73]}, {"formula_id": "formula_9", "formula_text": "d(G) |E| 2 , thus the expected time staying in G is |E| 2 d(G) . Without counting the self loops, the probability of visiting graph G is d(G)", "formula_coordinates": [5.0, 316.8, 187.11, 239.11, 40.89]}, {"formula_id": "formula_10", "formula_text": "TF = X G\u2208S |E| 2 d(G) \u2022 d(G) 2|T | = |E| 2 2 \u2022 |S| |T | . (1", "formula_coordinates": [5.0, 363.36, 254.97, 188.67, 26.97]}, {"formula_id": "formula_11", "formula_text": ")", "formula_coordinates": [5.0, 552.03, 263.44, 3.92, 8.97]}, {"formula_id": "formula_12", "formula_text": "Lemma 1. For bipartite graphs G = (U, V, E) in which the maximum degree is o(|E|), we have |T |/|S| = \u2126(|E| 2 ).", "formula_coordinates": [5.0, 316.8, 329.68, 238.84, 19.41]}, {"formula_id": "formula_13", "formula_text": "|T |/|S| = \u2126(|E| 2 ).", "formula_coordinates": [5.0, 316.8, 491.16, 74.03, 10.45]}, {"formula_id": "formula_14", "formula_text": "d(G) = J(G) \u2212 Z(G) + 2K22(G),(2)", "formula_coordinates": [6.0, 107.04, 148.24, 185.85, 8.97]}, {"formula_id": "formula_15", "formula_text": "J(G) = 1 2 |E|(|E| + 1) \u2212 X i\u2208U r 2 i \u2212 X j\u2208V c 2 j ! is the number of disjoint pairs of edges, Z(G) = X (i,j)\u2208E (ri \u2212 1)(cj \u2212 1)", "formula_coordinates": [6.0, 53.76, 172.53, 206.4, 78.84]}, {"formula_id": "formula_16", "formula_text": "{(a, b), (c, d), (c, b) \u2208 E, with a, b, c, d all distinct}, and K22(G) = X i,k\u2208U i =k M (i, k) 2 ! = 1 2 X i,k\u2208U i =k M (i, k) 2 \u2212 M (i, k)", "formula_coordinates": [6.0, 53.76, 275.8, 231.33, 64.5]}], "doi": ""}