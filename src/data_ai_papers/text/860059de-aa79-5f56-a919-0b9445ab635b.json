{"title": "Considerations for meaningful sign language machine translation based on glosses", "authors": "Mathias M\u00fcller; Zifan Jiang; Amit Moryossef; Annette Rios; Sarah Ebling", "pub_date": "", "abstract": "Automatic sign language processing is gaining popularity in Natural Language Processing (NLP) research . In machine translation (MT) in particular, sign language translation based on glosses is a prominent approach. In this paper, we review recent works on neural gloss translation. We find that limitations of glosses in general and limitations of specific datasets are not discussed in a transparent manner and that there is no common standard for evaluation. To address these issues, we put forward concrete recommendations for future research on gloss translation. Our suggestions advocate awareness of the inherent limitations of glossbased approaches, realistic datasets, stronger baselines and convincing evaluation.", "sections": [{"heading": "Introduction", "text": "Automatic sign language processing is becoming more popular in NLP research . In machine translation (MT) in particular, many recent publications have proposed sign language translation (SLT) based on glosses. Glosses provide semantic labels for individual signs. They typically consist of the base form of a word in the surrounding spoken language written in capital letters (see Table 1). Even though glosses are not a complete representation of signs (see e.g. Pizzuto et al. 2006), they are often adopted in MT because, by virtue of being textual, they fit seamlessly into existing MT pipelines and existing methods seemingly require the least modification.\nIn this paper, we review recent works on neural gloss translation. We find that limitations of gloss-based approaches in general and limitations of specific datasets are not transparently discussed as inherent shortcomings. Furthermore, among gloss translation papers there is no common standard for evaluation, especially regarding the exact method to compute BLEU scores.  Example of sign language glosses. DSGS=Swiss German Sign Language, DE=German, EN=English. English translations are provided for convenience. Example is adapted from a lexicon of the three sign languages of Switzerland, where a sign language video of this sentence is available (https:// signsuisse.sgb-fss.ch/de/lexikon/g/ferien/).\nExperiments in SLT should be informed by sign language expertise and should be performed according to the best practices already established in the MT community.\nTo alleviate these problems going forward, we make practical recommendations for future research on gloss translation.\nOur paper makes the following contributions:\n\u2022 We provide a review of recent works on gloss translation ( \u00a72).\n\u2022 We outline recommendations for future work which promote awareness of the inherent limitations of gloss-based approaches, realistic datasets, stronger baselines and convincing evaluation ( \u00a73).", "publication_ref": ["b26"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Related work", "text": "For a general, interdisciplinary introduction to sign language processing see Bragg et al. (2019).\nFor an overview in the context of NLP see ;   \n\u2714 - \u2714 - - \u2714 \u2714 \u2714 \u2714 - Tensorflow Stoll et al. (2018) - \u2714 - - \u2714 - - \u2714 \u2714 \u2714 - (unclear) Camg\u00f6z et al. (2020b) \u2714 \u2714 - \u2714 - - \u2714 \u2714 \u2714 - WER (unclear) Camg\u00f6z et al. (2020a) \u2714 \u2714 - \u2714 - - - - \u2714 \u2714 - (unclear)\nYin and Read (2020\n) \u2714 \u2714 ASLG-PC12 \u2714 - ASL\u2192EN \u2714 \u2714 \u2714 \u2714 METEOR NLTK Saunders et al. (2020) \u2714 \u2714 - - \u2714 - \u2714 \u2714 \u2714 \u2714 - (unclear) Stoll et al. (2020) \u2714 \u2714 - - \u2714 - - \u2714 \u2714 \u2714 WER (unclear)\nOrbay and Akarun (2020\n) - \u2714 - \u2714 - - - \u2714 \u2714 \u2714 - (unclear) Moryossef et al. (2021) - \u2714 NCSLGR \u2714 - ASL\u2192EN (\u2714) - \u2714 - COMET SacreBLEU\nZhang and Duh (2021 of sign language machine translation (including, but not limited to, gloss-based approaches).\n) - \u2714 - \u2714 \u2714 - - - \u2714 - - (unclear) Egea G\u00f3mez et al. (2021) - \u2714 - - \u2714 - \u2714 - \u2714 \u2714 METEOR, TER SacreBLEU Saunders et al. (2022) - \u2714 DGS Corpus - \u2714 - - - \u2714 \u2714 - (unclear) Angelova et al. (2022) \u2714 \u2714 DGS Corpus \u2714 - - \u2714 - \u2714 - - SacreBLEU Walsh et al. (2022) - \u2714 DGS Corpus - \u2714 - - \u2714 \u2714 \u2714 - (unclear)\nWe conduct a more narrow literature review of 14 recent publications on gloss translation. We report characteristics such as the datasets used, translation directions, and evaluation details (Table 2). Our informal procedure of selecting papers is detailed in Appendix A.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Awareness of limitations of gloss approach", "text": "We find that 8 out of 14 reviewed works do not include an adequate discussion of the limitations of gloss approaches, inadvertently overstating the potential usefulness of their experiments.\nIn the context of sign languages, glosses are unique identifiers for individual signs. However, a linear sequence of glosses is not an adequate representation of a signed utterance, where different channels (manual and non-manual) are engaged simultaneously. Linguistically relevant cues such as non-manual movement or use of three-dimensional space may be missing .\nThe gloss transcription conventions of different corpora vary greatly, as does the level of detail (see Kopf et al. (2022) for an overview of differences and commonalities between corpora). Therefore, glosses in different corpora or across languages are not comparable. Gloss transcription is an enormously laborious process done by expert linguists.\nBesides, glosses are a linguistic tool, not a writing system established in Deaf communities. Sign language users generally do not read or write glosses in their everyday lives.\nTaken together, this means that gloss translation suffers from an inherent and irrecoverable information loss, that creating an abundance of translations transcribed as glosses is unrealistic, and that gloss translation systems are not immediately useful to end users.", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "Choice of dataset", "text": "All reviewed works use the RWTH-PHOENIX Weather 2014T (hereafter abbreviated as PHOENIX) dataset (Forster et al., 2014; while other datasets are used far less frequently. Besides, we note a distinct paucity of languages and translation directions: 12 out of 14 works are concerned only with translation between German Sign Language (DGS) and German (DE), the language pair of the PHOENIX dataset.\nWhile PHOENIX was a breakthrough when it was published, it is of limited use for current research. The dataset is small (8k sentence pairs) and contains only weather reports, covering a very narrow linguistic domain. It is important to discuss the exact nature of glosses, how the corpus was created and how it is distributed.   3: Comparison between PHOENIX and a small selection of alternative corpora. DGS=German Sign Language, DE=German, BSL=British Sign Language, DSGS=Swiss German Sign Language, #signs=number of unique signs (if available), #signers=number of individual signers, LI=live interpretation, OS=signing is the original source material, then translated to spoken language text. * =after preprocessing the glosses as described in Appendix C.\nGlossing PHOENIX is based on German weather reports interpreted into DGS and broadcast on the TV station Phoenix. The broadcast videos served as input for the DGS side of the parallel corpus. Compared to the glossing conventions of other well-known corpora, PHOENIX glosses are simplistic and capture mostly manual features (with mouthings as the only non-manual activity), which is not sufficient to represent meaning ( \u00a72.1).", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Live interpretation and translationese effects", "text": "The fact that PHOENIX data comes from interpretation in a live setting has two implications: Firstly, since information was conveyed at high speed, the sign language interpreters omitted pieces of information from time to time. This leads to an information mismatch between some German sentences and their DGS counterparts. Secondly, due to the high speed of transmission, the (hearing) interpreters sometimes followed the grammar of German more closely than that of DGS, amounting to a translationese effect.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Preprocessing of spoken language", "text": "The German side of the PHOENIX corpus is available only already tokenized, lowercased and with punctuation symbols removed. From an MT perspective this is unexpected since corpora are usually distributed without such preprocessing.\nPHOENIX is popular because it is freely available and is a benchmark with clearly defined data splits introduced by Camg\u00f6z et al. (2018). SLT as a field is experiencing a shortage of free and open datasets and, with the exception of PHOENIX, there are no agreed-upon data splits.\nEssentially, from a scientific point of view achieving higher gloss translation quality on the PHOENIX dataset is near meaningless. The apparent overuse of PHOENIX is reminiscent of the overuse of MNIST (LeCun et al., 2010) in machine learning, or the overuse of the WMT 14 English-German benchmark in the MT community, popularized by Vaswani et al. (2017).\nAlternative corpora In Table 3 we list several alternatives to PHOENIX, to exemplify how other corpora are preferable in different ways. For example, in PHOENIX the sign language data is produced by hearing interpreters in a live interpretation setting. In contrast, the Public DGS Corpus and Fo-cusNews contain original (non-translated) signing material produced by deaf signers. PHOENIX is limited to weather reports, while all other corpora listed in Table 3 feature much broader domains. The number of different signs found in PHOENIX is also small compared to alternative corpora. For instance, the sign vocabulary of BOBSL is twice as large as for PHOENIX, which corroborates that the language data in BOBSL indeed is more varied. Besides, BOBSL also is vastly bigger than PHOENIX and features more individual signers.", "publication_ref": ["b38"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "As evaluation metrics, all works use some variant of BLEU (Papineni et al., 2002), and ten out of 14 use some variant of ROUGE (Lin, 2004  Non-standard metrics ROUGE is a metric common in automatic summarization but not in MT, and was never correlated with human judgement in a large study. In eight out of 14 papers, BLEU is used with a non-standard maximum ngram order, producing variants such as BLEU-1, BLEU-2, etc. Similar to ROUGE, these variants of BLEU have never been validated as metrics of translation quality, and their use is scientifically unmotivated.\nTokenization BLEU requires tokenized machine translations and references. Modern tools therefore apply a tokenization procedure internally and implicitly (independently of the MT system's preprocessing). Computing BLEU with tokenization on glosses leads to seemingly better scores but is misleading since tokenization creates trivial matches. For instance, in corpora that make use of the character $ in glosses (e.g. the DGS Corpus (Konrad et al., 2022)), $ is split off as a single character, inflating the ngram sub-scores. For an illustration see Table 4 (and Appendix B for a complete code listing) where we demonstrate that using or omitting tokenization leads to a difference of 15 BLEU.\nSpurious gains Different implementations of BLEU or different tokenizations lead to differences in BLEU bigger than what many papers describe as an \"improvement\" over previous work (Post, 2018). Incorrectly attributing such improvements to, for instance, changes to the model architecture amounts to a \"failure to identify the sources of empirical gains\" (Lipton and Steinhardt, 2019). In a similar vein, we observe that papers on gloss translation tend to copy scores from previous papers without knowing whether the evaluation procedures are in fact the same. This constitutes a general trend in recent MT literature (Marie et al., 2021).\nIn summary, some previous works on gloss translation have used 1) automatic metrics that are not suitable for MT or 2) well-established MT metrics in ways that are not recommended. BLEU with standard settings and tools is inappropriate for gloss outputs.\nThe recommended way to compute BLEU on gloss output is to use the tool SacreBLEU (Post, 2018) and to disable internal tokenization. Nevertheless, even with these precautions, it is important to note that BLEU was never validated empirically as an evaluation metric for gloss output. Some aspects of BLEU may not be adequate for a sequence of glosses, such as its emphasis on whitespaces to mark the boundaries of meaningful units that are the basis of the final score.\nOther string-based metrics such as CHRF (Popovi\u0107, 2016) may be viable alternatives for gloss evaluation. CHRF is a character-based metric and its correlation with human judgement is at least as good as BLEU's (Kocmi et al., 2021).\nOn a broader note, we do not advocate BLEU in particular, but advocate that any evaluation metric is used according to best practices in MT. Some of the best practices (such as reporting the metric signature) equally apply to all metrics. A key limitation regarding choosing a metric is that many metrics that are indeed advocated today, such as COMET (Rei et al., 2020), cannot be used for gloss outputs because this \"language\" is not supported by COMET. There are also hardly any human judgement scores to train new versions of neural metrics. 2), we observe that most papers do not process glosses in any corpus-specific way, and that particular modeling and training decisions may not be ideal for low-resource gloss translation.", "publication_ref": ["b25", "b18", "b14", "b28", "b19", "b20", "b28", "b27", "b13", "b29"], "figure_ref": [], "table_ref": ["tab_6", "tab_2"]}, {"heading": "Further observations", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "More informally (beyond what we show in Table", "text": "Preprocessing glosses Glosses are created for linguistic purposes ( \u00a72.1), not necessarily with machine translation in mind. Particular gloss parts are not relevant for translation and, if kept, make the problem harder unnecessarily. For instance, a corpus transcription and annotation scheme might prescribe that meaning-equivalent, minor form variants of signs are transcribed as different glosses.\nAs the particular nature of glosses is specific to each corpus, it is necessary to preprocess glosses in a corpus-specific way. We illustrate corpus-specific gloss processing in Appendix C, using the Public DGS Corpus (Hanke et al., 2020) as an example.\nModeling and training decisions Gloss translation experiments are certainly low-resource scenarios and therefore, best practices for optimizing MT systems on low-resource datasets apply (Sennrich and Zhang, 2019). For example, dropout rates or label smoothing should be set accordingly, and the vocabulary of a subword model should be generally small (Ding et al., 2019). Gloss translation models are often compared to other approaches as baselines, it is therefore problematic if those gloss baselines are weak and unoptimized (Denkowski and Neubig, 2017).", "publication_ref": ["b12", "b32", "b9", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Recommendations for gloss translation", "text": "Based on our review of recent works on gloss translation, we make the following recommendations for future research:\n\u2022 Demonstrate awareness of limitations of gloss approaches ( \u00a72.1) and explicitly discuss them.\n\u2022 Focus on datasets beyond PHOENIX. Openly discuss the limited size and linguistic domain of PHOENIX ( \u00a72.2).\n\u2022 Use metrics that are well-established in MT.\nIf BLEU is used, compute it with SacreBLEU, report metric signatures and disable internal tokenization for gloss outputs. Do not compare to scores produced with a different or unknown evaluation procedure ( \u00a72.3).\n\u2022 Given that glossing is corpus-specific ( \u00a72.1), process glosses in a corpus-specific way, informed by transcription conventions ( \u00a72.4).\n\u2022 Optimize gloss translation baselines with methods shown to be effective for lowresource MT ( \u00a72.4).\nWe also believe that publishing reproducible code makes works on gloss translation more valuable to the scientific community.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Justification for recommendations", "text": "There is an apparent tension between making recommendations for future work on gloss translation and at the same time claiming that the paradigm of gloss translation is inadequate to begin with ( \u00a72.1). But importantly, further works on gloss translation are likely because MT researchers have a preference for textbased translation problems and little awareness of sign linguistics. If further research is conducted, it should be based on sound scientific methodology.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Alternatives to gloss translation", "text": "In previous sections we have established that glosses are a lossy representation of sign language. We also argued that the most prominent benchmark corpus for gloss translation (PHOENIX) is inadequate, but other, preferable corpora do not contain glosses. This begs the question: if not gloss translation, what other approach should be pursued?\nRepresenting sign language Alternatives include translation models that extract features directly from video, generate video directly or use pose estimation data as a sign language representation (Tarr\u00e9s et al., 2023;M\u00fcller et al., 2022). A distinct advantage of such systems is that they produce a sign language output that is immediately useful to a user, whereas glosses are only an intermediate output that are not intelligible by themselves.\nIf a system generates a continuous output such as a video, then evaluating translation quality with an automatic metric is largely an unsolved problem. Even though there are recent proposals for metrics (e.g. Arkushin et al., 2023), more fundamental research in this direction is still required.", "publication_ref": ["b37", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper we have shown that some recent works on gloss translation lack awareness of the inherent limitations of glosses and common datasets, as well as a standardized evaluation method ( \u00a72). In order to make future research on gloss translation more meaningful, we make practical recommendations for the field ( \u00a73).\nWe urge researchers to spell out limitations of gloss translation approaches, e.g. in the now mandatory limitation sections of *ACL papers, and to strengthen their findings by implementing existing best practices in MT.\nFinally, we also caution that researchers should consider whether gloss translation is worthwhile, and if time and effort would be better spent on basic linguistic tools (such as segmentation, alignment or coreference resolution), creating training corpora or translation methods that do not rely on glosses.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Our approach to surveying the research literature has limitations. Firstly, some characterizations of the published works we survey are subjective. For example, it is somewhat subjective whether a paper \"includes an adequate discussion of the limitations of glosses\" and somewhat subjective whether the evaluation procedure is explained in enough detail.\nFurthermore, it is likely that our survey missed some existing publications, especially if published in other contexts than NLP and machine learning conferences and journals. This may have skewed our findings.\nFinally, the statements and recommendations in this paper are valid only as long as automatic glossing from video is not feasible. If a scientific breakthrough is achieved in the future, the relevance of glosses for sign language translation may need to be re-evaluated.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data licensing", "text": "The license of the Public DGS Corpus 1 (which we use only as examples in Table 4 and Appendix C) does not allow any computational research except if express permission is given by the University of Hamburg.\nA Informal procedure of selecting papers for review Since our paper is first and foremost a position paper we did not follow a rigorous process when selecting papers to review. Our informal criteria are as follows:\n\u2022 Discover papers indexed by the ACL anthology, published at a more general machine learning conference or published in a computational linguistics journal.\n\u2022 Limit our search to papers on gloss translation (as opposed to other MT papers on sign language).\n\u2022 Only consider neural approaches to gloss translation, excluding statistical or rule-based works.\n\u2022 Limit to recent works published in the last five years.\nB Impact of internal tokenization when computing BLEU on gloss sequences  Listing 1: Impact of enabling or disabling internal tokenization (mtv13a) when computing BLEU on gloss outputs.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "C Example for corpus-specific gloss preprocessing", "text": "For this example, we recommend downloading and processing release 3.0 of the corpus. To DGS glosses we suggest to apply the following modifications derived from the DGS Corpus transcription conventions (Konrad et al., 2022):\n\u2022 Removing entirely two specific gloss types that cannot possibly help the translation: $GEST-OFF and $$EXTRA-LING-MAN.\n\u2022 Removing ad-hoc deviations from citation forms, marked by *. Example: ANDERS1* \u2192 ANDERS1.\n\u2022 Removing the distinction between type glosses and subtype glosses, marked by\u02c6. Example: WISSEN2B\u02c6\u2192 WISSEN2B.\n\u2022 Collapsing phonological variations of the same type that are meaning-equivalent. Such variants are marked with uppercase letter suffixes. Example: WISSEN2B \u2192 WISSEN2.\n\u2022 Deliberately keep numerals ($NUM), list glosses ($LIST) and finger alphabet ($ALPHA) intact, except for removing handshape variants.\nSee Table 5 for examples for this preprocessing step. Overall these simplifications should reduce the number of observed forms while not affecting the machine translation task. For other purposes such as linguistic analysis our preprocessing would of course be detrimental. While this preprocessing method provides a good baseline, it can certainly be refined further. For instance, the treatment of two-handed signs could be improved. If a gloss occurs simultaneously on both hands, we either keep both glosses or remove one occurrence. In both cases, information about the simultaneity of signs is lost during preprocessing and preserving it could potentially improve translation. the only way we used artifacts is in the sense of using examples from public corpora, in Table 1, Table 3 and Appendix B B1. Did you cite the creators of artifacts you used? Table 1, Table 3 and Appendix B B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nbefore $INDEX1 ENDE1\u02c6ANDERS1* SEHEN1 M\u00dcNCHEN1B* BEREICH1A* after $INDEX1 ENDE1 ANDERS1 SEHEN1 M\u00dcNCHEN1 BEREICH1 before ICH1 ETWAS-PLANEN-UND-UMSETZEN1 SELBST1A* KLAPPT1* $GEST-OFF\u02c6BIS-JETZT1 GEWOHNHEIT1* $GEST-OFF\u02c6* after ICH1 ETWAS-PLANEN-UND-UMSETZEN1 SELBST1 KLAPPT1 BIS-JETZT1 GEWOHNHEIT1\nwe will discuss the license terms explicitly in the camera-ready version. We omitted this on purpose in the review version as a precaution for anonymity B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Not applicable. Left blank.\nB4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. Left blank.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Not applicable. Left blank.\nB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. Not applicable. Left blank.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": ["tab_7", "tab_0", "tab_0"]}, {"heading": "C Did you run computational experiments?", "text": "Left blank.\nC1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? No response.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This work was funded by the EU Horizon 2020 project EASIER (grant agreement no. 101016982), the Swiss Innovation Agency (Innosuisse) flagship IICT (PFFS-21-47) and the EU Horizon 2020 project iEXTRACT (grant agreement no. 802774).\nWe thank the DGS Corpus team at the University of Hamburg for helpful discussions on gloss preprocessing. Finally, we thank the anonymous reviewers for their help in improving this paper.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "BOBSL: BBC-Oxford British Sign Language Dataset", "journal": "", "year": "2021", "authors": "G\u00fcl Samuel Albanie; Liliane Varol; Hannah Momeni; Triantafyllos Bull; Himel Afouras; Neil Chowdhury; Bencie Fox; Rob Woll; Andrew Cooper; Andrew Mcparland;  Zisserman"}, {"ref_id": "b1", "title": "Using neural machine translation methods for sign language translation", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Galina Angelova; Eleftherios Avramidis; Sebastian M\u00f6ller"}, {"ref_id": "b2", "title": "Ham2pose: Animating sign language notation into pose sequences", "journal": "", "year": "2023", "authors": "Rotem Shalev Arkushin"}, {"ref_id": "b3", "title": "Sign language recognition, generation, and translation: An interdisciplinary perspective", "journal": "", "year": "2019", "authors": "Danielle Bragg; Oscar Koller; Mary Bellard; Larwan Berke; Patrick Boudreault; Annelies Braffort; Naomi Caselli; Matt Huenerfauth; Hernisa Kacorri; Tessa Verhoef"}, {"ref_id": "b4", "title": "Neural sign language translation", "journal": "", "year": "2018", "authors": "Simon Necati Cihan Camg\u00f6z; Oscar Hadfield; Hermann Koller; Richard Ney;  Bowden"}, {"ref_id": "b5", "title": "Multi-channel transformers for multi-articulatory sign language translation", "journal": "Springer-Verlag", "year": "2020-08-23", "authors": "Oscar Necati Cihan Camg\u00f6z; Simon Koller; Richard Hadfield;  Bowden"}, {"ref_id": "b6", "title": "Sign language transformers: Joint end-to-end sign language recognition and translation", "journal": "", "year": "2020", "authors": "Oscar Necati Cihan Camg\u00f6z; Simon Koller; Richard Hadfield;  Bowden"}, {"ref_id": "b7", "title": "Mieke Van Herreweghe, and Joni Dambre. 2022. Machine translation from signed to spoken languages: State of the art and challenges", "journal": "", "year": "", "authors": "Dimitar Mathieu De Coster;  Shterionov"}, {"ref_id": "b8", "title": "Stronger baselines for trustable results in neural machine translation", "journal": "", "year": "2017", "authors": "Michael Denkowski; Graham Neubig"}, {"ref_id": "b9", "title": "A call for prudent choice of subword merge operations in neural machine translation", "journal": "European Association for Machine Translation", "year": "2019", "authors": "Shuoyang Ding; Adithya Renduchintala; Kevin Duh"}, {"ref_id": "b10", "title": "Syntax-aware transformers for neural machine translation: The case of text to sign gloss translation", "journal": "", "year": "2021", "authors": "Euan Santiago Egea G\u00f3mez;  Mcgill"}, {"ref_id": "b11", "title": "Extensions of the sign language recognition and translation corpus RWTH-PHOENIX-weather", "journal": "", "year": "2014", "authors": "Jens Forster; Christoph Schmidt; Oscar Koller; Martin Bellgardt; Hermann Ney"}, {"ref_id": "b12", "title": "Extending the Public DGS Corpus in size and depth", "journal": "", "year": "2020", "authors": "Thomas Hanke; Marc Schulder; Reiner Konrad; Elena Jahn"}, {"ref_id": "b13", "title": "To ship or not to ship: An extensive evaluation of automatic metrics for machine translation", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Tom Kocmi; Christian Federmann; Roman Grundkiewicz; Marcin Junczys-Dowmunt"}, {"ref_id": "b14", "title": "Rie Nishio, and Anja Regen", "journal": "", "year": "2022", "authors": "Reiner Konrad; Thomas Hanke; Gabriele Langer; Susanne K\u00f6nig; Lutz K\u00f6nig"}, {"ref_id": "b15", "title": "Specification for the harmonization of sign language annotations", "journal": "", "year": "2022", "authors": "Maria Kopf; Marc Schulder; Thomas Hanke; Sam Bigeard"}, {"ref_id": "b16", "title": "Mnist handwritten digit database", "journal": "", "year": "2010", "authors": "Yann Lecun; Corinna Cortes; C J Burges"}, {"ref_id": "b17", "title": "Available", "journal": "", "year": "", "authors": ""}, {"ref_id": "b18", "title": "ROUGE: A package for automatic evaluation of summaries", "journal": "Association for Computational Linguistics", "year": "2004", "authors": "Chin-Yew Lin"}, {"ref_id": "b19", "title": "Troubling trends in machine learning scholarship: Some ml papers suffer from flaws that could mislead the public and stymie future research", "journal": "Queue", "year": "2019", "authors": "Zachary C Lipton; Jacob Steinhardt"}, {"ref_id": "b20", "title": "Scientific credibility of machine translation research: A meta-evaluation of 769 papers", "journal": "Long Papers", "year": "2021", "authors": "Benjamin Marie; Atsushi Fujita; Raphael Rubino"}, {"ref_id": "b21", "title": "Sign Language Processing", "journal": "", "year": "2021", "authors": "Amit Moryossef; Yoav Goldberg"}, {"ref_id": "b22", "title": "Data augmentation for sign language gloss translation", "journal": "", "year": "2021", "authors": "Amit Moryossef; Kayo Yin; Graham Neubig; Yoav Goldberg"}, {"ref_id": "b23", "title": "Sandra Sidler-miserez, and Katja Tissi. 2022. Findings of the first WMT shared task on sign language translation (WMT-SLT22)", "journal": "Association for Computational Linguistics", "year": "", "authors": "Mathias M\u00fcller; Sarah Ebling; Eleftherios Avramidis; Alessia Battisti; Mich\u00e8le Berger; Richard Bowden; Annelies Braffort; Cristina Necati Cihan Camg\u00f6z; Roman Espa\u00f1a-Bonet; Zifan Grundkiewicz; Oscar Jiang; Amit Koller; Regula Moryossef; Sabine Perrollaz; Annette Reinhard; Dimitar Rios;  Shterionov"}, {"ref_id": "b24", "title": "Neural sign language translation by learning tokenization", "journal": "", "year": "2020", "authors": "Alptekin Orbay; Lale Akarun"}, {"ref_id": "b25", "title": "Bleu: a method for automatic evaluation of machine translation", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"ref_id": "b26", "title": "Representing signed languages in written form: Questions that need to be posed", "journal": "", "year": "2006", "authors": "Elena Antinoro Pizzuto; Paolo Rossini; Tommaso Russo"}, {"ref_id": "b27", "title": "chrF deconstructed: beta parameters and n-gram weights", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Maja Popovi\u0107"}, {"ref_id": "b28", "title": "A call for clarity in reporting BLEU scores", "journal": "", "year": "2018", "authors": "Matt Post"}, {"ref_id": "b29", "title": "COMET: A neural framework for MT evaluation", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Ricardo Rei; Craig Stewart; Ana C Farinha; Alon Lavie"}, {"ref_id": "b30", "title": "Progressive Transformers for End-to-End Sign Language Production", "journal": "", "year": "2020", "authors": "Ben Saunders"}, {"ref_id": "b31", "title": "Signing at Scale: Learning to Co-Articulate Signs for Large-Scale Photo-Realistic Sign Language Production", "journal": "", "year": "2022", "authors": "Ben Saunders; Richard Necati Cihan Camg\u00f6z;  Bowden"}, {"ref_id": "b32", "title": "Revisiting lowresource neural machine translation: A case study", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Rico Sennrich; Biao Zhang"}, {"ref_id": "b33", "title": "Sign language production using neural machine translation and generative adversarial networks", "journal": "", "year": "2018", "authors": "Stephanie Stoll; Simon Necati Cihan Camg\u00f6z; Richard Hadfield;  Bowden"}, {"ref_id": "b34", "title": "British Machine Vision Conference", "journal": "", "year": "2018", "authors": ""}, {"ref_id": "b35", "title": "British Machine Vision Association", "journal": "", "year": "", "authors": ""}, {"ref_id": "b36", "title": "Text2sign: towards sign language production using neural machine translation and generative adversarial networks", "journal": "International Journal of Computer Vision", "year": "2020", "authors": "Stephanie Stoll; Simon Necati Cihan Camg\u00f6z; Richard Hadfield;  Bowden"}, {"ref_id": "b37", "title": "Sign language translation from instructional videos", "journal": "", "year": "2023", "authors": "Laia Tarr\u00e9s; I Gerard; Amanda G\u00e1llego; Jordi Duarte; Xavier Gir\u00f3-I Torres;  Nieto"}, {"ref_id": "b38", "title": "Attention is All you Need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b39", "title": "Changing the representation: Examining language representation for neural sign language production", "journal": "", "year": "2022", "authors": "Harry Walsh; Ben Saunders; Richard Bowden"}, {"ref_id": "b40", "title": "Including signed languages in natural language processing", "journal": "Long Papers", "year": "2021", "authors": "Kayo Yin; Amit Moryossef; Julie Hochgesang; Yoav Goldberg; Malihe Alikhani"}, {"ref_id": "b41", "title": "Better sign language translation with STMC-transformer", "journal": "", "year": "2020", "authors": "Kayo Yin; Jesse Read"}, {"ref_id": "b42", "title": "Approaching sign language gloss translation as a low-resource machine translation task", "journal": "", "year": "2021", "authors": "Xuan Zhang; Kevin Duh"}, {"ref_id": "b43", "title": "Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? No response", "journal": "", "year": "", "authors": ""}, {"ref_id": "b44", "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run", "journal": "", "year": "", "authors": ""}, {"ref_id": "b45", "title": "for preprocessing, for normalization, or for evaluation", "journal": "", "year": "", "authors": ""}, {"ref_id": "b46", "title": "crowdworkers) or research with human participants? Left blank", "journal": "", "year": "", "authors": ""}, {"ref_id": "b47", "title": "Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators", "journal": "", "year": "", "authors": " D1"}, {"ref_id": "b48", "title": "crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic", "journal": "", "year": "", "authors": ""}, {"ref_id": "b49", "title": "Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?", "journal": "", "year": "", "authors": " D3"}, {"ref_id": "b50", "title": "Was the data collection protocol approved (or determined exempt) by an ethics review board? No response", "journal": "", "year": "", "authors": " D4"}, {"ref_id": "b51", "title": "Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response", "journal": "", "year": "", "authors": " D5"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Glosses (DSGS) KINDER FREUEN WARUM FERIEN N\u00c4HER-KOMMENTranslation (DE)Die Kinder freuen sich, weil die Ferien n\u00e4her r\u00fccken.Glosses (EN) ('CHILDREN REJOICE WHY HOLIDAYS APPROACHING')Translation (EN) ('The children are happy because the holidays are approaching.')", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "##English translation: Many young families like living in the city of Hamburg.7 German translation: Viele junge Familien leben gerne in Hamburg in der Stadt.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": ".18 57.1/16.7/10.0/6.2 (BP = 0.651 ratio = 0.700 hyp_len = 7 ref_len = 10) 24", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "you describe the limitations of your work? Section without number, after conclusion A2. Did you discuss any potential risks of your work? Not applicable. there are no pertinent risks in this particular paper A3. Do the abstract and introduction summarize the paper's main claims? Left blank. A4. Have you used AI writing assistants when working on this paper? Left blank. B Did you use or create scientific artifacts?", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "domains language pair #signs # hours #signers signing origin glosses?", "figure_data": "PHOENIX (Forster et al., 2014) (Camg\u00f6z et al., 2018)weatherDGS\u2194DE1066119LI\u2714Public DGS Corpus (Hanke et al., 2020)conversation, storytellingDGS\u2194DE8580  *50330OS\u2714BOBSL (Albanie et al., 2021)general broadcast programsBSL\u2194EN2281146739LI-FocusNews (M\u00fcller et al., 2022)general newsDSGS\u2194DE-1912OS-"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "). All but four papers do not contain enough information about how exactly BLEU was computed. Different BLEU implementations, settings (e.g. ngram orders, tokenization schemes) and versions are used.", "figure_data": "Reference HypothesisVIEL1A FAMILIE1* JUNG1 FAMILIE1 GERN1* IN1* HAMBURG1* STADT2* WOHNUNG2B* FAMILIE1 VIEL1B JUNG1 LEBEN1 GERN1* HAMBURG1* STADT2* $INDEX1BLEU with tokenization BLEU without tokenization 10.18 25.61"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Impact of applying or disabling internal tokenization (mtv13a) when computing BLEU on gloss outputs. Example taken from the Public DGS Corpus(Hanke et al., 2020).", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Examples for preprocessing of DGS glosses.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u2714 - \u2714 - - \u2714 \u2714 \u2714 \u2714 - Tensorflow Stoll et al. (2018) - \u2714 - - \u2714 - - \u2714 \u2714 \u2714 - (unclear) Camg\u00f6z et al. (2020b) \u2714 \u2714 - \u2714 - - \u2714 \u2714 \u2714 - WER (unclear) Camg\u00f6z et al. (2020a) \u2714 \u2714 - \u2714 - - - - \u2714 \u2714 - (unclear)", "formula_coordinates": [2.0, 79.84, 99.4, 433.37, 45.23]}, {"formula_id": "formula_1", "formula_text": ") \u2714 \u2714 ASLG-PC12 \u2714 - ASL\u2192EN \u2714 \u2714 \u2714 \u2714 METEOR NLTK Saunders et al. (2020) \u2714 \u2714 - - \u2714 - \u2714 \u2714 \u2714 \u2714 - (unclear) Stoll et al. (2020) \u2714 \u2714 - - \u2714 - - \u2714 \u2714 \u2714 WER (unclear)", "formula_coordinates": [2.0, 79.84, 150.1, 428.28, 32.55]}, {"formula_id": "formula_2", "formula_text": ") - \u2714 - \u2714 - - - \u2714 \u2714 \u2714 - (unclear) Moryossef et al. (2021) - \u2714 NCSLGR \u2714 - ASL\u2192EN (\u2714) - \u2714 - COMET SacreBLEU", "formula_coordinates": [2.0, 79.84, 188.12, 435.59, 19.88]}, {"formula_id": "formula_3", "formula_text": ") - \u2714 - \u2714 \u2714 - - - \u2714 - - (unclear) Egea G\u00f3mez et al. (2021) - \u2714 - - \u2714 - \u2714 - \u2714 \u2714 METEOR, TER SacreBLEU Saunders et al. (2022) - \u2714 DGS Corpus - \u2714 - - - \u2714 \u2714 - (unclear) Angelova et al. (2022) \u2714 \u2714 DGS Corpus \u2714 - - \u2714 - \u2714 - - SacreBLEU Walsh et al. (2022) - \u2714 DGS Corpus - \u2714 - - \u2714 \u2714 \u2714 - (unclear)", "formula_coordinates": [2.0, 79.84, 213.47, 435.59, 57.9]}, {"formula_id": "formula_4", "formula_text": "before $INDEX1 ENDE1\u02c6ANDERS1* SEHEN1 M\u00dcNCHEN1B* BEREICH1A* after $INDEX1 ENDE1 ANDERS1 SEHEN1 M\u00dcNCHEN1 BEREICH1 before ICH1 ETWAS-PLANEN-UND-UMSETZEN1 SELBST1A* KLAPPT1* $GEST-OFF\u02c6BIS-JETZT1 GEWOHNHEIT1* $GEST-OFF\u02c6* after ICH1 ETWAS-PLANEN-UND-UMSETZEN1 SELBST1 KLAPPT1 BIS-JETZT1 GEWOHNHEIT1", "formula_coordinates": [10.0, 95.39, 162.85, 404.71, 57.35]}], "doi": "10.18653/v1/2022.acl-srw.21"}