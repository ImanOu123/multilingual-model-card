{"title": "Can Graph Neural Networks Learn to Solve MaxSAT Problem?", "authors": "Minghao Liu; Fuqi Jia; Pei Huang; Fan Zhang; Yuchen Sun; Shaowei Cai; Feifei Ma; Jian Zhang; Inspir Ai", "pub_date": "2021-11-15", "abstract": "With the rapid development of deep learning techniques, various recent work has tried to apply graph neural networks (GNNs) to solve NP-hard problems such as Boolean Satisfiability (SAT), which shows the potential in bridging the gap between machine learning and symbolic reasoning. However, the quality of solutions predicted by GNNs has not been well investigated in the literature. In this paper, we study the capability of GNNs in learning to solve Maximum Satisfiability (MaxSAT) problem, both from theoretical and practical perspectives. We build two kinds of GNN models to learn the solution of MaxSAT instances from benchmarks, and show that GNNs have attractive potential to solve MaxSAT problem through experimental evaluation. We also present a theoretical explanation of the effect that GNNs can learn to solve MaxSAT problem to some extent for the first time, based on the algorithmic alignment theory.", "sections": [{"heading": "Introduction", "text": "The propositional logic has long been recognized as one of the corner stones of reasoning in philosophy and mathematics (Biere et al. 2009). The satisfiability problem of propositional logic formulas (SAT) has significant impact on many areas of computer science and artificial intelligence. SAT is the first problem proved to be NP-complete (Cook 1971), requiring the worst-case exponential time to be solved unless P=NP. The Maximum Satisfiability problem (MaxSAT) is a generalization of SAT, which aims to optimize the number of satisfied clauses, and it is also an NP-hard problem with a wide range of applications (Ans\u00f3tegui, Bonet, and Levy 2013). There have been tremendous efforts in solving SAT and MaxSAT efficiently, and numerous solvers have been developed in the last few decades such as (Cai and Zhang 2021). Nevertheless, these off-the-shelf solvers are mainly based on search algorithms with elaborate handcrafted strategies. Thanks to recent advances in deep representation learning, particularly for non-euclidean structure, there have been initial efforts to represent and solve combinatorial problems such as SAT through data-driven approaches. It relies on the fact that the distribution of problem instances in practice is generally domain-specific, which means that we can solely replace the training data to obtain efficient implicit heuristics instead of carefully modifying the strategies for certain domain.\nThe current application of deep learning to SAT problem can be grouped into two categories. One is to build an endto-end model that inputs a problem instance and directly predicts the satisfiability or solution, as demonstrated in Figure 1. A pioneering work is NeuroSAT , which shows that graph neural networks (GNNs) have capability to learn the satisfiability of SAT instances in specific domain (i.e., whether a group of assignments to the variables exists such that all the clauses are satisfied). (Cameron et al. 2020) further explores the performance of GNN models on predicting the satisfiability of random 3SAT problems which are still challenging for the state-of-the-art solvers. The other category combines neural networks with the traditional search frameworks, trying to improve the effects of some key heuristics. For example, GNNs are used to replace the variable selection function in the WalkSAT solver (Yolcu and P\u00f3czos 2019), produce the initialization assignment of local search (Zhang et al. 2020), or guide the search by predicting the variables in the unsat core (Selsam and Bj\u00f8rner 2019). These efforts indicate that GNN models are believed to have a certain degree of capability for learning from SAT problem instances, and also the potential to help improve SAT solving techniques in the future.\nNevertheless, the current research barely discusses the quality of solution predicted by the models, which should be an important indicator to check what GNNs learn from SAT problems. The end-to-end models mentioned above mainly focus on the prediction of satisfiability. For a satisfiable instance, the predicted solution given by models is infeasible most of the time, and we also have no idea how far it is from a feasible solution. In order to better understand the capability of GNN models, we target the subject as learning the solution of MaxSAT problem, an optimization version of SAT which aims at finding a solution that maximizes the number of satisfied clauses. The advantage is that as an optimization problem, we can analyze the quality of solution in more detail, such as the number of satisfied clauses and the approximation ratio, while a solution of SAT provides little information that it is valid or not. It is worth mentioning that although the 2SAT problem (i.e., each clause contains two literals) is in the complexity class P, the Max2SAT problem is still NP-hard (Garey, Johnson, and Stockmeyer 1976), so even generating near-optimal solution for Max2SAT is not trivial.\nMoreover, despite GNNs have shown their effectiveness on SAT problem through experiments, the theoretical analysis about why they can work is still absent so far. Recently, there have been some theoretical research about learning to solve combinatorial problems with GNNs. (Xu et al. 2019) proves that the discriminative power of many popular GNN variants, such as Graph Convolutional Networks (GCN) and GraphSAGE, is not sufficient for the graph isomorphism problem. (Xu et al. 2020) proposes the algorithmic alignment theory to explain what reasoning tasks GNNs can learn well, and shows GNNs can align with dynamic programming algorithms. So, many classical reasoning tasks could be solved relatively well with a GNN model, such as visual question answering and shortest paths. However, it also raises an issue: as a polynomial-time procedure, any GNN model cannot align with an exact algorithm for NP-hard problems unless P=NP, whereas some approximation algorithms may be used to analyze the capability of GNNs to solve such problems. (Sato, Yamada, and Kashima 2019) has employed a class of distributed local algorithms to prove that GNNs can learn to solve some combinatorial optimization problems, such as minimum dominating set and minimum vertex cover, with some approximation ratios. These theoretical contributions provide a basis for us to further study the capability of GNNs on the MaxSAT problem.\nIn this paper, we investigate the capability of GNNs in learning to solve MaxSAT problem both from theoretical and practical perspectives. Inspired from the algorithmic alignment theory, we design a distributed local algorithm for MaxSAT, which is guaranteed to have an approximation ratio of 1/2. As the algorithm aligns well with a singlelayer GNN, it can provide a theoretical explanation of the effect that a simple GNN can learn to solve MaxSAT problem to some extent. More importantly, for the first time, this work leverages GNNs to learn the solution of MaxSAT problem. Specifically, we build two typical GNN models, MS-NSFG and MS-ESFG, based on summarizing the common forms proposed for SAT, to investigate the capability of GNNs from the practical perspective. We have trained and tested both models on several random MaxSAT datasets in different settings. The experimental results demonstrate that both models have achieved pretty high accuracy on the testing sets, as well as the satisfactory generalization to larger and more difficult problems. This implies that GNNs are expected to be promising alternatives to improve the state-ofthe-art solvers. We are hopeful that the results in this paper can provide preliminary knowledge about the capability of GNNs to solve MaxSAT problem, and become a basis for more theoretical and practical research in the future.\nThe contributions can be summarized as follows:\n\u2022 We build two typical GNN models to solve MaxSAT problem, which is the first work to predict the solution of MaxSAT problem with GNNs in an end-to-end fashion.\n\u2026 \u2026\n= 1 = 0 = 1 \u2026 \u2026 Classifiers \u2026 CNF Formula Solution Figure 1:\nThe general framework of applying GNNs to solve SAT and MaxSAT problems. The CNF formula is firstly transformed to a bipartite factor graph, then each node is assigned an initial embedding. The embeddings are updated iteratively through the message passing process. Finally, a classifier decodes the assignment of each variable from its embedding.\n\u2022 We analyze the capability of GNNs to solve MaxSAT problem theoretically, and prove that even a single-layer GNN can achieve the approximation ratio of 1/2. \u2022 We evaluate the GNN models on several datasets of random MaxSAT instances with different distributions, and show that GNNs can achieve good performance and generalization on this task.", "publication_ref": ["b4", "b0", "b5", "b6", "b30", "b31", "b22", "b9", "b28", "b29", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "In this section, we firstly give the definition of SAT and MaxSAT problems, and then give an introduction to graph neural networks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SAT and MaxSAT", "text": "Given a set of Boolean variables {x 1 , x 2 , . . . , x n }, a literal is either a variable x i or its negation \u00acx i . A clause is a disjunction of literals, such as C 1 := x 1 \u2228 \u00acx 2 \u2228 x 3 . A propositional logic formula in conjunctive normal form (CNF) is a conjunction of clauses, such as\nF := C 1 \u2227 C 2 \u2227 C 3 .\nFor a CNF formula F , the Boolean Satisfiability (SAT) problem is to find a truth assignment for each variable, such that all the clauses are satisfied (i.e., at least one literal in each clause is True). And the Maximum Satisfiability (MaxSAT) problem is to find a truth assignment for each variable, such that the number of satisfied clauses is maximized. SAT and MaxSAT have long been proved to be NPhard, which means there is no polynomial-time algorithm to solve them unless P=NP. Specifically, if every clause in F contains exactly k literals, the problem is called 'kSAT' or 'MaxkSAT'.\nThere is a close relationship between MaxSAT and SAT. Firstly, MaxSAT can be seen as an optimization version of SAT, which can provide more information when handling over-constrained problems. Besides, many solving techniques that are effective in SAT have also been adapted\nto MaxSAT, such as lazy data structures and variable selection heuristics. Accordingly, SAT can also benefit from the progress of MaxSAT.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Graph Neural Networks", "text": "Graph neural networks (GNNs) are a family of neural network architectures that operate on graphs, which have shown great power on many tasks across domains, such as protein interface prediction, recommendation systems and traffic prediction (Zhou et al. 2020). For a graph G = V, E where V is a set of nodes and E \u2286 V \u00d7 V is a set of edges, GNN models accept G as input, and represent each node v as an embedding vector h (0) v . Most popular GNN models follow the message passing process that updates the embedding of a node by aggregating the information of its neighbors iteratively. The operation of the k-th iteration (layer) of GNNs can be formalized as\ns (k) v = AGG (k) ({h (k\u22121) u |u \u2208 N (v)}), h (k) v = UPD (k) (h (k\u22121) v , s (k) v ),(1)\nwhere h\n(k) v\nis the embedding vector of node v after the kth iteration. In the aggregating (messaging) step, a message s (k) v is generated for each node v by collecting the embeddings from its neighbors N (v). Next, in the updating (combining) step, the embedding of each node is updated combined with the message generated above. After T iterations, the final embedding h\n(T ) v\nfor each node v is obtained. If the task GNNs need to handle is at the graph level such as graph classification, an embedding vector of the entire graph should be generated from that of all the nodes:\nh G = READOUT({h (T ) u |u \u2208 V }). (2\n)\nThe final embeddings can be decoded into the outputs by a learnable function such as multi-layer perceptron (MLP), whether for node-level or graph-level tasks. The modern GNN variants have made different choices of aggregating, updating and readout functions, such as concatenation, summation, max-pooling and mean-pooling. A more comprehensive review of GNNs can be found in (Wu et al. 2021).", "publication_ref": ["b32"], "figure_ref": [], "table_ref": []}, {"heading": "GNN Models for MaxSAT", "text": "As described in Section 1, there have been several attempts that learn to solve SAT problem with GNNs in recent years. \n(x 1 \u2228 x 2 \u2228 x 3 ) \u2227 (x 1 \u2228 \u00acx 3 ) \u2227 (\u00acx 1 \u2228 \u00acx 2 \u2228 \u00acx 3 )\nwith 3 variables and 3 clauses.\nThe pipeline of such work generally consists of three parts. Firstly, a CNF formula is transformed to a graph through some rules. Next, a GNN variant is employed that maps the graph representation to the labels, e.g., the satisfiability of a problem or the assignment of a variable. Finally, the prediction results are further analyzed and utilized after the training converges. Intuitively, they can almost be transfered to work on MaxSAT problem without much modification, since these two problems have the same form of input. Factor graph is a common representation for CNF formulas, which is a bipartite structure to represent the relationship between literals and clauses. There are mainly two kinds of factor graphs that have appeared in the previous work. The first one is node-splitting factor graph (NSFG), which splits the two literals (x i , \u00acx i ) corresponding to a variable x i into two nodes. NSFG is used by many work such as ) and (Zhang et al. 2020). The other one is edgesplitting factor graph (ESFG), which establishes two types of edges, connected the clauses with positive and negative literals separately. (Yolcu and P\u00f3czos 2019) uses ESFG with a pair of biadjacency matrices. An example that represents a CNF formula with these two kinds of factor graphs is illustrated in Figure 2.\nThe models generally follow the message passing process of GNNs. Considering the bipartite structure of graph, in each layer the process is divided in two directions executed in sequence. For NSFG, the operation of the k-th layer of GNNs can be formalized as\nC (k) j = UPD C (C (k\u22121) j , AGG L (L (k\u22121) i |(i, j) \u2208 E)), (L (k) i , L (k) i ) = UPD L (L (k\u22121) i , L (k\u22121) i , AGG C (C (k\u22121) j |(i, j) \u2208 E)),(3)\nwhere\nL (k) i , C (k) j\nis the embedding of literal i and clause j in the k-th layer. The message is firstly collected from the literals in clause j, and the embedding of j is updated by that of the last layer and the message. Next, the embedding of literal i is updated in almost the same way, except that the embedding of literal i is updated with that of its negation together (denoted by L (k) i ). This operation maintains the consistency of positive and negative literals of the same variable.\nFor ESFG, there are two types of edges, which can be denoted as E + and E \u2212 . If a positive literal i 1 appears in clause j, we have (i 1 , j) \u2208 E + , and similarly have (i 2 , j) \u2208 E \u2212 if i 2 is a negative literal. The operation of the k-th layer of GNNs can be formalized as\nC (k) j = UPD C (C (k\u22121) j , AGG + L (L (k\u22121) i |(i, j) \u2208 E + ), AGG \u2212 L (L (k\u22121) i |(i, j) \u2208 E \u2212 )), L (k) i = UPD L (L (k\u22121) i , AGG + C (C (k\u22121) j |(i, j) \u2208 E + ), AGG \u2212 C (C (k\u22121) j |(i, j) \u2208 E \u2212 )).(4)\nFinally, for both NSFG and ESFG, a binary classifier is applied to map the embedding of each variable to its assignment. We use the binary cross entropy (BCE) as loss function, which can be written as\nBCE(y, p) = \u2212(y log(p) + (1 \u2212 y) log(1 \u2212 p)), (5\n)\nwhere p \u2208 [0, 1] is the predicted probability of a variable being assigned True, and y is the binary label from an optimal solution. By averaging the loss of each variable, we obtain the loss of a problem instance, which should be minimized. We will investigate the performance of these two models through experiments in Section 5.", "publication_ref": ["b31"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Theoretical Analysis", "text": "With the results and prospects revealed in practice, GNNs are considered to be a suitable choice in learning to solve SAT problem from benchmarks. However, the knowledge about why these models would work from a theoretical perspective is still limited so far. In this section, we are committed to explaining the possible mechanism of GNNs to solve MaxSAT problem. More specifically, we prove that a singlelayer GNN can achieve an approximation ratio of 1/2 with the help of a distributed local algorithm.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithmic Alignment", "text": "Our theoretical analysis framework is inspired by the algorithmic alignment theory proposed by (Xu et al. 2020), which provides a new point of view to understand the reasoning capability of neural networks. Formally, suppose a neural network N with n modules N i , if by replacing each N i with a function f i it can simulate, and f 1 , . . . , f n generate a reasoning function g, we say N aligns with g. As shown in that paper, even if different models such as MLPs, deep sets and GNNs have the same expressive power theoretically, GNNs tend to achieve better performance and generalization stably in the experiments. This can be explained by the fact that the computational structure of GNNs aligns well with the dynamic programming (DP) algorithms. Therefore, compared with other models, each component of GNNs only needs to approximate a simpler function, which means the algorithmic alignment can improve the sample complexity.", "publication_ref": ["b29"], "figure_ref": [], "table_ref": []}, {"heading": "Distributed Local Algorithm", "text": "According to the algorithmic alignment theory, it is possible to reasonably infer the capability that a GNN model can achieve with the help of an algorithm aligned with its computational structure. However, this also raises an issue: as a polynomial-time procedure, any GNN model cannot align with an exact algorithm for NP-hard problems such as MaxSAT, under the assumption that P =NP. In this case, we turn to employ a weaker approximation algorithm to analyze the capability of GNNs. Although there has been a lot of research on the approximation algorithms of MaxSAT (Vazirani 2001), it is regrettable that most of them cannot align with the structure of GNNs in an intuitive way.\nIn fact, there is a class of algorithms called distributed local algorithm (DLA) (Elkin 2004), which have been found to be a good choice to align with GNNs for combinatorial problems. DLA assumes a distributed computing system that there are a set of nodes in a graph, where any two nodes do not know the existence of each other at first. The algorithm then runs in a constant number of synchronous communication rounds. In each round, a node performs local computation, and sends one message to its neighboring nodes, while Algorithm 1: A distributed local algorithm for MaxSAT Input: The set of literals L, the set of clauses C Output: The assignments of literals \u03a6 1: Set up a factor graph such as NSFG for L and C.\n2: W (L i ) \u2190 0 for each L i \u2208 L. 3: S(C j ) \u2190 {} for each C j \u2208 C. 4: S(C j ) \u2190 S(C j ) \u222a {L i } for each edge (L i , C j ) in the factor graph. 5: for each C j \u2208 C do 6: L * \u2190 Pick a literal from S(C j ). 7: W (L * ) \u2190 W (L * ) + 1. 8: end for 9: for each L i \u2208 L do 10: if W (L i ) >= W ( L i ) then 11: \u03a6(L i ) \u2190 True.\n12: else 13: \u03a6(L i ) \u2190 False.", "publication_ref": ["b24", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "14:", "text": "end if 15: end for 16: return \u03a6 receiving one from each of them. Finally, each node computes the output in terms of the information it holds. DLAs have been widely used in many applications, such as designing sublinear-time algorithms (Parnas and Ron 2007) and controlling wireless sensor networks (Kubisch et al. 2003).", "publication_ref": ["b19", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Analyzing GNNs for MaxSAT", "text": "Most DLAs are designed for solving combinatorial problems in graph theory. (Sato, Yamada, and Kashima 2019) has employed DLAs to clarify the approximation ratios of GNNs for several NP-hard problems on graphs such as minimum dominating set and minimum vertex cover. This also encourages us to analyze the capability of GNNs to solve MaxSAT problem with the help of a DLA. As the DLA for MaxSAT has not been studied in the literature, we design an algorithm that aligns well with the message passing process described in Section 3, and the pseudo code is shown in Algorithm 1.\nThis algorithm accepts the description of a MaxSAT problem instance as input, and returns the assignment of literals, while the value of objective (i.e., the number of satisfied clauses) is easy to compute from it. Following the rules of DLA, the literals and clauses do not have any information about the problem initially. W and S correspond to the states of literals and clauses, respectively. In line 4, the message is passed from literals to clauses, so that each clause is aware of the literals it contains, and stores the information into S. Next, in lines 5-8, the message is generated by clauses and sent to literals. Finally, the assignment of each literal is decoded from W by a greedy-like method. It can be seen that the algorithm only carries out one round of communication. It is interesting to know whether a single-layer GNN may also have some capabilities to solve the MaxSAT problem theoretically. This can be analyzed by aligning it with the algorithm. Theorem 1. There exists a single-layer GNN to solve the MaxSAT problem, which is guaranteed to have an approximation ratio of 1/2.\nProof. We give a proof sketch here because of the space limitation, and the full proof is available in Appendix. Firstly, we show that there exists a single-layer GNN model that can align with the proposed distributed local algorithm (Algorithm 1), so that every part of the algorithm can be effectively approximated by a component of GNN. This implies that the GNN model can achieve the same performance with the algorithm when solving MaxSAT problem. Next, we prove that it is a 1/2-approximation algorithm. The picking operation of literal (line 6) is equivalent to transforming the original problem into another, where each clause only contains one literal. Given a solution, the number of satisfied clauses of the original problem is at least as large as that of the new one, so we get the approximation ratio by analyzing the new Max1SAT problem. It is not hard to find that the algorithm has a guarantee that at least half of the clauses are satisfied for any Max1SAT instance.\nThe proposed DLA can be considered as a candidate explanation for why a single-layer GNN model works and generalizes on the MaxSAT instances. The methodologies and results may also serve as a basis for future work to make theoretical progress on this task, such as explaining the improvement of capability as the number of GNN layers increases in the experiments and finding a tighter bound.", "publication_ref": ["b21"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Evaluation", "text": "Although the GNN models have shown their capability on many reasoning tasks such as SAT problem, the experimental evidence of whether GNNs can learn to solve MaxSAT problem is still under exploration. In order to demonstrate the capability of GNNs on this task, we firstly build two models, using NSFG and ESFG separately. After constructing the datasets with a commonly used generator for random MaxSAT instances, the GNN models are trained and tested in different settings, so that we can obtain a comprehensive understanding of their capabilities. The experimental results show that GNNs have attractive potential in learning to solve MaxSAT problem with good performance and generalization.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Building the Models", "text": "In Section 3, we have described the general GNN frameworks that can learn to solve MaxSAT problem. Based on the successful early attempts, we build two GNN models that accept a CNF formula as input, and output the assignment of literals. We abbreviate them to MS-NSFG and MS-ESFG, because they use NSFG and ESFG as the graph representation, respectively. The structures of models follow those in Eq. (3) and (4). Here we only describe the implementation of some key components, and the complete frameworks are shown in Appendix. Aggregating functions. The implementation of aggregating functions: AGG L , AGG C in MS-NSFG, and AGG + L , AGG \u2212 L , AGG + C , AGG \u2212 C in MS-ESFG, consists of two steps. First, an MLP module maps the embedding of each node to a message vector. After that, the messages from relevant nodes are summed up to get the final output. For example, the function AGG L in MS-NSFG which generates the message from literals and sends it to clause j, can be formalized as (i,j)\u2208E MLP(L i ), where L i is the embedding of literal i. Updating functions. The updating functions: UPD L and UPD C in both MS-NSFG and MS-ESFG can be implemented by the LSTM module (Hochreiter and Schmidhuber 1997). For example, to implement the function UPD C , the embedding of clause j (denoted as C j ) is taken as the hidden state, and the message generated from aggregation is taken as the input of LSTM.", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "Data Generation", "text": "For data-driven approaches, a large number of labeled instances are necessary. So, the dataset should be easy to solve by off-the-shelf MaxSAT solvers to ensure the size of dataset, meanwhile it should also have the ability to produce larger instances in the same distribution to test the generalization. As a result, we construct the datasets of random MaxSAT instances by running a generator proposed by (Mitchell, Selman, and Levesque 1992), which is also used to provide benchmarks for the random track of MaxSAT competitions 1 . The generator can produce CNF formulas with three parameters: the number of literals in each clause k, the number of variables n, and the number of clauses m.\nIn order to better observe the performance of GNN models, we generate multiple datasets with different distributions, which are listed in Table 1. For R2(60,600), R2(60,800) and R3(30,300), we generate 20K instances, and divide them into training set, validation set and testing set according to the ratio of 8:1:1. For R2(80,800) and R3(50,500), we only generate 2K instances as testing sets. Then we call MaxHS (Bacchus 2020), a state-of-the-art MaxSAT solver, to find an optimal solution for each instance as the labels of variables. The average time MaxHS spends to solve the instances in each dataset is also presented, so that we can know about their difficulty. ", "publication_ref": ["b18"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Implementation Details", "text": "We implement the models MS-NSFG and MS-ESFG in Python, and examine their practical performance to solve MaxSAT problem 2 . The models are trained by the Adam optimizer (Kingma and Ba 2015). All the experiments are run-ning on a machine with Intel Core i7-8700 CPU (3.20GHz) and NVIDIA Tesla V100 GPU.\nFor reproducibility, we also summarize the setting of hyper-parameters as follows. In our configuration, the dimension of embeddings and messages d = 128, and the learning rate is 2 \u00d7 10 \u22125 with a weight decay of 10 \u221210 . Unless otherwise specified, the number of GNN layers T = 20. The instances are fed into models in batches, with each batch containing 20K nodes.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "Accuracy of Models", "text": "We firstly evaluate the performance of both GNN models, including their convergence and the quality of predicted solution. The accuracy of prediction is reported with two values in the following paragraphs. The first one is the gap to optimal objective, which is the distance between the predicted objective and the corresponding optimal value. The predicted objective can be computed by counting the satisfied clauses given the predict solution. The other is the accuracy of assignments, which is the percentage of correctly classified variables, i.e., the assignment of a variable from the predicted solution is the same as its label. Generally, the gap to optimal objective could be a better indicator, because the optimal solution of a MaxSAT problem may not be unique.\nWe have trained MS-NSFG and MS-ESFG separately on three different datasets: R2(60,600), R2(60,800) and R3(30,300), and illustrate the evolution curves of accuracy throughout training on R2(60,600) in Figure 3 as an example. All the models can converge within 150 epochs, and achieve pretty good performance. The average gaps to optimal objectives are less than 2 clauses for all the models, with the approximation ratio >99.5%. Besides, the accuracy of assignments is around 92% (Max2SAT) and 83% (Max3SAT). There is no significant difference between the two models, while MS-ESFG performs slightly better than MS-NSFG. The experimental results show that both GNN models can be used in learning to solve MaxSAT problem.  ", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Influence of GNN Layers", "text": "According to the theoretical analysis, the number of GNN layers determines the information that each node can receive from its neighborhood, thereby affecting the accuracy of prediction. We examine this phenomenon from the experimental perspective by training MS-NSFG and MS-ESFG on the three datasets separately, with different hyper-parameter T from 1 to 30. The changes of accuracy on testing sets are illustrated in Figure 4. It can be seen that when T = 1, the capabilities of both models are weak, where the predicted objective is far from the optimal value, and the accuracy of assignments is not ideal. However, the effectiveness of both models has been significantly improved when T = 5. If we increase T to 20, the number of clauses satisfied by the predicted solution is only one less than the optimal value on average. We have continued to increase T to 60, but no obvious improvement occurred. This indicates that increasing the number of layers -within an appropriate range -will improve the capability of GNN models. ", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Generalizing to Other Distributions", "text": "Generalization is an important factor in evaluating the possibility to apply GNN models to solve those harder problems. We make the predictions by MS-NSFG and MS-ESFG on the testing sets with different distributions from the training sets, and the results are shown in Table 1 and 2, respectively. The first column is the dataset used for training the model, and the first row is the testing set. For each pair of training and testing datasets, we report the gap to optimal objective together with the approximation ratio in the brackets above, and the accuracy of assignments below. Here, the approximation ratio is defined as the ratio of predicted and optimal objectives.\nHere we mainly focus on three kinds of generalization. The first is generalizing to the datasets with different clause-variable proportion. From the results, both models trained on R2(60,600) and tested on R2(60,800) (or   vice versa) can maintain almost the same prediction accuracy. The second is generalizing to larger and more difficult problems. We use two testing sets, R2(80,800) and R3(50,500), where the number of variables is larger than those appeared in the training sets, as well as more difficult to solve by MaxHS. It can be found that both models trained on Max2SAT datasets can generalize to work on R2(80,800) with a satisfactory accuracy. This also holds when the training set is R3(30,300) and the testing set is R3(50,500), which implies that GNN models are expected to be promising alternatives to help solve those difficult MaxSAT problems. The last is generalizing to other datasets with different parameter k. For example, the model is trained on Max2SAT but tested on Max3SAT problems. The results show that both models have limitations under this condition, since they cannot achieve an accuracy of assignments >80% on every pair of training and testing sets, and the gap to optimal objective is not close enough, especially when trained on R3(30,300) and tested on Max2SAT datasets.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Related Work", "text": "Although the mainstream approaches for solving combinatorial problems, not limited to SAT or MaxSAT, are based on the search algorithms from symbolism, there have always been attempts trying to tackle these problems through datadriven techniques. A class of research is to integrate machine learning model in the traditional search framework, which has made progress on a number of problems such as mixed integer programming (MIP) (Khalil et al. 2016), satisfiability modulo theories (SMT) (Balunovic, Bielik, and Vechev 2018) and quantified boolean formulas (QBF) (Lederman et al. 2020). Here, we work on building end-to-end models which do not need the aid of search algorithm. The earliest research work can be traced back to the Hopfield network (Hopfield and Tank 1985) to solve TSP problem. Recently, many variants of neural networks have been proposed, which directly learn to solve combinatorial problems. (Vinyals, Fortunato, and Jaitly 2015) introduces Pointer Net, a sequential model that performs well on solving TSP and convex hull problems. (Khalil et al. 2017) uses the com-bination of reinforcement learning and graph embedding, and learns greedy-like strategies for minimum vertex cover, maximum cut and TSP problems. With the development of graph neural networks, there have been a series of work that uses GNN models to solve combinatorial problems. An important reason is that many of such problems are directly defined on graph, and in general the relation between variables and constraints can be naturally represented as a bipartite graph. Except for the mentioned NeuroSAT ) and its improvement, there have been some efforts learning to solve TSP (Prates et al. 2019), pseudo-Boolean (Liu et al. 2020) and graph coloring (Lemos et al. 2019) problems with GNNbased models. The results indicate that GNNs have become increasingly appealing alternatives in solving combinatorial problems. Moreover, there are also some well-organized literature reviews on this subject, such as (Bengio, Lodi, and Prouvost 2021), (Cappart et al. 2021) and.", "publication_ref": ["b12", "b2", "b16", "b11", "b25", "b13", "b20", "b17", "b3", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion and Future Work", "text": "Graph neural networks (GNNs) have been considered as a promising technique that can learn to solve combinatorial problems in the data-driven fashion, especially for the Boolean Satisfiability (SAT) problem. In this paper, we further study the quality of solution predicted by GNNs in learning to solve Maximum Satisfiability (MaxSAT) problem, both from theoretical and practical perspectives. Based on the graph construction methods in the previous work, we build two kinds of GNN models, MS-NSFG and MS-ESFG, which can predict the solution of MaxSAT problem. The models are trained and tested on randomly generated benchmarks with different distributions. The experimental results show that both models have achieved pretty high accuracy, and also satisfactory generalization to larger and more difficult instances. In addition, this paper is the first that attempts to present an explanation of the capability of GNNs to solve MaxSAT problem from a theoretical point of view. On the basis of algorithmic alignment theory, we prove that even a single-layer GNN model can solve the MaxSAT problem with an approximation ratio of 1/2.\nWe hope the results in this paper can inspire future work from multiple perspectives. A promising direction is to integrate the GNN models into a powerful search framework to handle more difficult problems in the specific domain, such as weighted and partial MaxSAT problems. It is also interesting to further analyze the theoretical capability of multilayer GNNs to achieve better approximation.\nLemma 2. Given the algorithm A, there exists a singlelayer graph neural network N , such that for any input \u03c6 = (L, C), A(\u03c6) = N (\u03c6) holds.\nLemma 3. The algorithm A has an approximation ratio of 1/2 for any input \u03c6 = (L, C).\nIf these lemmas hold, we have found a single-layer GNN N that achieves the same performance as A when solving MaxSAT problem, which is guaranteed to have a 1/2approximation. As a consequence, Theorem 1 holds.\nProof of Lemma 2. We use a single-layer GNN N to align with the algorithm A. Let L be a finite set of literals, and d = |L|. We assume that the embedding of each clause is a 0-1 vector of length d, which represents a set of literals, and the embedding of each literal L i is composed of two values: W (L i ) and W ( L i ). Then, we construct some key components of N to align with the operations in A.\nConsider the operation S(C j ) \u2190 S(C j ) \u222a {L i } (line 4). As S(C j ) and {L i } are sets of literals, this set union operation is equivalent to a logical AND function of two 0-1 vectors, which is apparently linearly separable. So there exists a learnable function f 1 : R d \u00d7 R d \u2192 R d to simulate the operation exactly.\nConsider the operation of picking a literal L * from S(C j ), and W (L * ) \u2190 W (L * ) + 1 (lines 6-7). According to the universal approximation theorem, there exists a learnable function f 2 : R \u00d7 R d \u2192 R to approximate this operation with arbitrarily small error . Next, we show the error will not break the exact simulation of the assignment operation (lines 10-14). When W (L i ) = W ( L i ), the condition W (L i ) \u2265 W ( L i ) still holds if < 0.5, since the elements in W are integers. Besides, when W (L i ) = W ( L i ), either L i or L i can be assigned True. So there exists an f 2 with < 0.5 to simulate the operations exactly. The alignment and simulation of other parts are straightforward. As shown above, each component of N can align with an operation in A. Therefore, for any input \u03c6 = (L, C), their outputs must be equal, i.e., A(\u03c6) = N (\u03c6).\nProof of Lemma 3. In the algorithm A, the picking operation of literal (line 6) transforms the original MaxSAT problem P into a new Max1SAT problem P , where W (L i ) counts the number of occurrences of literal L i in P . Given a solution of P , the number of satisfied clauses of P is at least as large as that of P . Then, we consider the assignment operation of literal (lines 9-15) in A. For a literal L i appearing in P , according to the condition W (L i ) \u2265 W ( L i ), the satisfied clauses are no less than the rejected ones if L i is assigned True. Therefore, a solution produced by A satisfies at least |C|/2 clauses, which implies an approximation ratio of 1/2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Frameworks of Models", "text": "The frameworks of MS-NSFG and MS-ESFG are shown below. The embeddings of literals and clauses in the k-th layer are denoted as L (k) and C (k) , respectively. d is the dimension of embeddings, and T is the number of GNN layers. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix Proof of Theorem 1", "text": "A CNF formula \u03c6 can be represented as a pair (L, C), where L is the set of literals, and C is the set of clauses. Let A be the proposed distributed local algorithm for MaxSAT problem. Fist of all, we present the following two lemmas:", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "SAT-based MaxSAT algorithms", "journal": "Artif. Intell", "year": "2013", "authors": "C Ans\u00f3tegui; M L Bonet; J Levy"}, {"ref_id": "b1", "title": "MaxHS in the 2020 MaxSat Evaluation. MaxSAT Evaluation", "journal": "", "year": "2019", "authors": "F Bacchus"}, {"ref_id": "b2", "title": "Learning to Solve SMT Formulas", "journal": "", "year": "2018-12-03", "authors": "M Balunovic; P Bielik; M T Vechev; S Bengio; H M Wallach; H Larochelle; K Grauman; N Cesa-Bianchi; R Garnett"}, {"ref_id": "b3", "title": "Machine learning for combinatorial optimization: A methodological tour d'horizon", "journal": "Eur. J. Oper. Res", "year": "2021", "authors": "Y Bengio; A Lodi; A Prouvost"}, {"ref_id": "b4", "title": "Handbook of Satisfiability", "journal": "Frontiers in Artificial Intelligence and Applications", "year": "2009", "authors": "A Biere; M Heule; H Van Maaren; T Walsh"}, {"ref_id": "b5", "title": "Deep Cooperation of CDCL and Local Search for SAT", "journal": "Springer", "year": "2021-07-05", "authors": "S Cai; X Zhang"}, {"ref_id": "b6", "title": "Predicting Propositional Satisfiability via End-to-End Learning", "journal": "AAAI Press", "year": "2020-02-07", "authors": "C Cameron; R Chen; J S Hartford; K Leyton-Brown"}, {"ref_id": "b7", "title": "Combinatorial Optimization and Reasoning with Graph Neural Networks", "journal": "ACM", "year": "1971-05-03", "authors": "Q Cappart; D Ch\u00e9telat; E B Khalil; A Lodi; C Morris; P Velickovic"}, {"ref_id": "b8", "title": "Distributed approximation: a survey", "journal": "SIGACT News", "year": "2004", "authors": "M Elkin"}, {"ref_id": "b9", "title": "Some Simplified NP-Complete Graph Problems", "journal": "Theor. Comput. Sci", "year": "1976", "authors": "M R Garey; D S Johnson; L J Stockmeyer"}, {"ref_id": "b10", "title": "Long Short-Term Memory", "journal": "Neural Comput", "year": "1997", "authors": "S Hochreiter; J Schmidhuber"}, {"ref_id": "b11", "title": "Neural\" computation of decisions in optimization problems", "journal": "Biological cybernetics", "year": "1985", "authors": "J J Hopfield; D W Tank"}, {"ref_id": "b12", "title": "Learning to Branch in Mixed Integer Programming", "journal": "AAAI Press", "year": "2016-02-12", "authors": "E B Khalil; P L Bodic; L Song; G L Nemhauser; B Dilkina"}, {"ref_id": "b13", "title": "Learning Combinatorial Optimization Algorithms over Graphs", "journal": "", "year": "2017-12-04", "authors": "E B Khalil; H Dai; Y Zhang; B Dilkina; L Song; I Guyon; U Von Luxburg; S Bengio; H M Wallach; R Fergus; S V N Vishwanathan; R Garnett"}, {"ref_id": "b14", "title": "Adam: A Method for Stochastic Optimization", "journal": "", "year": "2015-05-07", "authors": "D P Kingma; J Ba"}, {"ref_id": "b15", "title": "Distributed algorithms for transmission power control in wireless sensor networks", "journal": "", "year": "2003-03-20", "authors": "M Kubisch; H Karl; A Wolisz; L C Zhong; J M Rabaey;  Ieee; L C Lamb; A S Garcez; M Gori; M O R Prates; P H C Avelar; M Y Vardi"}, {"ref_id": "b16", "title": "Graph Colouring Meets Deep Learning: Effective Graph Neural Network Models for Combinatorial Problems", "journal": "IEEE", "year": "2019-11-04", "authors": "G Lederman; M N Rabe; S Seshia; E A. ; H Lee; M O R Prates; P H C Avelar; L C Lamb"}, {"ref_id": "b17", "title": "Learning the Satisfiability of Pseudo-Boolean Problem with Graph Neural Networks", "journal": "Springer", "year": "2020-09-07", "authors": "M Liu; F Zhang; P Huang; S Niu; F Ma; J Zhang"}, {"ref_id": "b18", "title": "Hard and Easy Distributions of SAT Problems", "journal": "AAAI Press / The MIT Press", "year": "1992-07-12", "authors": "D G Mitchell; B Selman; H J Levesque"}, {"ref_id": "b19", "title": "Approximating the minimum vertex cover in sublinear time and a connection to distributed algorithms", "journal": "Theor. Comput. Sci", "year": "2007", "authors": "M Parnas; Ron ; D "}, {"ref_id": "b20", "title": "Learning to Solve NP-Complete Problems: A Graph Neural Network for Decision TSP", "journal": "AAAI Press", "year": "2019-01-27", "authors": "M O R Prates; P H C Avelar; H Lemos; L C Lamb; M Y Vardi"}, {"ref_id": "b21", "title": "Approximation Ratios of Graph Neural Networks for Combinatorial Problems", "journal": "", "year": "2019-12-08", "authors": "R Sato; M Yamada; H ; H M Kashima; H Larochelle; A Beygelzimer; F ; D'alch\u00e9-Buc; E B Fox; R Garnett"}, {"ref_id": "b22", "title": "Guiding High-Performance SAT Solvers with Unsat-Core Predictions", "journal": "Springer", "year": "2019-07-09", "authors": "D Selsam; N Bj\u00f8rner"}, {"ref_id": "b23", "title": "Learning a SAT Solver from Single-Bit Supervision", "journal": "", "year": "2019-05-06", "authors": "D Selsam; M Lamm; B B\u00fcnz; P Liang; L De Moura; D L Dill"}, {"ref_id": "b24", "title": "Approximation algorithms", "journal": "Springer", "year": "2001", "authors": "V V Vazirani"}, {"ref_id": "b25", "title": "Pointer Networks", "journal": "", "year": "2015-12-07", "authors": "O Vinyals; M Fortunato; N Jaitly; C Cortes; N D Lawrence; D D Lee; M Sugiyama; R Garnett"}, {"ref_id": "b26", "title": "", "journal": "", "year": "", "authors": "Z Wu; S Pan; F Chen; G Long; C Zhang; P Yu"}, {"ref_id": "b27", "title": "", "journal": "A Comprehensive Survey on Graph Neural Networks. IEEE Trans. Neural Networks Learn. Syst", "year": "", "authors": ""}, {"ref_id": "b28", "title": "How Powerful are Graph Neural Networks", "journal": "", "year": "2019-05-06", "authors": "K Xu; W Hu; J Leskovec; S Jegelka"}, {"ref_id": "b29", "title": "What Can Neural Networks Reason About", "journal": "", "year": "2020-04-26", "authors": "K Xu; J Li; M Zhang; S S Du; K Kawarabayashi; S Jegelka"}, {"ref_id": "b30", "title": "Learning Local Search Heuristics for Boolean Satisfiability", "journal": "", "year": "2019-12-08", "authors": "E Yolcu; B ; H M P\u00f3czos; H Larochelle; A Beygelzimer; F ; D'alch\u00e9-Buc; E B Fox; R Garnett"}, {"ref_id": "b31", "title": "NLocalSAT: Boosting Local Search with Solution Prediction", "journal": "", "year": "2020", "authors": "W Zhang; Z Sun; Q Zhu; G Li; S Cai; Y Xiong; L Zhang"}, {"ref_id": "b32", "title": "Graph neural networks: A review of methods and applications. AI Open", "journal": "", "year": "2020", "authors": "J Zhou; G Cui; S Hu; Z Zhang; C Yang; Z Liu; L Wang; C Li; M Sun"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Two kinds of factor graphs to represent the CNF formula(x 1 \u2228 x 2 \u2228 x 3 ) \u2227 (x 1 \u2228 \u00acx 3 ) \u2227 (\u00acx 1 \u2228 \u00acx 2 \u2228 \u00acx 3 )with 3 variables and 3 clauses.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: The evolution of accuracy of MS-NSFG and MS-ESFG during a training process of 150 epochs on the dataset R2(60,600).", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: The accuracy of MS-NSFG and MS-ESFG trained with different number of GNN layers.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Algorithm 2 :2MS-NSFGInput: An NSFG G = V L , V C , E Output:The predicted assignments of literals \u03a6 1: Build the adjacency matrix M of graph G; 2: Initialize L (0) \u2208 R |V L |\u00d7d \u223c U (0, 1); 3: Initialize C (0) \u2208 R |V C |\u00d7d \u223c U (0, 1); 4: for k from 1 to T do 5:C (k) \u2190 UPD C (C (k\u22121) , M \u2022 AGG L (L (k\u22121) )); 6: L (k) \u2190 UPD L (L (k\u22121) , M T \u2022 AGG C (C (k\u22121) )); 7: end for 8: L pred \u2208 R |V L | \u2190 PRED L (L (T ) ); 9: \u03a6 \u2190 Round(Sigmoid(L pred )); 10: return \u03a6 Algorithm 3: MS-ESFG Input: An ESFG G = V L , V C , E + , E \u2212 Output:The predicted assignments of literals \u03a6 1: Build the adjacency matrices M + , M \u2212 of graph G; 2: Initialize L (0) \u2208 R |V L |\u00d7d \u223c U (0, 1); 3: Initialize C (0) \u2208 R |V C |\u00d7d \u223c U (0, 1); 4: for k from 1 to T do 5:C (k) \u2190 UPD C (C (k\u22121) , M + \u2022 AGG + L (L (k\u22121) ) + M \u2212 \u2022 AGG \u2212 L (L (k\u22121) )); 6: L (k) \u2190 UPD L (L (k\u22121) , (M + ) T \u2022 AGG + C (C (k\u22121) ) + (M \u2212 ) T \u2022 AGG \u2212 C (C (k\u22121))); 7: end for 8: L pred \u2208 R |V L | \u2190 PRED L (L (T ) ); 9: \u03a6 \u2190 Round(Sigmoid(L pred )); 10: return \u03a6", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "The parameters and difficulty of the datasets.", "figure_data": "Datasetk nmTime(s)R2(60,600) 2 60 6004.01R2(60,800) 2 60 80018.21R2(80,800) 2 80 800 105.19R3(30,300) 3 30 3001.92R3(50,500) 3 50 500 216.26"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "The accuracy of MS-NSFG on different combinations of training and testing sets.", "figure_data": "Train \\ TestR2(60,600) R2(60,800) R3(30,300) R2(80,800) R3(50,500)R2(60,600)0.54 (99.8%) 92.3%1.12 (99.8%) 92.2%5.98 (97.9%) 74.5%0.69 (99.9%) 92.2%9.92 (97.9%) 73.9%R2(60,800)0.48 (99.9%) 92.3%0.78 (99.8%) 92.8%6.13 (97.8%) 74.7%0.62 (99.9%) 92.2%10.24 (97.8%) 74.2%R3(30,300)14.83 (97.1%) 16.44 (97.5%) 78.8% 79.5%1.32 (99.5%) 83.5%20.22 (97.0%) 78.6%1.98 (99.5%) 82.5%"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "The accuracy of MS-ESFG on different combinations of training and testing sets.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "= 1 = 0 = 1 \u2026 \u2026 Classifiers \u2026 CNF Formula Solution Figure 1:", "formula_coordinates": [2.0, 319.5, 93.2, 223.2, 112.83]}, {"formula_id": "formula_1", "formula_text": "F := C 1 \u2227 C 2 \u2227 C 3 .", "formula_coordinates": [2.0, 452.03, 518.4, 83.24, 9.65]}, {"formula_id": "formula_2", "formula_text": "s (k) v = AGG (k) ({h (k\u22121) u |u \u2208 N (v)}), h (k) v = UPD (k) (h (k\u22121) v , s (k) v ),(1)", "formula_coordinates": [3.0, 96.34, 250.54, 196.16, 26.15]}, {"formula_id": "formula_3", "formula_text": "(k) v", "formula_coordinates": [3.0, 87.48, 284.52, 10.63, 12.46]}, {"formula_id": "formula_4", "formula_text": "(T ) v", "formula_coordinates": [3.0, 146.38, 355.47, 12.0, 12.46]}, {"formula_id": "formula_5", "formula_text": "h G = READOUT({h (T ) u |u \u2208 V }). (2", "formula_coordinates": [3.0, 100.89, 407.5, 187.74, 12.69]}, {"formula_id": "formula_6", "formula_text": ")", "formula_coordinates": [3.0, 288.63, 409.89, 3.87, 8.64]}, {"formula_id": "formula_7", "formula_text": "(x 1 \u2228 x 2 \u2228 x 3 ) \u2227 (x 1 \u2228 \u00acx 3 ) \u2227 (\u00acx 1 \u2228 \u00acx 2 \u2228 \u00acx 3 )", "formula_coordinates": [3.0, 87.95, 680.95, 204.55, 9.65]}, {"formula_id": "formula_8", "formula_text": "C (k) j = UPD C (C (k\u22121) j , AGG L (L (k\u22121) i |(i, j) \u2208 E)), (L (k) i , L (k) i ) = UPD L (L (k\u22121) i , L (k\u22121) i , AGG C (C (k\u22121) j |(i, j) \u2208 E)),(3)", "formula_coordinates": [3.0, 327.11, 365.62, 230.89, 50.27]}, {"formula_id": "formula_9", "formula_text": "L (k) i , C (k) j", "formula_coordinates": [3.0, 345.94, 421.51, 40.8, 14.07]}, {"formula_id": "formula_10", "formula_text": "C (k) j = UPD C (C (k\u22121) j , AGG + L (L (k\u22121) i |(i, j) \u2208 E + ), AGG \u2212 L (L (k\u22121) i |(i, j) \u2208 E \u2212 )), L (k) i = UPD L (L (k\u22121) i , AGG + C (C (k\u22121) j |(i, j) \u2208 E + ), AGG \u2212 C (C (k\u22121) j |(i, j) \u2208 E \u2212 )).(4)", "formula_coordinates": [3.0, 325.15, 572.41, 232.85, 69.79]}, {"formula_id": "formula_11", "formula_text": "BCE(y, p) = \u2212(y log(p) + (1 \u2212 y) log(1 \u2212 p)), (5", "formula_coordinates": [3.0, 334.39, 695.2, 219.74, 8.96]}, {"formula_id": "formula_12", "formula_text": ")", "formula_coordinates": [3.0, 554.13, 695.51, 3.87, 8.64]}, {"formula_id": "formula_13", "formula_text": "2: W (L i ) \u2190 0 for each L i \u2208 L. 3: S(C j ) \u2190 {} for each C j \u2208 C. 4: S(C j ) \u2190 S(C j ) \u222a {L i } for each edge (L i , C j ) in the factor graph. 5: for each C j \u2208 C do 6: L * \u2190 Pick a literal from S(C j ). 7: W (L * ) \u2190 W (L * ) + 1. 8: end for 9: for each L i \u2208 L do 10: if W (L i ) >= W ( L i ) then 11: \u03a6(L i ) \u2190 True.", "formula_coordinates": [4.0, 320.0, 105.0, 238.01, 120.49]}], "doi": ""}