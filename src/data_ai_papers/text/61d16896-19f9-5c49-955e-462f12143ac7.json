{"title": "Learning Sequential Acquisition Policies for Robot-Assisted Feeding", "authors": "Priya Sundaresan; Jiajun Wu; Dorsa Sadigh", "pub_date": "", "abstract": "A robot providing mealtime assistance must perform specialized maneuvers with various utensils in order to pick up and feed a range of food items. Beyond these dexterous low-level skills, an assistive robot must also plan these strategies in sequence over a long horizon to clear a plate and complete a meal. Previous methods in robot-assisted feeding introduce highly specialized primitives for food handling without a means to compose them together. Meanwhile, existing approaches to long-horizon manipulation lack the flexibility to embed highly specialized primitives into their frameworks. We propose Visual Action Planning OveR Sequences (VAPORS), a framework for long-horizon food acquisition. VAPORS learns a policy for high-level action selection by leveraging learned latent plate dynamics in simulation. To carry out sequential plans in the real world, VAPORS delegates action execution to visually parameterized primitives. We validate our approach on complex real-world acquisition trials involving noodle acquisition and bimanual scooping of jelly beans. Across 38 plates, VAPORS acquires much more efficiently than baselines, generalizes across realistic plate variations such as toppings and sauces, and qualitatively appeals to user feeding preferences in a survey conducted across 49 individuals. Code, datasets, videos, and supplementary materials can be found on our website.", "sections": [{"heading": "Introduction", "text": "Millions of people are impacted logistically, socially, and physically by the inability to eat independently due to upper mobility impairments or age and health-related changes [1,2,3]. Robotassisted feeding has the potential to greatly improve the quality of life for these individuals while reducing caregiver burden. However, realizing a performant system in practice remains challenging. For instance, humans eat spaghetti noodles as shown in Fig. 1 using nuanced fork-twirling motions. Dishes like ramen require even more diverse strategies like scooping soup or acquiring meat and noodles. Thus, not only must an autonomous feeding system employ various utensils and strategies to handle different foods and quantities, but it must also operate over long horizons to finish a meal. Figure 1: Visual Action Planning OveR Sequences (VAPORS) employs a high level policy \u03c0H to select amongst discrete manipulation strategies h, such as grouping and twirling, and a low-level vision-parameterized policy \u03c0L to execute these actions at for long-horizon dexterous food acquisition.\nPrior assistive feeding work has focused on learning individual low-level vision-parameterized primitives for food manipulation. Examples include separate policies for skewering [4,5,6], scooping [7], bite transfer [8,9,10], cutting [11,12,13], and pushing food piles [14]. While highly specialized, these policies cannot reason over an extended horizon or make use of multiple strategies for more effective plate clearance. Humans, on the other hand, interleave acquisition and rearrangement actions with ease-pushing multiple peas together before scooping instead of painstakingly acquiring each individual pea or gathering noodles closer to each other before twirling with a fork. Replicating this long-horizon foresight in robotic feeding has yet to be demonstrated. uncertain, and state estimation is notoriously challenging, rendering these approaches ineffective. An alternative approach is model-based planning and control, with recent impressive results on complex tasks like dough manipulation [16,32,33]. This family of methods leverage learned environment dynamics over visual states like images [34,35,36,37,33], keypoints [38], or particlebased representations [16,32] to sample and plan action sequences that maximize predicted rewards. However, these methods do not scale well to high-dimensional continuous action spaces such as that of food acquisition. To address this, hierarchical RL decouples policies into a high-level planner which selects amongst discrete but parameterized low-level primitives [39]. These works have demonstrated promising results on simulated long-horizon tabletop manipulation [18,19,15], but have yet to consider (1) real-world deployment beyond carefully controlled experimental setups, or (2) complex manipulation beyond commonplace primitives like pick-place, path-planning, and grasping. In contrast, we consider highly diverse plates requiring specialized primitives and tools.\nLearning and Control for Manipulation in the Real World. A large body of robotics research focuses on learning real-world policies for manipulation either through sim-to-real transfer or exclusively from real interactions. With sufficient domain randomization, sim-to-real transfer has proven effective for tasks involving rigid objects or a limited set of deformable items like cloth, which state-of-the-art simulators support [40,41,42]. However, adapting these simulators to modeling food appearance and deformation is highly non-trivial. Meanwhile, learning exclusively from real data has been shown to work well in challenging domains such as semantic grasping [43] or cable untangling [44,45,46]. These approaches rely on state representations that are scalable to learn, such as descriptors learned from self-supervised interaction [43] or keypoints learned from a small amount of manually annotated images [47,48,49,50]. In our setting, it is difficult to scale real-world data collection across the range of food shapes, appearances, and properties a robot may encounter. Self-supervised learning is also complicated due to resets and utensil interchange. We instead take a hybrid approach which takes advantage of simulation for modeling high-level plate dynamics from large-scale interactions, but leverages visual planning at the low level for precise real manipulation.", "publication_ref": ["b0", "b1", "b2", "b3", "b4", "b5", "b6", "b7", "b8", "b9", "b10", "b11", "b12", "b13", "b15", "b31", "b32", "b33", "b34", "b35", "b36", "b32", "b37", "b15", "b31", "b38", "b17", "b18", "b14", "b39", "b40", "b41", "b42", "b43", "b44", "b45", "b42", "b46", "b47", "b48", "b49"], "figure_ref": ["fig_8", "fig_8"], "table_ref": []}, {"heading": "Problem Statement", "text": "We formalize the long-horizon food acquisition setting by considering an agent interacting in a finite-horizon Partially Observable Markov Decision Process (POMDP). This is defined by the tuple (S, O, A, T , R, T, \u03c1 0 ). We assume access to plate image observations o t \u2208 R W \u00d7H\u00d7C + = O of unknown plate states S, with the initial state distribution given by \u03c1 0 . Here, W , H, and C denote the image dimensions. A denotes the action space, and T : S \u00d7 A \u2192 S represents the unknown transition function mapping states and actions to future states. The time horizon T denotes the discrete budget of actions to clear the plate and R(s, a) refers to the reward which measures progress towards plate clearance. Our goal is to learn a policy \u03c0(a t |o t ) that maximizes expected total return: E \u03c0,\u03c10,T [ t R(s t , a t )], with t \u2264 T .\nTo do so, we decouple \u03c0 into separate high and low-level sub-policies. We assume access to K discrete manipulation primitives h k , k \u2208 {1, . . . , K}, and learn a high-level policy \u03c0 H which selects amongst these primitives. Additionally, we learn a low-level policy \u03c0 L which continuously parameterizes a selected primitive according to visual input. The components we aim to learn are summarized below, where h k denotes a discrete primitive type and a t denotes its continuous low-level instantiation:\nHigh-level policy : \u03c0 H (h k |o \u2264t , a \u2264t\u22121 )\nLow-level policy : \u03c0 L (a t |o t , h k )\nWe consider low-level actions a t , parameterized by the position of the tip of a utensil (x, y, z) and utensil roll and pitch (\u03b3, \u03b2). Here, \u03b2 = 0 \u2022 corresponds to an untilted fork handle, for instance, and \u03b3 = 180 \u2022 corresponds to the fork tines being horizontal when viewed top-down (Fig. 2).", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "State-Action Representations", "text": "In this section, we outline the visual state and action representations which are at the core of our learning approach introduced in Section 4.\nVisual State Space. Our approach makes use of RGB-D images and segmented plate observations,\nI t \u2208 R W \u00d7H\u00d73 + , D t \u2208 R W \u00d7H , M t \u2208 R W \u00d7H +\nat different levels of abstraction. We leverage binary segmentation masks to capture the spread of food items on a plate, informing high-level planning with \u03c0 H , and RGB-D observations as input to \u03c0 L which better capture fine geometric details of food.\nAction Parameterization. We consider an agent that may either perform acquisition or rearrangement actions, parameterized below. Acquisition actions attempt to pick up a bite of food, and rearrangement actions consolidate items. For example, as a plate of noodles becomes more empty, the robot may need to employ a rearrangement action by pushing multiple strands together before twirling (acquiring) for a satisfactory bite size. In acquisition, a robot with a utensil-mounted endeffector approaches the position \n(x d , y d , z d )\n= (x d , y d , z d , \u03b3, \u03b2) (1).\nThe intent of rearrangement is to bring food items from the sparsest plate region to the densest by pushing from (x f , y f , z f ) ro (x d , y d , z d ), while maintaining contact with the plate throughout. As this is a planar push, we simply orient the tool orthogonal to the push direction, such that \u03b3 = arctan\ny f \u2212y d x f \u2212x d , and is untilted (\u03b2 = 0 \u2022 ): a t,rearrange = (x d , y d , z d , x f , y f , z f ) (2).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "VAPORS: Visual Action Planning OveR Sequences", "text": "Within the visual state and action space outlined in Section 3.1, we present our approach VAPORS for tackling long-horizon food acquisition. First, VAPORS learns a policy \u03c0 H , detailed in Section 4.1, to select amongst high-level strategies for long-horizon plate clearance via model-based planning. Finally, VAPORS learns a low-level policy \u03c0 L , which leverages visually-parameterized primitives to carry out generated sequential plans for real-world food acquisition detailed in Section 4.2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning High-Level Plans from Simulation", "text": "Our goal is to first learn a policy \u03c0 H for selecting amongst K discrete acquisition or rearrangement strategies without concern for the low-level action parameters. To do so, we learn a latent dynamics model of the plate from segmented image observations, and instantiate \u03c0 H (h k |M \u2264t , a \u2264t\u22121 ), k \u2208 {1, . . . , K} with model-based planning over this learned dynamics model. In this section, \u03c4 denotes the running counter of high-level primitives executed so far, and t denotes the current timestep. Simulator Overview. We train \u03c0 H entirely in simulation, where interactions can be collected at scale as opposed to the real world where manual plate resets and potential food waste are prohibitively expensive. As current simulators lack out-of-the-box support for many feeding scenarios, we develop a custom simulated food manipulation environment, visualized in Figure 3 in Blender 2.92 [40], further detailed in Appendix C.1. The simulator exposes RGB images I t , binary food segmentation masks M t , and food item positional states s t = {(x i , y i , z i )} i\u2208(1,...,N ) . Using this information, we design rewards for food acquisition in terms of ground truth plate state and collect transitions to train \u03c0 H .\nReward Design. With access to a simulated testbed for feeding, we train \u03c0 H to select amongst strategies via model-based reinforcement learning (RL). Our goal of efficient plate clearance can be specified with a reward that incentivizes either (1) successfully picking up food, or (2) reducing the spread of items on a plate. Optimizing for the first objective alone might lead to plate clearance, but at a slow pace of taking low-volume bites. The second objective encourages rearrangement when the plate is sparse to aid downstream acquisition. Concretely, we express this as a weighted reward with tunable weight \u03b1 \u2208 [0, 1]: r t = \u03b1(PICKUP GAIN) + (1 \u2212 \u03b1)(COVERAGE LOSS) (3). Here, PICKUP measures the quantity of food items picked up. COVERAGE measures the spread of items on the plate, illustrated in blue in Fig. 2). We provide the details for computing both in Appendix C.2. , and a reward model p(rt|zt). We use this model to select action sequences that maximize future rewards.\nLearning Latent Plate Dynamics.\nWith a means of measuring task progress via r t and access to plate observations M t , we propose a model-based agent that learns plate dynamics from segmented observations and uses the learned model to plan actions that maximize reward.\nWe achieve this by training a multi-headed latent dynamics model with the following (Fig. 4): (1) An encoder q(z t |M \u2264t , a \u2264t\u22121 ) compressing high-dimensional segmented images M t to compressed latent states z t , (2) A transition function over the latent states p(z \u03c4 |z \u03c4 \u22121 , h k \u03c4 \u22121 ) with which to imagine rollouts, and (3) A decoded reward model given by p(r t |z t ), such that at test time, we can sample action sequences and determine which maximize predicted rewards. We note that the transition function learns to predict high-level plate state changes between \u03c4 \u2212 1 and \u03c4 as a result of executing a primitive h k \u03c4 , rather than between individual timesteps t \u2212 1 and t due to a t .\nDuring training, we collect simulated transitions consisting of the masked image, high-level primitive, low-level action, and reward {(M t , h k \u03c4 , a t , r r )}. We train each head of this network using the objectives detailed in Appendix D.1.\nWe note that this approach is highly related to [37] with several crucial design choices. First, we learn plate dynamics over segmented image observations M t of food items on a plate, as opposed to raw RGB observations. This allows the dynamics model to attend to food items rather than the whole plate, provides an easily transferable representation between simulation and reality, and eases pressure for latent representations to capture irrelevant details in pixel space. Additionally, we learn a policy within an action space of discrete but continuously parameterized primitives as opposed to a high-dimensional space like joint-motor commands. This encourages actions that induce meaningful and perceptible plate changes likely to be encountered in downstream feeding.\nModel-Based Planning. Once trained, we leverage the learned encoder, transition model, and reward model towards instantiating \u03c0 H as an MPC-style planner with a receding T -step horizon. At timestep t, we enumerate all K T future candidate action sequences for the small library of primitives K. Conditioned on a history of observations M 1:t and actions a 1:t\u22121 , we imagine the future latent states z \u03c4 :\u03c4 +T +1 under each action sequence h k \u03c4 :\u03c4 +T via the transition function. Next, we predict decoded rewards according to the reward model p(r t |z t ) for each candidate sequence:\nR = \u03c4 +T +1 i=\u03c4 +1 E [p(r i |z i )]\n. Given the sequence of actions (\u0125 k \u03c4 ,\u0125 k \u03c4 +1 , . . . ,\u0125 k T ) which maximizes predicted cumulative reward R, we take \u03c0 H (M \u2264t , a \u2264t\u22121 ) =\u0125 k \u03c4 , the first primitive in the predicted sequence. After executing this action, we replan with \u03c0 H , terminating when \u03c4 = T . Details of the full planning pipeline, adapted from [37], are provided in Appendix D.2.", "publication_ref": ["b39", "b2", "b36", "b36"], "figure_ref": ["fig_1", "fig_0", "fig_2"], "table_ref": []}, {"heading": "Visual Policies for Low-Level Real Manipulation", "text": "Our learned simulated task dynamics model from Section 4.1 relies on segmented images M t as an observation space and parameterized primitives as an action space. In this section, we describe the visual state estimation pipelines we use to instantiate our state-action representations on real data. Food Segmentation. To define acquisition and rearrangement actions relative to the poses of food, we learn to segment food items on a plate as shown in Fig. 1. We learn a binary segmentation  , where for a real image I t \u2208 R + W \u00d7H\u00d73 , f seg (I t ) yields a binary segmentation maskM t which serves as input to \u03c0 H . To train f seg , we require a paired dataset of real plate images and ground truth segmentation masks. However, manually labeling pixel-level segmentation annotations on images is a painstaking and time-consuming process for real plates of food. Instead, we use a self-supervised annotation process which starts by taking an image of an empty plate, gradually adding food items to the plate, and using the absolute frame difference between the empty plate image and current observation to obtain the food segmentation mask. We implement f seg as a fully convolutional FPN (Feature Pyramid Network) and train it according to the procedure detailed in Appendix D.3.\nFood Orientation. Although segmentation provides a means to sense global positional information about food on the plate, we also care about precisely orienting a utensil with respect to the local geometry of a food item. For instance, using a fork to pick up a group of noodles requires orienting the fork tines opposite the grain of the strands. This is crucial to preventing slippage during twirling (Fig. 12), which tends to occur when the tines and strands run parallel. To address this, we also learn a network f ori : R + W \u2032 \u00d7H \u2032 \u00d73 \u2192 R mapping a local RGB crop of a food item of dimensions W \u2032 \u00d7 H \u2032 to the desired roll orientation of the utensil \u03b3. Prior work has shown that acquiring a food item orthogonal to its main principal axis, such as skewering a carrot against its length-wise axis rather than width-wise, can improve acquisition stability [4,6]. Thus, we implement f ori as a fully convolutional network with a ResNet backbone and train it from a small amount of real food item crops (200), manually annotated with keypoints defining the principal food item axis as in [4].\nAction Instantiation. With the visual state estimation pipelines f seg and f ori trained offline, we can instantiate \u03c0 L (a t |o t , h k ) for real-world manipulation. Given an RGBD image observation I t , D t , we first infer the segmentation maskM t = f seg (I t ). Next, we query \u03c0 H to obtain a selected primitiv\u00ea\nh k = \u03c0 H (M \u2264t ,\u00e2 (H) \u2264t\u22121 ).\nIf\u0125 k is an acquisition primitive, we instantiate the continuous action a t,acquis according to Eq. (1) by estimating the densest plate region (x d ,\u0177 d ,\u1e91 d ) and utensil orientation\u03b3. To do so, we apply a standard 2D Gaussian kernel overM t yieldingM \u2032 t . This blurs the image such that high-density regions in the original segmentation mask remain saturated but sparse regions have lower intensity. From this, we take the 2D argmax\u00fb\nd ,v d = arg max (u,v)\u2208M \u2032 tM \u2032 t [u, v\n] to be the densest pixel in the image, deprojected to a 3D location (x d ,\u0177 d ,\u1e91 d ) via D t and known camera intrinsics. Given a food item crop centered at the densest pixel, I \u2032 t (Fig. 2) we also infer the utensil orientation wit\u0125 \u03b3 = f ori (I \u2032 t ). For a rearrangement primitive, we parameterize a t,rearrange according to Eq. (2). In addition to sensing the densest plate region, we sense the furthest region (x f ,\u0177 f ,\u1e91 f ) by finding the lowest intensity pixel inM \u2032 t . This yields the following instantiations:\n\u03c0 L (o t , h k,acquis ) = (x d ,\u0177 d ,\u1e91 d ,\u03b3) \u03c0 L (o t , h k,rearrange ) = (x d ,\u0177 d ,\u1e91 d ,x f ,\u0177 f ,\u1e91 f )\nFinally, VAPORS operates in a perception-action loop using \u03c0 H to generate sequential plans and \u03c0 L to execute them. The full algorithm can be found in Algorithm 1 of the Appendix.", "publication_ref": ["b3", "b5", "b3"], "figure_ref": ["fig_8", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Experiments", "text": "We seek to evaluate VAPORS ability to clear plates, by effectively leveraging diverse strategies and planning over long horizons. Thus, we compare against a single-strategy baseline with no long-horizon reasoning and a multi-strategy approach that plans long-term actions heuristically rather than via learned plate dynamics. We consider two challenging real-world feeding scenarios to test the capabilities of VAPORS compared to other approaches: noodle acquisition and bimanual scooping.\nExperimental Setup: In noodle acquisition (Fig. 10), a Franka robot with a wrist-mounted custom motorized fork and RGBD camera must decide amongst twirling (acquisition) or grouping (rearrangement) to clear a plate of noodles. In bimanual scooping (Fig. 11), two Franka robots operating from overhead RGBD cameras must select amongst scooping (acquisition) or grouping (rearrangement) to clear a plate of jelly beans. For both tasks, we consider a half-full initial plate distribution ( 50 g. noodles, 15 jelly beans) and a hard count of \u03c4 = 10 actions for spaghetti and \u03c4 = 8 actions for jelly beans, encouraging the acquisition of multiple items at once to finish a plate. For both tasks, we assume access to hand-eye calibration between the RGB-D camera and robot end-effector. In Appendix E we outline the hardware setup and control stack, low-level action instantiations, and training details for each task.\nBaselines: Acquire-Only is identical to VAPORS in terms of \u03c0 L , but does not perform any longhorizon reasoning. Instead, at each timestep, this approach only acquires via twirling or scooping, with no rearrangement in between. Heuristic also utilizes \u03c0 L in the same manner, but replaces \u03c0 H with a na\u00efve group-then-acquire strategy. This method senses the COVERAGE, as defined in Eq. (3), overM t to heuristically determine when acquiring or rearrangement is appropriate. When the area exceeds a pre-defined threshold, the policy defaults to rearrangement and otherwise acquires. Plate Clearance Results: We evaluate VAPORS, Acquire-Only, and Heuristic on clearing plates across 10 trials for each task (Fig. 5). We see that VAPORS achieves the most efficient and highest cumulative plate clearance. As expected, Acquire-Only optimizes only for acquisition in the current instant, without exploiting the benefits of grouping for a more substantial pickup of multiple items at once. Scooping one jelly bean at a time or attempting to twirl just a few strands of noodles repeatedly leads to the observed slow rate of overall clearance. Heuristic's greedy group-then-acquire approach plans based on detected coverage thresholds, which we find is brittle in practice especially for any artifacts in segmentation mask predictions. This naive metric also does not encourage acquiring any bite-sized piles that may form intermittently, but rather aims to amass everything into one large pile before acquiring. This delays acquisition gains and wastes the action budget.\nUser Evaluation: Additionally, we conducted a user study with 49 non-disabled participants (age range 27.0 \u00b1 9.5, 46.9% female and 53.1% male) to gauge user preferences across methods. Of this pool, 77.6% reported prior experience interacting with robots before, 75.5% reported having fed someone before, and 28.6% reported having been fed as an adult. We hypothesized: H1. Compared to baselines, VAPORS use of multiple strategies and long horizon foresight will lead to more preferable feeding in terms of quantitative and qualitative metrics.\nWe used a within-subjects design where we presented each participant with videos of all 10 plate clearance trials per each of the three methods, for either noodle acquisition or bimanual scooping. For each participant, we randomized the method order, the order of trials per method, and the food group. In the study, we ask participants to rate efficiency, bite size, similarity to human feeding, practicality, likelihood for reuse, safety, and generalization. After watching all trials, we provided users with a 7-point Likert survey to assess these criteria (Fig. 6).\nVAPORS incurs the highest qualitative user ratings across criteria, compared to the Acquire-Only and Heuristic baselines, and with statistical significance for certain categories (p < 0.05, denoted '*'). Users noted that VAPORS \"mimicked natural feeding,\" and \"showed a capacity for clustering as the plate got more and more empty, which felt like a great and efficient approach,\" while Heuristic and Acquire-Only \"seem like extreme policies, where [Acquire-Only] never tries to cluster and [Heuristic] focus too much on making big piles.\" These results align with the hypothesis that VAPORS' use of multiple strategies and ability to reason over long horizons benefits a user's mealtime experience. We provide additional user study findings in Appendix E.\nGeneralization Testing: Finally, we stress-test VAPORS' generalization capabilities by experimenting with noodle dishes prepared with sauces and garnishes as well as ordered from DoorDash (Fig. 7). We conduct 18 additional trials of plate clearance on unseen plates, separated into three tiers of difficulty with 6 trials per tier. We summarize our findings in Table 1.  VAPORS achieves near full plate clearance for Tier 1 noodles, demonstrating generalization to noodle shapes and sizes (Table 1).", "publication_ref": [], "figure_ref": ["fig_8", "fig_8", "fig_4", "fig_5", "fig_6"], "table_ref": ["tab_2", "tab_2"]}, {"heading": "Failure Categorization", "text": "While VAPORS is still able to make decent progress towards plate clearance in Tier 2, we observe the occurrence of more slip failures (D) and misplanned actions (A, B) due to the addition of sauce and distractor food items. Somewhat surprisingly, the performance gap between Tier 2 and Tier 3 is minimal, with VAPORS being able to clear well over half the noodles for a fully out-of-distribution plate. The main challenges include misperceiving cabbage for noodles in the chow mein, as well as dropping twirled noodles heavily coated in pesto or soy sauce (D) (Fig. 12). Regardless, VAPORS demonstrates promising signs of zero-shot generalization.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Discussion", "text": "We present VAPORS, which to our knowledge is the first framework to address the multi-step food acquisition problem in robot-assisted feeding. Our hybrid approach leverages simulation to learn to model high-level plate dynamics at scale, and uses visual pose estimation in order to perform dexterous maneuvers for complex low-level food pickup. We experimentally validate VAPORS on a complex suite of real-world food acquisition tasks such as noodle acquisition and bimanual scooping of beans. VAPORS demonstrates the ability to clear plates efficiently over non-learned baselines while appealing to the feeding preferences of real users.\nLimitations and Future Work. The largest current limitation is a lack of user testing on individuals with mobility impairments that affect their ability to eat independently, discussed in detail in Appendix A. Additionally, although this work highlights promising initial results toward generalization across food variations such as shape, sauces, and toppings, we acknowledge that our library of low-level primitives is currently limited. One actionable future direction is expanding our library with prior work on skewering, cutting, and even toppling unstable items to tackle a more expansive set of plates. Our initial prototypes for dexterous food acquisition, such as the motorized fork, also open up interesting possibilities for future designs of dexterous interchangeable utensils which would enable rapid strategy switching. Currently, the system also executes primitives in an open-loop fashion, but we hope to use reactive control in the future to adapt online to slippage or imprecision.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning Sequential Acquisition Policies for Robot-Assisted Feeding", "text": "Please refer to our website for videos, code, and supplementary material, as well as the 'Additional Experiments' page for supplementary ablations and a comparison to additional baselines. We first include a discussion around VAPORS' greatest current limitations in the larger context of robotassisted feeding (Section A). In the subsequent sections, we also provide an overview of the main design choices behind VAPORS and a thorough overview of implementation and experimental details.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Limitations and Risks", "text": "In this work, we evaluate VAPORS quantitatively in plate clearance experiments and qualitatively in user studies. However, the largest current limitation is a lack of comprehensive user testing on individuals with mobility impairments that affect their ability to eat independently. These individuals are not represented in the demographic of individuals surveyed as per Section 5, and we are actively working on expanding our pool of participants to include such users. We fully acknowledge that performing user studies with non-disabled participants is neither representative of the experience of individuals with difficulties eating, nor in any way a replacement for the feedback and insights these individuals could offer towards improving assistive feeding research. We also note that plate clearance success is not on its own an indicative metric of how useful or effective a system like VAPORS would be for assisting real users with eating difficulties. Extending VAPORS as part of a user-facing system for feeding would require several changes and considerations not explored in this work, such as taking into account different feeding preferences, mobility levels, involuntary head or body movements, comfort levels, and preferences surrounding shared versus full autonomy across different users. A full-stack feeding system would also need to carefully consider how to do in-mouth bite transfer safely, comfortably, and reliably. None of these considerations is exhaustive in any way; we merely see VAPORS as a step towards addressing challenges surrounding dexterity and long-horizon reasoning within bite acquisition, and we only speculate this having an impact for assistive feeding down the line. Towards this goal, we provide additional results below containing initial feedback from users with mobility impairments. Users with and without mobility impairments favor VAPORS in terms of all criteria, but especially efficiency, bite size, and generalizability, suggesting the promise of our approach. For more user-facing considerations, such as practicality and whether users would reuse VAPORS given a choice, we find a small drop in average ratings between users without and with mobility impairments. In Section A, we discuss additional considerations to improve VAPORS in these aspects. We note that the larger error bars and lack of statistical significant findings for users with mobility impairments is due to the small sample size of users tested (3) as compared to 49 users without mobility impairments. We are actively working to increase this pool for larger-scale user testing in the future.\nStanford University. In a survey sent out to users, we received three responses from participants (2 female, 1 male with ages in the range of [31][32][33][34][35][36][37][38][39][40][41][42][43][44]. All participants reported difficulties and/or mobility impairments that affect their ability to eat independently, including a C5 quadriplegic injury with no finger or wrist function, and other impairments causing limited use of arms and hands.\nWe ask all users to evaluate the spaghetti acquisition trials from Section 5. In Figure 8, we visualize the average Likert ratings for users without versus with mobility impairments. We find that users with eating difficulties still rate VAPORS high in terms of efficiency, bite-size, and generalizability, and comparatively much higher than baselines. We do see that these users ratings for other considerations of a feeding system, such as practicality and reuse, are lower on average than users without mobility impairments, but VAPORS is still favored. Users reported that the twirling primitives occasionally struggled when there were very few strands of noodles left on the plate, or noodles were located at plate edges, which could lead to messy bites in practice.\nThus, while promising in terms of its algorithmic effectiveness, we fully acknowledge that VAPORS requires further refinement and ample testing to be a truly effective assistive feeding system, discussed in Appendix Section A. We further acknowledge that the sample size of users with mobility impairments is very small, and in the future, we hope to expand to a much larger pool of users to fully understand VAPORS' capabilities.", "publication_ref": ["b30", "b31", "b32", "b33", "b34", "b35", "b36", "b37", "b38", "b39", "b40", "b41", "b42", "b43"], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "C Simulator Details", "text": "C.1 Simulator Design Figure 9: Blender Food Simulation Environment: We implement a custom food manipulation simulator in Blender 2.92 with an Open AI gym-style environment. The simulator supports softbody objects, such as noodles in different shape variations, as well as rigid, granular piles of items. We implement cutlery with arbitrary utensil meshes such as forks and spoons, and implement actions using the keyframing feature of Blender to control the position and orientation of a tool across frames.\nWe use Blender 2.92, a physics and rendering engine, to develop a custom feeding environment supporting deformable items, rigid items, and cutlery interactions. To instantiate deformable items like noodles (Fig. 9), we represent each item as a group of particles simulated with soft body physics. We treat granular piles of food such as jelly beans as separate rigid bodies. Additionally, we provide support for mesh-based utensils including a fork, spoon, and pusher tool, where we programatically keyframe the position and orientation of the tool across simulation frames to implement actions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.2 Reward Design", "text": "In this section, we describe the implementation of the reward function given in Eq. (3). For a set of known food item states in simulation s t = {(x i , y i , z i )} i\u2208(1,...,N ) , PICKUP measures the quantity of food items picked up out of N total items. We detect a picked up food item in simulation by thresholding the z position of all items before and after an action, relative to plate height. Analogous to task progress metrics in cloth smoothing work [51,52], we use COVERAGE to measure of spread of items on the plate. We compute this via the area of the convex hull of {(x i , y i )} i\u2208(1,...,N ) , depicted in Fig. 2, via the Scipy Python library.", "publication_ref": ["b50", "b51"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "D Details of Learning-Based Methods", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.1 Latent Dynamics Training Details", "text": "We implement the latent plate dynamics model using the recurrent state space model from [37], with 64 \u00d7 64 input images and 30-dimensional diagonal Gaussian latent variables. This is a multi-headed deep recurrent network comprised of a learned encoder, transition model, and reward model. We supervise each head of the network with the following objectives:\n\u2022 For the encoder q(z t |M \u2264t , a \u2264t\u22121 ), we use an auxiliary decoder head that upsamples latent variables z t to predicted imagesM t and take the mean-squared error between (M t , M t ) as a standard reconstruction objective. This encourages the learned latent representations to preserve the notion of food spread captured in segmented image observations. \u2022 We supervise the transition function p(z \u03c4 |z \u03c4 \u22121 , h k \u03c4 \u22121 ) head using the KL-divergence for multi-step predictions as defined in [37].\n\u2022 Finally, for the reward model given by p(r t |z t ), we take the mean-squared error between predicted rewards and ground truth rewards (r t , r t ). This objective promotes accurately decoding rewards of future states to inform planning at test-time.", "publication_ref": ["b36", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "D.2 Planning with Learned Dynamics Model", "text": "Once trained, we use an MPC-style loop to sample and plan actions that maximize predicted rewards under the learned reward model.\nAt time \u03c4 , we can enumerate all K T future candidate action sequences for the small library of primitives K, where T is the planning horizon. Conditioned on a history of observations M 1:t and actions a 1:t\u22121 , we imagine the future latent states z \u03c4 :\u03c4 +T +1 under each action sequence h k \u03c4 :\u03c4 +T :\nz t:t+T +1 \u223c q(z \u03c4 |M 1:t , a 1:t\u22121 ) \u03c4 +T +1 i=\u03c4 +1 p(z i |z i\u22121 , h k i\u22121 ),(4)\nwhere q(z t |M \u2264t , a <t\u22121 ) is the learned encoder and p(z\n\u03c4 |z \u03c4 \u22121 , h k \u03c4 \u22121 )\nis the learned transition model. Next, we predict decoded rewards according to the reward model p(r t |z t ) for each candidate sequence:\nR = i+T +1 i=\u03c4 +1 E [p(r i |z i )] .(5)\nNext, we select the sequence of actions (\u0125 k \u03c4 ,\u0125 k \u03c4 +1 , . . . ,\u0125 k T ) which maximizes predicted cumulative reward R. The final step of the MPC planning loop is we take \u03c0 H (M \u2264t , a \u2264t\u22121 ) =\u0125 k \u03c4 , which is simply the first primitive in the predicted sequence. After executing this action, we replan with \u03c0 H , thus obtaining a second action and so on until \u03c4 = T (Algorithm 1). ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.3 Food Segmentation Training Details", "text": "Self-Supervised Dataset Generation. To circumvent the painstaking process of pixel-level segmentation annotation for real food images, we design a self-supervised annotation procedure. First, we record a grayscale RGB image of an empty plate, I empty \u2208 R W \u00d7H + . Next, we manually place food items on the plate at random without changing the position of the plate, yielding a new grayscale observation I t . Let I diff = |I t \u2212 I empty |, the framewise absolute difference between the full and empty plate. We initialize the ground truth segmentation mask M t corresponding to I t as a 2D array of zeros, and then assign M t [I diff > THRESH] = 1. In practice, we find that THRESH = 20 reasonably separates the foreground from the background to detect food. With this procedure, we can scalably collect 280 paired RGB food images and segmentation masks in real within an hour and a half of data collection. This includes plate resets, food placement, image capture, and offline background subtraction post-processing.\nAugmentation. We augment this dataset 8X by randomizing the linear contrast, gamma contrast, Gaussian blur amount, saturation, additive Gaussian noise, translation, and rotation of each RGB image, applying only the affine component of these same transformations to the associated segmentation masks.\nTraining Objective. We train f seg , implemented as a fully convolutional FPN (Feature Pyramid Network) using Dice loss:\nL dice = 1 \u2212 2 \u00d7 TP 2 \u00d7 TP + FN + FP (6)\nThis objective encourages high overlap between predicted and ground truth masks, as TP, FN, FP denote the number of pixel-level true positives, false negatives, and false positives in a predictionM t compared to ground truth M t .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E Experimental Details E.1 Noodle Acquisition Hardware Setup", "text": "Using a Franka Panda 7DoF robot, we aim to clear a plate of cooked noodles within a horizon of T = 10 actions. We fit the end-effector with a custom 3D-printed mount consisting of a RealSense D435 camera and a fork. To enable autonomous twirling and scooping capabilities, we extend the fork's range of motion via two servo motors (Dynamixel XC330-M288-T). We control the robot with a Cartesian impedance controller, where the programmable servos are integrated in the forward kinematics chain for positional control of the fork tip. The action space consists of either group (rearrangement) or twirl (acquisition) actions, instantiated according to the learned segmentation and pose estimation models detailed in Section 4.2.\nA group action consolidates a sparsely distributed plate by sensing the furthest and densest points, (x f ,\u0177 f ,\u1e91 f ) and (x d ,\u0177 d ,\u1e91 d ), and executing a planar push from the furthest to densest point. In a twirl action, we infer the densest point and appropriate insertion angle\u03b3, roughly orthogonal to the grain of majority of the noodles. We use positional control to insert the fork into the densest noodle pile, and execute a fixed twirling motion by making two rotations at 6 radians per second. Finally, the fork scoops upward until nearly horizontal (\u03b2 = 80 \u2022 ) and the robot brings the acquired noodles to a neutral position in the workspace.\nFor all trials, we use a non-slip plastic dinner plate, and mimick a bite successfully taken by a user after twirling by autonomously untwirling onto a discard plate.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.2 Bimanual Scooping Hardware Setup", "text": "We assume access to two Franka Panda robots, equipped with a pusher tool and a metal spoon, respectively, and an external RealSense D435 camera for perception. With this setup, we aim to acquire granular items on a plate using either group (rearrangement) or scoop (acquisition) actions, with a total action budget of T = 8 actions. In particular, we evaluate our system on the task of scooping jelly beans, but VAPORS is agnostic to the exact choice of food. Following the experimental setup of Grannen et al. [7], the spoon is mounted at an angle to the robot end-effector (\u03b2 = 30 \u2022 ). The pusher is a concave 3D-printed tool intended to push piles of items into the spoon and maintain contact during lifting so as to prevent spillage.\nGrouping actions are unimanual and use the pusher tool to push the sensed furthest item to the densest region on the tray. In a scoop action, we sense the densest pile and execute a parameterized motion in which the pusher and spoon move towards each other synchronously at a fixed \u03b3 = 180 \u2022 . Once they arms are within a fixed threshold apart, the spoon scoops by tilting to \u03b2 = 80 \u2022 and lifting to a neutral workspace position.\nWe conduct all trials on a standard cooking tray due to the enlarged manipulation workspace for two arms. To simulate a user's bite between actions, we manually discard the spoon contents after a scoop action.  ", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "E.3 Implementation Details", "text": "For each task, we use the following training procedures. We train \u03c0 H on simulated segmentation observations of size 64 \u00d7 64 for 2, 250 update steps, where we collect 1 episode every 150 update steps. We instantiate the reward as per Eq. (3) with \u03b1 = 0.66, and train each model using the Adam optimizer with with a learning rate of 10 \u22123 , \u03f5 = 10 \u22124 , and gradient clipping norm of 1000 with batch size B = 32, based on the training procedure from [37]. Each model takes approximately 1 hour to train on an Nvidia RTX A4000 GPU. To instantiate \u03c0 L , we train f seg and f ori from real data. For segmentation, we collect 280 paired examples of images and binary segmentation masks using the self-supervised annotation process from Section 4.2, where we use cooked noodles of randomized shape and sauce variations as well as jelly beans of randomized colors. We augment each dataset 10X and train for 50 epochs, which takes approximately 3 hours on an NVIDIA GeForce RTX 2080 GPU.\nIn order to instantiate the twirl primitive for noodle acquisition, we additionally train f ori to predict fork tine orientation \u03b3 from 280 manually annotated crops of noodles as per Section 4.2, augmented 8X. The train time for f ori is approximately 1 hour on an NVIDIA GeForce RTX 2080 GPU. For deployment, we use an Intel NUC 7 for inference and robot control via a ROS 2-based control stack.", "publication_ref": ["b36"], "figure_ref": [], "table_ref": []}, {"heading": "E.4 Additional Experimental Results", "text": "In this section, we supplement the experimental findings from Section 5 with additional results. Occasionally, \u03c0H may acquire when rearrangement is more appropriate, leading to a low-volume bite (B). In terms of action execution, food acquisition requires care so as to not miss food (C), as seen in the grouping motion which fails to group singular noodle strands due to system imprecision. Finally, slippage (D) can happen during acquisition with hard-to-model items such as those coated in sauce.\nFigure 13: Overall Ratings Left: After observing all methods perform acquisition across 10 trials, we ask users to rank all three methods from most to least preferable. We find the VAPORS is most consistently ranked the best by a statistically significant margin (p < 0.05, denoted '*') compared to the baselines. Right: For jelly bean acquisition, we control for the initial state of the plate by arranging the beans in a 4 \u00d7 4 grid, and ask users to select their preferred method across 6 side by side acquisition videos of different methods. VAPORS is the preferred method by a large margin compared to Heuristic and Acquire-Only.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Acknowledgments", "text": "This work is in part supported by funds from NSF Awards 2132847, 2006388, 2218760, as well as Stanford HAI, the Office of Naval Research, AFOSR YIP FA9550-23-1-0127, and Ford. We thank Lorenzo Shaikewitz for designing the motorized fork used in this work which made real-world experimentation possible. We also thank Rajat Kumar Jenamani, Suneel Belkhale, Jennifer Grannen, Yuchen Cui, and Yilin Wu for their helpful feedback and suggestions. Priya Sundaresan is supported by an NSF GRFP.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Plate Clearance: Fig. 10 and Fig. 11 visualize two rollouts of VAPORS on plate clearance. We note that visually, VAPORS tends to favor grouping as the plates become sparser and otherwise acquires when there is a reasonably sized bite available.\nVAPORS Failure Mode Categorization: In addition to evaluating the percentage of the plate cleared, we observe the occurrence of a few failure modes, as depicted in Fig. 12. A misplanned action (A) can occur due to a perception error, such as accidentally perceiving sauce, a garnish, a vegetable, or plate specularity for noodles and erroneously grouping or twirling in that region. Alternatively, this can happen when (B) the robot twirls when grouping is more appropriate or vice versa. A mis-executed action failure occurs when (C) the fork fails to group or acquire due to system imprecision or (D) the noodles slip during acquisition due to sauce. In Table 1, we also report the per-action failure rate, computed as the total number of failures over the total number of actions (60 = 6 trials \u00d7 T = 10).\nQualitative User Study: In the Likert survey administered to gauge user preferences across methods, we report in Fig. 6 the statistical findings which are significant. In Table 2, we indicate the specific margin of significance for each of the criteria, obtained via 1-way ANOVA testing.  In addition to the user study outlined in Section 5, we administered a second part of the study, in randomized order to the first, in which users were asked to pick a preferred method for feeding in side-by-side comparisons of jelly bean acquisition trials. To control for the initial state of the jelly beans, we purposely arrange 16 beans into a 4 \u00d7 4 grid initially, and conduct two trials per method which are randomly selected for the comparisons. Although we would like to include an analogous side-by-side comparisons survey for noodle acquisition for completeness, we find in practice that controlling for the initial state of noodles is nontrivial due to their highly deformable nature and vast set of feasible initial configurations. This makes it difficult to present users with unbiased comparisons across methods.\nThus, for bimanual scooping, we presented all permutations of pairs of the three methods, for a total of six comparisons overall. Empirically, we find that VAPORS is the preferred method by a large margin compared to both baselines (Fig. 13).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Americans with disabilities: 2010", "journal": "", "year": "2012", "authors": "M W Brault"}, {"ref_id": "b1", "title": "The role of assistive robotics in the lives of persons with disability", "journal": "American Journal of Physical Medicine & Rehabilitation", "year": "2010", "authors": "S W Brose; D J Weber; B A Salatin; G G Grindle; H Wang; J J Vazquez; R A Cooper"}, {"ref_id": "b2", "title": "Evaluation of the jaco robotic arm: Clinicoeconomic study for powered wheelchair users with upper-extremity disabilities", "journal": "IEEE", "year": "2011", "authors": "V Maheu; P S Archambault; J Frappier; F Routhier"}, {"ref_id": "b3", "title": "Learning visuo-haptic skewering strategies for robot-assisted feeding", "journal": "", "year": "2022", "authors": "P Sundaresan; S Belkhale; D Sadigh"}, {"ref_id": "b4", "title": "Adaptive robot-assisted feeding: An online learning framework for acquiring previously unseen food items", "journal": "IEEE", "year": "2020", "authors": "E K Gordon; X Meng; T Bhattacharjee; M Barnes; S S Srinivasa"}, {"ref_id": "b5", "title": "Robot-assisted feeding: Generalizing skewering strategies across food items on a plate", "journal": "Springer", "year": "2019", "authors": "R Feng; Y Kim; G Lee; E K Gordon; M Schmittle; S Kumar; T Bhattacharjee; S S Srinivasa"}, {"ref_id": "b6", "title": "Learning bimanual scooping policies for food acquisition", "journal": "", "year": "2022", "authors": "J Grannen; Y Wu; S Belkhale; D Sadigh"}, {"ref_id": "b7", "title": "-mouth robotic bite transfer with visual and haptic sensing", "journal": "", "year": "2022", "authors": "L Shaikewitz; Y Wu; S Belkhale; J Grannen; P Sundaresan; D Sadigh"}, {"ref_id": "b8", "title": "Balancing efficiency and comfort in robot-assisted bite transfer", "journal": "IEEE", "year": "2022", "authors": "S Belkhale; E K Gordon; Y Chen; S Srinivasa; T Bhattacharjee; D Sadigh"}, {"ref_id": "b9", "title": "Transfer depends on acquisition: Analyzing manipulation strategies for robotic feeding", "journal": "IEEE", "year": "2019", "authors": "D Gallenberger; T Bhattacharjee; Y Kim; S S Srinivasa"}, {"ref_id": "b10", "title": "Learning semantic embedding spaces for slicing vegetables", "journal": "", "year": "2019", "authors": "M Sharma; K Zhang; O Kroemer"}, {"ref_id": "b11", "title": "Leveraging multimodal haptic sensory data for robust cutting", "journal": "IEEE", "year": "2019", "authors": "K Zhang; M Sharma; M Veloso; O Kroemer"}, {"ref_id": "b12", "title": "Disect: A differentiable simulator for parameter inference and control in robotic cutting", "journal": "", "year": "2022", "authors": "E Heiden; M Macklin; Y Narang; D Fox; A Garg; F Ramos"}, {"ref_id": "b13", "title": "The surprising effectiveness of linear models for visual foresight in object pile manipulation", "journal": "", "year": "", "authors": "H Suh; R Tedrake"}, {"ref_id": "b14", "title": "Skill-based model-based reinforcement learning", "journal": "", "year": "2022", "authors": "L X Shi; J J Lim; Y Lee"}, {"ref_id": "b15", "title": "Planning with spatialtemporal abstraction from point clouds for deformable object manipulation", "journal": "", "year": "2022", "authors": "X Lin; C Qi; Y Zhang; Z Huang; K Fragkiadaki; Y Li; C Gan; D Held"}, {"ref_id": "b16", "title": "Reinforcement learning based control of imitative policies for near-accident driving", "journal": "", "year": "2020", "authors": "Z Cao; E B\u0131y\u0131k; W Z Wang; A Raventos; A Gaidon; G Rosman; D Sadigh"}, {"ref_id": "b17", "title": "Accelerating robotic reinforcement learning via parameterized action primitives", "journal": "", "year": "2021", "authors": "M Dalal; D Pathak; R R Salakhutdinov"}, {"ref_id": "b18", "title": "Augmenting reinforcement learning with behavior primitives for diverse manipulation tasks", "journal": "IEEE", "year": "2022", "authors": "S Nasiriany; H Liu; Y Zhu"}, {"ref_id": "b19", "title": "Taps: Task-agnostic policy sequencing", "journal": "", "year": "2022", "authors": "C Agia; T Migimatsu; J Wu; J Bohg"}, {"ref_id": "b20", "title": "Rma: Rapid motor adaptation for legged robots", "journal": "", "year": "2021", "authors": "A Kumar; Z Fu; D Pathak; J Malik"}, {"ref_id": "b21", "title": "Hierarchical neural dynamic policies", "journal": "", "year": "2021", "authors": "S Bahl; A Gupta; D Pathak"}, {"ref_id": "b22", "title": "Concept2robot: Learning manipulation concepts from instructions and human demonstrations", "journal": "The International Journal of Robotics Research", "year": "2021", "authors": "L Shao; T Migimatsu; Q Zhang; K Yang; J Bohg"}, {"ref_id": "b23", "title": "", "journal": "Meet Obi. Meet Obi", "year": "2022-06", "authors": ""}, {"ref_id": "b24", "title": "Meal-Mate -Made2Aid", "journal": "", "year": "2022-06", "authors": " Meal-Mate"}, {"ref_id": "b25", "title": "Leveraging post hoc context for faster learning in bandit settings with applications in robot-assisted feeding", "journal": "IEEE", "year": "2021", "authors": "E K Gordon; S Roychowdhury; T Bhattacharjee; K Jamieson; S S Srinivasa"}, {"ref_id": "b26", "title": "Hierarchical planning in the now", "journal": "", "year": "2010", "authors": "L P Kaelbling; T Lozano-P\u00e9rez"}, {"ref_id": "b27", "title": "Combined task and motion planning through an extensible planner-independent interface layer", "journal": "IEEE", "year": "2014", "authors": "S Srivastava; E Fang; L Riano; R Chitnis; S Russell; P Abbeel"}, {"ref_id": "b28", "title": "Integrated task and motion planning", "journal": "robotics, and autonomous systems", "year": "2021", "authors": "C R Garrett; R Chitnis; R Holladay; B Kim; T Silver; L P Kaelbling; T Lozano-P\u00e9rez"}, {"ref_id": "b29", "title": "Guided search for task and motion plans using learned heuristics", "journal": "IEEE", "year": "2016", "authors": "R Chitnis; D Hadfield-Menell; A Gupta; S Srivastava; E Groshev; C Lin; P Abbeel"}, {"ref_id": "b30", "title": "Ffrob: An efficient heuristic for task and motion planning", "journal": "Springer", "year": "2015", "authors": "C R Garrett; T Lozano-P\u00e9rez; L P Kaelbling"}, {"ref_id": "b31", "title": "Robocraft: Learning to see, simulate, and shape elasto-plastic objects with graph networks", "journal": "", "year": "2022", "authors": "H Shi; H Xu; Z Huang; Y Li; J Wu"}, {"ref_id": "b32", "title": "Skill abstraction from differentiable physics for deformable object manipulations with tools", "journal": "", "year": "2022", "authors": "X Lin; Z Huang; Y Li; J B Tenenbaum; D Held; C Gan;  Diffskill"}, {"ref_id": "b33", "title": "Deep visual foresight for planning robot motion", "journal": "IEEE", "year": "2017", "authors": "C Finn; S Levine"}, {"ref_id": "b34", "title": "Visual foresight: Model-based deep reinforcement learning for vision-based robotic control", "journal": "", "year": "2018", "authors": "F Ebert; C Finn; S Dasari; A Xie; A Lee; S Levine"}, {"ref_id": "b35", "title": "Dream to control: Learning behaviors by latent imagination", "journal": "", "year": "2019", "authors": "D Hafner; T Lillicrap; J Ba; M Norouzi"}, {"ref_id": "b36", "title": "Learning latent dynamics for planning from pixels", "journal": "PMLR", "year": "2019", "authors": "D Hafner; T Lillicrap; I Fischer; R Villegas; D Ha; H Lee; J Davidson"}, {"ref_id": "b37", "title": "Causal discovery in physical systems from videos", "journal": "", "year": "2020", "authors": "Y Li; A Torralba; A Anandkumar; D Fox; A Garg"}, {"ref_id": "b38", "title": "Hierarchical reinforcement learning: A comprehensive survey", "journal": "ACM Computing Surveys (CSUR)", "year": "2021", "authors": "S Pateria; B Subagdja; A Tan; C Quek"}, {"ref_id": "b39", "title": "Blender foundation. The essential Blender: guide to 3D creation with the open source suite Blender", "journal": "", "year": "2007", "authors": "T "}, {"ref_id": "b40", "title": "pybullet, a python module for physics simulation for games, robotics and machine learning", "journal": "", "year": "2016", "authors": "E Coumans; Y Bai"}, {"ref_id": "b41", "title": "Isaac gym: High performance gpu-based physics simulation for robot learning", "journal": "", "year": "2021", "authors": "V Makoviychuk; L Wawrzyniak; Y Guo; M Lu; K Storey; M Macklin; D Hoeller; N Rudin; A Allshire; A Handa"}, {"ref_id": "b42", "title": "Dense object nets: Learning dense visual object descriptors by and for robotic manipulation", "journal": "", "year": "2018", "authors": "P R Florence; L Manuelli; R Tedrake"}, {"ref_id": "b43", "title": "Autonomously untangling long cables", "journal": "", "year": "2022", "authors": "V Viswanath; K Shivakumar; J Kerr; B Thananjeyan; E Novoseller; J Ichnowski; A Escontrela; M Laskey; J E Gonzalez; K Goldberg"}, {"ref_id": "b44", "title": "Untangling dense non-planar knots by learning manipulation features and recovery policies", "journal": "", "year": "2021", "authors": "P Sundaresan; J Grannen; B Thananjeyan; A Balakrishna; J Ichnowski; E Novoseller; M Hwang; M Laskey; J E Gonzalez; K Goldberg"}, {"ref_id": "b45", "title": "Untangling dense knots by learning task-relevant keypoints", "journal": "", "year": "2020", "authors": "J Grannen; P Sundaresan; B Thananjeyan; J Ichnowski; A Balakrishna; M Hwang; V Viswanath; M Laskey; J E Gonzalez; K Goldberg"}, {"ref_id": "b46", "title": "Keypoints into the future: Self-supervised correspondence in model-based reinforcement learning", "journal": "", "year": "2020", "authors": "L Manuelli; Y Li; P Florence; R Tedrake"}, {"ref_id": "b47", "title": "kpam 2.0: Feedback control for category-level robotic manipulation", "journal": "IEEE Robotics and Automation Letters", "year": "2021", "authors": "W Gao; R Tedrake"}, {"ref_id": "b48", "title": "Cliport: What and where pathways for robotic manipulation", "journal": "PMLR", "year": "2022", "authors": "M Shridhar; L Manuelli; D Fox"}, {"ref_id": "b49", "title": "Transporter networks: Rearranging the visual world for robotic manipulation", "journal": "PMLR", "year": "2021", "authors": "A Zeng; P Florence; J Tompson; S Welker; J Chien; M Attarian; T Armstrong; I Krasin; D Duong; V Sindhwani"}, {"ref_id": "b50", "title": "Deep imitation learning of sequential fabric smoothing from an algorithmic supervisor", "journal": "IEEE", "year": "2020", "authors": "D Seita; A Ganapathi; R Hoque; M Hwang; E Cen; A K Tanwani; A Balakrishna; B Thananjeyan; J Ichnowski; N Jamali"}, {"ref_id": "b51", "title": "Flingbot: The unreasonable effectiveness of dynamic manipulation for cloth unfolding", "journal": "PMLR", "year": "2022", "authors": "H Ha; S Song"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Action Parameterization: We parameterize acquisition and rearrangement actions relative to the densest (x d , y d , z d ) and furthest (x f , y f , z f ) regions on the plate, as well as the utensil roll \u03b3 and pitch \u03b2.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Simulation vs. Real: We visualize the task of bimanual scooping of jelly beans. Due to the sim-to-real gap, we merely leverage simulation to learn high-level food dynamics, and leave low-level action planning to real vision-parameterized primitives.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Latent Plate Dynamics Model: We learn a latent dynamics model of the plate comprised of an encoder q, transition model p(z\u03c4 |z\u03c4\u22121, h k \u03c4 \u22121 ), and a reward model p(rt|zt). We use this model to select action sequences that maximize future rewards.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Across 10 trials for spaghetti (a) and jelly bean (b) acquisition, we visualize the cumulative amount acquired across individual trials (left) and averaged overall (right). Shading denotes the standard error. model f seg : R W \u00d7H\u00d73 +", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 6 :6Figure 6: Likert Ratings: We administer a 7-point Likert survey to users after observing 10 trials per method. VAPORS elicits the most positive feedback across all criteria. '*' indicates statistical significance (p < 0.05).", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 7 :7Figure 7: Noodle Acquisition Tiers of Difficulty: Tier 1 consists of plain noodle varieties: Dan Dan, Udon, and Pappardelle noodles. Tier 2 includes Tier 1 plates along with soy sauce, marinara sauce, and garnishes such as parsley or cilantro. Tier 3 plates include noodle dishes such as pesto pasta and chow mein ordered from DoorDash.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 8 :8Figure8: Likert Ratings for Users with and without Mobility Impairments: Error bars indicate standard error, and '*' indicates statistical siginficant findings via 1-way ANOVA testing. Users with and without mobility impairments favor VAPORS in terms of all criteria, but especially efficiency, bite size, and generalizability, suggesting the promise of our approach. For more user-facing considerations, such as practicality and whether users would reuse VAPORS given a choice, we find a small drop in average ratings between users without and with mobility impairments. In Section A, we discuss additional considerations to improve VAPORS in these aspects. We note that the larger error bars and lack of statistical significant findings for users with mobility impairments is due to the small sample size of users tested (3) as compared to 49 users without mobility impairments. We are actively working to increase this pool for larger-scale user testing in the future.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Algorithm 11Planning with VAPORS 1: for \u03c4 \u2208 {1, . . . , T } do 2:I t , D t \u2190 Get current RGBD image observation 3:M t = f seg (I t ) // Infer segmentation mask 4:\u0125 k \u03c4 = \u03c0 H (M 1:t , a 1:t\u22121 ) // Select high-level action 5: Execute \u03c0 L (M t ,\u0125 k \u03c4 )", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 10 :10Figure 10: Noodle Acquisition Rollout: We visualize 6 actions performed by VAPORS on the task of clearing an initially half-full plate of Tier 3 noodles. As the distribution of noodles on the plate becomes sparse (t = 0, 42, 54), VAPORS employs grouping strategies (black) to push noodles in close proximity. Once consolidated, VAPORS employs twirling (t = 12, 66, 86), as shown in red, for efficient plate clearance, where t denotes the clock time in seconds.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 11 :11Figure 11: Bimanual Scooping Rollout: Using a bimanual setup with two Franka Emika Panda robots, VAPORS performs 6 actions consisting of grouping (black arrows) and scooping (red arrows) to acquire jelly beans on a tray. By grouping when the tray is sparse and acquiring when a bite-sized clump forms, VAPORS demonstrates efficient acquisition. The annotated timestamps denote clock time in seconds.", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 12 :12Figure12: VAPORS Failure Modes: We illustrate the 4 most commonly observed failure modes with VAPORS on noodle acquisition. Misperception (A) occurs when \u03c0L erroneously senses vegetables, sauce, or plate glare as a noodle due to false positives with fori, leading to a misplanned action such as grouping in that region. Occasionally, \u03c0H may acquire when rearrangement is more appropriate, leading to a low-volume bite (B). In terms of action execution, food acquisition requires care so as to not miss food (C), as seen in the grouping motion which fails to group singular noodle strands due to system imprecision. Finally, slippage (D) can happen during acquisition with hard-to-model items such as those coated in sauce.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "High-level policy : \u03c0 H (h k |o \u2264t , a \u2264t\u22121 )", "formula_coordinates": [3.0, 152.77, 592.02, 156.34, 11.72]}, {"formula_id": "formula_1", "formula_text": "I t \u2208 R W \u00d7H\u00d73 + , D t \u2208 R W \u00d7H , M t \u2208 R W \u00d7H +", "formula_coordinates": [3.0, 108.0, 710.97, 179.12, 12.86]}, {"formula_id": "formula_2", "formula_text": "(x d , y d , z d )", "formula_coordinates": [4.0, 243.25, 181.76, 45.75, 9.65]}, {"formula_id": "formula_3", "formula_text": "= (x d , y d , z d , \u03b3, \u03b2) (1).", "formula_coordinates": [4.0, 194.8, 253.49, 96.09, 9.65]}, {"formula_id": "formula_4", "formula_text": "y f \u2212y d x f \u2212x d , and is untilted (\u03b2 = 0 \u2022 ): a t,rearrange = (x d , y d , z d , x f , y f , z f ) (2).", "formula_coordinates": [4.0, 163.5, 330.28, 302.5, 15.34]}, {"formula_id": "formula_5", "formula_text": "R = \u03c4 +T +1 i=\u03c4 +1 E [p(r i |z i )]", "formula_coordinates": [5.0, 108.0, 585.58, 108.86, 14.11]}, {"formula_id": "formula_6", "formula_text": "h k = \u03c0 H (M \u2264t ,\u00e2 (H) \u2264t\u22121 ).", "formula_coordinates": [6.0, 108.0, 476.37, 96.43, 14.23]}, {"formula_id": "formula_7", "formula_text": "d ,v d = arg max (u,v)\u2208M \u2032 tM \u2032 t [u, v", "formula_coordinates": [6.0, 258.26, 544.36, 135.54, 16.21]}, {"formula_id": "formula_8", "formula_text": "\u03c0 L (o t , h k,acquis ) = (x d ,\u0177 d ,\u1e91 d ,\u03b3) \u03c0 L (o t , h k,rearrange ) = (x d ,\u0177 d ,\u1e91 d ,x f ,\u0177 f ,\u1e91 f )", "formula_coordinates": [6.0, 124.65, 627.88, 362.7, 11.72]}, {"formula_id": "formula_9", "formula_text": "z t:t+T +1 \u223c q(z \u03c4 |M 1:t , a 1:t\u22121 ) \u03c4 +T +1 i=\u03c4 +1 p(z i |z i\u22121 , h k i\u22121 ),(4)", "formula_coordinates": [14.0, 195.76, 307.16, 308.91, 30.32]}, {"formula_id": "formula_10", "formula_text": "\u03c4 |z \u03c4 \u22121 , h k \u03c4 \u22121 )", "formula_coordinates": [14.0, 326.23, 350.7, 56.77, 12.2]}, {"formula_id": "formula_11", "formula_text": "R = i+T +1 i=\u03c4 +1 E [p(r i |z i )] .(5)", "formula_coordinates": [14.0, 256.45, 380.65, 248.22, 30.32]}, {"formula_id": "formula_12", "formula_text": "L dice = 1 \u2212 2 \u00d7 TP 2 \u00d7 TP + FN + FP (6)", "formula_coordinates": [15.0, 242.58, 200.53, 262.08, 22.59]}], "doi": ""}