{"title": "", "authors": "Rachid Riad; Olivier Teboul; David Grangier; Neil Zeghidour", "pub_date": "2022-02-03", "abstract": "Convolutional neural networks typically contain several downsampling operators, such as strided convolutions or pooling layers, that progressively reduce the resolution of intermediate representations. This provides some shift-invariance while reducing the computational complexity of the whole architecture. A critical hyperparameter of such layers is their stride: the integer factor of downsampling. As strides are not differentiable, finding the best configuration either requires crossvalidation or discrete optimization (e.g. architecture search), which rapidly become prohibitive as the search space grows exponentially with the number of downsampling layers. Hence, exploring this search space by gradient descent would allow finding better configurations at a lower computational cost. This work introduces DiffStride, the first downsampling layer with learnable strides. Our layer learns the size of a cropping mask in the Fourier domain, that effectively performs resizing in a differentiable way. Experiments on audio and image classification show the generality and effectiveness of our solution: we use DiffStride as a drop-in replacement to standard downsampling layers and outperform them. In particular, we show that introducing our layer into a ResNet-18 architecture allows keeping consistent high performance on CIFAR10, CIFAR100 and ImageNet even when training starts from poor random stride configurations. Moreover, formulating strides as learnable variables allows us to introduce a regularization term that controls the computational complexity of the architecture. We show how this regularization allows trading off accuracy for efficiency on ImageNet. * This work was conducted while interning at Google.", "sections": [{"heading": "INTRODUCTION", "text": "Convolutional neural networks (CNNs) (Fukushima, 1980;LeCun et al., 1989) have been the most widely used neural architecture across a wide range of tasks, including image classification (Krizhevsky et al., 2012;He et al., 2016a;Huang et al., 2017;Bello et al., 2021), audio pattern recognition (Kong et al., 2020), text classification (Conneau et al., 2017), machine translation (Gehring et al., 2017) and speech recognition (Amodei et al., 2016;Sercu et al., 2016;Zeghidour et al., 2018). Convolution layers, which are the building block of CNNs, project input features to a higher-level representation while preserving their resolution. When composed with non-linearities and normalization layers, this allows for learning rich mappings at a constant resolution, e.g. autogressive image synthesis (van den Oord et al., 2016). However, many tasks infer high-level low-resolution information (identity of a speaker (Muckenhirn et al., 2018), presence of a face (Chopra et al., 2005)) by integrating over low-level, high-resolution measurements (waveform, pixels). This integration requires extracting the right features, discarding irrelevant information over several downsampling steps. To that end, pooling layers and strided convolutions aggressively reduce the resolution of their inputs, providing several benefits. First, they act as a bottleneck that forces features to focus on information relevant to the task at hand. Second, pooling layers such as low-pass filters (Zhang, 2019) improve shift-invariance. Third, a reduced resolution implies a reduced number of floating-point operations and a higher receptive field in the subsequent layers.\nPooling layers can usually be decomposed into two basic steps: (1) computing local statistics densely over the whole input (2) sub-sampling these statistics by an integer striding factor. Past work has mostly focused on improving (1), by proposing better alternatives to max and average pooling that avoid aliasing (Zhang, 2019;Fonseca et al., 2021), preserve the important local details (Saeedan et al., 2018), or adapt to the training data distribution (Gulcehre et al., 2014;Lee et al., 2016). Observing that integer strides reduce resolution too quickly (e.g. a (2, 2) striding reduces the output size by 75%), Graham (2014) proposed fractional max-pooling, that allows for fractional (i.e. rational) strides, allowing for integration of more downsampling layers into a network. Similarly, Rippel et al. (2015) introduce spectral pooling which, by cropping its inputs in the Fourier domain, performs downsampling with fractional strides while emphasizing lower frequencies.\nWhile fractional strides give more flexibility in designing downsampling layers, they increase the size of an already gigantic search space. Indeed, as strides are hyperparameters, finding the best combination requires cross-validation or architecture search (Zoph & Le, 2017;Baker et al., 2017;, which rapidly become infeasible as the number of configurations grows exponentially with the number of downsampling layers. This led Zoph & Le (2017) not to search for strides in most of their experiments. Talebi & Milanfar (2021) and Jin et al. (2021) proposed a neural network that learns a resizing function for natural images, but the scaling factor (i.e. the stride) still required cross-validation. Thus, the nature of strides as hyperparameters -rather than trainable parameters -hinders the discovery of convolutional architectures and learning strides by backpropagation would unlock a virtually infinite search space.\nIn this work, we introduce DiffStride, the first downsampling layer that learns its strides jointly with the rest of the network. Inspired by Rippel et al. (2015), DiffStride casts downsampling in the spatial domain as cropping in the frequency domain. However, and unlike Rippel et al. (2015), rather than cropping with a fixed bounding box controlled by a striding hyperparameter, DiffStride learns the size of its cropping box by backpropagation. To do so, we propose a 2D version of an attention window with learnable size proposed by Sukhbaatar et al. (2019) for language modeling. On five audio classification tasks, using DiffStride as a drop-in replacement to strided convolutions improves performance overall while providing interpretability on the optimal per-task receptive field. By integrating DiffStride into a ResNet-18 (He et al., 2016a), we show on CIFAR  and ImageNet (Deng et al., 2009) that even when initializing strides randomly, our model converges to the best performance obtained with the properly cross-validated strides of He et al. (2016a). Moreover, casting strides as learnable parameters allows us to propose a regularization that directly minimizes computation and memory usage. We release our implementation of DiffStride 1 .", "publication_ref": ["b17", "b38", "b35", "b22", "b28", "b2", "b33", "b8", "b18", "b0", "b48", "b60", "b57", "b41", "b7", "b62", "b62", "b15", "b47", "b21", "b39", "b20", "b45", "b63", "b1", "b63", "b54", "b31", "b45", "b45", "b51", "b22", "b10", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "METHODS", "text": "We first provide background on spatial and spectral pooling, and propose DiffStride for learning strides of downsampling layers. We focus on 2D CNNs since they are generic enough to be used for image (LeCun et al., 1989;Krizhevsky et al., 2012;He et al., 2016a) and audio (Amodei et al., 2016;Kong et al., 2020) processing (taking time-frequency representations as inputs). However, these methods are equally applicable to the 1D (e.g. time-series) and 3D (e.g. video) cases.", "publication_ref": ["b38", "b35", "b22", "b0", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "NOTATIONS", "text": "Let x \u2208 R H\u00d7W , its Discrete Fourier Transform (DFT) y = F(x) \u2208 C H\u00d7W is obtained through the decomposition on a fixed set of basis filters (Lyons, 2004):\nF(x) mn = 1 \u221a HW H\u22121 h=0 W \u22121 w=0 x hw e \u22122\u03c0i( mh H + nw W ) , \u2200m \u2208 {0, . . . , H \u2212 1}, \u2200n \u2208 {0, . . . , W \u2212 1}. (1)\nThe DFT transformation is linear and its inverse is given by its conjugate F(.) \u22121 = F(.) * . The Fourier transform of a real-valued signal x \u2208 R H\u00d7W being conjugate symmetric (Hermitiansymmetry), we can reconstruct x from the positive half frequencies for the width dimension and omit the negative frequencies (y mn = y * (H\u2212m)modH,(W \u2212n)modW ). In addition, the DFT and its inverse are differentiable with regard to their inputs and the derivative of the DFT (resp. inverse DFT) is its conjugate linear operator, i.e. the inverse DFT (resp. DFT). More formally, if we consider L : C H\u00d7W \u2212\u2192 R as a loss taking as input the Fourier representation y, we can compute the gradient of L with regard to x, by using the inverse DFT:\nx \u2208 R H\u00d7W , y = F(x), \u2202L \u2202x = F * ( \u2202L \u2202y ) = F \u22121 ( \u2202L \u2202y ).(2)\nWe denote by L the total number of convolution layers in a CNN architecture and each layer is indexed by l. The \u2022 symbol represents the element-wise product between two tensors, . is the floor operation and \u2297 the outer product between two vectors. S represents the stride parameters, and sg is the stop gradient operator (Bengio et al., 2013;Yin et al., 2019), defined has the identity function during forward pass and with zero partial derivatives.", "publication_ref": ["b40", "b3", "b59"], "figure_ref": [], "table_ref": []}, {"heading": "DOWNSAMPLING IN CONVOLUTIONAL NEURAL NETWORKS", "text": "A basic mechanism for downsampling representations in a CNN is strided convolutions which jointly convolve inputs and finite impulse response filters and downsample the output. Alternatively, one can disentangle both operations by first applying a non-strided convolution followed by a pooling operation that computes local statistics (e.g. using an average, max (Boureau et al., 2010)) before downsampling. In both settings, downsampling does not benefit from the global structure of its inputs and can discard important information (Hinton, 2014;Saeedan et al., 2018). Moreover, and as observed by Graham (2014), the integer nature of strides only allows for drastic reductions in resolution: a 2D-convolution with strides S = (2, 2) reduces the dimension of its inputs by 75%. Furthermore, stride configurations are cumbersome to explore as the number of stride combinations grows exponentially with the number of downsampling layers. This means that cross-validation can only explore a limited subset of the stride hyperparameter configurations. This limitation is likely to translate into lower performance, as Section 3.2 shows that an inappropriate choice of strides for a ResNet-18 architecture can account for a drop of > 18% in accuracy on CIFAR-100.", "publication_ref": ["b6", "b25", "b47", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "SPECTRAL POOLING", "text": "Energy of natural signals is typically not uniformly distributed in the frequency domain, with signals such as sounds (Singh & Theunissen, 2003), images (Ruderman, 1994) and surfaces (Kuroki et al., 2018) concentrating most of the information in the lower frequencies. Rippel et al. (2015) build on this observation to introduce spectral pooling which alleviates the loss of information of spatial pooling, while enabling fractional downsizing factors. Spectral pooling also preserves low frequencies without aliasing, a known weakness of spatial/temporal convnets (Zhang, 2019;Ribeiro & Sch\u00f6n, 2021).\nWe consider an input x \u2208 R H\u00d7W and strides S = (S h , S\nw ) \u2208 [1, H) \u00d7 [1, W ).\nFirst, the DFT is computed y = F(x) \u2208 C H\u00d7W and for simplicity we assume that the center of this matrix is the DC component -the zero frequency. Then, a bounding box of size H S h \u00d7 W Sw crops this matrix around its center to produce\u1ef9 \u2208 C\nH S h \u00d7 W Sw .\nFinally, this output is brought back to the spatial domain with an inverse DFT:\nx = F \u22121 (\u1ef9) \u2208 R H S h \u00d7 W Sw .\nIn practice, x is typically a multichannel input (i.e. x \u2208 R H\u00d7W \u00d7C ) and the same cropping is applied to all channels. Moreover, since x is real-valued and thanks to Hermitian symmetry (see Section 2.1 for more details), only the positive half of the DFT coefficients are computed, which allows saving computation and memory while ensuring that the outputx remains real-valued.\nUnlike spatial pooling that requires integer strides, spectral pooling only requires integer output dimensions, which allows for much more fine-grained downsizing. Moreover, spectral pooling acts as a low-pass filter over the entire input, only keeping the lower frequencies i.e. the most relevant information in general and avoiding aliasing (Zhang, 2019). However, and similarly to spatial pooling, spectral pooling is differentiable with respect to its inputs but not with respect to its strides. Thus, one still needs to provide S as hyperparameters for each downsampling layer. In this case, the search space is even bigger than with spatial pooling since strides are not constrained to integer values anymore.\nFigure 1: DiffStride forward and backward pass, using a single-channel image. We only compute the positive half of DFT coefficients along the horizontal axis due to conjugate symmetry. The zoomed frame shows the horizontal mask mask w (Sw,W,R) (n). Here S = (S h , S w ) = (2.6, 3.1).", "publication_ref": ["b49", "b46", "b37", "b45", "b62", "b44", "b62"], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "DIFFSTRIDE", "text": "To address the difficulty of searching stride parameters, we propose DiffStride, a novel downsampling layer that allows spectral pooling to learn its strides through backpropagation. To downsample x \u2208 R H\u00d7W , DiffStride performs cropping in the Fourier domain similarly to spectral pooling. However, instead of using a fixed bounding box, DiffStride learns the box size via backpropagation. The learnable box W is parametrized by the shape of the input, a smoothness factor R and the strides.\nWe design this mask W as the outer product between two differentiable 1D masking functions (depicted in the lower right corner of Figure 1), one along the horizontal axis and one along the vertical axis. These 1D masks are directly derived from the adaptive attention span introduced by Sukhbaatar et al. (2019) to learn the attention span of self-attention models for natural language processing. Exploiting the conjugate symmetry of the coefficients, we only consider positive frequencies along the horizontal axis, while we mirror the vertical mask around frequency zero. Therefore, the two masks are defined as follows:\nmask h (S h ,H,R) (m) = min max 1 R (R + H 2S h \u2212 | H 2 \u2212 m|), 0 , 1 , m \u2208 [0, H] (3) mask w (Sw,W,R) (n) = min max 1 R (R + W 2S w + 1 \u2212 n), 0 , 1 , n \u2208 [0, W 2 + 1](4)\nwhere S = (S h , S w ) are the strides and R an hyperparameter that controls the smoothness of the mask. We build the 2D differentiable mask W as the outer product between the two 1D masks:\nW(S h , S w , H, W, R) = mask h (S h ,H,R) \u2297 mask w (Sw,W,R)(5)\nWe use W in two ways: (1) we apply it to the Fourier representation of the inputs via an elementwise product, which performs low-pass filtering (2) we crop the Fourier coefficients where the mask is zero (i.e. the output has dimensions\nH S h + 2 \u00d7 R \u00d7 W Sw + 2 \u00d7 R ).\nThe first step is differentiable with respect to strides S, however the cropping operation is not. Therefore, we apply a stop gradient operator (Bengio et al., 2013) to the mask before cropping. This way, gradients can flow to the strides through the differentiable low-pass filtering operation, but not through the non-differentiable cropping. Finally, the cropped tensor is transformed back into the Published as a conference paper at ICLR 2022 Algorithm 1: DiffStride layer\nInputs : Input x \u2208 R H\u00d7W , strides S = (S h , S w ) \u2208 [1, H) \u00d7 [1, W ), smoothness factor R. Output: Downsampled outputx \u2208 R H S h +2\u00d7R \u00d7 W Sw +2\u00d7R 1 y \u2190\u2212 F(x)\nProject input to the Fourier domain. 2 mask \u2190\u2212 W(S h , S w , H, W, R)\nConstruct the mask. See Equation 5.\n3 y masked \u2190\u2212 y \u2022 mask\nApply the mask as a low-pass filter. 4 y cropped \u2190\u2212 Crop(y masked , sg(mask)) Crop the tensor with the mask after stopping gradients. 5x \u2190\u2212 F \u22121 (y cropped )\nReturn to the spatial domain.\n3x3 Conv Strides=(2,2) 3x3 Conv Strides=(1,1) 3x3 Conv Strides=(2,2) (a)\nResidual block with a strided convolution.  spatial domain using an inverse DFT. All these steps are summarized by Algorithm 1 and illustrated on a single channel image in the Figure 1.\n3x3 Conv Strides=(1,1) 3x3 Conv Strides=(1,1) 3x3 Conv Strides=(1,1)", "publication_ref": ["b51", "b3"], "figure_ref": ["fig_7", "fig_7"], "table_ref": []}, {"heading": "During training we constrain strides", "text": "S = (S h , S w ) to remain in [1, H) \u00d7 [1, W ).\nWhen x is a multi-channel input (i.e. x \u2208 R H\u00d7W \u00d7C ), we learn the same strides S for all channels to ensure uniform spatial dimensions across channels. In spatial and spectral pooling, strides are typically tied along the spatial axes (i.e. S w = S h ), which we can also do in DiffStride by sharing a single learnable stride on both dimensions. However, our experiments in Section 3 show that learning specific strides for the vertical and horizontal axis is beneficial, not only when processing timefrequency representations of audio, but also -more surprisingly-when classifying natural images.\nAdding an hyperparameter R to each downsampling layer would conflict with the goal of removing strides as hyperparameters. Thus, not only we use a single R value for all layers, but we found no significant impact of this choice and all our experiments use R = 4. While we focus on the 2D case, using a single 1D mask allows deriving DiffStride in 1D CNNs, while performing the outer product between three 1D masks allows applying DiffStride to 3D inputs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "RESIDUAL BLOCK WITH DIFFSTRIDE", "text": "Unlike systems that only feed outputs of the l th layer to the (l + 1) th (Krizhevsky et al., 2012), ResNets (He et al., 2016a; introduce skip-connections that operate in parallel to the main branch.\nResNets stack two types of blocks: (1) identity blocks that maintain the input channel dimension and spatial resolution and (2) shortcut blocks that increase the output channel dimension while reducing the spatial resolution with a strided convolution (see Figure 2a). We integrate DiffStride into these shortcut blocks by replacing strided convolutions by convolutions without strides followed by DiffStride. Besides, sharing DiffStride strides between the main and residual branches ensures that their respective outputs have identical spatial dimensions and can be summed (See Figure 2b).", "publication_ref": ["b35", "b22"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "REGULARIZING COMPUTATION AND MEMORY COST WITH DIFFSTRIDE", "text": "The number of activations in a network depends on the strides and learning these parameters gives control over the space and time complexity of an architecture in a differentiable manner. This contrasts with previous work, as measures of complexity such as the number of floating-point operations (FLOPs) are typically not differentiable with respect to the parameters of a model and searching for efficient architectures is done via high-level exploration (e.g. introducing separable convolutions (Howard et al., 2017)), architecture search  or using continuous relaxations of complexity (Paria et al., 2020).\nA standard 2D convolution with a square kernel of size k 2 and C output channels has a computational cost of k 2 \u00d7C \u00d7C \u00d7H \u00d7W when operating on x \u2208 R H\u00d7W \u00d7C . Its memory usage-in terms of the number of activations to store-is C \u00d7 H \u00d7 W . Considering a fixed number of channels and kernel size, both the computational complexity and memory usage of a convolution layer are thus linear functions of its input size H \u00d7 W . This illustrates our argument made in Section 1 that downsampling does not only improve performance by discarding irrelevant information, but also reduces the complexity of the upper layers. More importantly, in the context of DiffStride the input size H l \u00d7 W l of layer l is determined as follows:\nH l \u00d7 W l = H l\u22121 S l\u22121 h + 2 \u00d7 R \u00d7 W l\u22121 S l\u22121 w + 2 \u00d7 R ,\nwhich is differentiable with respect to the strides at the previous layer S l\u22121 . Furthermore, it also depends on spatial dimensions at the previous layer H l\u22121 \u00d7 W l\u22121 , which themselves are a function of S l\u22122 . By induction over layers, the total computational cost and memory usage are proportional to\nl=L l=1 l i=1 1 S i h \u00d7S i w .\nSince in the context of DiffStride the kernel size and number of channels remain constant during training, we can directly regularize our model towards time and space efficiency by adding the following regularizer to our training loss:\n\u03bbJ((S l ) l=L l=1 ) = \u03bb l=L l=1 l i=1 1 S i h \u00d7 S i w , (6\n)\nwhere \u03bb is the regularization weight. In Section 3.2, we show that training on ImageNet with different values for \u03bb allows us to trade-off accuracy for efficiency in a smooth fashion.", "publication_ref": ["b26", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTS", "text": "We evaluate DiffStride on eight classification tasks, both on audio and images. For each comparison, we keep the same architecture and replace strided convolutions by convolutions with no stride followed by DiffStride. To avoid the confounding factor of downsampling in the Fourier domain, we also compare our approach to the spectral pooling of Rippel et al. (2015), which only differs from DiffStride by the fact that its strides are not learnable.", "publication_ref": ["b45"], "figure_ref": [], "table_ref": []}, {"heading": "AUDIO CLASSIFICATION", "text": "Experimental setup We perform single-task and multi-task audio classification on 5 tasks: acoustic scene classification (Heittola et al., 2018), birdsong detection (Stowell et al., 2018), musical instrumental classification and pitch estimation on the NSynth dataset (Engel et al., 2017) and speech command classification (Warden, 2018). The statistics of the datasets are summarized in Table A.1.\nThe audio sampled at 16 kHz is decomposed into log-compressed mel-spectrograms with 64 channels, computed with a window of 25 ms every 10 ms.\nA 2D-CNN, based on (Tagliasacchi et al., 2019) takes these spectrograms as inputs and alternates blocks of strided convolutions along time ((3\u00d71) kernel) and frequency ((1\u00d73) kernel). Each strided convolution is followed by a ReLU (Glorot et al., 2011) and batch normalization (Ioffe & Szegedy, 2015). The sequence of channels dimensions is defined as (64,128,256,256,512,512) and the strides are initialized as ((2, 2), (2, 2), (1, 1), (2, 2), (1, 1), (2, 2) for all downsampling methods. The output of the CNN passes through a global max-pooling and feeds into a single linear classification layer for single-task, and multiple classification layers for multi-task classification. As examples vary in length, we train models on random 1 s windows with ADAM (Kingma & Ba, 2015) and a learning rate of 10 \u22124 for 1 M batches, with batch size 256. Evaluation is run by splitting full sequences into 1 s non-overlapping windows and averaging the logits over windows.\nResults Table 1 summarizes the results for single-task and multi-task audio classification. In both settings, DiffStride improves over strided convolutions and spectral pooling, with strided convolutions only outperforming DiffStride for acoustic scene classification in the single task setting. Table 2 shows the strides learned by the first layer of DiffStride, which downsamples mel-spectrograms along frequency and time axes. Learning allows the strides to deviate from their initialization ((2, 2)) and to adapt to the task at hand. Converting strides to cut-off frequencies shows that the learned Acoustic scenes 99.1 \u00b1 0.2 98.6 \u00b1 0.1 98.6 \u00b1 0.2 97.7 \u00b1 0.4 97.7 \u00b1 0.7 97.7 \u00b1 0.3 Birdsong detection 78.8 \u00b1 0.3 79.7 \u00b1 0.3 81.3 \u00b1 0.1 77.3 \u00b1 0.2 77.8 \u00b1 0.3 78.6 \u00b1 0.5 Music (instrument) 72.6 \u00b1 0.3 72.9 \u00b1 0.5 75.4 \u00b1 0.0 69.8 \u00b1 0.4 70.4 \u00b1 0.4 73.0 \u00b1 0.8 Music (pitch) 91.8 \u00b1 0.1 90.1 \u00b1 0.0 92.2 \u00b1 0.1 89.4 \u00b1 0.3 87.6 \u00b1 0.7 89.9 \u00b1 0.3 Speech commands 87.3 \u00b1 0.1 88.5 \u00b1 0.3 90.5 \u00b1 0.3 83.5 \u00b1 0.6 83.9 \u00b1 0.4 86.2 \u00b1 0.8", "publication_ref": ["b24", "b50", "b13", "b58", "b53", "b19", "b30"], "figure_ref": [], "table_ref": ["tab_5", "tab_1"]}, {"heading": "Mean Accuracy", "text": "85.0 \u00b1 9.3 86.0 \u00b1 9.2 88.3 \u00b1 8.7 83.5 \u00b1 10.0 83.5 \u00b1 9.6 85.0 \u00b1 8.9 Table 2: Learned strides (% \u00b1 sd over 3 runs) of the first layer for the single and multi-task models.\nThe sampling rate of the input spectrogram being known (10 ms), we can convert the strides to upper cut-off frequencies (i.e. the maximum frequency kept by the lowpass-filter).\nstrides fall in a range showed by behavioral studies and direct neural recordings (Hullett et al., 2016;Flinker et al., 2019) to be necessary for e.g. speech intelligibility at 25 Hz (Elliott & Theunissen, 2009). Moreover, DiffStride learns different strides for the time and frequency axes. Table A.7\nshows the benefits of learning a per-dimension value rather than sharing strides. Another notable phenomenon is the per-task discrepancy on NSynth, with the pitch estimation requiring faster spectral modulations (as represented by a higher cutt-off frequency along the frequency axis). Finally, multi-task models do not converge to the mean of strides, but rather to a higher value that passes more frequencies not to negatively impact individual tasks.", "publication_ref": ["b29", "b14", "b12"], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "IMAGE CLASSIFICATION", "text": "Experimental setup We use the ResNet-18 (He et al., 2016a) architecture, comparing the original strided convolutions (see Figure 2a) to spectral pooling and DiffStride (both as in Figure 2b). We randomly sample 6 striding configurations for the three shortcut blocks of the ResNet-18, each stride being sampled in [1, 3], with (2, 2, 2) being the configuration of the original ResNet of He et al. (2016a). The horizontal and vertical strides are initialized equally at start. These random configurations simulate cross-validation of stride configurations to: (1) showcase the sensitivity of the architecture to these hyperparameters, (2) test our hypothesis that DiffStride can benefit from learning its strides to recover from a poor initialization. On Imagenet, as inputs are bigger than CIFAR we also allow the first ResNet-18 identity block to learn its strides which are 1 by default.\nWe first benchmark the three methods on the two CIFAR datasets . CIFAR10 consists of 32 \u00d7 32 images labeled in 10 classes with 6000 images per class. We use the official split, with 50,000 images for training and 10,000 images for testing. CIFAR100 uses the same images as CIFAR10, but with a more detailed labelling with 100 classes. We also compare the ResNet-18 architectures on the ImageNet dataset (Deng et al., 2009), which contains 1,000 classes. The models are trained on the official training split of the Imagenet dataset (1.28M images) and we report our results on the validation set (50k images). Here, we evaluate performance in terms of top-1 and top-5 accuracy. We train on all datasets with stochastic gradient descent (SGD) (Bottou et al., 1998) with a learning rate of 0.1, a batch size of 256 and a momentum (Qian, 1999) of 0.9.\nOn CIFAR, we train models for 400 epochs dividing the learning rate by 10 at 200 epochs and again by 10 at 300 epochs, with a weight decay of 5.10 \u22123 . For CIFAR, we apply random cropping on the CIFAR10 CIFAR100\nInit. Strides Strided Conv. Spectral DiffStride Strided Conv. Spectral DiffStride (2, 2, 2) 91.4 \u00b1 0.2 92.4 \u00b1 0.1 92.5 \u00b1 0.1 66.8 \u00b1 0.2 73.7 \u00b1 0.1 73.4 \u00b1 0.5 (2, 2, 3) 90.5 \u00b1 0.1 92.2 \u00b1 0.2 92.8 \u00b1 0.1 63.4 \u00b1 0.5 73.7 \u00b1 0.2 73.5 \u00b1 0.0 (1, 3, 1) 90.0 \u00b1 0.4 91.1 \u00b1 0.1 92.4 \u00b1 0.1 64.9 \u00b1 0.5 70.3 \u00b1 0.3 73.4 \u00b1 0. 2  (3, 1, 3) 85.7 \u00b1 0.1 90.9 \u00b1 0.2 92.4 \u00b1 0.1 55.3 \u00b1 0.8 69.4 \u00b1 0.4 73.7 \u00b1 0.4 (3, 1, 2) 86.4 \u00b1 0.1 90.9 \u00b1 0.2 92.3 \u00b1 0.1 56.2 \u00b1 0.3 69.9 \u00b1 0.2 73.4 \u00b1 0. 3  (3, 2, 3) 82.0 \u00b1 0.6 89.2 \u00b1 0.2 92.3 \u00b1 0.1 48.2 \u00b1 0.2 66.6 \u00b1 0.5 73.6 \u00b1 0.4\nMean accuracy 87.7 \u00b1 3.4 91.1 \u00b1 1.1 92.4 \u00b1 0.2 59.1 \u00b1 6.7 70.6 \u00b1 2.6 73.5 \u00b1 0.3 Table 3: Accuracies (% \u00b1 sd over 3 runs) on CIFAR10 and CIFAR100. First column represents strides at each shortcut block, (2, 2, 2) being the configuration of (He et al., 2016a). For reference, the state-of-the-art on CIFAR10 (CIFAR100) is (Dosovitskiy et al., 2020) ((Foret et al., 2020) with an accuracy of 99.5% (96.1%).   input images and left-right random flipping. On ImageNet, we train with a weight decay of 1.10 \u22123 for 90 epochs, dividing the learning rate by 10 at epochs 30, 60 and 80. We apply random cropping on the input images as in  and left-right random flipping.\nStrides Layers S 0 h S 0 w S 1 h S 1 w S 2 h S 2 w (a) Trajectories of the different strides (S l ) l\u2208(1,2,3) for a single run. S 1 h S 1 w S 2 h S 2 w S 3 h S\nResults We report the results on the CIFAR datasets and Imagenet in Tables 3 and 4 respectively, with the accuracy of our baseline ResNet-18 (first row, Strided Conv.) being consistent with previous work (Bianco et al., 2018). First, we observe that strides are indeed critical hyperparameters for the performance of a standard ResNet-18 on the three datasets, with the accuracy on CIFAR100 dropping from 66.8% average to 48.2% between the best and worst configurations. Remarkably, spectral pooling is much more robust to bad initializations than strided convolutions, even though its strides are also fixed. However, DiffStride is overall much more robust to poor choices of strides, converging consistently to a high accuracy on the three datasets, with a variance over initializations that is lower by an order of magnitude. This shows that backpropagation allows DiffStride to find a better configuration during training avoiding a cross-validation which would require 6,561 experiments for testing all combinations of strides in [1, 3] on Imagenet. Tables A.5 and A.6 confirm these observations on the EfficientNet-B0  architecture.\nLearning dynamics and equivalence classes Figure 3 illustrates the learning dynamics of Diff-Stride on CIFAR10. Figure 3a plots the strides as a function of the epoch for a run with the baseline (2,2,2) configuration as initialization. The strides all deviate from their initialization while converging rapidly, with the lower layers keeping more information while higher layers downsample more drastically. Interestingly, we discover equivalence classes: despite converging to the same accuracy (as reported in Table 3) the various initializations yield very diverse strides configurations at convergence, both in terms of total striding factor (defined as the product of strides, see Figure 3c) and of repartition of downsampling factors along the architecture (see Figure 3b). We obtain similar conclusions on CIFAR100 and Imagenet (see Figures A.1 and A.2). In the non-regularized case, it could seem counter-intuitive that minimizing the training loss yields positive stride updates, i.e. dropping more information through cropping. It highlights that loss optimization is a trade-   (He et al., 2016a). For reference, state-of-the-art on Imagenet is (Dai et al., 2021) with a top-1 accuracy of 90.88%. off between preserving information (no striding, no cropping) and downscaling such that the next convolution kernel accesses a wider spatial context.\nRegularizing the complexity The existence of equivalence classes suggests that DiffStride can find more computationally efficient configurations for a same accuracy. We thus train DiffStride on ImageNet using the complexity regularizer defined in Equation 6, with \u03bb varying between 0.1 and 10, always initializing strides with the baseline ((1, 1), (2, 2), (2, 2), (2, 2)). Figure 4 plots accuracy versus computational complexity (as measured by the value of the regularization term at convergence) of DiffStride. For comparison, we also plot the models with strided convolutions with the random initializations of  (i.e. ((11, 32), (1, 3), (1, 2), (2, 5))), the model converges to a 24.54% top-1 accuracy. This suggests that performing pooling in the spectral domain is more robust to aggressive downsampling, which corroborates the remarkable advantage of spectral pooling over strided convolutions when using poor strides choices in Tables 3 and 4 despite both models having fixed strides.\nLimitations Pooling in the spectral domain comes at higher computational cost than strided convolutions as it requires (1) computing a non-strided convolution and (2) a DFT and its inverse (see Table A.2). This could be alleviated by computing the convolution in the Fourier domain as an element-wise multiplication and summation over channels. Further improvements could be obtained by replacing the DFT by a real-valued counterpart, such as the Hartley transform (Zhang & Ma, 2018), which would remove the need for complex-valued operations that may be poorly optimized in deep learning frameworks. We also observe no benefits of DiffStride when training DenseNets (Huang et al., 2017), see Tables A.3 and A.4. We hypothesize that this is due to the limited number of downsampling layers, which reduces the space of stride configurations to a few, equivalent ones when sampling strides in [1; 3]. Finally, some hardware (e.g. TPUs) require a static computation graph. As DiffStride changes the spatial dimensions of intermediate representationsand thus the computation graph-between each gradient update, we currently only train on GPUs.", "publication_ref": ["b22", "b22", "b10", "b5", "b22", "b11", "b16", "b4", "b22", "b9", "b61", "b28"], "figure_ref": ["fig_1", "fig_1", "fig_4", "fig_4", "fig_4", "fig_4", "fig_5"], "table_ref": ["tab_1", "tab_1", "tab_3", "tab_3", "tab_5"]}, {"heading": "CONCLUSION AND FUTURE WORK", "text": "We introduce DiffStride the first downsampling layer with learnable strides. We show on audio and image classification that DiffStride can be used as a drop-in replacement to strided convolutions, removing the need for cross-validating strides. As we observe that our method discovers multiple equally-accurate stride configurations, we introduce a regularization term to favor the most computationally advantageous. In future work, we will extend the scope of applications of DiffStride, to e.g. 1D and 3D architectures. Moreover, learning strides by backpropagation opens new avenues in designing adaptive convolutional architectures, such as multi-scale models that would learn to operate at various scales in parallel by using independent branches with separate instances of DiffStride, or by predicting strides parameters of DiffStride on a per-example basis.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ACKNOWLEDGMENTS", "text": "Authors thank F\u00e9lix de Chaumont Quitry for the useful discussions and assistance throughout this project, as well as the reviewers of ICLR 2022 for their feedback that helped improving this manuscript. Authors also thank Oren Rippel, Jasper Snoek, and Ryan P. Adams for their inspiring work on spectral pooling.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "REPRODUCIBILITY STATEMENT", "text": "We describe DiffStride in details in the text as well as with Algorithm 1 and Figure 1. We mention all relevant hyperparameters to reproduce our experiments, as well as describe audio datasets in A.1. Moreover, we release Tensorflow 2.0 code for training a Pre-Act ResNet-18 with strided convolutions, spectral pooling or DiffStride on CIFAR10 and CIFAR100, with DiffStride being implemented as a stand-alone, reusable Keras layer. This open-source code can be found at https://github.com/google-research/diffstride.\nA APPENDIX     2), we show the distributions of learned strides and the global striding factor at convergence on CIFAR100 (Imagenet), starting from random stride initializations. On CIFAR100, we observe equivalence classes, i.e. model that learns various stride configurations for a same accuracy. On Imagenet, even though we also observe a significant variance of the global striding factor, models tend to downsample only in the upper layers. Striding late in the architecture comes at a higher computational cost, which furthermore justifies regularizing DiffStride to reduce complexity as shown in Section 3.2.", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "", "text": "Time and space complexity in practice While Figure 4 reports theoretical estimates of computational complexity based on stride configurations, both spectral pooling and DiffStride require computing a DFT and its inverse. Moreover, DiffStride requires accumulating gradients with respect to the strides during training. Table A.2 reports the duration and peak memory usage of the multitask architecture described in 3.1, for a single batch. Replacing strided convolutions with spectral pooling increases the wall time by 32% due to the DFT and inverse DFT, while the peak memory usage is almost unaffected. DiffStride furthermore increases the wall time (by 43% w.r.t strided convolutions) as the backward pass is more expensive. Similarly, it almost doubles the peak memory usage. However, in inference, DiffStride does not need to compute and store gradients w.r.t. the strides, thus the time and space complexity become identical to that of spectral pooling. DenseNet experiments on CIFAR We also evaluate DiffStride in DenseNet (Huang et al., 2017), especially the DenseNet-BC architecture with a depth of 121 and a growth rate of 32. The DenseNet architecture halves spatial dimensions during transition blocks. We replace the 2D average pooling in the transition blocks by spectral pooling or DiffStride. The considered architecture for DenseNet has two downsampling steps. We run a similar experiment as in 3.2 with random strides between the dense blocks on the two CIFAR datasets. We observe that initializing strides randomly does not affect the performance of the standard Densenet-BC architecture with average pooling. Consequently, DiffStride does not improve over alternatives.  (Huang et al., 2017).\nEfficientNet experiments on CIFAR We evaluate DiffStride in an EfficientNet-B0 architecture , a lightweight model discovered by architecture search. This architecture has seven strided convolutions. Unlike Tan & Le (2019), we do not pre-train on ImageNet, but rather train from scratch on CIFAR, which explains the lower accuracy of the baseline. As the model has seven downsampling layers, we rescale the images from 32 \u00d7 32 to 128 \u00d7 128, and only sample strides in [1; 2]. We run a similar experiment as in 3.2 with random strides on the two CIFAR datasets. Consistently with the results obtained with a ResNet-18, spectral pooling is much more robust to poor strides than strided convolutions, with DiffStride outperforming all alternatives.", "publication_ref": ["b28", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Init. Strides Average Pooling Spectral DiffStride", "text": "(1, 2, 2, 2, 1, 2, 1) 87.2 \u00b1 0.1 90.4 \u00b1 0.2 91.1 \u00b1 0.0 (1, 1, 2, 2, 2, 1, 1) 89.7 \u00b1 0.1 90.9 \u00b1 0.3 90.9 \u00b1 0.1 (1, 2, 2, 2, 2, 2, 1) 83.7 \u00b1 0.2 90.0\nMean accuracy 87.5 \u00b1 2.5 90.4 \u00b1 0.6 90.9 \u00b1 0.1 Table A.5: Accuracies (% \u00b1 sd over 3 runs) for CIFAR10 for each downsampling method with the EfficientNet-B0 architecture. First column represents strides at each strided convolution, with (1, 2, 2, 2, 1, 2, 1) being the configuration of .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Init. Strides Average Pooling Spectral DiffStride", "text": "(1, 2, 2, 2, 1, 2, 1) 55.2 \u00b1 0.3 66.0 \u00b1 0.6 66.6 \u00b1 0.5 (1, 1, 2, 2, 2, 1, 1) 62.0 \u00b1 0.7 66.4 \u00b1 0.6 66.6 \u00b1 0.3 (1, 2, 2, 2, 2, 2, 1) 46.8 \u00b1 2.0 65.9 \u00b1 0.5 66.3 \u00b1 0.7 (2, 1, 2, 1, 2, 1, 1) 60.4 \u00b1 0.1 65.5 \u00b1 0.1 67.0 \u00b1 0.1", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Mean accuracy", "text": "56.1 \u00b1 6.3 65.9 \u00b1 0.5 66.6 \u00b1 0.5 Table A.6: Accuracies (% \u00b1 sd over 3 runs) for CIFAR100 for each downsampling method with the EfficientNet-B0 architecture. First column represents strides at each strided convolution, with (1, 2, 2, 2, 1, 2, 1) being the configuration of . ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Deep speech 2: End-toend speech recognition in english and mandarin", "journal": "PMLR", "year": "2016", "authors": "Dario Amodei; Rishita Sundaram Ananthanarayanan; Jingliang Anubhai; Eric Bai; Carl Battenberg; Jared Case; Bryan Casper; Qiang Catanzaro; Guoliang Cheng;  Chen"}, {"ref_id": "b1", "title": "Designing neural network architectures using reinforcement learning", "journal": "", "year": "2017-04-24", "authors": "Bowen Baker; Otkrist Gupta; Nikhil Naik; Ramesh Raskar"}, {"ref_id": "b2", "title": "Revisiting resnets: Improved training and scaling strategies", "journal": "ArXiv", "year": "2021", "authors": "Irwan Bello; William Fedus; Xianzhi Du; Ekin Dogus Cubuk; A Srinivas; Tsung-Yi Lin; Jonathon Shlens; Barret Zoph"}, {"ref_id": "b3", "title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "journal": "", "year": "2013", "authors": "Yoshua Bengio; Nicholas L\u00e9onard; Aaron Courville"}, {"ref_id": "b4", "title": "Benchmark analysis of representative deep neural network architectures", "journal": "IEEE Access", "year": "2018", "authors": "Simone Bianco; R\u00e9mi Cad\u00e8ne; Luigi Celona; Paolo Napoletano"}, {"ref_id": "b5", "title": "Online learning and stochastic approximations", "journal": "", "year": "1998", "authors": "L\u00e9on Bottou"}, {"ref_id": "b6", "title": "A theoretical analysis of feature pooling in visual recognition", "journal": "", "year": "2010", "authors": "Y-Lan Boureau; Jean Ponce; Yann Lecun"}, {"ref_id": "b7", "title": "Learning a similarity metric discriminatively, with application to face verification", "journal": "", "year": "2005", "authors": "S Chopra; R Hadsell; Y Lecun"}, {"ref_id": "b8", "title": "Very deep convolutional networks for text classification", "journal": "Association for Computational Linguistics", "year": "2017-04", "authors": "Alexis Conneau; Holger Schwenk; Lo\u00efc Barrault; Yann Lecun"}, {"ref_id": "b9", "title": "Coatnet: Marrying convolution and attention for all data sizes", "journal": "", "year": "2021", "authors": "Zihang Dai; Hanxiao Liu; V Quoc; Mingxing Le;  Tan"}, {"ref_id": "b10", "title": "Imagenet: A large-scale hierarchical image database", "journal": "Ieee", "year": "2009", "authors": "Jia Deng; Wei Dong; Richard Socher; Li-Jia Li; Kai Li; Li Fei-Fei"}, {"ref_id": "b11", "title": "An image is worth 16x16 words: Transformers for image recognition at scale", "journal": "", "year": "2020", "authors": "Alexey Dosovitskiy; Lucas Beyer; Alexander Kolesnikov; Dirk Weissenborn; Xiaohua Zhai; Thomas Unterthiner; Mostafa Dehghani; Matthias Minderer; Georg Heigold; Sylvain Gelly"}, {"ref_id": "b12", "title": "The modulation transfer function for speech intelligibility", "journal": "PLoS computational biology", "year": "2009", "authors": "M Taffeta;  Elliott;  Fr\u00e9d\u00e9ric E Theunissen"}, {"ref_id": "b13", "title": "Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders", "journal": "", "year": "2017-04", "authors": "Jesse Engel; Cinjon Resnick; Adam Roberts; Sander Dieleman; Douglas Eck; Karen Simonyan; Mohammad Norouzi"}, {"ref_id": "b14", "title": "Spectrotemporal modulation provides a unifying framework for auditory cortical asymmetries", "journal": "Nature human behaviour", "year": "2019", "authors": "Adeen Flinker; K Werner;  Doyle; D Ashesh; Orrin Mehta; David Devinsky;  Poeppel"}, {"ref_id": "b15", "title": "Improving sound event classification by increasing shift invariance in convolutional neural networks", "journal": "ArXiv", "year": "2021", "authors": "Eduardo Fonseca; Andr\u00e9s Ferraro; Xavier Serra"}, {"ref_id": "b16", "title": "Sharpness-aware minimization for efficiently improving generalization", "journal": "", "year": "2020", "authors": "Pierre Foret; Ariel Kleiner; Hossein Mobahi; Behnam Neyshabur"}, {"ref_id": "b17", "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "journal": "Biological Cybernetics", "year": "1980", "authors": "K Fukushima"}, {"ref_id": "b18", "title": "Convolutional sequence to sequence learning", "journal": "", "year": "2017", "authors": "Jonas Gehring; Michael Auli; David Grangier; Denis Yarats; Yann N Dauphin"}, {"ref_id": "b19", "title": "Deep sparse rectifier neural networks", "journal": "", "year": "2011", "authors": "Xavier Glorot; Antoine Bordes; Yoshua Bengio"}, {"ref_id": "b20", "title": "", "journal": "", "year": "2014", "authors": "Benjamin Graham"}, {"ref_id": "b21", "title": "Learned-norm pooling for deep feedforward and recurrent neural networks", "journal": "Springer", "year": "2014", "authors": "Caglar Gulcehre; Kyunghyun Cho; Razvan Pascanu; Yoshua Bengio"}, {"ref_id": "b22", "title": "Deep residual learning for image recognition", "journal": "", "year": "2009", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b23", "title": "Identity mappings in deep residual networks", "journal": "Springer", "year": "2016", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b24", "title": "", "journal": "TUT Urban Acoustic Scenes", "year": "2018-04", "authors": "Toni Heittola; Annamaria Mesaros; Tuomas Virtanen"}, {"ref_id": "b25", "title": "What's wrong with convolutional nets. MIT Brain and Cognitive Sciences-Fall Colloquium Series", "journal": "", "year": "2014-12", "authors": "Geoffrey Hinton"}, {"ref_id": "b26", "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications", "journal": "ArXiv", "year": "2017", "authors": "Andrew G Howard; Menglong Zhu; Bo Chen; Dmitry Kalenichenko; Weijun Wang; Tobias Weyand; Marco Andreetto; Hartwig Adam"}, {"ref_id": "b27", "title": "Searching for mobilenetv3", "journal": "", "year": "2019", "authors": "Andrew G Howard; Mark Sandler; Grace Chu; Liang-Chieh Chen; Bo Chen; Mingxing Tan; Weijun Wang; Yukun Zhu; Ruoming Pang; Vijay Vasudevan; Quoc V Le; Hartwig Adam"}, {"ref_id": "b28", "title": "Densely connected convolutional networks", "journal": "", "year": "2009", "authors": "Gao Huang; Zhuang Liu; Laurens Van Der Maaten; Kilian Q Weinberger"}, {"ref_id": "b29", "title": "Human superior temporal gyrus organization of spectrotemporal modulation tuning derived from speech stimuli", "journal": "Journal of Neuroscience", "year": "2016", "authors": "W Patrick; Liberty S Hullett; Nima Hamilton; Christoph E Mesgarani; Edward F Schreiner;  Chang"}, {"ref_id": "b30", "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "journal": "PMLR", "year": "2015", "authors": "Sergey Ioffe; Christian Szegedy"}, {"ref_id": "b31", "title": "Learning to downsample for segmentation of ultra-high resolution images", "journal": "", "year": "2021", "authors": "Chen Jin; Ryutaro Tanno; Thomy Mertzanidou; Eleftheria Panagiotaki; Daniel C Alexander"}, {"ref_id": "b32", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b33", "title": "Panns: Large-scale pretrained audio neural networks for audio pattern recognition", "journal": "", "year": "2020", "authors": "Qiuqiang Kong; Yin Cao; Turab Iqbal; Yuxuan Wang; Wenwu Wang; Mark D Plumbley"}, {"ref_id": "b34", "title": "Learning multiple layers of features from tiny images", "journal": "", "year": "2009", "authors": "A Krizhevsky"}, {"ref_id": "b35", "title": "Imagenet classification with deep convolutional neural networks", "journal": "", "year": "2005", "authors": "Alex Krizhevsky; Ilya Sutskever; Geoffrey E Hinton"}, {"ref_id": "b36", "title": "Learning multiple layers of features from tiny images", "journal": "", "year": "2009", "authors": "Alex Krizhevsky"}, {"ref_id": "b37", "title": "Haptic texture perception on 3d-printed surfaces transcribed from visual natural textures", "journal": "Springer", "year": "2018", "authors": "Scinob Kuroki; Masataka Sawayama; Shin'ya Nishida"}, {"ref_id": "b38", "title": "Handwritten digit recognition with a back-propagation network", "journal": "", "year": "1989", "authors": "Yann Lecun; Bernhard Boser; John Denker; Donnie Henderson; Richard Howard; Wayne Hubbard; Lawrence Jackel"}, {"ref_id": "b39", "title": "Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree", "journal": "", "year": "2016", "authors": "Chen-Yu Lee; W Patrick; Zhuowen Gallagher;  Tu"}, {"ref_id": "b40", "title": "Understanding Digital Signal Processing", "journal": "Prentice Hall PTR", "year": "2004", "authors": "Richard G Lyons"}, {"ref_id": "b41", "title": "Towards directly modeling raw speech signal for speaker verification using cnns", "journal": "", "year": "2018", "authors": "M Hannah Muckenhirn;  Magimai; S Doss;  Marcel"}, {"ref_id": "b42", "title": "Minimizing flops to learn efficient sparse representations. ArXiv, abs", "journal": "", "year": "2004", "authors": "Biswajit Paria; Chih-Kuan Yeh; N Xu; B P\u00f3czos; Pradeep Ravikumar; I E Yen"}, {"ref_id": "b43", "title": "On the momentum term in gradient descent learning algorithms", "journal": "Neural networks : the official journal of the International Neural Network Society", "year": "1999", "authors": " Ning Qian"}, {"ref_id": "b44", "title": "How convolutional neural networks deal with aliasing", "journal": "IEEE", "year": "2021", "authors": "H Ant\u00f4nio; Thomas B Ribeiro;  Sch\u00f6n"}, {"ref_id": "b45", "title": "Spectral representations for convolutional neural networks", "journal": "", "year": "2006", "authors": "Oren Rippel; Jasper Snoek; Ryan P Adams"}, {"ref_id": "b46", "title": "The statistics of natural images", "journal": "Network: computation in neural systems", "year": "1994", "authors": " Daniel L Ruderman"}, {"ref_id": "b47", "title": "Detail-preserving pooling in deep networks", "journal": "", "year": "2018", "authors": "Faraz Saeedan; Nicolas Weber; Michael Goesele; Stefan Roth"}, {"ref_id": "b48", "title": "Very deep multilingual convolutional neural networks for lvcsr", "journal": "IEEE", "year": "2016", "authors": "Tom Sercu; Christian Puhrsch; Brian Kingsbury; Yann Lecun"}, {"ref_id": "b49", "title": "Modulation spectra of natural sounds and ethological theories of auditory processing", "journal": "The Journal of the Acoustical Society of America", "year": "2003", "authors": "C Nandini;  Singh;  Fr\u00e9d\u00e9ric E Theunissen"}, {"ref_id": "b50", "title": "Automatic acoustic detection of birds through deep learning: the first Bird Audio Detection challenge", "journal": "", "year": "2018", "authors": "Dan Stowell; , -Mike Wood; Hanna Pamu\u0142a; Yannis Stylianou; Herv\u00e9 Glotin"}, {"ref_id": "b51", "title": "Adaptive attention span in transformers", "journal": "", "year": "2019", "authors": "Sainbayar Sukhbaatar; \u00c9douard Grave; Piotr Bojanowski; Armand Joulin"}, {"ref_id": "b52", "title": "Going deeper with convolutions", "journal": "", "year": "2015", "authors": "Christian Szegedy; Wei Liu; Yangqing Jia; Pierre Sermanet; Scott Reed; Dragomir Anguelov; Dumitru Erhan; Vincent Vanhoucke; Andrew Rabinovich"}, {"ref_id": "b53", "title": "Self-supervised audio representation learning for mobile devices", "journal": "", "year": "2019", "authors": "Marco Tagliasacchi; Beat Gfeller"}, {"ref_id": "b54", "title": "Learning to resize images for computer vision tasks", "journal": "ICCV", "year": "2021", "authors": "Hossein Talebi; Peyman Milanfar"}, {"ref_id": "b55", "title": "Efficientnet: Rethinking model scaling for convolutional neural networks", "journal": "PMLR", "year": "2019", "authors": "Mingxing Tan; Quoc Le"}, {"ref_id": "b56", "title": "Mnasnet: Platform-aware neural architecture search for mobile", "journal": "", "year": "2019", "authors": "Mingxing Tan; Bo Chen; Ruoming Pang; Vijay Vasudevan; Mark Sandler; Andrew Howard; Quoc V Le"}, {"ref_id": "b57", "title": "Conditional image generation with pixelcnn decoders", "journal": "", "year": "2016", "authors": "A\u00e4ron Van Den Oord; Nal Kalchbrenner; Lasse Espeholt; K Kavukcuoglu; Oriol Vinyals; A Graves"}, {"ref_id": "b58", "title": "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition", "journal": "", "year": "2018", "authors": "Pete Warden"}, {"ref_id": "b59", "title": "Understanding straight-through estimator in training activation quantized neural nets", "journal": "", "year": "2019", "authors": " P Yin;  Lyu; S Zhang; Y Y Osher; J Qi;  Xin"}, {"ref_id": "b60", "title": "Fully convolutional speech recognition", "journal": "", "year": "2018", "authors": "Neil Zeghidour; Qiantong Xu; Vitaliy Liptchinsky; Nicolas Usunier; Gabriel Synnaeve; Ronan Collobert"}, {"ref_id": "b61", "title": "Hartley spectral pooling for deep learning", "journal": "", "year": "2018", "authors": "Hao Zhang; Jianwei Ma"}, {"ref_id": "b62", "title": "Making convolutional networks shift-invariant again", "journal": "", "year": "2003", "authors": "Richard Zhang"}, {"ref_id": "b63", "title": "Neural architecture search with reinforcement learning", "journal": "", "year": "2017", "authors": "Barret Zoph; Quoc V Le"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Residual block with a shared DiffStride layer.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Comparison side by side of the shortcut blocks in classic ResNet architectures with strided convolutions, and with DiffStride that learns the strides of the block.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: Learning dynamics of DiffStride on the CIFAR10 dataset.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 :4Figure 4: Top-1 accuracy (%) on the Imagenet validation set as a function of the regularization term J((S l ) l=L l=1 ) as defined in equation 6, after training with \u03bb \u2208 [0.1, 10].", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Distribution of the final global striding factors for the different initializations.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure A. 1 :1Figure A.1: Learned strides by DiffStride on the CIFAR100 dataset.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Distribution of the final global striding factors for the different initializations.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure A. 2 :2Figure A.2: Learned strides by DiffStride on the Imagenet dataset.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Test accuracy (% \u00b1 sd over 3 runs) for audio classification in the single (one model per task) and multi-task (one model for all tasks) settings.", "figure_data": "Learned StridesEquivalent cut-off frequenciesTimeFrequencyTime (Hz)Frequency (Cyc/Mel)Acoustic scenes1.89 \u00b1 0.05 1.99 \u00b1 0.03 26.25 \u00b1 0.630.2448 \u00b1 0.009Birdsong detection 1.91 \u00b1 0.02 1.96 \u00b1 0.01 25.83 \u00b1 0.360.2500 \u00b1 0.000Music (Instrument) 1.29 \u00b1 0.06 2.12 \u00b1 0.01 38.33 \u00b1 1.570.2292 \u00b1 0.009Music (Pitch)1.32 \u00b1 0.10 1.61 \u00b1 0.07 37.50 \u00b1 2.720.3021 \u00b1 0.018Speech commands1.97 \u00b1 0.00 1.95 \u00b1 0.01 25.00 \u00b1 0.000.2500 \u00b1 0.000Multi-task model1.46 \u00b1 0.01 1.79 \u00b1 0.03 34.17 \u00b1 0.300.2708 \u00b1 0.0074"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "\u00b1 0.26 69.01 \u00b1 0.19 69.66 \u00b1 0.06 88.5 \u00b1 0.15 88.48 \u00b1 0.02 89.07 \u00b1 0.03 (1, 1, 3, 1) 69.79 \u00b1 0.15 69.88 \u00b1 0.05 68.22 \u00b1 0.07 89.43 \u00b1 0.18 89.15 \u00b1 0.07 88.10 \u00b1 0.08 \u00b1 0.11 64.44 \u00b1 0.01 69.43 \u00b1 0.11 80.42 \u00b1 0.11 85.22 \u00b1 0.09 89.03 \u00b1 0.02 Mean accuracy 65.53 \u00b1 4.49 67.58 \u00b1 1.88 69.28 \u00b1 0.50 86.39 \u00b1 3.15 87.53 \u00b1 1.36 88.85 \u00b1 0.35", "figure_data": "Top-1Top-5Init. StridesStrided Conv.SpectralDiffStride Strided Conv.SpectralDiffStride(1, 2, 2, 2) 68.65 (1, 3, 1, 3) 68.86 \u00b1 0.28 68.63 \u00b1 0.08 69.41 \u00b1 0.16 88.64 \u00b1 0.15 88.42 \u00b1 0.01 88.98 \u00b1 0.04(2, 2, 2, 3)63.45 \u00b1 0.09 67.16 \u00b1 0.17 69.53 \u00b1 0.08 85.09 \u00b1 0.04 87.25 \u00b1 0.06 89.05 \u00b1 0.05(2, 3, 1, 2)65.35 \u00b1 0.03 66.35 \u00b1 0.24 69.42 \u00b1 0.06 86.27 \u00b1 0.05 86.67 \u00b1 0.15 88.91 \u00b1 0.05(3, 3, 2, 3)57.11"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ": Top-1 and top-5 accuracies (% \u00b1 sd over 3 runs) on Imagenet, (1, 2, 2, 2) being the config-uration of"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": ", showing that DiffStride finds configurations with a lower computational cost for the same accuracy. Some of these are quite extreme, e.g. with \u03bb = 10 a model converges to strides ((10.51, 32.23), (1.20, 2.68), (1.20, 2.04), (1.96, 4.53)) for a 58.57% top-1 accuracy. When training a ResNet-18 with strided convolutions using the closest integer strides", "figure_data": ""}, {"figure_label": "A", "figure_type": "table", "figure_id": "tab_5", "figure_caption": ".1: Datasets used for audio classification. Default train/test splits are always adopted.", "figure_data": "TaskNameClasses Train examples Test examplesAcoustic scenesTUT Urban 2018107,829810Birdsong detection DCASE2018232,1293,561Music (instrument) Nsynth11289,20512,678Music (pitch)Nsynth128289,20512,678Speech commandsSpeech commands3584,77110,700642S 1 h S 1 w S 2 h S 2 w S 3 h S 3 w(a) Distribution of learned strides inResNet-18 for the different initializations."}], "formulas": [{"formula_id": "formula_0", "formula_text": "F(x) mn = 1 \u221a HW H\u22121 h=0 W \u22121 w=0 x hw e \u22122\u03c0i( mh H + nw W ) , \u2200m \u2208 {0, . . . , H \u2212 1}, \u2200n \u2208 {0, . . . , W \u2212 1}. (1)", "formula_coordinates": [2.0, 112.31, 611.99, 391.69, 40.14]}, {"formula_id": "formula_1", "formula_text": "x \u2208 R H\u00d7W , y = F(x), \u2202L \u2202x = F * ( \u2202L \u2202y ) = F \u22121 ( \u2202L \u2202y ).(2)", "formula_coordinates": [3.0, 197.04, 135.95, 306.96, 22.31]}, {"formula_id": "formula_2", "formula_text": "w ) \u2208 [1, H) \u00d7 [1, W ).", "formula_coordinates": [3.0, 341.32, 531.2, 91.22, 9.65]}, {"formula_id": "formula_3", "formula_text": "H S h \u00d7 W Sw .", "formula_coordinates": [3.0, 257.68, 566.63, 42.57, 13.45]}, {"formula_id": "formula_4", "formula_text": "x = F \u22121 (\u1ef9) \u2208 R H S h \u00d7 W Sw .", "formula_coordinates": [3.0, 231.77, 581.08, 125.94, 13.79]}, {"formula_id": "formula_5", "formula_text": "mask h (S h ,H,R) (m) = min max 1 R (R + H 2S h \u2212 | H 2 \u2212 m|), 0 , 1 , m \u2208 [0, H] (3) mask w (Sw,W,R) (n) = min max 1 R (R + W 2S w + 1 \u2212 n), 0 , 1 , n \u2208 [0, W 2 + 1](4)", "formula_coordinates": [4.0, 141.88, 526.4, 362.12, 51.12]}, {"formula_id": "formula_6", "formula_text": "W(S h , S w , H, W, R) = mask h (S h ,H,R) \u2297 mask w (Sw,W,R)(5)", "formula_coordinates": [4.0, 193.69, 625.04, 310.31, 13.71]}, {"formula_id": "formula_7", "formula_text": "H S h + 2 \u00d7 R \u00d7 W Sw + 2 \u00d7 R ).", "formula_coordinates": [4.0, 267.25, 670.02, 128.87, 14.14]}, {"formula_id": "formula_8", "formula_text": "Inputs : Input x \u2208 R H\u00d7W , strides S = (S h , S w ) \u2208 [1, H) \u00d7 [1, W ), smoothness factor R. Output: Downsampled outputx \u2208 R H S h +2\u00d7R \u00d7 W Sw +2\u00d7R 1 y \u2190\u2212 F(x)", "formula_coordinates": [5.0, 100.03, 101.45, 371.23, 36.05]}, {"formula_id": "formula_9", "formula_text": "3 y masked \u2190\u2212 y \u2022 mask", "formula_coordinates": [5.0, 100.03, 150.68, 93.29, 9.81]}, {"formula_id": "formula_10", "formula_text": "3x3 Conv Strides=(2,2) 3x3 Conv Strides=(1,1) 3x3 Conv Strides=(2,2) (a)", "formula_coordinates": [5.0, 171.05, 218.45, 105.78, 87.56]}, {"formula_id": "formula_11", "formula_text": "3x3 Conv Strides=(1,1) 3x3 Conv Strides=(1,1) 3x3 Conv Strides=(1,1)", "formula_coordinates": [5.0, 333.86, 216.06, 94.19, 51.34]}, {"formula_id": "formula_12", "formula_text": "S = (S h , S w ) to remain in [1, H) \u00d7 [1, W ).", "formula_coordinates": [5.0, 259.6, 394.72, 186.36, 9.65]}, {"formula_id": "formula_13", "formula_text": "H l \u00d7 W l = H l\u22121 S l\u22121 h + 2 \u00d7 R \u00d7 W l\u22121 S l\u22121 w + 2 \u00d7 R ,", "formula_coordinates": [6.0, 306.71, 188.68, 197.29, 18.51]}, {"formula_id": "formula_14", "formula_text": "l=L l=1 l i=1 1 S i h \u00d7S i w .", "formula_coordinates": [6.0, 129.39, 240.83, 71.95, 16.98]}, {"formula_id": "formula_15", "formula_text": "\u03bbJ((S l ) l=L l=1 ) = \u03bb l=L l=1 l i=1 1 S i h \u00d7 S i w , (6", "formula_coordinates": [6.0, 233.65, 282.63, 266.48, 30.55]}, {"formula_id": "formula_16", "formula_text": ")", "formula_coordinates": [6.0, 500.13, 293.36, 3.87, 8.64]}, {"formula_id": "formula_17", "formula_text": "Strides Layers S 0 h S 0 w S 1 h S 1 w S 2 h S 2 w (a) Trajectories of the different strides (S l ) l\u2208(1,2,3) for a single run. S 1 h S 1 w S 2 h S 2 w S 3 h S", "formula_coordinates": [8.0, 124.71, 274.92, 230.61, 111.97]}], "doi": "10.5281/zenodo.1228142"}