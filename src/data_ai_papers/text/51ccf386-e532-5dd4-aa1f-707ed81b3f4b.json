{"title": "Faster Minimum Bayes Risk Decoding with Confidence-based Pruning", "authors": "Julius Cheng; Andreas Vlachos", "pub_date": "", "abstract": "Minimum Bayes risk (MBR) decoding outputs the hypothesis with the highest expected utility over the model distribution for some utility function. It has been shown to improve accuracy over beam search in conditional language generation problems and especially neural machine translation, in both human and automatic evaluations. However, the standard samplingbased algorithm for MBR is substantially more computationally expensive than beam search, requiring a large number of samples as well as a quadratic number of calls to the utility function, limiting its applicability. We describe an algorithm for MBR which gradually grows the number of samples used to estimate the utility while pruning hypotheses that are unlikely to have the highest utility according to confidence estimates obtained with bootstrap sampling. Our method requires fewer samples and drastically reduces the number of calls to the utility function compared to standard MBR while being statistically indistinguishable in terms of accuracy. We demonstrate the effectiveness of our approach in experiments on three language pairs, using chrF++ and COMET as utility/evaluation metrics.", "sections": [{"heading": "Introduction", "text": "Minimum Bayes risk (MBR) decoding (Bickel and Doksum, 1977;Goel and Byrne, 2000) has recently gained renewed attention as a decision rule for conditional sequence generation tasks, especially neural machine translation (NMT). In MBR, the sequence with the highest expected utility with respect to thez model distribution is chosen as the output, where the utility is usually some measure of text similarity. This contrasts with the more commonly used maximum a posteriori (MAP) decision rule, which returns the sequence with the highest probability under the model. MAP is generally intractable, and beam search is typically used to find an approximation. MBR is likewise intractable, and Eikema and Aziz (2020) propose an samplingbased approximation algorithm.\nMBR has been shown to outperform MAP beam search in both automatic and qualitative evaluation in a diverse range of tasks (Suzgun et al., 2023), including NMT (Freitag et al., 2022a) and code generation (Shi et al., 2022). MBR also generalizes other previously proposed decoding methods and explains their success (Bertsch et al., 2023).\nThe accuracy improvement from MBR comes at a heavy cost: the number of samples used can reach thousands (Freitag et al., 2023), and the number of calls to the utility function required is quadratic in the number of samples. Often, the utility function itself is a deep neural model, rendering MBR prohibitively expensive for many use cases.\nIn this work, we address the computational efficiency of MBR with an iterative pruning algorithm where low-performing hypotheses are removed while the number of samples used to estimate utilities grows. Hypotheses are pruned based on their estimated probability of being the true best hypothesis under the MBR objective, thus avoiding making expensive fine-grained utility estimates for hypotheses which are unlikely to be the final prediction.\nIn NMT experiments on three language pairs using chrF++ (Popovi\u0107, 2015), and COMET (Rei et al., 2020) as MBR utility and evaluation metrics, we show that our method maintains the same level of accuracy as standard MBR while reducing the number of utility calls by a factor of at least 7 for chrF++ and 15 for COMET. Our algorithm can also use fewer samples to reach a prediction by terminating early, unlike standard MBR.", "publication_ref": ["b1", "b12", "b4", "b21", "b9", "b0", "b8", "b16", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Minimum Bayes risk decoding", "text": "Conditional sequence generation problems such as neural machine translation (NMT) model the probability of the next token y t given a source sequence x and prefix y <t with a neural network p \u03b8 . This model can be used to assign probabilities to full sequences p \u03b8 (y|x) via the chain rule.\nAt test time, a decision rule is employed to select a single \"best\" sequence. The most common decision rule is to return the highest probability sequence y M AP = arg max y p \u03b8 (y|x). The exact solution is generally intractable, and beam search is used to find an approximation.\nIn contrast, minimum Bayes risk decoding (MBR) (Goel and Byrne, 2000) outputs:\ny M BR = arg max y E\u0233 \u223cp \u03b8 (\u2022|x) [u(y,\u0233)] = arg max y U (y, p \u03b8 (\u2022|x)),(1)\nfor some utility function u, a measure of similarity between sequences, and\nU (y, Y) = E\u0177 \u223cY [u(y,\u0177)],\nwhere Y is either a probability distribution or an array of samples. We call U the expected utility function. Eikema and Aziz (2020) propose a sampling method for neural language models where hypotheses H and pseudo-references R are generated with unbiased sampling, and y M BR is estimated as:\ny M BR \u2248 arg max y\u2208H U (y, R).(2)\nThis method, which we refer to as \"standard\" MBR, requires |H| + |R| samples (assuming H \u0338 = R) and |H||R| calls to u. The latter is the main computational bottleneck which we address in this work.\nRecent works on MBR focus on identifying accurate and efficient generation methods (Eikema and Aziz, 2021;Freitag et al., 2023;Yan et al., 2022), and pruning H to a smaller size with a faster method prior to running standard MBR (Eikema and Aziz, 2021;Fernandes et al., 2022). Freitag et al. (2022a) and Eikema and Aziz (2021) show that MBR is more effective relative to MAP when the utility metric has high segmentlevel accuracy as measured by Freitag et al. (2021). So, we use COMET (Rei et al., 2020), one of the best available metrics for NMT, and chrF++ (Popovi\u0107, 2015), as a simpler and faster yet reasonably good lexical metric. We heed the call of Freitag et al. (2022b) and do not use BLEU, which is obsoleted by newer metrics as both an evaluation and utility metric (Freitag et al., 2022a).", "publication_ref": ["b12", "b4", "b5", "b8", "b23", "b5", "b6", "b9", "b5", "b19", "b16", "b10", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Confidence-based hypothesis pruning", "text": "Sampling-based MBR returns the highest-utility hypothesis from a set measured over pseudoreferences sampled from the model. Speedups can be achieved if low-performing hypotheses are removed from consideration based on coarse utility estimates obtained from a subset of the pseudoreferences. In other words, we can save time by not computing precise utility estimates for hypotheses which are unlikely to be chosen in the end.\nWe propose an iterative algorithm for MBR where the hypothesis set is gradually shrunk while the pseudo-reference list grows. The procedure is shown in Algorithm 1. We start with an initial hypothesis set H 1 , and at each time step t, a pruning function uses a pseudo-reference list R t of size r t to select H t+1 \u2286 H t . After the maximum time step is reached or when the current hypothesis set contains one element, terminate and return the highest utility hypothesis under all available pseudo-references. The size of R t grows according to a pre-defined \"schedule\" r 1 , ..., r T . \nU (y, R t\u22121 )\nThe goal of the pruning function is to exclude as many hypotheses as possible to reduce the number of utility calls made in the future without excluding the true top-ranking MBR hypothesis arg max y\u2208Ht U (y, p \u03b8 (\u2022|x)), the true \"winner\".\nWe propose to prune hypotheses in H t with low probability of being the true winner and to estimate this probability using nonparametric bootstrap resampling (Efron, 1979;Koehn, 2004); given an initial collection of i.i.d. samples S from an unknown distribution X , our beliefs about the true value of any statistic T (X ) are represented by the distribu-tion p(T (\u015c)), where\u015c \u223c boot(S) , and boot(S) returns a with-replacement size-|S| resample of S.\nIn our case, we want to estimate the probability that y is the true winner in H t :\np \u0233\u2208H U (y, p \u03b8 (\u2022|x)) \u2265 U (\u0233, p \u03b8 (\u2022|x)) , (3)\nwhich we estimate as the chance that y wins in a bootstrap resample. LetR t \u223c boot(R t ). Then the bootstrap estimator is:\n\u00ca Rt\u223cboot(Rt) 1 \u0233\u2208Ht (U (y,R t ) \u2265 U (\u0233,R t ) . (4)\nThis estimator can be high variance because the probability y winning in a bootstrap sample is very small when H t is large, so instead we use the probability that y outranks a particular\u0233 \u2208 H t :\n\u00ca Rt\u223cboot(Rt) 1(U (y,R t ) \u2265 U (\u0233,R t )) . (5)\nThis statistic is invariant to the size of H t because it only considers utility estimates of y and y. It is an upper bound of Equation 4 because the probability of y winning against all\u0233 \u2208 H t cannot be higher than the probability of winning against a particular\u0233 \u2208 H t .\u0233 can be any element in H t , but we set it to the winner under R t , i.e. y = arg max\u0233 \u2208Ht U (\u0233, R t ), to achieve a tighter upper bound.\nWe propose to prune y if its estimated probability of beating\u0233 is less than 1 \u2212 \u03b1, where \u03b1 is a confidence threshold 0 \u2264 \u03b1 \u2264 1. In summary, this procedure prunes some but not necessarily all y \u2208 H t which are estimated to have less than 1 \u2212 \u03b1 chance of being the true winner. Algorithm 2 shows the procedure in detail.\nNote that bootstrapped utility estimates do not require additional utility function calls because they reuse utility values already computed over H t , R t . Also note that the bootstrap estimator is always biased because R t is never equal to p \u03b8 (\u2022|x), and the bias is worse when R t is small. Nonetheless, we show empirically that bootstrapping is effective in our pruning algorithm for modest sizes of R t .\nAnother benefit of our pruning method compared to standard MBR is that it can terminate early if H t has only one remaining hypothesis, reducing the total number of pseudo-references needed.\nAs a baseline pruning function for comparison, we rank each y \u2208 H t by U (y, R t ) and prune the \nw \u2190 1 n n i 1(U (y,R i ) \u2265 U (\u0233,R i ))\n9:\nif w > 1 \u2212 \u03b1 then 10:\nH new \u2190 H new \u222a {y} 11:\nend if 12: end for 13: return H new bottom-\u03b2 proportion. At \u03b2 = 0, no pruning occurs and standard MBR decoding is recovered. We refer to this baseline as prune \u03b2 and our confidencebased method as prune \u03b1 .", "publication_ref": ["b3", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We perform our experiments on NMT models which we train on the German-English (de-en), English-Estonian (en-et), and Turkish-English (tren) news datasets from WMT18. We use the data preprocessing steps provided by the WMT18 organizers, except that we exclude Paracrawl from the de-en dataset following Eikema and Aziz (2021). The final training sets have 5.8 million, 1.9 million, and 207 thousand sentence pairs respectively. All models are transformers of the base model size from Vaswani et al. (2017) and are trained without label smoothing until convergence.\nFor all language pairs and validation/test datasets, we generate H 1 with beam top-k with k = 256 1 . We generate 1024 pseudo-references R * with epsilon sampling (Hewitt et al., 2022) with \u03f5 = 0.02, following Freitag et al. (2023). In order to run multiple random trials efficiently, we simulate sampling pseudo-references by sampling from R * without replacement. The experiments in Sections 4.1 and 4.2 show averaged results from 10 trials. We use chrF++ and COMET as utility func-tions and always match the evaluation metric to the utility. chrF++ is computed using SacreBLEU (Post, 2018) with default settings. COMET is computed with the COMET-22 model from . We use 500 bootstrap samples for prune \u03b1 .\nFor the pseudo-reference sample size schedule r 1 , ..., r T , the choice of r 1 is pivotal in the speedaccuracy trade-off; |H 1 ||R 1 | is a lower bound on the number of utility calls needed, but the bootstrap estimate is more biased when the sample size is small. In a preliminary experiment on the validation set, we measure the \"false pruning rate\", the rate that the estimated true winner, arg max\u0233 \u2208H U (\u0233, R * ) is pruned under different choices of \u03b1 and |R|. Based on results shown in Figure 1, we set r 1 to 8 for COMET and 16 for chrF++ for all experiments. r 2 , ..., r T are set by doubling at each time step until reaching 256.\nMore experimental details and figures for language pairs not shown here are in the Appendix. Our code is publicly available 2 . 4.1 Speed-accuracy trade-off prune \u03b1 and prune \u03b2 allow control over the speedaccuracy trade-off with a single parameter. We observe this trade-off over the validation set by comparing the number of utility function calls made against various measures of accuracy. High evaluation score is underlying goal, but we find that the score changes very little across settings, so we also evaluate pruning quality in terms of how well final predictions match those under standard MBR with R * . We use exact accuracy, whether the prediction y equals arg max\u0233 \u2208H U (\u0233, R * ), and reciprocal rank (RR), equal to ( \u0233\u2208H 1(U (\u0233, R * ) \u2265 U (y, R * ))) \u22121 as a soft accuracy measure adapted from the mean reciprocal rank used in search (Craswell, 2009). Figure 2 shows that prune \u03b1 generally outperforms prune \u03b2 on all metrics. ", "publication_ref": ["b5", "b22", "b13", "b8", "b17"], "figure_ref": ["fig_0", "fig_1"], "table_ref": []}, {"heading": "Test results", "text": "We evaluate our methods on the test set with \u03b1 = 0.99 and \u03b1 = 0.9 and compare them to standard MBR on the accuracy metrics described in Section 4.1 as well as the number of utility calls and pseudo-references used. Table 1 shows that across all language pairs and metrics, our method achieves similar evaluation scores as standard MBR while using much less computation, with the most dramatic case being en-et and COMET with \u03b1 = 0.9 which uses 3.5% as many utility calls and 32% as many pseudo-references as the baseline.\nWhen comparing \u03b1 = 0.99 with \u03b1 = 0.9, we see that while exact accuracy and RR differ, the evaluation score differs very little if at all, suggesting that high-ranking hypotheses are often equally good as one another, and finely discriminating between them has diminishing returns.\nMetric: chrF++ de-en en-et tr-en \u03b2 = 0 \u03b1 = 0.99 \u03b1 = 0.9 \u03b2 = 0 \u03b1 = 0.99 \u03b1 = 0.9 \u03b2 = 0 \u03b1 = 0.99 \u03b1 = 0.9 \u03b2 = 0 \u03b1 = 0.99 \u03b1 = 0.9 \u03b2 = 0 \u03b1 = 0.99 \u03b1 = 0.9 \u03b2 = 0 \u03b1 = 0.99 \u03b1 = 0.9  \nScore", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Human evaluation", "text": "We confirm that our method is indistinguishable from standard MBR in human evaluation. On the de-en test set, for each instance, we sample 256 pseudo-references without replacement from R * and use this pool to decode with both standard MBR and prune \u03b1 , \u03b1 = 0.99. 85% of the predictions are the same, and for the rest, we asked bilingual readers of German and English to state which prediction they preferred. We obtained 125 ratings.\nThe standard MBR prediction won in 48 cases, lost in 42, and tied in 35. This fails the significance test of Koehn (2004), so we conclude that prune \u03b1 with \u03b1 = 0.99 is not significantly different from standard MBR on de-en.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "Run times", "text": "To measure realistic run times, we implement a practical version of our algorithm and compare our method against standard MBR decoding and beam search. We run our algorithm with COMET as the utility function and \u03b1 = 0.99. With COMET, sentence embeddings can be cached, which greatly speeds up utility function calls. Some of the theoretical gains seen in Section 4.2 are diminished in practice due to our iterative algorithm dividing computations over smaller batches. Our best implementation samples all 256 pseudo-references at once but generates pseudo-reference sentence embeddings as needed. We run each algorithm on a set of 100 randomly sampled test instances. Further implementation details are given in Appendix A.2.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We propose an iterative pruning algorithm for MBR along with a pruning criterion based on confidence estimates derived from bootstrap resampling. In experiments across diverse language pairs and metrics, we show that our method consistently outperforms our proposed baseline and achieves significant computational savings over standard samplingbased MBR without sacrificing accuracy. Our method is a drop-in substitute for standard MBR that requires no knowledge about the model p \u03b8 , how H 1 is generated, or the utility function.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Even with our pruning algorithm, MBR is many times more costly to run than beam search. More An important hyperparameter in our method is the sample size schedule. We show why it is important to carefully choose the size of the first sample, but not how the remaining schedule should be set, opting to simply double the size at each step. We leave this issue to future work.\nMethods such as MBR and reranking that directly optimize a metric may exploit noise in the metric to improve the score without actually improving quality (Fernandes et al., 2022). In these settings, automatic evaluation is less trustworthy and should ideally be combined with human evaluation. However, human evaluation is difficult and expensive to obtain. Figure 6: Speed-accuracy trade-off curves of pruning functions for \u03b1 \u2208 0.8, 0.9, 0.95, 0.98, 0.99 and \u03b2 \u2208 {0.05, ..., 0.95} on the tr-en validation set.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "Julius Cheng is supported by a scholarship from Huawei. The authors would like to thank the bilingual readers who helped with the human evaluation and the anonymous reviewers for their helpful suggestions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Additional experimental details A.1 All experiments", "text": "Preliminary experiments showed no significant difference between 500 and 1000 bootstrap samples when running prune \u03b1 , so we use 500 for all experiments.\nFor efficiency, we use average sentence-level chrF++ instead of corpus-level chrF++ for corpuslevel evaluations. This allows us to pre-compute the sentence-level chrF++ for each hypothesis and obtain the corpus-level score of a set of predictions by simple averaging.\nAll experiments are implemented on top of our fork of Fairseq (Ott et al., 2019).", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Run times", "text": "This section contains additional details for the experiment in Section 4.4.\nFor both the standard and pruning MBR algorithms, we deduplicate and cache computations whenever possible. For each unique pseudoreference, its sentence embedding and utility scores against each y \u2208 H t are only computed once.\nFor simplicity, all decoding methods are run on one sequence at a time. Batching across sequences would likely affect the relative performance characteristics of each method.\nAll experiments are conducted on the same machine with one Nvidia Quadro RTX 8000 GPU.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B False pruning rates for en-et, tr-en", "text": "Figure 3 shows the false pruning rates for en-et and tr-en. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Hypotheses remaining per time step", "text": "Figure 4 shows the distribution of the number of remaining hypotheses after each time step when running our method on the de-en validation set, following the experimental setup of Section 4.1 where |H 1 | = 256. This is provided to further illustrate the pruning process.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "It's mbr all the way down: Modern generation techniques through the lens of minimum bayes risk", "journal": "", "year": "2023", "authors": "Amanda Bertsch; Alex Xie; Graham Neubig; Matthew R Gormley"}, {"ref_id": "b1", "title": "Mathematical Statistics: Basic Ideas and Selected Topics. Holden-Day Company", "journal": "", "year": "1977", "authors": "P J Bickel; K A Doksum"}, {"ref_id": "b2", "title": "Mean Reciprocal Rank", "journal": "Springer US", "year": "2009", "authors": ""}, {"ref_id": "b3", "title": "Bootstrap Methods: Another Look at the Jackknife. The Annals of Statistics", "journal": "", "year": "1979", "authors": "B Efron"}, {"ref_id": "b4", "title": "Is MAP decoding all you need? the inadequacy of the mode in neural machine translation", "journal": "", "year": "2020", "authors": "Bryan Eikema; Wilker Aziz"}, {"ref_id": "b5", "title": "Sampling-based minimum bayes risk decoding for neural machine translation", "journal": "", "year": "2021", "authors": "Bryan Eikema; Wilker Aziz"}, {"ref_id": "b6", "title": "Quality-aware decoding for neural machine translation", "journal": "", "year": "2022", "authors": "Patrick Fernandes; Ant\u00f3nio Farinhas; Ricardo Rei; Jos\u00e9 De Souza; Perez Ogayo; Graham Neubig; Andre Martins"}, {"ref_id": "b7", "title": "Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "journal": "", "year": "", "authors": ""}, {"ref_id": "b8", "title": "Epsilon sampling rocks: Investigating sampling strategies for minimum bayes risk decoding for machine translation", "journal": "", "year": "2023", "authors": "Markus Freitag; Behrooz Ghorbani; Patrick Fernandes"}, {"ref_id": "b9", "title": "High quality rather than high model probability: Minimum Bayes risk decoding with neural metrics", "journal": "Transactions of the Association for Computational Linguistics", "year": "2022", "authors": "Markus Freitag; David Grangier; Qijun Tan; Bowen Liang"}, {"ref_id": "b10", "title": "Results of WMT22 metrics shared task: Stop using BLEU -neural metrics are better and more robust", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Markus Freitag; Ricardo Rei; Nitika Mathur; Chi-Kiu Lo; Craig Stewart; Eleftherios Avramidis; Tom Kocmi; George Foster; Alon Lavie; Andr\u00e9 F T Martins"}, {"ref_id": "b11", "title": "Alon Lavie, and Ond\u0159ej Bojar. 2021. Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain", "journal": "", "year": "", "authors": "Markus Freitag; Ricardo Rei; Nitika Mathur; Chi-Kiu Lo; Craig Stewart; George Foster"}, {"ref_id": "b12", "title": "Minimum bayes-risk automatic speech recognition", "journal": "Computer Speech & Language", "year": "2000", "authors": "Vaibhava Goel; J William;  Byrne"}, {"ref_id": "b13", "title": "Truncation sampling as language model desmoothing", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "John Hewitt; Christopher Manning; Percy Liang"}, {"ref_id": "b14", "title": "Statistical significance tests for machine translation evaluation", "journal": "Association for Computational Linguistics", "year": "2004", "authors": "Philipp Koehn"}, {"ref_id": "b15", "title": "fairseq: A fast, extensible toolkit for sequence modeling", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Myle Ott; Sergey Edunov; Alexei Baevski; Angela Fan; Sam Gross; Nathan Ng; David Grangier; Michael Auli"}, {"ref_id": "b16", "title": "chrF: character n-gram F-score for automatic MT evaluation", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Maja Popovi\u0107"}, {"ref_id": "b17", "title": "A call for clarity in reporting BLEU scores", "journal": "", "year": "2018", "authors": "Matt Post"}, {"ref_id": "b18", "title": "COMET-22: Unbabel-IST 2022 submission for the metrics shared task", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Ricardo Rei; G C Jos\u00e9; Duarte Souza; Chrysoula Alves; Ana C Zerva; Taisiya Farinha; Alon Glushkova; Luisa Lavie; Andr\u00e9 F T Coheur;  Martins"}, {"ref_id": "b19", "title": "COMET: A neural framework for MT evaluation", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Ricardo Rei; Craig Stewart; Ana C Farinha; Alon Lavie"}, {"ref_id": "b20", "title": "2022. Natural language to code translation with execution", "journal": "Association for Computational Linguistics", "year": "", "authors": "Freda Shi; Daniel Fried; Marjan Ghazvininejad; Luke Zettlemoyer; Sida I Wang"}, {"ref_id": "b21", "title": "Follow the wisdom of the crowd: Effective text generation via minimum Bayes risk decoding", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Mirac Suzgun; Luke Melas-Kyriazi; Dan Jurafsky"}, {"ref_id": "b22", "title": "Attention is all you need", "journal": "Curran Associates, Inc", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Illia Kaiser;  Polosukhin"}, {"ref_id": "b23", "title": "Dc-mbr: Distributional cooling for minimum bayesian risk decoding", "journal": "", "year": "2022", "authors": "Jianhao Yan; Jin Xu; Fandong Meng; Jie Zhou; Yue Zhang"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: False pruning rates for different choices of \u03b1 and |R| measured on the validation set.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Speed-accuracy trade-off curves of pruning functions for \u03b1 \u2208 0.8, 0.9, 0.95, 0.98, 0.99 and \u03b2 \u2208 {0.05, ..., 0.95} on the de-en validation set. The x-axes are truncated for better visual comparison.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Number of remaining hypotheses after each time step while running the pruning MBR algorithm for various choices of \u03b1 on the de-en validation set. The x-axis is the number of pseudo-references at a time step, and the y-axis is the number of hypotheses remaining after pruning. Colored bars show the mean, and error bars show the interquartile range.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Algorithm 2 Confidence-based pruning function Input: Hypothesis set H, pseudo-reference list R. Constants: Expected utility function U , confidence threshold \u03b1, number of bootstrap samples n. Output: A subset of H.", "figure_data": "3: H 1 \u2190 gen(x, \u03b8) 4: for i \u2190 1 to n do 5:R i \u2190 boot(R) 6: end for 7: for y \u2208 H do 8:\u0233, R)"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Statistics for MBR decoding on the test set for all language pair and metric settings. \u03b2 = 0 indicates standard MBR. All values are averaged across 10 random trials.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "provides a detailed breakdown of run times. As expected, our method achieves the greatest time savings on utility computation. However, it is still substantially slower than beam search.", "figure_data": "Run time (seconds)Prune MBR Std. MBR BeamGenerate H Generate R Embed sentences Compute utilities Bootstrap pruning36.56 59.92 100.50 12.96 0.6336.56 26.79 59.92 108.33 193.01Total210.57397.82 26.79"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Summary of run times for decoding methods on 100 sentences from the de-en test set for pruning MBR (\u03b1 = 0.99), standard MBR, and beam search with beam size 10, averaged over 3 trials.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "y M BR = arg max y E\u0233 \u223cp \u03b8 (\u2022|x) [u(y,\u0233)] = arg max y U (y, p \u03b8 (\u2022|x)),(1)", "formula_coordinates": [2.0, 98.62, 216.27, 191.24, 45.92]}, {"formula_id": "formula_1", "formula_text": "U (y, Y) = E\u0177 \u223cY [u(y,\u0177)],", "formula_coordinates": [2.0, 175.14, 284.18, 115.36, 20.51]}, {"formula_id": "formula_2", "formula_text": "y M BR \u2248 arg max y\u2208H U (y, R).(2)", "formula_coordinates": [2.0, 118.88, 392.14, 170.99, 21.86]}, {"formula_id": "formula_3", "formula_text": "U (y, R t\u22121 )", "formula_coordinates": [2.0, 417.8, 578.38, 51.34, 18.93]}, {"formula_id": "formula_4", "formula_text": "p \u0233\u2208H U (y, p \u03b8 (\u2022|x)) \u2265 U (\u0233, p \u03b8 (\u2022|x)) , (3)", "formula_coordinates": [3.0, 94.31, 139.59, 195.55, 25.34]}, {"formula_id": "formula_5", "formula_text": "\u00ca Rt\u223cboot(Rt) 1 \u0233\u2208Ht (U (y,R t ) \u2265 U (\u0233,R t ) . (4)", "formula_coordinates": [3.0, 77.7, 230.25, 212.17, 28.09]}, {"formula_id": "formula_6", "formula_text": "\u00ca Rt\u223cboot(Rt) 1(U (y,R t ) \u2265 U (\u0233,R t )) . (5)", "formula_coordinates": [3.0, 86.82, 333.2, 203.05, 27.06]}, {"formula_id": "formula_7", "formula_text": "w \u2190 1 n n i 1(U (y,R i ) \u2265 U (\u0233,R i ))", "formula_coordinates": [3.0, 341.06, 249.19, 162.56, 21.48]}, {"formula_id": "formula_8", "formula_text": "H new \u2190 H new \u222a {y} 11:", "formula_coordinates": [3.0, 307.78, 278.84, 145.52, 24.53]}, {"formula_id": "formula_9", "formula_text": "Score", "formula_coordinates": [5.0, 94.61, 230.49, 20.42, 10.81]}], "doi": "10.1007/978-0-387-39940-9_488"}