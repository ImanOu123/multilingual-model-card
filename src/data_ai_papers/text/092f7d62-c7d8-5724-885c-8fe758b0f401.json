{"title": "What's the Meaning of Superhuman Performance in Today's NLU?", "authors": "Simone Tedeschi; Johan Bos; Thierry Declerck; Jan Haji\u010d; Daniel Hershcovich; Eduard H Hovy; Alexander Koller; Simon Krek; Steven Schockaert; Rico Sennrich; Ekaterina Shutova; Roberto Navigli", "pub_date": "", "abstract": "In the last five years, there has been a significant focus in Natural Language Processing (NLP) on developing larger Pretrained Language Models (PLMs) and introducing benchmarks such as SuperGLUE and SQuAD to measure their abilities in language understanding, reasoning, and reading comprehension. These PLMs have achieved impressive results on these benchmarks, even surpassing human performance in some cases. This has led to claims of superhuman capabilities and the provocative idea that certain tasks have been solved. In this position paper, we take a critical look at these claims and ask whether PLMs truly have superhuman abilities and what the current benchmarks are really evaluating. We show that these benchmarks have serious limitations affecting the comparison between humans and PLMs and provide recommendations for fairer and more transparent benchmarks.", "sections": [{"heading": "Introduction", "text": "In recent years, research in the field of Natural Language Processing (NLP) has been driven by a frantic race to reach the top spot in popular benchmarks (Wang et al., 2018(Wang et al., , 2019Lai et al., 2017;Rajpurkar et al., 2018;Reddy et al., 2019). Typically the race takes the shape of a rapid cycle of parameter tuning updates by several teams, communicating their results using a shared leaderboard. Not infrequently, systems achieve better-than-human performance on several tasks (see Figure 1). Yet what does this level of performance really mean for NLP? The impressive capabilities of ChatGPT make this question even more urgent. It is relatively easy to outperform humans with simple procedural tasks like arithmetic and extreme memory-intensive tasks involving vast amounts of data. But most tasks involving natural language typically require knowledge and inference. Do high-performing NLP algorithms really have (super)human capabilities? Or are the metrics that deliver these scores suspect?\nGiven the impact of claiming superhuman performance, it is important for researchers to understand exactly what is going on. As many in NLP have experienced, the false sense of accomplishment of superhuman performance often leads to an abrupt disappointment when a supposedly superb system is applied to realistic data in a real-world situation. By propounding unrealistic claims, NLP researchers harm themselves and the field as a whole. Some problems result from the metrics used to assess systems, which are invariably automated, and the data these metrics employ, which may be skewed in various ways. The metrics might give incomplete or biased reports of performance, or simply not apply in certain situations.\nOther problems arise from the 'boundary parameters' that shape the task, which are usually not adequately reflected in the evaluation metric, very seldom in the kinds of automated metrics used in leaderboards. Specifically, the correctness of a task setup and its dataset instances should not be taken for granted. Also, humans and machines are often evaluated under different conditions, such as the level and type of knowledge provided to perform the task and the test items used to compute performance.\nYet other problems result from the nature of leaderboard-based evaluation. Despite the obvious benefit of driving development through competition with little human effort, these evaluations typically do not foster understanding. Teams driven by a rapid evaluation turnaround cycle in a competitive mode tend to focus more on quantitative results than on error analyses which aim at improving awareness of their problem. As currently set up, benchmarks and comparisons do not incentivize a deeper understanding of the systems' performance, nor do they foster research geared towards producing automatic explanations: it is one thing to produce a numerical system performance score, but quite another to rate the adequacy and understandability of an explanation.\nIn this paper, we explore the interpretation of the superhuman performance and the utility of leaderboards, discuss how human performance is actually computed in a range of tasks, and how requirements differ for humans and automatic systems across tasks. We hope to encourage leaderboard creators to be more circumspect when setting up their challenges and provide clear 'boundary conditions' and descriptions of the limitations of their evaluations.", "publication_ref": ["b71", "b70", "b33", "b58", "b60"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Popular Leaderboards Are Saturated", "text": "Leaderboard-based evaluation has become a popular practice in NLP 1 (Wang et al., 2018(Wang et al., , 2019Lai et al., 2017;Rajpurkar et al., 2018;Reddy et al., 2019). The goal of these leaderboards is to encourage the development of systems capable of solving certain tasks and to measure their progress by comparing the best systems against humans. Their great success has led many researchers to focus on just the proposed tasks, resulting in a rapid saturation of the scores which, in many tasks, are equal to or greater than those obtained by humans. As a consequence, many have attributed superhuman performance to such systems, and some tasks have been deemed solved. However, while systems in some areas of AI are compared with the best possible humans, e.g. IBM Deep Blue vs. Garry Kasparov in chess 2 or IBM Watson vs. Ken Jennings and Brad Rutter in the Jeopardy! quiz show 3 , NLP researchers often naively or vaguely estimate the \"human baseline\", assuming it is a uniform and accepted term of comparison, an established level that systems need to simply beat. In this section we provide a broad overview of existing NLP benchmarks, with a particular focus on NLU leaderboards where human baselines are outperformed by systems, and then show that the construction of such benchmarks is fraught with inconsistencies.\nThe SuperGLUE benchmark (Wang et al., 2019) is a well-known framework for evaluating research towards general-purpose language understanding models for English. It is a collection of 10 language understanding tasks built on existing public datasets, together with private test data, an evaluation server, and a single-number performance metric. In many tasks, humans are outperformed by the best-scoring systems, often by a large margin, ranking 8th in the current overall leaderboard. Likewise, the SuperGLUE predecessor, i.e. GLUE (Wang et al., 2018), was built to measure advances in NLU, and the systems' scores quickly saturated the benchmark, thereby sliding the human baseline down to the 23rd position in the ranking.\nThe RACE benchmark (Lai et al., 2017) was designed specifically for evaluating NLP models on a set of challenging reading comprehension tasks, such as Question Answering (QA) and text summarization. It consists of a large dataset of more than 28,000 multiple-choice questions, which are drawn from middle and high school problems extracted from English examinations in China. These questions cover a wide range of topics and require the ability to reason, understand context, and make inferences based on the provided text. Human baselines rank 21st on the public leaderboard, with a gap of almost 20 points compared to the bestscoring system. Similarly, the SQuAD2.0 benchmark (Rajpurkar et al., 2018) is another popular collection of reading comprehension questions and answers based on Wikipedia articles. The questions are created by crowdworkers and the answers are a portion of text from the corresponding article. The peculiar difference of this benchmark compared to SQuAD1.1 (Rajpurkar et al., 2016) is that some of the questions may not have answers, hence systems are required to learn to abstain as well. Again, the human baseline is placed in low positions of the ranking, reaching just the 30th place. Another notable, related benchmark is CoQA (Reddy et al., 2019), a large-scale dataset focused on Conversational QA systems. In this task, humans rank 7th, with a gap of 2 points from the top system.\nQuite different results are observed when moving to a cross-lingual scenario or when systems are required to perform mathematical and logical reasoning. In particular, XTREME (Hu et al., 2020) is a benchmark for cross-lingual transfer evaluation that covers dozens of languages spanning 12 language families, and includes 9 tasks that require reasoning about different levels of syntax or semantics. In this case, the human baselines beat the systems in all tasks, with an overall score 8 points higher than that of the best-performing system. XTREME has been succeeded by XTREME-R (Ruder et al., 2021), a more challenging multilingual benchmark that covers 14 language families and includes 10 tasks, and similar results have been observed. Furthermore, when systems are evaluated over MathQA (Amini et al., 2019) inputs, i.e. mathematical questions in the form of natural language, systems perform poorly compared to humans. Indeed, humans achieve an accuracy of 87% while systems only reach 54.2%. Since systems are still far from human-level performance in these benchmarks, they are out of the scope of our study. However, the highlighted gaps should encourage further research in these areas.\nAn alternative view on system evaluation is presented by the adversarial evaluation framework (Nie et al., 2020;Kiela et al., 2021), where the evaluation is performed through an iterative \"humanand-model-in-the-loop\" annotation process. Hu-mans are asked to inspect the model output and produce adversarial examples that target specific model weaknesses. The evaluation target is thus a moving goalpost, as opposed to the static targets of most other benchmarks, which saturate quickly. The Dynabench benchmark (Kiela et al., 2021) embraces this approach, incorporating tasks such as NLI, QA, sentiment analysis and hate speech detection. It provides a platform for the annotators to examine model output and create adversarial examples. At the time of writing, most of the tasks within Dynabench do not report human performance, however. Exceptions include the adversarial visual QA task (Sheng et al., 2021), where the proposed adversarial examples are solved by other humans and agreement is computed in terms of accuracy. Model performance in this setting falls far below the human performance.\nUsing more challenging examples for model evaluation, and possibly subsequent re-training, is an appealing approach, likely to strengthen the models with respect to the aspects that the examples target. The caveat is, however, that special care needs to be taken to avoid loss of generality. The annotation of adversarial examples directly depends on the behavior of the model (or set of models) under consideration; the addition of a large number of adversarial examples will likely change the data distribution by potentially overemphasizing rare events; finally, the annotators may focus on a small number of properties of the model, thus \"overfitting\" the models.\nAlthough there are many other popular NLP benchmarks to be investigated, e.g. XGLUE (Liang et al., 2020) and SentiBench (Ribeiro et al., 2016), we limit our review to those benchmarks in which human performance is provided and that can therefore help us answer the main question of this paper concerning the meaning of superhuman performance.", "publication_ref": ["b71", "b70", "b33", "b58", "b60", "b70", "b71", "b33", "b58", "b59", "b60", "b24", "b65", "b1", "b44", "b29", "b29", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "Human Baselines Are Not Reliable", "text": "As discussed above, many NLU benchmarks are saturated (cf. Figure 1). Here we dive deeper into some of them, identify the reasons for their quick saturation, and discuss whether it is fair to claim superhuman performance of state-of-the-art models.\nIn particular, we study SuperGLUE (Wang et al., 2019) and SQuAD (Rajpurkar et al., 2016(Rajpurkar et al., , 2018, as the representatives for general language understanding and reading comprehension, respectively.", "publication_ref": ["b70", "b59", "b58"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "SuperGLUE", "text": "For each of the ten tasks in SuperGLUE, human performance is provided and systems are compared against it. Specifically, for four of these tasks -Word in Context (WiC, Pilehvar and Camacho-Collados, 2019), Multi-Sentence Reading Comprehension (MultiRC, Khashabi et al., 2018), Recognizing Textual Entailment (RTE, , Reading Comprehension with Commonsense Knowledge (ReCoRD, Zhang et al., 2018) -human performance is computed by the authors of the corresponding papers, while for the remaining tasks 4 humans are evaluated by the creators of the SuperGLUE benchmark.\nWiC For this lexical-semantic task, four sets of 100 instances with an overlap of 50 instances between two of the annotators were randomly sampled from the test set. Each set was then assigned to an annotator, resulting in a total of 300 annotated instances. The annotators were not lexicographers and were not provided with sense distinctions to resemble the more difficult scenario for unsupervised models (cf. Appendix C). A final score of 80% was then obtained by averaging the individual scores achieved by the humans on the 4 sets (between 79% and 82%).\nMultiRC In the Multi-Sentence Reading Comprehension task, four native-speaker annotators tagged the entire test set of 166 instances. Human performance was obtained by combining the individual predictions of the different annotators via majority voting. RTE To establish the human performance on the RTE task, annotators were hired through the Hybrid data collection platform. Each annotator first completed a short training procedure, during which they were provided with task-specific guidelines and annotated 20 random examples from the dev set. Only annotators with \u2265 65% accuracy qualified for the main task. 500 examples were randomly taken from the test set and, for each instance, the final label was obtained by combining 5 different annotations via majority voting, reporting a final accuracy of 93.6%. The average pay rate was $17/hr for the main task, and $7.6/hr for training.\nReCoRD For the Reading Comprehension with Commonsense Knowledge task, 2,257 crowdworkers were hired through the Amazon Mechanical Turk platform (AMT). For first-time workers, the HIT 5 assignments were accompanied with guidelines. Crowdworkers were required to have \u2265 50 HITs with a 95% HIT acceptance rate and to be located in the USA, Canada, or UK. The average pay rate was $3.6/hr.\nOther SuperGLUE Tasks For the six remaining tasks, the SuperGLUE authors hired crowdworkers through AMT: the annotators first completed a short training phase where 30 random development set examples were provided for each task. Only workers who completed 5 HITs during training with performance at, or above, the median across all workers were admitted to the main task. Human performance was estimated on a random set of 100 test samples from each task, by applying majority voting on the annotations of 5 workers. During both phases, workers had access to task-specific guidelines, with a pay rate of $23.75/hr.", "publication_ref": ["b27", "b73"], "figure_ref": [], "table_ref": []}, {"heading": "SQuAD", "text": "In SQuAD1.1 (Rajpurkar et al., 2016), the researchers obtained \u2265 3 answers from human workers for each question in the dev and test sets, and estimated human performance by using only one of the answers as the \"human prediction\" and the remaining answers as \"ground truth\" for comparison. Specifically, workers were shown the questions and relevant paragraphs of an article and were asked to select the shortest paragraph span that answered the question. They were advised to complete 5 questions in 2 minutes with a $9/hr pay rate.\nIn SQuAD2.0 (Rajpurkar et al., 2018), instead, the authors collected multiple answers for each question (i.e. 4.8 answers, on average) and selected the final human prediction by majority voting. The answers were collected by providing annotators with a paragraph and its associated questions -unanswerable and answerable ones shuffled together -and asking them either to highlight the answer in the paragraph or to mark the question as unanswerable. They were asked to spend one minute per question with a $10.50/hr pay rate. Table 1: Results on SuperGLUE (from https://super.gluebenchmark.com/leaderboard). On top, we report the results of the 5 best-performing models, on average, while on bottom we report those of the human baselines on the various tasks. We mark in bold the best scores per task. AX-g and AX-b are the two diagnostic datasets.", "publication_ref": ["b59", "b58"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Issues", "text": "Comparing the performance of the five best systems against humans on SuperGLUE (Table 1), it is immediately apparent that the machines outperform humans on 6 out of 10 tasks, and often by a large margin (e.g. 7.8 F 1 points on MultiRC). Similarly, best systems substantially outperform humans on SQuAD1.1 and SQuAD2.0, with a margin of 8.3 and 4.1 points in exact match accuracy, respectively. Interestingly, (zero-shot) ChatGPT performs poorly compared to both human baselines and best-performing (fine-tuned) systems. Indeed, compared to the scores reported in  Qin et al. (2023) and Koco\u0144 et al. (2023). Additionally, Koco\u0144 et al. (2023) also showed that ChatGPT performs 20% worse than state-of-theart systems on the SQuAD2.0 benchmark, and demonstrated that it is, on average, 25% worse than specialized ML systems on a wide array of tasks. Hence it is not relevant for our study as its performance is still far from human-level. What does appear relevant, instead, are the extremely high, often superhuman, scores achieved by specialized systems. Nevertheless, notwithstanding such scores, in the above-mentioned benchmarks multiple factors make human-tosystem comparisons unfair because they limit human performance while facilitating systems. We list them in the remainder of this section.", "publication_ref": ["b56", "b32", "b32"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Apples and oranges", "text": "The most glaring problem is that, on almost all SuperGLUE tasks, humans and systems are evaluated on different test sets (i.e. on a small subset vs. the full test set). Specifically, in the WiC and RTE tasks, humans are assessed on 21.4% and 16.6% of the test set (i.e. 300 out of 1400 and 500 out of 3000 instances), respectively. Similarly, in the other SuperGLUE tasks humans are evaluated on a subset of 100 instances per task, which -in the worst case of the BoolQ dataset -amounts to just 3% of the test set. We provide more details in Appendix B.\nHuman evaluation metrics Different metrics are used to assess humans across tasks. While most of the tasks employ majority voting, WiC merely averages the scores achieved by humans on 4 small distinct subsets. In SQuAD1.1, humans are evaluated by comparing the tags of an arbitrary annotator against those of two other \"ground truth\" annotators, thereby likely underestimating the final score.\nHeterogeneous and unknown pay rates Pay rates varied considerably across the various tasks, ranging from undeclared pay rates to $23.75/hr. Low and mediocre wages, as in ReCoRD and SQuAD, may have contributed to suboptimal human performance: the $3.6/hr pay rate on ReCoRD could be one of the reasons for the large gap between systems and humans, while the unknown pay rate for MultiRC might explain the 18.2% human error rate on this binary classification task.\nGround-truth data quality We identified several errors and ambiguous instances in the goldstandard datasets, some of which we report in Table 2. Importantly, we note that, while systems can find spurious correlations between training and evaluation instances, and therefore provide the correct answer without clear evidence, humans cannot find such correlations, or otherwise may genuinely disagree on what the correct answer is. We elaborate on this point in Appendix A, by analyzing several examples per task, as well as in Appendix C, where we report the results of an ad hoc study concerning the WiC dataset.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Information about annotators and instructions", "text": "Details of the annotator pool (e.g. the number of annotators, their background and nationality, etc.) are often omitted. Similarly, the absence of training instructions and task guidelines raises questions about the quality of the training phase, if any.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "BoolQ", "text": "Passage: Shower gel -Shower gels for men may contain the ingredient menthol, which gives a cooling and stimulating sensation on the skin, and some men's shower gels are also designed specifically for use on hair and body. Shower gels contain milder surfactant bases than shampoos, and some also contain gentle conditioning agents in the formula. This means that shower gels can also double as an effective and perfectly acceptable substitute to shampoo, even if they are not labelled as a hair and body wash. Washing hair with shower gel should give approximately the same result as using a moisturising shampoo. Question: is it bad to wash your hair with shower gel Answer: TRUE CB Premise: A: I do too, so she couldn't possibly turn them out like some of these popular writers, B: Huh-uh. A: but oh, her books are just incredible. I don't think they've ever made a movie, do you? Hypothesis: they've ever made a movie Entailment: FALSE   2: Some errors and ambiguous instances we have found in the gold standard datasets of various SuperGLUE tasks. We limited our analysis to tasks where suspiciously low human performance was reported (cf. Table 1).", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Setups Favor Misleading Comparisons", "text": "Summarizing the above observations, we find four main sources of human-to-system comparison error. These correspond to the following key aspects of the evaluation process: system performance, the evaluation data, the measurement process, and humans themselves. We discuss each in turn.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Systems: Right for the Wrong Reasons", "text": "Across a variety of tasks,  report that random train-test splits consistently overestimate model performance: randomization at the sentence level reduces discrepancies between training and test sets as sentences from the same documents occur in both. Non-random standard splits also bring the danger of inadvertent, communitywide overfitting (Gorman and Bedrick, 2019) 6 . In natural language inference (NLI), multiple authors have found that BERT achieves what looks like near-human accuracy by exploiting idiosyncrasies of the data: they are \"right for the wrong reasons\" (McCoy et al., 2019;Niven and Kao, 2019). Here much of BERT's success is attributed to its ability to learn syntactic and lexical cues for inference, which happen to be mostly correct on the original test data. However, these cues do not actually support such inferences on adversarial datasets, taking BERT's accuracy to chance level or below. Poliak et al. (2018) ", "publication_ref": ["b40", "b52"], "figure_ref": [], "table_ref": []}, {"heading": "Data: Monolithicity Obscures Details", "text": "A further cause of systematic performance overestimation is that test sets include instances with varied, often unfathomable, levels of difficulty, so the exact reported accuracy will be a weighted average that depends directly on the mixture of easy and hard instances in the test data. The composition of train-test splits can thus make a big difference (Swayamdipta et al., 2020).\nIn QA, Lewis et al. (2021) investigated the traintest splits of several popular datasets. They found that there can be substantial overlap between the answers and even the questions of the training and test sets. The evaluation results differed greatly be-tween seen and unseen questions and answers; for instance, the exact-match accuracy of BART as a closed-book QA system on WebQuestions dropped from 76% to below 2% when neither the question nor the answer were ever seen during training.\nIn semantic parsing, seq2seq models such as BART and T5 are very accurate when evaluated in-domain on broad-coverage parsing tasks, e.g. Bevilacqua et al. (2021a). Yao and Koller (2022) report that their accuracy drops to close to zero on test subsets that require them to generalize to language that is structurally more complex than the training data. This is corroborated when constructing hard sets, i.e. train-test splits based on compositional generalization, forcing the accuracy of seq2seq models below 20% (Bogin et al., 2022).", "publication_ref": ["b69", "b36", "b5", "b72", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Measurement: Automation Is Limiting", "text": "A third key limitation of current evaluations, and especially existing leaderboards, is that they assume that the performance of a model can be measured automatically. While this has not been discussed very much in NLU, in other communities it has long been recognized that automatic evaluations are imperfect proxies of human judgments (Novikova et al., 2017). Machine translation papers report BLEU scores because they are drastically cheaper to calculate than the cost to collect human judgments about the fluency and adequacy of text; but one system that outperforms another on BLEU is not necessarily judged better by humans (Callison-Burch et al., 2006;Popel et al., 2020). While recent automatic metrics correlate better with human judgments (Kocmi et al., 2021), automatic evaluation has consistently been found problematic when comparing top-performing systems (Ma et al., 2019). Similarly, Byron et al. (2009) recommend crowdsourced evaluations to counter the inadequacy of automated evaluation for NLG.\nThe deeper issue with our reliance on automated evaluations is that they constrain the tasks on which we can evaluate systems. New shared tasks and datasets are specifically designed to make automated evaluations possible. However, many skills that show competent language use cannot easily be approximated by automatic measures (Dunietz et al., 2020): there are entire facets of language competence that are systematically out of scope for the tasks we design. One might argue that these are the most interesting parts of the actual mastery of language. Therefore, human-level performance on automatically-evaluated tasks does not equate to human-level performance on real language use.", "publication_ref": ["b9", "b53", "b31", "b38", "b8", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Humans: They Often Disagree", "text": "The final and possibly most problematic issue with system evaluation lies in the creation of the evaluation data itself. Common evaluation methodology assumes that there exists a single ground-truth for evaluation. This is a great oversimplification. We argue that evaluation should be conducted with reference to different groups of annotators to go beyond a one-dimensional performance score, to reflect multiple possible 'truths'.\nA great deal depends on how annotators are instructed to produce the data. It is well-known that human annotation quality may suffer from errors resulting from lack of attention given to the task, both by annotators themselves and by the annotation managers, often resulting from the need to drive annotation costs down (Mishra and Gorana, 2021). Importantly, however, human label variation does not always reflect poor annotation. Label variation can also result from stimulus characteristics or the context in which annotation occurs, including factors like the identity of the annotators, their background, and world knowledge. Plank (2022) identifies three main reasons for human label variation, namely annotator disagreement, subjectivity (multiple possible perspectives) and underspecfication (multiple plausible answers). While subjectivity (e.g., due to cultural differences) is a clear issue in tasks like hate speech detection , inherent disagreements, ambiguous sentence meaning, underspecification in guidelines and annotator behavior have been identified not only in fine-grained Word Sense Disambiguation tasks (Navigli, 2009), but even in NLI (Pavlick and Kwiatkowski, 2019;Zhang and de Marneffe, 2021;Jiang and de Marneffe, 2022).\nWhile the standard approach for training and evaluating NLP systems is to use a single gold label for each example, a growing body of work deals with multiple labels by varying model training in various ways: different aggregation methods (Paun et al., 2018), training on the distributional labels (Potts et al., 2020), learning from agreement signals (Plank et al., 2014), or modeling the annotators (Geva et al., 2019;Sap et al., 2022;Gordon et al., 2022). Recently, Basile et al. (2021) proposed extending this approach to evaluation. Fully benefiting from this extension requires releasing annotator characteristics labels , including socio-demographic information, and carefully documenting the annotation process (Gebru et al., 2018;Bender and Friedman, 2018;.\nAnnotator disagreement often results from differences across individuals -not just in NLP but also in fields such as cognitive science (Levinson, 2012) and psycholinguistics (Kidd et al., 2018). This phenomenon is often underestimated, since experiments tend to focus on a homogeneous sub-sample of the human population (Henrich et al., 2010). Annotators have different natural biases (Reidsma and op den Akker, 2008), and models often learn annotator-specific signals that are not generalizable (Geva et al., 2019), including opinion, personality (Sap et al., 2022) and culture (Hershcovich et al., 2022), but also different interpretation of guidelines (Hansen and S\u00f8gaard, 2021;Parmar et al., 2022). To deal with subjectivity, Rottger et al. (2022) recently introduced two contrasting data annotation paradigms: the descriptive and prescriptive ones. While the former encourages annotator subjectivity by capturing and modelling different beliefs, the latter, instead, discourages it and enforces annotators to encode one specific belief, formulated in the annotation guidelines. Depending on the downstream application of the dataset, one paradigm can be more suitable than the other, but neither paradigm is inherently superior. However, dataset annotators should explicitly aim for one of the two paradigms to facilitate the intended downstream use of their dataset, and to document, for the benefit of others, how exactly their dataset was annotated.\nIn conclusion, without more attention to the \"science of annotation\", the methodological laxity in today's dataset creation will continue to foster inaccurate estimations of human performance.", "publication_ref": ["b41", "b43", "b48", "b74", "b25", "b47", "b54", "b51", "b17", "b66", "b18", "b3", "b14", "b4", "b35", "b28", "b22", "b61", "b17", "b66", "b23", "b21", "b64"], "figure_ref": [], "table_ref": []}, {"heading": "Humans Can Explain Their Answers", "text": "When performing language tasks, humans are capable of explaining why they provided a given answer. Thus, when models are claimed to attain human-level language understanding, we can reasonably expect to be able to elicit explanations from them. This has proven highly challenging, however, which casts further doubts on such claims.\nWhy do we need explanations? At the level of an individual problem instance, explanations can help users assess whether to trust a given answer. At the level of a system, they help regulators and the general public to assess whether, or in what contexts, a system is safe to use, e.g. by uncovering unwanted biases or by revealing that the system relies on outdated knowledge. In the context of this paper, explanations can help NLP researchers understand the behaviour of their systems, e.g. to make sure that models are right for the right reasons (McCoy et al., 2019;Niven and Kao, 2019), or to uncover some of the shortcuts that the model may have learned (Geirhos et al., 2020), as discussed in \u00a74.1. Indeed, the absence of explanations can lead researchers astray. For example, in a prize-winning paper, Kaushik and Lipton (2018) analysed several state-of-the-art QA systems and found that they simply classified the best matching answer using their pre-stored knowledge about each question candidate, without performing any 'reading'. None of the papers in which these QA systems were introduced had considered this possibility.\nWhat are the challenges? While the importance of explanations is well-understood, progress has been hampered by various issues. One issue is that the evaluation of system-generated explanations is hard to automate ( \u00a74.3). Another issue is that it is not always clear what form the explanations should take. For tasks such as sentiment classification, it may be sufficient to highlight which words from the input text have mostly affected a given prediction. However, for NLI and QA, providing informative explanations can be challenging, even for humans. This can be observed by inspecting datasets that include human explanations (Camburu et al., 2018;Rajani et al., 2019;Aggarwal et al., 2021). Finally, system-generated explanations are typically not faithful, i.e. they do not necessarily reflect the process used by the model. For instance, Camburu et al. (2020) found that models can generate contradictory explanations for a given input.", "publication_ref": ["b40", "b16", "b26", "b10", "b57", "b0", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Recommendations", "text": "Based on the findings of the previous sections, we argue that current claims regarding superhuman performance are not adequately grounded, leading to unjustified hype. Here we provide a set of recommendations aimed at making comparisons between humans and machines fairer and more reliable.\nDo not favor machines against humans Various actions can be taken to set a level playing field between humans and machines, so as to provide a more realistic sense of their actual performance:\n1. Avoid using the same documents for training and evaluation ( \u00a74.1): in fact, using the same documents inherently reduces discrepancies across splits (Gorman and Bedrick, 2019), encouraging models to learn specific idiosyncrasies that appear in both (McCoy et al., 2019).\n2. Balance easy and hard test set items ( \u00a74.2), so as to report accuracies and enable analyses based on their difficulty level.\n3. Occasionally refresh test sets ( \u00a72), as suggested by recent trends in adversarial evaluation (Kiela et al., 2021).\n4. Complement automatic evaluations with human judgements ( \u00a74.3), so as to compare systems with humans on facets of language use that cannot be evaluated automatically.", "publication_ref": ["b19", "b40", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "5.", "text": "Adequately train and motivate humans ( \u00a73.3), aiming to increase the quality of human annotations through a solid training process and higher pay, in a sense mimicking the effort taken in improving systems.\nMake human performance evaluation transparent and reproducible We suggest carrying out an effort similar to systems' reproducibility for evaluating humans as well, including:\n1. Document the annotator pool composition ( \u00a73.3), by explicitly answering the following questions: how many annotators were hired? Following what process? What is their cultural background, nationality, languages and areas of expertise? What is their hourly pay rate?\n2. Specify the annotation process ( \u00a73.3 and \u00a74.4): it is important to state how many annotators were assigned to each instance, the training process they underwent, the guidelines they received (and how such guidelines were fine-tuned), and the metrics used to compute the overall human performance (averaging individual scores, majority voting, etc.).\n3. Provide individual annotations ( \u00a74.4): this allows recalculation of overall human performance whenever new metrics are tried, identifying the best metrics, calculating the scores of the best and worst annotators, the gap between the two, and the correlation between metrics and individual annotators -all aspects that the annotation community has long advocated. Importantly, the availability of individual answers, combined with the annotators' profiles, opens the door to deeper investigations about why and when humans disagree.\nIncrease annotation accountability Multiple measures can be implemented to make both systems and benchmarks more reliable, transparent and informative:\n1. Include explanations in your benchmark ( \u00a75): requiring annotators to provide the rationale behind their choices implicitly enforces them to devote more attention to the annotation task, thus yielding higher-quality and more consistent annotations. Moreover, annotators' explanations can be used to study subjectivity, and discover (and mark) ambiguous instances.\n2. Let systems produce explanations ( \u00a75): before claiming superhuman performance, it is important that, similarly to humans, systems can explain the inferences behind their predictions. This is key both for increasing systems' credibility and for discovering their limitations. However, it is not impossible that a system will produce the right answer with the wrong explanation, or vice versa. For this reason, we believe that a system must be able to provide explanations that support its answers without knowing that answer a priori, inferring the answer based on its knowledge.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions", "text": "We have discussed the distressing tendency of many NLP researchers to claim superhuman performance for their systems, and outlined why such claims are not (yet) grounded. We identified problems with evaluation data, evaluation measures and methodology, system understandability, and the human creation of data, all of which contribute to our conclusion.\nAs a final remark, with this paper we hope to make the reader more suspicious and rigorous when claims about \"superhuman\" performance are made, and, more importantly, to incentivize benchmark creators to address current limitations and design more solid and transparent benchmarks that will advance our scientific understanding of NLP systems and humans.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "In this paper, we have unearthed a variety of problems present in current evaluation benchmarks that favor systems over humans, or that simply make such comparisons unfair. We conclude that there is no real evidence to claim that today's language models possess superhuman performance. However, without empirical results obtained under the right setups, we cannot even claim the opposite, namely that humans are still better than systems. We leave such demonstrations for future work.\nAdditionally, while a good portion of the NLP research effort is devoted to natural language generation (NLG) tasks (which includes MT), here we provide only some pointers to NLG/MT. Indeed, as discussed in Section 4.3, these problems exist in the NLG universe as well, but, due to space constraints, we limit our analysis to NLU tasks. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Ground Truth Data Quality", "text": "In Table 2, we reported one problematic example for each of the most crucial tasks in the Super-GLUE benchmark. Here we comment on those examples and provide additional problematic cases which we identified by manually inspecting the datasets. Some of these cases appear recurrently.\nBoolQ The example in Table 2 is blatantly wrong, as it explicitly says that shower gel is an effective and perfectly acceptable substitute to shampoo, hence the label should be FALSE. We provide more errors in Table 3. Specifically, we believe that some of these examples are wrongly annotated, ambiguous, or highly misleading. In the first example, from the premise, it seems that some scientists and ornithologists differentiate between doves and pigeons, so the answer might be subjective, and therefore ambiguous. In the second example, instead, it seems there is no evidence that a red back spider bite can kill a human being, but the answer is TRUE. Similarly to the first case, in the third example the premise states that in most prisons possession of mobile phones is not allowed, thus\nPassage: Columbidae -The distinction between \"doves\" and \"pigeons\" is not consistent. In modern everyday speech, as opposed to scientific usage or formal usage, \"dove\" frequently indicates a pigeon that is white or nearly white. However, some people use the terms \"dove\" and \"pigeon\" interchangeably. In contrast, in scientific and ornithological practice, \"dove\" tends to be used for smaller species and \"pigeon\" for larger ones, but this is in no way consistently applied. Historically, the common names for these birds involve a great deal of variation between the terms. The species most commonly referred to as \"pigeon\" is the species known by scientists as the rock dove, one subspecies of which, the domestic pigeon, is common in many cities as the feral pigeon. Question: is there a difference between doves and pigeons Answer: FALSE Passage: Redback spider -The redback is one of the few spider species that can be seriously harmful to humans, and its liking for habitats in built structures has led it to being responsible for a large number of serious spider bites in Australia. Predominantly neurotoxic to vertebrates, the venom gives rise to the syndrome of latrodectism in humans; this starts with pain around the bite site, which typically becomes severe and progresses up the bitten limb and persists for over 24 hours. Sweating in localised patches of skin occasionally occurs and is highly indicative of latrodectism. Generalised symptoms of nausea, vomiting, headache, and agitation may also occur and indicate severe envenomation. An antivenom has been available since 1956. There have been no deaths directly due to redback bites since its introduction, however Isbister et al. have suggested patients for whom antivenom is considered should be fully informed \"there is considerable weight of evidence to suggest it is no better than placebo\", and in light of a risk of anaphylaxis and serum sickness, \"routine use of the antivenom is therefore not recommended\". As of the 2013 (updated 2014) edition of the Snakebite & Spiderbite Clinical Management Guidelines from NSW HEALTH (latest available in 2017), Red-back spider bites were considered not life-threatening but capable of causing severe pain and systemic symptoms that could continue for hours to days. Question: can a red back spider bite kill you Answer: TRUE Passage: Mobile phones in prison -In most prisons, inmates are forbidden from possessing mobile phones due to their ability to communicate with the outside world and other security issues. Mobile phones are one of the most smuggled items into prisons. They provide inmates the ability to make and receive unauthorized phone calls, send email and text messages, use social media, and follow news pertaining to their case, among other forbidden uses.\nQuestion: are you allowed to have your phone in prison Answer: FALSE Passage: Vena amoris -Vena amoris is a Latin name meaning, literally, \"vein of love\". Traditional belief established that this vein ran directly from the fourth finger of the left hand to the heart. This theory has been cited in western cultures as one of the reasons the engagement ring and/or wedding ring was placed on the fourth finger, or \"ring finger\". This traditional belief is factually inaccurate as all the fingers in the hand have a similar vein structure. Question: is it true that the ring finger is connected to the heart Answer: FALSE Passage: Substitute (association football) -Most competitions only allow each team to make a maximum of three substitutions during a game and a fourth substitute during extra time, although more substitutions are often permitted in non-competitive fixtures such as friendlies. A fourth substitution in extra time was first implemented in recent tournaments, including the 2016 Summer Olympic Games, the 2017 FIFA Confederations Cup and the 2017 CONCACAF Gold Cup final. A fourth substitute in extra time has been approved for use in the elimination rounds at the 2018 FIFA World Cup, the UEFA Champions League and the UEFA Europa League. Each team nominates a number of players (typically between five and seven, depending on the competition) who may be used as substitutes; these players typically sit in the technical area with the coaches, and are said to be \"on the bench\". When the substitute enters the field of play it is said they have come on or have been brought on, while the player they are substituting is coming off or being brought off. Question: can a player be substituted twice in football Answer: TRUE the answer might change depending on the prison.\nIn the fourth example, the fact that all the fingers have a similar vein structure does not mean that the ring finger is not connected to the heart, on the contrary, this reinforces the hypothesis. Finally, while two or more players can be substituted in a football game, the same player cannot be substituted twice. CommitmentBank In the CB example reported in Table 2 we have that A does not know whether they've ever made a movie and, indeed, asks if B thinks they have. Therefore, we cannot conclude that the movie was never made, and the answer should be NEUTRAL. By inspecting the dataset, we discovered that its instances follow standard patterns that can be easily learned by machines, but, at the same time, confuse humans. Indeed, most of the time, the entailment is TRUE when a fragment of the hypothesis appears (as an exact match) in the premise (see the second and third examples in Table 5). To the contrary, the entailment is FALSE when the same text fragment appears negated either in the premise or in the hypothesis, e.g. preceded by don't think or similar, standard constructs (see the first, fourth and fifth examples in Table 5). However, as argued before, the mere fact of not thinking that a thing is true does not necessarily imply that thing is not true.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5", "tab_8", "tab_8"]}, {"heading": "MultiRC", "text": "Regarding the MultiRC example of Table 2, in this case, the error is in the candidate answers. Specifically, two candidate answers are Paragraph: Amateur tennis star Guy Haines wants to divorce his vulgar and unfaithful wife Miriam , so he can marry the elegant and beautiful Anne Morton , daughter of a senator . While on a train to meet Miriam , Haines meets Bruno Anthony , a forward stranger who recognizes Guy from gossip items in the newspapers that detail his marital problems . During lunch in Bruno's compartment , Bruno tells Guy about his idea for the perfect \" Criss-cross \" murder : he will kill Miriam and in exchange , Guy will kill Bruno's father . Since both are strangers , otherwise unconnected , there is no identifiable motive for the crimes , Bruno contends , hence no suspicion . Guy hurriedly leaves the compartment but leaves Bruno thinking he has agreed to the deal . Guy accidentally leaves his cigarette lighter behind , a gift from Anne to Guy , Which Bruno pockets . Bruno heads to Guy's hometown of Metcalf and follows Miriam and her two beaux to an amusement park , where he briefly illuminates her face with Guy's lighter , then strangles her to death . Guy's problems begin when his alibi an inebriated college professor on the same train as Guy can not remember their meeting . But they increase exponentially when Bruno makes repeated appearances into Guy's life as he seeks to remind Guy that he is now obliged to kill Bruno's father , according to the bargain he thinks they struck on the train . Bruno sends Guy the keys to his house , a map to his father's room , and a pistol . Soon after , Bruno appears at a party at Senator Morton's house and hobnobs with the guests , much to Guy's apprehension and Anne's increasing suspicion. Question: Who are the two that Guty and Bruno are planning to murder? Candidate Answers: Bruno's father (TRUE), Guy's father (FALSE), Bruno's wife (FALSE), Miriam and Bruno's father (TRUE), Guy's wife (TRUE), . . . Paragraph: Albert Bandura OC (/baen'dU@r@/; born December 4, 1925) is a psychologist who is the David Starr Jordan Professor Emeritus of Social Science in Psychology at Stanford University. For almost six decades, he has been responsible for contributions to the field of education and to many fields of psychology, including social cognitive theory, therapy and personality psychology, and was also influential in the transition between behaviorism and cognitive psychology. He is known as the originator of social learning theory and the theoretical construct of self-efficacy, and is also responsible for the influential 1961 Bobo doll experiment. Social learning theory is how people learn through observing others. An example of social learning theory would be the students imitating the teacher. Self-efficacy is \"The belief in one's capabilities to organize and execute the courses of action required to manage prospective situations.\" To paraphrase, self-efficiacy is believing in yourself to take action. The Bobo Doll Experiment was how Albert Bandura studied aggression and non-aggression in children. A 2002 survey ranked Bandura as the fourth most-frequently cited psychologist of all time, behind B. F. Skinner, Sigmund Freud, and Jean Piaget, and as the most cited living one. Bandura is widely described as the greatest living psychologist, and as one of the most influential psychologists of all time. In 1974 Bandura was elected to be the Eighty-Second President of the American Psychological Association (APA). He was one of the youngest president-elects in the history of the APA at the age of 48. Bandura served as a member of the APA Board of Scientific Affairs from 1968 to 1970 and is well known as a member of the editorial board of nine psychology journals including the Paragraph: (CNN) -German art collector Cornelius Gurlitt, whose nearly priceless collection was confiscated because it was suspected to contain pieces looted by the Nazis, died Tuesday and left the masterpieces to a Swiss museum. One day after Gurlitt's death at the age of 81, the Museum of Fine Arts Bern announced that Gurlitt had named it\u1e27is unrestricted and unfettered sole heir.The news came as a surprise, the museum said Wednesday, because Gurlitt had never had any connection to it. The museum's directors are delighted at the news, they said in a statement, but also recognize that there are outstanding legal and ethical questions surrounding the collection. Gurlitt had undergone major heart surgery and was hospitalized for many weeks, his representative said in a statement. Gurlitt grabbed the attention of the art world when German prosecutors seized more than 1,200 paintings from his Munich apartment in 2012, including works by Picasso and Matisse. The collection was confiscated as part of an investigation into tax fraud, but then it was thought that some of the paintings may have been works that were looted by the Nazis. Just last month, part of the collection was returned to Gurlitt as part of a deal with Germany's cultural authorities and the Bavarian Justice Ministry. Under the agreement, works owned by Gurlitt that were not under suspicion were returned to him. Those suspected of being stolen were to be held securely while a task force investigates their provenance -and will be returned to their original Jewish owners or their descendants if a claim is proven. Gurlitt's representative said that with the art collector's death, the investigation into the collection ceases. The court that was handling the investigation proceedings will now function as an estate court in the case.  equivalent, i.e. Mass of the object and The object's mass, but they are labeled differently, namely with TRUE and FALSE tags, respectively. In Table 4 we provide additional errors for this task. Specifically, in the first example, the question explicitly asks \"Who are the two that Guty and Bruno are planning to murder?\", but the possible answers are i) Miriam and Bruno's father, ii) Bruno's father and iii) Guy's wife. Although, by design, MultiRC creators explicitly state that multiple answers can be correct, answers are judged independently, so it would not be valid to form a correct answer by combining ii) and iii). These cases are very frequent in MultiRC and might have negatively affected human performance. Furthermore, typos in the questions and/or paragraphs (i.e. Guty, in this case) might have further limited their scores. In the second example, the ground truth answers are \"2002\" and  Finally, in the third example, from the paragraph, it is clear that the German art collector Cornelius Gurlitt passed away at the age of 81. However, there are three errors in the possible answers for this entry. First, \"At the age of 81\" and \"81\" are labeled as TRUE and FALSE, respectively. Second, \"80 years old\" is labeled as TRUE, hence contradicting the first answer. Finally, \"80\" is labeled as FALSE further contradicting the penultimate answer. RTE In the RTE example (Table 2), the specific premise regarding Pacific countries is not sufficient to entail the general hypothesis, thus the answer should be FALSE. We provide more examples in Table 6. In particular, in some of them, we believe that the label is incorrect (examples 1, 3 and 5), or at least highly misleading, while in some others we Premise: Compuware claims that Allan Tortorice and Jim Hildner were among several former employees who revealed trade secrets after they moved to IBM. Hypothesis: Trade secrets were stolen. Entailment: FALSE Premise: It has been observed that in those countries of the world where capital punishment is still in operation, the crime rate, especially murder, is distinctively low in comparison to countries where capital punishment has been discarded. Hypothesis: Capital punishment is a deterrent to crime. Entailment: TRUE Premise: A farmer who was in contact with cows suffering from BSE -the so-called mad cow disease -has died from what is regarded as the human form of the disease. Hypothesis: Bovine spongiform encephalopathy is another name for the \"mad cow disease\" Entailment: TRUE    think that not enough information is provided to entail the hypothesis (examples 2 and 4). WiC In the WiC example provided in Table 2, the word criticism is used with the same meaning in the two contexts, namely disapproval expressed by pointing out faults or shortcomings according to WordNet. We provide additional ambiguous or wrongly annotated examples in Table 7.\nBy inspecting the WiC dataset, it is immediately apparent that, in many negative examples, the semantic gap between the meanings of the same lemma in the two contexts is very narrow. Although such cases are difficult even for machines, we posit that for humans (especially if sense distinctions are not provided and annotators are not lexicographers, as in WiC) they are way more difficult.\nSQuAD For the SQuAD dataset, studies about errors in the annotations have already been performed by Rodriguez et al. (2021) through automatic error detection methods. Specifically, they annotated SQuAD items by discriminability, difficulty, and Item Response Theory (IRT) prediction errors, and discovered that items with negative discriminability, or where IRT's prediction is wrong, have a much higher rate of annotation error, i.e. they are often \"flawed\" or \"wrong\". We believe that tools for error detection (Klie et al., 2022) should play a key role in the improvement of existing benchmarks and in the creation of new ones.\nFinally, still related to the topic of wronglyannotated or ambiguous instances in the datasets,  performed an interesting study on the GLUE benchmark. In order to investigate the effect of these instances, they looked at human performance when there is 5-way annotator agreement. Using unanimous agreement has the effect of filtering out examples for which: i) the annotation guidelines supplied do not provide clear advice, and ii) humans understand the expectations of the task but find the example genuinely difficult or uncertain. They discovered that this widened the gap between humans and systems by more than 3 points on average, hence confirming the hypothesis that humans were often penalized by unclear guidelines or other factors. Even more interestingly, they found that in some tasks, when systems are evaluated on the unanimous subsets they obtain lower scores compared to those obtained on the entire test sets containing wrong or ambiguous instances, hence suggesting that systems had learned specific idiosyncrasies appearing in both training and test sets (Section 4.1).", "publication_ref": ["b63", "b30"], "figure_ref": [], "table_ref": ["tab_6", "tab_9", "tab_10"]}, {"heading": "B Apples and Oranges", "text": "In Section 3.3, the first issue that we pointed out was that, on almost all SuperGLUE tasks, humans and machines are evaluated on different test sets (i.e. on a small subset vs. the full test set). Here, we provide more details (see Table 8). Specifically, it can be observed that only 3 out of 10 tasks are fully annotated by humans, while for the remaining 7 tasks only a small portion is annotated, ranging from 3% to 40% of the full dataset size.  ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_12"]}, {"heading": "C The Copenhagen Experiment", "text": "In this Section, we report on an experiment that was conducted in Copenhagen at the Danish Language Technology Conference in 2022 8 . The main goal was to verify the claim that contextual sentence examples from open lexical resources, such as those used to create the WiC dataset, i.e. WordNet, Verb-Net and Wiktionary, \"constitute a reliable base for the construction of the dataset, as they are curated in a way to be clearly distinguishable across different senses of the word\" (Pilehvar and Camacho-Collados, 2019). Based on this assumption, \"the [WiC dataset] annotators were not provided with knowledge from any external lexical resource\", and were asked to label each instance solely based on whether they thought the two occurrences of the word referred to the same meaning or not. We repeated the above annotation task by asking 25 conference participants to provide \"true\" (T) and \"false\" (F) answers for the six instances in Table 9. The results show a high degree of disagreement, suggesting that the above claim is not always valid, especially when subtle sense distinctions are involved. We posit that the presence of a certain amount of intrinsically debatable items hampers fair comparisons between humans and systems.\nIndeed, in WSD evaluation tasks, where the granularity of senses is a key concern (Bevilacqua et al., 2021b), we advocate that the starting point for the task design should consist either of the warning made by many lexicographers that \"there is very little agreement about what word senses are or how broad their scope should be, and no definitive way Table 9: Results of the Copenhagen experiment. Context-1 and Context-2 provide two sentences in which a Target word w appears. The WiC column specifies the label in the WiC dataset, indicating whether w has the same meaning in the two contexts. Finally, the F and T columns represent the number of people who voted for F and T, respectively.\nof knowing when one sense ends and another begins\" (Atkins and Rundell, 2008), or the one from the famous lexicographer, James Murray, that \"the best any lexicographer could hope for would be that readers would feel, on scanning a multisense dictionary entry, that this is not an unreasonable way of exhibiting the facts\". With large corpora and the latest advances in language modeling, we now have the possibility to measure differences between contexts in which words are used, and we need not and should not rely on made-up sentences from the times when corpora were not available at all. This is corroborated, for instance, by the inter-tagger agreement and the systems' results of the multilingual version of the WiC task, where sentences come from real text and dictionary definitions are used as a help for annotators (Martelli et al., 2021).", "publication_ref": ["b49", "b6", "b2", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "This work grew out of a brainstorming session held at the Rome Workshop on Ten Years of BabelNet in July 2022. 7    We gratefully acknowledge the support of the ERC Consolidator Grant MOUSSE No. 726487 under the European Union's Horizon 2020 research and innovation programme, and the support of the PNRR MUR project PE0000013-FAIR. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Sections 2 and 3 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. Left blank.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Sections 2 and 3 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. Sections 2, 3, and Appendix C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? No response.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Explanations for Common-senseQA: New Dataset and Models", "journal": "Long Papers", "year": "2021", "authors": "Shourya Aggarwal; Divyanshu Mandowara; Vishwajeet Agrawal; Dinesh Khandelwal; Parag Singla; Dinesh Garg"}, {"ref_id": "b1", "title": "MathQA: Towards interpretable math word problem solving with operation-based formalisms", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Aida Amini; Saadia Gabriel; Shanchuan Lin; Rik Koncel-Kedziorski; Yejin Choi; Hannaneh Hajishirzi"}, {"ref_id": "b2", "title": "The Oxford guide to practical lexicography", "journal": "Oxford University Press", "year": "2008", "authors": "Sue Bt; Michael Atkins;  Rundell"}, {"ref_id": "b3", "title": "We need to consider disagreement in evaluation", "journal": "", "year": "2021", "authors": "Valerio Basile; Michael Fell; Tommaso Fornaciari; Dirk Hovy; Silviu Paun"}, {"ref_id": "b4", "title": "Data statements for natural language processing: Toward mitigating system bias and enabling better science", "journal": "Transactions of the Association for Computational Linguistics", "year": "2018", "authors": "Emily M Bender; Batya Friedman"}, {"ref_id": "b5", "title": "One SPRING to rule them both: Symmetric AMR semantic parsing and generation without a complex pipeline", "journal": "AAAI Press", "year": "2021", "authors": "Michele Bevilacqua; Rexhina Blloshmi; Roberto Navigli"}, {"ref_id": "b6", "title": "Recent trends in word sense disambiguation: A survey", "journal": "", "year": "2021", "authors": "Michele Bevilacqua; Tommaso Pasini; Alessandro Raganato; Roberto Navigli"}, {"ref_id": "b7", "title": "Unobserved local structures make compositional generalization hard", "journal": "", "year": "2022", "authors": "Ben Bogin; Shivanshu Gupta; Jonathan Berant"}, {"ref_id": "b8", "title": "Report on the First NLG Challenge on Generating Instructions in Virtual Environments (GIVE)", "journal": "Association for Computational Linguistics", "year": "2009", "authors": "Donna Byron; Alexander Koller; Kristina Striegnitz; Justine Cassell; Robert Dale; Johanna Moore; Jon Oberlander"}, {"ref_id": "b9", "title": "Re-evaluating the role of Bleu in machine translation research", "journal": "Association for Computational Linguistics", "year": "2006", "authors": "Chris Callison; - Burch; Miles Osborne; Philipp Koehn"}, {"ref_id": "b10", "title": "e-snli: Natural language inference with natural language explanations", "journal": "Curran Associates, Inc", "year": "2018", "authors": "Oana-Maria Camburu; Tim Rockt\u00e4schel; Thomas Lukasiewicz; Phil Blunsom"}, {"ref_id": "b11", "title": "Make up your mind! adversarial generation of inconsistent natural language explanations", "journal": "", "year": "2020", "authors": "Oana-Maria Camburu; Brendan Shillingford; Pasquale Minervini; Thomas Lukasiewicz; Phil Blunsom"}, {"ref_id": "b12", "title": "Dealing with disagreements: Looking beyond the majority vote in subjective annotations", "journal": "", "year": "2021", "authors": "Aida Mostafazadeh Davani; Mark D\u00edaz; Vinodku "}, {"ref_id": "b13", "title": "To test machine comprehension, start by defining comprehension", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Jesse Dunietz; Greg Burnham; Akash Bharadwaj; Owen Rambow; Jennifer Chu-Carroll; Dave Ferrucci"}, {"ref_id": "b14", "title": "", "journal": "", "year": "2018", "authors": "Timnit Gebru; Jamie Morgenstern; Briana Vecchione; Jennifer Wortman Vaughan; Hanna Wallach; Iii Hal; Kate Daum\u00e9;  Crawford"}, {"ref_id": "b15", "title": "Garbage in, garbage out? Do machine learning application papers in social computing report where human-labeled training data comes from?", "journal": "", "year": "2020", "authors": "Kevin Stuart Geiger; Yanlai Yu; Mindy Yang; Jie Dai; Rebekah Qiu; Jenny Tang;  Huang"}, {"ref_id": "b16", "title": "Shortcut learning in deep neural networks", "journal": "Nature Machine Intelligence", "year": "2020", "authors": "Robert Geirhos; J\u00f6rn-Henrik Jacobsen; Claudio Michaelis; Richard Zemel; Wieland Brendel; Matthias Bethge; Felix A Wichmann"}, {"ref_id": "b17", "title": "Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Mor Geva; Yoav Goldberg; Jonathan Berant"}, {"ref_id": "b18", "title": "Jury learning: Integrating dissenting voices into machine learning models", "journal": "", "year": "2022", "authors": "Michelle S Mitchell L Gordon; Joon Sung Lam; Kayur Park; Jeff Patel; Tatsunori Hancock; Michael S Hashimoto;  Bernstein"}, {"ref_id": "b19", "title": "We need to talk about standard splits", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Kyle Gorman; Steven Bedrick"}, {"ref_id": "b20", "title": "Annotation artifacts in natural language inference data", "journal": "CoRR", "year": "2018", "authors": "Swabha Suchin Gururangan; Omer Swayamdipta; Roy Levy; Samuel R Schwartz; Noah A Bowman;  Smith"}, {"ref_id": "b21", "title": "Guideline bias in Wizard-of-Oz dialogues", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Bach Victor Petr\u00e9n; Anders Hansen;  S\u00f8gaard"}, {"ref_id": "b22", "title": "The weirdest people in the world?", "journal": "Behavioral and brain sciences", "year": "2010", "authors": "Joseph Henrich; J Steven; Ara Heine;  Norenzayan"}, {"ref_id": "b23", "title": "Challenges and strategies in crosscultural NLP", "journal": "Long Papers", "year": "2022", "authors": "Daniel Hershcovich; Stella Frank; Heather Lent; Mostafa Miryam De Lhoneux; Stephanie Abdou; Emanuele Brandl; Laura Cabello Bugliarello; Ilias Piqueras; Ruixiang Chalkidis; Constanza Cui; Katerina Fierro; Phillip Margatina; Anders Rust;  S\u00f8gaard"}, {"ref_id": "b24", "title": "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation", "journal": "PMLR", "year": "2020", "authors": "Junjie Hu; Sebastian Ruder; Aditya Siddhant; Graham Neubig; Orhan Firat; Melvin Johnson"}, {"ref_id": "b25", "title": "Investigating reasons for disagreement in natural language inference", "journal": "", "year": "2022", "authors": "Nan-Jiang Jiang; Marie-Catherine De Marneffe"}, {"ref_id": "b26", "title": "How much reading does reading comprehension require? a critical investigation of popular benchmarks", "journal": "", "year": "2018", "authors": "Divyansh Kaushik; Zachary C Lipton"}, {"ref_id": "b27", "title": "Looking beyond the surface: A challenge set for reading comprehension over multiple sentences", "journal": "", "year": "2018", "authors": "Daniel Khashabi; Snigdha Chaturvedi; Michael Roth; Shyam Upadhyay; Dan Roth"}, {"ref_id": "b28", "title": "Individual differences in language acquisition and processing", "journal": "", "year": "2018", "authors": "Evan Kidd; Seamus Donnelly; Morten H Christiansen"}, {"ref_id": "b29", "title": "Dynabench: Rethinking benchmarking in NLP", "journal": "", "year": "2021", "authors": "Douwe Kiela; Max Bartolo; Yixin Nie; Divyansh Kaushik; Atticus Geiger; Zhengxuan Wu; Bertie Vidgen; Grusha Prasad; Amanpreet Singh; Pratik Ringshia; Zhiyi Ma; Tristan Thrush; Sebastian Riedel; Zeerak Waseem; Pontus Stenetorp; Robin Jia; Mohit Bansal; Christopher Potts; Adina Williams"}, {"ref_id": "b30", "title": "Annotation Error Detection: Analyzing the Past and Present for a More Coherent Future", "journal": "", "year": "2022", "authors": "Jan-Christoph Klie; Bonnie Webber; Iryna Gurevych"}, {"ref_id": "b31", "title": "To ship or not to ship: An extensive evaluation of automatic metrics for machine translation", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Tom Kocmi; Christian Federmann; Roman Grundkiewicz; Marcin Junczys-Dowmunt"}, {"ref_id": "b32", "title": "Chatgpt: Jack of all trades", "journal": "", "year": "2023", "authors": "Jan Koco\u0144; Igor Cichecki; Oliwier Kaszyca; Mateusz Kochanek; Dominika Szyd\u0142o; Joanna Baran; Julita Bielaniewicz; Marcin Gruza; Arkadiusz Janz; Kamil Kanclerz; Anna Koco\u0144; Bart\u0142omiej Koptyra; Wiktoria Mieleszczenko-Kowszewicz; Piotr Mi\u0142kowski; Marcin Oleksy; Maciej Piasecki; \u0141ukasz Radli\u0144ski; Konrad Wojtasik; Stanis\u0142aw Wo\u017aniak; Przemys\u0142aw Kazienko"}, {"ref_id": "b33", "title": "RACE: Large-scale ReAding comprehension dataset from examinations", "journal": "", "year": "2017", "authors": "Guokun Lai; Qizhe Xie; Hanxiao Liu; Yiming Yang; Eduard Hovy"}, {"ref_id": "b34", "title": "A Set of Recommendations for Assessing Human-Machine Parity in Language Translation", "journal": "Journal of Artifial Intelligence Research (JAIR)", "year": "2020", "authors": "Samuel L\u00e4ubli; Sheila Castilho; Graham Neubig; Rico Sennrich; Qinlan Shen; Antonio Toral"}, {"ref_id": "b35", "title": "The original sin of cognitive science", "journal": "Topics in cognitive science", "year": "2012", "authors": " Stephen C Levinson"}, {"ref_id": "b36", "title": "Question and answer test-train overlap in opendomain question answering datasets", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Patrick Lewis; Pontus Stenetorp; Sebastian Riedel"}, {"ref_id": "b37", "title": "XGLUE: A new benchmark datasetfor cross-lingual pre-training, understanding and generation", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Yaobo Liang; Nan Duan; Yeyun Gong; Ning Wu; Fenfei Guo; Weizhen Qi; Ming Gong; Linjun Shou; Daxin Jiang; Guihong Cao; Xiaodong Fan; Ruofei Zhang; Rahul Agrawal; Edward Cui; Sining Wei; Taroon Bharti; Ying Qiao; Jiun-Hung Chen; Winnie Wu; Shuguang Liu; Fan Yang; Daniel Campos; Rangan Majumder; Ming Zhou"}, {"ref_id": "b38", "title": "Results of the WMT19 metrics shared task: Segment-level and strong MT systems pose big challenges", "journal": "", "year": "2019", "authors": "Qingsong Ma; Johnny Wei; Ond\u0159ej Bojar; Yvette Graham"}, {"ref_id": "b39", "title": "SemEval-2021 task 2: Multilingual and cross-lingual word-in-context disambiguation (MCL-WiC)", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Federico Martelli; Najla Kalach; Gabriele Tola; Roberto Navigli"}, {"ref_id": "b40", "title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Tom Mccoy; Ellie Pavlick; Tal Linzen"}, {"ref_id": "b41", "title": "Who decides if AI is fair? the labels problem in algorithmic auditing", "journal": "", "year": "2021", "authors": "Abhilash Mishra; Yash Gorana"}, {"ref_id": "b42", "title": "Human vs. muppet: A conservative estimate of human performance on the GLUE benchmark", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Nikita Nangia; Samuel R Bowman"}, {"ref_id": "b43", "title": "Word sense disambiguation: A survey", "journal": "ACM Comput. Surv", "year": "2009", "authors": "Roberto Navigli"}, {"ref_id": "b44", "title": "", "journal": "Adversarial", "year": "2020", "authors": "Yixin Nie; Adina Williams; Emily Dinan; Mohit Bansal; Jason Weston; Douwe Kiela"}, {"ref_id": "b45", "title": "2022. Don't blame the annotator: Bias already starts in the annotation instructions", "journal": "", "year": "", "authors": "Mihir Parmar; Swaroop Mishra; Mor Geva; Chitta Baral"}, {"ref_id": "b46", "title": "Don't blame the annotator: Bias already starts in the annotation instructions", "journal": "", "year": "2023", "authors": "Mihir Parmar; Swaroop Mishra; Mor Geva; Chitta Baral"}, {"ref_id": "b47", "title": "Comparing Bayesian models of annotation", "journal": "", "year": "2018", "authors": "Bob Silviu Paun; Jon Carpenter; Dirk Chamberlain; Udo Hovy; Massimo Kruschwitz;  Poesio"}, {"ref_id": "b48", "title": "Inherent disagreements in human textual inferences", "journal": "", "year": "2019", "authors": "Ellie Pavlick; Tom Kwiatkowski"}, {"ref_id": "b49", "title": "WiC: the word-in-context dataset for evaluating context-sensitive meaning representations", "journal": "Long and Short Papers", "year": "2019", "authors": "Mohammad Taher Pilehvar; Jose Camacho-Collados"}, {"ref_id": "b50", "title": "The 'problem' of human label variation: On ground truth in data, modeling and evaluation", "journal": "", "year": "2022", "authors": "Barbara Plank"}, {"ref_id": "b51", "title": "Learning part-of-speech taggers with inter-annotator agreement loss", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Barbara Plank; Dirk Hovy; Anders S\u00f8gaard"}, {"ref_id": "b52", "title": "Hypothesis only baselines in natural language inference", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Adam Poliak; Jason Naradowsky; Aparajita Haldar; Rachel Rudinger; Benjamin Van Durme"}, {"ref_id": "b53", "title": "Transforming machine translation: a deep learning system reaches news translation quality comparable to human professionals", "journal": "Nature Communications", "year": "2020", "authors": "Martin Popel; Marketa Tomkova; Jakub Tomek; \u0141ukasz Kaiser; Jakob Uszkoreit; Ond\u0159ej Bojar; Zden\u011bk \u017dabokrtsk\u00fd"}, {"ref_id": "b54", "title": "DynaSent: A dynamic benchmark for sentiment analysis", "journal": "", "year": "2020", "authors": "Christopher Potts; Zhengxuan Wu; Atticus Geiger; Douwe Kiela"}, {"ref_id": "b55", "title": "On releasing annotator-level labels and information in datasets", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Aida Mostafazadeh Vinodkumar Prabhakaran; Mark Davani;  Diaz"}, {"ref_id": "b56", "title": "Is chatgpt a general-purpose natural language processing task solver?", "journal": "", "year": "2023", "authors": "Chengwei Qin; Aston Zhang; Zhuosheng Zhang; Jiaao Chen; Michihiro Yasunaga; Diyi Yang"}, {"ref_id": "b57", "title": "Explain yourself! leveraging language models for commonsense reasoning", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Bryan Nazneen Fatema Rajani; Caiming Mccann; Richard Xiong;  Socher"}, {"ref_id": "b58", "title": "Know what you don't know: Unanswerable questions for SQuAD", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Pranav Rajpurkar; Robin Jia; Percy Liang"}, {"ref_id": "b59", "title": "SQuAD: 100,000+ questions for machine comprehension of text", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Pranav Rajpurkar; Jian Zhang; Konstantin Lopyrev; Percy Liang"}, {"ref_id": "b60", "title": "CoQA: A conversational question answering challenge", "journal": "Transactions of the Association for Computational Linguistics", "year": "2019", "authors": "Siva Reddy; Danqi Chen; Christopher D Manning"}, {"ref_id": "b61", "title": "Exploiting 'subjective' annotations", "journal": "", "year": "2008", "authors": "Dennis Reidsma; Rieks Op Den Akker"}, {"ref_id": "b62", "title": "Sentibench-a benchmark comparison of stateof-the-practice sentiment analysis methods", "journal": "EPJ Data Science", "year": "2016", "authors": "Matheus Filipe N Ribeiro; Pollyanna Ara\u00fajo;  Gon\u00e7alves"}, {"ref_id": "b63", "title": "Evaluation examples are not equally informative: How should that change NLP leaderboards?", "journal": "Long Papers", "year": "2021", "authors": "Pedro Rodriguez; Joe Barrow; Alexander Miserlis Hoyle; John P Lalor; Robin Jia; Jordan Boyd-Graber"}, {"ref_id": "b64", "title": "Two contrasting data annotation paradigms for subjective NLP tasks", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Paul Rottger; Bertie Vidgen; Dirk Hovy; Janet Pierrehumbert"}, {"ref_id": "b65", "title": "XTREME-R: Towards more challenging and nuanced multilingual evaluation", "journal": "", "year": "2021", "authors": "Sebastian Ruder; Noah Constant; Jan Botha; Aditya Siddhant; Orhan Firat; Jinlan Fu; Pengfei Liu; Junjie Hu; Dan Garrette; Graham Neubig; Melvin Johnson"}, {"ref_id": "b66", "title": "Annotators with attitudes: How annotator beliefs and identities bias toxic language detection", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Maarten Sap; Swabha Swayamdipta; Laura Vianna; Xuhui Zhou; Yejin Choi; Noah A Smith"}, {"ref_id": "b67", "title": "Wojciech Galuba, Devi Parikh, and Douwe Kiela. 2021. Human-adversarial visual question answering", "journal": "", "year": "", "authors": "Sasha Sheng; Amanpreet Singh; Vedanuj Goswami; Jose Alberto Lopez Magana"}, {"ref_id": "b68", "title": "We need to talk about random splits", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Anders S\u00f8gaard; Sebastian Ebert; Jasmijn Bastings; Katja Filippova"}, {"ref_id": "b69", "title": "Dataset cartography: Mapping and diagnosing datasets with training dynamics", "journal": "", "year": "2020", "authors": "Swabha Swayamdipta; Roy Schwartz; Nicholas Lourie; Yizhong Wang; Hannaneh Hajishirzi; Noah A Smith; Yejin Choi"}, {"ref_id": "b70", "title": "Superglue: A stickier benchmark for general-purpose language understanding systems", "journal": "Curran Associates Inc", "year": "2019", "authors": "Alex Wang; Yada Pruksachatkun; Nikita Nangia; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel R Bowman"}, {"ref_id": "b71", "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding", "journal": "", "year": "2018", "authors": "Alex Wang; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel Bowman"}, {"ref_id": "b72", "title": "Structural generalization is hard for sequence-to-sequence models", "journal": "", "year": "2022", "authors": "Yuekun Yao; Alexander Koller"}, {"ref_id": "b73", "title": "Record: Bridging the gap between human and machine commonsense reading comprehension", "journal": "", "year": "2018", "authors": "Sheng Zhang; Xiaodong Liu; Jingjing Liu; Jianfeng Gao; Kevin Duh; Benjamin Van Durme"}, {"ref_id": "b74", "title": "Identifying inherent disagreement in natural language inference", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Frederick Xinliang; Marie-Catherine Zhang;  De Marneffe"}, {"ref_id": "b75", "title": "Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? No response", "journal": "", "year": "", "authors": ""}, {"ref_id": "b76", "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run", "journal": "", "year": "", "authors": ""}, {"ref_id": "b77", "title": "for preprocessing, for normalization, or for evaluation", "journal": "", "year": "", "authors": ""}, {"ref_id": "b78", "title": "crowdworkers) or research with human participants? Left blank", "journal": "", "year": "", "authors": ""}, {"ref_id": "b79", "title": "Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators", "journal": "", "year": "", "authors": " D1"}, {"ref_id": "b80", "title": "crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic", "journal": "", "year": "", "authors": ""}, {"ref_id": "b81", "title": "Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?", "journal": "", "year": "", "authors": " D3"}, {"ref_id": "b82", "title": "Was the data collection protocol approved (or determined exempt) by an ethics review board? No response", "journal": "", "year": "", "authors": " D4"}, {"ref_id": "b83", "title": "Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response", "journal": "", "year": "", "authors": " D5"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: Difference between the scores obtained by the best-performing systems and humans in various popular NLP benchmarks. The systems outperform humans on 6 out of 8 of the reported benchmarks (best seen in color).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "MultiRCParagraph:What causes a change in motion? The application of a force. Any time an object changes motion, a force has been applied.[. . . ]  It depends on the strength of the force. It also depends on the objects mass. Think about some simple tasks you may regularly do. You may pick up a baseball. This requires only a very small force. Question: What factors cause changes in motion of a moving object? Candidate Answers: Shape of the object (FALSE), Mass of the object (TRUE), The object's mass (FALSE), . . .RTE Premise: In most Pacific countries there are very few women in parliament. Hypothesis: Women are poorly represented in parliament. Entailment: TRUE WiC Context 1: The senator received severe criticism from his opponent. Context 2: The politician received a lot of public criticism for his controversial stance on the issue. Sense Match: FALSE", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Journal of Personality and Social Psychology from 1963 to 1972. At the age of 82, Bandura was awarded the Grawemeyer Award for psychology. Question: In what year was Bandura awarded the Grawemeyer Award for psychology. Candidate Answers: 2010 (FALSE), 2007 (TRUE), 2000 (FALSE), 2002 (TRUE)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Question: How old was the art collector Cornelius Gurlitt when he died? Candidate Answers: At the age of 81 (TRUE), 80 (FALSE), 80 years old (TRUE), 81 (FALSE)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Premise:The girl was found in Drummondville. Hypothesis: Drummondville contains the girl. Entailment: FALSE Premise: The official visit of the Argentine minister marks a further step in the normalisation of UK-Argentine relations. Hypothesis: Relations between Argentina and Great Britain are growing more cooperative. Entailment: FALSE", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Context 1 :1I tried to make a call, but the line was dead. Context 2: A dedicated line. Sense Match: TRUE Context 1: The author gives a depressing picture of life in Poland. Context 2: He had no clear picture of himself or his world. Sense Match: FALSE Context 1: Instant replay caused too long a delay. Context 2: The delay before the echo of a sound. Sense Match: FALSE Context 1: Stop a car. Context 2: Stop the thief. Sense Match: TRUE Context 1: Fall asleep. Context 2: She fell to pieces after she lost her work. Sense Match: TRUE", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "report an even more extreme case of being \"right for the wrong reason\": several NLI datasets support what they call hypothesisonly models, which perform surprisingly well without exposure to the premise(Gururangan et al., 2018), e.g. outperforming the majority-class baseline.Poliak et al. (2018) attribute this to statistical irregularities in the data (often single words indicating negation), caused by obvious annotation strategies chosen by crowdworkers who were not stimulated enough to come up with more creative ways to produce contradictions or entailments. Along the same lines,Parmar et al. (2023) recently identified instruction bias in 14 NLU benchmarks. Specifically, they found that this phenomenon is evident in most of these datasets, showing that \u223c73% of instruction examples, on average, share a few clear bias patterns, and that models often fail to generalize beyond such patterns.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885-4901, Online. Association for Computational Linguistics.", "figure_data": "Timothy Niven and Hung-Yu Kao. 2019. Probing neu-ral network comprehension of natural language argu-ments. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4658-4664, Florence, Italy. Association for Compu-tational Linguistics.Jekaterina Novikova, Ond\u0159ej Du\u0161ek, Amanda Cer-cas Curry, and Verena Rieser. 2017. Why we need new evaluation metrics for NLG. In Proceedings of the 2017 Conference on Empirical Methods in Natu-ral Language Processing, pages 2241-2252, Copen-hagen, Denmark. Association for Computational Lin-guistics."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Additional problematic instances we have found in the BoolQ dataset.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Additional problematic instances in the Multi-Sentence Reading Comprehension (MultiRC) dataset.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Premise: A: Your turn. B: Okay. Uh, I don't think they should abolish it. Hypothesis: they should abolish it Entailment: FALSE Premise: The lunch trade had mostly disappeared so he wasn't hard to spot. He was at a window table but he was ignoring the river, being deep in conversation with a middle-aged man wearing a suit and a short sheepskin car coat with matching brown suede shoes. Even from this distance you could guess the guy's tailor was based in Dublin.", "figure_data": "Hypothesis: the guy's tailor was based in Dublin Entailment: TRUEPremise: B: and, you know, they just love kittens. A: Yeah. B: They just are fascinated. A: Oh, yeah. B: So she doesn't know thatthis is a cat yet.Hypothesis: this is a cat Entailment: TRUEPremise: A: Well, actually, uh, A: I don't think I'm in the, uh, majority in TexasHypothesis: she is in the majority in Texas Entailment: FALSEPremise: B: Because too often, there can be extremism that hurts from any direction, regardless of whatever you're arguing orconcerned about. A: Yeah. Right. Yeah, I know, you're right, theywould lobby that and I see that, and that's why, you know, I'm like,okay, what's my role in this thing\" you know, what's my part, B:Yeah. A: because I don't think the system is going to get fixed.Hypothesis: the system is going to get fixed. Entailment: FALSE"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Additional problematic instances we have found in the CommitmentBank (CB) dataset.", "figure_data": "\"2007\". However, while \"2007\" can be inferredby adding 82 years (i.e. the age at which AlbertBandura received the Grawemeyer award) to hisbirth date (i.e. 1925), \"2002\" is a wrong answer.Indeed, the paragraph says that \"A 2002 surveyranked Bandura as the fourth most-frequently citedpsychologist of all time\", but there is no evidencethat he received the award in 2002."}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Some problematic instances we have found in the Recognizing Textual Entailment (RTE) dataset.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Additional problematic instances we have found in the Word-in-Context (WiC) dataset.", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Comparison of the size of the test sets annotated by humans (H) and systems (S) in the various tasks. % represents the ratio between H and S.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "The author gives a depressing picture of life in Poland. He had no clear picture of himself or his world. 7 18 F take (V) Do you take sugar in your coffee? A reading was taken of the earth's tremors. 21 4", "figure_data": "WiC TargetContext-1Context-2F TTline (N)I tried to make a call, but the line was dead.A dedicated line.14 11Flove (N)A mother's love is not easily shaken.The theater was her first love.169Fwork (V)This dough does not work easily.Work the phones.169Tfall (V)Fall asleep.She fell to pieces after she lost her work.177Fpicture (N)"}], "formulas": [], "doi": "10.18653/v1/2021.acl-long.238"}