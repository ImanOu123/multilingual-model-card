{"title": "Scaling Data-Constrained Language Models", "authors": "Niklas Muennighoff; Alexander M Rush; Boaz Barak; Teven Le Scao; Aleksandra Piktus; Nouamane Tazi; Sampo Pyysalo; Thomas Wolf; Colin Raffel", "pub_date": "2023-10-26", "abstract": "The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github.com/huggingface/datablations.", "sections": [{"heading": "", "text": "Allocating compute when repeating", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data-Constrained Scaling Laws", "text": "Models trained Loss assuming repeated data is worth the same as new data Loss predicted by our data-constrained scaling laws Regime of same compute (IsoFLOP) Efficient frontier assuming repeated data is worth the same as new data Efficient frontier predicted by our data-constrained scaling laws To maximize performance when repeating, our data-constrained scaling laws and empirical data suggest training smaller models for more epochs in contrast to what assuming Chinchilla scaling laws [42] hold for repeated data would predict ( \u00a75).", "publication_ref": ["b41"], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Recent work on compute-optimal language models [42] shows that many previously trained large language models (LLMs, which we define as having more than one billion parameters) could have attained better performance for a given compute budget by training a smaller model on more data. Notably, the 70-billion parameter Chinchilla model [42] outperforms the 280-billion parameter Gopher model [89] while using a similar compute budget by being trained on four times more data. Extrapolating these laws for compute allocation (hereafter \"Chinchilla scaling laws\") to a 530 billion parameter model, such as the under-trained MT-NLG model [99], would require training on a massive 11 trillion tokens, corresponding to more than 30 terabytes of text data. For most languages, available data is several orders of magnitude smaller, meaning that LLMs in those languages are already data-constrained. Villalobos et al. [112] estimate that even high-quality English language data will be exhausted by the year 2024 given the Chinchilla scaling laws and the trend of training ever-larger models. This motivates the question [112,81]: what should we do when we run out of data?\nIn this work we investigate scaling large language models in a data-constrained regime, and whether training an LLM with multiple epochs of repeated data impacts scaling. Using multiple epochs is, of course, standard in machine learning generally; however, most prior large language models have been trained for a single epoch [51,15] and some work explicitly advocates against reusing data [40]. An exception is the recent Galactica models [108] that were trained for 4.25 epochs and exhibit continually decreasing validation loss and improving downstream performance throughout training. However, the experiments of Galactica do not compare this setup to an alternative non-dataconstrained model trained for one epoch on unique data. Without this comparison, it is difficult to quantify the trade-off between additional compute versus additional data collection.\nOur main focus is to quantify the impact of multiple epochs in LLM training such that practitioners can decide how to allocate compute when scaling models. Toward this end, we assembled a battery of empirical training runs of varying data and compute constraints. Specifically, we train more than 400 models ranging from 10 million to 9 billion parameters for up to 1500 epochs and record final test loss. We use these results to fit a new data-constrained scaling law that generalizes the Chinchilla scaling law [42] to the repeated data regime and yields a better prediction of loss in this setting. Figure 1 summarizes our main results targeting the value of repeated data (Return) and optimal allocation of resources in that regime (Allocation). We find that, while models trained for a single epoch consistently have the best validation loss per compute, differences tend to be insignificant among models trained for up to 4 epochs and do not lead to differences in downstream task performance. Additional epochs continue to be beneficial, but returns eventually diminish to zero. We find that, in the data-constrained regime, allocating new compute to both more parameters and epochs is necessary, and that epochs should be scaled slightly faster. These findings suggest a simple way to continue scaling total training compute budgets further ahead in the future than the previously anticipated limits.\nFinally, given the challenges imposed by data constraints, we consider methods complementary to repeating for improving downstream accuracy without adding new natural language data. Experiments consider incorporating code tokens and relaxing data filtering. For code, English LLMs, such as PaLM [19] or Gopher [89], are trained on a small amount of code data alongside natural language data, though no benchmarking was reported to justify that decision. We investigate training LLMs on a mix of language data and Python data at 10 different mixing rates and find that mixing in code is able to provide a 2\u00d7 increase in effective tokens even when evaluating only natural language tasks. For filtering, we revisit perplexity and deduplication filtering strategies on both noisy and clean datasets and find that data filtering is primarily effective for noisy datasets.", "publication_ref": ["b41", "b41", "b14", "b39", "b41", "b18"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Background", "text": "Predicting the scaling behavior of large models is critical when deciding on training resources. Specifically, two questions are of interest: (Allocation) What is the optimal balance of resources? (Return) What is the expected value of additional resources? For scaling LLMs, the resource is compute (measured in FLOPs), and it can be allocated to training a larger model or training for more steps. 1 The metric used to quantify progress is the model's loss on held-out data, i.e. the ability to predict the underlying data as measured in the model's cross-entropy [2,42]. We aim to minimize the loss (L) subject to a compute resource constraint (C) via optimal allocation to N and D as: Currently, there are established best practices for scaling LLMs. Return follows a power-law: loss scales as a power-law with the amount of compute used for training [39,46,6,35,7,41]. Allocation is balanced: resources are divided roughly equally between scaling of parameters and data [42]. These scaling laws were established empirically by training LLMs and carefully extrapolating behavior.\nChinchilla [42] uses three methods for making scaling predictions:\n\u2022 (Fixed Parameters) Train with a fixed model size but on varying amounts of data.\n\u2022 (Fixed FLOPs) Train with fixed computation while parameters and training tokens vary.\n\u2022 (Parametric Fit) Derive and fit a formula for the loss.\nFor the parametric fit, the loss (L) is a function of parameters (N ) and training tokens (D):\nL(N, D) = A N \u03b1 + B D \u03b2 + E (2)\nWhere {A, \u03b1, B, \u03b2, E} are learned variables fit using the training runs from the first two approaches [42]. Using these learned variables, they propose calculating the optimal allocation of compute (C) to N and D as follows:\nN opt (C) = G(C/6) a D opt (C) = G \u22121 (C/6) b\nwhere G = \u03b1A \u03b2B\n1 \u03b1+\u03b2 a = \u03b2 \u03b1 + \u03b2 b = \u03b1 \u03b1 + \u03b2 (3)\nThese methods lead to the conclusion that \u03b1 \u2248 \u03b2 and hence N and D should be scaled proportionally for compute-optimal training. As loss can be an imperfect proxy for performance on natural language tasks [123,97,105], they also validate their conclusions on various downstream tasks.", "publication_ref": ["b0", "b1", "b41", "b38", "b45", "b5", "b34", "b6", "b40", "b41", "b41", "b41", "b57"], "figure_ref": [], "table_ref": []}, {"heading": "Method: Data-Constrained Scaling Laws", "text": "We are interested in scaling behavior in the data-constrained regime. Specifically, given a limited amount of unique data, what is the best Allocation of and Return for computational resources. Prior work [46,42] assumes that the necessary data to support scaling is unlimited. Our aim is therefore to introduce a modified version of Equation 2 that accounts for data constraints and fit the terms in the modified scaling law to data from a large body of experiments.\nThe primary method we consider is repeating data, i.e. allocating FLOPs to multiple epochs on the same data. Given a budget of unique data D C , we split the Chinchilla total data term D into two parts: the number of unique tokens used, U D , and the number of repetitions, R D (i.e. epochs -1). Given total training tokens D and data budget D Symmetrically, for mathematical convenience, we split the parameter term N into two parts: the base number of parameters needed to optimally fit the unique tokens U N , and the number of times to \"repeat\" this initial allocation, R N . We compute U N by first rearranging Equation 3 to find the optimal compute budget for the unique tokens used (U D ). We input this value into the N opt formula of Equation 3 to get U N = min{N opt , N }. U N thus corresponds to the compute-optimal number of parameters for U D or less if N < N opt . Once we have U N , we compute the repeat value as\nR N = (N/U N ) \u2212 1.\nTo empirically explore the scaling behavior in a data-limited setting we train LLMs under these constraints. We consider three different experimental protocols in this work:\n\u2022 (Fixed Unique Data) In \u00a75 we fix the data constraint D C and train models varying epochs and parameters. These experiments target Allocation, specifically tradeoff of D and N .\n\u2022 (Fixed FLOPs) In \u00a76 we fix the computation available and vary D C (and thus also U D and U N ). These experiments target Return, i.e. how well does repeating scale compared to having more unique data.\n\u2022 (Parametric Fit) We fit a formula introduced in \u00a73.1 on all our training runs and evaluate its predictive capability throughout \u00a75 and \u00a76.\nBefore discussing experimental results we describe the parametric assumptions.", "publication_ref": ["b45", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Parametric Fit", "text": "To extrapolate scaling curves, it is necessary to incorporate repetition into the Chinchilla formula (Equation 2). We generalize Equation 2 by replacing D and N with terms corresponding to the effective data (D \u2032 ) and effective model parameters (N \u2032 ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "L(N, D)", "text": "= A N \u2032\u03b1 + B D \u2032\u03b2 + E\nIntuitively, D \u2032 should be smaller or equal to D where D is the total number of processed tokens since repeated tokens provide less useful information to the model than new ones. We use an exponential decay formulation, where the value of a data token processed loses roughly (1 \u2212 1/R * D ) fraction of its value per repetition, where R * D is a learned constant. After some derivations and approximations (see Appendix A), this boils down to\nD \u2032 = U D + U D R * D (1 \u2212 e \u2212R D R * D ) .(5)\nNote that for R D = 0 (no repetitions),\nD \u2032 = U D = D. For R D \u226a R * D , e \u2212R D /R * D \u2248 1 \u2212 R D R * D\nand so\nD \u2032 \u2248 U D + U D R * D (1 \u2212 1 + R D /R * D ) = U D (1 + R D ) = D\nand hence in this case, repeated data is worth almost the same as fresh data. (This is also consistent with the predictions of the \"deep bootstrap\" framework [76].) As R D grows, the value of repeated tokens tends to zero, and the effective data D \u2032 becomes much smaller than D. The formula implies that no matter how many times we repeat the data, we will not get a better loss than could be obtained with a single epoch on U D + U D R * D fresh tokens. Just as processing repeated tokens yields a diminishing return, both intuitively and empirically, models with sizes that vastly outstrip the available data also offer diminishing returns per parameter. Hence we use a symmetric formula for the number of effective parameters, where again R * N is learned,\nN \u2032 = U N + U N R * N (1 \u2212 e \u2212R N R * N ) .(6)\nThe learned constants R * D , R * N roughly correspond to the \"half-life\" of repeated data and excess parameters. For example, at R D = R * D , the number of effective tokens\nD \u2032 is U D + U D R D (1 \u2212 e \u22121\n) which means that the U D R D repeated tokens are worth on average 1 \u2212 1/e fraction of fresh ones.\nUsing a methodology similar to [42], R * N and R * D can be fit on empirical measurements, which yields data-driven estimates. See Appendix A for more details on the derivations and the fitting procedure.    For all experiments, we train transformer language models with the GPT-2 architecture and tokenizer [88]. Models have up to 8.7 billion parameters and are trained for up to 900 billion total tokens. Following [42] we use cosine learning rate schedules that decay 10\u00d7 over the course of training for each model (different schedules led to different estimates in [46]). Unlike [46], we do not use early stopping to also explore the extent of overfitting when repeating. Other hyperparameters are based on prior work [89,42] and detailed in Appendix S. Models are trained on subsets of C4 [90]. The data constraints are carefully defined to ensure maximal overlap as shown in Figure 2. Unlike [40], we always repeat the entire available data rather than subsets of it. Data is shuffled after each epoch. As repeating data can result in extreme overfitting (see Appendix H), we report loss on a held-out test set unless otherwise specified (see Appendix K). This contrasts training loss used in [42], but should not alter our findings as the held-out data stems from the same underlying dataset.", "publication_ref": ["b41", "b41", "b45", "b45", "b41", "b39", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Results: Resource Allocation for Data-Constrained Scaling", "text": "Our first experimental setting considers scaling in a setting where all models have the same data constraint. For these experiments, the unique training data budget D C is fixed at either 100M, 400M or 1.5B tokens. For each data budget, we train a set of language models with increasing amounts of compute that is allocated to either more parameters or more epochs on the unique training data. according to scaling laws from [42] has U N of approximately 7M parameters (see Appendix B for the scaling coefficients we use). Results show that more than a 50% reduction in loss can be attained by training for several epochs (R D > 0) and increasing model size beyond what would be compute-optimal for 100M tokens (R N > 0). We find the best loss to be at around 20-60\u00d7 more parameters and epochs, which corresponds to spending around 7000\u00d7 more FLOPs. These results suggest that one-epoch models significantly under-utilize their training data and more signal can be extracted by repeating data and adding parameters at the cost of sub-optimal compute utilization.\nFigure 3 (right) shows the predicted contours created by fitting our data-constrained scaling laws on 182 training runs. In the single-epoch case (R D = 0) with near compute-optimal parameters (R N = 0) our scaling equation ( \u00a73.1) reduces to the Chinchilla equation. In this case, both formulas predict the optimal allocation of compute to parameters and data to be the same, resulting in overlapping efficient frontiers. As data is repeated for more than a single epoch, our fit predicts that excess parameters decay faster in value than repeated data (R * N < R * D ). As a result, the dataconstrained efficient frontier suggests allocating most additional compute to more epochs rather than more parameters. This contrasts the Chinchilla scaling laws [42], which suggest equally scaling both. However, note that they do not repeat the entire training data and their parametric fit explicitly relies on the assumption that models are trained for a single epoch only. Thus, there is no guarantee that their scaling predictions hold for repeated data.\nFor all three data budgets, our results suggest that Allocation is optimized by scaling epochs faster than additional parameters. We confirm this at scale by training the data-constrained compute-optimal model for 9.3 \u00d7 10 21 FLOPs and 25 billion unique tokens as suggested by our efficient frontier. Despite having 27% less parameters, this model achieves better loss than the model suggested by the Chinchilla scaling laws (Figure 1, right). Similarly, the 120 billion parameter Galactica model trained on repeated data should have been significantly smaller according to data-constrained scaling laws (Appendix G). An additional benefit of using a smaller model is cheaper inference, though adding parameters can make it easier to parallelize training across GPUs.  Extrapolating from the proposed data-constrained scaling law shows that at small numbers epochs are benign, but at large number of epochs loss stops improving.\nAdding parameters and epochs causes the loss to decrease and eventually increase again, suggesting that too much compute can hurt performance. Results from [46] also show that loss can increase when too many parameters are used, even with early stopping. However, we expect that appropriate regularization (such as simply removing all excess parameters as an extreme case) could prevent this behavior. Thus, our formula presented in \u00a73 and its predicted isoLoss contours in Figure 3 do not model the possibility that excess epochs or parameters could hurt performance.\n6 Results: Resource Return for Data-Constrained Scaling\nNext, consider the question of Return on scaling. To quantify this value, we run experiments with three FLOP budgets across eight respective data budgets to compare return on FLOPs.\nFigure 4 shows the configurations and validation curves for models trained on the same number of total tokens. Conforming to intuition and prior work on deduplication [55], repeated data is worth less, thus models trained on less unique data (and, correspondingly, more epochs) have consistently higher loss. However, the loss difference for a few epochs is negligible. For example, the N = 8.7 billion parameter model trained for four epochs (D C = 44 billion unique tokens) finishes training with only 0.5% higher validation loss than the single-epoch model (D C = 178 billion unique tokens).\nIn Figure 5 (left), we compare the final test loss of each model to predictions from our parametric fit. The data-constrained scaling laws can accurately measure the decay in the value of repeated data as seen by the proximity of empirical results (dots) and parametric fit (lines). We note however that it significantly underestimates the final test loss of failing models where loss increases midway through training, such as models trained for 44 epochs (not depicted).\nIn Figure 5 (right), we extrapolate the three budgets by further scaling compute while keeping the data constraints (D C ) at 55B, 84B, and 178B tokens, respectively. The parameter R * D introduced in \u00a73 represents roughly the \"half-life\" of epochs: specifically the point where repeated tokens have lost 1 e of their value. Through our fitting in Appendix A, we found R * D \u2248 15, corresponding to 15 repetitions (or 16 epochs). Graphically, this can be seen by the stark diminishing returns in the proximity of the 16-epoch marker and the flattening out soon after.\nOverall, the Return when repeating data is relatively good. Meaningful gains from repeating data can be made up to around 16 epochs (R * D ) beyond which returns diminish extremely fast.   While repeating data is effective, it has diminishing returns. We therefore consider strategies for scaling D targeting improved downstream performance as opposed to directly minimizing loss. Figure 6 (left) illustrates the strategies: (a) Code augmentation: We use Python code from The Stack [49] to make up for missing natural language data. The combined dataset consisting of code and natural language samples is shuffled randomly. (b) Adapting filtering: We investigate the performance impact of deduplication and perplexity filtering, two common filtering steps that can severely limit available data. Removing such filtering steps can free up additional training data.\nFor these experiments, we set a maximum data budget (D C ) of 84 billion tokens. For repetition and code filling, only a subset of D C is available and the rest needs to be compensated for via repeating or adding code. For both filtering methods, we start out with approximately twice the budget (178 billion tokens), as it is easier to gather noisy data and filter it than it is to gather clean data for training. For perplexity filtering, we select the top 25% samples with the lowest perplexity according to a language model trained on Wikipedia. This results in 44 billion tokens that are repeated for close to two epochs to reach the full data budget. For deduplication filtering, all samples with a 100-char overlap are removed resulting in 21 billion tokens that are repeated for four epochs during training. See Appendix N for more details on the filtering procedures.\nWhen comparing across data strategies, loss ceases to be a good evaluation metric as the models are trained on different data distributions. We thus evaluate models on 19 natural language tasks with zero to five in-context few-shot exemplars [15] producing 114 scores per model. As our evaluation tasks cover different metrics and random baselines, we re-scale all scores to be in the same range to better reflect performance ranges before averaging. Details on the evaluation datasets are in Appendix K.\nIn Figure 6 (right) we compare the downstream performance of all strategies. For repeating data, differences in downstream performance are insignificant for up to around 4 epochs (25% budget) and then start dropping, which aligns with our results on test loss in \u00a76. Filling up to 50% of data with code (42 billion tokens) also shows no deterioration. Beyond that, performance decreases quickly on natural language tasks. However, adding more code data may benefit non-natural language tasks, which are not considered in the benchmarking. Two of the tasks benchmarked, WebNLG [17,34], a generation task, and bAbI [122,57], a reasoning task, see jumps in performance as soon as code is added, possibly due to code enabling models to learn long-range state-tracking capabilities beneficial for these tasks.\nOf the filtering approaches, we find perplexity-filtering to be effective, while deduplication does not help. Prior work found deduplication was able to improve perplexity [55]; however, it did not evaluate on downstream tasks. Deduplication may have value not captured in our benchmark, such as reducing memorization [45,40,16,10]. We also investigate filtering on a different noisier dataset in Appendix O, where we find it to be more effective. Overall, in a data-constrained regime, we recommend reserving filtering for noisy datasets and using both code augmentation and repeating to increase data tokens. For example, first doubling the available data by adding code and then repeating the new dataset for four epochs results in 8\u00d7 more training tokens that are expected to be just as good as having had 8\u00d7 more unique data from the start.", "publication_ref": ["b41", "b41", "b45", "b0", "b48", "b14", "b16", "b33", "b56", "b44", "b39", "b15", "b9"], "figure_ref": ["fig_0", "fig_4", "fig_5", "fig_5", "fig_6", "fig_6"], "table_ref": []}, {"heading": "Related Work", "text": "Large language models Scaling up transformer language models [111] across parameter count and training data has been shown to result in continuous performance gains [19]. Starting with the 1.4 billion parameter GPT-2 model [88], a variety of scaled-up language models have been trained, commonly referred to as large language models (LLMs). They can be grouped into dense models [15,47,58,89,20,13,132,109,103,108,130,95,56,65] and sparse models [30,131,28,135] depending on whether each forward pass makes use of all parameters. These models are generally pre-trained to predict the next token in a sequence, which makes them applicable to various language tasks directly after pre-training [15,118,50,71, 102] by reformulating said NLP tasks as context continuation tasks (see [67] for an earlier proposal on this topic). We focus on the most common scenario, where a dense transformer model is trained to do next-token prediction on a large corpus and evaluated directly after pre-training using held-out loss or zero-to few-shot prompting.\nScaling laws Prior work has estimated an optimal allocation of compute for the training of LLMs. Kaplan et al. [46] suggested a 10\u00d7 increase in compute should be allocated to a 5.5\u00d7 increase in model size and a 1.8\u00d7 increase in training tokens. This first scaling law has led to the creation of very large models trained on relatively little data, such as the 530 billion parameter MT-NLG model trained on 270 billion tokens [99]. More recent work [42], however, showed that model size and training data should rather be scaled in equal proportions. These findings called for a renewed focus on the scaling of pre-training data rather than scaling model size via complex parallelization strategies [98,91,9,78]. Up-sampling is often employed when pre-training data is partly limited, such as data from a high-quality domain like Wikipedia or text in a rare language for training multilingual LLMs [60, 82]. Hernandez et al. [40] study up-sampling of data subsets and find that repeating only 0.1% of training data 100 times significantly degrades performance. In contrast, our work focuses on repeating the entire pre-training corpus for multiple epochs rather than up-sampling parts of it.\nAlternative data strategies Large pre-training datasets are commonly filtered to remove undesired samples or reduce noise [101]. Perplexity-based filtering, whereby a trained model is used to filter out samples with high perplexity, has been found beneficial to reduce noise in web-crawled datasets [121]. Mixing of data is employed for the pre-training data of multilingual LLMs, where text data from different languages is combined [23,126,100,74]. However, both for code and natural language models, mixing different (programming) languages has been reported to under-perform monolingual models [80,113]. Some work has investigated mixing code and natural language data for prediction tasks, such as summarizing code snippets [44] or predicting function names [4]. Several pre-training datasets for LLMs include low amounts of code data [31,89,95]. However, these past works generally do not provide any ablation on the drawbacks of including code or the benefits for natural language task performance. We perform a detailed benchmarking of mixing Python and natural language in LLM pre-training at 10 different mixing rates.", "publication_ref": ["b18", "b14", "b46", "b19", "b12", "b66", "b64", "b29", "b65", "b27", "b69", "b14", "b52", "b49", "b45", "b41", "b8", "b39", "b55", "b22", "b60", "b43", "b3", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "This work studies data-constrained scaling, focusing on the optimal use of computational resources when unique data is limited. We propose an extension to the Chinchilla scaling laws that takes into account the decay in value of repeated data, and we fit this function using a large set of controlled experiments. We find that despite recommendations of earlier work, training large language models for multiple epochs by repeating data is beneficial and that scaling laws continue to hold in the multi-epoch regime, albeit with diminishing returns. We also consider complementary approaches to continue scaling models, and find that code gives the ability to scale an additional 2\u00d7. We believe that our findings will enable further scaling of language models to unlock new capabilities with current data. However, our work also indicates that there are limits on the scaling horizon. In addition to collecting additional data, researchers should explore using current data in a more effective manner.\n[51] Aran Komatsuzaki. 2019. One epoch is all you need. arXiv preprint arXiv:1906.06669. Let N be the number of model parameters, D be the training tokens and U be the \"unique\" training tokens i.e. the size of the dataset that is to be trained on for one or more epochs. Chinchilla [42] only deals with non-repeated tokens, thus D = U and we can write their formula (\"Approach 3\") as:\nL(N, U ) = A N \u03b1 + B U \u03b2 + E (7\n)\nwhere E represents the irreducible loss. A, B, \u03b1 and \u03b2 are learned parameters.\nWe now want to generalize this expression to multiple epochs where tokens are repeated. We repeat the data R D times, where R D = 0 corresponds to the base case of a single epoch. We let D \u2032 be the \"effective data size\": the number of unique data needed to get the same value as repeating U unique tokens for R D repeats. Hence, if R D = 0, the effective data is the same as the total data processed.\nIntuitively, each time a sample is repeated, it is worth less as the model has already learned some of its information. Assume that each time a model trains on a token, it learns a 1 \u2212 \u03b4 fraction of the information in it for some constant 0 \u2264 \u03b4 \u2264 1. (Thus, if \u03b4 = 0 repeated tokens are as good as new ones, and if \u03b4 = 1, repeated tokens are worth nothing.) In other words, we expect the decrease in value of each repetition to be proportional to the value of the prior repetition, which is equivalent to exponential decay. As we would like to sum up the value of all repetitions, we temporarily assume an integral number of repeats and express it as a geometric series:\nD \u2032 = U + (1 \u2212 \u03b4)U + (1 \u2212 \u03b4) 2 U + \u2022 \u2022 \u2022 + (1 \u2212 \u03b4) R D U(8)\nWe know that the sum S of a geometric series with a common ratio r is:\nS = a(1 \u2212 r n ) 1 \u2212 r (9\n)\nwhere a is the first term and n the number of terms in the series. As r = (1 \u2212 \u03b4) and a = (1 \u2212 \u03b4)U :\nD \u2032 = U + U R D k=1 (1 \u2212 \u03b4) k = U + (1 \u2212 \u03b4)U (1\u2212(1\u2212\u03b4) R D ) \u03b4 (10\n)\nNote that Equation 10 can also be used with a non-integer number of repetitions. We can directly use Equation 10 as our effective data and learn \u03b4 but for convenience and interpretability, we redefine it in terms of the number of epochs beyond which repeating does not help. Note that as more data is repeated, the right-hand side tends to\n(1\u2212\u03b4)U \u03b4 , as lim R D \u2192\u221e (1 \u2212 (1 \u2212 \u03b4) R D ) = 1. Let R * D = 1\u2212\u03b4 \u03b4 , hence D \u2032 \"plateaus\" at U + R * D U\nas R D goes to infinity. If we assume \u03b4 to be small, 1 \u2212 \u03b4 tends to one and we can approximate 1/R * D = \u03b4 1\u2212\u03b4 \u2248 \u03b4. Next, define e x in terms of its Taylor series expansion:\ne x = 1 + x + x 2 2! + x 3 3! + \u2022 \u2022 \u2022 \u2248 1 + x (11\n)\nIf x is small later terms become increasingly small, thus e x \u2248 1 + x. As we have assumed \u03b4 to be small, let x = \u2212\u03b4, which yields\n(1 + x) = (1 \u2212 \u03b4) \u2248 e \u2212\u03b4 \u2248 e \u22121/R * D (12\n)\nNow inserting (1 \u2212 \u03b4)/\u03b4 = R * D and (1 \u2212 \u03b4) R D = e (\u22121/R * D ) R D into Equation 10 we get our final equation representing the effective data:\nD \u2032 = U + U \u2022 R * D \u2022 (1 \u2212 e \u2212R D /R * D )(13)\nwhere U and R D are given while R * D is a learned constant. If no repeats are done, the second part of the sum is zero and the term simplifies to the single-epoch scaling laws from Equation 7. While R D \u226a R * D , the second term is approximated as U \u2022 R D and for R D \u226b R * D , it plateaus at U \u2022 R * D . Hence R * D corresponds to the number of times we can repeat tokens before seeing sharply diminishing returns.\nLet us consider a concrete example to show that Equation 13 is a very good approximation of Equation 10 and make the equations more intuitive. Suppose repeated data retains 75% of its value (\u03b4 = 0.25) and we train on a single token or data unit (U = 1) for five epochs, i.e. we repeat it four times (R D = 4). In that case Equation 10\nyields D \u2032 = U + (1 \u2212 \u03b4)U (1\u2212(1\u2212\u03b4) R D ) \u03b4 = 1 + (0.75) * 4 *\n(1 \u2212 0.75 4 ) = 3.05. Thus despite training for 5 total units (4 of which are repetitions), we only get the value equivalent to 3.05 units. As we have defined R *\nD = (1 \u2212 \u03b4)/\u03b4, the corresponding R * D value is 3. Setting R * D = 3 in Equation 13 yields D \u2032 = U +U \u2022R * D \u2022(1\u2212e \u2212R D /R * D ) = 1+3 * (1\u2212e \u22124/\n3 ) = 3.21. Due to our approximations, the results are not the same, i.e. 3.21 is slightly higher than 3.05. However, note that the data term is additionally raised to a power of \u03b2 = 0.353 (see Equation 7; Appendix B), thus the actual difference calculated as ((3.21 0.353 )/(3.05 0.353 )) \u2212 1 is a mere 1.8% despite this relatively large \u03b4 of 0.25. Equation 13 has the benefit that we can interpret R * D as the number of repetitions beyond which repeating yields sharply diminishing returns and flattens out soon after. Consider R D = 100 then D \u2032 = 1 + 3 * (1 \u2212 e \u2212100/3 ) = 3.99. No matter how many repeats are done the effective data will never exceed 4 i.e. it plateaus at U + R * D U as R D tends to infinity. Similarly, we consider repeating parameters. Symmetric to seeing the same data, excess parameters learn the same features and do not add any value in the extreme. For the Chinchilla equation (Equation 7) increasing parameters from 1 billion to 10 billion yields the same absolute decrease in loss regardless of whether the dataset is a single token or 1 billion tokens. However, intuition and our data (Appendix F) suggest that in the first case, adding parameters should not decrease loss at all, as the additional 9 billion parameters cannot possibly learn anything from the single token that the first 1 billion parameters have not already learned. Thus, to allow excess parameters to decay to adding nothing, we also replace N with a symmetric version of Equation 13 yielding our final equation:\nL(U N , U D , R N , R D ) = A (U N + U N R * N (1 \u2212 e \u2212R N R * N )) \u03b1 + B (U D + U D R * D (1 \u2212 e \u2212R D R * D )) \u03b2 + E(14)\nWe define U N , as the number of \"unique\" parameters that provide an optimal fit for U D . Additional parameters decay with a symmetric version of the expression for repeated data. R N is the number that the \"unique\" parameters are repeated i.e. R N = max{(N/U N ) \u2212 1, 0}. If R * N = \u221e, additional parameters do not decay at all and (U\nN + U N R * N (1 \u2212 e \u2212R N R * N )\n) reduces to N . We compute U N from U D by setting D opt = U D and rearranging Equation 3 to map from D opt to N opt . U N is then min{N opt , N }. This is equivalent to the following:\nU N = min{((U D \u2022 G) \u03b2/\u03b1 ) \u2022 G, N } where G = \u03b1A \u03b2B 1 \u03b1+\u03b2 (15)\nEquation 14 is a generalization of Equation 7: It provides the same estimates for optimal model and data size in the single epoch case, but allows for decay in the value of parameters and tokens, thus generalizing to training for multiple epochs and with excess parameters. It can thus be used as a direct replacement of Equation 7. If R * N and R * D are unknown, one can simply set them to infinity by default, which will make Equation 14 completely equivalent to Equation 7.\nTo learn the parameters R * N and R * D , we largely follow the approach from [42]. We fix a, b, e, \u03b1, \u03b2 to the values learned on C4 in Appendix B and minimize:\nmin R * N ,R * D Run i Huber \u03b4 LSE a \u2212 \u03b1 log(U i N + U i N R * N (1 \u2212 e \u2212R i N R * N )), b \u2212 \u03b2 log(U i D + U i D R * D (1 \u2212 e \u2212R i D R * D )), e \u2212 log L i (16)\nWe use the LBFGS algorithm to find local minima of the objective above, started on a grid of initialization given by: R * N \u2208 {0., 4., . . . , 20.} and R * D \u2208 {0., 4., . . . , 20.}. We fit on 182 samples with parameters varying from 7 million up to 9 billion and epochs ranging from 1 to 500. We removed outliers referenced in Appendix F from our fitting, as our formulas do not allow for excess parameters or excess epochs to negatively impact performance. We assume excess parameters or epochs only cause performance to plateau but never to worsen. However, it is difficult to identify all samples where excess parameters or epochs hurt, as for some data budgets we only train a single model, thus we do not know if the loss of that model is already in the range where it starts to increase again. Further, there are samples where loss initially increases and then decreases as a function of epochs (double descent, see Appendix D), which further contributes to noise in the fitting. Nevertheless, we are able to get a fairly stable fit resulting in R * N = 5.309743 and R * D = 15.387756. Since R * D > R * N , excess parameters decay faster. Hence, the data-constrained efficient frontiers in Figures 1,3 suggest scaling compute allocated to epochs faster than to parameters. This value of R * D yields \u03b4 \u2248 6 * 10 \u22122 (0.19 for R * N ), which respects the assumption that \u03b4 is small. Inserting these learned parameters and the parameters from Appendix B, and simplifying Equation 15 yields the precise formulation we use to predict loss (L) given unique tokens (U N ), parameter repetitions (R N ) and data repetitions (R D ):  We experiment with different versions of our formula and display the learned values in Table 1. No decay or decaying only D or N of Equation 14 leads to worse loss and R 2 than Equation 14. Thus, it is important to decay both the value of excess parameters and data repetitions. We also consider an explicit exponential where\nL(U D , R N , R D ) = 521 (U N + 5.3 \u2022 U N (1 \u2212 e \u2212R N\nD \u2032 = R D k=0\nU * e \u2212R * D k , hence from Equation 9 it follows:\nD \u2032 = U 1\u2212(e \u2212R * D ) R D +1 1\u2212e \u2212R * D (18)\nThis explicit decay, Equation 10, and Equation 14 all yield similar results with R 2 around 80. Equation 14 fits the data slightly worse than Equation 10, likely due to our approximations. Nevertheless, we use Equation 14 throughout as it has fewer terms, and we find it easier to interpret.", "publication_ref": ["b41", "b41"], "figure_ref": ["fig_0"], "table_ref": ["tab_8"]}, {"heading": "A.1 Analytical properties of compute-optimal point", "text": "In our case, consider the setting of a fixed compute budget C and a fixed budget of unique tokens U D implying a set of unique parameters U N . Let R D denote the number of times we repeat data (we assume that we are in the multi-epoch regime and hence R D > 0).  while increasing R N by \u03f5 corresponds to increasing the number of parameters by \u03f5U N . For small positive R D , R N , our curve agrees with Chinchilla and so we need to increase R N , R D by the same amount to maintain the proportionality. Hence up to some value r > 0, the optimal compute curve corresponds to R N = R D = r. Our curve differs from Chinchilla when r gets closer to either R * N or R * D . At this point, we start to see sharply diminishing returns. In our setting, R * D > R * N which means that we reach the point r \u2248 R * N first. At this point, each added parameter is worth less (specifically worth e \u2212r/R * N ), than an added data point, despite them having equal computational cost. Hence processing more tokens will be more effective than increasing the number of parameters, and we expect the optimal compute curve to break away from proportionality. This is indeed what we see.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B C4 Scaling Coefficients", "text": "While Hoffmann et al. [42] have shown that the equal scaling of model parameters and training tokens holds across different training datasets, the precise ratios vary considerably across datasets and approaches. For example given the Gopher [89] compute budget of 5.76 \u00d7 10 23 FLOPs, their parametric loss function fitted on MassiveWeb predicts an optimal allocation of 40 billion parameters. Meanwhile, if the training dataset is C4 [90] their IsoFLOP approach predicts 73 billion parameters to be optimal, almost twice as much. However, for C4, which is our training dataset, they do not provide the coefficients necessary to compute loss with their parametric loss function. Based on their IsoFLOP training runs on C4, they only provide the information that for C4, compute (C) allocated to data (D) and parameters (N ) should be scaled exactly equally for optimality, i.e. a = b = 0.5 in the relationship N opt \u221d C a and D opt \u221d C b . This corresponds to \u03b1 = \u03b2 in the parametric loss function (Equation 2). Thus, we use this information together with the methodology and C4 data points from [42] to fit the parametric loss function. We tie the parameters \u03b1 and \u03b2 to be equal and optimize\nmin a,b,e,\u03b1,\u03b2 Run i Huber \u03b4 LSE a \u2212 \u03b1 log N i , b \u2212 \u03b2 log D i , e \u2212 log L i (19\n)\nwhere LSE is the log-sum-exp operator and N i , D i and L i the model size, dataset size and loss of the ith run, and \u03b4 = 10 \u22123 . We fit on 54 samples on a grid of initialization given by: \u03b1 \u2208 {0., 0. To verify the accuracy of our fit, we benchmark the predictions with those of the IsoFLOP C4 curves in [42]. Following [42], we can compute the optimal number of parameters N opt and tokens D opt for our fit using:\nN opt (C) = G C 6 a , D opt (C) = G \u22121 C 6 b where G = \u03b1A \u03b2B 1 \u03b1+\u03b2 , a = \u03b2 \u03b1 + \u03b2 , and b = \u03b1 \u03b1 + \u03b2 (21)\nGiven the Gopher compute budget of C = 5.76 \u00d7 10 23 our fitted parameters predict an optimal allocation of N opt = 70.0 billion parameters and D opt = 1.37 trillion tokens. This is very close to the 73 billion parameters and 1.3 trillion tokens predicted by the IsoFLOP curves on C4 from [42] and thus we consider it a good fit. We use these fitted parameters rather than the MassiveWeb parameters for all computations involving Chinchilla scaling laws.   ", "publication_ref": ["b41", "b41", "b41", "b41", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "C Additional Contour Plots", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Double Descent", "text": "Prior work has reported double descent phenomena when repeating data, where the loss initially increases and then decreases again as the model is trained for more epochs [75,40]. In Figure 9, we plot the loss curves of several models trained for varying epochs on 100 million tokens. We find double descent phenomena with the loss of all models increasing at 200 epochs before decreasing again. This contributes to additional noise in the fitting of our functions in Appendix A, as our functional form assumes loss to be monotonically decreasing as epochs increase. Thus, we remove most such examples from the fitting.  ", "publication_ref": ["b39"], "figure_ref": ["fig_14"], "table_ref": []}, {"heading": "E Repeating on Heavily Deduplicated Data", "text": "To investigate whether Figure 3 is dependent on the inherent amount of duplicates in the selected 100 million tokens, we train several models on a deduplicated version of C4 (see Appendix N). We plot the performance of the models trained on the deduplicated C4 versus the regular C4 in Figure 10. All models are evaluated on the same validation dataset from the regular C4. Regardless of deduplication we find 59 epochs to be optimal and the overall trend to be very similar. Together with our results on OSCAR (Appendix I), this suggests that our work generalizes to different datasets with different inherent amounts of duplicates. ", "publication_ref": [], "figure_ref": ["fig_4", "fig_0"], "table_ref": []}, {"heading": "Final test loss", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Training data", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Deduplicated C4 C4", "text": "Figure 10: Optimal loss on deduplicated data. 146 million parameter models trained on 100 million unique tokens that are either directly from C4 or undergo additional deduplication. Each dot is a single model. While deduplication results in a higher test loss, the optimal number of epochs remains the same whether or not deduplication is performed (see also Figure 3).", "publication_ref": [], "figure_ref": ["fig_0", "fig_4"], "table_ref": []}, {"heading": "F Do Excess Parameters Hurt, Plateau or Help?", "text": "Figures 3, 8 suggest that excess parameters (or epochs) can harm performance. We hypothesize that this is due to suboptimal hyperparameters and could be prevented with better regularization. Thus, we expect with optimal regularization hyperparameters excess parameters would never hurt, but performance would merely plateau, as in extreme cases regularization could just take the form of removing the excess parameters. One approach to selecting optimal hyperparameters is \u00b5P [127]. We compare excessively large models trained with a data constraint of D C = 100 million tokens in Figure 11 across \u00b5P, our default hyperparameters (Appendix S) and scaling law predictions. Surprisingly, \u00b5P leads to even higher test loss than our default hyperparameters. Nevertheless, we find that also with \u00b5P excessive parameters hurt: The models with more than 2 billion parameters have significantly higher validation loss after training than the models with 200 million to 1 billion parameters when trained on only 100 million tokens. However, \u00b5P only covers hyperparameters such as the learning rate, but not explicit regularization hyperparameters like dropout rates, which we hypothesize would prevent this behavior. Thus, our proposed scaling equations predict loss to plateau, as seen in the straight line. As the compute-optimal parameter count for 100 million tokens is around 7 million, all depicted models have a significant amount of excess parameters and data-constrained scaling laws predict their losses to be all the same (R * N \u226a R N ). Meanwhile, the default Chinchilla scaling law [42] predicts loss to continue decreasing as parameters are added, which is in stark contrast to the empirical data.\nIf one wants to incorporate excess parameters hurting performance into the scaling law equations, one could consider (a) Modifying the exponential decay formulation introduced in Appendix A such that instead of the value of repeated data decaying to 0 it decays to a large negative value (b) decaying the exponents \u03b1 and \u03b2 in Equation 7 instead of D and N . Decaying the exponents to 0 has the effect of more repetitions eventually hurting performance as lim \u03b1\u21920 D \u03b1 = 1 and the same for \u03b2. Thus, initially as D and N increase loss decreases, but ultimately the decay of \u03b1 and \u03b2 pushes D and N back to 1 resulting in loss to increase. Specifically, approach (b) could take the form of:\nL(N, D, R N , R D ) = E + A N \u03b1 * max(0,1\u2212(R N /R * N )) + B D \u03b2 * max(0,1\u2212(R D /R * D ))(22)\nLike the equations in Appendix A this formulation also reduces to the Chinchilla scaling laws in the base case of R D = 0 or R N = 0. As the exponents decrease with more repetitions adding parameters or epochs becomes less beneficial. Eventually, the decay in \u03b1 or \u03b2 causes loss to increase again as it pushes N or D back down to 1. We fit this formula using the same approach outlined in Appendix A but including samples where excess parameters or epochs hurt (296 total samples). We use a grid of initialization given by: R * N \u2208 {0., 2000., . . . , 100000.} and R * D \u2208 {0., 2000., . . . , 100000.}. This results in R * D = 26530.611 and R * N = 2040.8163. R * N is significantly lower resulting in excess parameters hurting faster than excess epochs, which is in line with empirical data from Figure 3. We visualize Figure 3 with the predictions from this alpha-beta decay formulation in Figure 12. Expected parameters eventually hurt resulting in circle-shaped contours. Due to the very high R * D the area where epochs start to hurt is outside of the boundaries of Figure 12. While the predicted optimal allocation (efficient frontier) is similar to Figure 3, the predicted return from repeated data differs significantly. The alpha-beta decay formulation incorrectly predicts returns to diminish significantly slower as seen by the longer efficient frontier and the smaller distance in contours early on as compared to Figure 3. Beyond its potentially useful properties, we do not have a rigorous mathematical justification for this alpha-beta decay formulation which could be the cause of the incorrect return predictions.\nUltimately, we settle on our exponential decay formulation from Appendix A that does not allow excess parameters or epochs to hurt, as preventing such behavior is trivial by stopping training (in the case of epochs hurting) or removing excess parameters (in the case of model parameters hurting). Further, accurately predicting how much loss increases in the limit is not very useful, as in practice one would want to stop training when it's expected to plateau anyways.  Excess parameters empirically hurt performance, but this may be due to a lack of regularization. Thus, our scaling formula predicts loss to plateau, while Chinchilla predicts loss to improve. By decaying the exponent \u03b1 (and \u03b2) instead, one can allow excess parameters to hurt.    The Galactica models [108] are the only publicly known LLMs that explicitly trained for a significant number of epochs prior to this work. They trained their models on 106 billion unique tokens for 4.25 epochs. Our findings on Return from repeated data agree with their conclusion that multiple epochs are beneficial, however, we find that even more epochs can be beneficial and a small spike in validation loss does not justify stopping training (Appendix J). Meanwhile, our findings on Allocation significantly deviate from Galactica. Figure 13 visualizes the Galactica models with our predicted efficient frontier in the same style as Figure 1. The creators of Galactica decided to train a 120 billion parameter model on 450 billion tokens, a significant overallocation to parameters even in Chinchilla terms (black efficient frontier). This decision was likely driven by the intuition that repeated data is worth less, thus one should spend more compute on parameters. However, our empirical data contradicts this. Parameters learning from repeated data are worth even less than repeated data, thus one should overallocate to epochs, not parameters. Our data-constrained scaling laws thus predict that a better model could have been trained by allocating significantly more FLOPs to epochs rather than parameters for the largest Galactica model with 120 billion parameters. Specifically, 40 billion parameters trained for 1.35 trillion tokens (12.75 epochs) would have been optimal according to data-constrained scaling laws. Note that these scaling laws have been fitted on C4, which is not the dataset used to pre-train Galactica. The Galactica models are pre-trained on a predominantly scientific dataset, which includes code data among other data sources. Results from [42] show that there are differences in the scaling coefficients when training on C4 as compared to GitHub code, however, the overall allocation trend is the same. Thus, while we expect a smaller model trained for more epochs to be better than the 120 billion parameter model, the optimal allocation is unlikely to be exactly 40 billion parameters and 1.35 trillion tokens.", "publication_ref": ["b61", "b41", "b41"], "figure_ref": ["fig_4", "fig_0", "fig_4", "fig_4", "fig_0", "fig_0", "fig_4", "fig_4", "fig_0", "fig_0"], "table_ref": []}, {"heading": "H Training Loss", "text": "Hoffmann et al. [42] use training loss as their core metric. However, when repeating data for multiple epochs, training loss is a bad metric as models will overfit to the limited data available as shown in Figure 14. Thus, we use loss on a held-out test set as our key performance metric.  ", "publication_ref": ["b41"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "I Scaling Curves on the OSCAR Corpus", "text": "To ensure our findings are not dataset-dependent, we train models with the same configurations from Figure 4   parameter OSCAR model shows loss spikes. However, these are temporary and loss continues to go down smoothly thereafter. Thus, we hypothesize that the Galactica models could have attained better performance by continuing pre-training beyond the loss spike experienced at the beginning of the fifth epoch.  4, the configurations are displayed in Table 2. The small number of evaluation tokens for the 8.7 billion parameter models likely contributes to the loss spikes for 8.7 billion parameter models seen in Figure 4. Thus, we smooth the validation loss curves of 8.7 billion parameter models with exponential moving average smoothing and a weight of 0.85. For training OSCAR, configurations are the same, however, the validation split used is a held-out part from the OSCAR training split, as there is no official validation split for OSCAR. All training loss curves for C4 and OSCAR models are smoothed with exponential moving average smoothing and a weight of 0.999.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_13"]}, {"heading": "K Evaluation Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Downstream evaluation", "text": "We provide statistics of all downstream evaluation datasets in Table 3.\nWe use the evaluation-harness frameworks from BigScience and EleutherAI [32] to evaluate models on 19 evaluation datasets. For each dataset, a maximum of 3000 samples are evaluated with 0,1,2,3,4 and 5 few-shots [15] to produce six scores which are then averaged. We normalize scores to range from the random baseline of each task to 1 and report them as percentages. For example, if random guessing produces 50% accuracy and the maximum accuracy possible is 100%, then a raw accuracy of 55% would be normalized to 10%, and a raw accuracy of 45% would be normalized to -10% since it is worse than random. This is done to give all tasks the same weight. Otherwise average performance would heavily depend on generative tasks, where the random baselines are 0. Prompts are sourced from GPT-3 [15] and PromptSource [5] and detailed in Appendix T. We note that our evaluation is in no means comprehensive and a larger benchmarking would be helpful [102,73]. However, by training five seeds for most models benchmarked, always averaging 0-5 fewshots, and ensuring maximum data overlap for repeated data ( \u00a74) we significantly reduce uncertainty. ", "publication_ref": ["b31", "b14", "b14", "b4"], "figure_ref": [], "table_ref": ["tab_14"]}, {"heading": "L Downstream Repetition Results", "text": "In Tables 4-9 we report downstream results of all models trained on C4 [90] and OSCAR [83] according to the configurations in Figure 4. All scores are from the final checkpoints at the end of training. OSCAR is a noisier dataset than C4 due to less filtering, thus models trained on C4 generally perform better. Notably, models trained on C4 completely fail on bAbI [122], while OSCAR models are able to perform better than random. This is likely due to code data being present in OSCAR, which enables state-tracking capabilities like for code augmented models in \u00a77. For C4 the creators strictly removed all data that resembles code [90]. There are no significant differences between models trained for a single epoch and models trained for up to 4 epochs. Even models trained for more epochs (and thus on less unique data) have similar performance.        To compare complementary data strategies in \u00a77, we have used downstream performance on natural language tasks detailed in Appendix K instead of loss. This is because validation loss gives an unfair advantage to models trained on a larger fraction of data from the same distribution. For example, when making up for missing natural language data with code, models that are trained on more code will have better validation loss on code data while having worse loss on the natural language data as seen in Figure 19: The model pre-trained on 90% of Python code data and 10% of C4 has the highest C4 validation loss, but the lowest Python validation loss.\nModels trained on deduplicated or perplexity-filtered data have higher validation loss as the held-out validation data has not gone through the same filtering steps. Thus, its distribution more closely resembles the training data of models trained on the unfiltered data resulting in worse validation loss for the two filtering strategies in Figure 20 (left). Meanwhile, for training loss in Figure 20 (right) the model trained on perplexity-filtered data has the lowest loss. ", "publication_ref": ["b56"], "figure_ref": ["fig_0", "fig_24", "fig_24"], "table_ref": []}, {"heading": "R Contributions", "text": "Niklas Muennighoff led experiments, analysis, writing, and the overall project. He implemented, trained and evaluated all models.\nAlexander M. Rush contributed to framing, results analysis, and paper writing.\nBoaz Barak contributed to formal and experimental analysis as well as paper writing.\nTeven Le Scao provided guidance, led data choices and preprocessing, and contributed to framing and writing.\nAleksandra Piktus created perplexity and deduplication datasets and contributed to writing.\nNouamane Tazi contributed to enabling high-performance training on AMD hardware.\nSampo Pyysalo contributed to enabling high-performance training and early repetition experiments.\nThomas Wolf provided guidance on experimental design and contributed to paper writing.\nColin Raffel provided guidance on experimental design and contributed to paper writing.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "S Hyperparameters and Setup", "text": "For all training runs we use 1% of tokens for linear warm-up of the learning rate to a maximum learning rate of 2e-4 that is decayed to 2e-5 following a cosine schedule. We use a batch size of 256 for models with fewer than 2 billion parameters, 512 for models with 2 -5 billion parameters and 1024 for models with more than 5 billion parameters. All models are trained in bfloat16 precision using the Adam optimizer [48] with eps = 1e \u2212 8, beta1 = 0.9. For beta2, we found a value of 0.95 to result in slightly lower final loss and fewer loss spikes than the default value of 0.999 in implementations such as PyTorch. However, except for models with FLOP budgets of C = 9.3\u00d710 20 and 2.1 \u00d7 10 21 , we always use beta2 = 0.999. We use a dropout rate of 0.1, a weight decay rate of 0.1 and clip gradients at 1.0. These hyperparameter choices are largely based on prior work [42,110] and performance on test runs. As none of our hyperparameter choices is particularly exotic, we expect our setup to generalize to many other setups. In Table 15 we list the model architectures we use. They are an extended version of the architectures from [42]. We calculate model parameters following [78], which includes embedding parameters:\nP = 12lh 2 1 + 13 12h + V + s 12lh (23\n)\nwhere P is the final parameter count, l are layers, h is the hidden dimension, V = 50257 the vocabulary size and s = 2048 the sequence length. We find the parameter counts reported in Chinchilla [42] to be significantly different than our calculations, especially at larger scales. We report both in Table 15, but we use our parameter estimates everywhere in this work. Further, we have corrected the number of heads of the 3,530 and 4,084 million parameter models from [42] to obey the relationship d_model = kv_size \u2022 n_heads.\nTo train our models, we have forked the Megatron-DeepSpeed [91, 99] framework and adapted it for ROCm to enable training on AMD GPUs. We have made our training code publicly available at https://github.com/TurkuNLP/Megatron-DeepSpeed. Models are trained using data, tensor and pipeline parallelism on up to 256 AMD Instinct MI250X GPUs distributed across up to 64 nodes on the LUMI supercomputer located in Finland. As of June 2023, LUMI is the largest supercomputer in Europe and ranks third worldwide with a performance of around 310 PFLOPs. 5 We trained models in parallel using up to 2,200 nodes at a single point in time (equivalent to around 8,800 GPUs or 17,600 GCDs or 86% of all GPUs on LUMI). We have used a total of around 3 million GPU hours. The cluster is powered 100% by renewable energy (hydroelectricity) and its waste heat is used for heating the nearby city reducing the city's carbon emissions by up to 20%. Thanks to the low temperatures in Finland, relatively little cooling for the cluster is required further reducing its impact on the environment. As of June 2023, it ranks as the seventh greenest supercomputer. 6 Parameters (millions)     Figure 32: Formatted dataset example from SciQ evaluated using accuracy as described in Appendix K.\nContext \u2192 Attributes are placed within the tag itself, making additional alterations to the \u00eblement contentbetween the start and end tag. They never stand alone. They are written in the format name=value, where name is the name of the attribute (for instancecolor), and value describes this specific instance (for instancered). You've actually seen attributes before, if you followed the tutorial in the basic HTML section. <img> tags use the src attribute, anchors use the name attribute, and links use the href attribute. See how those all follow the ___=___format? Making a  ", "publication_ref": ["b47", "b41", "b41", "b41", "b41", "b4", "b5"], "figure_ref": ["fig_4"], "table_ref": ["tab_8", "tab_8"]}, {"heading": "Acknowledgments and Disclosure of Funding", "text": "This work was co-funded by the European Union under grant agreement No 101070350. The authors wish to acknowledge CSC -IT Center for Science, Finland, for generous computational resources on the LUMI supercomputer. 3 We are thankful for the immense support from teams at LUMI and AMD, especially Samuel Antao. Hugging Face provided storage and additional compute instances. This work was supported by a Simons Investigator Fellowship, NSF grant DMS-2134157, DARPA grant W911NF2010021, and DOE grant DE-SC0022199. We are grateful to Harm de Vries, Woojeong Kim, Mengzhou Xia and the EleutherAI community for exceptional feedback. We thank Loubna Ben Allal for help with the Python data and Big Code members for insightful discussions on scaling laws. We thank Thomas Wang, Helen Ngo and TurkuNLP members for support on early experiments.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "ANLI R1 0.4 \u00b1 1.6 0.7 \u00b1 0.8 0.3 \u00b1 0.5 -0.3 \u00b1 1.8 0.4 \u00b1 1.8 0.4 \u00b1 0.7 0.0 \u00b1 0.9 -0.6 \u00b1 0.6 ANLI R2 0.9 \u00b1 0.4 1.4 \u00b1 0.8 0.8 \u00b1 0.8 1.1 \u00b1 0.7 0.5 \u00b1 0.7 0.6 \u00b1 1.0 1.1 \u00b1 1.1 2.7 \u00b1 1.6 ANLI R3 1.7 \u00b1 0.5 1.2 \u00b1 0.4 0.4 \u00b1 0.5 1.9 \u00b1 0.7 0.6 \u00b1 1.0 0.8 \u00b1 0.8 1.7 \u00b1 0.7 0.7 \u00b1 1.7 ARC-Challenge 1.6 \u00b1 1.0 0.9 \u00b1 0.5 1.2 \u00b1 0.6 1.1 \u00b1 0.6 1.1 \u00b1 1.2 1.3 \u00b1 0.5 0.3 \u00b1 0.6 -2.9 \u00b1 1.0 ARC-Easy 44.5 \u00b1 0.5 44.9 \u00b1 0.4 44.7 \u00b1 0.7 44.3 \u00b1 0.4 44.0 \u00b1 0.5 44.2 \u00b1 0.9 41.4 \u00b1 0.2 28.9 \u00b1 0. 83.2 \u00b1 0.6 82.5 \u00b1 0.6 82.7 \u00b1 1.1 81.9 \u00b1 0.6 81.9 \u00b1 0.8 81.6 \u00b1 0.9 78.5 \u00b1 1.1 59.3 \u00b1 1.6 StoryCloze 2016 58.7 \u00b1 0.2 58.7 \u00b1 0.5 58.5 \u00b1 0.3 58.3 \u00b1 0.3 58.5 \u00b1 0.6 58.4 \u00b1 0.3 56.7 \u00b1 0.5 52.0 \u00b1 0.6 WinoGrande XL 11.6 \u00b1 0.8 10.8 \u00b1 1.1 10.9 \u00b1 1.3 10.6 \u00b1 0.5 11.1 \u00b1 0.9 10.6 \u00b1 0.9 6.4 \u00b1 1.3 2.9 \u00b1 1.3 E2E NLG 17.0 \u00b1 1.4 17.7 \u00b1 0.5 17.0 \u00b1 1.2 16.9 \u00b1 1.1 15.1 \u00b1 2.3 13.3 \u00b1 2.2 14.9 \u00b1 0.9 9.8 \u00b1 0.9 XSUM 2.4 \u00b1 0.1 2.4 \u00b1 0.1 2.5 \u00b1 0.1 2.3 \u00b1 0.2 2.4 \u00b1 0.1 2.4 \u00b1 0.1 2.1 \u00b1 0.1 1.6 \u00b1 0.1 WebNLG EN 5.3 \u00b1 0.1 5.5 \u00b1 0.2 5.4 \u00b1 0.1 5.4 \u00b1 0.1 5.1 \u00b1 0.1 5.4 \u00b1 0.2 5.1 \u00b1 0.3 2.9 \u00b1 0.2 WikiLingua EN 3.0 \u00b1 0.1 3.1 \u00b1 0.1 2.9 \u00b1 0.1 2.9 \u00b1 0.3 2.9 \u00b1 0.2 2.9 \u00b1 0.1 2.6 \u00b1 0.1 2.0 \u00b1 0.2 bAbI 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 Average 20.9 \u00b1 0.4 20.4 \u00b1 0.3 20.4 \u00b1 0.2 20.6 \u00b1 0.2 19.9 \u00b1 0.9 19.7 \u00b1 0.2 19.2 \u00b1 0.5 14. ANLI R1 -0.3 \u00b1 0.5 -0.6 \u00b1 1.3 0.2 \u00b1 0.6 0.3 \u00b1 1.1 0.2 \u00b1 1.2 -0.1 \u00b1 1.1 -0.1 \u00b1 0.5 -0.7 \u00b1 1.3 ANLI R2\n1.0 \u00b1 1.0 1.1 \u00b1 0.3 1.7 \u00b1 0.8 2.3 \u00b1 0.8 1.4 \u00b1 1.0 0.8 \u00b1 0.7 1.0 \u00b1 0.7 2.3 \u00b1 0.7 ANLI R3 0.4 \u00b1 0.8 0.5 \u00b1 0.5 -0.2 \u00b1 0.8 -0.1 \u00b1 1.0 1.1 \u00b1 0.6 0.7 \u00b1 0.4 -0.2 \u00b1 0.9 0.5 \u00b1 1.2 ARC-Challenge -1.4 \u00b1 0.8 -0.6 \u00b1 0.8 -1.7 \u00b1 0.1 -1.6 \u00b1 0.7 -1.6 \u00b1 0.6 -1.4 \u00b1 0.5 -1.9 \u00b1 0.8 -5.0 \u00b1 1.1 ARC-Easy 39.7 \u00b1 0.3 39.6 \u00b1 0.8 39.5 \u00b1 0.6 39.3 \u00b1 0.5 38.7 \u00b1 0.6 38.7 \u00b1 0.4 36.9 \u00b1 0. 4 \n0.8 \u00b1 0.5 0.4 \u00b1 0.8 0.3 \u00b1 1.0 0.6 \u00b1 0.5 0.5 \u00b1 0.6 0.3 \u00b1 0.6 0.8 \u00b1 0.7 0.4 \u00b1 0.5   [37] trained on Wikipedia introductions and available to download from their repository. 4 We compute the model's perplexity on all OSCAR and C4 samples and only select samples that fall within a certain percentile threshold. For example, to select the top 25%, we only select samples with perplexity lower than the 25th percentile. Figure 18 provides a visual representation of perplexity distribution for respective datasets, highlighting the relevant percentile thresholds.\nDeduplication We perform deduplication leveraging the suffix array-based approach proposed by Lee et al. [55]. We remove any document with at least a 100-character span overlapping with any other document in the corpus. We deduplicate the full C4 dataset. In the case of OSCAR, the memory requirements of the deduplication procedure make performing the full dataset deduplication infeasible. Instead, we select a 25% subset of the full OSCAR and build a suffix array for this subset. We experiment with leveraging the 25% OSCAR suffix array in two ways. First, we deduplicate the selected subset. This is very strict and preserves less than 5% of the full OSCAR. Subsequently, we use the 25% suffix array to deduplicate the full OSCAR, i.e. we remove any document which has at least a 100-character span overlapping with the 25% subset we selected. This is more permissive and allows us to preserve 31% of the original dataset. We refer to the latter as expanded in Table 12 and it is used for the training of the 4.2 billion parameter model in Table 14, while the smaller deduplicated version of OSCAR is used for the 2.8 billion parameter model.", "publication_ref": ["b3", "b36", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "ROOTS filter", "text": "In addition, we benchmark with the filtering procedure from the ROOTS corpus [54]. It applies the following set of filters:\n\u2022 Discarding documents with too few words \u2022 Discarding documents with overly repeated character-and word-n-grams \u2022 Discarding documents with too many special characters    ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "T Prompts and Samples", "text": "The following figures illustrate the prompts with samples from each evaluation dataset. Prompts stem from PromptSource [5] or GPT-3 [15]. All data comes from the ground truth datasets in this section, and no generations are shown here.   Context \u2192 An astronomer observes that a planet rotates faster after a meteorite impact. Which is the most likely effect of this increase in rotation?\nCorrect Answer \u2192 Planetary days will become shorter. Incorrect Answer \u2192 Planetary years will become longer. Incorrect Answer \u2192 Planetary gravity will become stronger.\nFigure 24: Formatted dataset example from ARC-Challenge evaluated using accuracy as described in Appendix K.\nContext \u2192 To express the distance between the Milky Way galaxy and other galaxies, the most appropriate unit of measurement is the   Context \u2192 Bob went to the gas station to fill up his car. His tank was completely empty and so was his wallet. The cashier offered to pay for his gas if he came back later to pay. Bob felt grateful as he drove home. Answer:\nCorrect Answer \u2192 Bob believed that there were good people in the world. Incorrect Answer \u2192 Bob contemplated how unfriendly the world was.  Generate some text about this restaurant.", "publication_ref": ["b4", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Target \u2192", "text": "The Wrestlers offers Japanese food and pub with cheap price near Raja Indian Cuisine in riverside. Context \u2192 Article: The artificial intelligence system -LipNet -watches video of a person speaking and matches the text to the movement of their mouths with 93% accuracy, the researchers said.\nAutomating the process could help millions, they suggested. But experts said the system needed to be tested in real-life situations.\nLip-reading is a notoriously tricky business with professionals only able to decipher what someone is saying up to 60% of the time. \"Machine lip-readers have enormous potential, with applications in improved hearing aids, silent dictation in public spaces, covert conversations, speech recognition in noisy environments, biometric identification and silent-movie processing,\" wrote the researchers. They said that the AI system was provided with whole sentences so that it could teach itself which letter corresponded to which lip movement.\nTo train the AI, the team -from Oxford University's AI lab -fed it nearly 29,000 videos, labelled with the correct text. Each video was three seconds long and followed a similar grammatical pattern. While human testers given similar videos had an error rate of 47.7%, the AI had one of just 6.6%. The fact that the AI learned from specialist training videos led some on Twitter to criticise the research. Writing in OpenReview, Neil Lawrence pointed out that the videos had \"limited vocabulary and a single syntax grammar\". \"While it's promising to perform well on this data, it's not really groundbreaking. While the model may be able to read my lips better than a human, it can only do so when I say a meaningless list of words from a highly constrained vocabulary in a specific order,\" he writes.\nThe project was partially funded by Google's artificial intelligence firm DeepMind.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Summary:", "text": "Target \u2192 Scientists at Oxford University have developed a machine that can lip-read better than humans.   The Pile We have also trained several models on The Pile [31] and found similar trends as for OSCAR and C4. We make these models publicly available.", "publication_ref": ["b30"], "figure_ref": [], "table_ref": []}, {"heading": "V Release of Artifacts", "text": "We open-source all of our models and code under Apache 2.0 licenses. Our filtered datasets are released with the same licenses as the datasets they stem from. All material can be found at: https://github.com/ huggingface/datablations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "W Version Control", "text": "V3 \u2192 V4:\n\u2022 Added comparison of different fits in terms of loss and R 2 in Table 1 \u2022 Small writing improvements V2 \u2192 V3:\n\u2022 Added loss curves of complementary strategies in Appendix P\n\u2022 Fixed OSCAR validation plot in Appendix I\n\u2022 Clarified the usage of smoothing in training and validation plots in Appendix K\n\u2022 Added more references V1 \u2192 V2:\n\u2022 Added experiments decaying alpha and beta to allow excess epochs or paramters to hurt in Appendix F\n\u2022 Added Galactica case study in Appendix G\n\u2022 Added more details on the calculation of UN given UD in Appendix A\n\u2022 Added hyperparameter sensitivity limitation in Appendix Q\n\u2022 Added more detail on how score normalization is done in Appendix K\n\u2022 Mentioned modification of number of heads in Appendix S", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "X Broader Impacts", "text": "Large Language Models carry potential risks such as outputting offensive language, propagating social biases, and leaking private information [119,8]. By publicly releasing all of our models and providing new insights to improve the scaling of LLMs we may contribute to the further proliferation of these harms. However, we note that there are already much larger and more capable models freely available [14,13,95,11] that can be used in such harmful ways. Thus, we consider the open-source release of our models and research to significantly outweigh its downsides.", "publication_ref": ["b53", "b7", "b13", "b12", "b10"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Scaling Laws for Generative Mixed-Modal Language Models", "journal": "", "year": "2023", "authors": "Armen Aghajanyan; Lili Yu; Alexis Conneau; Wei-Ning Hsu; Karen Hambardzumyan; Susan Zhang; Stephen Roller; Naman Goyal; Omer Levy; Luke Zettlemoyer"}, {"ref_id": "b1", "title": "Revisiting neural scaling laws in language and vision", "journal": "", "year": "2022", "authors": "Behnam Ibrahim M Alabdulmohsin; Xiaohua Neyshabur;  Zhai"}, {"ref_id": "b2", "title": "SantaCoder: don't reach for the stars! arXiv preprint", "journal": "", "year": "2023", "authors": "Raymond Loubna Ben Allal; Denis Li; Chenghao Kocetkov; Christopher Mou; Carlos Akiki; Niklas Munoz Ferrandis; Mayank Muennighoff; Alex Mishra; Manan Gu;  Dey"}, {"ref_id": "b3", "title": "Suggesting accurate method and class names", "journal": "", "year": "2015", "authors": "Miltiadis Allamanis; T Earl; Christian Barr; Charles Bird;  Sutton"}, {"ref_id": "b4", "title": "Urmish Thakker, Khalid Almubarak, Xiangru Tang, Xiangru Tang, Mike Tian-Jian Jiang, and Alexander M. Rush. 2022. PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts", "journal": "", "year": "", "authors": "H Stephen; Victor Bach; Zheng-Xin Sanh; Albert Yong; Colin Webson;  Raffel; V Nihal; Abheesht Nayak; Taewoon Sharma;  Kim; Thibault Bari; Zaid Fevry; Manan Alyafeai; Andrea Dey; Zhiqing Santilli; Srulik Sun; Canwen Ben-David; Gunjan Xu; Han Chhablani; Jason Alan Wang; Maged S Fries; Shanya Al-Shaibani;  Sharma"}, {"ref_id": "b5", "title": "Jaehoon Lee, and Utkarsh Sharma. 2021. Explaining neural scaling laws", "journal": "", "year": "", "authors": "Yasaman Bahri; Ethan Dyer; Jared Kaplan"}, {"ref_id": "b6", "title": "Colin Cherry, Behnam Neyshabur, and Orhan Firat. 2022. Data scaling laws in NMT: The effect of noise and architecture", "journal": "PMLR", "year": "", "authors": "Yamini Bansal; Behrooz Ghorbani; Ankush Garg; Biao Zhang"}, {"ref_id": "b7", "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?", "journal": "", "year": "2021", "authors": "Emily M Bender; Timnit Gebru; Angelina Mcmillan-Major; Shmargaret Shmitchell"}, {"ref_id": "b8", "title": "Colossal-AI: A unified deep learning system for large-scale parallel training", "journal": "", "year": "2021", "authors": "Zhengda Bian; Hongxin Liu; Boxiang Wang; Haichen Huang; Yongbin Li; Chuanrui Wang; Fan Cui; Yang You"}, {"ref_id": "b9", "title": "Emergent and Predictable Memorization in Large Language Models", "journal": "", "year": "2023", "authors": "Stella Biderman; Lintang Usvsn Sai Prashanth; Hailey Sutawika; Quentin Schoelkopf; Shivanshu Anthony; Edward Purohit;  Raf"}, {"ref_id": "b10", "title": "Pythia: A suite for analyzing large language models across training and scaling", "journal": "", "year": "2023", "authors": "Stella Biderman; Hailey Schoelkopf; Quentin Anthony; Herbie Bradley; O' Kyle; Eric Brien; Mohammad Aflah Hallahan; Shivanshu Khan;  Purohit; Edward Usvsn Sai Prashanth;  Raff"}, {"ref_id": "b11", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language", "journal": "", "year": "2020", "authors": "Yonatan Bisk; Rowan Zellers; Jianfeng Ronan Le Bras; Yejin Gao;  Choi"}, {"ref_id": "b12", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model", "journal": "", "year": "2022", "authors": "Sid Black; Stella Biderman; Eric Hallahan; Quentin Anthony; Leo Gao; Laurence Golding; Horace He; Connor Leahy; Kyle Mcdonell; Jason Phang"}, {"ref_id": "b13", "title": "GPT-Neo: Large scale autoregressive language modeling with mesh-tensorflow. If you use this software, please cite it using these metadata", "journal": "", "year": "2021", "authors": "Sid Black; Leo Gao; Phil Wang; Connor Leahy; Stella Biderman"}, {"ref_id": "b14", "title": "Language models are few-shot learners", "journal": "", "year": "2020", "authors": "Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell"}, {"ref_id": "b15", "title": "Quantifying memorization across neural language models", "journal": "", "year": "2022", "authors": "Nicholas Carlini; Daphne Ippolito; Matthew Jagielski; Katherine Lee; Florian Tramer; Chiyuan Zhang"}, {"ref_id": "b16", "title": "The 2020 Bilingual, Bi-Directional WebNLG+ Shared Task Overview and Evaluation Results", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Claire Thiago Castro Ferreira; Nikolai Gardent; Chris Ilinykh; Simon Van Der Lee; Diego Mille; Anastasia Moussallem;  Shimorina"}, {"ref_id": "b17", "title": "Generative pretraining from pixels", "journal": "PMLR", "year": "2020", "authors": "Mark Chen; Alec Radford; Rewon Child; Jeffrey Wu; Heewoo Jun; David Luan; Ilya Sutskever"}, {"ref_id": "b18", "title": "Palm: Scaling language modeling with pathways", "journal": "", "year": "2022", "authors": "Aakanksha Chowdhery; Sharan Narang; Jacob Devlin; Maarten Bosma; Gaurav Mishra; Adam Roberts; Paul Barham;  Hyung Won; Charles Chung; Sebastian Sutton;  Gehrmann"}, {"ref_id": "b19", "title": "Scaling Instruction-Finetuned Language Models", "journal": "", "year": "2022", "authors": " Hyung Won; Le Chung; Shayne Hou; Barret Longpre; Yi Zoph; William Tay; Eric Fedus; Xuezhi Li; Mostafa Wang; Siddhartha Dehghani; Albert Brahma;  Webson; Shane Shixiang; Zhuyun Gu; Mirac Dai; Xinyun Suzgun; Aakanksha Chen; Sharan Chowdhery; Gaurav Narang; Adams Mishra; Vincent Yu; Yanping Zhao; Andrew Huang; Hongkun Dai; Slav Yu; Ed H Petrov; Jeff Chi; Jacob Dean; Adam Devlin; Denny Roberts; Quoc V Zhou; Jason Le;  Wei"}, {"ref_id": "b20", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions", "journal": "", "year": "2019", "authors": "Christopher Clark; Kenton Lee; Ming-Wei Chang; Tom Kwiatkowski; Michael Collins; Kristina Toutanova"}, {"ref_id": "b21", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge", "journal": "", "year": "2018", "authors": "Peter Clark; Isaac Cowhey; Oren Etzioni; Tushar Khot; Ashish Sabharwal; Carissa Schoenick; Oyvind Tafjord"}, {"ref_id": "b22", "title": "Unsupervised Cross-lingual Representation Learning at Scale", "journal": "", "year": "2019", "authors": "Alexis Conneau; Kartikay Khandelwal; Naman Goyal; Vishrav Chaudhary; Guillaume Wenzek; Francisco Guzm\u00e1n; Edouard Grave; Myle Ott; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b23", "title": "The pascal recognising textual entailment challenge", "journal": "Springer", "year": "2005", "authors": "Oren Ido Dagan; Bernardo Glickman;  Magnini"}, {"ref_id": "b24", "title": "The commitmentbank: Investigating projection in naturally occurring discourse", "journal": "", "year": "2019", "authors": "Marie-Catherine De Marneffe; Mandy Simons; Judith Tonhauser"}, {"ref_id": "b25", "title": "", "journal": "Ibrahim Alabdulmohsin", "year": "", "authors": "Mostafa Dehghani; Josip Djolonga; Basil Mustafa; Piotr Padlewski; Jonathan Heek; Justin Gilmer; Andreas Steiner; Mathilde Caron; Robert Geirhos"}, {"ref_id": "b26", "title": "", "journal": "", "year": "2018", "authors": "Mostafa Dehghani; Stephan Gouws; Oriol Vinyals; Jakob Uszkoreit; \u0141ukasz Kaiser"}, {"ref_id": "b27", "title": "Glam: Efficient scaling of language models with mixture-of-experts", "journal": "PMLR", "year": "2022", "authors": "Nan Du; Yanping Huang; M Andrew; Simon Dai; Dmitry Tong; Yuanzhong Lepikhin; Maxim Xu; Yanqi Krikun; Adams Wei Zhou; Orhan Yu;  Firat"}, {"ref_id": "b28", "title": "Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E NLG Challenge", "journal": "Computer Speech & Language", "year": "2020", "authors": "Ond\u0159ej Du\u0161ek; Jekaterina Novikova; Verena Rieser"}, {"ref_id": "b29", "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity", "journal": "J. Mach. Learn. Res", "year": "2021", "authors": "William Fedus; Barret Zoph; Noam Shazeer"}, {"ref_id": "b30", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling", "journal": "", "year": "2020", "authors": "Leo Gao; Stella Biderman; Sid Black; Laurence Golding; Travis Hoppe; Charles Foster; Jason Phang; Horace He; Anish Thite; Noa Nabeshima; Shawn Presser; Connor Leahy"}, {"ref_id": "b31", "title": "", "journal": "", "year": "", "authors": "Leo Gao; Jonathan Tow; Stella Biderman; Sid Black; Anthony Dipofi; Charles Foster; Laurence Golding; Jeffrey Hsu; Kyle Mcdonell"}, {"ref_id": "b32", "title": "Realtoxicityprompts: Evaluating neural toxic degeneration in language models", "journal": "", "year": "2020", "authors": "Suchin Samuel Gehman; Maarten Gururangan; Yejin Sap; Noah A Choi;  Smith"}, {"ref_id": "b33", "title": "The gem benchmark: Natural language generation, its evaluation and metrics", "journal": "", "year": "2021", "authors": "Sebastian Gehrmann; Tosin Adewumi; Karmanya Aggarwal; Pawan Sasanka Ammanamanchi; Aremu Anuoluwapo; Antoine Bosselut; Miruna Khyathi Raghavi Chandu; Dipanjan Clinciu;  Das; D Kaustubh;  Dhole"}, {"ref_id": "b34", "title": "Ciprian Chelba, and Colin Cherry. 2021. Scaling laws for neural machine translation", "journal": "", "year": "", "authors": "Behrooz Ghorbani; Orhan Firat; Markus Freitag; Ankur Bapna; Maxim Krikun; Xavier Garcia"}, {"ref_id": "b35", "title": "Santosh Mashetty, and Chitta Baral. 2023. Instruction Tuned Models are Quick Learners", "journal": "", "year": "", "authors": "Himanshu Gupta;  Saurabh Arjun; Swaroop Sawant; Mutsumi Mishra; Arindam Nakamura;  Mitra"}, {"ref_id": "b36", "title": "KenLM: Faster and Smaller Language Model Queries", "journal": "Association for Computational Linguistics", "year": "2011", "authors": "Kenneth Heafield"}, {"ref_id": "b37", "title": "Pile of law: Learning responsible data filtering from the law and a 256gb open-source legal dataset", "journal": "Advances in Neural Information Processing Systems", "year": "2022", "authors": "Peter Henderson; Mark Krass; Lucia Zheng; Neel Guha; D Christopher; Dan Manning; Daniel Jurafsky;  Ho"}, {"ref_id": "b38", "title": "Scaling laws for autoregressive generative modeling", "journal": "", "year": "2020", "authors": "Tom Henighan; Jared Kaplan; Mor Katz; Mark Chen; Christopher Hesse; Jacob Jackson; Heewoo Jun; B Tom; Prafulla Brown; Scott Dhariwal;  Gray"}, {"ref_id": "b39", "title": "Scaling Laws and Interpretability of Learning from Repeated Data", "journal": "", "year": "", "authors": "Danny Hernandez; Tom Brown; Tom Conerly; Nova Dassarma; Dawn Drain; Sheer El-Showk; Nelson Elhage; Zac Hatfield-Dodds; Tom Henighan; Tristan Hume"}, {"ref_id": "b40", "title": "Scaling laws for transfer", "journal": "", "year": "2021", "authors": "Danny Hernandez; Jared Kaplan; Tom Henighan; Sam Mccandlish"}, {"ref_id": "b41", "title": "Training Compute-Optimal Large Language Models", "journal": "", "year": "2022", "authors": "Jordan Hoffmann; Sebastian Borgeaud; Arthur Mensch; Elena Buchatskaya; Trevor Cai; Eliza Rutherford; Diego De Las; Lisa Anne Casas; Johannes Hendricks; Aidan Welbl;  Clark"}, {"ref_id": "b42", "title": "Music transformer", "journal": "", "year": "2018", "authors": "Cheng-Zhi Anna Huang; Ashish Vaswani; Jakob Uszkoreit; Noam Shazeer; Ian Simon; Curtis Hawthorne; M Andrew;  Dai; D Matthew; Monica Hoffman; Douglas Dinculescu;  Eck"}, {"ref_id": "b43", "title": "Summarizing source code using a neural attention model", "journal": "Long Papers", "year": "2016", "authors": "Srinivasan Iyer; Ioannis Konstas; Alvin Cheung; Luke Zettlemoyer"}, {"ref_id": "b44", "title": "Deduplicating Training Data Mitigates Privacy Risks in Language Models", "journal": "", "year": "2022", "authors": "Nikhil Kandpal; Eric Wallace; Colin Raffel"}, {"ref_id": "b45", "title": "Scaling laws for neural language models", "journal": "", "year": "2020", "authors": "Jared Kaplan; Sam Mccandlish; Tom Henighan; B Tom; Benjamin Brown; Rewon Chess; Scott Child; Alec Gray; Jeffrey Radford; Dario Wu;  Amodei"}, {"ref_id": "b46", "title": "YaLM 100B", "journal": "", "year": "2022", "authors": "Mikhail Khrushchev; Ruslan Vasilev; Alexey Petrov; Nikolay Zinov"}, {"ref_id": "b47", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b48", "title": "The Stack: 3 TB of permissively licensed source code", "journal": "", "year": "", "authors": "Denis Kocetkov; Raymond Li; Loubna Ben Allal; Jia Li; Chenghao Mou; Carlos Mu\u00f1oz Ferrandis; Yacine Jernite"}, {"ref_id": "b49", "title": "Large language models are zero-shot reasoners", "journal": "", "year": "2022", "authors": "Takeshi Kojima; Shane Shixiang; Machel Gu; Yutaka Reid; Yusuke Matsuo;  Iwasawa"}, {"ref_id": "b50", "title": "Self-Instruct: Aligning Language Model with Self Generated Instructions", "journal": "", "year": "2022", "authors": "Yizhong Wang; Yeganeh Kordi; Swaroop Mishra; Alisa Liu; A Noah; Daniel Smith; Hannaneh Khashabi;  Hajishirzi"}, {"ref_id": "b51", "title": "Finetuned language models are zero-shot learners", "journal": "", "year": "2021", "authors": "Jason Wei; Maarten Bosma; Y Vincent; Kelvin Zhao; Adams Wei Guu; Brian Yu; Nan Lester;  Du; M Andrew; Quoc V Dai;  Le"}, {"ref_id": "b52", "title": "Chain of thought prompting elicits reasoning in large language models", "journal": "", "year": "2022", "authors": "Jason Wei; Xuezhi Wang; Dale Schuurmans; Maarten Bosma; Ed Chi; Quoc Le; Denny Zhou"}, {"ref_id": "b53", "title": "Atoosa Kasirzadeh, et al. 2021. Ethical and social risks of harm from language models", "journal": "", "year": "", "authors": "Laura Weidinger; John Mellor; Maribeth Rauh; Conor Griffin; Jonathan Uesato; Po-Sen Huang; Myra Cheng; Mia Glaese; Borja Balle"}, {"ref_id": "b54", "title": "Crowdsourcing multiple choice science questions", "journal": "", "year": "2017", "authors": "Johannes Welbl; F Nelson; Matt Liu;  Gardner"}, {"ref_id": "b55", "title": "CCNet: Extracting high quality monolingual datasets from web crawl data", "journal": "", "year": "2019", "authors": "Guillaume Wenzek; Marie-Anne Lachaux; Alexis Conneau; Vishrav Chaudhary; Francisco Guzm\u00e1n; Armand Joulin; Edouard Grave"}, {"ref_id": "b56", "title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "journal": "", "year": "2015", "authors": "Jason Weston; Antoine Bordes; Sumit Chopra; Alexander M Rush; Bart Van Merri\u00ebnboer; Armand Joulin; Tomas Mikolov"}, {"ref_id": "b57", "title": "Training Trajectories of Language Models Across Scales", "journal": "", "year": "2022", "authors": "Mengzhou Xia; Mikel Artetxe; Chunting Zhou; Xi Victoria Lin; Ramakanth Pasunuru; Danqi Chen; Luke Zettlemoyer; Ves Stoyanov"}, {"ref_id": "b58", "title": "MetaXL: Meta representation transformation for lowresource cross-lingual learning", "journal": "", "year": "2021", "authors": "Mengzhou Xia; Guoqing Zheng; Subhabrata Mukherjee; Milad Shokouhi; Graham Neubig; Ahmed Hassan Awadallah"}, {"ref_id": "b59", "title": "Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions", "journal": "", "year": "", "authors": "Can Xu; Qingfeng Sun; Kai Zheng; Xiubo Geng; Pu Zhao; Jiazhan Feng"}, {"ref_id": "b60", "title": "Aditya Barua, and Colin Raffel. 2020. mT5: A massively multilingual pre-trained text-to-text transformer", "journal": "", "year": "", "authors": "Linting Xue; Noah Constant; Adam Roberts; Mihir Kale; Rami Al-Rfou; Aditya Siddhant"}, {"ref_id": "b61", "title": "Tuning large neural networks via zero-shot hyperparameter transfer", "journal": "Advances in Neural Information Processing Systems", "year": "2021", "authors": "Ge Yang; Edward Hu; Igor Babuschkin; Szymon Sidor; Xiaodong Liu; David Farhi; Nick Ryder; Jakub Pachocki; Weizhu Chen; Jianfeng Gao"}, {"ref_id": "b62", "title": "BLOOM+ 1: Adding Language Support to BLOOM for Zero-Shot Prompting", "journal": "", "year": "2022", "authors": "Zheng-Xin Yong; Hailey Schoelkopf; Niklas Muennighoff; Alham Fikri Aji; David Ifeoluwa Adelani; Khalid Almubarak; Lintang Bari; Jungo Sutawika; Ahmed Kasai;  Baruwa"}, {"ref_id": "b63", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?", "journal": "", "year": "2019", "authors": "Rowan Zellers; Ari Holtzman; Yonatan Bisk; Ali Farhadi; Yejin Choi"}, {"ref_id": "b64", "title": "GLM-130B: An Open Bilingual Pre-trained Model", "journal": "", "year": "2022", "authors": "Aohan Zeng; Xiao Liu; Zhengxiao Du; Zihan Wang; Hanyu Lai; Ming Ding; Zhuoyi Yang; Yifan Xu; Wendi Zheng; Xiao Xia"}, {"ref_id": "b65", "title": "PanGu-alpha: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation", "journal": "", "year": "2021", "authors": "Wei Zeng; Xiaozhe Ren; Teng Su; Hui Wang; Yi Liao; Zhiwei Wang; Xin Jiang; Zhenzhang Yang; Kaisheng Wang; Xiaoda Zhang"}, {"ref_id": "b66", "title": "", "journal": "", "year": "", "authors": "Susan Zhang; Stephen Roller; Naman Goyal; Mikel Artetxe; Moya Chen; Shuohui Chen; Christopher Dewan"}, {"ref_id": "b67", "title": "", "journal": "", "year": "", "authors": "Kun Wayne Xin Zhao; Junyi Zhou; Tianyi Li; Xiaolei Tang; Yupeng Wang; Yingqian Hou; Beichen Min; Junjie Zhang; Zican Zhang;  Dong"}, {"ref_id": "b68", "title": "Lima: Less is more for alignment", "journal": "", "year": "2023", "authors": "Chunting Zhou; Pengfei Liu; Puxin Xu; Srini Iyer; Jiao Sun; Yuning Mao; Xuezhe Ma; Avia Efrat; Ping Yu; Lili Yu"}, {"ref_id": "b69", "title": "experimented with the UL2 objective [106, 107] for a causal model but did not find it to outperform regular causal language modeling on our evaluation tasks", "journal": "", "year": "2022", "authors": "Barret Zoph; Irwan Bello; Sameer Kumar; Nan Du; Yanping Huang; Jeff Dean; Noam Shazeer; William Fedus"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Return and Allocation when repeating data. (Left): Loss of LLMs (4.2B parameters) scaled on repeated data decays predictably ( \u00a76). (Right): To maximize performance when repeating, our data-constrained scaling laws and empirical data suggest training smaller models for more epochs in contrast to what assuming Chinchilla scaling laws [42] hold for repeated data would predict ( \u00a75).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": ", D) s.t. FLOPs(N, D) = C (1)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "C these terms are simply computed as U D = min{D C , D} and R D = (D/U D ) \u2212 1. When training for a single epoch like done in prior scaling studies, R D = 0. We are thus interested in minimizing Equation 1 with the additional constraint of a data budget D C : argmin N,D L(N, D) s.t. FLOPs(N, D) = C, U D \u2264 D C (4)", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: IsoLoss contours for 100 million unique tokens. (Left): 93 models trained with varying parameters and epochs on a fixed dataset. Contours show an interpolation of results with the same final test loss. (Right): Comparison with the loss predictions from our proposed scaling laws for the same budget of 100 million unique tokens and the predicted efficient frontier. The diminishing returns from training on repeated data can be seen in the increase in distance of the contour curves.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure 5: Empirical and Extrapolated loss with constrained data. (Left): Loss as a function of repeated tokens for three different training budgets each with fixed number of parameters. Loss curves predicted by our data-constrained scaling laws are shifted to exactly match the loss at 100% unique data. Return on FLOPs decays with repeated data in a regular pattern. (Right): Extrapolating from the proposed data-constrained scaling law shows that at small numbers epochs are benign, but at large number of epochs loss stops improving.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 :6Figure 6: Strategies for data-constrained settings and their downstream performance. (Left): Schematic showing alternative data use strategies of code filling and filtering. (Right): N = 4.2 billion parameter models trained for a total of D = 84 billion tokens with varying budgets D C . For repeating and filling with code, five models with different seeds are trained for each dot and the standard deviation is visualized as the shaded area.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "7Results: Complementary Strategies for Obtaining Additional Data", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Write U D = cU N (for Chinchilla c \u2248 20). When R D \u226a R * D and R N \u226a R * N , our scaling agrees with Chinchilla, and so the point (U N , U D ), corresponding to R D = R N = 0 is on the optimal compute curve. Increasing R D by \u03f5 corresponds to increasing the number of tokens by \u03f5U D = \u03f5cU N ,", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 7 :7Figure7: A cartoon of how the compute-optimal tradeoff deviates from Chinchilla as we increase the number of epochs. Initially the model size and tokens processed grow proportionally (R N = R D ) but since R * N < R * D , at some point adding parameters offers worse returns compared to increasing the number of tokens processed, and hence we deviate from the Chinchilla curve.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "5, . . . , 2.}, \u03b2 \u2208 {0., 0.5, . . . , 2.}, e \u2208 {\u22121., \u2212.5, . . . , 1.}, a \u2208 {0, 5, . . . , 25}, and b \u2208 {0, 5, . . . , 25}. Our fit results in a = 6.255414, b = 7.3049974, e = 0.6254804, \u03b1 = \u03b2 = 0.3526596. Exponentiating a, b and e to get A, B and E and inserting all learned coefficients into Equation 2 then allows us to compute loss (L) as a function of parameters and data:", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 88Figure8contains additional empirical isoLoss contours for 400 million and 1.5 billion unique tokens. Results show that like in Figure3significantly lower loss can be achieved by increasing parameters and epochs beyond what is compute-optimal at a single epoch. The lowest loss is also achieved by allocating more extra compute to repeating data rather than to adding parameters.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "model for given unique tokens and one epoch Lowest loss for given unique tokens Chinchilla scaling laws efficient frontier Models trained", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Figure 8 :8Figure 8: Empirical isoLoss curves for 400 million and 1.5 billion unique tokens. 34 models trained on 400 million unique tokens and 37 models trained on 1.5 billion unique tokens with varying parameters and epochs.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Figure 9 :9Figure 9: Double descent. Each dot is a model trained on 100 million unique tokens. Loss initially increases at 200 epochs and then decreases again; this is known as epoch-wise double descent [75].", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_17", "figure_caption": "Figure 11 :11Figure11: Empirical and predicted losses of LLMs trained on 100 million tokens for a single epoch. Excess parameters empirically hurt performance, but this may be due to a lack of regularization. Thus, our scaling formula predicts loss to plateau, while Chinchilla predicts loss to improve. By decaying the exponent \u03b1 (and \u03b2) instead, one can allow excess parameters to hurt.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_18", "figure_caption": "model for 100M tokens and one epoch Lowest loss for 100M tokens Chinchilla scaling laws efficient frontier Alpha-beta decay efficient frontier Models trained", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_19", "figure_caption": "Figure 12 :12Figure12: IsoLoss contours for 100 million unique tokens with contours predicted by parametric decay of alpha and beta. The same models from Figure3with the contour predictions being done by the alpha-beta decay formulation introduced in Appendix F.", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_20", "figure_caption": "Figure 13 :13Figure13: Optimal compute allocation for Galactica. Efficient frontier assuming repeated data is worth the same as new data (Chinchilla scaling laws) and data-constrained efficient frontier assuming a unique token budget of 106 billion tokens like for the Galactica models [108]. For optimal compute allocation according to our proposed data-constrained scaling laws, the 120 billion Galactica model should have been significantly smaller and trained for more epochs.", "figure_data": ""}, {"figure_label": "14", "figure_type": "figure", "figure_id": "fig_21", "figure_caption": "Figure 14 :14Figure 14: Training loss smoothed with exponential moving average smoothing and a weight of 0.999. Models trained on fewer unique tokens (more epochs) have better training loss as they overfit.", "figure_data": ""}, {"figure_label": "1617", "figure_type": "figure", "figure_id": "fig_22", "figure_caption": "Figure 16 :Figure 17 :1617Figure 16: Training loss for models trained on OSCAR smoothed with exponential moving average smoothing and a weight of 0.999. Models trained on fewer unique tokens (more epochs) have better training loss as they overfit.", "figure_data": ""}, {"figure_label": "20", "figure_type": "figure", "figure_id": "fig_24", "figure_caption": "Figure 20 :20Figure 20: Validation and training loss of models trained with different data strategies. Training loss is smoothed with exponential moving average smoothing and a weight of 0.999. Downstream performance of the models is in Figure 6.", "figure_data": ""}, {"figure_label": "29", "figure_type": "figure", "figure_id": "fig_25", "figure_caption": "Figure 29 :29Figure 29: Formatted dataset example from HellaSwag evaluated using accuracy as described in Appendix K.", "figure_data": ""}, {"figure_label": "30", "figure_type": "figure", "figure_id": "fig_26", "figure_caption": "Figure 30 :30Figure 30: Formatted dataset example from PiQA evaluated using accuracy as described in Appendix K.", "figure_data": ""}, {"figure_label": "31", "figure_type": "figure", "figure_id": "fig_27", "figure_caption": "Figure 31 :31Figure31: Formatted dataset example from RTE evaluated using accuracy as described in Appendix K.", "figure_data": ""}, {"figure_label": "38", "figure_type": "figure", "figure_id": "fig_28", "figure_caption": "Figure 38 :38Figure 38: Formatted dataset example from WikiLingua evaluated using ROUGE as described in Appendix K.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": "Final test loss2.2 2.3 2.4 2.5 2.6 2.7 2.88.7B Empirical Loss (Fixed training length)8.7B parameters 4.2B parameters 2.8B parameters 2 Ep. 4 Ep. 8 Ep. 16 Ep. 32 Ep. 64 Ep. 4 Ep. 8 Ep. 16 Ep. 32 Ep. 64 Ep. 4 Ep. 8 Ep. 16 Ep. 32 Ep. 64 Ep. 1 Ep. 2 Ep. 2 Ep. 1 Ep. 1 Epoch Repeating for 4 epochs is almost as good as new data Predicted Loss (Variable training length)100%50% Fraction of training tokens that are unique 25% 14% 10%10B100B1T Total training tokens10T100TLoss of models trained Loss assuming training is stopped when exhausting all unique dataLoss assuming repeated data is worth the same as new data Loss predicted by our data-constrained scaling laws"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Leandro von Werra, and Harm de Vries. 2023. StarCoder: may the source be with you! arXiv preprint arXiv:2305.06161. Ssu Huang, and Richard Socher. 2020. Progen: Language modeling for protein generation. arXiv preprint arXiv:2004.03497. [67] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2018. The Natural Language Decathlon: Multitask Learning as Question Answering. CoRR, abs/1806.08730. [68] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2021. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943. 71] Niklas Muennighoff. 2022. SGPT: GPT Sentence Embeddings for Semantic Search. arXiv preprint arXiv:2202.08904. [72] Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. 2023. OctoPack: Instruction Tuning Code Large Language Models. arXiv preprint arXiv:2308.07124. [73] Niklas Muennighoff, Nouamane Tazi, Lo\u00efc Magne, and Nils Reimers. 2022. MTEB: Massive Text Embedding Benchmark. arXiv preprint arXiv:2210.07316. [74] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. 2022. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786. [75] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. 2021. Deep double descent: Where bigger models and more data hurt. Journal of Statistical Mechanics: Theory and Experiment, 2021(12):124003. Jason Weston, and Douwe Kiela. 2020. Adversarial NLI: A New Benchmark for Natural Language Understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics. Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474. [81] nostalgebraist. 2022. chinchilla's wild implications. lesswrong. [82] Gabriel Orlanski, Kefan Xiao, Xavier Garcia, Jeffrey Hui, Joshua Howland, Jonathan Malmaud, Jacob Austin, Rishah Singh, and Michele Catasta. 2023. Measuring The Impact Of Programming Language Distribution. arXiv preprint arXiv:2302.01973. [83] Pedro Javier Ortiz Su'arez, Laurent Romary, and Benoit Sagot. 2020. A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1703-1714, Online. Association for Computational Linguistics. [84] Pedro Javier Ortiz Su'arez, Benoit Sagot, and Laurent Romary. 2019. Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures. Proceedings of the 86] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only. arXiv preprint arXiv:2306.01116. [87] Shrimai Prabhumoye, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. 2023. Adding Instructions during Pretraining: Effective Way of Controlling Toxicity in Language Models. arXiv preprint arXiv:2302.07388. [88] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. [89] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446. [90] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67. Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S Morcos. 2022. Beyond neural scaling laws: beating power law scaling via data pruning. arXiv preprint arXiv:2206.14486. [102] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615. Analytical properties of compute-optimal point . . . . . . . . . . . . . . . . . .", "figure_data": "AppendixContentsA Derivation of Data-Constrained Scaling LawsA.1 B C4 Scaling CoefficientsC Additional Contour PlotsD Double DescentE Repeating on Heavily Deduplicated DataF Do Excess Parameters Hurt, Plateau or Help?G Case Study: GalacticaH Training LossI Scaling Curves on the OSCAR CorpusJ Validation Loss by EpochK Evaluation DetailsL Downstream Repetition ResultsM Detailed Code Augmentation ResultsN Filtering ProcedureO Detailed Filtering ResultsP Loss Curves for Complementary StrategiesQ Limitations and Future WorkR ContributionsS Hyperparameters and SetupT Prompts and SamplesU Other ExperimentsV Release of ArtifactsW Version ControlX Broader Impacts"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "", "figure_data": "N ."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "on the OSCAR corpus[83]. OSCAR is considered noisier than C4 [90] due to its less stringent duplication. Figures 15,16 depict the validation and training loss of these models. We find the trend to be the same as for models trained on C4: While models with fewer repeats have better loss, differences for a few repeats are insignificant.", "figure_data": "2.8B parameters trained for 55B tokens4.2B parameters trained for 84B tokens8.7B parameters trained for 178B tokens3.53.0Training loss2.0 2.51.55B 15B 25B 35B 45B 55B Training tokens5B25B Training tokens 45B 65B85B5B 40B100B 140B 180B Training tokens123Epochs 45714442.8B parameters trained for 55B tokens8.7B parameters trained for 178B tokens4.24.03.8Validation loss3.4 3.63.23.02.85B 15B 25B 35B 45B 55B Training tokens5B25B Training tokens 45B 65B85B5B 40B100B 140B 180B Training tokens123Epochs 4571444"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Setup for computing validation loss during training. At every Evaluation Interval, loss is computed on Evaluation Tokens many tokens from the validation set. The evaluation tokens vary with the interval, i.e. the evaluation tokens at 100 steps are not the same as at 200 steps. However, the tokens do not vary across data budgets for the same FLOP budget (Figure4). For example, N = 2.8 billion parameter models with D C = 55 billion tokens are evaluated on the same data as models with D C = 28 billion tokens at each evaluation interval.", "figure_data": "FLOP budget Parameters Evaluation Interval Evaluation Tokens9.3 \u00d7 10 202.8B100105 million2.1 \u00d7 10 214.2B1000105 million9.3 \u00d7 10 218.7B10002.1 millionLoss evaluation For all models trained on C4, the final test loss is computed on the same 210million tokens from the C4 validation set after training. For held-out evaluation during training, suchas in Figure"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Downstream evaluation datasets. We evaluate on 19 datasets: The first 14 are evaluated using accuracy (ANLI counted as three), the next 4 using ROUGE-2 f-measure [59] and bAbI using exact match.", "figure_data": "DatasetSplit(s)Samples Baseline URLANLI [79]dev_r1,2,3300033.3hf.co/datasets/anliARC-Easy [22]test117225.0hf.co/datasets/ai2_arcARC-Challenge [22]test237625.0hf.co/datasets/ai2_arcBoolQ [21]validation327050.0hf.co/datasets/boolqCB [25]validation5633.3hf.co/datasets/super_glueCopa [92]validation10050.0hf.co/datasets/super_glueHellaSwag [129]test1000325.0hf.co/datasets/hellaswagPiQA [12]validation183850.0hf.co/datasets/piqaRTE [24, 114]validation27750.0hf.co/datasets/super_glueSciQ [120]test100025.0hf.co/datasets/sciqStoryCloze 2016 [69]test187125.0hf.co/datasets/story_clozeWinoGrande XL [93]test126750.0hf.co/datasets/winograndeE2E NLG [29]test46930.0hf.co/datasets/e2e_nlg_cleanedXSUM [77, 34]test113340.0hf.co/datasets/GEM/xsumWebNLG EN [17, 34]test51500.0hf.co/datasets/GEM/web_nlgWikiLingua EN [53, 34] sampled_test 30000.0hf.co/datasets/GEM/wiki_linguabAbI [122]test190000.0hf.co/datasets/Muennighoff/babi"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Results for 2.8B parameter models trained on repeated data on C4 for 55B total tokens. Scores are normalized averages of 0-5 few-shots and reported as percentages. We report mean/std. err. across five different models, each trained with a different random seed.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "Results for 2.8B parameter models trained on repeated data on OSCAR for 55B total tokens. Scores are normalized averages of 0-5 few-shots and reported as percentages. We report mean/std. err. across five different models, each trained with a different random seed.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "Results for 4.2B parameter models trained on repeated data on OSCAR for 84B total tokens. Scores are normalized averages of 0-5 few-shots and reported as percentages. We report mean/std. err. across five different models, each trained with a different random seed.", "figure_data": "Unique Tokens84B42B28B21B17B12B6B1.9B"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "Results for 8.7B parameter models trained on repeated data on C4 for 178B total tokens and a data-constrained compute-optimal 6.3B model. Scores are normalized averages of 0-5 few-shots and reported as percentages. The two models with 25 billion unique tokens are the ones depicted in Figure1(right). The data-constrained compute-optimal variant (6.3 billion parameters) performs better by using fewer parameters and repeating more data.", "figure_data": "Parameters8.7B6.3BUnique Tokens178B 88B 58B44B 35B 25B13B4B25BEpochs12345714449.7ANLI R1-0.9-1.2 -4.20.7-1.3 0.11.22.1-0.9ANLI R2-0.4-1.2 -0.20.2-0.4 -0.10.42.21.0ANLI R30.70.50.71.80.41.62.04.02.6ARC-Challenge12.211.9 10.512.2 10.6 11.88.32.212.7ARC-Easy58.558.0 56.957.4 56.7 58.552.937.4 57.2BoolQ26.131.8 31.330.3 28.8 28.527.94.130.6CB7.612.9 -15.2 17.9 14.3 -22.8 -12.1 17.4 6.2COPA68.064.7 62.366.3 63.3 70.057.045.0 66.0HellaSwag37.837.8 37.337.4 37.1 37.536.127.5 38.1PiQA55.955.6 54.756.5 55.8 53.952.445.7 54.3RTE14.111.4 11.08.715.9 -2.6-1.8-3.2 7.7SciQ90.491.1 90.790.0 89.8 89.887.972.9 90.3StoryCloze 2016 68.367.3 67.267.6 67.8 66.866.258.9 68.4WinoGrande XL 26.327.7 26.529.0 26.1 23.518.110.0 27.0E2E NLG20.517.9 18.720.0 17.2 17.717.411.2 16.9XSUM3.63.33.83.83.53.03.32.03.8WebNLG EN5.35.85.95.65.85.25.74.95.3WikiLingua EN4.14.24.24.14.24.03.52."}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "Results for 8.7B parameter models trained on repeated data on OSCAR for 178B total tokens. Scores are normalized averages of 0-5 few-shots and reported as percentages. Discarding documents with too few grammatical function words (e.g. \"of\", \"and\")\u2022 Discarding documents with too many flagged words \u2022 Discarding documents with a low fasttext language identification score", "figure_data": "Unique Tokens178B 88B 58B 44B 35B 25B 13B 4BEpochs1234571444ANLI R1-1.3-2.3 -0.5 -1.8 0.1-0.3 2.6-0.4ANLI R20.83.2-0.2 -1.3 1.00.21.50.5ANLI R31.11.21.30.92.8-0.4 1.1-0.1ARC-Challenge6.96.76.93.86.64.84.0-0.9ARC-Easy50.251.6 51.2 51.0 51.9 50.8 47.0 33.0BoolQ18.411.7 19.4 22.4 17.5 20.8 7.64.1CB11.213.4 16.1 19.6 21.4 25.0 9.820.1COPA46.753.0 52.0 53.7 51.0 53.3 48.7 41.7HellaSwag27.427.2 26.8 26.8 27.3 26.7 25.5 19.6PiQA49.249.3 50.1 48.7 48.1 47.2 45.6 37.0RTE-0.51.10.21.210.2 3.2-3.0 -7.8SciQ88.188.0 88.4 87.9 87.9 87.4 86.3 64.6StoryCloze 2016 61.661.1 60.2 60.6 61.3 59.0 58.8 52.7WinoGrande XL 17.6"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "Sizes of filtered datasets.In Table13, we report detailed perplexity filtering results on C4 and OSCAR. For C4, perplexity filtering is only effective at 4.2B parameters. Meanwhile, for OSCAR, which is noisier than C4, perplexity filtering seems effective both for 2.8B and 4.2B parameters. Table14contains deduplication results and results for the ROOTS filter. Deduplication does not improve downstream performance for C4 while being effective for OSCAR which has significantly more noise. Applying the ROOTS filter on OSCAR is not better than the unfiltered OSCAR on our benchmark, but might have other beneficial effects, such as reducing obscenity, templated messages, or repetition, depending on the final use case. Validation loss of models trained on a mix of natural language (C4) and Python data.", "figure_data": "Base DatasetFilter Tokens after filteringC4Deduplication21 billionC4Perplexity Top 25%44 billionC4Perplexity Top 50%89 billionC4Perplexity 25-75%89 billionOSCARDeduplication9 billionOSCARDeduplication-expanded94 billionOSCARPerplexity Top 25%80 billionOSCARROOTS99 billionO Detailed Filtering Results"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_21", "figure_caption": "Its training data has been filtered to the top 25% of examples with the lowest perplexity (Appendix N) thus high loss examples have been explicitly filtered out from the training data resulting in low training loss. The model trained on deduplicated data has the highest validation and training loss. This is because commonly repeated sequences have been filtered out from its training data. Thus, when encountering these common sequences in the unfiltered validation set, its loss is comparatively high as other models have likely simply memorized them. Similarly, fewer repeated sequences during training results in higher training loss as unseen sequences are harder to predict.Q Limitations and Future WorkRepeating fractions of the data In this work we focus on repeating the entire unique dataset for several epochs. Alternatively, one can repeat only a fraction of the dataset. For example, repeating 10% of the dataset for 10 epochs while repeating the rest only for a single epoch as done by Hernandez et al.[40]. To predict loss in that scenario, one may need to adapt our scaling laws with an additional parameter to account for the fraction that is repeated and possibly a parameter that captures at what point in training the data is repeated. Repeating earlier in training when most model weights are still randomly initialized is likely to cause less damage than later in training. Adapting our parametric fit to make concrete scaling predictions for such scenarios is an exciting future research direction.Sensitivity to hyperparametersThe returns from additional epochs may heavily depend on hyperparameters such as learning rate, dropout, or the optimizer choice. It is likely that increasing the learning rate, for example, would lead to diminishing returns from additional epochs kicking in earlier. In this work, we have fixed most hyperparameters to commonly used values for the training of LLMs and leave such explorations to future work.Other datasetsThe optimal data strategy is dependent on the dataset at hand and we cannot give universally applicable filtering recommendations. By looking into C4 and OSCAR, we have covered two of the most commonly used English text datasets. Our findings on both datasets were overall in agreement with each other. We have highlighted some of the differences, such as deduplication being more effective on OSCAR due to it being more noisy than C4. Further, we have focused on large-scale pre-training datasets. There is a lot of research on the optimal fine-tuning dataset and methodology for LLMs[94, 62,128, 85,117, 68,116,134, 115,36,125, 72, 63]. More investigations of resolving data-constraints when fine-tuning LLMs may be of interest for future work.Other modalities or architectures Our work focuses on text datasets and uses the GPT transformer architecture[88]. Prior work has experimented with many variations to the GPT or transformer architecture[27, 104, 96], as well as scaling laws for non-text datasets[1]. Overall, variations of the GPT or transformer architecture have proven very robust and generalizable to other domains[43,18,  70, 66, 71, 104,26]. Nonetheless, it may be of interest for future work to test the applicability of our findings in this work to different data modalities or model architectures.Other strategies There are numerous strategies to solve data constraints not covered in this work that are worth exploring. Like we have shown for Python, future research may consider to what extent augmenting with a natural language (e.g. Chinese) improves performance in another language (e.g. English) and what is the best language to choose[61,124]. Similarly, while we have looked at deduplication and perplexity filtering, other filtering strategies, such as popularity-based filters[3,133] and toxicity filters[33,38, 64, 87, 86] are worth exploring.", "figure_data": ""}, {"figure_label": "15", "figure_type": "table", "figure_id": "tab_23", "figure_caption": "Model architectures. We list the architectures of all models trained as part of this work. Many shown models have been trained multiple times on different amounts of unique data and for varying epochs. \u2192 Canoeing: Two women in a child are shown in a canoe while a man pulls the canoe while standing in the water, with other individuals visible in the background. The child and a different man Correct Answer \u2192 sit in a canoe while the man paddles. Incorrect Answer \u2192 are then shown paddling down a river in a boat while a woman talks. Incorrect Answer \u2192 are driving the canoe, they go down the river flowing side to side. Incorrect Answer \u2192 walking go down the rapids, while the man in his helicopter almost falls and goes out of canoehood.", "figure_data": "Context"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_24", "figure_caption": "table, or chart, requires several different tags. Play with these tags, or learn about HTML tables in more detail. Start with table tags around the entire table:<table></table> Row tags around the contents of each row: <tr> Column headers in the first row: <th> Cells in subsequent rows: <td> Here's an example of how it all fits together:<table><tr><th>Column 1: Month</th><th>Column 2: Money Saved</th></tr><tr><td>January</td><td>$100</td></tr></table> You've already learned the <head> tag, which shows up at the start of each document. Besides the <title> tag, it can include the following types of tags: Meta tags, which are used to provide metadata about a web page. This data can be used by search engines when the robot scours the internet to locate and list websites. To make your website more visible on search engines, use one or more <meta> start tags (no end tags necessary), each with exactly one name attribute and one content attribute, for example: <meta name=descriptioncontent=\u1e85rite a description here>; or <meta name=keywordscontent=\u1e85rite a list of keywords, each separated by a comma> <link> tags are used to associate other files with the page. This is mainly used to link to CSS stylesheets, which are made using a different type of coding to alter your HTML page by adding color, aligning your text, and many other things. <script> tags are used to link the page to JavaScript files, which can cause the page to change as... Learn about attributes. Experiment with HTML tables. Learn the miscellaneous head tags. Play around with HTML found on websites. Learn more advanced web design from comprehensive guides.", "figure_data": "TL;DR in English:Target \u2192"}], "formulas": [{"formula_id": "formula_0", "formula_text": "L(N, D) = A N \u03b1 + B D \u03b2 + E (2)", "formula_coordinates": [3.0, 248.48, 296.68, 256.18, 22.31]}, {"formula_id": "formula_1", "formula_text": "N opt (C) = G(C/6) a D opt (C) = G \u22121 (C/6) b", "formula_coordinates": [3.0, 229.26, 374.52, 192.26, 12.62]}, {"formula_id": "formula_2", "formula_text": "1 \u03b1+\u03b2 a = \u03b2 \u03b1 + \u03b2 b = \u03b1 \u03b1 + \u03b2 (3)", "formula_coordinates": [3.0, 292.81, 391.38, 211.86, 26.81]}, {"formula_id": "formula_3", "formula_text": "R N = (N/U N ) \u2212 1.", "formula_coordinates": [4.0, 108.0, 118.79, 83.4, 9.65]}, {"formula_id": "formula_4", "formula_text": "= A N \u2032\u03b1 + B D \u2032\u03b2 + E", "formula_coordinates": [4.0, 284.98, 361.3, 80.26, 22.31]}, {"formula_id": "formula_5", "formula_text": "D \u2032 = U D + U D R * D (1 \u2212 e \u2212R D R * D ) .(5)", "formula_coordinates": [4.0, 238.84, 456.66, 265.83, 17.68]}, {"formula_id": "formula_6", "formula_text": "D \u2032 = U D = D. For R D \u226a R * D , e \u2212R D /R * D \u2248 1 \u2212 R D R * D", "formula_coordinates": [4.0, 260.45, 481.22, 214.1, 16.78]}, {"formula_id": "formula_7", "formula_text": "D \u2032 \u2248 U D + U D R * D (1 \u2212 1 + R D /R * D ) = U D (1 + R D ) = D", "formula_coordinates": [4.0, 183.46, 505.42, 244.8, 12.69]}, {"formula_id": "formula_8", "formula_text": "N \u2032 = U N + U N R * N (1 \u2212 e \u2212R N R * N ) .(6)", "formula_coordinates": [4.0, 237.96, 634.68, 266.71, 17.68]}, {"formula_id": "formula_9", "formula_text": "D \u2032 is U D + U D R D (1 \u2212 e \u22121", "formula_coordinates": [4.0, 388.14, 673.41, 112.65, 11.23]}, {"formula_id": "formula_10", "formula_text": "L(N, U ) = A N \u03b1 + B U \u03b2 + E (7", "formula_coordinates": [21.0, 251.65, 146.68, 249.15, 13.6]}, {"formula_id": "formula_11", "formula_text": ")", "formula_coordinates": [21.0, 500.8, 148.88, 3.87, 8.64]}, {"formula_id": "formula_12", "formula_text": "D \u2032 = U + (1 \u2212 \u03b4)U + (1 \u2212 \u03b4) 2 U + \u2022 \u2022 \u2022 + (1 \u2212 \u03b4) R D U(8)", "formula_coordinates": [21.0, 194.61, 321.26, 310.06, 11.03]}, {"formula_id": "formula_13", "formula_text": "S = a(1 \u2212 r n ) 1 \u2212 r (9", "formula_coordinates": [21.0, 274.64, 368.61, 226.16, 23.89]}, {"formula_id": "formula_14", "formula_text": ")", "formula_coordinates": [21.0, 500.8, 377.25, 3.87, 8.64]}, {"formula_id": "formula_15", "formula_text": "D \u2032 = U + U R D k=1 (1 \u2212 \u03b4) k = U + (1 \u2212 \u03b4)U (1\u2212(1\u2212\u03b4) R D ) \u03b4 (10", "formula_coordinates": [21.0, 193.39, 429.33, 307.12, 30.67]}, {"formula_id": "formula_16", "formula_text": ")", "formula_coordinates": [21.0, 500.52, 440.18, 4.15, 8.64]}, {"formula_id": "formula_17", "formula_text": "(1\u2212\u03b4)U \u03b4 , as lim R D \u2192\u221e (1 \u2212 (1 \u2212 \u03b4) R D ) = 1. Let R * D = 1\u2212\u03b4 \u03b4 , hence D \u2032 \"plateaus\" at U + R * D U", "formula_coordinates": [21.0, 108.0, 503.11, 397.25, 25.61]}, {"formula_id": "formula_18", "formula_text": "e x = 1 + x + x 2 2! + x 3 3! + \u2022 \u2022 \u2022 \u2248 1 + x (11", "formula_coordinates": [21.0, 228.41, 578.03, 272.11, 23.89]}, {"formula_id": "formula_19", "formula_text": ")", "formula_coordinates": [21.0, 500.52, 586.67, 4.15, 8.64]}, {"formula_id": "formula_20", "formula_text": "(1 + x) = (1 \u2212 \u03b4) \u2248 e \u2212\u03b4 \u2248 e \u22121/R * D (12", "formula_coordinates": [21.0, 232.49, 646.67, 268.03, 12.67]}, {"formula_id": "formula_21", "formula_text": ")", "formula_coordinates": [21.0, 500.52, 650.71, 4.15, 8.64]}, {"formula_id": "formula_22", "formula_text": "D \u2032 = U + U \u2022 R * D \u2022 (1 \u2212 e \u2212R D /R * D )(13)", "formula_coordinates": [21.0, 232.75, 709.48, 271.91, 14.34]}, {"formula_id": "formula_23", "formula_text": "yields D \u2032 = U + (1 \u2212 \u03b4)U (1\u2212(1\u2212\u03b4) R D ) \u03b4 = 1 + (0.75) * 4 *", "formula_coordinates": [22.0, 272.8, 166.96, 231.2, 16.03]}, {"formula_id": "formula_24", "formula_text": "D = (1 \u2212 \u03b4)/\u03b4, the corresponding R * D value is 3. Setting R * D = 3 in Equation 13 yields D \u2032 = U +U \u2022R * D \u2022(1\u2212e \u2212R D /R * D ) = 1+3 * (1\u2212e \u22124/", "formula_coordinates": [22.0, 108.0, 193.27, 397.74, 25.16]}, {"formula_id": "formula_25", "formula_text": "L(U N , U D , R N , R D ) = A (U N + U N R * N (1 \u2212 e \u2212R N R * N )) \u03b1 + B (U D + U D R * D (1 \u2212 e \u2212R D R * D )) \u03b2 + E(14)", "formula_coordinates": [22.0, 119.72, 405.76, 384.95, 39.15]}, {"formula_id": "formula_26", "formula_text": "N + U N R * N (1 \u2212 e \u2212R N R * N )", "formula_coordinates": [22.0, 264.8, 484.7, 100.73, 17.96]}, {"formula_id": "formula_27", "formula_text": "U N = min{((U D \u2022 G) \u03b2/\u03b1 ) \u2022 G, N } where G = \u03b1A \u03b2B 1 \u03b1+\u03b2 (15)", "formula_coordinates": [22.0, 175.83, 537.6, 328.84, 26.81]}, {"formula_id": "formula_28", "formula_text": "min R * N ,R * D Run i Huber \u03b4 LSE a \u2212 \u03b1 log(U i N + U i N R * N (1 \u2212 e \u2212R i N R * N )), b \u2212 \u03b2 log(U i D + U i D R * D (1 \u2212 e \u2212R i D R * D )), e \u2212 log L i (16)", "formula_coordinates": [22.0, 162.42, 668.68, 342.25, 53.64]}, {"formula_id": "formula_29", "formula_text": "L(U D , R N , R D ) = 521 (U N + 5.3 \u2022 U N (1 \u2212 e \u2212R N", "formula_coordinates": [23.0, 108.89, 269.08, 186.48, 26.11]}, {"formula_id": "formula_30", "formula_text": "D \u2032 = R D k=0", "formula_coordinates": [23.0, 215.88, 529.73, 49.61, 14.11]}, {"formula_id": "formula_31", "formula_text": "D \u2032 = U 1\u2212(e \u2212R * D ) R D +1 1\u2212e \u2212R * D (18)", "formula_coordinates": [23.0, 259.68, 555.8, 244.99, 20.26]}, {"formula_id": "formula_32", "formula_text": "min a,b,e,\u03b1,\u03b2 Run i Huber \u03b4 LSE a \u2212 \u03b1 log N i , b \u2212 \u03b2 log D i , e \u2212 log L i (19", "formula_coordinates": [24.0, 170.66, 625.86, 329.86, 20.07]}, {"formula_id": "formula_33", "formula_text": ")", "formula_coordinates": [24.0, 500.52, 626.18, 4.15, 8.64]}, {"formula_id": "formula_34", "formula_text": "N opt (C) = G C 6 a , D opt (C) = G \u22121 C 6 b where G = \u03b1A \u03b2B 1 \u03b1+\u03b2 , a = \u03b2 \u03b1 + \u03b2 , and b = \u03b1 \u03b1 + \u03b2 (21)", "formula_coordinates": [25.0, 194.97, 158.83, 309.69, 56.83]}, {"formula_id": "formula_35", "formula_text": "L(N, D, R N , R D ) = E + A N \u03b1 * max(0,1\u2212(R N /R * N )) + B D \u03b2 * max(0,1\u2212(R D /R * D ))(22)", "formula_coordinates": [27.0, 151.44, 389.76, 353.23, 23.04]}, {"formula_id": "formula_36", "formula_text": "P = 12lh 2 1 + 13 12h + V + s 12lh (23", "formula_coordinates": [42.0, 238.29, 315.44, 262.23, 22.31]}, {"formula_id": "formula_37", "formula_text": ")", "formula_coordinates": [42.0, 500.52, 322.5, 4.15, 8.64]}], "doi": "10.1016/j.csl.2019.06.009"}