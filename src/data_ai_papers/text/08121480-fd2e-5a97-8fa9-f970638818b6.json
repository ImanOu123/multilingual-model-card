{"title": "Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets", "authors": "Patrick Lewis; Pontus Stenetorp; Sebastian Riedel", "pub_date": "", "abstract": "Ideally Open-Domain Question Answering models should exhibit a number of competencies, ranging from simply memorizing questions seen at training time, to answering novel question formulations with answers seen during training, to generalizing to completely novel questions with novel answers. However, single aggregated test set scores do not show the full picture of what capabilities models truly have. In this work, we perform a detailed study of the test sets of three popular open-domain benchmark datasets with respect to these competencies. We find that 30% of test-set questions have a near-duplicate paraphrase in their corresponding train sets. In addition, we find that 60-70% of answers in the test sets are also present in the train sets. Using these findings, we evaluate a variety of popular open-domain models to obtain greater insight into what extent they can generalize, and what drives their overall performance. We find that all models perform substantially worse on questions that cannot be memorized from train sets, with a mean absolute performance difference of 61% between repeated and nonrepeated data. Finally we show that simple nearest-neighbor models outperform a BART closed-book QA model, further highlighting the role that train set memorization plays in these benchmarks.", "sections": [{"heading": "Introduction", "text": "Open-domain Question Answering (ODQA) is a task that examines the ability of models to produce answers to natural language factoid questions drawn from an open set of domains. ODQA has received significant attention for its potential practical applications, and more recently as a popular method to analyse how well NLP systems can capture and recall factual knowledge. This interest in ODQA as a challenging \"knowledge-intensive\" task has led to a flurry of recent works that have driven test-set performance on standard ODQA datasets to new heights Guu et al., 2020;Izacard and Grave, 2020, inter alia). Whilst there have been several works examining other kinds of QA datasets (Manjunatha et al., 2018;Kaushik and Lipton, 2018;Sugawara et al., 2018Sugawara et al., , 2020, however, we know comparatively little about how the questions and answers are distributed in ODQA benchmarks, making it hard to understand and contextualize the results we are observing.\nIn this work, we address these issues via an analysis of the test sets of three popular ODQA datasets, namely WebQuestions (Berant et al., 2013), Triv-iaQA (Joshi et al., 2017) and Open NaturalQuestions (Kwiatkowski et al., 2019;. We identify three types of desired behaviour of a trained ODQA system, in increasing order of difficulty: 1) to recall the answer to a question that the model has seen at training time. 2) to answer novel questions at test time and choose an answer from the set of answers it has seen during training. 3) to answer novel questions which have answers which are not contained in the training data. It is not clear to what extent our current ODQA datasets measure each of these three behaviours.\nTo address this, we stratify the test sets of these datasets. Firstly, we split the test data by whether answers in the test set also appear somewhere in the train set. We find that 58-71% of test answers also occur somewhere in the training data, demonstrating that the majority of the test data does not probe for answer generalization. Secondly, we annotate 1,000 question-answer pairs from each test set for near-duplicate questions in their respective train sets. We find that a surprisingly high 28-34% have paraphrased questions in the training data, the vast majority of which are near-duplicates differing by one or two words. This result implies that 30% of the test set of these datasets only probe for how  well models can memorize question-answer pairs seen at training. Equipped with these observations, we compute the performance of several recently proposed ODQA models on our test subsets. We test both Open-book approaches, which leverage retrieval from a large corpus of documents and Closed-book approaches, which focus on training large parametric seq2seq models with no external knowledge source (Roberts et al., 2020). We find that test data with train-overlapping data contribute the bulk of the overall performance of all the models studied.\nThese issues seem to be more acute for closedbook models. Strikingly, we find that a closedbook BART-based model (Lewis et al., 2019)  In summary, we make the following contributions: 1) We provide insights into how answer entities are distributed between dataset splits for ODQA datasets 2) We provide annotated subsets of ODQA test sets indicating whether test-time questions are duplicates of training time questions. 1 3) We evaluate a variety of models on our dataset splits, and derive insights into what kinds of question answering behaviour different models achieve.", "publication_ref": ["b5", "b14", "b9", "b20", "b21", "b0", "b7", "b10", "b19", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Datasets", "text": "We analyse three widely used ODQA datasets, We-bQuestions (Berant et al., 2013), TriviaQA (Joshi et al., 2017), and Open-NaturalQuestions . All three datasets consist of factual natural language questions and short multi-token answers, differing slightly in the style of questions and format of answers.\nWebQuestions consists of 3,778 train and 2,032 test instances. Questions were obtained by mining a search engine, and answers are Freebase entities (Bollacker et al., 2008) annotated by crowdworkers. The ODQA task consists of predicting the name of the Freebase entity. We use the standard train/test splits from Berant et al. (2013) and the development split from , which was randomly split from the train set.\nTriviaQA consists of 78,785 train, 8,837 development and 11,313 test instances obtained by scraping trivia websites. Answers are Wikipedia entities, and any alias for the answer entity is considered a correct answer. We use the ODQA splits, which correspond to the unfiltered-train and unfiltereddev reading comprehension splits Min et al., 2019Min et al., , 2020b.\nOpen-NaturalQuestions consists of search engine questions with crowdsourced answer spans in Wikipedia articles. The ODQA version consists of question-answer pairs from NaturalQuestions which have short answer spans less than 6 tokens in length. We use the standard open-domain splits in our experiments, consisting of 79,168 train, 8,757 development and 3,610 question answer pairs. For all three datasets, the canonical train, development and test splits were obtained by randomly splitting the question-answer pairs, and there are no exact duplicate questions in any dataset. We exclude development data from our overlap analyses, and focus purely on train-test overlap to explicitly assess the effects of training memorization.", "publication_ref": ["b0", "b7", "b1", "b0", "b16", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Test-Train Overlaps", "text": "We explore two ways of examining the test sets based on overlaps between training and test data. Consider a question-answer pair (q, a) from the test set D test where the answer consists of at least one answer reference a = {s 1 ..s n }. We can consider answer overlap where there exists at least one (q , a ) \u2208 D train which shares at least one answer reference with (q, a). We can also consider question overlap, where there exists some (q , a ) \u2208 D train where q is a duplicate of q, such that q and q are paraphrases and have the same answer.\nAnswer Overlap Following Rajpurkar et al. (2016), we apply answer normalization (lowercasing, stripping punctuation, removing articles and normalizing whitespace) on answer references  Jason Marsden who plays max voice in a goofy movie who does max voice in a goofy movie January 23 2018 when will the 2018 oscar nominations be announced when are the oscar nominations for 2018 announced Alan Shearer who has scored more goals in the premier league most goals scored by a premier league player retina where are the cones in the eye located where are cone cells located in the eye francisco pizarro who led the conquest of the incas in south america conquistador who defeated the incan empire in peru Training questions are selected for annotation if one of the following is true: they share an answer reference with a test question, a test answer reference is a sub-sequence of a training answer reference, or the other way around (a training reference answer is a sub-sequence of a test answer reference). If there are more than 50 such questions, the top 50 are chosen by the highest degree of word overlap with the test question. test question and had the same answer.\nThe results from the annotation can be seen in Table 1 and examples of overlapping questions in Table 3. A sample of 100 2-way annotated examples indicated 93% agreement, corresponding to a Cohen's Kappa of 0.85 (Cohen, 1960). What we observe is a high degree of question overlap, with between 27.5% and 33.6% of the 1,000 annotated test questions having a duplicate in the train set. It is also common to see several duplicates per test question, with an average of 2.8 duplicate questions per overlapping test question in NaturalQuestions.", "publication_ref": ["b18", "b3"], "figure_ref": [], "table_ref": ["tab_1", "tab_5"]}, {"heading": "Implications for Modelling", "text": "Earlier we identified three classes of answering behaviours: 1) questions that can be memorized at training time, 2) novel questions that can be answered with answers memorized at training time, 3) novel questions with novel answers. We refer to these behaviours as Question memorization, Answer classification and QA generalization.\nQuestion memorization To perform well on the question-overlap subset, a model only needs to memorize (q, a) pairs at training time, then recognize which training question matches a test-time question. The reasoning required ranges from trivial duplicate detection for very similar questions such as \"who played pink in pink floyd the wall\" and \"who played pink in the movie the wall\", to more challenging inference problems for subtler duplicates like \"On which island in the North Sea did both St Aidan and St Cuthbert live?\" and \"irish born missionary st aidan founded a monastery in  653 on which english island which is also the name of a 1970s uk folk-rock band?\". Manual annotation of 100 question-overlap pairs showed 81% were simple duplicates differing by one or two words, 14% needed some paraphrase recognition ability, and 5% needed more sophisticated language understanding. To measure performance on question memorization, we build a test subset of (q, a) pairs with question overlap to the train set.\nAnswer Classification For a model to handle answer-overlap questions, a classifier over train set answers would be sufficient, as answers never appear at test time that do not appear at training time.\nWe build a test subset of (q, a) pairs which have answer overlap, but not question overlap. Questionoverlap pairs are excluded as they are significantly easier and would inflate scores.\nQA Generalization Here, models cannot rely on memorizing their training data. To measure performance on this most challenging split, we build a test subset of (q, a) pairs which do not have answer overlap with the train set. We also note that we expect higher frequency answers, such as countries, integers and public figures will naturally appear less often in this subset. As such, models that perform well on the head of the answer distribution may struggle to perform well here, despite being able to perform some generalization at test time.\nNext we briefly describe the models included in our analysis. For published models, we obtain test set predictions directly from the authors.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Open-Book Models", "text": "Open-book Models first retrieve relevant documents from Wikipedia and then either ex-tract or generate answers conditioned on those documents. We consider Dense Passage Retriever (DPR) , a pipeline model which retrieves documents using dense embeddings, before feeding them into a conventional reader-reranker which extracts spans of text as answers. We also include Retrieval-Augmented Generation , a model that jointly learns to retrieve and generate answers in a seq2seq framework. Finally we include Fusion-in-Decoder (FID) (Izacard and Grave, 2020), a pipeline model which retrieves 100 documents and fuses them so that the decoder can attend to all documents at once. We do not include FID results on WebQuestions as the authors did not use it in their original work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Closed-Book Models", "text": "Closed-book models store the knowledge required to answer their questions entirely within the model's parameters, rather than in an external corpus. Typically these models consist of seq2seq transformers directly fine-tuned on (q, a) pairs. In our analysis, we train a BART-large closed-book model, which is trained with questions as input and generates (q, a) pairs as output. Checkpoints are selected by Exact Match score on a development set. We also include a more powerful T5-11B model from (Roberts et al., 2020). We use the T5-11B model pretrained with a special \"Salient Span Masking\" objective (Guu et al., 2020), which improves downstream ODQA performance. We do not include TriviaQA results for T5 since their model used a different data splitting scheme.", "publication_ref": ["b19", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Nearest-Neighbor Models", "text": "Given the high levels of train-test overlaps in these datasets, we also experiment with some simple nearest-neighbor models. Here, we simply retrieve a (q, a) pair from the train set based on question similarity to the test question, and return its answer. We experiment with two models, one using TF-IDF and the other using maximum inner product of question embeddings from the DPR retriever. These models cannot generalize to nonoverlapping answers, and have limited capacity to answer non-overlapping questions. However, they are attractive from the perspective of model size and efficiency (Min et al., 2020a).", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Question Memorization Earlier, we found that \u223c30% of test set questions overlap with the train set. The \"Question overlap\" columns in Table 4 shows performance on Question Memorization. Comparing this column with the total performance column shows that all models perform significantly higher on memorizable questions. This effect is most pronounced for closed book models. The T5-11B performs especially well for question memorization on both NaturalQuestions and WebQuestions. This suggests that its very large capacity, coupled with more powerful question understanding may allow it to store and recall training questions more effectively than other models.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Answer Classification", "text": "The \"Answer overlap only\" column in Table 4 shows performance on answer classification. Answer classification has a large drop in performance compared to question memorization, dropping by an average of 45% Exact Match. Open-book models handle this setting better than closed book models. The BART model in particular struggles here, scoring only 10.2%.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "QA Generalization", "text": "The \"No overlap\" column in Table 4 shows performance on QA generalization. All models significantly lose performance on QA generalization, highlighting the shortcomings of the total performance metric. For example, we may expect the FID state-of-the-art model to answer half of NaturalQuestions-style questions correctly, but once we account for repeated questions and answers, it can only answer about a third of questions correctly. This difference is even more pronounced for other models, with an average absolute drop of 25% with respect to total performance.\nNearest-Neighbor Models The bottom two rows of Table 4 show results for nearest-neighbor models. TF-IDF, despite being completely untrained, can answer about 20% of test questions correctly, purely by retrieving questions from the train sets. The dense retrieval model outperforms the BART closed-book model on NaturalQuestions and TriviaQA. Further, the dense nearest neighbor model also outperforms the significantly more complex DPR open-book model on TriviaQA and WebQuestions on the question overlap subset.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7", "tab_7"]}, {"heading": "Related Work", "text": "Examining what behaviours are learnt by models has received attention in language understanding tasks, such as GLUE (Wang et al., 2018), which includes tools for probing for different reasoning types. There has also been critical and careful analysis of QA systems and datasets. Chen et al. (2016), Sugawara et al. (2020) and Kaushik and Lipton (2018) analyse the difficulty of various machine reading datasets, and Manjunatha et al. (2018) show that visual QA models memorize common question-answer relationships in training data. F\u00e9vry et al. (2020) analyse various closed-book models' TriviaQA predictions. Kwiatkowski et al. (2019) note that the machine reading NaturalQuestions dataset has train-test overlap of Wikipedia titles, and provide baselines for \"long-answer\" QA. Verga et al. (2020) observe answer overlap effects in a related modality (knowledgebase QA), but no not consider question overlap.", "publication_ref": ["b23", "b2", "b21", "b9", "b14", "b4", "b10", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We performed an analysis of popular open-domain QA datasets. We found that 60% of test set answers overlap with the train set and 30% of test set questions have at least one duplicate in the train set. Given these observations, we contextualize performance of seven ODQA models, stratifying by the extent of overlap, exploring how well these models generalize verses simply memorizing training data. It is clear that performance on these datasets cannot be properly understood by overall QA accuracy. In the future, a greater emphasis should be placed on more behaviour-driven evaluation, rather than pursuing single-number overall accuracy.      ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Semantic Parsing on Freebase from Question-Answer Pairs", "journal": "", "year": "2013", "authors": "Jonathan Berant; Andrew Chou; Roy Frostig; Percy Liang"}, {"ref_id": "b1", "title": "Freebase: a collaboratively created graph database for structuring human knowledge", "journal": "Association for Computing Machinery", "year": "2008", "authors": "Kurt Bollacker; Colin Evans; Praveen Paritosh; Tim Sturge; Jamie Taylor"}, {"ref_id": "b2", "title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Danqi Chen; Jason Bolton; Christopher D Manning"}, {"ref_id": "b3", "title": "A Coefficient of Agreement for Nominal Scales", "journal": "Educational and Psychological Measurement", "year": "1960", "authors": "Jacob Cohen"}, {"ref_id": "b4", "title": "Entities as Experts: Sparse Memory Access with Entity Supervision", "journal": "", "year": "2020", "authors": "Thibault F\u00e9vry; Baldini Livio; Nicholas Soares; Eunsol Fitzgerald; Tom Choi;  Kwiatkowski"}, {"ref_id": "b5", "title": "REALM: Retrieval-Augmented Language Model Pre-Training", "journal": "", "year": "2020", "authors": "Kelvin Guu; Kenton Lee; Zora Tung; Panupong Pasupat; Ming-Wei Chang"}, {"ref_id": "b6", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering", "journal": "", "year": "2020", "authors": "Gautier Izacard; Edouard Grave"}, {"ref_id": "b7", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Mandar Joshi; Eunsol Choi; Daniel Weld; Luke Zettlemoyer"}, {"ref_id": "b8", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "journal": "", "year": "2020", "authors": "Vladimir Karpukhin; Barlas Oguz; Sewon Min; Patrick Lewis; Ledell Wu; Sergey Edunov; Danqi Chen; Wen-Tau Yih"}, {"ref_id": "b9", "title": "How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Divyansh Kaushik; Zachary C Lipton"}, {"ref_id": "b10", "title": "Natural Questions: a Benchmark for Question Answering Research. Transactions of the Association of Computational Linguistics", "journal": "", "year": "2019", "authors": "Tom Kwiatkowski; Jennimaria Palomaki; Olivia Redfield; Michael Collins; Ankur Parikh; Chris Alberti; Danielle Epstein; Illia Polosukhin; Matthew Kelcey; Jacob Devlin; Kenton Lee; Kristina N Toutanova; Llion Jones; Ming-Wei Chang; Andrew Dai; Jakob Uszkoreit; Quoc Le; Slav Petrov"}, {"ref_id": "b11", "title": "Latent Retrieval for Weakly Supervised Open Domain Question Answering", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Kenton Lee; Ming-Wei Chang; Kristina Toutanova"}, {"ref_id": "b12", "title": "BART: Denoising Sequence-to-Sequence Pretraining for Natural Language Generation", "journal": "", "year": "2019", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal ; Abdelrahman Mohamed; Omer Levy; Ves Stoyanov; Luke Zettlemoyer"}, {"ref_id": "b13", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "journal": "", "year": "2020", "authors": "Patrick Lewis; Ethan Perez; Aleksandara Piktus; Fabio Petroni; Vladimir Karpukhin; Naman Goyal; Heinrich K\u00fcttler; Mike Lewis; Wen-Tau Yih; Tim Rockt\u00e4schel; Sebastian Riedel; Douwe Kiela"}, {"ref_id": "b14", "title": "Explicit Bias Discovery in Visual Question Answering Models", "journal": "", "year": "2018", "authors": "Varun Manjunatha; Nirat Saini; Larry S Davis"}, {"ref_id": "b15", "title": "NeurIPS 2020 Ef-ficientQA Competition: Systems, Analyses and Lessons Learned", "journal": "", "year": "2020", "authors": "Sewon Min; Jordan Boyd-Graber; Chris Alberti; Danqi Chen; Eunsol Choi; Michael Collins; Kelvin Guu; Hannaneh Hajishirzi; Kenton Lee; Jennimaria Palomaki; Colin Raffel; Adam Roberts; Tom Kwiatkowski; Patrick Lewis; Yuxiang Wu; Heinrich K\u00fcttler; Linqing Liu; Pasquale Minervini; Pontus Stenetorp; Sebastian Riedel; Sohee Yang; Minjoon Seo; Gautier Izacard; Fabio Petroni; Lucas Hosseini; Nicola De Cao; Edouard Grave; Ikuya Yamada; Sonse Shimaoka; Masatoshi Suzuki; Shumpei Miyawaki; Shun Sato; Ryo Takahashi; Jun Suzuki; Martin Fajcik; Martin Docekal; Karel Ondrej; Pavel Smrz; Hao Cheng; Yelong Shen; Xiaodong Liu; Pengcheng He; Weizhu Chen; Jianfeng Gao; Barlas Oguz; Xilun Chen; Vladimir Karpukhin; Stan Peshterliev; Dmytro Okhonko; Michael Schlichtkrull; Sonal Gupta; Yashar Mehdad; Wen-Tau Yih"}, {"ref_id": "b16", "title": "A Discrete Hard EM Approach for Weakly Supervised Question Answering", "journal": "", "year": "2019", "authors": "Sewon Min; Danqi Chen; Hannaneh Hajishirzi; Luke Zettlemoyer"}, {"ref_id": "b17", "title": "Knowledge Guided Text Retrieval and Reading for Open Domain Question Answering", "journal": "", "year": "2020", "authors": "Sewon Min; Danqi Chen; Luke Zettlemoyer; Hannaneh Hajishirzi"}, {"ref_id": "b18", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "journal": "", "year": "2016", "authors": "Pranav Rajpurkar; Jian Zhang; Konstantin Lopyrev; Percy Liang"}, {"ref_id": "b19", "title": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?", "journal": "", "year": "2020", "authors": "Adam Roberts; Colin Raffel; Noam Shazeer"}, {"ref_id": "b20", "title": "What Makes Reading Comprehension Questions Easier?", "journal": "", "year": "2018", "authors": "Saku Sugawara; Kentaro Inui; Satoshi Sekine; Akiko Aizawa"}, {"ref_id": "b21", "title": "Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets", "journal": "AAAI Press", "year": "2020-02-07", "authors": "Saku Sugawara; Pontus Stenetorp; Kentaro Inui; Akiko Aizawa"}, {"ref_id": "b22", "title": "Facts as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge", "journal": "", "year": "2020", "authors": "Pat Verga; Haitian Sun; Baldini Livio; William W Soares;  Cohen"}, {"ref_id": "b23", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Alex Wang; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel Bowman"}], "figures": [{"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Test-train set overlap for the datasets.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Randomly sampled overlapping and non-overlapping answers from all three test sets.", "figure_data": "AnswerTest QuestionTrain Question"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "This answer similarity function is designed for high recall to obtain a tight lower bound on question overlap. If there were no questions with similar answers in the train set, the question was automatically annotated as not overlapping. Three expert annotators annotated the remaining questions and indicated if any were paraphrases of the 2", "figure_data": "per"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Exact Match scores on our dataset splits. \"Total\" is the overall performance on the dataset. \"Question Overlap\" is the subset with train-test question overlap, and probes for simple question memorization. \"Answer Overlap Only\" is subset without train-test question overlap, but with train-test answer overlap, which probes for answer classification. \"No overlap\" is the subset with no train-test answer overlap and probes for QA generalization", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "gives details of how many instances are in each test set, as well as the number of instances in each test subset used in the main paper.", "figure_data": "A.2 Additional Question Overlap ExamplesTables 6, 7 and 8 give more question overlap exam-ples for the three datasets."}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Number of instances in each test set subset", "figure_data": "AnswerTest QuestionTrain QuestionBob Geldofwho played pink in pink floyd the wallwho played pink in the movie the wallDaren Maxwell Kaga-who played ricky in secret life of the american teenager who played ricky on the secret life of the american teenagersoffAndywho does april end up with on parks and recwho does april marry in parks and recmay 5 2017when did gaurdians of the galaxy 2 come outwhen is guardians of the galaxy vol 2 releasednorman pritchardwho won the first medal in olympics for indiawho won the first individual olympic medal for indiamoira kellywho does the voice of nala in the lion kingwho played nala in the lion king moviesupreme courtwho enforces the charter of rights and freedomswho has final authority of interpretation of the canadiancharter of rights and freedoms554most passing yards by nfl qb in a gamewhat is the nfl record for most passing yards in a single gameJohn Rosswho ran the fastest 40 yard dash in the nflwho has the fastest 40 yard dash everinternational border ibwhat is the name of india pakistan borderwhat is the border name between india and pakistanAndrew Wrightwho wrote when a man loves a womanwho wrote song when a man loves a womannew england patriotswho has participated in the most super bowlswhat nfl team has been to most super bowls"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Additional examples of test-train overlapping questions in Open NaturalQuestions", "figure_data": "AnswerTest QuestionTrain QuestionPicassoWho painted \"Boy With a Pipe\" which, in May 2004,painted in 1905, the painting garcon a la pipe was a famouswas sold for a record price of $104 million?painting by which famous artist who died in 1973?WensumOn what river is the city of Norwichthe english city of norwich lies on which river?MantleComprising around two-thirds of the Earth's mass , whatwhat do we call the layer of the earth between its crust andis found between the core of the Earth and its crust?its core?Live and Let DieIn which James Bond film does actress Jane Seymourjane seymour played the character \"solitaire\" in which bondplay Solitaire?film?EsauWho, in the Bible, was the eldest son of Isaac?in the bible, who was the first born of isaac?Alanis MorrisetteWho made the 1995 album 'Jagged Little Pill' whichwho released the 1995 hit album \"jagged little pill\"?sold 33 million copies?ExcaliburIn British legend, what is the name of King Arthur'swhat was the name of king arthur's sword?sword?HumidityWhat is measured by a Hygrometer?what does a hygrometer measure?A StormOn the Beaufort scale what is defined as force 11?what is force 11 (eleven) on the beaufort scale?Jeremy IronsActress Sinead Cusack is married to which 'Oscar' win-which actor is the husband of sinead cusack?ning actor?Sir Cloudesley ShovellWho was the British Admiral who died in 1707 whenin 1707 a fleet of navy ships was wrecked off the scillyfour of his ships were wrecked in the Scilly Isles?islands. who was the commander who lost his life in thedisaster?Tony HartWhich famous individual created the 'Blue Peter' sail-which artist designed the logo for uk television children'sing ship logo?show 'blue peter'?"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Examples of test-train overlapping questions in TriviaQA the last time the mets won the world series? when did the mets win the pennant? abbottabad where was bin laden found and killed? what country was osama bin laden killed in? believer what other movies has ryan gosling been in? what movies does ryan gosling star in? sculpture what type of art did leonardo da vinci make? what kind of art did leonardo da vinci produce? origin of species what book did charles darwin wrote in 1859? what was the name of the book that charles darwin wrote? morehouse college what college did martin luther king jr go to? where did dr. martin luther king jr. go to school? communist state what type of government did soviet union have? what type of government does the former soviet union have? turkish lira what money to take to turkey? what currency to take to side turkey? spanish language what is the most common language spoken in argentina? what is language in argentina? opera OR classical music what music period did beethoven live in? what music did beethoven composed? harry s truman who was president after franklin d. roosevelt? who became president when roosevelt died in office?", "figure_data": "AnswerTest QuestionTrain Questioncosta ricawhere is isthmus of panama located on the map?where is isthmus of panama located?1986 world serieswhen's"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "", "figure_data": ""}], "formulas": [], "doi": "10.1145/1376616.1376746"}