{"title": "Memory-Augmented Monte Carlo Tree Search", "authors": "Chenjun Xiao; Jincheng Mei; Martin M\u00fcller", "pub_date": "", "abstract": "This paper proposes and evaluates Memory-Augmented Monte Carlo Tree Search (M-MCTS), which provides a new approach to exploit generalization in online realtime search. The key idea of M-MCTS is to incorporate MCTS with a memory structure, where each entry contains information of a particular state. This memory is used to generate an approximate value estimation by combining the estimations of similar states. We show that the memory based value approximation is better than the vanilla Monte Carlo estimation with high probability under mild conditions. We evaluate M-MCTS in the game of Go. Experimental results show that M-MCTS outperforms the original MCTS with the same number of simulations.", "sections": [{"heading": "Introduction", "text": "The key idea of Monte Carlo Tree Search (MCTS) is to construct a search tree of states evaluated by fast Monte Carlo simulations (Coulom 2006). Starting from a given game state, many thousands of games are simulated by randomized self-play until an outcome is observed. The state value is then estimated as the mean outcome of the simulations. Meanwhile, a search tree is maintained to guide the direction of simulation, for which bandit algorithms can be employed to balance exploration and exploitation (Kocsis and Szepesv\u00e1ri 2006). However, with large state spaces, the accuracy of value estimation cannot be effectively guaranteed, since the mean value estimation is likely to have high variance under relatively limited search time. Inaccurate estimation can mislead building the search tree and severely degrade the performance of the program.\nRecently, several machine learning approaches have been proposed to deal with this drawback of MCTS. For example, deep neural networks are employed to learn domain knowledge and approximate a state value function. They are integrated with MCTS to provide heuris-Copyright c 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\ntics which can improve the search sample efficiency in practice (Silver et al. 2016;Tian and Zhu 2015).\nThe successes of the machine learning methods can be mostly contributed to the power of generalization, i.e., similar states share information. Generalized domain knowledge is usually represented by function approximation, such as a deep neural network, which is trained offline from an expert move dataset or selfgenerated simulations (Silver et al. 2016).\nCompared with the amount of research done on discovering generalization from an offline learning procedure, not too much attention has focused on exploiting the benefits of generalization during the online realtime search. The current paper proposes and evaluates a Memory-Augmented MCTS algorithm to provide an alternative approach that takes advantage of online generalization. We design a memory, where each entry contains information about a particular state, as the basis to construct an online value approximation. We demonstrate that this memory-based framework is useful for improving the performance of MCTS in both theory and practice, using an experiment in the game of Go.\nThe remainder of the paper is organized as follows: After preliminaries introduced in Section 2, we theoretically analyze the memory framework in Section 3. The proposed Memory-Augmented MCTS algorithm is presented in Section 4. Related work and experimental results are shown in Section 5 and 6, respectively. In Section 7, we come to our conclusion and future work.", "publication_ref": ["b4", "b13", "b16", "b19", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "The Setting Let S be the set of all possible states of a search problem. For s \u2208 S, letV (s) = 1 Ns Ns t=1 R s,t denote the value estimation of state s from simulations, where R s,t is the outcome of a simulation, and N s is the number of simulations starting from state s. The true value of a state s is denoted by V * (s). The main idea of our Memory-Augmented MCTS algorithm is to approximate value estimations with the help of a memory, each entry of which contains the feature representation and simulation statistics of a particular state. The approximate value estimation is performed as follows: given a memory M and a state x, we find the M most similar states M x \u2282 M according to a distance metric d(\u2022, x), and compute a memory-based value estimation\nV M (x) = M i=1 w i (x)V (i), s.t. M i=1 w i (x) = 1.\nLet X s,t = |R s,t \u2212V * (s)| be the sampling error from the tth simulation of state s. In the analysis of the most popular MCTS algorithm UCT (Kocsis and Szepesv\u00e1ri 2006), X s,t is assumed to be \u03c3 2 -subgaussian, so the sampling error has zero mean and its variance is upper bounded by \u03c3 2 (Boucheron, Lugosi, and Massart 2013). We also adopt the same assumption in our analysis. We denote the value estimation error of state s by \u03b4 s = |V (s) \u2212 V * (s)|, and the true value difference between states s and x by \u03b5 s,x = |V * (s) \u2212 V * (x)|. Using the property of subgaussian variables, \u03b4 s is \u03c3 2\nNssubgaussian (Boucheron, Lugosi, and Massart 2013). Let \u03b5 M = max i\u2208Mx \u03b5 i,x , we assume that our memory addressing scheme is able to control \u03b5 M within the range [0, \u03b5]. The following lemma states the concentration property of subgaussian variables. Lemma 1. (Boucheron, Lugosi, and Massart 2013) \nIf X is \u03c3 2 -subgaussian, then P (X \u2265 ) \u2264 exp(\u2212 2 2\u03c3 2 ).", "publication_ref": ["b13", "b0", "b0", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Monte Carlo Tree Search", "text": "MCTS builds a tree to evaluate states with fast simulations (Coulom 2006). Each node in the tree corresponds to a specific state s \u2208 S, and contains simulation statis-ticsV (s) and N (s). At each iteration of the algorithm, one simulation starts from an initial state s 0 , and proceeds in two stages: in-tree and rollout. When a state s t is already represented in the current search tree, a tree policy is used to select an action to go to the next state. The most popular choice of the tree policy is to use bandit algorithms such as UCB1 (Kocsis and Szepesv\u00e1ri 2006). For states outside the tree, a roll-out policy is used to simulate a game until the end, where a trajectory of visited states T = {s 0 , s 1 , . . . , s T } and a final return R are obtained. The statistics of s \u2208 T in the tree are updated according to:\nN (s) \u2190 N (s) + 1 V (s) \u2190V (s) + R \u2212V (s) N (s)\nIn addition the search tree is grown. In the simplest scheme, the first visited node that is not yet in the tree is added to it.", "publication_ref": ["b4", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Entropy Regularized Policy Optimization", "text": "We denote the probability simplex by \u2206 = {w : w \u2265 0, 1 \u2022 w = 1}, and denote the entropy function by H(w) = \u2212w \u2022 log w. For any vector q \u2208 R n , the entropy-regularized optimization problem is to find the solution of max\nw\u2208\u2206 {w \u2022 q + \u03c4 H(w)} (1)\nwhere \u03c4 > 0 is the temperature parameter. This problem has recently drawn much attention in the reinforcement learning community (Nachum et al. 2017;Haarnoja et al. 2017;Ziebart 2010). One nice property of this problem is that given the vector q, it has a closed form solution. Define the scalar value function F \u03c4 (the \"softmax\") by F \u03c4 (q) = \u03c4 log( M i=1 e qi/\u03c4 ), and the vector-valued function f \u03c4 (q) (the \"soft indmax\") by f \u03c4 (q) = e q/\u03c4 M i=1 e q i /\u03c4 = e (q\u2212F\u03c4 (q))/\u03c4 , where the exponentiation is component-wise. Note that f \u03c4 maps any real valued vector into a probability distribution. The next lemma states the connection between F \u03c4 , f \u03c4 and the entropy regularized optimization problem. Lemma 2. (Nachum et al. 2017;Haarnoja et al. 2017;Ziebart 2010)\nF \u03c4 (q) = max w\u2208\u2206 {w \u2022 q + \u03c4 H(w)} =f \u03c4 (q) \u2022 q + \u03c4 H(f \u03c4 (q))", "publication_ref": ["b15", "b10", "b23", "b15", "b10", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Value Approximation with Memory", "text": "In our approach, the memory is used to provide an approximate value functionV\nM (x) = M i=1 w i (x)V (i),\nwhere M i=1 w i (x) = 1 are the weights and M is a parameter defining the neighbouring states in the memory according to some distance metric d(\u2022, x). One question naturally arises, is this memory-based value approximation better than the vanilla mean outcome estimation? In this section we attempt to answer this question by showing that |V M (x) \u2212 V * (x)| \u2264 \u03b4 x for state x with high probability under a mild condition. We first show a trivial bound for Pr(|V M (x) \u2212 V * (x)| \u2264 \u03b4 x ), then provide an improved bound with entropy regularized policy.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Trivial Probability Bound", "text": "The first step is to upper bound |V M (x) \u2212 V * (x)| using the triangle inequality:\n| M i=1 w i (x)V (i) \u2212 V * (x)| \u2264 M i=1 w i (x)|V (i) \u2212 V * (x)| \u2264 M i=1 w i (x)(|V (i) \u2212 V * (i)| + |V * (i) \u2212 V * (x)|) = M i=1 w i (x)(\u03b4 i + \u03b5 i,x ) (2) Let \u03b4 M = max i\u2208Mx \u03b4 i and \u03b5 M = max i\u2208Mx \u03b5 i,x . Using the fact that M i=1 w i (x) = 1, we can further take an upper bound of (2) by M i=1 w i (x)(\u03b4 i + \u03b5 i,x ) \u2264 \u03b4 M + \u03b5 M .\nThis upper bound is very loose, since we do not specify any particular choice of the weights w. With a standard probability argument we can immediately get the following: Theorem 1. For states x satisfying \u03b1 x = \u03b4 x \u2212 \u03b5 > 0, let n min = min i\u2208Mx N i . Then with probability at least 1 \u2212 \u03b2, our memory-based value function approximation has less error than \u03b4 x provided that:\nn min \u2265 2\u03c3 2 \u03b1 2 x log(M/\u03b2)(3)\nCondition ( 3), under which the high probability bound can be guaranteed, is quite severe. It requires that the minimum simulation numbers of all addressed memory entries are sufficiently large. This trivial bound is weak since the upper bound (2) depends on the worst memory entry addressed, without specifying any choice of the weights w. We show that the entropy regularized policy optimization can help us to fix this problem.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Improved Probability Bound with Entropy Regularized Policy", "text": "We now provide an improvement of the previous bound by specifying the choice of the weights w using entropy regularized optimization. Let c be a vector where\nc i = \u03b4 i + \u03b5 i,x , 1 \u2264 i \u2264 M .\nOur choice of w should minimize the upper bound (2), which is equivalent to:\nmax w\u2208\u2206 {w \u2022 (\u2212c)} (4)\nThis linear optimization problem has solution w j = 1 for j = argmin i (\u03b4 i + \u03b5 i,x ) and w k = 0 for k = j. However, in practice we do not know the accurate value of \u03b4 i and \u03b5 i,x and applying this deterministic policy may cause the problem of addressing the wrong entries. We provide an approximation by solving the entropy regularized version of this optimization problem:\nmax w\u2208\u2206 {w \u2022 (\u2212c) + \u03c4 H(w)} (5)\nAs \u03c4 approaches zero, we recover the original problem (4). According to Lemma 2, the closed form solution of problem ( 5) is\nF \u03c4 (\u2212c) = \u03c4 log( M i=1 e \u2212ci/\u03c4 ) by set- ting w = f \u03c4 (\u2212c). By equation (2), \u2212f \u03c4 (\u2212c) \u2022 (\u2212c) = \u2212F \u03c4 (\u2212c) + \u03c4 H(f \u03c4 (\u2212c)) \u2264 \u2212F \u03c4 (\u2212c) + \u03c4 log M . Therefore, to show Pr{(2) \u2264 \u03b4} \u2265 1 \u2212 \u03b2 for some small constant \u03b2, it suffices to show that Pr{\u2212F \u03c4 (\u2212c)+ \u03c4 log M \u2264 \u03b4} \u2265 1 \u2212 \u03b2. Theorem 2. For states x satisfying \u03b1 x = \u03b4 x \u2212\u03b5 > 0, let n = M i=1 N i . By choosing the weight w = f \u03c4 (\u2212c) = e \u2212c/\u03c4 / M i=1 e \u2212ci/\u03c4\n, with probability at least 1\u2212\u03b2 our memory-based value function approximation has less error than \u03b4 x provided that:\nn \u2265 2\u03c3 2 (\u03b1 x \u2212 \u03c4 log M ) 2 log(1/\u03b2) (6)\nProof. We show that under condition (6), it can be guaranteed that Pr (\u2212\nF \u03c4 (\u2212c) + \u03c4 log M \u2264 \u03b4 x ) \u2265 1 \u2212 \u03b2. Pr \u2212\u03c4 log( M i=1 exp(\u2212ci/\u03c4 )) \u2264 \u03b4x \u2212 \u03c4 log M = Pr M i=1 exp(\u2212ci/\u03c4 ) \u2265 exp(\u2212(\u03b4x \u2212 \u03c4 log M )/\u03c4 ) \u2265 Pr M i=1 exp(\u2212\u03b4i/\u03c4 ) \u2265 exp(\u2212(\u03b4x \u2212 \u03b5 \u2212 \u03c4 log M )/\u03c4 ) \u2265 Pr(\u2203 i, exp(\u03b4i/\u03c4 ) \u2264 exp((\u03b4x \u2212 \u03b5 \u2212 \u03c4 log M )/\u03c4 ) = 1 \u2212 M i=1 Pr (\u03b4i \u2265 \u03b1 \u2212 \u03c4 log M ) \u2265 1 \u2212 M i=1 exp(\u2212 (\u03b1x \u2212 \u03c4 log M ) 2 Ni 2\u03c3 2 ) = 1 \u2212 exp(\u2212 (\u03b1x \u2212 \u03c4 log M ) 2 n 2\u03c3 2 )\nThe first inequality comes from our assumption that all \u03b5 i,x \u2264 \u03b5, and the last inequality comes from the concentration property of subgaussian variables (Lemma 1). All other inequalities can be obtained using standard probability arguments. Equation ( 6) can be derived directly with standard algebra.\nThe probability bound provided by Theorem 2 is much better than the one in Theorem 1, since n is the sum of simulation counts of all addressed memory entries, which has to be greater than n min .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Memory-Augmented MCTS", "text": "In the previous section, we prove that our memorybased value function approximation is better than the mean outcome evaluation used in MCTS with high probability under mild conditions. The remaining question is to design a practical algorithm and incorporate it with MCTS. In particular, this first requires choosing an approximation of the weight function w = f \u03c4 (\u2212c).\nApproximating w = f \u03c4 (\u2212c) Let \u03c6 : S \u2192 R D be a function to generate the feature representation of a state. For two states s, x \u2208 S, we approximate the difference between V * (s) and V * (x) by a distance function d(s, x) which is set to be the negative cosine of the two states' feature representations:\n\u03b5 s,x \u2248 d(s, x) = \u2212 cos(\u03c6(s), \u03c6(x))(7)\nWe apply two steps to create \u03c6. First, take the output of an inner layer of a deep convolutional neural network and normalize it. We denote this process as \u03b6 : S \u2192 R L . In practice L will be very large which is time-consuming when computing (7). We overcome this problem by applying a feature hashing function h : R L \u2192 R D (Weinberger et al. 2009), and the feature representation is computed by \u03c6(s) = h(\u03b6(s)).\nOne nice property of feature hashing is that it can keep the inner product unbiased. Since \u03b6(s) is normalized, we have:\nE[cos(\u03c6(s), \u03c6(x))] = cos(\u03b6(s), \u03b6(x))\n\u03b4 x is the term corresponding to the sampling error, which is inversely proportional to the simulation numbers: \u03b4 x \u221d 1/N x . Combining with ( 7) and the fact that e y is very close to y + 1 for small y we can get our approximation of f \u03c4 (\u2212c):\nw i (x) = N i exp(\u2212d(i, x)/\u03c4 ) M j=1 N j exp(\u2212d(j, x)/\u03c4 )(8)\nBy applying these approximations our model becomes a special case of kernel based methods, such as Locally Weighted Regression and Kernel Regression (Friedman, Hastie, and Tibshirani 2001), where the kernel function can be defined by\nk i (x) = exp(\u2212d(i, x)/\u03c4 )/ M j=1 exp(\u2212d(j, x)/\u03c4\n). \u03c4 acts like the smoothing factor in those kernel based methods. Our model is also similar to the \"attention\" scheme used in memory based neural networks (Graves et al. 2016;Weston, Chopra, and Bordes 2015;Vinyals et al. 2016;Pritzel et al. 2017).", "publication_ref": ["b21", "b6", "b9", "b22", "b20", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Memory Operations", "text": "One memory M is maintained in our approach. Each entry of M corresponds to one particular state s \u2208 S. It contains the state's feature representation \u03c6(s) as well as its simulation statisticsV (s) and N (s). There are three operations to access M: update, add and query.\nUpdate If the simulation statistics of a state s have been updated during MCTS, we also update its corresponding valuesV (s) and N (s) in the memory.\nAdd To include state s, we add a new memory entry {\u03c6(s),V (s), N (s)}. If s has already been stored in the memory, we only updateV (s) and N (s) at the corresponding entry. If the maximum size of the memory is reached, we replace the least recently queried or updated memory entry with the new one.  The two advantages of addressing the top M similar states are: first, to restrict the maximum value difference of addressed states with V * (x) within a range, which is shown to be useful in our analysis; second, to make queries in a very large memory scalable. We use an approximate nearest neighbours algorithm to perform the queries based on SimHash (Charikar 2002).", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Integrating Memory with MCTS", "text": "We are now ready to introduce our Memory-Augmented MCTS (M-MCTS) algorithm. Figure (1) provides a brief illustration. The main difference between the proposed M-MCTS and regular MCTS is that, in each node of a M-MCTS search tree, we store an extended set of statistics:\n{N (s),V (s), N M (s),V M (s)}\nHere, N M is the number of evaluations of the approximated memory valueV M (s). During in-tree search of MCTS, instead ofV (s), we use (1 \u2212 \u03bb s )V (s) + \u03bb sVM (s) as the value of state s, which is used for intree selection, for example in the UCB formula. \u03bb s is a decay parameter to guarantee no bias asymptotically.\nIn original MCTS, a trajectory of visited states T = {s 0 , s 1 , . . . , s T } is obtained at the end of each simulation. The statistics of all states s \u2208 T in the tree are updated. In M-MCTS, we also update the in-memory statistics by performing the update(s) operation of M. Furthermore, when a new state s is searched by MCTS, we compute \u03c6(s) and use the add(s) operation to include s in the memory M.\nThe most natural way to obtainV M (s) and N M (s) is to compute and update their value every time s is visited during the in-tree search stage. However, this direct method is time-consuming, especially when the memory size is large. Instead, we only compute the memory value at the leaf node and backpropagate the value to its ancestors. Specifically, let s h \u2208 T be the state just added to the tree whose feature representation \u03c6(s h ) has already been computed, and its memory approx-\nimated valueV M (s h ) is computed by query(s h ). Let N M (s h ) = M j=1 k j (s h )N j , R =V M (s h ) * N M (s h ).\nFor state s i \u2208 {s 0 , . . . , s h }, we perform the following updates, where \u03b7 \u2265 1 is a decay parameter.\nX \u2190 max(N M (s h )/\u03b7 |i\u2212h| , 1) N M (s i ) \u2190 N M (s i ) + X V M (s i ) \u2190V M (s i ) + R \u2212V M (s i ) * X N M (s i )(9)\nThe reason for the decay parameter \u03b7 is because the memory-approximated value of a state is more similar to its closer ancestors.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "The idea of utilizing information of similar states has been previously studied in game solver. (Kawano 1996) provided a technique where proofs of similar positions are reused for proving another nodes in a game tree. (Kishimoto and M\u00fcller 2004) applied this to provide an efficient Graph History Interaction solution, for solving the game of Checkers and Go. Memory architectures for neural networks and reinforcement learning have been recently described in Memory Networks (Weston, Chopra, and Bordes 2015), Differentiable Neural Computers (Graves et al. 2016), Matching Network (Vinyals et al. 2016) and Neural Episodic Control (NEC) (Pritzel et al. 2017). The most similar work with our M-MCTS algorithm is NEC, which applies a memory framework to provide action value function approximation in reinforcement learning. The memory architecture and addressing method are similar to ours. In contrast to their work, we provide theoretical analysis about how the memory can affect value estimation. Furthermore, to our best knowledge, this work is the first one to apply a memory architecture in MCTS.\nThe role of generalization has been previously exploited in transposition tables (Childs, Brodeur, and Kocsis 2008), Temporal-Difference search (TD search) (Silver, Sutton, and M\u00fcller 2012), Rapid Action Value Estimation (RAVE) (Gelly and Silver 2011), and mNN-UCT (Srinivasan et al. 2015). A transposition table provides a simple form of generalization. All nodes in the tree corresponding to the same state share the same simulation statistics. Our addressing scheme can closely resemble a transposition table by setting \u03c4 close to zero. In M-MCTS with \u03c4 > 0 the memory can provide more generalization, which we show to be beneficial both theoretically and practically.\nTD search uses linear function approximation to generalize between related states. This linear function approximation is updated during the online real-time search. However, with complex non-linear function approximation such as neural networks, such updates are impossible to perform online. Since our memory based method is non-parametric, it provides an alternative approach for generalization during real time search.\nRAVE uses the all-moves-as-first heuristic based on the intuition that the value of an action is independent of when it is taken. Simulation results are not only updated to one, but to all actions along the simulation path. mNN-UCT applies kernel regression to approximate a state value function, which has been shown equivalent to our addressing scheme using our choice of approximations in Section 4. However, we use the difference between feature representations as the distance metric, while mNN-UCT applies the distance between nodes in the tree. Also, both RAVE and mNN-UCT do not provide any theoretical justifications.", "publication_ref": ["b11", "b12", "b22", "b9", "b20", "b16", "b2", "b17", "b8", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We evaluate M-MCTS in the ancient Chinese game of Go (M\u00fcller 2002).", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "Implementation Details", "text": "Our implementation applies a deep convolutional neural network (DCNN) from (Clark and Storkey 2015), which is trained for move prediction by professional game records. It has 8 layers in total, including one convolutional layer with 64 7 \u00d7 7 filters, two convolutional layers with 64 5 \u00d7 5 filters, two layers with 48 5 \u00d7 5 filters, two layers with 32 5 \u00d7 5 filters, and one fully connected layer. The network has about 44% prediction accuracy on professional game records. The feature vector \u03c6(s) is first extracted from the output of Conv7 which is the last layer before the final fully connected layer of the neural network. The dimension of this output is 23104. A dimension reduction step using feature hashing as described in Section 4 is then applied. The feature hashing dimension is set to 4096, which gives \u03c6(s) \u2208 R 4096 .\nThe hash code in our SimHash implementation has 16 bits. We use 8 hash tables, each of which corresponds to a unique hash function. We also apply a multiple probing strategy. Suppose that a feature vector \u03c6(s) is mapped to the hash bin b i at the ith hash table. Let the hash code of b i be h i . To search the neighbours of \u03c6(s) in the ith table, we search those bins whose hash codes' hamming distance to h i is less than a threshold, set to 1 in our implementation. The discount parameter \u03b7 in equation ( 9) to update memory approximated values is set to 2.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "Baseline", "text": "Our baseline is based on the open source Go program Fuego (Enzenberger andM\u00fcller 2008 2017), but adds We implement DCNN in MCTS in a synchronized way, where the search continues after the DCNN evaluation is returned. To increase speed, we restrict DCNN calls to the first 100 nodes visited during the search. This baseline achieves a win rate of 97% against original Fuego with 10,000 simulations per move. We implement M-MCTS based on this baseline. The same DCNN is used to extract features for the memory.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We first study how the parameters M and \u03c4 can affect the performance of M-MCTS, since these two parameters together control the degree of generalization. The parameter M is chosen from {20, 50, 100}, and \u03c4 from {0.05, 0.1, 1}. The size of M is set to 10000. We vary the number of simulations per move from {1000, 5000, 10000}. Results are summarized in Figure 2(a)-(c). The best result we have is from the setting {M = 50, \u03c4 = 0.1}, which achieves a 71% win rate against the baseline with 10,000 simulations per move. The standard error of each result is around 2.5%. For M = 20 and M = 50, the performance of M-MCTS scales well with the number of simulations per move with \u03c4 = 1 and \u03c4 = 0.1. A small temperature \u03c4 = 0.05 cannot beat the baseline at all. We believe the reason is that in this setting M-MCTS only focuses on the closest neighbours for generalization, but does not do enough exploration. For M = 100, M-MCTS does not perform well in any setting of \u03c4 , since larger M increases the chance of including less similar states.\nWe then investigate the impact of the size of M by varying it from {1000, 5000, 10000}. M and \u03c4 are set to 50 and 0.1 respectively. Results with different number of simulations per move are summarized in Figure 2(d). Intuitively, a large memory can provide better performance, since more candidate states are included for query. The results shown in Figure 2(d) confirm this intuition: M-MCTS achieves the best performance with |M| = 10000, while small memory size |M| = 1000 can even lead to negative effects for value estimation in MCTS. We also compare M-MCTS with the baseline with equal computational time per move. By setting M = 50, \u03c4 = 0.1 and with 5 seconds per move, M-MCTS achieves a 61% win rate against the baseline.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "Conclusion and Future Work", "text": "In this paper, we present an efficient approach to exploit online generalization during real-time search. Our method, Memory-Augmented Monte Carlo Tree Search (M-MCTS), combines the original MCTS algorithm with a memory framework, to provide a memory-based online value approximation. We demonstrate that this can improve the performance of MCTS in both theory and practice. We plan to explore the following two potential future directions. First, we would like to investigate if we can obtain better generalization by combining an offline learned value approximation with our online memory-based framework. Second, the feature representation used in M-MCTS reuses a neural network designed for move prediction. Instead, we plan to explore approaches that incorporate feature representation learning with M-MCTS in an end-to-end fashion, similar to (Pritzel et al. 2017;Graves et al. 2016).", "publication_ref": ["b16", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "The authors wish to thank Andrew Jacobsen for providing source code of Fuego with the neural network, and the anonymous referees for their valuable advice. This research was supported by NSERC, the Natural Sciences and Engineering Research Council of Canada.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Concentration inequalities: A nonasymptotic theory of independence", "journal": "Oxford University Press", "year": "2013", "authors": "S Boucheron; G Lugosi; P Massart"}, {"ref_id": "b1", "title": "Similarity estimation techniques from rounding algorithms", "journal": "ACM", "year": "2002", "authors": "M S Charikar"}, {"ref_id": "b2", "title": "Transpositions and move groups in Monte Carlo tree search", "journal": "", "year": "2008", "authors": "B E Childs; J H Brodeur; L Kocsis"}, {"ref_id": "b3", "title": "Training deep convolutional neural networks to play Go", "journal": "", "year": "2015-07-11", "authors": "C Clark; A J Storkey"}, {"ref_id": "b4", "title": "Efficient selectivity and backup operators in Monte-Carlo tree search. In van den Herik", "journal": "Springer", "year": "2006", "authors": "R Coulom"}, {"ref_id": "b5", "title": "", "journal": "", "year": "2008", "authors": "M Enzenberger; M M\u00fcller"}, {"ref_id": "b6", "title": "The elements of statistical learning", "journal": "Springer", "year": "2001", "authors": "J Friedman; T Hastie; R Tibshirani"}, {"ref_id": "b7", "title": "Combining online and offline knowledge in UCT", "journal": "ACM", "year": "2007", "authors": "S Gelly; D Silver"}, {"ref_id": "b8", "title": "Monte-Carlo Tree Search and Rapid Action Value Estimation in computer Go", "journal": "Artificial Intelligence", "year": "2011", "authors": "S Gelly; D Silver"}, {"ref_id": "b9", "title": "Hybrid computing using a neural network with dynamic external memory", "journal": "Nature", "year": "2016", "authors": "A Graves; G Wayne; M Reynolds; T Harley; I Danihelka; A Grabska-Barwi\u0144ska; S G Colmenarejo; E Grefenstette; T Ramalho; J Agapiou"}, {"ref_id": "b10", "title": "Reinforcement learning with deep energy-based policies", "journal": "", "year": "2017-08-11", "authors": "T Haarnoja; H Tang; P Abbeel; S Levine"}, {"ref_id": "b11", "title": "Using similar positions to search game trees", "journal": "Cambridge University Press", "year": "1996", "authors": "Y Kawano"}, {"ref_id": "b12", "title": "A general solution to the graph history interaction problem", "journal": "", "year": "2004", "authors": "A Kishimoto; M M\u00fcller"}, {"ref_id": "b13", "title": "Bandit based Monte-Carlo planning", "journal": "Springer", "year": "2006", "authors": "L Kocsis; C Szepesv\u00e1ri"}, {"ref_id": "b14", "title": "", "journal": "Computer Go. Artificial Intelligence", "year": "2002", "authors": "M M\u00fcller"}, {"ref_id": "b15", "title": "Bridging the gap between value and policy based reinforcement learning", "journal": "", "year": "2017", "authors": "O Nachum; M Norouzi; K Xu; D Schuurmans"}, {"ref_id": "b16", "title": "Mastering the game of Go with deep neural networks and tree search", "journal": "", "year": "2016", "authors": "A Pritzel; B Uria; S Srinivasan; A Puigdom\u00e8nech; O Vinyals; D Hassabis; D Wierstra; C Blundell; D Silver; A Huang; C J Maddison; A Guez; L Sifre; G Van Den Driessche; J Schrittwieser; I Antonoglou; V Panneershelvam; M Lanctot"}, {"ref_id": "b17", "title": "Temporaldifference search in computer Go", "journal": "Machine Learning", "year": "2012", "authors": "D Silver; R Sutton; M M\u00fcller"}, {"ref_id": "b18", "title": "Improving exploration in UCT using local manifolds", "journal": "", "year": "2015", "authors": "S Srinivasan; E Talvitie; M H Bowling; C Szepesv\u00e1ri"}, {"ref_id": "b19", "title": "Better computer Go player with neural network and long-term prediction", "journal": "", "year": "2015", "authors": "Y Tian; Y Zhu"}, {"ref_id": "b20", "title": "Matching networks for one shot learning", "journal": "", "year": "2016", "authors": "O Vinyals; C Blundell; T Lillicrap; D Wierstra"}, {"ref_id": "b21", "title": "Feature hashing for large scale multitask learning", "journal": "ACM", "year": "2009", "authors": "K Weinberger; A Dasgupta; J Langford; A Smola; J Attenberg"}, {"ref_id": "b22", "title": "Memory networks", "journal": "", "year": "2015", "authors": "J Weston; S Chopra; A Bordes"}, {"ref_id": "b23", "title": "Modeling purposeful adaptive behavior with the principle of maximum causal entropy", "journal": "", "year": "2010", "authors": "B D Ziebart"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "QueryThe query operation computes a memory based approximate value given a state x \u2208 S. We first find the top M similar states in M based on the distance function d(\u2022, x). The approximated memory value is then computed byV M (x) = M i=1 w i (x)V (i) where the weights are computed according to equation (8).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: A brief illustration of M-MCTS. When a leaf state s is searched, the feature representation \u03c6(s) is generated, which is then used to query the memory based value approximationV M (s).V M (s) is used to update s and all its ancestors according to equation (9), as indicated by the red arrows in the figure.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Experimental results. Figure (a)-(c) shows the results of testing different value of M . Figure (d) shows the results of testing different size of memory. In all figures, x-axis is the number of simulations per move, y-axis means the winning rate against the baseline.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "V M (x) = M i=1 w i (x)V (i), s.t. M i=1 w i (x) = 1.", "formula_coordinates": [2.0, 72.0, 140.25, 201.2, 14.11]}, {"formula_id": "formula_1", "formula_text": "If X is \u03c3 2 -subgaussian, then P (X \u2265 ) \u2264 exp(\u2212 2 2\u03c3 2 ).", "formula_coordinates": [2.0, 72.0, 339.15, 220.5, 24.65]}, {"formula_id": "formula_2", "formula_text": "N (s) \u2190 N (s) + 1 V (s) \u2190V (s) + R \u2212V (s) N (s)", "formula_coordinates": [2.0, 126.11, 558.49, 133.81, 39.02]}, {"formula_id": "formula_3", "formula_text": "w\u2208\u2206 {w \u2022 q + \u03c4 H(w)} (1)", "formula_coordinates": [2.0, 383.2, 97.07, 156.8, 14.61]}, {"formula_id": "formula_4", "formula_text": "F \u03c4 (q) = max w\u2208\u2206 {w \u2022 q + \u03c4 H(w)} =f \u03c4 (q) \u2022 q + \u03c4 H(f \u03c4 (q))", "formula_coordinates": [2.0, 364.38, 298.37, 130.74, 29.29]}, {"formula_id": "formula_5", "formula_text": "M (x) = M i=1 w i (x)V (i),", "formula_coordinates": [2.0, 429.74, 376.14, 110.26, 14.11]}, {"formula_id": "formula_6", "formula_text": "| M i=1 w i (x)V (i) \u2212 V * (x)| \u2264 M i=1 w i (x)|V (i) \u2212 V * (x)| \u2264 M i=1 w i (x)(|V (i) \u2212 V * (i)| + |V * (i) \u2212 V * (x)|) = M i=1 w i (x)(\u03b4 i + \u03b5 i,x ) (2) Let \u03b4 M = max i\u2208Mx \u03b4 i and \u03b5 M = max i\u2208Mx \u03b5 i,x . Using the fact that M i=1 w i (x) = 1, we can further take an upper bound of (2) by M i=1 w i (x)(\u03b4 i + \u03b5 i,x ) \u2264 \u03b4 M + \u03b5 M .", "formula_coordinates": [2.0, 325.18, 560.27, 214.82, 135.18]}, {"formula_id": "formula_7", "formula_text": "n min \u2265 2\u03c3 2 \u03b1 2 x log(M/\u03b2)(3)", "formula_coordinates": [3.0, 134.21, 211.45, 158.29, 25.77]}, {"formula_id": "formula_8", "formula_text": "c i = \u03b4 i + \u03b5 i,x , 1 \u2264 i \u2264 M .", "formula_coordinates": [3.0, 72.0, 390.88, 220.5, 20.61]}, {"formula_id": "formula_9", "formula_text": "max w\u2208\u2206 {w \u2022 (\u2212c)} (4)", "formula_coordinates": [3.0, 149.89, 431.08, 142.61, 14.58]}, {"formula_id": "formula_10", "formula_text": "max w\u2208\u2206 {w \u2022 (\u2212c) + \u03c4 H(w)} (5)", "formula_coordinates": [3.0, 128.42, 542.5, 164.08, 14.58]}, {"formula_id": "formula_11", "formula_text": "F \u03c4 (\u2212c) = \u03c4 log( M i=1 e \u2212ci/\u03c4 ) by set- ting w = f \u03c4 (\u2212c). By equation (2), \u2212f \u03c4 (\u2212c) \u2022 (\u2212c) = \u2212F \u03c4 (\u2212c) + \u03c4 H(f \u03c4 (\u2212c)) \u2264 \u2212F \u03c4 (\u2212c) + \u03c4 log M . Therefore, to show Pr{(2) \u2264 \u03b4} \u2265 1 \u2212 \u03b2 for some small constant \u03b2, it suffices to show that Pr{\u2212F \u03c4 (\u2212c)+ \u03c4 log M \u2264 \u03b4} \u2265 1 \u2212 \u03b2. Theorem 2. For states x satisfying \u03b1 x = \u03b4 x \u2212\u03b5 > 0, let n = M i=1 N i . By choosing the weight w = f \u03c4 (\u2212c) = e \u2212c/\u03c4 / M i=1 e \u2212ci/\u03c4", "formula_coordinates": [3.0, 72.0, 583.67, 220.5, 110.79]}, {"formula_id": "formula_12", "formula_text": "n \u2265 2\u03c3 2 (\u03b1 x \u2212 \u03c4 log M ) 2 log(1/\u03b2) (6)", "formula_coordinates": [3.0, 365.62, 100.69, 174.38, 24.8]}, {"formula_id": "formula_13", "formula_text": "F \u03c4 (\u2212c) + \u03c4 log M \u2264 \u03b4 x ) \u2265 1 \u2212 \u03b2. Pr \u2212\u03c4 log( M i=1 exp(\u2212ci/\u03c4 )) \u2264 \u03b4x \u2212 \u03c4 log M = Pr M i=1 exp(\u2212ci/\u03c4 ) \u2265 exp(\u2212(\u03b4x \u2212 \u03c4 log M )/\u03c4 ) \u2265 Pr M i=1 exp(\u2212\u03b4i/\u03c4 ) \u2265 exp(\u2212(\u03b4x \u2212 \u03b5 \u2212 \u03c4 log M )/\u03c4 ) \u2265 Pr(\u2203 i, exp(\u03b4i/\u03c4 ) \u2264 exp((\u03b4x \u2212 \u03b5 \u2212 \u03c4 log M )/\u03c4 ) = 1 \u2212 M i=1 Pr (\u03b4i \u2265 \u03b1 \u2212 \u03c4 log M ) \u2265 1 \u2212 M i=1 exp(\u2212 (\u03b1x \u2212 \u03c4 log M ) 2 Ni 2\u03c3 2 ) = 1 \u2212 exp(\u2212 (\u03b1x \u2212 \u03c4 log M ) 2 n 2\u03c3 2 )", "formula_coordinates": [3.0, 321.37, 141.09, 209.47, 208.79]}, {"formula_id": "formula_14", "formula_text": "\u03b5 s,x \u2248 d(s, x) = \u2212 cos(\u03c6(s), \u03c6(x))(7)", "formula_coordinates": [3.0, 358.38, 656.41, 181.62, 9.65]}, {"formula_id": "formula_15", "formula_text": "E[cos(\u03c6(s), \u03c6(x))] = cos(\u03b6(s), \u03b6(x))", "formula_coordinates": [4.0, 107.11, 181.15, 150.27, 9.3]}, {"formula_id": "formula_16", "formula_text": "w i (x) = N i exp(\u2212d(i, x)/\u03c4 ) M j=1 N j exp(\u2212d(j, x)/\u03c4 )(8)", "formula_coordinates": [4.0, 108.13, 261.83, 184.37, 26.56]}, {"formula_id": "formula_17", "formula_text": "k i (x) = exp(\u2212d(i, x)/\u03c4 )/ M j=1 exp(\u2212d(j, x)/\u03c4", "formula_coordinates": [4.0, 72.0, 340.58, 220.5, 24.41]}, {"formula_id": "formula_18", "formula_text": "{N (s),V (s), N M (s),V M (s)}", "formula_coordinates": [4.0, 367.96, 463.0, 123.58, 9.65]}, {"formula_id": "formula_19", "formula_text": "imated valueV M (s h ) is computed by query(s h ). Let N M (s h ) = M j=1 k j (s h )N j , R =V M (s h ) * N M (s h ).", "formula_coordinates": [5.0, 72.0, 120.62, 220.5, 24.41]}, {"formula_id": "formula_20", "formula_text": "X \u2190 max(N M (s h )/\u03b7 |i\u2212h| , 1) N M (s i ) \u2190 N M (s i ) + X V M (s i ) \u2190V M (s i ) + R \u2212V M (s i ) * X N M (s i )(9)", "formula_coordinates": [5.0, 100.97, 171.44, 191.53, 55.96]}], "doi": ""}