{"title": "Quality Does Matter: A Detailed Look at the Quality and Utility of Web-Mined Parallel Corpora", "authors": "Surangika Ranathunga; Nisansa De Silva; Menan Velayuthan; Aloka Fernando; Charitha Rathnayake", "pub_date": "", "abstract": "We conducted a detailed analysis on the quality of web-mined corpora for two low-resource languages (making three language pairs, English-Sinhala, English-Tamil and Sinhala-Tamil). We ranked each corpus according to a similarity measure and carried out an intrinsic and extrinsic evaluation on different portions of this ranked corpus. We show that there are significant quality differences between different portions of web-mined corpora and that the quality varies across languages and datasets. We also show that, for some web-mined datasets, Neural Machine Translation (NMT) models trained with their highest-ranked 25k portion can be on par with human-curated datasets.", "sections": [{"heading": "Introduction", "text": "Despite the advances in NMT research, the availability of parallel corpora is still a deciding factor of NMT model performance. This puts low-resource languages at a clear disadvantage . Even the use of Pre-trained Language Models (PLMs) is not quite enough to overcome the impact of data scarcity (Lee et al., 2022).\nPublicly available web-mined parallel corpora (bitext) such as CCMatrix (Schwenk et al., 2021b), CCAlign , WikiMatrix (Schwenk et al., 2021a), NLLB (Team et al., 2022), and ParaCrawl (Ba\u00f1\u00f3n et al., 2020) bring a glimmer of hope against this data scarcity problem. Compared to human-curated datasets, these are larger in quantity and contain data for hundreds of languages, including several low-resource languages. There are further initiatives to mine bitext for yet more languages as well (Bapna et al., 2022).\nHowever, Kreutzer et al. (2022) analysed a sample of 100 sentence pairs from some of these corpora and showed that these web-mined corpora have serious quality issues, especially for lowresource languages. Lee et al. (2022) noticed a drop in NMT results when a model was trained using a random 100k sample of CCAlign. Khayrallah and Koehn (2018) injected different noise types found in web-mined corpora (by analysing a random sample) into a clean parallel corpus and showed that it has a debilitating impact on NMT performance.\nThese findings paint a grim picture of the utility of web-mined corpora. However, they all considered a random sample of these corpora to determine their quality. This implicitly assumes that the quality is consistent throughout the corpus.\nIn this research, we show that analysing a random sample of such large web-mined corpora can be misleading. We selected parallel corpora for two low-resource languages Sinhala and Tamil, which made three language pairs pairs: English-Sinhala (En-Si), English-Tamil (En-Ta) and Sinhala-Tamil (Si-Ta). Instead of quality checking a very small random sample of a web-mined corpus as done by Kreutzer et al. (2022), we ranked the sentence pairs by means of a similarity measure and extracted top 25k, bottom 25k and a random 25k portions of each corpus.\nWe improved the error taxonomy of Kreutzer et al. (2022) and carried out a human (intrinsic) evaluation on a random sample of 250 from each of these portions. Our results show that there are significant quality differences between the three portions, and the quality of the top 25k portion is much better than the other portions. We also noted major variations of quality across web-mined corpora belonging to different language pairs. We then carried out an extrinsic evaluation. We separately trained NMT systems by using these top, bottom, as well as the random 25k samples of the corpora and tested them with two different evaluation sets. These results also showed that NMT models trained with the top 25k portion are significantly better. NMT models trained with the full version of some of these corpora were even lagging behind models trained with their top 25k portion. The NMT model trained with the top 25k portion of the En-Si and En-Ta parts of the NLLB corpus performed even better than a model trained with a human-curated corpus.\nWe then fixed the translation issues in the top 25k of the NLLB corpus using human translators. The time taken to clean the corpus was slightly less than the time taken to translate the corpus from scratch. Although an NMT model trained with this cleaned corpus outperformed the uncleaned corpus, the resultant meagre gains cannot be justified when considering the time and money spent on the translators.\nIn summary, our results caution the researchers not to haphazardly use the web-mined corpora with just random sampling. Simply ranking a web-mined corpus first and then using only the high-quality portion would result in better accuracy in much less training time. We also hope other researchers (especially those working on lowresource languages) would carry out similar analyses for datasets of their languages. This will help future researchers make informed decisions when selecting web-mined corpora for NMT research.", "publication_ref": ["b28", "b38", "b37", "b3", "b28", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Web-mined parallel corpora are gathered from any available website without guarantees about quality. Khayrallah and Koehn (2018); Lee et al. (2022) pointed out that NMT systems built with such webmined corpora have performance issues.\nThe common way to determine the quality of a parallel corpus is by analysing the performance of a Machine Translation system trained with that corpus (Khayrallah and Koehn, 2018;Schwenk et al., 2021a;. However, this does not indicate the types of noise in the corpus.\nHuman evaluation of the quality of parallel sentences (let them be web-mined, machine-generated, or human-generated) requires some criteria for the evaluators to make a judgement. Bojar et al. (2016) introduced the Direct Assessment criteria, where each sentence pair is ranked on a 0-100 scale. However, such a numerical scale does not shed light on the different types of noise in web-mined corpora. Khayrallah and Koehn (2018) analysed a webmined corpus and introduced the first categorisation of noise. The categories are: misaligned sentences, mis-ordered words, wrong language, untranslated sentences, and short segments. Herold et al. (2022)   In contrast to the above categorisations, Kreutzer et al. (2022)'s taxonomy has labels for both correct and erroneous sentence pairs: 1.) Correct translation -natural sentence, 2.) Correct translation but Boilerplate or low quality, 3.) Correct translationshort, 4.) Incorrect translation but both correct languages, 5.) Source OR target wrong language but both still linguistic content, and 6.) Not a language. Kreutzer et al. (2022) conducted a human evaluation using their taxonomy for three web-mined corpora (CCAligned, ParaCrawl v7.1, WikiMatrix) and covered data from both high and low resource languages. de Gibert Bonet et al. (2022) used that taxonomy to evaluate English-Catalan corpora.", "publication_ref": ["b23", "b28", "b23", "b37", "b6", "b23", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Languages", "text": "We selected three language pairs: English-Sinhala (En-Si), English-Tamil (En-Ta) and Sinhala-Tamil (Si-Ta). Tamil (Ta) and Sinhala (Si) are large institutional languages (Eberhard et al., 2021). However, considering their data availability, Joshi et al. (2020) categorised Tamil as a mid-resource language and Sinhala as an extremely low-resource language. In the more recent language categorization by Ranathunga and de Silva (2022), Sinhala has moved one class up, and the position of Tamil is unchanged. Sinhala, in particular, is contained only in the island nation of Sri Lanka, and has only seen slow progress in language technologies (Ranathunga and de Silva, 2022;de Silva, 2023). But, being a multilingual country, translation systems are of utmost importance to Sri Lanka. This is particularly true for Si-Ta, as most government documents are first prepared in Sinhala and then translated to Tamil and English (Farhath et al., 2018).", "publication_ref": ["b9", "b20", "b35", "b35", "b8", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Web-mined Parallel Corpora", "text": "Table 1 lists the web-mined corpora that we considered for evaluation. Other web-mined corpora available in OPUS (Tiedemann, 2012) were omitted because they did not have at least 100k samples for at least two of the language pairs we considered. Out  ", "publication_ref": ["b43"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Quality Estimation by Humans", "text": "As mentioned earlier, Kreutzer et al. (2022) carried out the first human evaluation on the quality of web-mined corpora. Although they reported results for a large number of languages including low-resource languages, their discussion was mainly centred around the language-wise aggregated results. Thus they only used randomly selected 100 sentences from each language-specific corpus. de Gibert Bonet et al. (2022) carried out a similar study for Catalan-English, but they also considered only 100 samples from a corpus. In contrast, we carried out a more detailed analysis of web-mined corpora belonging to the three language pairs by first ordering each parallel corpus according to the quality of the sentence pair. Our hypothesis is that the quality of a web-mined corpus is not consistent across a dataset, thus analysing a random portion of the corpus would not give a clear picture of the quality of the corpus.   Herold et al. (2022) Participants: Fifteen translators were employed to conduct the human evaluation across the three language pairs. Evaluator selection and training details are in Appendix B.\nSample Selection: Calculating a similarity measure over the source and target sentence embeddings is a popular method to get an indication of the quality of a parallel sentence pair . We picked LASER-3 (Heffernan et al., 2022) as our apparatus to score the alignment between the bitext. Heffernan et al. (2022)    better than LaBSE 1 , the other commonly used multilingual sentence encoder. Sentences in each corpus were ordered by the LASER-3 score. For the NLLB corpus, we used the LASER-3 scores that were already provided within the dataset. For other datasets, we calculated this score 2 . From this sorted corpus, we randomly selected 250 sentences each from the top 25k split, the bottom 25k split, as well as from the entire corpus. There was no overlap between the sentences selected from the random set and the top/bottom sets. Once again, be reminded that Kreutzer et al. (2022) used only 100 random sentences from the entire corpus.\nTaxonomy: Our error taxonomy shown in Table 2 is based on Kreutzer et al. (2022) and Herold et al. (2022). Unlike Kreutzer et al. (2022), we manually cleaned a web-mined corpus to determine its effect on NMT performance (see Section 9). Therefore our taxonomy indicates the level of human effort needed to fix the translation of a pair of sentences. We believe this provides more guidance to humans conducting quality evaluations of the corpora. Compared to Kreutzer et al. (2022), our taxonomy has two other differences: (1) We used WL to denote when the source or target is in some third language, and UN to denote when source or target has been copied to the other side. In contrast, Kreutzer et al. (2022) used WL to denote both of these scenarios. (2) Kreutzer et al. (2022) used CC to denote both perfect and near-perfect translations. In contrast, we used CC only for perfect translations and introduced CN for a near-perfect translation. While a bitext mining system may not be able to distinguish between CC and CN, this difference is important when manually cleaning the corpus.\nComparison of our taxonomy against Herold et al. ( 2022) is given in Table 3. Since they only focused on identifying errors, they do not have any category related to correct translation pairs. Herold et al. (2022) used their error categories to introduce synthetic errors to a clean corpus. Therefore they could easily generate data that corresponds to Mis-ordered Words (src|trg), Raw Crawled Data, Over/Under translation and Synthetic Translations. However, such errors are not directly distinguishable by a human. On the other hand, a sentence pair with at least one of these errors requires significant human effort to get cleaned. Therefore we grouped those categories as CB.", "publication_ref": ["b18", "b17", "b17", "b18", "b18"], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Human Evaluation Results", "text": "Each sentence pair was evaluated by three evaluators. The average agreement (measured in Pearson correlation) per language pair is as follows: En-Si 0.40, En-Ta 0.55 and Si-Ta 0.57 (Detailed results are in Table 9 of Appendix B). Results in Table 4 confirm 3 important points:\n1. The quality of a web-mined corpus is not consistent throughout. We see drastic differences in quality between the top 25k and the bottom 25k. For example, the top 250 samples of the En-Si WikiMatrix corpus have 34.3% sentences falling into CC+CN categories, while its bottom portion has only 0.4% in the same categories. Kreutzer et al. (2022) portrays a high amount of quality issues. For WikiMatrix, CCMatrix, and NLLB, random sampling gives results that are closer to the bottom than the top. CCAligned defies this trend strongly in En-Si and weakly in En-Ta.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_17", "tab_7"]}, {"heading": "Carrying out a human evaluation on a random sample as done by", "text": "3. Quality of corpora can vary significantly depending on the language pair. For example, CCMatrix En-Si top 25k has 46.7% of CC+CN categories, and the same for En-Ta is 14.8%.\nTogether, these observations warn us against haphazardly using these web-mined corpora without studying their quality distribution. The result for non-English-centric Si-Ta is of particular interest. For Si-Ta, both NLLB and CCMatrix top portion seem to be extremely good. In fact, the 97.9% total value for the Correct (C) group is the highest across all the results. Kreutzer et al. (2022) did not consider En-Ta or Si-Ta in their evaluation. Even for En-Si, only the ParaCrawl v7.1 corpus was considered. Therefore we cannot draw a direct comparison with their results. However, we can compare their microaveraged results with our results for the random split, for the same corpora. For the CCAligned corpus, our random split results for both En-Si and En-Ta are significantly higher than Kreutzer et al. (2022)'s. In contrast, the same for WikiMatrix is lower than Kreutzer et al. (2022) by 10.24 and 18.34 (respectively). Even though Kreutzer et al. (2022) reported that 7.3% of the languages they analyzed did not contain a single correct sentence, we observed a similar phenomenon only with the bottom 25k split of WikiMatrix. These observations further justify the need for language-specific detailed analysis of web-mined corpora.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Qualitative Analysis of Corpora", "text": "In addition to the human evaluation discussed in the previous section, we also carried out a manual inspection of the top 25k portion of each corpus.\nen \" What makes you think that it will be the truth, or even accurate?\" en: As it is the confluence of two great oceans, sea turbulence, will toss the ship left and right, fore and back.\nen Is that evidence that he is God ?\nsi \u0dd9\u0db8\u0dca\u0dc0\u0da7 \u0d9a\u0dd2\u0dba\u0db1\u0dca\u0dd9\u0db1\u0dca \u0dd9\u0daf\u0dba\u0dd2\u0dd9\u0dba\u0dcf\u0dca \u0dc3\u0dcf\u0d9a\u0dca\u0d9a\u0dd2 \u0d9a\u0dd2\u0dba\u0dbd\u0daf?\nen: Are these told as gods are the witnesses?\nen \"Yes,\" they said , \" you are not a person whom we doubt.\"\nta \" \" \u0b85\u0bb5\u0bb0\u0bcd\u0b95\u0bb3\u0bcd \u0bc6\u0b9a\u0bbe\u0bb2\u0bcd\u0bb5\u0bbe\u0bb0\u0bcd\u0b95\u0bb3\u0bcd \u0b83 \" \u0ba8\u0bc0 \u0b8e\u0b99\u0bcd\u0b95\u0bb3\u0bcd \u0ba4\u0b99\u0bcd\u0b95 \u0bae\u0b95\u0ba9\u0bb2\u0bcd\u0bb2\u0bb5\u0bbe! en: \" \"They will say, \"Aren't you our golden son!  tences that are structurally and semantically similar but not parallel, presented as pairs. Table 5 shows some such interesting examples from NLLB (An extended version is in Appendix E as Table 14). We show instances where text from the Bible has been aligned with Buddhist scripture as well as instances where simple negation and noun matching have resulted in faulty alignments. Kreutzer et al. (2022) noted that such misaligned data may cause trained models to hallucinate fake facts.\nFurther, we observed some qualitative issues in the top 25k splits that are idiocentric to each dataset (or at least more prevalent in a particular dataset than others). CCMatrix has many untranslated/partially translated pairs. WikiMatrix, on the other hand, has many partial sentences. CCAligned has many concatenated lists coming from product advertisements (e.g., cameras, dongles, cables). Further, this dataset also has a comparatively higher amount of short entries. In general, NLLB was free of the above faults. However, as touched on in Table 5, the top pairs of NLLB are predominantly religious text with many misalignments. Informa-0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1. 1 1.2 1.3 1.4 1.5 1 tion in some of the aligned NLLB sentences was not balanced (i.e. one side has more information).\nThe fact that NLLB has more religious text is worth noting because (1) NLLB is presented as a general domain dataset and not one in the religious domain (2) The phrasing and language used in these religious texts are more archaic than modern. Thus a model trained on the top 25k of NLLB might have a domain bias toward religious text and be unable to handle contemporary language.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_9", "tab_1", "tab_9", "tab_1"]}, {"heading": "Impact of Corpus Quality on NMT Model Performance", "text": "In Section 6, it was evident that different splits of a large web-mined corpus have different levels of quality. To determine whether this quality difference has any impact when it is used to train NMT models, we ran a series of experiments.\nDataset: For each corpus, we trained separate NMT models from the top, bottom, and random 25k portions of each of the web-mined En-Si corpora. We used two separate datasets for testing: FLORES-101 (Goyal et al., 2022), and the test set of the SITA parallel corpus (Fernando et al., 2020). FLORES was created from Wikipedia articles, and SITA from government documents of Sri Lanka. showed that NMT models built on mBART (Tang et al., 2021) outperformed those built on vanilla Transformer models. NMT-specific models such as M2M (Fan et al., 2021) and NLLB (Team et al., 2022) (henceforth referred to as NLLBm, to distinguish from the NLLB dataset) have been shown to be generally better for low-resource languages (Zhu et al., 2023). However, these models have not been tested for the considered languages. Despite their performance, there is a possibility that the datasets considered in our experiments have already been included in these models (Jacovi et al., 2023). Thus, we trained vanilla Transformer NMT models with all data splits, and ran an ablation study with CC-Matrix En-Si for NMT models trained on mBART, NLLB and M2M 3 . Model and training details are in Appendix D.\nResults were recorded in chrF (Popovi\u0107, 2015), chrF++ (Popovi\u0107, 2017), BLEU (Papineni et al., 2002) and spBLEU (Goyal et al., 2022). chrF++ results are used in our discussion. All results are in Appendix F.\nResults in Figure 1 (Raw result in Table 15 in Appendix F) confirm the observations we derived from human evaluation -the top 25k split is significantly better than the other splits. With respect to the SITA test set, the performance ordering of the corpora also tallies with human evaluation results for the Correct (C) category: CCAligned is the best, followed by CCMatrix, WikiMatrix, and NLLB. For FLORES test, CCMatrix is the best, followed by CCAligned, WikiMatrix, and NLLB. Interestingly, despite being created from Wikipedia, Wiki-Matrix could not beat CCAligned or CCMatrix for FLORES, which was also created from Wikipedia. The lowest result from NLLB could be due to its quality issues, as well as its religious content (see Section 7). Except in CCAligned, both bottom and random splits show roughly similar performance. The high result for the random split in CCAligned correlates with the higher value reported for the C category during human evaluation.\nFigure 2 (Raw result in Table 16 in Appendix F) shows how NMT systems built with different pretrained models perform on CCMatrix En-Si data splits. Overall, NMT models built on top of NLLBm show the best performance, followed by mBART and M2M-based models. Despite modelwise differences, these results reaffirm that NMT models trained with different splits of the same corpus have different levels of performance. This difference is least pronounced in the NLLBm model. Even in mBART and M2M models, the results gap between top and random splits is minimal, compared to the vanilla transformer model. This confirms that NMT systems built on pre-trained models are more robust to noise in parallel corpora.\nThese findings naturally lead to the question 'what would happen to the NMT performance if the dataset size is gradually increased beyond 25k?'. To answer this question, we trained vanilla transformer-based NMT models, by gradually increasing the size of the CCMatrix En-Si corpus up to 1.6M 4 . Figure 3 shows the results (Raw results are in Table 17 in Appendix F). Despite fluctuations, when the training dataset size increases, the results gradually decrease. Also note that for this corpus, the peak result is achieved when the training set is 200k. This number may vary from corpus to corpus 5 .\nWe also trained vanilla Transformer models from the full CCMatrix, CCAligned and WikiMatrix for En-Si. Corresponding chrF++ results are 17.8, 41.7 and 17.3 (respectively) for SITA and 20.4, 31.7 and 19 (respectively) for FLORES. Comparing these values with those in Figure 2 shows that for some corpora, training an NMT model just with the top 25k split is better than using the full corpus.\n9 Impact of Corpus Cleaning", "publication_ref": ["b16", "b14", "b12", "b46", "b19", "b33", "b34", "b32", "b16"], "figure_ref": ["fig_5", "fig_1", "fig_2", "fig_1"], "table_ref": ["tab_1", "tab_1", "tab_1"]}, {"heading": "Process and Human Evaluation", "text": "Creating high-quality corpora is a challenging task, especially for low-resource languages. In this context, employing human translators to clean webmined corpora can be considered an alternative to creating parallel corpora from scratch.\nIn order to determine the benefit of corpus cleaning, we cleaned the top 25k of NLLB En-Si and En-Ta corpora. 11 En-Si translators and 16 En-Ta translators were used for this task 6 . Details of translator selection and training are shown in Table 10 of Appendix B. The translators were asked to first indicate the decision they took on a given sentence pair. The set of decisions and subsequent actions expected by the translator are given in Table 6.\nDue to rewrites and deletes, the resulting corpus now has a final cleaned sentence pair count of 27,813 for En-Si and 26,526 for En-Ta. Table 7 shows the statistics of decisions taken by the translators. We see a significant number of updates, which confirms that the original corpus had more pairs falling into the C category. The lesser, but significant rewrites and very low count of deletes confirm that the E category was relatively small.\nRecall that we conducted a human evaluation of 250 random samples from this portion of the NLLB corpus for both En-Si and En-Ta corpora. Each of these 250 samples was cleaned by three separate translators, while each sentence in the rest of the corpus was cleaned by a single translator. The 250 sentences of the top 25k portion of NLLB Si-Ta corpus that were used for quality estimation were also cleaned by three translators. The last three rows in Table 4 show this result. We see a significant drop in error (E) categories and a significant increase in CC category. However, human data cleaning has not produced a perfect result -had it been perfect, we should have seen 100% for CC+CS categories.\nWe manually reviewed the cleaned En-Si translation pairs that did not fall into CC or CS categories to identify why they were not cleaned to be perfect translation pairs. Our observations are as follows:\n\u2022 NLLB has a high concentration of religious text. Jargon and structure used in the religious text are very different to contemporary vernacular. Some translators found it difficult to find equivalent wording in Sinhala for the religious-specific language.\n\u2022 Some English sentences had structural issues. Some translators have not bothered to fix these structural issues and have simply translated that ill-formed English sentence into Sinhala.\n\u2022 In the cases where the English sentence is partial (e.g. interrupted utterance), translating it to Sinhala was difficult due to differences in grammatical word ordering.\n\u2022 Some English sentences that discuss ideas that are rooted in Western culture had no concise way of translating (e.g. I am taking her on a date).\n\u2022 Spelling errors 7 , errors caused due to overlooking punctuation errors.   To prepare a sample of a hundred sentence pairs, an average duration of 3hrs 3 minutes with a standard deviation of 1hr and 9 minutes was taken. Cleaning of 25k En-Ta sample produced 26,801 sentences consuming a total duration of 539:52 (hr:m). On average per sentence, the duration spent was 1.2 minutes. The average duration spent for a hundred sentences was 3hrs 47minutes with a standard deviation of 3hrs 57minutes. In both instances, the standard deviation is noticeably high. This is due to the individual capabilities/circumstances of translators or even a translator wrongly recording time(see Appendix C), it could also be due to the quality of the dataset portion received by a translator and the translator's judgment on the action to be carried out on a given sentence pair. This assumption is strengthened by results in Table 11 in Appendix C -there is a high variance in the actions selected by the translators.\nTo see if cleaning a web-mined corpus is more effective than translating a source from scratch, we selected three translators from the corpus cleaning task, gave them 100 sentences from NLLB En-Si corpus (that they had not seen before), and asked them to translate from scratch. We compared the time they took for the fresh translation and corpus cleaning.\nAs per Table 13 in Appendix C, corpus cleaning on average took 14 minutes less than fresh translation. However, the time taken to clean a corpus may vary depending on its quality. For the sake of completion, the freshly translated 100 sentences were evaluated by evaluators. CC, CN, and CB percentages are 57.00%, 10.67% and 32.33% respectively, which are on par with corpus cleaning results.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1", "tab_11", "tab_12", "tab_7", "tab_1", "tab_1"]}, {"heading": "Impact on NMT Performance", "text": "For both En-Si and En-Ta, we built NMT models from the fully cleaned NLLB corpus, its top 25k, as well as from the random and the top 25k splits of SITA corpus. Figures 4 and 5 show the respective results. Raw results tables are in Table 18 in Appendix F 8 . For both language pairs, cleaned NLLB top 25k corpus beats the uncleaned version for SITA and FLORES test sets. But, compared to human effort to clean the corpus, this gain cannot be justified. As per Figures 4 and 5, the top 25k split of both web-mined corpora performed better than SITA top 25k split for FLORES test set. Nayak et al. (2023) showed that NMT results could be affected by domain divergence. To determine whether this drop in SITA result is due to domain divergence, we calculated JS Divergence between different corpora (see Table 20 in Appendix G). The divergence between SITA and FLORES is the highest, but it is only slightly higher (0.1 points) than that of CCAligned. However, CCAligned result for FLORES is 2.3 chrF++ points higher than SITA. Therefore it is safe to assume the low performance of SITA may not be due to domain divergence, but due to its quality. However, the high domain divergence between NLLB and SITA is noteworthy. We remind the reader that we noticed NLLB having higher amounts of religious content (see Section 7).\nThe full NLLB cleaned En-Si corpus of 27k+ lags behind the top 25k split. Similarly, for both language pairs, SITA random 25k lags behind the top 25k.", "publication_ref": ["b30"], "figure_ref": [], "table_ref": ["tab_1", "tab_3"]}, {"heading": "Impact of Embedding Technique", "text": "We used LASER-3 for ranking sentence pairs. The other commonly used measurements are LaBSE and XLM-R (Conneau et al., 2020). mBERT is another option, however, Sinhala is not included in this model. In fact, Fernando et al. (2023) compared LASER-1, LaBSE, and XLM-R embedding performance for sentence alignment and reported that LaBSE is superior to the other two. To determine whether the embedding technique has a noticeable impact, we ranked the CCMatrix En-Si corpus using LaBSE and XLM-R. Then we selected the top, bottom, and random splits from this corpus and trained vanilla transformer-based NMT models. Figure 6 shows the comparison (Full result in Table 19 of Appendix F). XLM-R result is close to zero, so is not shown. Overall, the top 25k ranked by LASER-3 has a higher result than the other two. However, note that for a given language pair, the actual result may depend on the language representation in the model and the characteristics of the corpus. Thus, for a new pair of languages, it is worthwhile to experiment with different embedding models.", "publication_ref": ["b7", "b15"], "figure_ref": ["fig_4"], "table_ref": ["tab_1"]}, {"heading": "Conclusion", "text": "We presented a fine-grained evaluation of the quality of web-mined corpora for three low-resource language pairs. We showed that the quality of such corpora significantly varies across different portions. Our findings also indicate that simply using the highest quality portion of a web-mined corpus yields NMT results that may be on par with humancurated corpora in some instances. However, we are wary of further cleaning this top portion in hopes of better results, as the result gains do not justify the required human effort. Project artefacts are released and the details are shared in the project GitHub 9 .\nFor our analysis, we considered the web-mined corpora without any pre-processing. If they were pre-processed (say) to remove duplicates, short phrases, or text in the wrong language, the performance of the embedding techniques may vary. We plan to investigate this in future. We also plan to expand this analysis to other low-resource languages.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Our evaluation involves only three languages. This was inevitable because these are the only languages we had provisions to find human translators to carry out a meaningful evaluation. Due to financial constraints, we could carry out data cleaning only for the En-Si and En-Ta portions of the NLLB corpus. For the NLLB cleaning task, we reviewed only the first 100 sentences produced by the human translators. Therefore this corpus could still have some noise. From each corpus, we reviewed only 750 sentences. While this number is much larger than what Kreutzer et al. (2022) considered, it may still not be representative enough. Due to computing resource constraints, we could not train NMT models with all pre-trained models or train NMT models for various sizes of all parallel corpora. Our technique works only for languages included in embedding models such as LASER, LaBSE, and XLM-R.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "We used publicly available parallel corpora that are free to use. Fernando et al. (2020) provided their dataset. We paid all the translators according to the government's stipulated rates. Before assigning them to the task, they were given a pilot to try out. They were given the chance to decide whether they were adequately compensated for their efforts. We only collected personal information that is needed for us to determine their suitability for the task and to arrange their payment. None of these personal details has been publicly released. More details are in the Appendix C. As mentioned under limitations, we could not manually review the corpus cleaned by translators. While they fixed the issues in a publicly available corpus, we cannot guarantee that the cleaned corpus does not have any unnecessary content that was not there in the original corpus.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "A Parallel Corpora used in the Study", "text": "All following artefacts were used consistent with their intended use when and where it was specified. The creators of the respective artefacts have checked whether their data contains any information that uniquely identifies individual people or offensive content. In the cases where the data is updated or re-written by translators as discussed in Section 8, the guideline discussed in Appendix C ensured that no information that uniquely identifies individual people or offensive content is inserted. The licences and terms of usage of the artefacts are as discussed in each of the cited sources below.\nCCAligned (El-Kishky et al., 2020) is a dataset created using 68 snapshots of CommonCrawl 10 . Document alignment was done using FastText LangID (Joulin et al., 2016(Joulin et al., , 2017) by mapping documents with the same URL but different language codes. The alignments were then refined using LASER embeddings (Artetxe and Schwenk, 2019b).\nWikiMatrix (Schwenk et al., 2021a) is a parallel corpus mined from Wikipedia. It has 135M parallel sentences in 1620 language pairs (85 languages). 34M of these are aligned with English. Duplicates have been removed after sentence splitting. FastText LangID has been used to identify the languages of the text and then LASER has been used to identify bitext.\nCCMatrix (Schwenk et al., 2021b) was created using snapshots of CommonCrawl. It contains around 4.5 billion parallel sentences across 576 language pairs. In building CCMatrix, it was assumed the aligned sentence could appear anywhere on CommonCrawl. Thus, margin-based mining (Artetxe and Schwenk, 2019a) was used for sentence alignment.\nNLLB (Team et al., 2022) was released along with a translation model of the same name. This dataset contains: (1) public primary bitext collected from various sources, (2) bitext mined with LASER-3 teacher-student training (Heffernan et al., 2022), and (3) Backtranslated bitext created from the monolingual corpus.  A flow-chart (See Figure 7) was prepared to explain the evaluation task. Then they were given a pilot set to practice the task. We evaluated their work, refined the guidelines and provided them with the final instructions along with a demonstration video. They were paid for each sentence they evaluated. Before assigning work, we informed them of the rates. Thus, based on the time taken for the pilot task, the translators were given the option to decide whether they wanted to continue with the full task under the proposed payment rates. Table 9 contains the raw data used for the Pearson correlation study.", "publication_ref": ["b21", "b22", "b2", "b37", "b38", "b1", "b17"], "figure_ref": [], "table_ref": ["tab_17"]}, {"heading": "B Human Evaluator Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Web-mined Corpus Cleaning", "text": "To clean the top 25k sentences from the NLLB corpus, translators were selected following the same procedure described in Section 5. Table 10 gives details about the human participants involved with the NLLB cleaning task.\nTranslators were issued a guideline (Figure 8) and a demonstration video. The authors reviewed the first 100 sentence pairs cleaned by the translators. Then an Extended Guidelines document was created to cover the common mistakes made during the task and to give specific instructions on the corrective action. The translators were asked  to address the reviewer comments given for those hundred sentences. Once the reviewers were satisfied that a translator had fully understood the task, they were given the OK to continue with corpus cleaning. They were asked to record the exact time they spent on the corpus cleaning task. Translators were paid as follows: For reading and deciding on the action to be carried out on a sentence pair, a fixed amount was paid. When the translator updates or rewrites sentences, they are paid for each word they write/modify. They were informed of the rates in advance and were given the chance to opt-out of the task after participating in the pilot task. Table 11 shows the counts of decisions taken by each translator.\nIn Table 12, we have summarised the number of sentence pairs cleaned by each translator and the total time taken for it. Owing to the availability of translators, the number of sentence-pairs cleaned by each person was different. Therefore in our calculation, the average time spent by each translator to clean 100 sentence-pairs was considered. Based on these statistics, to clean a sample of hundred sentence pairs from the top 25k of the En-Si corpus, an average duration of 3hrs 3 minutes with a standard deviation of 1hr and 9 minutes was taken. For En-Ta the average duration was 3hrs 37minutes with a standard deviation of 3hrs 57minutes. We contacted the translators to confirm the times they reported. The three translators who have taken the longest time revealed that they have been recovering from illness/accident, therefore the work had been slow.\nTo compare these durations to what was taken for translating from scratch, we then asked three translators to provide fresh Si translations for a hundred En sentences and to record the time taken.\nThen we calculated and compared the average time taken along with the standard deviation with the values obtained for the corpus cleaning duration. This information is available in Table 13. The difference between the averages comes to 14 minutes, which means as the number of sentences increases, the time taken for the cleaning task will further be increased.", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": ["tab_1", "tab_1", "tab_1", "tab_1"]}, {"heading": "D Model Details", "text": "As discussed under Section 8, experiments were performed using three state-of-the-art (SOTA) NMT models (NLLB, mBART, and M2M) and vanilla transformer. To perform a fair evaluation between the SOTA NMT models, we chose model variants with similar model sizes. The model sizes utilized in our experiments for NLLB, mBART, and M2M are approximately 600 Million (M), 600M, and 418M, respectively. We perform bilingual finetuning on the SOTA models by training up to 3 epochs with a learning rate of 5 \u00d7 10 \u22125 , maximum token length of 200 was set for both source and target. A batch-size of 10 was used for finetuning. We utilized the implementations provided by the HuggingFace Transformer library (Wolf et al., 2020), and Nvidia Quadro RTX 6000 for hardware-level parallelism. For the decoding process, default settings provided by HuggingFace were retained for each model. In the case of mBART and M2M, a beam search with a beam size of 5 was employed, while for NLLB, a beam size of 4 was utilized.\nVanilla-transformer: We train the Transformer models implemented in FAIRSEQ library (Ott et al., 2019) for our experiments. We train a model consisting of 6 encoder and decoder layers, encoder and decoder embedding of 256, 2 attention heads, dropout of 0.4, the learning rate of 1\u00d710 \u22127 , weight decay of 1 \u00d7 10 \u22124 and a batch-size of 32. For decoding, we use beam search with a beam size of 5.", "publication_ref": ["b44", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "E Extended Misalignment Analysis", "text": "Table 14 is an extended version of Table 5. As mentioned in the discussion in Section 7, there are some interesting observations where text from the Bible has been aligned with Buddhist scripture. There were indeed multiple occurrences of Bible being aligned with the Quran. However, unlike in the case of Bible-Buddhist pairings, we are not showing Bible-Quran pairings here given that both of     , 2018), they do share some information and it is reasonable for even a human evaluator to align some of these scripture by mistake. In the last En-Si example, it is also evident that the sentence structure arising from the use of parentheses has played a part in aligning the wrong sentences. In row number 12 of the extended version En-Ta, another interesting observation is that the punctuation count (specifically the quotation marks) has also been a contributor to the misalignment.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1", "tab_9"]}, {"heading": "F NMT Results", "text": "As discussed in Section 8 we report the Chrf++ as our primary evaluation metric. Apart from this we also calculate the Chrf, BLEU, and spBLEU scores as well. Since HuggingFace library doesn't support spBLEU score, we are only able to report spBLEU for vanilla-transformer. Tables 15, 16 ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "G Domain Divergence Evaluation", "text": "We calculate the Jensen Shannon Divergence (JSdiv) (Lu et al., 2020)  In the case of (a),there have to be two rows now, and the decision should be selected as \" Re-write \" in both rows . You can either insert a new row and copy-paste the row above it entirely (Figure 1.1) OR insert a new row and copy only the Si or En sentence as needed (Figure 1.2). In situations as shown in Figure 1.3 below, it is NOT essential to include numbers (sequence numbers/citations, etc) or punctuations that are not relevant to the sentence. Eg: [11] and \" can be removed The preferred updated sentences are shown in Figure 1.4. Note that both sides are updated.   en: Death will also disappear like a fruit behind the leaf.\nen and fasten them into the back of the dot.\nsi \u0dad\u0dc0\u0daf \u0d85\u0db4\u0dd2 \u0d94\u0dc0\u0dd4\u0db1\u0dca\u0dc0 \u0d9d\u0db1 \u0dd9\u0dc3\u0dc0\u0dd9\u0dab\u0dc4\u0dd2 \u0d87\u0dad\u0dd4\u0dc5\u0dd4 \u0d9a\u0dbb\u0db1\u0dca\u0dd9\u0db1\u0db8\u0dd4 .\nen: And we are admitting them in the dense shade.\nen \"And we do not reveal the signs except to strike fear..\" en: Their horses do not have an otherworldly power;\nen \"Yes,\" they said , \" you are not a person whom we doubt.\"\nta \" \" \u0b85\u0bb5\u0bb0\u0bcd\u0b95\u0bb3\u0bcd \u0bc6\u0b9a\u0bbe\u0bb2\u0bcd\u0bb5\u0bbe\u0bb0\u0bcd\u0b95\u0bb3\u0bcd \u0b83 \" \u0ba8\u0bc0 \u0b8e\u0b99\u0bcd\u0b95\u0bb3\u0bcd \u0ba4\u0b99\u0bcd\u0b95 \u0bae\u0b95\u0ba9\u0bb2\u0bcd\u0bb2\u0bb5\u0bbe! en: \" \"They will say, \"Aren't you our golden son! en Thou hast given him his heart's desire, * and hast not denied him the request of his lips .     We calculate the JS-div for each of the test datasets (SITA and FLORES) against the following portions of the web-mined corpora: NLLB top 25k, WikiMatrix top 25k, CCAligned top 25k, SITA top 25k and NLLB top 25k.", "publication_ref": ["b29"], "figure_ref": ["fig_5", "fig_5", "fig_5", "fig_5"], "table_ref": []}, {"heading": "Acknowledgements", "text": "This work was funded by the Google Award for Inclusion Research (AIR) 2022 received by Surangika Ranathunga and Nisansa de Silva.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "by (Nayak et al., 2023). The results of the JS-div can be found in Table 20.\nJS-div calculation can be described as follows. It is calculated between two distributions P and Q using the formula shown in Equation 1, where M is an equally weighted sum of M = 1 2 P + 1 2 Q and KL(\u2022||\u2022) represents the Kullback-Leibler divergence (Kullback and Leibler, 1951).\nJSD(P ||Q) = 1 2 KL(P ||M ) + 1 2 KL(Q||M )\n(1) JS-div ranges from 0 to 1 with lower values indicating that the two distributions are more similar.", "publication_ref": ["b30", "b27"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Exploring religious tourist experiences in jerusalem: The intersection of abrahamic religions", "journal": "Tourism Management", "year": "2018", "authors": "Ram Tahir Albayrak; Meltem Herstein; Netanel Caber; M\u00fcjde Drori; Ron Bideci;  Berger"}, {"ref_id": "b1", "title": "Marginbased parallel corpus mining with multilingual sentence embeddings", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Mikel Artetxe; Holger Schwenk"}, {"ref_id": "b2", "title": "Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond", "journal": "Transactions of the Association for Computational Linguistics", "year": "2019", "authors": "Mikel Artetxe; Holger Schwenk"}, {"ref_id": "b3", "title": "ParaCrawl: Web-scale acquisition of parallel corpora", "journal": "", "year": "2020", "authors": "Marta Ba\u00f1\u00f3n; Pinzhen Chen; Barry Haddow; Kenneth Heafield; Hieu Hoang; Miquel Espl\u00e0-Gomis; Mikel L Forcada; Amir Kamran; Faheem Kirefu; Philipp Koehn; Sergio Ortiz Rojas; Leopoldo Pla Sempere; Gema Ram\u00edrez-S\u00e1nchez"}, {"ref_id": "b4", "title": "Annual Meeting of the Association for Computational Linguistics", "journal": "Online. Association for Computational Linguistics", "year": "", "authors": ""}, {"ref_id": "b5", "title": "Wolfgang Macherey, et al. 2022. Building machine translation systems for the next thousand languages", "journal": "", "year": "", "authors": "Ankur Bapna; Isaac Caswell; Julia Kreutzer; Orhan Firat; Aditya Daan Van Esch; Mengmeng Siddhant; Pallavi Niu; Xavier Baljekar;  Garcia"}, {"ref_id": "b6", "title": "Findings of the 2016 conference on machine translation", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Ond\u0159ej Bojar; Rajen Chatterjee; Christian Federmann; Yvette Graham; Barry Haddow; Matthias Huck; Antonio Jimeno Yepes; Philipp Koehn; Varvara Logacheva; Christof Monz; Matteo Negri; Aur\u00e9lie N\u00e9v\u00e9ol; Mariana Neves; Martin Popel; Matt Post; Raphael Rubino; Carolina Scarton; Lucia Specia; Marco Turchi; Karin Verspoor; Marcos Zampieri"}, {"ref_id": "b7", "title": "Unsupervised cross-lingual representation learning at scale", "journal": "", "year": "2020", "authors": "Alexis Conneau; Kartikay Khandelwal; Naman Goyal; Vishrav Chaudhary; Guillaume Wenzek; Francisco Guzm\u00e1n; Edouard Grave; Myle Ott; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b8", "title": "Survey on Publicly Available Sinhala Natural Language Processing Tools and Research", "journal": "", "year": "2023", "authors": "Silva Nisansa De"}, {"ref_id": "b9", "title": "", "journal": "Ethnologue: Languages of the World", "year": "2021", "authors": "David M Eberhard; Gary F Simons; Charles D Fennig"}, {"ref_id": "b10", "title": "CCAligned: A massive collection of cross-lingual web-document pairs", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Ahmed El-Kishky; Vishrav Chaudhary; Francisco Guzm\u00e1n; Philipp Koehn"}, {"ref_id": "b11", "title": "XLEnt: Mining a large cross-lingual entity dataset with lexical-semantic-phonetic word alignment", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Ahmed El-Kishky; Adithya Renduchintala; James Cross; Francisco Guzm\u00e1n; Philipp Koehn"}, {"ref_id": "b12", "title": "Beyond english-centric multilingual machine translation", "journal": "The Journal of Machine Learning Research", "year": "2021", "authors": "Angela Fan; Shruti Bhosale; Holger Schwenk; Zhiyi Ma; Ahmed El-Kishky; Siddharth Goyal; Mandeep Baines; Onur Celebi; Guillaume Wenzek; Vishrav Chaudhary"}, {"ref_id": "b13", "title": "Integration of bilingual lists for domain-specific statistical machine translation for sinhala-tamil", "journal": "IEEE", "year": "2018", "authors": "Fathima Farhath; Surangika Ranathunga; Sanath Jayasena; Gihan Dias"}, {"ref_id": "b14", "title": "Data augmentation and terminology integration for domain-specific sinhala-englishtamil statistical machine translation", "journal": "", "year": "2020", "authors": "Aloka Fernando; Surangika Ranathunga; Gihan Dias"}, {"ref_id": "b15", "title": "Exploiting bilingual lexicons to improve multilingual embedding-based document and sentence alignment for low-resource languages", "journal": "Knowledge and Information Systems", "year": "2023", "authors": "Aloka Fernando; Surangika Ranathunga; Dilan Sachintha; Lakmali Piyarathna; Charith Rajitha"}, {"ref_id": "b16", "title": "The Flores-101 evaluation benchmark for low-resource and multilingual machine translation", "journal": "Transactions of the Association for Computational Linguistics", "year": "2022", "authors": "Naman Goyal; Cynthia Gao; Vishrav Chaudhary; Peng-Jen Chen; Guillaume Wenzek; Da Ju; Sanjana Krishnan; Marc'aurelio Ranzato; Francisco Guzm\u00e1n; Angela Fan"}, {"ref_id": "b17", "title": "Bitext mining using distilled sentence representations for low-resource languages", "journal": "", "year": "2022", "authors": "Kevin Heffernan; Onur \u00c7elebi; Holger Schwenk"}, {"ref_id": "b18", "title": "Detecting various types of noise for neural machine translation", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Christian Herold; Jan Rosendahl; Joris Vanvinckenroye; Hermann Ney"}, {"ref_id": "b19", "title": "Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Alon Jacovi; Avi Caciularu; Omer Goldman; Yoav Goldberg"}, {"ref_id": "b20", "title": "The state and fate of linguistic diversity and inclusion in the NLP world", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Pratik Joshi; Sebastin Santy; Amar Budhiraja; Kalika Bali; Monojit Choudhury"}, {"ref_id": "b21", "title": "Fasttext. zip: Compressing text classification models", "journal": "", "year": "2016", "authors": "Armand Joulin; Edouard Grave; Piotr Bojanowski; Matthijs Douze; H\u00e9rve J\u00e9gou; Tomas Mikolov"}, {"ref_id": "b22", "title": "Bag of tricks for efficient text classification", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Armand Joulin; Edouard Grave; Piotr Bojanowski; Tomas Mikolov"}, {"ref_id": "b23", "title": "On the impact of various types of noise on neural machine translation", "journal": "", "year": "2018", "authors": "Huda Khayrallah; Philipp Koehn"}, {"ref_id": "b24", "title": "Findings of the WMT 2020 shared task on parallel corpus filtering and alignment", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Philipp Koehn; Vishrav Chaudhary; Ahmed El-Kishky; Naman Goyal; Peng-Jen Chen; Francisco Guzm\u00e1n"}, {"ref_id": "b25", "title": "Kelechi Ogueji, Andre Niyongabo Rubungo", "journal": "Jamshidbek Mirzakhalov", "year": "", "authors": "Julia Kreutzer; Isaac Caswell; Lisa Wang; Ahsan Wahab; Nasanbayar Daan Van Esch; Allahsera Ulzii-Orshikh; Nishant Tapo; Artem Subramani; Claytone Sokolov; Monang Sikasote; Supheakmungkol Setyawan; Sokhar Sarin; Beno\u00eet Samb; Clara Sagot; Annette Rivera; Isabel Rios; Salomey Papadimitriou; Pedro Ortiz Osei; Iroro Suarez;  Orife"}, {"ref_id": "b26", "title": "Oghenefego Ahia, Sweta Agrawal, and Mofetoluwa Adeyemi. 2022. Quality at a glance: An audit of web-crawled multilingual datasets", "journal": "Transactions of the Association for Computational Linguistics", "year": "", "authors": "Yacine Jernite; Mathias Jenny; Orhan Firat; F P Bonaventure; Sakhile Dossou;  Dlamini; Sakine \u00c7abuk Nisansa De Silva; Stella Ball\u0131; Alessia Biderman; Ahmed Battisti; Ankur Baruwa; Pallavi Bapna;  Baljekar"}, {"ref_id": "b27", "title": "On information and sufficiency. The annals of mathematical statistics", "journal": "", "year": "1951", "authors": "Solomon Kullback; A Richard;  Leibler"}, {"ref_id": "b28", "title": "Pre-trained multilingual sequence-to-sequence models: A hope for lowresource language translation?", "journal": "", "year": "2022", "authors": "Sarubi En-Shiun Lee; Shravan Thillainathan; Surangika Nayak; David Ranathunga; Ruisi Adelani; Arya Su;  Mccarthy"}, {"ref_id": "b29", "title": "Diverging divergences: Examining variants of Jensen Shannon divergence for corpus comparison tasks", "journal": "", "year": "2020", "authors": "Jinghui Lu; Maeve Henchion; Brian Mac Namee"}, {"ref_id": "b30", "title": "Leveraging auxiliary domain parallel data in intermediate task fine-tuning for low-resource translation", "journal": "", "year": "2023", "authors": "Shravan Nayak; Surangika Ranathunga; Sarubi Thillainathan; Rikki Hung; Anthony Rinaldi; Yining Wang; Jonah Mackey; Andrew Ho; En-Shiun Annie Lee"}, {"ref_id": "b31", "title": "fairseq: A fast, extensible toolkit for sequence modeling", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Myle Ott; Sergey Edunov; Alexei Baevski; Angela Fan; Sam Gross; Nathan Ng; David Grangier; Michael Auli"}, {"ref_id": "b32", "title": "Bleu: a method for automatic evaluation of machine translation", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"ref_id": "b33", "title": "chrF: character n-gram F-score for automatic MT evaluation", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Maja Popovi\u0107"}, {"ref_id": "b34", "title": "chrF++: words helping character n-grams", "journal": "", "year": "2017", "authors": "Maja Popovi\u0107"}, {"ref_id": "b35", "title": "Some languages are more equal than others: Probing deeper into the linguistic disparity in the NLP world", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Surangika Ranathunga; Silva Nisansa De"}, {"ref_id": "b36", "title": "Neural machine translation for low-resource languages: A survey", "journal": "ACM Computing Surveys", "year": "2023", "authors": "Surangika Ranathunga; Annie En-Shiun; Marjana Prifti Lee; Ravi Skenduli; Mehreen Shekhar; Rishemjit Alam;  Kaur"}, {"ref_id": "b37", "title": "Wiki-Matrix: Mining 135M parallel sentences in 1620 language pairs from Wikipedia", "journal": "", "year": "2021", "authors": "Holger Schwenk; Vishrav Chaudhary; Shuo Sun; Hongyu Gong; Francisco Guzm\u00e1n"}, {"ref_id": "b38", "title": "CCMatrix: Mining billions of high-quality parallel sentences on the web", "journal": "", "year": "2021", "authors": "Holger Schwenk; Guillaume Wenzek; Sergey Edunov; Edouard Grave; Armand Joulin; Angela Fan"}, {"ref_id": "b39", "title": "Sinhala spell correction: A novel benchmark with neural spell correction", "journal": "", "year": "2021", "authors": "Charana Sonnadara; Surangika Ranathunga; Sanath Jayasena"}, {"ref_id": "b40", "title": "Jiatao Gu, and Angela Fan. 2021. Multilingual translation from denoising pre-training", "journal": "", "year": "", "authors": "Yuqing Tang; Chau Tran; Xian Li; Peng-Jen Chen; Naman Goyal; Vishrav Chaudhary"}, {"ref_id": "b41", "title": "", "journal": "", "year": "", "authors": " Nllb Team; R Marta; James Costa-Juss\u00e0; Onur Cross; Maha \u00c7elebi; Kenneth Elbayad; Kevin Heafield; Elahe Heffernan; Janice Kalbassi; Daniel Lam; Jean Licht;  Maillard"}, {"ref_id": "b42", "title": "Fine-tuning self-supervised multilingual sequence-to-sequence models for extremely low-resource NMT", "journal": "", "year": "2021", "authors": "Surangika Sarubi Thillainathan; Sanath Ranathunga;  Jayasena"}, {"ref_id": "b43", "title": "Parallel data, tools and interfaces in OPUS", "journal": "", "year": "2012", "authors": "J\u00f6rg Tiedemann"}, {"ref_id": "b44", "title": "Transformers: State-of-the-art natural language processing", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; Remi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander Lhoest;  Rush"}, {"ref_id": "b45", "title": "Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer", "journal": "Association for Computational Linguistics", "year": "", "authors": "Linting Xue; Noah Constant; Adam Roberts; Mihir Kale; Rami Al-Rfou; Aditya Siddhant"}, {"ref_id": "b46", "title": "Multilingual machine translation with large language models: Empirical results and analysis", "journal": "", "year": "2023", "authors": "Wenhao Zhu; Hongyi Liu; Qingxiu Dong; Jingjing Xu; Lingpeng Kong; Jiajun Chen; Lei Li; Shujian Huang"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: NMT results of different models trained on CCMatrix En-Si top, bottom and average 25K splits.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: NMT results of vanilla transformer model trained on CCMatrix En-Si in jumps of 100K.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Baseline Models: For Si-Ta, En-Si, and En-Ta,Thillainathan et al. (2021);Lee et al. (2022) ", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure 6: NMT Results on CCMatrix En-Si Top, Bottom and Random 25K for LASER-3 or LaBSE.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 11Figure 1.1 -Duplicating the En and Si original sentences during Rewrite", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 1 . 3 -13Figure 1.3 -Example with punctuations/numbering that are not related to the sentence", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 8 :8Figure 8: Snapshot of the Extended Guidelines given for the translators conducting the web-mining corpus cleaning", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "extended this categorisation with three new classes: raw crawled data, over/under-translation, and synthetic translation.", "figure_data": "CCAligned WikimatrixCCMatrixXLEntNLLBEn-Si619,729115,0456,270,800 690,186 24,336,367En-Ta878,68995,1617,291,118 634,299 42,588,178Si-Ta--215,965 153,5321,493,318SourceCommon CrawlWikipedia Common CrawlFiltering LeveldocumentsentencesentenceAlignmentLASERLASERLASER LASERLASER-3"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Not a language: at least one of source and target are not linguistic content en Many Melanesian societies, however, have become hostile towards same-sex relationships since the introduction of Christianity by European missionaries.[50] si [1] en Verily, you pass by them in the morning. ta 37:137. WL: Source OR target in some other language, but both still linguistic content en \u053b \u057a\u0561\u057f\u056b\u0581 \u057a\u0561\u0580\u0578\u0576 \u0533\u0578\u056c\u057b\u056b\u056b: si \u0dd9\u0d9c\u0dcf\u0dbd\u0dca\u0d9c\u0dd2 \u0db8\u0dc4\u0dad\u0dcf \u0dc0\u0dd2\u0dc3\u0dd2\u0db1\u0dca \u0d91\u0dad\u0dd4\u0db8\u0dcf\u0dd9\u0d9c\u0dca \u0db1\u0db8\u0dd2\u0db1\u0dca \u0db8 \u0db1\u0db8\u0dca \u0dd9\u0d9a\u0dbb\u0dd2\u0dab\u0dd2. en I would probably go to Australia and I would study finance or communications. si Ben, san\u0131r\u0131m Avustralya'ya gider ve finans ya da ileti\u015fim okurdum. en God is Sufficient (feat. ta 3\u0908 \u0930 \u092a\u092f\u093e\u0930\u094d \u092a\u094d\u0924 \u0939\u0948 (\u0915\u0930\u0924\u092c) UN: Most part of the source/target has been copied to target/source en Create a new tab in an existing window rather than creating a new window si Create a new tab in an existing window rather than creating a new window en This certainly is the loss of revenue through Google AdSense.ta This certainly is the Google AdSense . Correct source and target language, but the translation is completely wrong en Several of William's children changed their surname as well.si \u0d91\u0dd9\u0dc3\u0dca \u0db8 \u0dc3\u0dd2\u0dba \u0db4\u0dd4\u0dad\u0dcf\u0dba\u0dd9\u0d9c\u0dca \u0db1\u0db8 \u0daf \u0dc0\u0dcf\u0dc1\u0dd2\u0dc2\u0dca \u0da8\u0dd3\u0db4\u0dd4\u0dad\u0dca\u0dbb \u0db4\u0dd4\u0dbd\u0db8\u0dcf\u0dc0\u0dd2 \u0dd9\u0dbd\u0dc3 \u0dd9\u0dc0\u0db1\u0dc3\u0dca \u0d9a\u0dbb\u0dba\u0dd2. en \"My lord would understand. Correct translation but boilerplate or low-quality. Requires considerable effort to derive the correct translation. en No, you're right.si \u0db1\u0dd0\u0dc4\u0dd0 \u0d94\u0dba\u0dcf \u0d87\u0dad\u0dca\u0dad \u0d9a\u0dd2\u0dba\u0db1\u0dca\u0dd9\u0db1\u0dca en It will be available for 30 days during which you can save, listen to, or share with others. Near-perfect translation (minor grammar or spelling mistakes). Requires minor effort to derive the correct translation en And in the Egyptian revolution, the Revolution 2.0, everyone has contributed something, small or big.", "figure_data": "Error (E) CodesNL: ta \u0ba8\u0bc0\u0b99\u0bcd\u0b95\u0bb3\u0bcd \u0b85\u0bb1\u0bbf\u0ba8\u0bcd\u0ba4\u0bb5\u0bc8\u0bb0 \u0b87\u0bc8\u0bb1\u0bb5\u0ba9\u0bcd \u0b8e\u0ba9\u0bcd\u0baa\u0ba4\u0ba9\u0bcd \u0b8e\u0bb3\u0bbf\u0baf \u0b85\u0bae\u0bcd\u0b9a\u0b99\u0bcd\u0b95\u0bc8\u0bb3 \u0bb5\u0bbf-\u0bb5\u0bb0\u0bbf\u0ba4\u0bcd\u0ba4\u0bbe\u0bb2\u0bcd \u0b8e\u0ba9\u0bcd \u0bc7\u0baa\u0bbe\u0ba9\u0bcd\u0bb1\u0bb5\u0bb0\u0bcd\u0b95\u0bb3\u0bc1\u0b95\u0bcd\u0b95\u0bc1 \u0bb5\u0bbf\u0bb3\u0b95\u0bcd\u0b95\u0bae\u0bcd \u0b95\u0bbf\u0bc8\u0b9f\u0b95\u0bcd\u0b95\u0bc1\u0bae\u0bcd, \u0ba4\u0b9f\u0bc1\u0bae\u0bbe\u0bb1\u0b87\u0baf\u0bb2\u0bbe\u0ba4\u0bc1.Correct (C) CodesCS: Correct translation but very short sentencesen Supported platformssi \u0dc3\u0dc4\u0dcf\u0dba \u0daf\u0d9a\u0dca\u0dc0\u0db1 \u0dd9\u0dc0\u0dca\u0daf\u0dd2\u0d9a\u0dcf\u0dc0\u0db1\u0dcaen Religion 101.ta \u0bae\u0ba4\u0bae\u0bcd 101CB: si \u0d92\u0dc0\u0dd9\u0d9c\u0dca\u0db8 \u0d8a\u0da2\u0dd2\u0db4\u0dca\u0dad\u0dd2\u0dba\u0dcf\u0db1\u0dd4 \u0dc0\u0dd2\u0db4\u0dca\u0dbd\u0dc0\u0dd9\u0dba\u0dca\u0daf\u0dd3 \u0dc0\u0dd2\u0db4\u0dca\u0dbd\u0dc0\u0dba \u0d85o\u0d9a 2.0 \u0dc3\u0dd0\u0dd9\u0dc0\u0dcf\u0db8 \u0dba\u0db8\u0dca\u0d9a\u0dd2\u0dc3\u0dd2 \u0d85\u0dba\u0dd4\u0dbb\u0d9a\u0dd2\u0db1\u0dca \u0daf\u0dcf\u0dba\u0d9a\u0dc0\u0dd6\u0dc0\u0dcfen \"50 children died yesterday.\"ta \"\u0bc7\u0ba8\u0bb1\u0bcd\u0bb1\u0bc1 50 \u0b95\u0bc1\u0bb4\u0ba8\u0bcd\u0bc8\u0ba4\u0b95\u0bb3\u0bcd \u0b87\u0bb1\u0ba8\u0bcd\u0ba4\u0bc1\u0bb5\u0bbf\u0b9f\u0bcd\u0b9f\u0ba9.CC: Perfect translation (no modification by the human is needed)en A 5-year trusteeship was discussed, and a joint Soviet-Americancommission was established."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": "examples"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "The average percentage of tag counts over 3 independent evaluators for En-Si, En-Ta, and Si-Ta for 250 samples from top, bottom and random splits. C -sum of CS, CB, CN and CC. E -sum of NL, WL, UN, and X.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "si \u0db8\u0dc4\u0dd9\u0dab\u0db1\u0dd2, \u0db1\u0dd4\u0db9\u0dbd\u0dcf \u0d9a\u0dd4\u0db8\u0d9a\u0dd0\u0dba\u0dd2 \u0dc3\u0dd2\u0dad\u0db1\u0dca\u0db1\u0dc4\u0dd4\u0daf , \u0dbb\u0dd6\u0db4\u0dba \u0db1\u0dd2\u0dad\u0dca \u0dba\u0dba \u0dd9\u0dc4\u0dcf\u0dca \u0dd9\u0dc0\u0dba\u0dd2\u0daf? en: Monks, what do you think, is form constant? en And he opens up the refrigerator, and all he sees is the bright light . ta \u0b95\u0ba4 \u0bb0\u0bb5\u0ba9\u0bcd \u0ba4\u0bbe\u0ba9\u0bcd \u0b92\u0bb3\u0bbf \u0bc8\u0baf\u0ba4\u0bcd\u0ba4\u0bb0\u0bc1\u0b95 \u0bb1\u0bbe\u0ba9\u0bcd, \u0b85\u0bc8\u0ba9\u0ba4\u0bcd\u0bc8\u0ba4\u0baf\u0bc1\u0bae\u0bcd \u0b95\u0bbe\u0ba3 \u0bc6\u0b9a\u0baf\u0b95 \u0bb1\u0bbe\u0ba9\u0bcd. The Sun is the one who gives light and makes everything visible.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Examples of parallel sentences from NLLB where the translated Si or Ta sentence has a different meaning than the original En sentences. We colourcoded the pairs of semantically close words that possibly contributed to the misalignment. Correct En translation of the Si/ Ta sentence is also given for comparison.Similar toKreutzer et al. (2022), in En-Si and En-Ta corpora, we found instances where sen-", "figure_data": "25Top 25K24.42020.1 Bottom 25K Random 25K22.421.920.9 21.120.718.617.815.715Chrf++108.910.47.86.97.4 6.97.18.07.17.754.64.85.14.60SITAFLORESSITAFLORESSITAFLORESSITAFLORESNLLBCCMatrixCCAlignedWiki MatrixDatasetsFigure 1: Vanilla-transformer performance trained on Top, Bottom and Random 25K splits of NLLB, CCMa-trix, CCAligned and WikiMatrix for En-Si (higher the better).SITAFLORESSITAFLORESSITAFLORESSITAvtNLLBmmBARTM2M"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Appendix C shows the time taken by translators for the corpus cleaning task. To produce 28,090 sentences from the noisy 25k En-Si corpus, the translators have collectively spent a total En and/or Si has to be updated (CN, CS and CB in taxonomy) UPDATE Update En and/or Si En AND Si both are either meaningless (i.e NL or WL), contain repetitive words (eg: No no no), or contain very short phrases (CS) (e.g. name of a place or a person) En should be translated to Si, and Si should be translated to En", "figure_data": "Sentence pair statusDecision Subsequent actionPerfect translation (CC) Acceptable translation, but DELETE ACCEPTKeep as it is Keep as it isEn AND Si are meaningful sentences but not related (X) Add two separate entries -Only En OR Si are meaningful (i.e. one is NL, WL, UN, REWRITE CS) REWRITE Rewrite the un-meaningful side to be the translation of the meaningful side"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Decision set employed for manual cleaning of the corpus (We remove ones marked as DELETE from the corpus before using it for NMT training.)", "figure_data": "DecisionEn-Si Total SentencesEn-Ta % Total Sentences%Accept4813 17.136621 24.70Update14852 52.8715047 56.14Re-write8148 29.014858 18.13Delete2770.992751.03Total2809026801"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "provides details about the human participants involved with the evaluation task. They all", "figure_data": "Name En -SiExperience Qualification (Years)Translator11BA in German LanguageTranslator12BA (Hons) Sinhala Sp.Translator22BA (Hons) in Translation StudiesTranslator31BA (Hons) in Translation StudiesTranslator41BA (Hons) in Translation StudiesTranslator54BA (Hons) in Translation StudiesTranslator62BA (Hons) in Translation StudiesEn -TaTranslator77BSc in AgricultureTranslator812B.Sc. Applied Mathematics and ComputingPGD in Professional Practicein EnglishTranslator95BSc (Hons) EngineeringTranslator103MBBSTranslator115BASi -TaTranslator132BA (Hons) in Translation StudiesTranslator142BA (Hons) in Translation StudiesTranslator1520Diploma in Translation and Interpretation"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Details of Translators Involved in Corpus Evaluation task", "figure_data": ""}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "Raw results used for Pearson correlation study for agreement between evaluators (Eval) on 250 samples for En-Si, En-Ta, and Si-Ta", "figure_data": ""}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "Snapshot of a translator's worksheet", "figure_data": ""}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "Translator-wise final decision counts along with their percentages for the cleaning task", "figure_data": "En-Si"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_21", "figure_caption": "Cleaning duration analysis for Translators", "figure_data": "Time taken (hh:mm) Translation of 100 sentence-pairs Cleaning of 100 sentence-pairsTranslator1803:064:19Translator2104:123:29Translator2604:253:12Total Duration Average (SD)11:43 03:54 (00:35)11:00 03:40 (0:28)"}, {"figure_label": "13", "figure_type": "table", "figure_id": "tab_22", "figure_caption": "Time spent to translate 100 En sentences from scratch and for cleaning of 100 En-Si sentence-pairs them being Abrahamic religions (Albayrak et al.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_24", "figure_caption": "There are three possible scenarios to handle a re-write. a)If the two sentences are meaningful but not related, you need to translate En to Si and Si to En (so two rewrites). b) If only En is meaningful, translate that to Si (so only one rewrite). c) If only Si is meaningful, translate that to En (so only one rewrite).", "figure_data": "Extended Guidelines1.between the training datasets(NLLB original, NLLB Cleaned top 25K, NLLBCleand Complete (27K+), SITA Top25K, andSITA Random 25K) and the test sets (SITA andFLORES). We use the code implementation used"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_25", "figure_caption": "en \" What makes you think that it will be the truth, or even accurate?\"si \u0db8\u0dc4\u0dd9\u0dab\u0db1\u0dd2, \u0db1\u0dd4\u0db9\u0dbd\u0dcf \u0d9a\u0dd4\u0db8\u0d9a\u0dd0\u0dba\u0dd2 \u0dc3\u0dd2\u0dad\u0db1\u0dca\u0db1\u0dc4\u0dd4\u0daf , \u0dbb\u0dd6\u0db4\u0dba \u0db1\u0dd2\u0dad\u0dca \u0dba\u0dba \u0dd9\u0dc4\u0dcf\u0dca \u0dd9\u0dc0\u0dba\u0dd2\u0daf? en: Monks, what do you think, is form constant? en And he opens up the refrigerator, and all he sees is the bright light . ta \u0b95\u0ba4 \u0bb0\u0bb5\u0ba9\u0bcd \u0ba4\u0bbe\u0ba9\u0bcd \u0b92\u0bb3\u0bbf \u0bc8\u0baf\u0ba4\u0bcd\u0ba4\u0bb0\u0bc1\u0b95 \u0bb1\u0bbe\u0ba9\u0bcd, \u0b85\u0bc8\u0ba9\u0ba4\u0bcd\u0bc8\u0ba4\u0baf\u0bc1\u0bae\u0bcd \u0b95\u0bbe\u0ba3 \u0bc6\u0b9a\u0baf\u0b95 \u0bb1\u0bbe\u0ba9\u0bcd. en: The Sun is the one who gives light and makes everything visible. en God is All-knowing All-aware. si \u0d85\u0db4\u0dd9\u0d9c\u0dca \u0dc1\u0dcf\u0dc3\u0dca \u0dad\u0dd8 \u0dc0\u0dd6 \u0db6\u0dd4\u0daf\u0dd4 \u0dbb\u0da2\u0dcf\u0dab\u0db1\u0dca \u0dc0\u0dc4\u0db1\u0dca\u0dd9\u0dc3\u0dca \u0dc3\u0dd2\u0dba\u0dbd\u0dca\u0dbd \u0daf\u0db1\u0dca\u0db1\u0dcf \u0dd9\u0dc3\u0dca \u0d9a . en: Our teacher the Lord Buddha is all-knowing. en The two sea caves are linked, water goes in the one on the left and comes out the one on the right ta \u0b87\u0bb0\u0ba3\u0bcd\u0b9f\u0bc1 \u0bae\u0b95\u0bbe \u0b95\u0b9f\u0bb2\u0bcd\u0b95\u0bb3\u0bcd \u0b9a\u0b99\u0bcd\u0b95\u0bae\u0bbf\u0b95\u0bcd\u0b95\u0bc1\u0bae\u0bcd \u0baa\u0b95\u0bc1\u0ba4 \u0b8e\u0ba9\u0bcd\u0baa\u0ba4\u0bbe\u0bb2\u0bcd, \u0b95\u0b9f\u0bb2\u0bcd \u0bc6\u0b95\u0bbe\u0ba8\u0bcd\u0ba4\u0bb3\u0bbf\u0baa\u0bcd\u0baa\u0bbe\u0ba9\u0ba4\u0bc1, \u0b87\u0b9f\u0ba4\u0bc1\u0bae\u0bcd \u0bb5\u0bb2\u0ba4\u0bc1\u0bae\u0bbe\u0baf\u0bcd , \u0bae\u0bc1\u0ba9\u0bcd\u0ba9\u0bc1\u0bae\u0bcd \u0baa \u0ba9\u0bcd\u0ba9\u0bc1\u0bae\u0bbe\u0baf\u0bcd \u0b95\u0baa\u0bcd\u0baa\u0bc8\u0bb2 \u0b85\u0bc8\u0bb2\u0b95\u0bcd \u0b95\u0bb4\u0bbf\u0b95\u0bcd\u0b95\u0bc1\u0bae\u0bcd. en: As it is the confluence of two great oceans, sea turbulence, will toss the ship left and right, fore and back. en \"My Lord, the fierce beasts of the two towns are coming!\" si ''\u0db8\u0dc4\u0dd9\u0dab\u0db1\u0dd2, \u0dd9\u0db8\u0dca \u0dd9\u0daf\u0dd9\u0daf\u0db1 \u0dc0\u0db1\u0dcf\u0dc4\u0dd2 \u0db6\u0dcf\u0dbd\u0dd9\u0dba\u0dcf\u0dca ( \u0d85\u0da5\u0dcf\u0db1\u0dd9\u0dba\u0dcf\u0dca ) \u0dd9\u0dc0\u0dad\u0dca.'' en: \"Monks, these two are low (ignorant).\" en \"And I brought you some water with a straw.\" ta 8 \u0ba8\u0bbe\u0ba9\u0bcd \u0b89\u0b99\u0bcd\u0b95\u0bb3\u0bc1\u0b95\u0bcd\u0b95\u0bc1\u0ba4\u0bcd \u0ba4\u0ba3\u0bcd\u0ba3\u0bc0\u0bb0\u0bbe\u0bb2\u0bcd \u0ba4 \u0bb0\u0bc1\u0bae\u0bc1\u0bb4\u0bc1\u0b95\u0bcd\u0b95\u0bc1\u0b95\u0bcd \u0bc6\u0b95\u0bbe\u0b9f\u0bc1\u0ba4\u0bcd\u0bc7\u0ba4\u0ba9\u0bcd \u0b83 \u0b85\u0bb5\u0bc7\u0bb0\u0bbe \u0b89\u0b99\u0bcd\u0b95\u0bb3\u0bc1\u0b95\u0bcd\u0b95\u0bc1\u0ba4\u0bcd \u0ba4\u0bc2\u0baf \u0b86\u0bb5 -\u0baf\u0bbe\u0bb2\u0bcd \u0ba4 \u0bb0\u0bc1\u0bae\u0bc1\u0bb4\u0bc1\u0b95\u0bcd\u0b95\u0bc1\u0b95\u0bcd \u0bc6\u0b95\u0bbe\u0b9f\u0bc1\u0baa\u0bcd\u0baa\u0bbe\u0bb0\u0bcd\" \u0b8e\u0ba9\u0baa\u0bcd \u0baa\u0bc8\u0bb1\u0b9a\u0bbe\u0bb1\u0bcd\u0bb1 \u0ba9\u0bbe\u0bb0\u0bcd. en: 8 I have baptized you with water, and he will baptize you with the Holy Spirit.\" he declared. en Is that evidence that he is God ? si \u0dd9\u0db8\u0dca\u0dc0\u0da7 \u0d9a\u0dd2\u0dba\u0db1\u0dca\u0dd9\u0db1\u0dca \u0dd9\u0daf\u0dba\u0dd2\u0dd9\u0dba\u0dcf\u0dca \u0dc3\u0dcf\u0d9a\u0dca\u0d9a\u0dd2 \u0d9a\u0dd2\u0dba\u0dbd\u0daf?en: Are these told as gods are the witnesses?en The die , then, is the equivalent of a cookie cutter.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_26", "figure_caption": "ta \u0b8e\u0ba9\u0bbf\u0bae\u0bc1\u0bae\u0bcd, \u0bae\u0bbf\u0b95\u0bb5\u0bc1\u0bae\u0bcd \u0ba8\u0ba9\u0bcd\u0bb1 \u0bc6\u0b95\u0b9f\u0bcd\u0b9f, \u0bc6\u0baa\u0bb0\u0bc1\u0ba8\u0bcd\u0ba4\u0bc1\u0bc7\u0bb0\u0bbe\u0b95 \u0b95\u0bc8\u0bb3\u0ba4\u0bcd \u0ba4\u0bb5 \u0bb0 \u0bc7\u0bb5\u0bb1\u0bc1 \u0b8e\u0bb5\u0bb0\u0bc1\u0bae\u0bcd \u0ba8\u0bae\u0bcd \u0b85\u0ba4\u0bcd\u0ba4\u0bbe\u0b9f\u0bcd\u0b9a \u0b95\u0bc8\u0bb3 \u0ba8 \u0bb0\u0bbe\u0b95\u0bb0\u0bbf\u0baa\u0bcd-\u0baa\u0ba4 \u0bb2\u0bcd\u0bc8\u0bb2 ! en: However, nobody rejects the evidence except the most ungrateful, traitors ! en No horses permitted on the said [sic] property.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_27", "figure_caption": "si``\u0db1\u0dd4\u0db9 \u0d85\u0dd9\u0db4\u0dca \u0dc3\u0dd4\u0dbb\u0dad\u0dbd\u0dd4\u0db1\u0dca\u0da7 \u0dc0\u0dc3 \u0dd9\u0db4\u0dc0\u0dca\u0dc0\u0dcf,'' \u0dba\u0db1 \u0d85\u0daf\u0dc4\u0dc3 \u0d94\u0dc4\u0dd4\u0dd9\u0d9c\u0dca \u0dc3\u0dd2\u0dad\u0da7 \u0db1\u0dd0\u0d9c\u0dd2\u0dab; \u0daf\u0dd2\u0dc0\u0da7 \u0dd9\u0db1\u0dcf\u0db1\u0dd0\u0d9c\u0dd2\u0dab . en: \"You poisoned our pets,\" the idea came to his mind; but not his lips.enThen introduce your family one at a time.ta \u0baa \u0bb1\u0b95\u0bc1 \"\u0b87\u0bc8\u0ba4 \u0b89\u0bae\u0ba4\u0bc1 \u0b95\u0bc1\u0b9f\u0bc1\u0bae\u0bcd\u0baa\u0ba4\u0bcd\u0ba4\u0bbe\u0bb0\u0bc1\u0b95\u0bcd\u0bc7\u0b95 \u0b89\u0ba3\u0bcd\u0ba3\u0b95\u0bcd \u0bc6\u0b95\u0bbe\u0b9f\u0bc1\u0ba4\u0bcd\u0ba4\u0bc1 \u0bb5 \u0b9f\u0bc1\u0bb5\u0bc0\u0bb0\u0bbe\u0b95! en:Then \"Give this to your family to eat!\" en We have such ADD in this town (there's something in the water!). si \u0d85\u0dd9\u0db4\u0dca \u0db4\u0dc5\u0dcf\u0dd9\u0dad \u0dad\u0dd2\u0dd9\u0dba\u0db1 \u0d91\u0dd9\u0d9a\u0dad\u0dca [\u0dd9\u0db4\u0dca\u0dbb\u0dcf\u0dd9\u0daf\u0dab\u0dd2 \u0db8\u0dbd\u0dca\u0dc0\u0dad\u0dca\u0dad] \u0dd9\u0db8\u0dca \u0dc0\u0dd2\u0daf\u0dd2\u0dd9\u0dc4\u0db8 \u0db4\u0dcf\u0dbb\u0d9a\u0dca \u0dad\u0dd2\u0dd9\u0dba\u0db1\u0dc0\u0dcf. en: There's a similar road at the one in our area [Peradeniya botanical garden] too. en between them for you to practice. ta \u0b85\u0bc8\u0bb5\u0b95\u0bc8\u0bb3 , \u0ba8\u0bc0\u0b99\u0bcd\u0b95\u0bb3\u0bcd \u0bc6\u0b9a\u0baf\u0bb2\u0bcd\u0baa\u0b9f\u0bc1\u0ba4\u0bcd\u0ba4 \u0b89\u0b99\u0bcd\u0b95\u0bb3\u0bc1\u0b95\u0bcd\u0b95\u0bc1\u0bb3\u0bcd \u0baa\u0bb0\u0bbf\u0b9a\u0bc1\u0ba4\u0bcd\u0ba4 \u0b85\u0bb2\u0b99\u0bcd\u0b95\u0bbe\u0bb0\u0bae\u0bcd \u0bae\u0bbf\u0b95\u0bb5\u0bc1\u0bae\u0bcd \u0b85\u0bb5\u0b9a \u0baf\u0bae\u0bcd. en: For you to implement them, you need a holy adornment within you.", "figure_data": ""}, {"figure_label": "14", "figure_type": "table", "figure_id": "tab_28", "figure_caption": "Extended set of examples of parallel sentences from NLLB where the translated Si or Ta sentence has a different meaning than the original En sentences. We highlighted in colour code the pairs of semantically close words that possibly contributed to the misalignment. Correct En translation of the Si or Ta sentence is given for comparison.", "figure_data": "Dataset"}, {"figure_label": "18", "figure_type": "table", "figure_id": "tab_29", "figure_caption": "Chrf++ scores visualized in Figure4and Figure5as well as other scores used for evaluation.", "figure_data": "NMT ModelChrfChrf++SITA spBLEUBLEUChrfFLORES Chrf++ spBLEUBLEULASER-3Top Random Bottom24.90 5.60 8.8022.40 4.60 7.108.00 0.10 0.101.90 0.00 0.0024.10 5.70 9.8021.90 4.80 8.008.40 0.10 0.001.70 0.00 0.00LaBSETop Random Bottom9.70 7.80 7.608.80 6.60 6.100.90 0.20 0.000.20 0.00 0.0010.50 8.30 8.709.60 7.20 6.900.90 0.30 0.100.00 0.00 0.00"}, {"figure_label": "19", "figure_type": "table", "figure_id": "tab_30", "figure_caption": "Chrf++ scores visualized in Figure6as well as other scores used for evaluations.", "figure_data": "Datasets SITA Test Set FLORES Test SetNLLB Top 25K BC 0.71 0.51WikiMatrix Top 25K 0.55 0.44CCAligned Top 25K 0.64 0.61CCMatrix Top 25K 0.59 0.51SITA Top 25K 0.16 0.62NLLB Top 25K AC 0.69 0.47"}, {"figure_label": "20", "figure_type": "table", "figure_id": "tab_31", "figure_caption": "Domain divergence between datasets for En-Si. BC-Before cleaning, AC-after cleaning", "figure_data": ""}], "formulas": [], "doi": "10.18653/v1/P19-1309"}