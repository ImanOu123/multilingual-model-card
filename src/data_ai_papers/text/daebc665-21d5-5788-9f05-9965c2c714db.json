{"title": "Tuning-free Plug-and-Play Proximal Algorithm for Inverse Imaging Problems", "authors": "Kaixuan Wei; Angelica Aviles-Rivero; Jingwei Liang; Ying Fu; Carola-Bibiane Sch\u00f6nlieb; Hua Huang", "pub_date": "", "abstract": "Plug-and-play (PnP) is a non-convex framework that combines ADMM or other proximal algorithms with advanced denoiser priors. Recently, PnP has achieved great empirical success, especially with the integration of deep learning-based denoisers. However, a key problem of PnP based approaches is that they require manual parameter tweaking. It is necessary to obtain high-quality results across the high discrepancy in terms of imaging conditions and varying scene content. In this work, we present a tuning-free PnP proximal algorithm, which can automatically determine the internal parameters including the penalty parameter, the denoising strength and the terminal time. A key part of our approach is to develop a policy network for automatic search of parameters, which can be effectively learned via mixed modelfree and model-based deep reinforcement learning. We demonstrate, through numerical and visual experiments, that the learned policy can customize different parameters for different states, and often more efficient and effective than existing handcrafted criteria. Moreover, we discuss the practical considerations of the plugged denoisers, which together with our learned policy yield state-of-the-art results. This is prevalent on both linear and nonlinear exemplary inverse imaging problems, and in particular, we show promising results on Compressed Sensing MRI and phase retrieval.", "sections": [{"heading": "Introduction", "text": "The problem of recovering an underlying unknown image x \u2208 R N from noisy and/or incomplete measured data y \u2208 R M is fundamental in computational imaging, in applications including magnetic resonance imaging (MRI) (Fessler, 2010), computed tomography (CT) (Elbakri & Fessler, 2002), microscopy (Aguet et al., 2008;Zheng et al., 2013), and inverse scattering (Katz et al., 2014;Metzler et al., 2017b), to name a few. This image recovery task is often formulated as an optimization problem that minimizes a cost function, i.e., minimize\nx\u2208R N D (x) + \u03bbR (x) , (1\n)\nwhere D is a data-fidelity term that ensures consistency between the reconstructed image and measured data. R is a regularizer that imposes certain prior knowledge, e.g. smoothness (Osher et al., 2005;Ma et al., 2008), sparsity (Yang et al., 2010;Liao & Sapiro, 2008;Ravishankar & Bresler, 2010), low rank (Semerci et al., 2014;Gu et al., 2017) and nonlocal self-similarity (Mairal et al., 2009;Qu et al., 2014), regarding the unknown image. The problem in Eq. ( 1) is often solved by first-order iterative proximal algorithms, e.g. fast iterative shrinkage/thresholding algorithm (FISTA) (Beck & Teboulle, 2009) and alternating direction method of multipliers (ADMM) (Boyd et al., 2011), to tackle the nonsmoothness of the regularizers.\nTo handle the nonsmoothness caused by regularizers, firstorder algorithms rely on the proximal operators (Beck & Teboulle, 2009;Boyd et al., 2011;Chambolle & Pock, 2011;Parikh et al., 2014;Geman, 1995;Esser et al., 2010) defined by\nProx \u03c3 2 R (v) = argmin x R(x) + 1 2\u03c3 2 x \u2212 v 2 2 . (2)\nInterestingly, given the mathematical equivalence of the proximal operator to the regularized denoising, the proximal operators Prox \u03c3 2 R can be replaced by any off-the-shelf denoisers H \u03c3 with noise level \u03c3, yielding a new framework namely plug-and-play (PnP) prior (Venkatakrishnan et al., 2013). The resulting algorithms, e.g. PnP-ADMM, can be written as\nx k+1 = Prox \u03c3 2 k R (z k \u2212 u k ) = H \u03c3 k (z k \u2212 u k ) ,(3)\nz k+1 = Prox 1 \u00b5 k D (x k+1 + u k ) ,(4)\nu k+1 = u k + x k+1 \u2212 z k+1 ,(5)\nwhere k \u2208 [0, \u03c4 ) denotes the k-th iteration, \u03c4 is the terminal time, \u03c3 k and \u00b5 k indicate the denoising strength (of the arXiv:2002.09611v2 [eess.IV] 18 Nov 2020 denoiser) and the penalty parameter used in the k-th iteration respectively.\nIn this formulation, the regularizer R can be implicitly defined by a plugged denoiser, which opens a new door to leverage the vast progress made on the image denoising front to solve more general inverse imaging problems. To plug well-known image denoisers, e.g. BM3D (Dabov et al., 2007) and NLM (Buades et al., 2005), into optimization algorithms often leads to sizeable performance gain compared to other explicitly defined regularizers, e.g. total variantion. That is PnP as a stand-alone framework can combine the benefits of both deep learning based denoisers and optimization methods, e.g. (Zhang et al., 2017b;Rick Chang et al., 2017;Meinhardt et al., 2017). These highly desirable benefits are in terms of fast and effective inference whilst circumventing the need of expensive network retraining whenever the specific problem changes.\nWhilst a PnP framework offers promising image recovery results, a major drawback is that its performance is highly sensitive to the internal parameter selection, which generically includes the penalty parameter \u00b5, the denoising strength (of the denoiser) \u03c3 and the terminal time \u03c4 . The body of literature often utilizes manual tweaking e.g. (Rick Chang et al., 2017;Meinhardt et al., 2017) or handcrafted criteria e.g. (Chan et al., 2017;Zhang et al., 2017b;Eksioglu, 2016;Tirer & Giryes, 2018) to select parameters for each specific problem setting. However, manual parameter tweaking requires several trials, which is very cumbersome and time-consuming. Semi-automated handcrafted criteria (for example monotonically decreasing the denoising strength) can, to some degree, ease the burden of exhaustive search of large parameter space, but often leads to suboptimal local minimum. Moreover, the optimal parameter setting differs image-by-image, depending on the measurement model, noise level, noise type and unknown image itself. These differences can be noticed in the further detailed comparison in Fig. 1, where peak signal-to-noise ratio (PSNR) curves are displayed for four images under varying denoising strength.\nThis paper is devoted to addressing the aforementioned challenge -how to deal with the manual parameter tuning problem in a PnP framework. To this end, we formulate the internal parameter selection as a sequential decision-making problem. To do this, a policy is adopted to select a sequence of internal parameters to guide the optimization. Such problem can be naturally fit into a reinforcement learning (RL) framework, where a policy agent seeks to map observations to actions, with the aim of maximizing cumulative-reward.\nThe reward reflects the to do or not to do events for the agent, and a desirable high reward can be obtained if the policy leads to a faster convergence and better restoration accuracy.\nWe demonstrate, through extensive numerical and visual experiments, the advantage of our algorithmic approach on Compressed Sensing MRI and phase retrieval problems. We show that the policy well approximates the intrinsic function that maps the input state to its optimal parameter setting. By using the learned policy, the guided optimization can reach comparable results to the ones using oracle parameters tuned via the inaccessible ground truth. An overview of our algorithm is shown in Fig. 2. Our contributions are as follows:\n1. We present a tuning-free PnP algorithm that can customize parameters towards diverse images, which often demonstrates faster practical convergence and better empirical performance than handcrafted criteria. 2. We introduce an efficient mixed model-free and modelbased RL algorithm. It can optimize jointly the discrete terminal time, and the continuous denoising strength/penalty parameters. 3. We validate our approach with an extensive range of numerical and visual experiments, and show how the performance of the PnP is affected by the parameters. We also show that our well-designed approach leads to better results than state-of-the-art techniques on compressed sensing MRI and phase retrieval.", "publication_ref": ["b21", "b17", "b1", "b94", "b38", "b52", "b56", "b43", "b84", "b40", "b61", "b68", "b28", "b45", "b59", "b2", "b3", "b2", "b3", "b7", "b57", "b24", "b19", "b80", "b11", "b4", "b90", "b62", "b47", "b62", "b47", "b9", "b90", "b16", "b79"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "The body of literature has reported several PnP algorithmic techniques. In this section, we provide a short overview of these techniques.  2011; Venkatakrishnan et al., 2013), which has attracted great attention owing to its effectiveness and flexibility to handle a wide range of inverse imaging problems. Following this philosophy, several works have been developed, and can be roughly categorized in terms of four aspects, i.e., proximal algorithms, imaging applications, denoiser priors, and the convergence. (i) proximal algorithms include half-quadratic splitting (Zhang et al., 2017b), primaldual method (Ono, 2017), generalized approximate message passing (Metzler et al., 2016b) and (stochastic) accelerated proximal gradient method (Sun et al., 2019a). (ii) imaging applications have such as bright field electronic tomography (Sreehari et al., 2016); diffraction tomography (Sun et al., 2019a); low-dose CT imaging (He et al., 2018); Compressed Sensing MRI (Eksioglu, 2016); electron microscopy (Sreehari et al., 2017); single-photon imaging (Chan et al., 2017); phase retrieval (Metzler et al., 2018); Fourier ptychography microscopy (Sun et al., 2019b); light-field photography (Chun et al., 2019); hyperspectral sharpening (Teodoro et al., 2018); denoising (Rond et al., 2016); and image processinge.g. demosaicking, deblurring, super-resolution and inpainting (Heide et al., 2014;Meinhardt et al., 2017;Zhang et al., 2019a;Tirer & Giryes, 2018).\nX STEP 1 Z STEP 1 U STEP 1 X STEP 2 Z STEP 2 U STEP 2 RECOVERED IMAGE 1 - \uf074 \uf073 0 \uf073 1 \uf073 0 \uf06d 1 \uf06d 1 - \uf074 \uf06d X STEP \uf074 POLICY AT STEP 1 0 x 0 u 0 z 0 2 a 2 a 2 a U\nMoreover, (iii) denoiser priors include BM3D (Heide et al., 2014;Dar et al., 2016;Rond et al., 2016;Sreehari et al., 2016;Chan et al., 2017), nonlocal means (Venkatakrishnan et al., 2013;Heide et al., 2014;Sreehari et al., 2016), Gaussian mixture models (Teodoro et al., 2016;, weighted nuclear norm minimization (Kamilov et al., 2017), and deep learning-based denoisers (Meinhardt et al., 2017;Zhang et al., 2017b;Rick Chang et al., 2017). Finally, (iv) theoretical analysis on the convergence include the symmetric gradient (Sreehari et al., 2016), the bounded denoiser (Chan et al., 2017) and the nonexpansiveness assumptions (Sreehari et al., 2016;Teodoro et al., 2018;Sun et al., 2019a;Ryu et al., 2019;Chan, 2019).\nDiffering from these aspects, in this work we focus on the challenge of parameter selection in PnP, where a bad choice of parameters often leads to severe degradation of the results (Romano et al., 2017;Chan et al., 2017). Unlike existing semi-automated parameter tuning criteria Chan et al., 2017;Zhang et al., 2017b;Eksioglu, 2016;Tirer & Giryes, 2018), our method is fully automatic and is purely learned from the data, which significantly eases the burden of manual parameter tuning.\nAutomated Parameter Selection. There are some works that considering automatic parameter selection in inverse problems. However, the prior term in these works is restricted to certain types of regularizers, e.g. Tikhonov regularization (Hansen & O'Leary, 1993;Golub et al., 1979), smoothed versions of the p norm (Eldar, 2008;Giryes et al., 2011), or general convex functions (Ramani et al., 2012). To the best of our knowledge, none of them can be applicable to the PnP framework with sophisticated non-convex and learned priors.\nDeep Unrolling. Perhaps the most confusable concept to PnP in the deep learning era is the so-called deep unrolling methods (Gregor & LeCun, 2010;Hershey et al., 2014;Wang et al., 2016;Yang et al., 2016;Zhang & Ghanem, 2018;Diamond et al., 2017;Metzler et al., 2017a;Adler & Oktem, 2018;Dong et al., 2018;Xie et al., 2019), which explicitly unroll/truncate iterative optimization algorithms into learnable deep architectures. In this way, the penalty parameters (and the denoiser prior) are treated as trainable parameters, meanwhile the number of iterations has to be fixed to enable end-to-end training. By contrast, our PnP approach can adaptively select a stop time and penalty parameters given varying input states, though using the off-the-shelf denoiser as prior.\nReinforcement Learning for Image Recovery. Although Reinforcement Learning (RL) has been applied in a range of domains, from game playing (Mnih et al., 2013;Silver et al., 2016) to robotic control (Schulman et al., 2015), only few works have successfully employed RL to the image recovery tasks. Authors of that (Yu et al., 2018) learned a RL policy to select appropriate tools from a toolbox to progressively restore corrupted images. The work of (Zhang et al., 2019b) proposed a recurrent image restorer whose endpoint was dynamically controlled by a learned policy.\nIn (Furuta et al., 2019), authors used RL to select a sequence of classic filters to process images gradually. The work of (Yu et al., 2019) learned network path selection for image restoration in a multi-path CNN. In contrast to these works, we apply a mixed model-free and model-based deep RL approach to automatically select the parameters for the PnP image recovery algorithm.", "publication_ref": ["b80", "b90", "b55", "b51", "b73", "b71", "b73", "b30", "b16", "b72", "b9", "b49", "b74", "b10", "b78", "b64", "b32", "b47", "b92", "b79", "b32", "b13", "b64", "b71", "b9", "b80", "b32", "b71", "b77", "b36", "b47", "b90", "b62", "b71", "b9", "b71", "b78", "b73", "b66", "b8", "b63", "b9", "b9", "b90", "b16", "b79", "b29", "b26", "b18", "b25", "b60", "b27", "b33", "b81", "b85", "b88", "b14", "b48", "b0", "b15", "b83", "b53", "b70", "b67", "b86", "b93", "b23", "b87"], "figure_ref": [], "table_ref": []}, {"heading": "Tuning-free PnP Proximal Algorithm", "text": "In this work,we elaborate on our tuning-free PnP proximal algorithm, as described in ( 3)-( 5). This section describes in detail our approach, which contains three main parts. Firstly, we describe how the automated parameter selection is driven.\nSecondly, we introduce our environment model, and finally, we introduce the policy learning, which is guided by a mixed model-free and a model-based RL.\nIt is worth mentioning that our method is generic, and can be applicable to PnP methods derived from other proximal algorithms, e.g. forward backward splitting, as well. The reason is that these are distinct methods, they share the same fixed points as PnP-ADMM (Meinhardt et al., 2017).", "publication_ref": ["b47"], "figure_ref": [], "table_ref": []}, {"heading": "RL Formulation for Automated Parameter Selection", "text": "This work mainly focuses on the automated parameter selection problem in the PnP framework, where we aim to select a sequence of parameters (\u03c3\n0 , \u00b5 0 , \u03c3 1 , \u00b5 1 , \u2022 \u2022 \u2022 , \u03c3 \u03c4 \u22121 , \u00b5 \u03c4 \u22121 )\nto guide optimization such that the recovered image x \u03c4 is close to the underlying image x. We formulate this problem as a Markov decision process (MDP), which can be addressed via reinforcement learning (RL).\nWe denote the MDP by the tuple (S, A, p, r), where S is the state space, A is the action space, p is the transition function describing the environment dynamics, and r is the reward function. Specifically, for our task, S is the space of optimization variable states, which includes the initialization (x 0 , z 0 , u 0 ) and all intermedia results (x k , z k , u k ) in the optimization process. A is the space of internal parameters, including both discrete terminal time \u03c4 and the continuous denoising strength/penalty parameters (\u03c3 k , \u00b5 k ). The transition function p : S \u00d7 A \u2192 S maps input state s \u2208 S to its outcome state s \u2208 S after taking action a \u2208 A. The state transition can be expressed as s t+1 = p(s t , a t ), which is composed of one or several iterations of optimization. On each transition, the environment emits a reward in terms of the reward function r : S \u00d7A \u2192 R, which evaluates actions given the state. Applying a sequence of parameters to the initial state s 0 results in a trajectory T of states, actions and rewards:\nT = {s 0 , a 0 , r 0 , \u2022 \u2022 \u2022 , s N , a N , r N }.\nGiven a trajectory T , we define the return r \u03b3 t as the summation of discounted rewards after s t ,\nr \u03b3 t = N \u2212t t =0 \u03b3 t r(s t+t , a t+t ),(6)\nwhere \u03b3 \u2208 [0, 1] is a discount factor and prioritizes earlier rewards over later ones.\nOur goal is to learn a policy \u03c0, denoted as \u03c0(a|s) : S \u2192 A for the decision-making agent, in order to maximize the objective defined as\nJ (\u03c0) = E s0\u223cS0,T \u223c\u03c0 [r \u03b3 0 ] ,(7)\nwhere E represents expectation, s 0 is the initial state, and S 0 is the corresponding initial state distribution. Intuitively, the objective describes the expected return over all possible trajectories induced by the policy \u03c0. The expected return on states and state-action pairs under the policy \u03c0 are defined by state-value functions V \u03c0 and action-value functions Q \u03c0 respectively, i.e.,\nV \u03c0 (s) = E T \u223c\u03c0 [r \u03b3 0 |s 0 = s] ,(8)\nQ \u03c0 (s, a) = E T \u223c\u03c0 [r \u03b3 0 |s 0 = s, a 0 = a] .(9)\nIn our task, we decompose actions into two parts: a discrete decision a 1 on terminal time and a continuous decision a 2 on denoising strength and penalty parameter. The policy also consists of two sub-policies: \u03c0 = (\u03c0 1 , \u03c0 2 ), a stochastic policy and a deterministic policy that generate a 1 and a 2 respectively. The role of \u03c0 1 is to decide whether to terminate the iterative algorithm when the next state is reached. It samples a boolean-valued outcome a 1 from a two-class categorical distribution \u03c0 1 (\u2022|s), whose probability mass function is calculated from the current state s. We move forward to the next iteration if a 1 = 0, otherwise the optimization would be terminated to output the final state. Compared to the stochastic policy \u03c0 1 , we treat \u03c0 2 deterministically, i.e. a 2 = \u03c0 2 (s) since \u03c0 2 is differentiable with respect to the environment, such that its gradient can be precisely estimated.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Environment Model", "text": "In RL, the environment is characterized by two components: the environment dynamics and reward function. In our task, the environment dynamics is described by the transition function p related to the PnP-ADMM. Here, we elucidate the detailed setting of the PnP-ADMM as well as the reward function used for training policy.\nDenoiser Prior. Differentiable environment makes the policy learning more efficient. To make the environment differentiable with respect to \u03c0 2 1 , we take a convolutional neural network (CNN) denoiser as the image prior. In practice, we use a residual U-Net (Ronneberger et al., 2015) architecture, which was originally designed for medical image segmentation, but was founded to be useful in image denoising recently. Besides, we incorporate an additional tunable noise level map into the input as , enabling us to provide continuous noise level control (i.e. different denoising strength) within a single network.\nProximal operator of data-fidelity term. Enforcing consistency with measured data requires evaluating the proximal operator in (4). For inverse problems, there might exist fast solutions due to the special structure of the observation model. We adopt the fast solution if feasible (e.g. closedform solution using fast Fourier transform, rather than the general matrix inversion) otherwise a single step of gradient descent is performed as an inexact solution for (4).\nTransition function. To reduce the computation cost, we define the transition function p to involve m iterations of the optimization. At each time step, the agent thus needs to decide the internal parameters for m iterates. We set m = 5 and the max time step N = 6 in our algorithm, leading to 30 iterations of the optimization at most. Reward function. To take both image recovery performance and runtime efficiency into account, we define the reward function as\nr(s t , a t ) = \u03b6(p(s t , a t )) \u2212 \u03b6(s t ) \u2212 \u03b7. (10\n)\nThe first term, \u03b6(p(s t , a t )) \u2212 \u03b6(s t ), denotes the PSNR increment made by the policy, where \u03b6(s t ) denotes the PSNR of the recovered image at step t. A higher reward is acquired if the policy leads to higher performance gain in terms of PSNR. The second term, \u03b7, implies penalizing the policy as it does not select to terminate at step t, where \u03b7 sets the degree of penalty. A negative reward is given if the PSNR gain does not exceed the degree of penalty, thereby encouraging the policy to early stop the iteration with diminished return. We set \u03b7 = 0.05 in our algorithm 2 .", "publication_ref": ["b65"], "figure_ref": [], "table_ref": []}, {"heading": "RL-based policy learning", "text": "In this section, we present a mixed model-free and modelbased RL algorithm to learn the policy. Specifically, modelfree RL (agnostic to the environment dynamics) is used to train \u03c0 1 , while model-based RL is utilized to optimize \u03c0 2 to make full use of the environment model 3 . We apply the actor-critic framework (Sutton et al., 2000), that uses a policy network \u03c0 \u03b8 (a t |s t ) (actor) and a value network V \u03c0 \u03c6 (s t ) (critic) to formulate the policy and the state-value function respectively. For convenience, we follow (Huang et al., 2019) that uses residual structures similar to ResNet-18 (He et al., 2016) as the feature extractor in the policy and value networks, followed by fully-connected layers and activation functions to produce desired outputs 4 . The policy and the value networks are learned in an interleaved manner. For each gradient step, we optimize the value network parameters \u03c6 by minimizing\nL \u03c6 = E s\u223cB,a\u223c\u03c0 \u03b8 (s) 1 2 (r(s, a) + \u03b3V \u03c0 \u03c6 (p(s, a)) \u2212 V \u03c0 \u03c6 (s)) 2 , (11\n)\nwhere B is the distribution of previously sampled states, practically implemented by a state buffer. This partly serves as a role of the experience replay mechanism (Lin, 1992), which is observed to \"smooth\" the training data distribution  (Mnih et al., 2013). The update makes use of a target value network V \u03c0 \u03c6 , where\u03c6 is the exponentially moving average of the value network weights and has been shown to stabilize training (Mnih et al., 2015).\nThe policy network has two sub-policies, which employs shared convolutional layers to extract image features, followed by two separated groups of fully-connected layers to produce termination probability \u03c0 1 (\u2022|s) (after softmax) or denoising strength/penalty parameters \u03c0 2 (s) (after sigmoid). We denote the parameters of the sub-polices as \u03b8 1 and \u03b8 2 respectively, and we seek to optimize \u03b8 = (\u03b8 1 , \u03b8 2 ) so that the objective J(\u03c0 \u03b8 ) is maximized. The policy network is trained using policy gradient methods (Peters & Schaal, 2006). The gradient of \u03b8 1 is estimated by a likelihood estimator in a model-free manner, while the gradient of \u03b8 2 is estimated relying on backpropagation via environment dynamics in a model-based manner. Specifically, for discrete terminal time decision \u03c0 1 , we apply the policy gradient theorem (Sutton et al., 2000) to obtain unbiased Monte Carlo estimate of \u03b81 J(\u03c0 \u03b8 ) using advantage function A \u03c0 (s, a) = Q \u03c0 (s, a) \u2212 V \u03c0 (s) as target, i.e.,\n\u03b81 J(\u03c0 \u03b8 ) =E s\u223cB,a\u223c\u03c0 \u03b8 (s) [ \u03b81 log \u03c0 1 (a 1 |s) A \u03c0 (s, a)] .(12)\nFor continuous denoising strength and penalty parameter selection \u03c0 2 , we utilize the deterministic policy gradient theorem (Silver et al., 2014) to formulate its gradient, i.e.,\n\u03b82 J(\u03c0 \u03b8 ) =E s\u223cB,a\u223c\u03c0 \u03b8 (s) [ a2 Q \u03c0 (s, a) \u03b82 \u03c0 2 (s)] ,(13)\nwhere we approximate the action-value function Q \u03c0 (s, a) by r(s, a) + \u03b3V \u03c0 \u03c6 (p(s, a)) given its unfolded definition. Using the chain rule, we can directly obtain the gradient of \u03b8 2 by backpropagation via the reward function, the value network and the transition function, in contrast to relying on the gradient backpropagated from only the learned actionvalue function in the model-free DDPG algorithm (Lillicrap et al., 2016). ", "publication_ref": ["b75", "b35", "b31", "b42", "b53", "b54", "b58", "b75", "b69", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section, we detail the experiments and evaluate our proposed algorithm. We mainly focus on the tasks of Compressed Sensing MRI (CS-MRI) and phase retrieval (PR), which are the representative linear and nonlinear inverse imaging problems respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Implementation Details", "text": "Our algorithm requires two training processes for: the denoising network and the policy network (and value network).\nFor training the denoising network, we follow the common practice that uses 87,000 overlapping patches (with size 128 \u00d7 128) drawn from 400 images from the BSD dataset (Martin et al., 2001). For each patch, we add white Gaussian noise with noise level sampled from [1,50]. The denoising networks are trained with 50 epoch using L 1 loss and Adam optimizer (Kingma & Ba, 2014) with batch size 32. The base learning rate is set to 10 \u22124 and halved at epoch 30, then reduced to 10 \u22125 at epoch 40.\nTo train the policy network and value network, we use the 17,125 resized images with size 128\u00d7128 from the PASCAL VOC dataset (Everingham et al., 2014). Both networks are trained using Adam optimizer with batch size 48 and 1500 iterations, with a base learning rate of 3 \u00d7 10 \u22124 for the policy network and 10 \u22123 for the value network. Then we set these learning rates to 10 \u22124 and 3 \u00d7 10 \u22124 at iteration 1000. We perform 10 gradient steps at every iteration.\nFor the CS-MRI application, a single policy network is trained to handle multiple sampling ratios (with x2/x4/x8 acceleration) and noise levels (5/10/15), simultaneously. Similarly, one policy network is learned for phase retrieval under different settings.", "publication_ref": ["b46", "b39", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Compressed sensing MRI", "text": "The forward model of CS-MRI can be mathematically described as y = F p x + \u03c9, where x \u2208 C N is the underlying image, the operator F p : C N \u2192 C M , with M < N , denotes the partially-sampled Fourier transform, and \u03c9 \u223c N (0, \u03c3 n I M ) is the additive white Gaussian noise. The data-fidelity term is D(x) = 1 2 y \u2212 F p x 2 whose proximal operator is given in (Eksioglu, 2016).\nDenoiser priors. To show how denoiser priors affect the performance of the PnP, we train three state-of-the-art CNNbased denoisers, i.e. DnCNN (Zhang et al., 2017a), Mem-Net (Tai et al., 2017) and residual UNet (Ronneberger et al., 2015), with tunable noise level map. We compare both the Gaussian denoising performance and the PnP performance 5 using these denoisers. As shown in Table 1, the residual UNet and MemNet consistently outperform DnCNN in terms of denoising and CS-MRI. It seems to imply a better Gaussian denoiser is also a better denoiser prior for the PnP framework 6 . Since UNet is significantly faster than MemNet, we choose UNet as our denoiser prior.\nComparisons of different policies. We start by giving some insights of our learned policy by comparing the performance of PnP-ADMM with different polices: i) the handcrafted policy used in IRCNN (Zhang et al., 2017b); ii) the fixed policy that uses fixed parameters (\u03c3 = 15, \u00b5 = 0.1); iii) the fixed optimal policy that adopts fixed parameters searched to maximize the average PSNR across all testing images; iv) the oracle policy that uses different parameters for different images such that the PSNR of each image is maximized and v) our learned policy based on a learned policy network to optimize parameters for each image. We remark that all compared polices are run for 30 iteration whilst ours automatically choose the terminal time.\nTo understand the usefulness of the early stopping mechanism, we also report the results of these polices with optimal early stopping 7 . Moreover, we analyze whether the modelbased RL benefits our algorithm by comparing it with the learned policy by model-free RL whose \u03c0 2 is optimized using the model-free DDPG algorithm (Lillicrap et al., 2016).\nThe results of all aforementioned policies are provided in Table 2. We can see that the bad choice of parameters (see \"fixed\") induces poor results, in which the early stopping is quite needed to rescue performance (see \"fixed * \"). When the parameters are properly assigned, the early stopping would be helpful to reduce computation cost. Our learned policy leads to fast practical convergence as well as excellent performance, sometimes even outperforms the oracle policy tuned via inaccessible ground truth (in \u00d72 case). We note this is owing to the varying parameters across iterations generated automatically in our algorithm, which yield extra flexibility than constant parameters over iterations. Besides, we find the learned model-free policy produces suboptimal denoising strength/penalty parameters compared with our mixed model-free and model-based policy, and it also fails to learn early stopping behavior.\nComparisons with state-of-the-arts. We compare our method against six state-of-the-art methods for CS-MRI, including the traditional optimization-based approaches (RecPF (Yang et al., 2010) and FCSA (Huang et al., 2010)), the PnP approaches (BM3D-MRI (Eksioglu, 2016) and IR-CNN (Zhang et al., 2017b)), and the deep unrolling approaches (ADMMNet (Yang et al., 2016) and ISTANet (Zhang & Ghanem, 2018)). To keep comparison fair, for each deep unrolling method, only single network is trained to tackle all the cases using the same dataset as ours. Table 3 shows the method performance on two set of medical images, i.e. 7 widely used medical images (Medical7) (Huang et al., 2010) and 50 medical images from MICCAI 2013 grand challenge dataset 8 . The visual comparison can be found in Fig. 3. It can be seen that our approach significantly outperforms the state-of-the-art PnP method (IRCNN) by a large margin, especially under the difficult \u00d78 case. In the simple cases (e.g. \u00d72), our algorithm only runs 5 iterations to arrive at the desirable performance, in contrast with 30 or 70 iterations required in IRCNN and BM3D-MRI respectively.", "publication_ref": ["b16", "b89", "b76", "b65", "b90", "b41", "b84", "b34", "b16", "b90", "b85", "b88", "b34"], "figure_ref": [], "table_ref": ["tab_3", "tab_4", "tab_5"]}, {"heading": "Phase retrieval", "text": "The goal of phase retrieval (PR) is to recover the underlying image from only the amplitude, or intensity of the output of a complex linear system. Mathematically, PR can be defined as the problem of recovering a signal x \u2208 R N or C N from measurement y of the form y 2 = |Ax| 2 + \u03c9, where the measurement matrix A represents the forward operator of the system, and \u03c9 represents shot noise. We approximate it with \u03c9 \u223c N (0, \u03b1 2 |Ax| 2 ). The term \u03b1 controls the sigmato-noise ratio in this problem.\nWe test algorithms with coded diffraction pattern (CDP) (Cand\u00e8s et al., 2015). Multiple measurements, with different random spatial modulator (SLM) patterns are recorded. We model the capture of four measurements using a phase-only SLM as (Metzler et al., 2018). Each measurement operator can be mathematically described as  transform and D i is diagonal matrices with nonzero elements drawn uniformly from the unit circle in the complex planes.\nA i = FD i , i \u2208 [1,\nWe compare our method with three classic approaches (HIO (Fienup, 1982), WF (Candes et al., 2014), and DOLPHIn (Mairal et al., 2016)) and three PnP approaches (SPAR (Katkovnik, 2017), BM3D-prGAMP (Metzler et al., 2016a) and prDeep (Metzler et al., 2018)). Table 4 and Fig. 4 summarize the results of all competing methods on twelve images used in (Metzler et al., 2018). It can be seen that our method still leads to state-of-the-art performance in this nonlinear inverse problem, and produces cleaner and clearer results than other competing methods.", "publication_ref": ["b6", "b49", "b22", "b5", "b44", "b37", "b50", "b49", "b49"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Conclusion", "text": "In this work, we introduce RL into the PnP framework, yielding a novel tuning-free PnP proximal algorithm for a wide range of inverse imaging problems. We underline the main message of our approach the main strength of our proposed method is the policy network, which can customize well-suited parameters for different images. Through numerical experiments, we demonstrate our learned policy often generates highly-effective parameters, which even often reaches to the comparable performance to the \"oracle\" parameters tuned via the inaccessible ground truth.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "Authors gratefully acknowledge the financial support of the CMIH and CCIMI University of Cambridge, and Graduate school of Beijing Institute of Technology. This work was also supported by the National Natural Science Foundation of China under Grant No. 61672096.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Learned primal-dual reconstruction", "journal": "IEEE Transactions on Medical Imaging", "year": "2018", "authors": "J Adler; O Oktem"}, {"ref_id": "b1", "title": "Model-based 2.5-d deconvolution for extended depth of field in brightfield microscopy", "journal": "IEEE Transactions on Image Processing", "year": "2008", "authors": "F Aguet; D Van De Ville; M Unser"}, {"ref_id": "b2", "title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "journal": "SIAM Journal on Imaging Sciences", "year": "2009", "authors": "A Beck; M Teboulle"}, {"ref_id": "b3", "title": "Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends\u00ae in Machine learning", "journal": "", "year": "2011", "authors": "S Boyd; N Parikh; E Chu; B Peleato; J Eckstein"}, {"ref_id": "b4", "title": "A non-local algorithm for image denoising", "journal": "", "year": "2005", "authors": "A Buades; B Coll; J.-M Morel"}, {"ref_id": "b5", "title": "Phase retrieval via wirtinger flow: Theory and algorithms", "journal": "IEEE Transactions on Information Theory", "year": "2014-07", "authors": "E Candes; X Li; M Soltanolkotabi"}, {"ref_id": "b6", "title": "Phase retrieval from coded diffraction patterns. Applied and Computational Harmonic Analysis", "journal": "", "year": "2015", "authors": "E J Cand\u00e8s; X Li; M Soltanolkotabi"}, {"ref_id": "b7", "title": "A first-order primal-dual algorithm for convex problems with applications to imaging", "journal": "Journal of Mathematical Imaging and Vision", "year": "2011", "authors": "A Chambolle; T Pock"}, {"ref_id": "b8", "title": "Performance analysis of plug-and-play admm: A graph signal processing perspective", "journal": "IEEE Transactions on Computational Imaging", "year": "2019", "authors": "S H Chan"}, {"ref_id": "b9", "title": "Plug-and-play admm for image restoration: Fixed-point convergence and applications", "journal": "IEEE Transactions on Computational Imaging", "year": "2017", "authors": "S H Chan; X Wang; O A Elgendy"}, {"ref_id": "b10", "title": "Momentum-net: Fast and convergent iterative neural network for inverse problems", "journal": "", "year": "2019", "authors": "I Y Chun; Z Huang; H Lim; J A Fessler"}, {"ref_id": "b11", "title": "Image denoising by sparse 3-d transform-domain collaborative filtering", "journal": "IEEE Transactions on Image Processing", "year": "2007", "authors": "K Dabov; A Foi; V Katkovnik; K Egiazarian"}, {"ref_id": "b12", "title": "Image deblurring by augmented lagrangian with bm3d frame prior", "journal": "", "year": "2010", "authors": "A Danielyan; V Katkovnik; K Egiazarian"}, {"ref_id": "b13", "title": "Postprocessing of compressed images via sequential denoising", "journal": "IEEE Transactions on Image Processing", "year": "2016", "authors": "Y Dar; A M Bruckstein; M Elad; R Giryes"}, {"ref_id": "b14", "title": "Unrolled optimization with deep priors", "journal": "", "year": "2017", "authors": "S Diamond; V Sitzmann; F Heide; G Wetzstein"}, {"ref_id": "b15", "title": "Denoising prior driven deep neural network for image restoration", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2018", "authors": "W Dong; P Wang; W Yin; G Shi; F Wu; X Lu"}, {"ref_id": "b16", "title": "Decoupled algorithm for mri reconstruction using nonlocal block matching model: Bm3d-mri", "journal": "Journal of Mathematical Imaging and Vision", "year": "2016", "authors": "E M Eksioglu"}, {"ref_id": "b17", "title": "Segmentation-free statistical image reconstruction for polyenergetic x-ray computed tomography", "journal": "", "year": "2002", "authors": "I A Elbakri; J A Fessler"}, {"ref_id": "b18", "title": "Generalized sure for exponential families: Applications to regularization", "journal": "IEEE Transactions on Signal Processing", "year": "2008", "authors": "Y C Eldar"}, {"ref_id": "b19", "title": "A general framework for a class of first order primal-dual algorithms for convex optimization in imaging science", "journal": "SIAM Journal on Imaging Sciences", "year": "2010", "authors": "E Esser; X Zhang; T F Chan"}, {"ref_id": "b20", "title": "The pascal visual object classes challenge: A retrospective", "journal": "International Journal of Computer Vision", "year": "2014-01", "authors": "M Everingham; S Eslami; L Van Gool; C Williams; J Winn; A Zisserman"}, {"ref_id": "b21", "title": "Model-based image reconstruction for mri", "journal": "IEEE Signal Processing Magazine", "year": "2010", "authors": "J A Fessler"}, {"ref_id": "b22", "title": "Phase retrieval algorithms: a comparison", "journal": "Applied Optics", "year": "1982", "authors": "J R Fienup"}, {"ref_id": "b23", "title": "Fully convolutional network with multi-step reinforcement learning for image processing", "journal": "", "year": "2019", "authors": "R Furuta; N Inoue; Yamasaki ; T "}, {"ref_id": "b24", "title": "Nonlinear image recovery with half-quadratic regularization", "journal": "IEEE Transactions on Image Processing", "year": "1995", "authors": "D Geman"}, {"ref_id": "b25", "title": "The projected gsure for automatic parameter tuning in iterative shrinkage methods", "journal": "Applied and Computational Harmonic Analysis", "year": "2011", "authors": "R Giryes; M Elad; Y C Eldar"}, {"ref_id": "b26", "title": "Generalized crossvalidation as a method for choosing a good ridge parameter", "journal": "Technometrics", "year": "1979", "authors": "G H Golub; M Heath; G Wahba"}, {"ref_id": "b27", "title": "Learning fast approximations of sparse coding", "journal": "", "year": "2010", "authors": "K Gregor; Y Lecun"}, {"ref_id": "b28", "title": "Weighted nuclear norm minimization and its applications to low level vision", "journal": "International Journal of Computer Vision", "year": "2017", "authors": "S Gu; Q Xie; D Meng; W Zuo; X Feng; L Zhang"}, {"ref_id": "b29", "title": "The use of the l-curve in the regularization of discrete ill-posed problems", "journal": "SIAM Journal on Scientific Computing", "year": "1993", "authors": "P C Hansen; D P Leary"}, {"ref_id": "b30", "title": "Optimizing a parameterized plug-and-play admm for iterative low-dose ct reconstruction", "journal": "IEEE Transactions on Medical Imaging", "year": "2018", "authors": "J He; Y Yang; Y Wang; D Zeng; Z Bian; H Zhang; J Sun; Z Xu; J Ma"}, {"ref_id": "b31", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "K He; X Zhang; S Ren; J Sun"}, {"ref_id": "b32", "title": "Flexisp: A flexible camera image processing framework", "journal": "ACM Transactions on Graphics", "year": "2014", "authors": "F Heide; M Steinberger; Y.-T Tsai; M Rouf; D Pajak; D Reddy; O Gallo; J Liu; W Heidrich; K Egiazarian"}, {"ref_id": "b33", "title": "Deep unfolding: Model-based inspiration of novel deep architectures", "journal": "", "year": "2014", "authors": "J R Hershey; J L Roux; F Weninger"}, {"ref_id": "b34", "title": "Efficient mr image reconstruction for compressed mr imaging", "journal": "Medical Image Analysis", "year": "2010", "authors": "J Huang; S Zhang; D Metaxas"}, {"ref_id": "b35", "title": "Learning to paint with model-based deep reinforcement learning", "journal": "", "year": "2019", "authors": "Z Huang; W Heng; S Zhou"}, {"ref_id": "b36", "title": "A plugand-play priors approach for solving nonlinear imaging inverse problems", "journal": "IEEE Signal Processing Letters", "year": "2017", "authors": "U S Kamilov; H Mansour; B Wohlberg"}, {"ref_id": "b37", "title": "Phase retrieval from noisy data based on sparse approximation of object phase and amplitude", "journal": "", "year": "2017", "authors": "V Katkovnik"}, {"ref_id": "b38", "title": "Noninvasive single-shot imaging through scattering layers and around corners via speckle correlations", "journal": "Nature Photonics", "year": "2014", "authors": "O Katz; P Heidmann; M Fink; S Gigan"}, {"ref_id": "b39", "title": "A method for stochastic optimization", "journal": "", "year": "2014", "authors": "D P Kingma; J Ba;  Adam"}, {"ref_id": "b40", "title": "Sparse representations for limited data tomography", "journal": "IEEE", "year": "2008", "authors": "H Y Liao; G Sapiro"}, {"ref_id": "b41", "title": "Continuous control with deep reinforcement learning. international conference on learning representations (ICLR)", "journal": "", "year": "2016", "authors": "T Lillicrap; J J Hunt; A Pritzel; N Heess; T Erez; Y Tassa; D Silver; D Wierstra"}, {"ref_id": "b42", "title": "Self-improving reactive agents based on reinforcement learning, planning and teaching", "journal": "", "year": "1992", "authors": "L Lin"}, {"ref_id": "b43", "title": "An efficient algorithm for compressed mr imaging using total variation and wavelets", "journal": "IEEE", "year": "2008", "authors": "S Ma; W Yin; Y Zhang; A Chakraborty"}, {"ref_id": "b44", "title": "Dolphin-dictionary learning for phase retrieval", "journal": "IEEE Transactions on Signal Processing", "year": "2016", "authors": "Julien Mairal; Andreas Tillmann; M Eldar; Yonina ; C "}, {"ref_id": "b45", "title": "Non-local sparse models for image restoration", "journal": "", "year": "2009", "authors": "J Mairal; F R Bach; J Ponce; G Sapiro; A Zisserman"}, {"ref_id": "b46", "title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "journal": "", "year": "2001", "authors": "D Martin; C Fowlkes; D Tal; J Malik"}, {"ref_id": "b47", "title": "Learning proximal operators: Using denoising networks for regularizing inverse imaging problems", "journal": "", "year": "2017-10", "authors": "T Meinhardt; M Moller; C Hazirbas; D Cremers"}, {"ref_id": "b48", "title": "Learned damp: Principled neural network based compressive image recovery", "journal": "", "year": "2017", "authors": "C Metzler; A Mousavi; R Baraniuk"}, {"ref_id": "b49", "title": "Robust phase retrieval with a flexible deep network", "journal": "", "year": "2018", "authors": "C Metzler; P Schniter; A Veeraraghavan"}, {"ref_id": "b50", "title": "Bm3d-prgamp: Compressive phase retrieval based on bm3d denoising", "journal": "", "year": "2016", "authors": "C A Metzler; A Maleki; R G Baraniuk"}, {"ref_id": "b51", "title": "From denoising to compressed sensing", "journal": "IEEE Transactions on Information Theory", "year": "2016", "authors": "C A Metzler; A Maleki; R G Baraniuk"}, {"ref_id": "b52", "title": "Coherent inverse scattering via transmission matrices: Efficient phase retrieval algorithms and a public dataset", "journal": "", "year": "2017", "authors": "C A Metzler; M K Sharma; S Nagesh; R G Baraniuk; O Cossairt; A Veeraraghavan"}, {"ref_id": "b53", "title": "Playing atari with deep reinforcement learning", "journal": "", "year": "2013", "authors": "V Mnih; K Kavukcuoglu; D Silver; A Graves; I Antonoglou; D Wierstra; M Riedmiller"}, {"ref_id": "b54", "title": "Human-level control through deep reinforcement learning", "journal": "Nature", "year": "2015", "authors": "V Mnih; K Kavukcuoglu; D Silver; A A Rusu; J Veness; M G Bellemare; A Graves; M Riedmiller; A K Fidjeland; G Ostrovski"}, {"ref_id": "b55", "title": "Primal-dual plug-and-play image restoration", "journal": "IEEE Signal Processing Letters", "year": "2017", "authors": "S Ono"}, {"ref_id": "b56", "title": "An iterative regularization method for total variation-based image restoration", "journal": "Multiscale Modeling and Simulation", "year": "2005", "authors": "S Osher; M Burger; D Goldfarb; J Xu; Yin ; W "}, {"ref_id": "b57", "title": "Proximal algorithms. Foundations and Trends\u00ae in Optimization", "journal": "", "year": "2014", "authors": "N Parikh; S Boyd"}, {"ref_id": "b58", "title": "Policy gradient methods for robotics. International Conference on Intelligent Robots and Systems (IROS)", "journal": "", "year": "2006", "authors": "J Peters; S Schaal"}, {"ref_id": "b59", "title": "Magnetic resonance image reconstruction from undersampled measurements using a patch-based nonlocal operator", "journal": "Medical Image Analysis", "year": "2014", "authors": "X Qu; Y Hou; F Lam; D Guo; J Zhong; Chen ; Z "}, {"ref_id": "b60", "title": "Regularization parameter selection for nonlinear iterative image restoration and mri reconstruction using gcv and sure-based methods", "journal": "IEEE Transactions on Image Processing", "year": "2012", "authors": "S Ramani; Z Liu; J Rosen; J.-F Nielsen; J A Fessler"}, {"ref_id": "b61", "title": "Mr image reconstruction from highly undersampled k-space data by dictionary learning", "journal": "IEEE Transactions on Medical Imaging", "year": "2010", "authors": "S Ravishankar; Y Bresler"}, {"ref_id": "b62", "title": "One network to solve them all -solving linear inverse problems using deep projection models", "journal": "", "year": "2017", "authors": "Rick Chang; J H Li; C.-L Poczos; B Vijaya Kumar; B V K Sankaranarayanan; A C "}, {"ref_id": "b63", "title": "The little engine that could: Regularization by denoising (red)", "journal": "SIAM Journal on Imaging Sciences", "year": "2017", "authors": "Y Romano; M Elad; P Milanfar"}, {"ref_id": "b64", "title": "Poisson inverse problems by the plug-and-play scheme", "journal": "Journal of Visual Communication and Image Representation", "year": "2016", "authors": "A Rond; R Giryes; M Elad"}, {"ref_id": "b65", "title": "U-net: Convolutional networks for biomedical image segmentation", "journal": "", "year": "2015", "authors": "O Ronneberger; P Fischer; T Brox"}, {"ref_id": "b66", "title": "Plug-and-play methods provably converge with properly trained denoisers", "journal": "", "year": "2019", "authors": "E Ryu; J Liu; S Wang; X Chen; Z Wang; Yin ; W "}, {"ref_id": "b67", "title": "Trust region policy optimization", "journal": "", "year": "2015", "authors": "J Schulman; S Levine; P Abbeel; M Jordan; P Moritz"}, {"ref_id": "b68", "title": "Tensor-based formulation and nuclear norm regularization for multienergy computed tomography", "journal": "IEEE Transactions on Image Processing", "year": "2014", "authors": "O Semerci; N Hao; M E Kilmer; E L Miller"}, {"ref_id": "b69", "title": "Deterministic policy gradient algorithms", "journal": "", "year": "2014", "authors": "D Silver; G Lever; N Heess; T Degris; D Wierstra; M Riedmiller"}, {"ref_id": "b70", "title": "Mastering the game of go with deep neural networks and tree search", "journal": "Nature", "year": "2016", "authors": "D Silver; A Huang; C J Maddison; A Guez; L Sifre; G Van Den Driessche; J Schrittwieser; I Antonoglou; V Panneershelvam; M Lanctot"}, {"ref_id": "b71", "title": "Plug-and-play priors for bright field electron tomography and sparse interpolation", "journal": "IEEE Transactions on Computational Imaging", "year": "2016", "authors": "S Sreehari; S V Venkatakrishnan; B Wohlberg; G T Buzzard; L F Drummy; J P Simmons; C A Bouman"}, {"ref_id": "b72", "title": "Multi-resolution data fusion for super-resolution electron microscopy", "journal": "", "year": "2017", "authors": "S Sreehari; S Venkatakrishnan; K L Bouman; J P Simmons; L F Drummy; C A Bouman"}, {"ref_id": "b73", "title": "An online plugand-play algorithm for regularized image reconstruction", "journal": "IEEE Transactions on Computational Imaging", "year": "2019", "authors": "Y Sun; B Wohlberg; U S Kamilov"}, {"ref_id": "b74", "title": "Regularized fourier ptychography using an online plug-and-play algorithm", "journal": "", "year": "2019", "authors": "Y Sun; S Xu; Y Li; L Tian; B Wohlberg; U S Kamilov"}, {"ref_id": "b75", "title": "Policy gradient methods for reinforcement learning with function approximation", "journal": "", "year": "2000", "authors": "R Sutton; D Mcallester; S Singh; Y Mansour"}, {"ref_id": "b76", "title": "Memnet: A persistent memory network for image restoration", "journal": "", "year": "2017-10", "authors": "Y Tai; J Yang; X Liu; C Xu"}, {"ref_id": "b77", "title": "Image restoration and reconstruction using variable splitting and class-adapted image priors", "journal": "", "year": "2016", "authors": "A M Teodoro; J M Bioucas-Dias; M A Figueiredo"}, {"ref_id": "b78", "title": "A convergent image fusion algorithm using scene-adapted gaussian-mixture-based denoising", "journal": "IEEE Transactions on Image Processing", "year": "2018", "authors": "A M Teodoro; J M Bioucas-Dias; M A Figueiredo"}, {"ref_id": "b79", "title": "Image restoration by iterative denoising and backward projections", "journal": "IEEE Transactions on Image Processing", "year": "2018", "authors": "T Tirer; R Giryes"}, {"ref_id": "b80", "title": "Plug-and-play priors for model based reconstruction", "journal": "", "year": "2013", "authors": "S V Venkatakrishnan; C A Bouman; B Wohlberg"}, {"ref_id": "b81", "title": "Proximal deep structured models", "journal": "", "year": "2016", "authors": "S Wang; S Fidler; R Urtasun"}, {"ref_id": "b82", "title": "Parameter-free plug-and-play admm for image restoration", "journal": "", "year": "2017", "authors": "X Wang; S H Chan"}, {"ref_id": "b83", "title": "Differentiable linearized admm", "journal": "", "year": "2019", "authors": "X Xie; J Wu; G Liu; Z Zhong; Lin ; Z "}, {"ref_id": "b84", "title": "A fast alternating direction method for tvl1-l2 signal reconstruction from partial fourier data", "journal": "IEEE Journal of Selected Topics in Signal Processing", "year": "2010", "authors": "J Yang; Y Zhang; Yin ; W "}, {"ref_id": "b85", "title": "Deep admm-net for compressive sensing mri", "journal": "", "year": "2016", "authors": "Y Yang; J Sun; H Li; Z Xu"}, {"ref_id": "b86", "title": "Crafting a toolchain for image restoration by deep reinforcement learning", "journal": "", "year": "2018", "authors": "K Yu; C Dong; L Lin; Change Loy; C "}, {"ref_id": "b87", "title": "Path-restore: Learning network path selection for image restoration", "journal": "", "year": "2019", "authors": "K Yu; X Wang; C Dong; X Tang; C C Loy"}, {"ref_id": "b88", "title": "Ista-net: Interpretable optimization-inspired deep network for image compressive sensing", "journal": "", "year": "2018", "authors": "J Zhang; B Ghanem"}, {"ref_id": "b89", "title": "Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising", "journal": "IEEE Transactions on Image Processing", "year": "2017", "authors": "K Zhang; W Zuo; Y Chen; D Meng; L Zhang"}, {"ref_id": "b90", "title": "Learning deep cnn denoiser prior for image restoration", "journal": "", "year": "2017", "authors": "K Zhang; W Zuo; S Gu; L Zhang"}, {"ref_id": "b91", "title": "Ffdnet: Toward a fast and flexible solution for cnn-based image denoising", "journal": "IEEE Transactions on Image Processing", "year": "2018", "authors": "K Zhang; W Zuo; L Zhang"}, {"ref_id": "b92", "title": "Deep plug-and-play super-resolution for arbitrary blur kernels", "journal": "", "year": "2019", "authors": "K Zhang; W Zuo; L Zhang"}, {"ref_id": "b93", "title": "Dynamically unfolding recurrent restorer: A moving endpoint control method for image restoration", "journal": "", "year": "2019", "authors": "X Zhang; Y Lu; J Liu; Dong ; B "}, {"ref_id": "b94", "title": "Wide-field, highresolution fourier ptychographic microscopy", "journal": "Nature Photonics", "year": "2013", "authors": "G Zheng; R Horstmeyer; Yang ; C "}, {"ref_id": "b95", "title": "From learning models of natural image patches to whole image restoration", "journal": "", "year": "2011", "authors": "D Zoran; Y Weiss"}], "figures": [{"figure_label": "4", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 4 .4Figure 4. Recovered images from noisy intensity-only CDP measurements with seven PR algorithms. (Details are better appreciated on screen.).", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Comparisons of different CNN-based denoisers: we show the results of (1) Gaussian denoising performance (PSNR) under noise level \u03c3 = 50; (2) the CS-MRI performance (PSNR) when plugged into the PnP-ADMM; (3) the GPU runtime (ms) of denoisers when processing an image with size 256 \u00d7 256.", "figure_data": "PerformanceDnCNN MemNet UNetDENOISING PERF.27.1827.3227.40PNP PERF.25.4325.6725.76TIMES8.0964.655.65"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Comparisons of different policies used in PnP-ADMM algorithm for CS-MRI on seven widely used medical images under various acceleration factors (x2/x4/x8) and noise level 15. We show both PSNR and the number of iterations (#IT.) used to induce results. * denotes to report the best PSNR over all iterations (i.e. with optimal early stopping). The best results are indicated by orange color and the second best results are denoted by blue color.", "figure_data": "\u00d72\u00d74\u00d78POLICIESPSNR #IT. PSNR #IT. PSNR #IT.handcrafted30.05 30.0 27.90 30.0 25.76 30.0handcrafted  *30.06 29.1 28.20 18.4 26.06 19.4fixed23.94 30.0 24.26 30.0 22.78 30.0fixed  *28.451.626.673.424.197.3fixed optimal30.02 30.0 28.27 30.0 26.08 16.7fixed optimal  *30.036.728.34 12.6 26.16 30.0oracle30.25 30.0 28.60 30.0 26.41 30.0oracle  *30.268.028.61 13.9 26.45 21.6model-free28.79 30.0 27.95 30.0 26.15 30.0Ours30.335.028.425.026.44 15.0"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Quantitative results (PSNR) of different CS-MRI methods on two datasets under various acceleration factors f and noise levels \u03c3n. The best results are indicated by orange color and the second best results are denoted by blue color.", "figure_data": "DATASETf\u03c3nTRADITIONAL RecPF FCSADEEP UNROLLING ADMMNet ISTANetBM3D-MRIPNP IRCNNOurs532.4631.7033.1034.5833.3334.6734.78\u00d721029.4828.3331.3731.8129.4431.8032.001527.0825.5229.1629.9926.9029.9630.27528.6728.2130.2431.3430.3331.3631.62Medical7\u00d741026.9826.6729.2029.7128.3029.5229.681525.5824.9327.8728.3826.6627.9428.43524.7224.6226.5727.6526.5327.3228.26\u00d781023.9424.0426.2126.9025.8126.4427.351523.1823.3625.4926.2325.0925.5326.41536.3934.9036.7438.1736.0038.4238.57\u00d721031.9530.1234.2034.8131.3934.9335.061528.9126.6831.4232.6528.4632.8133.09533.0532.3034.1535.4634.7935.8036.11MICCAI\u00d741030.2129.5632.5833.1331.6332.9933.071528.1326.9330.5531.4829.3530.9831.42528.3528.7130.3631.6231.3431.6632.64\u00d781026.8627.6829.7830.5429.8630.1630.8925.7026.3528.8329.5028.5328.7229.65Table 4. Quantitative results of different PR algorithms on fourCDP measurements and varying amount of Possion noise (large \u03b1indicates low sigma-to-noise ratio).\u03b1 = 9 \u03b1 = 27 \u03b1 = 81AlgorithmsPSNRPSNRPSNRHIO35.9625.7614.82WF34.4624.9615.76DOLPHIn29.9327.4519.35SPAR35.2031.8222.44BM3D-prGAMP 40.2532.8425.43prDeep39.7033.5426.82Ours40.3333.9027.23"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "2, 3, 4], where F can be represented by the 2D Fourier CS-MRI reconstruction results of different algorithms on medical images. (best view on screen with zoom).", "figure_data": "RecPFFCSAADMMNetISTANetBM3D-MRIIRCNNOursGroundTruth22.5722.2724.1524.6123.6424.1625.28PSNR18.7419.2320.4821.3720.6220.9122.02PSNR24.8924.4726.8527.9026.7227.7428.65PSNRFigure 3. HIOWFDOLPHInSPARBM3D-prGAMPprDeepOursGroundTruth14.4015.5219.3522.4825.6627.7228.01PSNR15.1016.2719.6222.5123.6124.5925.12PSNR"}], "formulas": [{"formula_id": "formula_0", "formula_text": "x\u2208R N D (x) + \u03bbR (x) , (1", "formula_coordinates": [1.0, 373.45, 259.49, 164.11, 15.21]}, {"formula_id": "formula_1", "formula_text": ")", "formula_coordinates": [1.0, 537.57, 259.81, 3.87, 8.64]}, {"formula_id": "formula_2", "formula_text": "Prox \u03c3 2 R (v) = argmin x R(x) + 1 2\u03c3 2 x \u2212 v 2 2 . (2)", "formula_coordinates": [1.0, 320.42, 513.33, 221.02, 22.81]}, {"formula_id": "formula_3", "formula_text": "x k+1 = Prox \u03c3 2 k R (z k \u2212 u k ) = H \u03c3 k (z k \u2212 u k ) ,(3)", "formula_coordinates": [1.0, 323.65, 639.12, 217.79, 12.36]}, {"formula_id": "formula_4", "formula_text": "z k+1 = Prox 1 \u00b5 k D (x k+1 + u k ) ,(4)", "formula_coordinates": [1.0, 323.65, 656.06, 217.79, 14.37]}, {"formula_id": "formula_5", "formula_text": "u k+1 = u k + x k+1 \u2212 z k+1 ,(5)", "formula_coordinates": [1.0, 323.65, 675.02, 217.79, 9.65]}, {"formula_id": "formula_6", "formula_text": "X STEP 1 Z STEP 1 U STEP 1 X STEP 2 Z STEP 2 U STEP 2 RECOVERED IMAGE 1 - \uf074 \uf073 0 \uf073 1 \uf073 0 \uf06d 1 \uf06d 1 - \uf074 \uf06d X STEP \uf074 POLICY AT STEP 1 0 x 0 u 0 z 0 2 a 2 a 2 a U", "formula_coordinates": [3.0, 106.45, 84.02, 181.89, 71.24]}, {"formula_id": "formula_7", "formula_text": "0 , \u00b5 0 , \u03c3 1 , \u00b5 1 , \u2022 \u2022 \u2022 , \u03c3 \u03c4 \u22121 , \u00b5 \u03c4 \u22121 )", "formula_coordinates": [4.0, 167.67, 239.53, 122.44, 9.65]}, {"formula_id": "formula_8", "formula_text": "T = {s 0 , a 0 , r 0 , \u2022 \u2022 \u2022 , s N , a N , r N }.", "formula_coordinates": [4.0, 111.77, 508.52, 141.22, 9.65]}, {"formula_id": "formula_9", "formula_text": "r \u03b3 t = N \u2212t t =0 \u03b3 t r(s t+t , a t+t ),(6)", "formula_coordinates": [4.0, 115.77, 550.76, 173.67, 30.47]}, {"formula_id": "formula_10", "formula_text": "J (\u03c0) = E s0\u223cS0,T \u223c\u03c0 [r \u03b3 0 ] ,(7)", "formula_coordinates": [4.0, 119.54, 661.61, 169.9, 13.56]}, {"formula_id": "formula_11", "formula_text": "V \u03c0 (s) = E T \u223c\u03c0 [r \u03b3 0 |s 0 = s] ,(8)", "formula_coordinates": [4.0, 354.75, 121.47, 186.69, 13.56]}, {"formula_id": "formula_12", "formula_text": "Q \u03c0 (s, a) = E T \u223c\u03c0 [r \u03b3 0 |s 0 = s, a 0 = a] .(9)", "formula_coordinates": [4.0, 345.2, 136.41, 196.24, 13.56]}, {"formula_id": "formula_13", "formula_text": "r(s t , a t ) = \u03b6(p(s t , a t )) \u2212 \u03b6(s t ) \u2212 \u03b7. (10", "formula_coordinates": [5.0, 98.64, 193.15, 186.66, 9.65]}, {"formula_id": "formula_14", "formula_text": ")", "formula_coordinates": [5.0, 285.29, 193.47, 4.15, 8.64]}, {"formula_id": "formula_15", "formula_text": "L \u03c6 = E s\u223cB,a\u223c\u03c0 \u03b8 (s) 1 2 (r(s, a) + \u03b3V \u03c0 \u03c6 (p(s, a)) \u2212 V \u03c0 \u03c6 (s)) 2 , (11", "formula_coordinates": [5.0, 55.44, 565.39, 247.74, 34.53]}, {"formula_id": "formula_16", "formula_text": ")", "formula_coordinates": [5.0, 285.29, 591.28, 4.15, 8.64]}, {"formula_id": "formula_17", "formula_text": "\u03b81 J(\u03c0 \u03b8 ) =E s\u223cB,a\u223c\u03c0 \u03b8 (s) [ \u03b81 log \u03c0 1 (a 1 |s) A \u03c0 (s, a)] .(12)", "formula_coordinates": [5.0, 322.31, 483.12, 219.13, 22.98]}, {"formula_id": "formula_18", "formula_text": "\u03b82 J(\u03c0 \u03b8 ) =E s\u223cB,a\u223c\u03c0 \u03b8 (s) [ a2 Q \u03c0 (s, a) \u03b82 \u03c0 2 (s)] ,(13)", "formula_coordinates": [5.0, 327.67, 575.78, 213.77, 22.98]}, {"formula_id": "formula_19", "formula_text": "A i = FD i , i \u2208 [1,", "formula_coordinates": [7.0, 307.44, 670.72, 234.0, 20.69]}], "doi": ""}