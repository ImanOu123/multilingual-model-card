{"title": "Modeling Mutual Context of Object and Human Pose in Human-Object Interaction Activities", "authors": "Bangpeng Yao; Li Fei-Fei", "pub_date": "", "abstract": "Detecting objects in cluttered scenes and estimating articulated human body parts are two challenging problems in computer vision. The difficulty is particularly pronounced in activities involving human-object interactions (e.g. playing tennis), where the relevant object tends to be small or only partially visible, and the human body parts are often self-occluded. We observe, however, that objects and human poses can serve as mutual context to each other -recognizing one facilitates the recognition of the other. In this paper we propose a new random field model to encode the mutual context of objects and human poses in human-object interaction activities. We then cast the model learning task as a structure learning problem, of which the structural connectivity between the object, the overall human pose, and different body parts are estimated through a structure search approach, and the parameters of the model are estimated by a new max-margin algorithm. On a sports data set of six classes of human-object interactions [12], we show that our mutual context model significantly outperforms state-of-theart in detecting very difficult objects and human poses.", "sections": [{"heading": "Introduction", "text": "Using context to aid visual recognition is recently receiving more and more attention. Psychology experiments show that context plays an important role in recognition in the human visual system [3,24]. In computer vision, context has been used in problems such as object detection and recognition [25,14,8], scene recognition [23], action classification [22], and segmentation [28]. While the idea of using context is clearly a good one, a curious observation shows that most of the context information has contributed relatively little to boost performances in recognition tasks. In the recent Pascal VOC challenge dataset [9], the difference between context based methods and sliding window based methods for object detection (e.g. detecting bicycles) is only within a small margin of 3 \u2212 4% [7,13].  One reason to account for the relatively small margin is, in our opinion, the lack of strong context. While it is nice to detect cars in the context of roads, powerful car detectors [20] can nevertheless detect cars with high accuracy whether they are on the road or not. Indeed, for the human visual system, detecting visual abnormality out of context is crucial for survival and social activities (e.g. detecting a cat in the fridge, or an unattended bag in the airport) [15].\nSo is context oversold? Our answer is 'no'. Many important visual recognition tasks rely critically on context. One such scenario is the problem of human pose estimation and object detection in human-object interaction (HOI) activities [12,32]. As shown in Fig. 1, without knowing that the human is making a defensive shot with the cricket bat, it is not easy to accurately estimate the player's pose (Fig. 1(a)); similarly, without seeing the player's pose, it is difficult to detect the small ball in the player's hand, which is nearly invisible even to the human eye (Fig. 1(b)).\nHowever, the two difficult tasks can benefit greatly from serving as context for each other, as shown in Fig. 1. The goal of this paper is to model the mutual context of objects and human poses in HOI activities so that each can facilitate the recognition of the other. Given a set of training images, our model automatically discovers the relevant poses for each type of HOI activity, and furthermore the connectivity and spatial relationships between the objects and body parts. We formulate this task as a structure learning problem, of which the connectivity is learned by a structure search approach, and the model parameters are discriminatively estimated by a novel max-margin approach. By modeling the mutual co-occurrence and spatial relations of objects and human poses, we show that our algorithm significantly improves the performance of both object detection and pose estimation on a dataset of sports images [12].\nThe rest of this paper is organized as follows. Sec.2 describes related work. Details of our model, as well as model learning and inference are elaborated in Sec.3, 4, and 5 respectively. Experimental results are given in Sec. 6.", "publication_ref": ["b2", "b23", "b24", "b13", "b7", "b22", "b21", "b27", "b8", "b6", "b12", "b19", "b14", "b11", "b31", "b11"], "figure_ref": ["fig_0", "fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Related work", "text": "The two central tasks, human pose estimation and object detection, have been studied in computer vision for many years. Most of the pose estimation work uses a tree structure of the human body [10,26,1] which allows fast inference. In order to capture more complex body articulations, some non-tree models have also been proposed [27,31]. Although those methods have been demonstrated to work well on the images with clean backgrounds, human pose estimation in cluttered scenes remains a challenging problem. Furthermore, to our knowledge, no existing method has explored context information for human pose estimation.\nSliding window is one of the most successful strategies for object detection. Some techniques have been proposed to avoid exhaustively searching the image [30,19], which makes the algorithm more efficient. While the most popular detectors are still based on sliding windows, more recent work has tried to integrate context to obtain better performance [25,14,8]. However, in most of the works the performance is improved by a relatively small margin.\nIt is out of the scope of this paper to develop an object detection or pose estimation method that generally applies to all situations. Instead, we focus on the role of context in these problems. Our work is inspired by a number of previous works that have used context in vision tasks [23,17,28,25,14,8,22]. In most of these works, one type of scene information serves as contextual facilitation to a main recognition problem. For example, ground planes and horizons can help to refine pedestrian detections. In this paper, we try to bridge the gap between two seemingly unrelated problems -object detection and human pose (a) The relevant objects that interact with the human may be very small, partially occluded, or tilted to an unusual angle.\n(b) Human poses of the same activity might be inconsistent in different images due to different camera angles (the left two images), or the way that the human interacts with the object (the right two images). estimation, in which the mutual contexts play key roles for understanding their interactions. The problem of classifying HOI activities has been studied in [12] and [32], but no detailed understanding of the human pose (e.g. parsing the body parts) is offered in these works. To our knowledge, our work is the first one that explicitly models the mutual contexts of human poses and objects and allows them to facilitate the recognition of each other.", "publication_ref": ["b9", "b25", "b0", "b26", "b30", "b29", "b18", "b24", "b13", "b7", "b22", "b16", "b27", "b24", "b13", "b7", "b21", "b11", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Modeling mutual context of object and pose", "text": "Given an HOI activity, our goal is to estimate the human pose and to detect the object that the human interacts with. Fig. 2 illustrates that both tasks are challenging. The relevant objects are often small, partially occluded, or tilted to an unusual angle by the human. The human poses, on the other hand, are usually highly articulated and many body parts are self-occluded. Furthermore, even in the same activity, the configurations of body parts might differ in different images due to different shooting angles or human poses.\nHere we propose a novel model to exploit the mutual context of human poses and objects in one coherent framework, where object detection and human pose estimation can benefit from each other. For simplicity, we assume that only one object is involved in each activity.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "The model", "text": "A graphical illustration of our model is shown in Fig. 3(a). Our model can be thought of as a hierarchical random field, where the overall activity class , object , and human pose all contribute to the recognition and detection of each other. The human pose is further decomposed into some body parts, denoted by { } =1 . For each body part and the object , and denote the visual features that describe the corresponding image regions respectively. Note that because of the difference between the human poses in each HOI activity (Fig. 2(b)), we allow each activity class ( ) to have more than one types of human pose ( ), which are latent (unobserved) variables to be learned in training.\nOur model encodes the mutual connections between the object, the human pose and the body parts. Intuitively speaking, this allows the model to capture important connections between, say, the tennis racket and the right arm that is serving the tennis ball (Fig. 3(b)). We observe, however, that the left leg in tennis serving is often less relevant to the detection of the ball. The model should therefore have the flexibility in deciding what parts of the body should be connected to the object and the overall pose . Dashed lines in Fig. 3(a) indicate that these connections will be decided through structure learning. Depending on , and , these connections might differ in different situations. Putting everything together, the overall model can be computed as \u03a8 = \u2211 , where is an edge of the model, and are its potential function and weight respectively. We now enumerate the potentials of this model: ), and ( , ) model the agreement between the class labels of , , and , each estimated by counting the co-occurrence frequencies of the pair of variables on training images.\n\u2022 ( , ), (,", "publication_ref": [], "figure_ref": ["fig_2", "fig_1", "fig_2", "fig_2"], "table_ref": []}, {"heading": "\u2022", "text": "( , ) models the spatial relationship between the object and the body part , which is computed by\nbin(l \u2212 l ) \u22c5 bin( \u2212 ) \u22c5 ( / )(1)\nwhere (l, , ) is the position, orientation, and scale of an image part. bin(\u22c5) is a binning function as in [26] and (\u22c5) is a Gaussian distribution.\n\u2022 ( , ) models the spatial relationship between different body parts, computed similarly to Eq.1.", "publication_ref": ["b25"], "figure_ref": [], "table_ref": []}, {"heading": "\u2022", "text": "( , ) models the compatibility between the pose class and a body part . It is computed by considering the spatial layout of given a reference point in the image, in this case the center of the human face ( 1 ).\n( , ) = bin(l \u2212 l 1 ) \u22c5 bin( ) \u22c5 ( )(2)\n\u2022 ( , ) and ( , ) model the dependence of the object and a body part with their corresponding image evidence. We use the shape context [2] feature for image representation, and train a detector [30] for each body part and each object in each activity. Detection outputs are normalized as in [1].\nIn our algorithm, all the above potential functions are dependent on and except those between , , and (the first bullet). We omit writing this point every time for space consideration. For example, for different human pose , ( , ) is estimated with different parameters, which represents a specific spatial configuration between and the object , conditioned on the particular human pose . ", "publication_ref": ["b1", "b29", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Properties of the model", "text": "Central to our model formulation is the hypothesis that both human pose estimation and object detection can benefit from each other in HOI activities. Without knowing the location of the arm, it is difficult to spot the location of the tennis racket in tennis serving. Without seeing the croquet mallet, the heavily occluded arms and legs can become too obscured for robust pose estimation. We highlight here some important properties of our model. Co-occurrence context for the activity class, object, and human pose. Given the presence of a tennis racket, the human pose is more likely to be playing tennis instead of playing croquet. That is to say, co-occurrence information can be beneficial for coherently modeling the object, the human pose, and the activity class. Multiple types of human poses for each activity. Our model allows each activity ( ) to consist of more than one human pose ( ). Treating as a hidden variable, our model automatically discovers the possible poses from training images. This gives us more flexibility to deal with the situations where the human poses in the same activity are inconsistent, as shown in Fig. 2(b). We show in Fig. 4 the pose variability for each HOI activity. Spatial context between object and body parts. Different poses imply that the object is handled by the human in different manners, which are modeled by { ( , )} =1 . Furthermore, not all these relationships are critical for understanding an HOI activity. Therefore for each combination of and , our algorithm automatically discovers the connectivity between and each , as well as the connectivity among and { } =1 . Relations with the other models. Our model has drawn inspirations from a number of previous works, such as mod-  eling spatial layout of different image parts [10,26,1], using agreement of different image components [25], using multiple models to describe the same concept (human pose in our problem) [21], non-tree models for better pose estimation [31,4], and discriminative training [1]. Our model integrates all the properties in one coherent framework to perform two seemingly different tasks, human pose estimation and object detection, to the benefit of each other.", "publication_ref": ["b9", "b25", "b0", "b24", "b20", "b30", "b3", "b0"], "figure_ref": ["fig_1", "fig_3"], "table_ref": []}, {"heading": "Model learning", "text": "Given the training images of HOI activities with labeled objects and body parts, the learning step needs to achieve two goals: structure learning to discover the hidden human poses and the connectivity among the object, human pose, and body parts; and parameter estimation for the potential weights to maximize the discrimination between different activities. The output of our learning method is a set of models, each representing one connectivity pattern and potential weights for one type of human pose in one activity class. Algorithm 1 is a sketch of the overall framework. We discover new human poses by clustering the samples in the model that has the weakest discriminative ability in each iteration, which results to some sub-classes. Structure learning is applied to each sub-class respectively. The learning process terminates when the number of mis-classified samples in each sub-class is small (less than three in this paper).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Hill-climbing structure learning", "text": "Our algorithm performs structure learning for each subclass, i.e. each pose in each activity, respectively. Given the images of a sub-class which are obtained from clustering (see Algorithm 1), our objective is to learn a connectivity pattern between the object, the human pose, and the body parts (the dashed lines in Fig. 3(a)). Here we omit the edges between , , and because they do not affect the structure learning results.\nWe use a hill-climbing approach with tabu list [18] to search the structure space. The hill-climbing approach adds or removes edges one at a time until a maximum is reached. During the search procedure, we keep a tabu list of history operations to guarantee that those operations will not be reversed. As in [14], we include a Gaussian prior over the number of edges to avoid overfitting. Furthermore, in order to reduce the impact of local maxima, we randomly initialize the structure for three times and apply the hill-climbing approach to each one, from which the best result is selected. Please find more details of our structure learning approach in the supplementary document of this paper.", "publication_ref": ["b17", "b13"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Max-margin parameter estimation", "text": "Given the model outputs by the structure learning step, the parameter estimation step aims to obtain a set of po-tential weights that maximize the discrimination between different classes of activities ( in Fig. 3(a)). But unlike the traditional random field parameter estimation setting [29], in our model each class can contain more than one pose ( ), which can be thought of as multiple sub-classes. Our learning algorithm needs to, therefore, estimate parameters for each pose (i.e. sub-class) while optimizing for maximum discrimination among the global activity classes.\nWe propose a novel max-margin learning approach to tackle this problem. Let (x , , ( )) be a training sample, where x is a data point, is the sub-class label of x , and ( ) maps to a class label. We want to find a function \u2131 that assigns an instance x to a sub-class. We say that x is correctly classified if and only if (\u2131(x )) = ( ). Our classifier is then formulated as \u2131(x ) = arg max {w \u22c5x }, where w is a weight vector for the -th sub-class. Inspired by the traditional max-margin learning problems [5], we introduce a slack variable for each sample x , and optimize the following objective function:\nmin w, 1 2 \u2211 \u2225w \u2225 2 2 + \u2211 (3) subject to: \u2200 , \u2265 0 \u2200 , where ( ) \u2215 = ( ), w \u22c5 x \u2212 w \u22c5 x \u2265 1 \u2212\nwhere \u2225w \u2225 2 is the L2 norm of w , is a normalization constant. Again, note that the weights are defined with respect to sub-classes while the classification results are measured with respect to classes. We optimize Eq.3 by using the multiplier method [16]. Mapping the above symbols to our model, x are the potential function values computed on an image. Potential values for the disconnected edges are set to 0. In order to obtain better discrimination among different classes, we compute the potential values of an image on the models of all the sub-classes, and concatenate these values to form the feature vector. Sub-class variable indicates human pose , and ( ) is the class label . Please refer to the supplemental document of this paper for more detail about the method.", "publication_ref": ["b28", "b4", "b15"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Analysis of our learning algorithm", "text": "Fig. 4 illustrates the two models (correspond to two types of human poses) learned by our algorithm for each HOI class. We can see the big difference of human poses in some activities (e.g. croquet-shot and tennis-serve), and such wide intra-class variability can be effectively captured by our algorithm. In these cases, using only one human pose for each HOI class is not enough to characterize well all the images in this class. Furthermore, we observe that by using structure learning, our model can learn meaningful connectivity between the object and the body parts, e.g. croquet mallet and legs, right forehand and tennis racket.\n\" \" \n'LVFULPLQDWLYH SDUW GHWHFWRUV +HDG 7RUVR 7HQQLV UDFNHW 9ROOH\\EDOO \" 7HQQLV IRUHKDQG PRGHOV 9ROOH\\EDOO VPDVK PRGHOV \" \" \u00b0\u00ae\u00b0( ) \u03ed \u03ed \u03ed \u0355 \u0355 \u0102\u018c\u0150\u0175\u0102\u01c6 \u0355 \u0355 \u0355 K , K , K , / = \u03a8 ( ) \u0355 \u0355 \u0102\u018c\u0150\u0175\u0102\u01c6 \u0355 \u0355 \u0355 < < < K , K , K , / = \u03a8 ( ) \u0355 \u0102\u018c\u0150\u0175\u0102\u01c6 \u0355 \u0355 \u0355 \u016c \u016c \u016c \u016c , K , / * * = \u03a8", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Model inference, object detection, and human pose estimation", "text": "Given a new testing image \u2110, our objective is to estimate the pose of the human in the image, and to detect the object that is interacting with the human. An illustration of the inference procedure is shown in Fig. 5. In order to detect the tennis racket in this image, we maximize the likelihood of this image given the models that are learned for tennis-forehand. This is achieved by finding a best configuration of human body parts and the object (tennis racket) in the image, which is denoted as max , \u03a8( , , , \u2110) in Fig. 5. In order to estimate the human pose, we compute max , \u03a8( , , , \u2110) for each activity class and find the class * that corresponds to the maximum likelihood score. This score can be used to measure the confidence of activity classification as well as human pose estimation.\nFor each model, the above inference procedure involves a step to find the best spatial configuration of the object and different body parts for an image. We solve this problem by using the compositional inference method [4]. The inference algorithm contains a bottom-up stage to make proposals for the image parts. Each bottom-up step is followed by a top-down stage to validate the proposals. More details of the compositional inference method can be found in the supplementary document of this paper.", "publication_ref": ["b3"], "figure_ref": ["fig_5", "fig_5"], "table_ref": []}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The sports dataset", "text": "We evaluate our approach on a known HOI dataset of six activity classes [12]: cricket-defensive shot (player\n5HFDOO 3UHFLVLRQ 2XU 0HWKRG 3HGHVWULDQ DV FRQWH[W 6FDQQLQJ ZLQGRZ GHWHFWRU 5HFDOO 3UHFLVLRQ 2XU 0HWKRG 3HGHVWULDQ DV FRQWH[W 6FDQQLQJ ZLQGRZ GHWHFWRU 5HFDOO 3UHFLVLRQ 2XU 0HWKRG 3HGHVWULDQ DV FRQWH[W 6FDQQLQJ ZLQGRZ GHWHFWRU D &ULFNHW %DW F &URTXHW 0DOOHW H 9ROOH\\EDOO 5HFDOO 3UHFLVLRQ 2XU 0HWKRG 3HGHVWULDQ DV FRQWH[W 6FDQQLQJ ZLQGRZ GHWHFWRU 5HFDOO 3UHFLVLRQ 2XU 0HWKRG 3HGHVWULDQ DV FRQWH[W 6FDQQLQJ ZLQGRZ GHWHFWRU E &ULFNHW %DOO G 7HQQLV 5DFNHW Figure 6\n. Object detection results measured by precisionrecall curves. We compare our algorithm to a scanning window detector and a detector that uses pedestrian detection as the human context for object detection. and cricket bat), cricket-bowling (player and cricket ball), croquet-shot (player and croquet mallet), tennis-forehand (player and tennis racket), tennis-serve (player and tennis racket), and volleyball-smash (player and volleyball). There are 50 images in each activity class. We use the same setting as that in [12]: 30 images for training and 20 for testing. In [12] only activity classification results were reported. In this work we also evaluate our method on the tasks of object detection and human pose estimation.", "publication_ref": ["b11", "b11", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Better object detection by pose context", "text": "In this experiment, our goal is to detect the presence and location of the object given an HOI activity. To evaluate the effectiveness of our model, we compare our results with two control experiments: a scanning window detector as a baseline measure of object detection without any context, and a second experiment in which the approximate location of the person is provided by a pedestrian detector [6], hence providing only a co-occurrence context and a very weak location context. Results of these three experiments, measured by precision-recall curves, are shown in Fig. 6. The curves of our algorithm are obtained by considering the scores \u03a8( , , , \u2110) of all the results that are proposed by the compositional inference method. To ensure fair comparison, all experiments use the same input features and object detectors described in Sec.3, and non-max suppression is applied equally to all methods. The results in Fig. 6 show that our detection method achieves the best performance. By using human pose as context, more detailed spatial relationship between different image parts can be discovered, which greatly helps to detect objects that are traditionally very difficult. For example, in the case of the cricket ball (Fig. 6(b)), a sliding window method yields an average precision of 17%, whereas our model with pose-context measure is 46%. In almost all the cases of the five objects, the average precision score of our method is more than 3 times as the sliding window method. Fig. 7 shows an example of using the three methods for object detection. Fig. 9 shows more object detection results on a variety of testing images.", "publication_ref": ["b5"], "figure_ref": ["fig_6", "fig_7"], "table_ref": []}, {"heading": "Better pose estimation by object context", "text": "Similarly to object detection, we show in this experiment that human pose estimation is significantly improved by object context. Here we compare our full model with four different control experiments.\n\u2022 An iterative parsing method by Ramanan et al [26];\n\u2022 A state-of-the-art pictorial structure model [1];\n\u2022 We re-train the model in [1] with a pictorial structure model per class for better modeling of each class; \u2022 Our proposed model by imposing only one sub-class (human pose, ) per HOI activity, examining the importance of allowing a flexible number of pose models to account for the intra-activity variability.\nAll of the models are trained using the same training data described in Sec.6.1. Following the convention proposed in [11], a body part is considered correctly localized if the endpoints of its segment lie within 50% of the ground-truth segment length from their true positions. Experimental results are shown in Table 1. The percentage correctness tells us that pose estimation still remains a difficult problem. No method offers a solution near 100%. Our full model significantly outperforms the other approaches, even showing a 10% average improvement over a class-based, discrimi-Table 1. Pose estimation results by our full model and four comparison methods for all testing images. The average part detection percent correctness and standard deviation over 6 HOI classes are presented for each body part. If two numbers are reported in one cell, the left one indicates the left body part and right one indicates the right body part. The best result for each body part is marked in bold font. natively trained pictorial structure model. Furthermore, we can see that allowing multiple poses for each activity class proves to be useful for improving pose estimation accuracy. More sample results are shown in Fig. 9, where we visualize the pose estimation results by comparing our model with the state-of-the-art pictorial structure model by [1]. We show that given the object context, poses estimated by our model are less prone to errors that result in strange looking body gestures (e.g. horizontal legs in Fig. 9(d)), or a completely wrong location (e.g. nearly all body parts landed on the background in Fig. 9(h)).", "publication_ref": ["b25", "b0", "b0", "b10", "b0"], "figure_ref": ["fig_7", "fig_7", "fig_7"], "table_ref": []}, {"heading": "Combining object and pose for HOI activity classification", "text": "As shown in Fig. 5, by inferring the human pose and object in the image, our model gives a prediction of the class label of the human-object interaction. We compare our method with the results reported in [12], and use a bagof-words representation with a linear SVM classifier as the baseline. The results are shown in Fig. 8.\nFig. 8 shows that our model significantly outperforms the bag-of-words method and performs slightly better than [12]. Note that the method in [12] uses predominantly the background scene context (e.g. appearance differences in sport courts), which turns out to be highly discriminative among most of these classes of activities. Our method, on the other hand, focuses on the core problem of human-object interactions. It is therefore less data set dependent. Figure 8. Activity recognition accuracy of different methods: our model, Gupta et al [12], and bagof-words.", "publication_ref": ["b11", "b11", "b11", "b11"], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Conclusion", "text": "In this work, we treat object and human pose as the context of each other in different HOI activity classes. We develop a random field model that uses a structure learning method to learn important connectivity patterns between objects and human body parts. Experiments show that our model significantly outperforms other state-of-the-art methods in both problems. Our model can be further improved in a number of directions. For example, inspired by [23,12], we can incorporate useful background scene context to facilitate the recognition of foreground objects and activities. Improving the model to deal with more than one object is also one of the directions of our future research.  ", "publication_ref": ["b22", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgement. This research is partially supported by an NSF CAREER grant (IIS-0845230), a Google research award, and a Microsoft Research Fellowship to L.F-F. We also would like to thank Hao Su for helpful comments.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Pictorial structures revisited: People detection and articulated pose estimation", "journal": "", "year": "2008", "authors": "M Andriluka; S Roth; B Schiele"}, {"ref_id": "b1", "title": "Shape matching and object recognition using shape contexts", "journal": "IEEE T. Pattern Anal", "year": "2002", "authors": "S Belongie; J Malik; J Puzicha"}, {"ref_id": "b2", "title": "Scene perception: detecting and judging objects undergoing relational violations", "journal": "Cognitive Psychol", "year": "1982", "authors": "I Biederman; R Mezzanotte; J Rabinowitz"}, {"ref_id": "b3", "title": "Rapid inference on a novel AND/OR graph for object detection, segmentation and parsing", "journal": "", "year": "2007", "authors": "Y Chen; L Zhu; C Lin; A Yuille; H Zhang"}, {"ref_id": "b4", "title": "On the algorithmic implementation of multiclass kernel-based vector machines", "journal": "J. Mach. Learn. Res", "year": "2001", "authors": "K Crammer; Y Singer"}, {"ref_id": "b5", "title": "Histograms of oriented gradients for human detection", "journal": "", "year": "2005", "authors": "N Dalal; B Triggs"}, {"ref_id": "b6", "title": "Discriminative models for multi-class object layout", "journal": "", "year": "2009", "authors": "C Desai; D Ramanan; C Fowlkes"}, {"ref_id": "b7", "title": "An empirical study of context in object detection", "journal": "", "year": "2009", "authors": "S Divvala; D Hoiem; J Hays; A Efros; M Hebert"}, {"ref_id": "b8", "title": "The PASCAL VOC2008 Results", "journal": "", "year": "", "authors": "M Everingham; L V Gool; C Williams; J Winn; A Zisserman"}, {"ref_id": "b9", "title": "Pictorial structures for object recognition", "journal": "Int. J. Comput. Vision", "year": "2005", "authors": "P Felzenszwalb; D Huttenlocher"}, {"ref_id": "b10", "title": "Progressive search space reduction for human pose estimation", "journal": "", "year": "2008", "authors": "V Ferrari; M Mar\u00edn-Jim\u00e9nez; A Zisserman"}, {"ref_id": "b11", "title": "Observing human-object interactions: Using spatial and functional compatibility for recognition", "journal": "IEEE T. Pattern Anal", "year": "2007", "authors": "A Gupta; A Kembhavi; L Davis"}, {"ref_id": "b12", "title": "Combining efficient object localization and image classification", "journal": "", "year": "2009", "authors": "H Harzallah; F Jurie; C Schmid"}, {"ref_id": "b13", "title": "Learning spatial context: using stuff to find things", "journal": "", "year": "2004", "authors": "G Heitz; D Koller"}, {"ref_id": "b14", "title": "Human gaze control during real-world scene perception", "journal": "Trends Cogn. Sci", "year": "2003", "authors": "J Henderson"}, {"ref_id": "b15", "title": "Multiplier and gradient methods", "journal": "J. Optimiz. Theory App", "year": "1969", "authors": "M Hestenes"}, {"ref_id": "b16", "title": "Putting objects in perspective", "journal": "", "year": "2006", "authors": "D Hoiem; A Efros; M Hebert"}, {"ref_id": "b17", "title": "Probabilistic Graphical Models: Principles and Techniques", "journal": "MIT Press", "year": "2009", "authors": "D Koller; N Friedman"}, {"ref_id": "b18", "title": "Beyond sliding windows: object localization by efficient subwindow search", "journal": "", "year": "2008", "authors": "C Lampert; M Blaschko; T Hofmann"}, {"ref_id": "b19", "title": "Combined object categorization and segmentation with an implicit shape model", "journal": "", "year": "2004", "authors": "B Leibe; A Leonardis; B Schiele"}, {"ref_id": "b20", "title": "Viewpoint-independent object class detection using 3D feature maps", "journal": "", "year": "2008", "authors": "J Liebelt; C Schmid; K Schertler"}, {"ref_id": "b21", "title": "Actions in context", "journal": "", "year": "2002", "authors": "M Marszalek; I Laptev; C Schmid"}, {"ref_id": "b22", "title": "Using the forest to see the trees: a graphical model relating features, objects, and scenes", "journal": "", "year": "2003", "authors": "K Murphy; A Torralba; W Freeman"}, {"ref_id": "b23", "title": "The role of context in object recognition", "journal": "Trends Cogn. Sci", "year": "2007", "authors": "A Oliva; A Torralba"}, {"ref_id": "b24", "title": "Objects in context", "journal": "", "year": "2004", "authors": "A Rabinovich; A Vedaldi; C Galleguillos; E Wiewiora; S Belongie"}, {"ref_id": "b25", "title": "Learning to parse images of articulated objects", "journal": "", "year": "2006", "authors": "D Ramanan"}, {"ref_id": "b26", "title": "Recovering human body configurations using pairwise constraints between parts", "journal": "", "year": "2005", "authors": "X Ren; A Berg; J Malik"}, {"ref_id": "b27", "title": "TextonBoost: joint appearance, shape and context modeling for multi-class object recognition and segmentation", "journal": "", "year": "2002", "authors": "J Shotton; J Winn; C Rother; A Criminisi"}, {"ref_id": "b28", "title": "Discriminative probabilistic models for relational data", "journal": "", "year": "2002", "authors": "B Taskar; P Abbeel; D Koller"}, {"ref_id": "b29", "title": "Robust real-time object detection", "journal": "Int. J. Comput. Vision", "year": "2001", "authors": "P Viola; M Jones"}, {"ref_id": "b30", "title": "Multiple tree models for occlusion and spatial constraints in human pose estimation", "journal": "", "year": "2008", "authors": "Y Wang; G Mori"}, {"ref_id": "b31", "title": "Grouplet: A structured image representation for recognizing human and object interactions", "journal": "", "year": "2010", "authors": "B Yao; L Fei-Fei"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. Objects and human poses can serve as mutual context to facilitate the recognition of each other. In (a), the human pose is better estimated by seeing the cricket bat, from which we can have a strong prior of the pose of the human. In (b), the cricket ball is detected by understanding the human pose of throwing the ball.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 .2Figure 2. Challenges of both object detection and human pose estimation in HOI activities.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 .3Figure 3. (a) A graphical illustration of our model. The edges represented by dashed lines indicate that their connectivity will be obtained by structure learning. denotes an HOI activity class, the human pose class, a body part, and the object. and 's are image appearance information of and respectively. (b) Illustration of our model on an image of a human playing tennis. Different types of potentials are denoted by lines with different colors. Line widths represent the importance of the potentials for the human-object interaction of playing tennis.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 .4Figure 4. Visualization of the learned HOI models. Each row shows two models (due to two types of poses) and their corresponding image examples for one activity. The illustrative figure for each model represents the average spatial layout of the object and body parts of all the images that are assigned to the model. Shaded half-transparent body parts indicate that the structure learning algorithm has assigned strong connectivity to these parts with respect to the object. The different color codes are: object = double red box, head and torso = magenta, arms = green, legs = cyan.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "1 :1Hill-climbing structure learning for each activity class. foreach Iteration do -Model parameter estimation by max-margin learning; -Choose the model with the largest number of mis-classified images; -Cluster the images in the selected model into two sub-classes; -Structure learning for the two new sub-classes; end Algorithm The learning framework. Each sub-class corresponds to a type of human pose in an HOI activity. Initially there are one sub-class for each activity.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 .5Figure 5. The framework of our inference method. Given an input image \u2110, the inference results are: (1) object detection results (e.g. 1 is the tennis racket detection result); (2) human pose estimation result * ; (3) activity classification result * .", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 7 .7Figure 7. Object (cricket bat) detection results (red double-line bounding boxes) obtained by: a sliding window detector (left), the same detector using pedestrian detection as context (middle), and our method (right). Pedestrian detection is shown in a blue bounding box. And the human pose estimation results are shown in colored rectangles in the right image.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 9 .9Figure 9. Example testing results of object detection and pose estimation. Each sub-figure contains one testing image, tested on the following four conditions: upper-left\u2192object detection by our model, lower-left\u2192object detection by a scanning window, upper-right\u2192pose estimation by our model, and lower-right\u2192pose estimation by the state-of-the-art pictorial structure method in [1]. Detected objects are shown in double-line red bounding boxes. The color codes for different body parts are: head and torso -magenta, arms -green, legs -cyan. (This figure is best viewed in color.)", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u2022 ( , ), (,", "formula_coordinates": [3.0, 52.06, 318.91, 80.02, 10.41]}, {"formula_id": "formula_1", "formula_text": "bin(l \u2212 l ) \u22c5 bin( \u2212 ) \u22c5 ( / )(1)", "formula_coordinates": [3.0, 77.82, 399.78, 208.55, 9.96]}, {"formula_id": "formula_2", "formula_text": "( , ) = bin(l \u2212 l 1 ) \u22c5 bin( ) \u22c5 ( )(2)", "formula_coordinates": [3.0, 78.98, 537.05, 207.39, 11.21]}, {"formula_id": "formula_3", "formula_text": "min w, 1 2 \u2211 \u2225w \u2225 2 2 + \u2211 (3) subject to: \u2200 , \u2265 0 \u2200 , where ( ) \u2215 = ( ), w \u22c5 x \u2212 w \u22c5 x \u2265 1 \u2212", "formula_coordinates": [5.0, 56.1, 309.76, 230.27, 59.47]}, {"formula_id": "formula_4", "formula_text": "'LVFULPLQDWLYH SDUW GHWHFWRUV +HDG 7RUVR 7HQQLV UDFNHW 9ROOH\\EDOO \" 7HQQLV IRUHKDQG PRGHOV 9ROOH\\EDOO VPDVK PRGHOV \" \" \u00b0\u00ae\u00b0( ) \u03ed \u03ed \u03ed \u0355 \u0355 \u0102\u018c\u0150\u0175\u0102\u01c6 \u0355 \u0355 \u0355 K , K , K , / = \u03a8 ( ) \u0355 \u0355 \u0102\u018c\u0150\u0175\u0102\u01c6 \u0355 \u0355 \u0355 < < < K , K , K , / = \u03a8 ( ) \u0355 \u0102\u018c\u0150\u0175\u0102\u01c6 \u0355 \u0355 \u0355 \u016c \u016c \u016c \u016c , K , / * * = \u03a8", "formula_coordinates": [5.0, 309.13, 81.25, 235.13, 176.8]}, {"formula_id": "formula_5", "formula_text": "5HFDOO 3UHFLVLRQ 2XU 0HWKRG 3HGHVWULDQ DV FRQWH[W 6FDQQLQJ ZLQGRZ GHWHFWRU 5HFDOO 3UHFLVLRQ 2XU 0HWKRG 3HGHVWULDQ DV FRQWH[W 6FDQQLQJ ZLQGRZ GHWHFWRU 5HFDOO 3UHFLVLRQ 2XU 0HWKRG 3HGHVWULDQ DV FRQWH[W 6FDQQLQJ ZLQGRZ GHWHFWRU D &ULFNHW %DW F &URTXHW 0DOOHW H 9ROOH\\EDOO 5HFDOO 3UHFLVLRQ 2XU 0HWKRG 3HGHVWULDQ DV FRQWH[W 6FDQQLQJ ZLQGRZ GHWHFWRU 5HFDOO 3UHFLVLRQ 2XU 0HWKRG 3HGHVWULDQ DV FRQWH[W 6FDQQLQJ ZLQGRZ GHWHFWRU E &ULFNHW %DOO G 7HQQLV 5DFNHW Figure 6", "formula_coordinates": [6.0, 55.92, 72.69, 219.35, 291.11]}], "doi": ""}