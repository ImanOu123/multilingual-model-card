{"title": "Is Supervised Syntactic Parsing Beneficial for Language Understanding Tasks? An Empirical Investigation", "authors": "Goran Glava\u0161; Ivan Vuli\u0107", "pub_date": "", "abstract": "Traditional NLP has long held (supervised) syntactic parsing necessary for successful higher-level semantic language understanding (LU). The recent advent of end-to-end neural models, self-supervised via language modeling (LM), and their success on a wide range of LU tasks, however, questions this belief. In this work, we empirically investigate the usefulness of supervised parsing for semantic LU in the context of LM-pretrained transformer networks. Relying on the established fine-tuning paradigm, we first couple a pretrained transformer with a biaffine parsing head, aiming to infuse explicit syntactic knowledge from Universal Dependencies treebanks into the transformer. We then fine-tune the model for LU tasks and measure the effect of the intermediate parsing training (IPT) on downstream LU task performance. Results from both monolingual English and zero-shot language transfer experiments (with intermediate target-language parsing) show that explicit formalized syntax, injected into transformers through IPT, has very limited and inconsistent effect on downstream LU performance. Our results, coupled with our analysis of transformers' representation spaces before and after intermediate parsing, make a significant step towards providing answers to an essential question: how (un)availing is supervised parsing for high-level semantic natural language understanding in the era of large neural models? 1 Disclaimer 1: In this work, we make a clear distinction between Computational Linguistics (CL), i.e., the area of linguistics leveraging computational methods for analyses of human languages and NLP, the area of artificial intelligence tackling human language in order to perform intelligent tasks. This work scrutinizes the usefulness of supervised parsing and explicit syntax only for the latter. We find the usefulness of explicit syntax in CL to be self-evident. 2 Disclaimer 2: The purpose of this work is definitely not to invalidate the admirable efforts on syntactic annotation and modeling, but rather to make an empirically driven step towards a deeper understanding of the relationship between LU and formalised syntactic knowledge, and the extent of its impact to modern semantic LU and applications.", "sections": [{"heading": "Introduction", "text": "Structural analysis of sentences, based on a variety of syntactic formalisms (Charniak, 1996;Taylor et al., 2003;De Marneffe et al., 2006;Hockenmaier and Steedman, 2007;Nivre et al., 2016, inter alia), has been the beating heart of NLP pipelines for decades (Klein and Manning, 2003;Chen and Manning, 2014;Dozat and Manning, 2017;Kondratyuk and Straka, 2019), establishing rather strong common belief that high-level semantic language understanding (LU) crucially depends on explicit syntax. The unprecedented success of neural language learning models based on transformer networks (Vaswani et al., 2017), trained on unlabeled corpora via language modeling (LM) objectives (Devlin et al., 2019;Liu et al., 2019b;Clark et al., 2020, inter alia) on a wide variety of LU tasks (Wang et al., 2018;Hu et al., 2020), however, questions this widely accepted assumption.\nThe question of necessity of supervised parsing for LU and NLP in general has been raised before. More than a decade ago, Bod (2007) questioned the superiority of supervised parsing over unsupervised induction of syntactic structures in the context of statistical machine translation. Nonetheless, the NLP community has since still managed to find sufficient evidence for the usefulness of explicit syntax in higher-level LU tasks (Levy and Goldberg, 2014;Cheng and Kartsaklis, 2015;Bastings et al., 2017;Kasai et al., 2019;Zhang et al., 2019a, inter alia). However, we believe that the massive improvements brought about by the LM-pretrained transformers -unexposed to any explicit syntactic signal -warrant a renewed scrutiny of the utility of supervised parsing for high-level language understanding. 1,2 The research question we address in this work can be summarized as follows:\n(RQ) Is explicit structural language information, provided in the form of a widely adopted syntactic formalism (Universal Dependencies, UD) (Nivre et al., 2016) and injected in a supervised manner into LM-pretrained transformers beneficial for transformers' downstream LU performance?\nWhile existing body of work (Lin et al., 2019;Liu et al., 2019a;Kulmizev et al., 2020;Chi et al., 2020) probes transformers for structural phenomena, our work is more pragmatically motivated. We directly evaluate the effect of infusing structural language information from UD treebanks, via intermediate dependency parsing (DP) training, on transformers' performance in downstream LU. To this end, we couple a pretrained transformer with a biaffine parser similar to Dozat and Manning (2017), and train the model (i.e., fine-tune the transformer) for DP. Our parser on top of RoBERTa (Liu et al., 2019b) and XLM-R (Conneau et al., 2020) produces DP results which are comparable to state of the art. We then fine-tune the syntactically-informed transformers for three downstream LU tasks: natural language inference (NLI) Conneau et al., 2018), paraphrase identification (Zhang et al., 2019b;Yang et al., 2019), and causal commonsense reasoning (Sap et al., 2019;Ponti et al., 2020). We quantify the contribution of explicit syntax by comparing LU performance of the transformer exposed to intermediate parsing training (IPT) and its counterpart directly fine-tuned for the downstream task. We investigate the effects of IPT (1) monolingually, by fine-tuning English transformers, BERT and RoBERTa, on an English UD treebank and for (2) downstream zero-shot language transfer, by fine-tuning massively multilingual transformers (MMTs) -mBERT and XLM-R (Conneau et al., 2020) -on treebanks of downstream target languages, before the downstream fine-tuning on source language (English) data.\nWhile intermediate parsing training is obviously not the only way of bringing syntactic knowledge to downstream tasks (Kuncoro et al., 2019;Swayamdipta et al., 2019;Kuncoro et al., 2020), it is arguably the most straightforward way of injecting syntactic signal in the context of the predominant pretraining-fine-tuning paradigm that has, nonetheless, not been investigated up to this point. Other methods of bringing syntactic signal to downstream tasks such as knowledge distillation (Kuncoro et al., 2020) and pre-training on shallow trees instead of sequences (Swayamdipta et al., 2019) have failed to demonstrate significant gains on higher-level LU tasks.\nOur results also render supervised UD parsing largely inconsequential to LU. We observe limited and inconsistent gains only in zero-shot downstream language transfer: further analyses reveal that (1) intermediate LM training yields comparable gains and (2) IPT only marginally changes representation spaces of transformers exposed to sufficient amount of language data in LM-pretraining. We hope that these empirical findings will shed new light on the relationship between supervised parsing (and manually labeled treebanks) and LU with transformer networks, and guide further similar investigations in future work, in order to fully understand the impact of formal syntactic knowledge on LU performance with modern neural architectures.", "publication_ref": ["b2", "b44", "b9", "b16", "b34", "b22", "b3", "b11", "b23", "b47", "b10", "b31", "b6", "b50", "b18", "b1", "b28", "b4", "b0", "b19", "b34", "b29", "b30", "b25", "b5", "b11", "b31", "b7", "b8", "b55", "b52", "b42", "b39", "b7", "b26", "b43", "b27", "b27", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Bringing Explicit Syntax to LMs. Previous work has attempted to enrich language models with explicit syntactic knowledge in ways other than intermediate parsing training. Swayamdipta et al. (2019) modify the pretraining objective of ELMo (Peters et al., 2018) to learn from shallowly parsed (i.e., chunked) corpora. They, however, report no notable improvements on downstream tasks. Kuncoro et al. (2019) propose to distil the knowledge from a Recurrent NN Grammar (RNNG) teacher trained on a small syntactically annotated corpus (by modeling the joint probability of surface sequence and phrase structure tree) into an LSTMbased student pretrained on a much larger corpus. They show that distillation helps the student in structured prediction tasks, but their downstream evaluation does not involve LU tasks. Their subsequent work (Kuncoro et al., 2020) replaces the RNN student with BERT (Devlin et al., 2019): syntactic distillation again helps structured prediction, but hurts (slightly) the performance on LU tasks from the GLUE benchmark (Wang et al., 2018).\nTransformer-Based Dependency Parsing. Building on the success of preceding neural parsers (Chen and Manning, 2014;Kiperwasser and Goldberg, 2016), Dozat and Manning (2017) proposed a biaffine parsing head on top of a Bi-LSTM encoder: contextualized word vectors are fed to two feedforward networks, producing dependent-and headspecific token representations, respectively. Arc and relation scores are produced via biaffine prod-ucts between these dependent-and head-specific representation matrices. Finally, the Edmonds algorithm induces the optimal tree from pairwise arc predictions. Most recent DP work (Kondratyuk and Straka, 2019;\u00dcst\u00fcn et al., 2020) replaces the Bi-LSTM encoder with multilingual BERT's transformer, reporting state-of-the-art parsing performance. Kondratyuk and Straka (2019) fine-tune mBERTs parameters on the concatenation of all UD treebanks, whereas\u00dcst\u00fcn et al. (2020) freeze the original transformer's parameters and inject adapters (Houlsby et al., 2019) for parsing.\nWe propose and work with a simpler transformerbased biaffine parser: we apply biaffine attention directly on representations from transformer's output layer, eliminating the head-and dependendantbased feed-forward mapping. Despite this simplification, our biaffine parser produces DP results comparable to current state-of-the-art parsers.\nSyntactic BERTology. The substantial body of syntactic probing work shows that BERT (Devlin et al., 2019) (a) encodes text in a hierarchical manner (i.e., it encodes some implicit underlying syntax) (Lin et al., 2019); and (b) captures specific shallow syntactic information (parts-of-speech and syntactic chunks) Liu et al., 2019a). Hewitt and Manning (2019) find that linear transformations, when applied on BERT's contextualized word vectors, reflect distances in dependency trees. This suggests that BERT encodes sufficient structural information to reconstruct dependency trees (though without arc directionality and relations). Chi et al. (2020) extend the analysis to multilingual BERT, finding that its representation subspaces may recover trees also for other languages. They also provide evidence that clusters of head-dependency pairs roughly correspond to UD relations. Similarly, Kulmizev et al. (2020) show that BERT's latent syntax corresponds more to UD trees than to shallower SUD (Gerdes et al., 2018) structures. Despite the evident similarity between BERT's latent syntax and formalisms such as UD, there is ample evidence that BERT insufficiently leverages syntax in downstream tasks: it often produces similar predictions for syntactically valid as well as for structurally corrupt sentences (e.g., with random word order) (Wallace et al., 2019;Ettinger, 2020;Zhao et al., 2020  training is a transfer learning setup in which one trains an LM-pretrained transformer on one or more supervised tasks (ideally with large training sets) before final fine-tuning for the target task. Phang et al. (2018) show that intermediate NLI training of BERT on the Multi-NLI dataset  benefits several language understanding tasks. Subsequent work Pruksachatkun et al., 2020) ", "publication_ref": ["b43", "b36", "b26", "b27", "b10", "b50", "b3", "b21", "b11", "b23", "b23", "b17", "b10", "b29", "b30", "b15", "b5", "b25", "b13", "b48", "b12", "b56", "b38", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "Biaffine Parser. Our parsing model, illustrated in Figure 1, consists of a biaffine attention layer applied directly on the transformer's output (BERT, RoBERTa, mBERT, or XLM-R). We first obtain word-level vectors by averaging transformed representations of their constituent subwords, produced by the transformer. Let X \u2208 R N \u00d7H denote the encoding of a sentence with N word-level tokens, consisting of N H-dimensional vectors (where H is the transformer's hidden size). We use the transformed representation of the sentence start token (e.g., [CLS] for BERT), x CLS \u2208 R H , as the representation for the root node of the parse tree, and prepend it to X, X = [x CLS ; X] \u2208 R (N +1)\u00d7H . We then use X as the representation of syntactic dependants and X as the representation of dependency heads. We then directly compute the arc and relation scores as biaffine products of X and X :\nYarc = XWarcX + Barc; Y rel = XW rel X + B rel\nwhere W arc \u2208 R H\u00d7H and W rel \u2208 R H\u00d7H\u00d7R denote, respectively, the arc classification matrix and relation classification tensor (with R as the number of relations); B arc and B rel denote the corresponding bias parameters. We greedily select the dependency head for each word by finding the maximal score in each row of Y arc : while this is not guaranteed to produce a tree, Zhang et al. (2017) show that in most cases it does. 3 Our arc prediction loss is the cross-entropy loss with sentence words (plus the root node) as categorical labels: this implies a different number of labels for different sentences. We compute the relation prediction loss as a cross-entropy loss over gold arcs. Our final loss is the sum of the arc loss and relation loss.\nNote that, in comparison with the original biaffine parser (Dozat and Manning, 2017) and its other transformer-based variants (Kondratyuk and Straka, 2019;\u00dcst\u00fcn et al., 2020), we feed wordlevel representations derived from the transformer's output directly to biaffine products, omitting the dependent-and head-specific MLP transformations. Deep task-specific architectures go against the finetuning idea: deep transformers have plenty of their own parameters that can be tuned for DP. We want to propagate as much of the explicit syntactic knowledge as possible into the transformer: a deep(er) DP-specific architecture on top of the transformer would impede the propagation of this knowledge to the transformer's parameters. Downstream Models. After IPT, we fine-tune transformers for two types of LU tasks: (1) sequence classification (SEQC) tasks, where a sequence of text needs to be assigned a discrete label; and (2) multiple choice classification (MCC) tasks where we need to select the correct answer between two or more options for a given a premise and/or question. For SEQC, we simply apply a softmax classifier on the transformed representation of the sequence start token: y = softmax (x CLS W sc + b sc ) (with W sc \u2208 R H\u00d7C and b sc \u2208 R C as classifier's parameters and C as the number of task's labels).\nFor MCC tasks, we first concatenate each of the offered answer choices (independently of each other) to the premise and/or question, and encode it with the transformer. Since some of these tasks, e.g., COPA (Roemmele et al., 2011;Ponti et al., 2020), have very small training sets, we would like to support model transfer between different MCC tasks. Different multiple-choice classification tasks, however, may differ in the number of choices: a classifier with the number of parameters depending on the number of labels is thus not a good fit; instead, we follow Sap et al. (2019) and Ponti et al. (2020), and couple the transformer with a feed-forward network outputting a single scalar for each answer. Let x i CLS \u2208 R H be the representation of the sequence start token (i.e., [CLS] or <s>) for the concatenation of the premise/question and the i-th answer. We obtain the score for the i-th answer as follows:\nyi = W o mcc tanh W h mcc x i CLS + b h mcc with W h mcc \u2208 R H\u00d7H , b h mcc \u2208 R H and W o mcc \u2208 R 1\u00d7H as parameters.\nWe then apply a softmax function on the concatenation of y i scores of all answers: y = softmax([y 1 , . . . , y K ]), with K as the number of answers (i.e., labels) in the task. Finally, we compute the cross-entropy loss on y.", "publication_ref": ["b54", "b11", "b23", "b41", "b39", "b42", "b39"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Experimental Setup", "text": "We now detail experimental setup, where LU finetuning follows Intermediate Parsing Training (IPT).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sequential Fine-Tuning", "text": "Our primary goal is to identify if injection of explicit syntax into transformers via supervised parsing training improves their downstream LU performance -this translates into sequential fine-tuning:\n(1) we first attach a biaffine parser from \u00a73 on the transformer and train the whole model on a UD treebank; (2) we then couple the syntactically-informed transformer with the corresponding downstream classification head and perform final fine-tuning. We then compare the downstream performance of transformers with and without the IPT step.\nMono-vs. Cross-Lingual IPT Experiments. In the monolingual setup, we work with English (EN ) transformers, BERT and RoBERTa, pretrained on EN corpora. In the zero-shot language transfer setup, where we work with multilingual models, mBERT and XLM-R (Conneau et al., 2020), we first train transformers via IPT on the UD treebank of the target language (i.e., a language with no downstream training data) before fine-tuning it on the EN training set of the LU task. We experiment with four target languages: German (DE ), French (FR ), Turkish (TR ), and Chinese (ZH ). 4 Standard vs. Adapter-Based Fine-Tuning. Standard fine-tuning updates all transformer's parameters, which, for tasks with large training sets may have some drawbacks: (i) fine-tuning may last long and (ii) task-specific information may overwrite the useful distributional knowledge obtained during LM-pretraining. Adapter-based fine-tuning (Houlsby et al., 2019;Pfeiffer et al., 2020) remedies for these potential issues by keeping the original transformer's parameters frozen and inserting new adapter parameters in transformer layers. In finetuning, both sets of parameters are used to make predictions, but we only update adapters based on loss gradients. As the number of adapter parameters is only a fraction of the number of original parameters (3-8%), fine-tuning is also much faster.\nTherefore, to account for the possibility of forgetting distributional knowledge in standard IPT fine-tuning, we also carry out adapter-based IPT. We follow Houlsby et al. (2019) and inject two bottleneck adapters into each transformer layer: first after the multi-head attention sublayer and another after the feed-forward sublayer. In downstream LU tasks, however, we unfreeze the original transformer parameters and fine-tune them together with adapters (now containing syntactic knowledge).", "publication_ref": ["b7", "b17", "b37", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Language Understanding Tasks", "text": "We now outline the downstream LU tasks. For brevity, we report all the technical training and optimization details in the Supplementary Material.\nNLI is a ternary sentence-pair classification task. We predict if the hypothesis is entailed by the premise, contradicts it, or neither. For monolingual EN experiments, we use Multi-NLI . In zero-shot transfer experiments, we train on EN Multi-NLI and evaluate on target language (DE , FR , TR , ZH ) test portions of the multilingual XNLI dataset (Conneau et al., 2018). Models trained on the Multi-NLI datasets have been shown, however, to capture certain heuristics (e.g., lexical overlap) useful for many training instances rather than more complex and generalizable language inference (McCoy et al., 2020 (Zhang et al., 2019b). In zero-shot language transfer, we evaluate on the test DE , FR , and ZH portions of the PAWS-X dataset (Yang et al., 2019).\nCommonsense Reasoning. We evaluate on two multiple-choice classification (MCC) datasets. In monolingual evaluation, we use the SocialIQA (SIQA) dataset (Sap et al., 2019), testing models' ability to reason about social interactions. Each SIQA instance consists of a premise, a question, and three possible answers. For zero-shot language transfer experiments, we resort to the recently published XCOPA dataset (Ponti et al., 2020), obtained by translating test portions of the EN COPA (Choice of Plausible Alternatives) dataset (Roemmele et al., 2011) to 11 languages. As mentioned, (X)COPA is an MCC task, with each instance containing a premise, a question, 5 and two possible answers. Due to the very limited size of the EN COPA training set (mere 400 instances), we follow Ponti et al.\n(2020) and evaluate the models fine-tuned on SIQA (EN ) on the XCOPA test portions (in TR and ZH).", "publication_ref": ["b8", "b32", "b55", "b52", "b42", "b39", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Training and Optimization Details", "text": "All the transformer models with which we experiment -EN BERT, mBERT, EN RoBERTa, and XLM-R have L = 12 layers and hidden representations of size H = 768. We apply a dropout (p = 0.1) on the transformer outputs before forwarding them to the task-specific classification heads (i.e., biaffine parsing head in intermediate parsing training, and MCC or SEQC heads in downstream fine-tuning). We optimize the parameters using the Adam algorithm (Kingma and Ba, 2015): we found the initial learning rate of 10   SIQA we train in batches of size 8, whereas on Multi-NLI and PAWS we train in batches of size 32. In Adapter-based IPT, we set the adapter size to 64 and use GELU (Hendrycks and Gimpel, 2016) as the activation function in adapter layers.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "We first discuss parsing performance of our novel biaffine parser (see \u00a73). We then show transformers' downstream LU performance after IPT, both in monolingual EN setting and in zero-shot transfer.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results and Discussion", "text": "Parsing Performance. In order to judge the benefits of IPT in downstream LU, we must first verify parsing performance of our biaffine parser, i.e., that we successfully fine-tune transformers for DP.\nTable 1 shows that our biaffine parser gives stateof-the-art performance for all five languages in our study. Our (m)BERT-based parser outperforms UDify (Kondratyuk and Straka, 2019), also based on mBERT, for EN , FR , and TR , and performs comparably for ZH . 7 Our parser based on XLM-R additionally yields an improvement over UDify for DE as well. It is worth noting that UDify trains the mBERT-based parser (1) on the concatenation of all UD treebanks and that it (2) additionally exploits gold UPOS and lemma annotations. We train our parsers only on the training portion of the respective treebank without using any additional morphosyntactic information. 8 Our mBERT-based parser outperforms our XLM-R-based parser only for ZH : this is likely due to a tokenization mismatch between XLM-R's subword tokenization for ZH and gold tokenization in the ZH -GSD treebank. 9\nMonolingual EN Results.  for XLM-R, the improvements are less consistent and less pronounced. This might be due to XLM-R's larger capacity which makes it less susceptible to the \"curse of multilinguality\" (Conneau et al., 2020): with the subword vocabulary twice as large as mBERT's, XLM-R is able to store more language-specific information. Also, XLM-R has seen substantially more target language data in LMpretraining than mBERT for each language. This might mean that the larger IPT gains for mBERT come from mere exposure to additional target language text rather than from injection of explicit syntactic UD signal (see further analyses in \u00a75.2).", "publication_ref": ["b23", "b7"], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Further Analysis and Discussion", "text": "We first compare the impact of IPT with the effect of additional LM training on the same raw data. We then quantify the topological modification that IPT makes in transformers' representation spaces.\nExplicit Syntax or Just More Language Data?\nWe scrutinize the IPT gains that we observe in some zero-shot language transfer experiments. We hypothesize that these gains may, at least in part, be credited to transformer simply seeing more target language data. former to supervised parsing, i.e., to the injection of explicit UD knowledge.\nILMT Details. We mask 15% of subword tokens in each sentence and predict them with a linear classifier applied on transformed representations of [MASK] tokens. We compute the cross-entropy loss and use the same hyperparameter configuration as described in \u00a74.3. The development set, used for early stopping, is subdued to fixed masking, whereas we mask the training sentences dynamically, before feeding them to the transformer.\nResults. We run this analysis for setups in which we observe substantial gains from IPT: PAWS-X for mBERT (Adapter fine-tuning, for FR and ZH ) and XCOPA for mBERT (Standard fine-tuning, TR and ZH ). The comparison between IPT and ILMT for these setups is provided in Figure 2. Like IPT, ILMT on mBERT generates downstream gains over direct downstream fine-tuning (i.e., no intermediate training) in all four setups. The gains from ILMT (with the exception of XCOPA for ZH ) are almost as large as gains from IPT. This suggests that most of the gain with IPT comes from seeing more target language text, and prevents us from concluding that the explicit syntactic annotation is BERT (EN) responsible for the LU improvements in zero-shot downstream transfer. This interpretation is corroborated by the fact that IPT gains roughly correlate with the amount of language-specific data seen in LM-pretraining: the gains are more prominent for mBERT than for XLM-R and for TR and ZH than for FR and DE (see Table 3).\nRoBERTa\nChanges in Representation Spaces. . We analyze the representations in each transformer layer separately: we represent each sentence s \u2208 S with the average of subword vectors from that layer (excluding sequence start and end tokens). Let X 1 and X 2 \u2208 R |S|\u00d7H contain corresponding representations of sentences from S from the i-th layer of two transformer variants (e.g., B and P). We measure the topological similarity of the i-th layers of the two transformers with the linear centered kernel alignment (l-CKA) (Kornblith et al., 2019\n): 11 l-CKA(X1, X2) = X 2 X1 2 F X 1 X1 F X 2 X2 F .\nAlthough not invariant to all linear transformations, l-CKA is invariant to orthogonal projection and isotropic scaling, which suffices for our purposes. We base our analysis on the following assumption:\nthe extent of change in transformers' representation space topology (reflected by l-CKA), is proportional to the novelty of knowledge injected in fine-tuning. Put differently, injection of new (i.e., missing) knowledge should substantially change the topology of the space (low l-CKA score).\nFigure 3 shows the heatmap of l-CKA scores for pairs of BERT and RoBERTa variants, for layers L8-L12. 12 Comparing B-P and B-N reveals that IPT changes the topology of BERT's higher layers roughly as much as NLI fine-tuning does, implying that both the English UD treebank (EN -EWT) and Multi-NLI data contain a non-negligible amount of novel knowledge for BERT. However, the direct N-P comparison shows that IPT and NLI enrich BERT (also RoBERTa) with different type of knowledge, i.e., they change the representation spaces of its layers in different ways. This suggests that the transformers cannot acquire the missing knowledge needed for NLI from IPT (i.e., from EN -EWT), and explains why IPT is not effective for NLI.\nIPT (comparison B-P) injects more new information than ILMT (comparison B-M), and this is more pronounced for BERT than for RoBERTa. IPT and ILMT change RoBERTa's parameters much less than BERT's (see B-M and B-P l-CKA scores for L11/L12), which we interpret as additional evidence, besides RoBERTa consistently outscoring BERT, that RoBERTa encodes richer language representations, due to its larger-scale and longer training. It also agrees with suggestions that BERT is \"undertrained\" for its capacity (Liu et al., 2019b).\nVery high B-P (and B-AP) l-CKA scores in lower layers suggest that the explicit syntactic knowledge from human-curated treebanks is redundant w.r. mizev et al., 2020) showing (some) correspondence between structural knowledge of (m)BERT and UD syntax. Finally, we observe highest l-CKA scores in the P-AP column, suggesting that Standard and Adapter IPT inject roughly the same syntactic information, despite different fine-tuning mechanisms.\nFigure 4 illustrates the results of the same analysis for language transfer experiments, for DE and TR (scores for FR and ZH are in the Appendix). The effects of ILMT and IPT (B-M, B-P/B-AP) for DE and TR with mBERT and XLM-R resemble those for EN with BERT and RoBERTa: ILMT changes transformers less than IPT. The amount of new syntactic knowledge IPT injects is larger (l-CKA scores are lower) than for EN , especially for XLM-R (vs. RoBERTa for EN ). We believe that it reflects the relative under-representation of the target language in the model's multilingual pretraining corpus (e.g., for TR): this leads to poorer representations of target language structure by mBERT and XLM-R compared to BERT's and RoBERTa's representation of EN structure. This gives us two seemingly conflicting empirical findings: (a) IPT appears to inject a fair amount of target-language UD syntax, but (b) this translates to (mostly) insignificant and inconsistent gains in language transfer in LU tasks (especially so for XLM-R, cf. Table 3). A plausible reconciling hypothesis is that there is a substantial mismatch between the type of structural information we obtain through supervised (UD) parsing and the type of structural knowledge beneficial for LU tasks. If true, this hypothesis would render supervised parsing rather unavailing for high-level language understanding, at least in the context of LM-pretrained transformers, the current state of the art in NLP. This warrants further investigation, and we hope that our work will inspire further discussion and additional studies.", "publication_ref": ["b24", "b31"], "figure_ref": ["fig_1", "fig_3"], "table_ref": ["tab_7", "tab_7"]}, {"heading": "Conclusion", "text": "We thoroughly examined the effects of leveraging formalized syntactic structures (UD) in state-of-theart neural language models (e.g., RoBERTa, XLM-R) for downstream language understanding (LU) tasks, both in monolingual and language transfer settings. The key results, obtained through intermediate parsing training (IPT) based on a state-of-theart-level dependency parser, indicate that explicit syntax, at least in our extensive experiments, provides negligible impact on LU tasks.\nBesides offering extensive empirical evidence of the mismatch between explicit syntax and improved LU performance with state-of-the-art transformers, this study sheds new light on some fundamental questions such as the one in the title. Similar to word embeddings (Mikolov et al., 2013) removing sparse lexical features from the NLP horizon, will transformers make supervised parsing obsolete for LU applications or not? More dramatically, in the words of Rens Bod (2007): \"Is the end of supervised parsing in sight\" for semantic LU tasks? 13 ", "publication_ref": ["b33", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "A Reproducibility", "text": "We first provide details on where to obtain datasets and code used in this work.  6 we provide links to language understanding datasets used in our study.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_13"]}, {"heading": "A.1 Datasets", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Code and Dependencies", "text": "We make our code available at: https: //github.com/codogogo/parse_stilt. Our code is built on top of the HuggingFace Transformers framework: https://github. com/huggingface/transformers (v. 2.7). Table 4 details the LM-pretrained transformer models from this framework which we exploited in this work. Besides the Transformers library, our code only relies on standard Python's scientific computing libraries (e.g., numpy).", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_11"]}, {"heading": "B ZH Tokenization: XLM-R vs. GSD", "text": "A word-level token from the parse tree normally corresponds to one or more transformer's subword tokens: we thus average subword vectors to obtain word vectors for biaffine parsing. For XLM-R and the ZH GSD treebank, however, a single XLM-R's subword token often corresponds to two treebank tokens. E.g., the sequence \"\u53ea\u662f\u4e8c\u9078\u4e00\u505a\u6c7a\u64c7\" with treebank tokenization ['\u53ea', '\u662f', '\u4e8c', '\u9078', '\u4e00', '\u505a', '\u6c7a\u64c7'] is tokenized as ['\u53ea\u662f', '\u4e8c', '\u9078', '\u4e00', '\u505a', '\u6c7a', '\u64c7'] by XLM-R. Two treebank tokens, '\u53ea' and '\u662f', are captured with a single XLM-R \"subword\" token, '\u53ea\u662f'. To ensure that each XLM-R subword token corresponds to exactly one treebank token, we inject spaces between treebank tokens before XLM-R tokenization: we then obtain the subword tokenization ['\u53ea', '\u662f', '\u4e8c', '\u9078', '\u4e00', '\u505a', '\u6c7a, '\u64c7']. However, this is suboptimal for XLM-R: its representations of tokens '\u53ea' and '\u662f' are probably less reliable than that of the '\u53ea\u662f' token. We believe this is why mBERT (without tokenization mismatches for ZH ) outperforms XLM-R in ZH parsing.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Complete Topology Analysis Results", "text": "Finally, we show the complete results (for all layers, all transformers, and all languages covered in our experiments) of our topological analysis of transformers' representations before and after different fine-tuning steps. Figure 5 shows the analysis results for monolingual EN transformers, BERT and RoBERTa. Figure 6 and Figure 7 show the results for multilingual transformers, mBERT and XLM-R, respectively, for all four target languages included in our experiments: DE , FR , TR , and ZH .     ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers (especially R2!) for the exceptionally meaningful and helpful comments. Goran Glava\u0161 is supported by the Baden W\u00fcrttemberg Stiftung (Eliteprogramm, AGREE grant). The work of Ivan Vuli\u0107 is supported by the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no. 648909).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Graph convolutional encoders for syntax-aware neural machine translation", "journal": "", "year": "2017", "authors": "Jasmijn Bastings; Ivan Titov; Wilker Aziz; Diego Marcheggiani; Khalil Sima'an"}, {"ref_id": "b1", "title": "Is the end of supervised parsing in sight?", "journal": "", "year": "2007", "authors": "Rens Bod"}, {"ref_id": "b2", "title": "Tree-bank grammars", "journal": "", "year": "1996", "authors": "Eugene Charniak"}, {"ref_id": "b3", "title": "A fast and accurate dependency parser using neural networks", "journal": "", "year": "2014", "authors": "Danqi Chen; D Christopher;  Manning"}, {"ref_id": "b4", "title": "Syntaxaware multi-sense word embeddings for deep compositional models of meaning", "journal": "", "year": "2015", "authors": "Jianpeng Cheng; Dimitri Kartsaklis"}, {"ref_id": "b5", "title": "Finding universal grammatical relations in multilingual BERT", "journal": "", "year": "2020", "authors": "A Ethan; John Chi; Christopher D Hewitt;  Manning"}, {"ref_id": "b6", "title": "ELECTRA: Pretraining text encoders as discriminators rather than generators", "journal": "", "year": "2020", "authors": "Kevin Clark; Minh-Thang Luong; V Quoc; Christopher D Le;  Manning"}, {"ref_id": "b7", "title": "Unsupervised cross-lingual representation learning at scale", "journal": "", "year": "2020", "authors": "Alexis Conneau; Kartikay Khandelwal; Naman Goyal; Vishrav Chaudhary; Guillaume Wenzek; Francisco Guzm\u00e1n; Edouard Grave; Myle Ott; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b8", "title": "XNLI: Evaluating cross-lingual sentence representations", "journal": "", "year": "2018", "authors": "Alexis Conneau; Ruty Rinott; Guillaume Lample; Adina Williams; Samuel R Bowman; Holger Schwenk; Veselin Stoyanov"}, {"ref_id": "b9", "title": "Generating typed dependency parses from phrase structure parses", "journal": "", "year": "2006", "authors": "Marie-Catherine De Marneffe; Bill Maccartney; D Christopher;  Manning"}, {"ref_id": "b10", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b11", "title": "Deep biaffine attention for neural dependency parsing", "journal": "", "year": "2017", "authors": "Timothy Dozat; D Christopher;  Manning"}, {"ref_id": "b12", "title": "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models", "journal": "TACL", "year": "2020", "authors": "Allyson Ettinger"}, {"ref_id": "b13", "title": "Sud or surface-syntactic universal dependencies: An annotation scheme nearisomorphic to UD", "journal": "", "year": "2018", "authors": "Kim Gerdes; Bruno Guillaume; Sylvain Kahane; Guy Perrier"}, {"ref_id": "b14", "title": "Gaussian error linear units (GELUs). CoRR", "journal": "", "year": "2016", "authors": "Dan Hendrycks; Kevin Gimpel"}, {"ref_id": "b15", "title": "A structural probe for finding syntax in word representations", "journal": "", "year": "2019", "authors": "John Hewitt; D Christopher;  Manning"}, {"ref_id": "b16", "title": "CCGbank: A corpus of CCG derivations and dependency structures extracted from the penn treebank", "journal": "Computational Linguistics", "year": "2007", "authors": "Julia Hockenmaier; Mark Steedman"}, {"ref_id": "b17", "title": "Parameter-efficient transfer learning for NLP", "journal": "", "year": "2019", "authors": "Neil Houlsby; Andrei Giurgiu; Stanislaw Jastrzebski; Bruna Morrone; Quentin De Laroussilhe; Andrea Gesmundo; Mona Attariyan; Sylvain Gelly"}, {"ref_id": "b18", "title": "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalization", "journal": "", "year": "2020", "authors": "Junjie Hu; Sebastian Ruder; Aditya Siddhant; Graham Neubig; Orhan Firat; Melvin Johnson"}, {"ref_id": "b19", "title": "Syntax-aware neural semantic role labeling with supertags", "journal": "", "year": "2019", "authors": "Jungo Kasai; Dan Friedman; Robert Frank; Dragomir Radev; Owen Rambow"}, {"ref_id": "b20", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b21", "title": "Simple and accurate dependency parsing using bidirectional lstm feature representations", "journal": "TACL", "year": "2016", "authors": "Eliyahu Kiperwasser; Yoav Goldberg"}, {"ref_id": "b22", "title": "Accurate unlexicalized parsing", "journal": "", "year": "2003", "authors": "Dan Klein; Christopher D Manning"}, {"ref_id": "b23", "title": "75 languages, 1 model: Parsing Universal Dependencies universally", "journal": "", "year": "2019", "authors": "Dan Kondratyuk; Milan Straka"}, {"ref_id": "b24", "title": "Similarity of neural network representations revisited", "journal": "", "year": "2019", "authors": "Simon Kornblith; Mohammad Norouzi; Honglak Lee; Geoffrey Hinton"}, {"ref_id": "b25", "title": "Do neural language models show preferences for syntactic formalisms?", "journal": "", "year": "2020", "authors": "Artur Kulmizev; Vinit Ravishankar; Mostafa Abdou; Joakim Nivre"}, {"ref_id": "b26", "title": "Scalable syntaxaware language models using knowledge distillation", "journal": "", "year": "2019", "authors": "Adhiguna Kuncoro; Chris Dyer; Laura Rimell; Stephen Clark; Phil Blunsom"}, {"ref_id": "b27", "title": "Syntactic structure distillation pretraining for bidirectional encoders", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "Adhiguna Kuncoro; Lingpeng Kong; Daniel Fried; Dani Yogatama; Laura Rimell; Chris Dyer; Phil Blunsom"}, {"ref_id": "b28", "title": "Dependencybased word embeddings", "journal": "", "year": "2014", "authors": "Omer Levy; Yoav Goldberg"}, {"ref_id": "b29", "title": "Open sesame: Getting inside bert's linguistic knowledge", "journal": "", "year": "2019", "authors": "Yongjie Lin; Yi Chern Tan; Robert Frank"}, {"ref_id": "b30", "title": "Linguistic knowledge and transferability of contextual representations", "journal": "", "year": "2019", "authors": "F Nelson; Matt Liu; Yonatan Gardner;  Belinkov; E Matthew; Noah A Peters;  Smith"}, {"ref_id": "b31", "title": "RoBERTa: A robustly optimized BERT pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b32", "title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference", "journal": "", "year": "2020", "authors": "Thomas Mccoy; Ellie Pavlick; Tal Linzen"}, {"ref_id": "b33", "title": "Distributed representations of words and phrases and their compositionality", "journal": "", "year": "2013", "authors": "Tomas Mikolov; Ilya Sutskever; Kai Chen; Greg S Corrado; Jeff Dean"}, {"ref_id": "b34", "title": "Universal Dependencies v1: A multilingual treebank collection", "journal": "", "year": "2016", "authors": "Joakim Nivre; Marie-Catherine De Marneffe; Filip Ginter; Yoav Goldberg; Jan Hajic; D Christopher; Ryan Manning; Slav Mcdonald; Sampo Petrov; Natalia Pyysalo;  Silveira"}, {"ref_id": "b35", "title": "Universal Dependencies v2: An evergrowing multilingual treebank collection", "journal": "", "year": "2020", "authors": "Joakim Nivre; Marie-Catherine De Marneffe; Filip Ginter; Jan Haji\u010d; D Christopher; Sampo Manning;  Pyysalo"}, {"ref_id": "b36", "title": "Deep contextualized word representations", "journal": "", "year": "2018", "authors": "E Matthew; Mark Peters; Mohit Neumann; Matt Iyyer; Christopher Gardner; Kenton Clark; Luke Lee;  Zettlemoyer"}, {"ref_id": "b37", "title": "AdapterHub: A framework for adapting Transformers", "journal": "", "year": "2020", "authors": "Jonas Pfeiffer; Andreas R\u00fcckl\u00e9; Clifton Poth; Aishwarya Kamath; Ivan Vuli\u0107; Sebastian Ruder; Kyunghyun Cho; Iryna Gurevych"}, {"ref_id": "b38", "title": "Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks", "journal": "", "year": "2018", "authors": "Jason Phang; Thibault F\u00e9vry; Samuel R Bowman"}, {"ref_id": "b39", "title": "XCOPA: A multilingual dataset for causal commonsense reasoning", "journal": "", "year": "2020", "authors": "Goran Edoardo Maria Ponti; Olga Glava\u0161; Qianchu Majewska; Ivan Liu; Anna Vuli\u0107;  Korhonen"}, {"ref_id": "b40", "title": "Intermediate-task transfer learning with pretrained models for natural language understanding: When and why does it work?", "journal": "", "year": "2020", "authors": "Yada Pruksachatkun; Jason Phang; Haokun Liu; Xiaoyi Phu Mon Htut; Richard Yuanzhe Zhang; Clara Pang; Katharina Vania; Samuel R Kann;  Bowman"}, {"ref_id": "b41", "title": "Choice of plausible alternatives: An evaluation of commonsense causal reasoning", "journal": "", "year": "2011", "authors": "Melissa Roemmele; Andrew S Cosmin Adrian Bejan;  Gordon"}, {"ref_id": "b42", "title": "Social IQa: Commonsense reasoning about social interactions", "journal": "", "year": "2019", "authors": "Maarten Sap; Hannah Rashkin; Derek Chen; Yejin Ronan Le Bras;  Choi"}, {"ref_id": "b43", "title": "Shallow syntax in deep water", "journal": "", "year": "2019", "authors": "Swabha Swayamdipta; Matthew Peters; Brendan Roof; Chris Dyer; Noah A Smith"}, {"ref_id": "b44", "title": "The Penn treebank: An overview", "journal": "Springer", "year": "2003", "authors": "Ann Taylor; Mitchell Marcus; Beatrice Santorini"}, {"ref_id": "b45", "title": "What do you learn from context? probing for sentence structure in contextualized word representations", "journal": "", "year": "2019", "authors": "Ian Tenney; Patrick Xia; Berlin Chen; Alex Wang; Adam Poliak; Thomas Mccoy; Najoung Kim; Benjamin Van Durme; R Samuel; Dipanjan Bowman;  Das"}, {"ref_id": "b46", "title": "UDapter: Language adaptation for truly universal dependency parsing", "journal": "", "year": "2020", "authors": "Arianna Ahmet\u00fcst\u00fcn; Gosse Bisazza; Gertjan Bouma;  Van Noord"}, {"ref_id": "b47", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b48", "title": "Universal adversarial triggers for attacking and analyzing NLP", "journal": "", "year": "2019", "authors": "Eric Wallace; Shi Feng; Nikhil Kandpal; Matt Gardner; Sameer Singh"}, {"ref_id": "b49", "title": "Can you tell me how to get past Sesame street? sentence-level pretraining beyond language modeling", "journal": "", "year": "2019", "authors": "Alex Wang; Jan Hula; Patrick Xia; Raghavendra Pappagari; Thomas Mccoy; Roma Patel; Najoung Kim; Ian Tenney; Yinghui Huang; Katherin Yu"}, {"ref_id": "b50", "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding", "journal": "", "year": "2018", "authors": "Alex Wang; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel Bowman"}, {"ref_id": "b51", "title": "A broad-coverage challenge corpus for sentence understanding through inference", "journal": "", "year": "2018", "authors": "Adina Williams; Nikita Nangia; Samuel Bowman"}, {"ref_id": "b52", "title": "PAWS-X: A cross-lingual adversarial dataset for paraphrase identification", "journal": "", "year": "2019", "authors": "Yinfei Yang; Yuan Zhang; Chris Tar; Jason Baldridge"}, {"ref_id": "b53", "title": "Syntax-enhanced neural machine translation with syntax-aware word representations", "journal": "", "year": "2019", "authors": "Meishan Zhang; Zhenghua Li; Guohong Fu; Min Zhang"}, {"ref_id": "b54", "title": "Dependency parsing as head selection", "journal": "", "year": "2017", "authors": "Xingxing Zhang; Jianpeng Cheng; Mirella Lapata"}, {"ref_id": "b55", "title": "Paws: Paraphrase adversaries from word scrambling", "journal": "", "year": "2019", "authors": "Yuan Zhang; Jason Baldridge; Luheng He"}, {"ref_id": "b56", "title": "On the limitations of cross-lingual encoders as exposed by reference-free machine translation evaluation", "journal": "", "year": "2020", "authors": "Wei Zhao; Goran Glava\u0161; Maxime Peyrard; Yang Gao; Robert West; Steffen Eger"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Architecture of our transformer-based biaffine dependency parser.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Topological similarity (l-CKA) for pairs of BERT and RoBERTa variants, before and after different finetuning steps (B, M, P, AP, and N). Rows: transformer layers; Columns: pairs of transformer variants in comparison.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Finally, we analyze how fine-tuning transformers on different tasks modifies the topology of their representation spaces. We encode the set of sentences S from the test portions of treebanks used in IPT 10 with different variants: (a) Base (B): original LM-pretrained transformer, no further training; (b) MLM (M): after ILMT; (c) Parsing (P): after Standard IPT; and (d) Adapter-Parsing (AP): after Adapter-based IPT; for monolingual transformers (BERT and RoBERTa), also with (e) NLI (N): after NLI finetuning (without any intermediate training)", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Analysis of topological similarity (l-CKA) for variants of mBERT and XLM-R before and after IPT and ILMT (B, M, P, AP) in zero-shot transfer experiments. Results shown for intermediate parsing on DE and TR data.", "figure_data": ""}, {"figure_label": "567", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :Figure 6 :Figure 7 :567Figure 5: Full results of the topological similarity analysis (l-CKA) for pairs of BERT and RoBERTa variants, before and after different fine-tuning steps (B, M, P, AP, and N). Rows: transformer layers; Columns: pairs of transformer variants in comparison.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": ").", "figure_data": "Arc classifierRelation classifierXX'concatconcat[root]brownjumps...++Word-level average pooling...Transformer(BERT / RoBERTa)[CLS] The quick br ##own fox jump ##s ... [SEP]Intermediate Training. Sometimes called Sup-plementary Training on Intermediate Labeled-dataTasks (STILT) (Phang et al., 2018), intermediate"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "). Because of this, we additionally evaluate on the HANS dataset (McCoy 4 Selected languages vary in typological and etymological proximity to EN as the source language: DE is in the same (Germanic) branch of Indo-European languages, FR is from the different branch of the same family, whereas TR (Turkic) and ZH (Sino-Tibetan) belong to different language families. et al., 2020), consisting of adversarial examples on which models that capture such heuristics fail. Paraphrase Identification is a binary classification task where we predict if two sentences are mutual paraphrases. For EN, we train, validate, and test on respective portions of the PAWS dataset", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Dependency parsing performance of our transformer-based biaffine parsers.", "figure_data": "Transf.Parsing FT NLI HANS PAWS SIQANone84.153.392.460.7BERTStandard84.456.791.958.8Adapter84.153.392.458.3None88.467.494.767.2RoBERTaStandard87.764.594.966.5Adapter87.966.394.767.3"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Downstream LU performance of monolingual", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": "The reported results do not favor supervised pars-ing (i.e., explicit syntax): compared to originaltransformers that have not been exposed to any ex-plicit syntactic supervision, variants exposed to UDsyntax via IPT (Standard, Adapter) fail to produceany significant gains for any of the downstream LUtasks. One cannot argue that the cause of this mightbe forgetting (i.e., overwriting) of the distributionalknowledge obtained in LM pretraining during IPT:Adapter IPT variants, in which all distributionalknowledge is preserved by design, also fail to yieldany significant LU gains. IPT yields the largest gain(+3.4%) for BERT on HANS -the NLI dataset con-sisting of adversarial examples for which syntaxdeliberately affects the sentence meaning more di-rectly. The same effect, however, is not there forRoBERTa, suggesting that the additional syntacticknowledge that BERT gets through IPT, RoBERTaseems to obtain through larger-scale pretraining.Zero-Shot Language Transfer. We show the re-sults obtained for zero-shot downstream language"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Performance of multilingual transformers, mBERT and XLM-R, in zero-shot language transfer for downstream LU tasks, with and without prior intermediate dependency parsing training on target language treebanks.", "figure_data": "transfer setup, for both mBERT and XLM-R, inTable 3. Again, these results do not particularlyfavor the intermediate injection of explicit syntac-tic information in general. However, in few caseswe do observe gains from the intermediate target-language parsing training: e.g., 3% gain on PAWS-X for ZH as well as 4% and 5% gains on XCOPAfor ZH and TR , respectively. Interestingly, all sub-stantial improvements are obtained for mBERT;"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "lists the sizes (in number of sentences) of Universal Dependencies treebanks that we use for our intermediate parsing training and evaluation of our biaffine dependency parsers. The UD treebanks v.2.5, which we used in this work, are available at: http://hdl.handle.net/11234/ 1-3105. In Table", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "/huggingface.co/bert-base-cased RoBERTa EN 50K 110M https://huggingface.co/roberta-base mBERT Multiling. 119K 125M https://huggingface.co/bert-base-multilingual-cased XLM-R Multiling. 250K 125M https://huggingface.co/xlm-roberta-base", "figure_data": "NameLangVocab Params URLBERTEN29K110Mhttps:/"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "LM-pretrained transformer models used in our study.", "figure_data": "LangTreebankTrainDevTestENEWT12,5382,0022,077DEGSD13,810799977FRGSD14,4401,475416TRIMST3,664988983ZHGSD3,996500500"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Universal Dependencies treebanks used in our study. We display sizes of train, development, and test portions in terms of number of sentences. https://github.com/google-research-datasets/paws Commonsense social reasoning SIQA https://maartensap.github.io/social-iqa Commonsense causal reasoning COPA https://people.ict.usc.edu/\u02dcgordon/copa.html Commonsense causal reasoning XCOPA https://github.com/cambridgeltl/xcopa", "figure_data": "TaskDatasetURLNatural Language InferenceMulti-NLI https://cims.nyu.edu/\u02dcsbowman/multinliNatural Language InferenceXNLIhttps://github.com/facebookresearch/XNLIParaphrase identificationPAWS(-X)"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Links to downstream language understanding datasets used in our work.", "figure_data": "BERT (EN)"}], "formulas": [{"formula_id": "formula_0", "formula_text": "Yarc = XWarcX + Barc; Y rel = XW rel X + B rel", "formula_coordinates": [3.0, 308.49, 756.82, 214.79, 8.37]}, {"formula_id": "formula_1", "formula_text": "yi = W o mcc tanh W h mcc x i CLS + b h mcc with W h mcc \u2208 R H\u00d7H , b h mcc \u2208 R H and W o mcc \u2208 R 1\u00d7H as parameters.", "formula_coordinates": [4.0, 306.88, 265.73, 218.66, 50.64]}, {"formula_id": "formula_2", "formula_text": "): 11 l-CKA(X1, X2) = X 2 X1 2 F X 1 X1 F X 2 X2 F .", "formula_coordinates": [8.0, 89.56, 596.59, 183.15, 53.98]}], "doi": ""}