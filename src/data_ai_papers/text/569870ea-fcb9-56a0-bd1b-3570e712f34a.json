{"title": "Vanishing Component Analysis with Contrastive Normalization", "authors": "Ryosuke Masuya; Yuichi Ike; Hiroshi Kera", "pub_date": "2022-10-27", "abstract": "Vanishing component analysis (VCA) computes approximate generators of vanishing ideals of samples, which are further used for extracting nonlinear features of the samples. Recent studies have shown that normalization of approximate generators plays an important role and different normalization leads to generators of different properties. In this paper, inspired by recent self-supervised frameworks, we propose a contrastive normalization method for VCA, where we impose the generators to vanish on the target samples and to be normalized on the transformed samples. We theoretically show that a contrastive normalization enhances the discriminative power of VCA, and provide the algebraic interpretation of VCA under our normalization. Numerical experiments demonstrate the effectiveness of our method. This is the first study to tailor the normalization of approximate generators of vanishing ideals to obtain discriminative features.", "sections": [{"heading": "INTRODUCTION", "text": "Exploring the geometry of data is a common task across various fields, such as machine learning, computer vision, and systems biology. There, it is essential to find a non-linear structure of data that is smaller than the original feature space. One of the methods for extracting non-linear features from data is to describe the data as an algebraic set 1 . The method is based on the algebraic set assumption that asserts that data in R n is located in an algebraic set whose dimension is much smaller than n.\nGiven a point set X \u2282 R n , the vanishing ideal I(X) is the set of polynomials as follows:\nI(X) = {g \u2208 R[x 1 , . . . , x n ] | g(x) = 0 for all x \u2208 X}\nIt is well-known that I(X) has a finite set of generators. However, an exact vanishing polynomial may result in a corrupted model that overfits the noisy data and be far from the actual structure. To avoid overfitting the noisy data, we consider a generator that approximately takes 0 on X, which is called an approximate vanishing generator. In the last decade, the computation of approximate vanishing generators of I(X) has been extensively studied in computer algebra and machine learning and exploited in applications such as signal processing and computer vision [Torrente, 2008, Livni et al., 2013, Hou et al., 2016, Kera and Hasegawa, 2016, Kera and Iba, 2016, Shao et al., 2016, Iraji and Chitsaz, 2017, Wang and Ohtsuki, 2018, Wang et al., 2019, Antonova et al., 2020, Karimov et al., 2020.\nAmong the algorithms that provide approximate vanishing generators, the vanishing component analysis (VCA; Livni et al. [2013]) computes approximate vanishing generators without monomial orderings. Thus, the effect of the choice of a monomial ordering on the results need not be considered; otherwise, several runs with different monomial orderings are required to ease the effect Stigler, 2004, Laubenbacher andSturmfels, 2009]. The VCA is further generalized to the normalized vanishing component analysis (normalized VCA; Kera and Hasegawa 2019), which computes approximate vanishing generators satisfying a given normalization. Normalization such as coefficient normalization [Kera and Hasegawa, 2019], gradient normalization [Kera and Hasegawa, 2020], or gradientweighted normalization [Kera, 2022] provides approximate vanishing generators with various properties that reflect geometric intuition.\nIn computation algorithms for approximate vanishing basis, low-degree polynomials of the computed polynomials play important roles in discovering an algebraic set of X. The intersection of algebraic sets represented by low-degree polynomials is considered a positive-dimensional non-linear structure of the data. Such an algebraic set is thought to describe the geometry behind the data. However, such a non-linear structure is not necessarily class-discriminative as it is not aware of other classes.\nIn this paper, we propose a new normalization method for VCA, contrastive normalization, which enhances the specificity of the generators to the class of given samples. Unlike existing methods, our framework constructs discriminative generators even without using the samples from other classes, which allows us to exploit generators for single-class classification or anomaly detection. Our strategy is to compute generators that approximately vanish for a given data X, and are normalized on another data Y that is designed to be similar to X but at the same time belongs to a different class, thereby focusing the generator computation on the discriminative features. For example, when X is a set of hand-written images of digit 0, Y can be the samples from other digits. This forces the generators to be discriminative within the hand-written-digit space (see Fig. 1). By contrast, if Y is a set of random images, the generators can have poor discriminability (e.g., only distinguishing handwritten digits from other random images). We theoretically and empirically show that this is the case and that the design of Y has a great impact on the discriminability. Furthermore, inspired by recent selfsupervised frameworks [Golan andEl-Yaniv, 2018, Bergman andHoshen, 2020], our normalization also works by generating good Y from X, which broadens the applications of generators to the tasks where one can only get access to a single class (e.g., anomaly\nV X V Y V H R n\nFigure 1: The hand-written-digit space and its subspaces associated with classes. Here, V H (surface) denotes the hand-written-digit space \u2282 R n , and X, Y \u2282 V H denote different classes. The different classes are contained in subspaces V X (blue horizontal curve) and V Y (red dashed vertical curve) of V H , respectively. If a basis vanishing polynomial for X is normalized on Y , the polynomial is expected to only vanish V X in V H . detection).\nOur contributions are summarized as follows.\n1. We propose contrastive normalization for VCA, which is the first class-discriminative and selfsupervised normalization that is accompanied by an algebraic and geometric interpretation of the computed generators. In particular, we show that an ideal quotient occurs by extending the field of real numbers to that of complex numbers.\n2. We prove the importance of the choice of samples Y to which generators are normalized. In particular, we prove that, when Y is set to random samples, the generators lose the discriminability in high probability. In addition, we empirically show that good Y can be generated in a selfsupervised manner.\n3. Exploiting the self-supervised nature of the proposed framework, we apply the approximate basis computation to anomaly detection for the first time. The results support our theoretical arguments and also show the effectiveness of contrastive normalization.", "publication_ref": ["b0", "b25", "b8", "b14", "b19", "b9", "b1", "b10", "b25", "b22", "b23", "b16", "b16", "b17", "b13", "b5", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORK", "text": "The approximate computation algorithms of generators of vanishing ideals have been developed first in computer algebra and then imported to machine learning [Abbott et al., 2008, Heldt et al., 2009, Fassino, 2010, Limbeck, 2013, Livni et al., 2013, Kir\u00e1ly et al., 2014, Kera and Hasegawa, 2018, Kera, 2022, Wirth and Pokutta, 2022, Wirth et al., 2022.\nIn computer algebra, most algorithms are based on Buchberger-M\u00f6ller algorithm [M\u00f6ller and Buchberger, 1982] and its variants Kreuzer [2005, 2006]. Particularly, several algorithms were dedicated to handling perturbed samples [Abbott et al., 2008, Heldt et al., 2009, Fassino, 2010, Limbeck, 2013, which are more practical for data-centric applications such as machine learning. However, although there are a few exceptions [Sauer, 2007, Hashemi et al., 2019, these algorithms depend on the monomial order. Different monomial orders can give different sets of generators, which becomes a problem outside computer algebra. In machine learning, Livni et al. [2013] proposed a monomial-order-free algorithm, VCA, and theoretically showed an advantage of the use of generators of vanishing ideals for the classification task. VCA has been followed by several variants [Kir\u00e1ly et al., 2014, Hou et al., 2016, Kera and Hasegawa, 2018; however, these algorithms, including VCA, suffer from the spurious vanishing problem [Kera and Hasegawa, 2019]any nonvanishing polynomials become approximately vanishing polynomials by scaling if no normalization is used. To resolve this issue, Kera and Hasegawa [2019] proposed normalized VCA, which incorporates a normalization in VCA. Since then, several normalizations have been proposed and shown to be superior to the conventional coefficient normalization [Kera and Hasegawa, 2020, 2021, Kera, 2022.\nOur study also proposes a new normalization. In contrast to the existing ones, our normalizationcontrastive normalization-focuses on enhancing the class-discriminability of generators. While Kir\u00e1ly et al. [2012], Hou et al. [2016] also consider classdiscriminative generators, there are two critical differences. First, inspired by recent self-supervised frameworks in machine learning [Golan andEl-Yaniv, 2018, Bergman andHoshen, 2020], our framework is designed to work even when there are no accessible samples from other classes. This allows us, for the first time, to apply approximate generators to anomaly detection, where only a single class (i.e., the normal class) is given. Second, we provide the algebraic and geometric interpretation of the output generators while [Kir\u00e1ly et al., 2012, Hou et al., 2016 do not.", "publication_ref": ["b0", "b7", "b4", "b24", "b25", "b21", "b15", "b13", "b26", "b0", "b7", "b4", "b24", "b6", "b25", "b21", "b8", "b15", "b16", "b16", "b17", "b18", "b13", "b20", "b8", "b5", "b2", "b20", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "PRELIMINARIES", "text": "Throughout this paper, we focus on the polynomial ring R n = R[x 1 , . . . , x n ], where x i is the i-th indeterminate.\nFirst, we introduce the vanishing ideal of a point set.\nDefinition 1 (Vanishing Ideal). The vanishing ideal I(X) \u2282 R n of a subset X of R n is the set of polynomials that vanish for any point in X:\nI(X) = {g \u2208 R[x 1 , . . . , x n ] | g(x) = 0 for all x \u2208 X}.\nDefinition 2 (Evaluation Vector and Evaluation Matrix). Given a point set X = {x 1 , x 2 , . . . , x |X| }, the evaluation vector of a polynomial h is defined as\nh(X) = h(x 1 ) h(x 2 ) \u2022 \u2022 \u2022 h(x |X| ) \u2208 R |X| , where | \u2022 | denotes the cardinality of a set. For a set of polynomials H = {h 1 , h 2 , . . . , h |H| }, its evalua- tion matrix is H(X) = h 1 (X) h 2 (X) \u2022 \u2022 \u2022 h |H| (X) \u2208 R |X|\u00d7|H| .\nWe represent a polynomial h by its evaluation vector h(X). Then the product and the weighted sum of polynomials are represented as linear algebraic operations as follows. Let H = {h 1 , h 2 , . . . , h |H| } be a set of polynomials. The product of h, h \u2208 H is represented as h(X) h (X), where denotes the Hadamard product. The weighted sum\n|H| i=1 w i h i , where w i \u2208 R, is represented as |H| i=1 w i h i (X). We denote |H| i=1 w i h i by Hw with w = w 1 \u2022 \u2022 \u2022 w |H| T .\nSimilarly, we denote the product between a polynomial set H and a matrix W = (w 1 w 2 \u2022 \u2022 \u2022 w s ) \u2208 R |H|\u00d7s by HW := {Hw 1 , Hw 2 , . . . , Hw s }. Note that (Hw)(X) = H(X)w and (HW )(X) = H(X)W . We define span(H\n) = { h\u2208H a h h | a h \u2208 R} \u2282 R n and H = { h\u2208H f h h | f h \u2208 R n } \u2282 R n .\nWe call the former the span of H and the latter the ideal generated by H. We consider the following approximate vanishing polynomials.\nDefinition 3 ( -vanishing Polynomial). A polynomial g is an -vanishing polynomial for a point set X if g(X) \u2264 , where \u2022 denotes the Euclidean norm; otherwise, g is an -nonvanishing polynomial. Kera and Hasegawa [2019] proposed a basis construction algorithm of vanishing ideals, called the normalized VCA. The normalized VCA constructs vanishing polynomials for a point set under given normalization. They used coefficient and gradient normalization of vanishing polynomials, which overcome the spurious vanishing problem in vanishing polynomials. We propose to require vanishing polynomials for a point set to be normalized on another point set.", "publication_ref": ["b16"], "figure_ref": [], "table_ref": []}, {"heading": "PROPOSED METHOD", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm", "text": "The input to our algorithm is two point sets X, Y \u2282 R n and error tolerance \u2265 0. The algorithm outputs a basis set G of -vanishing polynomials and a basis set of -nonvanishing polynomials F . The algorithm proceeds from degree-0 polynomials to those of higher degrees. For each t, a set of degree-tvanishing polynomials G t and a set of degree-tnonvanishing polynomials F t are generated. We use notations F t = t \u03c4 =0 F \u03c4 and G t = t \u03c4 =0 G \u03c4 . For t = 0, F 0 = {m} and G 0 = \u2205, where m = 0 is a constant polynomial. At each degree t \u2265 1, the following procedures (Step 1, Step 2, and Step 3) are conducted 2 .\nStep 1: Generate a set of candidate polynomials. Pre-candidate polynomials of degree t for t > 1 are generated by multiplying nonvanishing polynomials across F 1 and F t\u22121 :\nC pre t = {pq | p \u2208 F 1 , q \u2208 F t\u22121 }. At t = 1, C pre 1 = {x 1 , x 2 , . . . , x n }.\nThe candidate basis is then generated through orthogonalization:\nC t = C pre t \u2212 F t\u22121 (F t\u22121 (X)) \u2020 C pre t (X),(1)\nwhere (\u2022) \u2020 is the pseudo-inverse of a matrix.\nStep 2: Solve a generalized eigenvalue problem. We solve the following generalized eigenvalue problem:\nC t (X) C t (X)V = N(C t )V \u039b, (2\n)\nwhere V is the matrix that has generalized eigenvectors v 1 , v 2 , . . . , v |Ct| for its columns, \u039b is the diagonal matrix with generalized eigenvalues \u03bb 1 , \u03bb 2 , . . . , \u03bb |Ct| , and N(C t ) \u2208 R |Ct|\u00d7|Ct| is the normalization matrix whose (i, j)-th entry is c i (Y ) T c j (Y )/|Y | with c i , c j \u2208 C t .\nStep 3: Construct sets of basis polynomials.\nBasis polynomials are generated by linearly combining polynomials in C t with {v 1 , v 2 , . . . , v |Ct| }:\nG t = {C t v i | \u03bb i \u2264 }, F t = {C t v i | \u03bb i > }.\nIf |F t | = 0, the algorithm terminates with output G = G t and F = F t .\nRemark 4. By solving a generalized eigenvalue problem in Step2, we have\ng(Y ) 2 /|Y | = 1 for any g \u2208 G. Hence, g is a |Y |-nonvanishing polynomial for Y .\nIn particular, if < |Y |, then g is -vanishing for X and -nonvanishing for Y .\nWe denote the algorithm for two sets X, Y by VCA(X, Y ).\nThe choice of Y . Here, for a data set X, we consider the choice of Y such that basis polynomials obtained by VCA(X, Y ) have discriminability in a target space containing X. We first remark that, if basis polynomials are constant 0 on the target space, then they have no discriminability. In our method, by using another data set Y \u2282 X in the same space, VCA(X, Y ) gives basis polynomials that vanish on X and not on Y , which means that they are not constant 0 on the target space. Hence, it is important to choose a suitable point set Y from the target space for discriminability. Furthermore, as explained as follows, we can naturally consider the choice of Y in self-supervised frameworks.\nIn self-supervised learning, some methods learn given data and those transformed by suitable transformations. In particular, using transformations with features of data enables a classifier to learn highquality data. For instance, for hand-writing data, image processing transformations are considered (rotation by 0, 90, 180, 270 degrees). Self-supervised frameworks also enable VCA(X, Y ) to learn highquality data. In particular, we set Y to an appropriately transformed X.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Our Strategy for Anomaly Detection", "text": "In this section, we propose our anomaly detection method by using VCA(X, Y ) and the idea of GOAD [Bergman and Hoshen, 2020], which is a deep anomaly detection method. Similarly to GOAD, our method leans the normal data and transformations. In the following, we describe the outline of our strategy. First, we transform X \u2282 R n to X i \u2282 R n using a transformation\nT i : R n \u2192 R n for i = 1, . . . , M . We then compute VCA(X i , Y i ), where Y i = j =i X j . The obtained set of generators is de- noted by G i = {g (i) 1 , . . . , g (i) |Gi| }. By our construction, G i (x) for new sample x \u2208 R n is expected to take small values if x \u2208 V (G i ). Moreover, an algebraic set V (G i ) = |Gi| j=1 V (g (i) j\n) is expected to discriminate X i from the other classes.\nWe define the classifier that predicts transforma-tion T i given a transformed point as follows:\nP i (x) = e \u2212 Gi(Tix) 2 M k=1 e \u2212 G k (Tix) 2 .\nThis means that if P i (x) \u2248 1, the sample T i x approximately resides in V (G i ) and not in V (G j ). By assuming independence between transformations, the probability of x \u2208 X is the product of P i 's. Therefore, an anomaly score of x \u2208 X is induced from the probability that all transformed samples are in their respective algebraic sets as follows:\nScore(x) = \u2212 log M i=1 P i (x) = \u2212 M i=1 log P i (x). (3)\nA high score for x indicates that x is anomalous.\nThe feature vectors G i (x)'s for x are not strictly zero vectors. Therefore, we replace the zero vectors by centers c i , which are given by the average feature over the training set for every transformation, i.e., c i = 1", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "|X|", "text": "x\u2208X G i (x). Also, GOAD added a small regularizing constant to the probability of each transformation to avoid uncertain probabilities. We added similarly:\nP i (x) = e \u2212 Gi(Tix)\u2212ci 2 + M k=1 e \u2212 G k (Tix)\u2212c k 2 + M .\nNote that, in GOAD, G i is implemented by a deep neural network. Furthermore, we omit algebraic sets not used in anomaly detection as follows. We replace the basis set G i by wG i = (w 1 g\n(i) 1 , . . . , w |Gi| g (i)\n|Gi| ), where w = (w 1 , . . . , w |Gi| ) \u2208 R n . Updating w by optimizing the score Eq. (3), we pick up effective algebraic sets in anomaly detection.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discriminative Power of VCA(X,Y)", "text": "In Section 4.1, we described that, for data X and another data Y that is similar to X but belongs to a different class, VCA(X, Y ) gives a nonlinear structure of X. Then, the nonlinear structure has highly discriminative power in the space where both X and Y belong. In this section, we theoretically prove that setting Y to a set of random samples gives a nonlinear structure of X without discriminative power in the space.\nFor simplicity, we assume that all spaces are subspaces of R n and that data X is of mean 0.\nLet V be a subspace of R n containing X, where we consider that V is the feature space of X. Let V X \u2282 V be a subspace, which we consider as a feature space of X smaller than V . Since V X is the feature space of X, it is naturally considered that the feature space V X of X is almost reproduced by X, namely, V X = span(X). In the following, we may assume V X = span(X).\nLet Z be a set of random samples of R n and let G be the output of VCA(X, Z). We formulate that a polynomial normalized on a random sample has no discriminative power in the target space V as follows:\nLet p \u2208 V X and q \u2208 R n be random points. For g \u2208 G, both P[|g(p)| < t 0 ] and P[|g(q)| > t 1 ] are high for some t 1 > t 0 > 0.\nSince Z is a set of random samples, each coefficient of g is a random variable induced by Z. However, it is difficult to represent the coefficients. To avoid this problem, we consider a polynomial that is a ''probabilistic model\" for g as follows: First, recall that g satisfies the following relations:\ng(X) = 0 and g(Z) 2 |Z| = 1.\nInstead of g, we use a polynomial g satisfying the above relations with high probability. In particular, when g is of degree 1, g can be designed by the following proposition.\nProposition 5. Let X \u2282 R n be a point set and let {u 1 , . . . , u n } be an orthogonal basis of R n such that span(X) = span(u 1 , . . . , u k ). Choose a set, Z, of random points satisfying that any point p i \u2208 Z is of the form p i = n j=1 a i,j u j , where a i,j 's\n(1 \u2264 i \u2264 |Z|, 1 \u2264 j \u2264 n) are i.i.d. \u223c N (0, 1). If w = n i=k+1 w i u i is a random vector such that w i 's are i.i.d. \u223c N (0, 1/(n \u2212 k))\n, then, for all > 0, we have\nP [ g (X) = 0] = 1, P g (Z) 2 |Z| \u2212 1 \u2265 2 \u2264 e \u2212 2 N/6 + e \u2212 N/3 + (N 2 \u2212 N )e \u2212\u03b7 2 |Z|/2 + N (e \u2212\u03b7 2 |Z|/6 + e \u2212\u03b7|Z|/3 ),\nwhere g (x) = w T x, N = n \u2212 k and \u03b7 = (1+ )N .\nSince we use a lot of random samples, we may assume |Z| n. We may also assume that n k, as the dimension of V X is much smaller than n. Using a probabilistic model described in Proposition 5, we can prove the following theorem, which indicates the desired statement. Theorem 6. Let X, Y \u2282 R n be sets of points and we denote V X = span(X) and V Y = span(Y ). Let {u 1 , . . . , u n } be an orthogonal basis of R n such that V X = span(u 1 , . . . , u k ) and\nV X + V Y = span(u 1 , . . . , u k+m ).\nLet w = n i=k+1 w i u i be a random vector as in Proposition 5. Choose random vectors\np X \u2208 V X , p Y \u2208 V Y and p \u22a5 \u2208 (V X + V Y ) \u22a5 such that coeffi- cients of u 1 , . . . , u n are i.i.d.\u223c N (0, 1). Then, for t > 0, we have P [|g(p X )| = 0] = 1, P [|g(p Y )| < t] \u2265 1 \u2212 m n \u2212 k \u2022 1 t 2 , P |g(p \u22a5 )| \u2265 n \u2212 (k + m) 2(n \u2212 k) \u2265 n \u2212 (k + m) 4(n \u2212 (k + m) + 2)\n.\nIn Theorem 6, we interpret V X + V Y as the feature space V described just before Proposition 5. Under the assumption that m, k < m+k = dim V n, from Theorem 6, we have m n\u2212k \u2022 1 t 2 \u2248 0 and n\u2212(k+m) (n\u2212(k+m)+2) \u2248 1. Namely, Theorem 6 states that VCA(X, Z) constructs polynomials with no discriminative power in V .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "An Algebraic Interpretation of", "text": "VCA(X, Y )\nFor error tolerance = 0 and a set, X, of points, VCA, the normalized VCA with gradient, and the normalized VCA with coefficient are known to construct bases of I(X), respectively. In this section, we consider an ideal generated by the basis polynomials of VCA(X, Y ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Basic Definitions and Notations for Ideals", "text": "We here state some basic terminology and facts about ideals. Let k be a field and let k[x 1 , . . . , x n ] be a polynomial ring, where x i is the i-th indeterminate.\nWe assume that ideals are defined in k[x 1 , . . . , x n ], unless otherwise stated.\nDefinition 7. Let G \u2282 k[x 1 , . . . , x n ]. Then we set V (G) = {x \u2208 k n | f (x) = 0 for all f \u2208 G}.\nWe call V (G) the algebraic set defined by G over k.\nWhen we emphasize a field k, we denote V (G) by\nV k (G). An algebraic set V \u2282 k n is irreducible if V = V 1 \u222a V 2 , where V 1 and V 2 are algebraic sets over k, then V 1 = V or V 2 = V . Definition 8. Let V \u2282 k n be an algebraic set. A decomposition V = V 1 \u222a \u2022 \u2022 \u2022 \u222a V s , where each V i is an irreducible variety, is called a minimal decomposition if V i \u2282 V j for i = j. Also, we call the V i the irreducible components of V i . Theorem 9. Let V \u2282 k n be an algebraic set. Then, V has a minimal decomposition V = V 1 \u222a \u2022 \u2022 \u2022 \u222a V s .\nFurthermore, this minimal decomposition is unique up to the order in which V 1 , . . . , V s are written. Now we define the dimension of an irreducible variety.\nDefinition 10. Let V \u2282 k n be an irreducible algebraic set. We define\ndim V = sup{r | V 0 V 1 \u2022 \u2022 \u2022 V r = V, V i : irreducible over k}. We call dim V the dimension of V .\nIt is well-known that the dimension of an irreducible algebraic set is finite.\nFor a subset X of k n , we set I(X)\nI(X) = {g \u2208 k[x 1 , . . . , x n ] | g(x) = 0 for all x \u2208 X}.\nWhen we emphasize a field k, we denote I(X) by I k (X).\nDefinition 11. Let S \u2282 k n . We define S = V (I(S)).\nWe introduce the notion of ideal quotient to describe the ideal associated with V \\ W . Definition 12. Let I and J be ideals. Then we set\nI : J = {g \u2208 k[x 1 , . . . , x n ] | gJ \u2282 I}.\nWe call I : J the ideal quotient of I by J.\nNote that an ideal quotient is an ideal.\nProposition 13 ([Cox et al., 2015, Ch. 4 Sect. 4 Corollary 11]). Let V and W be algebraic sets over k. Then we have I(V\n) : I(W ) = I(V \\ W ).\nWe are interested in describing I(V (G)). For that purpose, we introduce the radical ideal. Definition 14. Let I be an ideal. The radical ideal, \u221a I, of I is the set\n{f | f m \u2208 I for some integer m \u2265 1}.\nTheorem 15 (The Strong Nullstellensatz, [Cox et al., 2015, Ch 4 Sect.\n2 Theorem 6]). Let G \u2282 k[x 1 , . . . , x n ]. If k is algebraically closed, then I(V (G)) = G .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "An Ideal Given by VCA(X,Y)", "text": "The original VCA(X) gives a set of generators G 0 that generates I(X), i.e., I(X) = G 0 . In contrast, our method VCA(X, Y ) only outputs a set of vanishing polynomials G that are normalized over Y . As a consequence, its output only generate a subset of I(X), i.e., I(X) \u2283 G . Here, we provide the algebraic interpretation of this subset; in particular, we show that the radical ideal of G generates an ideal quotient.\nIn the following, we consider the case k = R or C.\nDefinition 16. Let I be an ideal in R n = R[x 1 , . . . , x n ].\nThen we define an ideal I C in C[x 1 , . . . , x n ] as follows:\nI C = i f i g i | f i \u2208 C[x 1 , . . . , x n ], g i \u2208 I .\nTheorem 17. Let X, Y \u2282 R n be distinct sets of points and let G be the out put of VCA(X, Y ) for = 0. We put V = V k (G) and denote its irreducible components by V 1 , . . . , V s . Then, for any irreducible algebraic set W \u2282 k n satisfying dim W = min 1\u2264i\u2264s dim V i and Y \u2282 W , we have\n(1) V \u2282 W and W \u2282 V .\n(\n) V = V \\ W . (2\n) I k (V ) = I k (V \\ W ) = I k (V ) : I k (W ). Moreover, if k = C, then we have G C = I C (V ) = I C (V \\ W ) = I C (V ) : I C (W ).3", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTS 6.1 Synthetic Dataset", "text": "In this section, by using synthetic data, we confirm Theorem 6, which states that a polynomial normalized on random samples does not have discriminative power in a target space. In the following, we use notation as in Theorem 6. Let u i \u2208 R n be a standard basis whose entries are all zero except the i-th entry that equals 1. Let X = {x 1 , . . . , x |X| } \u2282 span(u 1 , . . . , u k ) be a set of random points such that (i) for x i \u2208 X,  The probability that a basis polynomial is t-vanishing for a point in V . Basis polynomials g 1 , . . . , g 19 (solid lines in different colors) obtained by VCA(X, Z). All the probabilities are higher than the theoretical value. This means justification on Theorem 6.\nits j-th entries (1 \u2264 j \u2264 5) are i.i.d.\u223c N (0, 1) and (ii) span(X) = span(u 1 , . . . , u k ). Then, V X = span(X). We define a target space V = V X + V Y by span(u 1 , . . . , u k+m ). Let Z = {p 1 , . . . , p |Z| }, where entries of p i are i.i.d.\u223c N (0, 1). We consider the vanishing basis of degree 1 computed by VCA(X, Z).\nIn our setting with n = 105, k = 5, m = 5 and |X| = |Z| =10,000, the VCA(X, Z) basis set of degree 1 consists of 18 vanishing polynomials g 1 , . . . , g 18 . We choose random points from V = V X + V Y as follows: Let R be a set of 1,000 random points whose i-th entries (1 \u2264 i \u2264 10) are i.i.d.\u223c N (0, 1) and otherwise 0. We define the probability of |g i (p)| < t for t > 0 by\nP i (t) = |{p \u2208 R | |g i (p)| < t}|/|R|.\nIn Fig. 2, the probability P i (t) is plotted for t = 0.24, 0.25, . . . , 1.0. We also plot the theoretical curve of 1 \u2212 m n\u2212k \u2022 1 t 2 = 1 \u2212 0.01/t 2 , which is given as a low bound in Theorem 6. As shown in Fig. 2, we confirmed the justification of Theorem 6; namely, P i (t) is higher than the lower bound described in Theorem 6.", "publication_ref": [], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Anomaly Detection for Benchmark Datasets", "text": "In Sections 4 and 5, we have discussed the discriminability of basis polynomials vanishing in an original data set and normalized on another data set. In particular, we have suggested that (i) the discriminative power of basis polynomials depends on the choice of normalizing data, and (ii) the normalizing data should be similar to the original data but different from the original data. Theorem 6 states that basis polynomials have no discriminability when the normalizing data set is a set of random samples. This theorem was confirmed by using synthetic data in Section 6.1. In this section, we further study (ii) through experiments for anomaly detection. In particular, we chose random affine transformed data as the normalizing data and confirmed the random affine transformed data version of Theorem 6. Let us describe our setting of experiments.\nDatasets. We used two standard datasets, the MNIST datasets and the FashionMNIST datasets.\nIn both cases, we consider that normal data set is a collection of samples labeled as 0, 2, 4, 6, 8, and anomalous data is a collection of samples labeled as 1, 3, 5, 7, 9. We experimented with the number of training data in the following manner: The training data was the data labeled as normal out of (i) the full 60,000 training data or (ii) the first 10,000 training data. Also, it is considered that high-dimensional data (e.g., image data) has a low effective dimensionality in many applications. Therefore, we first project training data and test data onto low dimensional space by a dimensionality reduction (e.g., the principal component analysis; PCA) and the change of coordinates. The preprocessing follows [Kera and Hasegawa, 2021]. We extracted polynomials of degree \u2264 4.\nTransformations. We used random affine transformations and rotation transformations. In particular, rotation transformations are rotation by 0, 90, 180, 270 degrees.\nHyperparamaters. We optimized our method using naive gradient descent with a learning rate of 1.0 \u00d7 10 \u22125 and 300 epochs.\nBaselines. The baseline methods evaluated are: Vanishing Component Analysis (VCA Livni et al. [2013]), Normalized Vanishing Component Analysis with gradient normalization (nVCA, Kera and Hasegawa [2020]). To show that basis polynomial algorithms for a given normal data produce basis polynomials without discriminability in a target space (e.g., the hand-written-digit space), we compared these methods in anomaly detection tasks. Since VCA and nVCA do not require another data set, we simply design an anomaly detector using VCA and nVCA as follows: First, following Livni et al. [2013], the feature vector F(x) of a data point x is defined as\nF(x) = (|g 1 (x)| , . . . , g |G| (x) ) ,(4)\nwhere G = {g 1 , . . . , g |G| } is the basis set computed for normal data points. Because of its construction, F(x) is expected to take small values if x belongs the normal data set. Therefore, for a new data, we define a score function by ||F(x)||. The score function indicates that x is anomalous if the score for x is high.\nThe results are reported in terms of AUC and shown in Table 1. Also, as a reference, we also choose GOAD as a self-supervised baseline. Note that we used the official source code of GOAD as it is including the hyperparemters 3 .\nOther baselines. Our strategy for anomaly detection (an analogy of GOAD) is effective when we use basis polynomials with information of two datasets. On the other hand, the effectiveness of our anomaly strategy is not able to be shown when we use basis polynomials having information of just one dataset (e.g., basis polynomials computed by VCA and nVCA), but we can apply GOAD frameworks to VCA and nVCA below. We denote the VCA for a data set X and the nVCA for X by VCA(X) and nVCA(X), respectively. By replacing VCA(X i , Y i ) in Section 4.2 by VCA(X i ) or nVCA(X i ), we can also perform anomaly detection using them. We also performed these experiments to confirm that focusing on similar but different data is effective for anomaly detection. We use AUC as a score and show the results in Table 2. Note that, in these experiments, discriminative power is not focused on.\nResults (rotation transformations vs. random affine transformations). When we use 60,000 data points, comparing the fifth and sixth columns in Table 1, we see that anomaly detection using rotation transformations outperforms that using random affine transformations. Increasing the size of training points enhances the rotation transformation version of our method, while the random affine transformation version of our method has constant discriminative power. This shows that random affine transformed data outside the target space does not enable basis polynomials to have discriminability in the target space. Namely, the random affine version of Theorem 6 is confirmed. Note that a random affine transformation is often considered a geometric transformation, and plays an important role in manifold learning and dimensionality reduction. However, the data after transformation is not located in the tar-get space that the original data belongs to. Hence, in our argument focusing on the target space, using a random affine transformed data set as the normalized part does not produce basis polynomials with discriminability.\nResults (VCA and nVCA). As shown in the VCA and nVCA columns in Table 1, the computed polynomials have no discriminative power, and the results are comparable if we use 60,000 data points or 10,000 points. This allows us to obtain the following interpretation of the discriminative power of polynomials by previous basis computation algorithms. When we compute basis polynomials for an original data set by VCA or nVCA, the computed polynomials do not necessarily have information of other data because of VCA and nVCA algorithms. Hence, in the setting of the experiments using VCA and nVCA described in the baselines paragraph, we can not ensure that polynomials obtained by VCA and nVCA have discriminability.\nResults (VCA and nVCA applied to the GOAD frameworks). Observing the second column in Table 2, VCA and nVCA applied to the GOAD frameworks achieved higher performance ", "publication_ref": ["b18", "b25", "b17", "b25"], "figure_ref": [], "table_ref": ["tab_0", "tab_1", "tab_0", "tab_0", "tab_1"]}, {"heading": "MNIST", "text": "76.1 52.4 53.5 48.4 FashionMNIST 84.5 51.7 76.0 45.0 than simple VCA and nVCA, respectively. Because of gradient-weighted normalization, basis polynomials obtained by nVCA have no discriminability. On the other hand, the result of VCA is mostly comparable with that of our proposed method despite no justification of discriminative information. However, it is known that VCA increases the feature dimension (the number of basis polynomials), while normalized VCA (e.g., nVCA, our proposed method) reduces it (See [Kera and Hasegawa, 2021] for the reduction of feature dimensions).", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSION", "text": "In this paper, we proposed to exploit polynomials normalized on other data in the monomial-orderfree basis construction of the vanishing ideal. The normalization on other data allows us to construct polynomials with discriminability for the normalizing data. Throughout this paper, depending on selfsupervised learning, we focused on the choice of normalizing data. We theoretically and experimentally showed that, if normalizing data is designed outside the space where the original data belongs, then the polynomials have no discriminability in the space. Specifically, anomaly detection was performed for the situation where no other classes can be accessed.\nAs a consequence, the effectiveness of our proposed method was shown. An interesting future direction is to find another way to obtain the normalizing set. It is also important to design a more scalable algorithm to compute basis polynomials. \nE e \u03bbX1Y1 = 1 2\u03c0 \u221e \u2212\u221e \u221e \u2212\u221e e \u03bbxy e \u2212x 2 2 e \u2212y 2 2 dxdy = 1 2\u03c0 \u221e \u2212\u221e e \u2212y 2 2 \u221e \u2212\u221e e \u22121 2 (x\u2212\u03bby) 2 + 1 2 \u03bb 2 y 2 dxdy = 1 \u221a 2\u03c0 \u221e \u2212\u221e e \u2212y 2 2 e 1 2 \u03bb 2 y 2 dy = 1 \u221a 1 \u2212 \u03bb 2 1 2\u03c0 1 1\u2212\u03bb 2 \u221e \u2212\u221e e \u2212y 2 2/(1\u2212\u03bb 2 ) dy = 1 \u221a 1 \u2212 \u03bb 2 .\nLet \u2208 (0, 1). Applying Chernoff's bounding method we get that\nP [Z/M \u2265 ] = P [Z \u2265 M ] = P e \u03bbZ \u2265 e \u03bb M \u2264 e \u2212\u03bb M E e \u03bbZ = e \u2212\u03bb M (1 \u2212 \u03bb 2 ) \u2212M/2 \u2264 e \u2212\u03bb M e \u03bb 2 M/2 ,\nwhere the last inequality occurs because (1 \u2212 a) \u2264 e \u2212a for a \u2265 0. Setting \u03bb = , we obtain the first inequality of (5). (b) Proof of the second inequality of (5). We can similarly compute\n( * ) E e \u2212\u03bbX1Y1 = (1 \u2212 \u03bb 2 ) \u22121/2 ,\nwhere \u03bb \u2208 (0, 1). Also, applying Chernoff's bounding method, we have\nP [Z/M \u2264 \u2212 ] = P [\u2212Z \u2265 M ] \u2264 e \u2212\u03bb M E e \u2212\u03bbZ .\nUsing the above inequality and the equality ( * ), we can prove the desired inequality similarly to (a).\nLemma A.4. Let a i,j (1 \u2264 i \u2264 M, 1 \u2264 j \u2264 N ) be M N independent normally distributed variables. Put A = [a i,j ] 1\u2264i\u2264M, 1\u2264j\u2264N .\nThen, for all \u2208 (0, 1) we have\nP 1 M A T A \u2212 I N max \u2265 \u2264 (N 2 \u2212 N )e \u2212 2 M/2 + N (e \u2212 2 M/6 + e \u2212 M/3 ),\nwhere X max means the max norm of a matrix X and I N is the identity matrix of size N .\nProof. Put a i = (a 1,i , . . . , a M,i ) T . The (i, j)-th entry of 1 M A T A \u2212 I N is of the form a T i a j /M \u2212 \u03b4 ij , where \u03b4 ij is the Kronecker delta. Hence, we have\nP 1 M A T A \u2212 I N max \u2265 \u2264 1\u2264i\u2264j\u2264N P a T i a j M \u2212 \u03b4 ij \u2265 = N P a i 2 M \u2212 1 \u2265 + (N 2 \u2212 N ) 2 P a T 1 a 2 M \u2212 \u03b4 ij \u2265\nBy Corollary A.2 and Lemma A.3, the statement is proven.\nLemma A.5. Let C \u2208 R N \u00d7N be a matrix and let b \u2208 R N be a vector. If b 2 \u2212 1 < and C \u2212 I N max < (1+ )N , then |b T Cb \u2212 1| < 2 .\nProof. We first remark that\nb T (C \u2212 I N )b \u2264 C \u2212 I N max ( b 1 ) 2 and b 1 \u2264 \u221a N b ,\nwhere x 1 means the L1 norm of a vector x. If b 2 \u2212 1 < and C \u2212 I N max < (1+ )N , then we have\nb T (C \u2212 I N )b \u2264 C \u2212 I N max ( b 1 ) 2 < (1 + )N ( b 1 ) 2 \u2264 1 + b 2 \u2264 .\nHence, by the triangle inequality, we obtain\nb T Cb \u2212 1 \u2264 b T Cb \u2212 b T b + b T b \u2212 1 = b T (C \u2212 I N )b + b 2 \u2212 1 < 2 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Now we prove Proposition 5.", "text": "Proposition A.6 (Proposition 5). Let X \u2282 R n be a set of points and let {u 1 , . . . , u n } be an orthogonal basis of R n such that span(X) = span(u 1 , . . . , u k ). Choose a set, Z, of random points satisfying that any point p i \u2208 Z is of the form p i = n j=1 a i,j u j , where a i,j 's\n(1 \u2264 i \u2264 |Z|, 1 \u2264 j \u2264 n) are i.i.d. \u223c N (0, 1). If w = n i=k+1 w i u i is a random vector such that w i 's are i.i.d. \u223c N (0, 1/(n \u2212 k)), then we have P [ g(X) = 0] = 1, P g(Z) 2 |Z| \u2212 1 \u2265 2 \u2264 e \u2212 2 N/6 + e \u2212 N/3 + (N 2 \u2212 N )e \u2212\u03b7 2 |Z|/2 + N (e \u2212\u03b7 2 |Z|/6 + e \u2212\u03b7|Z|/3 ),(6)\nwhere g(x) = w T x, N = n \u2212 k and \u03b7 = (1+ )N .\nProof. We first prove the first equality of (6). As w \u22a5 span(X) = span(u 1 , . . . , u k ), we have g(x) = 0 for any x \u2208 X. Therefore, P [ g(X) = 0] = 1. We next prove the second inequality of (6). Putting A = [a i,j ] 1\u2264i\u2264|Z|,k+1\u2264j\u2264n and b = (w k+1 , . . . , w n ) T , we have\ng(Z) 2 = |Z| i=1 (w T p i ) 2 = b T A T Ab.\nBy Lemma A.5, we obtain\nP |Z| i=1 (w T p i ) 2 |Z| \u2212 1 \u2265 2 \u2264 P 1 |Z| A T A \u2212 I N max \u2265 \u03b7 + P | w 2 \u2212 1| \u2265 ,\nwhere \u03b7 = (1+ )N . Hence, the second inequality of ( 6) is immediate if the following inequalities are proven:\nP 1 |Z| A T A \u2212 I N max \u2265 \u03b7 \u2264 (N 2 \u2212 N )e \u2212\u03b7 2 |Z|/2 + N (e \u2212\u03b7 2 |Z|/6 + e \u2212\u03b7|Z|/3 ), P w 2 \u2212 1 \u2265 \u2264 e \u2212 2 N/6 + e \u2212 N/3 .(7)\n(a) Proof of the first inequality of (7). Sinece 0 < \u03b7 < 1, by Lemma A.4, we obtain the first inequality of ( 7).\n(b) Proof of the second inequality of (7). Since \u221a n \u2212 kw i 's are i.i.d.\u223c N (0, 1), we have\nN b 2 = n i=k+1 ( \u221a n \u2212 kw i ) 2 \u223c \u03c7 2 N .\nBy Corollary A.2, the desired inequality holds.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Proof of Theorem 6", "text": "In this section, we prove Theorem 6.\nTheorem A.7 (Theorem 6). Let X, Y \u2282 R n be sets of points and we denote V X = span(X) and V Y = span(Y ). Let {u 1 , . . . , u n } be an orthogonal basis of R n such that V X = span(u 1 , . . . , u k ) and V X + V Y = span(u 1 , . . . , u k+m ).\nLet w = n i=k+1 w i u i be a random vector as in Proposition A.6. Choose random vectors\np X \u2208 V X , p Y \u2208 V Y and p \u22a5 \u2208 (V X + V Y ) \u22a5 such that coefficients of u 1 , . . . , u n are i.i.d.\u223c N (0, 1). Then, for t > 0, we have P [|g(p X )| = 0] = 1, P [|g(p Y )| < t] \u2265 1 \u2212 m n \u2212 k \u2022 1 t 2 , P |g(p \u22a5 )| \u2265 n \u2212 (k + m) 2(n \u2212 k) \u2265 n \u2212 (k + m) 4(n \u2212 (k + m) + 2) .(8)\nProof. (a) Let us first prove the first equality of (8). Since w \u22a5 span(X) = span(u 1 , . . . , u k ), we have g(p x ) = 0. Therefore, P [|g(p X )| = 0] = 1. (b) For the second inequality of ( 8), we use the Chebyshev inequality. In order to use the Chebyshev inequality, we need to compute E[g(p Y )] and Var[g(p Y )].\nLet p Y = k+m i=1 a i u i , where a 1 , . . . , a k+m are i.i.d.\u223c N (0, 1). Since w \u22a5 V X = span(u 1 , . . . , u k ), we have g(p Y ) = w T p Y = k+m i=k+1 a i w i . As a i and w i are independent, we obtain E[a i w i ] = 0 and Var[a i w i ] = 1 n \u2212 k .\nAlso since a 1 w 1 , . . . , a n w n are independent, we have\nE[g(p Y )] = k+m i=k+1 E[a i w i ] = 0, Var[g(p Y )] = k+m i=k+1 Var [a i w i ] = m n \u2212 k .\nTherefore, by the Chebyshev inequality, we have\nP [|g(p Y )| < t] \u2265 1 \u2212 m n \u2212 k \u2022 1 t 2 . (c)\nFor the third inequality of (8), we use the Paley-Zygmund inequality. In order to use the Paley-Zygmund inequality, we need to compute E g(p \u22a5 ) 2 and E g(p \u22a5 ) 4 .\nLet p \u22a5 = n i=1 a i u i , where a 1 , . . . , a n are i.i.d.\u223c N (0, 1). Since, w \u22a5 V X = span(u 1 , . . . , u k ), we have g(p \u22a5 ) = w T p \u22a5 = n i=k+1 a i w i . By a similar way as the above case, we have\nE g(p \u22a5 ) 2 = E g(p \u22a5 ) 2 \u2212 (E [g(p \u22a5 )]) 2 = Var w T p \u22a5 = n \u2212 (k + m) n \u2212 k .\nWe next compute E g(p \u22a5 ) 4 . Remarking that a i w i and a j w j (i = j) are independent variables with means 0, we have\nE g(p \u22a5 ) 4 = E[(w T p \u22a5 ) 4 ] = E \uf8ee \uf8f0 n i=k+m+1 a i w i 4 \uf8f9 \uf8fb = (i1,i2,i3,i4)\u2208{k+m+1,...,n} 4 E i1,i2,i3,i4 ,\nwhere E i1,i2,i3,i4 = E[a i1 a i1 a i3 a i4 w i1 w i2 w i3 w i4 ]. We remark that\nE i1,i2,i3,i4 = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 9/(n \u2212 k) 2 if i 1 = i 2 = i 3 = i 4 1/(n \u2212 k) 2 if i 1 = i 2 = i 3 = i 4 1/(n \u2212 k) 2 if i 1 = i 3 = i 2 = i 4 1/(n \u2212 k) 2 if i 1 = i 4 = i 2 = i 3 0 otherwise.\nand we have\nE g(p \u22a5 ) 4 = 9 (n \u2212 k) 2 (n \u2212 k \u2212 m) + 1 (n \u2212 k) 2 3(n \u2212 k \u2212 m)(n \u2212 k \u2212 m \u2212 1) = 3(n \u2212 k \u2212 m)(n \u2212 k \u2212 m + 2) (n \u2212 k) 2 .\nBy the Paley-Zygmund inequality, the following holds:\nP |g(p \u22a5 )| > \u03b8E[g(p \u22a5 ) 2 ] = P g(p \u22a5 ) 2 > \u03b8E g(p \u22a5 ) 2 \u2265 (1 \u2212 \u03b8) 2 (E g(p \u22a5 ) 2 ) 2 E[g(p \u22a5 ) 4 ] = (1 \u2212 \u03b8) 2 n \u2212 (k + m) n \u2212 (k + m) + 2 ,\nwhere \u03b8 \u2208 [0, 1]. Setting \u03b8 = 1/2, we have the third inequality of (8).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.3 Proof of Theorem 17", "text": "In this section, we present the detailed proof of Theorem 17.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.3.1 Basic Definitions and Notations for Ideals", "text": "We here some basic facts and terminology about ideals. Let k be a field and let k[x 1 , . . . , x n ] be a polynomial ring, where x i is the i-th indeterminate. We assume that ideals are defined in k[x 1 , . . . , x n ], unless otherwise stated.\nDefinition A.8. An ideal I is radical if f m \u2208 I for some integer m \u2265 1 implies that f \u2208 I.\nLet I be an ideal. The radical ideal, \u221a I, of I is the set {f | f m \u2208 I for some integer m \u2265 1}.\nRemark A.9. A radical ideal is an ideal. We call V (G) the algebraic set defined by G over k. When we emphasize a field k, we denote V (G) by V k (G).\nDefinition A.11. An algebraic set V \u2282 k n is irreducible if V = V 1 \u222a V 2 , where V 1 and V 2 are algebraic sets over k, then V 1 = V or V 2 = V .\nDefinition A.12. Let V \u2282 k n be an algebraic set. A decomposition\nV = V 1 \u222a \u2022 \u2022 \u2022 \u222a V s ,\nwhere each V i is an irreducible algebraic set, is called a minimal decomposition if V i \u2282 V j for i = j. Also, we call the V i the irreducible components of V i .\nTable 3: Anomaly detection accuracy (AUC score \u00d7100) in MNIST and FashionMNIST in the case when a normal class is a collection of samples labeled as 0, 2, 4, 6, 8. Here, Rot. denotes to use the rotation transformations. R.A. denotes to use random affine transformations. In each experiment, 3 transformations are used. In particular, rotation transformations are rotation by 0, 90 and 180 degrees. Compared to using 4 transformations ( Table 1 of Section 6.2), using 3 transformations improves the results. In particular, using 3 rotation transformations for MNIST improves an anomaly score. It is considered that kinds of normalizing data sets in the hand-written-digit space have an impact on discriminability. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "B ADDITIONAL EXPERIMENTS", "text": "We experimented the proposed method under other setting. Using three transformations T 1 , T 2 and T 3 and the size of training sets =30,000, we experimented our methods of Section 6.2. Here, when we use rotation transformations, T 1 , T 2 and T 3 denote rotation by 0, 90 and 180 degrees. The scores of the Rot. column of Table 3 are enhanced over those of Table 1 of Section 6.2. In particular, when we use the MNIST sets, the score growth is better when the number of transformations is three. It would also be interesting to note that, depending on the number of transformations, the training of our method is different. We have discussed the choice of normalizing datasets and stated that they should be chosen from the hand-written-digit space. Furthermore, based on the results of this experiment, kinds of normalizing data sets in the hand-written-digit space are expected to have an impact on discriminability, but this is beyond the scope of this paper.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "A OMITTED PROOFS", "text": "A.1 Proof of Proposition 5\nIn this section, we present the detailed proof of Proposition 5. Before we prove Proposition 5, we start with the following lemmas.\nLemma A.1. Let Z \u223c \u03c7 2 M be a random variable, where \u03c7 2 M is the chi-square distribution with M degrees of freedom. Then, for all > 0 we have\n] \u2264 e \u2212 2 M/6 , and we have\nwhere \u03bb \u2208 (0, 1/2).\nProof. By using the argument of Lemma B.12 in [Shalev-Shwartz and Ben-David, 2014], we can immediately prove our statement. Indeed, before one sets \u03bb = /6 in the argument of Lemma B.12 in [Shalev-Shwartz and Ben-David, 2014], the desired inequalities are obtained\nThen, for all > 0 we have\nProof. Our statement is immediate if \u03bb = 1/3 in the second inequality in Lemma A.1.\nThen, for all \u2208 (0, 1) we have\nProof. In order to prove our statement, it is enough to prove the two following inequalities:\nTo prove both bounds, we use Chernoff's bounding method.\n(a) Proof of the first inequality of (5). We first compute E e \u03bbX1Y1 . Since \u03bb \u2208 (0, 1) and X 1 , Y 1 are Definition A.13. Let V \u2282 k n be an irreducible algebraic set. We define\nWe call dim V the dimension of V .\nRemark 18. It is well-known that the dimension of an irreducible algebraic set is finite.\nDefinition A.14. Let X \u2282 k n be a subset of k n . Then we set\nWhen we emphasize a field k, we denote I(X) by I k (X).\nDefinition A.15. Let S \u2282 k n . We define S = V (I(S)).\nDefinition A.16. Let I and J be ideals. Then we set\nWe call I : J the ideal quotient of I by J.\nRemark A.17. An ideal quotient is an ideal.\nThe following facts are well-known. Our main reference is [Cox et al., 2015].\nLemma A.18. Let S and T be subsets of k n . Then we have Theorem A.19 ([Cox et al., 2015, Ch. 4 Sect. 6 Theorem 4]). Let V \u2282 k n be an algebraic set. Then, V has a minimal decomposition\nFurthermore, this minimal decomposition is unique up to the order in which V 1 , . . . , V s are written. Cox et al., 2015, Ch. 4 Sect. 4 Corollary 11]). Let V and W be algebraic sets over k.\nThen we have I(V ) : I(W ) = I(V \\ W ).\nTheorem A.22 (The Strong Nullstellensatz, [Cox et al., 2015, Ch 4 Sect.\nLemma A.24. Let V be an irreducible algebraic set and let W be an algebraic set. If W \u2282 V , then V \\ W = V .", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "A.3.2 An Ideal Given by VCA(X,Y)", "text": "In this section, we prove Theorem 7. In the following, we consider the case when k = R or C.\nDefinition A.25. Let I be an ideal in R n . Then we define an ideal I C in C[x 1 , . . . , x n ] as follows:\nProof. By the definitions of \u221a \u2022 and ( \u2022 ) C , we can prove the statement easily.\nTheorem A.27 (Theorem 17). Let X, Y \u2282 R n be distinct point sets and let G be a polynomial set, which is the output of VCA(X, Y ) for = 0. We put V = V k (G) and denote its irreducible components by V 1 , . . . , V s . Then, for any irreducible algebraic set W \u2282 k n satisfying dim W = min dim V i and Y \u2282 W , we have\n(1) V \u2282 W and W \u2282 V .\n(2) V = V \\ W .\n(\nMoreover, if k = C, then we have\nProof.\n(1): We first prove V \u2282 W . If we assume V \u2282 W , then we have V i \u2282 W for all i. Also, by the assumption, dim V i0 = dim W for some i 0 . Since V i0 \u2282 W , by Lemma A.23, V i0 = W . Hence, we have Y \u2282 W = V i0 \u2282 V . This leads us to a contradiction. Therefore, V \u2282 W . We next prove W \u2282 V . If we assume that W \u2282 V , then there exits i 0 such that W \u2282 V i0 by Lemma A.20. Hence, we have Y \u2282 W \u2282 V i0 \u2282 V . This also leads us to a contradiction.\n(2) and ( 3): Before we prove (2) and (3), we start with the following claims. Claim: Y \u2282 V . Proof of Claim. We assume Y \u2282 V = V k (G). This means that g(Y ) = 0 for g \u2208 G. However, this is impossible as g is nonvanishing for Y . Therefore, Y \u2282 V .\nClaim: V i \u2282 W for all V i . Proof of Claim. If we assume V i \u2282 W for some V i , then dim V i \u2264 dim W holds. By the condition of the dimension of W , we have dim W = dim V i . By Lemma A.23, W = V i . Hence, we have Y \u2282 W = V i \u2282 V . This leads us to a contradiction. Now we go back to prove (2) and (3). Using Lemmas A.18 and A.24, we get that\nHence, by Lemma A.18 and Proposition A.21, we have\n(3)': The statement is proven by the Nullstellensatz and Lemma A.26.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Stable border bases for ideals of points", "journal": "Journal of Symbolic Computation", "year": "2008", "authors": "J Abbott; C Fassino; M.-L Torrente"}, {"ref_id": "b1", "title": "Analytic manifold learning: Unifying and evaluating representations for continuous control", "journal": "", "year": "2020", "authors": "R Antonova; M Maydanskiy; D Kragic; S Devlin; K Hofmann"}, {"ref_id": "b2", "title": "Classification-based anomaly detection for general data", "journal": "", "year": "2020", "authors": "L Bergman; Y Hoshen"}, {"ref_id": "b3", "title": "Ideals, varieties, and algorithms. Undergraduate Texts in Mathematics", "journal": "Springer", "year": "2015", "authors": "D A Cox; J Little; D O'shea"}, {"ref_id": "b4", "title": "Almost vanishing polynomials for sets of limited precision points", "journal": "Journal of Symbolic Computation", "year": "2010", "authors": "C Fassino"}, {"ref_id": "b5", "title": "Deep anomaly detection using geometric transformations", "journal": "Curran Associates, Inc", "year": "2018", "authors": "I Golan; R El-Yaniv"}, {"ref_id": "b6", "title": "Computing all border bases for ideals of points", "journal": "Journal of Algebra and Its Applications", "year": "2019", "authors": "A Hashemi; M Kreuzer; S Pourkhajouei"}, {"ref_id": "b7", "title": "Approximate computation of zero-dimensional polynomial ideals", "journal": "Journal of Symbolic Computation", "year": "2009", "authors": "D Heldt; M Kreuzer; S Pokutta; H Poulisse"}, {"ref_id": "b8", "title": "Discriminative vanishing component analysis", "journal": "AAAI Press", "year": "2016", "authors": "C Hou; F Nie; D Tao"}, {"ref_id": "b9", "title": "Principal variety analysis", "journal": "PMLR", "year": "2017", "authors": "R Iraji; H Chitsaz"}, {"ref_id": "b10", "title": "Algebraic method for the reconstruction of partially observed nonlinear systems using differential and integral embedding", "journal": "Mathematics", "year": "2020", "authors": "A Karimov; E G Nepomuceno; A Tutueva; D Butusov"}, {"ref_id": "b11", "title": "Characterizations of border bases", "journal": "Journal of Pure and Applied Algebra", "year": "2005", "authors": "A Kehrein; M Kreuzer"}, {"ref_id": "b12", "title": "Computing border bases", "journal": "Journal of Pure and Applied Algebra", "year": "2006", "authors": "A Kehrein; M Kreuzer"}, {"ref_id": "b13", "title": "Border basis computation with gradientweighted normalization", "journal": "", "year": "2022", "authors": "H Kera"}, {"ref_id": "b14", "title": "Noise-tolerant algebraic method for reconstruction of nonlinear dynamical systems", "journal": "Nonlinear Dynamics", "year": "2016", "authors": "H Kera; Y Hasegawa"}, {"ref_id": "b15", "title": "Approximate vanishing ideal via data knotting", "journal": "AAAI Press", "year": "2018", "authors": "H Kera; Y Hasegawa"}, {"ref_id": "b16", "title": "Spurious vanishing problem in approximate vanishing ideal", "journal": "IEEE Access", "year": "2019", "authors": "H Kera; Y Hasegawa"}, {"ref_id": "b17", "title": "Gradient boosts the approximate vanishing ideal", "journal": "AAAI Press", "year": "2020", "authors": "H Kera; Y Hasegawa"}, {"ref_id": "b18", "title": "Monomial-agnostic computation of vanishing ideals", "journal": "", "year": "2021", "authors": "H Kera; Y Hasegawa"}, {"ref_id": "b19", "title": "Vanishing ideal genetic programming", "journal": "IEEE", "year": "2016", "authors": "H Kera; H Iba"}, {"ref_id": "b20", "title": "Regression for sets of polynomial equations", "journal": "", "year": "2012", "authors": "F J Kir\u00e1ly; P Von; J S B\u00fcnau; D A M\u00fcller; F C Blythe; K.-R Meinecke;  M\u00fcller"}, {"ref_id": "b21", "title": "Dualto-kernel learning with ideals", "journal": "", "year": "2014", "authors": "F J Kir\u00e1ly; M Kreuzer; L Theran"}, {"ref_id": "b22", "title": "A computational algebra approach to the reverse engineering of gene regulatory networks", "journal": "Journal of Theoretical Biology", "year": "2004", "authors": "R Laubenbacher; B Stigler"}, {"ref_id": "b23", "title": "Computer algebra in systems biology", "journal": "American Mathematical Monthly", "year": "2009", "authors": "R Laubenbacher; B Sturmfels"}, {"ref_id": "b24", "title": "Computation of approximate border bases and applications", "journal": "", "year": "2013", "authors": "J Limbeck"}, {"ref_id": "b25", "title": "Vanishing component analysis", "journal": "PMLR", "year": "2013", "authors": "R Livni; D Lehavi; S Schein; H Nachliely; S Shalev-Shwartz; A Globerson"}, {"ref_id": "b26", "title": "The construction of multivariate polynomials with preassigned zeros", "journal": "Springer", "year": "1982", "authors": "H M M\u00f6ller; B Buchberger"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "probability that gi is t-vanishing for a point in V Theoretical lower bound", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure2: The probability that a basis polynomial is t-vanishing for a point in V . Basis polynomials g 1 , . . . , g 19 (solid lines in different colors) obtained by VCA(X, Z). All the probabilities are higher than the theoretical value. This means justification on Theorem 6.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Definition A.10. Let G \u2282 k[x 1 , . . . , x n ]. Then we set V (G) = {x \u2208 k n | f (x) = 0 for all f \u2208 G}.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Anomaly detection accuracy (AUC score \u00d7100) in MNIST and FashionMNIST in the case when a normal class is a collection of samples labeled as 0, 2, 4, 6, 8. Here, Rot. denotes the rotation transformations. R.A. denotes random affine transformations. The VCA and the nVCA resulted in no discriminability between data, which implies that basis polynomials have no discriminability in the hand-written-digit space. The Rot. version of our proposed method outperforms the R.A. version. This implies that, by choosing normalizing data from the target space, we can obtain basis polynomials with discriminability. Note that the results of GOAD are reported as a reference.", "figure_data": "Size ofMethodData settraining setOursGOAD VCA nVCA Rot. R.A.MNIST60,000 10,00076.2 -51.8 48.449.1 50.779.1 54.9 50.2 59.1FashionMNIST60,000 10,00093.5 -50.2 53.951.7 46.283.8 53.3 82.3 49.3"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ": Anomaly detection accuracy (AUC score\u00d7100) in MNIST and FashionMNIST in the casewhen a normal class is a collection of samples labeledas 0, 2, 4, 6, 8. Here, Rot. denotes the rotation trans-formations. R.A. denotes random affine transforma-tions. The results imply that the GOAD frameworkis improved by using feature extract transformation(e.g., rotation for image data).MethodTraining setVCAnVCARot. R.A. Rot. R.A."}], "formulas": [{"formula_id": "formula_0", "formula_text": "I(X) = {g \u2208 R[x 1 , . . . , x n ] | g(x) = 0 for all x \u2208 X}", "formula_coordinates": [1.0, 312.03, 307.83, 225.8, 9.68]}, {"formula_id": "formula_1", "formula_text": "V X V Y V H R n", "formula_coordinates": [2.0, 332.07, 127.01, 163.56, 104.71]}, {"formula_id": "formula_2", "formula_text": "I(X) = {g \u2208 R[x 1 , . . . , x n ] | g(x) = 0 for all x \u2208 X}.", "formula_coordinates": [3.0, 310.64, 441.39, 228.57, 9.68]}, {"formula_id": "formula_3", "formula_text": "h(X) = h(x 1 ) h(x 2 ) \u2022 \u2022 \u2022 h(x |X| ) \u2208 R |X| , where | \u2022 | denotes the cardinality of a set. For a set of polynomials H = {h 1 , h 2 , . . . , h |H| }, its evalua- tion matrix is H(X) = h 1 (X) h 2 (X) \u2022 \u2022 \u2022 h |H| (X) \u2208 R |X|\u00d7|H| .", "formula_coordinates": [3.0, 310.61, 509.55, 228.65, 71.88]}, {"formula_id": "formula_4", "formula_text": "|H| i=1 w i h i , where w i \u2208 R, is represented as |H| i=1 w i h i (X). We denote |H| i=1 w i h i by Hw with w = w 1 \u2022 \u2022 \u2022 w |H| T .", "formula_coordinates": [3.0, 501.91, 662.97, 37.34, 14.11]}, {"formula_id": "formula_5", "formula_text": ") = { h\u2208H a h h | a h \u2208 R} \u2282 R n and H = { h\u2208H f h h | f h \u2208 R n } \u2282 R n .", "formula_coordinates": [4.0, 72.0, 202.77, 228.14, 23.1]}, {"formula_id": "formula_6", "formula_text": "C pre t = {pq | p \u2208 F 1 , q \u2208 F t\u22121 }. At t = 1, C pre 1 = {x 1 , x 2 , . . . , x n }.", "formula_coordinates": [4.0, 310.61, 204.08, 180.87, 31.37]}, {"formula_id": "formula_7", "formula_text": "C t = C pre t \u2212 F t\u22121 (F t\u22121 (X)) \u2020 C pre t (X),(1)", "formula_coordinates": [4.0, 341.88, 251.65, 197.37, 13.35]}, {"formula_id": "formula_8", "formula_text": "C t (X) C t (X)V = N(C t )V \u039b, (2", "formula_coordinates": [4.0, 361.31, 340.12, 173.7, 9.73]}, {"formula_id": "formula_9", "formula_text": ")", "formula_coordinates": [4.0, 535.0, 340.2, 4.24, 8.74]}, {"formula_id": "formula_10", "formula_text": "G t = {C t v i | \u03bb i \u2264 }, F t = {C t v i | \u03bb i > }.", "formula_coordinates": [4.0, 374.09, 488.7, 101.67, 27.28]}, {"formula_id": "formula_11", "formula_text": "g(Y ) 2 /|Y | = 1 for any g \u2208 G. Hence, g is a |Y |-nonvanishing polynomial for Y .", "formula_coordinates": [4.0, 310.61, 563.11, 228.64, 35.62]}, {"formula_id": "formula_12", "formula_text": "T i : R n \u2192 R n for i = 1, . . . , M . We then compute VCA(X i , Y i ), where Y i = j =i X j . The obtained set of generators is de- noted by G i = {g (i) 1 , . . . , g (i) |Gi| }. By our construction, G i (x) for new sample x \u2208 R n is expected to take small values if x \u2208 V (G i ). Moreover, an algebraic set V (G i ) = |Gi| j=1 V (g (i) j", "formula_coordinates": [5.0, 72.0, 559.59, 228.65, 91.83]}, {"formula_id": "formula_13", "formula_text": "P i (x) = e \u2212 Gi(Tix) 2 M k=1 e \u2212 G k (Tix) 2 .", "formula_coordinates": [5.0, 363.51, 148.03, 122.84, 29.77]}, {"formula_id": "formula_14", "formula_text": "Score(x) = \u2212 log M i=1 P i (x) = \u2212 M i=1 log P i (x). (3)", "formula_coordinates": [5.0, 321.83, 281.88, 217.42, 30.32]}, {"formula_id": "formula_15", "formula_text": "P i (x) = e \u2212 Gi(Tix)\u2212ci 2 + M k=1 e \u2212 G k (Tix)\u2212c k 2 + M .", "formula_coordinates": [5.0, 342.69, 440.6, 164.47, 29.77]}, {"formula_id": "formula_16", "formula_text": "(i) 1 , . . . , w |Gi| g (i)", "formula_coordinates": [5.0, 459.5, 514.76, 67.92, 13.95]}, {"formula_id": "formula_17", "formula_text": "g(X) = 0 and g(Z) 2 |Z| = 1.", "formula_coordinates": [6.0, 125.78, 537.55, 126.06, 23.88]}, {"formula_id": "formula_18", "formula_text": "(1 \u2264 i \u2264 |Z|, 1 \u2264 j \u2264 n) are i.i.d. \u223c N (0, 1). If w = n i=k+1 w i u i is a random vector such that w i 's are i.i.d. \u223c N (0, 1/(n \u2212 k))", "formula_coordinates": [6.0, 310.61, 127.96, 228.64, 44.6]}, {"formula_id": "formula_19", "formula_text": "P [ g (X) = 0] = 1, P g (Z) 2 |Z| \u2212 1 \u2265 2 \u2264 e \u2212 2 N/6 + e \u2212 N/3 + (N 2 \u2212 N )e \u2212\u03b7 2 |Z|/2 + N (e \u2212\u03b7 2 |Z|/6 + e \u2212\u03b7|Z|/3 ),", "formula_coordinates": [6.0, 310.61, 191.8, 231.93, 72.84]}, {"formula_id": "formula_20", "formula_text": "V X + V Y = span(u 1 , . . . , u k+m ).", "formula_coordinates": [6.0, 352.94, 428.31, 140.32, 9.68]}, {"formula_id": "formula_21", "formula_text": "p X \u2208 V X , p Y \u2208 V Y and p \u22a5 \u2208 (V X + V Y ) \u22a5 such that coeffi- cients of u 1 , . . . , u n are i.i.d.\u223c N (0, 1). Then, for t > 0, we have P [|g(p X )| = 0] = 1, P [|g(p Y )| < t] \u2265 1 \u2212 m n \u2212 k \u2022 1 t 2 , P |g(p \u22a5 )| \u2265 n \u2212 (k + m) 2(n \u2212 k) \u2265 n \u2212 (k + m) 4(n \u2212 (k + m) + 2)", "formula_coordinates": [6.0, 310.61, 459.51, 228.65, 120.52]}, {"formula_id": "formula_22", "formula_text": "Definition 7. Let G \u2282 k[x 1 , . . . , x n ]. Then we set V (G) = {x \u2208 k n | f (x) = 0 for all f \u2208 G}.", "formula_coordinates": [7.0, 72.0, 343.94, 220.92, 30.59]}, {"formula_id": "formula_23", "formula_text": "V k (G). An algebraic set V \u2282 k n is irreducible if V = V 1 \u222a V 2 , where V 1 and V 2 are algebraic sets over k, then V 1 = V or V 2 = V . Definition 8. Let V \u2282 k n be an algebraic set. A decomposition V = V 1 \u222a \u2022 \u2022 \u2022 \u222a V s , where each V i is an irreducible variety, is called a minimal decomposition if V i \u2282 V j for i = j. Also, we call the V i the irreducible components of V i . Theorem 9. Let V \u2282 k n be an algebraic set. Then, V has a minimal decomposition V = V 1 \u222a \u2022 \u2022 \u2022 \u222a V s .", "formula_coordinates": [7.0, 72.0, 411.52, 228.65, 198.5]}, {"formula_id": "formula_24", "formula_text": "dim V = sup{r | V 0 V 1 \u2022 \u2022 \u2022 V r = V, V i : irreducible over k}. We call dim V the dimension of V .", "formula_coordinates": [7.0, 310.61, 163.97, 202.98, 47.73]}, {"formula_id": "formula_25", "formula_text": "I(X) = {g \u2208 k[x 1 , . . . , x n ] | g(x) = 0 for all x \u2208 X}.", "formula_coordinates": [7.0, 311.49, 275.09, 226.87, 9.68]}, {"formula_id": "formula_26", "formula_text": "I : J = {g \u2208 k[x 1 , . . . , x n ] | gJ \u2282 I}.", "formula_coordinates": [7.0, 346.91, 416.52, 156.04, 9.65]}, {"formula_id": "formula_27", "formula_text": ") : I(W ) = I(V \\ W ).", "formula_coordinates": [7.0, 405.45, 510.74, 95.35, 8.74]}, {"formula_id": "formula_28", "formula_text": "{f | f m \u2208 I for some integer m \u2265 1}.", "formula_coordinates": [7.0, 344.26, 602.88, 161.34, 10.81]}, {"formula_id": "formula_29", "formula_text": "2 Theorem 6]). Let G \u2282 k[x 1 , . . . , x n ]. If k is algebraically closed, then I(V (G)) = G .", "formula_coordinates": [7.0, 310.61, 642.03, 228.64, 32.65]}, {"formula_id": "formula_30", "formula_text": "Definition 16. Let I be an ideal in R n = R[x 1 , . . . , x n ].", "formula_coordinates": [8.0, 72.0, 274.69, 228.64, 21.64]}, {"formula_id": "formula_31", "formula_text": "I C = i f i g i | f i \u2208 C[x 1 , . . . , x n ], g i \u2208 I .", "formula_coordinates": [8.0, 91.0, 329.81, 190.65, 19.91]}, {"formula_id": "formula_32", "formula_text": ") V = V \\ W . (2", "formula_coordinates": [8.0, 81.96, 444.85, 69.39, 20.84]}, {"formula_id": "formula_33", "formula_text": ") I k (V ) = I k (V \\ W ) = I k (V ) : I k (W ). Moreover, if k = C, then we have G C = I C (V ) = I C (V \\ W ) = I C (V ) : I C (W ).3", "formula_coordinates": [8.0, 81.96, 456.95, 216.26, 52.27]}, {"formula_id": "formula_34", "formula_text": "P i (t) = |{p \u2208 R | |g i (p)| < t}|/|R|.", "formula_coordinates": [8.0, 324.44, 572.54, 149.09, 9.65]}, {"formula_id": "formula_35", "formula_text": "F(x) = (|g 1 (x)| , . . . , g |G| (x) ) ,(4)", "formula_coordinates": [9.0, 353.43, 357.38, 185.82, 9.96]}, {"formula_id": "formula_36", "formula_text": "E e \u03bbX1Y1 = 1 2\u03c0 \u221e \u2212\u221e \u221e \u2212\u221e e \u03bbxy e \u2212x 2 2 e \u2212y 2 2 dxdy = 1 2\u03c0 \u221e \u2212\u221e e \u2212y 2 2 \u221e \u2212\u221e e \u22121 2 (x\u2212\u03bby) 2 + 1 2 \u03bb 2 y 2 dxdy = 1 \u221a 2\u03c0 \u221e \u2212\u221e e \u2212y 2 2 e 1 2 \u03bb 2 y 2 dy = 1 \u221a 1 \u2212 \u03bb 2 1 2\u03c0 1 1\u2212\u03bb 2 \u221e \u2212\u221e e \u2212y 2 2/(1\u2212\u03bb 2 ) dy = 1 \u221a 1 \u2212 \u03bb 2 .", "formula_coordinates": [15.0, 187.55, 147.88, 235.8, 144.25]}, {"formula_id": "formula_37", "formula_text": "P [Z/M \u2265 ] = P [Z \u2265 M ] = P e \u03bbZ \u2265 e \u03bb M \u2264 e \u2212\u03bb M E e \u03bbZ = e \u2212\u03bb M (1 \u2212 \u03bb 2 ) \u2212M/2 \u2264 e \u2212\u03bb M e \u03bb 2 M/2 ,", "formula_coordinates": [15.0, 229.44, 329.85, 151.86, 76.61]}, {"formula_id": "formula_38", "formula_text": "( * ) E e \u2212\u03bbX1Y1 = (1 \u2212 \u03bb 2 ) \u22121/2 ,", "formula_coordinates": [15.0, 236.69, 468.9, 137.86, 10.81]}, {"formula_id": "formula_39", "formula_text": "P [Z/M \u2264 \u2212 ] = P [\u2212Z \u2265 M ] \u2264 e \u2212\u03bb M E e \u2212\u03bbZ .", "formula_coordinates": [15.0, 197.62, 516.69, 216.0, 10.81]}, {"formula_id": "formula_40", "formula_text": "Lemma A.4. Let a i,j (1 \u2264 i \u2264 M, 1 \u2264 j \u2264 N ) be M N independent normally distributed variables. Put A = [a i,j ] 1\u2264i\u2264M, 1\u2264j\u2264N .", "formula_coordinates": [15.0, 72.0, 570.46, 467.25, 21.64]}, {"formula_id": "formula_41", "formula_text": "P 1 M A T A \u2212 I N max \u2265 \u2264 (N 2 \u2212 N )e \u2212 2 M/2 + N (e \u2212 2 M/6 + e \u2212 M/3 ),", "formula_coordinates": [15.0, 140.35, 605.94, 320.59, 24.86]}, {"formula_id": "formula_42", "formula_text": "P 1 M A T A \u2212 I N max \u2265 \u2264 1\u2264i\u2264j\u2264N P a T i a j M \u2212 \u03b4 ij \u2265 = N P a i 2 M \u2212 1 \u2265 + (N 2 \u2212 N ) 2 P a T 1 a 2 M \u2212 \u03b4 ij \u2265", "formula_coordinates": [16.0, 175.9, 149.15, 247.38, 87.07]}, {"formula_id": "formula_43", "formula_text": "b T (C \u2212 I N )b \u2264 C \u2212 I N max ( b 1 ) 2 and b 1 \u2264 \u221a N b ,", "formula_coordinates": [16.0, 174.24, 315.96, 269.41, 18.57]}, {"formula_id": "formula_44", "formula_text": "b T (C \u2212 I N )b \u2264 C \u2212 I N max ( b 1 ) 2 < (1 + )N ( b 1 ) 2 \u2264 1 + b 2 \u2264 .", "formula_coordinates": [16.0, 226.34, 370.2, 161.4, 73.77]}, {"formula_id": "formula_45", "formula_text": "b T Cb \u2212 1 \u2264 b T Cb \u2212 b T b + b T b \u2212 1 = b T (C \u2212 I N )b + b 2 \u2212 1 < 2 .", "formula_coordinates": [16.0, 206.33, 475.75, 201.91, 28.18]}, {"formula_id": "formula_46", "formula_text": "(1 \u2264 i \u2264 |Z|, 1 \u2264 j \u2264 n) are i.i.d. \u223c N (0, 1). If w = n i=k+1 w i u i is a random vector such that w i 's are i.i.d. \u223c N (0, 1/(n \u2212 k)), then we have P [ g(X) = 0] = 1, P g(Z) 2 |Z| \u2212 1 \u2265 2 \u2264 e \u2212 2 N/6 + e \u2212 N/3 + (N 2 \u2212 N )e \u2212\u03b7 2 |Z|/2 + N (e \u2212\u03b7 2 |Z|/6 + e \u2212\u03b7|Z|/3 ),(6)", "formula_coordinates": [16.0, 72.0, 578.66, 467.25, 74.17]}, {"formula_id": "formula_47", "formula_text": "g(Z) 2 = |Z| i=1 (w T p i ) 2 = b T A T Ab.", "formula_coordinates": [17.0, 234.23, 183.2, 147.78, 31.18]}, {"formula_id": "formula_48", "formula_text": "P |Z| i=1 (w T p i ) 2 |Z| \u2212 1 \u2265 2 \u2264 P 1 |Z| A T A \u2212 I N max \u2265 \u03b7 + P | w 2 \u2212 1| \u2265 ,", "formula_coordinates": [17.0, 119.11, 245.79, 363.07, 27.96]}, {"formula_id": "formula_49", "formula_text": "P 1 |Z| A T A \u2212 I N max \u2265 \u03b7 \u2264 (N 2 \u2212 N )e \u2212\u03b7 2 |Z|/2 + N (e \u2212\u03b7 2 |Z|/6 + e \u2212\u03b7|Z|/3 ), P w 2 \u2212 1 \u2265 \u2264 e \u2212 2 N/6 + e \u2212 N/3 .(7)", "formula_coordinates": [17.0, 137.9, 310.55, 401.35, 39.76]}, {"formula_id": "formula_50", "formula_text": "N b 2 = n i=k+1 ( \u221a n \u2212 kw i ) 2 \u223c \u03c7 2 N .", "formula_coordinates": [17.0, 231.14, 407.29, 148.97, 30.55]}, {"formula_id": "formula_51", "formula_text": "p X \u2208 V X , p Y \u2208 V Y and p \u22a5 \u2208 (V X + V Y ) \u22a5 such that coefficients of u 1 , . . . , u n are i.i.d.\u223c N (0, 1). Then, for t > 0, we have P [|g(p X )| = 0] = 1, P [|g(p Y )| < t] \u2265 1 \u2212 m n \u2212 k \u2022 1 t 2 , P |g(p \u22a5 )| \u2265 n \u2212 (k + m) 2(n \u2212 k) \u2265 n \u2212 (k + m) 4(n \u2212 (k + m) + 2) .(8)", "formula_coordinates": [17.0, 72.0, 572.15, 467.25, 101.89]}, {"formula_id": "formula_52", "formula_text": "E[g(p Y )] = k+m i=k+1 E[a i w i ] = 0, Var[g(p Y )] = k+m i=k+1 Var [a i w i ] = m n \u2212 k .", "formula_coordinates": [18.0, 220.67, 262.13, 169.9, 66.64]}, {"formula_id": "formula_53", "formula_text": "P [|g(p Y )| < t] \u2265 1 \u2212 m n \u2212 k \u2022 1 t 2 . (c)", "formula_coordinates": [18.0, 81.96, 350.83, 293.18, 37.17]}, {"formula_id": "formula_54", "formula_text": "E g(p \u22a5 ) 2 = E g(p \u22a5 ) 2 \u2212 (E [g(p \u22a5 )]) 2 = Var w T p \u22a5 = n \u2212 (k + m) n \u2212 k .", "formula_coordinates": [18.0, 213.69, 432.05, 183.87, 40.32]}, {"formula_id": "formula_55", "formula_text": "E g(p \u22a5 ) 4 = E[(w T p \u22a5 ) 4 ] = E \uf8ee \uf8f0 n i=k+m+1 a i w i 4 \uf8f9 \uf8fb = (i1,i2,i3,i4)\u2208{k+m+1,...,n} 4 E i1,i2,i3,i4 ,", "formula_coordinates": [18.0, 201.77, 507.66, 207.7, 80.48]}, {"formula_id": "formula_56", "formula_text": "E i1,i2,i3,i4 = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 9/(n \u2212 k) 2 if i 1 = i 2 = i 3 = i 4 1/(n \u2212 k) 2 if i 1 = i 2 = i 3 = i 4 1/(n \u2212 k) 2 if i 1 = i 3 = i 2 = i 4 1/(n \u2212 k) 2 if i 1 = i 4 = i 2 = i 3 0 otherwise.", "formula_coordinates": [18.0, 200.18, 612.57, 204.22, 58.52]}, {"formula_id": "formula_57", "formula_text": "E g(p \u22a5 ) 4 = 9 (n \u2212 k) 2 (n \u2212 k \u2212 m) + 1 (n \u2212 k) 2 3(n \u2212 k \u2212 m)(n \u2212 k \u2212 m \u2212 1) = 3(n \u2212 k \u2212 m)(n \u2212 k \u2212 m + 2) (n \u2212 k) 2 .", "formula_coordinates": [19.0, 141.67, 143.84, 327.9, 49.83]}, {"formula_id": "formula_58", "formula_text": "P |g(p \u22a5 )| > \u03b8E[g(p \u22a5 ) 2 ] = P g(p \u22a5 ) 2 > \u03b8E g(p \u22a5 ) 2 \u2265 (1 \u2212 \u03b8) 2 (E g(p \u22a5 ) 2 ) 2 E[g(p \u22a5 ) 4 ] = (1 \u2212 \u03b8) 2 n \u2212 (k + m) n \u2212 (k + m) + 2 ,", "formula_coordinates": [19.0, 183.33, 224.59, 241.77, 71.47]}, {"formula_id": "formula_59", "formula_text": "V = V 1 \u222a \u2022 \u2022 \u2022 \u222a V s ,", "formula_coordinates": [19.0, 266.53, 633.57, 78.19, 9.65]}], "doi": "10.1016/j.jsc.2008.05.002"}