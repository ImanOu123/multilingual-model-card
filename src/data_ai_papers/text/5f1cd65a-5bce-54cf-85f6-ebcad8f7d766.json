{"title": "On the Versatile Uses of Partial Distance Correlation in Deep Learning", "authors": "Xingjian Zhen; Zihang Meng; Rudrasis Chakraborty; Vikas Singh", "pub_date": "", "abstract": "Comparing the functional behavior of neural network models, whether it is a single network over time or two (or more networks) during or post-training, is an essential step in understanding what they are learning (and what they are not), and for identifying strategies for regularization or efficiency improvements. Despite recent progress, e.g., comparing vision transformers to CNNs, systematic comparison of function, especially across different networks, remains difficult and is often carried out layer by layer. Approaches such as canonical correlation analysis (CCA) are applicable in principle, but have been sparingly used so far. In this paper, we revisit a (less widely known) from statistics, called distance correlation (and its partial variant), designed to evaluate correlation between feature spaces of different dimensions. We describe the steps necessary to carry out its deployment for large scale models -this opens the door to a surprising array of applications ranging from conditioning one deep model w.r.t. another, learning disentangled representations as well as optimizing diverse models that would directly be more robust to adversarial attacks. Our experiments suggest a versatile regularizer (or constraint) with many advantages, which avoids some of the common difficulties one faces in such analyses 1 .", "sections": [{"heading": "Introduction", "text": "The extent to which popular architectures in computer vision even partly mimic human vision continues to be studied (and debated) in our community. But consider the following hypothetical scenario. Let us say that a fully functional computational model of the visual system -perhaps a modern version of the Neocognitron [20] -was somehow provided to us. And we wished to \"compare\" its behavior to modern CNN models [33,28]. To do so, two options appear sensible. The first -inspired by analogies between computational vision and biological vision -would draw a correspondence between how simple/complex cells in the visual cortex process scenes and their induced receptive fields with those of activations of units/blocks in a modern deep neural network architecture [61]. While this process is often difficult to carry out systematically, it is powerful and, 1 Code is at https://github.com/zhenxingjian/Partial_Distance_Correlation arXiv:2207.09684v3 [cs.CV] 8 Nov 2022 in some ways, has contributed to interest in biologically inspired deep learning, see [69]. Updated forms of this intuition -associating different subsets of cells (or neural network units) to different semantic/visual concepts -remains the default approach we use in debugging and interpretation. The second option for tackling the hypothetical setting above is to pose it in an information theoretic setting. That is, for two models \u0398 X and \u0398 Y , we ask the following question: what has \u0398 X learned that \u0398 Y has not? Or vice versa. The asymmetry is intentional because if we consider two random variables (r.v.) X, Y , the question simply takes the form of \"conditioning\", i.e., compare P(X) versus P(X|Y ). This form suffices if our interest is restricted to the predictions of the two models. If we instead wish to capture the model's behavior more globally -when X and Y denote the full set of feature responses -we can use divergence measures on high dimensional probability measures given by the two models (\u0398 X and \u0398 Y ) responses on the training samples. Importantly, notice that our description assumes that, at least, the probability measures are defined on the same domain.\nMore general use cases. While the above discussion was cast as comparing two networks, it is representative of a broad basket of tasks in deep learning. (a) Consider the problem of learning fair representations [73,17,72,45] where the model must be invariant to one (or more) sensitive attributes. We seek latent representations, say \u03a8 pred (X) for the prediction task, which minimizes mutual information w.r.t. the latent representation relevant for predicting the sensitive attribute \u03a8 sens (X). Indeed, if information regarding the sensitive attribute is partially preserved or leaks into \u03a8 pred (X), the relative entropy will be low [50]. Observe that this calculation is possible partly because the latent space specifies the same probability space for the two distributions. (b) The setting is identical in common approaches for learning disentangled representations, where disentanglement is measured via various information theoretic measures [8,1,21,62]. If we now segue back to comparing two different networks, but without the convenience of a common coordinate system to measure divergence, the options turn out to be limited. (c) Recently, in trying to understand whether vision Transformers \"see\" similar to convolutional neural networks [57], one option utilized recently was a kernel-based representation similarity, in a layer-by-layer manner. What we may actually want is a mechanism for conditioning -for example, if one of the models is thought of a \"nuisance variable\", we wish to check the residual in the other after the first has been controlled for (or marginalized out). Importantly, this should be possible without assuming that the probability distributions live in the same space (or networks \u0398 X and \u0398 Y are the same).\nA direct application of CCA? Consider two different feature spaces (X and Y), say in dimensions R p and R q , pertaining to feature activations from two different models. Comparison of these two feature spaces is possible. One natural choice is canonical correlation analysis (CCA) [5], a generalization of correlation, specifically suited when p \u0338 = q. The idea has been utilized for studying representation similarity in deep neural network models [49], albeit in a posttraining setting for reasons that will be clear shortly, as well as for identifying more efficient training regimes (i.e., can lower layers be sequentially frozen after a certain number of timesteps). CCA has also been shown to be implementable within DNN pipelines for multi-view training, called DeepCCA [4], although efficiency can be a bottleneck limiting its broader deployment. A stochastic version of CCA suitable for DNN training with mini-batches has been proposed very recently, and strong experimental evidence was presented [48], also see [25]. Given that a stochastic CCA is now available, its extensions to the partial CCA setting are not yet available. If successful, this may eventually provide a scheme, suitable for deep learning, for controlling the influence of one model (or a set of variables) on another model. This work. The starting point of this work is a less widely used statistical concept to measure the correlation between two different feature spaces (X , Y) of different dimensions, called distance correlation (and the method of dissimilarities). In shallow settings, CCA and distance correlation offers very similar functionalityfor the most part, they can be used interchangeably although distance correlation would also need specification of distances (or dissimilarities). In other words, CCA may be easier to deploy. On the other hand, deep variants of CCA involve specialized algorithms [4,48]. Further, deep versions of partial CCA have not been reported. In contrast, as long as feature distances can be calculated, the differences between the shallow and the deep versions of distance correlation are minimal at best, and adjustments needed are quite minor. These advantages carry over to partial distance correlation, directly enabling conditioning one model w.r.t. another (or using such a term as a regularizer). The main contribution of this paper is to study distance correlation (and partial distance correlation) as a powerful measure in a broad suite of tasks in vision. We review the relevant technical steps which enable its instantiation in deep learning settings and show its broad applications ranging from learning disentangled representations to understanding the differences between what two (or more) networks are learning to training \"mutually distinct\" deep models (akin to earlier works on M best solutions to MAP estimation in graphical models [19,71]) or training M diverse models for foreground-background segmentation as well as other tasks [27].", "publication_ref": ["b19", "b32", "b27", "b61", "b0", "b69", "b73", "b16", "b72", "b44", "b50", "b7", "b0", "b20", "b62", "b57", "b4", "b49", "b3", "b48", "b24", "b3", "b48", "b18", "b71", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Related Works", "text": "Four distinct lines of work are related to our development, which we review next. Similarity between networks. Understanding the similarity between different networks is an active topic [38,24,51] also relevant in adversarial models [15,9]. Early attempts to compare neural network representations were approached via linear regression [59], whose applicability to nonlinear models is limited. As noted above, canonical correlation analysis (CCA) [3,31] is a suitable off-the-shelf method for model comparisons. To this end, singular vector CCA (SVCCA) [56], Projection-Weighted CCA [49], DeepCCA [4], and stochastic CCA [23] are all potentially useful. Recently, [37] studied the invariance properties for a good similarity measurement and proposed the centered kernel alignment (CKA). CKA offers invariance to invertible linear transformations, orthogonal transformations, and isotropic scaling. Separately, [52,57] used CKA to study similarities between deep and wide neural networks and also between different network structures.\nInformation theoretic divergence measures. Another body of related work pertains to approximately measuring the mutual information [12] to remove this information, mainly in the context of fair representation learning. Here, mutual information (MI) is measured between features and the sensitive attribute [50]. In [64], another information theoretic bound for learning maximally expressive representations subject to the given attributes is presented. In [10], MI between prediction and the sensitive attributes is used to train a fair classifier whereas [2] describes the use of inverse contrastive loss. Group-theoretic approaches have also been described in [11,46]. The work in [41] gives an empirical solution to remove specific visual features from the latent variables using adversarial training. Repulsion/Diversity. If we consider the ensemble of neural networks, there are several different strategies to maintain functional diversity between ensemble members -we acknowledge these results here because they are loosely related to one of the use cases we evaluate later. SVGD [14] shows the benefits of choosing the kernel to measure the similarity between ensemble members. In [13], the authors introduce a kernelized repulsive term in the training loss, which endows deep ensembles with Bayesian convergence properties. The so-called quality diversity (QD) is interesting: [54] tries to maximize a given objective function with diversity to a set of pre-defined measure functions [22,58]. When both the objective and measure functions in QD are differentiable, [18] offers an efficient way to explore the latent space of the objective w.r.t. the measure functions. Distance correlation (DC). The central idea motivating our work is distance correlation described in [66]. It has been used in the analysis of nonlinear dependence in time-series [74], and feature screening in ultra high-dimensional data analysis tasks [42] and we will review it in detail shortly.", "publication_ref": ["b37", "b23", "b51", "b14", "b8", "b59", "b2", "b30", "b56", "b49", "b3", "b22", "b36", "b52", "b57", "b11", "b50", "b64", "b9", "b1", "b10", "b46", "b40", "b13", "b12", "b54", "b21", "b58", "b17", "b66", "b74", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Review: Distance (and Partial Distance) Correlation", "text": "Given two random variables X, Y \u2208 R (in the same domain), correlation (say, the Pearson correlation) helps measure their association. One can derive meaningful conclusions by statistical testing. As noted in \u00a71, one generalization of correlation to a higher dimension is CCA, which seeks to find projection matrices such that correlation among the projected data is maximized, see [5]. Benefits of Distance Correlation. In many applications, the notion of distances or dissimilarities appears quite naturally. Motivated by the need for a scheme that can capture both linear and non-linear correlations when provided with such dissimilarity information, in [66], the authors proposed a new measure of dependence between random vectors, called distance correlation. The key benefits of distance correlation are:\n1. The distance correlation R satisfies 0 \u2264 R \u2264 1, and R = 0 if and only if X, Y are independent. 2. R(X, Y ) is defined for X and Y in arbitrary dimensions, e.g., R(X, Y ) is well-defined when X is of dimension p while Y is of dimension q for p \u0338 = q. We focus on empirical distance correlation for n samples drawn from the unknown joint distribution, and review its calculation.\nFor an observed random sample (x, y) = {(X i , Y i ) : i = 1, \u2022 \u2022 \u2022 , n} from the joint distribution of random vectors X in R p and Y in R q , define:\na k,l = \u2225X k \u2212 X l \u2225,\u0101 k,\u2022 = 1 n n l=1 a k,l ,\u0101 \u2022,l = 1 n n k=1 a k,l , a \u2022,\u2022 = 1 n 2 n k,l=1 a k,l , A k,l = a k,l \u2212\u0101 k,\u2022 \u2212\u0101 \u2022,l +\u0101 \u2022,\u2022(1)\nwhere k, l \u2208 {1, \u2022 \u2022 \u2022 , n}. Similarly, we can define b k,l = \u2225Y k \u2212 Y l \u2225, and\nB k,l = b k,l \u2212b k,\u2022 \u2212b \u2022,l +b \u2022,\n\u2022 , and based on these quantities we have. Definition 1. (Distance correlation) [66]. The empirical distance correlation R n (x, y) is the square root of\nR 2 n (x, y) = V 2 n (x,y) \u221a V 2 n (x,x)V 2 n (y,y) , V 2 n (x, x)V 2 n (y, y) > 0 0 , V 2 n (x, x)V 2 n (y, y) = 0 (2)\nwhere the empirical distance covariance (variance\n) V n (x, y), V n (x, x) are defined as V 2 n (x, y) = 1 n 2 n k,l=1 A k,l B k,l , V 2 n (x, x) = 1 n 2 n k,l=1 A 2 k,l , with A in (A(1)\n). Examples. We show a few simple 2D examples to contrast Pearson Correlation and Distance Correlation in Fig. 1. Notice that if the relationship between the two random variables is not linear, Pearson Correlation might be small while Distance Correlation remains meaningful. Extensions to conditioning. Given three random variables X, Y , and Z, we want to measure the correlation between X and Y but \"controlling for\" Z (thinking of it as a nuisance variable), i.e., we want to estimate R(X|Z, Y |Z) = R * (X, Y ; Z). Such a quantity is key in existing approaches in disentangled learning, deriving invariant representations and understanding what one or more networks are learning after concepts learned by another network have been accounted for. Consider how this task would be accomplished in linear regression. We would project X and Y into the space of Z, and only use the residuals to measure the correlation. Nonetheless, defining partial distance correlation is more involved -in [65], the authors introduced a new Hilbert space where we can define the projection of distance matrix. To do so, the authors calculate a U-centered matrix\u00c3 from the distance matrix (a k,l ) so that the inner product of the U-centered matrices will be the distance covariance. Definition 2. Let A = (a k,l ) be a symmetric, real valued n \u00d7 n matrix (n > 2) with zero diagonal. Define the U-centered matrix\u00c3 = (\u00e3 kl ) as follows. \ny = 0.15x 3 + 0.75n + 2.5, n \u223c N (0, 1); (c): x y \u223c N 0 2.5 , 1 0.75 0.75 1.25 ; (d): x y \u223c N 0 2.5 , 1 0 0 1.25 a kl = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 a k,l \u2212 1 n \u2212 2 n i=1 a i,l \u2212 1 n \u2212 2 n j=1 a k,j + 1 (n \u2212 1)(n \u2212 2) n i,j=1 ai,j , k \u0338 = l 0 , k = l (3)\nFurther, the inner product between\u00c3,B is defined as (\u00c3\u2022B) :=\n1 n(n\u22123) k\u0338 =l\u00c3 k,lBk,l , and is an unbiased estimator of squared population distance covariance V 2 (x, y).\nBefore defining partial distance covariance formally, we recall the definition of orthogonal projection on these matrices. Definition 3. Let\u00c3,B,C corresponding to samples x, y, z respectively, and let\nP z \u22a5 (x) =\u00c3 \u2212 (\u00c3\u2022C) (C\u2022C)C , P z \u22a5 (y) =B \u2212 (B\u2022C) (C\u2022C)C\ndenote the orthogonal projection of\u00c3(x) onto (C(z)) \u22a5 and the orthogonal projection ofB(y) onto (C(z)) \u22a5 . Now, we are ready to define the partial distance covariance and the partial distance correlation. Definition 4. Let (x,y,z) be a random sample observed from the joint distribution of (X, Y, Z). The sample partial distance covariance is defined by:\npdCov(x, y; z) = (P z \u22a5 (x) \u2022 P z \u22a5 (y)) = 1 n(n \u2212 3) i\u0338 =j (P z \u22a5 (x)) i,j (P z \u22a5 (y)) i,j (4)\nAnd the partial distance correlation is defined as: R * 2 (x, y; z) :=\n(P z \u22a5 (x)\u2022P z \u22a5 (y)) \u2225P z \u22a5 (x)\u2225\u2225P z \u22a5 (y)\u2225 where \u2225P z \u22a5 (x)\u2225 = (P z \u22a5 (x) \u2022 P z \u22a5 (x)) 1/2 is the norm.\nPartial distance correlation enables asking various interesting questions. By projecting the original U-centered matrix\u00c3 ontoC, the correlation between the residual andB will be a measure of what does X learn that Z does not.", "publication_ref": ["b4", "b66", "b66", "b65"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Optimizing Distance Correlation in Neural Networks", "text": "While distance correlation can be implemented in a differentiable way, and thereby used as an appropriate loss function in a neural network, we must take efficiency into account. For two p dimensional random variables, let the number of samples for the empirical estimate of DC be n. Observe that the total cost for computing (a k,l ) is O(n 2 p), and the memory to store the intermediate matrices is also O(n 2 ). So, we use a stochastic estimate of DC by averaging over minibatches, with each minibatch containing m samples. We describe why this approximation is sensible. Notation. We use \u0398 X , \u0398 Y to denote the parameters of the neural networks, and X, Y as features extracted by the respective neural networks. Let the minibatch size be m, and the dataset D = (D X , D Y ) be of size n. We use (x t , y t ) T t=1 , x t \u2282 D X , y t \u2282 D Y to represent the data samples at step t, T is the total number of training steps. The distance matrices A t , B t are computed when given X t , Y t using (A(1)), which is of dimension m \u00d7 m for each minibatch. Further, we use (X t ) k to represent the k th element in X t . And (A t ) k,l is the k th row and l th column element in the matrix A t . The inner-product between two matrices A, B is defined as \u27e8A, B\u27e9 = m i,j (A) i,j (B) i,j .\nObjective function. Consider the case where we minimize DC between two networks \u0398 X , \u0398 Y . Since the parameters between \u0398 X , \u0398 Y are separable, we can use the block stochastic gradient iteration in [70] with some simple modifications.\nTo minimize the distance correlation, we need to solve the following problem min\n\u0398 X ,\u0398 Y \u27e8A(\u0398 X ; x), B(\u0398 Y ; y)\u27e9 \u27e8A(\u0398 X ; x), A(\u0398 X ; x)\u27e9\u27e8B(\u0398 Y ; y), B(\u0398 Y ; y)\u27e9 (5) (A) k,l =||(X) k \u2212 (X) l || 2 , X = \u0398 X (x), (B) k,l = ||(Y ) k \u2212 (Y ) l || 2 , Y = \u0398 Y (y)\nWe slightly abuse the notation of \u0398 X (x) as applying the network \u0398 X onto data x, and reuse A to simplify the notation A(\u0398 X ; x) and the distance matrix. We can rewrite the expression (with A, B defined above) using:\nmin \u0398 X ,\u0398 Y \u27e8A, B\u27e9 s.t. max x\u2282D X \u27e8A, A\u27e9 \u2264 m; max y\u2282D Y \u27e8B, B\u27e9 \u2264 m (6)\nwhere (x, y) are the minibatch of samples from the data space (D X , D Y ).\nWe can rewrite the above into the following equation similar to (1) in [70].\nmin \u0398 X ,\u0398 Y \u03a6(\u0398 X , \u0398 Y ) = E x,y f (\u0398 X , \u0398 Y ; x, y) + \u03b3(\u0398 X ) + \u03b3(\u0398 Y )(7)\nwhere\nf (\u0398 X , \u0398 Y ; x, y) is \u27e8A, B\u27e9 and \u03b3(\u0398 X ) encodes the convex constraint of network \u0398 X : max x\u2282D X \u27e8A, A\u27e9 \u2264 m. Similarly, \u03b3(\u0398 Y ) encodes max y\u2282D Y \u27e8B, B\u27e9 \u2264 m. \u03a6(\u0398 X , \u0398 Y )\nis the constrained objective function to be optimized. Block stochastic gradient iteration. We adjust Alg. 1 from [70] to our case in Alg. 1. Since we will need the entire minibatch (x t , y t ) to compute the objective function, there will be no mean term when computing the sample gradientg t X . Further, since both blocks (\u0398 X , \u0398 Y ) are constrained, line 3, 5 will use (5) from [70]. The detailed algorithm is presented in Alg. 1. \nOutput:\u0398 T X ,\u0398 T Y 1: for t = 1, \u2022 \u2022 \u2022 , T do 2: Compute sample gradient for \u0398X g t X = \u2207\u0398 X f (\u0398 t X , \u0398 t Y ; xt, yt) 3: \u0398 t+1 X = arg min \u0398 X \u27e8g t X +\u2207\u03b3X (\u0398 t X ), \u0398X \u2212 \u0398 t X \u27e9 + 1 2\u03b7 X \u2225\u0398X \u2212 \u0398 t X \u2225 2 4: Compute sample gradient for \u0398\u1ef8 g t Y = \u2207\u0398 Y f (\u0398 t+1 X , \u0398 t Y ; xt, yt) 5: \u0398 t+1 Y = arg min \u0398 Y \u27e8g t Y +\u2207\u03b3Y (\u0398 t Y ), \u0398Y \u2212 \u0398 t Y \u27e9 + 1 2\u03b7 Y \u2225\u0398Y \u2212 \u0398 t Y \u2225 2 6: end for 7:\u0398 T X = 1 T T t=1 \u0398 t X 8:\u0398 T Y = 1 T T t=1 \u0398 t Y Proposition 1. After T iterations of Algorithm 1 with step size \u03b7 X = \u03b7 Y = \u03b7 \u221a T < 1 L\n, for some positive constant \u03b7 < 1 L , where L is the Lipschitz constant of the partial gradient of f , by Theorem. 6 in [70], we know there exists an index subsequence T such that:\nlim t\u2192\u221e,t\u2208T E[dist(0, \u2207\u03a6(\u0398 t X , \u0398 t Y ))] = 0 (8)\nwhere dist(y, X ) = min x\u2208X \u2225x \u2212 y\u2225.\nBut empirically, we find that simply applying Stochastic Gradient Decent (SGD) is sufficient, but this choice is available to the user.", "publication_ref": ["b70", "b70", "b70", "b70", "b70"], "figure_ref": [], "table_ref": []}, {"heading": "Independent Features Help Robustness", "text": "Goal. We show how distance correlation can help us train multiple deep networks that learn mutually independent features, roughly similar to finding diverse M -best solutions in structured SVM models [60]. We describe how such an approach can lead to better robustness against adversarial attacks. Rationale. Recently, several efforts have explored generating of adversarial examples that can transfer to different networks and how to defend against such attacks [15,63,6]. It is often observed that an adversarial sample for one trained network is relatively easy to transfer to another network with the same architecture [15]. Here, we show that even for as few as two networks (same architecture; trained on the same data), we can, to some extent, prevent adversarial examples from transferring between them by seeking independent features. Setup. We formulate the problem considering a classification task as an example. Given two deep neural networks with the same architecture denoted as f 1 (\u2022), f 2 (\u2022), we train them using image-label pairs (x, y) using the cross-entropy loss Loss CE . If we train f 1 and f 2 using only the cross-entropy loss, the adversarial examples generated on f 1 can relatively easily transfer to f 2 (see the performance of \"Baseline\" in Table 1). To enforce f 1 and f 2 to learn independent features, let the extracted feature of x in some intermediate layer of f be given as g(x) (in this section we use the feature before the last fully connected layer as an example). We can still train f 1 using Loss CE , and then, we train f 2 using,\nLoss total = Loss CE (f 2 (x), y) + \u03b1 \u2022 Loss DC (g 1 (x), g 2 (x)) (9\n)\nwhere \u03b1 is a constant scalar and Loss DC is the distance correlation from Def. 1. Note that we do not require g 1 (x) and g 2 (x) to be in the same dimension, so in principle we could easily use features from different layers for these two networks. Experimental settings. We first conduct experiments on CIFAR10 [39] using Resnet 18 [28]. We then use four different architectures (mobilenet-v3-small [32], efficientnet-B0 [67], Resnet 34, and Resnet152) and train them on ImageNet [40]. For each network architecture, we first train two networks using only Loss CE . Next, we train a network using only Loss CE before training a second network using the loss in (9). On CIFAR10, we utilize the SGD optimizer with momentum 0.9 and train for 200 epochs using an initial learning rate 0.1 with a cosine learning rate scheduler [53]. The mini-batch size is set to 128. On ImageNet [40], we train for 40 epochs using an initial learning rate 0.1, which decays by 0.1 every 10 epochs. The mini-batch size is 512. Our \u03b1 in ( 9) is set to 0.05 for all cases. For each combination of the dataset and the network architecture, we train two networks f 1 and f 2 , after which we generate adversarial examples on f 1 and use them to attack f 2 and measure its classification accuracy. We construct a baseline by training f 1 and f 2Baseline without constraints. And train f 2Our using (9) to learn independent features w.r.t. f 1 . We report performance under two widely used attack methods: fast gradient sign method (FGM) [26] and projected gradient descent method (PGD) [47], where the latter is considered among the strongest attacks. The scale \u03f5 of the adversarial perturbation is chosen from {0.03, 0.05, 0.1} and the maximum number of iterations of PGD is set to 40.\nResults. The results are shown in Table 1. We see that we get significant improvement in accuracy over the baseline under adversarial attacks, with comparable performance on clean inputs. Notably, our method achieves more than 10% absolute improvement in accuracy under PGD attack on Resnet-18 and Mobilenet-v3-small. This provides evidence supporting the benefits of enforcing the networks to learn independent features using our distance correlation loss. In Fig. 2, we show correlation results using Picasso [29,7] to lower the dimension of features for each network. The embedding dimension is 2 for visualization. In Fig. 2(a), we show the embedding of different networks. f 1 represents the network to generate the adversarial examples. f 2Baseline denotes the baseline network, trained without distance correlation constraint. Also, f 2Ours is the same network trained to be independent to f 1 . In Fig. 2(b), we visualize the correlation between f 1 and f 2Baseline for each dimension, and the correlation between f 1 and f 2Ours . If the scatter plot looks circle-like, we can infer that the two models are independent. We see that in different networks, the use of DC shows stronger independence. From Fig. 2/Tab. 1, we also see that the more independent the models are, the better is the gain for transferred attack robustness.", "publication_ref": ["b60", "b14", "b63", "b5", "b14", "b38", "b27", "b31", "b67", "b39", "b8", "b53", "b39", "b25", "b47", "b28", "b6"], "figure_ref": ["fig_1", "fig_1", "fig_1", "fig_1"], "table_ref": ["tab_0", "tab_0"]}, {"heading": "Informative Comparisons between Networks", "text": "Overview. As discussed in \u00a71, there is much interest in understanding whether two different models learn similar concepts from the data -for example, whether vision Transformers \"see\" similar to convolutional neural networks [57]. Here, we first follow [57] and discuss similarities between different layers of ViT and ResNets using distance correlation. Next, we investigate that after taking out the influence of Resnets from ViT (or vice versa), what are the residual learned concepts remaining in the network.", "publication_ref": ["b57", "b57"], "figure_ref": [], "table_ref": []}, {"heading": "Measure Similarity between Neural Networks", "text": "Goal. We first want to understand whether ViTs represent features across all layers differently from CNNs (such as Resnets). However, analyzing the features in the hidden layers can be challenging, because the features are spread across neurons. Also, different layers have different numbers of neurons. Recently, [57] applied the Centered Kernel Alignment (CKA) for this task. CKA is effective because it involves no constraint on the number of neurons. It is also independent to the orthogonal transformations of representations. Here, we want to demonstrate that distance correlation is a reasonable alternative for CKA in these settings. Experimental settings. First, as described in [57], we show that similarity between layers within a single neural network can be assessed using distance correlation (see Fig. 3(a) ). We pick ViT Base with patch 16, and three commonly used Resnets. All networks are pretrained on ImageNet. For ViT, we pick the embedding layer and all the normalization, attention, and fully connected layers within each block. The total number of layers is 63. For Resnets, we use all convolutional layers and the last fully connected layer, which is the same counting method to build Resnet models. Results (a). Our findings add to those from [57]. Using distance correlation, we find that the ViT layers can be split into small blocks and the similarity between different blocks from shallow layers to the deeper layers is higher. For most Resnets, the feature similarity shows that there are a few large blocks in the network, which contains more than 30 layers each, and the last few layers share minimal similarity with the shallow layers. Results (b). After within-model distance correlation, we perform across-model distance correlation comparisons between ViT and Resnets, see Fig. 3(b). We notice that in the initial 1/6 layers, the two networks share high similarities. But later, the similarity spreads across all different layers between ViT and Resnets. Notably, the last few layers share the least similarity between two networks.\nBy using the distance correlation to calculate the heatmap of the similarity matrices, we can qualitatively describe the difference between the patterns of the features in different layers from different networks. What is even more interesting is to quantitatively show the difference, for example, to answer which network contains more information for the ground truth classes. We discuss this next.", "publication_ref": ["b57", "b57", "b57"], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "What Remains", "text": "When \"Taking out\" Y from X Goal. Even measuring information contained in one neural network is challenging, and often tackled by measuring the accuracy on the test dataset. But the association between accuracy and the information contained in a network may be weak. Based on existing literature, conditioning one network w.r.t. another remains unresolved. Despite the above challenges, we can indeed measure the similarity between the features of the network X and the ground truth labels. If the similarity is higher, we can say that the feature space of X contains more information regarding the true labels. Distance correlation enables this. Interestingly, partial distance correlation extends this idea to multiple networks allowing us to approach the \"conditioning\" question posed above. Rationale/setup. Here, we choose the last layer before the final fully-connected layer as the feature layer similar to the setup in \u00a74. Our first attempt involved directly applying the distance correlation measurement to feature X and the one-hot ground truth embedding. However, the one-hot embedding for the label contains very little information, e.g., it does not show the difference between \"cat\" vs. \"dog\" and \"cat\" vs. \"airplane\". So, we use the pretrained BERT [16] to linguistically embed the class labels into the hidden space. We then measure the distance correlation between the feature space of X and the pretrained hidden space GT . R 2 (X, GT ) = m n n/m t=1 dCor(x t , gt t ) where x t is the feature for one minibatch, and gt t is the BERT embedding vector of the corresponding label. To further extend this metric to measure the \"remaining\" or residual information, we apply the partial distance correlation calculation by removing Y out of X, or say X conditioned on Y . Then, we have R 2 ((X|Y ), GT ) = m n n/m t=1 dCor ((x t |y t ), gt t ) using (4). This capability has not been shown before.  Experimental settings. In order to measure the information remaining when conditioning network \u0398 Y out of \u0398 X , we first use pretrained networks on ImageNet. We use the validation set of the ImageNet for evaluation. We want to evaluate which network contains the richest information regarding linguistic embedding. Interestingly, we can go beyond such an evaluation, instead, asking the network \u0398 X to learn concepts above and beyond what the network \u0398 Y has learned. To do so, we include the partial distance correlation into the loss. Unlike the experiment discussed above (minimizing distance correlation), in this setup, we seek to maximize partial distance correlation. The Loss total is\nNetwork \u0398X Network \u0398Y R 2 (X, GT ) R 2 (Y, GT ) R 2 ((X|Y ), GT ) R 2 ((Y |X), GT ) ViT 1 Resnet\nLoss CE (f 1 (x), y) \u2212 \u03b1 \u2022 Loss PDC ((g 1 (x)|g 2 (x)), gt)(10)\nWe take pretrained networks \u0398 X , \u0398 Y and then finetune \u0398 X using (10). The learning rate is set to be 1e \u2212 5 and \u03b1 in the loss term is 1. To check the benefits of partial DC, we use Grad-CAM [61] to highlight the areas that each network is looking at, together with what \u0398 X conditioned on \u0398 Y sees then. Results (a). We first show information comparison between two networks. The details of DC and partial DC are shown in Table . 2. The reader will notice that since ViT achieves the best test accuracy, it also contains the most information. Additionally, although better test accuracy normally coincides with more information, this is not always true. Resnet 50 contains more linguistic information than the much deeper Resnet 152, perhaps a compensation mechanism. For Resnet 152, the network is deep enough to focus on local structures that overwhelm the linguistic information (or this information is unnecessary). This experiment suggests a new strategy to compare two networks beyond test accuracy. Results (b). After using a pretrained network, we can also check that by including the partial distance correlation in the loss, which regions does the model pay attention to, using Grad-CAM. We replace the loss term of Grad-CAM with the partial distance correlation. The results are shown in Fig. 4. We see that the pretrained ViT sees across the whole image in different locations, while the Resnet (VGG) tends to focus on only one area of the image. After training, ViT (conditioned on Resnet) pays more attention to the subjects, especially locations outside the Resnet focus. Such experiments help understand how ViT learns beyond Resnets (CNNs).", "publication_ref": ["b15", "b9", "b61"], "figure_ref": ["fig_9"], "table_ref": []}, {"heading": "Disentanglement", "text": "Overview. This experiment studies disentanglement [30,36,8,44,21]. It is believed that the image data are generated from low dimensional latent variables -but isolating and disentangling the latent variables is challenging. A key in disentangled latent variable learning is to make the factors in the latent variables independent [2]. Distance correlation fits perfectly and can handle a variety of dimensions for the latent variables. When the distance correlation is 0, we know that the two variables are independent. Experimental settings. We follow [21] which focuses on semi-supervised disentanglement to generate high-resolution images. In [21], one divides the latent variables into two categories: (a) attributes of interest -a set of semantic and interpretable attributes, e.g. hair color and age; (b) residual attributes -the remaining information. Formally,\nx i = G(f 1 i , ..., f k i , r i ),\nwhere G is the generator that uses the factors of interest f l i and the residual to generate image x i . In order to enforce the condition that the information regarding the attributes of interest is not leaking into the residual representations, the authors of [21] introduced the loss L res = n i=1 ||r i || 2 to limit the residual information. This is Fig. 4: Grad-CAM results on ImageNet using ViT, Resnet18 and VGG16. After using Partial DC to remove the information learned by another network, ViT can focus on detail places and Resnet can only look in major spots. Similar issue happens to VGG.\nsub-optimal as there can be cases where r i is not 0 but still independent to the factors of interest (f l i ) k l=1 . Thus, we use distance correlation to replace this loss:\nL res = dCor([f 1 ; f 2 ; ...; f k ], r)(11)\nWe use the same structure proposed in [21], while the generator architecture is adopted from StyleGAN2 [35]. The dataset is the human face dataset FFHQ [34], and the attributes are: age, gender, etc. We use CLIP [55] to partially label the attributes to generate the semi-supervised dataset for training. All losses from [21] are used, except that L res is replaced by (11). Results. (Shown in Fig. 5) Our model shows the ability to change specific attributes without affecting residual features, such as posture (also see supplement). If you use distance correlation for disentanglement, please give credit to the following paper [43] which discusses a nice demonstration of distance correlation helps content style disentanglement. We were not aware of this paper when we wrote the paper last year and thank Sotirios Tsaftaris for communicating his findings with us.", "publication_ref": ["b29", "b35", "b7", "b43", "b20", "b1", "b20", "b20", "b20", "b20", "b34", "b33", "b55", "b20", "b10", "b42"], "figure_ref": ["fig_9", "fig_10"], "table_ref": []}, {"heading": "Conclusions", "text": "In this paper, we studied how distance correlation (and partial distance correlation) has a wide variety of uses in deep learning tasks in vision. The measure offers various properties that are often enforced using alternative means, that are often far more involved. Further, it is extremely simple to incorporate in contrast to various divergence-based measures often used in invariant representation learning. Notably, the use of partial distance correlation offers the ability of conditioning, which is underexplored in the community. We showcase three very different settings, ranging from network comparison to training distinct/different Fig. 5: Representative generated images using our training on FFHQ. Note that these results only use semi-supervised dataset by CLIP. Our methods shows the ability to disentangle the attributes of interest and the remaining information. models to disentanglement where the idea is immediately beneficial, and expect that numerous other applications will emerge in short order.\nthe partial gradient of f , by Theorem. 6 in [70], we know there exists an index subsequence T such that:\nlim t\u2192\u221e,t\u2208T E[dist(0, \u2207\u03a6(\u0398 t X , \u0398 t Y ))] = 0 (A(6))\nwhere dist(y, X ) = min x\u2208X \u2225x \u2212 y\u2225.\nFurther, in the special case where E x,y f (\u0398 X , \u0398 Y ; x, y) is convex, by Theorem. 1 in [70], the following statement holds:\nE[\u03a6(\u0398 T X ,\u0398 T Y ) \u2212 \u03a6(\u0398 * X , \u0398 * Y )] \u2264 D\u03b7 1 + log T \u221a 1 + T + \u2225\u0398X \u2212 \u0398 1 X \u2225 2 + \u2225\u0398Y \u2212 \u0398 1 Y \u2225 2 2\u03b7 \u221a 1 + T (A(7))\nwhere\u0398 X ,\u0398 Y is computed in Algorithm 1, \u0398 * X , \u0398 * Y are the optimum of the desired function, and D is a constant depending on \u2225(\u0398 * X ; \u0398 * Y )\u2225.\nA.4 Modification from BSG [70] to DC\nThe statement of Prop. 1 is similar to the statement of Theorem 1 and 6 in [70]. So, we can use the statement in [70] with some modification of our setup. We define\nF (\u0398 X , \u0398 Y ) = E x,y f (\u0398 X , \u0398 Y ; x, y), \u0393 (\u0398 X , \u0398 Y ) = \u03b3(\u0398 X ) + \u03b3(\u0398 Y )\nThen, for the gradient w.r.t. X, we have the following expression (similar for Y ):g\nt X = \u2207 \u0398 X f (\u0398 t X , \u0398 t Y ; x t , y t ) g t X = \u2207 \u0398 X F (\u0398 t X , \u0398 t Y ) \u03b4 t X =g t X \u2212 g t X\nWe first restate four assumptions from [70].\nAssumption 1 There exist a constant c and a sequence {\u03c3 k } such that for any t,\n\u2225E[\u03b4 t X |x t , y t ]\u2225 \u2264 c \u2022 max(\u03b7 X , \u03b7 Y ), E\u2225\u03b4 t X \u2225 2 \u2264 \u03c3 2 t Assumption 2\nThe objective function is lower bounded, i.e., \u03a6(\u0398 X , \u0398 Y ) > \u2212\u221e.\nAnd there is a uniform Lipschitz constant L > 0 such that:\n\u2225\u2207 X F (\u0398 X , \u0398 Y ) \u2212 \u2207 X F (\u0398 \u2032 X , \u0398 \u2032 Y )\u2225 \u2264 L\u2225(\u0398 X ; \u0398 Y ) \u2212 (\u0398 \u2032 X ; \u0398 \u2032 Y )\u2225, \u2200(\u0398 X , \u0398 Y ), (\u0398 \u2032 X , \u0398 \u2032 Y )\nAssumption 3 There exists a constant \u03c1 such that E\u2225(\u0398 t X ; \u0398 t Y )\u2225 2 \u2264 \u03c1 2 for all t.", "publication_ref": ["b70", "b70", "b70", "b70", "b70", "b70"], "figure_ref": ["fig_10"], "table_ref": []}, {"heading": "Assumption 4", "text": "The constraint function \u03b3 is Lipschitz continuous. There is a constant L \u03b3 , such that:\n\u2225\u03b3(\u0398 X ) \u2212 \u03b3(\u0398 \u2032 X )\u2225 \u2264 L \u03b3 \u2225\u0398 X \u2212 \u0398 \u2032 X \u2225, \u2200\u0398 X , \u0398 \u2032 X Theorem 6.\n(from [70]) Let {\u0398 t } be generated from Algorithm 1 with \u03b7 t X , \u03b7 t Y , being constained as,\n0 < inf t \u03b7 t X \u2264 sup t \u03b7 t X < 1 L 0 < inf t \u03b7 t Y \u2264 sup t \u03b7 t Y < 1 L Under Assumptions 1 through 4, if either X = R n X , Y = R n Y or \u03b3 = 0, and \u221e t=1 \u03c3 2 t < \u221e\nthen there exists an index subsequence T such that\nlim t\u2192\u221e,t\u2208T E[dist(0, \u2207\u03a6(\u0398 t X , \u0398 t Y ))] = 0,\nwhere dist(y, X ) = min x\u2208X \u2225x \u2212 y\u2225.\nRemark 1. In our case, we have that \u0398 X , \u0398 Y are the parameters of neural networks. During training, we have no constraint on the weights and biases, so the space of \u0398 X , which is X , is the Euclidean space. Also, we can have\n\u03b7 t X = \u03b7 t Y = \u03b7 \u221a t < 1 L .\nAll the other assumptions are similar to [70]. Thus, we have the same result in Prop. 1.\nIn the convex case, we use the Theorem 1 from [70]. Theorem 1. [70] (Ergodic convergence for non-smooth convex case). Let {\u0398 t } be generated from Algorithm 1 with \u03b7 t X = \u03b7 t Y = \u03b7 t = \u03b7 \u221a t < 1 L , \u2200t, for some positive constant \u03b7 < 1 L . Under Assumptions 1 through 4, if F and \u03b3 are both convex, \u0398 * X , \u0398 * Y is a solution of (A(5)), and \u03c3 = sup t \u03c3 t < \u221e, then\nE[\u03a6(\u0398 t X , \u0398 t Y ) \u2212 \u03a6(\u0398 * X , \u0398 * Y )] \u2264 D\u03b7 1 + log T \u221a 1 + T + \u2225(\u0398 * X ; \u0398 * Y ) \u2212 (\u0398 1 X ; \u0398 1 Y )\u2225 2 2\u03b7 \u221a 1 + T where\u0398 T X = \u03b7t\u0398 t+1 X T t=1 \u03b7t ,\u0398 T Y = \u03b7t\u0398 t+1 Y T t=1 \u03b7t , and D = s(\u03c3 2 + 4L 2 \u03b3 ) 1 \u2212 L\u03b7 + \u221a s(\u2225(\u0398 * X ; \u0398 * Y )\u2225 + \u03c1)(c + L 8M 2 \u03c1 + 8\u03c3 2 t + 4L 2 \u03b3 )\nwhere\nM \u03c1 = 4L 2 \u03c1 2 + 2 max(\u2225\u2207 \u0398 X F (0)\u2225 2 , \u2225\u2207 \u0398 Y F (0)\u2225 2 )\nRemark 2. Our case is a special case of the Block Stochastic Gradient problem with s = 2 (s is the number of blocks). So the above theorem can be directly applied to our analysis when F, \u03b3 are both convex. However, this may not be true in most deep neural networks, we obtain the O(T 1/2 ) convergence rate using Algorithm 1.", "publication_ref": ["b70", "b70", "b70", "b70"], "figure_ref": [], "table_ref": []}, {"heading": "B Experimental Details in Section 4: Independent Features Help Robustness", "text": "When we train f 1 using cross entropy loss and f 2 using cross entropy loss plus our distance correlation loss (to learn independent features with f 1 ), we first train f 1 for one epoch, and then train f 2 for one epoch given the current f 1 , and we repeat this process for the total number of epochs (200 for CIFAR10 and 40 for ImageNet). Our hyperparameter \u03b1 controls the tradeoff between the cross entropy loss and the distance correlation loss. In practice, we could increase \u03b1 to emphasize (or weight) learning independent features more, and decrease \u03b1 if we want to keep the classification accuracy of f 2 in standard setting (non adversarial) even closer to that of f 1 . During training, we do not utilize data augmentation for all experiments. The training is done on Nvidia A100 GPUs. Our distance correlation adds approximtely 20% cost to the training time compared with training only using cross entropy loss. We also include some more visualization of feature spaces in addition to those shown in our main paper in We take the pretrained neural network from [68]. The features are reshaped to a 1D vector and we compute the Euclidean distance between samples from the official validation set of ImageNet [40]. No finetuning was used in this experiment.\nThe results are shown in the main paper.\nC.2 What remains when \"taking out\" (aka controlling for) Y from X We will first discuss the details of the heatmap that we plotted using Grad-CAM [61]. In the original implementation of Grad-CAM, the model uses one layer as the target and uses both gradients (from the loss function) and the activation in that layer for the visualization.\nIn the original Grad-CAM, the loss is extracted as the intensity before the softmax layer of one given class. For example, assume \"dog\" is the 6 th class in the dataset. If we want to see which location in the image is related to \"dog\", we will use f (x) [6] as the loss function, where Softmax(f (x)) is the final output of the model when given image x.\nIn our case, the activation remains the same. But the loss function is different. We use the distance correlation between the features extracted by the neural network and the ground truth linguistics embedding, i.e., Loss = R 2 (X, GT ), where X is the feature of input image extracted by the neural network.\nAfter showing the Grad-CAM results for each individual network, we want to check if the partial distance correlation can help the network focus on a different location. Thus, we finetune the network X with an extra loss term Loss PDC .\nWe take ViT-B/16 as our model X and Resnet 18 as our model Y . We first load the pretrained weights from [68] and finetune model X with model Y being fixed. \u03b1 in our case is set as 1 in the loss term Loss CE (f 1 (x), y) \u2212 \u03b1 \u2022 Loss PDC ((g 1 (x)|g 2 (x)), gt)\nLearning rate is set as 1e \u22125 and batch size is set as 64. We train 15 epochs in total. The finetuning on two RTX 2080Ti takes 2 days.", "publication_ref": ["b68", "b39", "b61", "b5", "b68"], "figure_ref": [], "table_ref": []}, {"heading": "C.3 Extra Results", "text": "We show several additional heat maps using Grad-CAM in addition to those in our main paper in Fig.  \nC", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Experimental Details in Section 6: Disentanglement", "text": "We follow the setup in [21] where the dataset contains both labeled and unlabeled data. The provided labels are indicated by the function \u2113: \u2113(i, j) = 1, f j i exists (attribute j of image i is labeled) 0, otherwise For each attributes, we train k classifiers of the form C j : X \u2192 [m j ] where m j denotes the number of values of attribute j. The gender attribute here contains male and female, and age attribute contains kid, teenage, adult, and old person.  For the classifiers when given the true label, we use the cross-entropy loss\nL cls = n i=1 n j=1 \u2113(i, j) \u2022 H(Softmax(C j (x i )), f j i ) (D(8))\nFor the classifiers without the true label, we use the entropy so that the information is not leaking\nL ent = n i=1 n j=1 (1 \u2212 \u2113(i, j)) \u2022 H(Softmax(C j (x i ))) (D(9))\nFor the residual, we use the distance correlation loss in our paper L res = dCor([f 1 ; f 2 ; ...; f k ], r) (D(10))\nLet the value for each of the attributes of interest j to bef j \u0129 f j i = f j i , \u2113(i, j) = 1 Softmax(C j (x i )) , otherwise Also, we include the reconstruction loss to generate the target image 11))\nL rec = n i=1 \u03d5(G(f 1 i , ...,f k i , r \u2032 i ), x i ) (D(\nThe final loss will be the linear combination of all the loss above.  While the results are qualitative, perceptually the generated results appear meaningful.     ", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "A Technical Analysis Using Block Stochastic Gradient", "text": "In this section, we describe results (which were briefly mentioned in the main paper) showing the viability of a stochastic scheme for using distance correlation within the loss when training our neural network models.\nIn the paper, we noted the existence of an algorithm where the convergence rate of the stochastic version of distance correlation in the deep neural network setting is O( 1 \u221a T ). Here, we will describe it more formally. We note that SGD works well for the distance correlation objective -and so a majority of users will revert to such mature implementations anyway. Despite desirable practical behavior, its theoretical analysis of the form included here is more involved. Therefore, the analysis shown below, a modified version of [70], is reassuring in the sense that we know that stochastic updates (carried out in a specific way) can provably optimize our loss.", "publication_ref": ["b70"], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Notation in the Proof", "text": "We use \u0398 X , \u0398 Y to denote the parameters of neural networks, and X, Y as features extracted by the respective neural networks. Let the minibatch size be m, and the dataset D = (D X , D Y ) be of size n. Let, X \u2208 R m\u00d7p , Y \u2208 R m\u00d7q , with p, q be the dimension of features. We use (x t , y t ) T t=1 , x t \u2282 D X , y t \u2282 D Y to represent the data samples at step t, T is the total number of training steps. The distance matrices A t , B t are computed when given X t , Y t using (A(1)), which is of dimension m\u00d7m for each minibatch. Further, we use (X t ) k to represent the k th element in X t . Also, (A t ) k,l is the k th row and l th column element in the matrix A t . The inner-product between two matrices A, B is defined as \u27e8A, B\u27e9 = m i,j (A) i,j (B) i,j .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Objective Function", "text": "Consider the case where we minimize DC between two networks \u0398 X , \u0398 Y . Since the parameters between \u0398 X , \u0398 Y are separable, we can use block stochastic gradient iteration in [70] with some modification.\nTo minimize the distance correlation, we need to solve the following problem min\nWe slightly abuse the notation of \u0398 X (x) to correspond to applying the network \u0398 X on the data x, and reuse A to simplify the notation A(\u0398 X ; x) and the distance matrix. We can rewrite the expression (with A, B defined above) using:\nwhere (x, y) are the minibatch of samples from the data space (D X , D Y ).\nWe can rewrite as the following expression similar to Eq. ( 1) in [70].\nwhere f (\u0398 X , \u0398 Y ; x, y) is \u27e8A, B\u27e9 and \u03b3(\u0398 X ) encodes the convex constraint on the network \u0398 X , i.e., max\nis the constrained objective function to be optimized.", "publication_ref": ["b70", "b70"], "figure_ref": [], "table_ref": []}, {"heading": "A.3 Block Stochastic Gradient Iteration", "text": "We adapt Alg. 1 from [70] to our case in Alg. 1. Since we will need the entire minibatch (x t , y t ) to compute the objective function, there will be no mean term when computing the sample gradientg t X . Further, since both blocks (\u0398 X , \u0398 Y ) are constrained, line 3, 5 will use ( 5) from [70]. The detailed algorithm is presented in Alg. 1. \nAfter T iterations of Algorithm 1 with step size \u03b7 X = \u03b7 Y = \u03b7 \u221a T < 1 L , for some positive constant \u03b7 < 1 L , where L is the Lipschitz constant of", "publication_ref": ["b70", "b70"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Emergence of invariance and disentanglement in deep representations", "journal": "The Journal of Machine Learning Research", "year": "2018", "authors": "A Achille; S Soatto"}, {"ref_id": "b1", "title": "Learning invariant representations using inverse contrastive loss", "journal": "", "year": "2021", "authors": "A K Akash; V S Lokhande; S N Ravi; V Singh"}, {"ref_id": "b2", "title": "An introduction to multivariate statistical analysis", "journal": "", "year": "1958", "authors": "T W Anderson"}, {"ref_id": "b3", "title": "Deep canonical correlation analysis", "journal": "PMLR", "year": "2013", "authors": "G Andrew; R Arora; J Bilmes; K Livescu"}, {"ref_id": "b4", "title": "A probabilistic interpretation of canonical correlation analysis", "journal": "", "year": "2005", "authors": "F R Bach; M I Jordan"}, {"ref_id": "b5", "title": "What it thinks is important is important: Robustness transfers through input gradients", "journal": "", "year": "2020", "authors": "A Chan; Y Tay; Y S Ong"}, {"ref_id": "b6", "title": "The specious art of single-cell genomics", "journal": "bioRxiv", "year": "2021", "authors": "T Chari; J Banerjee; L Pachter"}, {"ref_id": "b7", "title": "Isolating sources of disentanglement in vaes", "journal": "", "year": "2018", "authors": "R T Chen; X Li; R Grosse; D Duvenaud"}, {"ref_id": "b8", "title": "Improving black-box adversarial attacks with a transfer-based prior", "journal": "Advances in neural information processing systems", "year": "2019", "authors": "S Cheng; Y Dong; T Pang; H Su; J Zhu"}, {"ref_id": "b9", "title": "A fair classifier using mutual information", "journal": "IEEE", "year": "2020", "authors": "J Cho; G Hwang; C Suh"}, {"ref_id": "b10", "title": "Group equivariant convolutional networks", "journal": "PMLR", "year": "2016", "authors": "T Cohen; M Welling"}, {"ref_id": "b11", "title": "Elements of information theory", "journal": "John Wiley & Sons", "year": "1999", "authors": "T M Cover"}, {"ref_id": "b12", "title": "Repulsive deep ensembles are bayesian", "journal": "Advances in Neural Information Processing Systems", "year": "2021", "authors": "F D'angelo; V Fortuin"}, {"ref_id": "b13", "title": "On stein variational neural network ensembles", "journal": "", "year": "2021", "authors": "F D'angelo; V Fortuin; F Wenzel"}, {"ref_id": "b14", "title": "Why do adversarial attacks transfer? explaining transferability of evasion and poisoning attacks", "journal": "", "year": "2019", "authors": "A Demontis; M Melis; M Pintor; M Jagielski; B Biggio; A Oprea; C Nita-Rotaru; F Roli"}, {"ref_id": "b15", "title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2018", "authors": "J Devlin; M W Chang; K Lee; K Toutanova"}, {"ref_id": "b16", "title": "Certifying and removing disparate impact", "journal": "", "year": "2015", "authors": "M Feldman; S A Friedler; J Moeller; C Scheidegger; S Venkatasubramanian"}, {"ref_id": "b17", "title": "Differentiable quality diversity", "journal": "Advances in Neural Information Processing Systems", "year": "2021", "authors": "M Fontaine; S Nikolaidis"}, {"ref_id": "b18", "title": "An lp view of the m-best map problem", "journal": "Advances in Neural Information Processing Systems", "year": "2009", "authors": "M Fromer; A Globerson"}, {"ref_id": "b19", "title": "Neocognitron: A neural network model for a mechanism of visual pattern recognition", "journal": "", "year": "1983", "authors": "K Fukushima; S Miyake; T Ito"}, {"ref_id": "b20", "title": "An image is worth more than a thousand words: Towards disentanglement in the wild", "journal": "Advances in Neural Information Processing Systems", "year": "2021", "authors": "A Gabbay; N Cohen; Y Hoshen"}, {"ref_id": "b21", "title": "Discovering representations for black-box optimization", "journal": "", "year": "2020", "authors": "A Gaier; A Asteroth; J B Mouret"}, {"ref_id": "b22", "title": "Stochastic canonical correlation analysis", "journal": "J. Mach. Learn. Res", "year": "2019", "authors": "C Gao; D Garber; N Srebro; J Wang; W Wang"}, {"ref_id": "b23", "title": "Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness", "journal": "", "year": "2019", "authors": "R Geirhos; P Rubisch; C Michaelis; M Bethge; F A Wichmann; W Brendel"}, {"ref_id": "b24", "title": "The generalized eigenvalue problem as a nash equilibrium", "journal": "", "year": "2022", "authors": "I Gemp; C Chen; B Mcwilliams"}, {"ref_id": "b25", "title": "Explaining and harnessing adversarial examples", "journal": "", "year": "2014", "authors": "I J Goodfellow; J Shlens; C Szegedy"}, {"ref_id": "b26", "title": "Efficiently enforcing diversity in multi-output structured prediction", "journal": "Artificial Intelligence and Statistics", "year": "2014", "authors": "A Guzman-Rivera; P Kohli; D Batra; R Rutenbar"}, {"ref_id": "b27", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "K He; X Zhang; S Ren; J Sun"}, {"ref_id": "b28", "title": "Picasso: A modular framework for visualizing the learning process of neural network image classifiers", "journal": "", "year": "2017", "authors": "R Henderson; R Rothe"}, {"ref_id": "b29", "title": "beta-vae: Learning basic visual concepts with a constrained variational framework", "journal": "", "year": "2016", "authors": "I Higgins; L Matthey; A Pal; C Burgess; X Glorot; M Botvinick; S Mohamed; A Lerchner"}, {"ref_id": "b30", "title": "Relations between two sets of variates", "journal": "Springer", "year": "1992", "authors": "H Hotelling"}, {"ref_id": "b31", "title": "Searching for mobilenetv3", "journal": "", "year": "2019", "authors": "A Howard; M Sandler; G Chu; L C Chen; B Chen; M Tan; W Wang; Y Zhu; R Pang; V Vasudevan"}, {"ref_id": "b32", "title": "Densenet: Implementing efficient convnet descriptor pyramids", "journal": "", "year": "2014", "authors": "F Iandola; M Moskewicz; S Karayev; R Girshick; T Darrell; K Keutzer"}, {"ref_id": "b33", "title": "A style-based generator architecture for generative adversarial networks", "journal": "", "year": "2019", "authors": "T Karras; S Laine; T Aila"}, {"ref_id": "b34", "title": "Analyzing and improving the image quality of stylegan", "journal": "", "year": "2020", "authors": "T Karras; S Laine; M Aittala; J Hellsten; J Lehtinen; T Aila"}, {"ref_id": "b35", "title": "Disentangling by factorising", "journal": "PMLR", "year": "2018", "authors": "H Kim; A Mnih"}, {"ref_id": "b36", "title": "Similarity of neural network representations revisited", "journal": "PMLR", "year": "2019", "authors": "S Kornblith; M Norouzi; H Lee; G Hinton"}, {"ref_id": "b37", "title": "Do better imagenet models transfer better? In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "journal": "", "year": "2019", "authors": "S Kornblith; J Shlens; Q V Le"}, {"ref_id": "b38", "title": "Learning multiple layers of features from tiny images", "journal": "", "year": "2009", "authors": "A Krizhevsky; G Hinton"}, {"ref_id": "b39", "title": "Imagenet classification with deep convolutional neural networks", "journal": "Advances in neural information processing systems", "year": "2012", "authors": "A Krizhevsky; I Sutskever; G E Hinton"}, {"ref_id": "b40", "title": "Fader networks: Manipulating images by sliding attributes", "journal": "NIPS", "year": "2017", "authors": "G Lample; N Zeghidour; N Usunier; A Bordes; L Denoyer; M Ranzato"}, {"ref_id": "b41", "title": "Feature screening via distance correlation learning", "journal": "Journal of the American Statistical Association", "year": "2012", "authors": "R Li; W Zhong; L Zhu"}, {"ref_id": "b42", "title": "Measuring the biases and effectiveness of content-style disentanglement", "journal": "", "year": "2021", "authors": "X Liu; S Thermos; G Valvano; A Chartsias; A O'neil; S A Tsaftaris"}, {"ref_id": "b43", "title": "Disentangling factors of variations using few labels", "journal": "", "year": "2019", "authors": "F Locatello; M Tschannen; S Bauer; G R\u00e4tsch; B Sch\u00f6lkopf; O Bachem"}, {"ref_id": "b44", "title": "Fairalm: Augmented lagrangian method for training fair models with little regret", "journal": "", "year": "", "authors": "V S Lokhande; A K Akash; S N Ravi; V Singh"}, {"ref_id": "b45", "title": "European Conference on Computer Vision: proceedings. European Conference on Computer Vision", "journal": "NIH Public Access", "year": "2020", "authors": ""}, {"ref_id": "b46", "title": "Equivariance allows handling multiple nuisance variables when analyzing pooled neuroimaging datasets", "journal": "", "year": "2022", "authors": "V S Lokhande; R Chakraborty; S N Ravi; V Singh"}, {"ref_id": "b47", "title": "Towards deep learning models resistant to adversarial attacks", "journal": "", "year": "2018", "authors": "A Madry; A Makelov; L Schmidt; D Tsipras; A Vladu"}, {"ref_id": "b48", "title": "An online riemannian pca for stochastic canonical correlation analysis", "journal": "Curran Associates, Inc", "year": "2021", "authors": "Z Meng; R Chakraborty; V Singh; M Ranzato; A Beygelzimer; Y Dauphin; P Liang;  Vaughan"}, {"ref_id": "b49", "title": "Insights on representational similarity in neural networks with canonical correlation", "journal": "NeurIPS", "year": "2018", "authors": "A S Morcos; M Raghu; S Bengio"}, {"ref_id": "b50", "title": "Invariant representations without adversarial training", "journal": "NeurIPS", "year": "2018", "authors": "D Moyer; S Gao; R Brekelmans; A Galstyan; G Ver Steeg"}, {"ref_id": "b51", "title": "What is being transferred in transfer learning?", "journal": "Curran Associates, Inc", "year": "2020", "authors": "B Neyshabur; H Sedghi; C Zhang; H Larochelle; M Ranzato; R Hadsell; M Balcan"}, {"ref_id": "b52", "title": "Do wide and deep networks learn the same things? uncovering how neural network representations vary with width and depth", "journal": "", "year": "2020", "authors": "T Nguyen; M Raghu; S Kornblith"}, {"ref_id": "b53", "title": "Pytorch: An imperative style, highperformance deep learning library", "journal": "Advances in neural information processing systems", "year": "2019", "authors": "A Paszke; S Gross; F Massa; A Lerer; J Bradbury; G Chanan; T Killeen; Z Lin; N Gimelshein; L Antiga"}, {"ref_id": "b54", "title": "Quality diversity: A new frontier for evolutionary computation", "journal": "Frontiers in Robotics and AI", "year": "2016", "authors": "J K Pugh; L B Soros; K O Stanley"}, {"ref_id": "b55", "title": "Learning transferable visual models from natural language supervision", "journal": "PMLR", "year": "2021", "authors": "A Radford; J W Kim; C Hallacy; A Ramesh; G Goh; S Agarwal; G Sastry; A Askell; P Mishkin; J Clark"}, {"ref_id": "b56", "title": "Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability", "journal": "NIPS", "year": "2017", "authors": "M Raghu; J Gilmer; J Yosinski; J Sohl-Dickstein"}, {"ref_id": "b57", "title": "Do vision transformers see like convolutional neural networks?", "journal": "Curran Associates, Inc", "year": "2021", "authors": "M Raghu; T Unterthiner; S Kornblith; C Zhang; A Dosovitskiy; M Ranzato; A Beygelzimer; Y Dauphin; P Liang;  Vaughan"}, {"ref_id": "b58", "title": "Policy manifold search: Exploring the manifold hypothesis for diversity-based neuroevolution", "journal": "", "year": "2021", "authors": "N Rakicevic; A Cully; P Kormushev"}, {"ref_id": "b59", "title": "Matrix correlation", "journal": "Psychometrika", "year": "1984", "authors": "J Ramsay; J Ten Berge; G Styan"}, {"ref_id": "b60", "title": "Learning diverse models: The coulomb structured support vector machine", "journal": "ECCV", "year": "2016", "authors": "M Schiegg; F Diego; F A Hamprecht"}, {"ref_id": "b61", "title": "Gradcam: Visual explanations from deep networks via gradient-based localization", "journal": "", "year": "2017", "authors": "R R Selvaraju; M Cogswell; A Das; R Vedantam; D Parikh; D Batra"}, {"ref_id": "b62", "title": "Weakly supervised disentanglement with guarantees", "journal": "", "year": "2019", "authors": "R Shu; Y Chen; A Kumar; S Ermon; B Poole"}, {"ref_id": "b63", "title": "Sitatapatra: Blocking the transfer of adversarial samples", "journal": "", "year": "2019", "authors": "I Shumailov; X Gao; Y Zhao; R Mullins; R Anderson; C Z Xu"}, {"ref_id": "b64", "title": "Learning controllable fair representations", "journal": "PMLR", "year": "2019", "authors": "J Song; P Kalluri; A Grover; S Zhao; S Ermon"}, {"ref_id": "b65", "title": "Partial distance correlation with methods for dissimilarities", "journal": "The Annals of Statistics", "year": "2014", "authors": "G J Sz\u00e9kely; M L Rizzo"}, {"ref_id": "b66", "title": "Measuring and testing dependence by correlation of distances", "journal": "The annals of statistics", "year": "2007", "authors": "G J Sz\u00e9kely; M L Rizzo; N K Bakirov"}, {"ref_id": "b67", "title": "Efficientnet: Rethinking model scaling for convolutional neural networks", "journal": "PMLR", "year": "2019", "authors": "M Tan; Q Le"}, {"ref_id": "b68", "title": "Pytorch image models", "journal": "", "year": "2019", "authors": "R Wightman"}, {"ref_id": "b69", "title": "Deep learning incorporating biologically inspired neural dynamics and in-memory computing", "journal": "Nature Machine Intelligence", "year": "2020", "authors": "S Wo\u017aniak; A Pantazi; T Bohnstingl; E Eleftheriou"}, {"ref_id": "b70", "title": "Block stochastic gradient iteration for convex and nonconvex optimization", "journal": "SIAM Journal on Optimization", "year": "2015", "authors": "Y Xu; W Yin"}, {"ref_id": "b71", "title": "Diverse m-best solutions in mrfs", "journal": "", "year": "2011", "authors": "P Yadollahpour; D Batra; G Shakhnarovich"}, {"ref_id": "b72", "title": "Fairness constraints: Mechanisms for fair classification", "journal": "Artificial Intelligence and Statistics", "year": "2017", "authors": "M B Zafar; I Valera; M G Rogriguez; K P Gummadi"}, {"ref_id": "b73", "title": "Learning fair representations", "journal": "PMLR", "year": "2013", "authors": "R Zemel; Y Wu; K Swersky; T Pitassi; C Dwork"}, {"ref_id": "b74", "title": "Measuring nonlinear dependence in time-series, a distance correlation approach", "journal": "Journal of Time Series Analysis", "year": "2012", "authors": "Z Zhou"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig. 1 :1Fig. 1: Examples of Pearson Correlation and Distance Correlation in different settings. (a): y = 0.5x 2 + 0.75n, n \u223c N (0, 1); (b): y = 0.15x 3 + 0.75n + 2.5, n \u223c N (0, 1); (c): x y \u223c N 0 2.5 , 1 0.75 0.75 1.25 ; (d): x y \u223c N 0 2.5 , 1 0 0 1.25", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Fig. 2 :2Fig. 2: Picasso visualization of features space and the correlation between different models. (a) Feature space distribution. (b) Cross-correlation between the feature space of f1 and f2 trained with/without DC. We get better independence. (c) By increasing the balance parameter \u03b1 of DC loss, Mobilenet is more independent to f1.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Fig. 3 :3Fig. 3: (a) Left 4: Similarity between layers within one single model. ViT can be split into small blocks and the similarity from shallow layers to the deeper layers is higher. Most Resnet models show few large blocks in the network, and the last few layers share minimal similarity with the shallow layers. (b) Right 3: Similarity between layers across ViT and Resnets. In the initial 1/6 layers (highlighted in green), the two networks share high similarity. And the last few layers share the least similarity", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Fig. B. 1 .1Our method shows more independence than the baseline model. This implies training with Distance Correlation (DC) can help independence, thus improve robustness to transferred samples. C Experimental Details in Section 5: Informative Comparison Between Networks C.1 Measure similarity between neural networks", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Fig. B. 1 :1Fig. B.1: Picasso visualization of features space and the correlation between different models for all three models. (a) Feature space distribution. (b) Cross-correlation between the feature space of f1 and f2 trained with/without DC. We get better independence.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": ".2 and C.3.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Fig. C. 2 :2Fig. C.2: Extra Grad-CAM results on ImageNet using ViT, Resnet18 and VGG16. After using Partial DC to remove the information learned by another network, ViT can focus on detail places and Resnet can only look in major spots. Similar issue happens to VGG.", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "1 Fig. C. 3 :13Fig. C.3: Extra Grad-CAM results on ImageNet using ViT, Resnet18 and VGG16. After using Partial DC to remove the information learned by another network, ViT can focus on detail places and Resnet can only look in major spots. Similar issue happens to VGG.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Ldisentangle = L rec + \u03bb cls L cls + \u03bb ent L ent + \u03bb res L res (D(12)) In our implementation, \u03bb cls = 0.1, \u03bb ent = 0.01, \u03bb res = 1e \u22125 . D.1 Additional examples of generated images Some more generated images of the same individual are shown in Fig. D.4, D.5, D.6, D.7, D.8, and D.9. We can see that our model can maintain most features in the image (keeps unchanged) and changes the attributes of interest separately. The results here are mostly qualitative.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Fig. D. 4 :4Fig. D.4: Generated images pertaining to the different ages for the same individual.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Fig. D. 5 :5Fig. D.5: Generated images pertaining to different beard levels for the same individual.The first two rows appear perceptually meaningful.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Fig. D. 6 :6Fig. D.6: Generated images pertaining to different ethnicity for the same individual.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Fig. D. 7 :7Fig. D.7: Generated images pertaining to different gender for the same individual.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Fig. D. 8 :8Fig. D.8: Generated images pertaining to different level of \"glasses\" attribute for the same individual.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Fig. D. 9 :9Fig. D.9: Generated images pertaining to different hair color for the same individual.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Block Stochastic Gradient for Updating Distance Correlation Input: Two neural network with starting point \u0398 1 X , \u0398 1 Y . Training data {(xt, yt)} T t=1 , step size \u03b7X , \u03b7Y , and batch size m.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "The test accuracy (%) of a model f2 on the adversarial examples generated using f1 with the same architecture. \"Baseline\": train without constraint. \"Ours\": f2 is independent to f1. \"Clean\": test accuracy without adversarial examples.", "figure_data": "DatasetNetworkMethod Clean FGM\u03f5=0.03 PGD\u03f5=0.03 FGM\u03f5=0.05 PGD\u03f5=0.05 FGM\u03f5=0.1 PGD\u03f5=0.1CIFAR10Resnet 18Baseline 89.1472.1066.3462.0049.4248.2327.41CIFAR10Resnet 18Ours 87.6174.7672.8565.5659.3350.2436.11ImageNet Mobilenet-v3-small Baseline 47.1629.6430.0023.5224.8113.9017.15ImageNet Mobilenet-v3-small Ours 42.3434.4736.9829.5333.7719.5328.04ImageNet Efficientnet-B0 Baseline 57.8526.7228.2218.9619.4512.0411.17ImageNet Efficientnet-B0Ours 55.8230.4235.9922.0527.5614.1617.62ImageNetResnet 34Baseline 64.0152.6256.6145.4551.1133.7541.70ImageNetResnet 34Ours 63.7753.1957.1846.5052.2835.0043.35ImageNetResnet 152Baseline 66.8856.5659.1950.6153.4940.5044.49ImageNetResnet 152Ours 68.0458.3461.3352.5956.0542.6147.17"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Partial DC between the network \u0398X conditioned on the network \u0398Y , and the ImageNet class name embedding. The higher value indicates the more information.", "figure_data": ""}, {"figure_label": "D1", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Values that we use in the disentangle experiment on FFHQ dataset.", "figure_data": "AttributeValuesagekid, teenage, adult, old persongendermale, femaleethnicity African person, white person, Asian personhairbrunette, blond, red, white, black, baldbeardbeard, mustache, goatee, shavedglassesglasses, shades, without glasses"}], "formulas": [{"formula_id": "formula_0", "formula_text": "a k,l = \u2225X k \u2212 X l \u2225,\u0101 k,\u2022 = 1 n n l=1 a k,l ,\u0101 \u2022,l = 1 n n k=1 a k,l , a \u2022,\u2022 = 1 n 2 n k,l=1 a k,l , A k,l = a k,l \u2212\u0101 k,\u2022 \u2212\u0101 \u2022,l +\u0101 \u2022,\u2022(1)", "formula_coordinates": [5.0, 185.92, 137.95, 476.35, 54.01]}, {"formula_id": "formula_1", "formula_text": "B k,l = b k,l \u2212b k,\u2022 \u2212b \u2022,l +b \u2022,", "formula_coordinates": [5.0, 134.77, 197.14, 345.83, 21.61]}, {"formula_id": "formula_2", "formula_text": "R 2 n (x, y) = V 2 n (x,y) \u221a V 2 n (x,x)V 2 n (y,y) , V 2 n (x, x)V 2 n (y, y) > 0 0 , V 2 n (x, x)V 2 n (y, y) = 0 (2)", "formula_coordinates": [5.0, 196.51, 242.95, 284.09, 32.13]}, {"formula_id": "formula_3", "formula_text": ") V n (x, y), V n (x, x) are defined as V 2 n (x, y) = 1 n 2 n k,l=1 A k,l B k,l , V 2 n (x, x) = 1 n 2 n k,l=1 A 2 k,l , with A in (A(1)", "formula_coordinates": [5.0, 134.0, 278.8, 346.31, 23.55]}, {"formula_id": "formula_4", "formula_text": "y = 0.15x 3 + 0.75n + 2.5, n \u223c N (0, 1); (c): x y \u223c N 0 2.5 , 1 0.75 0.75 1.25 ; (d): x y \u223c N 0 2.5 , 1 0 0 1.25 a kl = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 a k,l \u2212 1 n \u2212 2 n i=1 a i,l \u2212 1 n \u2212 2 n j=1 a k,j + 1 (n \u2212 1)(n \u2212 2) n i,j=1 ai,j , k \u0338 = l 0 , k = l (3)", "formula_coordinates": [5.0, 139.63, 629.88, 342.24, 31.5]}, {"formula_id": "formula_5", "formula_text": "P z \u22a5 (x) =\u00c3 \u2212 (\u00c3\u2022C) (C\u2022C)C , P z \u22a5 (y) =B \u2212 (B\u2022C) (C\u2022C)C", "formula_coordinates": [6.0, 134.77, 239.34, 202.43, 15.51]}, {"formula_id": "formula_6", "formula_text": "pdCov(x, y; z) = (P z \u22a5 (x) \u2022 P z \u22a5 (y)) = 1 n(n \u2212 3) i\u0338 =j (P z \u22a5 (x)) i,j (P z \u22a5 (y)) i,j (4)", "formula_coordinates": [6.0, 144.37, 327.33, 336.22, 26.88]}, {"formula_id": "formula_7", "formula_text": "(P z \u22a5 (x)\u2022P z \u22a5 (y)) \u2225P z \u22a5 (x)\u2225\u2225P z \u22a5 (y)\u2225 where \u2225P z \u22a5 (x)\u2225 = (P z \u22a5 (x) \u2022 P z \u22a5 (x)) 1/2 is the norm.", "formula_coordinates": [6.0, 134.11, 357.34, 345.28, 29.55]}, {"formula_id": "formula_8", "formula_text": "\u0398 X ,\u0398 Y \u27e8A(\u0398 X ; x), B(\u0398 Y ; y)\u27e9 \u27e8A(\u0398 X ; x), A(\u0398 X ; x)\u27e9\u27e8B(\u0398 Y ; y), B(\u0398 Y ; y)\u27e9 (5) (A) k,l =||(X) k \u2212 (X) l || 2 , X = \u0398 X (x), (B) k,l = ||(Y ) k \u2212 (Y ) l || 2 , Y = \u0398 Y (y)", "formula_coordinates": [7.0, 142.42, 170.23, 338.17, 40.0]}, {"formula_id": "formula_9", "formula_text": "min \u0398 X ,\u0398 Y \u27e8A, B\u27e9 s.t. max x\u2282D X \u27e8A, A\u27e9 \u2264 m; max y\u2282D Y \u27e8B, B\u27e9 \u2264 m (6)", "formula_coordinates": [7.0, 191.58, 260.23, 289.01, 15.2]}, {"formula_id": "formula_10", "formula_text": "min \u0398 X ,\u0398 Y \u03a6(\u0398 X , \u0398 Y ) = E x,y f (\u0398 X , \u0398 Y ; x, y) + \u03b3(\u0398 X ) + \u03b3(\u0398 Y )(7)", "formula_coordinates": [7.0, 179.09, 319.87, 301.5, 15.2]}, {"formula_id": "formula_11", "formula_text": "f (\u0398 X , \u0398 Y ; x, y) is \u27e8A, B\u27e9 and \u03b3(\u0398 X ) encodes the convex constraint of network \u0398 X : max x\u2282D X \u27e8A, A\u27e9 \u2264 m. Similarly, \u03b3(\u0398 Y ) encodes max y\u2282D Y \u27e8B, B\u27e9 \u2264 m. \u03a6(\u0398 X , \u0398 Y )", "formula_coordinates": [7.0, 134.77, 343.71, 345.83, 33.56]}, {"formula_id": "formula_12", "formula_text": "Output:\u0398 T X ,\u0398 T Y 1: for t = 1, \u2022 \u2022 \u2022 , T do 2: Compute sample gradient for \u0398X g t X = \u2207\u0398 X f (\u0398 t X , \u0398 t Y ; xt, yt) 3: \u0398 t+1 X = arg min \u0398 X \u27e8g t X +\u2207\u03b3X (\u0398 t X ), \u0398X \u2212 \u0398 t X \u27e9 + 1 2\u03b7 X \u2225\u0398X \u2212 \u0398 t X \u2225 2 4: Compute sample gradient for \u0398\u1ef8 g t Y = \u2207\u0398 Y f (\u0398 t+1 X , \u0398 t Y ; xt, yt) 5: \u0398 t+1 Y = arg min \u0398 Y \u27e8g t Y +\u2207\u03b3Y (\u0398 t Y ), \u0398Y \u2212 \u0398 t Y \u27e9 + 1 2\u03b7 Y \u2225\u0398Y \u2212 \u0398 t Y \u2225 2 6: end for 7:\u0398 T X = 1 T T t=1 \u0398 t X 8:\u0398 T Y = 1 T T t=1 \u0398 t Y Proposition 1. After T iterations of Algorithm 1 with step size \u03b7 X = \u03b7 Y = \u03b7 \u221a T < 1 L", "formula_coordinates": [7.0, 149.71, 510.7, 294.22, 123.11]}, {"formula_id": "formula_13", "formula_text": "lim t\u2192\u221e,t\u2208T E[dist(0, \u2207\u03a6(\u0398 t X , \u0398 t Y ))] = 0 (8)", "formula_coordinates": [8.0, 228.52, 170.72, 252.07, 16.66]}, {"formula_id": "formula_14", "formula_text": "Loss total = Loss CE (f 2 (x), y) + \u03b1 \u2022 Loss DC (g 1 (x), g 2 (x)) (9", "formula_coordinates": [8.0, 190.37, 525.44, 285.98, 9.65]}, {"formula_id": "formula_15", "formula_text": ")", "formula_coordinates": [8.0, 476.35, 525.44, 4.24, 8.74]}, {"formula_id": "formula_16", "formula_text": "Network \u0398X Network \u0398Y R 2 (X, GT ) R 2 (Y, GT ) R 2 ((X|Y ), GT ) R 2 ((Y |X), GT ) ViT 1 Resnet", "formula_coordinates": [12.0, 179.43, 141.07, 268.6, 16.23]}, {"formula_id": "formula_17", "formula_text": "Loss CE (f 1 (x), y) \u2212 \u03b1 \u2022 Loss PDC ((g 1 (x)|g 2 (x)), gt)(10)", "formula_coordinates": [12.0, 201.88, 440.04, 278.71, 9.65]}, {"formula_id": "formula_18", "formula_text": "x i = G(f 1 i , ..., f k i , r i ),", "formula_coordinates": [13.0, 279.71, 344.24, 91.02, 12.32]}, {"formula_id": "formula_19", "formula_text": "L res = dCor([f 1 ; f 2 ; ...; f k ], r)(11)", "formula_coordinates": [14.0, 243.58, 142.06, 237.01, 11.72]}, {"formula_id": "formula_20", "formula_text": "lim t\u2192\u221e,t\u2208T E[dist(0, \u2207\u03a6(\u0398 t X , \u0398 t Y ))] = 0 (A(6))", "formula_coordinates": [23.0, 228.52, 150.76, 252.07, 16.66]}, {"formula_id": "formula_21", "formula_text": "E[\u03a6(\u0398 T X ,\u0398 T Y ) \u2212 \u03a6(\u0398 * X , \u0398 * Y )] \u2264 D\u03b7 1 + log T \u221a 1 + T + \u2225\u0398X \u2212 \u0398 1 X \u2225 2 + \u2225\u0398Y \u2212 \u0398 1 Y \u2225 2 2\u03b7 \u221a 1 + T (A(7))", "formula_coordinates": [23.0, 142.64, 220.43, 337.95, 22.46]}, {"formula_id": "formula_22", "formula_text": "F (\u0398 X , \u0398 Y ) = E x,y f (\u0398 X , \u0398 Y ; x, y), \u0393 (\u0398 X , \u0398 Y ) = \u03b3(\u0398 X ) + \u03b3(\u0398 Y )", "formula_coordinates": [23.0, 163.14, 362.21, 289.09, 9.65]}, {"formula_id": "formula_23", "formula_text": "t X = \u2207 \u0398 X f (\u0398 t X , \u0398 t Y ; x t , y t ) g t X = \u2207 \u0398 X F (\u0398 t X , \u0398 t Y ) \u03b4 t X =g t X \u2212 g t X", "formula_coordinates": [23.0, 246.62, 415.86, 122.12, 44.64]}, {"formula_id": "formula_24", "formula_text": "\u2225E[\u03b4 t X |x t , y t ]\u2225 \u2264 c \u2022 max(\u03b7 X , \u03b7 Y ), E\u2225\u03b4 t X \u2225 2 \u2264 \u03c3 2 t Assumption 2", "formula_coordinates": [23.0, 134.77, 523.43, 244.09, 70.76]}, {"formula_id": "formula_25", "formula_text": "\u2225\u2207 X F (\u0398 X , \u0398 Y ) \u2212 \u2207 X F (\u0398 \u2032 X , \u0398 \u2032 Y )\u2225 \u2264 L\u2225(\u0398 X ; \u0398 Y ) \u2212 (\u0398 \u2032 X ; \u0398 \u2032 Y )\u2225, \u2200(\u0398 X , \u0398 Y ), (\u0398 \u2032 X , \u0398 \u2032 Y )", "formula_coordinates": [23.0, 166.77, 617.22, 281.83, 27.64]}, {"formula_id": "formula_26", "formula_text": "\u2225\u03b3(\u0398 X ) \u2212 \u03b3(\u0398 \u2032 X )\u2225 \u2264 L \u03b3 \u2225\u0398 X \u2212 \u0398 \u2032 X \u2225, \u2200\u0398 X , \u0398 \u2032 X Theorem 6.", "formula_coordinates": [24.0, 134.77, 177.18, 272.77, 47.16]}, {"formula_id": "formula_27", "formula_text": "0 < inf t \u03b7 t X \u2264 sup t \u03b7 t X < 1 L 0 < inf t \u03b7 t Y \u2264 sup t \u03b7 t Y < 1 L Under Assumptions 1 through 4, if either X = R n X , Y = R n Y or \u03b3 = 0, and \u221e t=1 \u03c3 2 t < \u221e", "formula_coordinates": [24.0, 133.06, 243.12, 324.26, 117.63]}, {"formula_id": "formula_28", "formula_text": "lim t\u2192\u221e,t\u2208T E[dist(0, \u2207\u03a6(\u0398 t X , \u0398 t Y ))] = 0,", "formula_coordinates": [24.0, 227.14, 383.83, 161.09, 16.66]}, {"formula_id": "formula_29", "formula_text": "\u03b7 t X = \u03b7 t Y = \u03b7 \u221a t < 1 L .", "formula_coordinates": [24.0, 134.77, 460.0, 86.42, 15.02]}, {"formula_id": "formula_30", "formula_text": "E[\u03a6(\u0398 t X , \u0398 t Y ) \u2212 \u03a6(\u0398 * X , \u0398 * Y )] \u2264 D\u03b7 1 + log T \u221a 1 + T + \u2225(\u0398 * X ; \u0398 * Y ) \u2212 (\u0398 1 X ; \u0398 1 Y )\u2225 2 2\u03b7 \u221a 1 + T where\u0398 T X = \u03b7t\u0398 t+1 X T t=1 \u03b7t ,\u0398 T Y = \u03b7t\u0398 t+1 Y T t=1 \u03b7t , and D = s(\u03c3 2 + 4L 2 \u03b3 ) 1 \u2212 L\u03b7 + \u221a s(\u2225(\u0398 * X ; \u0398 * Y )\u2225 + \u03c1)(c + L 8M 2 \u03c1 + 8\u03c3 2 t + 4L 2 \u03b3 )", "formula_coordinates": [24.0, 134.11, 564.09, 329.17, 82.3]}, {"formula_id": "formula_31", "formula_text": "M \u03c1 = 4L 2 \u03c1 2 + 2 max(\u2225\u2207 \u0398 X F (0)\u2225 2 , \u2225\u2207 \u0398 Y F (0)\u2225 2 )", "formula_coordinates": [24.0, 162.25, 655.28, 226.46, 11.1]}, {"formula_id": "formula_32", "formula_text": "L cls = n i=1 n j=1 \u2113(i, j) \u2022 H(Softmax(C j (x i )), f j i ) (D(8))", "formula_coordinates": [28.0, 208.9, 529.47, 271.69, 30.32]}, {"formula_id": "formula_33", "formula_text": "L ent = n i=1 n j=1 (1 \u2212 \u2113(i, j)) \u2022 H(Softmax(C j (x i ))) (D(9))", "formula_coordinates": [28.0, 203.65, 595.49, 276.94, 30.32]}, {"formula_id": "formula_34", "formula_text": "L rec = n i=1 \u03d5(G(f 1 i , ...,f k i , r \u2032 i ), x i ) (D(", "formula_coordinates": [29.0, 237.34, 194.49, 224.36, 30.32]}], "doi": "10.1007/978-3-319-46487-9_36"}