{"title": "The Information Retrieval Experiment Platform", "authors": "Maik Fr\u00f6be; Jan Heinrich Reimer; Sean Macavaney; Simon Reich; Janek Bevendorff; Benno Stein; Matthias Hagen; Niklas Deckers; Martin Potthast", "pub_date": "2023-05-30", "abstract": "We integrate ir_datasets, ir_measures, and PyTerrier with TIRA in the Information Retrieval Experiment Platform (TIREx) to promote more standardized, reproducible, scalable, and even blinded retrieval experiments. Standardization is achieved when a retrieval approach implements PyTerrier's interfaces and the input and output of an experiment are compatible with ir_datasets and ir_measures. However, none of this is a must for reproducibility and scalability, as TIRA can run any dockerized software locally or remotely in a cloud-native execution environment. Version control and caching ensure efficient (re)execution. TIRA allows for blind evaluation when an experiment runs on a remote server or cloud not under the control of the experimenter. The test data and ground truth are then hidden from public access, and the retrieval software has to process them in a sandbox that prevents data leaks. We currently host an instance of TIREx with 15 corpora (1.9 billion documents) on which 32 shared retrieval tasks are based. Using Docker images of 50 standard retrieval approaches, we automatically evaluated all approaches on all tasks (50 \u2022 32 = 1,600 runs) in less than a week on a midsize cluster (1,620 CPU cores and 24 GPUs). This instance of TIREx is open for submissions and will be integrated with the IR Anthology, as well as released open source.\u2022 Information systems \u2192 Retrieval models and ranking; Evaluation of retrieval results.", "sections": [{"heading": "INTRODUCTION", "text": "Research and development in information retrieval (IR) has been predominantly experimental. In its early days in the 1960s, the IR community saw the need to develop and validate experimental procedures, giving rise to the Cranfield paradigm [28], which became the de facto standard for shared tasks hosted at TREC [92] and beyond. Organizers of typical shared IR tasks provide a task description, a document corpus, and topics. Participants implement retrieval approaches for the task and run them on each topic to produce document rankings (a so-called \"run\"). The rankings are then usually submitted as files to the organizers who pool all runs, gather (reusable) relevance judgments for the pools, and calculate the evaluation scores [91]. Finally, the participants describe their methodology and findings in a published \"notebook\" paper. This division of labor allowed the community to scale up collaborative laboratory experiments, especially at a time of limited bandwidths for data exchange, since run files occupy only a few kilobytes. With many research laboratories working independently on the same task, the community draws on the \"wisdom of crowds\" while ensuring rigorous comparative evaluation.\nDespite the lasting success, this way of organizing shared tasks also has shortcomings. First, as with many other disciplines in computer science and beyond, the retrieval approach of a run described in a notebook paper might not be reproducible. There are well-documented cases where reproductions failed, despite putting much effort into it, even for approaches with diligently archived code repositories [1,65]. Second, run submissions require that participants have access to the test topics, which has severe implications [45], such as informing (biasing) the research hypothesis or retrieval approach, unless researchers make a point of not looking at the topics, ever, during development. Third, it cannot be ruled out that current or future large language models have been trained, by mistake or deliberately, on publicly available test data, or that a usage warning stating not to use the data for training would go unnoticed. 1 In any case, the current best practices for shared tasks do not enforce \"blinded experimentation\" 2 with sufficient rigor, compared to other empirical disciplines.\nTo address all of these shortcomings, we have developed the IR Experiment Platform (TIREx; cf. Figure 1 for an overview). Available as open source, 3 a key feature of TIREx is the full integration of tools for working with IR data (ir_datasets [68]), for executing retrieval pipelines (PyTerrier [69]), and for evaluating IR systems (ir_measures [66]) with the TIRA Integrated Research Architecture [43], a continuous integration service for reproducible shared tasks and experiments. TIREx is designed to for reproducibility through software submissions while keeping an experimenter's or task organizer's workload comparable to run file submissions.\nOn our Betaweb and Gammaweb clusters, 4 we have deployed an instance of TIREx that is open for software submissions and experiments. A substantial efficiency boost comes from integrating GPU cores and result caching into the platform to accelerate neural IR approaches. As a proof of concept, we conducted a largescale evaluation of 50 \"standard\" retrieval approaches on 32 shared retrieval tasks (based on 15 corpora with a total of 1.9 billion documents). This experiment consists of 1,600 runs and was started by just clicking a button. It finished unattended in less than a week.", "publication_ref": ["b27", "b91", "b90", "b0", "b64", "b44", "b0", "b2", "b67", "b68", "b65", "b42", "b3"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "BACKGROUND AND RELATED WORK", "text": "We review ad hoc retrieval experiments in evaluation campaigns, common problems and pitfalls in IR experiments, best practices for leaderboards, existing reproducibility initiatives, and tools to support reproducibility. Insights from all these areas have influenced our implementation decisions for TIREx.\nAd hoc Retrieval Experiments in Evaluation Campaigns. Today's shared task-style experiments for ad hoc retrieval evolved from the Cranfield experiments [92]. In the 1960s, the Cranfield experiments [28,29] were conducted on a corpus of 1,400 documents with complete relevance judgments for 225 topics. Since corpus sizes grew substantially, complete judgments became infeasible almost immediately thereafter [92]. The current practice at shared tasks in IR thus is to only assess the relevance of per-topic pools of the submitted systems' top-ranked documents [92]. Subsequent evaluations on the same corpus usually are based on the assumption that the pools are \"essentially complete\", i.e., unjudged documents 4 webis.de/facilities.html#hardware that were not in the pool are non-relevant [92]. Although this completeness assumption is reasonable for tasks with a diverse set of submitted runs pooled at high depth [97], recent observations suggest that scenarios with many relevant documents per query (e.g., corpora with many duplicates [94]) or with topics representing broad information needs [86] are rather problematic. Especially for shared tasks that do not attract diverse submissions, TIREx can help to produce a more diverse judgment pool, as a wide range of different baseline retrieval systems is directly available and can be applied to any imported retrieval task.\nCommon Problems and Pitfalls in IR Experiments. Even though the current discussion about how to conduct IR experiments [44,83,104] includes some controversial points (e.g., whether MRR should be abandoned [44] or not [72,83]), there is still a consensus in the IR community on many characteristics of \"bad\" or \"good\" experiments. For instance, it is rather undisputed that retrieval studies should be internally valid (conclusions must be supported by the data) and externally valid (repeating an experiment on different but similar data should yield similar observations) [46]. Still, external validity of IR experiments remains an open problem [45]. TIREx can help to further improve both: the internal validity via archiving all experiments and results on some corpus (e.g., to accurately correct for multiple hypothesis tests), and the external validity via simplifying to run a submitted software on different data.\nThakur et al. [86] attempted to address the external validity problem by combining diverse retrieval corpora in the BEIR benchmark for en masse evaluation. However, in practice, running an approach on all corpora in BEIR requires some effort, so that many studies still only report results for a selected subset (e.g., [12,41,47])often even without clearly justifying the selection. In contrast, a software in TIREx can rather easily be evaluated against many if not all corpora so that analyzing improvements and limitations of an approach on diverse data is not much effort.\nAn often criticized practice is that many IR studies compare a new approach against weak or \"wrong\" baselines (i.e., not the best or most reasonable previous approaches). Any improvements claimed in such studies are not really meaningful [3,62]. One reason for choosing a wrong baseline could be that neither the researchers nor the reviewers are actually aware of what previous approaches exist for a specific corpus since results are often scattered across multiple publications [62]. Centralized leaderboards that directly show the effectiveness of diverse approaches for a wide range of tasks would address this problem, but multiple efforts have failed so far [62]. In TIREx, we include many popular corpora and standard retrieval approaches right from the start so that the TIREx leaderboards can initially gain traction. The more shared tasks (but also researchers) employ TIREx for software submissions, the broader TIREx' coverage will get over time.\nMaintaining Ongoing Leaderboards. Inspired by the observation that many IR studies do not compare a new approach against reasonable baselines (e.g., the most effective TREC runs) [3], Armstrong et al. [2] released EvaluateIR, a public leaderboard accepting run file submissions. Although the concept was highly valuable for the community in helping researchers and reviewers alike to select appropriate baselines, \"EvaluateIR never gained traction, and a number of similar efforts following it have also floundered\" [62].\nWhile there is still no centralized general leaderboard for IR, certain task-specific leaderboards are quite popular. For instance, the leaderboard of the recent MIRACL Challenge [103] received 25 submissions within one week, and the MS MARCO leaderboard [63] has been popular for years. Maintaining such long-running leaderboards comes with some caveats, as they are conceptually turnbased games where every leaderboard submission might leak information from the test set [63]. Lin et al. [63] propose best practices, inspired by previous problems of the Netflix prize. 5 Most importantly, Lin et al. note that, while submissions to a leaderboard are open, the retrieval results should not be public, nor should system descriptions or implementations, as this would potentially leak information from the test set and foster \"uninteresting\" approaches like ensembles of all the top submissions. With TIREx and its blind evaluation, organizers can choose to blind all submissions as long as they need to, with the ability to unblind approaches and submissions as they see fit, so that TIREx supports the best practices recommended by Lin et al. [63].\nReproducibility Initiatives in IR. Reproducibility is a major challenge in research. For instance, a survey among 1,576 researchers revealed that more than 50% failed at least once to reproduce their own experiments [5]. The IR community makes substantial efforts to foster reproducibility. There are, for instance, dedicated reproducibility tracks at conferences 6 and dedicated reproducibility initiatives like OSIRRC [1,21] or CENTRE [39,40,84,85]. OSIRRC aims to produce archived versions of retrieval systems that are replicable, while CENTRE runs replicability and reproducibility challenges across IR evaluation campaigns. Lin and Zhang [65] looked at all the artifacts produced in the OSIRRC 2015 challenge [1] to verify which results are still replicable four years after their creation. Out of the seven systems that participated in the challenge, only the results of Terrier [75] were fully reproducible out of the box, while two other systems could still be fixed by manual adjustments to the code. The main reasons for failure were that external dependencies could not be loaded anymore, or that platform dependencies changed (i.e., the operating system with its packages). To mitigate the problem of changing platform dependencies, the follow-up iteration of OSIRRC [21] focused on Docker images that had to implement a strict specification (enforced by the companion tool \"jig\") that triggered the indexing and subsequent retrieval via Docker hooks. Even though 17 systems have been dockerized to follow the jig specification, the concept has not gained traction. By centering TIREx around shared tasks in the beginning, we hope that we can kick off and maintain the attention of the community. Furthermore, we believe that there are many retrieval scenarios that can not be encapsulated into the two-step index-then-retrieve pipeline that jig imposes (e.g., explicit relevance feedback). We thus minimize the TIREx requirements: just Docker images in which commands are executed without Internet access on read-only mounted data.\nTooling for Reproducibility. Many tools have been developed to support shared tasks by reducing the workload of organizers and participants while increasing the reproducibility [18,43,54,56,87,88,100]. For instance, as documenting the metadata of experiments improves reproducibility [61], ir_metadata [17] simplifies the documentation of IR experiments according to the PRIMAD model [38] (platform, research goal, implementation, method, actor, data). There are also platforms that support organizing and running shared tasks, among which four are still active: CodaLab, EvalAI, STELLA, and TIRA. 7 They implement the so-called evaluationas-a-service paradigm in the form of cloud-based web services for evaluations [55]. Of these four systems, STELLA and TIRA are hosted within universities, while CodaLab and EvalAI use Microsoft Azure and Amazon S3, respectively. We use TIRA for TIREx as it supports blinded experimentation and as it is based on (private) git repositories hosted on GitLab or GitHub to versionize shared tasks and to distribute the workloads via runners connected to the corresponding repositories. The computation can thus be done in the cloud but also on private machines. We substantially extend large parts of TIRA as part of TIREx so that it supports the current IR workflows like chaining multiple retrieval stages.", "publication_ref": ["b91", "b27", "b28", "b91", "b91", "b3", "b91", "b96", "b93", "b85", "b43", "b82", "b103", "b43", "b71", "b82", "b45", "b44", "b85", "b11", "b40", "b46", "b2", "b61", "b61", "b61", "b2", "b1", "b61", "b102", "b62", "b62", "b62", "b4", "b62", "b4", "b5", "b0", "b20", "b38", "b39", "b83", "b84", "b64", "b0", "b74", "b20", "b17", "b42", "b53", "b55", "b86", "b87", "b99", "b60", "b16", "b37", "b6", "b54"], "figure_ref": [], "table_ref": []}, {"heading": "THE IR EXPERIMENT PLATFORM", "text": "We have constructed the Information Retrieval Experiment Platform (TIREx) to facilitate reproducible, shared task-style IR experiments based on software submissions. This has been achieved by integrating ir_datasets, ir_measures, and PyTerrier into TIRA. We anticipate the sustained availability and maintenance of these components, as evidenced by TIRA's and PyTerrier's consistent upkeep since 2012 [48] and 2020 [70], respectively, and the growing popularity of ir_datasets in recent years. Previously, conducting shared task-style IR experiments within TIRA was already possible, but required significant effort from both organizers and participants due to their unique nature, compared to standard Machine Learning or Natural Language Processing experiments. IR experiments typically involve intermediate artifacts (like indexes), and retrieval systems involve multi-stage \"telescoping\" pipelines. 8 To address these requirements, TIREx extends TIRA with common IR tools for data access, indexing, retrieval, and evaluation, and implements multistage pipelines on top of TIRA's underlying execution protocol. Below, we elaborate on how TIREx supports IR experiments, discuss the interaction between integrated tools, provide examples of using available retrieval approaches in TIREx, and demonstrate how TIREx promotes post-experiment replicability and reproducibility through declarative PyTerrier pipelines.", "publication_ref": ["b47", "b69", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments in the IR Experiment Platform", "text": "As illustrated in Figure 1, TIREx facilitates the entire process of conducting retrieval experiments. It allows shared task organizers and individual experimenters to import data and utilize any pre-existing retrieval software submitted to TIREx as baselines. Following that, submissions of new retrieval approaches for evaluation can be made as software submissions or, if enabled, also as run submissions. Any submission can be accompanied by descriptive annotations and metadata; for instance, run submissions can be grouped to denote that they were generated by the same retrieval approach for multiple retrieval tasks. By providing relevance judgments, organizers or experimenters can directly evaluate all available runs.\nTo incorporate a new corpus and topics into TIREx, they can be easily added to ir_datasets, utilizing a private branch if the data is sensitive. This data can then be imported by TIRA through a Docker image with a matching ir_datasets installation. Participants submit their software as Docker images as well. TIRA ensures their reproducibility and prevents test data leaks by executing them in a sandbox. Among other things, the sandbox disables Internet connectivity for the running software, which ensures that the software and its dependencies are fully installed and no data is sent to unauthorized third parties. Participants can provide additional data their software needs during execution by uploading it to TIRA. This is particularly useful for non-reproducible elements of a submission, such as manual query reformulations. TIREx also provides a \"starter implementation\" for five commonly used IR research frameworks, which participants can use as a development base. The simplest starter uses BM25 retrieval, which is implemented using a few lines of declarative PyTerrier code in a Jupyter notebook. 9 TIREx allows for software submissions to be executed on demand within a cloud-based execution environment, utilizing GitLab or GitHub CI/CD pipelines. In order to meet varying demand, experiment organizers can incorporate additional runners as necessary. TIREx maintains a comprehensive record of every artifact of a retrieval experiment within a specific git repository (Figure 1, right), which can be exported and published. This \"archived shared task\" is entirely self-contained, enabling the independent re-execution of approaches with identical or differing data using PyTerrier pipelines. The availability of every software that generated a run as part of the repository makes it a key outcome and asset of an experiment. Consequently, TIREx facilitates \"always-on\" shared tasks for the IR community, along with an extensive variety of ablation studies.", "publication_ref": ["b8"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Reproducible Shared Tasks with TIRA", "text": "TIRA is used to handle software submissions in shared tasks since 2012 [48,77]-the CLEF labs PAN and Touch\u00e9 being two longrunning examples. 10 A first version of TIRA provided participants with access to virtual machines to deploy their software. However, this setup required manual overhead on the part of organizers thus did not scale far beyond these two events. Moreover, software reexecution was possible in principle and has been demonstrated once at scale [49], but proved to be error-prone and required manual bug fixing inside the virtual machines as participant software was not robust against slight data format variations that were in principle supported by the underlying formatting schema. This also has prevented external researchers from reproducing the collected software for a given task at scale.\nMeanwhile, Docker has gained maturity and widespread adoption and is now supported by many cluster computing frameworks such as Kubernetes. Especially their integration as GitHub and Git-Lab runners made automatic deployment widely available. Hence, TIRA was completely redeveloped based on the now industrystandard CI/CD pipelines (continuous integration and deployment) using Git, Docker, and Kubernetes [43]. In the new version of TIRA, participants upload their software implemented in Docker images to a private Docker registry dedicated to their team, ensuring that different teams do not influence each other while a shared task is running-the approaches can remain private until the task ends. For on-demand execution, TIRA presently runs the software on our Kubernetes cluster (1,620 CPU cores, 25.4 TB RAM, 24 GeForce GTX 1080 GPUs). This version of TIRA was first used in two NLP tasks hosted at SemEval 2023 to which 71 of 170 registered teams submitted 647 runs based on software submissions [42,60].\nWhile preparing the TIRA setup for the retrieval-oriented Touch\u00e9 2023 tasks [9], we realized that the new TIRA still had some shortcomings. There was no unified access to IR data, no separation between full-rank or re-rank approaches, no modularization of software components with caching, and typical IR workflows were only realizable inefficiently or via workarounds. For instance, full-rank retrieval in TIRA would have required any software to build an index from scratch and different re-rank approaches would each have to re-create the baseline rankings. A re-ranking approach for the ClueWeb22-based Task 2 of Touch\u00e9 2023 [9], for example, should have been able to use a ChatNoir baseline ranker [7] from within TIRA, but our pilot experiments showed that retrieving the top-1000 ChatNoir results for some set of 50 Touch\u00e9 topics [8][9][10][11] takes 54 to 134 minutes (ChatNoir requests can fail so that a client has to retry the requests). Blocking GPUs-often required by re-rankers-for such a long time would waste resources and the baseline's top-1000 results should ideally be cached so that different re-rankers can directly use them. To solve all these problems, we substantially expanded TIRA and redeveloped major parts to integrate ir_datasets, ir_measures, and PyTerrier.", "publication_ref": ["b47", "b76", "b9", "b48", "b42", "b41", "b59", "b8", "b8", "b6", "b7", "b8", "b9", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Standardized Data Access with ir_datasets", "text": "The ir_datasets toolkit [68] provides a standard interface to access over 200 corpora and over 500 topic sets frequently used in IR experiments. The data is kept up-to-date (e.g., most TREC 2022 tracks are included) and processing documents or topics is possible via a single line of Python code. Thus, ir_datasets already serves as a common data layer in numerous IR frameworks and tools (e.g., Capreolus [102], Experimaestro-IR [76], FlexNeuART [14], OpenNIR [67], Patapsco [32], PyTerrier [69]) and can be easily incorporated by most others (e.g., Anserini [101], PISA [71]). We integrate ir_datasets into TIRA via Docker images that can import complete corpora (for full-rank approaches) and that can create re-rankings for any given run file (for re-ranking approaches). To configure an IR experiment in TIRA, the experiment organizer only needs to provide an ir_datasets Docker image-standard images are available in TIREx but other images are also possible (e.g., for proprietary data). In the following, we further describe the new 'default_text' fields that we added to ir_datasets to enable re-using single-field retrieval software on corpora with multiple text fields, and we describe how the integration of ir_datasets into Docker images that run on-demand also ensures interchangeability and compatibility of retrieval components in retrieval pipelines.\nRe-Usable Retrieval Software via default_text. While some corpora have a single text field for each document (e.g., the MS MARCO passage ranking corpus [36,37,73]), others provide rich structural information or metadata (e.g., the Touch\u00e9 corpora [10,11] with structured arguments or comparison aspects). Similarly, some retrieval tasks have a single text field per topic (e.g., Antique [50]), while others provide metadata for each topic and/or multiple fields for versions of a query (e.g., TREC Precision Medicine [81,82]).\nCorpora and retrieval tasks with fine-grained structure usually address the development of built-for-purpose retrieval systems that exploit the task-specific setup. For instance, an argument retrieval system submitted to Touch\u00e9 may specifically focus on the argumentative premises contained in a document, and an approach in the Precision Medicine track may use a query's structure to adjust the relevance criteria. Instead, corpora and tasks with single fields for document texts and queries often rather address \"general search\" scenarios (i.e., retrieval approaches that can be applied in a variety of contexts rather than targeting one specific case). To also enable the evaluation of such general purpose retrieval systems (that expect a single document text field and a single query field) on data with more fields, we created default_text fields for every dataset in ir_datasets. There often is a natural choice for a document's or a query's \"default text\" (e.g., we simply concatenated the two fields 'title' and 'abstract' of MEDLINE documents as the default document text and we often selected a TREC topic's title as the query text-after a manual review). Still, there also are more difficult cases for which we then carefully tried to select the most important content of the documents or topics-being open to corrective pull requests from the community. The new default_text fields now are part of the ir_datasets package and thus also applicable in TIREx to ensure reusability of single-field retrieval approaches on data originally only available with multiple fields.\nEnsuring Compatibility of Modularized Retrieval Stages. TIREx aims to support experiments in which components for the individual stages of modularized retrieval pipelines can be easily replaced and compared without having to adapt the complete retrieval software each time. Therefore, TIRA distinguishes between two types of retrieval approaches: (1) full-rank approaches with a document corpus and topics as input, and (2) re-rankers with a re-rank file as input (basically, query-document pairs). From any retrieval software's output, a re-rank file can be automatically created and cached in TIREx by the ir_datasets integration. As the structure of these re-rank files always is the same, any re-ranker can easily run on the output of any previous retrieval approach. Note that some data in ir_datasets can not be downloaded from the Web and/or requires license agreements (e.g., the ClueWeb and GOV corpora). As we have valid license agreements on our local TIREx instance, we can directly mount such data into the ir_datasets container, but, by default, then only show effectiveness scores for a run and no retrieval results (i.e., participants do not get access to the corpus as their software is executed in a sandbox and all outputs other than effectiveness scores are not shown on confidential datasets).\nTable 1 shows the data fields that the ir_datasets integration makes available. For full-rank software, the documents.jsonl.gz file for each document contains an identifier 'docno', the new de-fault_text in the field 'text', and all original structured fields of a document in 'original_document'. The topics.jsonl.gz file for each topic contains an identifier 'qid', the new default_text in the field 'query', and all original structured fields of a topic in 'origi-nal_topic'. For re-rankers, the ir_datasets integration creates a file re-rank.jsonl.gz from the output of a previous retrieval stage (i.e., the run file), where each entry contains query-document pairs to be reranked along their score and rank assigned by the previous stage. When relevance judgments exist, the ir_datasets integration can also make them available in a qrels.txt file so that the evaluator software specified by the experiment organizer can automatically evaluate submitted retrieval approaches.", "publication_ref": ["b67", "b101", "b75", "b13", "b66", "b31", "b68", "b100", "b70", "b35", "b36", "b72", "b9", "b10", "b49", "b80", "b81"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Sanity-checked Evaluation with ir_measures", "text": "TIRA can automatically evaluate run files (created by retrieval software submissions or uploads) via an ir_measures evaluator. First, the evaluator performs a sanity check to test whether a run file can be parsed and warns of potential errors (e.g., score ties, NaN scores, empty result sets, unknown queries, scores contradicting the ranks, etc.). Then, if relevance judgments have been provided, the evaluator derives all specified measures averaged over all queries and per query (suitable for significance tests).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Reproducible IR Pipelines with TIRA", "text": "To improve the efficiency of common IR workflows in TIREx, we redeveloped and extended TIRA's ability to define and run modularized software even spanning multiple Docker images. All software in TIRA is immutable so that outputs of one software (e.g., an index) can be cached and reused by another software.\nModularized Software with Multiple Components. Retrieval software in TIRA can have multiple components that form a sequence similar to UNIX pipes or even a directed acyclic graph (DAG). Each component has a Docker image with a command to be executed and can have none, one, or many preceding components, respectively. TIRA passes the corresponding input and output directories to each component via three variables (cf. Table 2). The variable $inputDataset points to the directory that contains the actual input (e.g., re-rank.jsonl.gz for re-ranking software). The variable  $inputRun is only available when a component has preceding components and then points to a directory with all outputs of the directly preceding components. The variable $outputDir specifies the location where TIRA expects a component's outputs. All three variables $inputDataset, $inputRun, and $outputDir can be included in the command to be executed but are also available as environment variables within a container.\nComponents for Additional Data. A retrieval approach might use data unavailable in an experiment's original corpus and topics. An example are user query variants for ClueWeb or Common Core topics [4,6]-using the variants would count as a \"manual\" run at TREC. TIREx supports such cases of additional data via file uploads which becomes available to subsequent software via the $inputRun variable. Uploaded files can be grouped, documented, and configured as components treated like a software component (e.g., to precede some ranking component). Thus, TIREx supports manual runs and other kinds of use cases, but isolates such steps as much as possible to keep the software part of a pipeline replicable.\nDefining Retrieval Pipelines. Figure 2 illustrates the conceptual data flow for the two simplest-possible sequence and DAG retrieval pipelines, respectively. The upper pipeline shows a full-rank approach that first creates an index (component 'Index Corpus' with output file-01 and file-02) that TIRA then makes available as $inputRun for the second component 'BM25 Retrieval'. Figure 3 shows how a BM25 retrieval component that depends on a PyTerrier index can be defined in TIRA. Since many different components of a software may use a created artifact like an index, we cache all outputs to make pipelines more efficient.\nThe lower pipeline in Figure 2 shows a learning-to-rank component that depends on a BM25 retrieval and on uploaded query   features (e.g., user behavior data like clicks and dwell-times obtained from a user study). When a component has inputs from more than one component, TIRA makes them available in the order in which the components have been defined. Preceding components or uploads must exist when defining a new component and TIRA decouples the command to be executed from the Docker image so that the same image can be used to run different retrieval approaches (e.g., by switching parameters). In combination with caching, this improves the efficiency for a wide range of common multi-stage retrieval pipelines.\n$\nEfficiency via Caching. As every software in TIRA is immutable, coding errors in a component can only be fixed by adding a new Listing 1: Full-rank retrieval from a complete corpus. pipeline = tira . pt . retriever ( ' < task -name >/ < user -name >/ < software > ', dataset = '< dataset > ' ) advanced_pipeline = pipeline >> advanced_reranker Listing 2: Re-ranking BM25 with a submitted software. bm25 = pt . BatchRetrieve ( index , wmodel = \" BM25 \" ) reranker = bm25 >> tira . pt . reranker ( ' < task -name >/ < user -name >/ < software > ' ) version of the component. Immutability enables the implementation of efficient and reliable retrieval pipelines, since their output can both be cached and traced back and replicated by that same (version) of a component. TIRA disallows the deletion of components or outputs that have been used as inputs by some other component. When any component is requested to produce an output on some data, it is first checked whether that output already is cached in which case executing the component is not necessary. This way, retrieval pipelines in TIRA can efficiently re-use components and remain replicable, as the steps to produce a final run are fully tracked and versioned in the experiment repository.", "publication_ref": ["b3", "b5"], "figure_ref": ["fig_1", "fig_2", "fig_1"], "table_ref": ["tab_1"]}, {"heading": "Local Pipeline Reproduction with PyTerrier", "text": "When an experiment repository is exported and published by the organizers, by default, the test data is kept private but the run files are published via TIRA and software submissions are uploaded as Docker images to Docker Hub. All possible follow-up studies (e.g., a reproducibility study for a shared task) can be conducted independent of TIRA, as archived experiment repositories are fully self-contained. In the following, we briefly showcase some post-hoc experiments in PyTerrier. 11 Listing 1 shows how a full-rank approach from a TIRA experiment repository can be reproduced with a declarative PyTerrier pipeline. The approach is identified as the <software> submitted by team <user-name> to the shared task <task-name> and is applied to <dataset> (does not need to be the original task data). Internally, the required Docker images are downloaded and run in their required order to obtain the results. These results can then be re-ranked by any PyTerrier re-ranker, allowing for experiments to improve an original submission. Also re-rankers available in some TIRA experiment repository can be used in post-hoc PyTerrier experiments (cf. Listing 2 for an example re-ranking of BM25).\nListing 3 shows how run files resulting from some (software) submission can be loaded into PyTerrier. The from_submission method allows to access some submitted approach's output without having to re-run it (e.g., this also eases pooling for task organizers). The PyTerrier integration allows easy replicability experiments if the dataset is the same as in the original experiment, and reproducibility experiments if some other dataset is used for retrieval approaches. 11 Examples available at: github.com/tira-io/ir-experiment-platform#reproducibility Listing 3: Re-ranking a run created by a software submission. first_stage = tira . pt . from_submission ( ' <task -name >/ < user -name >/ < software > ', dataset = '< dataset > ' ) advanced_pipeline = first_stage >> advanced_reranker ", "publication_ref": ["b10", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "EVALUATION", "text": "To demonstrate the scalability of TIREx, we report about an experiment with 50 retrieval approaches on 32 retrieval tasks based on 15 corpora (1.9 billion documents). The resulting leaderboards are public and new submissions can be made at any time. 12 We also describe a repro_eval-based [16] case study on system preference reproducibility for different tasks.", "publication_ref": ["b11", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Scalable Retrieval Experiments", "text": "Table 3 shows the 15 corpora currently available in TIREx. Each has been used for 1 to 4 shared retrieval tasks, consists of 1,400 to 1 billion documents, and comes with the relevance judgments created during the respective shared tasks. Table 4 overviews the 50 retrieval approaches that we imported into TIREx from 5 retrieval frameworks: BEIR [86], ChatNoir [7], Pyserini [64] (our import was not ready during the experiments), PyGaggle [64], PyTerrier [69] (including two PyTerrier plugins for duoT5 [79] and ColBERT [59]). From BEIR, we use 17 dense retrieval approaches (e.g., ANCE [99], DPR [58], and TAS-B [53]) by using the different SBERT [80] models available in BEIR. ChatNoir is an Elasticsearch-based BM25F search engine hosting all three ClueWeb corpora. It can be accessed from within TIRA to allow retrieval approaches on huge corpora with a REST-API that is kept consistent to ensure reproducibility. From Pyserini, we use the 4 lexical models available trough the SimpleSearcher interface. From PyGaggle, we use 8 variants of monoBERT [74] and monoT5 [79] (including the state-of-the-art monoT5 with 3 billion parameters), and from PyTerrier, we use 20 lexical retrieval models (e.g., BM25, PL2, etc.). From the duoT5 plugin of PyTerrier, we use 3 variants based on different duoT5 models (including the state-of-the-art model with 3 billion parameters). For all retrieval approaches, we keep all parameters at their default values. Almost all approaches use the default_textbased fields that we added to ir_datasets, except for ChatNoir that is a full-rank software for the ClueWeb corpora and uses different fields (title, body, etc.). The lexical approaches in PyTerrier and the dense approaches in BEIR can be configured as full-rank software (i.e., a first component building an index and a second component retrieving from the index) or re-rank software-but are just counted as one approach in Table 4. All duoT5 and PyGaggle approaches only work as re-rankers. For ColBERT, we only use the re-rank variant, as ColBERT indices become very large.\nIn TIREx, all of these variants are available. To increase result comparability, however, our analysis fixes the first stage rankers to ChatNoir for the ClueWeb corpora and PyTerrier BM25 on all other corpora. Their respective results are then handed to the total of 50 available re-ranking approaches mentioned above. Altogether, 50 approaches are executed on all 32 tasks listed in Table 3. We executed the lexical approaches using 1 CPU and 10 GB RAM, while all other approaches had additional access to a GeForce GTX 1080 GPU with 8 GB RAM. Some models fail on this GPU as 8 GB of RAM do not suffice: ColBERT and two SBERT models failed on a few tasks, while the 3 billion parameter monoT5 / duoT5 failed on all tasks. To handle these cases, we added two runners with access to an A100 GPU with 40 GB RAM to TIRA, which was sufficient. TIRA manages metadata about the resources used to produce a run, making hardware difference between evaluations transparent.\nTable 5 shows the aggregated evaluation results on 31 tasks (leaving out the ClueWeb22 as there are no judgments yet). We report the effectiveness as nDCG@10 (macro-averaged in case a corpus is associated with multiple tasks) for BM25, ColBERT, TAS-B, all three duoT5 variants, and monoT5 (in its default configuration with its default model) and the best, median, and worst approaches from the groups of 20 lexical, 17 bi-encoder, and 8 PyGaggle approaches. All deep learning models were trained on MS MARCO and thus substantially improve upon the lexical models on MS MARCO. However, on other corpora the deep learning models work in a zero-shot manner so that sometimes a lexical approach achieves the highest effectiveness (Args.me, ClueWeb09, and MEDLINE). Our results further show that BM25 is not always the best lexical ranker (e.g., on Args.me: 0.43 vs. 0.57). The effectiveness gap between the best and the worst model of a group can be substantial on some corpora (e.g., lexical models on Args.me: 0.14 vs. 0.57), while being negligible on others (e.g., lexical models on NFCorpus). The leaderboards of TIREx as aggregated in Table 5 allow to easily select competitive baselines for very different tasks-often much easier than before.", "publication_ref": ["b85", "b6", "b63", "b63", "b68", "b78", "b58", "b98", "b57", "b52", "b79", "b73", "b78"], "figure_ref": [], "table_ref": ["tab_3", "tab_4", "tab_4", "tab_3", "tab_5", "tab_5"]}, {"heading": "Case Study: Reproducibility Analysis", "text": "As an example of a post-hoc analysis enabled by TIREx, we use repro_eval to analyze to which degree system preferences from the TREC Deep Learning 2019 task can be reproduced on other tasks. For each preference between approaches on TREC Deep Learning 2019 (e.g., monoT5 with an nDCG@10 of 0.71 compared to BM25's 0.48 induces a clear system preference), we set the approach with the lower effectiveness on TREC Deep Learning 2019 as the \"baseline\" in repro_eval and the other approach as the \"advanced system\". We study the reproducibility of the preferences on two dimensions [15]: (1) the effect ratio of the reproduction, and (2) the delta relative improvement of the reproduction. The effect ratio measures to which degree the advanced system is still better than the baseline on the different task (1 indicates a perfect reproducibility, values between 0 and 1 indicate reproducibility with diminished improvements on the different task, and 0 indicates failed reproducibility), while the delta relative improvement measures the relative effectiveness difference of the advanced system to the baseline (0 indicates perfect reproducibility, values between -1 and 0 indicate an increased relative improvement of the advanced system, values between 0 and 1 indicate a smaller relative improvement, and 1 indicates failed reproducibility).\nTable 6 shows the results of the preference reproducibility analysis. We report the ratio of system preferences with a successful reproduction (i.e., effect ratio > 0) and the 25%, 50%, and 75% quantiles for the effect ratio and the relative delta improvement. We order the tasks by the percentage of successfully reproduced preferences and show the top-5 tasks and every fifth lower ranked task. Not that surprising, the reproducibility on the very similar TREC Deep Learning 2020 is very good (88.1%) but declines fast for other tasks (e.g., only 57.8% for the Web track 2003 on rank 15). Analyzing the quantiles yields similar observations (e.g., 50% of the system preferences have an almost perfect effect ratio of 0.90 or higher for TREC Deep Learning 2020, while the Web track 2003 on rank 15 has a median effect ratio of 0.04).", "publication_ref": ["b14"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "DISCUSSION", "text": "Potential Impact of TIREx. We believe that TIREx can have a substantial conceptual impact as we see no alternative to blinded retrieval evaluations in the future (given the practice of training LLMs on basically all available ground truth for IR and NLP tasks [20]). Additionally, the platform eases the organization of reproducible IR experiments with software submissions. Shared task organizers can simply provide the well-documented open-source baselines from TIRA as starting points for the participants and can also use the baselines to ensure some more diverse judgment pools, especially for tasks that attract few participants. For shared tasks that   run multiple years on different data, the organizers can automatically re-run all approaches submitted to previous editions to track progress. TIREx combines leaderboards with immutable software, promoting provenance of results, and enabling researchers and reviewers to identify and locally reproduce good baselines. The submission platform TIRA proved robust after its complete redevelopment [43]: two NLP tasks used TIRA at SemEval 2023 [42,60] for which 71 of the 171 registered teams created 647 runs with software submissions. Our initial retrieval experiments with TIREx produced another 1,600 runs on standard corpora in less than a week, showing the platform to be robust and to have the potential for scaling up. When adopted by shared tasks and in individual IR experiments, TIREx can become a (federated) hub for IR resources and serve as a reference for reviewers. If a sufficient number of retrieval approaches, corpora, and supplementary data (e.g., manual query reformulations) are available through TIREx, integrating new resources gives direct access to an entire ecosystem, furthering the nascent standardization of IR experiments.\nFuture Extensions of TIREx. Interesting directions for future development besides including further IR frameworks and libraries are integrations of TIREx with the IR Anthology [78] and with Diff-IR [57]. An integration with the IR Anthology would enable links between entries in the TIREx leaderboards and the corresponding publications in the IR Anthology to provide more detailed information on an approach but also to \"extend\" a publication by adding results on different corpora than originally used and putting an approach in a broader context with other approaches run on the same data. An integration with DiffIR would enable the rendering of runs as search engine result pages to easily contrast the quantitative evaluations already possible via the integrated ir_measures with more qualitative evaluations of ranking differences or even (basic) user studies.", "publication_ref": ["b19", "b42", "b41", "b59", "b77", "b56"], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSION", "text": "With TIREx-The IR Experiment Platform-we aim to substantially ease conducting (blinded) IR experiments and organizing \"always-on\" reproducible shared tasks on the basis of software submissions. TIREx integrates ir_datasets, ir_measures, and PyTerrier with TIRA. Retrieval workflows can be executed on-demand via cloud-native orchestration, reducing the effort for reproducing IR experiments since software submitted to TIREx can be reexecuted in post-hoc experiments. The platform has no lock-in effect, as archived experiments are fully self-contained, work standalone, and are easily exported. By keeping test data private, TIREx promotes further standardization and provenance of IR experiments following the example of, e.g., medicine, where blinded experiments are the norm. TIREx is open to the IR community and ready to include more corpora, shared tasks, and retrieval approaches.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ACKNOWLEDGMENTS", "text": "This work has been partially supported by the OpenWebSearch.eu project (funded by the EU; GA 101070014).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "SIGIR 2015 workshop on reproducibility, inexplicability, and generalizability of results (RIGOR). SIGIR 2015", "journal": "", "year": "", "authors": "J Arguello; F Diaz; J Lin; A Trotman"}, {"ref_id": "b1", "title": "EvaluatIR: An online tool for evaluating and comparing IR systems", "journal": "", "year": "2009", "authors": "T G Armstrong; A Moffat; W Webber; J Zobel"}, {"ref_id": "b2", "title": "Improvements that don't add up: Ad-hoc retrieval results since 1998", "journal": "", "year": "2009", "authors": "T G Armstrong; A Moffat; W Webber; J Zobel"}, {"ref_id": "b3", "title": "UQV100: A test collection with query variability", "journal": "", "year": "2016", "authors": "P Bailey; A Moffat; F Scholer; P Thomas"}, {"ref_id": "b4", "title": "1,500 scientists lift the lid on reproducibility", "journal": "Nature", "year": "2016", "authors": "M Baker"}, {"ref_id": "b5", "title": "RMIT at the 2018 TREC CORE Track. TREC", "journal": "", "year": "2018", "authors": "R Benham; L Gallagher; J M Mackenzie; B Liu; X Lu; F Scholer; J Shane Culpepper; A Moffat"}, {"ref_id": "b6", "title": "Elastic ChatNoir: Search engine for the ClueWeb and the Common Crawl", "journal": "", "year": "2018", "authors": "J Bevendorff; B Stein; M Hagen; M Potthast"}, {"ref_id": "b7", "title": "Overview of Touch\u00e9 2020: Argument retrieval", "journal": "", "year": "", "authors": "A Bondarenko; M Fr\u00f6be; M Beloucif; L Gienapp; Y Ajjour; A Panchenko; C Biemann; B Stein; H Wachsmuth; M Potthast; M Hagen"}, {"ref_id": "b8", "title": "Overview of Touch\u00e9 2023: Argument and causal retrieval. ECIR 2023", "journal": "", "year": "", "authors": "A Bondarenko; M Fr\u00f6be; J Kiesel; F Schlatt; V Barriere; B Ravenet; L Hemamou; S Luck; J Heinrich Reimer; B Stein; M Potthast; M Hagen"}, {"ref_id": "b9", "title": "Overview of Touch\u00e9 2022: Argument retrieval. CLEF 2022", "journal": "", "year": "", "authors": "A Bondarenko; M Fr\u00f6be; J Kiesel; S Syed; T Gurcke; M Beloucif; A Panchenko; C Biemann; B Stein; H Wachsmuth; M Potthast; M Hagen"}, {"ref_id": "b10", "title": "Overview of Touch\u00e9 2021: Argument retrieval. CLEF 2021", "journal": "", "year": "", "authors": "A Bondarenko; L Gienapp; M Fr\u00f6be; M Beloucif; Y Ajjour; A Panchenko; C Biemann; B Stein; H Wachsmuth; M Potthast; M Hagen"}, {"ref_id": "b11", "title": "InPars: Unsupervised dataset generation for information retrieval", "journal": "", "year": "", "authors": "L Bonifacio; H Abonizio; M Fadaee; R Nogueira"}, {"ref_id": "b12", "title": "A full-text learning to rank dataset for medical information retrieval", "journal": "", "year": "2016", "authors": "V Boteva; D Ghalandari; A Sokolov; S Riezler"}, {"ref_id": "b13", "title": "Flexible retrieval with NMSLIB and FlexNeuART. NLP-OSS 2020", "journal": "", "year": "", "authors": "L Boytsov; E Nyberg"}, {"ref_id": "b14", "title": "How to measure the reproducibility of system-oriented IR experiments", "journal": "", "year": "", "authors": "T Breuer; N Ferro; N Fuhr; M Maistro; T Sakai; P Schaer; I Soboroff"}, {"ref_id": "b15", "title": "repro_eval: A Python interface to reproducibility measures of system-oriented IR experiments", "journal": "", "year": "", "authors": "T Breuer; N Ferro; M Maistro; P Schaer"}, {"ref_id": "b16", "title": "ir_metadata: An extensible metadata schema for IR experiments", "journal": "", "year": "", "authors": "T Breuer; J Keller; P Schaer"}, {"ref_id": "b17", "title": "STELLA: Towards a framework for the reproducibility of online search experiments", "journal": "", "year": "", "authors": "T Breuer; P Schaer; N Tavakolpoursaleh; J Schaible; B Wolff; B M\u00fcller"}, {"ref_id": "b18", "title": "The TREC 2006 Terabyte track. TREC", "journal": "", "year": "2006", "authors": "S B\u00fcttcher; C L A Clarke; I Soboroff"}, {"ref_id": "b19", "title": "", "journal": "", "year": "2022", "authors": "H Chung; L Hou; S Longpre; B Zoph; Y Tay; W Fedus; E Li; X Wang; M Dehghani; S Brahma; A Webson; S Shane Gu; Z Dai; M Suzgun; X Chen; A Chowdhery; S Narang; G Mishra; A Yu; V Y Zhao; Y Huang; A M Dai; H Yu; S Petrov; E H Chi; J Dean; J Devlin; A Roberts; D Zhou; Q V Le; J Wei"}, {"ref_id": "b20", "title": "Overview of the 2019 Open-Source IR Replicability Challenge (OSIRRC", "journal": "", "year": "2019", "authors": "R Clancy; N Ferro; C Hauff; J Lin; T Sakai; Z Z Wu"}, {"ref_id": "b21", "title": "Overview of the TREC 2004 Terabyte track", "journal": "", "year": "2004", "authors": "C L A Clarke; N Craswell; I Soboroff"}, {"ref_id": "b22", "title": "Overview of the TREC", "journal": "", "year": "2009", "authors": "C L A Clarke; N Craswell; I Soboroff"}, {"ref_id": "b23", "title": "Overview of the TREC 2010 Web track", "journal": "TREC", "year": "2010", "authors": "C L A Clarke; N Craswell; I Soboroff; G V Cormack"}, {"ref_id": "b24", "title": "Overview of the TREC", "journal": "", "year": "2011", "authors": "C L A Clarke; N Craswell; I Soboroff; E M Voorhees"}, {"ref_id": "b25", "title": "Overview of the TREC 2012 Web track", "journal": "", "year": "2012", "authors": "C L A Clarke; N Craswell; E M Voorhees"}, {"ref_id": "b26", "title": "The TREC 2005 Terabyte track. TREC", "journal": "", "year": "2005", "authors": "C L A Clarke; F Scholer; I Soboroff"}, {"ref_id": "b27", "title": "The Cranfield tests on index language devices", "journal": "ASLIB Proceedings", "year": "1967", "authors": "C Cleverdon"}, {"ref_id": "b28", "title": "The significance of the Cranfield tests on index languages", "journal": "", "year": "1991", "authors": "C Cleverdon"}, {"ref_id": "b29", "title": "TREC 2013 Web track overview. TREC", "journal": "", "year": "2013", "authors": "K Collins-Thompson; P N Bennett; F Diaz; C Clarke; E M Voorhees"}, {"ref_id": "b30", "title": "", "journal": "", "year": "2014", "authors": "K Collins-Thompson; C Macdonald; P N Bennett; F Diaz; E "}, {"ref_id": "b31", "title": "Patapasco: A Python framework for cross-language information retrieval experiments", "journal": "", "year": "2022", "authors": "C Costello; E Yang; D Lawrie; J Mayfield"}, {"ref_id": "b32", "title": "Overview of the TREC-2002 Web track", "journal": "", "year": "2002", "authors": "N Craswell; D Hawking"}, {"ref_id": "b33", "title": "Overview of the TREC 2004 Web track", "journal": "TREC", "year": "2004", "authors": "N Craswell; D Hawking"}, {"ref_id": "b34", "title": "Overview of the TREC", "journal": "", "year": "2003", "authors": "N Craswell; D Hawking; R Wilkinson; M Wu"}, {"ref_id": "b35", "title": "Overview of the TREC 2020 Deep Learning track", "journal": "", "year": "", "authors": "N Craswell; B Mitra; E Yilmaz; D Campos"}, {"ref_id": "b36", "title": "Overview of the TREC 2019 Deep Learning track", "journal": "", "year": "2019", "authors": "N Craswell; B Mitra; E Yilmaz; D Campos; E M Voorhees"}, {"ref_id": "b37", "title": "Increasing reproducibility in IR: Findings from the Dagstuhl seminar on \"Reproducibility of Data-Oriented Experiments in E-Science", "journal": "SIGIR Forum", "year": "2016", "authors": "N Ferro; N Fuhr; K J\u00e4rvelin; N Kando; M Lippold; J Zobel"}, {"ref_id": "b38", "title": "Overview of CEN-TRE@CLEF 2019: Sequel in the systematic reproducibility realm", "journal": "", "year": "2019", "authors": "N Ferro; N Fuhr; M Maistro; T Sakai; I Soboroff"}, {"ref_id": "b39", "title": "Overview of CENTRE@CLEF 2018: A first tale in the systematic reproducibility realm", "journal": "", "year": "2018", "authors": "N Ferro; M Maistro; T Sakai; I Soboroff"}, {"ref_id": "b40", "title": "SPLADE v2: Sparse lexical and expansion model for information retrieval", "journal": "", "year": "2021", "authors": "T Formal; C Lassance; B Piwowarski; S Clinchant"}, {"ref_id": "b41", "title": "SemEval-2023 task 5: Clickbait spoiling. SemEval-2023", "journal": "", "year": "", "authors": "M Fr\u00f6be; T Gollub; M Hagen; M Potthast"}, {"ref_id": "b42", "title": "Continuous integration for reproducible shared tasks with TIRA.io. ECIR 2023", "journal": "", "year": "", "authors": "M Fr\u00f6be; M Wiegmann; N Kolyada; B Grahm; T Elstner; F Loebe; M Hagen; B Stein; M Potthast"}, {"ref_id": "b43", "title": "Some common mistakes in IR evaluation, and how they can be avoided", "journal": "SIGIR Forum", "year": "2017", "authors": "N Fuhr"}, {"ref_id": "b44", "title": "Proof by experimentation? Towards better IR research", "journal": "SIGIR Forum", "year": "2020", "authors": "N Fuhr"}, {"ref_id": "b45", "title": "Proof by experimentation? Towards better IR research", "journal": "", "year": "", "authors": "N Fuhr"}, {"ref_id": "b46", "title": "Precise zero-shot dense retrieval without relevance labels", "journal": "", "year": "2022", "authors": "L Gao; X Ma; J Lin; J Callan"}, {"ref_id": "b47", "title": "TIRA: Configuring, executing, and disseminating information retrieval experiments. TIR 2012 at DEXA", "journal": "", "year": "", "authors": "T Gollub; B Stein; S Burrows; D Hoppe"}, {"ref_id": "b48", "title": "Overview of the author obfuscation task at PAN 2017: Safety evaluation revisited", "journal": "", "year": "2017", "authors": "M Hagen; M Potthast; B Stein"}, {"ref_id": "b49", "title": "ANTIQUE: A non-factoid question answering benchmark", "journal": "", "year": "", "authors": "H Hashemi; M Aliannejadi; H Zamani; W Bruce Croft"}, {"ref_id": "b50", "title": "Genomics track overview. TREC", "journal": "", "year": "2004", "authors": "W R Hersh; R Bhupatiraju; L Ross; A M Cohen; D Kraemer; P Johnson;  Trec"}, {"ref_id": "b51", "title": "TREC 2005 Genomics track overview. TREC", "journal": "", "year": "2005", "authors": "W R Hersh; A M Cohen; J Yang; R Bhupatiraju; P M Roberts; M A Hearst"}, {"ref_id": "b52", "title": "Efficiently teaching an effective dense retriever with balanced topic aware sampling", "journal": "", "year": "", "authors": "S Hofst\u00e4tter; S Lin; J Yang; J Lin; A Hanbury"}, {"ref_id": "b53", "title": "Benchmarking news recommendations: The CLEF NewsREEL use case", "journal": "SIGIR Forum", "year": "2015", "authors": "F Hopfgartner; T Brodt; J Seiler; B Kille; A Lommatzsch; M A Larson; R Turrin; A Ser\u00e9ny"}, {"ref_id": "b54", "title": "Evaluation-as-a-service for the computational sciences: Overview and outlook", "journal": "Journal of Data and Information Quality", "year": "2018", "authors": "F Hopfgartner; A Hanbury; H M\u00fcller; I Eggel; K Balog; T Brodt; G V Cormack; J Lin; J Kalpathy-Cramer; N Kando; M P Kato; A Krithara; T Gollub; M Potthast; E Viegas; S Mercer"}, {"ref_id": "b55", "title": "OpenSearch: Lessons learned from an online evaluation campaign", "journal": "Journal of Data and Information Quality", "year": "2018", "authors": "R Jagerman; K Balog; M De Rijke"}, {"ref_id": "b56", "title": "DiffIR: Exploring differences in ranking models' behavior. SIGIR 2021", "journal": "", "year": "", "authors": "K M Jose; T Nguyen; S Macavaney; J Dalton; A Yates"}, {"ref_id": "b57", "title": "Dense passage retrieval for open-domain question answering", "journal": "", "year": "", "authors": "V Karpukhin; B Oguz; S Min; P S H Lewis; L Wu; S Edunov; D Chen; W Yih"}, {"ref_id": "b58", "title": "ColBERT: Efficient and effective passage search via contextualized late interaction over BERT", "journal": "", "year": "", "authors": "O Khattab; M Zaharia"}, {"ref_id": "b59", "title": "SemEval-2023 task 4: ValueEval: Identification of human values behind arguments. SemEval-2023", "journal": "", "year": "", "authors": "J Kiesel; M Alshomary; N Mirzakhmedova; M Heinrich; N Handke; H Wachsmuth; B Stein"}, {"ref_id": "b60", "title": "The role of metadata in reproducible computational research", "journal": "Patterns", "year": "2021", "authors": "J Leipzig; D N\u00fcst; C Hoyt; K Ram; J Greenberg"}, {"ref_id": "b61", "title": "The neural hype and comparisons against weak baselines", "journal": "SIGIR Forum", "year": "2018", "authors": "J Lin"}, {"ref_id": "b62", "title": "Fostering coopetition while plugging leaks: The design and implementation of the MS MARCO leaderboards", "journal": "", "year": "", "authors": "J Lin; D Campos; N Craswell; B Mitra; E Yilmaz"}, {"ref_id": "b63", "title": "Pyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations", "journal": "", "year": "", "authors": "J Lin; X Ma; S Lin; J Yang; R Pradeep; R Nogueira"}, {"ref_id": "b64", "title": "Reproducibility is a process, not an achievement: The replicability of IR reproducibility experiments", "journal": "", "year": "", "authors": "J Lin; Q Zhang"}, {"ref_id": "b65", "title": "Streamlining evaluation with irmeasures", "journal": "", "year": "", "authors": "S Macavaney; C Macdonald; I Ounis"}, {"ref_id": "b66", "title": "OpenNIR: A complete neural ad-hoc ranking pipeline", "journal": "", "year": "", "authors": "S Macavaney; A Yates; S Feldman; D Downey; A Cohan; N Goharian"}, {"ref_id": "b67", "title": "Simplified data wrangling with ir_datasets", "journal": "", "year": "", "authors": "S Macavaney; A Yates; S Feldman; D Downey; A Cohan; N Goharian"}, {"ref_id": "b68", "title": "PyTerrier: Declarative experimentation in Python from BM25 to dense retrieval", "journal": "", "year": "", "authors": "C Macdonald; N Tonellotto; S Macavaney; I Ounis"}, {"ref_id": "b69", "title": "Declarative experimentation in information retrieval using PyTerrier", "journal": "", "year": "", "authors": "C Macdonald; N Tonellotto"}, {"ref_id": "b70", "title": "PISA: Performant indexes and search for academia", "journal": "", "year": "", "authors": "A Mallia; M Siedlaczek; J M Mackenzie; T Suel"}, {"ref_id": "b71", "title": "Batch evaluation metrics in information retrieval: Measures, scales, and meaning", "journal": "IEEE Access", "year": "2022", "authors": "A Moffat"}, {"ref_id": "b72", "title": "MS MARCO: A human cenerated machine reading comprehension dataset", "journal": "", "year": "2016", "authors": "T Nguyen; M Rosenberg; X Song; J Gao; S Tiwary; R Majumder; L Deng"}, {"ref_id": "b73", "title": "Multi-stage document ranking with BERT", "journal": "", "year": "2019", "authors": "R Nogueira; W Yang; K Cho; J Lin"}, {"ref_id": "b74", "title": "Terrier information retrieval platform", "journal": "", "year": "2005", "authors": "I Ounis; G Amati; V Plachouras; B He; C Macdonald; D Johnson"}, {"ref_id": "b75", "title": "Experimaestro and Datamaestro: Experiment and dataset managers (for IR)", "journal": "", "year": "", "authors": "B Piwowarski"}, {"ref_id": "b76", "title": "TIRA integrated research architecture", "journal": "Information Retrieval Evaluation in a Changing World", "year": "2019", "authors": "M Potthast; T Gollub; M Wiegmann; B Stein"}, {"ref_id": "b77", "title": "The information retrieval anthology. SIGIR 2021", "journal": "", "year": "", "authors": "M Potthast; S G\u00fcnther; J Bevendorff; J P Bittner; A Bondarenko; M Fr\u00f6be; C Kahmann; A Niekler; M V\u00f6lske; B Stein; M Hagen"}, {"ref_id": "b78", "title": "The Expando-Mono-Duo design pattern for text ranking with pretrained sequence-to-sequence models", "journal": "", "year": "2021", "authors": "R Pradeep; R Nogueira; J Lin"}, {"ref_id": "b79", "title": "Sentence-BERT: Sentence embeddings using Siamese BERT-networks. EMNLP-IJCNLP 2019", "journal": "", "year": "", "authors": "N Reimers; I Gurevych"}, {"ref_id": "b80", "title": "Overview of the TREC 2018 Precision Medicine track", "journal": "", "year": "2018", "authors": "K Roberts; D Demner-Fushman; E M Voorhees; W R Hersh; S Bedrick; A J Lazar"}, {"ref_id": "b81", "title": "Overview of the TREC 2017 Precision Medicine track", "journal": "", "year": "2017", "authors": "K Roberts; D Demner-Fushman; E M Voorhees; W R Hersh; S Bedrick; A J Lazar; S Pant"}, {"ref_id": "b82", "title": "On Fuhr's guideline for IR evaluation", "journal": "SIGIR Forum", "year": "2020", "authors": "T Sakai"}, {"ref_id": "b83", "title": "Overview of the NTCIR-14 CENTRE task", "journal": "", "year": "2019", "authors": "T Sakai; N Ferro; I Soboroff; Z Zeng; P Xiao; M Maistro"}, {"ref_id": "b84", "title": "Overview of the NTCIR-15 We Want Web with CENTRE (WWW-3) task", "journal": "", "year": "", "authors": "T Sakai; S Tao; Z Zeng; Y Zheng; J Mao; Z Chu; Y Liu; M Maistro; Z Dou; N Ferro"}, {"ref_id": "b85", "title": "BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models", "journal": "NeurIPS Datasets and Benchmarks", "year": "2021", "authors": "N Thakur; N Reimers; A R\u00fcckl\u00e9; A Srivastava; I Gurevych"}, {"ref_id": "b86", "title": "An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition", "journal": "BMC Bioinformatics", "year": "2015", "authors": "G Tsatsaronis; G Balikas; P Malakasiotis; I Partalas; M Zschunke; M R Alvers; D Weissenborn; A Krithara; S Petridis; D Polychronopoulos; Y Almirantis; J Pavlopoulos; N Baskiotis; P Gallinari; T Arti\u00e8res; A Ngomo; N Heino; \u00c9 Gaussier; L Barrio-Alvers; M Schroeder; I Androutsopoulos; G Paliouras"}, {"ref_id": "b87", "title": "OpenML: Networked science in machine learning", "journal": "SIGKDD Explor", "year": "2013", "authors": "J Vanschoren; J N Van Rijn; B Bischl; L Torgo"}, {"ref_id": "b88", "title": "Overview of the TREC 2004 Robust Retrieval track", "journal": "TREC", "year": "2004", "authors": "E Voorhees"}, {"ref_id": "b89", "title": "NIST TREC Disks 4 and 5: Retrieval test collections document set", "journal": "", "year": "1996", "authors": "E M Voorhees"}, {"ref_id": "b90", "title": "The philosophy of information retrieval evaluation", "journal": "", "year": "2001", "authors": "E M Voorhees"}, {"ref_id": "b91", "title": "The evolution of Cranfield", "journal": "Information Retrieval Evaluation in a Changing World", "year": "2019", "authors": "E M Voorhees"}, {"ref_id": "b92", "title": "TREC-COVID: Constructing a pandemic information retrieval test collection", "journal": "SIGIR Forum", "year": "2020", "authors": "E M Voorhees; T Alam; S Bedrick; D Demner-Fushman; W R Hersh; K Lo; K Roberts; I Soboroff; L. Lu Wang"}, {"ref_id": "b93", "title": "Too many relevants: Whither Cranfield test collections?", "journal": "", "year": "", "authors": "E M Voorhees; N Craswell; J Lin"}, {"ref_id": "b94", "title": "Overview of the seventh text retrieval conference (TREC-7)", "journal": "TREC", "year": "1998", "authors": "E M Voorhees; D Harman"}, {"ref_id": "b95", "title": "Overview of the eighth text retrieval conference (TREC-8)", "journal": "TREC", "year": "1999", "authors": "E M Voorhees; D Harman"}, {"ref_id": "b96", "title": "Can old TREC collections reliably evaluate modern neural retrieval models?", "journal": "", "year": "2022", "authors": "E M Voorhees; I Soboroff; J Lin"}, {"ref_id": "b97", "title": "CORD-19: The Covid-19 open research dataset", "journal": "", "year": "2020", "authors": "L Lu Wang; K Lo; Y Chandrasekhar; R Reas; J Yang; D Eide; K Funk; R Kinney; Z Liu; W Merrill; P Mooney; D A Murdick; D Rishi; J Sheehan; Z Shen; B Stilson; A D Wade; K Wang; C Wilhelm; B Xie; D Raymond; D S Weld; O Etzioni; S Kohlmeier"}, {"ref_id": "b98", "title": "Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval", "journal": "", "year": "", "authors": "L Xiong; C Xiong; Y Li; K Tang; J Liu; P N Bennett; J Ahmed; A "}, {"ref_id": "b99", "title": "EvalAI: Towards better evaluation systems for AI agents", "journal": "", "year": "2019", "authors": "D Yadav; R Jain; H Agrawal; P Chattopadhyay; T Singh; A Jain; S Singh; S Lee; D Batra"}, {"ref_id": "b100", "title": "Anserini: Enabling the use of Lucene for information retrieval research", "journal": "", "year": "2017", "authors": "P Yang; H Fang; J Lin"}, {"ref_id": "b101", "title": "Capreolus: A toolkit for end-to-end neural ad hoc retrieval", "journal": "", "year": "", "authors": "A Yates; S Arora; X Zhang; W Yang; K Martin Jose; J Lin"}, {"ref_id": "b102", "title": "Making a MIRACL: Multilingual information retrieval across a continuum of languages", "journal": "", "year": "2022", "authors": "X Zhang; N Thakur; O Ogundepo; E Kamalloo; D Alfonso-Hermelo; X Li; Q Liu; M Rezagholizadeh; J Lin"}, {"ref_id": "b103", "title": "When measurement misleads: The limits of batch assessment of retrieval systems", "journal": "SIGIR Forum", "year": "2022", "authors": "J Zobel"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Overview of typical shared task-like IR experiments and how the tools in TIREx support them.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure2: Data flow of two retrieval pipelines in TIRA. The upper retrieval pipeline creates an index so that the second stage retrieves from the index with BM25. The bottom retrieval pipeline uses a BM25 ranking and a manually uploaded file with query features as input for an LTR algorithm.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Defining a BM25 retrieval component in TIRA that depends on a previously created PyTerrier index.features (e.g., user behavior data like clicks and dwell-times obtained from a user study). When a component has inputs from more than one component, TIRA makes them available in the order in which the components have been defined. Preceding components or uploads must exist when defining a new component and TIRA decouples the command to be executed from the Docker image so that the same image can be used to run different retrieval approaches (e.g., by switching parameters). In combination with caching, this improves the efficiency for a wide range of common multi-stage retrieval pipelines.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "0.43 -0.21 0.13 0.06 0.30 0.63", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Overview of what data TIRA makes available to full-rank and re-rank approaches. The 'Access' columns indicate the default accessibility to participants (P), organizers (O; can make data accessible as indicated by \u2020), and unregistered users (U).", "figure_data": "TypeResourceFieldsAccessExample EntryPOUFull-Rankdocuments.jsonl.gzdocno, text, original_document\u2713\u2713\u2717  \u2020{\"docno\": \"8182161\", \"text\": \"Goldfish can grow up to 18 inches . . . \", \"original_document\": {. . . }}topics.jsonl.gzqid, query, original_topic\u2713\u2713\u2717  \u2020{\"qid\": \"156493\", \"query\": \"do goldfish grow\", \"original_query\": {. . . }}qid, query, original_topic, docno,{\"qid\": \"156493\", \"query\": \"do goldfish grow\", \"original_query\": {. . . },Re-Rankre-rank.jsonl.gztext, original_document\u2713\u2713\u2717  \u2020\"docno\": 8182161, \"text\": \"Goldfish can grow up to 18 inches . . . \",score, rank\"original_document\": {. . . }, \"rank\": 1, \"score\": 31.16}Bothqrels.txttopic, iteration, docno, relevance\u2717  \u2020\u2713\u2717  \u2020156493 Q0 8182161 2"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Overview of variables available for software in TIRA.", "figure_data": "VariableAvailabilityDescription$inputDatasetAlwaysDirectory containing the input data.$outputDirAlwaysDirectory with expected output data.$inputRunMulti-Comp.Output(s) of previous stage(s)."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "The 15 corpora and the associated 32 retrieval tasks currently available in TIREx (all are open for submissions).", "figure_data": "CorpusAssociated Retrieval TasksNameDocs.Size Details#Args.me0.4 m8.3 GB Touch\u00e9 2020-2021 [8, 11]2Antique0.4 m 90.0 MB QA Benchmark [50]1ClueWeb091.0 b4.0 TB Web tracks 2009-2012 [23-26]4ClueWeb12731.7 m4.5 TB Web tracks [30, 31], Touch\u00e9 [10, 11] 4ClueWeb22B 200.0 m6.8 TB Touch\u00e9 2023 [9] (ongoing)1CORD-190.2 m7.1 GB TREC-COVID [93, 98]1Cranfield1,400 0.5 MB Fully Judged Corpus [28, 29]1Disks4+50.5 m 602.5 GB TREC-7/8 [95, 96], Robust04 [89, 90] 3GOV1.2 m4.6 GB Web tracks 2002-2004 [33-35]3GOV225.2 m 87.1 GB TREC TB 2004-2006 [19, 22, 27]3MEDLINE3.7 m5.1 GB TREC Genomics [51, 52], PM [81, 82] 4MS MARCO8.8 m2.9 GB Deep Learning 2019-2020 [36, 37]2NFCorpus3,633 30.0 MB Medical LTR Benchmark [13]1Vaswani11,429 2.1 MB Scientific Abstracts1WaPo0.6 m1.6 GB TREC Core 20181= 15 corpora 1.9 b 15.3 TB32"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Overview of the retrieval frameworks and the 50 retrieval approaches imported into TIREx. Our import of Pyserini was not ready during the experiments but is now available.", "figure_data": "FrameworkTypeDescriptionApproachesFull-rank Re-rankBEIR [86]Bi-encoderDense retrieval1717ChatNoir [7]BM25FElasticsearch cluster10ColBERT@PT [59] Late interaction PyTerrier plugin01DuoT5@PT [79] Cross-encoder Pairwise transformer 03PyGaggle [64]Cross-encoder Pointwise transformer 08PyTerrier [69]LexicalTraditional baselines 2020Pyserini  *  [64]LexicalTraditional baselines44"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Effectiveness scores (nDCG@10) on 14 corpora (31 tasks; ClueWeb22B excluded as no judgments yet) for selected approaches and the best, median, and worst of each group (scores macro-averaged for corpora with multiple associated tasks).", "figure_data": "CorpusChatNoirLexicalLate Int.Bi-EncoderduoT5PyGaggleBM25 Best Median Worst ColBERT TAS-B Best Median Worst Base Large 3b MonoT5 Best Median WorstAntique-0.51 0.530.510.360.470.40 0.490.440.30 0.54 0.46 0.520.510.540.510.45Args.me-0.43 0.570.430.140.260.17 0.330.240.13 0.33 0.29 0.290.300.390.340.27CORD-19-0.28 0.640.550.210.580.50 0.700.600.50 0.66 0.61 0.660.690.690.630.55ClueWeb090.160.18 0.240.180.120.170.16 0.200.170.13 0.15 0.15 0.180.170.190.170.12ClueWeb120.360.24 0.270.250.140.230.25 0.280.260.23 0.33 0.30 0.350.260.280.260.23Cranfield-0.01 0.010.010.010.010.01 0.010.010.00 0.01 0.01 0.010.010.010.010.01Disks4+5-0.44 0.460.440.370.460.39 0.490.430.37 0.45 0.38 0.440.530.570.530.43GOV-0.22 0.240.220.150.230.22 0.270.240.21 0.19 0.15 0.220.260.290.260.22GOV2-0.47 0.490.440.250.450.34 0.460.420.34 0.47 0.43 0.480.480.510.480.41MS MARCO-0.49 0.500.480.370.690.64 0.710.660.64 0.64 0.57 0.630.710.740.710.63MEDLINE-0.34 0.420.270.180.250.14 0.260.210.14 0.34 0.32 0.360.250.350.270.24NFCorpus-0.27 0.280.270.260.270.25 0.290.260.24 0.28 0.24 0.290.300.310.300.28Vaswani-0.45 0.460.450.300.430.34 0.440.380.22 0.41 0.34 0.460.310.480.410.08WaPo-0.38 0.390.370.240.430.34 0.430.370.33 0.40 0.28 0.400.450.490.450.40Avg.-0.34 0.390.350.220.350.30 0.380.330.27 0.37 0.32 0.380.370.420.380.31"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Reproducibility of TREC DL 2019 system preferences on other tasks. Success rate in percent (effect ratio > 0; tasks ordered by success rate) and the 25%, 50%, and 75% quantiles for the effect ratio and delta relative improvement.", "figure_data": "TaskRank Succ.Effect Ratio Delta Rel. Impr.25% 50% 75% 25% 50% 75%TREC DL 2020188.10.68 0."}], "formulas": [{"formula_id": "formula_0", "formula_text": "$", "formula_coordinates": [6.0, 321.22, 258.81, 3.29, 6.98]}], "doi": "10.1145/nnnnnnn.nnnnnnn"}