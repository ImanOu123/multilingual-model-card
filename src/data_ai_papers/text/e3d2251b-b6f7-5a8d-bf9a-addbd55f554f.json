{"title": "Gradient Descent: The Ultimate Optimizer", "authors": "Kartik Chandra; Mit Csail; Audrey Xie; Jonathan Ragan-Kelley; Erik Meijer", "pub_date": "2022-10-14", "abstract": "Working with any gradient-based machine learning algorithm involves the tedious task of tuning the optimizer's hyperparameters, such as its step size. Recent work has shown how the step size can itself be optimized alongside the model parameters by manually deriving expressions for \"hypergradients\" ahead of time. We show how to automatically compute hypergradients with a simple and elegant modification to backpropagation. This allows us to easily apply the method to other optimizers and hyperparameters (e.g. momentum coefficients). We can even recursively apply the method to its own hyper-hyperparameters, and so on ad infinitum. As these towers of optimizers grow taller, they become less sensitive to the initial choice of hyperparameters. We present experiments validating this for MLPs, CNNs, and RNNs. Finally, we provide a simple PyTorch implementation of this algorithm (see people.csail.mit.edu/kach/gradient-descent-the-ultimate-optimizer).", "sections": [{"heading": "Introduction", "text": "When we train deep neural networks by gradient descent, we have to select a step size \u03b1 for our optimizer. If \u03b1 is too small, the optimizer runs very slowly, whereas if \u03b1 is too large, the optimizer fails to converge. Choosing an appropriate \u03b1 is thus itself an optimization task that machine learning practitioners face every day. Why not apply gradient descent here, too? To do so, we need to compute the derivative of the loss function not only with respect to the neural network's weights, but also with respect to \u03b1. Baydin et al. (2018), applying an insight from Almeida et al. (1999), describe how to efficiently compute such \"hypergradients\" by manually differentiating standard optimizer update rules with respect to the step size hyperparameter. This allows for on-line learning rate adaptation, which generally improves convergence, especially when the initial \u03b1 is sub-optimal. However, the above method has three limitations: (1) manually differentiating optimizer update rules is tedious and error-prone, and must be re-done for each optimizer variant; (2) the method only tunes the step size hyperparameter, not other hyperparameters such as the momentum coefficient; and\n(3) the method introduces a new hyperparameter, the hyper-step-size, which must also be tuned.\nIn this paper, we address all three limitations by replacing manual differentiation with automatic differentiation (AD), which (1) automatically computes correct derivatives without any additional human effort, and (2) naturally generalizes to other hyperparameters (e.g. momentum coefficient) for free. As for (3), we observe that AD can be applied to optimize not only the hyperparameters, but also the hyper-hyperparameters, and the hyper-hyper-hyperparameters, and so on. In fact, we can implement arbitrarily tall towers of recursive optimizers, which are increasingly robust to the choice of initial hyperparameter. These \"hyperoptimizers\" therefore reduce the burden on humans responsible for tuning the hyperparameters. (Such an effect was hypothesized by Baydin et al., but not tested because manual differentiation of complex sequences of nested optimizers is impractical.)\nAlthough \"just apply AD\" is a seemingly straightforward recipe, an efficient implementation that properly allows for recursive self-application requires some care. To close the loop, we take inspiration from the study of recursion and combinators in programming language theory (and the long tradition of programming language papers named \"Lambda: The Ultimate X\"). We spell out the details in Section 2, and evaluate our method in Section 3. We find that across a variety of architectures (MLPs, CNNs, and RNNs) our hyperoptimizers are robust to suboptimal choices of initial hyperparameters, and that this robustness increases as we grow the stacks of optimizers taller.", "publication_ref": ["b1", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Implementing hyperoptimizers", "text": "Consider some loss function f that we want to minimize using gradient descent, and let w i be the weights at the beginning of step i (we will omit subscripts on f , even though it varies at each step due to the stochasticity of minibatches). First, recall the standard weight update rule at step i for SGD, using some fixed step size \u03b1:\nw i+1 = w i \u2212 \u03b1 \u2202f (w i ) \u2202w i\nWe would like to also update \u03b1 at each step, so we will index it as well with the step number; that is, let \u03b1 i be the step size at the beginning of step i. At each step, we will first update \u03b1 i to \u03b1 i+1 using some update rule yet to be derived, and then use the updated step size \u03b1 i+1 to update the weights from w i to w i+1 .\n\u03b1 i+1 = \u03b1 i \u2212 adjustment for \u03b1 i w i+1 = w i \u2212 \u03b1 i+1 \u2202f (w i ) \u2202w i\nWhat should the adjustment for \u03b1 i be? By analogy to w, we want to adjust \u03b1 i in the direction of the gradient of the loss function with respect to \u03b1 i , scaled by some hyper-step size \u03ba. In other words, the adjustment should be \u03ba(\u2202f (w i )/\u2202\u03b1 i ). Our modified update rule is therefore:\n\u03b1 i+1 = \u03b1 i \u2212 \u03ba \u2202f (w i ) \u2202\u03b1 i (1) w i+1 = w i \u2212 \u03b1 i+1 \u2202f (w i ) \u2202w i (2)\nAll that remains is to compute \u2202f (w i )/\u2202\u03b1 i in equation ( 1). Below, we first review how Baydin et al. (2018) take this derivative by hand. Then, we show how to obtain the same result automatically and efficiently using AD. Finally, we discuss how this automation allows us to generalize the method.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Computing the step-size update rule by hand", "text": "One option to compute \u2202f (w i )/\u2202\u03b1 i , explored by Baydin et al. (2018), is to proceed by direct manual computation of the partial derivative. Applying the chain rule to (1), we have\n\u2202f (w i ) \u2202\u03b1 i = \u2202f (w i ) \u2202w i \u2022 \u2202w i \u2202\u03b1 i = \u2202f (w i ) \u2202w i \u2022 \u2202 w i\u22121 \u2212 \u03b1 i \u2202f (wi\u22121) \u2202wi\u22121 \u2202\u03b1 i (3) = \u2202f (w i ) \u2202w i \u2022 \u2212 \u2202f (w i\u22121 ) \u2202w i\u22121 (4)\nwhere ( 3) is obtained by substituting the update rule in (2) for w i and ( 4) is obtained by observing that w i\u22121 and f (w i\u22121 ) do not depend on \u03b1 i . As Baydin et al. note, this expression lends itself to a simple and efficient implementation: simply remember the past two gradients from backpropagation, and take their dot product to obtain the hypergradient with respect to the step size.\nWe were able to take this derivative by hand because the update rule for SGD is simply a multiplication by a constant, whose derivative is trivial. What about other optimizers? Consider the Adam optimizer (Kingma and Ba, 2014), which has a much more sophisticated update rule involving the four hyperparameters \u03b1, \u03b2 1 , \u03b2 2 , (though is typically not tuned). Differentiating the update rule by hand, we obtain significantly more complex expressions for the hypergradients:\n\u2202w i \u2202\u03b1 i = \u2212m i i + \u221av i \u2202w i \u2202\u03b2 1i = \u2212 \u03b1 i \u2212 \u2202f (wi\u22121) \u2202wi\u22121 + m i\u22121 + i\u03b2 1 (i\u22121) im i 1 \u2212 \u03b2 1 i i i + \u221av i \u2202w i \u2202 i = \u03b1 imi i + \u221av i 2 \u2202w i \u2202\u03b2 2i = \u03b1 imi \u221av i \u2212 \u2202f (wi\u22121) \u2202wi\u22121 2 + v i\u22121 + i\u03b2 2 (i\u22121) iv i 2v i i + \u221av i 2\nThis manual approach to derive hypergradients simply does not scale: it is tedious and error-prone, and must be repeated for every optimizer variant. However, with a little bit of care, we can compute hypergradients automatically and efficiently alongside the regular gradients.", "publication_ref": ["b1", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Computing the step-size update rule automatically", "text": "In order to compute hypergradients automatically, let us first briefly review the mechanics of reversemode AD. Differentiable programming systems that provide reverse-mode AD typically build up a computation graph as the function is computed forwardly. For example, when a user computes the function f (w i ), the system internally stores a DAG whose leaves are the weights w i , whose internal nodes are intermediate computations, and whose root is the final loss. It can then backpropagate through the computation graph starting at this root node, depositing gradients in each internal node as it descends, until the weights w i at the leaf nodes have accumulated their gradients \u2202f (w i )/\u2202w i .\nOnce the gradient \u2202f (w i )/\u2202w i is computed by the backwards pass, we update the weights w i+1 = w i \u2212 \u03b1 \u2022 \u2202f (w i )/\u2202w i , and repeat the cycle for the next step of gradient descent.\nAn important consideration at this point is for the weights to be \"detached\" from the computation graph before the next iteration of this algorithm -that is, for the weights to be forcibly converted to leaves of the graph by removing any inbound edges. The effect of the \"detach\" operation is depicted in Figure 1a. If this step were skipped, backpropagation at the next iteration would continue beyond the current weights and into the previous iteration's computation graph. Over time the computation graph would grow taller linearly in the number of steps taken; because backpropagation is linear in the size of the graph, the overall training would become quadratic-time and intractable.\nLet us take PyTorch as an example. In the built-in SGD optimizer (Paszke et al., 2017, optim/sgd.py, commit ff94c9d), this is implemented by wrapping the update in the @torch.no _ grad() context manager. Here, we need finer grained control over gradient flow, so will make the .detach() operations explicit. Below is pseudocode for an SGD optimizer that uses .detach() as we have discussed. The highlighted calls to .detach() correspond to detaching the weights and their gradients. Now, in order to have backpropagation deposit the gradient with respect to \u03b1 i as well as w i , we can simply refrain from detaching \u03b1 i from the graph, detaching instead its parents. This is depicted in Figure 1b. Because we want to compute \u2202f (w i )/\u2202\u03b1 i , the edge from \u03b1 i to w i needs to remain intact. To implement this, instead of calling .detach() on alpha directly, we detach its parents when applying equation ( 1). This yields the following fully-automated hyperoptimization algorithm:  Since we only extend the computation graph by a little extra amount, corresponding to evaluating the optimizer, the hyperoptimizer's computational overhead is negligible (see Figure 4f).", "publication_ref": ["b17"], "figure_ref": ["fig_2", "fig_2", "fig_4"], "table_ref": []}, {"heading": "Extending to other optimizers", "text": "As suggested by Maclaurin et al. (2015), it should be possible to apply gradient-based methods to tune hyperparameters of common variations on SGD such as AdaGrad (Duchi et al., 2011), AdaDelta (Zeiler, 2012), or Adam (Kingma and Ba, 2014). The above implementation of HyperSGD generalizes quite easily to these optimizers -we simply replace the last line with the new update rule.\nUnlike previous work, our method also allows for simultaneously optimizing all hyperparameters of these optimizers (e.g. all of \u03b1, \u03b2 1 , and \u03b2 2 for Adam) \"for free.\" We simply treat them just like alpha in the implementation. Our evaluation in Section 3.2 demonstrates that this indeed advantageous to do. There are, however, two important subtleties: First, because hyperparameters like \u03b2 1 and \u03b2 2 must be strictly in the domain (0, 1), we clamp the \"raw\" values to this domain using a scaled sigmoid. Without this step, we might accidentally adjust these values outside their domains. Second, the Adam optimizer in particular involves the term \u221av i , which is continuous but not differentiable atv i = 0. Because Adam normally initializesv 0 = 0, backpropagation fails on the first step due to division by zero. We fix this problem by initializingv 0 to rather than 0.", "publication_ref": ["b15", "b4", "b24", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Stacking hyperoptimizers recursively", "text": "At this point it is natural to ask whether the hyperoptimizer can itself be optimized; that is, whether the hyper-hyperparameters can be adjusted by a hyper-hyperoptimizer. The possibility of doing so recursively ad infinitum to obtain an optimization algorithm that is highly robust to the humanchosen hyperparameter was hypothesized by Baydin et al. (2018). Computing the gradients of these higher-order hyperparameters by hand is impossible without knowing the exact sequence of stacked optimizers in advance, and, as discussed above, would be extremely tedious and error-prone.\nHowever, the ability to compute these gradients automatically by AD makes it possible to realize this vision. To do so, let us revisit our previous implementation of HyperSGD. Notice that there is an opportunity for recursion lurking here: the adjustment to alpha can be factored out with a call to SGD.step, where SGD's hyperparameter is kappa. Because SGD is already careful to properly detach its parameter (typically w, but in this case \u03b1), this implementation is functionally identical to the one above. Indeed, any optimizer that observes this protocol would suffice, so let us abstract out the optimizer as a parameter to HyperSGD: Finally, after this refactoring, we can recursively feed HyperSGD itself as the optimizer, obtaining a level-2 hyperoptimizer HyperSGD(0.01, HyperSGD(0.01, SGD(0.01))). Similarly, we can imagine taller towers, or towers that mix and match multiple different kinds of optimizers, such as Adamoptimized-by-SGD-optimized-by-Adam.\nA natural concern is whether this process actually exacerbates the hyperparameter optimization problem by introducing even more hyperparameters. Baydin et al. (2018) predicted that as the towers of hyperoptimizers grew taller, the resulting algorithms would become less sensitive to the human-chosen hyperparameters. This is indeed the case; Section 3.4 presents an empirical evaluation.", "publication_ref": ["b1", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section we evaluate the hyperoptimizers made possible by our system, exploring in particular the benefits of optimizing hyperparameters beyond just the step size, and of stacking hyperoptimizers to multiple levels. Each of these experiments was conducted on a single NVIDIA TITAN Xp GPU.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Hyperoptimization for SGD", "text": "First, we establish some basic properties about hyperoptimizers: (1) whether an SGD hyperoptimizer performs better than a regular SGD optimizer, and (2) whether the final learned step size is better than the initial human-chosen step size. We test the latter property by running a fresh regular SGD optimizer with the final learned step size of the hyperoptimizer. Following authors of prior work (Maclaurin et al., 2015;Baydin et al., 2018), we conducted initial experiments on the MNIST dataset (Lecun et al., 1998) using a neural network with one fully-connected hidden layer of size 128, tanh activations, and a batch size of 256. We trained all networks for 30 epochs, reporting statistics over 3 runs. As a baseline we used SGD with \u03b1 = 0.01.\nTable 1a summarizes the results of our experiments. We find that hyperoptimized SGD outperforms the baseline by a significant margin. This holds even if we use other optimizers (e.g. Adam) to adjust the step size of the SGD optimizer. Furthermore, when we re-ran the regular optimizers with the new learned hyperparameters, we found that they performed better than the initial hyperparameter.", "publication_ref": ["b15", "b1", "b13"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Hyperoptimization for Adam, AdaGrad and RMSProp", "text": "In Section 2.3, we described how to apply our system to the Adam optimizer, simultaneously optimizing not only the learning rate \u03b1, but also the momentum coefficients \u03b2 1,2 . Here, we ask three questions: (1) whether hyperoptimized Adam optimizers perform better than regular Adam optimizers, (2) whether the learned hyperparameters outperform the baseline, and (3) whether there is a benefit to optimizing all the hyperparameters, as opposed to only optimizing the learning rate as Baydin et al. (2018) do. Because Adam has significantly faster convergence than SGD, we only run these experiments for 5 epochs to avoid overfitting.\nTable 1b summarizes the results of our experiments. We find that indeed the hyperoptimized Adam optimizer outperforms the regular Adam optimizer on its \"default\" settings. As with SGD, the learned hyperparameters perform better than the initial hyperparameters when re-run with the regular optimizer. Inspecting the learned hyperparameters, we find that the algorithm raises the learning rate   Baydin et al. (2018); RMSProp \u03b1 is similar. If not specified, initial hyperparameters are PyTorch defaults (10 \u22122 for learning rates except 10 \u22123 for Adam; \u03b2 1 = 0.9, \u03b2 2 = 0.99 for Adam and \u03b3 = 0.99 for RMSProp). Each hyperoptimizer experiment is repeated using the final hyperparameters (typeset in pink) learned by the algorithm.\n\u03b1 and slightly lowers \u03b2 1 , but does not significantly affect \u03b2 2 . Nevertheless, learning \u03b2 1 does help slightly, though not when the top-level optimizer is itself another Adam optimizer.\nSimilarly, we can add any other optimizer to our system with just a few straightforward lines of code. Here, we show results for AdaGrad (Table 1c) and RMSProp (Table 1d; also run to 5 epochs). These experiments took less than an hour each to implement from scratch, and show that every hyperoptimizer stack outperforms the non-hyperoptimized baseline. We remark that AdaGrad is known to \"stall\" over time as the effective step size goes to zero; inspecting the learned \u03b1 over time, we find that the AdaGrad/AdaGrad hyperoptimizer increases \u03b1 to make up for this effect. Additionally, we tried to hyperoptimize RMSProp's new \u03b3 parameter, which modulates the accumulation of gradient RMS terms. This yielded even better results (compare \u03b1 to \u03b1,\u03b3 trials), and required only a 1-line change in our code.", "publication_ref": ["b1", "b1"], "figure_ref": [], "table_ref": ["tab_3", "tab_3", "tab_3"]}, {"heading": "Hyperoptimization at scale", "text": "Next, we evaluate our hyperoptimizers on two different real-world neural network architectures.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Convolutional neural networks for computer vision", "text": "We train a ResNet-20 (He et al., 2016) with and without hyperoptimization on the CIFAR-10 dataset (Krizhevsky, 2012). As a baseline, we replicate the training procedure of He et al. (2016): we (a) For a wide range of \"bad\" initial hyperparameter configurations, the hyperoptimizer improves on (or at least matches) final test accuracy, and often matches or even outperforms the \"good\" initial hyperparameters.\n(b) The hyperoptimizer matches performance of the hand-engineered learning rate decay schedule by He et al. (2016), learning a strikingly similar decay schedule (right plot). use the same network architecture, optimizer (SGD), step size (0.1), momentum (0.9), and weight decay (10 \u22124 ), though without their custom learning rate decay schedule (which we will address later). Experiments were run for 200 epochs, which takes around 3 hours on our hardware.\nFirst, we test how robust the hyperoptimizer is to \"bad\" initial choices of step size and momentum. We vary the initial step size and the momentum among \"small,\" \"good,\" and \"large\" values (that is, \u03b1 \u2208 {0.01, 0.1, 1.0} and \u00b5 \u2208 {0.09, 0.9, 0.99}), and add a hyperoptimizer (\u03b1\n\u03b1 = \u03b1 2 \u2022 10 \u22123 , \u03b1 \u00b5 = 1/(1 \u2212 \u00b5) \u2022 10 \u22126\n). The results of this experiment are shown in Figure 2a. In every configuration, the hyperoptimizer matches or outperforms the regular optimizer in final test accuracy. Furthermore, in nearly all of the configurations, the hyperoptimizer matches or exceeds the \"good\" hyperparameters' final test accuracy. Only when both hyperparameters are bad in the same direction (too small or too large) is it unable to manage this, and even then for the too-large case it dramatically lowers the loss compared to no hyperoptimizer. We conclude that hyperoptimizers are indeed beneficial for tuning both step size and momentum in this real-world setting.\nNext, we add in the learning rate decay schedule hand-engineered by He et al. (2016): the step size is divided by 10 at epochs 100 and 150. We compare this with a hyperoptimizer initialized with the same starting hyperparameters, training both variants for 500 epochs. Our results are shown in Figure 2b.\nThe hyperoptimizer not only matches the final test loss of the hand-engineered learning rate decay schedule, but also learns a decay schedule strikingly similar to one hand-engineered by He et al. Of course, both networks significantly outperform the baseline trained with a fixed step size.", "publication_ref": ["b8", "b12", "b8", "b8", "b8"], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Recurrent neural networks for language modeling", "text": "We train a character-level RNN (\"Char-RNN\") on the Tolstoy dataset, as proposed by Karpathy et al. (2015) as a convenient testbed for language models, which is now often used to benchmark optimizers (Schneider et al., 2018;Schmidt et al., 2021). We took the architecture (2-layer LSTM with 128 hidden nodes) and \"expert\" optimizer (Adam optimizer with \u03b1 = 2 \u00d7 10 \u22123 , run for 50,000 gradient Figure 3: Training RNNs with hyperoptimizers (Section 3.3.2). As the initial learning rate is lowered, the regular Adam optimizer's convergence slows, but the hyperoptimizer is able to accelerate it. The hyperoptimizer also slightly improves convergence when the initial learning rate is too high.\ndescent steps) directly from Johnson (2017) as recommended by Karpathy et al. We compare against our HyperAdam optimizer on a wide range of initial learning rates \u03b1 \u2208 {10 \u22124 , 2 \u00d7 10 \u22123 , 10 \u22122 }, with \u03b1 \u03b1 = \u03b1 \u2022 10 \u22122 . We do not vary initial \u03b2 1,2 because in our experience these hyperparameters are typically left at their default values. However, we do allow the hyperoptimizer to vary \u03b2 1,2 over the course of training (with \u03b1 \u03b21 = 10 \u22124 and \u03b1 \u03b22 = 2 \u00d7 10 \u22124 ). All runs took around 1 hour to train.\nThe results of this experiment are shown in Figure 3. We find that the hyperoptimizer performs comparably to the expert-chosen fixed step size (perplexity 5.41 \u00b1 0.26 with hyperoptimizer vs 5.27 \u00b1 0.31 without), and improves upon \"bad\" initial step sizes in both directions (5.45 \u00b1 0.76 vs 5.77 \u00b1 0.34 when too high; 6.51 \u00b1 0.88 vs 8.71 \u00b1 0.91 when too low).", "publication_ref": ["b10", "b21", "b20", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Higher-order hyperoptimization", "text": "In Section 2.4 we developed an interface for building arbitrarily tall towers of optimizers. Baydin et al. (2018) hypothesized that taller towers would yield hyperoptimizers that were increasingly robust to the initial human-chosen hyperparameters. To validate this behavior of higher-order hyperoptimizers, we ran each of our benchmarks from above (MLP on MNIST, CNN on CIFAR-10, RNN on Tolstoy) with towers of hyperoptimizers of increasing heights, and with bottom-level step sizes \u03b1 initialized across many orders of magnitude. In practice we find that if the initial hyper-step sizes are too large, the computation diverges for networks larger than the MNIST MLP. So, we initialize each level's hyperparameter to be smaller than that of the previous level. Specifically, we use the following scheme: from \u03b1 = 10 \u22128 to 10 \u22124 the higher layers' step sizes were initialized to [\u03b1 \u2022 10 2 , \u03b1 \u2022 10 0 , \u03b1 \u2022 10 \u22122 ] respectively, while for \u03b1 \u2265 10 \u22123 they were initialized to [\u03b1 \u2022 10 \u22123 , \u03b1 \u2022 10 \u22124 , 10 \u22128 ] respectively.\nFigure 4 shows our results. It is indeed the case across these different benchmarks (each of which has a different dataset, architecture, and optimizer type) that the taller the hyperoptimizer stack, the less sensitive the results become to the human-chosen hyperparameters. With a three-level optimizer stack, a single hyperoptimizer design obtains reasonable results in all of our benchmarks across several orders of magnitude of base-level step size.\nFurther tests of scalability To test if our hyperoptimizers continue to work in even larger regimes, we fine-tuned a ResNet-152 (pretrained on ImageNet) to the Caltech-256 dataset Griffin et al. (2007).\nFigure 4e shows the results: a height-1 hyperoptimizer recovers \u2248 11% error for both \u03b1 = 10 \u22126 and \u03b1 = 10 \u22124 (without a hyperoptimizer, \u03b1 = 10 \u22126 gives 91.5% error). A height-2 hyperoptimizer is additionally able to make significant progress when \u03b1 = 10 \u22122 . As we stack more layers of optimizers, the resulting hyperoptimizer is less sensitive to the initial choice of hyperparameters, but costs only 1-2% more in runtime.\nWe stress how lightweight and practical this method is. Figure 4f shows how runtime scales as a function of hyperoptimizer stack height for the above benchmarks. The scaling is linear: each additional level costs only 1-2% in additional runtime above the non-hyperoptimized baseline.", "publication_ref": ["b1", "b7"], "figure_ref": ["fig_4", "fig_4", "fig_4"], "table_ref": []}, {"heading": "Related work", "text": "Hyperparameter optimization has a long history, and we refer readers to a recent survey by Feurer and Hutter (2019) for the full story. Most existing work on gradient-based hyperparameter optimization (Bengio, 2000;Domke, 2012;Maclaurin et al., 2015;Pedregosa, 2016;Franceschi et al., 2017) has focused on computing hyperparameter gradients after several iterations of training, which is computationally expensive. Baydin et al. (2018), building on a technique first published by Almeida et al. (1999), propose instead updating hyperparameters at each step, and Rubio (2017) provides a convergence analysis. Luketina et al. (2016) apply a similar technique to regularization hyperparameters, though they note that their proposed method could work in principle for any continuous hyperparameter. As discussed above, we expand upon this line of work in three directions: (1) by fully automating this process, rather than requiring manual derivative computations; (2) by optimizing hyperparameters beyond just the learning rate; and (3) by realizing the vision of recursive higher-order hyperoptimizers and evaluating the resulting algorithms. We find that they are indeed more robust to the initial human-chosen hyperparameter, which relates our work to other learning algorithms that minimize sensitivity to learning rates (Orabona and Tommasi, 2017;Vaswani et al., 2019).", "publication_ref": ["b5", "b2", "b3", "b15", "b18", "b6", "b1", "b0", "b14", "b16", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Limitations and future work", "text": "As discussed in Section 3.4, one limitation of hyperoptimizers is that they cannot yet handle initial hyperparameters that are set far too high, because the system is unstable and diverges before the hyperoptimizer can have an effect. Designing hyperoptimizers robust in this regime requires further research, such as a deeper theoretical analysis of convergence. Our implementation also requires some care in avoiding certain bugs related to computation graph management. For example, loggers must detach what is logged to avoid memory leaks because tensors are not garbage collected unless all children are detached. Similarly, certain PyTorch modules (e.g. the built-in LSTM) cannot be used because they silently modify the computation graph, which may lead to incorrect gradients with our system. Further research is needed to design differentiable programming languages where methods like ours can be expressed in a modular and composable manner that minimizes the risk of such bugs.\nBroader impact Training a modern deep learning system consumes a tremendous amount of energy, and hyperparameter searches can multiply that energy impact by many orders of magnitude (Strubell et al., 2019). We hope that advances in on-line hyperparameter tuning can reduce this impact.", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We presented a technique that enables gradient descent optimizers like SGD and Adam to tune their own hyperparameters. Unlike prior work, our proposed hyperoptimizers require no manual differentiation, learn hyperparameters beyond just learning rates, and can be stacked recursively to many levels. We described an elegant recursive implementation of hyperoptimizers in a reverse-mode AD system and evaluated it on a variety of benchmarks, showing that as the stacks grow taller, they become less sensitive to the initial human-chosen hyperparameter.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments and Disclosure of Funding", "text": "We thank Samantha Andow, Emilio Arroyo-Fang, Irene Dea, Johann George, Melissa Grueter, Basil Hosmer, Steffi Stumpos, Alanna Tempest, and Shannon Yang for early discussions, Krishna Murthy Jatavallabhula and Josh Tenenbaum for their advice when preparing this paper, and the anonymous reviewers for their thoughtful feedback. KC and JRK were supported by NSF Grants #2105806, #CCF-1231216, #CCF-1723445 and #CCF-1846502, and ONR Grant #00010803 at MIT. Additionally, KC was supported by a Hertz Foundation Fellowship, the Paul and Daisy Soros Fellowship for New Americans, and an NSF Graduate Research Fellowship under Grant #2141064, and AX was supported by the MIT Undergraduate Research Opportunities Program (UROP).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Parameter adaptation in stochastic optimization", "journal": "", "year": "1999", "authors": "L E Almeida; T Langlois; J F M Amaral; A Plakhov"}, {"ref_id": "b1", "title": "Online learning rate adaptation with hypergradient descent", "journal": "", "year": "2018-05-03", "authors": "A G Baydin; R Cornish; D M Rubio; M Schmidt; F Wood"}, {"ref_id": "b2", "title": "Gradient-based optimization of hyperparameters", "journal": "Neural Computation", "year": "2000", "authors": "Y Bengio"}, {"ref_id": "b3", "title": "Generic methods for optimization-based modeling", "journal": "", "year": "2012-04", "authors": "J Domke"}, {"ref_id": "b4", "title": "Adaptive subgradient methods for online learning and stochastic optimization", "journal": "J. Mach. Learn. Res", "year": "2011-07", "authors": "J Duchi; E Hazan; Y Singer"}, {"ref_id": "b5", "title": "Hyperparameter Optimization", "journal": "Springer International Publishing", "year": "2019", "authors": "M Feurer; F Hutter"}, {"ref_id": "b6", "title": "Forward and reverse gradient-based hyperparameter optimization", "journal": "International Convention Centre", "year": "2017-08", "authors": "L Franceschi; M Donini; P Frasconi; M Pontil"}, {"ref_id": "b7", "title": "Caltech-256 object category dataset", "journal": "", "year": "2007", "authors": "G Griffin; A Holub; P Perona"}, {"ref_id": "b8", "title": "Deep residual learning for image recognition", "journal": "IEEE Computer Society", "year": "2016-06-27", "authors": "K He; X Zhang; S Ren; J Sun"}, {"ref_id": "b9", "title": "torch-rnn. Github repository", "journal": "", "year": "2017", "authors": "J Johnson"}, {"ref_id": "b10", "title": "Visualizing and understanding recurrent networks", "journal": "", "year": "2015", "authors": "A Karpathy; J Johnson; L Fei-Fei"}, {"ref_id": "b11", "title": "Adam: A method for stochastic optimization", "journal": "International Conference on Learning Representations", "year": "2014", "authors": "D Kingma; J Ba"}, {"ref_id": "b12", "title": "Learning multiple layers of features from tiny images", "journal": "", "year": "2012-05", "authors": "A Krizhevsky"}, {"ref_id": "b13", "title": "Gradient-based learning applied to document recognition", "journal": "Proceedings of the IEEE", "year": "1998-11", "authors": "Y Lecun; L Bottou; Y Bengio; P Haffner"}, {"ref_id": "b14", "title": "Scalable gradient-based tuning of continuous regularization hyperparameters", "journal": "", "year": "2016", "authors": "J Luketina; M Berglund; K Greff; T Raiko"}, {"ref_id": "b15", "title": "Gradient-based hyperparameter optimization through reversible learning", "journal": "", "year": "2015", "authors": "D Maclaurin; D Duvenaud; R P Adams"}, {"ref_id": "b16", "title": "Training deep networks without learning rates through coin betting", "journal": "Advances in Neural Information Processing Systems", "year": "2017", "authors": "F Orabona; T Tommasi"}, {"ref_id": "b17", "title": "Automatic differentiation in PyTorch", "journal": "", "year": "2017", "authors": "A Paszke; S Gross; S Chintala; G Chanan; E Yang; Z Devito; Z Lin; A Desmaison; L Antiga; A Lerer"}, {"ref_id": "b18", "title": "Hyperparameter optimization with approximate gradient", "journal": "", "year": "2016", "authors": "F Pedregosa"}, {"ref_id": "b19", "title": "Convergence analysis of an adaptive method of gradient descent", "journal": "", "year": "2017", "authors": "D M Rubio"}, {"ref_id": "b20", "title": "Descending through a crowded valley-benchmarking deep learning optimizers", "journal": "PMLR", "year": "2021", "authors": "R M Schmidt; F Schneider; P Hennig"}, {"ref_id": "b21", "title": "Deepobs: A deep learning optimizer benchmark suite", "journal": "", "year": "2018", "authors": "F Schneider; L Balles; P Hennig"}, {"ref_id": "b22", "title": "Energy and policy considerations for deep learning in NLP", "journal": "", "year": "2019", "authors": "E Strubell; A Ganesh; A Mccallum"}, {"ref_id": "b23", "title": "Painless stochastic gradient: Interpolation, line-search, and convergence rates", "journal": "", "year": "2019", "authors": "S Vaswani; A Mishkin; I Laradji; M Schmidt; G Gidel; S Lacoste-Julien"}, {"ref_id": "b24", "title": "ADADELTA: An adaptive learning rate method. CoRR, abs/1212", "journal": "", "year": "2012", "authors": "M D Zeiler"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "def SGD. __ init __ (self, alpha): self.alpha = alpha def SGD.step(w): d _ w = w.grad.detach() w = w.detach() -self.alpha.detach() * d _ w", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "defHyperSGD.step(w): # update alpha using equation (1) d _ alpha = self.alpha.grad.detach() self.alpha = self.alpha.detach() -kappa.detach() * d _ alpha # update w using equation (2) d _ w = w.grad.detach() w = w.detach() -self.alpha.detach() * d _ w (a) Computation graph of SGD with a single fixed hyperparameter \u03b1. (b) Computation graph of SGD with a continuouslyupdated hyperparameter \u03b1i.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 1 :1Figure 1: Visualizing the computation graphs of SGD and HyperSGD.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: Training ResNets on CIFAR-10 with hyperoptimizers (Section 3.3.1).", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 4 :4Figure4: Evaluating higher-order hyperoptimization across a variety of benchmarks (Section 3.4). As we stack more layers of optimizers, the resulting hyperoptimizer is less sensitive to the initial choice of hyperparameters, but costs only 1-2% more in runtime.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "def HyperSGD. __ init __ (self, alpha, opt):self.alpha = alpha self.optimizer = opt", "figure_data": "def HyperSGD.step(w):self.optimizer.step(self.alpha)d _ w = w.grad.detach()w = w.detach() -self.alpha * d _ wopt = HyperSGD(0.01, opt=SGD(kappa))"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Hyperoptimization experiments with MNIST. We denote hyperoptimizers by their constituent optimizers separated by slashes (the leftmost item adjusts the model's weights). Adam \u03b1 is an Adam optimizer where only \u03b1 is optimized as by", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "w i+1 = w i \u2212 \u03b1 \u2202f (w i ) \u2202w i", "formula_coordinates": [2.0, 258.38, 238.75, 94.04, 23.22]}, {"formula_id": "formula_1", "formula_text": "\u03b1 i+1 = \u03b1 i \u2212 adjustment for \u03b1 i w i+1 = w i \u2212 \u03b1 i+1 \u2202f (w i ) \u2202w i", "formula_coordinates": [2.0, 239.76, 321.19, 128.59, 40.23]}, {"formula_id": "formula_2", "formula_text": "\u03b1 i+1 = \u03b1 i \u2212 \u03ba \u2202f (w i ) \u2202\u03b1 i (1) w i+1 = w i \u2212 \u03b1 i+1 \u2202f (w i ) \u2202w i (2)", "formula_coordinates": [2.0, 251.7, 409.72, 252.3, 49.75]}, {"formula_id": "formula_3", "formula_text": "\u2202f (w i ) \u2202\u03b1 i = \u2202f (w i ) \u2202w i \u2022 \u2202w i \u2202\u03b1 i = \u2202f (w i ) \u2202w i \u2022 \u2202 w i\u22121 \u2212 \u03b1 i \u2202f (wi\u22121) \u2202wi\u22121 \u2202\u03b1 i (3) = \u2202f (w i ) \u2202w i \u2022 \u2212 \u2202f (w i\u22121 ) \u2202w i\u22121 (4)", "formula_coordinates": [2.0, 178.81, 566.18, 325.19, 56.48]}, {"formula_id": "formula_4", "formula_text": "\u2202w i \u2202\u03b1 i = \u2212m i i + \u221av i \u2202w i \u2202\u03b2 1i = \u2212 \u03b1 i \u2212 \u2202f (wi\u22121) \u2202wi\u22121 + m i\u22121 + i\u03b2 1 (i\u22121) im i 1 \u2212 \u03b2 1 i i i + \u221av i \u2202w i \u2202 i = \u03b1 imi i + \u221av i 2 \u2202w i \u2202\u03b2 2i = \u03b1 imi \u221av i \u2212 \u2202f (wi\u22121) \u2202wi\u22121 2 + v i\u22121 + i\u03b2 2 (i\u22121) iv i 2v i i + \u221av i 2", "formula_coordinates": [3.0, 129.66, 91.35, 344.86, 75.86]}, {"formula_id": "formula_5", "formula_text": "\u03b1 = \u03b1 2 \u2022 10 \u22123 , \u03b1 \u00b5 = 1/(1 \u2212 \u00b5) \u2022 10 \u22126", "formula_coordinates": [7.0, 108.0, 479.0, 397.25, 22.13]}], "doi": "10.1162/089976600300015187"}