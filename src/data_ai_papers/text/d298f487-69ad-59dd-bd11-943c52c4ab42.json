{"title": "A Neural Model for Aggregating Coreference Annotation in Crowdsourcing", "authors": "Maolin Li; Hiroya Takamura; Sophia Ananiadou", "pub_date": "", "abstract": "Coreference resolution is the task of identifying all mentions in a text that refer to the same real-world entity. Collecting sufficient labelled data from expert annotators to train a highperformance coreference resolution system is time-consuming and expensive. Crowdsourcing makes it possible to obtain the required amounts of data rapidly and cost-effectively. However, crowd-sourced labels can be noisy. To ensure high-quality data, it is crucial to infer the correct labels by aggregating the noisy labels. In this paper, we split the aggregation into two subtasks, i.e, mention classification and coreference chain inference. Firstly, we predict the general class of each mention using an autoencoder, which incorporates contextual information about each mention, while at the same time taking into account the mention's annotation complexity and annotators' reliability at different levels. Secondly, to determine the coreference chain of each mention, we use weighted voting which takes into account the learned reliability in the first subtask. Experimental results demonstrate the effectiveness of our method in predicting the correct labels. We also illustrate our model's interpretability through a comprehensive analysis of experimental results.", "sections": [{"heading": "Introduction", "text": "Coreference resolution is the task of identifying all mentions in a text that refer to the same real-world entity. However, it is time-consuming and expensive to collect the large amounts of data from expert annotators that are required to train high-performance coreference resolution systems. A rapid and costeffective alternative is to obtain labels through crowdsourcing (Snow et al., 2008). However, crowdsourced labels are often noisy. In the example in Figure 1, the mention it actually refers to The Super Lamb Banana. However, different crowd annotators produced conflicting labels for this mention. We can also observe that the coreference annotation is more complex than classification and sequence labels. Annotators have to determine an appropriate referent mention for some mentions. Because the performance of supervised learning models is highly dependent on the quality of training data, the aggregation of these noisy labels, (i.e., the process of determining the label that is most likely to be correct) is important to obtain a high-quality training corpus. Although label aggregation is a well-studied topic, most existing studies of natural language labelling tasks have only focused on aggregating classification or sequence labels. To the best of our knowledge, there is only one previous study (Paun et al., 2018) that has investigated how to aggregate crowd-sourced coreference labels.\nIn this paper, we propose a 2-step framework in which the aggregation task is broken down into two subtasks, i.e., mention classification and coreference chain inference.\nIn the mention classification subtask, our model predicts the general category of a mention as shown in Table 1. Our model is based on the autoencoder proposed in (Yin et al., 2017), but with significant extensions. Our encoder is a classifier which takes as its input the crowd labels for each mention, together with the mention's context information. Then it predicts the most plausible general class label for the Figure 1: An example (adapted from the Phrase Detectives Corpus (Chamberlain et al., 2016)) of crowd-sourced coreference annotation.", "publication_ref": ["b42", "b34", "b49", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "General Label Type Description Discourse New (DN)", "text": "The mention is a new entity in the text Discourse Old (DO) The mention refers to an entity which has already been introduced Non-Referring (NR)\nThe mention refers to no actual entity (e.g., it in expletive constructions) Property (PR) The mention refers to a property of an entity (e.g., the most durable light is a property of the bulb)\nTable 1: Four general label types in a coreference resolution labelling task (Chamberlain et al., 2016).\nmention, by taking into account the annotation complexity of the mention and annotators' reliability. The first challenge in proposing the encoder is how to incorporate mention context information. We explore the use of contextual embeddings for this purpose. The second challenge is how to effectively model annotators' behaviour in terms of their quality of annotation. Modelling annotator reliability is helpful in detecting unreliable annotators and in facilitating appropriate task allocation (Donmez and Carbonell, 2008;Donmez and Carbonell, 2010;. Modelling only per-category reliability may not be sufficient to characterise annotators' behaviour patterns for a given annotation task. The original encoder from (Yin et al., 2017) already estimates the per-category reliability. We additionally model overall and per-instance reliability. In addition, we also model the instance complexity. In the second subtask, i.e. coreference chain inference, based on the predicted general classes in the first subtask, we predict each mention's target (i.e., its referent entity which is usually another mention in the text). If a mention is classified as Discourse Old (i.e., it refers to an entity mentioned previously in the text) or as the Property of another mention, its coreference chain is inferred using weighted voting in which the annotators' labels are weighted by their reliability.\nOur contributions are as follows: a) We propose a simple but efficient two-step framework for aggregating crowd-sourced coreference labels. b) We investigate how information about context, annotator reliability and instance complexity can be incorporated into our encoder network to infer correct labels. c) Experimental results demonstrate that incorporating mention context, annotator reliability and instance complexity can increase the accuracy of correct label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable.", "publication_ref": ["b5", "b10", "b11", "b49"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995;Hirschman and Chinchor, 1998;Bagga and Baldwin, 1999;Doddington et al., 2004;Pradhan et al., 2012;Singh et al., 2012;Guillou et al., 2014;Garcia and Gamallo, 2014;Chaimongkol et al., 2014;Ghaddar and Langlais, 2016;Cohen et al., 2017;Fonseca et al., 2017;Webster et al., 2018;Bamman et al., 2020;Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979;Snow et al., 2008;Raykar et al., 2010;Hovy et al., 2013;Li et al., 2014;Felt et al., 2015;Zheng et al., 2017;Yin et al., 2017;Rodrigues and Pereira, 2018;Guan et al., 2018;Li et al., 2019;Zhang et al., 2019) or sequence labels (Hovy et al., 2014;Rodrigues et al., 2014;Huang et al., 2015;Nye et al., 2018;Lin et al., 2019). Note that Raykar et al. (2010) and Felt et al. (2015) also included contextual information. Raykar et al. (2010) incorporated a classifier into their Bayesian model. The classifier took an instance's representation as its input and then predicted an answer. The unsupervised Latent Dirichlet Allocation topic model (Blei et al., 2003) which can capture topics of words and documents was extended by Felt et al. (2015) to be able to handle crowdsourced noisy labels. However, these models can not be applied to the coreference annotation in a straightforward manner, because coreference labels are more complex and they are very different from classification and sequence labels.\nTo overcome the lack of a suitable aggregation method for coreference annotations, Paun et al. ( 2018) proposed the Mention-Pair Annotation model, which to the best of our knowledge, is the only study that attempts to address this challenge. They defined a graphical model and introduced a true label indicator for each <General Class, referent mention> pair, indicating whether or not the pair is correct. However, this model does not include contextual information, which may support the prediction of the correct labels, because the meaning of mentions usually depends on the context in which they occur.", "publication_ref": ["b43", "b19", "b1", "b9", "b37", "b41", "b18", "b14", "b4", "b15", "b6", "b13", "b46", "b2", "b44", "b17", "b5", "b7", "b42", "b38", "b20", "b26", "b12", "b51", "b49", "b39", "b16", "b28", "b50", "b21", "b40", "b22", "b33", "b29", "b38", "b12", "b38", "b3", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Model", "text": "We break down the aggregation of crowd-sourced coreference labels into two steps: mention classification and coreference chain inference, as illustrated in Figure 2. Below, we describe our method in more detail. All the biases in linear layer parameters are omitted for simplification.  Figure 3: Per-category reliability in the method. Each annotator's label is encoded as a vector which is described in Section 3.1.1.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Mention Classification", "text": "In this step, the encoder network, which is a feed-forward neural network classifier, receives as input a mention's contextual embedding and crowd-sourced labels weighted by the annotators' reliability. The output of this encoder is the mention's predicted general class (i.e., DN, DO, NR, or PR).\nTo prepare the input, we first obtain each mention's contextual representation from a pre-trained embedding model, and then concatenate the mention's crowd labels with its contextual representation. This concatenated vector is used as input to the complexity layer, which computes the instance complexity.\nThe output of the complexity layer is used to weight the encoder output, which indicates how much attention our model should pay to learn this instance. The goal is for our model to automatically pay more attention to learning difficult instances than easy ones. Next, we use the mention's contextual embedding to compute annotator reliability, in order to weight the crowd-sourced labels in the concatenated vector. Finally, the weighted concatenated vector is considered as the final input of our encoder.\nSince the ground truth is unavailable in real-world situations, the expert labels in the dataset cannot be used for training the encoder. We introduce a decoder network, which is another neural network that reconstructs the encoder input. By using the reconstructed input, we maximise the log-likelihood of all the observed crowd-sourced labels in the dataset to train the entire autoencoder.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Crowd Labels and Mention Contextual Embedding", "text": "Crowd Labels: For the n-th mention, we formulate its crowd-sourced general labels as a T \u00d7 K matrix C n . C n tk is set to 1 if the t-th annotator's label is the k-th general label, otherwise to 0. Mention Contextual Embedding: Since a mention could consist of more than one token , the average of each token's pre-trained contextual embedding is taken as the representation of the mention, which is represented by the symbol x n .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Instance Complexity", "text": "The complexity of annotating a certain instance is an important factor affecting the quality of annotations produced by crowd workers. If a given mention is more challenging to annotate than others, the annotators are likely to need more effort and time to assign a label for this mention. This is in contrast to assigning labels for easier instances. We aim for our model to perform similarly to annotators who pay more attention to more challenging instances than easier ones.\nMore specifically, we assume that the complexity of annotating the n-th instance, d n , can be estimated from an instance x n and its corresponding crowd labels c n . Therefore, d n is computed as:\nd n = sof tplus([x n ; c n ] \u2022 w f ).(1)\nWe concatenate the flattened C n represented by c n with x n . Then, we use the softplus activation function to compute the annotation complexity of the n-th instance. w f is the parameter vector which will be learned during training.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Annotator Reliability", "text": "Apart from modelling the per-category reliability of annotators, we introduce an additional layer to estimate the overall and instance-level reliability. t-th Annotator's Per-Category Reliability r ct : We consider the encoder network parameters corresponding to the crowd label as the per-category reliability, as illustrated in Figure 3. For each annotator, these parameters can be reshaped into a K \u00d7 K confusion matrix. The parameter located in the i-th row and the j-th column indicates the extent to which an annotator prefers to assign the j-th category if the true answer is the i-th category. If the matrix is nearer to a diagonal matrix with positive values, it means that this annotator can reliably annotate all categories. The parameters will be learned during training.\nt-th Annotator's Overall Reliability r ot : For the overall score, we assign a scalar w to , which will be learned during training, following a sigmoid function to the t-th annotator as:\nrot = (1 + e \u2212w to ) \u22121 .\n(2)\nt-th Annotator's Per-Instance Reliability r n t : For the per-instance reliability, we use x n and a sigmoid function to compute the t-th annotator's reliability on the n-th instance:\nr n t = (1 + e \u2212(x n \u2022w t r ) ) \u22121 .(3)\nw t r is the parameter vector which will be learned during training.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Encoder Network", "text": "The encoder maps its input to a probability distribution p(y n |x n , C n ) over the set of general classes, where y \u2208 {DN, DO, N R, P R}. We prepare the encoder input for the n-th mention in the following way. Firstly, the crowd label matrix C n and mention contextual representation x n are obtained as described in Section 3.1.1. Secondly, we weigh each annotator's label by multiplying each row of C n (whose values correspond to a given annotator's labels) by the reliability score of the annotator. The weighted matrix is denoted by the symbol C n .\nSpecifically, since the encoder already includes the per-category reliability, which will weight crowdsourced labels (described in Section 3.1.3), we let C n be C n when using only this reliability:\nC n = C n . (4\n)\nWhen per-category reliability is additionally supplemented by overall reliability (computed by Equation (2)), we compute C n as:\nC n = [ro1, ro2, ..., roT ] T C n . (5\n)\nis the element-wise multiplication and T is the number of annotators, while T means transpose of a matrix.\nWhen supplementing the per-category reliability with the per-instance reliability, we firstly estimate each annotator's reliability score on each instance r n t using Equation (3), and then compute C n as:\nC n = [r n 1 , r n 2 , ..., r n T ] T C n .(6)\nFinally, we flatten C n to a vector c n and implement the encoder as:\np(y n |x n , C n ) = softmax(d n ([ c n ; x n ] \u00d7 We)),(7)\nwhere the complexity d n is computed using Equation ( 1) and W e is the learnable encoder parameter.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Decoder Network", "text": "The decoder is another network which reconstructs the input crowd annotations 1 C n . We firstly sample a label y from the predicted general class distribution p(y n |x n , C n ) provided by the encoder. y is a one-hot encoding. We then compute the reconstructed crowd labelsC n as:\nv n = d n (y \u00d7 W d ). (8\n)\nC n = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 softmax(\u1e7d n(1) 1,K ) . . . softmax(\u1e7d n(t) 1,K ) . . . softmax(\u1e7d n(T ) 1,K ) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb , v n(t) 1,K =\u1e7d n (t\u22121)\u00d7K+1,tK .(9)\nC n = r Cn .\nW d is the learnable decoder parameter,\u1e7d n is the decoder output before application of the activation function (see Equation ( 8)) and\u1e7d\nn(t)\n1,K are the K elements from index (t \u2212 1) \u00d7 K + 1 to tK, which correspond to the t-th annotator's reconstructed crowd label for the n-th mention. We apply the annotator-wise softmax function to\u1e7d n as illustrated in Equation (9). Finally, using Equation (10), the annotator reliability r is used to weightC n in the same manner as was described above using Equations ( 4)-(6).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning and Predicting", "text": "Pre-Training: We firstly pre-train the encoder by using the majority voting labels as targets. This solves a potential problem of using the encoder, i.e., that the meaning of elements in the encoder output vector is exchangeable. Pre-training the encoder can instead make the output vector aware of which element should represent what category.\nTraining: To train the autoencoder, we maximise the lower bound of log-likelihood of the observed data x and C:\nlog p(x, C) = N n y\u2208classes p(y|x n , C n ) log p(y)p(x n ,C n |y) p(y|x n , C n ) + DKL(p(y|x n , C n )||ptrue(y|x n , C n )) \u2265 N n y\u2208classes p(y|x n , C n ) log p(y)p(x n ,C n |y) p(y|x n , C n ) = N n E p(y|x n ,C n ) log p(x n ,C n |y) \u2212 \u03bb1DKL(p(y|x n , C n )||p(y)),(11)\nwhere p true (y|x n , C n ) is the true distribution (which is unknown) and D KL is a Kullback-Leibler (KL) divergence (Kullback and Leibler, 1951), which measures the difference between probability distributions. The prior probability p(y) in the KL term is estimated from the labels predicted using majority voting. During learning, the KL divergence between the encoder prediction p(y|x n , C n ) and the prior p(y) ensures that our model retains an awareness of the category position information which is learned in the pre-training step. We also introduce a strength hyper-parameter \u03bb 1 on the KL term to weight the impact of p(y) on learning the encoder. Note that in this equation, the decoder p(x n ,C n |y) reconstructs both the mention representation x n and the crowd labels C n . However, since we found that this model not only runs more slowly, but also obtains lower performance than the model which only reconstructs crowd labels, we decided not to reconstruct the mention contextual embedding. 2 Therefore, the reconstruction probability p(x n ,C n |y) is computed as:\nlog p(x n ,C n |y) \u221d log p(C n |y) = T t K k C n tk logC n tk .(12)\nRegularisation: To prevent overfitting, we apply L1 and L2 regularisation to parameters as:\n\u03bb2(||wr||1 + ||We||1 + ||W d ||1) + \u03bb3(||w f ||2)(13)\n\u03bb 2 and \u03bb 3 are also strength hyper-parameters. Prediction: To infer the correct general class, we take each mention's general class as the most probable label according to the distribution p(y|x n , C n ) predicted by the encoder.", "publication_ref": ["b25"], "figure_ref": [], "table_ref": []}, {"heading": "Coreference Chain Inference", "text": "In this step, mentions classified as either DN or NR by the encoder are not processed further, as they do not refer to any other mentions in text. For each of the other mentions, we filter out the crowd-sourced DN and NR labels, and apply weighted voting to the remaining crowd labels to infer its referent mention. In other words, for a mention which is classified as DO or PR, we only aggregate those crowd-sourced referent mentions that appear in the set of mentions classified as DO(\u2022) or PR(\u2022) 3 labels. Each annotator's label is weighted by the product of the annotator's category and the overall/per-instance reliability.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "Dataset: We evaluate our method on the real-world dataset from (Chamberlain et al., 2016), which includes both crowd labels produced by 280 crowd workers and expert labels for 5,654 mentions (3,277 DNs, 2,192 DOs, 136 PRs and 49 NRs).\nMention Contextual Embedding: We compare the use of two pre-trained embeddings from ELMo 4 (Peters et al., 2018) and BERT (bert-base-uncased) 5 (Devlin et al., 2019). When using BERT, we represent each token by using the BERT model outputs from the last four hidden layers, which is the same setting as used in (Peters et al., 2018).\nLearning: We use the Adam (Kingma and Ba, 2015) optimiser (\u03b1 = 0.001, \u03b2 1 = 0.9, \u03b2 2 = 0.999). \u03bb 1 , \u03bb 2 and \u03bb 3 are set to 0.0001, 0.005 and 0.5, respectively. We pre-train the encoder for 100 epochs.  The Metric Marvels is a series of seven animated educational shorts featuring songs about meters, liters, Celsius, and grams, designed to teach American children how to use the metric system.", "publication_ref": ["b5", "b35", "b8", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "1.93e-22", "text": "An encounter with German tourists in New Zealand led to the formation of a group called \"Extreme Ironing International\", and the German Extreme Ironing Section or GEIS.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "2.01e-22", "text": "Taro Tsujimoto is an imaginary ice hockey player that was legally drafted by the National Hockey League's Buffalo Sabres in the 11th round of the 1974 NHL Entry Draft.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Complexity", "text": "Instances with Highest Complexities 0.7123 ... and so she ran from the path into the wood to look for flowers ... 0.7150\nNext they came to some fine meadows. 0.7188 'Pull off my boots,' and then he threw them in her face, and made her pick them up again, and clean and brighten them. We then run the entire autoencoder training by optimising the objective function in Equation ( 11) until either 300 iterations are reached, or the objective function stops improving. Evaluation: The baselines are: majority voting and the state-of-the-art method, Mention-Pair Annotation model (Paun et al., 2018). Four metrics are used for evaluation, MUC (Vilain et al., 1995), B-cubed (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), and CoNLL Score (Pradhan et al., 2011).", "publication_ref": ["b34", "b45", "b0", "b30", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "From Table 3, we observe that our method achieved better performance than baselines. 6 We also report the performance using different settings. Without Context and ELMo/BERT denote the models that do not use context, or which use context, respectively. In terms of reliability, each annotator's Per-Category Reliability is modelled in our model by default. Per-Category + Overall / Per-Instance Reliability indicates that the model supplements per-category reliability with modelling of annotator overall or per-instance reliability. As shown in Table 3, the model (ELMo/BERT, Per-Category Reliability) outperforms (Without Context, Per-Category Reliability), which suggests that the incorporation of context helps to improve the performance. We note that the performance also benefits from additionally capturing annotators' overall or per-instance reliability.\n6 Analysis and Discussion", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3", "tab_3"]}, {"heading": "Instance Complexity", "text": "To analyse complexities, we rank instances according to their complexities estimated by Equation (1). Table 4 lists the instances with the lowest and highest complexities. It can be observed that: 1) it is very easy to annotate instances which have low complexities. The meaning of these mentions in text is clear and explicit; 2) It seems that the instances with the highest complexities are short mentions, particularly those containing possessive pronouns or determiners. They are more difficult because the annotator is likely to have to look back to previous sentences to determine whether they are referring to an entity (or a property of an entity) that has previously been introduced. 3) Our model places more emphasis on learning more difficult instances than easier ones. Figure 5: Correlation between annotator's true accuracy with the weight value located in the i-th row and i-th column in annotator's per-category reliability matrix. Each point represents one annotator.\nWe also investigate the complexity distributions of the corpus and the correlation between each instance's complexity and annotation agreement in Figure 4. From the distribution (top of Figure 4) we can see that the model can distinguish between those instances that are useful for training and those that are not useful. To ascertain whether annotators achieve higher levels of agreement on less challenging instances, we use entropy to measure whether the annotators can make the same annotation decision for a certain instance. A low entropy value indicates a high agreement, and vice versa.\nAgreement(ai) = \u2212 K k Pi(k) log Pi(k), Pi(k) = T t I(ait = k) |ai|(14)\nwhere a i denotes the annotations for the i-th instance and a it is the t-th annotator's label. P i (k) is the probability that the i-th instance is annotated as the k-th class. |a i | indicates how many annotators annotated this instance. The indication function I(\u2022) = 1 when a it = k, otherwise I(\u2022) = 0. Figure 4 (lower part) shows no correlation between complexity and agreements, indicating that it is not accurate to measure complexity by relying solely on annotators' levels of agreements.", "publication_ref": [], "figure_ref": ["fig_1", "fig_1", "fig_1"], "table_ref": ["tab_4"]}, {"heading": "Annotator Reliability", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Per-Category Reliability", "text": "As described in Section 3.1.3, we consider an annotator's per-category reliability as a K \u00d7 K confusion matrix (K is the number of classes). The value located in the i-th row and the j-th column indicates the extent to which an annotator prefers to assign the j-th category if the true answer is the i-th category. To explore whether the value located in the i-th row and the i-th column can reflect each annotator's reliability for the i-th category, we visualise annotators' true accuracy of the i-th category and the weight value in Figure 5. 7 We observe that the correlation coefficient of each line is positive, i.e., higher per-category reliability is correlated with higher accuracy. We also present the per-category reliability matrices of six randomly selected annotators in Figure 6. The values in the diagonals of Annotator 0 and 9 are relatively large, indicating that they are both skilled at labelling all the categories. Meanwhile the matrix of Annotator 11 shows that this annotator is good at every category except NR. The matrices of Annotator 41, 57 and 85 suggest they are less reliable than the other annotators.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Per-Instance Reliability", "text": "To explore the per-instance reliability, we reduce each instance's embedding representation to a 2dimensional space for visualisation by using t-SNE (Maaten and Hinton, 2008) as shown in Figure 7. Each point is a single instance and is coloured according to each annotator's estimated reliability on this    ", "publication_ref": ["b31"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Overall Reliability", "text": "To analyse the overall reliability, we investigate the correlation between a) overall reliability and annotator's accuracy (left side of Figure 8) and b) overall reliability and the number of instances annotated (right side of Figure 8). Figure 8 shows positive correlations in both cases. This implies that when an annotator's overall reliability is high, then the true overall accuracy and the number of labels provided are large. Our model considers an annotator as generally as more reliable and experienced if this annotator has a higher accuracy and has annotated a great number of instances.", "publication_ref": [], "figure_ref": ["fig_4", "fig_4", "fig_4"], "table_ref": []}, {"heading": "Model", "text": "In addition, we conduct ablation analysis on our models, as shown in Table 5.\nWithout Pre-Training: For this experiment, we omit the pre-training step described in Section 3.1.6 and directly train the whole autoencoder. The significant performance drop shown in Table 5 indicates that training the entire autoencoder from scratch produces rather poor results.\nReconstruct Both Crowd Labels and Mention Embedding: We also investigate the performance when the decoder reconstructs both crowd and mention embedding. This reconstruction results in a slight drop in performance. We also found that the training takes much longer. Therefore, we recommend that the decoder should only reconstruct crowd labels.\n2-Layer Autoencoder: We increase the number of encoder/decoder layers from one to two, which results in a dramatic performance decrease. It suggests that it is not necessary to use deep models.\nWider Context (Window Size=3): We investigate whether the performance can benefit from combining integration of wider contextual information. We concatenate the mention's contextual embedding Table 5: Ablation analysis of our method in predicting correct labels.\nwith the averaged BERT embeddings of the three tokens before and after the current mention. However we find that this does not further improve the performance of the model. In order to better understand the behaviour of our model, the prediction errors are analysed. In the first step, the general class of a mention can be mistakenly classified. For example, the model wrongly predicted some DO mentions as DN mentions (DO\u2192DN). In the second step, the incorrect coreference chain of a mention can be determined. We found that 79.25% (DO\u2192DN 44.61%, PR\u2192DN 16.18%, DN\u2192DO 12.91%, Others 5.5%) and 20.75% of total errors were made in the first and second step respectively. After checking the mentions with wrong predictions, we summarise the possible reasons as follows: 1) The lack of contextual information from earlier sentences. For example, the bottle in the sentence \"[...] and break the bottle, and [...]\" was incorrectly predicted as a DN mention. This bottle had actually been introduced in an earlier sentence. 2) The difficulty of distinguishing between mentions which have closed meaning but belong to different types. Here are two examples: a wicked creature which should be considered as a property of a wolf instead of a new mention; The cakes in \" [...] was again taking cakes to [...]\" was predicted as a DO mention referring to another cake. However, it is actually a DN mention because the previously mentioned cake had been consumed and the cakes are new ones.\n3) The challenge of identifying if the mention it, is referring to a thing previously mentioned or is in an expletive construction (e.g., the it in \"How dark it was inside the wolf.\"). Incorporating information from a mention's neighbouring sentences may improve the model. Since there are not many PR and NR mentions in the training data, to investigate how well our model learned them, we checked their numbers of predictions after each iteration as shown in Figure 9. We can observe that the numbers are very close to 0 at first iterations and eventually become stable somewhere between 25 to 40. We also found that approximately 20% of PR predictions were wrong and 75% were not identified. Therefore, it is worth investigating an appropriate training method to deal with the imbalance in training data.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Conclusion and Future Work", "text": "We proposed a two-step framework for aggregating crowd-sourced coreference labels. In the mention classification subtask, the encoder classifies each mention as belonging to one of the four general categories, i.e., DN, DO, NR or PR. This encoder incorporates mention context, instance complexity and the annotator reliability at different levels (i.e., overall, per-category and per-instance). In the coreference chain inference subtask, we use the learned reliability to infer the coreference chains. Experimental results demonstrate the effectiveness of our model. Furthermore, our comprehensive analysis shows that the learned complexity and reliability are explainable, thus helping to explain how our model infers the correct label for each instance. Lastly, an error analysis was carried out to understand the incorrect predictions. As future work, we will explore the challenges of solving other complex annotation tasks and how our model can be used and adapted for them.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This research was supported by BBSRC Japan Partnering Award [Grant ID: BB/P025684/1] and by funding from the Artificial Intelligence Research Center, National Institute of Advanced Industrial Science and Technology, Japan. We would like to thank Paul Thompson for his valuable comments. M. Li thanks The University of Manchester for the School of Computer Science Kilburn Overseas Fees Bursary.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Algorithms for scoring coreference chains", "journal": "", "year": "1998", "authors": "Amit Bagga; Breck Baldwin"}, {"ref_id": "b1", "title": "Cross-document event coreference: Annotations, experiments, and observations", "journal": "", "year": "1999", "authors": "Amit Bagga; Breck Baldwin"}, {"ref_id": "b2", "title": "An annotated dataset of coreference in English literature", "journal": "European Language Resources Association", "year": "2020-05", "authors": "David Bamman; Olivia Lewke; Anya Mansoor"}, {"ref_id": "b3", "title": "Latent dirichlet allocation", "journal": "Journal of machine Learning research", "year": "2003-01", "authors": "M David;  Blei; Y Andrew; Michael I Jordan Ng"}, {"ref_id": "b4", "title": "Corpus for coreference resolution on scientific papers", "journal": "", "year": "2014", "authors": "Panot Chaimongkol; Akiko Aizawa; Yuka Tateisi"}, {"ref_id": "b5", "title": "Phrase detectives corpus 1.0 crowdsourced anaphoric coreference", "journal": "", "year": "2016-05", "authors": "Jon Chamberlain; Massimo Poesio; Udo Kruschwitz; ; ; Khalid Choukri; Thierry Declerck; Sara Goggi; Marko Grobelnik; Bente Maegaard; Joseph Mariani"}, {"ref_id": "b6", "title": "Coreference annotation and resolution in the colorado richly annotated full text (craft) corpus of biomedical journal articles", "journal": "BMC Bioinformatics", "year": "2017", "authors": "Arrick K Bretonnel Cohen; Miji Joo-Young Lanfranchi; Michael Choi;  Bada; A William; Natalya Baumgartner; Karin Panteleyeva; Martha Verspoor; Lawrence E Palmer;  Hunter"}, {"ref_id": "b7", "title": "Maximum likelihood estimation of observer error-rates using the em algorithm", "journal": "Applied Statistics", "year": "1979", "authors": "Alexander Philip Dawid; Allan M Skene"}, {"ref_id": "b8", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019-06", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b9", "title": "The automatic content extraction (ACE) program -tasks, data, and evaluation", "journal": "", "year": "2004-05", "authors": "George Doddington; Alexis Mitchell; Mark Przybocki; Lance Ramshaw; Stephanie Strassel; Ralph Weischedel"}, {"ref_id": "b10", "title": "Proactive learning: cost-sensitive active learning with multiple imperfect oracles", "journal": "Association for Computing Machinery", "year": "2008", "authors": "Pinar Donmez; Jaime G Carbonell"}, {"ref_id": "b11", "title": "From active to proactive learning methods", "journal": "Springer", "year": "2010", "authors": "Pinar Donmez; Jaime G Carbonell"}, {"ref_id": "b12", "title": "Making the most of crowdsourced document annotations: Confused supervised LDA", "journal": "Association for Computational Linguistics", "year": "2015-07", "authors": "Paul Felt; Eric Ringger; Jordan Boyd-Graber; Kevin Seppi"}, {"ref_id": "b13", "title": "Collective elaboration of a coreference annotated corpus for portuguese texts", "journal": "", "year": "2017-09", "authors": "Sandra Evandro Brasil Fonseca; Vinicius Collovini De Abreu; Ana Luisa Sesti; Paulo Leal; Renata Quaresma;  Vieira"}, {"ref_id": "b14", "title": "Multilingual corpora with coreferential annotation of person entities", "journal": "", "year": "2014", "authors": "Marcos Garcia; Pablo Gamallo"}, {"ref_id": "b15", "title": "WikiCoref: An English coreference-annotated corpus of Wikipedia articles", "journal": "", "year": "2016", "authors": "Abbas Ghaddar; Phillippe Langlais"}, {"ref_id": "b16", "title": "Who said what: Modeling individual labelers improves classification", "journal": "", "year": "2018-02", "authors": "Melody Guan; Varun Gulshan; Andrew Dai; Geoffrey Hinton"}, {"ref_id": "b17", "title": "Removing the training wheels: A coreference dataset that entertains humans and challenges computers", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Anupam Guha; Mohit Iyyer; Danny Bouman; Jordan Boyd-Graber"}, {"ref_id": "b18", "title": "ParCor 1.0: A parallel pronoun-coreference corpus to support statistical MT", "journal": "", "year": "2014-05", "authors": "Liane Guillou; Christian Hardmeier; Aaron Smith; J\u00f6rg Tiedemann; Bonnie Webber"}, {"ref_id": "b19", "title": "Appendix F: MUC-7 coreference task definition (version 3.0)", "journal": "", "year": "1998-04-29", "authors": "Lynette Hirschman; Nancy Chinchor"}, {"ref_id": "b20", "title": "Learning whom to trust with MACE", "journal": "Association for Computational Linguistics", "year": "2013-06", "authors": "Dirk Hovy; Taylor Berg-Kirkpatrick; Ashish Vaswani; Eduard Hovy"}, {"ref_id": "b21", "title": "Experiments with crowdsourced re-annotation of a POS tagging data set", "journal": "Association for Computational Linguistics", "year": "2014-06", "authors": "Dirk Hovy; Barbara Plank; Anders S\u00f8gaard"}, {"ref_id": "b22", "title": "Estimation of discourse segmentation labels from crowd data", "journal": "Association for Computational Linguistics", "year": "2015-09", "authors": "Ziheng Huang; Jialu Zhong; Rebecca J Passonneau"}, {"ref_id": "b23", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015-05", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b24", "title": "Singletons and coreference resolution evaluation", "journal": "Association for Computational Linguistics", "year": "2011", "authors": "Sandra K\u00fcbler; Desislava Zhekova"}, {"ref_id": "b25", "title": "On information and sufficiency", "journal": "The Annals of Mathematical Statistics", "year": "1951", "authors": "S Kullback; R A Leibler"}, {"ref_id": "b26", "title": "A confidenceaware approach for truth discovery on long-tail data", "journal": "", "year": "2014-12", "authors": "Qi Li; Yaliang Li; Jing Gao; Lu Su; Bo Zhao; Murat Demirbas; Wei Fan; Jiawei Han"}, {"ref_id": "b27", "title": "Proactive learning for named entity recognition", "journal": "Association for Computational Linguistics", "year": "2017-08", "authors": "Maolin Li; Nhung Nguyen; Sophia Ananiadou"}, {"ref_id": "b28", "title": "Modelling instance-level annotator reliability for natural language labelling tasks", "journal": "", "year": "2019-06", "authors": "Maolin Li; Arvid Fahlstr\u00f6m Myrman; Tingting Mu; Sophia Ananiadou"}, {"ref_id": "b29", "title": "AlpacaTag: An active learningbased crowd annotation framework for sequence tagging", "journal": "Association for Computational Linguistics", "year": "2019-07", "authors": "Dong-Ho Bill Yuchen Lin; Frank F Lee; Ouyu Xu; Xiang Lan;  Ren"}, {"ref_id": "b30", "title": "On coreference resolution performance metrics", "journal": "Association for Computational Linguistics", "year": "2005", "authors": "Xiaoqiang Luo"}, {"ref_id": "b31", "title": "Visualizing data using t-sne", "journal": "Journal of Machine Learning Research", "year": "2008-11", "authors": "Laurens Van Der Maaten; Geoffrey Hinton"}, {"ref_id": "b32", "title": "Aggregating and predicting sequence labels from crowd annotations", "journal": "Association for Computational Linguistics", "year": "2017-07", "authors": "An Thanh Nguyen; Byron Wallace; Junyi Jessy Li; Ani Nenkova; Matthew Lease"}, {"ref_id": "b33", "title": "A corpus with multi-level annotations of patients, interventions and outcomes to support language processing for medical literature", "journal": "Association for Computational Linguistics", "year": "2018-07", "authors": "Benjamin Nye; Junyi Jessy Li; Roma Patel; Yinfei Yang; Iain Marshall; Ani Nenkova; Byron Wallace"}, {"ref_id": "b34", "title": "A probabilistic annotation model for crowdsourcing coreference", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Jon Silviu Paun; Udo Chamberlain; Juntao Kruschwitz; Massimo Yu;  Poesio"}, {"ref_id": "b35", "title": "Deep contextualized word representations", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Matthew Peters; Mark Neumann; Mohit Iyyer; Matt Gardner; Christopher Clark; Kenton Lee; Luke Zettlemoyer"}, {"ref_id": "b36", "title": "Conll-2011 shared task: Modeling unrestricted coreference in ontonotes", "journal": "Association for Computational Linguistics", "year": "2011", "authors": "Sameer Pradhan; Lance Ramshaw; Mitchell Marcus; Martha Palmer; Ralph Weischedel; Nianwen Xue"}, {"ref_id": "b37", "title": "CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes", "journal": "Association for Computational Linguistics", "year": "2012-07", "authors": "Alessandro Sameer Pradhan; Nianwen Moschitti; Olga Xue; Yuchen Uryupina;  Zhang"}, {"ref_id": "b38", "title": "Learning from crowds", "journal": "Journal of Machine Learning Research", "year": "2010-04", "authors": "C Vikas; Shipeng Raykar; Linda H Yu; Gerardo Hermosillo Zhao; Charles Valadez; Luca Florin; Linda Bogoni;  Moy"}, {"ref_id": "b39", "title": "Deep learning from crowds", "journal": "", "year": "2018-02", "authors": "Filipe Rodrigues; Francisco Pereira"}, {"ref_id": "b40", "title": "Sequence labeling with multiple annotators", "journal": "", "year": "2014-05", "authors": "Filipe Rodrigues; Francisco Pereira; Bernardete Ribeiro"}, {"ref_id": "b41", "title": "Wikilinks: A largescale cross-document coreference corpus labeled via links to wikipedia", "journal": "", "year": "2012", "authors": "Sameer Singh; Amarnag Subramanya; Fernando Pereira; Andrew Mccallum"}, {"ref_id": "b42", "title": "Cheap and fast -but is it good? evaluating non-expert annotations for natural language tasks", "journal": "Association for Computational Linguistics", "year": "2008-10", "authors": "Rion Snow; O' Brendan; Daniel Connor; Andrew Jurafsky;  Ng"}, {"ref_id": "b43", "title": "Overview of results of the MUC-6 evaluation", "journal": "", "year": "1995-11-06", "authors": "Beth M Sundheim"}, {"ref_id": "b44", "title": "Anaphora resolution in chinese for analysis of medical q&a platforms", "journal": "Springer", "year": "2020-10", "authors": "Alena Tsvetkova"}, {"ref_id": "b45", "title": "A model-theoretic coreference scoring scheme", "journal": "Association for Computational Linguistics", "year": "1995", "authors": "Marc Vilain; John Burger; John Aberdeen; Dennis Connolly; Lynette Hirschman"}, {"ref_id": "b46", "title": "Mind the GAP: A balanced corpus of gendered ambiguous pronouns", "journal": "Transactions of the Association for Computational Linguistics", "year": "2018", "authors": "Kellie Webster; Marta Recasens; Vera Axelrod; Jason Baldridge"}, {"ref_id": "b47", "title": "Huggingface's transformers: State-of-theart natural language processing", "journal": "ArXiv", "year": "2019", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R'emi Louf; Morgan Funtowicz; Jamie Brew"}, {"ref_id": "b48", "title": "Adversarial learning for chinese ner from crowd annotations", "journal": "", "year": "2018", "authors": "Yaosheng Yang; Meishan Zhang; Wenliang Chen; Wei Zhang; Haofen Wang; Min Zhang"}, {"ref_id": "b49", "title": "Aggregating crowd wisdoms with label-aware autoencoders", "journal": "", "year": "2017", "authors": "Jianhua Li'ang Yin; Weinan Han; Yong Zhang;  Yu"}, {"ref_id": "b50", "title": "Crowdsourced label aggregation using bilayer collaborative clustering", "journal": "IEEE Transactions on Neural Networks and Learning Systems", "year": "2019-10", "authors": "J Zhang; V S Sheng; J Wu"}, {"ref_id": "b51", "title": "Truth inference in crowdsourcing: Is the problem solved?", "journal": "", "year": "2017-01", "authors": "Yudian Zheng; Guoliang Li; Yuanbing Li; Caihua Shan; Reynold Cheng"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Overview of the proposed method.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 4 :4Figure 4: Levels of complexity distributions of instances in the corpus (Top) and correlation between complexity and annotation agreement (Bottom).Figure5: Correlation between annotator's true accuracy with the weight value located in the i-th row and i-th column in annotator's per-category reliability matrix. Each point represents one annotator.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 6 :6Figure 6: Reliability matrix learned by our model. Darker colours denote higher weights.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 7 :7Figure 7: T-SNE visualisation of instances which are annotated by different annotators.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 8 :8Figure 8: Correlation between annotator's overall reliability with accuracy and number of annotated labels. Each point represents an individual annotator.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 9 :9Figure 9: Number of PR and NR predictions after each training iteration.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "contains the main notations of our model.", "figure_data": "Notation DescriptionNNumber of instances (i.e., mentions that have crowd-sourced labels) in a datasetTNumber of annotatorsKNumber of general classesC nCrowd general labels of n-th mention (a T \u00d7 K matrix)c nFlattened C \u00f1C nReconstructed C n by a decoder networkx nContextual embedding of n-th mentiond nAnnotation complexity of n-th mentionrctPer-category reliability of t-th annotator (a K \u00d7K matrix)rotOverall reliability of t-th annotator (a scalar)rntPer-instance reliability of t-th annotator on n-th mention(a scalar)wto, w t rLearnable parameters for computing the t-th annotator'soverall and per-instance reliability respectivelyw fLearnable parameters for computing instance complexityWe, W d Learnable parameters of the encoder and decoder networkrespectively"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Main notation for our proposed models.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Precision (P), Recall (R) and F scores (F) of our predicted labels.", "figure_data": "ComplexityInstances with Lowest Complexities6.98e-23"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Instances with lowest and highest complexities as estimated by our model. Mentions are highlighted in bold.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "d n = sof tplus([x n ; c n ] \u2022 w f ).(1)", "formula_coordinates": [4.0, 241.09, 397.4, 284.45, 10.61]}, {"formula_id": "formula_1", "formula_text": "rot = (1 + e \u2212w to ) \u22121 .", "formula_coordinates": [4.0, 257.95, 619.56, 81.64, 10.13]}, {"formula_id": "formula_2", "formula_text": "r n t = (1 + e \u2212(x n \u2022w t r ) ) \u22121 .(3)", "formula_coordinates": [4.0, 251.23, 670.02, 274.31, 12.96]}, {"formula_id": "formula_3", "formula_text": "C n = C n . (4", "formula_coordinates": [5.0, 277.49, 137.93, 244.57, 10.33]}, {"formula_id": "formula_4", "formula_text": ")", "formula_coordinates": [5.0, 522.06, 140.48, 3.48, 7.77]}, {"formula_id": "formula_5", "formula_text": "C n = [ro1, ro2, ..., roT ] T C n . (5", "formula_coordinates": [5.0, 237.67, 188.32, 284.39, 10.33]}, {"formula_id": "formula_6", "formula_text": ")", "formula_coordinates": [5.0, 522.06, 190.88, 3.48, 7.77]}, {"formula_id": "formula_7", "formula_text": "C n = [r n 1 , r n 2 , ..., r n T ] T C n .(6)", "formula_coordinates": [5.0, 241.96, 273.32, 283.59, 11.13]}, {"formula_id": "formula_8", "formula_text": "p(y n |x n , C n ) = softmax(d n ([ c n ; x n ] \u00d7 We)),(7)", "formula_coordinates": [5.0, 201.5, 311.21, 324.04, 10.33]}, {"formula_id": "formula_9", "formula_text": "v n = d n (y \u00d7 W d ). (8", "formula_coordinates": [5.0, 255.62, 431.41, 433.46, 10.61]}, {"formula_id": "formula_10", "formula_text": ")", "formula_coordinates": [5.0, 689.08, 433.97, 87.0, 7.77]}, {"formula_id": "formula_11", "formula_text": "C n = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 softmax(\u1e7d n(1) 1,K ) . . . softmax(\u1e7d n(t) 1,K ) . . . softmax(\u1e7d n(T ) 1,K ) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb , v n(t) 1,K =\u1e7d n (t\u22121)\u00d7K+1,tK .(9)", "formula_coordinates": [5.0, 247.96, 458.97, 506.41, 90.22]}, {"formula_id": "formula_13", "formula_text": "n(t)", "formula_coordinates": [5.0, 216.59, 586.25, 14.78, 6.99]}, {"formula_id": "formula_14", "formula_text": "log p(x, C) = N n y\u2208classes p(y|x n , C n ) log p(y)p(x n ,C n |y) p(y|x n , C n ) + DKL(p(y|x n , C n )||ptrue(y|x n , C n )) \u2265 N n y\u2208classes p(y|x n , C n ) log p(y)p(x n ,C n |y) p(y|x n , C n ) = N n E p(y|x n ,C n ) log p(x n ,C n |y) \u2212 \u03bb1DKL(p(y|x n , C n )||p(y)),(11)", "formula_coordinates": [6.0, 110.71, 81.86, 414.84, 92.68]}, {"formula_id": "formula_15", "formula_text": "log p(x n ,C n |y) \u221d log p(C n |y) = T t K k C n tk logC n tk .(12)", "formula_coordinates": [6.0, 196.06, 335.73, 329.49, 27.03]}, {"formula_id": "formula_16", "formula_text": "\u03bb2(||wr||1 + ||We||1 + ||W d ||1) + \u03bb3(||w f ||2)(13)", "formula_coordinates": [6.0, 209.14, 388.65, 316.4, 8.35]}, {"formula_id": "formula_17", "formula_text": "Agreement(ai) = \u2212 K k Pi(k) log Pi(k), Pi(k) = T t I(ait = k) |ai|(14)", "formula_coordinates": [8.0, 192.28, 395.1, 333.26, 21.54]}], "doi": ""}