{"title": "Binary TTC: A Temporal Geofence for Autonomous Navigation", "authors": "Abhishek Badki; Orazio Gallo; Jan Nvidia;  Kautz; U C Santa; Barbara Pradeep Sen", "pub_date": "2021-04-28", "abstract": "Time-to-contact (TTC), the time for an object to collide with the observer's plane, is a powerful tool for path planning: it is potentially more informative than the depth, velocity, and acceleration of objects in the scene-even for humans. TTC presents several advantages, including requiring only a monocular, uncalibrated camera. However, regressing TTC for each pixel is not straightforward, and most existing methods make over-simplifying assumptions about the scene. We address this challenge by estimating TTC via a series of simpler, binary classifications. We predict with low latency whether the observer will collide with an obstacle within a certain time, which is often more critical than knowing exact, per-pixel TTC. For such scenarios, our method offers a temporal geofence in 6.4 ms-over 25\u00d7 faster than existing methods. Our approach can also estimate per-pixel TTC with arbitrarily fine quantization (including continuous values), when the computational budget allows for it. To the best of our knowledge, our method is the first to offer TTC information (binary or coarsely quantized) at sufficiently high frame-rates for practical use.", "sections": [{"heading": "Introduction", "text": "Path planning, whether for robotics or automotive applications, requires accurate perception, which in turn, benefits from depth information. Many modalities exist to infer depth. Strategies such as lidar estimate depth accurately but only at sparse locations, in addition to being expensive. Depth can also be estimated with strategies such as stereo [35], but these introduce issues such as calibration drift over time.\nAn alternative is to use a monocular camera-an attractive, low-cost solution, with light maintenance requirements. The motion of the camera induces optical flow between consecutive frames, which carries information on the scene's depth. Depth, however, can only be estimated in the constrained case of static scenes. For dynamic scenes, the 2D flow of a pixel is a function of its depth, its velocity, and the velocity of the camera. Disentangling these three components is an under-constrained and challenging problem. Figure 1: Given I0 and I1, our binary time-to-contact (TTC) estimation acts as a temporal geofence detecting objects that will collide with the camera plane within a given time, B . It only takes 6.4 ms to compute. Our method can also output quantized TTC, Q , and continuous TTC, C .\nPrevious approaches either ignore dynamic regions [36], or use strong scene priors [27,45,47,44,26] to hallucinate their depth. Do we really need to disentangle them? The role of perception is to inform decisions. An object moving towards the camera is more critical than another that is potentially closer, but moving away from the camera. Differently put, predicting the time at which an object would make contact with the camera may be more valuable than knowing its actual depth, velocity, or acceleration [23].\nIn fact, time-to-contact (TTC), the time for an object to collide with the camera plane under the current velocity conditions, is a traditional concept in psychophysics [39,14] as well as computer vision [38,5]. TTC can be estimated from the ratio of an object's depth and its velocity relative to the camera, even when the problem of regressing either one independently is ill-posed. However, TTC estimation has its own challenges, forcing most of the existing approaches to severely constrain their scope. For instance, they assume that the scene is static, or that a mask for dynamic objects is provided [16,33]. The recent approach by Yang and Ramanan tackles some of these challenges by learning a mapping between optical flow and TTC directly, thus producing a per-pixel TTC estimate [43]. However, it relies on accurate optical flow estimation and inherits its limitations, including its heavy computational load.\nUnlike most existing approaches, we side-step the need to explicitly compute the optical flow. Our learning-based approach estimates per-pixel TTC directly from images. We leverage the relationship between an object's TTC and the ratio of the size of its image in different frames [5,15]. However, because regressing this scale factor exactly is challenging, we focus on whether the size of the object's image is increasing, indicating a collision at some time in the future, or decreasing, indicating that the object is moving away.\nMore concretely, inspired by Badki et al. [1], we perform a series of binary classifications with respect to different scale factors, each corresponding to a specific TTC. Each classification yields a binary TTC map with respect to the desired time threshold, Figure 1, insets B . Our binary map, efficient to compute, acts as a temporal geofence in front of the camera: it identifies objects within a given TTC, in 6.4 ms. 1 This is useful when a quick reaction time is important. We can also estimate per-pixel TTC with arbitrary quantization, as shown in Figure 1, insets Q , or continuous, Figure 1, insets C . These different levels of quantization, from binary to continuous, can be predicted with the same core network. In fact, quantization levels can be added, removed, or moved dynamically at inference time, based on the current needs of the autonomous agent. Given the scarcity of TTC ground truth data, to impose additional inductive bias to our network we also introduce binary optical flow estimation as an auxiliary task. We achieve competitive performance for TTC estimation against existing methods, even stereo-based methods, but we are from 25\u00d7 to several orders of magnitude faster.", "publication_ref": ["b34", "b35", "b26", "b44", "b46", "b43", "b25", "b22", "b38", "b13", "b37", "b4", "b15", "b32", "b42", "b4", "b14", "b0", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "While valuable for navigation, 3D information about the scene is challenging to gather with a monocular camera. Existing methods assume the scene to be static [9,40,17,25], or estimate single-image relative depth, rather than metric depth [8,11,13,10,34]. Single-image relative depth can be \"upgraded\" to metric by computing the optical flow between multiple images [27,45,47,44,26], but this requires strong priors, and it is brittle for complex, large motions. Nevertheless, with depth maps and optical flow we can estimate scene flow, which captures both depth and velocity [37]. A recent approach by Hur and Roth elegantly combines these principles by learning scene priors to decompose the depth and the velocity directly from the estimated optical flow information [18].\nInstead of regressing depth and velocity, we focus on estimating their ratio directly from images, which yields timeto-contact (TTC). We do this via multiple binary classifications. Here we discuss the state-of-the-art in terms of these two axes-TTC and regression via classification.", "publication_ref": ["b8", "b39", "b16", "b24", "b7", "b10", "b12", "b9", "b33", "b26", "b44", "b46", "b43", "b25", "b36", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Time-to-Contact", "text": "Time-to-contact (TTC) was studied in psychophysics and psychology, even before it attracted the attention of the 1 On an NVIDIA Tesla V100 for 384\u00d71152 images. computer vision community. Early work by Lee, for instance, suggested that TTC is sufficient for making decisions about braking, and is likely to be picked up by the driver faster than distance, speed, or acceleration of objects in the scene [23]. From a computational standpoint, TTC is appealing because it only depends on the ratio of depth and velocity, which can be estimated directly from images, even when estimating either one is an ill-posed problem [16]. Several traditional approaches have been proposed to estimate TTC from the estimated optical flow [32,33,3]. Horn et al. proposed a direct method for TTC estimation that only uses the constant brightness assumptions [15,16]. While these approaches estimate the TTC for an object that is moving relative to the camera, they require masks for dynamic objects, which limits their practical impact.\nWe propose a learning-based approach for TTC estimation that handles multiple dynamic objects-without needing any segmentation-and estimates per-pixel TTC. The elegant, closely related work by Yang and Ramanan estimates optical flow, uses it to compute the scaling factor of objects, and maps it to TTC [43]. Xu et al. also estimate the scaling of objects, but do so by modeling the change of objects' size explicitly for optical flow estimation [42]. However, computing the full optical flow is timeconsuming, and estimating TTC from optical flow inherits its limitations. Instead, following Horn et al. [16], we compute TTC directly from the input images, side-stepping optical flow computation altogether. Moreover, inspired by Badki et al. [1], we solve TTC estimation via a series of binary classifications. Each binary classification can be computed independently of the others at over 150 fps. This can be thought of as a temporal geofence, detecting pixels or objects within a given TTC. For existing methods, including the method by Yang and Ramanan [43], this can only be achieved by computing the TTC for all the pixels or objects in the scene, and then thresholding it.", "publication_ref": ["b0", "b22", "b15", "b31", "b32", "b2", "b14", "b15", "b42", "b41", "b15", "b0", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "3D Inference as a Classification Problem", "text": "The idea of posing 3D regression as a classification task has a rich history. Several learning-based approaches estimate depth via a multi-class classification task [21,4,46,20]. These are more accurate than other learning-based approaches that pose the problem as a regression task [30,7]. Badki et al. introduced a method that allows us to control the trade-off between latency and accuracy [1]. Instead of posing depth as a multi-class classification problem, they solve it via multiple binary classifications. Each classification provides useful information about the scene at high frame-rates. Our approach is based on the same intuition. We are the first learning-based approach to estimate perpixel TTC directly from the input images, and to pose it as a (binary) classification problem.\nI0 I1 I0 I1 \u03b1 < 1 \u03b1 \u2248 1 I0 I \u03b1 i 1 I0 I \u03b1 i 1 \u03b1 < \u03b1 i \u03b1 > \u03b1 i (a) Inputs I0 I1 I0 I1 \u03b1 < 1 \u03b1 \u2248 1 I0 I \u03b1 i 1 I0 I \u03b1 i 1 \u03b1 < \u03b1 i \u03b1 > \u03b1 i (b) Scaled inputs I0 I1 I0 I1 \u03b1 < 1 \u03b1 \u2248 1 I0 I \u03b1 i 1 I0 I \u03b1 i 1 \u03b1 < \u03b1 i \u03b1 > \u03b1 i (c) Temporal geofence\nFigure 2: Intuition. Given two images of a dynamic scene, I0 and I1, we define a temporal geofence to detect objects expected to cross the camera plane before a given time \u03c4i from the time of capture of I0. Compare I0 and I1, (a). The images of the orange and blue cars are smaller in I0 (\u03b1 < 1), while the image of the purple car is roughly unchanged. This allows us to predict that only the first two cars will collide with the camera plane. We propose to perform this comparison after scaling the source image by a factor \u03b1i (corresponding to TTC value \u03c4i). The orange car is still larger in I \u03b1 i 1 (\u03b1 < \u03b1i), indicating that it will cross the camera plane before the specified \u03c4i. On the other hand, the blue and purple cars will not. Rather than regressing the exact scale factor, we classify a pixel as making contact before or after \u03c4i by classifying if the objects scale up or down in I \u03b1 i 1 with respect to I0. This yields a binary TTC probability map for \u03c4i, (c).", "publication_ref": ["b20", "b3", "b45", "b19", "b29", "b6", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Method", "text": "Despite some time-to-contact (TTC) estimation methods dating back to the 1990s [38,5], TTC never rose to the popularity of other techniques that are now mainstream, such as optical flow or stereo estimation. This is due in part to its intrinsic limitations and to the challenges it poses, which we discuss below. Note that in the rest of the paper we often attribute the properties of objects to the corresponding pixels, to simplify the discussion. For instance, we talk of \"a pixel's velocity\" to indicate the projection on the image plane of the velocity of the object imaged by that pixel.", "publication_ref": ["b37", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "A Review on Time-to-Contact", "text": "Consider two frames of a static scene captured by a moving camera. Pixel-level correspondences allow us to compute depth, if the camera information is known. In the more realistic case of dynamic scenes, however, the problem becomes ill-posed: the displacement of a pixel is the result of its depth, its velocity, and the camera velocity, all of which cannot be disambiguated without strong priors. In this case, the concept of time-to-contact (TTC) comes to the rescue. Given an object O, the TTC \u03c4 , i.e., the time at which object O will (or did) cross the camera plane, can be written as\n\u03c4 = \u2212Z O dZ O dt = \u2212Z O /\u017b O ,(1)\nwhere the origin is at the camera, and we assume that the current velocity conditions will continue. Z O is the depth of the object from the camera plane, and\u017b O its relative velocity. Equation 1shows the first appealing feature of TTC: even if the depth and the velocity of the object cannot be estimated independently, \u03c4 can be computed from their ratio. However, we are interested in computing the TTC from pixel displacements alone. To do that, we need an additional piece of information: the location of the focus-ofexpansion (FOE). Given two frames of a static scene captured by translating the camera, all the pixels in the image move along lines originating from the FOE, the image of the point towards which the camera is moving. Under the same assumptions, the FOE coincides with the epipole. The relationship between the TTC, \u03c4 , and the velocity of the pixel is given by\u1e8b\n= x \u2212 x 0 \u03c4 and\u1e8f = y \u2212 y 0 \u03c4 ,(2)\nwhere (x 0 , y 0 ) is the FOE. Equation 2 can be easily derived by differentiating the projection of a 3D point onto the image plane with respect to time [15]. Note that the velocity (\u1e8b,\u1e8f) can be computed from the optical flow (u, v), which allows us to write\n\u03c4 = x \u2212 x 0 u \u2022 T and \u03c4 = y \u2212 y 0 v \u2022 T,(3)\nwhere T is the time elapsed between the two frames. Equation 3 shows a second compelling reason to use TTC: there is no need for camera calibration. 2 There are also challenges, however. If a rigid object is dynamic and translates with respect to the scene, its pixels move along lines centered around a different FOE. To estimate the TTC using Equation 3, then, we need to localize the FOE of each dynamic object. Moreover, if an object is deformable, the FOE varies with each pixel, and for general motion, we need to further compensate for rotations. This is why traditional approaches had to rely on oversimplifying assumptions.\nHowever, we can compute the size of the image of a fronto-parallel, planar, non-deformable object of size S O at distance Z O as [15] s\nO = f S O /Z O ,(4)\nwhere f is the focal length of the camera. Since f and S O are constant, differentiating Equation 4 yields\nZ O /\u017b O = \u2212s O /\u1e61 O ,(5)", "publication_ref": ["b14", "b1", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Inputs", "text": "Binary segmentation probability maps Figure 3: Binary TTC maps. Our method can directly identify the pixels that will contact the camera plane before a given time.\nFrom top to bottom, we show results for four TTC values for a case of highway driving and for a camera that rotates.\nwhich, plugged into Equation 1, allows us to estimate the TTC using only information about the size of an object's image and its rate of change:\n\u03c4 = s O /\u1e61 O .(6)\nNote that both Equation 3 and 6 effectively look at scaling. However, the latter is independent of the point with respect to which the scaling is performed, whereas for the former, that point is the FOE. Of course, under more realistic conditions (e.g., not planar or not fronto-parallel objects), Equation 6 becomes an approximation, which requires proper handling, as we show later.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "TTC via Multiple Binary Classifications", "text": "In this section, we first provide the intuition motivating our method. We then describe binary TTC, the core of our method, which detects pixels predicted to collide with the camera plane within a given time. We further show how an arbitrary number of binary classifications can be combined to estimate, for each pixel, a quantized version of the TTC. To simplify the description, here we assume positive TTCs, i.e., we focus on objects moving towards the camera rather than away from it. In Section 4, we describe a small adaptation of our method that allows us to seamlessly remove this distinction.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Intuition", "text": "Given two images captured at times t 0 and t 1 , Equation 1 can be approximated as\n\u03c4 = \u2212Z(t 0 ) Z(t 1 ) \u2212 Z(t 0 ) t 1 \u2212 t 0 = t 1 \u2212 t 0 1 \u2212 Z(t1) Z(t0) .(7)\nIf we assume fronto-parallel and planar objects that do not rotate, we can combine Equations 7 and 4, thus expressing the TTC as a function of observations in image space:\n\u03c4 = t 1 \u2212 t 0 1 \u2212 s(t0) s(t1) = t 1 \u2212 t 0 1 \u2212 \u03b1 ,(8)\nI 0 I 1 f 0 f 1 * \u03c6 \u03a8 GT L Task * \u03c6 \u03a8 L GT TTC binary scaling \u03b1 i B \u03c4 i BCE 1 \u03c4 * >\u03c4 i quantized scaling {\u03b1 i } {B \u03c4 i } BCE {1 \u03c4 * >\u03c4 i } continuous scaling {\u03b1 i } {B \u03c4 i }, \u03c4 BCE, SL1 {1 \u03c4 * >\u03c4 i }, \u03c4 * Optical flow horizontal shifting u i B u i BCE 1 u * >u i vertical shifting v i B v i BCE 1 v * >v i\nFigure 4: Architecture. We perform all of our tasks with minor modifications to the same backbone. We preprocess the input images by extracting features and applying a task dependent operation, * , to the features of the second image. The input parameter \u03c6, the loss, L, the ground truth, GT, and the output, \u03a8 for each task are listed in the table. Note that {\u2022} indicates a set, and 1 the indicator function.\nwhere s is the size of the image of an object or region in the scene. In other words, with Equation 8, the TTC can be computed from the scale factor \u03b1. This simplifies our task, compared with having to estimate per-pixel FOEs and optical flow. However, regressing \u03b1 for each pixel explicitly, which requires defining the size of objects or image regions and tracking its change over time, is not straightforward. Instead, we consider a pair of images, I 0 and I 1 , and compute I \u03b1i 1 , a version of I 1 scaled by a factor \u03b1 i . This scaling factor \u03b1 i corresponds to a unique TTC \u03c4 i . The regions whose size matches between I 0 and I \u03b1i 1 will collide with the camera plane exactly at \u03c4 i . Furthermore, we can expect regions that are larger (or smaller) in I \u03b1i 1 to collide with the camera before (or after) \u03c4 i , see Figure 2.\nInstead of regressing the scale factor \u03b1 directly, then, we propose to train a neural network to take such pairs of images and classify regions in I \u03b1i 1 as being larger or smaller than the corresponding regions in I 0 , where the correspondence is learned implicitly. Our approach is inspired by the work of Badki et al. [1] for stereo. They also learn to classify the disparity of a pixel as being larger or smaller than a given disparity, instead of regressing the disparity directly.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Binary TTC", "text": "The inputs to our method are two images and scale factor \u03b1 i corresponding to the TTC we want to analyze. We first extract features f 0 and f 1 from both images, and scale f 1 by \u03b1 i , to obtain f \u03b1i 1 . Rather than classifying the objects as scaling up or down between f 0 and f \u03b1i 1 , we train a lightweight network to directly classify whether each pixel's TTC is larger or smaller than \u03c4 i , using a binary crossentropy loss. That is, the network predicts a probability map\nB \u03c4i (x, y) = p(\u03c4 (x, y) > \u03c4 i ; f 0 , f \u03b1i 1 ),(9)\nwhich can be binarized by simple thresholding. We train directly to predict binary TTC instead of explicitly detecting if the size of the image of objects is getting larger or smaller with respect to \u03b1 i for two reasons. First, ground truth data for the scale factor \u03b1 is challenging to even define beyond a small neighborhood of pixels, unless objects move rigidly and only translate. Moreover, as discussed before, Equation 8is an approximation in the common case of objects that are not planar, and for non-rectilinear motions. Equation 7, which establishes the relationship between the change in depth and the TTC, allows us to generate the ground truth data from existing datasets, as we explain in Section 4.1. A network trained to classify the TTC directly can learn the necessary priors to compensate for the approximations introduced by Equation 8. Our core architecture is shown in Figure 4. This very architecture is also used for all the tasks we describe below, with the caveat that the terms in blue differ for each task.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Quantized TTC", "text": "Our core approach naturally extends to estimating a coarsely quantized TTC for each pixel, which may be more useful than binary in certain scenarios. We note that Equation 9 is a complementary cumulative distribution function. Therefore, for two time-to-contact values, \u03c4 j > \u03c4 i , we can compute\np(\u03c4 i < \u03c4 (x, y) \u2264 \u03c4 j ) = B \u03c4i (x, y) \u2212 B \u03c4j (x, y). (10\n)\nConsider a set of TTC values {\u03c4 i } i=1:N , and assume they are in increasing order. After computing Equation 9 for each of the N TTC values, we can estimate the quantization bin in which the TTC of a pixel falls as\nQ(x, y) = arg max i p \u03c4 i < \u03c4 (x, y) \u2264 \u03c4 i+1 . (11\n)\nThe different TTC values can be spaced non-uniformly.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Continuous and Selective TTC", "text": "While our method is specifically designed for binary and quantized TTC estimation, it can also estimate per-pixel, continuous TTC. In principle, we can approximate continuous values by predicting quantized TCC (Section 3.2.3) with a larger set of TTC values {\u03c4 i } i=1:N . This, however, fails to exploit the relationship between the binary classifications for different \u03c4 i 's, for a pixel. We slightly modify the approach while still preserving its binary classification core, as shown in Figure 4. Specifically, instead of taking consecutive pairs of probability maps and applying Equation 10, we stack them.\n\u03c4 1 \u03c4 2 \u03c4 * \u03c4 N B \u03c4 i (x 0 , y 0 ) ...\nFor a specific pixel (x 0 , y 0 ) we generally see a progression as in the plot on the right. That is, B \u03c4i (x 0 , y 0 ) (the probability that the object will collide after \u03c4 i ) is consistently high for \u03c4 i \u03c4 * , and low for \u03c4 i \u03c4 * , where \u03c4 * is the correct TTC value. In the transition region, the network is uncertain, which is why aggregating information across multiple \u03c4 i is beneficial. Badki et al. [1], who obtain a similar curve for disparity, propose to estimate the transition point, \u03c4 * in our case, by computing the area under the curve (AUC),\nAUC(x, y) = i (\u03c4 i+1 \u2212 \u03c4 i ) \u2022 B \u03c4i (x, y). (12\n)\nTo understand why Equation 12 yields the desired result, consider the case of a step function, i.e., \u03c4 * aligns with a quantization boundary: AUC= \u03c4 * \u2022 1 = \u03c4 * . This relationship holds for the more common case of a smooth transition region: because the transition is generally symmetric around \u03c4 * , the red and green areas in the plot have similar extent and compensate for each other. Because the AUC is differentiable, we can use it to fine-tune our network so that combining a set of probability values using the AUC operation yields continuous TTC values.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "A Note on Inference-Time Tradeoff", "text": "Binary, quantized, and continuous TTC estimation yield increasingly rich information for navigating an environment. We note that, when estimating quantized and continuous TTC, the multiple binary classifications can be run in parallel, as they are independent of each other. Therefore we can compute quantized and continuous TTC at roughly the same frame-rate as for binary quantization-just above 150 fps.\nIf multiple binary classifications cannot be run in parallel due to hardware limitations, the cost for computing quantized and continuous TTC grows linearly with the number of levels. In such cases, one can decide the number of quantization levels dynamically to best leverage the trade-off between accuracy and latency. For situations where a fast response time is critical, such as highway driving, binary TTC may be sufficient. For slower navigation (e.g., for robotics or for parking lot driving), our method can be adapted to trade latency for a finer quantization at inference time.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dealing with the Lack of Training Data", "text": "As for any learning-based method, having access to a large amount of data is critical to properly train our network. Unfortunately, there are no datasets with TTC ground truth data and few scene flow datasets that we can use to infer it (Section 4.1). To increase the training data, we leverage a closely related task: binary optical flow estimation. We use the same binary approach, but we shift the features (horizontally and vertically) rather than scaling them. We then classify the direction of the shift (left/right or up/down). For horizontal shifts, we seek to predict a probability map relative to a given shift u i\nB ui (x, y) = p(u(x, y) > u i ; f 0 , f ui 1 ),(13)\nwhere f ui 1 are the features extracted from the second image and shifted by u i . The equation for the vertical shift v i is analogous. Using optical flow to pre-train our network and continuing using it as an auxiliary task yields a stronger inductive bias, as we discuss in Section 5. Although optical flow is an auxiliary task, we show results in Figure 5 to allow for a visual evaluation.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Implementation Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "TTC Data Generation", "text": "A dataset providing per-pixel TTC ground truth does not exist. However, we can use ratio of the depth in different frames to compute the TTC with Equation 7. This ratio can be computed from datasets offering per-pixel scene flow\n[X(t 0 ), Y (t 0 ), Z(t 0 )] \u2192 [X(t 1 ), Y (t 1 ), Z(t 1 )].\nTo train for TTC estimation we use the Driving dataset from the SceneFlowDatasets [29]. We also use the KITTI15 [30] dataset to train for TTC estimation. We split the training dataset of KITTI15 into train and validation split, and show analysis on the validation part of the dataset. To pretrain our network for the task of binary optical flow, we use the FlyingChairs2 [7,19] and the FlyingThings3D [29] datasets. We also use optical flow data available from Driving [29] and KITTI15 [30], and train our network for both binary optical flow and TTC estimation.", "publication_ref": ["b28", "b29", "b6", "b18", "b28", "b28", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Working in the Inverse TTC Domain", "text": "We are interested in objects predicted to collide within \u03c4 i seconds. However, \u03c4 \u2208 (\u2212\u221e, \u221e), with positive and negative values indicating objects moving towards and away from the camera, respectively. That is, some of the pixels whose TTC is smaller than \u03c4 i will never collide with the camera plane. In practice, then, we seek to identify the pixels such that\n(\u03c4 (x, y) \u2264 \u03c4 i ) \u2229 (\u03c4 (x, y) > 0),(14)\nwhich introduces an additional complication. Moreover, the effect of TTC in the image space in not linear. A simple solution is to work with the ratio-of-depths, the inverse of the TTC domain:\n\u03b7 = Z(t 1 ) Z(t 0 ) = 1 \u2212 t 1 \u2212 t 0 \u03c4 .(15)\nYang and Ramanan termed it motion-in-depth [43]. Thanks to Equation 15, the effect of TTC is linear in image space and Equation 14 simply reduces to\n\u03b7(x, y) \u2264 \u03b7 i .(16)\nFor binary TTC, then, we simply scale the features of the source image by a factor \u03b1 i = \u03b7 i . Similarly, for continuous estimation, we sample planes uniformly in the inverse TTC domain. We apply uniformly spaced scale factors to the features of the source image: {\u03b1 i } i=1:N = {\u03b1 0 + i\u2206\u03b1} i=1:N . The AUC of the resulting segmentation gives us the map \u03b7(x, y), which we then covert to a TTC map.", "publication_ref": ["b42"], "figure_ref": [], "table_ref": []}, {"heading": "Architecture", "text": "Figure 4 shows our architecture. The feature extraction module uses spatial pyramid pooling layers as PSMNet [4]. The resulting 32-channel feature maps are at one-third the input image resolution. We then apply a task-dependent operator, * , parametrized by \u03c6, to f 1 and generate f \u03c6 1 . For instance, when estimating the horizontal component of binary optical flow, * shifts the features by a horizontal shift u i , and for binary TTC, it scales them by \u03b1 i . The full list for each task is in the table in Figure 4. To avoid cropping out features, we zero-pad f 0 and f \u03c6 1 to 1.5\u00d7 the feature map resolution. The concatenation of f 0 and f \u03c6 1 is fed to a 2D encoder-decoder network with skip connections. This module has three heads-one for binary TTC, one for binary horizontal optical flow, and one binary vertical optical flow. We apply a sigmoid to each output to obtain the respective probability maps. Intuitively, this network tells us if the features in f \u03c6 1 are scaling up/down, shifting right/left, or shifting up/down with respect to the corresponding features in f 0 .", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "Training", "text": "We pre-train our network for the binary optical flow task on the FlyingChairs2 dataset and train it further on the FlyingThings3D dataset [29]  Figure 6: Comparison on continuous TTC estimation. We show the predicted TTC and the error map (red and green indicate high and low error, respectively) for all methods. Our method and Yang and Ramanan's method explicitly train for TTC estimation and provide better results than the monocular scene flow method of Hur and Roth. Note the lower error in the TTC estimation for our method.\n(BCE) loss with respect to a thresholded version of the ground truth. The TTC head is left unsupervised in this stage. We then fine-tune our network for estimating binary TTC first on the Driving [29] and then on the KITTI15 [30] datasets. Since both datasets also offer optical flow data, in this second stage we train for both binary optical flow and binary TCC. In Section 5 we discuss the impact of this choice on the quality of the results. We use relative weights of 0.8 and 0.2 for binary TTC and binary optical flow tasks respectively. For training on the KITTI15 dataset, we use the same split as Yang and Ramanan [43].\nFor the continuous version, we pre-train the network as before, and fine-tune it on FlyingThings3D for continuous optical flow. For each training image pair we uniformly sample horizontal and vertical shifts and stack the maps corresponding to each shift. We use the resulting volumes to compute continuous optical flow via the AUC operation described in Section 3.2.4. We can then continue training the network using a BCE loss on the individual probability maps, and a Smooth-L1 (SL1) [12] regression loss on the output of the AUC module. We use relative weights of 0.1 and 0.9 respectively. To fine-tune our network for continuous TTC estimation, we follow a similar strategy as for the binary segmentation training. We continue training the network for the task of continuous optical flow and continuous TTC estimation on the Driving and the KITTI15 datasets. However, this time we form three volumes, two corresponding to optical flow and one to TTC. As discussed in Section 4.2, we work in the inverse TTC domain and uniformly sample scale factors. The network trained on the entire KITTI15 dataset is used for scene flow estimation on KITTI15 benchmark images, as explained in Section 5. We refer the reader to our Supplementary for the additional training details.", "publication_ref": ["b28", "b28", "b29", "b42", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation and Results", "text": "In this section we discuss qualitative and quantitative results for our method. First, to validate the importance of the training strategy described in Section 3.3, we compare three training strategies: (1) we train our network only to estimate binary TTC, (2) we pre-train it to estimate binary optical flow (OF) and then binary TTC, and (3) we pre-train with binary OF, and then continue training with both binary OF and binary TTC (our method). As shown in Table 1, our final method achieves a percentage error of 1.013 and a mean intersection-over-union (mIOU) of 0.9525. If we only train for TTC estimation after pre-training for OF, we observe an increase in percentage error to 1.174 and a drop in mIOU to 0.9453. Removing the binary OF estimation altogether, leads to an additional increase in percentage error to 2.670 and an additional drop in mIOU to 0.8808.\nWe also thoroughly validate our approach numerically on the KITTI15 validation set. We compare to Yang and Ramanan [43], who proposed the only existing approach that computes per-pixel TTC from a monocular camera, under practical assumptions. They too look at how the size of objects changes, but they estimate it explicitly (and locally) from the optical flow between frames. We also measure against PRSM [41] and OSF [30], both of which perform 3D scene flow estimation using stereo cameras. They represent images as a collection of planar super-pixels and jointly solve for geometry and 3D motion, which informs about the change of depth over time (Section 4.1). This can be used to estimate motion-in-depth for each pixel, \u03b7(x, y), using Equation 15, which can then be thresholded. Our final comparison is against Hur and Roth [18], a state-of-the-art monocular scene flow estimation approach. We compare against their best model, which uses a combination of selfsupervised learning on the KITTI15 raw dataset and supervised learning on the entire KITTI15 training dataset. Note that this approach is already fine-tuned on our validation set.\nTable 1 shows that our continuous TTC estimation, yields the lowest motion-in-depth error:\nMiD = ||log(\u03b7) \u2212 log(\u03b7 GT )|| 1 \u2022 10 4 .(17)\nHowever, our fundamental innovation is the ability to define a temporal geofence, i.e., detecting pixels with a TTC smaller than a given \u03c4 i without the need to estimate the full TTC first. On this task we perform on par with Yang and Ramanan, but our binary TTC is around 26\u00d7 faster. OSF performs better than our approach in terms of binary TTC, though, again, it uses a richer input. Moreover, OSF and PRSM, implemented on a CPU, take 390 s and 300 s respectively. The approach by Hur and Roth struggles despite fine-tuning the model on the validation set. However, unlike ours and Yang and Ramanan's approaches, they do not use a synthetic dataset to pre-train, nor train for a betterposed TTC estimation task. Instead, they focus on the more difficult task of monocular scene flow estimation.\nWe further evaluate our method on the KITTI15 benchmark [31] with the same strategy as Yang and Ramanan [43]: we use our motion-in-depth estimation to compute scene flow, the 3D motion\n[X(t 0 ), Y (t 0 ), Z(t 0 )] \u2192 [X(t 1 ), Y (t 1 ), Z(t 1 )\n] corresponding to each pixel. The first two scene flow components can be estimated by backprojecting the optical flow for each pixel. For the Z component, traditional approaches use stereo information. We can estimate the third component from the motion-in-depth ratio if we are given the depth in one frame. After computing the depth at t 0 with GANet [46] (evaluated by D1 in Table 2) and combining it with our TTC estimate, we can compute the depth for each pixel at time t 1 . Therefore, D2 in Table 2 effectively measures the quality of our results. The other numbers in the table evaluate other components and are reported for completeness. While our method is not designed to estimate scene flow, it performs on par, or slightly better than methods specifically optimized for it.\nIn certain scenarios, binary TTC can be more informative than depth. For instance, the top row of Figure 1 shows a car driving away from the camera, and one that is farther, but driving towards the camera. The latter, detected by our binary TTC, is potentially more impactful-in the true sense of the word!-for the ego vehicle path planning. The last row of Figure 3 shows that, as discussed in Section 3.2.2, our method can compensate for camera rotation, even if that breaks some of our assumptions. This is also visible for continuous TTC in the first row of Figure 5.\nWe show quantized TTC estimation results in Figure 1 Q . Note that even just 9 TTC quantization levels (8 binary classifications) provide a meaningful representation of the scene. Moreover, the underlying binary classifications can be run in parallel as they are independent of each other. Therefore, quantized TTC can be run at roughly the same frame-rate as the binary TTC.\nWe show a visual comparison with Hur and Roth [18], and Yang and Ramanan [43] on continuous TTC for KITTI15 images in Figure 6. Note the lower for our approach. In Figure 7 we show qualitative result of our approach on the Citiscapes dataset [6]. We show two failure cases on this dataset, one due to the sudden vertical motion caused by a road bump and another due to an object rotating significantly. Broadly, our method struggles for motions under-represented in the training dataset.    ", "publication_ref": ["b42", "b40", "b29", "b30", "b42", "b45", "b17", "b42", "b5"], "figure_ref": ["fig_1", "fig_2"], "table_ref": ["tab_2", "tab_2", "tab_3", "tab_3"]}, {"heading": "Conclusions", "text": "In certain scenarios, time-to-contact (TTC) information can be more useful than depth. However, existing TTC estimation methods either make impractical assumptions, or cannot be run in real time. We presented a framework to estimate time-to-contact (TTC) from a monocular input. In just 6.4 ms, our approach computes a temporal geofence to detect objects predicted to collide with the camera plane within a given TTC. By computing a number of such geofences, it can also estimate TTC with arbitrary quantization, including continuous TTC. We show that our method achieves competitive performance for TTC estimationeven when other methods use richer input data.\nBinary TTC: A Temporal Geofence for Autonomous Navigation (Supplementary)\n1. Additional Details on Architecture", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Binary Estimation", "text": "Our binary segmentation architecture has three main components: a feature extraction network (FeatExtractNet), a segmentation network (SegNet2D), and a refinement network (SegRefineNet). Our architecture blocks are adopted from Bi3DNet [1]. We do not use any batch normalization layers in our network.\nFeatExtractNet. We use the same feature extraction network as used in Bi3DNet [1]. This feature extraction module is based on the simplified version of the feature extraction network from PSMNet [4]. We normalize each color channel of the input images using the mean and standard deviation of 0.5 before passing to this network. The output is a 32-channel feature map at one third of the input image resolution.\nSegNet2D. We warp the source image features using a task-dependent operator. To avoid cropping out the image features during this warping operation, we zero pad the feature maps to 1.5\u00d7 the feature map resolution. The reference image feature map and the warped source image feature map are concatenated and fed to the SegNet2D network. SegNet2D is a 2D encoder-decoder network with skip-connections. The encoder is comprised of five blocks, each of which has a conv layer that downsamples the features with a stride of 2 followed by another conv layer with a stride of 1. We use 3 \u00d7 3 kernels. The feature sizes for each of these five blocks are 128, 256, 512, 1024, and 1024. The decoder is comprised of five blocks, each of which has of a deconv layer with 4 \u00d7 4 kernels and a stride of 2, followed by a conv layer with 3 \u00d7 3 kernels and a stride of 1. The feature sizes for each of these five blocks are 1024, 512, 256, 128, and 64. We use the LeakyReLU activation function in the network with a slope of 0.1. A final conv layer with 3 \u00d7 3 kernels, without any activation, is used to generate 3 outputs: one for binary TTC, one for binary horizontal optical flow, and one for binary vertical optical flow. The final segmentation probability maps can be obtained by cropping out the excess padding, upsampling the outputs to the input image resolution and applying a sigmoid function.\nSegRefineNet. SegRefineNet is used to refine the segmentation outputs from SegNet2D network using the reference image as a guide. First, we generate a 16 channel feature map for the reference image by applying 3 conv layers with 3 \u00d7 3 kernels and a stride of 1. The first two layers use ReLU activation and the final layer does not use any activation. This feature extraction is done only once and can be used for refining multiple segmentation outputs from the SegNet2D network. An upsampled segmentation output is concatenated with the reference image features and refined by applying 4 conv layers. Each conv layer uses 3 \u00d7 3 kernels, a feature-size of 8, and LeakyReLU activation with a slope of 0.1. The final output of this network is generated by a final conv layer with 3 \u00d7 3 kernel and without any activation. The final binary segmentation probability map can then be generated by applying a sigmoid function.", "publication_ref": ["b0", "b0", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Continuous Estimation", "text": "Given input images we generate the corresponding feature maps using the same FeatExtractNet as in Section 1.1. To generate continuous estimation results we uniformly sample the warping parameters in the desired range and use these parameters to warp the features of the source image. These warped source image features are concatenated with the reference image features to form an input volume for the SegNet2D network. SegNet2D generates an output volume which upon upsampling to the input image resolution, applying the sigmoid operator followed by AUC operator gives us the continuous estimation map. This continuous map is further refined using the input image as a guide using a refinement network, ContRefineNet. This network is based on the disparity refinement network proposed in StereoNet [22].", "publication_ref": ["b21"], "figure_ref": [], "table_ref": []}, {"heading": "Additional Details on Training", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Binary Estimation", "text": "We pre-train our network for the binary optical flow task on the FlyingChairs2 [7,19] dataset for 300 epochs and train it further on FlyingThings3D dataset [29] for 400 epochs using a binary cross-entropy (BCE) loss with respect to a thresholded version of the ground truth. Each batch for training is formed by randomly sampling 16 image pairs from the dataset and then randomly sampling shifts for the two components of the optical flow vector for each image. Note that our SegNet2D network has three output heads and we leave the head corresponding to binary TTC untouched during this training. We then fine-tune our network for estimating binary TTC first on the Driving [29] for 500 epochs and then on the KITTI15 [30] datasets for 10k epochs. Since both datasets also offer optical flow data, in this second stage we train for both binary optical flow and binary TCC. To do this we form a batch by randomly sampling 16 image pairs from the dataset. Then for each image pair we form two sets of warped image features: one by randomly sampling shifts for the two components of the optical flow vector and other by randomly sampling scales for training binary TTC. We select the appropriate segmentation output corresponding to the task and use BCE loss to train our network simultaneously for the binary optical flow and TTC estimation task. ", "publication_ref": ["b6", "b18", "b28", "b28", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Continuous Estimation", "text": "To train our network for the continuous estimation task, we start with the network trained on the FlyingChairs2 and FlyingThings3D datasets for the binary optical flow task and fine-tune it on FlyingThings3D for continuous optical flow for 100 epochs. Each batch for training is formed by randomly sampling 8 image pairs from the dataset. For each training image pair we uniformly sample horizontal and vertical shifts and stack the maps corresponding to each shift. We use the resulting volumes to compute continuous optical flow via the AUC operation which we refine using ContRefineNet. To fine-tune our network for continuous TTC estimation, we follow a similar strategy as for the binary segmentation training. We continue training the network for the task of continuous optical flow and continuous TTC estimation on Driving dataset for 100 epochs, followed by KITTI15 datasets for 600 epochs. We form three volumes, two corresponding for optical flow and one for TTC. Throughout the training, we use BCE loss on the estimated binary segmentation probability maps, and a SmoothL1 (SL1) regression loss on the output of the AUC module. We use relative weights of 0.1 and 0.9 respectively. While training on both TTC and OF estimation tasks, we use relative weights of 0.8 and 0.2 respectively. For optical flow we randomly sample a contiguous block of 16 shifts that are divisible by 3 and in the range [\u221299, 99] for both the components of the optical flow. We select the shifts to be a factor of 3 since we perform the shifting on the feature maps that are one third the input image resolution. The AUC operation seamlessly handles the objects with shifts that lie beyond sampled range. For TTC estimation, we work in the inverse TTC domain and uniformly sample 24 scales in the range [0.5, 1.3]. We perform the training on the cropped images of size 384\u00d7576. The cropping is done after the feature warping step. Again for the KITTI15 dataset, we split the dataset into train and validation sets and use the validation set to compare our method with competing approaches. The network trained on the entire KITTI15 dataset is used for scene flow estimation on KITTI15 benchmark images as explained in the main paper.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "The authors would like to thank Stan Birchfield for the inspiring discussions on the theory of time-to-contact, Gengshan Yang and Junhwa Hur for their kind help in the comparisons with previous works, and anonymous reviewers and ACs for their helpful suggestions.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Bi3D: Stereo depth estimation via binary classifications", "journal": "", "year": "2004", "authors": "Abhishek Badki; Alejandro Troccoli; Kihwan Kim; Jan Kautz; Pradeep Sen; Orazio Gallo"}, {"ref_id": "b1", "title": "Bounding boxes, segmentations and object coordinates: How important is recognition for 3D scene flow estimation in autonomous driving scenarios", "journal": "", "year": "2017", "authors": "Aseem Behl; Omid Hosseini Jafari; Siva Karthik Mustikovela; Hassan Abu Alhaija; Carsten Rother; Andreas Geiger"}, {"ref_id": "b2", "title": "Calculating time-to-contact using real-time quantized optical flow", "journal": "", "year": "1995", "authors": "Ted Camus"}, {"ref_id": "b3", "title": "Pyramid stereo matching network", "journal": "", "year": "2006", "authors": "Jia-Ren Chang; Yong-Sheng Chen"}, {"ref_id": "b4", "title": "Surface orientation and time to contact from image divergence and deformation", "journal": "", "year": "1992", "authors": "Roberto Cipolla; Andrew Blake"}, {"ref_id": "b5", "title": "The cityscapes dataset for semantic urban scene understanding", "journal": "", "year": "2016", "authors": "Marius Cordts; Mohamed Omran; Sebastian Ramos; Timo Rehfeld; Markus Enzweiler; Rodrigo Benenson; Uwe Franke; Stefan Roth; Bernt Schiele"}, {"ref_id": "b6", "title": "Learning optical flow with convolutional networks", "journal": "", "year": "2015", "authors": "Alexey Dosovitskiy; Philipp Fischer; Eddy Ilg; Philip Hausser; Caner Hazirbas; Vladimir Golkov; Patrick Van Der; Daniel Smagt; Thomas Cremers;  Brox;  Flownet"}, {"ref_id": "b7", "title": "Depth map prediction from a single image using a multi-scale deep network", "journal": "", "year": "2014", "authors": "David Eigen; Christian Puhrsch; Rob Fergus"}, {"ref_id": "b8", "title": "LSD-SLAM: Large-scale direct monocular slam", "journal": "", "year": "2014", "authors": "Jakob Engel; Thomas Sch\u00f6ps; Daniel Cremers"}, {"ref_id": "b9", "title": "Deep ordinal regression network for monocular depth estimation", "journal": "", "year": "2018", "authors": "Huan Fu; Mingming Gong; Chaohui Wang"}, {"ref_id": "b10", "title": "Unsupervised cnn for single view depth estimation: Geometry to the rescue", "journal": "", "year": "2016", "authors": "Ravi Garg; Vijay Kumar Bg; Gustavo Carneiro; Ian Reid"}, {"ref_id": "b11", "title": "Fast R-CNN", "journal": "", "year": "2015", "authors": "Ross Girshick"}, {"ref_id": "b12", "title": "Unsupervised monocular depth estimation with leftright consistency", "journal": "", "year": "2017", "authors": "Cl\u00e9ment Godard; Oisin Mac Aodha; Gabriel J Brostow"}, {"ref_id": "b13", "title": "Time-to-contact. Elsevier", "journal": "", "year": "2004", "authors": "Heiko Hecht; Geert Savelsbergh"}, {"ref_id": "b14", "title": "Time to contact relative to a planar surface", "journal": "", "year": "2007", "authors": "K P Berthold; Yajun Horn; Ichiro Fang;  Masaki"}, {"ref_id": "b15", "title": "Hierarchical framework for direct gradient-based time-to-contact estimation", "journal": "", "year": "2009", "authors": "K P Berthold; Yajun Horn; Ichiro Fang;  Masaki"}, {"ref_id": "b16", "title": "DeepMVS: Learning multi-view stereopsis", "journal": "", "year": "2018", "authors": "Po-Han Huang; Kevin Matzen; Johannes Kopf; Narendra Ahuja; Jia-Bin Huang"}, {"ref_id": "b17", "title": "Self-supervised monocular scene flow estimation", "journal": "", "year": "2008", "authors": "Junhwa Hur; Stefan Roth"}, {"ref_id": "b18", "title": "Occlusions, motion and depth boundaries with a generic network for disparity, optical flow or scene flow estimation", "journal": "", "year": "2018", "authors": "Eddy Ilg; Tonmoy Saikia; Margret Keuper; Thomas Brox"}, {"ref_id": "b19", "title": "DPSNet: End-to-end deep plane sweep stereo", "journal": "", "year": "2019", "authors": "Sunghoon Im; Hae-Gon Jeon; Stephen Lin; In So Kweon"}, {"ref_id": "b20", "title": "End-to-end learning of geometry and context for deep stereo regression", "journal": "", "year": "2017", "authors": "Alex Kendall; Hayk Martirosyan; Saumitro Dasgupta; Peter Henry; Ryan Kennedy; Abraham Bachrach; Adam Bry"}, {"ref_id": "b21", "title": "StereoNet: Guided hierarchical refinement for real-time edge-aware depth prediction", "journal": "", "year": "2018", "authors": "Sameh Khamis; Sean Fanello; Christoph Rhemann; Adarsh Kowdle; Julien Valentin; Shahram Izadi"}, {"ref_id": "b22", "title": "A theory of visual control of braking based on information about time-to-collision. Perception", "journal": "", "year": "1976", "authors": "N David;  Lee"}, {"ref_id": "b23", "title": "Two-stage adaptive object scene flow using hybrid cnn-crf model", "journal": "", "year": "2020", "authors": "Congcong Li; Haoyu Ma; Qingmin Liao"}, {"ref_id": "b24", "title": "Neural RGB \u2192 D sensing: Depth and uncertainty from a video camera", "journal": "", "year": "2019", "authors": "Chao Liu; Jinwei Gu; Kihwan Kim; G Srinivasa; Jan Narasimhan;  Kautz"}, {"ref_id": "b25", "title": "Every Pixel Counts ++: Joint learning of geometry and motion with 3D holistic understanding", "journal": "", "year": "2002", "authors": "Chenxu Luo; Zhenheng Yang; Peng Wang; Yang Wang; Wei Xu; Ram Nevatia; Alan L Yuille"}, {"ref_id": "b26", "title": "Consistent video depth estimation", "journal": "In ACM Transactions on Graphics", "year": "2002", "authors": "Xuan Luo; Jia-Bin Huang; Richard Szeliski; Kevin Matzen; Johannes Kopf"}, {"ref_id": "b27", "title": "Deep rigid instance scene flow", "journal": "", "year": "2019", "authors": "Wei-Chiu Ma; Shenlong Wang; Rui Hu; Yuwen Xiong; Raquel Urtasun"}, {"ref_id": "b28", "title": "A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation", "journal": "", "year": "2016", "authors": "Nikolaus Mayer; Eddy Ilg; Philip Hausser; Philipp Fischer; Daniel Cremers; Alexey Dosovitskiy; Thomas Brox"}, {"ref_id": "b29", "title": "Object scene flow for autonomous vehicles", "journal": "", "year": "2008", "authors": "Moritz Menze; Andreas Geiger"}, {"ref_id": "b30", "title": "Object scene flow", "journal": "ISPRS Journal of Photogrammetry and Remote Sensing (JPRS)", "year": "2018", "authors": "Moritz Menze; Christian Heipke; Andreas Geiger"}, {"ref_id": "b31", "title": "Estimation of timeto-collision maps from first order motion models and normal flows", "journal": "", "year": "", "authors": "Fran\u00e7ois Meyer; Patrick Bouthemy"}, {"ref_id": "b32", "title": "Time-to-collision from first-order models of the motion field", "journal": "IEEE Transactions on Robotics and Automation", "year": "1994", "authors": "G Fran\u00e7ois;  Meyer"}, {"ref_id": "b33", "title": "Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer", "journal": "", "year": "", "authors": "Ren\u00e9 Ranftl; Katrin Lasinger; David Hafner; Konrad Schindler; Vladlen Koltun"}, {"ref_id": "b34", "title": "A taxonomy and evaluation of dense two-frame stereo correspondence algorithms", "journal": "International Journal of Computer Vision (IJCV)", "year": "2002", "authors": "Daniel Scharstein; Richard Szeliski"}, {"ref_id": "b35", "title": "Structure-from-motion revisited", "journal": "", "year": "2016", "authors": "Johannes Lutz Sch\u00f6nberger; Jan-Michael Frahm"}, {"ref_id": "b36", "title": "Dense scene flow from stereo disparity and optical flow", "journal": "", "year": "2018", "authors": "Ren\u00e9 Schuster; Oliver Wasenm\u00fcller; Didier Stricker"}, {"ref_id": "b37", "title": "Bounds on time-to-collision and rotational component from first-order derivatives of image flow. Computer Vision, Graphics, and Image Processing", "journal": "", "year": "1990", "authors": "Muralidhara Subbarao"}, {"ref_id": "b38", "title": "Empirical and theoretical issues in the perception of time to contact", "journal": "", "year": "1991", "authors": " James R Tresilian"}, {"ref_id": "b39", "title": "DeMoN: Depth and motion network for learning monocular stereo", "journal": "", "year": "2017", "authors": "Benjamin Ummenhofer; Huizhong Zhou; Jonas Uhrig; Nikolaus Mayer; Eddy Ilg; Alexey Dosovitskiy; Thomas Brox"}, {"ref_id": "b40", "title": "3D scene flow estimation with a piecewise rigid scene model", "journal": "International Journal of Computer Vision (IJCV)", "year": "2008", "authors": "Christoph Vogel; Konrad Schindler; Stefan Roth"}, {"ref_id": "b41", "title": "Scale invariant optical flow", "journal": "", "year": "2012", "authors": "Li Xu; Zhenlong Dai; Jiaya Jia"}, {"ref_id": "b42", "title": "Upgrading optical flow to 3D scene flow through optical expansion", "journal": "", "year": "2008", "authors": "Gengshan Yang; Deva Ramanan"}, {"ref_id": "b43", "title": "GeoNet: Unsupervised learning of dense depth, optical flow and camera pose", "journal": "", "year": "2002", "authors": "Zhichao Yin; Jianping Shi"}, {"ref_id": "b44", "title": "Novel view synthesis of dynamic scenes with globally coherent depths from a monocular camera", "journal": "", "year": "2002", "authors": "Jae Shin Yoon; Kihwan Kim; Orazio Gallo; Hyun Soo Park; Jan Kautz"}, {"ref_id": "b45", "title": "GA-Net: Guided aggregation net for endto-end stereo matching", "journal": "", "year": "2008", "authors": "Feihu Zhang; Victor Prisacariu; Ruigang Yang; Philip H S Torr"}, {"ref_id": "b46", "title": "DF-Net: Unsupervised joint learning of depth and flow using cross-task consistency", "journal": "", "year": "2002", "authors": "Yuliang Zou; Zelun Luo; Jia-Bin Huang"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "This work was done while A. Badki was interning at NVIDIA. Project page: https://github.com/NVlabs/BiTTC", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 5 :5Figure5: Optical flow as an auxiliary task. We estimate optical flow as an auxiliary task to improve our TTC estimation. Like for continuous TTC, we estimate optical flow via a series of binary classifications. While optical flow is not a goal for us, this figure shows that our prediction is reasonable.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 7 :7Figure7: Unseen dataset. Our predicted TTC map on Citiscapes[6]. The first row shows an example of a car moving towards us and another where the car in the adjacent lane is speeding away. The bottom two rows show failures due to a road bump and a drastic rotation, respectively.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Throughout our training we randomly sample shifts in the range [\u221299, 99], and scales in the range of [0.5, 1.3]. We use relative weights of 0.8 and 0.2 for binary TTC and binary optical flow task respectively. For training on the KITTI15 dataset, we split our dataset into training (160 examples) and validation (40 examples) sets, following the split provided by Yang and Ramanan [43].", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Comparison on the validation set of KITTI15 for both binary TTC (averaged over a set {\u03b1i} uniformly sampled in the interval \u03c4 \u2208 [0.02s, 2s]) and continuous TTC. Note that PRSM and OSF both use richer input data (stereo vs mono). MiD, motion-indepth, directly evaluates TTC.", "figure_data": "D1-all D2-bg D2-fg D2-all Fl-all SF-all UberATG-DRISF [28] 2.55 2.90 9.73 4.04 4.73 6.31 ACOSF [24] 3.58 3.82 12.74 5.31 5.79 7.90 ISF [2] 4.46 4.88 11.34 5.95 6.22 8.08 Yang&Ramanan [43] 1.81 3.39 8.54 4.25 6.30 8.12Ours1.813.849.394.766.318.50"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Top 5  published methods on the KITTI scene flow benchmark. Our method performs reasonably well, despite not being designed for scene flow, see text.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "I0 I1 I0 I1 \u03b1 < 1 \u03b1 \u2248 1 I0 I \u03b1 i 1 I0 I \u03b1 i 1 \u03b1 < \u03b1 i \u03b1 > \u03b1 i (a) Inputs I0 I1 I0 I1 \u03b1 < 1 \u03b1 \u2248 1 I0 I \u03b1 i 1 I0 I \u03b1 i 1 \u03b1 < \u03b1 i \u03b1 > \u03b1 i (b) Scaled inputs I0 I1 I0 I1 \u03b1 < 1 \u03b1 \u2248 1 I0 I \u03b1 i 1 I0 I \u03b1 i 1 \u03b1 < \u03b1 i \u03b1 > \u03b1 i (c) Temporal geofence", "formula_coordinates": [3.0, 61.01, 71.58, 465.11, 88.76]}, {"formula_id": "formula_1", "formula_text": "\u03c4 = \u2212Z O dZ O dt = \u2212Z O /\u017b O ,(1)", "formula_coordinates": [3.0, 104.21, 542.48, 182.15, 23.78]}, {"formula_id": "formula_2", "formula_text": "= x \u2212 x 0 \u03c4 and\u1e8f = y \u2212 y 0 \u03c4 ,(2)", "formula_coordinates": [3.0, 368.36, 322.49, 176.75, 23.79]}, {"formula_id": "formula_3", "formula_text": "\u03c4 = x \u2212 x 0 u \u2022 T and \u03c4 = y \u2212 y 0 v \u2022 T,(3)", "formula_coordinates": [3.0, 345.76, 419.5, 199.35, 24.03]}, {"formula_id": "formula_4", "formula_text": "O = f S O /Z O ,(4)", "formula_coordinates": [3.0, 399.11, 627.11, 146.0, 12.05]}, {"formula_id": "formula_5", "formula_text": "Z O /\u017b O = \u2212s O /\u1e61 O ,(5)", "formula_coordinates": [3.0, 384.58, 683.17, 160.54, 18.77]}, {"formula_id": "formula_6", "formula_text": "\u03c4 = s O /\u1e61 O .(6)", "formula_coordinates": [4.0, 143.28, 287.66, 143.08, 12.05]}, {"formula_id": "formula_7", "formula_text": "\u03c4 = \u2212Z(t 0 ) Z(t 1 ) \u2212 Z(t 0 ) t 1 \u2212 t 0 = t 1 \u2212 t 0 1 \u2212 Z(t1) Z(t0) .(7)", "formula_coordinates": [4.0, 65.84, 611.09, 220.52, 32.99]}, {"formula_id": "formula_8", "formula_text": "\u03c4 = t 1 \u2212 t 0 1 \u2212 s(t0) s(t1) = t 1 \u2212 t 0 1 \u2212 \u03b1 ,(8)", "formula_coordinates": [4.0, 116.07, 687.38, 170.3, 32.99]}, {"formula_id": "formula_9", "formula_text": "I 0 I 1 f 0 f 1 * \u03c6 \u03a8 GT L Task * \u03c6 \u03a8 L GT TTC binary scaling \u03b1 i B \u03c4 i BCE 1 \u03c4 * >\u03c4 i quantized scaling {\u03b1 i } {B \u03c4 i } BCE {1 \u03c4 * >\u03c4 i } continuous scaling {\u03b1 i } {B \u03c4 i }, \u03c4 BCE, SL1 {1 \u03c4 * >\u03c4 i }, \u03c4 * Optical flow horizontal shifting u i B u i BCE 1 u * >u i vertical shifting v i B v i BCE 1 v * >v i", "formula_coordinates": [4.0, 321.34, 74.13, 218.35, 126.67]}, {"formula_id": "formula_10", "formula_text": "B \u03c4i (x, y) = p(\u03c4 (x, y) > \u03c4 i ; f 0 , f \u03b1i 1 ),(9)", "formula_coordinates": [4.0, 352.31, 701.79, 192.8, 13.61]}, {"formula_id": "formula_11", "formula_text": "p(\u03c4 i < \u03c4 (x, y) \u2264 \u03c4 j ) = B \u03c4i (x, y) \u2212 B \u03c4j (x, y). (10", "formula_coordinates": [5.0, 65.47, 411.27, 216.74, 18.77]}, {"formula_id": "formula_12", "formula_text": ")", "formula_coordinates": [5.0, 282.21, 411.27, 4.15, 12.0]}, {"formula_id": "formula_13", "formula_text": "Q(x, y) = arg max i p \u03c4 i < \u03c4 (x, y) \u2264 \u03c4 i+1 . (11", "formula_coordinates": [5.0, 63.78, 500.18, 218.43, 18.77]}, {"formula_id": "formula_14", "formula_text": ")", "formula_coordinates": [5.0, 282.21, 500.18, 4.15, 12.0]}, {"formula_id": "formula_15", "formula_text": "\u03c4 1 \u03c4 2 \u03c4 * \u03c4 N B \u03c4 i (x 0 , y 0 ) ...", "formula_coordinates": [5.0, 441.97, 79.16, 98.83, 47.49]}, {"formula_id": "formula_16", "formula_text": "AUC(x, y) = i (\u03c4 i+1 \u2212 \u03c4 i ) \u2022 B \u03c4i (x, y). (12", "formula_coordinates": [5.0, 344.69, 211.92, 196.27, 22.31]}, {"formula_id": "formula_17", "formula_text": ")", "formula_coordinates": [5.0, 540.96, 211.92, 4.15, 12.0]}, {"formula_id": "formula_18", "formula_text": "B ui (x, y) = p(u(x, y) > u i ; f 0 , f ui 1 ),(13)", "formula_coordinates": [6.0, 92.5, 324.39, 193.87, 13.61]}, {"formula_id": "formula_19", "formula_text": "[X(t 0 ), Y (t 0 ), Z(t 0 )] \u2192 [X(t 1 ), Y (t 1 ), Z(t 1 )].", "formula_coordinates": [6.0, 50.11, 523.33, 196.23, 17.29]}, {"formula_id": "formula_20", "formula_text": "(\u03c4 (x, y) \u2264 \u03c4 i ) \u2229 (\u03c4 (x, y) > 0),(14)", "formula_coordinates": [6.0, 359.13, 129.59, 185.98, 18.77]}, {"formula_id": "formula_21", "formula_text": "\u03b7 = Z(t 1 ) Z(t 0 ) = 1 \u2212 t 1 \u2212 t 0 \u03c4 .(15)", "formula_coordinates": [6.0, 372.88, 205.51, 172.23, 24.53]}, {"formula_id": "formula_22", "formula_text": "\u03b7(x, y) \u2264 \u03b7 i .(16)", "formula_coordinates": [6.0, 400.62, 281.76, 144.49, 18.77]}, {"formula_id": "formula_23", "formula_text": "MiD = ||log(\u03b7) \u2212 log(\u03b7 GT )|| 1 \u2022 10 4 .(17)", "formula_coordinates": [7.0, 353.04, 631.35, 192.08, 18.91]}, {"formula_id": "formula_24", "formula_text": "[X(t 0 ), Y (t 0 ), Z(t 0 )] \u2192 [X(t 1 ), Y (t 1 ), Z(t 1 )", "formula_coordinates": [8.0, 50.11, 218.89, 236.25, 22.91]}], "doi": ""}