{"title": "Fast and Accurate Least-Mean-Squares Solvers", "authors": "Alaa Maalouf; Dan Feldman; Abba Khoushy; Ave 199", "pub_date": "2020-09-06", "abstract": "Least-mean squares (LMS) solvers such as Linear / Ridge / Lasso-Regression, SVD and Elastic-Net not only solve fundamental machine learning problems, but are also the building blocks in a variety of other methods, such as decision trees and matrix factorizations. We suggest an algorithm that gets a finite set of n d-dimensional real vectors and returns a weighted subset of d + 1 vectors whose sum is exactly the same. The proof in Caratheodory's Theorem (1907) computes such a subset in O(n 2 d 2 ) time and thus not used in practice. Our algorithm computes this subset in O(nd + d 4 log n) time, using O(log n) calls to Caratheodory's construction on small but \"smart\" subsets. This is based on a novel paradigm of fusion between different data summarization techniques, known as sketches and coresets. For large values of d, we suggest a faster construction that takes O(nd) time (linear in the input's size) and returns a weighted subset of O(d) sparsified input points. Here, sparsified point means that some of its entries were replaced by zeroes. As an example application, we show how it can be used to boost the performance of existing LMS solvers, such as those in scikit-learn library, up to x100. Generalization for streaming and distributed (big) data is trivial. Extensive experimental results and complete open source code are also provided.", "sections": [{"heading": "Introduction and Motivation", "text": "Least-Mean-Squares (LMS) solvers are the family of fundamental optimization problems in machine learning and statistics that include linear regression, Principle Component Analysis (PCA), Singular Value Decomposition (SVD), Lasso and Ridge regression, Elastic net, and many more Golub and Reinsch (1971); Jolliffe (2011);Hoerl and Kennard (1970); Seber and Lee (2012); Zou and Hastie (2005); Tibshirani (1996); Safavian and Landgrebe (1991). See formal definition below. First closed form solutions for problems such as linear regression were published by e.g. Pearson Pearson (1900) around 1900 but were probably known before. Nevertheless, today they are still used extensively as building blocks in both academy and industry for normalization Liang et al. (2013); Kang et al. (2011); Afrabandpey et al. (2016), spectral clustering Peng et al. (2015), graph theory Rohe (2018), prediction Copas (1983); Porco et al. (2015), dimensionality reduction Laparra et al. (2015), feature selection Gallagher et al. (2017) and many more; see more examples in Golub and Van Loan (2012).\nLeast-Mean-Squares solver in this paper is an optimization problem that gets as input an n \u00d7 d real matrix A, and another n-dimensional real vector b (possibly the zero vector). It aims to minimize the sum of squared distances from the rows (points) of A to some hyperplane that is represented by its normal or vector of d coefficients x, that is constrained to be in a given set X \u2286 R d :\nmin x\u2208X f ( Ax \u2212 b 2 ) + g(x)\n.\n(1)\nHere, g is called a regularization term. For example: in linear regression X = R d , f (y) = y 2 for every y \u2208 R and g(x) = 0 for every x \u2208 X. In Lasso f (y) = y 2 for every y \u2208 R and g(x) = \u03b1 \u2022 x 1 for every x \u2208 R d and \u03b1 > 0. Such LMS solvers can be computed via the covariance matrix A T A. For example, the solution to linear regression of minimizing Ax \u2212 b 2 is (A T A) \u22121 A T b.", "publication_ref": ["b17", "b20", "b19", "b43", "b39", "b37", "b33", "b27", "b23", "b1", "b34", "b36", "b26", "b16", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Related work", "text": "While there are many LMS solvers and corresponding implementations, there is always a trade-off between their accuracy and running time; see comparison table in Bauckhage (2015) with references therein. The reason is related to the fact that computing the covariance matrix of A can be done essentially in one of two ways: (i) summing the d \u00d7 d outer product a i a T i of the ith row a T i of A over every i, 1 \u2264 i \u2264 n. This is due to the fact that A T A = n i=1 a i a T i , or (ii) factorization of A, e.g. using SVD or the QR decomposition Golub and Reinsch (1971). Numerical issues. Method (i) is easy to implement for streaming rows of A by maintaining only d 2 entries of the covariance matrix for the n vectors seen so far, or maintaining its inverse (A T A) \u22121 as explained e.g. in Golub and Van Loan (2012). This takes O(d 2 ) time for each vector insertion and requires O(d 2 ) memory, which is the same as the desired output covariance matrix. However, every such addition may introduce another numerical error which accumulates over time. This error increases significantly when running the algorithms using 32 bit floating point representation, which is common for GPU computations; see Fig. 2v for example. This solution is similar to maintaining the set of d rows of the matrix DV T , where A = U DV T is the SVD of A, which is not a subset of the original input matrix A but has the same covariance matrix A T A = V D 2 V . A common problem is that to compute (A T A) \u22121 , the matrix A T A must be invertible. This may not be the case due to numerical issues. In algorithms such as Lasso, the input cannot be a covariance matrix, but only a corresponding matrix whose covariance matrix is A T A, that can be computed from the Cholesky decomposition Bjorck (1967) that returns a left triangular matrix A for the given covariance matrix A T A. However, Cholesky decomposition can be applied only on positive-definite matrices, which is not the case even for small numerical errors that are added to A T A. See Section 8 for more details and empirical evidence.\nRunning-time issues. Method (ii) above utilizes factorizations such as SVD, i.e., A = U DV T to compute the covariance matrix via A T A = V D 2 V T or the QR decomposition A = QR to compute A T A = R T Q T QR T = R T R. This approach is known to be much more stable. However, it is much more time consuming: while in theory the running time is O(nd 2 ) as in the first method, the constants that are hidden in the O(\u2022) notation are significantly larger. Moreover, unlike Method (i), it is impossible to compute such factorizations exactly for streaming data Clarkson and Woodruff (2009). Caratheodory's Theorem Carath\u00e9odory (1907) states that every point contained in the convex hull of n points in R d can be represented as a convex combination of a subset of at most d + 1 points, which we call the Caratheodory set; see Section 2 and Fig. 1. This implies that we can maintain a weighted (scaled) set of d 2 + 1 points (rows) whose covariance matrix is the same as A, since (1/n) i a i a T i is the mean of n matrices and thus in the convex hull of their corresponding points in R (d 2 ) ; see Algorithm 2. The fact that we can maintain such a small sized subset of points instead of updating linear combinations of all the n points seen so far, significantly reduces the numerical errors as shown in Fig. 2v. Unfortunately, computing this set from Caratheodory's Theorem takes O(n 2 d 2 ) or O(nd 3 ) time via O(n) calls to an LMS solver. This fact makes it non-practical to use in an LMS solvers, as we aim to do in this work, and may explain the lack of software or source code for this algorithm on the web.\nApproximations via Coresets and Sketches. In the recent decades numerous approximation and data summarization algorithms were suggested to approximate the problem in (1); see e.g. Drineas et al. (2006); ; Clarkson and Woodruff (2017); Maalouf et al. (2019c) and references therein. One possible approach is to compute a small matrix S whose covariance S T S approximates, in some sense, the covariance matrix A T A of the input data A. The term coreset is usually used when S is a weighted (scaled) subset of rows from the n rows of the input matrix. The matrix S is sometimes called a sketch if each rows in S is a linear combination of few or all rows in A, i.e. S = W A for some matrix W \u2208 R s\u00d7n . However, those coresets and sketches usually yield (1 + \u03b5)-multiplicative approximations for Ax 2 2 by Sx 2 2 where the matrix S is of (d/\u03b5) O(1) rows and x may be any vector, or the smallest/largest singular vector of S or A; see lower bounds in Feldman et al. (2010). Moreover, a (1 + \u03b5)-approximation to Ax 2 2 by Sx 2 2 does not guarantee an approximation to the actual entries or eigenvectors of A by S that may be very different.\nAccurately handling big data. The algorithms in this paper return accurate coresets (\u03b5 = 0), which is less common in the literature; see Jubran et al. (2019b) for a brief summary. These algorithms can be used to compute the covariance matrix A T A via a scaled subset of rows from the input matrix A. Such coresets support unbounded stream of input rows using memory that is sub-linear in their size, and also support dynamic/distributed data in parallel. This is by the useful merge-and-reduce property of coresets that allow them to handle big data; see details e.g. in Agarwal et al. (2004). Unlike traditional coresets that pay additional logarithmic multiplicative factors due to the usage of merge-reduce trees and increasing error, the suggested weighted subsets in this paper do not introduce additional error to the resulting compression since they preserve the desired statistics accurately. The actual numerical errors are measured in the experimental results, with analysis that explain the differences.\nA main advantage of a coreset over a sketch is that it preserves sparsity of the input rows Feldman et al. (2016), which usually reduces theoretical running time. Our experiments show, as expected from the analysis, that coresets can also be used to significantly improve the numerical stability of existing algorithms. Another advantage is that the same coreset can be used for parameter tuning over a large set of candidates. In addition to other reasons, this significantly reduces the running time of such algorithms in our experiments; see Section 8.", "publication_ref": ["b3", "b17", "b18", "b5", "b7", "b6", "b12", "b31", "b14", "b22", "b2", "b15"], "figure_ref": ["fig_8", "fig_0", "fig_8"], "table_ref": []}, {"heading": "Our contribution", "text": "A natural question that follows from the previous section is: can we maintain the optimal solution for LMS problems both accurately and fast? We answer this question affirmably by suggesting:\n(i) the first algorithm that computes the Caratheodory set of n input points in O(nd + d 4 log n) time. This is by using a novel approach of coreset/skecthes fusion that is explained in the next section; see Algorithm 1 and Theorem 1. (ii) an algorithm that maintains a (\"coreset\") matrix S \u2208 R (d 2 +1)\u00d7d such that: (a) its set of rows is a scaled subset of rows from A \u2208 R n\u00d7d whose rows are the input points, and (b) the covariance matrices of S and A are the same, i.e., S T S = A T A; see Algorithm 2 and Theorem 4. (iii) a faster, yet potentially less numerically accurate, algorithm for computing a weaker variant of the Caratheodory set for high dimensional data; see Definition 5 and Algorithm 3. This algorithm runs in O(nd) time, which is the optimal time for this task. Using this improved algorithm, a (\"coreset\") matrix S as in (ii) above, whose rows are not a scaled subset of rows from A, can be computed in a faster (optimal) time. (iv) example applications for boosting the performance of existing solvers by running them on the matrix S above or its variants for Linear/Ridge/Lasso Regressions and Elasticnet. (v) extensive experimental results on synthetic and real-world data for common LMS solvers of Scikit-learn library with either CPython or Intel's distribution. Either the running time or numerical stability is improved up to two orders of magnitude. (vi) open code Maalouf et al. (2019b) for our algorithms that we hope will be used for the many other LMS solvers and future research as suggested in our Conclusion section; see Section 9.", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}, {"heading": "Novel approach: Coresets meet Sketches", "text": "As explained in Section 1.1, the covariance matrix A T A of A itself can be considered as a sketch which is relatively less numerically stable to maintain (especially its inverse, as desired by e.g. linear regression). The Caratheodory set, as in Definition 1, that corresponds to the set of outer products of the rows of A is a coreset whose weighted sum yields the covariance matrix A T A. Moreover, it is more numerically stable but takes much more time to compute; see Theorem 2.\nTo this end, we suggest a meta-algorithm that combines these two approaches: sketches and coresets. It may be generalized to other, not-necessarily accurate, \u03b5-coresets and sketches (\u03b5 > 0); see Section 9.\nThe input to our meta-algorithm is 1) a set P of n items, 2) an integer k \u2208 {1, \u2022 \u2022 \u2022 , n} where n is highest numerical accuracy but longest running time, and 3) a pair of coreset and sketch construction schemes for the problem at hand. The output is a coreset for the problem whose construction time is faster than the construction time of the given coreset scheme; see Fig. 1.\nStep I: Compute a balanced partition {P 1 , \u2022 \u2022 \u2022 , P k } of the input set P into k clusters of roughly the same size. While the correctness holds for any such arbitrary partition (e.g. see Algorithm 3), to reduce numerical errors -the best is a partition that minimizes the sum of loss with respect to the problem at hand.\nStep II: Compute a sketch S i for each cluster P i , where i \u2208 {1, \u2022 \u2022 \u2022 , k}, using the input sketch scheme. This step does not return a subset of P as desired, and is usually numerically less stable.\nStep III: Compute a coreset B for the union S = S 1 \u222a \u2022 \u2022 \u2022 \u222a S k of sketches from Step II, using the input coreset scheme. Note that B is not a subset (or coreset) of P .\nStep IV: Compute the union C of clusters in P 1 , \u2022 \u2022 \u2022 , P k that correspond to the selected sketches in Step III, i.e. C = S i \u2208B P i . By definition, C is a coreset for the problem at hand.\nStep V: Recursively compute a coreset for C until a sufficiently small coreset is obtained. This step is used to reduce running time, without selecting k that is too small.\nWe then run an existing solver on the coreset C to obtain a faster accurate solution for P . Algorithm 1 and 3 are special cases of this meta-algorithm, where the sketch is simply the sum of a set of points/matrices, and the coreset is the existing (slow) implementation of the Caratheodory set from Theorem 2. Paper organization. In Section 2 we give our notations, definitions and the current state-of-the-art result. Section 3 presents our main algorithms for efficient computation of the Caratheodory (core-)set and a subset that preserves the inputs covariance matrix, their theorems of correctness and proofs. Later, at section 4, we suggest an algorithm that computes a weaker variant of the Caratheodory set in a faster time, which also results in a faster time algorithm for computing a subset that preserves the inputs covariance. Sections 5, 6, and 7 demonstrate the applications of those algorithms to common LMS solvers and dimensionality reduction algorithms, while Section 8 shows the practical usage of this work using extensive experimental results on both real-world and synthetic data via the Scikit-learn library with either CPython or Intel's Python distributions. We conclude the paper with open problems and future work in Section 9.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Notation and Preliminaries", "text": "For a pair of integers n, d \u2265 1, we denote by R n\u00d7d the set of n \u00d7 d real matrices, and [n] = {1, \u2022 \u2022 \u2022 , n}. To avoid abuse of notation, we use the big O notation where O(\u2022) is a set Cormen et al. (2009). A weighted set is a pair (P, u) where P = {p 1 , \u2022 \u2022 \u2022 , p n } is an ordered finite set in R d , and u : P \u2192 [0, \u221e) is a positive weights function. We sometimes use a matrix notation whose rows contains the elements of P instead of the ordered set notation.\nGiven a point q inside the convex hull of a set of points P , Caratheodory's Theorem proves that there a subset of at most d + 1 points in P whose convex hull also contains q. This geometric definition can be formulated as follows.\nDefinition 1 (Caratheodory set) Let (P, u) be a weighted set of n points in R d such that p\u2208P u(p) = 1. A weighted set (S, w) is called a Caratheodory Set for (P, u) if: (i) S \u2286 P , (ii) its size is |S| \u2264 d + 1, (iii) its weighted mean is the same, p\u2208S w(p) \u2022 p = p\u2208P u(p) \u2022 p, and (iv) its sum of weights is p\u2208S w(p) = 1.\nCaratheodory's Theorem suggests a constructive proof for computing this set in O(n 2 d 2 ) time Carath\u00e9odory (1907); Cook and Webster (1972); see Algorithm 16 along with an overview and full proof in Section A of the Appendix. However, as observed e.g. in Nasser et al. (2015), it can be computed only for the first m = d + 1 points, and then be updated point by point in O(md 2 ) = O(d 3 ) time per point, to obtain O(nd 3 ) overall time. This still takes \u0398(n) calls to a linear system solver that returns x \u2208 R d satisfying Ax = b for a given matrix A \u2208 R (d+1)\u00d7d and vector b \u2208 R d+1 , in O(d 3 ) time per call.\nTheorem 2 (Carath\u00e9odory (1907), Nasser et al. (2015)) A Caratheodory set (S, w) can be computed for any weighted set (P, u) where\np\u2208P u(p) = 1 in t(n, d) \u2208 O(1) \u2022 min n 2 d 2 , nd 3 time.", "publication_ref": ["b11", "b6", "b9", "b32", "b6", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Faster Caratheodory Set", "text": "In this section, we present our main algorithm that reduces the running time for computing a Caratheodory set from O(min n 2 d 2 , nd 3 ) in Theorem 2 to O(nd) for sufficiently large n; see Theorem 3. A visual illustration of the corresponding Algorithm 1 is shown in Fig. 1. As an application, we present a second algorithm, called Caratheodory-Matrix, which computes a small weighted subset of a the given input that has the same covariance matrix as the input matrix; see Algorithm 2.\nTheorem 3 (Caratheodory-Set Booster) Let (P, u) be a weighted set of n points in R d such that p\u2208P u(p) = 1, and k \u2265 d + 2 be an integer. Let (C, w) be the output of a call to Fast-Caratheodory-Set(P, u, k); See Algorithm 1. Let t(k, d) be the time it takes to compute a Caratheodory Set for k points in R d , as in Theorem 2. Then (C, w) is a Caratheodory set of (P, u) that is computed in time\nO nd + t(k, d) \u2022 log n log(k/d) .\nProof See full proof of Theorem 10 in the Appendix.\nTuning Algorithm 1 for the fastest running time.\nTo achieve the fastest running time in Algorithm 1, simple calculations show that when t(k, d) = kd 3 , i.e., when applying the algorithm from Nasser et al. (2015), k = ed is the optimal value (that achieves the fastest running time), and when t(k, d) = k 2 d 2 , i.e., when applying the original Caratheodory algorithm (Algorithm 16 in the Appendix), k = \u221a ed is the value that achieves the fastest running time.\nAlgorithm 1 Fast-Caratheodory-Set(P, u, k); see Theorem 3\nInput : A set P of n points in R d , a (weight) function u : P \u2192 [0, \u221e) such that p\u2208P u(p) = 1\n, and an integer (number of clusters) k \u2208 {1, \u2022 \u2022 \u2022 , n} for the numerical accuracy/speed trade-off. Output: A Caratheodory set of (P, u); see Definition 1.\nP := P \\ {p \u2208 P | u(p) = 0}.\n// Remove all points with zero weight.\nif |P | \u2264 d + 1 then return (P, u) // |P | is already small {P 1 , \u2022 \u2022 \u2022 , P k } := a partition of P into k disjoint subsets (clusters), each contains at most n/k points. for every i \u2208 {1, \u2022 \u2022 \u2022 , k} do \u00b5 i := 1 q\u2208P i u(q) \u2022 p\u2208P i u(p) \u2022 p // the weighted mean of P i u (\u00b5 i ) := p\u2208P i u(p) //\nThe weight of the ith cluster.\n(\u03bc,w\n) := Caratheodory({\u00b5 1 , \u2022 \u2022 \u2022 , \u00b5 k } , u )\n// see Algorithm 16 in the Appendix.\nC :=\n\u00b5 i \u2208\u03bc P i // C\nis the union over all clusters P i \u2286 P whose representative \u00b5 i was chosen for\u03bc. for every \u00b5 i \u2208\u03bc and p \u2208 P i do w(p) :=w (\u00b5 i )u(p)\nq\u2208P i u(q) // assign weight for each point in C (C, w) := Fast-Caratheodory-Set(C, w, k) // recursive call return (C, w)\nAlgorithm 2 Caratheodory-Matrix(A, k); see Theorem 4\nInput : A matrix A = (a 1 | \u2022 \u2022 \u2022 | a n ) T \u2208 R n\u00d7d ,\nand an integer k \u2208 {1, \u2022 \u2022 \u2022 , n} for numerical accuracy/speed trade-off. Output: A matrix S \u2208 R (d 2 +1)\u00d7d whose rows are scaled rows from A, and\nA T A = S T S. for every i \u2208 {1 \u2022 \u2022 \u2022 , n} do Set p i \u2208 R (d 2 )\nas the concatenation of the d 2 entries of a i a T i \u2208 R d\u00d7d . // The order of entries may be arbitrary but the same for all points.\nu(p i ) := 1/n\nP := p i | i \u2208 {1, \u2022 \u2022 \u2022 , n} // P is a set of n vectors in R (d 2 ) . (C, w) := Fast-Caratheodory-Set(P, u, k) // C \u2286 P and |C| = d 2 + 1 by Theorem 3 S := a (d 2 + 1) \u00d7 d matrix whose ith row is n \u2022 w(p i ) \u2022 a T i for every p i \u2208 C. return S", "publication_ref": ["b32"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Caratheodory Matrix", "text": "Theorem 4 Let A \u2208 R n\u00d7d be a matrix, and k \u2265 d 2 + 2 be an integer. Let S \u2208 R (d 2 +1)\u00d7d be the output of a call to Caratheodory-Matrix(A, k); see Algorithm 2. Let t(k, d) be the computation time of Caratheodory (Algorithm 16) given k points in R d . Then\nA T A = S T S. Furthermore, S is computed in O nd 2 + t(k, d 2 ) \u2022 log n log (k/d 2 )) time.\nProof See full proof of Theorem 11 in the Appendix.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sparsified Caratheodory", "text": "The algorithms presented in the previous section managed to compute a lossless compression, which is a subset of the input data that preserves its covariance. As the experimental results in Section 8 show, those algorithms also maintained a very low numerical error, which was either very close or exactly equal to zero. However, their running time has a polynomial dependency on the dimension d, which makes them impractical for some use cases. Therefore, to support high dimensional data, in this section we provide new algorithms which reduce this dependency on d in their running times, by possibly compromising the numerical accuracy. Streaming data is widely common approach for reducing an algorithm's run time dependency on the number of points n, by simply applying the algorithm on chunks of the input, rather than on the entire input at once. The new algorithms utilize the streaming fashion, but rather on the coordinates (dimension) of the input, rather than chunks of the input. On each such dimensions-subset, the algorithms from the previous section are applied.\nThe experiments conducted in Section 8 demonstrate the expected improvement in running time when using those new and improved algorithms. Fortunately, the numerical error in practice of those new algorithms was not much larger compared to their slower (older) version, which was much lower than the numerical error of the competing methods in most cases.\nFor an integer d and an integer k \u2264 d, we define I k \u2286 R d\u00d7d to be the set of all diagonal matrices M \u2208 {0, 1} d\u00d7d which contain only ones and zeros and have exactly k ones and d \u2212 k zeros along its diagonal.\nA Caratheodory set (C, w) of an input weighted set (P, u) requires C to be a subset of P ; see Definition 1. In what follows we define a weaker variant called a k-Sparse Caratheodory Set. Now, C is not necessarily a subset of the input set P . However, we require that every c \u2208 C can obtained by some p \u2208 P after setting d \u2212 k of its entries to zero. A d-Sparse Caratheodory Set is a Caratheodory set.\nDefinition 5 (k-Sparse Caratheodory Set) Let (P, u) be a weighted set of n points in R d such that p\u2208P u(p) = 1, and let k \u2264 d be an integer. A weighted set (C, w) is called a k-Sparse Caratheodory set for (P, u) if: (i) for every c \u2208 C there is p \u2208 P and a diagonal matrix\u0128 \u2208 I k such that c =\u0128p (i.e., c is simply p with some coordinates set to zero), (ii) its size is |C| \u2264 d k \u2022 (k + 1), (iii) its weighted mean is the same, p\u2208C w(p) \u2022 p = p\u2208P u(p) \u2022 p, and (iv) its sum of weights is p\u2208S w(p) = d/k . Algorithm 3 Sparse-Caratheodory-Set(P, u, k 1 , k 2 ); see Theorem 6 // p j contains a subset of the coordinates of p, whose indices are in I j . C := \u2205 for every j \u2208 {1, \u2022 \u2022 \u2022 , k 2 } do P j := p j | p \u2208 P // P j contains all the points of P , when taking only a subset of their coordinates.\nInput : A set P = {p 1 , \u2022 \u2022 \u2022 , p n } \u2286 R d , a weights function u : P \u2192 [0, \u221e) such that p\u2208P u(p) = 1, and two integers k 1 , k 2 for numerical accuracy/speed trade-off such that k 1 \u2208 d k 2 + 2, \u2022 \u2022 \u2022 , n , and k 2 \u2208 {1, \u2022 \u2022 \u2022 , d}. Output: A d/k 2 -Sparse\nu j (p j ) = u(p) for every p \u2208 P .\n(C j , w j ) := Fast-Caratheodory-Set(P j , u j , k 1 ). // C j \u2286 P j and |C j | \u2264 d k 2 + 1 by Theorem 3.\nFor every c \u2208 C j define\u0109 \u2208 R d to be a vector of zeros in the coordinates {1, \u2022 \u2022 \u2022 , d} \\ I j , and plug the coordinates of c into indices I j of\u0109, and let\u0108 j = \u0109 | c \u2208 C j . // transform c back into R d by adding zeros in specific locations.\nw(\u0109) := w j (c) for every c \u2208 C j . // set the weight of the padded vector to be the weight of the original vector.\nC = C \u222a\u0108 j return (C, w)\nTheorem 6 Let (P, u) be a weighted set of n points in R d such that p\u2208P u(p) = 1, and\nk 1 , k 2 , d be three integers such that k 2 \u2208 {1, \u2022 \u2022 \u2022 , d}, d = d k 2\n, and k 1 \u2208 {d + 2, \u2022 \u2022 \u2022 , n}. Let (C, w) be the output of a call to Sparse-Caratheodory-Set(P, u, k 1 , k 2 ); See Algorithm 3. Let t(k 1 , d ) be the time it takes to compute a Caratheodory Set for k 1 points in R d , as in Theorem 2. Then (C, w) is a d -Sparse Caratheodory set of (P, u) that is computed in time\nO nd + t(k 1 , d ) \u2022 k 2 log n log(k 1 /d ) .\nProof See full proof of Theorem 12 in the Appendix.\nTuning Algorithm 1 for the fastest running time.\nTo achieve the fastest running time in Algorithm 3, simple calculations show that plugging, e.g., t(k, d) = kd 3 , i.e., when applying the algorithm from Nasser et al. (2015), k 2 = d and k 1 = 4 yields the optimal running time of O(nd).", "publication_ref": ["b32"], "figure_ref": [], "table_ref": []}, {"heading": "Sparsified Caratheodory Matrix", "text": "Recall that the covariance\nA T A \u2208 R d\u00d7d of a matrix A = (a 1 | \u2022 \u2022 \u2022 | a n ) T \u2208 R n\u00d7d is equal to the sum n i=1 a i a T i .\nUsing the SVD A T A = U DV T of the covariance matrix, one can compute a matrix S = \u221a DV T \u2208 R d\u00d7d of only d rows whose covariance is the same as A, i.e., S T S = A T A. Observe that this process requires computing the sum of n matrices of size d \u00d7 d.\nIn this section, we provide an algorithm which computes such a matrix S by summing over only O(d 2 ) sparse d \u00d7 d matrices. This algorithm requires the same computational time as the previous algorithm, but is more numerically stable due to summing over only a small number of sparse matrices; see Section 8 for such comparisons.\nTheorem 7 Let A \u2208 R n\u00d7d be a matrix, and\nk 1 , k 2 , d be three integers such that k 2 \u2208 1, \u2022 \u2022 \u2022 , d 2 , d = d 2 k 2 , and k 1 \u2208 {d + 2, \u2022 \u2022 \u2022 , n}. Let S \u2208 R d\u00d7d be the output of a call to Sparse-Caratheodory-Matrix(A, k 1 , k 2 ); see Algorithm 4. Let t(k 1 , d ) be the time it takes to compute a Caratheodory Set for k 1 points in R d , as in Theorem 2. Then A T A = S T S. Furthermore, S is computed in O nd 2 + t(k 1 , d ) \u2022 k 2 log n log(k 1 /d ) time.\nProof See full proof of Theorem 13 in the Appendix.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "From Caratheodory to LMS Solvers", "text": "In this section, we first show how Algorithm 2 can be used to boost the running time of LMS solvers (Lasso/Ridge/Linear/Elastic-net regression) without compromising the accuracy at all. Then, in Section 6, we show how to leverage Algorithm 4, instead of Algorithm 2, to boost the running time of LMS solvers potentially even more, in the cost of a potential decrease in numerical accuracy. As the experimental results in Section 8 show, although in some cases Algorithm 4 introduces an additional small numerical error, it still outperforms Algorithm 4 Sparse-Caratheodory-Matrix(A, k 1 , k 2 ); see Theorem 7\nInput :\nA matrix A = (a 1 | \u2022 \u2022 \u2022 | a n ) T \u2208 R n\u00d7d , and two integers k 1 , k 2 for numerical accuracy/speed trade-off such that k 2 \u2208 1, \u2022 \u2022 \u2022 , d 2 and k 1 \u2208 d 2 k 2 + 2, \u2022 \u2022 \u2022 , n . Output: A matrix S \u2208 R d\u00d7d such that A T A = S T S. for every i \u2208 {1 \u2022 \u2022 \u2022 , n} do Set p i \u2208 R d 2 as the column stacking of the d 2 entries of a i a T i \u2208 R d\u00d7d . //\nThe order of entries may be arbitrary but the same for all points.\nu(p i ) := 1/n Before, we remind the reader that LMS solvers use cross validation techniques to select the best hyper parameter values, such as \u03b1 and \u03c1 in table 1. In what follows we first explain about the m-folds cross validation, then we show how to construct a coreset for different LMS solvers while supporting the the m-folds cross validation. m-folds cross validation (CV). We briefly discuss the CV technique which is utilized in common LMS solvers. Given a parameter m and a set of real numbers A, to select the optimal value \u03b1 \u2208 A of the regularization term, the existing Python's LMS solvers partition the rows of A into m folds (subsets) and run the solver m \u2022 |A| times, each run is done on a concatenation of m \u2212 1 folds (subsets) and \u03b1 \u2208 A, and its result is tested on the remaining \"test fold\". Finally, the cross validation returns the parameter (\u03b1 \u2208 A) that yield the optimal (minimal) mean value on the test folds; see Kohavi et al. (1995) for details.\nP := p i | i \u2208 {1, \u2022 \u2022 \u2022 , n} // P is a set of n vectors in R (d 2 ) . (C, w) := Sparse-Caratheodory-Set(P, u, k 1 , k 2 ) //\nFrom Caratheodory Matrix to LMS solvers. As stated in Theorem 4, Algorithm 2 gets an input matrix A \u2208 R n\u00d7d and an integer k > d+1, and returns a matrix S \u2208 R (d 2 +1)\u00d7d of the same covariance A T A = S T S, where k is a parameter for setting the desired numerical accuracy. To \"learn\" a given label vector b \u2208 R n , Algorithm 5 partitions the matrix A = (A | b) into m partitions, computes a subset for each partition that preserves its covariance matrix, and returns the union of subsets as a pair (C, y) where C \u2208 R (m(d+1) 2 +m)\u00d7d and y \u2208 R m(d+1) 2 +m . For m = 1 and every\nx \u2208 R d , Ax \u2212 b = A (x | \u22121) T = (C | y)(x | \u22121) T = Cx \u2212 y ,(2)\nwhere the second and third equalities follow from Theorem 4 and the construction of C, respectively. This enables us to replace the original pair (A, b) by the smaller pair (C, y) for the solvers in Table 1 as in Algorithms 6-9. A scaling factor \u03b2 is also needed in Algorithms 8-9.\nTo support CV with m > 1 folds, Algorithm 5 computes a coreset for each of the m folds (subsets of the data) in Line 4 and concatenates the output coresets in Line 5. Thus, (2) holds similarly for each fold (subset) when m > 1.\nAlgorithm 5 LMS-Coreset(A, b, m, k) Input: A matrix A \u2208 R n\u00d7d , a vector b \u2208 R n , a number (integer) m of cross-validation folds, and an integer k \u2208 {1, \u2022 \u2022 \u2022 , n} that denotes accuracy/speed trade-off. Output: A matrix C \u2208 R O(md 2 )\u00d7d whose rows are scaled rows from A, and a vector y \u2208 R d . \nA := (A | b) // A matrix A \u2208 R n\u00d7(d+1) {A 1 , \u2022 \u2022 \u2022 , A m } := a partition of the rows of A into m matrices, each of size ( n m ) \u00d7 (d + 1) for every i \u2208 {1, \u2022 \u2022 \u2022 , m} do S i := Caratheodory-Matrix(A i , k) // see Algorithm 2 S := (S T 1 | \u2022 \u2022 \u2022 |S T m ) T //\nAlgorithm 8 Lassocv-Boost(A, b, A, m, k) (C, y) := LMS-Coreset(A, b, m, k) \u03b2 := m \u2022 d + 1) 2 + m /n (x, \u03b1) := LassoCV(\u03b2 \u2022 C, \u03b2 \u2022 y, A, m) return (x, \u03b1) Algorithm 9 Elasticcv-Boost(A, b, m, A, \u03c1, k) 1 (C, y) := LMS-Coreset(A, b, m, k) 2 \u03b2 := m \u2022 d + 1) 2 + m /n 3 (x, \u03b1) := ElasticNetCV(\u03b2\u2022C, \u03b2\u2022y, A, \u03c1, m) 4 return (x, \u03b1)", "publication_ref": ["b25"], "figure_ref": [], "table_ref": []}, {"heading": "From Sparse Caratheodory to LMS Solvers", "text": "In this section, we replace Algorithm 5 from the previous section by Algorithms 10, which utilizes Algorithm 4 instead of Algorithm 2 to reduce the running time's polynomial dependency on d. The fastest running time for Algorithms 10, after tuning its parameters, is O(nd 2 ). Algorithm 10 also partitions the input matrix A = (A | b) from the previous section into m folds. It then computes, for each fold, a set of only d rows that maintains the covariance of this fold using Algorithm 4 (instead of the (d + 1) 2 subset of rows from the previous section). The output is the union (C, y) of all those subsets where C \u2208 R md\u00d7d and y \u2208 R md . Therefore, C and y here (i) satisfy (2) for any m \u2265 1, (ii) are smaller than those computed in the previous section, but (iii) they are not a subset of A and b respectively.\nAlgorithm 10 LMS-Coreset++(A, b, m, k 1 , k 2 ) Input: A matrix A \u2208 R n\u00d7d , a vector b \u2208 R n , a number (integer) m of cross-validation folds, and two integers k 1 , k 2 for numerical accuracy/speed trade-off such that\nk 2 \u2208 1, \u2022 \u2022 \u2022 , (d + 1) 2 and k 1 \u2208 (d+1) 2 k 2 + 2, \u2022 \u2022 \u2022 , n.\nOutput: A matrix C \u2208 R O(md)\u00d7d , and a vector y \u2208 R d .\nA := (A | b) // A matrix A \u2208 R n\u00d7(d+1) {A 1 , \u2022 \u2022 \u2022 , A m } := a partition of the rows of A into m matrices, each of size ( n m ) \u00d7 (d + 1) for every i \u2208 {1, \u2022 \u2022 \u2022 , m} do S i := Sparse-Caratheodory-Matrix(A i , k 1 , k 2 ) // see Algorithm 4 S := (S T 1 | \u2022 \u2022 \u2022 |S T m )\nT // concatenation of the m matrices into a single matrix of md rows and d + 1 columns C :=the first d columns of S y :=the last column of S return (C, y)\nAlgorithm 11 LinReg-Boost++(A, b, m, k 1 , k 2 ) 1 (C, y) := LMS-Coreset++(A, b, m, k 1 , k 2 ) 2 x * := LinearRegression(C, y) 3 return x * Algorithm 12 Ridgevc-Boost++(A, b, A, m, k 1 , k 2 ) 1 (C, y) := LMS-Coreset++(A, b, m, k 1 , k 2 ) 2 (x, \u03b1) := RidgeCV(C, y, A, m, k 1 , k 2 ) 3 return (x, \u03b1) Algorithm 13 Lassocv-Boost++(A, b, A, m, k 1 , k 2 ) (C, y) := LMS-Coreset++(A, b, m, k 1 , k 2 ) \u03b2 := md n (x, \u03b1) := LassoCV(\u03b2 \u2022 C, \u03b2 \u2022 y, A, m) return (x, \u03b1) Algorithm 14 Elasticv-Boost++(A, b, m, A, \u03c1, k 1 , k 2 ) 1 (C, y) := LMS-Coreset++(A, b, m, k 1 , k 2 ) 2 \u03b2 := md n 3 (x, \u03b1) := ElasticNetCV(\u03b2\u2022C, \u03b2\u2022y, A, \u03c1, m) 4 return (x, \u03b1)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Coresets for SVD and PCA", "text": "In this section, we show how to leverage Algorithm 2 in order to construct coresets for dimensionality reduction algorithms such as the widely used Principal Component Analysis (PCA) and Singular Value Decomposition (SVD). We first briefly define the j-SVD and j-PCA problems. We then demonstrate how a coreset for the j-SVD problem can be obtained using Algorithm 2; see Observation 8. Finally, we suggest a coreset construction algorithm for the j-PCA problem; see Algorithm 15 and Observation 9.\nLMS solvers usually support data which is not centralized around the origin. The PCA is closely related to this uncetralized-data case, since it aims to find an affine subspace (does not intersect the origin), which best fits the data. Therefore, a coreset for PCA, as presented in this section, can also serve as a coreset for LMS solvers with uncentralized data. In common coding libraries, such as SKlearn, this property is usually referred to by a flag called fit intercept. j-SVD. In the j-SVD problem, we are given an input matrix A \u2208 R n\u00d7d and an iteger j \u2265 1, and the goal is to compute the linear (non-affine) j-dimensional subspace that minimizes its sum of squared distances to the rows of A. Here, a matrix C \u2208 R m\u00d7d is a coreset for the input matrix A if it satisfies the following pair of properties: (i) The rows of C are scaled rows of A, and (ii) the sum of the squared distances from every (non-affine) j-dimensional subspace to either the rows of C or the rows of A is approximately the same, up to some multiplicative factor. For the coreset to be effective, we aim to compute such C where m n. Formally, let H be a (non-affine) j-dimensional subspace of R d . As explained at Maalouf et al. (2019c), every such subspace H is spanned by the column space of a matrix X \u2208 R d\u00d7j whose columns are orthonormal, i.e., X T X = I j . Given this matrix X, for every i \u2208 {1, \u2022 \u2022 \u2022 , n} the squared distance from the ith row a i of A to H can be written as\na T i \u2212 a T i XX T 2 2 .\nLet Y \u2208 R d\u00d7(d\u2212j) to be a matrix whose columns are mutually orthogonal unit vectors that span the orthogonal complement subspace of H (i.e., Y T Y = I (d\u2212j) and [\nX | Y ] T [X | Y ] = I d ).\nThe squared distance from the ith row a i of A to H can now be written as a i Y 2 2 ; See full details in Section 3 at Maalouf et al. (2019c). Hence, the sum of squared distance from the rows of A to the j-subspace H is equal to\nn i=1 a i Y 2 2 = AY 2 F .\n(3) j-PCA. More generally, in the j-PCA problem, the goal is to compute the affine jdimensional subspace that minimizes its sum of squared distances to the rows of A, over every j-dimensional subspace that may be translated from the origin of R d . Formally, an affine j-dimensional subspace H is represented by a pair (X, ) where X \u2208 R d\u00d7j is an orthogonal matrix, and is a vector in R d that represents the translation of the subspace from the origin. Hence, the sum of squared distance from the rows of A to the affine\nj-dimensional subspace H is n i=1 (a i \u2212 ) \u2212 (a i \u2212 )XX T 2 . (4\n)\nAs above, by letting Y \u2208 R d\u00d7(d\u2212j) be an orthogonal matrix whose rows span the orthogonal complement subspace of H, the sum of squared distances from the rows of A to\nH is now equal to n i=1 (a i \u2212 T )Y 2 2 .\nAlgorithm 15 PCA-CORESET(A, k) Input : A matrix A \u2208 R n\u00d7d , and an integer k \u2208 {1, \u2022 \u2022 \u2022 , n} that denotes accuracy/speed trade-off. Output: A matrix C \u2208 R l\u00d7d whose rows are scaled rows in A, and a weights function w, where l = (d + 1) 2 + 1. See Observation 9.\nl = (d + 1) 2 + 1 A := [A | (1, \u2022 \u2022 \u2022 , 1) T ] S := Caratheodory-Matrix(A , k) Identify the ith row of S by s i = (s T i | z i )\n, where s i \u2208 R d and z i \u2208 R Set C \u2208 R l\u00d7d to be a matrix whose ith row is c\ni := s T i /z i . w(c i ) := z 2 i for every i \u2208 l. return (C, w) Observation 8 (j-SVD coreset) Let A \u2208 R n\u00d7d be a matrix, j \u2208 {1, \u2022 \u2022 \u2022 , d \u2212 1} be an in- teger, and k \u2265 d 2 +2. Let S \u2208 R (d 2 +1\n)\u00d7d be the output of a call to Caratheodory-Matrix(A, k); see Algorithm 2. Then for every matrix\nY \u2208 R d\u00d7(d\u2212j) such that Y T Y = I (d\u2212j) , we have that AY 2 F = SY 2 F .\nProof See full proof of Observation 14 in the Appendix.\nObservation 9 (j-PCA coreset)  d\u00d7(d\u2212j) such that Y T Y = I, and a vector \u2208 R d we have that\nLet A = (a 1 | \u2022 \u2022 \u2022 | a n ) T \u2208\nn i=1 (a i \u2212 T )Y 2 2 = l i=1 w i (c i \u2212 T )Y 2 2 ,\nProof See full proof of Observation 15 in the Appendix.", "publication_ref": ["b31", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results", "text": "In this section we apply our fast construction of the (Sparse) Carathoodory Set S from the previous sections to boost the running time of common LMS solvers in Table 1 by a factor of tens to hundreds, or to improve their numerical accuracy by a similar factor to support, Table 1: Four LMS solvers that were tested with Algorithm 5. Each procedure gets a matrix A \u2208 R n\u00d7d , a vector b \u2208 R n and aims to compute x \u2208 R d that minimizes its objective function. Additional regularization parameters include \u03b1 > 0 and \u03c1 \u2208 [0, 1]. The Python's solvers use m-fold cross validation over every \u03b1 in a given set A \u2286 [0, \u221e).\ne.g., 32 bit floating point representation as in Fig. 2v. This is by running the given solver as a black box on the small matrix C that is returned by Algorithms 6-9 and Algorithms 11-14, which is based on S. That is, our algorithm does not compete with existing solvers but relies on them, which is why we called it a \"booster\". Open code for our algorithms is provided Maalouf et al. (2019b).\nThe experiments. We applied our LMS-Coreset and LMS-Coreset++ coresets from Algorithms 5 and 10 on common Python's SKlearn LMS-solvers that are described in Table 1. Most of these experiments were repeated twice: using the default CPython distribution Wikipedia contributors (2019a) and Intel's distribution LTD ( 2019 The synthetic data consists of an n \u00d7 d matrix A and vector b of length n, both of uniform random entries in [0,1000]. As expected by the analysis, since our compression introduces no error to the computation accuracy, the actual values of the data had no affect on the results, unlike the size of the input which affects the computation time. Table 2 summarizes the experimental results.", "publication_ref": ["b22"], "figure_ref": ["fig_8"], "table_ref": ["tab_1"]}, {"heading": "Competing methods", "text": "We now present other sketches for improving the practical running time of LMS solvers; see discussion in Section 8.2. SKETCH + CHOLESKY is a method which simply sums the 1-rank matrices of outer products of rows in the input matrix A = (A | b) which yields its covariance matrix B = A T A . The Cholesky decomposition B = L T L then returns a small matrix L \u2208 R d\u00d7d that can be plugged to the solvers, similarly to our coreset. SKETCH + SVD is a method which simply sums the 1-rank matrices of outer products of rows in the input matrix A = (A | b), which yields its covariance matrix B = A T A . The SVD decomposition B = U DV T is then applied to return a small matrix \u221a DV T \u2208 R d\u00d7d that can be plugged to the solvers, similarly to our coreset. SKETCH + INVERSE is applied in the special case of linear regression, where one can avoid applying the Cholesky decomposition and can compute the solution (A T A) \u22121 A T b directly after maintaining A T A and A T b for the data seen so far.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discussion", "text": "Practical parameter tuning. As analyzed in Section 4, the theoretically optimal value for k 2 (for Algorithm 3) would be k 2 = d. When considering Algorithms 12-14, where the dimension of the data to be compressed is (d + 1) 2 , it is straightforward that the optimal theoretical value is k 2 = (d + 1) 2 . However, in practice, this might not be the case due to the following tradeoff: a larger value of k 2 in practice means a larger number of calls to the subprocedure Fast-Caratheodory-Set, though the dimension of the data in each call is smaller (i.e., smaller theoretical computational time), and vice versa. In our experiments we found that setting k 2 to be its maximum possible value ((d + 1) 2 ) divided by some constant (12 in our case) yields the fastest running time; see Table 2.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Running time.", "text": "Consider Algorithm 5. The number of rows in the reduced matrix C is O(d 2 ), which is usually much smaller than the number n of rows in the original matrix A. This also explains why some coresets (dashed red line) failed for small values of n in Fig. 2b,2c,2h and 2i. The construction of C takes O(nd 2 + poly(d)). Now consider the improved Algorithm 10. The number of rows in the reduced matrix C is only O(d) and requires only O(nd 2 ) time to compute for some tuning of the parameters as discussed in Section 4. Solving linear regression takes the same time, with or without the coreset. However, the constants hidden in the O notation are much smaller since the time for computing C becomes neglectable for large values of n, as shown in Fig. 2u. We emphasize that, unlike common coresets, there is no accuracy loss due to the use of our coreset, ignoring \u00b110 \u221215 additive errors/improvements. The improvement in running time due to our booster is in order of up to x10 compared to the algorithm's running time on the original data, for both small and large values of the dimension d, as shown in Fig. 2m-2p, and 3m-3n. The contribution of the coreset is significant, already for smaller values of n, when it boosts other solvers that use cross validation for parameter tuning as explained above. In this case, the time complexity reduces by a factor of m \u2022 |A| since the coreset is computed only once for each of the m folds, regardless of the size |A|. In practice, the running time is improved by a factor of x10-x100 as shown for example in Fig. 2a-2c and Fig. 3a-3c. As shown in the graphs, the computations via Intel's Python distribution reduced the running times by 15-40% compared to the default CPython distribution, with or without the booster. This is probably due to its tailored implementation for our hardware.\nFurthermore, as expected, the running time of Algorithm 10 was faster than of Algorithm 5 when tuned appropriately, without much increase in the numerical error.", "publication_ref": [], "figure_ref": ["fig_8", "fig_8", "fig_8", "fig_8"], "table_ref": []}, {"heading": "Numerical stability.", "text": "The SKETCH + CHOLESKY and SKETCH + SVD methods are simple and accurate in theory, and there is no hope to improve their running time via our much more involved booster. However, they are numerically unstable in practice for the reasons that are explained in Section 1.1. In fact, on most of our experiments we could not even apply the SKETCH + CHOLESKY technique at all using 32-bit floating point representation. This is because the resulting approximation to A T A was not a positive definite matrix as required by the Cholesky Decomposition, and we could not compute the matrix L at all. In case of success, the running time of our algorithms was slower by at most a factor of 2 but even in these cases numerical accuracy was improved up to orders of magnitude; See Fig. 2v and 3o for histogram of errors using such 32-bit float representation which is especially common in GPUs for saving memory, running time and power Wikipedia contributors (2019b). This is not surprising, even when considering our (potentially) less numerically accurate algorithm (Algorithm 10). During its cumputation, Algorithm 10 simply sums over only O(d 2 ) terms, where each is a sparse matrix, and then applies SVD, while the most numerically stable competing method SKETCH + SVD sums over n non-sparse matrices and then applies SVD, which makes it less accurate, since the numerical error usually accumulates as we sum over more terms.\nFor the special case of linear regression, we can apply SKETCH + INVERSE, which still has large numerical issues compared to our coreset computation as shown in Fig. 2v  and 3o.", "publication_ref": [], "figure_ref": ["fig_8", "fig_8"], "table_ref": []}, {"heading": "Conclusion and Future Work", "text": "We presented a novel framework that combines sketches and coresets. As an example application, we proved that the set from the Caratheodory Theorem can be computed in O(nd) overall time for sufficiently large n instead of the O(n 2 d 2 ) time as in the original theorem. We then generalized the result for a matrix S whose rows are a weighted subset of the input matrix and their covariance matrix is the same. Our experimental results section shows how to significantly boost the numerical stability or running time of existing LMS solvers by applying them on S. Future work includes: (a) applications of our framework to combine other sketch-coreset pairs e.g. as listed in Phillips ( 2016), (b) Experiments for streaming/distributed/GPU data, and (c) generalization of our approach for more complicated models and applications, e.g., deep learning, decision trees, and many more.   Intel's LTD (2019) distributions were used. The input: A \u2208 R n\u00d7d and b \u2208 R n , where n is \"Data size\". CV used m folds for evaluating each parameter in A. The chosen number of clusters in Algorithm is k = 2(d + 1) 2 + 2. The chosen parameters in Algorithm 10 were set to k 2 = (d + 1) 2 /d and k 1 = 2d + 2, where d is specified in the table. The parameters \u03c1 = 0.5 was used for Algorithms and 14. Computation time includes the computation of the reduced input (C, y); See Sections and 4. The histograms consist of bins along with the number of errors that fall in each bin.\n(a  x * = LinearRegression(A, b).\n) (b) (c) (d) (e) (f ) (g) (h) (i) (j) (k) (l) (m) (n) (o) (p) (q) (r) (s) (t)(\nx was computed using the methods specified in the legend; see Section 8.2. where the second equality is by (6), and the last is by (5). Hence, for every \u03b1 \u2208 R, the weighted mean of P is\nn i=1 u i p i = n i=1 u i p i \u2212 \u03b1 n i=1 v i p i = n i=1 (u i \u2212 \u03b1v i ) p i ,(9)\nwhere the first equality holds since n i=1 v i p i = 0 by (8). The definition of \u03b1 in Line 9 guarantees that \u03b1v i * = u i * for some i * \u2208 [n], and that u i \u2212 \u03b1v i \u2265 0 for every i \u2208 [n]. Hence, the set S that is defined in Line 11 contains at most n \u2212 1 points, and its set of weights {u i \u2212 \u03b1v i } is non-negative. Notice that if \u03b1 = 0, we have that w j = u j > 0 for some j \u2208 [n]. Otherwise, if \u03b1 > 0, by ( 7) there is j \u2208 [n] such that v j < 0, which yields that w j = u j \u2212 \u03b1v j > 0. Hence, in both cases there is w j > 0 for some j \u2208 [n]. Therefore, |S| = \u2205.\nThe sum of the positive weights is thus the total sum of weights,\nn p i \u2208S w i = n i=1 (u i \u2212 \u03b1v i ) = n i=1 u i \u2212 \u03b1 \u2022 n i=1 v i = 1,\nwhere the last equality hold by ( 6), and since u sums to 1. This and ( 9) proves that (S, w) is a Caratheodory set of size n \u2212 1 for (P, u); see Definition 1. In Line 12 we repeat this process recursively until there are at most d + 1 points left in S. For O(n) iterations, the overall time is thus O(n 2 d 2 ).", "publication_ref": ["b28"], "figure_ref": [], "table_ref": []}, {"heading": "Appendix B. Faster Caratheodory Set", "text": "Theorem 10 (Theorem 3) Let (P, u) be a weighted set of n points in R d such that p\u2208P u(p) = 1, and k \u2265 d+2 be an integer. Let (C, w) be the output of a call to Fast-Caratheodory-Set(P, u, k); See Algorithm 1. Let t(k, d) be the time it takes to compute a Caratheodory Set for k points in R d , as in Theorem 2. Then (C, w) is a Caratheodory set of (P, u) that is computed in time\nO nd + t(k, d) \u2022 log n log(k/d)\n.\nProof We use the notation and variable names as defined in Algorithm 1 from Section 3. First, at Line 1 we remove all the points in P which have zero weight, since they do not contribute to the weighted sum. Therefore, we now assume that u(p) > 0 for every p \u2208 P and that |P | = n. Identify the input set P = {p 1 , \u2022 \u2022 \u2022 , p n } and the set C that is computed at Line 9 of Algorithm 1 as C = c 1 , \u2022 \u2022 \u2022 , c |C| . We will first prove that the weighted set (C, w) that is computed in Lines 9-11 at an arbitrary iteration is a Caratheodory set for (P, u), i.e., C \u2286 P , p\u2208P u(p) \u2022 p = p\u2208C w(p) \u2022 p, p\u2208P u(p) = p\u2208C w(p) and |C| \u2264 (d + 1) \u2022 n k . Let (\u03bc,w) be the pair that is computed during the execution the current iteration at Line 8. By Theorem 2 and Algorithm 16, the pair (\u03bc,w) is a Caratheodory set of the weighted set ({\u00b5 1 , \u2022 \u2022 \u2022 , \u00b5 k } , u ). Hence,\n\u00b5 i \u2208\u03bcw (\u00b5 i ) = 1, \u00b5 i \u2208\u03bcw (\u00b5 i )\u00b5 i = k i=1 u (\u00b5 i ) \u2022 \u00b5 i ,\u03bc \u2286 {\u00b5 1 , \u2022 \u2022 \u2022 , \u00b5 k } and |\u03bc| \u2264 d + 1. (10) By the definition of \u00b5 i , for every i \u2208 {1, \u2022 \u2022 \u2022 , k} k i=1 u (\u00b5 i ) \u2022 \u00b5 i = k i=1 u (\u00b5 i ) \u2022 \uf8eb \uf8ed 1 u (\u00b5 i ) \u2022 p\u2208P i u(p) \u2022 p \uf8f6 \uf8f8 = k i=1 p\u2208P i u(p)p = p\u2208P u(p)p. (11)\nBy Line 9 we have that C \u2286 P.\nWe also have that p\u2208C\nw(p)p = \u00b5 i \u2208\u03bc p\u2208P iw (\u00b5 i )u(p) u (\u00b5 i ) \u2022 p = \u00b5 i \u2208\u03bcw (\u00b5 i ) p\u2208P i u(p) u (\u00b5 i ) p = \u00b5 i \u2208\u03bcw (\u00b5 i )\u00b5 i = k i=1 u (\u00b5 i ) \u2022 \u00b5 i = p\u2208P u(p)p,(13)\nwhere the first equality holds by the definitions of C and w, the third equality holds by the definition of \u00b5 i at Line 6, the fourth equality is by (10), and the last equality is by (11).\nThe new sum of weights is equal to p\u2208C\nw(p) = \u00b5 i \u2208\u03bc p\u2208P iw (\u00b5 i )u(p) u (\u00b5 i ) = \u00b5 i \u2208\u03bcw (\u00b5 i ) u (\u00b5 i ) \u2022 p\u2208P i u(p) = \u00b5 i \u2208\u03bcw (\u00b5 i ) u (\u00b5 i ) \u2022u (\u00b5 i ) = \u00b5 i \u2208\u03bcw (\u00b5 i ) = 1,(14)\nwhere the last equality is by (10).\nCombining ( 12), ( 13) and ( 14) yields that the weighted (C, w) computed before the recursive call at Line 13 of the algorithm is a Caratheodory set for the weighted input set (P, u). Since at each iteration we either return such a Caratheodory set (C, w) at Line 13 or return the input weighted set (P, u) itself at Line 3, by induction we conclude that the output weighted set of a call to Fast-Caratheodory-Set(P, u, k) is a Caratheodory set for the original input (P, u).\nBy ( 10 \nlog k d+1 (n) i=1 nd 2 i\u22121 + t(k, d) \u2264 2nd + log k d+1 (n) \u2022 t(k, d) \u2208 O nd + log n log(k/(d + 1)) \u2022 t(k, d) .\nTheorem 11 (Theorem 4) Let A \u2208 R n\u00d7d be a matrix, and k \u2265 d 2 + 2 be an integer. Let S \u2208 R (d 2 +1)\u00d7d be the output of a call to Caratheodory-Matrix(A, k); see Algorithm 2. Let t(k, d) be the computation time of Caratheodory given k point in R d 2 . Then S satisfies that A T A = S T S. Furthermore, S can be computed in O(nd\n2 + t(k, d 2 ) \u2022 log n log (k/d 2 )) ) time.\nProof We use the notation and variable names as defined in Algorithm 2 from Section 3.\nSince (C, w) at Line 5 of Algorithm 2 is the output of a call to Fast-Caratheodory-Set(P, u, k), by Theorem 3 we have that: (i) the weighted means of (C, w) and (P, u) are equal, i.e.,\np\u2208P u(p) \u2022 p = p\u2208C w(p) \u2022 p,(15)\n(ii) |C| \u2264 d 2 + 1 since P \u2286 R (d 2 ) , and (iii) C is computed in O(nd 2 + log k d 2 +1 (n) \u2022 t(k, d 2 ))\ntime.\nCombining ( 15) with the fact that p i is simply the concatenation of the entries of a i a T i , we have that\np i \u2208P u(p i )a i a T i = p i \u2208C w(p i ) \u2022 a i a T i .(16)\nBy the definition of S in Line 6, we have that\nS T S = p i \u2208C ( n \u2022 w(p i ) \u2022 a i )( n \u2022 w(p i ) \u2022 a i ) T = n \u2022 p i \u2208C w(p i ) \u2022 a i a T i .(17)\nWe also have that\nA T A = n i=1 a i a T i = n \u2022 p i \u2208P (1/n)a i a T i = n \u2022 p i \u2208P u(p i )a i a T i ,(18)\nwhere the second derivation holds since u \u2261 1/n. Theorem 4 now holds by combining ( 16), ( 17) and ( 18) as \nS T S = n \u2022 p i \u2208C w(p i ) \u2022 a i a T i = n \u2022 p i \u2208P u(p i )a i a T i = A T A.\nO(nd 2 + t(k, d 2 ) \u2022 log n log (k/d 2 ) ).\nProof We consider the variables from Algorithm 3. At Line 1 we define a partition I 1 , \u2022 \u2022 \u2022 , I k 2 of the coordinates (indices) into k 2 (almost) equal sized subsets, each of size at most d k 2 . Put j \u2208 [k 2 ]. At Line 5, we compute the set P j that contains the entire input points, where each point is restricted to only a subset of its coordinates whose indices are in I j . Each new point p j \u2208 P j \u2286 R |I j | , that contains a subset of the coordinates of some original point p \u2208 P , is assigned a weight u j (p j ) that is equal to the original weight u(p) of p at Line 6. In other words, the weighted set (P j , u j ) is basically a restriction of the input (P, u) to a subset of the coordinates.\nBy Theorem 3, the weighted set (C j , w j ) := Fast-Caratheodory-Set(P j , u j , k 1 ) computed at Line 7 via a call to Algorithm 1 is thus a Caratheodory set of (P j , u j ), where\n|C j | \u2264 |I j | + 1 = d + 1. Therefore, c\u2208\u0108 j w(c) = c\u2208C j w j (c) = p\u2208P j u j (p) = p\u2208P u(p) = 1,(19)\nand\nc\u2208C j w j (c)c = p\u2208P j u j (p)p.(20)\nThen, at Lines 8-9, we plug every c \u2208 C j into a d-dimensional zeros vector\u0109 in the coordinates contained in I j , and assign this new vector the same weight w(\u0109) = w j (c) of c. Combining that the weighted sum of (P j , u j ), which is a subset of the coordinates of P , is equal to the weighted sum of (C j , w j ) (by ( 20)) and the definition of\u0108 j to be the set of padded vectors in C j , we obtain that\nj\u2208[k 2 ] c\u2208\u0108 j w(c)c = p\u2208P u(p)p. (21\n)\nThe output weighted set (C, w) is then simply the union over all the padded vectors in C 1 , \u2022 \u2022 \u2022 ,\u0108 k 2 and their weights. Therefore, c\u2208C w(c) =\nj\u2208[k 2 ] c\u2208\u0108 j w(c) = j\u2208[k 2 ] 1 = k 2 ,\nwhere the second derivation is by ( 19\n), c\u2208C w(c)c = j\u2208[k 2 ] c\u2208\u0108 j w(c)c = p\u2208P u(p)P,\nwhere the second equality is by (21), and\n|C| = j\u2208[k 2 ] |C j | \u2264 j\u2208[k 2 ] (d + 1) = k 2 \u2022 (d + 1) \u2264 d d (d + 1).\nFurthermore, each vector in C is a padded vector of C j \u2286 P j for some j \u2208 [k 2 ], i.e., for every c \u2208 C there is p \u2208 P such that c is a subset of the coordinates of p. Hence, (C, w) is a d k 2 -Sparse Caratheodory set of (P, u).\nThe computation time of (C, w) is dominated by the loop at Line 4. Each iteration among the k 2 iterations of the loop is dominated by the call Fast-Caratheodory-Set(P j , u j , k ) at Line 7. By Theorem 3, since P j is of dimension at most d = d/k 2 by its construction, this call takes O nd + t(k 1 , d ) \u2022 log n log k 1 /d time. The total running time is therefore\nO nd + t(k 1 , d ) \u2022 k 2 log n log(k 1 /d ) as required.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix D. Sparsified Caratheodory Matrix", "text": "Theorem 13 Let A \u2208 R n\u00d7d be a matrix, and k 1 , k 2 , d be three integers such that k 2 \u2208 1, \u2022 \u2022 \u2022 , d 2 , d = d 2 k 2 , and k 1 \u2208 {d + 2, \u2022 \u2022 \u2022 , n}. Let S \u2208 R d\u00d7d be the output of a call to Sparse-Caratheodory-Matrix(A, k 1 , k 2 ); see Algorithm 4. Let t(k 1 , d ) be the time it takes to compute a Caratheodory Set for k 1 points in R d , as in Theorem 2. Then A T A = S T S. Furthermore, S is computed in O nd 2 + t(k 1 , d ) \u2022 k 2 log n log(k 1 /d ) time.\nProof We consider the variables from Algorithm 4. First, note that the covariance matrix is equal to A T A = n i=1 a i a T i . We wish to maintain this sum using a set of only d vectors. To this end, the for loop at Line 1 computes and flattens the d \u00d7 d matrix a i a T i \u2208 R d\u00d7d for every i \u2208 [n] into a vector p i \u2208 R t 2 , and assigns it a weight of 1/n.\nThe call Sparse-Caratheodory-Set(P, u, k 1 , k 2 ) at Line 5 returns a weighted set (C, w) that is a d 2 /k 2 -Sparse Caratheodory set for (P, u); see Theorem 6. Therefore, Combining that C \u2208 R d\u00d7d at Line 7 is a reshaped form of c , with the similar fact that a i a T i \u2208 R d\u00d7d is a reshaped form of p i , we have that\nC = n i=1\na i a T i = A T A.\nLet C = U DV T be the thin Singular Value Decomposition of C . Observe that U = V since C = A T A is a symmetric matrix. By setting S = \u221a DV T \u2208 R d\u00d7d at Line 8, we obtain that\nS T S = V \u221a D \u221a DV T = V DV T = C = A T A.\nWe thus represented the sum A T A = n i=1 a i a T i using an equivalent sum S T S = d i=1 s i s T i over d vectors only, as desired. Appendix E. Corsets for SVD and PCA Observation 14 (Observation 8) Let A \u2208 R n\u00d7d be a matrix, j \u2208 {1, \u2022 \u2022 \u2022 , d \u2212 1} be an integer, and k \u2265 d 2 +2. Let S \u2208 R (d 2 +1)\u00d7d be the output of a call to Caratheodory-Matrix(A, k); see Algorithm 2. Then for every matrix Y \u2208 R d\u00d7(d\u2212j) such that Y T Y = I (d\u2212j) , we have that AY 2\nF = SY 2 F .\nProof Combining the definition of S and Theorem 4, we have that\nA T A = S T S.(22)\nFor any matrix B \u2208 R d\u00d7d let T r(B) denote its trace. Observation 8 now holds as \nwhere the first equality in (24) holds by Observation 8, the first equality in (25) holds since s i = (s T i | z i ) T , and the first equality in ( 26) holds by the definition of c i and w i . Combining ( 27) with ( 23) proves the observation as\nn i=1 (a i \u2212 T )Y 2 2 = l i=1 w i (c i \u2212 T )Y 2 2 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix A. Slow Caratheodory Implementation", "text": "Algorithm 16 Caratheodory(P, u) Input : A weighted set (P, u) of n points in R d . Output: A Caratheodory set (S, w) for (P, u) \nOverview of Algorithm 16 and its correctness. The input is a weighted set (P, u) whose points are denoted by P = {p 1 , \u2022 \u2022 \u2022 , p n }. We assume n > d + 1, otherwise (S, w) = (P, u) is the desired coreset. Hence, the n \u2212 1 > d points p 2 \u2212 p 1 , p 3 \u2212 p 1 , . . . , p n \u2212 p 1 \u2208 R d must be linearly dependent. This implies that there are reals v 2 , \u2022 \u2022 \u2022 , v n , which are not all zeros, such that n i=2 v i (p i \u2212 p 1 ) = 0.\n(5)\nThese reals are computed in Line 7 by solving system of linear equations. This step dominates the running time of the algorithm and takes O(nd 2 ) time using e.g. SVD. The definition\nin Line 8, guarantees that v j < 0 for some j \u2208 [n],\nand that\nAppendix C. Sparsified Caratheodory\nTheorem 12 Let (P, u) be a weighted set of n points in R d such that p\u2208P u(p) = 1, and\n, and k 1 \u2208 {d + 2, \u2022 \u2022 \u2022 , n}. Let (C, w) be the output of a call to Sparse-Caratheodory-Set(P, u, k 1 , k 2 ); See Algorithm 3. Let t(k 1 , d ) be the time it takes to compute a Caratheodory Set for k 1 points in R d , as in Theorem 2. Then (C, w) is a d -Sparse Caratheodory set of (P, u) that is computed in time", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "consumption Data Set", "journal": "", "year": "2012", "authors": ""}, {"ref_id": "b1", "title": "Regression analysis in small-nlarge-p using interactive prior elicitation of pairwise similarities", "journal": "", "year": "2016", "authors": "Homayun Afrabandpey; Tomi Peltola; Samuel Kaski"}, {"ref_id": "b2", "title": "Approximating extent measures of points", "journal": "Journal of the ACM (JACM)", "year": "2004", "authors": "K Pankaj; Sariel Agarwal; Kasturi R Har-Peled;  Varadarajan"}, {"ref_id": "b3", "title": "Numpy/scipy recipes for data science: Ordinary least squares optimization. researchgate. net", "journal": "", "year": "2015-03", "authors": "Christian Bauckhage"}, {"ref_id": "b4", "title": "The million song dataset", "journal": "", "year": "2011", "authors": " Thierry Bertin-Mahieux; P W Daniel; Brian Ellis; Paul Whitman;  Lamere"}, {"ref_id": "b5", "title": "Solving linear least squares problems by gram-schmidt orthogonalization", "journal": "BIT Numerical Mathematics", "year": "1967", "authors": "Ake Bjorck"}, {"ref_id": "b6", "title": "\u00dcber den variabilit\u00e4tsbereich der koeffizienten von potenzreihen, die gegebene werte nicht annehmen", "journal": "Mathematische Annalen", "year": "1907", "authors": "Constantin Carath\u00e9odory"}, {"ref_id": "b7", "title": "Numerical linear algebra in the streaming model", "journal": "ACM", "year": "2009", "authors": "L Kenneth; David P Clarkson;  Woodruff"}, {"ref_id": "b8", "title": "Low-rank approximation and regression in input sparsity time", "journal": "Journal of the ACM (JACM)", "year": "2017", "authors": "L Kenneth; David P Clarkson;  Woodruff"}, {"ref_id": "b9", "title": "Caratheodory's theorem", "journal": "Canadian Mathematical Bulletin", "year": "1972", "authors": "W D Cook;  Webster"}, {"ref_id": "b10", "title": "Regression, prediction and shrinkage", "journal": "Journal of the Royal Statistical Society: Series B (Methodological)", "year": "1983", "authors": "B John;  Copas"}, {"ref_id": "b11", "title": "Introduction to algorithms", "journal": "MIT press", "year": "2009", "authors": "Charles E Thomas H Cormen; Ronald L Leiserson; Clifford Rivest;  Stein"}, {"ref_id": "b12", "title": "Sampling algorithms for l 2 regression and applications", "journal": "Society for Industrial and Applied Mathematics", "year": "2006", "authors": "Petros Drineas; W Michael; Shan Mahoney;  Muthukrishnan"}, {"ref_id": "b13", "title": "UCI machine learning repository", "journal": "", "year": "2017", "authors": "Dheeru Dua; Casey Graff"}, {"ref_id": "b14", "title": "Coresets and sketches for high dimensional subspace approximation problems", "journal": "", "year": "2010", "authors": "Dan Feldman; Morteza Monemizadeh; Christian Sohler; David P Woodruff"}, {"ref_id": "b15", "title": "Dimensionality reduction of massive sparse datasets using coresets", "journal": "", "year": "2016", "authors": "Dan Feldman; Mikhail Volkov; Daniela Rus"}, {"ref_id": "b16", "title": "Cross-spectral factor analysis", "journal": "", "year": "2017", "authors": "Neil Gallagher; R Kyle; Austin Ulrich; Kafui Talbot; Lawrence Dzirasa; David E Carin;  Carlson"}, {"ref_id": "b17", "title": "Singular value decomposition and least squares solutions", "journal": "Springer", "year": "1971", "authors": "H Gene; Christian Golub;  Reinsch"}, {"ref_id": "b18", "title": "", "journal": "Matrix computations", "year": "2012", "authors": "H Gene; Charles F Golub;  Van Loan"}, {"ref_id": "b19", "title": "Ridge regression: Biased estimation for nonorthogonal problems", "journal": "Technometrics", "year": "1970", "authors": "E Arthur; Robert W Hoerl;  Kennard"}, {"ref_id": "b20", "title": "Principal component analysis", "journal": "Springer", "year": "2011", "authors": "Ian Jolliffe"}, {"ref_id": "b21", "title": "Provable approximations for constrained lp regression", "journal": "", "year": "2019", "authors": "Ibrahim Jubran; David Cohn; Dan Feldman"}, {"ref_id": "b22", "title": "Introduction to coresets", "journal": "", "year": "2019", "authors": "Ibrahim Jubran; Alaa Maalouf; Dan Feldman"}, {"ref_id": "b23", "title": "Scalable kernel k-means via centroid approximation", "journal": "", "year": "2011", "authors": "Byung Kang; Woosang Lim; Kyomin Jung"}, {"ref_id": "b24", "title": "Building accurate 3d spatial networks to enable next generation intelligent transportation systems", "journal": "IEEE", "year": "2013", "authors": "Manohar Kaul; Bin Yang; Christian S Jensen"}, {"ref_id": "b25", "title": "A study of cross-validation and bootstrap for accuracy estimation and model selection", "journal": "", "year": "1995", "authors": "Ron Kohavi"}, {"ref_id": "b26", "title": "Dimensionality reduction via regression in hyperspectral imagery", "journal": "IEEE Journal of Selected Topics in Signal Processing", "year": "2015", "authors": "Valero Laparra; Jes\u00fas Malo; Gustau Camps-Valls"}, {"ref_id": "b27", "title": "Distributed pca and k-means clustering", "journal": "Citeseer", "year": "2013", "authors": "Yingyu Liang; Maria-Florina Balcan; Vandana Kanchanapally"}, {"ref_id": "b28", "title": "Accelerate python* performance", "journal": "", "year": "2019", "authors": "Ltd Intel"}, {"ref_id": "b29", "title": "Fast and accurate least-mean-squares solvers", "journal": "", "year": "2019", "authors": "Alaa Maalouf; Ibrahim Jubran; Dan Feldman"}, {"ref_id": "b30", "title": "Open source code for all the algorithms", "journal": "", "year": "", "authors": "Alaa Maalouf; Ibrahim Jubran; Dan Feldman"}, {"ref_id": "b31", "title": "Tight sensitivity bounds for smaller coresets", "journal": "", "year": "2019", "authors": "Alaa Maalouf; Adiel Statman; Dan Feldman"}, {"ref_id": "b32", "title": "Coresets for kinematic data: From theorems to real-time systems", "journal": "", "year": "2015", "authors": "Soliman Nasser; Ibrahim Jubran; Dan Feldman"}, {"ref_id": "b33", "title": "on the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. The London, Edinburgh, and Dublin Philosophical Magazine", "journal": "Journal of Science", "year": "1900", "authors": "Karl X Pearson"}, {"ref_id": "b34", "title": "Robust subspace clustering via thresholding ridge regression", "journal": "", "year": "2015", "authors": "Xi Peng; Zhang Yi; Huajin Tang"}, {"ref_id": "b35", "title": "", "journal": "", "year": "2016", "authors": "M Jeff;  Phillips"}, {"ref_id": "b36", "title": "Low-rank approximations for predicting voting behaviour", "journal": "", "year": "2015", "authors": "Aldo Porco; Andreas Kaltenbrunner; Vicen\u00e7 G\u00f3mez"}, {"ref_id": "b37", "title": "A survey of decision tree classifier methodology", "journal": "", "year": "1991", "authors": "David S Rasoul Safavian;  Landgrebe"}, {"ref_id": "b38", "title": "Linear regression analysis", "journal": "John Wiley & Sons", "year": "2012", "authors": "A F George; Alan J Seber;  Lee"}, {"ref_id": "b39", "title": "Regression shrinkage and selection via the lasso", "journal": "Journal of the Royal Statistical Society: Series B (Methodological)", "year": "1996", "authors": "Robert Tibshirani"}, {"ref_id": "b40", "title": "Wikipedia contributors. Cpython -Wikipedia, the free encyclopedia", "journal": "", "year": "2019", "authors": ""}, {"ref_id": "b41", "title": "List of nvidia graphics processing units -Wikipedia, the free encyclopedia", "journal": "", "year": "2019", "authors": ""}, {"ref_id": "b42", "title": "Understanding regularized spectral clustering via graph conductance", "journal": "", "year": "2018", "authors": "Yilin Zhang; Karl Rohe"}, {"ref_id": "b43", "title": "Journal of the royal statistical society: series B (statistical methodology", "journal": "", "year": "2005", "authors": "Hui Zou; Trevor Hastie"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Overview of Algorithm 1 and the steps in Section 1.3. Images left to right: Steps I and II (Partition and sketch steps): A partition of the input weighted set of n = 48 points (in blue) into k = 8 equal clusters (in circles) whose corresponding means are \u00b5 , . . . , \u00b5 8 (in red). The mean of P (and these means) is x (in green). Step III (Coreset step): Caratheodory (sub)set of d + 1 = 3 points (bold red) with corresponding weights (in green) is computed only for these k = 8 n means. Step IV (Recover step): the Caratheodory set is replaced by its corresponding original points (dark blue). The remaining points in P (bright blue) are deleted. Step V (Recursive step): Previous steps are repeated until only d + 1 = 3 points remain. This procedure takes O(log n) iterations for k = 2d + 2.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Caratheodory set of (P, u); see Definition 5. {I 1 , \u2022 \u2022 \u2022 , I k 2 } := a partition of the indices {1, \u2022 \u2022 \u2022 , d} into k 2 disjoint subsets, each containing at most d/k 2 indices. For every p \u2208 P and j \u2208 [k 2 ] define p j \u2208 R |I j | as the point containing only the coordinates of p \u2208 P whose indices are in I j .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "see Algorithm 3. C \u2286 P and |C| \u2208 O(d 2 + k 2 ) by Theorem 6 and Definition 5. c := n \u2022 c\u2208C w(c)c \u2208 R d 2 // The weighted sum of (C, w). Set C \u2208 R d\u00d7d as the matrix obtained by reshaping c into a matrix // Inverse column-stacking operation. Set S := \u221a DV T \u2208 R d\u00d7d where C = U DV T is the thin Singular Value Decomposition of C . return S the competing compression algorithms common used in practice, both as of running time and accuracy.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "concatenation of the m matrices into a single matrix of m(d + 1) 2 + m rows and d + 1 columns C :=the first d columns of S y :=the last column of S return (C, y) Algorithm 6 LinReg-Boost(A, b, m, k) 1 (C, y) := LMS-Coreset(A, b, m, k) 2 x * := LinearRegression(C, y) 3 return x * Algorithm 7 Ridgecv-Boost(A, b, A, m, k) 1 (C, y) := LMS-Coreset(A, b, m, k) 2 (x, \u03b1) := RidgeCV(C, y, A, m) 3 return (x, \u03b1)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "R n\u00d7d be a matrix, and j \u2208 {1, \u2022 \u2022 \u2022 , d \u2212 1}, l = (d + 1) 2 + 1, and k \u2265 d 2 + 2 be integers. Let (C, w) be the output of a call to PCA-CORESET(A, k); see Algorithm 15, where C = (c 1 | \u2022 \u2022 \u2022 | c l ) T \u2208 R l\u00d7d and w \u2208 R l . Then for every matrix Y \u2208 R", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "sklearn.linear model ElasticNetCV(A, b, A, \u03c1, m)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": ") of Python. All the experiments were conducted on a standard Lenovo Z70 laptop with an Intel i7-5500U CPU @ 2.40GHZ and 16GB RAM. We used the 3 following real-world datasets from the UCI Machine Learning Repository Dua and Graff (2017): (i) 3D Road Network (North Jutland, Denmark) Kaul et al. (2013). It contains n = 434874 records. We used the d = 2 attributes: \"Longitude\" [Double] and \"Latitude\" [Double] to predict the attribute \"Height in meters\" [Double]. (ii) Individual household electric power consumption dat (2012). It contains n = 2075259 records. We used the d = 2 attributes: \"global active power\" [kilowatt -Double], \"global reactive power\" [kilowatt -Double]) to predict the attribute \"voltage\" [volt -Double]. (iii) House Sales in King County, USA dat (2015). It contains n = 21, 600 records. We used the following d = 8 attributes: \"bedrooms\" [integer], \"sqft living\" [integer], \"sqft lot\" [integer], \"floors\" [integer], \"waterfront\" [boolean], \"sqft above\" [integer], \"sqft basement\" [integer], \"year built\" [integer]) to predict the \"house price\" [integer] attribute. (iv) Year Prediction Million Song Dataset Bertin-Mahieux et al. (2011). It contains n = 515345 records in d = 90 dimensional space. We used the attributes 2 till 90 [Double] to predict the song release year [Integer] (first attribute).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "u) (v) Accuracy comparison. (left): Dataset (i), (right): Dataset (ii). x * = LinearRegression(A, b). x was computed using the methods specified in the legend; see Section 8.2.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 2 :2Figure 2: Experimental results; see Table 2. 20", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Running time: Computing the weighted set (P, u) at Lines 1-4 takes O(nd 2 ) time, since it takes O(d 2 ) time to compute each of the n points in P .By Theorem 3, Line 5 takes O(nd2 + t(k, d 2 ) \u2022 log n log (k/d 2 )) to compute a Caratheodory for the the weighted set (P, u), and finally Line 6 takes O(d 3 ) for building the matrix S. Hence, the overall running time of Algorithm 2 is", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "and |C| \u2208 O(d 2 + k 2 ). To this end, c which is computed at Line 6 satisfies that", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "The running time of Algorithm 4 is dominated by the call to Algorithm 3 at Lines 5 and the computation of the SVD of the matrix C at Line 8. Since P \u2286 R d 2 and |P | = n, the call to Algorithm 4 takes O nd2 + t(k 1 , d ) \u2022 k 2 log n log(k 1 /d ) time by Theorem 6, where d = d 2 /k 2 . Computing the SVD of a d \u00d7 d matrix takes O(d 3 ) time. Therefore, the overall running time is O nd 2 + d 3 + t(k 1 , d ) \u2022 k 2 log n log(k 1 /d ) = O nd 2 + t(k 1 , d ) \u2022 k 2 log n log(k 1 /d )  where the equality holds since d \u2208 O(n).", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "AY 2 F2= T r(Y T (A T A)Y ) = T r(Y T (S T S)Y ) = SY 2 F ,where the second equality is by(22).Observation 15 (Observation 9) Let A = (a 1 | \u2022 \u2022 \u2022 | a n ) T \u2208 R n\u00d7d be a matrix, and j \u2208 {1, \u2022 \u2022 \u2022 , d \u2212 1}, l = (d + 1) 2 + 1, and k \u2265 d 2 + 2 be integers. Let (C, w) be the output of a call to PCA-CORESET(A, k); see Algorithm 15, where C = (c 1 | \u2022 \u2022 \u2022 | c l ) T \u2208 R l\u00d7d and w \u2208 R l . Then for every matrix Y \u2208 R d\u00d7(d\u2212j) such that Y T Y = I, and a vector \u2208 R d we have thatProof Let A = [A | (1, \u2022 \u2022 \u2022 , 1) T ] as defined at Line 2 of Algorithm 15. For every j \u2208 [d\u2212k], let y j be the jth column in Y , and let v j = T y j . We have thati | 1)(y T j | \u2212 T y j ) T ) 2 = d\u2212k j=1 A (y T j | \u2212v j ) T 2 2 , (23)where the last equality holds by the definition of A .Let S be the output of a call to Caratheodory-Matrix(A , k), and let S and w be defined as in Lines 5 and 6 of Algorithm 15. Hence,", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Summary of experimental results. CPython Wikipedia contributors (2019a) and", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "The time complexity of each iteration is n + t(k, d) where n = |P | \u2022 d is the number of points in the current iteration. Thus the total running time of Algorithm 1 is", "figure_data": ") we have that C contains at most (d + 1) clusters from P and at most |C| \u2264(d + 1) \u2022 n k points. Hence, there are at most log k d+1(n) recursive calls before the stoppingcondition in line 2 is satisfied."}], "formulas": [{"formula_id": "formula_0", "formula_text": "min x\u2208X f ( Ax \u2212 b 2 ) + g(x)", "formula_coordinates": [2.0, 246.12, 272.4, 115.08, 16.26]}, {"formula_id": "formula_1", "formula_text": "p\u2208P u(p) = 1 in t(n, d) \u2208 O(1) \u2022 min n 2 d 2 , nd 3 time.", "formula_coordinates": [6.0, 90.0, 560.64, 432.0, 24.61]}, {"formula_id": "formula_2", "formula_text": "O nd + t(k, d) \u2022 log n log(k/d) .", "formula_coordinates": [7.0, 346.88, 148.36, 122.36, 15.83]}, {"formula_id": "formula_3", "formula_text": "Input : A set P of n points in R d , a (weight) function u : P \u2192 [0, \u221e) such that p\u2208P u(p) = 1", "formula_coordinates": [7.0, 90.0, 351.21, 432.0, 27.72]}, {"formula_id": "formula_4", "formula_text": "P := P \\ {p \u2208 P | u(p) = 0}.", "formula_coordinates": [7.0, 90.0, 407.36, 135.51, 9.57]}, {"formula_id": "formula_5", "formula_text": "if |P | \u2264 d + 1 then return (P, u) // |P | is already small {P 1 , \u2022 \u2022 \u2022 , P k } := a partition of P into k disjoint subsets (clusters), each contains at most n/k points. for every i \u2208 {1, \u2022 \u2022 \u2022 , k} do \u00b5 i := 1 q\u2208P i u(q) \u2022 p\u2208P i u(p) \u2022 p // the weighted mean of P i u (\u00b5 i ) := p\u2208P i u(p) //", "formula_coordinates": [7.0, 90.0, 420.87, 432.0, 112.14]}, {"formula_id": "formula_6", "formula_text": ") := Caratheodory({\u00b5 1 , \u2022 \u2022 \u2022 , \u00b5 k } , u )", "formula_coordinates": [7.0, 112.41, 535.22, 187.57, 10.77]}, {"formula_id": "formula_7", "formula_text": "\u00b5 i \u2208\u03bc P i // C", "formula_coordinates": [7.0, 107.18, 565.28, 39.67, 36.16]}, {"formula_id": "formula_8", "formula_text": "q\u2208P i u(q) // assign weight for each point in C (C, w) := Fast-Caratheodory-Set(C, w, k) // recursive call return (C, w)", "formula_coordinates": [7.0, 90.0, 638.29, 344.57, 45.86]}, {"formula_id": "formula_9", "formula_text": "Input : A matrix A = (a 1 | \u2022 \u2022 \u2022 | a n ) T \u2208 R n\u00d7d ,", "formula_coordinates": [8.0, 90.0, 109.04, 224.22, 12.58]}, {"formula_id": "formula_10", "formula_text": "A T A = S T S. for every i \u2208 {1 \u2022 \u2022 \u2022 , n} do Set p i \u2208 R (d 2 )", "formula_coordinates": [8.0, 90.0, 135.59, 420.73, 41.37]}, {"formula_id": "formula_11", "formula_text": "P := p i | i \u2208 {1, \u2022 \u2022 \u2022 , n} // P is a set of n vectors in R (d 2 ) . (C, w) := Fast-Caratheodory-Set(P, u, k) // C \u2286 P and |C| = d 2 + 1 by Theorem 3 S := a (d 2 + 1) \u00d7 d matrix whose ith row is n \u2022 w(p i ) \u2022 a T i for every p i \u2208 C. return S", "formula_coordinates": [8.0, 90.0, 205.7, 430.98, 55.33]}, {"formula_id": "formula_12", "formula_text": "A T A = S T S. Furthermore, S is computed in O nd 2 + t(k, d 2 ) \u2022 log n log (k/d 2 )) time.", "formula_coordinates": [8.0, 90.0, 352.25, 383.24, 15.97]}, {"formula_id": "formula_13", "formula_text": "Input : A set P = {p 1 , \u2022 \u2022 \u2022 , p n } \u2286 R d , a weights function u : P \u2192 [0, \u221e) such that p\u2208P u(p) = 1, and two integers k 1 , k 2 for numerical accuracy/speed trade-off such that k 1 \u2208 d k 2 + 2, \u2022 \u2022 \u2022 , n , and k 2 \u2208 {1, \u2022 \u2022 \u2022 , d}. Output: A d/k 2 -Sparse", "formula_coordinates": [9.0, 90.0, 339.18, 432.0, 60.72]}, {"formula_id": "formula_14", "formula_text": "C = C \u222a\u0108 j return (C, w)", "formula_coordinates": [9.0, 90.0, 643.98, 72.58, 26.06]}, {"formula_id": "formula_15", "formula_text": "k 1 , k 2 , d be three integers such that k 2 \u2208 {1, \u2022 \u2022 \u2022 , d}, d = d k 2", "formula_coordinates": [10.0, 90.0, 110.4, 280.55, 15.77]}, {"formula_id": "formula_16", "formula_text": "O nd + t(k 1 , d ) \u2022 k 2 log n log(k 1 /d ) .", "formula_coordinates": [10.0, 90.0, 169.4, 136.17, 16.55]}, {"formula_id": "formula_17", "formula_text": "A T A \u2208 R d\u00d7d of a matrix A = (a 1 | \u2022 \u2022 \u2022 | a n ) T \u2208 R n\u00d7d is equal to the sum n i=1 a i a T i .", "formula_coordinates": [10.0, 90.0, 328.24, 432.0, 27.72]}, {"formula_id": "formula_18", "formula_text": "k 1 , k 2 , d be three integers such that k 2 \u2208 1, \u2022 \u2022 \u2022 , d 2 , d = d 2 k 2 , and k 1 \u2208 {d + 2, \u2022 \u2022 \u2022 , n}. Let S \u2208 R d\u00d7d be the output of a call to Sparse-Caratheodory-Matrix(A, k 1 , k 2 ); see Algorithm 4. Let t(k 1 , d ) be the time it takes to compute a Caratheodory Set for k 1 points in R d , as in Theorem 2. Then A T A = S T S. Furthermore, S is computed in O nd 2 + t(k 1 , d ) \u2022 k 2 log n log(k 1 /d ) time.", "formula_coordinates": [10.0, 90.0, 465.79, 432.0, 74.97]}, {"formula_id": "formula_19", "formula_text": "A matrix A = (a 1 | \u2022 \u2022 \u2022 | a n ) T \u2208 R n\u00d7d , and two integers k 1 , k 2 for numerical accuracy/speed trade-off such that k 2 \u2208 1, \u2022 \u2022 \u2022 , d 2 and k 1 \u2208 d 2 k 2 + 2, \u2022 \u2022 \u2022 , n . Output: A matrix S \u2208 R d\u00d7d such that A T A = S T S. for every i \u2208 {1 \u2022 \u2022 \u2022 , n} do Set p i \u2208 R d 2 as the column stacking of the d 2 entries of a i a T i \u2208 R d\u00d7d . //", "formula_coordinates": [11.0, 90.0, 109.04, 432.0, 87.46]}, {"formula_id": "formula_20", "formula_text": "P := p i | i \u2208 {1, \u2022 \u2022 \u2022 , n} // P is a set of n vectors in R (d 2 ) . (C, w) := Sparse-Caratheodory-Set(P, u, k 1 , k 2 ) //", "formula_coordinates": [11.0, 90.0, 212.45, 354.75, 27.59]}, {"formula_id": "formula_21", "formula_text": "x \u2208 R d , Ax \u2212 b = A (x | \u22121) T = (C | y)(x | \u22121) T = Cx \u2212 y ,(2)", "formula_coordinates": [11.0, 165.23, 633.29, 356.77, 35.05]}, {"formula_id": "formula_22", "formula_text": "A := (A | b) // A matrix A \u2208 R n\u00d7(d+1) {A 1 , \u2022 \u2022 \u2022 , A m } := a partition of the rows of A into m matrices, each of size ( n m ) \u00d7 (d + 1) for every i \u2208 {1, \u2022 \u2022 \u2022 , m} do S i := Caratheodory-Matrix(A i , k) // see Algorithm 2 S := (S T 1 | \u2022 \u2022 \u2022 |S T m ) T //", "formula_coordinates": [12.0, 90.0, 232.11, 426.68, 68.07]}, {"formula_id": "formula_23", "formula_text": "Algorithm 8 Lassocv-Boost(A, b, A, m, k) (C, y) := LMS-Coreset(A, b, m, k) \u03b2 := m \u2022 d + 1) 2 + m /n (x, \u03b1) := LassoCV(\u03b2 \u2022 C, \u03b2 \u2022 y, A, m) return (x, \u03b1) Algorithm 9 Elasticcv-Boost(A, b, m, A, \u03c1, k) 1 (C, y) := LMS-Coreset(A, b, m, k) 2 \u03b2 := m \u2022 d + 1) 2 + m /n 3 (x, \u03b1) := ElasticNetCV(\u03b2\u2022C, \u03b2\u2022y, A, \u03c1, m) 4 return (x, \u03b1)", "formula_coordinates": [12.0, 90.0, 480.99, 432.0, 88.91]}, {"formula_id": "formula_24", "formula_text": "k 2 \u2208 1, \u2022 \u2022 \u2022 , (d + 1) 2 and k 1 \u2208 (d+1) 2 k 2 + 2, \u2022 \u2022 \u2022 , n.", "formula_coordinates": [13.0, 139.79, 196.75, 263.06, 18.3]}, {"formula_id": "formula_25", "formula_text": "A := (A | b) // A matrix A \u2208 R n\u00d7(d+1) {A 1 , \u2022 \u2022 \u2022 , A m } := a partition of the rows of A into m matrices, each of size ( n m ) \u00d7 (d + 1) for every i \u2208 {1, \u2022 \u2022 \u2022 , m} do S i := Sparse-Caratheodory-Matrix(A i , k 1 , k 2 ) // see Algorithm 4 S := (S T 1 | \u2022 \u2022 \u2022 |S T m )", "formula_coordinates": [13.0, 90.0, 231.64, 426.68, 68.07]}, {"formula_id": "formula_26", "formula_text": "Algorithm 11 LinReg-Boost++(A, b, m, k 1 , k 2 ) 1 (C, y) := LMS-Coreset++(A, b, m, k 1 , k 2 ) 2 x * := LinearRegression(C, y) 3 return x * Algorithm 12 Ridgevc-Boost++(A, b, A, m, k 1 , k 2 ) 1 (C, y) := LMS-Coreset++(A, b, m, k 1 , k 2 ) 2 (x, \u03b1) := RidgeCV(C, y, A, m, k 1 , k 2 ) 3 return (x, \u03b1) Algorithm 13 Lassocv-Boost++(A, b, A, m, k 1 , k 2 ) (C, y) := LMS-Coreset++(A, b, m, k 1 , k 2 ) \u03b2 := md n (x, \u03b1) := LassoCV(\u03b2 \u2022 C, \u03b2 \u2022 y, A, m) return (x, \u03b1) Algorithm 14 Elasticv-Boost++(A, b, m, A, \u03c1, k 1 , k 2 ) 1 (C, y) := LMS-Coreset++(A, b, m, k 1 , k 2 ) 2 \u03b2 := md n 3 (x, \u03b1) := ElasticNetCV(\u03b2\u2022C, \u03b2\u2022y, A, \u03c1, m) 4 return (x, \u03b1)", "formula_coordinates": [13.0, 90.0, 401.18, 442.48, 201.68]}, {"formula_id": "formula_27", "formula_text": "a T i \u2212 a T i XX T 2 2 .", "formula_coordinates": [14.0, 268.98, 380.82, 80.11, 18.1]}, {"formula_id": "formula_28", "formula_text": "X | Y ] T [X | Y ] = I d ).", "formula_coordinates": [14.0, 90.0, 423.48, 432.0, 26.27]}, {"formula_id": "formula_29", "formula_text": "n i=1 a i Y 2 2 = AY 2 F .", "formula_coordinates": [14.0, 254.33, 486.74, 104.37, 33.71]}, {"formula_id": "formula_30", "formula_text": "j-dimensional subspace H is n i=1 (a i \u2212 ) \u2212 (a i \u2212 )XX T 2 . (4", "formula_coordinates": [14.0, 90.0, 613.44, 427.35, 54.37]}, {"formula_id": "formula_31", "formula_text": ")", "formula_coordinates": [14.0, 517.35, 645.72, 4.65, 9.57]}, {"formula_id": "formula_32", "formula_text": "H is now equal to n i=1 (a i \u2212 T )Y 2 2 .", "formula_coordinates": [15.0, 90.0, 94.37, 261.08, 54.47]}, {"formula_id": "formula_33", "formula_text": "l = (d + 1) 2 + 1 A := [A | (1, \u2022 \u2022 \u2022 , 1) T ] S := Caratheodory-Matrix(A , k) Identify the ith row of S by s i = (s T i | z i )", "formula_coordinates": [15.0, 90.0, 244.91, 199.54, 55.66]}, {"formula_id": "formula_34", "formula_text": "i := s T i /z i . w(c i ) := z 2 i for every i \u2208 l. return (C, w) Observation 8 (j-SVD coreset) Let A \u2208 R n\u00d7d be a matrix, j \u2208 {1, \u2022 \u2022 \u2022 , d \u2212 1} be an in- teger, and k \u2265 d 2 +2. Let S \u2208 R (d 2 +1", "formula_coordinates": [15.0, 90.0, 301.11, 432.0, 84.88]}, {"formula_id": "formula_35", "formula_text": "Y \u2208 R d\u00d7(d\u2212j) such that Y T Y = I (d\u2212j) , we have that AY 2 F = SY 2 F .", "formula_coordinates": [15.0, 90.0, 387.42, 432.0, 29.55]}, {"formula_id": "formula_36", "formula_text": "Let A = (a 1 | \u2022 \u2022 \u2022 | a n ) T \u2208", "formula_coordinates": [15.0, 264.68, 473.56, 138.92, 12.59]}, {"formula_id": "formula_37", "formula_text": "n i=1 (a i \u2212 T )Y 2 2 = l i=1 w i (c i \u2212 T )Y 2 2 ,", "formula_coordinates": [15.0, 206.05, 550.44, 200.94, 33.71]}, {"formula_id": "formula_38", "formula_text": ") (b) (c) (d) (e) (f ) (g) (h) (i) (j) (k) (l) (m) (n) (o) (p) (q) (r) (s) (t)(", "formula_coordinates": [20.0, 136.88, 176.67, 336.72, 504.8]}, {"formula_id": "formula_39", "formula_text": "n i=1 u i p i = n i=1 u i p i \u2212 \u03b1 n i=1 v i p i = n i=1 (u i \u2212 \u03b1v i ) p i ,(9)", "formula_coordinates": [26.0, 188.55, 127.83, 333.45, 33.71]}, {"formula_id": "formula_40", "formula_text": "n p i \u2208S w i = n i=1 (u i \u2212 \u03b1v i ) = n i=1 u i \u2212 \u03b1 \u2022 n i=1 v i = 1,", "formula_coordinates": [26.0, 191.82, 288.1, 228.36, 34.72]}, {"formula_id": "formula_41", "formula_text": "O nd + t(k, d) \u2022 log n log(k/d)", "formula_coordinates": [26.0, 238.06, 491.33, 121.81, 24.43]}, {"formula_id": "formula_42", "formula_text": "\u00b5 i \u2208\u03bcw (\u00b5 i ) = 1, \u00b5 i \u2208\u03bcw (\u00b5 i )\u00b5 i = k i=1 u (\u00b5 i ) \u2022 \u00b5 i ,\u03bc \u2286 {\u00b5 1 , \u2022 \u2022 \u2022 , \u00b5 k } and |\u03bc| \u2264 d + 1. (10) By the definition of \u00b5 i , for every i \u2208 {1, \u2022 \u2022 \u2022 , k} k i=1 u (\u00b5 i ) \u2022 \u00b5 i = k i=1 u (\u00b5 i ) \u2022 \uf8eb \uf8ed 1 u (\u00b5 i ) \u2022 p\u2208P i u(p) \u2022 p \uf8f6 \uf8f8 = k i=1 p\u2208P i u(p)p = p\u2208P u(p)p. (11)", "formula_coordinates": [26.0, 95.46, 673.46, 426.55, 34.62]}, {"formula_id": "formula_44", "formula_text": "w(p)p = \u00b5 i \u2208\u03bc p\u2208P iw (\u00b5 i )u(p) u (\u00b5 i ) \u2022 p = \u00b5 i \u2208\u03bcw (\u00b5 i ) p\u2208P i u(p) u (\u00b5 i ) p = \u00b5 i \u2208\u03bcw (\u00b5 i )\u00b5 i = k i=1 u (\u00b5 i ) \u2022 \u00b5 i = p\u2208P u(p)p,(13)", "formula_coordinates": [27.0, 137.02, 208.98, 384.98, 69.79]}, {"formula_id": "formula_45", "formula_text": "w(p) = \u00b5 i \u2208\u03bc p\u2208P iw (\u00b5 i )u(p) u (\u00b5 i ) = \u00b5 i \u2208\u03bcw (\u00b5 i ) u (\u00b5 i ) \u2022 p\u2208P i u(p) = \u00b5 i \u2208\u03bcw (\u00b5 i ) u (\u00b5 i ) \u2022u (\u00b5 i ) = \u00b5 i \u2208\u03bcw (\u00b5 i ) = 1,(14)", "formula_coordinates": [27.0, 108.33, 334.12, 416.26, 42.03]}, {"formula_id": "formula_46", "formula_text": "log k d+1 (n) i=1 nd 2 i\u22121 + t(k, d) \u2264 2nd + log k d+1 (n) \u2022 t(k, d) \u2208 O nd + log n log(k/(d + 1)) \u2022 t(k, d) .", "formula_coordinates": [27.0, 93.57, 552.17, 424.87, 40.42]}, {"formula_id": "formula_47", "formula_text": "2 + t(k, d 2 ) \u2022 log n log (k/d 2 )) ) time.", "formula_coordinates": [27.0, 90.0, 678.94, 432.0, 26.04]}, {"formula_id": "formula_48", "formula_text": "p\u2208P u(p) \u2022 p = p\u2208C w(p) \u2022 p,(15)", "formula_coordinates": [28.0, 244.4, 148.65, 277.6, 22.26]}, {"formula_id": "formula_49", "formula_text": "(ii) |C| \u2264 d 2 + 1 since P \u2286 R (d 2 ) , and (iii) C is computed in O(nd 2 + log k d 2 +1 (n) \u2022 t(k, d 2 ))", "formula_coordinates": [28.0, 90.0, 184.52, 432.0, 19.78]}, {"formula_id": "formula_50", "formula_text": "p i \u2208P u(p i )a i a T i = p i \u2208C w(p i ) \u2022 a i a T i .(16)", "formula_coordinates": [28.0, 225.77, 243.99, 296.24, 25.59]}, {"formula_id": "formula_51", "formula_text": "S T S = p i \u2208C ( n \u2022 w(p i ) \u2022 a i )( n \u2022 w(p i ) \u2022 a i ) T = n \u2022 p i \u2208C w(p i ) \u2022 a i a T i .(17)", "formula_coordinates": [28.0, 144.49, 302.17, 377.51, 25.59]}, {"formula_id": "formula_52", "formula_text": "A T A = n i=1 a i a T i = n \u2022 p i \u2208P (1/n)a i a T i = n \u2022 p i \u2208P u(p i )a i a T i ,(18)", "formula_coordinates": [28.0, 170.4, 360.16, 351.61, 34.72]}, {"formula_id": "formula_53", "formula_text": "S T S = n \u2022 p i \u2208C w(p i ) \u2022 a i a T i = n \u2022 p i \u2208P u(p i )a i a T i = A T A.", "formula_coordinates": [28.0, 174.69, 446.59, 262.62, 25.59]}, {"formula_id": "formula_54", "formula_text": "O(nd 2 + t(k, d 2 ) \u2022 log n log (k/d 2 ) ).", "formula_coordinates": [28.0, 329.55, 541.2, 130.71, 15.97]}, {"formula_id": "formula_55", "formula_text": "|C j | \u2264 |I j | + 1 = d + 1. Therefore, c\u2208\u0108 j w(c) = c\u2208C j w j (c) = p\u2208P j u j (p) = p\u2208P u(p) = 1,(19)", "formula_coordinates": [29.0, 90.0, 248.1, 432.0, 50.92]}, {"formula_id": "formula_56", "formula_text": "c\u2208C j w j (c)c = p\u2208P j u j (p)p.(20)", "formula_coordinates": [29.0, 245.06, 320.21, 276.94, 25.54]}, {"formula_id": "formula_57", "formula_text": "j\u2208[k 2 ] c\u2208\u0108 j w(c)c = p\u2208P u(p)p. (21", "formula_coordinates": [29.0, 239.09, 432.84, 278.07, 24.36]}, {"formula_id": "formula_58", "formula_text": ")", "formula_coordinates": [29.0, 517.15, 432.84, 4.85, 9.57]}, {"formula_id": "formula_59", "formula_text": "j\u2208[k 2 ] c\u2208\u0108 j w(c) = j\u2208[k 2 ] 1 = k 2 ,", "formula_coordinates": [29.0, 262.72, 505.04, 140.16, 24.36]}, {"formula_id": "formula_60", "formula_text": "), c\u2208C w(c)c = j\u2208[k 2 ] c\u2208\u0108 j w(c)c = p\u2208P u(p)P,", "formula_coordinates": [29.0, 209.02, 540.15, 193.96, 48.51]}, {"formula_id": "formula_61", "formula_text": "|C| = j\u2208[k 2 ] |C j | \u2264 j\u2208[k 2 ] (d + 1) = k 2 \u2022 (d + 1) \u2264 d d (d + 1).", "formula_coordinates": [29.0, 159.39, 620.53, 293.21, 30.89]}, {"formula_id": "formula_62", "formula_text": "O nd + t(k 1 , d ) \u2022 k 2 log n log(k 1 /d ) as required.", "formula_coordinates": [30.0, 90.0, 155.9, 192.03, 16.55]}, {"formula_id": "formula_63", "formula_text": "C = n i=1", "formula_coordinates": [30.0, 253.22, 574.09, 40.65, 33.71]}, {"formula_id": "formula_64", "formula_text": "S T S = V \u221a D \u221a DV T = V DV T = C = A T A.", "formula_coordinates": [30.0, 200.33, 652.09, 211.34, 19.34]}, {"formula_id": "formula_65", "formula_text": "F = SY 2 F .", "formula_coordinates": [31.0, 140.15, 284.56, 58.94, 15.24]}, {"formula_id": "formula_66", "formula_text": "A T A = S T S.(22)", "formula_coordinates": [31.0, 275.41, 328.87, 246.59, 12.07]}, {"formula_id": "formula_68", "formula_text": "n i=1 (a i \u2212 T )Y 2 2 = l i=1 w i (c i \u2212 T )Y 2 2 .", "formula_coordinates": [32.0, 206.05, 364.33, 200.94, 33.71]}], "doi": ""}