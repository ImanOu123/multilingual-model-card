{"title": "Adversarially Trained Actor Critic for Offline Reinforcement Learning", "authors": "Ching-An Cheng; Tengyang Xie; Nan Jiang; Alekh Agarwal", "pub_date": "", "abstract": "We propose Adversarially Trained Actor Critic (ATAC), a new model-free algorithm for offline reinforcement learning (RL) under insufficient data coverage, based on the concept of relative pessimism. ATAC is designed as a two-player Stackelberg game: A policy actor competes against an adversarially trained value critic, who finds dataconsistent scenarios where the actor is inferior to the data-collection behavior policy. We prove that, when the actor attains no regret in the twoplayer game, running ATAC produces a policy that provably 1) outperforms the behavior policy over a wide range of hyperparameters that control the degree of pessimism, and 2) competes with the best policy covered by data with appropriately chosen hyperparameters. Compared with existing works, notably our framework offers both theoretical guarantees for general function approximation and a deep RL implementation scalable to complex environments and large datasets. In the D4RL benchmark, ATAC consistently outperforms state-of-the-art offline RL algorithms on a range of continuous control tasks.", "sections": [{"heading": "Introduction", "text": "Online reinforcement learning (RL) has been successfully applied in many simulation domains (Mnih et al., 2015;Silver et al., 2016), demonstrating the promise of solving sequential decision making problems by direct exploratory interactions. However, collecting diverse interaction data is prohibitively expensive or infeasible in many real-world applications such as robotics, healthcare, and conversational agents. Due to these problems' risk-sensitive nature, data can only be collected by behavior policies that satisfy certain baseline performance or safety requirements.\nThe restriction on real-world data collection calls for offline RL algorithms that can reliably learn with historical experiences that potentially have limited coverage over the state-action space. Ideally, an offline RL algorithm should 1) always improve upon the behavior policies that collected the data, and 2) learn from large datasets to outperform any other policy whose state-action distribution is well covered by the data. The first condition is known as safe policy improvement (Fujimoto et al., 2019;Laroche et al., 2019), and the second is a form of learning consistency, that the algorithm makes the best use of the available data.\nIn particular, it is desirable that the algorithm can maintain safe policy improvement across large and anchored hyperparameter choices, a property we call robust policy improvement. Since offline hyperparameter selection is a difficult open question (Paine et al., 2020;Zhang & Jiang, 2021), robust policy improvement ensures the learned policy is always no worse than the baseline behavior policies and therefore can be reliably deployed in risk-sensitive decision making applications. For example, in healthcare, it is only ethical to deploy new treatment policies when we confidently know they are no worse than existing ones. In addition, robust policy improvement makes tuning hyperparameters using additional online interactions possible. While online interactions are expensive, they are not completely prohibited in many application scenarios, especially when the tested policies are no worse than the behavior policy that collected the data in the first place. Therefore, if the algorithm has robust policy improvement, then its performance can potentially be more directly tuned.\nHowever, few existing works possess all the desiderata above. Regarding consistency guarantees, deep offline RL algorithms (e.g. Kumar et al., 2020;Kostrikov et al., 2021) show strong empirical performance, but are analyzed theoretically in highly simplified tabular cases. Theoretical works (Liu et al., 2020;Jin et al., 2021;Xie et al., 2021;Uehara et al., 2021) provide systematic analyses of learning correctness and consistency, but most of them have little empirical evaluation (Liu et al., 2020) or consider only the linear case (Jin et al., 2021;Zanette et al., 2021).\nTurning to the robust policy improvement property, this is relatively rare in state-of-the-art offline RL literature. Behavior regularization approaches (Fujimoto et al., 2019;Kumar arXiv:2202.02446v2 [cs.LG] 5 Jul 2022 . Robust Policy Improvement. ATAC based on relative pessimism improves from behavior policies over a wide range of hyperparameters (\u03b2) that controls the degree of pessimism, and has a known safe policy improvement anchor point at \u03b2 = 0. Thus, we can gradually increase \u03b2 from zero to online tune ATAC, while not violating the performance baseline of the behavior policy. By contrast, offline RL based on absolute pessimism (e.g., Xie et al., 2021) has safe policy improvement only for well-tuned hyperparameters. The differences are most stark in panel (d) where ATAC outperforms behavior for \u03b2 ranging over 3 orders of magnitude (0.01 to 10), compared with the narrow band of choices for absolute pessimism. The plots show the 25 th , 50 th , 75 th percentiles over 10 random seeds. Wu et al., 2019;Laroche et al., 2019;Fujimoto & Gu, 2021) are scalable and show robustness for a broad range of hyperparameters that controls the pessimism degree; however, they are often more conservative, which ultimately limits the policy performance, as their robustness is achieved by a proximity regularization/constraint that ignores the reward information. Some pessimistic algorithms (Liu et al., 2020;Xie et al., 2021) have safe policy improvement guarantees but only for carefully selected hyperparameters. For a more detailed discussion of related works, see Appendix A.\nIn this paper, we propose a new model-free offline RL algorithm, Adversarially Trained Actor Critic (ATAC). Compared with existing works, ATAC 1) enjoys strong theoretical guarantees on robust policy improvement over hyperparameter that controls the pessimism degree and learning consistency for nonlinear function approximators, and 2) has a scalable implementation that can learn with deep neural networks and large datasets.\nATAC is designed based on the concept of relative pessimism, leading to a two-player Stackelberg game formulation of offline RL. We treat the actor policy as the leader that aims to perform well under a follower critic, and adversarially train the critic to find Bellman-consistent (Xie et al., 2021) scenarios where the actor is inferior to the behavior policy. Under standard function-approximation assumptions, we prove that, when the actor attains no regret in the two-player game, ATAC produces a policy that provably outperforms the behavior policies for a large anchored range of hyperparameter choices and is optimal when the offline data covers scenarios visited by an optimal policy.\nWe also provide a practical implementation of ATAC based on stochastic first-order two-timescale optimization. In particular, we propose a new Bellman error surrogate, called double Q residual algorithm (DQRA) loss, which is inspired by a related idea of Wang & Ueda (2021) and com-bines the double Q heuristic (Fujimoto et al., 2018) and the residual algorithm (Baird, 1995) to improve the optimization stability of offline RL. We test ATAC on the D4RL benchmark (Fu et al., 2020), and ATAC consistently outperforms state-of-the-art baselines across multiple continuous-control problems. These empirical results also validate the robust policy improvement property of ATAC (Fig. 1), which makes ATAC suitable for risk sensitive applications. The code is available at https: //github.com/microsoft/ATAC.", "publication_ref": ["b31", "b39", "b16", "b28", "b36", "b27", "b25", "b29", "b19", "b49", "b43", "b29", "b19", "b16", "b49", "b47", "b28", "b14", "b29", "b49", "b49", "b46", "b15", "b3", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "Markov Decision Process We consider RL in a Markov Decision Process (MDP) M, defined by (S, A, P, R, \u03b3). S is the state space, and A is the action space. P : S \u00d7 A \u2192 \u2206(S) is the transition function, where \u2206(\u2022) denotes the probability simplex, R : S \u00d7 A \u2192 [0, R max ] is the reward function, and \u03b3 \u2208 [0, 1) is the discount factor. Without loss of generality, we assume that the initial state of the MDP, s 0 , is deterministic. We use \u03c0 : S \u2192 \u2206(A) to denote the learner's decision-making policy, and J(\u03c0\n) := E[ \u221e t=0 \u03b3 t r t |a t \u223c \u03c0(\u2022|s t )]\nto denote the expected discounted return of \u03c0, with r t = R(s t , a t ).\nThe goal of RL is to find a policy that maximizes J(\u2022). For any policy \u03c0, we define the Q-value function as\nQ \u03c0 (s, a) := E[ \u221e t=0 \u03b3 t r t |(s 0 , a 0 ) = (s, a), a t \u223c \u03c0(\u2022|s t )]. By the boundedness of rewards, we have 0 \u2264 Q \u03c0 \u2264 Rmax 1\u2212\u03b3 =: V max . For a policy \u03c0, the Bellman operator T \u03c0 is defined as (T \u03c0 f ) (s, a) := R(s, a) + \u03b3E s |s,a [f (s , \u03c0)]\n, where f (s , \u03c0) := a \u03c0(a |s )f (s , a ). In addition, we use d \u03c0 to denote the normalized and discounted state-action occupancy measure of the policy \u03c0. That is,\nd \u03c0 (s, a) := (1 \u2212 \u03b3)E[ \u221e t=0 \u03b3 t 1(s t = s, a t = a)|a t \u223c \u03c0(\u2022|s t )].\nWe also use E \u03c0 to denote expectations with respect to d \u03c0 .\nOffline RL The goal of offline RL is to compute good policies based pre-collected offline data without environ-ment interaction. We assume the offline data D consists of N i.i.d. (s, a, r, s ) tuples, where (s, a) \u223c \u00b5, r = R(s, a), s \u223c P(\u2022|s, a). We also assume \u00b5 is the discounted stateaction occupancy of some behavior policy, which we also denote as \u00b5 with abuse of notation (i.e., \u00b5 = d \u00b5 ). We will use a \u223c \u00b5(\u2022|s) to denote actions drawn from this policy, and also (s, a, s ) \u223c \u00b5 to denote (s, a) \u223c \u00b5 and s \u223c P (\u2022|s, a).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Function Approximation", "text": "We assume access to a valuefunction class F \u2286 (S \u00d7 A \u2192 [0, V max ]) to model the Q-functions of policies, and we search for good policies from a policy class \u03a0 \u2286 (S \u2192 \u2206(A)). The combination of F and \u03a0 is commonly used in the literature of actor-critic or policy-based approaches (see, e.g., Bertsekas & Tsitsiklis, 1995;Konda & Tsitsiklis, 2000;Haarnoja et al., 2018). We now recall some standard assumptions on the expressivity of the value function class F which are needed for actor-critic methods, particularly in an offline setting.\nAssumption 1 (Approximate Realizability). For any pol- icy \u03c0 \u2208 \u03a0, min f \u2208F max admissible \u03bd f \u2212 T \u03c0 f 2 2,\u03bd \u2264 \u03b5 F , where admissibilty \u03bd means \u03bd \u2208 {d \u03c0 : \u2200\u03c0 \u2208 \u03a0}.\nAssumption 1 is a weaker form of stating Q \u03c0 \u2208 F, \u2200\u03c0 \u2208 \u03a0. This realizability assumption is the same as the one made by Xie et al. (2021) and is weaker than assuming a small error in \u221e norm (Antos et al., 2008).\nAssumption 2 (Approximate Completeness). For any \u03c0 \u2208 \u03a0 and f \u2208 F, we have min g\u2208F g \u2212 T \u03c0 f 2 2,\u00b5 \u2264 \u03b5 F ,F .\nHere \u2022 2,\u00b5 := E \u00b5 [(\u2022) 2 ] denotes the \u00b5-weighted 2-norm. 1 Again Assumption 2 weakens the typical completeness assumption of T \u03c0 f \u2208 F for all f \u2208 F, which is commonly assumed in the analyses of policy optimization methods with TD-style value function learning. We require the approximation to be good only on the data distribution.", "publication_ref": ["b4", "b24", "b18", "b49", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "A Game Theoretic Formulation of Offline RL with Robust Policy Improvement", "text": "In this section, we introduce the idea of relative pessimism and propose a new Stackelberg game (Von Stackelberg, 2010) formulation of offline RL, which is the foundation of our algorithm ATAC. For clarity, in this section we discuss solution concepts at the population level instead of using samples. This simplification is for highlighting the uncertainty in decision making due to missing coverage in the data distribution \u00b5 that offline RL faces. We will consider the effects of finite sample approximation when we introduce ATAC in Section 4.", "publication_ref": ["b44"], "figure_ref": [], "table_ref": []}, {"heading": "|D|", "text": "(s,a)\u2208D f (s, a) 2 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Stackelberg Game Formulation of Offline RL", "text": "Stackelberg game A Stackelberg game is a sequential game between a leader and a follower. It can be stated as a bilevel optimization problem, min x g(x, y x ), s.t. y x \u2208 argmin y h(x, y) where the leader and the follower are the variables x and y, respectively, and g, h are their objectives.\nThe concept of Stackelberg game has its origins in the economics literature and has been recently applied to design online model-based RL (Rajeswaran et al., 2020) and online actor critic algorithms (Zheng et al., 2021). The use of this formalism in an offline setting here is novel to our knowledge. Stackelberg games also generalize previous minimax formulations (Xie et al., 2021), which correspond to a two-player zero-sum game with h = \u2212g.\nOffline RL as a Stackelberg game Inspired by the minimax offline RL concept by Xie et al. (2021) and the pessimistic policy evaluation procedure by Kumar et al. (2020), we formulate the Stackelberg game for offline RL as a bilevel optimization problem, with the learner policy \u03c0 \u2208 \u03a0 as the leader and a critic f \u2208 F as the follower:\n\u03c0 * \u2208 argmax \u03c0\u2208\u03a0 L \u00b5 (\u03c0, f \u03c0 ) (1) s.t. f \u03c0 \u2208 argmin f \u2208F L \u00b5 (\u03c0, f ) + \u03b2E \u00b5 (\u03c0, f )\nwhere \u03b2 \u2265 0 is a hyperparamter, and\nL \u00b5 (\u03c0, f ) := E \u00b5 [f (s, \u03c0) \u2212 f (s, a)](2)\nE \u00b5 (\u03c0, f ) := E \u00b5 [((f \u2212 T \u03c0 f )(s, a)) 2 ].\n(3) Intuitively, \u03c0 * attempts to maximize the value predicted by f \u03c0 , and f \u03c0 performs a relatively pessimistic policy evaluation of a candidate \u03c0 with respect to the behavior policy \u00b5 (we will show L \u00b5 (\u03c0, f ) aims to estimate (1 \u2212 \u03b3)(J(\u03c0) \u2212 J(\u00b5))). In the definition of f \u03c0 , E \u00b5 (\u03c0, f ) ensures f \u03c0 's (approximate) Bellman-consistency on data and L \u00b5 (\u03c0, f ) promotes pessimism with \u03b2 being the hyperparameter that controls their relative contributions. In the rest of this section, we discuss how the relative pessimistic formulation in Eq.(1) leads to the desired property of robust policy improvement, and compare it to the solution concepts used in the previous offline RL works.", "publication_ref": ["b37", "b54", "b49", "b49", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Relative Pessimism and Robust Policy Improvement", "text": "Our design of the Stackelberg game in Eq.( 1) is motivated by the benefits of robust policy improvement in \u03b2 given by relative pessimism. As discussed in the introduction, such property is particularly valuable to applying offline RL in risk-sensitive applications, because it guarantees the learned policy is no worse than the behavior policy regardless of the hyperparameter choice and allows potentially direct online performance tuning. Note that prior works (e.g., Liu et al., 2020;Xie et al., 2021) have relied on well-chosen hyperparameters to show improvement upon the behavior policy (i.e., safe policy improvement). We adopt the term robust policy improvement here to distinguish from those weaker guarantees. While there are works (Laroche et al., 2019;Fujimoto et al., 2019;Kumar et al., 2019) that provide robust policy improvement in tabular problems, but their heuristic extensions to function approximation lose this guarantee.\nBelow we show that the solution \u03c0 * in Eq.( 1) is no worse than the behavior policy for any \u03b2 \u2265 0 under Assumption 1. This property is because in Eq.(1) the actor is trying to optimize a lower bound on the relative performance (1 \u2212 \u03b3)(J(\u03c0)\u2212J(\u00b5)) for \u03c0, and this lower bound is tight (exactly zero) at the behavior policy \u00b5, for any \u03b2 \u2265 0.\nProposition 3. If Assumption 1 holds with \u03b5 F = 0 and\n\u00b5 \u2208 \u03a0, then L \u00b5 (\u03c0, f \u03c0 ) \u2264 (1 \u2212 \u03b3)(J(\u03c0) \u2212 J(\u00b5)) \u2200\u03c0 \u2208 \u03a0, for any \u03b2 \u2265 0. This implies J( \u03c0 * ) \u2265 J(\u00b5).\nProof. By performance difference lemma (Kakade & Langford, 2002),\nJ(\u03c0) \u2212 J(\u00b5) = 1 1\u2212\u03b3 E \u00b5 [Q \u03c0 (s, \u03c0) \u2212 Q \u03c0 (s, a)]. Therefore, if Q \u03c0 \u2208 F on states of \u00b5, then (1 \u2212 \u03b3)(J(\u03c0) \u2212 J(\u00b5)) = L \u00b5 (\u03c0, Q \u03c0 ) = L \u00b5 (\u03c0, Q \u03c0 ) + \u03b2E(Q \u03c0 , \u03c0) \u2265 L \u00b5 (\u03c0, f \u03c0 ) + \u03b2E(f \u03c0 , \u03c0) \u2265 L \u00b5 (\u03c0, f \u03c0 ), where we use E(\u03c0, Q \u03c0 ) = 0 by definition of Q \u03c0 and E(\u03c0, f ) \u2265 0 for any f \u2208 F. Robust policy improvement follows, as J( \u03c0 * ) \u2212 J(\u00b5) \u2265 L \u00b5 ( \u03c0 * , f \u03c0 * ) \u2265 L \u00b5 (\u00b5, f \u00b5 ) = 0.\nRelative vs. absolute pessimism Our formulation is inspired by the maximin objective of Xie et al. (2021), which optimizes a pessimistic estimate of J(\u03c0) (which we call absolute pessimism) and learns a good policy with a wellchosen value of \u03b2. In contrast, our relative pessimism formulation optimizes the performance of \u03c0 relative to J(\u00b5), i.e., J(\u03c0)\u2212J(\u00b5). As Section 4.1 will show, algorithms based on both formulations enjoy similar optimality guarantees with well-chosen hyperparameters. But absolute pessimism gives policy improvement only for certain hyperparameters, while the relative approach enjoys robust policy improvement for all \u03b2 \u2265 0, which is practically significant.\nImprovement beyond behavior policy It is clear from Proposition 3 that the objective in Eq.(1) results in the optimization of a lower bound on the value gap (1 \u2212 \u03b3)(J(\u03c0) \u2212 J(\u00b5)). On the other hand, for appropriate settings of \u03b2, it turns out that this lower bound is not too loose for any \u03c0 \u2208 \u03a0 such that d \u03c0 is covered by the data distribution \u00b5, as implicitly shown in our detailed theoretical analysis presented in the next section. Consequently, maximizing the objective Eq.(1) generally results in policies that significantly outperform \u00b5 for appropriate choices of \u03b2, as long as the data has support for at least one such policy.\nImitation learning perspective An alternative interpretation of Proposition 3 follows from examining the special case of \u03b2 = 0 (i.e. not using any information of rewards and transitions). In this case, the objective Eq.(1) reduces to the maximin problem: max \u03c0\u2208\u03a0 min f \u2208F L \u00b5 (\u03c0, f ), which always yields robust policy improvement under Assumption 1. More generally, if the function class F is rich enough to approximate all bounded, Lipschitz functions, then the above objective with \u03b2 = 0 resembles behavior cloning to match the occupancy measures of \u03c0 and \u00b5 using an integral probability metric (IPM; M\u00fcller, 1997) (or equivalently, Wasserstein GAN; Arjovsky et al., 2017). With \u03b2 > 0, the algorithm gets more information and thus intuitively can perform better. This perspective shows how our formulation unifies the previously disparate literature on behavior regularization and pessimism.", "publication_ref": ["b29", "b49", "b28", "b16", "b26", "b20", "b49", "b32", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Adversarially Trained Actor Critic", "text": "We design our new model-free offline RL algorithm, Adversarially Trained Actor Critic (ATAC), based on the Stackelberg game of relative pessimism in Section 3. 2 In the following, we first describe a theoretical version of ATAC (Algorithm 1) in Section 4.1, which is based on a no-regret policy optimization oracle and a pessimistic policy evaluation oracle. We discuss its working principles and give theoretical performance guarantees. This theoretical algorithm further serves as a template that provides design principles for implementing ATAC. To show its effectiveness, in Section 4.2, we design Algorithm 2, a practical deep-learning implementation of ATAC. Algorithm 2 is a two-timescale first-order algorithm based on stochastic approximation, and uses a novel Bellman error surrogate (called double-Q residual algorithm loss) for off-policy optimization stability. Later in Section 5, we empirically demonstrate that the principally designed Algorithm 2 outperforms many state-of-the-art offline RL algorithms.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Theory of ATAC with Optimization Oracles", "text": "This section instantiates a version of the ATAC algorithm with abstract optimization oracles for the leader and follower, using the concepts introduced in Section 3. We first define the empirical estimates of L \u00b5 and E \u00b5 as follows. Given a dataset D, we define\nL D (f, \u03c0) := E D [f (s, \u03c0) \u2212 f (s, a)] ,(4)\nand the estimated Bellman error (Antos et al., 2008)\nE D (f, \u03c0) := E D (f (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 \u2212 min f \u2208F E D (f (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0))\n2 .\n(5) 4.1.1. ALGORITHM Using these definitions, Algorithm 1 instantiates a version of the ATAC approach. At a high-level, the k th iteration of 2 One can also use the formulation to design model-based algorithms, by constructing f as a Q-function Q \u03c0 \u03b8 computed from a model parameterized by \u03b8, and using E\u00b5(\u03c0, Q \u03c0 \u03b8 ) to capture the reward and transition errors of the model \u03b8.\nAlgorithm 1 ATAC (Theoretical Version) Input: Batch data D. coefficient \u03b2.\n1: Initialize policy \u03c0 1 as the uniform policy. 2: for k = 1, 2, . . . , K do 3:\nObtain the pessimistic estimation of \u03c0 k as f k ,\nf k \u2190 argmin f \u2208F k L D (f, \u03c0 k ) + \u03b2E D (f, \u03c0 k ).", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "4:", "text": "Compute \u03c0 k+1 by \u03c0 k+1 \u2190 PO(\u03c0 k , f k , D), where PO denotes a no-regret oracle (Def. 4). 5: end for 6: Output\u03c0 := Unif(\u03c0 [1:K] ). uniformly mix \u03c0 1 , . . . , \u03c0 K at the trajectory level the algorithm first finds a critic f k that is maximally pessimistic for the current actor \u03c0 k along with a regularization based on the estimated Bellman error of \u03c0 k (line 3), with a hyperparameter \u03b2 trading off the two terms. The actor \u03c0 k+1 then invokes a no-regret policy optimization oracle to update its policy, given f k (line 4). We now discuss some of the key aspects of the algorithm.\nPolicy optimization with a no-regret oracle In Algorithm 1, the policy optimization step (Line 4) is conducted by calling a no-regret policy optimization oracle (PO). We now define the property we expect from this oracle.\nDefinition 4 (No-regret policy optimization oracle). An algorithm PO is called a no-regret policy optimization oracle if for any sequence of functions 3 f 1 , . . . , f K with f k : S \u00d7 A \u2192 [0, V max ], the policies \u03c0 1 , . . . , \u03c0 K produced by PO satisfy, for any comparator \u03c0 \u2208 \u03a0:\n\u03b5 \u03c0 opt := 1 1\u2212\u03b3 K k=1 E \u03c0 [f k (s, \u03c0) \u2212 f k (s, \u03c0 k )] = o(K).\nThe notion of regret used in Definition 4 nearly corresponds to the standard regret definition in online learning (Cesa-Bianchi & Lugosi, 2006), except that we take an expectation over states as per the occupancy measure of the comparator. Algorithmically, a natural oracle might perform online learning with states and actions sampled from \u00b5 in the offline RL setting. This mismatch of measures between the optimization objective and regret definition is typical in policy optimization literature (see e.g. Kakade & Langford, 2002;Agarwal et al., 2021). One scenario in which we indeed have such an oracle is when PO corresponds to running a no-regret algorithm separately in each state 4 and the policy class is sufficiently rich to approximate the resulting iterates. There is a rich literature on such approaches using mirror-descent style methods (e.g., Neu et al., 2017;Geist et al., 2019), of which a particularly popular instance is soft policy iteration or natural policy gradient (Kakade, 2001) based on multiplicative weight updates (e.g. Even-Dar et al., 2009;Agarwal et al., 2021):\n\u03c0 k+1 (a|s) \u221d \u03c0 k (a|s) exp (\u03b7f k (s, a)) with \u03b7 = log |A| 2V 2 max K\n. This oracle is used by Xie et al. (2021), which leads to the regret bound\n\u03b5 \u03c0 opt \u2264 O Vmax 1\u2212\u03b3 K log |A| .", "publication_ref": ["b6", "b20", "b0", "b35", "b17", "b21", "b11", "b0", "b49"], "figure_ref": [], "table_ref": []}, {"heading": "THEORETICAL GUARANTEES", "text": "We now provide the theoretical analysis of Algorithm 1.\nRecall that with missing coverage, we can only hope to compete with policies whose distributions are wellcovered by data, and we need a quantitative measurement of such coverage. Following Xie et al. ( 2021), we use\nC (\u03bd; \u00b5, F, \u03c0) := max f \u2208F f \u2212T \u03c0 f 2 2,\u03bd f \u2212T \u03c0 f 2 2,\u00b5\nto measure how well a distribution of interest \u03bd (e.g., d \u03c0 ) is covered by the data distribution \u00b5 w.r.t. policy \u03c0 and function class F, which is a sharper measure than the more typical concentrability coefficients ) (e.g., C (\u03bd; \u00b5, F, \u03c0) \u2264 max s,a \u03bd(s, a)/\u00b5(s, a)).\nWe also use d F ,\u03a0 to denote the joint statistical complexity of the policy class \u03a0 and F. For example, when F and \u03a0 are finite, we have d F ,\u03a0 = O(log |F ||\u03a0| /\u03b4), where \u03b4 is a failure probability. Our formal proofs utilize the covering number to address infinite function classes; see Appendix B for details. In addition, we also omit the approximation error terms \u03b5 F and \u03b5 F ,F in the results presented in this section for the purpose of clarity. The detailed results incorporating these terms are provided in Appendix B. Theorem 5 (Informal). Let |D| = N , C \u2265 1 be any constant, \u03bd \u2208 \u2206(S \u00d7 A) be an arbitrarily distribution that satisfies max k\u2208[K] C (\u03bd; \u00b5, F, \u03c0 k ) \u2264 C, and \u03c0 \u2208 \u03a0 be an arbitrary competitor policy. Then, when \u03b5 F = \u03b5 F ,F = 0,\nchoosing \u03b2 = \u0398 3 VmaxN 2 d 2 F ,\u03a0\n, with high probability: At a high-level, our result shows that we can compete with any policy \u03c0 using a sufficiently large dataset, as long as our optimization regret is small and the data distribution \u00b5 has a good coverage for d \u03c0 . In particular, choosing \u03bd = d \u03c0 removes the off-support term, so that we always have a guarantee scaling with max k C (d \u03c0 , \u00b5, F, \u03c0 k ), but can benefit if other distributions \u03bd are better covered with a small off-support mass d \u03c0 \\ \u03bd 1 . The off-support term can also be small if a small Bellman error under \u00b5 generalizes to a small error out of support, due to properties of F.\nJ(\u03c0) \u2212 J(\u03c0) \u2264 \u03b5 \u03c0 opt + O Vmax \u221a C(d F ,\u03a0 ) 1 /3 (1\u2212\u03b3)N 1 /3 + 1 K(1\u2212\u03b3) K k=1 d \u03c0 \\ \u03bd, f k \u2212 T \u03c0 k f k , where (d \u03c0 \\ \u03bd)(s, a) := max(d \u03c0 (s, a) \u2212 \u03bd(s,", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Comparison with prior theoretical results", "text": "To compare our result with prior works, we focus on the two statistical error terms in our bound, ignoring the optimization regret. Relative to the information-theoretic bound of Xie et al. (2021), we observe a similar decomposition into a finite sample deviation term and an off-support bias. Their finite sample error decays as N \u22121/2 as opposed to our N \u22121/3 scaling, which arises from the use of regularization here. Indeed, we can get a N \u22121/2 bound for a constrained version, but such a version is not friendly to practical implementation. Prior linear methods (Jin et al., 2021;Zanette et al., 2021;Uehara et al., 2021) have roughly similar guarantees to Xie et al. (2021), so a similar comparison holds.\nMost related to Theorem 5 is the N \u22121/5 bound of Xie et al. (2021, Corollary 5) for their regularized algorithm PSPI, which is supposed to be computationally tractable though no practical implementation is offered. 5 While our bound is better, we use a bounded complexity \u03a0 while their result uses an unrestricted policy class. If we were to use the same policy class as theirs, the complexity of \u03a0 would grow with optimization iterates, requiring us to carefully balance the regret and deviation terms and yielding identical guarantees to theirs. To summarize, our result is comparable to Xie et al. (2021, Corollary 5) and stated in a more general form, and we enjoy a crucial advantage of robust policy improvement as detailed below.\nRobust policy improvement We now formalize the robust policy improvement of Algorithm 1, which can be viewed as the finite-sample version of Proposition 3. Proposition 6. Let\u03c0 be the output of Algorithm 1. If Assumption 1 holds with \u03b5 F = 0 and \u00b5 \u2208 \u03a0, with high probability,\nJ(\u00b5) \u2212 J(\u03c0) \u2264 O Vmax 1 \u2212 \u03b3 dF,\u03a0 N + \u03b2V 2 max dF,\u03a0 (1 \u2212 \u03b3)N + \u03b5 \u00b5 opt .\nProposition 6 provides the robust policy improvement guarantee in the finite-sample regime, under a weaker assumption on F than that in Theorem 5. In contrast to the regular safe policy improvement results in offline RL (e.g., Xie et al., 2021, Corollary 3) where the pessimistic hyperparamter is required to choose properly, the robust policy improvement from Proposition 6 could adapt to a wide range of \u03b2. As long as \u03b2 = o(N ), the learned policy\u03c0 from Algorithm 1 is guaranteed improve the behavior policy \u00b5 consistently.\nIn fact, for such a range of \u03b2, robust policy improvement holds regardless of the quality of the learned critic. For example, when \u03b2 = 0, Proposition 6 still guarantees a policy no worse than the behavior policy \u00b5, though the critic loss does not contain the Bellman error term anymore. (In this case, ATAC performs IL). In contrast, prior works based on absolute pessimism (e.g., Xie et al., 2021) immediately output degenerate solutions when the Bellman error term is removed.\nAlgorithm 2 ATAC (Practical Version)\nInput: Batch data D, policy \u03c0, critics f 1 , f 2 , constants \u03b2 \u2265 0, \u03c4 \u2208 [0, 1], w \u2208 [0, 1] 1: Initialize target networksf 1 \u2190 f 1 ,f 2 \u2190 f 2 2: for k = 1, 2, . . . , K do 3:\nSample minibatch D mini from dataset D.", "publication_ref": ["b49", "b19", "b43", "b49", "b49"], "figure_ref": [], "table_ref": []}, {"heading": "4:", "text": "For\nf \u2208 {f 1 , f 2 }, update critic networks l critic (f ) := L Dmini (f, \u03c0) + \u03b2E w Dmini (f, \u03c0) f \u2190 Proj F (f \u2212 \u03b7 fast \u2207l critic ) 5: Update actor network l actor (\u03c0) := \u2212L Dmini (f 1 , \u03c0) \u03c0 \u2190 Proj \u03a0 (\u03c0 \u2212 \u03b7 slow \u2207l actor ) 6: For (f,f ) \u2208 {(f i ,f i )} i=1,2 , update target f \u2190 (1 \u2212 \u03c4 )f + \u03c4 f . 7: end for\nIt is also notable that, compared with Theorem 5, Proposition 6 enjoys a better statistical rate with a proper \u03b2, i.e., \u03b2 \u2264 O(N 1/2 ), due to the decomposition of performance difference shown in the following proof sketch.\nProof sketch Theorem 5 is established based on the following decomposition of performance difference: \u2200\u03c0,\n(1 \u2212 \u03b3)(J(\u03c0) \u2212 J(\u03c0 k )) \u2264 E\u00b5 [f k \u2212 T \u03c0 k f k ] \u2212 E\u03c0 [f k \u2212 T \u03c0 k f k ] + E\u03c0 [f k (s, \u03c0) \u2212 f k (s, \u03c0 k )] + O V 2 max N + \u03b2V 2 max N .(6)\nDetails of this decomposition can be found in Appendix B.2, and the proof relies on the fact that f k is obtained by our pessimistic policy evaluation procedure. In Eq.(6), the first two terms are controlled by the Bellman error (both onsupport and off-support), and the third is controlled by the optimization error. Notably, when the comparator \u03c0 is the behavior policy \u00b5, the first two terms in Eq.( 6) cancel out, giving the faster rate of Proposition 6. This provides insight for why robust policy improvement does not depend on the quality of the learned critic.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Practical Implementation of ATAC", "text": "We present a scalable deep RL version of ATAC in Algorithm 2, following the principles of Algorithm 1. With abuse of notation, we use \u2207l actor , \u2207l critic to denote taking gradients with respect to the parameters of the actor and the critic, respectively; similarly Line 6 in Algorithm 2 refers to a moving average in the parameter space. In addition, every term involving \u03c0 in Algorithm 2 means a stochastic approximation based on sampling an action from \u03c0 when queried. In implementation, we use adaptive gradient descent algorithm ADAM (Kingma & Ba, 2015) for updates in Algorithm 2 (i.e. f \u2212 \u03b7 fast \u2207l critic and \u03c0 \u2212 \u03b7 slow \u2207l actor ).\nAlgorithm 2 is a two-timescale first-order algorithm (Borkar, 1997;Maei et al., 2009), where the critic is updated with a much faster rate \u03b7 fast than the actor with \u03b7 slow . This two-timescale update is designed to mimic the oracle updates in Algorithm 1. Using \u03b7 fast \u03b7 slow allows us to approximately treat the critic in Algorithm 2 as the solution to the pessimistic policy evaluation step in Algorithm 1 for a given actor (Maei et al., 2009); on the other hand, the actor's gradient update rule is reminiscent of the incremental nature of no-regret optimization oracles.", "publication_ref": ["b23", "b5", "b30", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "CRITIC UPDATE", "text": "The update in Line 4 of Algorithm 2 is a first-order approximation of Line 3 in Algorithm 1. We discuss the important design decisions of this practical critic update below.\nProjection Each critic update performs a projected minibatch gradient step, where the projection to F ensures bounded complexity for the critic. We parameterize F as neural networks with 2 bounded weights. 6 The projection is crucial to ensure stable learning across all \u03b2 values. The use of projection can be traced back to the training Wasserstein GAN (Arjovsky et al., 2017) or IPM-based IL (Swamy et al., 2021). We found alternatives such as weight decay penalty to be less reliable.\nDouble Q residual algorithm loss Off-policy optimization with function approximators and bootstrapping faces the notorious issue of deadly triad (Sutton & Barto, 2018). Commonly this is mitigated through the use of double Q heuristic (Fujimoto et al., 2018;Haarnoja et al., 2018); however, we found that this technique alone is insufficient to enable numerically stable policy evaluation when the policy \u03c0 takes very different actions 7 from the behavior policy \u00b5. To this end, we design a new surrogate for the Bellman error E D (f, \u03c0) for Algorithm 2, by combining the double Q heuristic and the objective of the Residual Algorithm (RA) (Baird, 1995), both of which are previous attempts to combat the deadly triad. Specifically, we design the surrogate loss as the convex combination of the temporal difference (TD) losses of the critic and its delayed targets: (s, a). We call the objective in Eq.(7), the DQRA loss. We found that using the DQRA loss significantly improves the optimization stability compared with just the double Q heuristic alone; see Figure 2. As a result, ATAC can perform stable optimization with higher \u03b2 values and make the learner less pessimistic. This added stability of DQRA comes from that the residual error E td D (f, f, \u03c0) is a fixed rather than a changing objective. This stabilization overcomes potential biases due to the challenges (related to double sampling) in unbiased gradient estimation of the RA objective. Similar observations were made by Wang & Ueda (2021) for online RL. In practice,  7). The plots show the policy performance and TD error across optimization epochs of ATAC with the hopper-mediumreplay dataset. The stability and performance are greatly improved when w \u2208 (0, 1). For each w, the plot shows the 25 th , 50 th , 75 th percentiles over 10 random seeds.\nE w D (f, \u03c0) := (1 \u2212 w)E td D (f, f, \u03c0) + wE td D (f,f min , \u03c0) (7) where w \u2208 [0, 1], E td D (f, f , \u03c0) := E D [(f (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 ], andf min (s, a) := min i=1,2fi\nwe found that w = 0.5 works stably; using w \u2248 0 ensures numerical stability, but has a worst-case exponentially slow convergence speed and often deteriorates neural network learning (Schoknecht & Merke, 2003;Wang & Ueda, 2021). In Section 5, we show an ablation to study the effects of w.", "publication_ref": ["b2", "b42", "b41", "b15", "b18", "b3", "b46", "b38", "b46"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "ACTOR UPDATE", "text": "The actor update aims to achieve no-regret with respect to the adversarially chosen critics. In Algorithm 2, we adopt a gradient based update (implemented as ADAM) mimicking the proximal nature of theoretical no-regret algorithms. Although ADAM has no formal no-regret guarantees for neural network learning, it works quite well in practice for RL and IL algorithms based on no-regret learning (Sun et al., 2017;Cheng et al., 2019;.", "publication_ref": ["b40", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Projection", "text": "We set \u03a0 to be a class of policies with a minimal entropy constraint, so the projection in Line 5 ensures that the updated policy has a non-zero entropy. Soft policy iteration style theoretical algorithms naturally keep a reasonable entropy, and practically this avoids getting trapped in poor local optima. We implement the constraint by a Lagrange relaxation similar to SAC (Haarnoja et al., 2018).\nActor loss with a single critic While the critic optimization uses the double Q heuristic for numerical stability, the actor loss only uses one of the critics (we select f 1 ). This actor loss is similar to TD3 (Fujimoto et al., 2018), but different from SAC (Haarnoja et al., 2018) which takes min i=1,2 f i (s, a) as the objective. This design choice is critical to enable ATAC's IL behavior when \u03b2 is low. On the contrary, using the SAC-style loss produces instability for small \u03b2, with the actor loss oscillating in a limit cycle between the two critics.", "publication_ref": ["b18", "b15", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We test the effectiveness of ATAC (Algorithm 2) in terms of performance and robust policy improvement using the D4RL offline RL benchmark's continuous control domains (Fu et al., 2020). More details are given in Appendix C.", "publication_ref": ["b13"], "figure_ref": [], "table_ref": []}, {"heading": "Setup and hyperaparameter selection", "text": "We compare ATAC (Algorithm 2) with recent offline RL algorithms CQL (Kumar et al., 2020), COMBO (Yu et al., 2021), TD3+BC (Fujimoto & Gu, 2021), IQL (Kostrikov et al., 2021), as well as the offline IL baseline, behavior cloning (BC). We also introduce an absolute pessimism version of ATAC (denoted ATAC 0 ), where we replace L Dmini (f, \u03c0) in l critic of Algorithm 2 with f (s 0 , \u03c0). ATAC 0 can be viewed as a deep learning implementation of the theoretical algorithm PSPI from Xie et al. (2021) with the template of Algorithm 2.\nIn Algorithm 2, we use \u03b7 fast = 0.0005 and \u03b7 slow = 10 \u22123 \u03b7 fast based on an offline tuning heuristic, \u03c4 = 0.005 from the work of Haarnoja et al. (2018), and w = 0.5, across all domains. We include an ablation for w later and further details of our setup are given in Appendix C. The regularization coefficient \u03b2 is our only hyperparameter which varies across datasets, based on an online selection. Specifically, we run 100 epochs of BC for warm start; followed by 900 epochs of ATAC, where 1 epoch denotes 2K gradient updates. For each dataset, we report the median results over 10 random seeds. Since ATAC does not have guarantees on last-iterate convergence, we report also the results of both the last iterate (denoted as ATAC and ATAC 0 ) and the best checkpoint (denoted as ATAC * and ATAC * 0 ) selected among 9 checkpoints (each was made every 100 epochs). The hyperparameter \u03b2 is picked separately for ATAC, ATAC 0 , ATAC * and ATAC * 0 . Comparison with offline RL baselines Overall the experimental results in Table 1 show that ATAC and ATAC * outperform other model-free offline RL baselines consistently and model-based method COMBO mostly. Especially significant improvement is seen in walker2d-medium, walker2d-medium-replay, hopper-medium-replay and penexpert, although the performance is worse than COMBO and CQL in the halfhcheetah-rand. It turns out that our fixed learning rate parameter does not result in sufficient convergence of ATAC on this domain. Our adaptation of PSPI (i.e. ATAC 0 and ATAC * 0 ) is remarkably competitive with state-of-the-art baselines. This is the first empirical evaluation of PSPI, which further demonstrates the effectiveness of our design choices in Algorithm 2. However, ATAC 0 and ATAC * 0 perform worse than ATAC and ATAC * , except for pen-human, door-human, and pen-cloned. In Appendix C we show ATAC and ATAC 0 's variability of performance across seeds by adding 25% and 75% quantiles of scores across 10 random seeds. (For baselines we only have scalar performance from the published results.)\nRobust policy improvement We study whether the practical version of ATAC also enjoys robust policy improvement as Proposition 6 proves for the theoretical version. We show how ATAC * performs with various \u03b2 values in Figure 1 on hopper. The results are consistent with the theoretical prediction in Proposition 6: ATAC robustly improves upon the behavior policy almost for all \u03b2 except very large ones.\nFor large \u03b2, Proposition 6 shows that the finite-sample statistical error dominates the bound. ATAC does, however, not improve from the behavior policy on *-human and *cloned even for well-tuned \u03b2; in fact, none of the offline RL algorithms does. We suspect that this is due to the failure of the realizability assumption \u00b5 \u2208 \u03a0, as these datasets contain human demonstrations which can be non-Markovian. We include the variation of results across \u03b2 for all datasets as well as statistics of robust policy improvement across \u03b2 and iterates in Appendix C. This robust policy improvement property of ATAC means that practitioners can tune the performance of ATAC by starting with \u03b2 = 0 and gradually increasing \u03b2 until the performance drops, without ever deploying a policy significantly worse than the previous behavior policy.\nAblation of DQRA loss We show that the optimization stability from the DQRA loss is a key contributor to ATAC's performance by an ablation. We run ATAC with various w on hopper-medium-replay. When w = 1 (i.e. using conventional bootstrapping with double Q), the Bellman minimization part becomes unstable and the TD error E td D (f, f, \u03c0) diverges. Using just the residual gradient (w = 0), while being numerical stable, leads to bad policy performance as also observed in the literature (Schoknecht & Merke, 2003;Wang & Ueda, 2021). For w \u2208 (0, 1), the stability and performance are usually significantly better than w \u2208 {0, 1}. For simplicity, we use w = 0.5 in our experiments.", "publication_ref": ["b27", "b51", "b14", "b25", "b49", "b18", "b38", "b46"], "figure_ref": [], "table_ref": []}, {"heading": "Discussion and Conclusion", "text": "We propose the concept of relative pessimism for offline RL and use it to design a new algorithm ATAC based on a Stackelberg game formulation. ATAC enjoys strong guarantees comparable to prior theoretical works, with an additional advantage of robust policy improvement due to relative pessimism. Empirical evaluation confirms the theoretical predictions and demonstrates ATAC's state-of-the-art performance on D4RL offline RL benchmarks.\nATAC shows a natural bridge between IL and offline RL. From its perspective, IL is an offline RL problem with the largest uncertainty on the value function (since IL does not have reward information), as captured by setting \u03b2 = 0 in ATAC. In this case, the best policy under relative pessimism is to mimic the behavior policy exactly; otherwise, there is always a scenario within the uncertainty where the agent performs worse than the behavior policy. Only by considering the reduced uncertainty due to labeled rewards, it becomes possible for offline RL to learn a policy that strictly improves over the behavior policy. Conversely, we can view IL as the most pessimistic offline RL algorithm, which ignores the information in the data reward labels. Indeed IL does not make assumption on the data coverage, which is the core issue offline RL attempts to solve. We hope that this insightful connection can encourage future research on advancing IL and offline RL.\nFinally, we remark on some limitations of ATAC. While ATAC has strong theoretical guarantees with general function approximators, it comes with a computational cost that its adversarial optimization problem (like that of Xie et al. (2021)) is potentially harder to solve than alternative offline RL approaches based on dynamic programming in a fixed pessimism MDP (Jin et al., 2021;Liu et al., 2020;Fujimoto & Gu, 2021;Kostrikov et al., 2021). For example, in our theoretical algorithm (Algorithm 1), we require having a no-regret policy optimization oracle (Definition 4), which we only know is provably time and memory efficient for linear function approximators and softmax policies (Xie et al., 2021). 8 This extra computational difficulty also manifests in the IL special case of ATAC (i.e. \u03b2 = 0): ATAC reduces to IPM-minimization or Wasserstein-GAN for IL which requires harder optimization than BC based on maximum likelihood estimation, though the adversarial training version can produce a policy of higher quality. How to strike a better balance between the quality of the objective function and its computational characteristics is an open question.", "publication_ref": ["b49", "b19", "b29", "b14", "b25", "b49"], "figure_ref": [], "table_ref": []}, {"heading": "A. Related Works", "text": "There is a rich literature on offline RL with function approximation when the data distribution \u00b5 is sufficiently rich to cover the state-action distribution d \u03c0 for any \u03c0 \u2208 \u03a0 (Antos et al., 2008;Munos, 2003;Farahmand et al., 2010;Chen & Jiang, 2019;Xie & Jiang, 2020). However, this is a prohibitive assumption in practice where the data distribution is typically constrained by the quality of available policies, safety considerations and existing system constraints which can lead it to have a significantly narrower coverage. Based on this observation, there has been a line of recent works in both the theoretical and empirical literature that systematically consider datasets with inadequate coverage.\nThe methods designed for learning without coverage broadly fall in one of two categories. Many works adopt the behavior regularization approach, where the learned policy is regularized to be close to the behavior policy in states where adequate data is not observed. On the theoretical side, some works (Laroche et al., 2019;Kumar et al., 2019;Fujimoto et al., 2018) provide safe policy improvement guarantees, meaning that the algorithms always do at least as well as the behavior policy, while improving upon it when possible. These and other works (Wu et al., 2019;Fujimoto & Gu, 2021) also demonstrate the benefits of this principle in comprehensive empirical evaluations.\nA second class of methods follow the principle of pessimism in the face of uncertainty, and search for a policy with the best value under all possible scenarios consistent with the data. Some papers perform this reasoning in a modelbased manner (Kidambi et al., 2020;Yu et al., 2020). In the model-free setting, Liu et al. (2020) define pessimism by truncating Bellman backups from states with limited support in the data and provide theoretical guarantees for the function approximation setting when the behavior distribution \u00b5 is known or can be easily estimated from samples, along with proof-of-concept experiments. The need to estimate \u00b5 has been subsequently removed by several recent works in both linear (Jin et al., 2021;Zanette et al., 2021) and non-linear (Xie et al., 2021;Uehara et al., 2021) settings.\nOf these, the work of Xie et al. (2021) is the closest to this paper. Their approach optimizes a maximin objective where the maximization is over policies and minimization over all f \u2208 F which are Bellman-consistent for that policy under the data distribution. Intuitively, this identifies an F-induced lower bound for the value of each policy through the Bellman constraint and maximizes that lower bound. They also develop a regularized version more amenable to practical implementation, but provide no empirical validation of their approach. While the optimization of a pessimistic estimate of J(\u03c0) results in a good policy with well-chosen hyperparameters, we argue that maximizing an alternative lower bound on the relative performance difference J(\u03c0)\u2212J(\u00b5) is nearly as good in terms of the absolute quality of the returned policy with well-chosen hyperparameters, but additionally improves upon the behavior policy for all possible choices of certain hyperparameters.\nOn the empirical side, several recent approaches (Kumar et al., 2020;Yu et al., 2021;Kostrikov et al., 2021) show promising empirical results for pessimistic methods. Many of these works consider policy iteration-style approaches where the policy class is implicitly defined in terms of a critic (e.g. through a softmax), whereas we allow explicit specification of both actor and critic classes. Somewhat related to our approach, the CQL algorithm (Kumar et al., 2020) trains a critic Q by maximizing the combination of a lower bound on J(\u03c0 Q ) \u2212 J(\u00b5), where \u03c0 Q is an implicit policy parameterized by Q, along with a Bellman error term for the current actor policy. The actor is trained with respect to the resulting critic. Lacking a clear objective like Eq.(1), this approach does not enjoy the robust policy improvement or other theoretical guarantees we establish in this paper. (We provide a detailed comparison with CQL in Appendix D) More generally, our experiments show that several elements of the theoretical design and practical implementation of our algorithm ATAC allow us to robustly outperform most of these baselines in a comprehensive evaluation.", "publication_ref": ["b1", "b33", "b12", "b7", "b48", "b28", "b26", "b15", "b47", "b14", "b22", "b50", "b29", "b19", "b49", "b43", "b49", "b27", "b51", "b25", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "B. Guarantees of Theoretical Algorithm", "text": "In this section, we provide the guarantees of theoretical algorithm including the the results provided in Section 4.1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1. Concentration Analysis", "text": "This section provides the main results regarding E D (f, \u03c0) and its corresponding Bellman error. The results in this section are analogs of the results of Xie et al. (2021, Appendix A), but we use covering numbers to provide finer characteristics of the concentration. We provide the background of covering number as follows.\nDefinition 7 (\u03b5-covering number). An \u03b5-cover of a set F with respect to a metric \u03c1 is a set {g 1 , . . . , g n } \u2286 F, such that for each g \u2208 F, there exists some g i \u2208 {g 1 , . . . , g n } such that \u03c1(g, g i ) \u2264 \u03b5. We define the \u03b5-covering number of a set F under metric \u03c1, N (F, \u03b5, \u03c1), to be the the cardinality of the smallest \u03b5-cover.\nFurther properties of covering number can be found in standard textbooks (see, e.g., Wainwright, 2019). In this paper, we will apply the \u03b5-covering number on both function class F and policy class \u03a0. For the function class, we use the following metric\n\u03c1 F (f 1 , f 2 ) := f 1 \u2212 f 2 \u221e = sup (s,a)\u2208S\u00d7A |f 1 (s, a) \u2212 f 2 (s, a)|.(8)\nWe use N \u221e (F, \u03b5) to denote the \u03b5-covering number of F w.r.t. metric \u03c1 F for simplicity.\nSimilarly, for the policy class, we define the metric as follows\n\u03c1 \u03a0 (\u03c0 1 , \u03c0 2 ) := \u03c0 1 \u2212 \u03c0 2 \u221e,1 = sup s\u2208S \u03c0 1 (\u2022|s) \u2212 \u03c0 2 (\u2022|s) 1 ,(9)\nand we use N \u221e,1 (\u03a0, \u03b5) to denote the \u03b5-covering number of \u03a0 w.r.t. metric \u03c1 \u03a0 for simplicity.\nThe following two theorems are the main results of this concentration analysis.\nTheorem 8. For any \u03c0 \u2208 \u03a0, let f \u03c0 be defined as follows,\nf \u03c0 := argmin f \u2208F sup admissible \u03bd f \u2212 T \u03c0 f 2 2,\u03bd .\nThen, for E D (f \u03c0 , \u03c0) (defined in Eq.( 5)), the following holds with probability at least 1 \u2212 \u03b4 for all \u03c0 \u2208 \u03a0:\nE D (f \u03c0 , \u03c0) \u2264 O V 2 max log |N\u221e(F , Vmax /N)||N\u221e,1(\u03a0, 1 /N)| /\u03b4 N + \u03b5 F =: \u03b5 r . We now show that E D (f, \u03c0) could effectively estimate f \u2212 T \u03c0 f 2 2,\u00b5 . Theorem 9. With probability at least 1 \u2212 \u03b4, for any \u03c0 \u2208 \u03a0, f \u2208 F, f \u2212 T \u03c0 f 2,\u00b5 \u2212 E D (f, \u03c0) \u2264 O V max log |N\u221e(F , Vmax /N)||N\u221e,1(\u03a0, 1 /N)| /\u03b4 N + \u221a \u03b5 F ,F .(10)\nWhen setting E D (f, \u03c0) = \u03b5 r , Eq.( 10) implies a bound on f \u2212 T \u03c0 f 2,\u00b5 which we denote as \u221a \u03b5 b and will be useful later.\nThat is,\n\u221a \u03b5 b := \u221a \u03b5 r + O V max log |N\u221e(F , Vmax /N)||N\u221e,1(\u03a0, 1 /N)| /\u03b4 N + \u221a \u03b5 F ,F . (11\n)\nWe first provide some complementary lemmas used for proving Theorems 8 and 9. The first lemma, Lemma 10, is the only place where we use concentration inequalities on E D , and all high-probability statements regarding E D follow deterministically from Lemma 10.\nLemma 10. With probability at least 1 \u2212 \u03b4, for any f, g 1 , g 2 \u2208 F and \u03c0 \u2208 \u03a0,\ng 1 \u2212 T \u03c0 f 2 2,\u00b5 \u2212 g 2 \u2212 T \u03c0 f 2 2,\u00b5 \u2212 1 N (s,a,r,s )\u2208D (g 1 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 + 1 N (s,a,r,s )\u2208D (g 2 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 \u2264 O \uf8eb \uf8ec \uf8edVmax g 1 \u2212 g 2 2,\u00b5 log |N\u221e(F , Vmax N )||N\u221e,1(\u03a0, 1 N )| \u03b4 N + V 2 max log |N\u221e(F , Vmax N )||N\u221e,1(\u03a0, 1 N )| \u03b4 N \uf8f6 \uf8f7 \uf8f8 .\nProof of Lemma 10. This proof follows a similar approach as the proof of Xie et al. (2021, Lemma A.4), but ours is established based on a more refined concentration analysis via covering number. We provide the full detailed proof here for completeness. By a standard calculation, 1 N (s,a,r,s )\u2208D\n(g 1 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 \u2212 1 N (s,a,r,s )\u2208D (g 2 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 = 1 N (s,a,r,s )\u2208D (g 1 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 \u2212 (g 2 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) = 1 N (s,a,r,s )\u2208D ((g 1 (s, a) \u2212 g 2 (s, a)) (g 1 (s, a) + g 2 (s, a) \u2212 2r \u2212 2\u03b3f (s , \u03c0))) .(12)\nSimilarly, letting \u00b5 \u00d7 (P, R) denote the distribution (s, a) \u223c \u00b5, r = R(s, a), s \u223c P(\u2022|s, a), we have\nE \u00b5\u00d7(P,R) (g 1 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 \u2212 E \u00b5\u00d7(P,R) (g 2 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 (a) = E \u00b5\u00d7(P,R) [(g 1 (s, a) \u2212 g 2 (s, a)) (g 1 (s, a) + g 2 (s, a) \u2212 2r \u2212 2\u03b3f (s , \u03c0))] = E \u00b5 [E [(g 1 (s, a) \u2212 g 2 (s, a)) (g 1 (s, a) + g 2 (s, a) \u2212 2r \u2212 2\u03b3f (s , \u03c0))|s, a]] = E \u00b5 [(g 1 (s, a) \u2212 g 2 (s, a)) (g 1 (s, a) + g 2 (s, a) \u2212 2 (T \u03c0 f ) (s, a))] (13\n) (b) = E \u00b5 (g 1 (s, a) \u2212 (T \u03c0 f ) (s, a)) 2 \u2212 E \u00b5 (g 2 (s, a) \u2212 (T \u03c0 f ) (s, a)) 2 ,(14)\nwhere (a) and (b) follow from the similar argument to Eq.(12).\nBy using Eq.( 12) and Eq.( 14), we know\nE \u00b5\u00d7(P,R) \uf8ee \uf8f0 1 N (s,a,r,s )\u2208D (g 1 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 \u2212 1 N (s,a,r,s )\u2208D (g 2 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 \uf8f9 \uf8fb = E \u00b5 (g 1 (s, a) \u2212 (T \u03c0 f ) (s, a)) 2 \u2212 E \u00b5 (g 2 (s, a) \u2212 (T \u03c0 f ) (s, a)) 2 .\nNow, let F \u03b51 be an \u03b5 1 -cover of F and \u03a0 \u03b52 be an \u03b5 2 -cover of \u03a0, so that we know: i)\n|F \u03b51 | = N \u221e (F, \u03b5 1 ), |\u03a0 \u03b52 | = N \u221e,1 (\u03a0, \u03b5 2 ); ii) there exist f , g 1 , g 2 \u2208 F \u03b51 and \u03c0 \u2208 \u03a0 \u03b52 , such that f \u2212 f \u221e , g 1 \u2212 g 1 \u221e , g 2 \u2212 g 2 \u221e \u2264 \u03b5 1 and \u03c0 \u2212 \u03c0 \u221e,1 \u2264 \u03b5 2\n, where \u2022 \u221e and \u2022 \u221e,1 are defined in Eq.( 8) and Eq.(9).\nThen, with probability at least 1 \u2212 \u03b4, for all f, g 1 , g 2 \u2208 F, \u03c0 \u2208 \u03a0, and the corresponding f , g 1 , g 2 , \u03c0,\nE \u00b5 g 1 (s, a) \u2212 T \u03c0 f (s, a) 2 \u2212 E \u00b5 g 2 (s, a) \u2212 T \u03c0 f (s, a) 2 \u2212 1 N (s,a,r,s )\u2208D g 1 (s, a) \u2212 r \u2212 \u03b3 f (s , \u03c0) 2 + 1 N (s,a,r,s )\u2208D g 2 (s, a) \u2212 r \u2212 \u03b3 f (s , \u03c0) 2 = E \u00b5 g 1 (s, a) \u2212 T \u03c0 f (s, a) 2 \u2212 E \u00b5 g 2 (s, a) \u2212 T \u03c0 f (s, a) 2 \u2212 1 N (s,a,r,s )\u2208D ( g 1 (s, a) \u2212 g 2 (s, a)) g 1 (s, a) + g 2 (s, a) \u2212 2r \u2212 2\u03b3 f (s , \u03c0) \u2264 4V \u00b5\u00d7(P,R) ( g 1 (s, a) \u2212 g 2 (s, a)) g 1 (s, a) + g 2 (s, a) \u2212 2r \u2212 2\u03b3 f (s , \u03c0) log |N\u221e(F ,\u03b51)||N\u221e,1(\u03a0,\u03b52)| \u03b4 N + 2V 2 max log |N\u221e(F ,\u03b51)||N\u221e,1(\u03a0,\u03b52)| \u03b4 3N ,\nwhere the first equation follows from Eq.( 13) and the last inequality follows from the Bernstein's inequality and union bounding over F \u03b51 and \u03a0 \u03b52 .\nWe now upper bound the variance term inside the squareroot of the above expression:\nV \u00b5\u00d7(P,R) ( g 1 (s, a) \u2212 g 2 (s, a)) g 1 (s, a) + g 2 (s, a) \u2212 2r \u2212 2\u03b3 f (s , \u03c0) \u2264 E \u00b5\u00d7(P,R) ( g 1 (s, a) \u2212 g 2 (s, a)) 2 g 1 (s, a) + g 2 (s, a) \u2212 2r \u2212 2\u03b3 f (s , \u03c0) 2 \u2264 4V 2 max E \u00b5 ( g 1 (s, a) \u2212 g 2 (s, a)) 2 .\nwhere the last inequality follows from the fact\nof | g 1 (s, a) + g 2 (s, a) \u2212 2r \u2212 2\u03b3 f (s , \u03c0)| \u2264 2V max . Therefore, w.p. 1 \u2212 \u03b4, g 1 \u2212 T \u03c0 f 2 2,\u00b5 \u2212 g 2 \u2212 T \u03c0 f 2 2,\u00b5 \u2212 1 N (s,a,r,s )\u2208D g 1 (s, a) \u2212 r \u2212 \u03b3 f (s , \u03c0) 2 + 1 N (s,a,r,s )\u2208D g 2 (s, a) \u2212 r \u2212 \u03b3 f (s , \u03c0) 2 \u2264 4V max g 1 \u2212 g 2 2,\u00b5 log |N\u221e(F ,\u03b51)||N\u221e,1(\u03a0,\u03b52)| \u03b4 N + 2V 2 max log |N\u221e(F ,\u03b51)||N\u221e,1(\u03a0,\u03b52)| \u03b4 3N\n.\nBy definitions of f , g 1 , g 2 and \u03c0, we know for any (s, a, r, s ) tuple,\n(g 1 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 + (g 2 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 \u2212 g 1 (s, a) \u2212 r \u2212 \u03b3 f (s , \u03c0) 2 + g 2 (s, a) \u2212 r \u2212 \u03b3 f (s , \u03c0) 2 = O(V max \u03b5 1 + V 2 max \u03b5 2 ),and\ng 1 \u2212 g 2 2,\u00b5 = g 1 \u2212 g 2 + (g 1 \u2212 g 1 ) \u2212 (g 2 \u2212 g 2 ) 2,\u00b5 \u2264 g 1 \u2212 g 2 2,\u00b5 + g 1 \u2212 g 1 2,\u00b5 + g 2 \u2212 g 2 2,\u00b5 \u2264 g 1 \u2212 g 2 2,\u00b5 + 2\u03b5 1 . These implies g 1 \u2212 T \u03c0 f 2 2,\u00b5 \u2212 g 2 \u2212 T \u03c0 f 2 2,\u00b5 \u2212 1 N (s,a,r,s )\u2208D (g 1 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 + 1 N (s,a,r,s )\u2208D (g 2 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 V max g 1 \u2212 g 2 2,\u00b5 log |N\u221e(F ,\u03b51)||N\u221e,1(\u03a0,\u03b52)| \u03b4 N + V 2 max log |N\u221e(F ,\u03b51)||N\u221e,1(\u03a0,\u03b52)| \u03b4 N + V max \u03b5 1 log |N\u221e(F ,\u03b51)||N\u221e,1(\u03a0,\u03b52)| \u03b4 N + V max \u03b5 1 + V 2 max \u03b5 2 .\n(x y means x \u2264 C \u2022 y for some absolute constant C)\nChoosing \u03b5 1 = O( Vmax N\n) and \u03b5 2 = O( 1 N ) completes the proof. Lemma 11. For any \u03c0 \u2208 \u03a0, let f \u03c0 and g be defined as follows,\nf \u03c0 := argmin f \u2208F sup admissible \u03bd f \u2212 T \u03c0 f 2 2,\u03bd g := argmin g \u2208F 1 N (s,a,r,s )\u2208D (g (s, a) \u2212 r \u2212 \u03b3f \u03c0 (s , \u03c0)) 2 .\nThen, with high probability,\nf \u03c0 \u2212 g 2,\u00b5 \u2264 O \uf8eb \uf8ec \uf8edVmax log |N\u221e(F , Vmax N )||N\u221e,1(\u03a0, 1 N )| \u03b4 N + \u221a \u03b5 F \uf8f6 \uf8f7 \uf8f8 .\nProof of Lemma 11. The proof of this lemma is obtained exactly the same as Xie et al. (2021, Proof of Lemma A.5), we we only need to change the use of Xie et al. (2021, Lemma A.4) to Lemma 10. This completes the proof.\nWe now ready to prove Theorem 8 and Theorem 9. Note that the proofs of Theorem 8 and Theorem 9 follow similar approaches as the proof of Xie et al. (2021, Theorem A.1, Theorem A.2), and we provide the full detailed proof here for completeness.\nProof of Theorem 8. This proof is obtained by exactly the same strategy of Xie et al. (2021, Proof of Theorem A.1), but we we change to change the corresponding lemmas to the new ones provided above. The correspondence of those lemmas are as follows: (Xie et al., 2021, Lemma A.4) \u2192 Lemma 10; (Xie et al., 2021, Lemma A.5) \u2192 Lemma 11. This completes the proof.\nProof of Theorem 9. This proof is obtained by the same exactly same strategy of Xie et al. (2021, Proof of Theorem A.2), but we we change to change the corresponding lemmas to the new ones provided above. The correspondence of those lemmas are as follows: (Xie et al., 2021, Lemma A.4) \u2192 Lemma 10; (Xie et al., 2021, Lemma A.5) \u2192 Lemma 11. This completes the proof.", "publication_ref": ["b45"], "figure_ref": [], "table_ref": []}, {"heading": "B.2. Decomposition of Performance Difference", "text": "This section proves Eq.(6). We provide a more general version of Eq.( 6) with its proof as follows.\nLemma 12. Let \u03c0 be an arbitrary competitor policy, \u03c0 \u2208 \u03a0 be some learned policy, and f be an arbitrary function over S \u00d7 A. Then we have,\nJ(\u03c0) \u2212 J( \u03c0) = 1 1 \u2212 \u03b3 E \u00b5 f \u2212 T \u03c0 f (s, a) + E \u03c0 T \u03c0 f \u2212 f (s, a) + E \u03c0 [f (s, \u03c0) \u2212 f (s, \u03c0)] + L \u00b5 ( \u03c0, f ) \u2212 L \u00b5 ( \u03c0, Q \u03c0 ) .\nProof of Lemma 12. Let R f, \u03c0 (s, a) := f (s, a) \u2212 \u03b3E s |s,a [f (s , \u03c0)] be a fake reward function given f and \u03c0. We use the subscript \"(\u2022) R f, \u03c0 \" to denote functions or operators under the true dynamics but the fake reward R f, \u03c0 . Since f (s, a) = (T \u03c0 R f, \u03c0 f )(s, a), we know f \u2261 Q \u03c0 R f, \u03c0 . We perform a performance decomposition:\nJ(\u03c0) \u2212 J( \u03c0) = (J(\u03c0) \u2212 J(\u00b5)) \u2212 (J( \u03c0) \u2212 J(\u00b5))\nand rewrite the second term as\n(1 \u2212 \u03b3) (J( \u03c0) \u2212 J(\u00b5)) = L \u00b5 ( \u03c0, Q \u03c0 ) = \u2206( \u03c0) + L \u00b5 ( \u03c0, f ) (\u2206( \u03c0) := L \u00b5 ( \u03c0, Q \u03c0 ) \u2212 L \u00b5 ( \u03c0, f )) = \u2206( \u03c0) + E \u00b5 [f (s, \u03c0) \u2212 f (s, a)] = \u2206( \u03c0) + (1 \u2212 \u03b3)(J R f, \u03c0 ( \u03c0) \u2212 J R f, \u03c0 (\u00b5))\n(by performance difference lemma (Kakade & Langford, 2002))\n= \u2206( \u03c0) + (1 \u2212 \u03b3)Q \u03c0 R f, \u03c0 (s 0 , \u03c0) \u2212 E \u00b5 [R \u03c0,f (s, a)] = \u2206( \u03c0) + (1 \u2212 \u03b3)f (s 0 , \u03c0) \u2212 E \u00b5 [R \u03c0,f (s, a)]. (by f (\u2022, \u2022) \u2261 Q \u03c0 R f, \u03c0 (\u2022, \u2022)) Therefore, (1 \u2212 \u03b3)(J(\u03c0) \u2212 J( \u03c0)) = (1 \u2212 \u03b3) (J(\u03c0) \u2212 f (d 0 , \u03c0)) (I) + E \u00b5 [R \u03c0,f (s, a)] \u2212 (1 \u2212 \u03b3)J(\u00b5)(II)\n\u2212\u2206( \u03c0).\nWe first analyze (II). We can expand it by the definition of R \u03c0,f as follows\n(II) = E \u00b5 [R \u03c0,f (s, a)] \u2212 (1 \u2212 \u03b3)J(\u00b5) = E \u00b5 [R \u03c0,f (s, a) \u2212 R(s, a)] = E \u00b5 [(f \u2212 T \u03c0 f )(s, a)].\nWe now write (I) as\n(I) = (1 \u2212 \u03b3) (J(\u03c0) \u2212 f (s 0 , \u03c0)) = (1 \u2212 \u03b3)J(\u03c0) \u2212 E d \u03c0 [R \u03c0,f (s, a)](Ia)\n+ E d \u03c0 [R \u03c0,f (s, a)] \u2212 (1 \u2212 \u03b3)f (s 0 , \u03c0)(Ib)\n.\nWe analyze each term above in the following.\n(Ib) = E d \u03c0 [R \u03c0,f (s, a)] \u2212 (1 \u2212 \u03b3)f (s 0 , \u03c0) = E d \u03c0 [f (s, \u03c0) \u2212 f (s, \u03c0)].\nOn the other hand, we can write\n(Ia) = (1 \u2212 \u03b3)J(\u03c0) \u2212 E d \u03c0 [R \u03c0,f (s, a)] = E d \u03c0 [R(s, a) \u2212 R \u03c0,f (s, a)] = E d \u03c0 [(T \u03c0 f \u2212 f )(s, a)].\nCombine them all, we have\nJ(\u03c0) \u2212 J( \u03c0) = 1 1 \u2212 \u03b3 ((Ia) + (Ib) + (II) \u2212 \u2206( \u03c0)) = 1 1 \u2212 \u03b3 E \u00b5 f \u2212 T \u03c0 f (s, a) + E \u03c0 T \u03c0 f \u2212 f (s, a) + E \u03c0 [f (s, \u03c0) \u2212 f (s, \u03c0)] + L \u00b5 ( \u03c0, f ) \u2212 L \u00b5 ( \u03c0, Q \u03c0 ) .\nThis completes the proof.\nWe now prove a general version of Eq.(6) using Lemma 12, which takes into account the approximation errors in the realizability and completeness assumptions (Assumption 1 and Assumption 2).\nLemma 13 (General Version of Eq.( 6)). Let \u03c0 be an arbitrary competitor policy. Also let \u03c0 k and f k be obtained by Algorithm 1 for k \u2208 [K]. Then with high probability, for any\nk \u2208 [K], (1 \u2212 \u03b3) (J(\u03c0) \u2212 J(\u03c0 k )) \u2264 E \u00b5 [f k \u2212 T \u03c0 k f k ] + E \u03c0 [T \u03c0 k f k \u2212 f k ] + E \u03c0 [f k (s, \u03c0) \u2212 f k (s, \u03c0 k )] + O V max log |N\u221e(F , Vmax /N)||N\u221e,1(\u03a0, 1 /N)| /\u03b4 N + \u221a \u03b5 F + \u03b2 \u2022 O V 2 max log |N\u221e(F , Vmax /N)||N\u221e,1(\u03a0, 1 /N)| /\u03b4 N + \u03b5 F .\nProof of Lemma 13. By Lemma 12, we have\nJ(\u03c0) \u2212 J(\u03c0 k ) = E \u00b5 [f k \u2212 T \u03c0 k f k ] 1 \u2212 \u03b3 + E \u03c0 [T \u03c0 k f k \u2212 f k ] 1 \u2212 \u03b3 + E \u03c0 [f k (s, \u03c0) \u2212 f k (s, \u03c0 k )] 1 \u2212 \u03b3 + L \u00b5 (\u03c0 k , f k ) \u2212 L \u00b5 (\u03c0 k , Q \u03c0 k ) 1 \u2212 \u03b3 .\nWe now bound the term of\nL \u00b5 (\u03c0 k , f k ) \u2212 L \u00b5 (\u03c0 k , Q \u03c0 k ). f \u03c0 := argmin f \u2208F sup admissible \u03bd f \u2212 T \u03c0 f 2 2,\u03bd , \u2200\u03c0 \u2208 \u03a0 \u03b5 stat := O V 2 max log |N\u221e(F , Vmax /N)||N\u221e,1(\u03a0, 1 /N)| /\u03b4 N , \u03b5 r := \u03b5 stat + O (\u03b5 F ) .\nThen, by Theorem 8, we know that with high probability, for any k \u2208 [K],\nE D (\u03c0 k , f \u03c0 k ) \u2264 \u03b5 r . (15\n)\nFor |L \u00b5 (\u03c0 k , Q \u03c0 k ) \u2212 L \u00b5 (\u03c0 k , f \u03c0 k )|, we have, L \u00b5 (\u03c0 k , Q \u03c0 k ) = E \u00b5 [Q \u03c0 k (s, \u03c0 k ) \u2212 Q \u03c0 k (s, a)] = (1 \u2212 \u03b3) (J(\u03c0 k ) \u2212 J(\u00b5)) = (1 \u2212 \u03b3) (f \u03c0 k (s 0 , \u03c0 k ) \u2212 J(\u00b5)) + (1 \u2212 \u03b3) (J(\u03c0 k ) \u2212 f \u03c0 k (s 0 , \u03c0 k )) = E \u00b5 [f \u03c0 k (s, \u03c0 k ) \u2212 (T \u03c0 k f \u03c0 k )(s, a)] + E d \u03c0 k [(T \u03c0 k f \u03c0 k )(s, a) \u2212 f \u03c0 k (s, a)]\n(by the extension of performance difference lemma (see, e.g., Cheng et al., 2020, Lemma 1\n)) = L \u00b5 (\u03c0 k , f \u03c0 k ) + E \u00b5 [f \u03c0 k (s, a) \u2212 (T \u03c0 k f \u03c0 k )(s, a)] + E d \u03c0 k [(T \u03c0 k f \u03c0 k )(s, a) \u2212 f \u03c0 k (s, a)] =\u21d2 |L \u00b5 (\u03c0 k , Q \u03c0 k ) \u2212 L \u00b5 (\u03c0 k , f \u03c0 k )| \u2264 f \u03c0 k \u2212 T \u03c0 k f \u03c0 k 2,\u00b5 + T \u03c0 k f \u03c0 k \u2212 f \u03c0 k 2,d \u03c0 k \u2264 O( \u221a \u03b5 F ),(16)\nwhere the last step is by Assumption 1. Also, by applying standard concentration inequalities on L D (the failure probability will be split evenly with that on E D from Lemma 10):\n|L \u00b5 (\u03c0 k , f k ) \u2212 L D (\u03c0 k , f k )| + |L \u00b5 (\u03c0 k , f \u03c0 k ) \u2212 L D (\u03c0 k , f \u03c0 k )| \u2264 \u221a \u03b5 stat , \u2200k \u2208 [K].(17)\nTherefore,\nL \u00b5 (\u03c0 k , f k ) \u2212 L \u00b5 (\u03c0 k , Q \u03c0 k ) \u2264 L \u00b5 (\u03c0 k , f k ) + \u03b2E D (\u03c0 k , f k ) \u2212 L \u00b5 (\u03c0 k , Q \u03c0 k ) (E D (\u2022) \u2265 0) \u2264 L \u00b5 (\u03c0 k , f k ) + \u03b2E D (\u03c0 k , f k ) \u2212 L \u00b5 (\u03c0 k , f \u03c0 k ) \u2212 \u03b2E D (\u03c0 k , f \u03c0 k ) + O( \u221a F ) + \u03b2\u03b5 r\n(by Eq.(15) and Eq.( 16))\n\u2264 L D (\u03c0 k , f k ) + \u03b2E D (\u03c0 k , f k ) \u2212 L D (\u03c0 k , f \u03c0 k ) \u2212 \u03b2E D (\u03c0 k , f \u03c0 k ) + O( \u221a F ) + \u221a \u03b5 stat + \u03b2 \u2022 O (\u03b5 stat + \u03b5 F ) (by Eq.(17)) \u2264 O ( \u221a \u03b5 F ) + \u221a \u03b5 stat + \u03b2 \u2022 O (\u03b5 stat + \u03b5 F ) (by the optimality of f k ) \u2264 O \u221a \u03b5 F + V max log |N\u221e(F , Vmax /N)||N\u221e,1(\u03a0, 1 /N)| /\u03b4 N + \u03b2 \u2022 O V 2 max log |N\u221e(F , Vmax /N)||N\u221e,1(\u03a0, 1 /N)| /\u03b4 N + \u03b5 F .\nThis completes the proof.", "publication_ref": ["b20", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "B.3. Performance Guarantee of the Theoretical Algorithm", "text": "This section proves a general version of Theorem 5 using Lemma 12, which relies on the approximate realizability and completeness assumptions (Assumption 1 and Assumption 2). Theorem 14 (General Version of Theorem 5). Under the same condition as Theorem 5, let C > 0 be any constant, \u03bd be an arbitrarily distribution that satisfies\nC (\u03bd; \u00b5, F, \u03c0 k ) \u2264 C, \u03b5 stat := O V 2 max log |N\u221e(F , Vmax /N )||N \u221e,1 (\u03a0, 1 /N )| /\u03b4 N\n, and \u03c0 be an arbitrary competitor policy. Then, we choose\n\u03b2 = O V 1/3 max (\u03b5 F +\u03b5stat) 2/3 and with probability at least 1 \u2212 \u03b4, J(\u03c0) \u2212 J(\u03c0) \u2264 O \u221a C \u221a \u03b5 F + \u221a \u03b5 F ,F + \u221a \u03b5 stat + (V max \u03b5 F + V max \u03b5 stat ) 1 /3 1 \u2212 \u03b3 + d \u03c0 \\ \u03bd, f k \u2212 T \u03c0 k f k 1 \u2212 \u03b3 .\nProof of Theorem 14. Over this proof, let\n\u03b5 stat := O V 2 max log |N\u221e(F , Vmax /N)||N\u221e,1(\u03a0, 1 /N)| /\u03b4 N .\nBy the definition of\u03c0, we have \nJ(\u03c0) \u2212 J(\u03c0) = 1 K K k=1 (J(\u03c0) \u2212 J(\u03c0 k )) \u2264 1 K K k=1 E \u00b5 [f k \u2212 T \u03c0 k f k ] 1 \u2212 \u03b3 (I) + E \u03c0 [T \u03c0 k f k \u2212 f k ] 1 \u2212 \u03b3 (II) + E \u03c0 [f k (s, \u03c0) \u2212 f k (s, \u03c0 k )] 1 \u2212 \u03b3 (III) + \u221a \u03b5 F + \u221a \u03b5 stat + \u03b2 \u2022 O(\u03b5 F + \u03b5 stat ) . (by\n\u2264 \u221a \u03b5 b + V max /\u03b2 1 \u2212 \u03b3 (\u03b5 b is defined in Equation (11)) and (II) \u2264 2 \u221a C( \u221a \u03b5 b + V max /\u03b2) 1 \u2212 \u03b3 + d \u03c0 \\ \u03bd, f k \u2212 T \u03c0 k f k 1 \u2212 \u03b3 ,(I)\nwhere C \u2265 1 can be selected arbitrarily and \u03bd is an arbitrarily distribution that satisfies C (\u03bd; \u00b5, F, \u03c0 k ) \u2264 C.\nAlso, using the property of the no-regret oracle, we have\n1 \u221a C \u221a \u03b5 F + \u221a \u03b5 F ,F + \u221a \u03b5 stat + V max /\u03b2 1 \u2212 \u03b3 + \u03b2(\u03b5 F + \u03b5 stat ) \uf8f6 \uf8f8 + 1 K K k=1 d \u03c0 \\ \u03bd, f k \u2212 T \u03c0 k f k 1 \u2212 \u03b3 . Algorithm 3 ATAC (Detailed Practical Version) Input: Batch data D, policy \u03c0, critics f 1 , f 2 , constants \u03b2 \u2265 0, \u03c4 \u2208 [0, 1], w \u2208 [0, 1], entropy lower bound Entropy min 1: Initialize target networksf 1 \u2190 f 1 ,f 2 \u2190 f 2 2: Initialize Lagrange multiplier \u03b1 \u2190 1 3: for k = 1, 2, . . . , K do 4:\nSample minibatch D mini from dataset D.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "5:", "text": "For\nf \u2208 {f 1 , f 2 }, update critic networks l critic (f ) := L Dmini (f, \u03c0) + \u03b2E w Dmini (f, \u03c0) # f \u2190 Proj F (f \u2212 \u03b7 fast \u2207l critic ) f \u2190 ADAM(f, \u2207l critic , \u03b7 fast ) f \u2190 ClipWeightL2(f ) 6: Update actor network # l actor (\u03c0) := \u2212L Dmini (f 1 , \u03c0) # \u03c0 \u2190 Proj \u03a0 (\u03c0 \u2212 \u03b7 slow \u2207l actor ) l actor (\u03c0, \u03b1) = \u2212L Dmini (f 1 , \u03c0) \u2212 \u03b1(E Dmini [\u03c0 log \u03c0] + Entropy min ) \u03c0 \u2190 ADAM(\u03c0, \u2207 \u03c0lactor , \u03b7 slow ) \u03b1 \u2190 ADAM(\u03b1, \u2212\u2207 \u03b1lactor , \u03b7 fast ) \u03b1 \u2190 max{0, \u03b1} 7: For (f,f ) \u2208 {(f i ,f i )} i=1,2 , update target networks f \u2190 (1 \u2212 \u03c4 )f + \u03c4 f . 8: end for Therefore, we choose \u03b2 = \u0398 V 1/3 max (\u03b5 F +\u03b5stat) 2/3 , and obtain J(\u03c0) \u2212 J(\u03c0) \u2264 O \u221a C \u221a \u03b5 F + \u221a \u03b5 F ,F + \u221a \u03b5 stat + (V max \u03b5 F + V max \u03b5 stat ) 1 /3 1 \u2212 \u03b3 + 1 K K k=1 d \u03c0 \\ \u03bd, f k \u2212 T \u03c0 k f k 1 \u2212 \u03b3 .\nThis completes the proof.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C. Experiment Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1. Implementation Details", "text": "We provide a more detailed version of our practical algorithm Algorithm 2 in Algorithm 3, which shows how the actor and critic updates are done with ADAM. As mentioned in Section 4.2, the projection in \u03c0 \u2190 Proj \u03a0 (\u03c0 \u2212 \u03b7 slow \u2207l actor ) of the pseudo-code Algorithm 2 is done by a further Lagrange relaxation through introducing a Lagrange multiplier \u03b1 \u2265 0. We update \u03b1 in the fast timescale \u03b7 fast , so the policy entropy E D [\u2212\u03c0 log \u03c0] can be maintained above a threshold Entropy min , roughly following the path of the projected update in Line 5 in the pseudo code in Algorithm 2. Entropy min is set based on the heuristic used in SAC (Haarnoja et al., 2018).\nIn implementation, we use separate 3-layer fully connected neural networks to realize the policy and the critics, where each hidden layer has 256 neurons and ReLU activation and the output layer is linear. The policy is Gaussian, with the mean and the standard deviation predicted by the neural network. We impose an l 2 norm constraint of 100 for the weight (not the bias) in each layer of the critic networks.\nThe first-order optimization is implemented by ADAM (Kingma & Ba, 2015) with a minibatch size |D mini | = 256, and the two-timescale stepsizes are set as \u03b7 fast = 0.0005 and \u03b7 slow = 10 \u22123 \u03b7 fast . These stepsizes \u03b7 fast and \u03b7 slow were selected offline with a heuristic: Since ATAC with \u03b2 = 0 is IPM-IL, we did a grid search (over \u03b7 fast \u2208 {5e \u2212 4, 5e \u2212 5, 5e \u2212 6} and \u03b7 slow = {5e \u2212 5, 5e \u2212 6, 5e \u2212 7}, on the hopper-medium and hopper-expert datasets) and selected the combination that attains the lowest 2 IL error after 100 epochs.\nWe set w = 0.5 in Eq.( 7), as we show in the ablation (Figure 3) that either w = 0 and w = 1 leads to bad numerical stability and/or policy performance. We use \u03c4 = 0.005 for target network update from the work of Haarnoja et al. (2018). The discount is set to the common \u03b3 = 0.99.  7). The plots show the policy performance and TD error across optimization epochs of ATAC with the hopper-medium-replay, hopper-medium, and hopper-medium-expert datasets from top to buttom. The stability and performance are greatly improved when w \u2208 (0, 1). For each w, the plot shows the 25 th , 50 th , 75 th percentiles over 10 random seeds.\nThe regularization coefficient \u03b2 is our only hyperparameter that varies across datasets based on an online selection. We consider \u03b2 in \u2208 {0, 4 \u22124 , 4 \u22123 , 4 \u22122 , 4 \u22121 , 1, 4, 4 2 , 4 3 , 4 4 }. For each \u03b2, we perform ATAC training with 10 different seeds: for each seed, we run 100 epochs of BC for warm start and 900 epochs of ATAC, where 1 epoch denotes 2K gradient updates.\nDuring the warmstart, the critics are optimized to minimize the Bellman surrogate E w Dmini (f, \u03c0) except for \u03b2 = 0. Since ATAC does not have guarantees on last-iterate convergence, we report also the results of both the last iterate (denoted as ATAC and ATAC 0 ) and the best checkpoint (denoted as ATAC * and ATAC * 0 ) selected among 9 checkpoints (each was made every 100 epochs).\nWe argue that the online selection of \u03b2 and few checkpoints are reasonable for ATAC, as ATAC theory provides robust policy improvement guarantees. While the assumptions made in the theoretical analysis does not necessarily apply to the practical version of ATAC, empirically we found that ATAC does demonstrate robust policy improvement properties in the D4RL benchmarks that we experimented with, which we will discuss more below.", "publication_ref": ["b18", "b23", "b18"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "C.2. Detailed Experimental Results", "text": "We used a selection of the Mujoco datasets (v2) and Adroit datasets (v1) from D4RL as our benchmark environments. For each evaluation, we roll out the mean part of the Gaussian policy for 5 rollouts and compute the Monte Carlo return. For each dataset, we report the statistical results over 10 random seeds in Table 2.\nCompared with the summary we provided in the main text (Table 1), Table 2 includes also the confidence interval which shows how much the 25 th and the 75 th percentiles of performance deviate from the median (i.e. the 50 th percentile). In addition, Table 2 also provides the selected hyperparamter \u03b2 for each method.\nOverall, we see that confidence intervals are small for ATAC, except for larger variations happening in hopper-rand, pen-human, and hammer-human. Therefore, the performance improvement of ATAC from other offline RL baselines and behavior policies is significant. We also see that ATAC most of the time picks \u03b2 = 64 for the Mujoco datasets, except for the halfcheetah domain, and has a tendency of picking smaller \u03b2 as the dataset starts to contain expert trajectories (i.e. in *-exp datasets). This is reasonable, since when the behavior policy has higher performance, an agent requires less information from the reward to perform well; in the extreme of learning with trajectories of the optimal policy, the learner can be optimal  just by IL.\nWe also include extra ablation results on the effectiveness of DQRA loss in stabilizing learning in Figure 3, which includes two extra hopper datasets compared with Figure 2. Similar to the results in the main paper, we see that w = 1 is unstable, w = 0 is stable but under-performing, while using w \u2208 (0, 1) strikes a balance between the two. Our choice w = 0.5 has the best performance in these three datasets and is numerically stable. We also experimented with the max-aggregation version recently proposed by Wang & Ueda (2021). It does address the instability issue seen in the typical bootstrapped version w = 1, but its results tend to be noisier compared with w = 0.5 as it makes the optimization landscape more non-smooth.\nLastly, we include experimental results of ATAC on D4RL mujoco-v0 datasets in Table 3. We used v2 instead of v0 in the main results, because 1) hopper-v0 has a bug (see D4RL github; for this reason they are grayed out in Table 3), and 2) some baselines we compare ATAC with also used v2 (or they didn't specify and we suspect so). Here we include these results for completeness.", "publication_ref": ["b46"], "figure_ref": ["fig_4", "fig_2"], "table_ref": []}, {"heading": "C.3. Robust Policy Improvement", "text": "We study empirically the robust policy improvement property of ATAC. First we provide an extensive validation on how ATAC * performs with different \u03b2 on all datasets in Figure 4 and Figure 5, which are the complete version of Figure 1. In these figures, we plot the results of ATAC * (relative pessimism) and ATAC * 0 (absolute pessimism) (which is a deep learning implementation of PSPI (Xie et al., 2021)) in view of the behavior policy's performance. These results show similar trends as we have observed in Figure 1. ATAC can robustly improve from the behavior policy over a wide range of \u03b2 values. In particular, we see the performance degrades below the behavior policy only for large \u03b2s, because of the following reasons. When \u03b2 \u2192 0 ATAC converges to the IL mode, which can recover the behavior policy performance if the realizability assumption is satisfied. On the other hand, when \u03b2 is too large, Proposition 6 shows that the statistical error will start to dominate and therefore lead to substandard performance. This robust policy improvement property means that practitioners of ATAC can online tune its performance by starting with \u03b2 = 0 and the gradually increasing \u03b2 until the performance drop, without ever dropping below the performance of behavior policy much. Figure 4 and Figure 5 show the robustness of ATAC * which uses the best checkpoint. Below in Table 4 we validate further whether safe policy improvement holds across iterates. To this end, we define a robust policy improvement score\nscore RPI (\u03c0) := J(\u03c0) \u2212 J(\u00b5) |J(\u00b5)| (18)\nwhich captures how a policy \u03c0 performs relatively to the behavior policy \u00b5. Table 4 shows the percentiles of the robust policy improvement score for each dataset, over all the \u03b2 choices, random seeds, and iterates from the 100 th epoch to the 900 th epoch of ATAC training. Overall, we see that in most datasets (excluding *-human and *-clone datasets which do not satisfy our theoretical realizability assumption), more than 50% of iterates generated by ATAC across all the experiments are better than the behavior policy. For others, more than 60% of iterates are within 80 % of the behavior policy's performance. This robustness result is quite remarkable as it includes iterates where ATAC has not fully converged as well as bad choices of \u03b2.    4. The robust policy improvement scores of ATAC. We report for each dataset, the percentiles of iterates over all 9 choices of \u03b2, 10 seeds, and 800 epochs (from the 100 th to the 900 th epochs). In most datasets (excluding *-human and *-clone datasets which likely do not satisfy our theoretical realizability assumption), more than 50% of iterates generated by ATAC across all seeds and \u03b2s are better than the behavior policy. For others, more than 60% of iterates are within 80% of the behavior policy's performance. \n-1.1 -1.1 -1.1 -1.1 -1.0 -1.0 -1.0 -1.0 -1.0 0.9 door-human -1.1 -1.1 -1.1 -1.1 -1.1 -1.1 -1.1 -1.0 -0.9 -0.1 relocate-human -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -0", "publication_ref": ["b49"], "figure_ref": [], "table_ref": []}, {"heading": "D. Comparison between ATAC and CQL", "text": "We compare ATAC with CQL (Kumar et al., 2020) in details, since they share a similar pessimistic policy evaluation procedure. In a high level, there are several major differences at the conceptual level:\n1. (Conceptual Algorithm) ATAC describes an explicit solution concept, whereas CQL does not have a clear objective but is described as an iterative procedure. Since the convergence property and fixed point of CQL is unclear for general setups, we cannot always compare ATAC and CQL.\n2. (Maximin vs Minimax) ATAC decouples the policy and the critic, whereas CQL aims to derive the policy from a critic. Specifically, ATAC uses a maximin formulation that finds policies performing well even for the worst case critic, whereas CQL uses a minimax formulation that finds the optimal policy for the worst case critic. In general, maximin and minimax leadto different policies.\n3. (Robust Policy Improvement) Because of the difference between maximin and minimax in the second point, ATAC recovers behavior cloning when the Bellman term is turned off but CQL doesn't. This property is crucial to establishing the robust policy improvement property of ATAC.\nATAC and CQL also differ noticeably in the implementation design. ATAC uses the novel DQRA loss, projections, and two-timescale update; on the other hand, CQL adds an inner policy maximization, uses standard double-Q bootstrapping, and more similar step sizes for the critic and the actor.\nGiven such differences in both abstract theoretical reasoning and practical implementations, ATAC and CQL are two fundamentally different approaches to general offline RL, though it is likely there are special cases where the two produce the same policy (e.g. bandit problems with linear policies and critics).\nBelow we discuss the core differences between the two algorithms in more details.", "publication_ref": ["b27"], "figure_ref": [], "table_ref": []}, {"heading": "D.1. Conceptual Algorithm", "text": "First we compare the two algorithms at the conceptual level, ignoring the finite-sample error. ATAC has a clear objective and an accompanying iterative algorithm to find approximate solutions, whereas CQL is described directly as an iterative algorithm whose fixed point property is not established in general.\nSpecifically, recall that ATAC aims to find the solution to the Stackelberg \nand we show that an approximate solution to the above can be found by a no-regret reduction in Algorithm 1.\nOn the other hand, CQL (specifically CQL (R) in Eq.(3) of (Kumar et al., 2020)) performs the update below 9 Kumar et al. (2020) propose this iterative procedure as an approximation of a pessimistic policy iteration scheme, which alternates between pessimistic policy evaluation and policy improvement with respect to the pessimistic critic:\nf k+1 \u2190 argmin f \u2208F max \u03c0\u2208\u03a0 \u03b1E \u00b5 [f (s, \u03c0) \u2212 f (s, a)] \u2212 R(\u03c0) + E \u00b5 [((f \u2212 T \u03c0 k f k )(s, a)) 2 ](20)\nWe could alternate between performing full off-policy evaluation for each policy iterate, \u03c0 k , and one step of policy improvement. However, this can be computationally expensive. Alternatively, since the policy \u03c0 k is typically derived from the Q-function, we could instead choose \u00b5(a|s) to approximate the policy that would maximize the current Q-function iterate, thus giving rise to an online algorithm. (Kumar et al., 2020).\nNote \u00b5 in the quote above corresponds to \u03c0 in the inner maximization in (20). When presenting this conceptual update rule, Kumar et al. (2020) however do not specify exactly how \u03c0 k is updated but only provide properties on the policy exp(f k (s, a)/Z(s)). Thus, below we will suppose CQL aims to find policies of quality similar to exp(f k (s, a)/Z(s)).", "publication_ref": ["b27", "b27", "b27", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "D.2. Maximin vs. Minimax", "text": "Although it is unclear what the fixed point of CQL is in general, we still can see ATAC and CQL aim to find very different policies. ATAC decouples the policy and the critic to find a robust policy, whereas CQL aims to derive the policy from a critic function.. This observation is reflected below.\n1. ATAC is based on a maximin formulation, whereas CQL is based on minimax formulation.\n2. ATAC updates policies by a no regret routine, where each policy is slow updated and determined by all the critics generated in the past iterations, whereas CQL is more akin to a policy iteration algorithm, where each policy is derived by a single critic.\nWe can see this difference concretely, if we specialize the two algorithms to bandit problems. In this case, CQL is no longer iterative and has a clear objective. Specifically, if we let \u03b1 = 1 \u03b2 , the two special cases can be written as \u03c0 \u2208 argmax If we further ignore the extra regularization term R(\u03c0) (as that can often be absorbed into the policy class), then the main difference between the two approaches, in terms of solution concepts, is clearly the order of max and min. It is well known maximin and minimax gives different solutions in general, unless when the objective is convex-concave (with respect to the policy and critic parameterizations). For example, in this bandit special case, suppose the states and actions are tabular; the objective is convex-concave when \u03a0 and F contains all tabular functions, but convex-concave objective is lost when F contains a finite set of functions. In the latter scenario, CQL and ATAC would give very different policies, and CQL would not enjoy the nice properties of ATAC.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.3. Robust Policy Improvement", "text": "We now illustrate concretely how the difference between ATAC and CQL affects the robust policy improvement property.\nFor simplicity, we only discuss in population level.\nBy Proposition 3 and Proposition 6, we know \u03c0 , the learned policy from ATAC, provably improves behavior policy \u00b5 under a wide range of \u03b2 choice of Eq.( 19), including \u03b2 = 0. In other word, as long as \u00b5 \u2208 \u03a0, ATAC has J( \u03c0 ) \u2265 J(\u00b5) even if \u03b2 = 0 in Eq.(19).\nHowever, the following argument shows that: In CQL, if \u03c0 f \u2208 \u03a0, \u2200f \u2208 F and F contains constant functions, then setting \u03b2 = 0 cannot guarantee policy improvement over \u00b5, even when \u00b5 \u2208 \u03a0, where \u03c0 f denotes the greedy policy with respect to f .\nBased on what's shown before, the corresponding CQL update rule with \u03b2 = 0 can be written as\nf k+1 \u2190 argmin f \u2208F max \u03c0\u2208\u03a0 E \u00b5 [f (s, \u03c0) \u2212 f (s, a)].\nWe now prove that f k+1 is constant across actions in every state on the support of \u00b5 for any k: 3. Combining the two bullets above, we obtain that f k+1 for all k must have f k+1 (s 1 , a 1 ) = f k+1 (s 1 , a 2 ) for all (s 1 , a 1 , a 2 ) \u2208 S \u00d7 A \u00d7 A such that \u00b5(s 1 ) > 0, i.e., f k+1 is constant across actions in every s \u2208 S in the support of \u00b5.\nTherefore, for CQL with \u03b2 = 0, the policies are updated with per-state constant functions, leading to arbitrary learned policies and failing to provide the safe policy improvement guarantee over the behavior policy \u00b5.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "On the theory of policy gradient methods: Optimality, approximation, and distribution shift", "journal": "Journal of Machine Learning Research", "year": "2021", "authors": "A Agarwal; S M Kakade; J D Lee; G Mahajan"}, {"ref_id": "b1", "title": "Learning near-optimal policies with bellman-residual minimization based fitted policy iteration and a single sample path", "journal": "", "year": "2008", "authors": "A Antos; C Szepesv\u00e1ri; R Munos"}, {"ref_id": "b2", "title": "Wasserstein generative adversarial networks", "journal": "PMLR", "year": "2017", "authors": "M Arjovsky; S Chintala; L Bottou"}, {"ref_id": "b3", "title": "Residual algorithms: Reinforcement learning with function approximation", "journal": "Elsevier", "year": "1995", "authors": "L Baird"}, {"ref_id": "b4", "title": "Neuro-dynamic programming: an overview", "journal": "IEEE", "year": "1995", "authors": "D P Bertsekas; J N Tsitsiklis"}, {"ref_id": "b5", "title": "Stochastic approximation with two time scales", "journal": "Systems & Control Letters", "year": "1997", "authors": "V S Borkar"}, {"ref_id": "b6", "title": "Prediction, learning, and games", "journal": "Cambridge university press", "year": "2006", "authors": "N Cesa-Bianchi; G Lugosi"}, {"ref_id": "b7", "title": "Information-theoretic considerations in batch reinforcement learning", "journal": "", "year": "2019", "authors": "J Chen; N Jiang"}, {"ref_id": "b8", "title": "Predictorcorrector policy optimization", "journal": "PMLR", "year": "2019", "authors": "C.-A Cheng; X Yan; N Ratliff; B Boots"}, {"ref_id": "b9", "title": "Policy improvement via imitation of multiple oracles", "journal": "Advances in Neural Information Processing Systems", "year": "2020", "authors": "C.-A Cheng; A Kolobov; A Agarwal"}, {"ref_id": "b10", "title": "Heuristicguided reinforcement learning", "journal": "", "year": "2021", "authors": "C.-A Cheng; A Kolobov; A Swaminathan"}, {"ref_id": "b11", "title": "Online markov decision processes", "journal": "Mathematics of Operations Research", "year": "2009", "authors": "E Even-Dar; S M Kakade; Y Mansour"}, {"ref_id": "b12", "title": "Error propagation for approximate policy and value iteration", "journal": "", "year": "2010", "authors": "A M Farahmand; R Munos; C Szepesv\u00e1ri"}, {"ref_id": "b13", "title": "Datasets for deep data-driven reinforcement learning", "journal": "", "year": "2020", "authors": "J Fu; A Kumar; O Nachum; G Tucker; S Levine"}, {"ref_id": "b14", "title": "A minimalist approach to offline reinforcement learning", "journal": "", "year": "2021", "authors": "S Fujimoto; S S Gu"}, {"ref_id": "b15", "title": "Addressing function approximation error in actor-critic methods", "journal": "PMLR", "year": "2018", "authors": "S Fujimoto; H Hoof; D Meger"}, {"ref_id": "b16", "title": "Off-policy deep reinforcement learning without exploration", "journal": "", "year": "2019", "authors": "S Fujimoto; D Meger; D Precup"}, {"ref_id": "b17", "title": "A theory of regularized markov decision processes", "journal": "PMLR", "year": "2019", "authors": "M Geist; B Scherrer; O Pietquin"}, {"ref_id": "b18", "title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", "journal": "PMLR", "year": "2018", "authors": "T Haarnoja; A Zhou; P Abbeel; S Levine"}, {"ref_id": "b19", "title": "Is pessimism provably efficient for offline rl", "journal": "PMLR", "year": "2021", "authors": "Y Jin; Z Yang; Wang ; Z "}, {"ref_id": "b20", "title": "Approximately optimal approximate reinforcement learning", "journal": "", "year": "2002", "authors": "S Kakade; J Langford"}, {"ref_id": "b21", "title": "A natural policy gradient", "journal": "Advances in neural information processing systems", "year": "2001", "authors": "S M Kakade"}, {"ref_id": "b22", "title": "Morel: Model-based offline reinforcement learning", "journal": "", "year": "2020", "authors": "R Kidambi; A Rajeswaran; P Netrapalli; Joachims ; T "}, {"ref_id": "b23", "title": "A method for stochastic optimization", "journal": "", "year": "2015", "authors": "D P Kingma; J Ba;  Adam"}, {"ref_id": "b24", "title": "Actor-critic algorithms", "journal": "Citeseer", "year": "2000", "authors": "V R Konda; J N Tsitsiklis"}, {"ref_id": "b25", "title": "Offline reinforcement learning with implicit q-learning", "journal": "", "year": "2021", "authors": "I Kostrikov; A Nair; S Levine"}, {"ref_id": "b26", "title": "Stabilizing off-policy q-learning via bootstrapping error reduction", "journal": "Advances in Neural Information Processing Systems", "year": "2019", "authors": "A Kumar; J Fu; M Soh; G Tucker; S Levine"}, {"ref_id": "b27", "title": "Conservative q-learning for offline reinforcement learning", "journal": "", "year": "2020", "authors": "A Kumar; A Zhou; G Tucker; S Levine"}, {"ref_id": "b28", "title": "Safe policy improvement with baseline bootstrapping", "journal": "PMLR", "year": "2019", "authors": "R Laroche; P Trichelair; Des Combes; R T "}, {"ref_id": "b29", "title": "Provably good batch reinforcement learning without great exploration", "journal": "Advances in Neural Information Processing Systems", "year": "2020", "authors": "Y Liu; A Swaminathan; A Agarwal; E Brunskill"}, {"ref_id": "b30", "title": "Convergent temporal-difference learning with arbitrary smooth function approximation", "journal": "", "year": "2009", "authors": "H R Maei; C Szepesvari; S Bhatnagar; D Precup; D Silver; R S Sutton"}, {"ref_id": "b31", "title": "Human-level control through deep reinforcement learning", "journal": "Nature", "year": "2015", "authors": "V Mnih; K Kavukcuoglu; D Silver; A A Rusu; J Veness; M G Bellemare; A Graves; M Riedmiller; A K Fidjeland; G Ostrovski"}, {"ref_id": "b32", "title": "Integral probability metrics and their generating classes of functions", "journal": "Advances in Applied Probability", "year": "1997", "authors": "A M\u00fcller"}, {"ref_id": "b33", "title": "Error bounds for approximate policy iteration", "journal": "", "year": "2003", "authors": "R Munos"}, {"ref_id": "b34", "title": "Finite-time bounds for fitted value iteration", "journal": "Journal of Machine Learning Research", "year": "2008", "authors": "R Munos; C Szepesv\u00e1ri"}, {"ref_id": "b35", "title": "A unified view of entropy-regularized markov decision processes", "journal": "", "year": "2017", "authors": "G Neu; A Jonsson; V G\u00f3mez"}, {"ref_id": "b36", "title": "Hyperparameter selection for offline reinforcement learning", "journal": "", "year": "2020", "authors": "T L Paine; C Paduraru; A Michi; C Gulcehre; K Zolna; A Novikov; Z Wang; N Freitas"}, {"ref_id": "b37", "title": "A game theoretic framework for model based reinforcement learning", "journal": "PMLR", "year": "2020", "authors": "A Rajeswaran; I Mordatch; V Kumar"}, {"ref_id": "b38", "title": "Td (0) converges provably faster than the residual gradient algorithm", "journal": "", "year": "2003", "authors": "R Schoknecht; A Merke"}, {"ref_id": "b39", "title": "Mastering the game of go with deep neural networks and tree search", "journal": "Nature", "year": "2016", "authors": "D Silver; A Huang; C J Maddison; A Guez; L Sifre; G Van Den Driessche; J Schrittwieser; I Antonoglou; V Panneershelvam; M Lanctot"}, {"ref_id": "b40", "title": "Deeply aggrevated: Differentiable imitation learning for sequential prediction", "journal": "PMLR", "year": "2017", "authors": "W Sun; A Venkatraman; G J Gordon; B Boots; J A Bagnell"}, {"ref_id": "b41", "title": "Reinforcement learning: An introduction", "journal": "MIT press", "year": "2018", "authors": "R S Sutton; A G Barto"}, {"ref_id": "b42", "title": "Of moments and matching: A game-theoretic framework for closing the imitation gap", "journal": "PMLR", "year": "2021", "authors": "G Swamy; S Choudhury; J A Bagnell; S Wu"}, {"ref_id": "b43", "title": "Representation learning for online and offline rl in low-rank mdps", "journal": "", "year": "2021", "authors": "M Uehara; X Zhang; W Sun"}, {"ref_id": "b44", "title": "Market structure and equilibrium", "journal": "Springer Science & Business Media", "year": "2010", "authors": "Von Stackelberg; H "}, {"ref_id": "b45", "title": "High-dimensional statistics: A nonasymptotic viewpoint", "journal": "Cambridge University Press", "year": "2019", "authors": "M J Wainwright"}, {"ref_id": "b46", "title": "A convergent and efficient deep q network algorithm", "journal": "", "year": "2021", "authors": "Z T Wang; M Ueda"}, {"ref_id": "b47", "title": "Behavior regularized offline reinforcement learning", "journal": "", "year": "2019", "authors": "Y Wu; G Tucker; O Nachum"}, {"ref_id": "b48", "title": "Q* approximation schemes for batch reinforcement learning: A theoretical comparison", "journal": "PMLR", "year": "2020", "authors": "T Xie; N Jiang"}, {"ref_id": "b49", "title": "Bellman-consistent pessimism for offline reinforcement learning", "journal": "", "year": "2021", "authors": "T Xie; C.-A Cheng; N Jiang; P Mineiro; A Agarwal"}, {"ref_id": "b50", "title": "Mopo: Model-based offline policy optimization", "journal": "", "year": "2020", "authors": "T Yu; G Thomas; L Yu; S Ermon; J Y Zou; S Levine; C Finn; T Ma"}, {"ref_id": "b51", "title": "Conservative offline model-based policy optimization", "journal": "", "year": "2021", "authors": "T Yu; A Kumar; R Rafailov; A Rajeswaran; S Levine; C Finn;  Combo"}, {"ref_id": "b52", "title": "Provable benefits of actor-critic methods for offline reinforcement learning", "journal": "", "year": "", "authors": "A Zanette; M J Wainwright; E Brunskill"}, {"ref_id": "b53", "title": "Towards hyperparameter-free policy selection for offline reinforcement learning", "journal": "", "year": "", "authors": "S Zhang; N Jiang"}, {"ref_id": "b54", "title": "Stackelberg actor-critic: Game-theoretic reinforcement learning algorithms", "journal": "", "year": "2021", "authors": "L Zheng; T Fiez; Z Alumbaugh; B Chasnov; L J Ratliff"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure1. Robust Policy Improvement. ATAC based on relative pessimism improves from behavior policies over a wide range of hyperparameters (\u03b2) that controls the degree of pessimism, and has a known safe policy improvement anchor point at \u03b2 = 0. Thus, we can gradually increase \u03b2 from zero to online tune ATAC, while not violating the performance baseline of the behavior policy. By contrast, offline RL based on absolute pessimism (e.g.,Xie et al., 2021) has safe policy improvement only for well-tuned hyperparameters. The differences are most stark in panel (d) where ATAC outperforms behavior for \u03b2 ranging over 3 orders of magnitude (0.01 to 10), compared with the narrow band of choices for absolute pessimism. The plots show the 25 th , 50 th , 75 th percentiles over 10 random seeds.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "a), 0), and d, f := (s,a)\u2208S\u00d7A d(s, a)f (s, a) for any d and f .", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 .2Figure2. Ablation of the DQRA loss with different mixing weights w in Eq.(7). The plots show the policy performance and TD error across optimization epochs of ATAC with the hopper-mediumreplay dataset. The stability and performance are greatly improved when w \u2208 (0, 1). For each w, the plot shows the 25 th , 50 th , 75 th percentiles over 10 random seeds.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Lemma 13) By the same argument of Xie et al. (2021, Proof of Theorem 4.1), we know for any k \u2208 [K],", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 .3Figure3. Ablation of the DQRA loss with different mixing weights w in Eq.(7). The plots show the policy performance and TD error across optimization epochs of ATAC with the hopper-medium-replay, hopper-medium, and hopper-medium-expert datasets from top to buttom. The stability and performance are greatly improved when w \u2208 (0, 1). For each w, the plot shows the 25 th , 50 th , 75 th percentiles over 10 random seeds.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "\u03c0 \u2208 argmax \u03c0\u2208\u03a0 E \u00b5 [f (s, \u03c0) \u2212 f (s, a)] s.t. f \u03c0 \u2208 argmin f \u2208F E \u00b5 [f (s, \u03c0) \u2212 f (s, a)] + \u03b2E \u00b5 [((f \u2212 T \u03c0 f )(s, a)) 2 ]", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "\u00b5 [f (s, \u03c0) \u2212 f (s, a)] + \u03b2E \u00b5 [((f \u2212 r)(s, a)) 2 ] (ATAC) f \u2190 argmin f \u2208F max \u03c0\u2208\u03a0 E \u00b5 [f (s, \u03c0) \u2212 f (s, a)] + \u03b2E \u00b5 [((f \u2212 r)(s, a)) 2 ] \u2212 \u03b2R(\u03c0) (CQL)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "1.min f \u2208F max \u03c0\u2208\u03a0 E \u00b5 [f (s, \u03c0) \u2212 f (s, a)] = 0, by 0 = min f \u2208F E \u00b5 [f (s, \u03c0) \u2212 f (s, a)] f (s, \u03c0) \u2212 f (s, a)] \u2264 max \u03c0\u2208\u03a0 E \u00b5 [f (s, \u03c0) \u2212 f (s, a)] any f \u2208 F, if there exists (s 1 , a 1 ) \u2208 S \u00d7 A such that \u00b5(s 1 ) > 0 and f (s 1 , a 1 ) > max a\u2208A\\a1 f (s 1 , a), then max \u03c0\u2208\u03a0 E \u00b5 [f (s, \u03c0) \u2212 f (s, a)] \u2265 E \u00b5 [f (s, \u03c0 f ) \u2212 f (s, a)] \u2265 \u00b5(s 1 )(f (s 1 , a 1 ) \u2212 f (s 1 , \u00b5)) \u2265 \u00b5(s 1 )(1 \u2212 \u00b5(a 1 |s 1 ))(f (s 1 , a 1 ) \u2212 max a\u2208A\\a1 f (s 1 , a)) > 0.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Behavior ATAC * ATAC ATAC * Table", "figure_data": "0ATAC0CQL COMBO TD3+BCIQLBChalfcheetah-rand-0.14.83.92.32.335.438.810.2-2.1walker2d-rand0.08.06.87.65.77.07.01.4-1.6hopper-rand1.231.817.531.618.210.817.911.0-9.8halfcheetah-med40.654.353.343.936.844.454.242.847.436.1walker2d-med62.091.089.690.589.674.575.579.778.36.6hopper-med44.2102.885.6103.594.886.694.999.566.329.0halfcheetah-med-replay27.149.548.049.247.246.255.143.344.238.4walker2d-med-replay14.894.192.594.289.832.656.025.273.911.3hopper-med-replay14.9102.8102.5102.7102.148.673.131.494.711.8halfcheetah-med-exp64.395.594.841.639.762.490.097.986.735.8walker2d-med-exp82.6116.3114.2114.5104.998.796.1101.1109.66.4hopper-med-exp64.7112.6111.983.046.5111.0111.1112.291.5111.9pen-human207.879.353.1106.161.737.5--71.534.4hammer-human25.46.71.53.81.24.4--1.41.5door-human28.68.72.512.27.49.9--4.30.5relocate-human86.10.30.10.50.10.2--0.10.0pen-cloned107.773.943.7104.968.939.2--37.356.9hammer-cloned8.12.31.13.20.42.1--2.10.8door-cloned12.18.23.76.00.00.4--1.6-0.1relocate-cloned28.70.80.20.30.0-0.1---0.2-0.1pen-exp105.7159.5136.2154.497.7107.0---85.1hammer-exp96.3128.4126.9118.399.286.7---125.6door-exp100.5105.599.3103.648.3101.5---34.9relocate-exp101.6106.599.4104.074.395.0---101.3"}, {"figure_label": "23", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Experimental results of ATAC and ATAC0 on the D4RL dataset and its confidence interval. We report the median score and the 25 th and 75 th percentiles, over 10 random seeds. Results of mujoco-v0 dataset. We grayed out the results of hopper-v0 because these datasets have bug (see D4RL github).", "figure_data": "ATAC  *  ATAC CQL TD3+BChalfcheetah-rand2.32.335.410.2walker2d-rand8.26.57.01.4hopper-rand12.112.010.811.0halfcheetah-med42.942.644.442.8walker2d-med84.083.074.579.7hopper-med53.333.586.699.5halfcheetah-med-replay43.341.746.243.3walker2d-med-replay33.721.832.625.2hopper-med-replay39.229.548.631.4halfcheetah-med-exp108.4107.562.497.9walker2d-med-exp111.8109.198.7101.1hopper-med-exp112.8112.5 111.0112.2"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "10 th 20 th 30 th 40 th 50 th 60 th", "figure_data": "70 th80 th90 th100 thhalfcheetah-rand0.91.01.01.01.01.11.41.51.85.5walker2d-rand1.92.33.55.616.5 64.6 134.0 139.1 159.2 519.1hopper-rand0.10.21.33.56.710.611.412.623.263.3halfcheetah-med-0.10.00.10.10.10.10.20.30.30.4walker2d-med0.10.20.30.30.30.40.40.40.40.5hopper-med0.20.20.30.40.40.60.70.91.11.4halfcheetah-med-replay0.60.60.60.70.70.80.80.90.91.0walker2d-med-replay-1.0-1.0-1.0-0.23.54.44.85.05.25.5hopper-med-replay-1.0-0.9-0.91.15.46.06.06.16.16.2halfcheetah-med-exp-0.6-0.5-0.4-0.3-0.2-0.00.20.40.50.5walker2d-med-exp-0.1-0.10.00.30.30.30.30.40.40.4hopper-med-exp-0.6-0.4-0.2-0.2-0.1-0.10.00.60.70.8pen-human-1.0-1.0-1.0-0.9-0.9-0.9-0.8-0.8-0.7-0.2hammer-human"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Robust Policy Improvement of ATAC in the Mujoco domains. ATAC based on relative pessimism improves from behavior policies over a wide range of hyperparameters that controls the degree of pessimism. On the contrary, absolute pessimism does not have this property and needs well-tuned hyperparameters to ensure safe policy improvement. The plots show the 25 th , 50 th , 75 th percentiles over 10 random seeds. Robust Policy Improvement of ATAC in the Adroit domains. ATAC based on relative pessimism improves from behavior policies over a wide range of hyperparameters that controls the degree of pessimism for the *-exp datasets. On the contrary, absolute pessimism does not have this property and needs well-tuned hyperparameters to ensure safe policy improvement. For *-human and *-cloned datasets, robust policy improvement is not observed empirically, likely because human demonstrators cannot be modeled by Markovian Gaussian policies (i.e. \u00b5 / \u2208 \u03a0). The plots show the 25 th , 50 th , 75 th percentiles over 10 random seeds.", "figure_data": "(a) halfcheetah-random (e) hopper-random (i) walker2d-random (d) hammer-human (b) halfcheetah-medium (f) hopper-medium (j) walker2d-medium (b) pen-cloned (c) halfcheetah-medium-replay (d) halfcheetah-medium-expert (g) hopper-medium-replay (h) hopper-medium-expert (k) walker2d-medium-replay (l) walker2d-medium-expert (c) pen-exp (e) hammer-cloned (f) hammer-exp Figure 4. (a) pen-human (g) door-human (h) door-cloned (i) door-exp(j) relocate-human(k) relocate-cloned(l) relocate-expFigure 5."}], "formulas": [{"formula_id": "formula_0", "formula_text": ") := E[ \u221e t=0 \u03b3 t r t |a t \u223c \u03c0(\u2022|s t )]", "formula_coordinates": [2.0, 370.23, 520.93, 130.53, 14.11]}, {"formula_id": "formula_1", "formula_text": "Q \u03c0 (s, a) := E[ \u221e t=0 \u03b3 t r t |(s 0 , a 0 ) = (s, a), a t \u223c \u03c0(\u2022|s t )]. By the boundedness of rewards, we have 0 \u2264 Q \u03c0 \u2264 Rmax 1\u2212\u03b3 =: V max . For a policy \u03c0, the Bellman operator T \u03c0 is defined as (T \u03c0 f ) (s, a) := R(s, a) + \u03b3E s |s,a [f (s , \u03c0)]", "formula_coordinates": [2.0, 307.44, 558.18, 235.39, 59.35]}, {"formula_id": "formula_2", "formula_text": "d \u03c0 (s, a) := (1 \u2212 \u03b3)E[ \u221e t=0 \u03b3 t 1(s t = s, a t = a)|a t \u223c \u03c0(\u2022|s t )].", "formula_coordinates": [2.0, 306.28, 641.87, 235.17, 24.67]}, {"formula_id": "formula_3", "formula_text": "Assumption 1 (Approximate Realizability). For any pol- icy \u03c0 \u2208 \u03a0, min f \u2208F max admissible \u03bd f \u2212 T \u03c0 f 2 2,\u03bd \u2264 \u03b5 F , where admissibilty \u03bd means \u03bd \u2208 {d \u03c0 : \u2200\u03c0 \u2208 \u03a0}.", "formula_coordinates": [3.0, 55.08, 290.23, 236.02, 35.71]}, {"formula_id": "formula_4", "formula_text": "\u03c0 * \u2208 argmax \u03c0\u2208\u03a0 L \u00b5 (\u03c0, f \u03c0 ) (1) s.t. f \u03c0 \u2208 argmin f \u2208F L \u00b5 (\u03c0, f ) + \u03b2E \u00b5 (\u03c0, f )", "formula_coordinates": [3.0, 341.08, 324.64, 200.36, 40.91]}, {"formula_id": "formula_5", "formula_text": "L \u00b5 (\u03c0, f ) := E \u00b5 [f (s, \u03c0) \u2212 f (s, a)](2)", "formula_coordinates": [3.0, 348.66, 383.05, 192.79, 10.0]}, {"formula_id": "formula_6", "formula_text": "E \u00b5 (\u03c0, f ) := E \u00b5 [((f \u2212 T \u03c0 f )(s, a)) 2 ].", "formula_coordinates": [3.0, 350.27, 397.5, 149.96, 11.72]}, {"formula_id": "formula_7", "formula_text": "\u00b5 \u2208 \u03a0, then L \u00b5 (\u03c0, f \u03c0 ) \u2264 (1 \u2212 \u03b3)(J(\u03c0) \u2212 J(\u00b5)) \u2200\u03c0 \u2208 \u03a0, for any \u03b2 \u2265 0. This implies J( \u03c0 * ) \u2265 J(\u00b5).", "formula_coordinates": [4.0, 55.44, 234.3, 235.74, 22.49]}, {"formula_id": "formula_8", "formula_text": "J(\u03c0) \u2212 J(\u00b5) = 1 1\u2212\u03b3 E \u00b5 [Q \u03c0 (s, \u03c0) \u2212 Q \u03c0 (s, a)]. Therefore, if Q \u03c0 \u2208 F on states of \u00b5, then (1 \u2212 \u03b3)(J(\u03c0) \u2212 J(\u00b5)) = L \u00b5 (\u03c0, Q \u03c0 ) = L \u00b5 (\u03c0, Q \u03c0 ) + \u03b2E(Q \u03c0 , \u03c0) \u2265 L \u00b5 (\u03c0, f \u03c0 ) + \u03b2E(f \u03c0 , \u03c0) \u2265 L \u00b5 (\u03c0, f \u03c0 ), where we use E(\u03c0, Q \u03c0 ) = 0 by definition of Q \u03c0 and E(\u03c0, f ) \u2265 0 for any f \u2208 F. Robust policy improvement follows, as J( \u03c0 * ) \u2212 J(\u00b5) \u2265 L \u00b5 ( \u03c0 * , f \u03c0 * ) \u2265 L \u00b5 (\u00b5, f \u00b5 ) = 0.", "formula_coordinates": [4.0, 55.13, 277.55, 236.05, 84.57]}, {"formula_id": "formula_9", "formula_text": "L D (f, \u03c0) := E D [f (s, \u03c0) \u2212 f (s, a)] ,(4)", "formula_coordinates": [4.0, 350.92, 545.62, 190.52, 10.0]}, {"formula_id": "formula_10", "formula_text": "E D (f, \u03c0) := E D (f (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 \u2212 min f \u2208F E D (f (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0))", "formula_coordinates": [4.0, 319.15, 571.56, 187.86, 40.06]}, {"formula_id": "formula_11", "formula_text": "f k \u2190 argmin f \u2208F k L D (f, \u03c0 k ) + \u03b2E D (f, \u03c0 k ).", "formula_coordinates": [5.0, 92.3, 133.05, 181.1, 11.26]}, {"formula_id": "formula_12", "formula_text": "\u03b5 \u03c0 opt := 1 1\u2212\u03b3 K k=1 E \u03c0 [f k (s, \u03c0) \u2212 f k (s, \u03c0 k )] = o(K).", "formula_coordinates": [5.0, 64.51, 442.5, 215.86, 14.56]}, {"formula_id": "formula_13", "formula_text": "\u03c0 k+1 (a|s) \u221d \u03c0 k (a|s) exp (\u03b7f k (s, a)) with \u03b7 = log |A| 2V 2 max K", "formula_coordinates": [5.0, 307.44, 94.67, 231.52, 15.99]}, {"formula_id": "formula_14", "formula_text": "\u03b5 \u03c0 opt \u2264 O Vmax 1\u2212\u03b3 K log |A| .", "formula_coordinates": [5.0, 360.41, 124.02, 121.72, 13.64]}, {"formula_id": "formula_15", "formula_text": "C (\u03bd; \u00b5, F, \u03c0) := max f \u2208F f \u2212T \u03c0 f 2 2,\u03bd f \u2212T \u03c0 f 2 2,\u00b5", "formula_coordinates": [5.0, 307.44, 228.27, 160.56, 19.31]}, {"formula_id": "formula_16", "formula_text": "choosing \u03b2 = \u0398 3 VmaxN 2 d 2 F ,\u03a0", "formula_coordinates": [5.0, 307.44, 473.31, 112.97, 16.51]}, {"formula_id": "formula_17", "formula_text": "J(\u03c0) \u2212 J(\u03c0) \u2264 \u03b5 \u03c0 opt + O Vmax \u221a C(d F ,\u03a0 ) 1 /3 (1\u2212\u03b3)N 1 /3 + 1 K(1\u2212\u03b3) K k=1 d \u03c0 \\ \u03bd, f k \u2212 T \u03c0 k f k , where (d \u03c0 \\ \u03bd)(s, a) := max(d \u03c0 (s, a) \u2212 \u03bd(s,", "formula_coordinates": [5.0, 307.44, 491.1, 231.21, 55.76]}, {"formula_id": "formula_18", "formula_text": "J(\u00b5) \u2212 J(\u03c0) \u2264 O Vmax 1 \u2212 \u03b3 dF,\u03a0 N + \u03b2V 2 max dF,\u03a0 (1 \u2212 \u03b3)N + \u03b5 \u00b5 opt .", "formula_coordinates": [6.0, 60.8, 442.38, 223.28, 21.51]}, {"formula_id": "formula_19", "formula_text": "Input: Batch data D, policy \u03c0, critics f 1 , f 2 , constants \u03b2 \u2265 0, \u03c4 \u2208 [0, 1], w \u2208 [0, 1] 1: Initialize target networksf 1 \u2190 f 1 ,f 2 \u2190 f 2 2: for k = 1, 2, . . . , K do 3:", "formula_coordinates": [6.0, 307.44, 83.17, 234.0, 58.62]}, {"formula_id": "formula_20", "formula_text": "f \u2208 {f 1 , f 2 }, update critic networks l critic (f ) := L Dmini (f, \u03c0) + \u03b2E w Dmini (f, \u03c0) f \u2190 Proj F (f \u2212 \u03b7 fast \u2207l critic ) 5: Update actor network l actor (\u03c0) := \u2212L Dmini (f 1 , \u03c0) \u03c0 \u2190 Proj \u03a0 (\u03c0 \u2212 \u03b7 slow \u2207l actor ) 6: For (f,f ) \u2208 {(f i ,f i )} i=1,2 , update target f \u2190 (1 \u2212 \u03c4 )f + \u03c4 f . 7: end for", "formula_coordinates": [6.0, 312.42, 145.01, 283.22, 104.53]}, {"formula_id": "formula_21", "formula_text": "(1 \u2212 \u03b3)(J(\u03c0) \u2212 J(\u03c0 k )) \u2264 E\u00b5 [f k \u2212 T \u03c0 k f k ] \u2212 E\u03c0 [f k \u2212 T \u03c0 k f k ] + E\u03c0 [f k (s, \u03c0) \u2212 f k (s, \u03c0 k )] + O V 2 max N + \u03b2V 2 max N .(6)", "formula_coordinates": [6.0, 307.44, 347.87, 239.04, 38.25]}, {"formula_id": "formula_22", "formula_text": "E w D (f, \u03c0) := (1 \u2212 w)E td D (f, f, \u03c0) + wE td D (f,f min , \u03c0) (7) where w \u2208 [0, 1], E td D (f, f , \u03c0) := E D [(f (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 ], andf min (s, a) := min i=1,2fi", "formula_coordinates": [7.0, 55.08, 518.47, 234.36, 38.13]}, {"formula_id": "formula_23", "formula_text": "\u03c1 F (f 1 , f 2 ) := f 1 \u2212 f 2 \u221e = sup (s,a)\u2208S\u00d7A |f 1 (s, a) \u2212 f 2 (s, a)|.(8)", "formula_coordinates": [13.0, 175.58, 109.69, 365.86, 17.33]}, {"formula_id": "formula_24", "formula_text": "\u03c1 \u03a0 (\u03c0 1 , \u03c0 2 ) := \u03c0 1 \u2212 \u03c0 2 \u221e,1 = sup s\u2208S \u03c0 1 (\u2022|s) \u2212 \u03c0 2 (\u2022|s) 1 ,(9)", "formula_coordinates": [13.0, 182.38, 167.52, 359.06, 16.87]}, {"formula_id": "formula_25", "formula_text": "f \u03c0 := argmin f \u2208F sup admissible \u03bd f \u2212 T \u03c0 f 2 2,\u03bd .", "formula_coordinates": [13.0, 218.44, 236.85, 160.01, 19.56]}, {"formula_id": "formula_26", "formula_text": "E D (f \u03c0 , \u03c0) \u2264 O V 2 max log |N\u221e(F , Vmax /N)||N\u221e,1(\u03a0, 1 /N)| /\u03b4 N + \u03b5 F =: \u03b5 r . We now show that E D (f, \u03c0) could effectively estimate f \u2212 T \u03c0 f 2 2,\u00b5 . Theorem 9. With probability at least 1 \u2212 \u03b4, for any \u03c0 \u2208 \u03a0, f \u2208 F, f \u2212 T \u03c0 f 2,\u00b5 \u2212 E D (f, \u03c0) \u2264 O V max log |N\u221e(F , Vmax /N)||N\u221e,1(\u03a0, 1 /N)| /\u03b4 N + \u221a \u03b5 F ,F .(10)", "formula_coordinates": [13.0, 54.97, 277.01, 486.47, 92.63]}, {"formula_id": "formula_27", "formula_text": "\u221a \u03b5 b := \u221a \u03b5 r + O V max log |N\u221e(F , Vmax /N)||N\u221e,1(\u03a0, 1 /N)| /\u03b4 N + \u221a \u03b5 F ,F . (11", "formula_coordinates": [13.0, 149.57, 413.75, 387.72, 22.49]}, {"formula_id": "formula_28", "formula_text": ")", "formula_coordinates": [13.0, 537.29, 420.98, 4.15, 8.64]}, {"formula_id": "formula_29", "formula_text": "g 1 \u2212 T \u03c0 f 2 2,\u00b5 \u2212 g 2 \u2212 T \u03c0 f 2 2,\u00b5 \u2212 1 N (s,a,r,s )\u2208D (g 1 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 + 1 N (s,a,r,s )\u2208D (g 2 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 \u2264 O \uf8eb \uf8ec \uf8edVmax g 1 \u2212 g 2 2,\u00b5 log |N\u221e(F , Vmax N )||N\u221e,1(\u03a0, 1 N )| \u03b4 N + V 2 max log |N\u221e(F , Vmax N )||N\u221e,1(\u03a0, 1 N )| \u03b4 N \uf8f6 \uf8f7 \uf8f8 .", "formula_coordinates": [13.0, 99.16, 509.04, 398.56, 88.94]}, {"formula_id": "formula_30", "formula_text": "(g 1 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 \u2212 1 N (s,a,r,s )\u2208D (g 2 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 = 1 N (s,a,r,s )\u2208D (g 1 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 \u2212 (g 2 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) = 1 N (s,a,r,s )\u2208D ((g 1 (s, a) \u2212 g 2 (s, a)) (g 1 (s, a) + g 2 (s, a) \u2212 2r \u2212 2\u03b3f (s , \u03c0))) .(12)", "formula_coordinates": [13.0, 119.28, 659.13, 357.82, 59.52]}, {"formula_id": "formula_31", "formula_text": "E \u00b5\u00d7(P,R) (g 1 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 \u2212 E \u00b5\u00d7(P,R) (g 2 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 (a) = E \u00b5\u00d7(P,R) [(g 1 (s, a) \u2212 g 2 (s, a)) (g 1 (s, a) + g 2 (s, a) \u2212 2r \u2212 2\u03b3f (s , \u03c0))] = E \u00b5 [E [(g 1 (s, a) \u2212 g 2 (s, a)) (g 1 (s, a) + g 2 (s, a) \u2212 2r \u2212 2\u03b3f (s , \u03c0))|s, a]] = E \u00b5 [(g 1 (s, a) \u2212 g 2 (s, a)) (g 1 (s, a) + g 2 (s, a) \u2212 2 (T \u03c0 f ) (s, a))] (13", "formula_coordinates": [14.0, 127.27, 121.22, 410.02, 65.24]}, {"formula_id": "formula_32", "formula_text": ") (b) = E \u00b5 (g 1 (s, a) \u2212 (T \u03c0 f ) (s, a)) 2 \u2212 E \u00b5 (g 2 (s, a) \u2212 (T \u03c0 f ) (s, a)) 2 ,(14)", "formula_coordinates": [14.0, 126.89, 177.13, 414.55, 28.66]}, {"formula_id": "formula_33", "formula_text": "E \u00b5\u00d7(P,R) \uf8ee \uf8f0 1 N (s,a,r,s )\u2208D (g 1 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 \u2212 1 N (s,a,r,s )\u2208D (g 2 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 \uf8f9 \uf8fb = E \u00b5 (g 1 (s, a) \u2212 (T \u03c0 f ) (s, a)) 2 \u2212 E \u00b5 (g 2 (s, a) \u2212 (T \u03c0 f ) (s, a)) 2 .", "formula_coordinates": [14.0, 92.28, 245.06, 412.33, 54.16]}, {"formula_id": "formula_34", "formula_text": "|F \u03b51 | = N \u221e (F, \u03b5 1 ), |\u03a0 \u03b52 | = N \u221e,1 (\u03a0, \u03b5 2 ); ii) there exist f , g 1 , g 2 \u2208 F \u03b51 and \u03c0 \u2208 \u03a0 \u03b52 , such that f \u2212 f \u221e , g 1 \u2212 g 1 \u221e , g 2 \u2212 g 2 \u221e \u2264 \u03b5 1 and \u03c0 \u2212 \u03c0 \u221e,1 \u2264 \u03b5 2", "formula_coordinates": [14.0, 55.44, 313.88, 486.0, 34.92]}, {"formula_id": "formula_35", "formula_text": "E \u00b5 g 1 (s, a) \u2212 T \u03c0 f (s, a) 2 \u2212 E \u00b5 g 2 (s, a) \u2212 T \u03c0 f (s, a) 2 \u2212 1 N (s,a,r,s )\u2208D g 1 (s, a) \u2212 r \u2212 \u03b3 f (s , \u03c0) 2 + 1 N (s,a,r,s )\u2208D g 2 (s, a) \u2212 r \u2212 \u03b3 f (s , \u03c0) 2 = E \u00b5 g 1 (s, a) \u2212 T \u03c0 f (s, a) 2 \u2212 E \u00b5 g 2 (s, a) \u2212 T \u03c0 f (s, a) 2 \u2212 1 N (s,a,r,s )\u2208D ( g 1 (s, a) \u2212 g 2 (s, a)) g 1 (s, a) + g 2 (s, a) \u2212 2r \u2212 2\u03b3 f (s , \u03c0) \u2264 4V \u00b5\u00d7(P,R) ( g 1 (s, a) \u2212 g 2 (s, a)) g 1 (s, a) + g 2 (s, a) \u2212 2r \u2212 2\u03b3 f (s , \u03c0) log |N\u221e(F ,\u03b51)||N\u221e,1(\u03a0,\u03b52)| \u03b4 N + 2V 2 max log |N\u221e(F ,\u03b51)||N\u221e,1(\u03a0,\u03b52)| \u03b4 3N ,", "formula_coordinates": [14.0, 82.27, 372.82, 429.96, 188.81]}, {"formula_id": "formula_36", "formula_text": "V \u00b5\u00d7(P,R) ( g 1 (s, a) \u2212 g 2 (s, a)) g 1 (s, a) + g 2 (s, a) \u2212 2r \u2212 2\u03b3 f (s , \u03c0) \u2264 E \u00b5\u00d7(P,R) ( g 1 (s, a) \u2212 g 2 (s, a)) 2 g 1 (s, a) + g 2 (s, a) \u2212 2r \u2212 2\u03b3 f (s , \u03c0) 2 \u2264 4V 2 max E \u00b5 ( g 1 (s, a) \u2212 g 2 (s, a)) 2 .", "formula_coordinates": [14.0, 139.52, 611.96, 312.09, 60.43]}, {"formula_id": "formula_37", "formula_text": "of | g 1 (s, a) + g 2 (s, a) \u2212 2r \u2212 2\u03b3 f (s , \u03c0)| \u2264 2V max . Therefore, w.p. 1 \u2212 \u03b4, g 1 \u2212 T \u03c0 f 2 2,\u00b5 \u2212 g 2 \u2212 T \u03c0 f 2 2,\u00b5 \u2212 1 N (s,a,r,s )\u2208D g 1 (s, a) \u2212 r \u2212 \u03b3 f (s , \u03c0) 2 + 1 N (s,a,r,s )\u2208D g 2 (s, a) \u2212 r \u2212 \u03b3 f (s , \u03c0) 2 \u2264 4V max g 1 \u2212 g 2 2,\u00b5 log |N\u221e(F ,\u03b51)||N\u221e,1(\u03a0,\u03b52)| \u03b4 N + 2V 2 max log |N\u221e(F ,\u03b51)||N\u221e,1(\u03a0,\u03b52)| \u03b4 3N", "formula_coordinates": [14.0, 127.31, 682.45, 414.07, 36.45]}, {"formula_id": "formula_38", "formula_text": "(g 1 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 + (g 2 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 \u2212 g 1 (s, a) \u2212 r \u2212 \u03b3 f (s , \u03c0) 2 + g 2 (s, a) \u2212 r \u2212 \u03b3 f (s , \u03c0) 2 = O(V max \u03b5 1 + V 2 max \u03b5 2 ),and", "formula_coordinates": [15.0, 55.44, 159.6, 425.3, 59.73]}, {"formula_id": "formula_39", "formula_text": "g 1 \u2212 g 2 2,\u00b5 = g 1 \u2212 g 2 + (g 1 \u2212 g 1 ) \u2212 (g 2 \u2212 g 2 ) 2,\u00b5 \u2264 g 1 \u2212 g 2 2,\u00b5 + g 1 \u2212 g 1 2,\u00b5 + g 2 \u2212 g 2 2,\u00b5 \u2264 g 1 \u2212 g 2 2,\u00b5 + 2\u03b5 1 . These implies g 1 \u2212 T \u03c0 f 2 2,\u00b5 \u2212 g 2 \u2212 T \u03c0 f 2 2,\u00b5 \u2212 1 N (s,a,r,s )\u2208D (g 1 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 + 1 N (s,a,r,s )\u2208D (g 2 (s, a) \u2212 r \u2212 \u03b3f (s , \u03c0)) 2 V max g 1 \u2212 g 2 2,\u00b5 log |N\u221e(F ,\u03b51)||N\u221e,1(\u03a0,\u03b52)| \u03b4 N + V 2 max log |N\u221e(F ,\u03b51)||N\u221e,1(\u03a0,\u03b52)| \u03b4 N + V max \u03b5 1 log |N\u221e(F ,\u03b51)||N\u221e,1(\u03a0,\u03b52)| \u03b4 N + V max \u03b5 1 + V 2 max \u03b5 2 .", "formula_coordinates": [15.0, 55.13, 224.47, 425.57, 182.21]}, {"formula_id": "formula_40", "formula_text": "Choosing \u03b5 1 = O( Vmax N", "formula_coordinates": [15.0, 55.44, 431.59, 93.49, 13.64]}, {"formula_id": "formula_41", "formula_text": "f \u03c0 := argmin f \u2208F sup admissible \u03bd f \u2212 T \u03c0 f 2 2,\u03bd g := argmin g \u2208F 1 N (s,a,r,s )\u2208D (g (s, a) \u2212 r \u2212 \u03b3f \u03c0 (s , \u03c0)) 2 .", "formula_coordinates": [15.0, 181.15, 463.47, 234.58, 51.43]}, {"formula_id": "formula_42", "formula_text": "f \u03c0 \u2212 g 2,\u00b5 \u2264 O \uf8eb \uf8ec \uf8edVmax log |N\u221e(F , Vmax N )||N\u221e,1(\u03a0, 1 N )| \u03b4 N + \u221a \u03b5 F \uf8f6 \uf8f7 \uf8f8 .", "formula_coordinates": [15.0, 165.18, 531.43, 271.51, 32.18]}, {"formula_id": "formula_43", "formula_text": "J(\u03c0) \u2212 J( \u03c0) = 1 1 \u2212 \u03b3 E \u00b5 f \u2212 T \u03c0 f (s, a) + E \u03c0 T \u03c0 f \u2212 f (s, a) + E \u03c0 [f (s, \u03c0) \u2212 f (s, \u03c0)] + L \u00b5 ( \u03c0, f ) \u2212 L \u00b5 ( \u03c0, Q \u03c0 ) .", "formula_coordinates": [16.0, 66.56, 193.33, 463.77, 36.3]}, {"formula_id": "formula_44", "formula_text": "J(\u03c0) \u2212 J( \u03c0) = (J(\u03c0) \u2212 J(\u00b5)) \u2212 (J( \u03c0) \u2212 J(\u00b5))", "formula_coordinates": [16.0, 198.24, 304.22, 200.41, 8.74]}, {"formula_id": "formula_45", "formula_text": "(1 \u2212 \u03b3) (J( \u03c0) \u2212 J(\u00b5)) = L \u00b5 ( \u03c0, Q \u03c0 ) = \u2206( \u03c0) + L \u00b5 ( \u03c0, f ) (\u2206( \u03c0) := L \u00b5 ( \u03c0, Q \u03c0 ) \u2212 L \u00b5 ( \u03c0, f )) = \u2206( \u03c0) + E \u00b5 [f (s, \u03c0) \u2212 f (s, a)] = \u2206( \u03c0) + (1 \u2212 \u03b3)(J R f, \u03c0 ( \u03c0) \u2212 J R f, \u03c0 (\u00b5))", "formula_coordinates": [16.0, 109.71, 333.67, 431.73, 58.5]}, {"formula_id": "formula_46", "formula_text": "= \u2206( \u03c0) + (1 \u2212 \u03b3)Q \u03c0 R f, \u03c0 (s 0 , \u03c0) \u2212 E \u00b5 [R \u03c0,f (s, a)] = \u2206( \u03c0) + (1 \u2212 \u03b3)f (s 0 , \u03c0) \u2212 E \u00b5 [R \u03c0,f (s, a)]. (by f (\u2022, \u2022) \u2261 Q \u03c0 R f, \u03c0 (\u2022, \u2022)) Therefore, (1 \u2212 \u03b3)(J(\u03c0) \u2212 J( \u03c0)) = (1 \u2212 \u03b3) (J(\u03c0) \u2212 f (d 0 , \u03c0)) (I) + E \u00b5 [R \u03c0,f (s, a)] \u2212 (1 \u2212 \u03b3)J(\u00b5)(II)", "formula_coordinates": [16.0, 55.13, 408.46, 486.31, 85.74]}, {"formula_id": "formula_47", "formula_text": "(II) = E \u00b5 [R \u03c0,f (s, a)] \u2212 (1 \u2212 \u03b3)J(\u00b5) = E \u00b5 [R \u03c0,f (s, a) \u2212 R(s, a)] = E \u00b5 [(f \u2212 T \u03c0 f )(s, a)].", "formula_coordinates": [16.0, 223.93, 521.24, 149.02, 45.17]}, {"formula_id": "formula_48", "formula_text": "(I) = (1 \u2212 \u03b3) (J(\u03c0) \u2212 f (s 0 , \u03c0)) = (1 \u2212 \u03b3)J(\u03c0) \u2212 E d \u03c0 [R \u03c0,f (s, a)](Ia)", "formula_coordinates": [16.0, 144.97, 594.25, 151.51, 40.28]}, {"formula_id": "formula_49", "formula_text": "+ E d \u03c0 [R \u03c0,f (s, a)] \u2212 (1 \u2212 \u03b3)f (s 0 , \u03c0)(Ib)", "formula_coordinates": [16.0, 298.15, 608.9, 149.34, 25.63]}, {"formula_id": "formula_50", "formula_text": "(Ib) = E d \u03c0 [R \u03c0,f (s, a)] \u2212 (1 \u2212 \u03b3)f (s 0 , \u03c0) = E d \u03c0 [f (s, \u03c0) \u2212 f (s, \u03c0)].", "formula_coordinates": [16.0, 214.51, 659.72, 167.87, 26.67]}, {"formula_id": "formula_51", "formula_text": "(Ia) = (1 \u2212 \u03b3)J(\u03c0) \u2212 E d \u03c0 [R \u03c0,f (s, a)] = E d \u03c0 [R(s, a) \u2212 R \u03c0,f (s, a)] = E d \u03c0 [(T \u03c0 f \u2212 f )(s, a)].", "formula_coordinates": [16.0, 221.3, 706.18, 154.28, 11.72]}, {"formula_id": "formula_52", "formula_text": "J(\u03c0) \u2212 J( \u03c0) = 1 1 \u2212 \u03b3 ((Ia) + (Ib) + (II) \u2212 \u2206( \u03c0)) = 1 1 \u2212 \u03b3 E \u00b5 f \u2212 T \u03c0 f (s, a) + E \u03c0 T \u03c0 f \u2212 f (s, a) + E \u03c0 [f (s, \u03c0) \u2212 f (s, \u03c0)] + L \u00b5 ( \u03c0, f ) \u2212 L \u00b5 ( \u03c0, Q \u03c0 ) .", "formula_coordinates": [17.0, 66.56, 124.04, 463.77, 62.22]}, {"formula_id": "formula_53", "formula_text": "k \u2208 [K], (1 \u2212 \u03b3) (J(\u03c0) \u2212 J(\u03c0 k )) \u2264 E \u00b5 [f k \u2212 T \u03c0 k f k ] + E \u03c0 [T \u03c0 k f k \u2212 f k ] + E \u03c0 [f k (s, \u03c0) \u2212 f k (s, \u03c0 k )] + O V max log |N\u221e(F , Vmax /N)||N\u221e,1(\u03a0, 1 /N)| /\u03b4 N + \u221a \u03b5 F + \u03b2 \u2022 O V 2 max log |N\u221e(F , Vmax /N)||N\u221e,1(\u03a0, 1 /N)| /\u03b4 N + \u03b5 F .", "formula_coordinates": [17.0, 57.93, 255.26, 499.63, 56.14]}, {"formula_id": "formula_54", "formula_text": "J(\u03c0) \u2212 J(\u03c0 k ) = E \u00b5 [f k \u2212 T \u03c0 k f k ] 1 \u2212 \u03b3 + E \u03c0 [T \u03c0 k f k \u2212 f k ] 1 \u2212 \u03b3 + E \u03c0 [f k (s, \u03c0) \u2212 f k (s, \u03c0 k )] 1 \u2212 \u03b3 + L \u00b5 (\u03c0 k , f k ) \u2212 L \u00b5 (\u03c0 k , Q \u03c0 k ) 1 \u2212 \u03b3 .", "formula_coordinates": [17.0, 65.04, 341.76, 466.81, 23.89]}, {"formula_id": "formula_55", "formula_text": "L \u00b5 (\u03c0 k , f k ) \u2212 L \u00b5 (\u03c0 k , Q \u03c0 k ). f \u03c0 := argmin f \u2208F sup admissible \u03bd f \u2212 T \u03c0 f 2 2,\u03bd , \u2200\u03c0 \u2208 \u03a0 \u03b5 stat := O V 2 max log |N\u221e(F , Vmax /N)||N\u221e,1(\u03a0, 1 /N)| /\u03b4 N , \u03b5 r := \u03b5 stat + O (\u03b5 F ) .", "formula_coordinates": [17.0, 163.49, 369.35, 243.77, 79.63]}, {"formula_id": "formula_56", "formula_text": "E D (\u03c0 k , f \u03c0 k ) \u2264 \u03b5 r . (15", "formula_coordinates": [17.0, 261.23, 470.44, 276.07, 10.32]}, {"formula_id": "formula_57", "formula_text": ")", "formula_coordinates": [17.0, 537.29, 470.76, 4.15, 8.64]}, {"formula_id": "formula_58", "formula_text": "For |L \u00b5 (\u03c0 k , Q \u03c0 k ) \u2212 L \u00b5 (\u03c0 k , f \u03c0 k )|, we have, L \u00b5 (\u03c0 k , Q \u03c0 k ) = E \u00b5 [Q \u03c0 k (s, \u03c0 k ) \u2212 Q \u03c0 k (s, a)] = (1 \u2212 \u03b3) (J(\u03c0 k ) \u2212 J(\u00b5)) = (1 \u2212 \u03b3) (f \u03c0 k (s 0 , \u03c0 k ) \u2212 J(\u00b5)) + (1 \u2212 \u03b3) (J(\u03c0 k ) \u2212 f \u03c0 k (s 0 , \u03c0 k )) = E \u00b5 [f \u03c0 k (s, \u03c0 k ) \u2212 (T \u03c0 k f \u03c0 k )(s, a)] + E d \u03c0 k [(T \u03c0 k f \u03c0 k )(s, a) \u2212 f \u03c0 k (s, a)]", "formula_coordinates": [17.0, 55.44, 484.42, 436.2, 72.29]}, {"formula_id": "formula_59", "formula_text": ")) = L \u00b5 (\u03c0 k , f \u03c0 k ) + E \u00b5 [f \u03c0 k (s, a) \u2212 (T \u03c0 k f \u03c0 k )(s, a)] + E d \u03c0 k [(T \u03c0 k f \u03c0 k )(s, a) \u2212 f \u03c0 k (s, a)] =\u21d2 |L \u00b5 (\u03c0 k , Q \u03c0 k ) \u2212 L \u00b5 (\u03c0 k , f \u03c0 k )| \u2264 f \u03c0 k \u2212 T \u03c0 k f \u03c0 k 2,\u00b5 + T \u03c0 k f \u03c0 k \u2212 f \u03c0 k 2,d \u03c0 k \u2264 O( \u221a \u03b5 F ),(16)", "formula_coordinates": [17.0, 55.44, 558.67, 492.43, 55.16]}, {"formula_id": "formula_60", "formula_text": "|L \u00b5 (\u03c0 k , f k ) \u2212 L D (\u03c0 k , f k )| + |L \u00b5 (\u03c0 k , f \u03c0 k ) \u2212 L D (\u03c0 k , f \u03c0 k )| \u2264 \u221a \u03b5 stat , \u2200k \u2208 [K].(17)", "formula_coordinates": [17.0, 135.91, 640.33, 405.53, 17.24]}, {"formula_id": "formula_61", "formula_text": "L \u00b5 (\u03c0 k , f k ) \u2212 L \u00b5 (\u03c0 k , Q \u03c0 k ) \u2264 L \u00b5 (\u03c0 k , f k ) + \u03b2E D (\u03c0 k , f k ) \u2212 L \u00b5 (\u03c0 k , Q \u03c0 k ) (E D (\u2022) \u2265 0) \u2264 L \u00b5 (\u03c0 k , f k ) + \u03b2E D (\u03c0 k , f k ) \u2212 L \u00b5 (\u03c0 k , f \u03c0 k ) \u2212 \u03b2E D (\u03c0 k , f \u03c0 k ) + O( \u221a F ) + \u03b2\u03b5 r", "formula_coordinates": [17.0, 64.54, 676.3, 476.9, 42.28]}, {"formula_id": "formula_62", "formula_text": "\u2264 L D (\u03c0 k , f k ) + \u03b2E D (\u03c0 k , f k ) \u2212 L D (\u03c0 k , f \u03c0 k ) \u2212 \u03b2E D (\u03c0 k , f \u03c0 k ) + O( \u221a F ) + \u221a \u03b5 stat + \u03b2 \u2022 O (\u03b5 stat + \u03b5 F ) (by Eq.(17)) \u2264 O ( \u221a \u03b5 F ) + \u221a \u03b5 stat + \u03b2 \u2022 O (\u03b5 stat + \u03b5 F ) (by the optimality of f k ) \u2264 O \u221a \u03b5 F + V max log |N\u221e(F , Vmax /N)||N\u221e,1(\u03a0, 1 /N)| /\u03b4 N + \u03b2 \u2022 O V 2 max log |N\u221e(F , Vmax /N)||N\u221e,1(\u03a0, 1 /N)| /\u03b4 N + \u03b5 F .", "formula_coordinates": [18.0, 64.54, 70.22, 476.9, 70.47]}, {"formula_id": "formula_63", "formula_text": "C (\u03bd; \u00b5, F, \u03c0 k ) \u2264 C, \u03b5 stat := O V 2 max log |N\u221e(F , Vmax /N )||N \u221e,1 (\u03a0, 1 /N )| /\u03b4 N", "formula_coordinates": [18.0, 203.89, 228.81, 276.17, 16.26]}, {"formula_id": "formula_64", "formula_text": "\u03b2 = O V 1/3 max (\u03b5 F +\u03b5stat) 2/3 and with probability at least 1 \u2212 \u03b4, J(\u03c0) \u2212 J(\u03c0) \u2264 O \u221a C \u221a \u03b5 F + \u221a \u03b5 F ,F + \u221a \u03b5 stat + (V max \u03b5 F + V max \u03b5 stat ) 1 /3 1 \u2212 \u03b3 + d \u03c0 \\ \u03bd, f k \u2212 T \u03c0 k f k 1 \u2212 \u03b3 .", "formula_coordinates": [18.0, 107.43, 248.11, 382.03, 63.7]}, {"formula_id": "formula_65", "formula_text": "\u03b5 stat := O V 2 max log |N\u221e(F , Vmax /N)||N\u221e,1(\u03a0, 1 /N)| /\u03b4 N .", "formula_coordinates": [18.0, 189.61, 341.7, 217.65, 23.89]}, {"formula_id": "formula_66", "formula_text": "J(\u03c0) \u2212 J(\u03c0) = 1 K K k=1 (J(\u03c0) \u2212 J(\u03c0 k )) \u2264 1 K K k=1 E \u00b5 [f k \u2212 T \u03c0 k f k ] 1 \u2212 \u03b3 (I) + E \u03c0 [T \u03c0 k f k \u2212 f k ] 1 \u2212 \u03b3 (II) + E \u03c0 [f k (s, \u03c0) \u2212 f k (s, \u03c0 k )] 1 \u2212 \u03b3 (III) + \u221a \u03b5 F + \u221a \u03b5 stat + \u03b2 \u2022 O(\u03b5 F + \u03b5 stat ) . (by", "formula_coordinates": [18.0, 60.44, 384.44, 476.01, 103.05]}, {"formula_id": "formula_67", "formula_text": "\u2264 \u221a \u03b5 b + V max /\u03b2 1 \u2212 \u03b3 (\u03b5 b is defined in Equation (11)) and (II) \u2264 2 \u221a C( \u221a \u03b5 b + V max /\u03b2) 1 \u2212 \u03b3 + d \u03c0 \\ \u03bd, f k \u2212 T \u03c0 k f k 1 \u2212 \u03b3 ,(I)", "formula_coordinates": [18.0, 55.44, 508.16, 486.0, 68.24]}, {"formula_id": "formula_68", "formula_text": "1 \u221a C \u221a \u03b5 F + \u221a \u03b5 F ,F + \u221a \u03b5 stat + V max /\u03b2 1 \u2212 \u03b3 + \u03b2(\u03b5 F + \u03b5 stat ) \uf8f6 \uf8f8 + 1 K K k=1 d \u03c0 \\ \u03bd, f k \u2212 T \u03c0 k f k 1 \u2212 \u03b3 . Algorithm 3 ATAC (Detailed Practical Version) Input: Batch data D, policy \u03c0, critics f 1 , f 2 , constants \u03b2 \u2265 0, \u03c4 \u2208 [0, 1], w \u2208 [0, 1], entropy lower bound Entropy min 1: Initialize target networksf 1 \u2190 f 1 ,f 2 \u2190 f 2 2: Initialize Lagrange multiplier \u03b1 \u2190 1 3: for k = 1, 2, . . . , K do 4:", "formula_coordinates": [18.0, 116.33, 615.85, 394.24, 102.09]}, {"formula_id": "formula_69", "formula_text": "f \u2208 {f 1 , f 2 }, update critic networks l critic (f ) := L Dmini (f, \u03c0) + \u03b2E w Dmini (f, \u03c0) # f \u2190 Proj F (f \u2212 \u03b7 fast \u2207l critic ) f \u2190 ADAM(f, \u2207l critic , \u03b7 fast ) f \u2190 ClipWeightL2(f ) 6: Update actor network # l actor (\u03c0) := \u2212L Dmini (f 1 , \u03c0) # \u03c0 \u2190 Proj \u03a0 (\u03c0 \u2212 \u03b7 slow \u2207l actor ) l actor (\u03c0, \u03b1) = \u2212L Dmini (f 1 , \u03c0) \u2212 \u03b1(E Dmini [\u03c0 log \u03c0] + Entropy min ) \u03c0 \u2190 ADAM(\u03c0, \u2207 \u03c0lactor , \u03b7 slow ) \u03b1 \u2190 ADAM(\u03b1, \u2212\u2207 \u03b1lactor , \u03b7 fast ) \u03b1 \u2190 max{0, \u03b1} 7: For (f,f ) \u2208 {(f i ,f i )} i=1,2 , update target networks f \u2190 (1 \u2212 \u03c4 )f + \u03c4 f . 8: end for Therefore, we choose \u03b2 = \u0398 V 1/3 max (\u03b5 F +\u03b5stat) 2/3 , and obtain J(\u03c0) \u2212 J(\u03c0) \u2264 O \u221a C \u221a \u03b5 F + \u221a \u03b5 F ,F + \u221a \u03b5 stat + (V max \u03b5 F + V max \u03b5 stat ) 1 /3 1 \u2212 \u03b3 + 1 K K k=1 d \u03c0 \\ \u03bd, f k \u2212 T \u03c0 k f k 1 \u2212 \u03b3 .", "formula_coordinates": [19.0, 55.13, 145.56, 449.01, 269.91]}, {"formula_id": "formula_70", "formula_text": "score RPI (\u03c0) := J(\u03c0) \u2212 J(\u00b5) |J(\u00b5)| (18)", "formula_coordinates": [22.0, 240.13, 579.52, 301.31, 22.31]}, {"formula_id": "formula_71", "formula_text": "-1.1 -1.1 -1.1 -1.1 -1.0 -1.0 -1.0 -1.0 -1.0 0.9 door-human -1.1 -1.1 -1.1 -1.1 -1.1 -1.1 -1.1 -1.0 -0.9 -0.1 relocate-human -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -0", "formula_coordinates": [23.0, 119.06, 381.68, 369.79, 28.05]}, {"formula_id": "formula_73", "formula_text": "f k+1 \u2190 argmin f \u2208F max \u03c0\u2208\u03a0 \u03b1E \u00b5 [f (s, \u03c0) \u2212 f (s, a)] \u2212 R(\u03c0) + E \u00b5 [((f \u2212 T \u03c0 k f k )(s, a)) 2 ](20)", "formula_coordinates": [26.0, 133.14, 540.9, 408.3, 18.67]}, {"formula_id": "formula_74", "formula_text": "f k+1 \u2190 argmin f \u2208F max \u03c0\u2208\u03a0 E \u00b5 [f (s, \u03c0) \u2212 f (s, a)].", "formula_coordinates": [27.0, 209.91, 523.51, 177.05, 16.6]}], "doi": ""}