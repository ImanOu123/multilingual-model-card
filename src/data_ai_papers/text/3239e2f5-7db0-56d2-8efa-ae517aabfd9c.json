{"title": "Computational Imaging on the Electric Grid", "authors": "Mark Sheinin; Yoav Y Schechner; Kiriakos N Kutulakos", "pub_date": "", "abstract": "Night beats with alternating current (AC) illumination. By passively sensing this beat, we reveal new scene information which includes: the type of bulbs in the scene, the phases of the electric grid up to city scale, and the light transport matrix. This information yields unmixing of reflections and semi-reflections, nocturnal high dynamic range, and scene rendering with bulbs not observed during acquisition. The latter is facilitated by a database of bulb response functions for a range of sources, which we collected and provide. To do all this, we built a novel codedexposure high-dynamic-range imaging technique, specifically designed to operate on the grid's AC lighting.", "sections": [{"heading": "Introduction", "text": "For more than a century we have been living in a world that literally pulses with artificial light. Whether outdoors at night or indoors at all hours, most of the light reaching our eyes-and our cameras-originates from artificial sources powered by the electric grid. These light sources change their intensity and spectral power distribution in response to the grid's alternating current (AC) [3,43] but their flicker is usually too subtle and too fast to notice with the naked eye (100Hz or more) [22]. Artificial lighting produces unnatural-looking colors in photos [13] and temporal aliasing in video [39]. As a result, it is broadly considered undesirable [16,21,40,48].\nIn this paper we argue that rather than being a mere nuisance, ubiquitous AC-induced lighting variations are a very powerful visual cue-about our indoor and outdoor environments, about the light sources they contain, and the electrical grid itself (Figure 1). To this end, we derive a model of time-varying appearance under AC lighting and describe a novel coded-exposure imaging technique to acquire it.\nOur approach yields several never-seen-before capabilities that we demonstrate experimentally with our \"ACam\" camera prototype: (1) acquiring a scene's transport matrix by passive observation only, (2) computing what a scene would look like if some of its lights were turned off or changed to a different bulb type, (3) recognizing bulb types from their temporal profiles, (4) analyzing city-scale grid phases in the electric grid, and (5) doing all the above under very challenging conditions-nocturnal imaging, an off-the-shelf (30Hz) camera, dimly-lit scenes, uncontrolled environments, distances of meters to kilometers, and operation in two continents using both 110V and 220V AC standards. To enable all this, we compiled a database [35,36] of temporal lighting response functions (DELIGHT) for a range of bulb types, the first of its kind in computer vision. The only essential constraints in our approach are access to a power outlet and a largely stationary scene.", "publication_ref": ["b2", "b42", "b21", "b12", "b38", "b15", "b20", "b39", "b47", "b1", "b34", "b35"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "City image", "text": "Our work draws inspiration from the large body of research on actively-controlled light sources. These techniques illuminate a scene with a variety of sources (e.g., projectors [18,24,32], lasers [11,25], computer dis-plays [49], flashes [28] and arrays of point sources [8,45], etc.) in order to impose predictable structure on an otherwise-unstructured visual world. Two lines of research in this area are particularly close to ours. First, methods for computational light transport [7] express the linear relation between controllable light sources and images as a transport matrix that can be acquired [33] or probed [26]. We adopt and extend the transport matrix formulation to AC light sources, and demonstrate scene re-lighting without any access to a programmable source. Second, recent work has treated the imaging process as a radio-like communication channel between active sources and cameras [15,19]. These techniques transmit periodic signals between lights and cameras at high speed but, like all active methods, they reject ambient AC light rather than use it.\nThe key observation behind our work is that ambient AC lighting has a great deal of structure already. This is because of two fortunate facts: (1) AC light sources often do not flicker with the same phase even if located in the same space and (2) their temporal intensity profile is different depending on bulb type, make and model. The former comes from a desire to spread evenly the three phases of AC across light sources in order to balance load on the grid and make flicker even less noticeable [43]. The latter comes from differences in power circuitry and in the mechanism of light emission (fluorescence [47], incandescence [4], LED, etc.) Thus, the light arriving at a camera pixel is a mixture of differentlyshifted and potentially very diverse signals: even among household LED bulbs, we have observed modulations down to 10% of maximum intensity in some products and nearconstant intensity in others. The precise mixture of these light signals differs from pixel to pixel in accordance with the scene's light transport properties.\nHere we undertake a first systematic study of how to passively record, untangle, and use these signals. In this sense, our work is another example of exploiting visual cues \"hidden in plain sight.\" [41,46] The AC cue is very difficult to acquire with high-speed cameras because there is seldom enough light to record useful images at the speed we would need (over 1000 frames per second). On the other hand, long-exposure photography is not an option either because of the cue's transient nature. To overcome these challenges we use a novel codedexposure imaging [12,26,31,42] technique. Our ACam acquires high-dynamic-range (HDR) images corresponding to fractions of the AC cycle by capturing long exposures while masking and unmasking pixels individually at 2.7kHz, in sync with the AC.", "publication_ref": ["b17", "b23", "b31", "b10", "b24", "b48", "b27", "b7", "b44", "b6", "b32", "b25", "b14", "b18", "b42", "b46", "b3", "b40", "b45", "b11", "b25", "b30", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Alternating-Current Illumination", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Alternating Current in The Grid", "text": "We now describe a model of AC-modulated lighting. Power suppliers strive for a zero-mean sinusoidal AC volt-age having a regular peak outlet amplitude 1 V max . There are two exclusive standards, having nominal frequencies 50Hz and 60Hz. The Americas use the former, while Asia and Europe mainly use the latter. Imperfections in electricity generation slightly wiggle the AC frequency randomly. Hence, the AC is quasi periodic: for a short time span, the effective frequency is a perturbation of the nominal frequency. The wiggle is practically spatially invariant in spatiotemporal scales typical to computer vision: the temporary frequency of the AC is essentially the same in any electrical outlet across the city. The reason is that electricity perturbations propagate at a speed on the order of the speed of light.\nIn practice, the temporary frequency of the AC is determined from the time interval \u2206 between two successive zero crossings (Figure 2[top-left]). Since there are two such crossings per period of the AC, its frequency is given by\nf = 1/(2\u2206) .(1)\nThe electric grid carries AC in a discrete set P of grid phases, using distinct, exclusive sets of cables. In most scenes there are three such phases spaced 2\u03c0/3 apart. Each outlet is connected to one of these grid phases. In our labs, we declared one outlet to be the reference, having phase \u03c6 = 0. Hence, P = {0, 2\u03c0/3, 4\u03c0/3} (see Figure 1). 2 Now suppose we count time t with a stopwatch, beginning from some negative-to-positive zero crossing of the voltage at the reference outlet (Figure 2[top-left]). The AC voltage is then\nV (t) = V max sin(2\u03c0f t \u2212 \u03c6) .(2)", "publication_ref": ["b1"], "figure_ref": ["fig_1", "fig_0", "fig_1"], "table_ref": []}, {"heading": "From AC Electricity to Light", "text": "A bulb \u03b2 is a system whose input is the voltage V (t) and its output is spectral flux L \u03b2 (t, \u03bb), where \u03bb denotes wavelength. Hypothesize for a moment a bulb which is electrically linear, i.e., the current J(t) satisfies a proportionality J(t) \u221d V (t). Then, hypothesize that this bulb is unmediated, converting electric power J(t)V (t) \u221d V 2 (t) to flux directly and instantaneously. Thus, the spectral flux L \u03b2 (t, \u03bb) is equivalent to V 2 (t). Consequently, the hypothetical bulb flickers at double the AC frequency and becomes dark whenever V (t) goes to zero. We call this flickering period a cycle, whose duration is \u2206.\nIn practice, the transition from electricity to radiance is mediated by various mechanisms. Optical mediators include heat, gas discharge and phosphorescence. Nonincandescent bulbs generally have electronic components inside the bulb fixture, to which the lay person is oblivious. These components (diodes, inductors, etc.) mediate \nL \u03b2 (t, \u03bb) is a distortion of V 2 (t):\nthere is a delay, and L \u03b2 (t, \u03bb) generally does not go to zero during a cycle. Denote by B the finite set of bulbs in use. Consider a bulb \u03b2 \u2208 B, such as a particular fluorescent bulb in a brand fixture, whose time-averaged spectral flux over one cycle is L \u03b2 (\u03bb). Relative to this average, at time t the bulb emission fluctuates as:\nL \u03b2 (t, \u03bb) = L \u03b2 (\u03bb) B \u03b2 (t, \u03bb) .(3)\nWe define the unit-less function B \u03b2 (t, \u03bb) to be the spectral bulb response function (SBRF). This function has a time average of 1 for each wavelength and serves as an intrinsic model of a bulb's temporal behavior. Acquiring a lamp's SBRF requires specialized equipment like integrating spheres and high-speed spectrometers. As such, measuring the SBRF directly is rather involved. A more practical model of bulb behavior is to consider the time-varying measurements from a camera or photodiode placed nearby (with or without color filters):\nI \u03b2 (t, \u03c3) = I \u03b2 (\u03c3) B * \u03b2 (t, \u03c3) .(4)\nHere I \u03b2 (t, \u03c3) is the intensity measured at a pixel or photodiode at time t and spectral band \u03c3, I \u03b2 (\u03c3) is its temporal average and B * \u03b2 (t, \u03c3) is the unit-less bulb response function (BRF). Unlike the SBRF, the BRF depends on the placement and spectral sensitivity of the device used. 3 In general, both the SBRF and the BRF may exhibit a slightly different temporal profile across cycles (e.g., due 3 Specifically, the spectral flux and measured intensity are related by the integral I \u03b2 (t, \u03c3) = G L \u03b2 (t, \u03bb)R(\u03c3, \u03bb)d\u03bb where R(\u03c3, \u03bb) is the sensor's spectral sensitivity. The geometric factor G converts emitted flux to pixel/photodiode intensity and depends on their placement, aperture, etc.\nto voltage polarity, warm-up period, ambient temperature, etc.) Here we ignore these secondary effects for the sake of simplicity, treating BRFs as essentially invariant to the number of cycles since time zero. Thus, our BRFs are fully specified by their values in very first cycle. In the following we restrict t to lie in the interval [0, \u2206] and treat the BRF as a function that is defined over just that interval.\nCameras and photodiodes provide discrete samples of the continuous intensity I \u03b2 (t, \u03c3). Suppose I \u03b2 (t, \u03c3) is resolved into K samples within a cycle. These samples correspond to integrals of I \u03b2 (t, \u03c3) over consecutive time intervals of duration \u2206/K. Thus, Eq. (4) becomes\ni \u03b2 (\u03c3) = I \u03b2 (\u03c3) b \u03b2 (\u03c3)(5)\n= \u03c3 I \u03b2 (\u03c3) brightness I \u03b2 (\u03c3) \u03c3 I \u03b2 (\u03c3) chromaticity Q \u03b2 (\u03c3) b \u03b2 (\u03c3) BRF (6\n)\nwhere the K-dimensional row vectors i \u03b2 (\u03c3) and b \u03b2 (\u03c3) hold the intensity and BRF samples, respectively. Figure 2 shows several examples of sampled BRFs. As can be seen, all bulbs flicker at double the AC frequency and are locked to individual cycles.", "publication_ref": ["b2", "b2"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "The DELIGHT Database of Bulb Responses", "text": "For the tasks in Sections 4 and 5 we created a Database of Electric LIGHTs (DELIGHT). We acquired a variety of bulbs and fixtures. Street lighting is dominated by a few bulb types, mainly high pressure sodium, metal halide, mercury and fluorescent. Each streetlight type is used rather consistently in large areas. Indoor lighting has higher variety, including halogen, fluorescent tubes, different compact fluorescent lamps (CFLs) and simple incandescent. LED lighting has an interesting variety of BRFs, some having very low and some very high BRF amplitudes (Figure 2).\nTo keep t common to all BRFs, DELIGHT was acquired by connecting all bulbs and fixtures to a single 50Hz reference outlet in Haifa. The AC voltage V (t) was simultaneously measured at this outlet. We used three sensing schemes: (1) a photodiode with one of three color filters; (2) the same photodiode without any filters; and (3) our ACam prototype described in Section 6, fitted with a color camera. For schemes (1) and ( 3), we save in DELIGHT the BRF of individual bulbs and their chromaticity. For scheme (2) only a monochrome BRF is saved. In all cases, metadata such as bulb wattage and sensor/filter used are stored as well. See [34,35,36] for more information.", "publication_ref": ["b33", "b34", "b35"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Recognizing AC Lights and Grid Phase", "text": "Let us point a camera at a bulb in the scene. The measured signal i(\u03c3) follows Eq. ( 6). This signal is normalized by the mean brightness, yielding i norm (\u03c3). Now, all temporal variations are due to the bulb's BRF, chromaticity and grid phase. We recognize the bulb and its phase using:\n\u03b2 ,\u03c6 = arg min \u03b2\u2208B,\u03c6\u2208P \u03c3 i norm (\u03c3) \u2212 Q \u03b2 (\u03c3) shift \u03c6, b \u03b2 (\u03c3) 2 (7)\nwhere B is the set of bulbs in DELIGHT, P is the set of possible grid phases, Q \u03b2 (\u03c3) is the chromaticity of bulb \u03b2 in the database, and shift() circularly shifts to the right the bulb's sampled BRF by phase \u03c6. When using a monochrome camera, there is only one spectral band so Q \u03b2 (\u03c3) = 1.\nFigures 1 and 3 show results from Haifa Bay, where the ACam was fitted with a monochrome camera. In this metropolitan scale, we recognize the bulb types and their three grid phases. Simple analysis shows that the distribution of grid phases is approximately uniform over the bulbs detected in the field of view.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Theory of AC Light Transport", "text": "To simplify notation, we drop the spectral band \u03c3 wherever we can. A scene contains static objects and is illuminated by S light sources. It is observed by a camera having a linear radiometric response and P pixels. As in Section 2.2, we resolve the time-varying image into K frames. Now suppose only source s is on, with chromaticity Q s , BRF b s and phase 0. Furthermore, suppose matrix I s holds the resulting single-source image sequence. Each column of I s is a frame and each row is the intensity of one pixel through time. At frame k pixel p's intensity follows Eq. (6):\nI s [p, k] = \u03c4 ps Q s b s [k](8)\nwhere brackets denote individual elements of I s and b s . The factor \u03c4 ps expresses light transport. This factor specifies the total flux transported from source s to pixel p via all possible paths. This transport encapsulates global factors such as the camera's numerical aperture and spectral   response; spatial and angular variations in radiance at pixel p by source s; the BRDF at p when illuminated by s; shadows, inter-reflections, etc. Expressing Eq. (8) in matrix form we obtain:\nI s = \u03c4 s Q s b s . (9\n)\nHere column vector \u03c4 s concatenates the transport factors of all pixels for source s. It follows that individual frames of the sequence are just scalings of vector \u03c4 s . Now, the scene is illuminated by S sources connected to phase zero. The image sequence becomes a superposition of S single-source sequences, one per source s:\nI = I 1 + \u2022 \u2022 \u2022 + I s + \u2022 \u2022 \u2022 + I S .(10)\nSuppose the chromaticities and BRFs of these sources are b 1 , . . . , b S and Q 1 , . . . , Q S , respectively. Combining Eqs. ( 9) and ( 10), factorizing various terms and denoting \u22a4 for transpose we obtain\nI = [\u03c41 \u2022 \u2022 \u2022 \u03c4S] [Q1b \u22a4 1 \u2022 \u2022 \u2022 QSb \u22a4 S ] \u22a4 (11) = [\u03c41 \u2022 \u2022 \u2022 \u03c4S] transport matrix T \uf8ee \uf8ef \uf8f0 Q1 0 . . . 0 QS \uf8f9 \uf8fa \uf8fb chromaticity matrix Q \uf8ee \uf8ef \uf8f0 b1 . . . bS \uf8f9 \uf8fa \uf8fb BRF matrix B(12)\n= TQB .  Matrix T is the scene's P \u00d7 S transport matrix. Each column of T describes the appearance of the scene when a specific source is turned on. This matrix is time-invariant and generally unknown.\nFinally, suppose the sources in the scene have phases \u03c6 1 , . . . , \u03c6 S instead of being zero. The BRF matrix in Eq. ( 13) now contains BRFs that have been circularly shifted individually according to their sources' phase:\nB = [ shift(\u03c6 1 , b 1 ) \u22a4 \u2022 \u2022 \u2022 shift(\u03c6 S , b S ) \u22a4 ] \u22a4 . (14)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Unmixing: Source Separation", "text": "Single-source sequences are linearly mixed in the data I. We seek unmixing, i.e., linear source separation [2]. The key is to estimate the transport matrix T based on Eq. (13).\nConsider any two sources s 1 and s 2 that are connected to the same phase and have the same BRF. According to Eq. (9), the two-source sequence due to these sources is\nI s1 + I s2 = (\u03c4 s1 Q s1 + \u03c4 s2 Q s2 ) shift(\u03c6 s1 , b s1 ) . (15)\nThus the contributions of the two sources add up as if the scene is illuminated by a single source having the same phase and BRF. The contribution of these sources is therefore unseparable. Divide all sources used in the scene into subsets of sources, where each subset has no linear dependency to another. We consider unmixing only across these linearly-independent subsets. For the rest of the paper we refer to these independent subsets as the S \"sources.\".\nAssume we know QB. This is measured in two ways:\n(a) Sources are very often in the field of view. Thus their BRFs and chromaticities can be acquired directly by our ACam. This is also possible for pixels dominated by one source (e.g., reflections from nearby surfaces).\n(b) Sampling the signal as in (a) and then using DELIGHT and the recognition method of Section 4.\nThe transport matrix is estimated usin\u011d\nT = arg min T\u22650 W \u2299 I \u2212 TQB 2 F ,(16)\nwhere \u2299 denotes a Hadamard (element-wise) multiplication and F is the Frobenius norm. The P \u00d7 K weight matrix W discards saturated data:\nW[p, k] = 0 if any spectral band is saturated at I[p, k] 1 otherwise . (17\n)\nEq. ( 16) is a simple least-squares estimator. Due to noise and minor differences between sources of the same class, the assumption of a known QB is not precisely met. To counter slight inconsistencies, a refinement allows B to change a bit. UsingT derived in Eq. (16), we compute:\nB = arg min B\u22650 W \u2299 I \u2212TQB 2 F . (18\n)\nAfter this least-squares estimation ofB, the estimation in Eq. ( 16) is applied again usingB. We have observed in our experiments that, unless this refinement is done, the result may suffer from minor artifacts (see example in [34]). Each column ofT is an unmixed image of the scene. This image is already white balanced because the chromaticities of all sources are factored into Q. Examples are shown in Figures 4 and 5.", "publication_ref": ["b1", "b12", "b33"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "High Dynamic Range, Denoised Rendering", "text": "We can now reconstruct the single-source image sequence of a source s usin\u011d\nI s =\u03c4 s Q s shift(\u03c6 s , b s )(19)\nOriginal Fluorescent Sodium Sodium where\u03c4 s is the corresponding column ofT. The intensities in this sequence can safely exceed the saturation level of the sensor. This is because Eqs. ( 16) and ( 17) bypass saturated data when estimatingT. We therefore obtain high dynamic range results thanks to the AC (Figure 4[middle plot]).\nThe unmixing process also leads to denoising. Intensities in the captured image sequence suffer from sensor readout noise. Yet, since Eq. ( 19) forces all pixels to vary in synchrony according to a common BRF, the rendered sequenc\u00ea I s is less noisy than the input data (Figure 4([right plot])).\nLast but not least, light sources can be changed to bulbs that were not seen at all during the acquisition. Changing bulbs means changing their chromaticity and BRF to that of other bulbs (e.g., in DELIGHT or merely hallucinated). Moreover, we can change the grid phase of light sources and can use a diagonal amplification matrix A to amplify or de-amplify them. This leads to generalized relighting:\nI relight =T [AQB] relight .(20)", "publication_ref": [], "figure_ref": ["fig_4", "fig_4"], "table_ref": []}, {"heading": "Semi-Reflection Separation", "text": "To separate a semi-reflection from a transmitted scene [1,20,37,38], we show a new principle: passive AC-based unmixing. We realize this principle using either one of the following two mechanisms:\n\u2022 AC-illuminated scene: When all light sources originate from AC-powered bulbs, unmixing is done as described in Section 5.1. See [34] for example results. \u2022 Natural illumination involved: Scene illumination contains an outdoor component from daylight.\nThe indoor environment is illuminated by two kinds of sources. First, part of the natural daylight illuminates the indoors through a window. The second light source indoors is connected to the AC grid. In this case \u03c4 out and \u03c4 ac correspond to the two sources. Since daylight is approximately time-invariant at timescales of a few thousand cycles, its BRF is a vector of all ones. The bulb's BRF b ac is unknown, i.e., we are not relying on any database or known grid phase. As before, our input data is an image sequence I. We ignore chromaticities for brevity. Now consider two frames\nk 1 and k 2 with b ac [k 1 ]>b ac [k 2 ].\nThe corresponding images, represented by columns of I, are:\nI[k 1 ] = \u03c4 out + \u03c4 ac b ac [k 1 ](21)\nI[k 2 ] = \u03c4 out + \u03c4 ac b ac [k 2 ] .(22)\nIt follows that vectors \u03c4 ac and\nI[k 1 ] \u2212 I[k 2 ]\nare equal up to a scale factor. Along similar lines, it is possible to show that vectors \u03c4 out and I[k 2 ] \u2212 AI[k 1 ] are also equal up to a scale factor for some unknown scalar A. We estimate this scalar using independent component analysis (ICA) [14]. Specifically, A is optimized to minimize the mutual information of vectors\nI[k 1 ] \u2212 I[k 2 ] and I[k 2 ] \u2212 AI[k 1 ]\n. This yields the result shown in Figure 6.", "publication_ref": ["b0", "b19", "b36", "b37", "b33", "b13"], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "The Alternating-Current Camera (ACam)", "text": "The previous section relies on a key image acquisition task: capturing a sequence of K frames that spans one cycle. Very little light, however, enters the camera at the timescale of 1/K-th the AC cycle. 4 This is especially problematic at night and indoors where light levels are usually low and sensor readout noise overwhelms the signal. Moreover, frame acquisition must support HDR imaging. This is because the field of view may include both bright light sources and poorly-lit surfaces (e.g., from shadows, squared-distance light fall-off, AC flicker, etc.) These issues make capturing K-frame sequences impractical with a high-speed camera.\nTo overcome them, our ACam keeps its electronic shutter open for hundreds of cycles while optically blocking its sensor at all times except during the same brief interval in each cycle. This is illustrated in Figure 7[top]. Since the light collected by the sensor is proportional to the number of cycles the electronic shutter is open, the ACam trades off acquisition speed for enhanced signal-to-noise ratio. Moreover, it can handle large variations in light level across the field of view by allowing some sensor pixels to integrate light for fewer cycles than others (Figure 7[bottom]). Just like other coded-exposure techniques [12,31,42], we implement high-speed pixel masking with a digital micromirror device (DMD) that is optically coupled to an off-the-shelf camera. We adopt the overall design proposed in [26], modifying it for the purpose of passive AC-modulated imaging. Figure 8 shows our ACam and highlights its main differences from the system in [26]. It operates correctly on 60Hz/120V and 50Hz/220V grids.\nEach ACam image yields exactly one frame of the K-frame sequence, indexed by k \u2208 [1 . . . K]. The procedure is applied K times to acquire all frames -and is potentially applied more times if HDR frames are needed. by three quantities: the number of cycles C, the matrix M holding the mask sequence, and the timing signal that forces the DMD to switch from one mask to the next.\nOur mask matrix has the following general form:\nM = [ m1 0 m2 0 . . . m M/2 0 ](23)\nwhere m m is a column vector representing a binary pixel mask and 0 is a mask of all zeros. The zero mask blocks the sensor completely and is active at all times except during the interval corresponding to frame k. The non-zero mask, on the other hand, determines which pixels are actually exposed to light during that interval. To acquire a non-HDR image we set m m = 1 for all m. This forces the DMD to act like a \"flutter-shutter\" [29] synchronized with the AC. To acquire an HDR image we modify M adaptively over repeated long-exposure acquisitions (see below).\nAC-locked mask switching We generate the maskswitching signal with an Arduino plugged into the reference outlet (Figure 7[top]). We found it very important to generate this signal in a closed loop, locked to the last-detected zero-crossing. Given that the duration of each cycle varies slightly, switching masks without accounting for this variation causes their position within a cycle to drift over time and leads to poor results (Figure 9). In contrast, locking the signal onto the zero-crossings gives temporal-blur-free images even after thousands of cycles.\nAcquiring frame k with HDR We first acquire the frame without HDR, using a long enough exposure time to achieve good signal-to-noise ratio at dimly-lit surfaces. If this frame AC-locked non-locked Figure 9. Non-locked versus AC-locked imaging. Using a 1500cycle integration to acquire frames corresponding to the maximum and minimum intensity of a bulb (LED2). Left: Without AC locking the integration time window drifts, causing temporally-blurred results. Right: When the ACam is continuously synchronized to the AC zero-crossings, temporal blur is minimal.\nhas saturated pixels, we repeat the acquisition with a modified mask matrix that exposes saturated pixels to light for a lot less. Specifically, let p be a saturated pixel and let M[p, :] be the row corresponding to p. We modify M[p, :] by zeroing out half its non-zero elements. This cuts in half the time that pixel p will be exposed to light. In contrast, the rows of M associated with unsaturated pixels are left as-is. The process of modifying M and re-acquiring the frame is repeated until either the number of saturated pixels falls below a threshold or M has rows with only one nonzero element. 5 In this way, the brightest points in a scene can be exposed up to M/2 times less than the darkest ones.", "publication_ref": ["b3", "b11", "b30", "b41", "b25", "b25", "b28", "b4"], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "Discussion", "text": "We believe we have only scratched the surface of imaging on the electric grid. Our unmixing of scene appearance to components associated with distinct bulb sets opens the door to further photometric processing. In particular, photometric stereo [45] can possibly be obtained in the wild using as few as S = 4 sources (bulbs in three AC phases and daylight). Because objects can be large relative to their distance to light sources, near-lighting effects will need to be accounted for [17,27]. Unmixing can also be followed by intrinsic image recovery [44], shape from shadows [10], surface texture and BRDF chracterization [6]. Moreover, flicker using different bulbs and AC phases can be intentionally used for controlled illumination of objects. This way, multiplexed illumination [5,30] is easily implemented.\nWe call for more sophisticated algorithms that are robust to deviations from assumptions. Deviations include situations where some scene bulb types or the number of sources S are unknown, as in Figure 10. Robustness is required for operation in the presence of non-AC temporal distractions: moving cars, blinking-advertisement neon lights and building-sized dynamic screens. Such non-stationary distractions are often unavoidable, because low light conditions demand acquisition times of seconds to minutes. . An unmixing experiment for a scene that deviates from our assumptions. Here, some scene bulbs are not in DELIGHT and are not observed directly due to their location deep inside the building. Lacking knowledge of the number of independent sources, BRFs and chromaticities, we set S = 5 for unmixing but in reality S is likely higher. The results suffer from residual crosstalk and color distortion, e.g., notice that some signal from sources 4 and 5 falsely appears in parts of sources 1 and 2.\nA system that yields denser image sequences, enlarging K, is desirable. This goal may be achieved by more elaborate coded imaging and processing, e.g., allowing different pixels in each 5\u00d75 neighborhood to sample a different interval of the AC cycle. Alternatively, compressed-sensing codes [23] can be used. Enhanced temporal resolution and signal-to-noise ratio expand the applications as well. These can involve distinguishing bulbs of the same type and AC phase, followed by their unmixing, as well as finer characterization of the electric grid.", "publication_ref": ["b44", "b16", "b26", "b43", "b9", "b5", "b4", "b29", "b22"], "figure_ref": ["fig_0"], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Removing photography artifacts using gradient projection and flashexposure sampling", "journal": "", "year": "2005", "authors": "A Agrawal; R Raskar; S K Nayar; Y Li"}, {"ref_id": "b1", "title": "Multiplexed fluorescence unmixing", "journal": "", "year": "2010", "authors": "M Alterman; Y Y Schechner; A Weiss"}, {"ref_id": "b2", "title": "Amplitude modulation of light from various sources", "journal": "Research and Technology", "year": "1994-09", "authors": "N Andersson; M Sandstr\u00f6m; A Berglund; K Hansson"}, {"ref_id": "b3", "title": "Illumination sources", "journal": "Springer", "year": "2012", "authors": "B G Batchelor"}, {"ref_id": "b4", "title": "Polarization multiplexing for bidirectional imaging", "journal": "", "year": "2005", "authors": "O G Cula; K J Dana; D K Pai; D Wang"}, {"ref_id": "b5", "title": "BRDF/BTF measurement device", "journal": "", "year": "2001", "authors": "K J Dana"}, {"ref_id": "b6", "title": "Acquiring the reflectance field of a human face", "journal": "", "year": "2000", "authors": "P Debevec; T Hawkins; C Tchou; H.-P Duiker; W Sarokin; M Sagar"}, {"ref_id": "b7", "title": "Multiview face capture using polarized spherical gradient illumination", "journal": "", "year": "2011", "authors": "A Ghosh; G Fyffe; J Busch; B Tunwattanapong; X Yu; P Debevec"}, {"ref_id": "b8", "title": "Power system analysis", "journal": "McGraw-Hill", "year": "1994", "authors": "J J Grainger; W D Stevenson"}, {"ref_id": "b9", "title": "Shape from shadows: a Hilbert space setting", "journal": "Journal of Complexity, 14", "year": "1998", "authors": "M Hatzitheodorou"}, {"ref_id": "b10", "title": "Lowbudget transient imaging using photonic mixer devices", "journal": "", "year": "2013", "authors": "F Heide; M B Hullin; J Gregson; W Heidrich"}, {"ref_id": "b11", "title": "Video from a single coded exposure photograph using a learned over-complete dictionary", "journal": "", "year": "2011", "authors": "Y Hitomi; J Gu; M Gupta; T Mitsunaga; S K Nayar"}, {"ref_id": "b12", "title": "Light mixture estimation for spatially varying white balance", "journal": "", "year": "2008", "authors": "E Hsu; T Mertens; S Paris; S Avidan; F Durand"}, {"ref_id": "b13", "title": "Independent component analysis: algorithms and applications", "journal": "", "year": "2008", "authors": "A Hyv\u00e4rinen; E Oja"}, {"ref_id": "b14", "title": "DisCo: Display-camera communication using rolling shutter sensors", "journal": "ACM TOG", "year": "2016", "authors": "K Jo; M Gupta; S K Nayar"}, {"ref_id": "b15", "title": "Exemplar-based color constancy and multiple illumination", "journal": "IEEE TPAMI", "year": "2014", "authors": "H R V Joze; M S Drew"}, {"ref_id": "b16", "title": "Depth and shape from shading using the photometric stereo method. CVGIP: Image understanding", "journal": "", "year": "1991", "authors": "B Kim; P Burger"}, {"ref_id": "b17", "title": "Light source separation from image sequences of oscillating lights", "journal": "", "year": "2014", "authors": "A Kolaman; R Hagege; H Guterman"}, {"ref_id": "b18", "title": "Amplitude modulated video camera-light separation in dynamic scenes", "journal": "", "year": "2016", "authors": "A Kolaman; M Lvov; R Hagege; H Guterman"}, {"ref_id": "b19", "title": "User assisted separation of reflections from a single image using a sparsity prior", "journal": "", "year": "2011", "authors": "A Levin; Y Weiss"}, {"ref_id": "b20", "title": "Evaluating Combinational Illumination Estimation Methods on Real-World Images", "journal": "IEEE TIP", "year": "2014-03", "authors": "B Li; W Xiong; W Hu; B Funt"}, {"ref_id": "b21", "title": "Modeling of the physiological behavior of human vision system under flicker condition", "journal": "", "year": "2008", "authors": "M G Masi; L Peretto; R Tinarelli; L Rovati"}, {"ref_id": "b22", "title": "Matrix Optimization for poisson compressed sensing", "journal": "", "year": "2014", "authors": "M Mordechay; Y Y Schechner"}, {"ref_id": "b23", "title": "Embedded phase shifting: Robust phase shifting with embedded signals", "journal": "", "year": "2015", "authors": "D Moreno; K Son; G Taubin"}, {"ref_id": "b24", "title": "Kutulakos. Homogeneous codes for energy-efficient illumination and imaging", "journal": "ACM SIGGRAPH", "year": "2015", "authors": "M O'toole; S Achar; S G Narasimhan; K "}, {"ref_id": "b25", "title": "3D shape and indirect appearance by structured light transport", "journal": "IEEE TPAMI", "year": "2016", "authors": "M O'toole; J Mather; K N Kutulakos"}, {"ref_id": "b26", "title": "Uncalibrated near-light photometric stereo", "journal": "", "year": "2014", "authors": "T Papadhimitri; P Favaro"}, {"ref_id": "b27", "title": "Digital photography with flash and no-flash image pairs", "journal": "", "year": "2004", "authors": "G Petschnigg; R Szeliski; M Agrawala; M F Cohen; H Hoppe; K Toyama"}, {"ref_id": "b28", "title": "Coded exposure photography: motion deblurring using a fluttered shutter", "journal": "", "year": "2006", "authors": "R Raskar; A Agrawal; J Tumblin"}, {"ref_id": "b29", "title": "Optimal multiplexed sensing: bounds, conditions and a graph theory link", "journal": "", "year": "2007", "authors": "N Ratner; Y Y Schechner; F Goldberg"}, {"ref_id": "b30", "title": "P2C2: Programmable pixel compressive camera for high speed imaging", "journal": "", "year": "2011", "authors": "D Reddy; A Veeraraghavan; R Chellappa"}, {"ref_id": "b31", "title": "Pattern codification strategies in structured light systems", "journal": "Pattern Recogn", "year": "2004", "authors": "J Salvi; J Pages; J Batlle"}, {"ref_id": "b32", "title": "Dual photography", "journal": "", "year": "2005", "authors": "P Sen; B Chen; G Garg; S Marschner; M Horowitz; M Levoy; H P A Lensch"}, {"ref_id": "b33", "title": "Computational imaging on the electric grid: supplementary material", "journal": "", "year": "2017", "authors": "M Sheinin; Y Y Schechner; K Kutulakos"}, {"ref_id": "b34", "title": "", "journal": "Computational Imaging on the Electric Grid: Webpage", "year": "2017", "authors": ""}, {"ref_id": "b35", "title": "Computational Imaging on the Electric Grid: Project webpage", "journal": "", "year": "2017", "authors": ""}, {"ref_id": "b36", "title": "Efficient separation of convolutive image mixtures", "journal": "", "year": "2006", "authors": "S Shwartz; Y Y Schechner; M Zibulevsky"}, {"ref_id": "b37", "title": "Image-based rendering for scenes with reflections", "journal": "", "year": "2006", "authors": "S N Sinha; J Kopf; M Goesele; D Scharstein; R Szeliski"}, {"ref_id": "b38", "title": "Exploiting rolling shutter for ENF signal extraction from video", "journal": "", "year": "2014", "authors": "H Su; A Hajj-Ahmad; R Garg; M Wu"}, {"ref_id": "b39", "title": "Illumination flicker correction and frequency classification methods", "journal": "", "year": "2007", "authors": "T Tajbakhsh; R.-R Grigat"}, {"ref_id": "b40", "title": "Accidental pinhole and pinspeck cameras", "journal": "IJCV", "year": "2014", "authors": "A Torralba; W T Freeman"}, {"ref_id": "b41", "title": "Coded strobing photography: compressive sensing of high speed periodic videos", "journal": "IEEE TPAMI", "year": "2011", "authors": "A Veeraraghavan; D Reddy; R Raskar"}, {"ref_id": "b42", "title": "Flickering lamps", "journal": "European Journal of Physics", "year": "2015", "authors": "M Vollmer; K.-P Moellmann"}, {"ref_id": "b43", "title": "Deriving intrinsic images from image sequences", "journal": "", "year": "2001", "authors": "Y Weiss"}, {"ref_id": "b44", "title": "Photometric method for determining surface orientation from multiple images", "journal": "Opt. Eng", "year": "1980", "authors": "R J Woodham"}, {"ref_id": "b45", "title": "Eulerian video magnification for revealing subtle changes in the world", "journal": "ACM SIGGRAPH", "year": "2012", "authors": "H.-Y Wu; M Rubinstein; E Shih; J Guttag; F Durand; W T Freeman"}, {"ref_id": "b46", "title": "Study on fluorescent lamp illumination and flicker", "journal": "", "year": "2003", "authors": "W Xiaoming; Z Ke; Z Ying; S Wenxiang"}, {"ref_id": "b47", "title": "Flicker removal for CMOS wide dynamic range imaging based on alternating current component analysis", "journal": "IEEE Trans. on Consumer Electronics", "year": "2014", "authors": "Y Yoo; J Im; J Paik"}, {"ref_id": "b48", "title": "Environment matting and compositing", "journal": "", "year": "1999", "authors": "D Zongker; D Werner; B Curless; D Salesin"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure1. Top and middle: City-scale scene. From the AC bulb response function, we can recognize which bulbs are used and the electric-grid phase of each (color coded). This enables statistical analysis of the grid. Inset is magnified in Figure3. Bottom: Unmixing a scene to single-light-source component images.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 .2Figure 2. Top left: A pure sine wave, fitted to the raw voltage at the reference outlet in Haifa. We count time t starting from a negativeto-positive zero crossing at this outlet. Bottom left: Raw signals from a bare photodiode for a sample of the bulbs in DELIGHT, spanning multiple cycles. Each signal was normalized by its temporal average over one flicker cycle. For better visualization, LED2's waveform was attenuated by 1/3. Bottom right: The corresponding monochrome BRFs. These were computed by acquiring signals like those on the bottom-left 200 times, averaging them, and cropping them at t \u2208 [0, \u2206]. Here LED1's BRF was amplified \u00d710 to illustrate that it is not actually constant. Top right: The three-band BRF of one of the bulbs, measured by placing color filters in front of the photodiode.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 .3Figure 3. Top: Close-up of Haifa bay from Figure 1. Kilometers away, lamps are recognized from DELIGHT and ACam-captured images. In conjunction, the grid phase at each bulb is recognized. Bottom left: Raw signals normalized by mean brightness, plotted along with their best-matching BRFs from DELIGHT. Colors correspond to the bulbs indicated above. Bottom right: To illustrate how well the measured signals on the bottom left compare to each other, we shifted them by minus their recognized phase, effectively placing them all on grid phase zero. Observe that the signals from bulbs of the same type are indeed very similar.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 4 .4Figure 4. Unmixing results. The second column shows the single-source images reconstructed by unmixing. We simulate bulb amplification/de-amplification by computing \u03c41 + 3.5\u03c43, and bulb replacement by replacing b1 with a Sodium BRF. The left plot shows the monochrome BRFs sampled directly by ACam. On the right we illustrate the successful reconstruction of saturated or noisy pixels.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 .5Figure 5. Unmixing results for an outdoor scene.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 .6Figure 6. Temporal variations due to AC flicker are attributed to the indoor scene. ICA separates the indoor reflection from the transmitted scene. Both are recovered up to an unknown scale.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 8 .8Figure 8. Our ACam combines an Arduino and voltage transformer with the DMD-based programmable mask in[26]. The Arduino senses the AC grid in real-time, switching between DMD masks over hundreds of cycles. The masks are loaded to the DMD by a PC (not shown). In each AC cycle, these masks expose individual pixels for a small fraction of the cycle duration.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "5Figure 1010Figure10. An unmixing experiment for a scene that deviates from our assumptions. Here, some scene bulbs are not in DELIGHT and are not observed directly due to their location deep inside the building. Lacking knowledge of the number of independent sources, BRFs and chromaticities, we set S = 5 for unmixing but in reality S is likely higher. The results suffer from residual crosstalk and color distortion, e.g., notice that some signal from sources 4 and 5 falsely appears in parts of sources 1 and 2.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "microseconds. The ACam supports K \u2264 26 for 50Hz grids and K \u2264 22 for 60Hz grids. Bottom: The corresponding DMD masks. Mask 0 is active most of the time and acts like a global shutter. Mask m1 briefly exposes all pixels to light. Mask m2, on the other hand, blocks light from some of the pixels in the next cycle to prevent their saturation.", "figure_data": "All pixels blockedAll pixels blockedAll pixels blockedPixels exposedPixels exposedby maskby mask3Activate maskActivate mask1 2VoltageActivate maskActivate mask0Elapsed time [ms]3035404550(default) MaskMaskMaskMaskMaskFigure 7. ACam operation. Top: The camera's pixels are re-peatedly blocked and unblocked over C cycles so that they canintegrate light only during the same brief interval in each cycle.Because each cycle's duration varies slightly, the timing of theseevents is controlled precisely with an Arduino that tracks AC zero-crossings in real time. Here we show the Arduino's input volt-age (blue) and the mask-switching signal it generates (red), mea-sured simultaneously with a high-speed oscilloscope. Masks areswitched at the signal's rising edge and must persist for at least\u2206/K"}], "formulas": [{"formula_id": "formula_0", "formula_text": "f = 1/(2\u2206) .(1)", "formula_coordinates": [2.0, 396.75, 274.09, 148.36, 9.96]}, {"formula_id": "formula_1", "formula_text": "V (t) = V max sin(2\u03c0f t \u2212 \u03c6) .(2)", "formula_coordinates": [2.0, 364.6, 410.29, 180.51, 9.96]}, {"formula_id": "formula_2", "formula_text": "L \u03b2 (t, \u03bb) is a distortion of V 2 (t):", "formula_coordinates": [3.0, 50.11, 308.45, 236.25, 21.92]}, {"formula_id": "formula_3", "formula_text": "L \u03b2 (t, \u03bb) = L \u03b2 (\u03bb) B \u03b2 (t, \u03bb) .(3)", "formula_coordinates": [3.0, 107.0, 413.6, 179.36, 10.32]}, {"formula_id": "formula_4", "formula_text": "I \u03b2 (t, \u03c3) = I \u03b2 (\u03c3) B * \u03b2 (t, \u03c3) .(4)", "formula_coordinates": [3.0, 109.04, 562.63, 177.32, 12.77]}, {"formula_id": "formula_5", "formula_text": "i \u03b2 (\u03c3) = I \u03b2 (\u03c3) b \u03b2 (\u03c3)(5)", "formula_coordinates": [3.0, 343.64, 441.3, 201.47, 9.28]}, {"formula_id": "formula_6", "formula_text": "= \u03c3 I \u03b2 (\u03c3) brightness I \u03b2 (\u03c3) \u03c3 I \u03b2 (\u03c3) chromaticity Q \u03b2 (\u03c3) b \u03b2 (\u03c3) BRF (6", "formula_coordinates": [3.0, 369.13, 458.46, 172.5, 39.2]}, {"formula_id": "formula_7", "formula_text": ")", "formula_coordinates": [3.0, 541.63, 464.93, 3.48, 8.02]}, {"formula_id": "formula_8", "formula_text": "\u03b2 ,\u03c6 = arg min \u03b2\u2208B,\u03c6\u2208P \u03c3 i norm (\u03c3) \u2212 Q \u03b2 (\u03c3) shift \u03c6, b \u03b2 (\u03c3) 2 (7)", "formula_coordinates": [4.0, 60.92, 310.13, 225.44, 30.06]}, {"formula_id": "formula_9", "formula_text": "I s [p, k] = \u03c4 ps Q s b s [k](8)", "formula_coordinates": [4.0, 120.06, 634.14, 166.3, 10.32]}, {"formula_id": "formula_10", "formula_text": "I s = \u03c4 s Q s b s . (9", "formula_coordinates": [4.0, 393.58, 443.33, 147.66, 10.32]}, {"formula_id": "formula_11", "formula_text": ")", "formula_coordinates": [4.0, 541.24, 444.07, 3.87, 8.91]}, {"formula_id": "formula_12", "formula_text": "I = I 1 + \u2022 \u2022 \u2022 + I s + \u2022 \u2022 \u2022 + I S .(10)", "formula_coordinates": [4.0, 364.69, 549.98, 180.42, 10.32]}, {"formula_id": "formula_13", "formula_text": "I = [\u03c41 \u2022 \u2022 \u2022 \u03c4S] [Q1b \u22a4 1 \u2022 \u2022 \u2022 QSb \u22a4 S ] \u22a4 (11) = [\u03c41 \u2022 \u2022 \u2022 \u03c4S] transport matrix T \uf8ee \uf8ef \uf8f0 Q1 0 . . . 0 QS \uf8f9 \uf8fa \uf8fb chromaticity matrix Q \uf8ee \uf8ef \uf8f0 b1 . . . bS \uf8f9 \uf8fa \uf8fb BRF matrix B(12)", "formula_coordinates": [4.0, 341.82, 625.22, 203.29, 69.85]}, {"formula_id": "formula_15", "formula_text": "B = [ shift(\u03c6 1 , b 1 ) \u22a4 \u2022 \u2022 \u2022 shift(\u03c6 S , b S ) \u22a4 ] \u22a4 . (14)", "formula_coordinates": [5.0, 59.53, 417.95, 226.83, 11.8]}, {"formula_id": "formula_16", "formula_text": "I s1 + I s2 = (\u03c4 s1 Q s1 + \u03c4 s2 Q s2 ) shift(\u03c6 s1 , b s1 ) . (15)", "formula_coordinates": [5.0, 58.24, 535.29, 228.12, 10.32]}, {"formula_id": "formula_17", "formula_text": "T = arg min T\u22650 W \u2299 I \u2212 TQB 2 F ,(16)", "formula_coordinates": [5.0, 351.16, 361.66, 193.95, 18.24]}, {"formula_id": "formula_18", "formula_text": "W[p, k] = 0 if any spectral band is saturated at I[p, k] 1 otherwise . (17", "formula_coordinates": [5.0, 313.77, 427.11, 227.61, 22.12]}, {"formula_id": "formula_19", "formula_text": ")", "formula_coordinates": [5.0, 541.38, 433.96, 3.73, 8.02]}, {"formula_id": "formula_20", "formula_text": "B = arg min B\u22650 W \u2299 I \u2212TQB 2 F . (18", "formula_coordinates": [5.0, 351.08, 523.52, 190.3, 20.93]}, {"formula_id": "formula_21", "formula_text": ")", "formula_coordinates": [5.0, 541.38, 529.79, 3.73, 8.02]}, {"formula_id": "formula_22", "formula_text": "I s =\u03c4 s Q s shift(\u03c6 s , b s )(19)", "formula_coordinates": [5.0, 374.61, 703.53, 170.5, 10.32]}, {"formula_id": "formula_23", "formula_text": "I relight =T [AQB] relight .(20)", "formula_coordinates": [6.0, 113.8, 453.98, 172.56, 11.32]}, {"formula_id": "formula_24", "formula_text": "k 1 and k 2 with b ac [k 1 ]>b ac [k 2 ].", "formula_coordinates": [6.0, 308.86, 297.67, 126.13, 10.49]}, {"formula_id": "formula_25", "formula_text": "I[k 1 ] = \u03c4 out + \u03c4 ac b ac [k 1 ](21)", "formula_coordinates": [6.0, 368.27, 332.25, 176.84, 10.49]}, {"formula_id": "formula_26", "formula_text": "I[k 2 ] = \u03c4 out + \u03c4 ac b ac [k 2 ] .(22)", "formula_coordinates": [6.0, 368.27, 347.2, 176.84, 10.49]}, {"formula_id": "formula_27", "formula_text": "I[k 1 ] \u2212 I[k 2 ]", "formula_coordinates": [6.0, 428.11, 369.83, 49.7, 9.96]}, {"formula_id": "formula_28", "formula_text": "I[k 1 ] \u2212 I[k 2 ] and I[k 2 ] \u2212 AI[k 1 ]", "formula_coordinates": [6.0, 350.89, 441.56, 129.51, 9.96]}, {"formula_id": "formula_29", "formula_text": "M = [ m1 0 m2 0 . . . m M/2 0 ](23)", "formula_coordinates": [7.0, 355.45, 409.04, 189.66, 9.61]}], "doi": ""}