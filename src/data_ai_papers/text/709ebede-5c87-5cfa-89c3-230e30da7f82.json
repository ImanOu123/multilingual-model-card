{"title": "K-Best A * Parsing", "authors": "Adam Pauls; Dan Klein", "pub_date": "", "abstract": "A * parsing makes 1-best search efficient by suppressing unlikely 1-best items. Existing kbest extraction methods can efficiently search for top derivations, but only after an exhaustive 1-best pass. We present a unified algorithm for k-best A * parsing which preserves the efficiency of k-best extraction while giving the speed-ups of A * methods. Our algorithm produces optimal k-best parses under the same conditions required for optimality in a 1-best A * parser. Empirically, optimal k-best lists can be extracted significantly faster than with other approaches, over a range of grammar types.", "sections": [{"heading": "Introduction", "text": "Many situations call for a parser to return the kbest parses rather than only the 1-best. Uses for k-best lists include minimum Bayes risk decoding (Goodman, 1998;Kumar and Byrne, 2004), discriminative reranking (Collins, 2000;Charniak and Johnson, 2005), and discriminative training (Och, 2003;McClosky et al., 2006). The most efficient known algorithm for k-best parsing (Jim\u00e9nez and Marzal, 2000;Huang and Chiang, 2005) performs an initial bottom-up dynamic programming pass before extracting the k-best parses. In that algorithm, the initial pass is, by far, the bottleneck (Huang and Chiang, 2005).\nIn this paper, we propose an extension of A * parsing which integrates k-best search with an A *based exploration of the 1-best chart. A * parsing can avoid significant amounts of computation by guiding 1-best search with heuristic estimates of parse completion costs, and has been applied successfully in several domains (Klein and Manning, 2002;Klein and Manning, 2003c;Haghighi et al., 2007). Our algorithm extends the speedups achieved in the 1-best case to the k-best case and is optimal under the same conditions as a stan-dard A * algorithm. The amount of work done in the k-best phase is no more than the amount of work done by the algorithm of Huang and Chiang (2005). Our algorithm is also equivalent to standard A * parsing (up to ties) if it is terminated after the 1-best derivation is found. Finally, our algorithm can be written down in terms of deduction rules, and thus falls into the well-understood view of parsing as weighted deduction (Shieber et al., 1995;Goodman, 1998;Nederhof, 2003).\nIn addition to presenting the algorithm, we show experiments in which we extract k-best lists for three different kinds of grammars: the lexicalized grammars of Klein and Manning (2003b), the state-split grammars of Petrov et al. (2006), and the tree transducer grammars of Galley et al. (2006). We demonstrate that optimal k-best lists can be extracted significantly faster using our algorithm than with previous methods.", "publication_ref": ["b5", "b15", "b1", "b0", "b19", "b17", "b8", "b7", "b7", "b10", "b13", "b6", "b7", "b23", "b5", "b18", "b12", "b21", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "A k-Best A * Parsing Algorithm", "text": "We build up to our full algorithm in several stages, beginning with standard 1-best A * parsing and making incremental modifications.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Parsing as Weighted Deduction", "text": "Our algorithm can be formulated in terms of prioritized weighted deduction rules (Shieber et al., 1995;Nederhof, 2003;Felzenszwalb and McAllester, 2007). A prioritized weighted deduction rule has the form\n\u03c6 1 : w 1 , . . . , \u03c6 n : w n p(w1,...,wn) \u2212 \u2212\u2212\u2212\u2212\u2212\u2212\u2212 \u2192 \u03c6 0 : g(w 1 , . . . , w n )\nwhere \u03c6 1 , . . . , \u03c6 n are the antecedent items of the deduction rule and \u03c6 0 is the conclusion item. A deduction rule states that, given the antecedents \u03c6 1 , . . . , \u03c6 n with weights w 1 , . . . , w n , the conclusion \u03c6 0 can be formed with weight g(w 1 , . . . , w n ) and priority p(w 1 , . . . , w n ).\nThese deduction rules are \"executed\" within a generic agenda-driven algorithm, which constructs items in a prioritized fashion. The algorithm maintains an agenda (a priority queue of unprocessed items), as well as a chart of items already processed. The fundamental operation of the algorithm is to pop the highest priority item \u03c6 from the agenda, put it into the chart with its current weight, and form using deduction rules any items which can be built by combining \u03c6 with items already in the chart. If new or improved, resulting items are put on the agenda with priority given by p(\u2022).", "publication_ref": ["b23", "b18", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "A * Parsing", "text": "The A * parsing algorithm of Klein and Manning (2003c) can be formulated in terms of weighted deduction rules (Felzenszwalb and McAllester, 2007). We do so here both to introduce notation and to build to our final algorithm.\nFirst, we must formalize some notation. Assume we have a PCFG 1 G and an input sentence s 1 . . . s n of length n. The grammar G has a set of symbols \u03a3, including a distinguished goal (root) symbol G. Without loss of generality, we assume Chomsky normal form, so each non-terminal rule r in G has the form r = A \u2192 B C with weight w r (the negative log-probability of the rule). Edges are labeled spans e = (A, i, j). Inside derivations of an edge (A, i, j) are trees rooted at A and spanning s i+1 . . . s j . The total weight of the best (minimum) inside derivation for an edge e is called the Viterbi inside score \u03b2(e). The goal of the 1-best A * parsing algorithm is to compute the Viterbi inside score of the edge (G, 0, n); backpointers allow the reconstruction of a Viterbi parse in the standard way.\nThe basic A * algorithm operates on deduction items I(A, i, j) which represent in a collapsed way the possible inside derivations of edges (A, i, j). We call these items inside edge items or simply inside items where clear; a graphical representation of an inside item can be seen in Figure 1(a). The space whose items are inside edges is called the edge space.\nThese inside items are combined using the single IN deduction schema shown in Table 1. This schema is instantiated for every grammar rule r 1 While we present the algorithm specialized to parsing with a PCFG, it generalizes to a wide range of hypergraph search problems as shown in Klein and Manning (2001). in G. For IN, the function g(\u2022) simply sums the weights of the antecedent items and the grammar rule r, while the priority function p(\u2022) adds a heuristic to this sum. The heuristic is a bound on the Viterbi outside score \u03b1(e) of an edge e; see Klein and Manning (2003c) for details. A good heuristic allows A * to reach the goal item I(G, 0, n) while constructing few inside items.\nIf the heuristic is consistent, then A * guarantees that whenever an inside item comes off the agenda, its weight is its true Viterbi inside score (Klein and Manning, 2003c). In particular, this guarantee implies that the goal item I(G, 0, n) will be popped with the score of the 1-best parse of the sentence. Consistency also implies that items are popped off the agenda in increasing order of bounded Viterbi scores:\n\u03b2(e) + h(e)\nWe will refer to this monotonicity as the ordering property of A * (Felzenszwalb and McAllester, 2007). One final property implied by consistency is admissibility, which states that the heuristic never overestimates the true Viterbi outside score for an edge, i.e. h(e) \u2264 \u03b1(e). For the remainder of this paper, we will assume our heuristics are consistent.", "publication_ref": ["b13", "b2", "b9", "b13", "b13", "b2"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "A Naive k-Best A * Algorithm", "text": "Due to the optimal substructure of 1-best PCFG derivations, a 1-best parser searches over the space of edges; this is the essence of 1-best dynamic programming. Although most edges can be built   ) for building inside edge items, using a supplied heuristic. This schema is sufficient on its own for 1-best A * , and it is used in KA * . Here, r is the rule A \u2192 B C.\nInside Derivation Deductions (Used in NAIVE)\nDERIV: D(T B , i, l) : w 1 D(T C , l, j) : w 2 w1+w2+wr+h(A,i,j) \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 D A T B T C , i, j : w 1 + w 2 + w r\nTable 2: The deduction schema for building derivations, using a supplied heuristic. T B and T C denote full tree structures rooted at symbols B and C. This schema is the same as the IN deduction schema, but operates on the space of fully specified inside derivations rather than dynamic programming edges. This schema forms the NAIVE k-best algorithm.\nOutside Edge Deductions (Used in KA * ) OUT-B: I(G, 0, n) : w 1 w1 \u2212 \u2212 \u2192 O(G, 0, n) : 0 OUT-L: O(A, i, j) : w 1 I(B, i, l) : w 2 I(C, l, j) : w 3 w1+w3+wr+w2 \u2212 \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 \u2192 O(B, i, l)\n:\nw 1 + w 3 + w r OUT-R: O(A, i, j) : w 1 I(B, i, l) : w 2 I(C, l, j) : w 3 w1+w2+wr+w3 \u2212 \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 \u2192 O(C, l, j)\n:\nw 1 + w 2 + w r\nTable 3: The deduction schemata for building ouside edge items. The first schema is a base case that constructs an outside item for the goal (G, 0, n) from the inside item I(G, 0, n). The second two schemata build outside items in a top-down fashion. Note that for outside items, the completion cost is the weight of an inside item rather than a value computed by a heuristic. , i, j : w 1 + w 2 + w r Table 4: The deduction schema for building derivations, using exact outside scores computed using OUT deductions. The dependency on the outside item O(A, i, j) delays building derivation items until exact Viterbi outside scores have been computed. This is the final search space for the KA * algorithm.\nRanked Inside Derivation Deductions (Lazy Version of NAIVE)\nBUILD: K(B, i, l, u) : w 1 K(C, l, j, v) : w 2 w1+w2+wr+h(A,i,j) \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 D(A, i, j, l, r, u, v) : w 1 + w 2 + w r RANK: D 1 (A, i, j, \u2022) : w 1 . . . D k (A, i, j, \u2022) : w k maxm wm+h(A,i,j) \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 K(A, i, j, k) : max m w m\nTable 5: The schemata for simultaneously building and ranking derivations, using a supplied heuristic, for the lazier form of the NAIVE algorithm. BUILD builds larger derivations from smaller ones. RANK numbers derivations for each edge. Note that RANK requires distinct D i , so a rank k RANK rule will first apply (optimally) as soon as the kth-best inside derivation item for a given edge is removed from the queue. However, it will also still formally apply (suboptimally) for all derivation items dequeued after the kth. In practice, the RANK schema need not be implemented explicitly -one can simply assign a rank to each inside derivation item when it is removed from the agenda, and directly add the appropriate ranked inside item to the chart.  using many derivations, each inside edge item will be popped exactly once during parsing, with a score and backpointers representing its 1-best derivation. However, k-best lists involve suboptimal derivations. One way to compute k-best derivations is therefore to abandon optimal substructure and dynamic programming entirely, and to search over the derivation space, the much larger space of fully specified trees. The items in this space are called inside derivation items, or derivation items where clear, and are of the form D(T A , i, j), specifying an entire tree T A rooted at symbol A and spanning s i+1 . . . s j (see Figure 1(c)). Derivation items are combined using the DERIV schema of Table 2. The goals in this space, representing root parses, are any derivation items rooted at symbol G that span the entire input.\nIn this expanded search space, each distinct parse has its own derivation item, derivable only in one way. If we continue to search long enough, we will pop multiple goal items. The first k which come off the agenda will be the k-best derivations. We refer to this approach as NAIVE. It is very inefficient on its own, but it leads to the full algorithm.\nThe correctness of this k-best algorithm follows from the correctness of A * parsing. The derivation space of full trees is simply the edge space of a much larger grammar (see Section 2.5).\nNote that the DERIV schema's priority includes a heuristic just like 1-best A * . Because of the context freedom of the grammar, any consistent heuristic for inside edge items usable in 1-best A * is also consistent for inside derivation items (and vice versa). In particular, the 1-best Viterbi outside score for an edge is a \"perfect\" heuristic for any derivation of that edge.\nWhile correct, NAIVE is massively inefficient. In comparison with A * parsing over G, where there are O(n 2 ) inside items, the size of the derivation space is exponential in the sentence length. By the ordering property, we know that NAIVE will process all derivation items d with\n\u03b4(d) + h(d) \u2264 \u03b4(g k )\nwhere g k is the kth-best root parse and \u03b4(\u2022) is the inside score of a derivation item (analogous to \u03b2 for edges). 2 Even for reasonable heuristics, this number can be very large; see Section 3 for empirical results.\nThis naive algorithm is, of course, not novel, either in general approach or specific computation. Early k-best parsers functioned by abandoning dynamic programming and performing beam search on derivations (Ratnaparkhi, 1999;Collins, 2000). Huang (2005) proposes an extension of Knuth's algorithm (Knuth, 1977) to produce k-best lists by searching in the space of derivations, which is essentially this algorithm. While Huang (2005) makes no explicit mention of a heuristic, it would be easy to incorporate one into their formulation.", "publication_ref": ["b22", "b1", "b7", "b14", "b7"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "A New k-Best A * Parser", "text": "While NAIVE suffers severe performance degradation for loose heuristics, it is in fact very efficient if h(\u2022) is \"perfect,\" i.e. h(e) = \u03b1(e) \u2200e. In this case, the ordering property of A * guarantees that only inside derivation items d satisfying\n\u03b4(d) + \u03b1(d) \u2264 \u03b4(g k )\nwill be placed in the chart. The set of derivation items d satisfying this inequality is exactly the set which appear in the k-best derivations of (G, 0, n) (as always, modulo ties). We could therefore use NAIVE quite efficiently if we could obtain exact Viterbi outside scores.\nOne option is to compute outside scores with exhaustive dynamic programming over the original grammar. In a certain sense, described in greater detail below, this precomputation of exact heuristics is equivalent to the k-best extraction algorithm of Huang and Chiang (2005). However, this exhaustive 1-best work is precisely what we want to use A * to avoid.\nOur algorithm solves this problem by integrating three searches into a single agenda-driven process. First, an A * search in the space of inside edge items with an (imperfect) external heuristic h(\u2022) finds exact inside scores. Second, exact outside scores are computed from inside and outside items. Finally, these exact outside scores guide the search over derivations. It can be useful to imagine these three operations as operating in phases, but they are all interleaved, progressing in order of their various priorities.\nIn order to calculate outside scores, we introduce outside items O(A, i, j), which represent best derivations of G \u2192 s 1 . . . s i A s j+1 . . . s n ; see Figure 1(b). Where the weights of inside items compute Viterbi inside scores, the weights of outside items compute Viterbi outside scores.\nTable 3 shows deduction schemata for building outside items. These schemata are adapted from the schemata used in the general hierarchical A * algorithm of Felzenszwalb and McAllester (2007). In that work, it is shown that such schemata maintain the property that the weight of an outside item is the true Viterbi outside score when it is removed from the agenda. They also show that outside items o follow an ordering property, namely that they are processed in increasing order of\n\u03b2(o) + \u03b1(o)\nThis quantity is the score of the best root derivation which includes the edge corresponding to o. Felzenszwalb and McAllester (2007) also show that both inside and outside items can be processed on the same queue and the ordering property holds jointly for both types of items.\nIf we delay the construction of a derivation item until its corresponding outside item has been popped, then we can gain the benefits of using an exact heuristic h(\u2022) in the naive algorithm. We realize this delay by modifying the DERIV deduction schema as shown in Table 4 to trigger on and prioritize with the appropriate outside scores.\nWe now have our final algorithm, which we call KA * . It is the union of the IN, OUT, and new \"delayed\" DERIV deduction schemata. In words, our algorithm functions as follows: we initialize the agenda with I(s i , i \u2212 1, i) and D(s i , i \u2212 1, i) for i = 1 . . . n. We compute inside scores in standard A * fashion using the IN deduction rule, using any heuristic we might provide to 1-best A * . Once the inside item I(G, 0, n) is found, we automatically begin to compute outside scores via the OUT deduction rules. Once O(s i , i \u2212 1, i) is found, we can begin to also search in the space of derivation items, using the perfect heuristics given by the just-computed outside scores. Note, however, that all computation is done with a single agenda, so the processing of all three types of items is interleaved, with the k-best search possibly terminating without a full inside computation. As with NAIVE, the algorithm terminates when a k-th goal derivation is dequeued.", "publication_ref": ["b7", "b2", "b2"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Correctness", "text": "We prove the correctness of this algorithm by a reduction to the hierarchical A * (HA * ) algorithm of Felzenszwalb and McAllester (2007). The input to HA * is a target grammar G m and a list of grammars G 0 . . . G m\u22121 in which G t\u22121 is a relaxed projection of G t for all t = 1 . . . m. A grammar G t\u22121 is a projection of G t if there exists some onto function \u03c0 t : \u03a3 t \u2192 \u03a3 t\u22121 defined for all symbols in G t . We use A t\u22121 to represent \u03c0 t (A t ). A projection is relaxed if, for every rule r = A t \u2192 B t C t with weight w r there is a rule r = A t\u22121 \u2192 B t\u22121 C t\u22121 in G t\u22121 with weight w r \u2264 w r .\nWe assume that our external heuristic function h(\u2022) is constructed by parsing our input sentence with a relaxed projection of our target grammar. This assumption, though often true anyway, is to allow proof by reduction to Felzenszwalb and McAllester (2007). 3 We construct an instance of HA * as follows: Let G 0 be the relaxed projection which computes the heuristic. Let G 1 be the input grammar G, and let G 2 , the target grammar of our HA * instance, be the grammar of derivations in G formed by expanding each symbol A in G to all possible inside derivations T A rooted at A. The rules in G 2 have the form T A \u2192 T B T C with weight given by the weight of the rule A \u2192 B C. By construction, G 1 is a relaxed projection of G 2 ; by assumption G 0 is a relaxed projection of G 1 . The deduction rules that describe KA * build the same items as HA * with same weights and priorities, and so the guarantees from HA * carry over to KA * .\nWe can characterize the amount of work done using the ordering property. Let g k be the kth-best derivation item for the goal edge g. Our algorithm processes all derivation items d, outside items o, and inside items i satisfying\n\u03b4(d) + \u03b1(d) \u2264 \u03b4(g k ) \u03b2(o) + \u03b1(o) \u2264 \u03b4(g k ) \u03b2(i) + h(i) \u2264 \u03b4(g k )\nWe have already argued that the set of derivation items satisfying the first inequality is the set of subtrees that appear in the optimal k-best parses, modulo ties. Similarly, it can be shown that the second inequality is satisfied only for edges that appear in the optimal k-best parses. The last inequality characterizes the amount of work done in the bottom-up pass. We compare this to 1-best A * , which pops all inside items i satisfying\n\u03b2(i) + h(i) \u2264 \u03b2(g) = \u03b4(g 1 )\nThus, the \"extra\" inside items popped in the bottom-up pass during k-best parsing as compared to 1-best parsing are those items i satisfying\n\u03b4(g 1 ) \u2264 \u03b2(i) + h(i) \u2264 \u03b4(g k )\nThe question of how many items satisfy these inequalities is empirical; we show in our experiments that it is small for reasonable heuristics. At worst, the bottom-up phase pops all inside items and reduces to exhaustive dynamic programming.\nAdditionally, it is worth noting that our algorithm is naturally online in that it can be stopped at any k without advance specification.", "publication_ref": ["b2", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Lazy Successor Functions", "text": "The global ordering property guarantees that we will only dequeue derivation fragments of top parses. However, we will enqueue all combinations of such items, which is wasteful. By exploiting a local ordering amongst derivations, we can be more conservative about combination and gain the advantages of a lazy successor function (Huang and Chiang, 2005).\nTo do so, we represent inside derivations not by explicitly specifying entire trees, but rather by using ranked backpointers. In this representation, inside derivations are represented in two ways, shown in Figure 1(d) and (e). The first way (d) simply adds a rank u to an edge, giving a tuple (A, i, j, u). The corresponding item is the ranked derivation item K(A, i, j, u), which represents the uth-best derivation of A over (i, j). The second representation (e) is a backpointer of the form (A, i, j, l, r, u, v), specifying the derivation formed by combining the uth-best derivation of (B, i, l) and the vth-best derivation of (C, l, j) using rule r = A \u2192 B C. The corresponding items D(A, i, j, l, r, u, v) are the new form of our inside derivation items.\nThe modified deduction schemata for the NAIVE algorithm over these representations are shown in Table 5. The BUILD schema produces new inside derivation items from ranked derivation items, while the RANK schema assigns each derivation item a rank; together they function like DERIV. We can find the k-best list by searching until K(G, 0, n, k) is removed from the agenda. The k-best derivations can then be extracted by following the backpointers for K(G, 0, n, 1) . . . K(G, 0, n, k). The KA * algorithm can be modified in the same way, shown in Table 6. The actual laziness is provided by additionally delaying the combination of ranked items. When an item K(B, i, l, u) is popped off the queue, a naive implementation would loop over items K(C, l, j, v) for all v, C, and j (and similarly for left combinations). Fortunately, little looping is actually necessary: there is a partial ordering of derivation items, namely, that D(A, i, j, l, r, u, v) will have a lower computed priority than D(A, i, j, l, r, u \u2212 1, v) and D(A, i, j, l, r, u, v \u2212 1) (Jim\u00e9nez and Marzal, 2000). So, we can wait until one of the latter two is built before \"triggering\" the construction of the former. This triggering is similar to the \"lazy frontier\" used by Huang and Chiang (2005). All of our experiments use this lazy representation.", "publication_ref": ["b7", "b8", "b7"], "figure_ref": ["fig_0"], "table_ref": ["tab_1"]}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "State-Split Grammars", "text": "We performed our first experiments with the grammars of Petrov et al. (2006). The training procedure for these grammars produces a hierarchy of increasingly refined grammars through statesplitting. We followed Pauls and Klein (2009) in computing heuristics for the most refined grammar from outside scores for less-split grammars.\nWe used the Berkeley Parser 4 to learn such grammars from Sections 2-21 of the Penn Treebank (Marcus et al., 1993). We trained with 6 split-merge cycles, producing 7 grammars. We tested these grammars on 100 sentences of length at most 30 of Section 23 of the Treebank. Our \"target grammar\" was in all cases the most split grammar. Heuristics computed from projections to successively smaller grammars in the hierarchy form successively looser bounds on the outside scores. This allows us to examine the performance as a function of the tightness of the heuristic. We first compared our algorithm KA * against the NAIVE algorithm. We extracted 1000-best lists using each algorithm, with heuristics computed using each of the 6 smaller grammars.\nIn Figure 2, we evaluate only the k-best extraction phase by plotting the number of derivation items and outside items added to the agenda as a function of the heuristic used, for increasingly loose heuristics. We follow earlier work (Pauls and Klein, 2009) in using number of edges pushed as the primary, hardware-invariant metric for evaluating performance of our algorithms. 5 While KA * scales roughly linearly with the looseness of the heuristic, NAIVE degrades very quickly as the heuristics get worse. For heuristics given by grammars weaker than the 4-split grammar, NAIVE ran out of memory.\nSince the bottom-up pass of k-best parsing is the bottleneck, we also examine the time spent in the 1-best phase of k-best parsing. As a baseline, we compared KA * to the approach of Huang and Chiang (2005), which we will call EXH (see below for more explanation) since it requires exhaustive parsing in the bottom-up pass. We performed the exhaustive parsing needed for EXH in our agenda-based parser to facilitate comparison. For KA * , we included the cost of computing the heuristic, which was done by running our agenda-based parser exhaustively on a smaller grammar to compute outside items; we chose the constructed these grammars using the Stanford Parser. 6 The model was trained on Sections 2-20 of the Penn Treebank and tested on 100 sentences of Section 21 of length at most 30 words. For this grammar, Klein and Manning (2003b) showed that a very accurate heuristic can be constructed by taking the sum of outside scores computed with the dependency model and the PCFG model individually. We report performance as a function of k for KA * in Figure 4. Both NAIVE and EXH are impractical on these grammars due to memory limitations. For KA * , computing the heuristic is the bottleneck, after which bottom-up parsing and k-best extraction are very fast.", "publication_ref": ["b21", "b20", "b16", "b20", "b7", "b12"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Tree Transducer Grammars", "text": "Syntactic machine translation (Galley et al., 2004) uses tree transducer grammars to translate sentences. Transducer rules are synchronous contextfree productions that have both a source and a target side. We examine the cost of k-best parsing in the source side of such grammars with KA * , which can be a first step in translation.\nWe extracted a grammar from 220 million words of Arabic-English bitext using the approach of Galley et al. (2006), extracting rules with at most 3 non-terminals. These rules are highly lexicalized. About 300K rules are applicable for a typical 30-word sentence; we filter the rest. We tested on 100 sentences of length at most 40 from the NIST05 Arabic-English test set.\nWe used a simple but effective heuristic for these grammars, similar to the FILTER heuristic suggested in Klein and Manning (2003c). We projected the source projection to a smaller grammar by collapsing all non-terminal symbols to X, and 6 http://nlp.stanford.edu/software/ also collapsing pre-terminals into related clusters. For example, we collapsed the tags NN, NNS, NNP, and NNPS to N. This projection reduced the number of grammar symbols from 149 to 36. Using it as a heuristic for the full grammar suppressed \u223c 60% of the total items (Figure 5).", "publication_ref": ["b3", "b4", "b13"], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "Related Work", "text": "While formulated very differently, one limiting case of our algorithm relates closely to the EXH algorithm of Huang and Chiang (2005). In particular, if all inside items are processed before any derivation items, the subsequent number of derivation items and outside items popped by KA * is nearly identical to the number popped by EXH in our experiments (both algorithms have the same ordering bounds on which derivation items are popped). The only real difference between the algorithms in this limited case is that EXH places k-best items on local priority queues per edge, while KA * makes use of one global queue. Thus, in addition to providing a method for speeding up k-best extraction with A * , our algorithm also provides an alternate form of Huang and Chiang (2005)'s k-best extraction that can be phrased in a weighted deduction system.", "publication_ref": ["b7", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "3-split grammar for the heuristic since it gives the best overall tradeoff of heuristic and bottom-up parsing time. We separated the items enqueued into items enqueued while computing the heuristic (not strictly part of the algorithm), inside items (\"bottom-up\"), and derivation and outside items (together \"k-best\"). The results are shown in Figure 3. The cost of k-best extraction is clearly dwarfed by the the 1-best computation in both cases. However, KA * is significantly faster over the bottom-up computations, even when the cost of computing the heuristic is included.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Lexicalized Parsing", "text": "We also experimented with the lexicalized parsing model described in Klein and Manning (2003b). This model is constructed as the product of a dependency model and the unlexicalized PCFG model in Klein and Manning (2003a). We", "publication_ref": ["b12", "b11"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Coarseto-fine n-best parsing and maxent discriminative reranking", "journal": "", "year": "2005", "authors": "Eugene Charniak; Mark Johnson"}, {"ref_id": "b1", "title": "Discriminative reranking for natural language parsing", "journal": "", "year": "2000", "authors": "Michael Collins"}, {"ref_id": "b2", "title": "The generalized A* architecture", "journal": "Journal of Artificial Intelligence Research", "year": "2007", "authors": "P Felzenszwalb; D Mcallester"}, {"ref_id": "b3", "title": "What's in a translation rule", "journal": "", "year": "2004", "authors": "Michel Galley; Mark Hopkins; Kevin Knight; Daniel Marcu"}, {"ref_id": "b4", "title": "Scalable inference and training of context-rich syntactic translation models", "journal": "", "year": "2006", "authors": "Michel Galley; Jonathan Graehl; Kevin Knight; Daniel Marcu; Steve Deneefe; Wei Wang; Ignacio Thayer"}, {"ref_id": "b5", "title": "", "journal": "", "year": "1998", "authors": "Joshua Goodman"}, {"ref_id": "b6", "title": "Approximate factoring for A* search", "journal": "", "year": "2007", "authors": "Aria Haghighi; John Denero; Dan Klein"}, {"ref_id": "b7", "title": "Unpublished manuscript", "journal": "Liang Huang", "year": "2005", "authors": "Liang Huang; David Chiang"}, {"ref_id": "b8", "title": "Computation of the n best parse trees for weighted and stochastic context-free grammars", "journal": "Springer-Verlag", "year": "2000", "authors": "M V\u00edctor; Andr\u00e9s Jim\u00e9nez;  Marzal"}, {"ref_id": "b9", "title": "Parsing and hypergraphs", "journal": "", "year": "2001", "authors": "Dan Klein; Christopher D Manning"}, {"ref_id": "b10", "title": "Fast exact inference with a factored model for natural language processing", "journal": "", "year": "2002", "authors": "Dan Klein; Chris Manning"}, {"ref_id": "b11", "title": "Accurate unlexicalized parsing", "journal": "", "year": "2003", "authors": "Dan Klein; Chris Manning"}, {"ref_id": "b12", "title": "Factored A* search for models over sequences and trees", "journal": "", "year": "2003", "authors": "Dan Klein; Chris Manning"}, {"ref_id": "b13", "title": "A* parsing: Fast exact Viterbi parse selection", "journal": "", "year": "2003", "authors": "Dan Klein; Christopher D Manning"}, {"ref_id": "b14", "title": "A generalization of Dijkstra's algorithm", "journal": "Information Processing Letters", "year": "1977", "authors": "Donald Knuth"}, {"ref_id": "b15", "title": "Minimum bayes-risk decoding for statistical machine translation", "journal": "", "year": "2004", "authors": "Shankar Kumar; William Byrne"}, {"ref_id": "b16", "title": "Building a large annotated corpus of English: The Penn Treebank", "journal": "", "year": "1993", "authors": "M Marcus; B Santorini; M Marcinkiewicz"}, {"ref_id": "b17", "title": "Effective self-training for parsing", "journal": "", "year": "2006", "authors": "David Mcclosky; Eugene Charniak; Mark Johnson"}, {"ref_id": "b18", "title": "Weighted deductive parsing and Knuth's algorithm", "journal": "Computationl Linguistics", "year": "2003", "authors": "- Mark"}, {"ref_id": "b19", "title": "Minimum error rate training in statistical machine translation", "journal": "", "year": "2003", "authors": "Franz Josef Och"}, {"ref_id": "b20", "title": "Hierarchical search for parsing", "journal": "", "year": "2009", "authors": "Adam Pauls; Dan Klein"}, {"ref_id": "b21", "title": "Learning accurate, compact, and interpretable tree annotation", "journal": "", "year": "2006", "authors": "Slav Petrov; Leon Barrett; Romain Thibaux; Dan Klein"}, {"ref_id": "b22", "title": "Learning to parse natural language with maximum entropy models", "journal": "", "year": "1999", "authors": "Adwait Ratnaparkhi"}, {"ref_id": "b23", "title": "Principles and implementation of deductive parsing", "journal": "Journal of Logic Programming", "year": "1995", "authors": "M Stuart; Yves Shieber; Fernando C N Schabes;  Pereira"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Representations of the different types of items used in parsing. (a) An inside edge item: I(VP, 2, 5). (b) An outside edge item: O(VP, 2, 5). (c) An inside derivation item: D(T VP , 2, 5) for a tree T VP . (d) A ranked derivation item: K(VP, 2, 5, 6). (e) A modified inside derivation item (with backpointers to ranked items): D(VP, 2, 5, 3, VP \u2192 VBZ NP, 1, 4).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "InsideEdge Deductions (Used in A * and KA * ) IN: I(B, i, l) : w 1 I(C, l, j) : w 2 w1+w2+wr+h(A,i,j) \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 I(A, i, j) : w 1 + w 2 + w r", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "DelayedInside Derivation Deductions (Used in KA * ) DERIV: D(T B , i, l) : w 1 D(T C , l, j) : w 2 O(A, i, j) :", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "DelayedRanked Inside Derivation Deductions (Lazy Version of KA * ) BUILD: K(B, i, l, u) : w 1 K(C, l, j, v) : w 2 O(A, i, j) : w 3 w1+w2+wr+w3 \u2212 \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 \u2192 D(A, i, j, l, r, u, v) : w 1 + w 2 + w r RANK: D 1 (A, i, j, \u2022) : w 1 . . . D k (A, i, j, \u2022) : w k O(A, i, j) : w k+1 maxm wm+wk+1 \u2212 \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 \u2192 K(A, i, j, k) : max m w m", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 2 :2Figure 2: Number of derivation items enqueued as a function of heuristic. Heuristics are shown in decreasing order of tightness. The y-axis is on a log-scale.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 3 :3Figure3: The cost of k-best extraction as a function of k for state-split grammars, for both KA * and EXH. The amount of time spent in the k-best phase is negligible compared to the cost of the bottom-up phase in both cases.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 5 :5Figure 5: k-best extraction as a function of k for tree transducer grammars, for both KA * and EXH.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "The deduction schema (IN", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "The deduction schemata for building and ranking derivations, using exact outside scores computed from OUT deductions, used for the lazier form of the KA", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u03c6 1 : w 1 , . . . , \u03c6 n : w n p(w1,...,wn) \u2212 \u2212\u2212\u2212\u2212\u2212\u2212\u2212 \u2192 \u03c6 0 : g(w 1 , . . . , w n )", "formula_coordinates": [1.0, 307.97, 661.72, 216.88, 14.12]}, {"formula_id": "formula_1", "formula_text": "DERIV: D(T B , i, l) : w 1 D(T C , l, j) : w 2 w1+w2+wr+h(A,i,j) \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 D A T B T C , i, j : w 1 + w 2 + w r", "formula_coordinates": [3.0, 80.05, 177.25, 439.79, 28.17]}, {"formula_id": "formula_2", "formula_text": "Outside Edge Deductions (Used in KA * ) OUT-B: I(G, 0, n) : w 1 w1 \u2212 \u2212 \u2192 O(G, 0, n) : 0 OUT-L: O(A, i, j) : w 1 I(B, i, l) : w 2 I(C, l, j) : w 3 w1+w3+wr+w2 \u2212 \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 \u2192 O(B, i, l)", "formula_coordinates": [3.0, 84.61, 277.38, 376.92, 47.23]}, {"formula_id": "formula_3", "formula_text": "w 1 + w 3 + w r OUT-R: O(A, i, j) : w 1 I(B, i, l) : w 2 I(C, l, j) : w 3 w1+w2+wr+w3 \u2212 \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 \u2192 O(C, l, j)", "formula_coordinates": [3.0, 84.61, 315.44, 427.65, 23.1]}, {"formula_id": "formula_4", "formula_text": "w 1 + w 2 + w r", "formula_coordinates": [3.0, 456.79, 329.37, 55.47, 9.17]}, {"formula_id": "formula_5", "formula_text": "BUILD: K(B, i, l, u) : w 1 K(C, l, j, v) : w 2 w1+w2+wr+h(A,i,j) \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 D(A, i, j, l, r, u, v) : w 1 + w 2 + w r RANK: D 1 (A, i, j, \u2022) : w 1 . . . D k (A, i, j, \u2022) : w k maxm wm+h(A,i,j) \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 K(A, i, j, k) : max m w m", "formula_coordinates": [3.0, 80.1, 544.42, 436.71, 27.39]}, {"formula_id": "formula_6", "formula_text": "\u03b4(d) + h(d) \u2264 \u03b4(g k )", "formula_coordinates": [4.0, 135.22, 673.54, 91.82, 10.77]}, {"formula_id": "formula_7", "formula_text": "\u03b4(d) + \u03b1(d) \u2264 \u03b4(g k )", "formula_coordinates": [4.0, 370.13, 351.22, 92.55, 10.77]}, {"formula_id": "formula_8", "formula_text": "\u03b2(o) + \u03b1(o)", "formula_coordinates": [5.0, 153.81, 233.24, 54.64, 9.57]}, {"formula_id": "formula_9", "formula_text": "\u03b4(d) + \u03b1(d) \u2264 \u03b4(g k ) \u03b2(o) + \u03b1(o) \u2264 \u03b4(g k ) \u03b2(i) + h(i) \u2264 \u03b4(g k )", "formula_coordinates": [5.0, 362.85, 537.13, 107.12, 43.85]}, {"formula_id": "formula_10", "formula_text": "\u03b2(i) + h(i) \u2264 \u03b2(g) = \u03b4(g 1 )", "formula_coordinates": [5.0, 354.19, 722.74, 124.45, 10.63]}, {"formula_id": "formula_11", "formula_text": "\u03b4(g 1 ) \u2264 \u03b2(i) + h(i) \u2264 \u03b4(g k )", "formula_coordinates": [6.0, 117.29, 112.06, 127.69, 10.77]}], "doi": ""}