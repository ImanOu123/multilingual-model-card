{"title": "Sparse Bayesian Multiview Learning for Simultaneous Association Discovery and Diagnosis of Alzheimer's Disease", "authors": "Shandian Zhe; Zenglin Xu; Yuan Qi; Peng Yu; Eli Lilly", "pub_date": "", "abstract": "In the analysis and diagnosis of many diseases, such as the Alzheimer's disease (AD), two important and related tasks are usually required: i) selecting genetic and phenotypical markers for diagnosis, and ii) identifying associations between genetic and phenotypical features. While previous studies treat these two tasks separately, they are tightly coupled due to the same underlying biological basis. To harness their potential benefits for each other, we propose a new sparse Bayesian approach to jointly carry out the two important and related tasks. In our approach, we extract common latent features from different data sources by sparse projection matrices and then use the latent features to predict disease severity levels; in return, the disease status can guide the learning of sparse projection matrices, which not only reveal interactions between data sources but also select groups of related biomarkers. In order to boost the learning of sparse projection matrices, we further incorporate graph Laplacian priors encoding the valuable linkage disequilibrium (LD) information. To efficiently estimate the model, we develop a variational inference algorithm. Analysis on an imaging genetics dataset for AD study shows that our model discovers biologically meaningful associations between single nucleotide polymorphisms (SNPs) and magnetic resonance imaging (MRI) features, and achieves significantly higher accuracy for predicting ordinal AD stages than competitive methods.", "sections": [{"heading": "Introduction", "text": "Alzheimer's disease (AD) is the most common neurodegenerative disorder (Khachaturian 1985). Given genetic variations, e.g., single nucleotide polymorphisms (SNPs), and phenotypical traits, e.g., magnetic resonance imaging (MRI), we want to develop noninvasive diagnosis methods Many approaches have been proposed to discover associations, such as canonical correlation analysis (CCA) and its extensions (Harold 1936;Bach and Jordan 2005a), or select features (or variables), such as lasso (Tibshirani 1994), elastic net (Zou and Hastie 2005), and Bayesian automatic relevance determination (MacKay 1991;Neal 1996). Despite their wide success in many applications, these approaches suffer several limitations: i) most association studies neglect the supervision from the disease status, while diseases as AD are a direct result of genetic variations and often highly correlated to clinical traits; ii) most feature selection approaches do not consider the disease severity order, while AD subjects have a natural severity order from being normal to mild cognitive impairment (MCI) and then from MCI to AD; iii) most previous approaches are not designed to handle heterogeneous data sources, e.g., the SNPs values are discrete and ordinal while the imaging features are continuous; iv) most previous methods ignore the valuable prior knowledge such as Linkage Disequilibrium (LD) (Falconer and Mackay 1996) measuring the non-random association of alleles. To our knowledge, this structure has not been utilized for association discovery in medical study.\nTo address these limitations, we propose a new Bayesian model that unifies multiview learning with sparse ordinal regression for joint association study and disease diagnosis. Specifically, genetic variations and phenotypical traits are generated from common latent features based on separate sparse projection matrices and suitable link functions, and the latent features are used to predict the disease status. To encourage sparsity, we assign spike and slab priors (George and McCulloch 1997) over the projection matrices; we further employ an additional graph Laplacian prior encoding LD knowledge to boost the learning of the sparse projection matrix on the genetic variation view. The sparse projection matrices then not only reveal critical interactions between the data sources but also identify biomarkers relevant to the disease. Meanwhile, via their direct connection to the latent features, the disease status will influence the estimation of the projection matrices and guide the discovery of disease-\nH Z y U X G S h S g \u03b7 w S w h j 0 L Figure 1:\nThe graphical representation of our model, where X is the continuous view, Z is the ordinal view, y are the ordinal labels and L is the graph Laplacian generated from the LD structure.\nsensitive data source associations. For efficient model estimation, we develop a variational inference approach, which iteratively minimizes the Kullback Leibler divergence between a tractable approximation and exact Bayesian posterior distributions. The results on Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset show that our model achieves highest prediction accuracy among all the competing methods and finds biologically meaningful associations between SNPs and MRI features.", "publication_ref": ["b9", "b8", "b1", "b19", "b25", "b12", "b15", "b3", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Methods", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Notations and Assumptions", "text": "We assume there are two heterogeneous data sources: one contains continuous data -e.g., MRI features -and the other contains discrete ordinal data -e.g., SNPs. Note that we can easily generalize our model below to handle more views and other data types by adopting suitable link functions (e.g., a Possion model for count data). Given data from n subjects, p continuous features and q discrete features, we denote the continuous data by a p \u00d7 n matrix X = [x 1 , . . . , x n ], the discrete ordinal data by a q \u00d7 n matrix Z = [z 1 , . . . , z n ] and the labels (i.e., the disease status) by a n \u00d7 1 vector y = [y 1 , . . . , y n ] . For the AD study, we let y i = 0, 1, and 2 if the i-th subject is in the normal, MCI or AD condition, respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model", "text": "To link the two data sources X and Z together, we introduce common latent features U = [u 1 , . . . , u n ] and assume X and Z are generated from U by sparse projection. The common latent feature assumption is sensible because both SNPs and MRI features are biological measurements of the same subjects. Note that u i is a k dimensional latent feature for the i-th subject. In a Bayesian framework, we assign a Gaussian prior over U, p(U) = i N (u i |0, I), and specify the rest of the model (see Figure 1) as follows.\nContinuous data distribution. Given U, X is generated from\np(X|U, G, \u03b7) = n i=1 N (x i |Gu i , \u03b7 \u22121 I)\nwhere G = [g 1 , g 2 , ...g p ] is a p \u00d7 k projection matrix, I is an identity matrix, and \u03b7 is a scalar. We assign an uninformative Gamma prior over \u03b7, p(\u03b7|r 1 , r 2 ) = Gamma(\u03b7|r 1 , r 2 ), where r 1 = r 2 = 10 \u22123 .\nOrdinal data distribution. For an ordinal variable z \u2208 {0, 1, . . . , R \u2212 1}, we introduce an auxiliary variable c and a segmentation of the number axis, \u2212\u221e = b 0 < b 1 < . . . < b R = \u221e. We define that z = r if and only if c falls in [b r , b r+1 ). Then given a q \u00d7 k projection matrix H = [h 1 , h 2 , ...h q ] and the auxiliary matrix C = {c ij }, the ordinal data Z are generated from\np(Z, C|U, H) = q i=1 n j=1 p(c ij |h i , u j )p(z ij |c ij ) where p(z ij |c ij ) = R\u22121 r=0 \u03b4(z ij = r)\u03b4(b r \u2264 c ij < b r+1 ) and p(c ij |h i , u j ) = N (c ij |h i u j , 1).\nHere \u03b4(a) = 1 if a is true and \u03b4(a) = 0 otherwise. In AD study, Z take values in {0, 1, 2} and hence R = 3.\nLabel distribution. The disease status labels y are ordinal variables too. To generate y, we use the ordinal regression model based the latent representation U,\np(y, f |U, w) = p(y|f )p(f |U, w),\nwhere f is the latent continuous values corresponding to y, w is the weight vector for the latent features, p(y\ni |f i ) = 2 r=0 \u03b4(y i = r)\u03b4(b r \u2264 f i < b r+1 ) and p(f i |u i , w) = N (f i |u i w, 1).\nNote that y are linked to X and Z via the latent features U and the projection matrices H and G. Due to the sparsity in H and G, only a few groups of variables in X and Z are selected to predict y.\nLD structure as additional priors for SNPs correlations. Linkage Disequilibrium records the occurrence of non-random combinations of alleles or genetic markers, which can be a natural indicator for the SNPs correlations. To utilize this valuable prior knowledge, we introduce a latent q \u00d7 k matrixH, which is tightly linked to H. Each columnh j ofH is regularized by the graph Laplacian of the LD structure, i.e.,\np(H|L) = j N (h j |0, L \u22121 ) = j N (0|h j , L \u22121 ) = p(0|H, L),\nwhere L is the graph Laplacian matrix of the LD structure. As shown above, the prior p(H|L) has the same form as p(0|H, L), which can be viewed as a generative model -in other words, the observation 0 is sampled fromH. This view enables us to combine the generative model for graph Laplacian regularization with the sparse projection model via a principled hybrid Bayesian framework (Lasserre, Bishop, and Minka 2006).\nTo link the two models together, we introduce a prior over H:\np(H|H) = j N (h j |h j , \u03bbI)\nwhere the variance \u03bb controls how similarH and H are in our model. For simplicity, we set \u03bb = 0 so that p(H|H) = \u03b4(H \u2212 H) where \u03b4(a) = 1 if a = 1 and \u03b4(a) = 0 if a = 0.\nSparse priors for projection matrices and weights vector. In order to discover critical interactions between data sources and enhance the model prediction, we use spike and slab prior (George and McCulloch 1997) to sparsify the projection matrices G and H and the weight vector w. Specifically, we use a p \u00d7 k matrix S g to represent the selection of elements in G: if s g ij = 1, g ij is selected and follows a Gaussian prior distribution with variance \u03c3 2 1 ; if s g ij = 0, g ij is not selected and forced to almost zero (i.e., sampled from a Gaussian with a very small variance \u03c3 2 2 ). Thereby, we have the following prior over G:\np(G|S g , \u03a0 g ) = p i=1 k j=1 p(g ij |s ij g )p(s ij g |\u03c0 ij g )\nwhere\n\u03c0 ij g in \u03a0 g is the probability of selecting g ij (i.e., s ij g = 1), p(g ij |s ij g ) = s ij g N (g ij |0, \u03c3 2 1 ) + (1 \u2212 s ij g )N (g ij |0, \u03c3 2 2 ), and p(s ij g |\u03c0 ij g ) = \u03c0 ij g s ij g (1 \u2212 \u03c0 ij g ) 1\u2212s ij g . Note that \u03c3 2\n2 should be close to 0 and we set \u03c3 2 1 = 1 and \u03c3 2 2 = 10 \u22126 in the experiment. For a full Bayesian treatment, we assign an uninformative hyperprior over \u03a0 g :\np(\u03a0 g |l 1 , l 2 ) = p i=1 k j=1 Beta(\u03c0 ij g |l 1 , l 2 )\nwhere l 1 = l 2 = 1. Similarly, we assign the prior for H,\np(H|S h , \u03a0 h ) = q i=1 k j=1 p(h ij |s ij h )p(s ij h |\u03c0 ij h ),\nwhere S h are binary selection variables, where s w are binary selection variables, \u03c0 j w is the selecting probability of w j , p(w j |s j w ) = s j w N (w j |0, \u03c3 2 1 ) + (1 \u2212 s j w )N (w j |0, \u03c3 2 2 ) and p(s j w |\u03c0 j w ) = \u03c0 j w s j w (1 \u2212 \u03c0 j w ) 1\u2212s j w . We assign Beta hyperpriors for \u03c0 w : p(\u03c0 w ) = k i=1 Beta(\u03c0 i w |e 1 , e 2 ) where e 1 = e 2 = 1. Joint distribution. Based on the aforementioned components, the joint distribution of our model is\n\u03c0 ij h is select- ing probability of h ij , p(h ij |s ij h ) = s ij h N (h ij |0, \u03c3 2 1 ) + (1 \u2212 s ij h )N (h ij |0, \u03c3 2 2 ) and p(s ij h |\u03c0 ij h ) = \u03c0 ij h s ij h (1 \u2212 \u03c0 ij h ) 1\u2212s ij h . We assign uninformative Beta hyperpriors for \u03a0 h : p(\u03a0 h |d 1 , d 2 ) = q i=1 k j=1 Beta(\u03c0 ij h |d 1 , d 2 ) where d 1 = d 2 = 1.\np(X, Z, y, U, G, Sg, \u03a0g, \u03b7, C, H,H, S h , \u03a0 h , Sw, \u03a0w, f ) = p(X|U, G, \u03b7)p(G|Sg)p(Sg|\u03a0g)p(\u03a0g|l1, l2)p(\u03b7|r1, r2) \u2022 p(Z, C|U, H)p(H|S h )p(S h |\u03a0 h )p(\u03a0 h |d1, d2)p(H|H) \u2022 p(0|H, L)p(y|f )p(f |U, w)p(w|Sw)p(Sw|\u03a0w)p(U).", "publication_ref": ["b11", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Estimation", "text": "Given the model, we present an efficient approach to estimate the latent features U, the projection matrices H and G, the selection indicators S g and S h , the selecting probabilities \u03a0 g and \u03a0 h , the variance \u03b7, the auxiliary variables C for ordinal data Z, the auxiliary variables f for ordinal labels y, the weights vector w for prediction and the corresponding selection indicators and probabilities s w and \u03c0 w . In a Bayesian framework, this amounts to computing their posterior distributions.\nHowever, computing the exact posteriors is infeasible because we cannot calculate the normalization constant for the posterior distributions. Therefore, we resort to a mean-field variational approach, which approximates the posterior distributions of\n{U, H, G, S g , S h , \u03a0 g , \u03a0 h , \u03b7, w, C, f } by a factorized distribution Q(\u2022) = Q(U)Q(H)Q(G)Q(S g )Q(S h ) Q(\u03a0 g )Q(\u03a0 h )Q(\u03b7)Q(w)Q(C)Q(f ).\nNote that since we set p(H|H) = \u03b4(H \u2212H), we do not need a separate distribution Q(H). We minimize the Kullback-Leibler (KL) divergence between the approximate and the exact posteriors, KL(Q||P ) where P represents the exact joint posterior distributions. We use coordinate descent: we update an approximate distribution, say, Q(H), while fixing the other approximate distributions, and iteratively refine all the approximate distributions. The detailed updates are given in the following sections. For brevity, the calculation of the required moments are provided in the supplementary material.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Updating variational distributions for continuous data", "text": "For continuous data X, the approximate distributions of the projection matrix G, the noise variance \u03b7, the selection indicators S g and the selection probabilities \u03a0 g are\nQ(G) = p i=1 N (g i ; \u03bb i , \u2126 i ),(1)\nQ(S g ) = p i=1 k j=1 \u03b2 s ij g ij (1 \u2212 \u03b2 ij ) 1\u2212s ij g ,(2)\nQ(\u03a0 g ) = p i=1 k j=1 Beta(\u03c0 ij g |l ij 1 ,l ij 2 ),(3)\nQ(\u03b7) = Gamma(\u03b7|r 1 ,r 2 ).(4)\nThe parameters for Q(G) are calculated by\n\u2126 i = \u03b7 UU + 1 \u03c3 2 1 diag( s i g ) + 1 \u03c3 2 2 diag(1 \u2212 s i g ) \u22121 and \u03bb i = \u2126 i ( \u03b7 U x i ),\nwhere \u2022 means the expectation over a distribution,x i and s i g are the transpose of the i-th row in X and S g respectively; the parameters for Q(S g ) are calculated by\n\u03b2 ij = 1/ 1 + exp( log(1 \u2212 \u03c0 ij g ) \u2212 log(\u03c0 ij g ) + 1 2 log( \u03c3 2 1 \u03c3 2 2 ) + 1 2 g 2 ij ( 1 \u03c3 2 1 \u2212 1 \u03c3 2 2 )) ; the parameters for Q(\u03a0 g ) are given byl ij 1 = \u03b2 ij + l 1 andl ij 2 = 1 \u2212 \u03b2 ij + l 2 ; the parameters for Q(\u03b7) are given byr 1 = r 1 + np 2 andr 2 = r 2 + 1 2 tr(XX ) \u2212 tr( G U X ) + 1 2 tr( UU G G ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Updating variational distributions for ordinal data", "text": "For ordinal data Z, we update the approximate distributions of the projection matrix H, the auxiliary variables C, the selection indicators S h and the selecting probabilities \u03a0 h . To make the variational distributions tractable, we factorize Q(H) in a column-wise way. This is different from Q(G) which are factorized in a row-wise way. Specifically, we denote the i-th column of H and S h byh i and s i h , the i-th row of U by\u0169 i , and calculate the variational distributions of C and H by\nQ(C) = q i=1 k j=1 Q(c ij ),(5)\nQ(c ij ) \u221d \u03b4(b zij \u2264 c ij < b zij +1 )N (c ij |c ij , 1), (6\n)\nQ(H) = k i=1 N (h i ; \u03b3 i , \u039b i ), (7\n)\nwherec ij = ( H U ) ij , \u039b i = \u0169 i \u0169 i I + L + 1 \u03c3 2 1 diag( s i h ) + 1 \u03c3 2 2 diag( 1 \u2212 s i h ) \u22121 , and \u03b3 i = \u039b i ( C \u2212 j =i \u03b3 j \u0169 j ) u i .\nThe variational distributions of S h and \u03a0 h are\nQ(S h ) = q i=1 k j=1 \u03b1 s ij h ij (1 \u2212 \u03b1 ij ) 1\u2212s ij h ,(8)\nQ(\u03a0 h ) = q i=1 k j=1 Beta(\u03c0 ij h |d ij 1 ,d ij 2 ),(9)\nwhere\n\u03b1 ij = 1/ 1 + exp( log(1 \u2212 \u03c0 ij h ) \u2212 log(\u03c0 ij h ) + 1 2 log( \u03c3 2 1 \u03c3 2 2 ) + 1 2 h 2 ij ( 1 \u03c3 2 1 \u2212 1 \u03c3 2 2 )) , andd ij 1 = \u03b1 ij + d 1 ,d ij 2 = 1 \u2212 \u03b1 ij + d 2 .\nNote that in Equation ( 6), Q(c ij ) is a truncated Gaussian and the truncation is controlled by the observed ordinal data z ij .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Updating variational distributions for labels", "text": "For ordinal labels y, we calculate the approximate distributions of the auxiliary variables f , the weights vector w, the selection indicators s w and the selecting probabilities \u03c0 w by\nQ(f ) = n i=1 Q(f i ),(10)\nQ(f i ) \u221d \u03b4(b yi \u2264 f i < b yi+1 )N (f i |f i , \u03c3 2 fi ),(11)\nQ(w) = N (w; m, \u03a3 w ),(12)\nQ(s w ) = k i=1 \u03c4 s i w i (1 \u2212 \u03c4 i ) 1\u2212s i w ,(13)\nQ(\u03c0 w ) = k i=1 Beta(\u03c0 i w ;\u1ebd i 1 ,\u1ebd i 2 ), (14\n)\nwheref i = ( U m) i , \u03a3 w = UU + 1 \u03c3 2 1 diag( s w ) + 1 \u03c3 2 2 diag( 1 \u2212 s w ) \u22121 , m = \u03a3 w U f , \u03c4 i = 1/ 1 + exp( log(1 \u2212 \u03c0 i w ) \u2212 log(\u03c0 i w ) + 1 2 w 2 i ( 1 \u03c3 2 1 \u2212 1 \u03c3 2 2 )) ,\u1ebd i 1 = \u03c4 i + e 1 and\u1ebd i 2 = 1 \u2212 \u03c4 i + e 2 .\nNote that Q(f i ) is also a truncated Gaussian and the truncated region is decided by the ordinal label y i . In this way, the supervised information from y is incorporated into estimation of f and then estimation of the other quantities by the recursive updates.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Updating variational distributions for latent feature U", "text": "The approximate distribution for U is\nQ(U) = i N (u i |\u00b5 i , \u03a3 i ) (15\n)\nwhere\n\u03a3 i = ww + \u03b7 G G + H H + I \u22121 and \u00b5 i = \u03a3 i ( w f i + \u03b7 G x i + H c i ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Prediction", "text": "Let us denote the training data by D train = {X train , Z train , y train } and the test data by D test = {X test , Z test }. We jointly carry out variational inference on D train and D test . After the latent features for D test are obtained (i.e., Q(U test )), we predict the labels by\nf test = U test m,(16)\ny i test = R\u22121 r=0 r \u2022 \u03b4(b r \u2264 f i test < b r+1 ),(17)\nwhere y i test is the prediction for i-th test sample.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results and Discussion", "text": "We conducted association analysis and AD diagnosis based on a dataset from Alzheimer's Disease Neuroimaging Initiative (ADNI). The ADNI study is a longitudinal multisite observational study of elderly individuals with normal cognition, mild cognitive impairmen (MCI) or AD. We applied our model to study the associations between genotypes and brain atrophy measured by MRI and to predict the subject status (normal vs MCI vs AD). Note that the statuses are ordinal since they represent increasing severity levels. The dataset was downloaded from http://adni.loni.ucla. edu/. After removing missing values, it consists of 618 subjects (183 normal, 308 MCI and 134 AD) and each subject contains 924 SNPs (selected as the top SNPs to separate normal subjects from AD in ADNI) and 328 MRI features (measuring the brain atrophies in different brain regions based on cortical thickness, surface area or volume using FreeSurfer software). Moreover, the LD structure was retrieved from www.ncbi.nlm.nih.gov/books/NBK44495/.\nTo evaluate the diagnosis accuracy, we compared with the following ordinal or multinomial regression methods: (1) lasso for multinomial regression (Tibshirani 1994), (2) elastic net for multinomial regression (Zou and Hastie 2005), (3) sparse ordinal regression with the spike and slab prior, (4) CCA + lasso, for which we first ran CCA to obtain the projected data and then applied lasso for prediction, (5) CCA + elastic net, which is the same as CCA + lasso except that we applied elastic net for prediction, (6) Gaussian Process Ordinal Regression (GPOR) (Chu and Ghahramani 2005), which employs Gaussian processes to learn the latent functions for ordinal regression, and (7) Laplacian Support Vector Machine (LapSVM) (Melacci and Mikhail 2011), a semi-supervised SVM classification scheme. We used the Glmnet package by (Friedman, Hastie, and Tibshirani 2010) for lasso and elastic net, the GPOR package by (Chu and Ghahramani 2005), and the LapSVM package by (Melacci and Mikhail 2011). For all the methods, we used 10-fold cross validation (i.e., each fold we have 556 training and 62 test samples) to tune free parameters, e.g., the kernel form and parameters for GPOR and LapSVM. Note that all the alternative methods stack X and Z together into a whole data matrix and ignore their heterogeneous nature.\nTo determine the dimension k for the latent features U in our method, we calculated the variational lower bound as an approximation to the model evidence, with various k values {10, 20, 40, 60}. The variational lower bound can be easily calculated based on what we have presented in the model estimation section. We chose the value with the largest approximate evidence, which led to k = 20 (see Figure 2).\nOur experiments confirmed that with k = 20, our model achieved the highest prediction accuracy, demonstrating the benefit of evidence maximization.\nAs shown in Figure 3, our method achieved the highest prediction accuracy, higher than that of the second best method, GP ordinal Regression, by 10% and than that of the worst method, CCA+lasso, by 22%. The two-sample t test shows our model outperforms the alternative methods significantly (p < 0.05).\nWe also examined the strongest associations discovered by our model. First of all, the ranking of MRI features in terms of prediction power for the three different disease populations (normal, MCI and AD) demonstrate that most of the top ranked features are based on the cortical thickness measurement. On the other hand, the features based on volume and surface area estimation are less predictive. Particularly, thickness measurements of middle temporal lobe, precuneus, and fusiform were found to be most predictive compared with other brain regions. These findings are consistent with the memory-related function in these regions and findings in the literature for their prediction power of AD (Whitwell et al. 2007;Risacher et al. 2009;Teipel et al. 2013). We also found that measurements of the same structure on the left and right side have similar weights, indicating that the algorithm can automatically select correlated features in groups, since no asymmetrical relationship has been found for the brain regions involved in AD (Kusbeci et al. 2009).\nSecond, the analysis of associating genotype to AD prediction generated interesting results. Similar to the MRI features, SNPs that are in the vicinity of each other are often selected together, indicating the group selection characteristics of the algorithm. For example, the top ranked SNPs are associated with a few genes including CAPZB (F-actin-capping protein subunit beta), NCOA2 (The nuclear receptor coactivator 2) and BCAR3(Breast cancer anti-estrogen resistance protein 3).\nAt last, biclustering of the gene-MRI associations, as shown in Figure 4 reveal interesting pattern in terms of the relationship between genetic variations and brain atrophy measured by structural MRI. For example, the top ranked SNPs are associated with a few genes including BCAR3 (Breast cancer anti-estrogen resistance protein 3) and NCOA2, which have been studied more carefully in cancer research (Stephens et al. 2012). The set of SNPs are associated with cingulate in negative direction, which is part of the limbic system and involve in emotion formation and processing, compared with other structures such as temporal lobe, which plays a more important role in the formation of long-term memory.", "publication_ref": ["b19", "b25", "b13", "b4", "b2", "b13", "b22", "b16", "b18", "b10", "b17"], "figure_ref": ["fig_1", "fig_2", "fig_3"], "table_ref": []}, {"heading": "Related Work", "text": "Our model is closely related to probabilistic factor analysis methods which try to learn a latent representation whose projection leads to the observed data (Tipping and Bishop 1999;Bach and Jordan 2005b;Guan and Dy 2009;Yu et al. 2006;Archambeau and Bach 2009;Virtanen, Klami, and Kaski 2011). In addition to learning latent representation, our model uses spike and slab prior to learn sparse projection matrices in order to select features in different data sources and find their critical associations. The spike and slab prior avoids confounding the degree of sparsity with the degree of regularization and has shown to outperform l 1 regularization in an unsupervised setting (Mohamed, Heller, and Ghahramani 2012). Our model is also connected with learning from multiple sources or multiview learning methods (Hardoon et al. 2008), many of which try to learn a better classifier for multi-label classification based on the correlation structure among the training data and the labels (Yu et al. 2006;Virtanen, Klami, and Kaski 2011). However, our work conducts two tasks-the association discovery and ordinal label prediction-simultaneously to benefit each other. Finally, our work can be considered as an extension of the work (Zhe et al. 2013) by incorporating the LD structure knowledge and using sparse ordinal regression to model disease severity level.", "publication_ref": ["b20", "b1", "b6", "b23", "b0", "b21", "b14", "b23", "b21", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions", "text": "We presented a new Bayesian multiview learning model for joint associations discovery and disease prediction in AD study. We expect that our model can be further applied to a wide range of applications in biomedical research -e.g., eQTL analysis supervised by additional labeling information.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgement", "text": "This work was supported by NSF IIS-0916443, IIS-1054903, CCF-0939370 and a Project 985 grant from University of Electronic Science and Technology (No. A1098531023601041).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Sparse probabilistic projections", "journal": "", "year": "2009", "authors": "C Archambeau; F Bach"}, {"ref_id": "b1", "title": "A probabilistic interpretation of canonical correlation analysis", "journal": "UC Berkley", "year": "2005", "authors": "F Bach; Jordan ; M Berkeley; F Bach; Jordan ; M "}, {"ref_id": "b2", "title": "Gaussian processes for ordinal regression", "journal": "Journal of Machine Learning Research", "year": "2005", "authors": "W Chu; Z Ghahramani"}, {"ref_id": "b3", "title": "Introduction to Quantitative Genetics", "journal": "Addison Wesley Longman", "year": "1996", "authors": "D Falconer; T Mackay"}, {"ref_id": "b4", "title": "Regularization paths for generalized linear models via coordinate descent", "journal": "Journal of Statistical Software", "year": "2010", "authors": "J Friedman; T Hastie; R Tibshirani"}, {"ref_id": "b5", "title": "Approaches for bayesian variable selection", "journal": "Statistica Sinica", "year": "1997", "authors": "E George; R Mcculloch"}, {"ref_id": "b6", "title": "Sparse probabilistic principal component analysis", "journal": "Journal of Machine Learning Research -Proceedings Track", "year": "2009", "authors": "Y Guan; J Dy"}, {"ref_id": "b7", "title": "2008. NIPS Workshop on Learning from Multiple Sources", "journal": "", "year": "", "authors": "D Hardoon; G Leen; S Kaski; J Shawe-Taylor"}, {"ref_id": "b8", "title": "Relations between two sets of variates", "journal": "Biometrika", "year": "1936", "authors": "H Harold"}, {"ref_id": "b9", "title": "Diagnosis of Alzheimer's disease", "journal": "Archives of Neurology", "year": "1985", "authors": "S Khachaturian"}, {"ref_id": "b10", "title": "Evaluation of cerebellar asymmetry in Alzheimer's disease: a stereological study", "journal": "Dement Geriatr Cogn Disord", "year": "2009", "authors": "O Y Kusbeci; O Bas; N Gocmen-Mas; H S Karabekir; A Yucel; T Ertekin; A C Yazici"}, {"ref_id": "b11", "title": "Principled hybrids of generative and discriminative models", "journal": "", "year": "2006", "authors": "J Lasserre; C M Bishop; T P Minka"}, {"ref_id": "b12", "title": "Bayesian interpolation", "journal": "Neural Computation", "year": "1991", "authors": "D Mackay"}, {"ref_id": "b13", "title": "Laplacian support vector machines trained in the primal", "journal": "Journal of Machine Learning Research", "year": "2011", "authors": "S Melacci; B Mikhail"}, {"ref_id": "b14", "title": "Bayesian and L1 approaches for sparse unsupervised learning", "journal": "", "year": "2012", "authors": "S Mohamed; K Heller; Z Ghahramani"}, {"ref_id": "b15", "title": "Bayesian Learning for Neural Networks", "journal": "", "year": "1996", "authors": "R Neal"}, {"ref_id": "b16", "title": "Baseline MRI predictors of conversion from MCI to probable AD in the ADNI cohort", "journal": "Curr. Alzheimer Res", "year": "2009", "authors": "S Risacher; A Saykin; J West; H Firpi; B Mcdonald"}, {"ref_id": "b17", "title": "The landscape of cancer genes and mutational processes in breast cancer", "journal": "Nature", "year": "2012", "authors": "P Stephens; P Tarpey; H Davies;  Van; P Loo; C Greenman; D Wedge"}, {"ref_id": "b18", "title": "Relevance of magnetic resonance imaging for early detection and diagnosis of Alzheimer disease", "journal": "Medical Clinics of North America", "year": "2013", "authors": "S Teipel; M Grothe; S Lista; N Toschi; F Garaci; H Hampel"}, {"ref_id": "b19", "title": "Regression shrinkage and selection via the lasso", "journal": "Journal of the Royal Statistical Society, Series B", "year": "1994", "authors": "R Tibshirani"}, {"ref_id": "b20", "title": "Probabilistic principal component analysis", "journal": "Journal of The Royal Statistical Society Series B-statistical Methodology", "year": "1999", "authors": "M Tipping; C Bishop"}, {"ref_id": "b21", "title": "Bayesian CCA via group sparsity", "journal": "", "year": "2011", "authors": "S Virtanen; A Klami; S Kaski"}, {"ref_id": "b22", "title": "3D maps from multiple MRI illustrate changing atrophy patterns as subjects progress from mild cognitive impairment to Alzheimer's disease", "journal": "Brain", "year": "2007", "authors": "J Whitwell; S Przybelski; S Weigand"}, {"ref_id": "b23", "title": "Supervised probabilistic principal component analysis", "journal": "", "year": "2006", "authors": "S Yu; K Yu; V Tresp; H Kriegel; M Wu"}, {"ref_id": "b24", "title": "Joint association discovery and diagnosis of alzheimer's disease by supervised heterogeneous multiview learning", "journal": "World Scientific", "year": "2013", "authors": "S Zhe; Z Xu; Y Qi; P Yu"}, {"ref_id": "b25", "title": "Regularization and variable selection via the elastic net", "journal": "Journal of the Royal Statistical Society, Series B", "year": "2005", "authors": "H Zou; T Hastie"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Finally, for weights vector w, we have p(w|s w , \u03c0 w ) = k j=1 p(w j |s j w )p(s j w |\u03c0 j w )", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: The variational lower bound for the model marginal likelihood.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Prediction accuracy with standard errors.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: The estimated associations between MRI features and SNPs. In each sub-figure, the MRI features are listed on the right and the SNP names are given at the bottom.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "H Z y U X G S h S g \u03b7 w S w h j 0 L Figure 1:", "formula_coordinates": [2.0, 54.0, 56.04, 290.34, 99.96]}, {"formula_id": "formula_1", "formula_text": "p(X|U, G, \u03b7) = n i=1 N (x i |Gu i , \u03b7 \u22121 I)", "formula_coordinates": [2.0, 94.75, 627.45, 157.01, 30.32]}, {"formula_id": "formula_2", "formula_text": "p(Z, C|U, H) = q i=1 n j=1 p(c ij |h i , u j )p(z ij |c ij ) where p(z ij |c ij ) = R\u22121 r=0 \u03b4(z ij = r)\u03b4(b r \u2264 c ij < b r+1 ) and p(c ij |h i , u j ) = N (c ij |h i u j , 1).", "formula_coordinates": [2.0, 319.5, 138.99, 238.5, 65.84]}, {"formula_id": "formula_3", "formula_text": "p(y, f |U, w) = p(y|f )p(f |U, w),", "formula_coordinates": [2.0, 370.46, 266.06, 136.59, 9.96]}, {"formula_id": "formula_4", "formula_text": "i |f i ) = 2 r=0 \u03b4(y i = r)\u03b4(b r \u2264 f i < b r+1 ) and p(f i |u i , w) = N (f i |u i w, 1).", "formula_coordinates": [2.0, 319.5, 295.53, 238.5, 36.18]}, {"formula_id": "formula_5", "formula_text": "p(H|L) = j N (h j |0, L \u22121 ) = j N (0|h j , L \u22121 ) = p(0|H, L),", "formula_coordinates": [2.0, 332.72, 462.06, 212.06, 31.89]}, {"formula_id": "formula_6", "formula_text": "p(H|H) = j N (h j |h j , \u03bbI)", "formula_coordinates": [2.0, 378.15, 624.55, 121.2, 14.8]}, {"formula_id": "formula_7", "formula_text": "p(G|S g , \u03a0 g ) = p i=1 k j=1 p(g ij |s ij g )p(s ij g |\u03c0 ij g )", "formula_coordinates": [3.0, 85.19, 163.09, 176.11, 30.79]}, {"formula_id": "formula_8", "formula_text": "\u03c0 ij g in \u03a0 g is the probability of selecting g ij (i.e., s ij g = 1), p(g ij |s ij g ) = s ij g N (g ij |0, \u03c3 2 1 ) + (1 \u2212 s ij g )N (g ij |0, \u03c3 2 2 ), and p(s ij g |\u03c0 ij g ) = \u03c0 ij g s ij g (1 \u2212 \u03c0 ij g ) 1\u2212s ij g . Note that \u03c3 2", "formula_coordinates": [3.0, 54.0, 201.16, 238.5, 53.75]}, {"formula_id": "formula_9", "formula_text": "p(\u03a0 g |l 1 , l 2 ) = p i=1 k j=1 Beta(\u03c0 ij g |l 1 , l 2 )", "formula_coordinates": [3.0, 54.0, 278.26, 181.78, 14.11]}, {"formula_id": "formula_10", "formula_text": "p(H|S h , \u03a0 h ) = q i=1 k j=1 p(h ij |s ij h )p(s ij h |\u03c0 ij h ),", "formula_coordinates": [3.0, 82.81, 309.47, 180.88, 30.79]}, {"formula_id": "formula_11", "formula_text": "\u03c0 ij h is select- ing probability of h ij , p(h ij |s ij h ) = s ij h N (h ij |0, \u03c3 2 1 ) + (1 \u2212 s ij h )N (h ij |0, \u03c3 2 2 ) and p(s ij h |\u03c0 ij h ) = \u03c0 ij h s ij h (1 \u2212 \u03c0 ij h ) 1\u2212s ij h . We assign uninformative Beta hyperpriors for \u03a0 h : p(\u03a0 h |d 1 , d 2 ) = q i=1 k j=1 Beta(\u03c0 ij h |d 1 , d 2 ) where d 1 = d 2 = 1.", "formula_coordinates": [3.0, 54.0, 347.54, 238.51, 83.85]}, {"formula_id": "formula_12", "formula_text": "p(X, Z, y, U, G, Sg, \u03a0g, \u03b7, C, H,H, S h , \u03a0 h , Sw, \u03a0w, f ) = p(X|U, G, \u03b7)p(G|Sg)p(Sg|\u03a0g)p(\u03a0g|l1, l2)p(\u03b7|r1, r2) \u2022 p(Z, C|U, H)p(H|S h )p(S h |\u03a0 h )p(\u03a0 h |d1, d2)p(H|H) \u2022 p(0|H, L)p(y|f )p(f |U, w)p(w|Sw)p(Sw|\u03a0w)p(U).", "formula_coordinates": [3.0, 63.88, 568.83, 218.75, 51.37]}, {"formula_id": "formula_13", "formula_text": "{U, H, G, S g , S h , \u03a0 g , \u03a0 h , \u03b7, w, C, f } by a factorized distribution Q(\u2022) = Q(U)Q(H)Q(G)Q(S g )Q(S h ) Q(\u03a0 g )Q(\u03a0 h )Q(\u03b7)Q(w)Q(C)Q(f ).", "formula_coordinates": [3.0, 319.5, 166.34, 238.5, 32.24]}, {"formula_id": "formula_14", "formula_text": "Q(G) = p i=1 N (g i ; \u03bb i , \u2126 i ),(1)", "formula_coordinates": [3.0, 368.1, 401.06, 189.9, 30.79]}, {"formula_id": "formula_15", "formula_text": "Q(S g ) = p i=1 k j=1 \u03b2 s ij g ij (1 \u2212 \u03b2 ij ) 1\u2212s ij g ,(2)", "formula_coordinates": [3.0, 366.11, 436.09, 191.89, 30.8]}, {"formula_id": "formula_16", "formula_text": "Q(\u03a0 g ) = p i=1 k j=1 Beta(\u03c0 ij g |l ij 1 ,l ij 2 ),(3)", "formula_coordinates": [3.0, 363.51, 472.48, 194.49, 30.79]}, {"formula_id": "formula_17", "formula_text": "Q(\u03b7) = Gamma(\u03b7|r 1 ,r 2 ).(4)", "formula_coordinates": [3.0, 371.81, 508.45, 186.2, 10.32]}, {"formula_id": "formula_18", "formula_text": "\u2126 i = \u03b7 UU + 1 \u03c3 2 1 diag( s i g ) + 1 \u03c3 2 2 diag(1 \u2212 s i g ) \u22121 and \u03bb i = \u2126 i ( \u03b7 U x i ),", "formula_coordinates": [3.0, 319.5, 527.55, 238.5, 38.02]}, {"formula_id": "formula_19", "formula_text": "\u03b2 ij = 1/ 1 + exp( log(1 \u2212 \u03c0 ij g ) \u2212 log(\u03c0 ij g ) + 1 2 log( \u03c3 2 1 \u03c3 2 2 ) + 1 2 g 2 ij ( 1 \u03c3 2 1 \u2212 1 \u03c3 2 2 )) ; the parameters for Q(\u03a0 g ) are given byl ij 1 = \u03b2 ij + l 1 andl ij 2 = 1 \u2212 \u03b2 ij + l 2 ; the parameters for Q(\u03b7) are given byr 1 = r 1 + np 2 andr 2 = r 2 + 1 2 tr(XX ) \u2212 tr( G U X ) + 1 2 tr( UU G G ).", "formula_coordinates": [3.0, 319.5, 589.91, 238.5, 69.41]}, {"formula_id": "formula_20", "formula_text": "Q(C) = q i=1 k j=1 Q(c ij ),(5)", "formula_coordinates": [4.0, 83.46, 143.58, 209.04, 30.79]}, {"formula_id": "formula_21", "formula_text": "Q(c ij ) \u221d \u03b4(b zij \u2264 c ij < b zij +1 )N (c ij |c ij , 1), (6", "formula_coordinates": [4.0, 80.41, 179.55, 208.22, 10.32]}, {"formula_id": "formula_22", "formula_text": ")", "formula_coordinates": [4.0, 288.63, 180.54, 3.87, 8.64]}, {"formula_id": "formula_23", "formula_text": "Q(H) = k i=1 N (h i ; \u03b3 i , \u039b i ), (7", "formula_coordinates": [4.0, 82.77, 195.55, 205.86, 30.32]}, {"formula_id": "formula_24", "formula_text": ")", "formula_coordinates": [4.0, 288.63, 206.28, 3.87, 8.64]}, {"formula_id": "formula_25", "formula_text": "wherec ij = ( H U ) ij , \u039b i = \u0169 i \u0169 i I + L + 1 \u03c3 2 1 diag( s i h ) + 1 \u03c3 2 2 diag( 1 \u2212 s i h ) \u22121 , and \u03b3 i = \u039b i ( C \u2212 j =i \u03b3 j \u0169 j ) u i .", "formula_coordinates": [4.0, 54.0, 235.33, 238.5, 40.62]}, {"formula_id": "formula_26", "formula_text": "Q(S h ) = q i=1 k j=1 \u03b1 s ij h ij (1 \u2212 \u03b1 ij ) 1\u2212s ij h ,(8)", "formula_coordinates": [4.0, 99.85, 293.48, 192.65, 30.79]}, {"formula_id": "formula_27", "formula_text": "Q(\u03a0 h ) = q i=1 k j=1 Beta(\u03c0 ij h |d ij 1 ,d ij 2 ),(9)", "formula_coordinates": [4.0, 97.25, 329.86, 195.25, 30.8]}, {"formula_id": "formula_28", "formula_text": "\u03b1 ij = 1/ 1 + exp( log(1 \u2212 \u03c0 ij h ) \u2212 log(\u03c0 ij h ) + 1 2 log( \u03c3 2 1 \u03c3 2 2 ) + 1 2 h 2 ij ( 1 \u03c3 2 1 \u2212 1 \u03c3 2 2 )) , andd ij 1 = \u03b1 ij + d 1 ,d ij 2 = 1 \u2212 \u03b1 ij + d 2 .", "formula_coordinates": [4.0, 54.0, 369.1, 238.5, 41.98]}, {"formula_id": "formula_29", "formula_text": "Q(f ) = n i=1 Q(f i ),(10)", "formula_coordinates": [4.0, 83.38, 506.73, 209.12, 30.32]}, {"formula_id": "formula_30", "formula_text": "Q(f i ) \u221d \u03b4(b yi \u2264 f i < b yi+1 )N (f i |f i , \u03c3 2 fi ),(11)", "formula_coordinates": [4.0, 79.77, 540.41, 212.73, 12.69]}, {"formula_id": "formula_31", "formula_text": "Q(w) = N (w; m, \u03a3 w ),(12)", "formula_coordinates": [4.0, 79.53, 557.29, 212.97, 10.32]}, {"formula_id": "formula_32", "formula_text": "Q(s w ) = k i=1 \u03c4 s i w i (1 \u2212 \u03c4 i ) 1\u2212s i w ,(13)", "formula_coordinates": [4.0, 76.99, 573.11, 215.51, 30.32]}, {"formula_id": "formula_33", "formula_text": "Q(\u03c0 w ) = k i=1 Beta(\u03c0 i w ;\u1ebd i 1 ,\u1ebd i 2 ), (14", "formula_coordinates": [4.0, 74.72, 608.14, 213.64, 30.32]}, {"formula_id": "formula_34", "formula_text": ")", "formula_coordinates": [4.0, 288.35, 618.87, 4.15, 8.64]}, {"formula_id": "formula_35", "formula_text": "wheref i = ( U m) i , \u03a3 w = UU + 1 \u03c3 2 1 diag( s w ) + 1 \u03c3 2 2 diag( 1 \u2212 s w ) \u22121 , m = \u03a3 w U f , \u03c4 i = 1/ 1 + exp( log(1 \u2212 \u03c0 i w ) \u2212 log(\u03c0 i w ) + 1 2 w 2 i ( 1 \u03c3 2 1 \u2212 1 \u03c3 2 2 )) ,\u1ebd i 1 = \u03c4 i + e 1 and\u1ebd i 2 = 1 \u2212 \u03c4 i + e 2 .", "formula_coordinates": [4.0, 54.0, 645.48, 238.5, 60.35]}, {"formula_id": "formula_36", "formula_text": "Q(U) = i N (u i |\u00b5 i , \u03a3 i ) (15", "formula_coordinates": [4.0, 384.58, 167.05, 169.27, 20.58]}, {"formula_id": "formula_37", "formula_text": ")", "formula_coordinates": [4.0, 553.85, 168.04, 4.15, 8.64]}, {"formula_id": "formula_38", "formula_text": "\u03a3 i = ww + \u03b7 G G + H H + I \u22121 and \u00b5 i = \u03a3 i ( w f i + \u03b7 G x i + H c i ).", "formula_coordinates": [4.0, 319.5, 194.47, 238.5, 26.56]}, {"formula_id": "formula_39", "formula_text": "f test = U test m,(16)", "formula_coordinates": [4.0, 365.58, 308.78, 192.42, 9.68]}, {"formula_id": "formula_40", "formula_text": "y i test = R\u22121 r=0 r \u2022 \u03b4(b r \u2264 f i test < b r+1 ),(17)", "formula_coordinates": [4.0, 364.2, 323.89, 193.8, 30.2]}], "doi": ""}