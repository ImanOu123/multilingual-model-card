{"title": "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation", "authors": "Kuan-Hao Huang; Varun Iyer; \u2295 I-Hung Hsu; Anoop Kumar; Kai-Wei Chang; Aram Galstyan", "pub_date": "", "abstract": "Paraphrase generation is a long-standing task in natural language processing (NLP). Supervised paraphrase generation models, which rely on human-annotated paraphrase pairs, are costinefficient and hard to scale up. On the other hand, automatically annotated paraphrase pairs (e.g., by machine back-translation), usually suffer from the lack of syntactic diversity -the generated paraphrase sentences are very similar to the source sentences in terms of syntax. In this work, we present PARAAMR, a large-scale syntactically diverse paraphrase dataset created by abstract meaning representation backtranslation. Our quantitative analysis, qualitative examples, and human evaluation demonstrate that the paraphrases of PARAAMR are syntactically more diverse compared to existing large-scale paraphrase datasets while preserving good semantic similarity. In addition, we show that PARAAMR can be used to improve on three NLP tasks: learning sentence embeddings, syntactically controlled paraphrase generation, and data augmentation for few-shot learning. Our results thus showcase the potential of PARAAMR for improving various NLP applications.", "sections": [{"heading": "Introduction", "text": "Paraphrase generation is a long-standing task in natural language processing (NLP) (McKeown, 1983;Barzilay and Lee, 2003;Kauchak and Barzilay, 2006). It has been applied to various downstream applications, such as question answering (Yu et al., 2018), chatbot engines (Yan et al., 2016), creative generation (Tian et al., 2021), and improving model robustness (Huang and Chang, 2021). Most existing paraphrase generation models require a large amount of annotated paraphrase pairs Gupta et al., 2018;Kumar et al., 2020). Since human-labeled instances are expensive and hard to scale up (Dolan et al., 2004;Madnani et al., 2012;Iyer et al., 2017), recent research has explored the possibility of generating paraphrase pairs automatically. One popular approach is back-translation Hu et al., 2019a,b), which generates paraphrases of a source sentence by translating it to another language and translating back to the original language. Although backtranslation creates large-scale automatically annotated paraphrase pairs, the generated paraphrases usually suffer from the lack of syntactic diversity -they are very similar to the source sentences, especially in syntactic features. Consequently, supervised paraphrase models trained with those datasets are also limited in their ability to generate syntactically diverse paraphrases. Furthermore, not all words can be perfectly translated into another language. As we will show in Section 4.3, this mismatch may produce subpar paraphrases.\nIn this work, we leverage abstract meaning representation (AMR) (Banarescu et al., 2013) to generate syntactically diverse paraphrase pairs. We present PARAAMR, a large-scale syntactically diverse paraphrase dataset based on AMR backtranslation. As illustrated by Figure 1, our approach works by encoding a source sentence to an AMR graph, modifying the focus of the AMR graph that represents the main assertion, linearizing the modified AMR graph, and finally decoding the linearized graph back to a sentence. Since the new sentence shares the same AMR graph structure as the source sentence, it preserves similar semantics to the source sentence. At the same time, the change of focus makes the new main assertion different from that source sentence. When linearizing the AMR graph, a different concept will be emphasized at the beginning of the string. Therefore, the decoded sentence may have a much different syntax from the source sentence. ", "publication_ref": ["b47", "b7", "b34", "b64", "b56", "b30", "b27", "b37", "b20", "b43", "b32", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Choose different focuses", "text": "Linearized AMR Graph I know they need statistical documentation to approve this price.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Linearized AMR Graph", "text": "They need statistical documentation to approve these prices, I know. The overall framework to construct PARAAMR based on AMR back-translation. We encode a source sentence to an AMR graph, modify the focus of the AMR graph, linearize the modified AMR graph, and finally decode the linearized graph to a syntactically diverse paraphrase.\nOur quantitative analysis (Section 4.2) and qualitative examples (Section 4.3) show that the paraphrases of PARAAMR are syntactically more diverse than existing datasets Hu et al., 2019a,b), while at the same time preserving good semantic similarity between paraphrased sentences. In addition, our human evaluation results (Section 4.4) confirm that PARAAMR is indeed more syntactically diverse than prior datasets. To showcase the benefits of syntactically diverse paraphrases, we conduct experiments on three downstream tasks: learning sentence embeddings (Section 5.1), syntactically controlled paraphrase generation (Section 5.2), and data augmentation for few-shot learning (Section 5.3). We observe that models trained on PARAAMR achieve better performance on all three downstream tasks compared to other datasets, thus indicating its potential value for various NLP applications. 1", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Paraphrase generation and datasets. Traditional paraphrase generation models are usually based on hand-crafted rules, including rule-based methods (McKeown, 1983), thesaurus-based methods (Bolshakov and Gelbukh, 2004;Kauchak and Barzilay, 2006), and lattice matching methods (Barzilay and Lee, 2003). In recent years, different neural models have been proposed for paraphrase generation (Prakash et al., 2016;Mallinson et al., 2017;Cao et al., 2017;Egonmwan and Chali, 2019;Gupta et al., 2018;Zhang et al., 2019c;Roy and Grangier, 2019;Iyyer et al., 2018;Huang and Chang, 2021). Some advanced techniques are proposed as well, such as multi-round generation (Lin and Wan, 2021), reinforcementlearning-based paraphrasing (Liu et al., 2020), andprompt-tuning (Chowdhury et al., 2022). To properly train those neural models, however, we needs a large corpus of annotated paraphrase pairs. Most existing paraphrase datasets and related resources, such as MRPC (Dolan et al., 2004), PAN (Madnani et al., 2012), PPDB (Ganitkevitch et al., 2013), and Quora (Iyer et al., 2017), have limited scale. Therefore, researchers have focused on automatically generating large-scale paraphrase corpora. One notable example is PARANMT , which is created by machine backtranslation -translating texts to another language and translating them back to the original language.", "publication_ref": ["b47", "b11", "b34", "b7", "b50", "b44", "b15", "b21", "b27", "b69", "b53", "b33", "b30", "b41", "b39", "b20", "b22", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Syntactically diverse paraphrase generation.", "text": "Another line of research focuses on diversifying the generated paraphrases in terms of syntax. This includes sampling from latent spaces (Roy and Grangier, 2019;Zhang et al., 2019c;Cao and Wan, 2020), controlling word order (Goyal and Durrett, 2020), and controlling syntax (Iyyer et al., 2018;Cao and Clark, 2019;Kumar et al., 2020;Huang and Chang, 2021;Sun et al., 2021;Huang et al., 2022;Lee et al., 2022). Although they can diversify the generated paraphrases based on different model designs, those models are still limited due to the lack of diversity in existing large-scale paraphrase datasets. Some works propose large-scale diverse paraphrases by considering different decoding methods during back-translation, including lexical constraints (Hu et al., 2019a) and cluster-based constrained sampling (Hu et al., 2019b). Although increasing the lexical diversity, the syntactic diversity of their datasets is still limited.\nText-to-AMR parsing. Abstract meaning representation (AMR) (Banarescu et al., 2013) is designed for capturing abstract semantics. Since it offers benefits to many NLP tasks, several works focus on parsing AMR from texts. Transitionbased methods maintain a stack and a buffer for parsing AMR (Wang et al., 2015;Damonte et al., 2017;Ballesteros and Al-Onaizan, 2017;Vilares and G\u00f3mez-Rodr\u00edguez, 2018;Naseem et al., 2019). Graph-based approaches extract AMR based on graph information (Zhang et al., 2019a,b;Cai and Lam, 2020;Zhou et al., 2020). Sequence-tosequence approaches directly linearize AMR and train end-to-end models to produce AMR (Konstas et al., 2017a;van Noord and Bos, 2017;Peng et al., 2017;Ge et al., 2019).\nAMR-to-Text generation. Generating texts from AMR graphs is a popular research direction as well. Most existing approaches can be grouped into two categories. The first group is based on structure-totext methods, where they build graphs to capture the structural information (Marcheggiani and Perez-Beltrachini, 2018;Song et al., 2018;Beck et al., 2018;Damonte and Cohen, 2019;Zhao et al., 2020;Wang et al., 2020). The second group is based on sequence-to-sequence methods (Konstas et al., 2017b;Ribeiro et al., 2021), where they treat AMR as a string and train end-to-end models.", "publication_ref": ["b53", "b69", "b14", "b26", "b33", "b13", "b37", "b30", "b55", "b31", "b38", "b28", "b29", "b6", "b60", "b18", "b5", "b58", "b48", "b12", "b71", "b35", "b57", "b49", "b24", "b46", "b54", "b8", "b17", "b70", "b61", "b36", "b52"], "figure_ref": [], "table_ref": []}, {"heading": "PARAAMR", "text": "We propose PARAAMR, a large-scale syntactically diverse paraphrase dataset. Figure 1 illustrates the overall framework to construct PARAAMR by AMR back-translation. In summary, we encode a source sentence to an AMR graph, modify the focus of the AMR graph (see Section 3.3), linearize the modified AMR graph, and finally decode the linearized graph to a syntactically diverse paraphrase. We describe the details in the following.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data Source", "text": "In order to fairly compare with prior works Hu et al., 2019a,b), we choose the same Czech-English dataset (Bojar et al., 2016) as our data source. Specifically, we directly use the English source sentences from the previous dataset (Hu et al., 2019b) as the source sentences for AMR back-translation. It is worth noting that our proposed method is not limited to this dataset but can be applied to any general texts for constructing syntactically diverse paraphrases.", "publication_ref": ["b10", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Translating Texts to AMR Graphs", "text": "We use a pre-trained AMR parser to encode source sentences to AMR graphs. Specifically, we consider SPRING (Bevilacqua et al., 2021), a BARTbased (Lewis et al., 2020) AMR parser trained on AMR 3.0 annotations 2 and implemented by amrlib. 3 As illustrated by Figure 1, each source sentence will be encoded to an AMR graph, which is a directed graph that has each node represents a semantic concept (e.g., know, need, and they) and each edge describe the semantic relations between two concepts (e.g., ARG0, ARG1-of, and mod) (Banarescu et al., 2013).\nAn AMR graph aims at capturing the meaning of a sentence while abstracting away syntactic, lexical, and other features. Each AMR graph has a focus, which is the root node of the graph, to represent the main assertion. For example, the focus of the AMR graph extracted from the source sentence in Figure 1 is know. Most of the time, the focus will be the main verb; however, it actually can be any concept node.", "publication_ref": ["b9", "b39", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Translating AMR Graphs to Texts", "text": "Usually, syntactically different sentences with similar meanings have similar undirected AMR graph structures and differ only in their focuses and the directions of edges. We plan to use this property to construct syntactically diverse paraphrases of a source sentence.\nChanging the focus of an AMR graph. After extracting the AMR graph from a source sentence, we construct several new graphs by changing the focus. More precisely, we randomly choose a node as the new focus and reverse all the incoming edges for that node. For instance, in Figure 1, when we choose need as the new focus, the incoming edge from know is reversed, and its edge label changes from ARG1 to ARG1-of. Similarly, when we choose they as the new focus, the incoming edge from need and approve are reversed, and their edge labels change from ARG0 to ARG0-of. Sometimes, to maintain a tree-like graph, some outgoing edges of the original focus node will be reversed as well (e.g., the edge between know and need is reversed when we choose they as the new focus). It is worth noting that when the focus changes, the undirected AMR graph structure remains the same, meaning that the new AMR graph preserves a similar abstract meaning to the old one. We implement the process of AMR re-focusing by the PENMAN package (Goodman, 2020). 4\nLinearizing AMR graph. After constructing several new graphs from the original AMR graph, we linearize the new graphs with the new focus (root node). This is done by traversing the AMR graph starting from the new focus node with a depth-first-search algorithm and converting it to the PENMAN notation. For example, the AMR graph with the focus being need can be linearized in the following format:\n(z3 / need :ARG1-of (z1 / know :ARG0 (z2 / i)) :ARG0 (z4 / they) :ARG1 (z5 / documentation :mod (z6 / statistic)) :purpose (z7 / approve :ARG0 z4 :ARG1 (z8 / thing :ARG2-of (z9 / price) :mod (z10 / this))))\nSimilarly, the AMR graph with the focus node they can be linearized in the following format:\n(z4 / they :ARG0-of (z3 / need :ARG1 (z5 / documentation :mod (z6 / statistic)) :purpose (z7 / approve :ARG0 z4 :ARG1 (z8 / thing :ARG2-of (z9 / price) :mod (z10 / this))) :ARG1-of (z1 / know :ARG0 (z2 / i))))\nDecoding AMR graph to texts. We use a T5based pre-trained AMR-to-text generator (Ribeiro et al., 2021) to translate the linearized graphs back to sentences. Since the generated sentences share the same undirected AMR graph as the source sentence, they should have similar meanings and thus can be considered as paraphrases of the source sentence. In addition, we observe that the pre-trained AMR-to-text generator tends to emphasize the focus node of an AME graph at the beginning of the generated sentence. Therefore, the generated sentences from the linearized graphs with different focuses are very likely syntactically different from the source sentence.", "publication_ref": ["b52"], "figure_ref": [], "table_ref": []}, {"heading": "Post-Processing", "text": "We notice that not all nodes are appropriate to be the focus. Choosing inappropriate nodes as the focus might generate paraphrases that are not grammatically fluent or natural. To avoid this situation, we use perplexity to filter out bad paraphrases. Specifically, we consider the GPT-2 model (Radford et al., 2019) implemented by HuggingFace's Transformers (Wolf et al., 2020) to compute the perplexity of a candidate paraphrase. We found that setting the filtering threshold to 120 is generally good enough, although some downstream applications may need different thresholds.", "publication_ref": ["b63"], "figure_ref": [], "table_ref": []}, {"heading": "Comparison to Prior Datasets", "text": "We compare PARAAMR with the following three datasets.\n(1) PARANMT  create paraphrase pairs by English-Czech-English back-translation. (2) PARABANK1 (Hu et al., 2019a) adds lexical constraints during the decoding of back-translation to increase the lexical diversity of generated paraphrases. (3) PARA-BANK2 (Hu et al., 2019b) proposes cluster-based constrained sampling to improve the syntactic diversity of generated paraphrases.  ", "publication_ref": ["b28", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Basic Statistics", "text": "(\u2191) 1 -BLEU (\u2191) 1 -\u2229/\u222a (\u2191) TED-3 (\u2191) TED-F (\u2191)\nPARANMT  84  6.91 paraphrases on average, which is more than the other three datasets.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Quantitative Analysis", "text": "Following previous work (Hu et al., 2019b) We use the following metrics to evaluate the semantic similarity of paraphrases:\n\u2022 Semantic similarity measure by SimCSE:\nGiven two paraphrase sentences, we use the supervised SimCSE model (Gao et al., 2021) to get the sentence embeddings, and compute the cosine similarity between the two sentence embeddings as the semantic similarity.\nFollowing the previous work (Hu et al., 2019b), we consider the following automatic metrics for lexical diversity:\n\u2022 1 -BLEU (\u2191): We compute one minus BLEU score as the diversity score.\n\u2022 1 -\u2229/\u222a (\u2191): We first compute the ratio of the number of shared tokens between the two sentences and the union of all tokens in the two sentences, then use one minus the ratio as the diversity score.\nWe consider the following automatic metrics for syntactic diversity:\n\u2022 TED-3 (\u2191): We first get the constituency parse trees of the two sentences by using the Stanford CoreNLP parser (Manning et al., 2014). Then, we only consider the top-3 layers of trees and compute the tree editing distance as the score.\n\u2022 TED-F (\u2191): We first get the constituency parse trees of the two sentences by using the Stanford CoreNLP parser (Manning et al., 2014). Then, we consider the whole tree and compute the tree editing distance as the score.\nFrom Table 2, we conclude that the paraphrases generated by PARAAMR increase much more syntactic diversity while preserving comparable semantics compared to prior datasets.", "publication_ref": ["b29", "b23", "b29", "b45", "b45"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Qualitative Examples", "text": "Table 3 shows some paraphrases generated by different datasets. We can observe that prior datasets based on machine back-translation tend to only replace synonyms as paraphrases. In contrast, PARAAMR is able to generate paraphrases that have much different word order and syntactic structures compared to the source sentence. This again showcases the syntactic diversity of PARAAMR.\nIn addition, we notice that other datasets may change the meaning of the source sentence (e.g., from price to prize and from paddle to row) due to the translation errors between different languages. PARAAMR, on the other hand, does not depend on other languages and thus is more reliable.\nSource Sentence I know for them to approve this price, they'll need statistical documentation.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "PARANMT", "text": "I know that in order to accept this award, they'll need a statistical analysis.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "PARABANK1", "text": "I know that to accept this prize, they're going to need statistical analysis. I know that in order to accept this prize, they're going to need a statistic analysis. I know that if they accept this prize, they're gonna need a statistical analysis.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "PARABANK2", "text": "I know that to accept that prize, they're going to need a statistical analysis. I know that in order to accept this prize, they will require a statistical analysis. I know they'll require statistical analysis to accept that prize.\nPARAAMR I know they need statistical documentation to approve this price.\nThere is statistic documentation I know they need to approve these prices. They need statistical documentation to approve these prices, I know.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Source Sentence", "text": "If I wanted to paddle down the river, where's the best place to launch out of?   ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Human Evaluation", "text": "We additionally conduct human evaluations to measure the semantic similarity and the syntactic diversity of different datasets. We used the Amazon Mechanical Turk 5 to conduct the human evaluation. We randomly sample 300 paraphrases from each dataset, and design questions to measure the semantic similarity and syntactic diversity.\nFor semantic similarity, we design a 3-point scale question and ask the annotators to answer the question:\n\u2022 Score 3: The two sentences are paraphrases of each other. Their meanings are near-equivalent.\n\u2022 Score 2: The two sentences have similar meanings but some unimportant details differ.\n5 https://www.mturk.com/\n\u2022 Score 1: Some important information differs or is missing, which alters the intent or meaning.\nFor syntactic diversity, we design a 3-point scale question and ask the annotators to answer the question:\n\u2022 Score 3: The two sentences are written in very different ways or have much different sentence structures. (For example, \"We will go fishing if tomorrow is sunny.\" and \"If tomorrow is sunny, we will go fishing\")\n\u2022 Score 2: Only some words in the two sentences differ. (For example, \"We will go fishing if tomorrow is sunny.\" and \"We are going to go fishing if tomorrow is sunny.\")\n\u2022 Score 1: The two sentences are almost the same.\nAppendix A lists more details of human evalua-tion. The average scores of human evaluation are shown in Table 4. We observe that PARAAMR gets a much higher score for syntactic diversity although it has a slightly lower score for semantic similarity.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Applications", "text": "We focus on three downstream applications of PARAAMR corpus: learning sentence embeddings (Section 5.1), syntactically controlled paraphrase generation (Section 5.2), and data augmentation for few-shot learning (Section 5.3). We demonstrate the strength of PARAAMR and compare with prior datasets: PARANMT , PARABANK1 (Hu et al., 2019a), and PARABANK2 (Hu et al., 2019b).", "publication_ref": ["b28", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Learning Sentence Embeddings", "text": "We conduct experiments to show that PARAAMR is beneficial to learn sentence embeddings because of its syntactic diversity.\nSettings. We consider the supervised SimCSE (Gao et al., 2021), a contrastive learning framework to learn sentence embeddings from (reference sentence, positive sentence, negative sentence) triplets.\nWe train different SimCSE models with the paraphrase pairs in all four datasets. Specifically, for each (source sentence, paraphrase sentence) pair in the dataset, we consider the source sentence as the reference sentence, consider the paraphrase sentence as the positive sentence, and randomly sample one sentence from the dataset as the negative sentence.\nTraining details. We use the script provided by the SimCSE paper 6 (Gao et al., 2021) to train a SimCSE model with the weights initialized by bert-base-uncased (Devlin et al., 2019). The batch size is set to 128 and the number of epochs is 3. We set the learning rate to 10 \u22125 and set other parameters as the default values from the script. It takes around 3 hours to train the SimCSE models for a single NVIDIA RTX A6000 GPU with 48GB memory. We set the perplexity threshold to 110 to filter PARAAMR. For each dataset, we train 5 different models with 5 different random seeds and report the average scores.\nEvaluation.   et al., 2012, 2013, 2014, 2015, 2016). We consider the script from SentEval 7 and use the learned sentence embeddings to calculate the cosine similarity between two sentences. We report the average Pearson correlation coefficient and the average Spearman correlation coefficient over all tasks.\nExperimental results. Table 5 lists the average score for STS 2012 to 2016. We observe that the sentence embeddings learned with PARAAMR get better scores than other datasets, especially for the Pearson correlation coefficient. We hypothesize that the syntactic diversity of PARAAMR makes the sentence embeddings capture semantics better and reduce the influence of syntactic similarity.", "publication_ref": ["b23", "b23", "b19"], "figure_ref": [], "table_ref": ["tab_9"]}, {"heading": "Syntactically Controlled Paraphrase Generation", "text": "We demonstrate that PARAAMR is better for training a syntactically controlled paraphrase generator.\nSettings. We consider the same setting as the previous works (Iyyer et al., 2018;Huang and Chang, 2021), which uses constituency parses as the control signal to train paraphrase generators. More precisely, the goal is to train a syntactically controlled paraphrase generator with the input being (source sentence, target constituency parse) pair and the output being a paraphrase sentence with syntax following the target constituency parse.\nWe consider the SCPN model (Iyyer et al., 2018), which is a simple sequence-to-sequence model, as our base model. We train different SCPN models with different datasets. for each (source sentence, paraphrase sentence) pair in the dataset, we treat the paraphrase sentence as the target sentence and use the Stanford CoreNLP toolkit (Manning et al., 2014) to extract constituency parse from the paraphrase sentence as the target parse.\nTraining details. Unlike the original SCPN paper (Iyyer et al., 2018)  the base model, we fine-tune the pre-trained bart-base (Lewis et al., 2020) to learn the syntactically controlled paraphrase generator. The batch size is set to 32 and the number of epochs is 40. The max lengths for source sentences, target sentences, and target syntax are set to 60, 60, and 200, respectively. We set the learning rate to 3 \u00d7 10 \u22125 and consider the Adam optimizer without weight decay. For the beam search decoding, the number of beams is set to 4. It takes around 12 hours to train the SCPN model for a single NVIDIA RTX A6000 GPU with 48GB memory. We set the perplexity threshold to 85 to filter PARAAMR. For each dataset, we train 5 different models with 5 different random seeds and report the average scores.\nEvaluation. We consider three human-annotated paraphrase datasets: Quora (Iyer et al., 2017), MRPC (Dolan et al., 2004), and PAN (Madnani et al., 2012), as the testing datasets. Specifically, we use the testing examples provided by previous work 8 (Huang and Chang, 2021) and calculate the BLEU score between the ground-truth and the generated output as the evaluation metric.\nExperimental results. Table 6 shows the results of syntactically controlled paraphrase generation. The paraphrase generator trained with PARAAMR performs significantly better than others. We believe this is because PARAAMR provides several syntactically different paraphrases for one source sentence, therefore helping the paraphrase generator to better learn the association between parse and words.", "publication_ref": ["b33", "b30", "b33", "b45", "b33", "b39", "b32", "b20", "b43", "b30"], "figure_ref": [], "table_ref": ["tab_11"]}, {"heading": "Data Augmentation for Few-Shot Learning", "text": "Finally, we show that PARAAMR is helpful to generate augmented data for few-shot learning.\nSettings. We choose the following three classification tasks from GLUE (Wang et al., 2019  MRPC, QQP, and RTE. We randomly sample 15 and 30 instances to train classifiers as the few-shot baseline. Since most tasks in GLUE do not provide the official test labels, we randomly sample 1/3 of instances from the dev set as the internal dev set and use the rest 2/3 instances as the testing set.\nFor each dataset, we use the learned syntactically controlled paraphrase generators from Section 5.2 to generate three augmented examples with different parses for each training instance. More specifically, we first use the pretrained SCPN model (Iyyer et al., 2018)  Training details. For the few-shot classifiers, we fine-tune bert-base-uncased (Devlin et al., 2019). We set the batch size to 8, set the learning rate to 10 \u22124 , and set the number of epochs to 20. We consider Adam optimizer with weight decay being 10 \u22125 . It takes around 5 minutes to train a fewshot classifier for a single NVIDIA RTX A6000 GPU with 48GB memory. We set the perplexity threshold to 110 to filter PARAAMR.\nExperimental results. The results in Table 7 demonstrate that leveraging PARAAMR for data augmentation in few-shot learning scenarios leads to consistently better results compared to other paraphrasing corpora. This observation, combined with the two previous experiments, showcases the potential value of PARAAMR for various NLP applications.", "publication_ref": ["b59", "b33", "b19"], "figure_ref": [], "table_ref": ["tab_13"]}, {"heading": "Conclusion", "text": "In this work, we present PARAAMR, a large-scale syntactically diverse paraphrase dataset created by AMR back-translation. Our quantitative analysis, qualitative examples, and human evaluation demonstrate that the paraphrases of PARAAMR are more syntactically diverse than prior datasets while preserving semantic similarity. In addition, we conduct experiments on three downstream tasks, including learning sentence embeddings, syntactically controlled paraphrase generation, and data augmentation for few-shot learning, to demonstrate the advantage of syntactically diverse paraphrases.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Our goal is to demonstrate the potential of using AMR to generate syntactically diverse paraphrases. Although we have shown the strength of diverse paraphrases, there are still some limitations. First, our proposed techniques are strongly based on the quality of pre-trained text-to-AMR parsers and pre-trained AMR-to-text generators. If we cannot get a strong pre-trained text-to-AMR parser and a pre-trained AMR-to-text generator, the generated paraphrases might not have good quality. Second, one step in our proposed framework is modifying the root node of the AMR graph and therefore changing the focus of the AMR graph. However, not all nodes can be good root nodes to generate appropriate paraphrases. Some of them can be not fluent and much different from natural sentences. Although we use perplexity to filter out those paraphrases, there must be some imperfect paraphrases remaining. This partially affects the semantic scores of PARAAMR. Nevertheless, we still show that the current quality of PARAAMR is good enough to improve at least three NLP tasks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Broader Impacts", "text": "Our dataset construction process relies on a pretrained AMR-to-text generator. It is known that the models trained with a large text corpus may capture the bias reflecting the training data. Therefore, it is possible that PARAAMR contains offensive or biased content learned from the data. We suggest to carefully examining the potential bias before applying our dataset to any real-world applications. B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Section 3, 4, and 5 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. Left blank.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Not applicable. Left blank. B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. Section 5 C Did you run computational experiments? Section 5 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? Section 5\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\nC2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? Section 5 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? Section 5 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? Section 5 D Did you use human annotators (e.g., crowdworkers) or research with human participants? Section 4 D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? Appendix A D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? Section 4 D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? Section 3 D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? Not applicable. Left blank.\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? Appendix A", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We thank anonymous reviewers for their helpful feedback. We thank Amazon Alexa AI and the UCLA-NLP group for the valuable discussions and comments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Details of Human Evaluation", "text": "We use the template shown in Figure 2 to conduct the human evaluation. We sampled 300 paraphrases from PARANMT, PARABANK1, PARA-BANK2, and PARAAMR that share the same source sentences for human evaluation.\nFor each paraphrase pair, we ask three MTurkers to annotate the quality of semantics preservation and syntactic diversity in a 3-point scale question. We filter the MTurkers by approval rate greater than 97% and the number of approval greater than 50. The pay rate is $0.1 per paraphrase pair. We do not collect any personal information of MTurkers. ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on interpretability", "journal": "", "year": "2015", "authors": "Eneko Agirre; Carmen Banea; Claire Cardie; Daniel M Cer; Mona T Diab; Aitor Gonzalez-Agirre; Weiwei Guo; I\u00f1igo Lopez-Gazpio; Montse Maritxalar; Rada Mihalcea; German Rigau; Larraitz Uria; Janyce Wiebe"}, {"ref_id": "b1", "title": "Semeval-2014 task 10: Multilingual semantic textual similarity", "journal": "", "year": "2014", "authors": "Eneko Agirre; Carmen Banea; Claire Cardie; Daniel M Cer; Mona T Diab; Aitor Gonzalez-Agirre; Weiwei Guo; Rada Mihalcea; German Rigau; Janyce Wiebe"}, {"ref_id": "b2", "title": "Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation", "journal": "", "year": "2016", "authors": "Eneko Agirre; Carmen Banea; Daniel M Cer; Mona T Diab; Aitor Gonzalez-Agirre; Rada Mihalcea; German Rigau; Janyce Wiebe"}, {"ref_id": "b3", "title": "Semeval-2012 task 6: A pilot on semantic textual similarity", "journal": "", "year": "2012", "authors": "Eneko Agirre; Daniel M Cer; Mona T Diab; Aitor Gonzalez-Agirre"}, {"ref_id": "b4", "title": "*sem 2013 shared task: Semantic textual similarity", "journal": "", "year": "2013", "authors": "Eneko Agirre; Daniel M Cer; Mona T Diab; Aitor Gonzalez-Agirre; Weiwei Guo"}, {"ref_id": "b5", "title": "AMR parsing using stack-lstms", "journal": "", "year": "2017", "authors": "Miguel Ballesteros; Yaser Al-Onaizan"}, {"ref_id": "b6", "title": "Abstract meaning representation for sembanking", "journal": "LAW-ID@ACL", "year": "2013", "authors": "Laura Banarescu; Claire Bonial; Shu Cai; Madalina Georgescu; Kira Griffitt; Ulf Hermjakob; Kevin Knight; Philipp Koehn; Martha Palmer; Nathan Schneider"}, {"ref_id": "b7", "title": "Learning to paraphrase: An unsupervised approach using multiplesequence alignment", "journal": "Association for Computational Linguistics", "year": "2003", "authors": "Regina Barzilay; Lillian Lee"}, {"ref_id": "b8", "title": "Graph-to-sequence learning using gated graph neural networks", "journal": "", "year": "2018", "authors": "Daniel Beck; Gholamreza Haffari; Trevor Cohn"}, {"ref_id": "b9", "title": "One SPRING to rule them both: Symmetric AMR semantic parsing and generation without a complex pipeline", "journal": "", "year": "2021", "authors": "Michele Bevilacqua; Rexhina Blloshmi; Roberto Navigli"}, {"ref_id": "b10", "title": "Czeng 1.6: Enlarged czech-english parallel corpus with processing tools dockered", "journal": "", "year": "2016", "authors": "Ondrej Bojar; Ondrej Dusek; Tom Kocmi; Jindrich Libovick\u00fd; Michal Nov\u00e1k; Martin Popel; Roman Sudarikov; Dusan Varis"}, {"ref_id": "b11", "title": "Synonymous paraphrasing using wordnet and internet", "journal": "", "year": "2004", "authors": "Igor A Bolshakov; Alexander F Gelbukh"}, {"ref_id": "b12", "title": "AMR parsing via graphsequence iterative inference", "journal": "", "year": "2020", "authors": "Deng Cai; Wai Lam"}, {"ref_id": "b13", "title": "Factorising AMR generation through syntax", "journal": "", "year": "2019", "authors": "Kris Cao; Stephen Clark"}, {"ref_id": "b14", "title": "Divgan: Towards diverse paraphrase generation via diversified generative adversarial network", "journal": "", "year": "2020", "authors": "Yue Cao; Xiaojun Wan"}, {"ref_id": "b15", "title": "Joint copying and restricted generation for paraphrase", "journal": "", "year": "2017", "authors": "Ziqiang Cao; Chuwei Luo; Wenjie Li; Sujian Li"}, {"ref_id": "b16", "title": "Novelty controlled paraphrase generation with retrieval augmented conditional prompt tuning", "journal": "", "year": "2022", "authors": "Jishnu Ray Chowdhury; Yong Zhuang; Shuyi Wang"}, {"ref_id": "b17", "title": "Structural neural encoders for amr-to-text generation", "journal": "", "year": "2019", "authors": "Marco Damonte; Shay B Cohen"}, {"ref_id": "b18", "title": "An incremental parser for abstract meaning representation", "journal": "", "year": "2017", "authors": "Marco Damonte; Shay B Cohen; Giorgio Satta"}, {"ref_id": "b19", "title": "BERT: pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b20", "title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "journal": "", "year": "2004", "authors": "Bill Dolan; Chris Quirk; Chris Brockett"}, {"ref_id": "b21", "title": "Transformer and seq2seq model for paraphrase generation", "journal": "", "year": "2019", "authors": "Elozino Egonmwan; Yllias Chali"}, {"ref_id": "b22", "title": "PPDB: the paraphrase database", "journal": "", "year": "2013", "authors": "Juri Ganitkevitch; Benjamin Van Durme; Chris Callison-Burch"}, {"ref_id": "b23", "title": "Simcse: Simple contrastive learning of sentence embeddings", "journal": "", "year": "2021", "authors": "Tianyu Gao; Xingcheng Yao; Danqi Chen"}, {"ref_id": "b24", "title": "Modeling source syntax and semantics for neural AMR parsing", "journal": "", "year": "2019", "authors": "Donglai Ge; Junhui Li; Muhua Zhu; Shoushan Li"}, {"ref_id": "b25", "title": "Penman: An opensource library and tool for AMR graphs", "journal": "", "year": "2020", "authors": " Michael Wayne Goodman"}, {"ref_id": "b26", "title": "Neural syntactic preordering for controlled paraphrase generation", "journal": "", "year": "2020", "authors": "Tanya Goyal; Greg Durrett"}, {"ref_id": "b27", "title": "A deep generative framework for paraphrase generation", "journal": "", "year": "2018", "authors": "Ankush Gupta; Arvind Agarwal; Prawaan Singh; Piyush Rai"}, {"ref_id": "b28", "title": "PARABANK: monolingual bitext generation and sentential paraphrasing via lexically-constrained neural machine translation", "journal": "", "year": "2019", "authors": "J Edward Hu; Rachel Rudinger; Matt Post; Benjamin Van Durme"}, {"ref_id": "b29", "title": "Large-scale, diverse, paraphrastic bitexts via sampling and clustering", "journal": "", "year": "2019", "authors": "J Edward Hu; Abhinav Singh; Nils Holzenberger; Matt Post; Benjamin Van Durme"}, {"ref_id": "b30", "title": "Generating syntactically controlled paraphrases without using annotated parallel pairs", "journal": "", "year": "2021", "authors": "Hao Kuan; Kai-Wei Huang;  Chang"}, {"ref_id": "b31", "title": "Unsupervised syntactically controlled paraphrase generation with abstract meaning representations", "journal": "", "year": "2022", "authors": "Kuan-Hao Huang; Varun Iyer; Anoop Kumar; Sriram Venkatapathy; Kai-Wei Chang; Aram Galstyan"}, {"ref_id": "b32", "title": "First quora dataset release: Question pairs", "journal": "", "year": "2017", "authors": "Shankar Iyer; Nikhil Dandekar; Korn\u00e9l Csernai"}, {"ref_id": "b33", "title": "Adversarial example generation with syntactically controlled paraphrase networks", "journal": "", "year": "2018", "authors": "Mohit Iyyer; John Wieting; Kevin Gimpel; Luke Zettlemoyer"}, {"ref_id": "b34", "title": "Paraphrasing for automatic evaluation", "journal": "Association for Computational Linguistics", "year": "2006", "authors": "David Kauchak; Regina Barzilay"}, {"ref_id": "b35", "title": "Neural AMR: sequence-to-sequence models for parsing and generation", "journal": "", "year": "2017", "authors": "Ioannis Konstas; Srinivasan Iyer; Mark Yatskar; Yejin Choi; Luke Zettlemoyer"}, {"ref_id": "b36", "title": "Neural AMR: sequence-to-sequence models for parsing and generation", "journal": "", "year": "2017", "authors": "Ioannis Konstas; Srinivasan Iyer; Mark Yatskar; Yejin Choi; Luke Zettlemoyer"}, {"ref_id": "b37", "title": "Syntax-guided controlled generation of paraphrases", "journal": "Trans. Assoc. Comput. Linguistics", "year": "2020", "authors": "Ashutosh Kumar; Kabir Ahuja; Raghuram Vadapalli; Partha P Talukdar"}, {"ref_id": "b38", "title": "Using structured content plans for fine-grained syntactic control in pretrained language model generation", "journal": "", "year": "2022", "authors": "Miguel Fei-Tzin Lee; Feng Ballesteros; Kathleen R Nan;  Mckeown"}, {"ref_id": "b39", "title": "BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension", "journal": "", "year": "2020", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal; Marjan Ghazvininejad; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"ref_id": "b40", "title": "Decomposable neural paraphrase generation", "journal": "", "year": "2019", "authors": "Zichao Li; Xin Jiang; Lifeng Shang; Qun Liu"}, {"ref_id": "b41", "title": "Pushing paraphrase away from original sentence: A multi-round paraphrase generation approach", "journal": "", "year": "2021", "authors": "Zhe Lin; Xiaojun Wan"}, {"ref_id": "b42", "title": "2020. A learning-exploring method to generate diverse paraphrases with multi-objective deep reinforcement learning", "journal": "", "year": "", "authors": "Mingtong Liu; Erguang Yang; Deyi Xiong; Yujie Zhang; Yao Meng; Changjian Hu; Jinan Xu; Yufeng Chen"}, {"ref_id": "b43", "title": "Re-examining machine translation metrics for paraphrase identification", "journal": "Association for Computational Linguistics", "year": "2012", "authors": "Nitin Madnani; Joel R Tetreault; Martin Chodorow"}, {"ref_id": "b44", "title": "Paraphrasing revisited with neural machine translation", "journal": "", "year": "2017", "authors": "Jonathan Mallinson; Rico Sennrich; Mirella Lapata"}, {"ref_id": "b45", "title": "The stanford corenlp natural language processing toolkit", "journal": "", "year": "2014", "authors": "Christopher D Manning; Mihai Surdeanu; John Bauer; Jenny Rose Finkel; Steven Bethard; David Mc-Closky"}, {"ref_id": "b46", "title": "Deep graph convolutional encoders for structured data to text generation", "journal": "", "year": "2018", "authors": "Diego Marcheggiani; Laura Perez-Beltrachini"}, {"ref_id": "b47", "title": "Paraphrasing questions using given and new information", "journal": "Am. J. Comput. Linguistics", "year": "1983", "authors": "Kathleen R Mckeown"}, {"ref_id": "b48", "title": "Rewarding smatch: Transition-based AMR parsing with reinforcement learning", "journal": "", "year": "2019", "authors": "Tahira Naseem; Abhishek Shah; Hui Wan; Radu Florian; Salim Roukos; Miguel Ballesteros"}, {"ref_id": "b49", "title": "Addressing the data sparsity issue in neural AMR parsing", "journal": "", "year": "2017", "authors": "Xiaochang Peng; Chuan Wang; Daniel Gildea; Nianwen Xue"}, {"ref_id": "b50", "title": "Neural paraphrase generation with stacked residual LSTM networks", "journal": "", "year": "2016", "authors": "Aaditya Prakash; A Sadid; Kathy Hasan;  Lee; V Vivek; Ashequl Datla; Joey Qadir; Oladimeji Liu;  Farri"}, {"ref_id": "b51", "title": "Language models are unsupervised multitask learners", "journal": "OpenAI blog", "year": "2019", "authors": "Alec Radford; Jeffrey Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"ref_id": "b52", "title": "Investigating pretrained language models for graph-to-text generation", "journal": "", "year": "2021", "authors": "F R Leonardo; Martin Ribeiro;  Schmitt"}, {"ref_id": "b53", "title": "Unsupervised paraphrasing without translation", "journal": "", "year": "2019", "authors": "Aurko Roy; David Grangier"}, {"ref_id": "b54", "title": "A graph-to-sequence model for amrto-text generation", "journal": "", "year": "2018", "authors": "Linfeng Song; Yue Zhang; Zhiguo Wang; Daniel Gildea"}, {"ref_id": "b55", "title": "AESOP: paraphrase generation with adaptive syntactic control", "journal": "", "year": "2021", "authors": "Jiao Sun; Xuezhe Ma; Nanyun Peng"}, {"ref_id": "b56", "title": "Hypogen: Hyperbole generation with commonsense and counterfactual knowledge", "journal": "", "year": "2021", "authors": "Yufei Tian; Arvind Krishna Sridhar; Nanyun Peng"}, {"ref_id": "b57", "title": "Neural semantic parsing by character-based translation: Experiments with abstract meaning representations", "journal": "", "year": "2017", "authors": "Rik Van Noord; Johan Bos"}, {"ref_id": "b58", "title": "A transition-based algorithm for unrestricted AMR parsing", "journal": "", "year": "2018", "authors": "David Vilares; Carlos G\u00f3mez-Rodr\u00edguez"}, {"ref_id": "b59", "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding", "journal": "", "year": "2019", "authors": "Alex Wang; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel R Bowman"}, {"ref_id": "b60", "title": "A transition-based algorithm for AMR parsing", "journal": "", "year": "2015", "authors": "Chuan Wang; Nianwen Xue; Sameer Pradhan"}, {"ref_id": "b61", "title": "Amr-to-text generation with graph transformer", "journal": "Trans. Assoc. Comput. Linguistics", "year": "2020", "authors": "Tianming Wang; Xiaojun Wan; Hanqi Jin"}, {"ref_id": "b62", "title": "Paranmt-50m: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations", "journal": "", "year": "2018", "authors": "John Wieting; Kevin Gimpel"}, {"ref_id": "b63", "title": "Transformers: State-of-the-art natural language processing", "journal": "", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander M Lhoest;  Rush"}, {"ref_id": "b64", "title": "Docchat: An information retrieval approach for chatbot engines using unstructured documents", "journal": "", "year": "2016", "authors": "Zhao Yan; Nan Duan; Junwei Bao; Peng Chen; Ming Zhou; Zhoujun Li; Jianshe Zhou"}, {"ref_id": "b65", "title": "", "journal": "", "year": "", "authors": "Adams Wei Yu; David Dohan; Minh-Thang Luong; Rui Zhao; Kai Chen; Mohammad Norouzi; V Quoc"}, {"ref_id": "b66", "title": "Qanet: Combining local convolution with global self-attention for reading comprehension", "journal": "", "year": "2018", "authors": "Le "}, {"ref_id": "b67", "title": "AMR parsing as sequence-to-graph transduction", "journal": "", "year": "2019", "authors": "Sheng Zhang; Xutai Ma; Kevin Duh; Benjamin Van Durme"}, {"ref_id": "b68", "title": "Broad-coverage semantic parsing as transduction", "journal": "EMNLP-IJCNLP", "year": "2019", "authors": "Sheng Zhang; Xutai Ma; Kevin Duh; Benjamin Van Durme"}, {"ref_id": "b69", "title": "Syntax-infused variational autoencoder for text generation", "journal": "", "year": "2019", "authors": "Xinyuan Zhang; Yi Yang; Siyang Yuan; Dinghan Shen; Lawrence Carin"}, {"ref_id": "b70", "title": "Bridging the structural gap between encoding and decoding for data-to-text generation", "journal": "", "year": "2020", "authors": "Chao Zhao; Marilyn Walker; Snigdha Chaturvedi"}, {"ref_id": "b71", "title": "AMR parsing with latent structural information", "journal": "", "year": "2020", "authors": "Qiji Zhou; Yue Zhang; Donghong Ji; Hao Tang"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Iknow for them to approve this price, they'll need statistical documentation.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure1: The overall framework to construct PARAAMR based on AMR back-translation. We encode a source sentence to an AMR graph, modify the focus of the AMR graph, linearize the modified AMR graph, and finally decode the linearized graph to a syntactically diverse paraphrase.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "to generate the full parse trees from the following three parse templates: (ROOT(S(NP)(VP)(.))), (ROOT(S(VP)(.))), and (ROOT(NP(NP)(.))). Then we use the generated full parse trees as the target parse for the syntactically controlled paraphrase generator. Finally, we train a classifier with the original 30 training instances and the augmented examples.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "you describe the limitations of your work? Please see the \"Limitations\" section. A2. Did you discuss any potential risks of your work? Please see the \"Broader Impacts\" section. A3. Do the abstract and introduction summarize the paper's main claims? Section 1 A4. Have you used AI writing assistants when working on this paper? Left blank. B Did you use or create scientific artifacts? Section 3, 4, and 5 B1. Did you cite the creators of artifacts you used? Section 3, 4, and 5 B2. Did you discuss the license or terms for use and / or distribution of any artifacts? Section 3, 4, and 5", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "", "figure_data": "lists the statistics of the PARANMT,PARABANK1, PARABANK2, and PARAAMR.PARAAMR contains syntactically diverse para-phrases to around 15 million source sentences. No-tice that we consider the same source sentencesas PARABANK2; however, some of the sentencesfail to be parsed into ARM graphs. Therefore, thesize of PARAAMR is slightly smaller than PARA-BANK2. The average length of paraphrases inPARAAMR is 15.20, which is similar to PARA-BANK2. Each source sentence in PARAAMR has"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Basic statistics of PARANMT, PARABANK1, PARABANK2, and PARAAMR.", "figure_data": "DatasetSemantic SimilarityLexical DiversitySyntactic Diversity"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Paraphrase diversity of different datasets. PARAAMR is syntactically more diverse than other datasets, while also showing comparable semantic similarity.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Paraphrases generated by different datasets. The generated paraphrases by PARANMT, PARABANK1, and PARABANK2 usually have similar syntactic structures to the source sentences. In contrast, PARAAMR generates more syntactically diverse paraphrases.", "figure_data": "DatasetsSemantic Similarity 3(%) 2(%) 1(%) Average 3(%) 2(%) 1(%) Average Syntactic DiversityPARANMT (Wieting and Gimpel, 2018) 28.7 46.7 24.6 PARABANK1 (Hu et al., 2019a) 26.8 49.0 24.22.04 2.0316.7 45.0 38.3 15.1 47.8 37.11.78 1.78PARABANK2 (Hu et al., 2019b) PARAAMR (Ours)26.8 50.3 22.9 26.5 47.2 26.32.04 2.0014.2 51.8 34.0 18.2 53.8 28.01.80 1.90"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Human evaluation results. We evaluate semantic similarity and syntactic diversity in a score of three and report the distribution and the average score.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Results of learning sentence embeddings. We report 5-run average scores for STS 2012 to 2016. PARAAMR achieves the best performance.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": ", which uses LSTM as \u00b1 0.39 45.24 \u00b1 0.61 39.45 \u00b1 0.50 PARABANK1 46.21 \u00b1 0.26 44.52 \u00b1 0.18 39.85 \u00b1 0.11 PARABANK2 46.86 \u00b1 0.45 45.17 \u00b1 0.39 40.20 \u00b1 0.56 PARAAMR (ours) 48.50 \u00b1 0.11 47.38 \u00b1 0.19 40.30 \u00b1 0.10", "figure_data": "DatasetQuoraMRPCPANPARANMT47.38"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Results of syntactically controlled paraphrase generation. We report 5-run average BLEU scores for Quora, MRPC, and PAN. PARAAMR performs the best.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "", "figure_data": ": PARAAMR has better performance of few-shot learning with data augmentation."}], "formulas": [{"formula_id": "formula_0", "formula_text": "(\u2191) 1 -BLEU (\u2191) 1 -\u2229/\u222a (\u2191) TED-3 (\u2191) TED-F (\u2191)", "formula_coordinates": [5.0, 290.06, 191.67, 227.64, 18.43]}], "doi": ""}