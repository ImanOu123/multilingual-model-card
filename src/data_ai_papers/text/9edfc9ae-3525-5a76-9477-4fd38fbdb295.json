{"title": "CodeSLAM -Learning a Compact, Optimisable Representation for Dense Visual SLAM", "authors": "Michael Bloesch; Jan Czarnowski; Ronald Clark; Stefan Leutenegger; Andrew J Davison", "pub_date": "2019-04-14", "abstract": "The representation of geometry in real-time 3D perception systems continues to be a critical research issue. Dense maps capture complete surface shape and can be augmented with semantic labels, but their high dimensionality makes them computationally costly to store and process, and unsuitable for rigorous probabilistic inference. Sparse feature-based representations avoid these problems, but capture only partial scene information and are mainly useful for localisation only. We present a new compact but dense representation of scene geometry which is conditioned on the intensity data from a single image and generated from a code consisting of a small number of parameters. We are inspired by work both on learned depth from images, and auto-encoders. Our approach is suitable for use in a keyframe-based monocular dense SLAM system: While each keyframe with a code can produce a depth map, the code can be optimised efficiently jointly with pose variables and together with the codes of overlapping keyframes to attain global consistency. Conditioning the depth map on the image allows the code to only represent aspects of the local geometry which cannot directly be predicted from the image. We explain how to learn our code representation, and demonstrate its advantageous properties in monocular SLAM.", "sections": [{"heading": "Introduction", "text": "The underlying representation of scene geometry is a crucial element of any localisation and mapping algorithm. Not only does it influence the type of geometric qualities that can be mapped, but also dictates what algorithms can be applied. In SLAM in general, but especially in monocular vision, where scene geometry cannot be retrieved from a single view, the representation of geometrical uncertainties is essential. However, uncertainty propagation quickly becomes intractable for large degrees of freedom. This difficulty has split mainstream SLAM approaches into two cat-Figure 1. Two view reconstruction on selected frames from the EuRoC dataset. The proposed compact representation of 3D geometry enables joint optimisation of the scene structure and relative camera motion without explicit priors and in near real-time performance.\negories: sparse SLAM [6,17,21] which represents geometry by a sparse set of features and thereby allows joint probabilistic inference of structure and motion (which is a key pillar of probabilistic SLAM [7]) and dense or semi-dense SLAM [22,10] that attempts to retrieve a more complete description of the environment at the cost of approximations to the inference methods (often discarding cross-correlation of the estimated quantities and relying on alternating optimisation of pose and map [23,9]).\nHowever, the conclusion that a dense representation of the environment requires a large number of parameters is not necessarily correct. The geometry of natural scenes is not a random collection of occupied and unoccupied space but exhibits a high degree of order. In a depth map, the values of neighbouring pixels are highly correlated and can often be accurately represented by well known geometric smoothness primitives. But more strongly, if a higher level of understanding is available, a scene could be decomposed into a set of semantic objects (e.g. a chair) together with some internal parameters (e.g. size of chair, number of legs) and a pose, following a direction indicated by the SLAM++ system [27] towards representation with very few parameters. Other more general scene elements which exhibit simple regularity such as planes can be recognised and efficiently parametrised within SLAM systems (e.g. [26,12]). However, such human-designed dense abstractions are limited in the fraction of natural, cluttered scenes which they can represent.\nIn this work we aim at a more generic compact representation of dense scene geometry by training an auto-encoder on depth images. While a straightforward auto-encoder might over-simplify the reconstruction of natural scenes, our key novelty is to condition the training on intensity images. Our approach is planned to fit within the common and highly scalable keyframe-based SLAM paradigm [17,10], where a scene map consists of a set of selected and estimated historical camera poses together with the corresponding captured images and supplementary local information such as depth estimates. The intensity images are usually required for additional tasks, such as descriptor matching for place recognition or visualisation, and are thus readily available for supporting the depth encoding.\nThe depth map estimate for a keyframe thus becomes a function of the corresponding intensity image and an unknown compact representation (henceforth referred to as 'code'). This allows for a compact representation of depth without sacrificing reconstruction detail. In inference algorithms the code can be used as dense representation of the geometry and, due to its limited size, this allows for full joint estimation of both camera poses and dense depth maps for multiple overlapping keyframes. We might think of the image providing local details and the code as supplying more global shape parameters which are often not predicted well by 'depth from single image' learning. Importantly though, these global shape parameters are not a designed geometric warp but have a learned space which tends to relate to semantic entities in the scene, and could be seen as a step towards enabling optimisation in general semantic space.\nOur work comes at a time when many authors are combining techniques from deep learning with estimation-based SLAM frameworks, and there is an enormously fertile field of possibilities for this. Some particularly eye-catching pieces of work over the past year have focused on supervised and self-supervised training of surprisingly capable networks which are able to estimate visual odometry, depth and other quantities from video [11,30,3,31,5,34,33]. These methods run with pure feed forward network operation at runtime, but rely on geometric and photometric formulation and understanding at training time to correctly for-mulate the loss functions which connect different network components. Other systems are looking towards making consistent long-term maps and for instance combine learned normal predictions with photometric constraints at test time [32]. Such systems are able to refine geometric estimates, and this is the domain in which we are particularly interested here. In CNN-SLAM [29] single image depth prediction and dense alignment are used to produce a dense 3D map and this gives a promising result, but it is not possible to optimise the predicted depth maps further for consistency when multiple keyframes overlap as it is in our approach.\nTo summarise, the two key contributions of our paper are:\n\u2022 The derivation of a compact and optimisable representation of dense geometry by conditioning a depth autoencoder on intensity images.\n\u2022 The implementation of the first real-time targeted monocular system that achieves such a tight joint optimisation of motion and dense geometry.\nIn the rest of this paper, we will first explain our method for depth learning and prediction, and then show the applicability of this approach in a SLAM setting.", "publication_ref": ["b5", "b16", "b20", "b6", "b21", "b9", "b22", "b8", "b26", "b25", "b11", "b16", "b9", "b10", "b29", "b2", "b30", "b4", "b33", "b32", "b31", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Intensity Conditioned Depth Auto-Encoding", "text": "Two important qualities of geometry representations are accuracy and practicality. While the accuracy of a representation simply relates to its ability to reproduce the geometry, the practicality describes how well the representation can be used in an overall system. For inference-based SLAM systems, the latter typically requires the representation to lead to an optimisable loss function. For a representation G of the geometry a loss function L(G) should be differentiable and have a clear minimum. Additionally, the size of the representation G should be limited in order to allow the estimation of second-order statistical moments (a covariance matrix) as part of more powerful inference methods.\nIn order to come up with a compact representation of the scene geometry we explore auto-encoder-like network architectures. Auto-encoders are networks which attempt to learn an identity mapping while being subject to an information bottleneck which forces the network to find a compact representation of the data [25]. In a naive attempt to autoencode depth this would lead to very blurry depth reconstruction since only the major traits of the depth image can make it through the bottleneck (see Figure 2). In a monocular vision setup, however, we have access to the intensity images, which we are very likely to store alongside every keyframe. This can be leveraged to make the encoding more efficient: We do not need to encode the full depth information, but only need to retain the part of the information (\n)1\nThe above equation also highlights the relation to depthfrom-mono architectures [8,18,11,34] which solve a codeless version of the problem, D = D(I). Essentially, the employed architecture is a combination of the depth-frommono-architecture of Zhou et al. [34] and a variational autoencoder for depth. We have chosen a variational autoencoder network [16] in order to increase the smoothness of the mapping between code and depth: small changes in the code should lead to small changes in the depth. While the practicality of our representation is thus addressed by the smoothness and the limited code size, the accuracy is maximised by training for the reconstruction error.", "publication_ref": ["b24", "b7", "b17", "b10", "b33", "b33", "b15"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Detailed Network Architecture", "text": "An overview of the network architecture is provided in Figure 3. The top part illustrates the U-Net [24] applied on the intensity image, which first computes an increasingly coarse but high-dimensional feature representation of the input image. This is followed by an up-sampling part with skip-layers. The computed intensity features are then used to encode and decode the depth in the lower part of the figure. This part is a fairly standard variational auto-encoder architecture with again a down-sampling part and an upsampling part. Embedded in the middle are two fully connected layers as well as the variational part, which samples the code from a Gaussian distribution and is subject to a regularisation cost (KL-divergence, see [16]). The conditioning of the auto-encoder is achieved by simply concatenating the intensity features of the corresponding resolution.\nInstead of predicting just raw depth values, we predict a mean \u00b5 and an uncertainty b for every depth pixel. The uncertainty is predicted from intensity only and thus is not directly influenced by the code. Subsequently, we derive a cost term by evaluating the negative log-likelihood of the observed depthd. This allows the network to attenuate the cost of difficult regions and to focus on reconstructing parts which can be well explained. At test time, the learned uncertainties can also serve to gauge the reliability of the reconstruction. In the present work we employ a Laplace distribution which has heavier tails than the traditional Gaussian distribution:\np(d|\u00b5, b)) = 1 2b exp \u2212 |d \u2212 \u00b5| b .(2)\nDiscarding a constant offset, the negative log-likelihood thus becomes:\n\u2212 log(p(d|\u00b5, b)) = |d \u2212 \u00b5| b + log(b) .(3)\nIntuitively, the network will tune the pixel-wise uncertainty b such that it best attenuates the reconstruction error |d \u2212 \u00b5| while being subject to a regularisation term log(b). Using likelihoods as cost terms is a well-established method and has previously been applied to deep learning problems in computer vision [13,4].\nIn analogy to previous work, we evaluate the error at multiple resolutions [34]. To this end, we create a depth image pyramid with four levels and derive the negative loglikelihood for every pixel at every level. We increase the weight on every level by a factor of 4 in order to account for  . These features are then fed into the depth autoencoder by concatenating them after the corresponding convolutions (denoted by arrows). Down-sampling is achieved by varying stride of the convolutions, while up-sampling uses bilinear interpolation (except for the last layer which uses a deconvolution). A variational component in the bottleneck of the depth auto-encoder is composed of two fully connected layers (512 output channels each) followed by the computation of the mean and variance, from which the latent space is then sampled. The network outputs the predicted mean \u00b5 and uncertainty b of the depth on four pyramid levels.\nthe lower pixel count. Except for the computation of the latent distribution and the output channels, the activations are all set to ReLu. Furthermore, for allowing pre-computation of the Jacobians (see Section 4.1), we explore identity activations for the depth decoder. However, in order to retain an influence from image to code-Jacobian, we add the element-wise multiplication of every concatenation to the concatenation itself. I.e., we increment every concatenation [L1, L2] of layers L1 and L2 to [L1, L2, L1 L2].", "publication_ref": ["b23", "b15", "b12", "b3", "b33"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Training Setup", "text": "The depth values of the dataset are transformed to the range [0, 1]. We do this by employing a hybrid depth parametrisation which we call proximity:\np = a d + a .(4)\nGiven an average depth value a, it maps the depth in [0, a] to [0.5, 1.0] (similar to regular depth) and maps the depths in [a, \u221e] to [0, 0.5] (similar to inverse depth). This parametrisation is differentiable and better relates to the actual observable quantity (see inverse depth parametrisation [20]).\nThe network is trained on the SceneNet RGB-D dataset [19] which is composed of photorealistic renderings of randomised indoor scenes. It provides colour and depth images as well as semantic labeling and poses, out of which we only make use of the two former ones. We make use of the ADAM optimiser [15] with an initial learning rate of 10 \u22124 . We train the network for 6 epochs while reducing the learning-rate to 10 \u22126 .", "publication_ref": ["b19", "b18", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Dense Warping", "text": "Due to the latent cost of the variational auto-encoder, the zero code can be used to obtain a likely single view depth prediction D(I, 0) (see Figure 6). However, if overlapping views are available we can leverage stereopsis to refine the depth estimates. This can be done by computing dense correspondences between the views: Given the image I A and the estimated code c A of a view A, as well as the relative transformation\nT B A = (R B A , B t B A ) \u2208 SO(3) \u00d7 R 3\nto a view B, we compute the correspondence for every pixel u with:\nw(u, c A , T B A ) = \u03c0(R B A \u03c0 \u22121 (u, D A [u]) + B t B A ) , (5\n)\nwhere \u03c0 and \u03c0 \u22121 are the projection and inverse projection operators. We use the shortcut D A = D(I A , c A ) and use square brackets to denote pixel lookup. If applied to intensity images we can for instance derive the following photometric error:\nI A [u] \u2212 I B [w(u, c A , T B A )] .(6)\nThe above expressions are differentiable w.r.t. to their inputs and we can compute the corresponding Jacobians using the chain rule:\n\u2202I B [v] \u2202 B t B A = \u2202I B [v] \u2202v \u2202\u03c0(x) \u2202x ,(7)\n\u2202I B [v] \u2202R B A = \u2202I B [v] \u2202v \u2202\u03c0(x) \u2202x (\u2212R B A \u03c0 \u22121 (u, d)) \u00d7 ,(8)\n\u2202I B [v] \u2202c a = \u2202I B [v] \u2202v \u2202\u03c0(x) \u2202x R B A \u2202\u03c0 \u22121 (u, d) \u2202d \u2202D A [u] \u2202c A ,(9)\nwhere \u00d7 refers to the skew symmetric matrix of a 3D vector and with the abbreviations:\nv = w(u, c A , T B A ) ,(10)\nx = R B A \u03c0 \u22121 (u, D A [u]) + B t B A ,(11)\nd = D(I A , c A )[u].(12)\nMost partial derivatives involved in Equations ( 7) to ( 9) are relatively well-known from dense tracking literature [14] and include the image gradient (\u2202I B [v]/\u2202v), the differential of the projection (\u2202\u03c0(x)/\u2202x), as well as transformation related derivatives (also refer to [1] for more details). The last term in Equation ( 9), \u2202D A [u]/\u2202c A , is the derivative of the depth w.r.t. the code. Since it involves many convolutions, it is computationally costly to evaluate (up to 1 sec depending on the size of the network). In case of a linear decoder this term can be pre-computed which significantly accelerates the evaluation of the Jacobians.", "publication_ref": ["b13", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Inference Framework", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "N-Frame Structure from Motion (Mapping)", "text": "The proposed depth parametrisation is used to construct a dense N -frame Structure from Motion (SfM) framework (see Figure 4). We do this by assigning an unknown code and an unknown pose to every frame. All codes and poses are initialised to zero and identity, respectively. For two frames A and B with overlapping field of view we then derive photometric and geometric residuals, E pho and E geo , as follows:\nE pho = L p I A [u] \u2212 I B [w(u, c A , T B A )] ,(13)\nE geo = L g D A [u] \u2212 D B [w(u, c A , T B A )] .(14)\nThe loss functions L pho and L geo have the following masking and weighting functionality: (i) mask invalid correspondences, (ii) apply relative weighting to geometric and photometric errors, (iii) apply a Huber weighting, (iv) downweight errors on strongly slanted surfaces, and (v) downweight pixels which might be occluded (only L pho ).\nIn order to optimise both sets of residuals w.r.t. our motion and geometry we compute the Jacobians w.r.t. all codes and poses according to Section 3. As mentioned above, we investigate the applicability of linear decoding networks Given estimated poses T i, we derive relative error terms between the frames (photometric and geometric). We then jointly optimise for geometry (ci) and motion (T i) by using a standard second-order method.\n(see Section 2.1) as this allows us to compute the Jacobian of the decoder D(I, c) w.r.t. the code c only once per keyframe. After computing all residuals and Jacobians we apply a damped Gauss-Newton algorithm in order to find the optimal codes and poses of all frames.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Tracking (Localisation)", "text": "The tracking system, responsible for estimating the pose of keyframes with respect to an existing keyframe map, can be built much in the spirit of the above SfM approach. The current frame is paired with the last keyframe and the estimated relative pose results from a cost-minimisation problem. In our vision-only setup we do not have access to the current depth image (except for a rough guess), and thus in contrast to the described SfM system we do not integrate a geometric cost.\nIn order to increase tracking robustness we perform a coarse to fine optimisation by first doing the dense alignment on the low depth image resolutions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SLAM System", "text": "We implement a preliminary system for Simultaneous Localisation and Mapping inspired by PTAM [17] where we alternate between tracking and mapping. The initialisation procedure takes two images and jointly optimises for their relative pose and the codes of each frame. After that we can track the current camera pose w.r.t. the last keyframe. Once a certain baseline is achieved we add a keyframe to the map and perform a global optimisation, before continuing with the tracking. If the maximum number of keyframes is reached we marginalise old keyframes and thereby obtain a linear prior on the remaining keyframes. In a 4-keyframes setup, we achieve a map update rate of 5 Hz, which if we do not have to add keyframes too frequently is enough for real-time performance. The system currently relies on Ten- sorflow for image warping, and could be sped up with a more targeted warping and optimisation system which are both part of future work.", "publication_ref": ["b16"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Evaluation and Discussion", "text": "Please also see our submitted video which includes demonstrations of our results and system http:// www.imperial.ac.uk/dyson-robotics-lab/ projects/codeslam/.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Image Conditioned Depth Encoding", "text": "First we present results and insights related to our key concept of encoding depth maps conditioned on intensity images.\nWe trained and compared multiple variations of our network. Our reference network has a code size of 128, employs greyscale image information only, and makes use of a linear decoder network in order to speed up Jacobian computation. Figure 5 shows results on reconstruction accuracy using different code sizes as well as setups with RGB information and nonlinear depth decoding. The use of colour or nonlinear decoding did not significantly affect the accuracy. With regard to code size, we observe a saturation of the accuracy at a code size of 128; there is little to be gained from making the code bigger. This value may be surprisingly low, but the size seems to be large enough to transmit the information that can be captured in the code by the proposed network architecture. Reconstr. Groundtr. conditioned depth encoding works. In Figure 6 we show how we encode a depth image into a code of size 128. Using the corresponding intensity image this can then be decoded into a reconstructed depth image, which captures all of the main scene elements well. We also show the reconstruction when passing a zero code to the decoder as well as with a code that is optimised for minimal reconstruction error. The zero code captures some of the geometrical details but fails to properly reconstruct the entire scene. The reconstruction with the optimised code is very similar to the one with the code from the encoder which indicates that the encoder part of the network works well. The associated depth uncertainty is also visualised and exhibits higher magnitudes   1. RMS of pixel proximity estimation error with different amounts of master keyframe-frame pairs in the optimisation problem. The error is evaluated between the master keyframe proximity and its corresponding ground truth proximity. Frames 1-3: downward-backwards motion. Frames 4-6: left-forward motion.\nin the vicinity of depth discontinuities and around shiny image regions (but not necessarily around high image gradients in general). Further examples of depth encoding are shown in Figure 7.\nIn Figure 8 we visualise the Jacobians of the depth image w.r.t. to the code entries. An interesting observation is that the single code entries seem to correspond to specific image regions and, to some extent, respect boundaries given by the intensity image. While the regions seem to be slightly fragmented, the final reconstructions will always be a linear combination of the effect of all code entries. We also compare the regions of influence for two different but similar images and can observe a certain degree of consistency.", "publication_ref": [], "figure_ref": ["fig_4", "fig_6"], "table_ref": []}, {"heading": "Structure from Motion", "text": "The proposed low dimensional encoding enables continuous refinement of the depth estimates as more overlapping keyframes are integrated. In order to test this, we have implemented an SfM system which incrementally pairs one pre-selected frame with all the remaining frames (which were selected from SceneNet RGB-D). Table 1 shows the obtained reconstruction error w.r.t. the number of frames that are connected to the first frame. The observed reduction of the reconstruction error well illustrates the strength of the employed probabilistic inference method, application of which is enabled by the low dimensionality of the optimisation space. The magnitude of depth refinement depends on the information content of the new frames (whether they present the scene under a new view and exhibit sufficient baseline). Figure 9 presents a 3D reconstruction based on 9 frames for the scene used in the above error computations. Since in this rendering all the frame depth maps are super-imposed, one can observe the quality of the alignment. In a future full SLAM system, these keyframes would be fused together in order to form a single global scene. Before visualisation, high frequency elements are removed from the depth maps with bilateral filtering and highly slanted mesh elements are cropped.\nBeing exposed to a large variety of depth images during training, the proposed network embeds geometry priors in its weights. These learned priors seem to generalise to real scenes as well: Figure 1 depicts a two-frame reconstruction with images from the real image EuRoC dataset [2] taken by a drone in an industrial setting. The result corresponds to 50 optimisation steps, each taking around 100 ms to complete. Since significant exposure changes occur between the images, we perform an affine illumination correction of the frames. The validation of the two-frame reconstruction performance is of high importance as it is directly connected to the initialisation procedure of the full SLAM system. In order to further highlight its effectiveness we include results on a selection of pairs taken from the NYU V2 dataset [28] (Figure 10).  ", "publication_ref": ["b1", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "SLAM System", "text": "In contrast to most dense approaches, our low dimensional geometry encoding allows joint optimisation of motion and geometry. Furthermore, due to the inherent prior contained in the encoding, the framework is able to deal with rotational motions only. The system is tested in a sliding window visual odometry mode on the EuRoC dataset on trajectory MH 02 easy. Even though the dataset is significantly different from the data the network is trained on (with many metallic parts and many reflections), the proposed system is able to run through most of this arguably very difficult dataset (we do not use the available IMU data).\nFigure 11 shows the error against traveled distance. While this cannot compete with a state-of-the art visualinertial system, it performs respectably for a vision only-system and exhibits an error of roughly 1 m for a traveled distance of 9 m. In Figure 12 the first and last key-frame of our 4-frame sliding window system are illustrated. This shows the intensity image of the encountered scene together with the estimated proximity image and a normal based shading. Considering that the network was trained on artificial images only which were very different in their nature, the reconstructed depth is sensible and allows for reliable camera tracking.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Conclusions", "text": "We have shown that a learned representation for depth which is conditioned on image data provides an important advance towards future SLAM systems. By employing an auto-encoder like training setup, the proposed representation can contain generic and detailed dense scene information while allowing efficient probabilistic joint optimisation together with camera poses.\nIn near future work, we will use the components demonstrated here to build a full real-time keyframe-based SLAM system. Learned visual motion estimation methods could surely be brought in here as priors for robust tracking. In addition to that, the training of the network should be extended in order to include real data as well. This could be done by using an RGB-D dataset, but might also be achieved with intensity information only in an self-supervised manner, based on photometric error as loss.\nIn the longer term, we would like to move beyond a keyframe-based approach, where our dense geometry representations are tied to single images, and work on learned but optimisable compact representations for general 3D geometry, eventually tying our work up with 3D object recognition.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "Research presented in this paper has been supported by Dyson Technology Ltd.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A Primer on the Differential Calculus of 3D Orientations", "journal": "CoRR", "year": "2016", "authors": "M Bloesch; H Sommer; T Laidlow; M Burri; G N\u00fctzi; P Fankhauser; D Bellicoso; C Gehring; S Leutenegger; M Hutter; R Siegwart"}, {"ref_id": "b1", "title": "The Eu-RoC Micro Aerial Vehicle Datasets", "journal": "International Journal of Robotics Research (IJRR)", "year": "2007", "authors": "M Burri; J Nikolic; P Gohl; T Schneider; J Rehder; S Omari; M W Achtelik; R Siegwart"}, {"ref_id": "b2", "title": "Multi-modal Auto-Encoders as Joint Estimators for Robotics Scene Understanding", "journal": "", "year": "2016", "authors": "C Cadena; A Dick; I D Reid"}, {"ref_id": "b3", "title": "VidLoc: A deep spatio-temporal model for 6-dof video-clip relocalization", "journal": "", "year": "2017", "authors": "R Clark; S Wang; H Wen; A Markham; N Trigoni"}, {"ref_id": "b4", "title": "VINet: Visual-inertial odometry as a sequence-to-sequence learning problem", "journal": "", "year": "2017", "authors": "R Clark; S Wang; H Wen; A Markham; N Trigoni"}, {"ref_id": "b5", "title": "Real-Time Simultaneous Localisation and Mapping with a Single Camera", "journal": "", "year": "2003", "authors": "A J Davison"}, {"ref_id": "b6", "title": "Simultaneous Localisation and Mapping (SLAM): Part I The Essential Algorithms", "journal": "IEEE Robotics and Automation Magazine", "year": "2006", "authors": "H Durrant-Whyte; T Bailey"}, {"ref_id": "b7", "title": "Depth Map Prediction from a Single Image using a Multi-Scale Deep Network", "journal": "", "year": "2014", "authors": "D Eigen; C Puhrsch; R Fergus"}, {"ref_id": "b8", "title": "Direct sparse odometry", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "", "authors": "J Engel; V Koltun; D Cremers"}, {"ref_id": "b9", "title": "LSD-SLAM: Largescale direct monocular SLAM", "journal": "", "year": "2002", "authors": "J Engel; T Schoeps; D Cremers"}, {"ref_id": "b10", "title": "Unsupervised CNN for single view depth estimation: Geometry to the rescue", "journal": "", "year": "2016", "authors": "R Garg; V K B G ; G Carneiro; I Reid"}, {"ref_id": "b11", "title": "Simultaneous localization and mapping with infinite planes", "journal": "", "year": "2015", "authors": "M Kaess"}, {"ref_id": "b12", "title": "What uncertainties do we need in bayesian deep learning for computer vision?", "journal": "", "year": "2017", "authors": "A Kendall; Y Gal"}, {"ref_id": "b13", "title": "Robust odometry estimation for RGB-D cameras", "journal": "", "year": "2013", "authors": "C Kerl; J Sturm; D Cremers"}, {"ref_id": "b14", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015", "authors": "D P Kingma; J Ba"}, {"ref_id": "b15", "title": "Auto-Encoding Variational Bayes", "journal": "", "year": "2014", "authors": "D P Kingma; M Welling"}, {"ref_id": "b16", "title": "Parallel Tracking and Mapping for Small AR Workspaces", "journal": "", "year": "2005", "authors": "G Klein; D W Murray"}, {"ref_id": "b17", "title": "Deep Convolutional Neural Fields for Depth Estimation from a Single Image", "journal": "", "year": "2015", "authors": "F Liu; C Shen; G Lin"}, {"ref_id": "b18", "title": "SceneNet RGB-D: Can 5M synthetic images beat generic ImageNet pre-training on indoor segmentation?", "journal": "", "year": "2017", "authors": "J Mccormac; A Handa; S Leutenegger; A J Davison"}, {"ref_id": "b19", "title": "Unified Inverse Depth Parametrization for Monocular SLAM", "journal": "", "year": "2006", "authors": "J M M Montiel; J Civera; A J Davison"}, {"ref_id": "b20", "title": "ORB-SLAM: a Versatile and Accurate Monocular SLAM System", "journal": "IEEE Transactions on Robotics", "year": "2015", "authors": "R Mur-Artal; J M M Montiel; J D Tard\u00f3s"}, {"ref_id": "b21", "title": "DTAM: Dense Tracking and Mapping in Real-Time", "journal": "", "year": "2011", "authors": "R A Newcombe; S Lovegrove; A J Davison"}, {"ref_id": "b22", "title": "Monocular visual odometry: Sparse joint optimisation or dense alternation?", "journal": "", "year": "2017", "authors": "L Platinsky; A J Davison; S Leutenegger"}, {"ref_id": "b23", "title": "U-Net: Convolutional networks for biomedical image segmentation", "journal": "", "year": "2015", "authors": "O Ronneberger; P Fischer; T Brox"}, {"ref_id": "b24", "title": "Learning internal representations by error propagation", "journal": "MIT Press", "year": "1986", "authors": "D E Rumelhart; G E Hinton; R J Williams"}, {"ref_id": "b25", "title": "Dense planar SLAM", "journal": "", "year": "2014", "authors": "R F Salas-Moreno; B Glocker; P H J Kelly; A J Davison"}, {"ref_id": "b26", "title": "SLAM++: Simultaneous Localisation and Mapping at the Level of Objects", "journal": "", "year": "2013", "authors": "R F Salas-Moreno; R A Newcombe; H Strasdat; P H J Kelly; A J Davison"}, {"ref_id": "b27", "title": "Indoor segmentation and support inference from RGBD images", "journal": "", "year": "2012", "authors": "N Silberman; D Hoiem; P Kohli; R Fergus"}, {"ref_id": "b28", "title": "CNN-SLAM: Real-time dense monocular slam with learned depth prediction", "journal": "", "year": "2017", "authors": "K Tateno; F Tombari; I Laina; N Navab"}, {"ref_id": "b29", "title": "DeMoN: Depth and motion network for learning monocular stereo", "journal": "", "year": "2016", "authors": "B Ummenhofer; H Zhou; J Uhrig; N Mayer; E Ilg; A Dosovitskiy; T Brox"}, {"ref_id": "b30", "title": "DeepVO: Towards end to end visual odometry with deep recurrent convolutional neural networks", "journal": "", "year": "2017", "authors": "S Wang; R Clark; H Wen; N Trigoni"}, {"ref_id": "b31", "title": "Dense monocular reconstruction using surface normals", "journal": "", "year": "2017", "authors": "C S Weerasekera; Y Latif; R Garg; I Reid"}, {"ref_id": "b32", "title": "GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose", "journal": "", "year": "2018", "authors": "Z Yin; J Shi"}, {"ref_id": "b33", "title": "Unsupervised learning of depth and ego-motion from video", "journal": "", "year": "2017", "authors": "T Zhou; M Brown; N Snavely; D G Lowe"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 .2Figure 2. Depth auto-encoder without the use of image intensity data. Due to the bottleneck of the auto-encoder only major traits of the depth image can be captured.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 .3Figure3. Network architecture of the variational depth autoencoder conditioned on image intensities. We use a U-Net to decompose the intensity image into convolutional features (the upper part of the figure). These features are then fed into the depth autoencoder by concatenating them after the corresponding convolutions (denoted by arrows). Down-sampling is achieved by varying stride of the convolutions, while up-sampling uses bilinear interpolation (except for the last layer which uses a deconvolution). A variational component in the bottleneck of the depth auto-encoder is composed of two fully connected layers (512 output channels each) followed by the computation of the mean and variance, from which the latent space is then sampled. The network outputs the predicted mean \u00b5 and uncertainty b of the depth on four pyramid levels.", "figure_data": ""}, {"figure_label": "34", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "3 Figure 4 .34Figure 4. Illustration of the SfM system. The image Ii and corresponding code ci in each frame are used to estimate the depth Di.Given estimated poses T i, we derive relative error terms between the frames (photometric and geometric). We then jointly optimise for geometry (ci) and motion (T i) by using a standard second-order method.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 .5Figure 5. Validation loss during training on the per-pixel proximity errors. As the reference implementation, we use a network trained on greyscale images with a linear decoder. Lower losses can be achieved by increasing the code size (increasing shades of grey). Using a nonlinear decoder or colour images during training does not affect the results in a significant way.", "figure_data": ""}, {"figure_label": "86", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figures 6 to 8 Figure 6 .86Figure 6. An example image passed through encoding and decoding. Top left: input image. Top right: ground truth depth. Middle left: zero code reconstruction (image only prediction). Middle right: decoded depth (code from encoder). Bottom left: estimated reconstruction uncertainty (scaled four times for visibility). Bottom right: optimised depth (code minimising reconstruction error).", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 7 .7Figure 7. Encodings of different depth images. The encoding allows to capture even fine geometrical details.", "figure_data": ""}, {"figure_label": "18", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "1 Figure 8 .18Figure 8. Visualisation of the influence of the code on depth reconstruction. The Jacobian of the depth w.r.t. a specific code entry is used to colourise the input image (blue and red depict negative and positive values, respectively). Columns represent code entries (1-3). Rows represent two different input images.", "figure_data": ""}, {"figure_label": "910", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 9 .Figure 10 .910Figure 9. Monocular 3D reconstruction using 9 keyframes. During optimisation a selected master keyframe is paired with the other frames. The depth images of all frames are used for the 3D rendering. The employed geometric error term ensures the consistency between the depth of the different views.", "figure_data": ""}, {"figure_label": "11112", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 11 . 1 Figure 12 .11112Figure 11. Translation error versus traveled distance on the EuRoC dataset MH02. Despite training the auto-encoder on SceneNet RGB-D, its decoder generalises to other datasets (after correcting for camera intrinsics).", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": ")1", "formula_coordinates": [3.0, 278.62, 276.17, 7.74, 8.64]}, {"formula_id": "formula_1", "formula_text": "p(d|\u00b5, b)) = 1 2b exp \u2212 |d \u2212 \u00b5| b .(2)", "formula_coordinates": [3.0, 354.74, 155.59, 190.38, 22.31]}, {"formula_id": "formula_2", "formula_text": "\u2212 log(p(d|\u00b5, b)) = |d \u2212 \u00b5| b + log(b) .(3)", "formula_coordinates": [3.0, 350.81, 222.63, 194.3, 22.31]}, {"formula_id": "formula_3", "formula_text": "p = a d + a .(4)", "formula_coordinates": [4.0, 143.95, 248.33, 142.41, 22.31]}, {"formula_id": "formula_4", "formula_text": "T B A = (R B A , B t B A ) \u2208 SO(3) \u00d7 R 3", "formula_coordinates": [4.0, 111.2, 547.17, 136.07, 12.99]}, {"formula_id": "formula_5", "formula_text": "w(u, c A , T B A ) = \u03c0(R B A \u03c0 \u22121 (u, D A [u]) + B t B A ) , (5", "formula_coordinates": [4.0, 62.66, 579.87, 219.83, 12.95]}, {"formula_id": "formula_6", "formula_text": ")", "formula_coordinates": [4.0, 282.49, 582.52, 3.87, 8.64]}, {"formula_id": "formula_7", "formula_text": "I A [u] \u2212 I B [w(u, c A , T B A )] .(6)", "formula_coordinates": [4.0, 110.95, 669.17, 175.42, 12.95]}, {"formula_id": "formula_8", "formula_text": "\u2202I B [v] \u2202 B t B A = \u2202I B [v] \u2202v \u2202\u03c0(x) \u2202x ,(7)", "formula_coordinates": [4.0, 317.93, 93.61, 227.18, 24.92]}, {"formula_id": "formula_9", "formula_text": "\u2202I B [v] \u2202R B A = \u2202I B [v] \u2202v \u2202\u03c0(x) \u2202x (\u2212R B A \u03c0 \u22121 (u, d)) \u00d7 ,(8)", "formula_coordinates": [4.0, 317.93, 121.83, 227.18, 25.4]}, {"formula_id": "formula_10", "formula_text": "\u2202I B [v] \u2202c a = \u2202I B [v] \u2202v \u2202\u03c0(x) \u2202x R B A \u2202\u03c0 \u22121 (u, d) \u2202d \u2202D A [u] \u2202c A ,(9)", "formula_coordinates": [4.0, 317.93, 149.6, 227.18, 24.8]}, {"formula_id": "formula_11", "formula_text": "v = w(u, c A , T B A ) ,(10)", "formula_coordinates": [4.0, 360.11, 216.47, 185.0, 12.95]}, {"formula_id": "formula_12", "formula_text": "x = R B A \u03c0 \u22121 (u, D A [u]) + B t B A ,(11)", "formula_coordinates": [4.0, 359.56, 233.18, 185.55, 12.95]}, {"formula_id": "formula_13", "formula_text": "d = D(I A , c A )[u].(12)", "formula_coordinates": [4.0, 360.94, 250.43, 184.17, 9.68]}, {"formula_id": "formula_14", "formula_text": "E pho = L p I A [u] \u2212 I B [w(u, c A , T B A )] ,(13)", "formula_coordinates": [4.0, 330.32, 555.74, 214.79, 12.95]}, {"formula_id": "formula_15", "formula_text": "E geo = L g D A [u] \u2212 D B [w(u, c A , T B A )] .(14)", "formula_coordinates": [4.0, 331.57, 572.45, 213.54, 12.95]}], "doi": ""}