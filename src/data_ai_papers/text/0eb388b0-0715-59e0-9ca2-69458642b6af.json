{"title": "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?", "authors": "Shuheng Liu; Alan Ritter", "pub_date": "", "abstract": "The CoNLL-2003 English named entity recognition (NER) dataset has been widely used to train and evaluate NER models for almost 20 years. However, it is unclear how well models that are trained on this 20-year-old data and developed over a period of decades using the same test set will perform when applied on modern data. In this paper, we evaluate the generalization of over 20 different models trained on CoNLL-2003, and  show that NER models have very different generalization. Surprisingly, we find no evidence of performance degradation in pre-trained Transformers, such as RoBERTa and T5, even when fine-tuned using decades-old data. We investigate why some models generalize well to new data while others do not, and attempt to disentangle the effects of temporal drift and overfitting due to test reuse. Our analysis suggests that most deterioration is due to temporal mismatch between the pre-training corpora and the downstream test sets. We found that four factors are important for good generalization: model architecture, number of parameters, time period of the pre-training corpus, in addition to the amount of fine-tuning data. We suggest current evaluation methods have, in some sense, underestimated progress on NER over the past 20 years, as NER models have not only improved on the original CoNLL-2003 test set, but improved even more on modern data. Our datasets can be found at https:// github.com/ShuhengL/acl2023_conllpp.", "sections": [{"heading": "Introduction", "text": "The progress of natural language processing (NLP) is typically measured using performance metrics like accuracy or F 1 score on public test sets. For instance, the top line in Figure 1 shows the steady improvement of selected models on the CoNLL-2003 English named entity recognition (NER) test set (Tjong Kim Sang and De Meulder, 2003) over the course of 15 years (2005)(2006)(2007)(2008)(2009)(2010)(2011)(2012)(2013)(2014)(2015)(2016)(2017)(2018)(2019)(2020) as measured by published F 1 scores.\nFigure 1: Progress on NER from 2005-2020, as measured using published F 1 scores on the CoNLL-2003CoNLL- (data from 1996 and CoNLL++ (data from 2020) English NER test set. The gap between the two grows smaller as time passes by, showing improved generalization of models developed over time.\nHowever, these scores are all calculated using the same publicly available test set, which raises several questions. One concern is how much of this progress is actually due to adaptive overfitting, i.e. over-estimating performance by reusing the same test set, as opposed to genuine improvement Gorman and Bedrick, 2019). In addition, there is also the issue of temporal drift as training data ages, which can negatively impact performance on modern data (Rijhwani and Preotiuc-Pietro, 2020;Agarwal and Nenkova, 2022;Luu et al., 2022).\nPerformance degradation is a significant concern in applications that use NER, such as text deidentification (Morris et al., 2022), relation extraction (Zhong and Chen, 2021), linking entities to a knowledge base (De Cao et al., 2022), etc. However, continuously annotating, training, and evaluating new models on new data is not always possible. NER models that are trained on decadesold data and evaluated on heavily-used public development and test sets may struggle to perform well on modern data, which highlights the need to consider these factors when assessing performance.  We report the publication time of the articles, the numbers of four different types of entities, the number of tokens, unique tokens and average number of tokens per sentence. 1 To understand how well NER works when models have been developed over 20 years using the same dataset, we created a new test set called CoNLL++. We closely modeled CoNLL++ after the CoNLL-2003 test set, using news articles from 2020 instead of 1996, as in the original dataset. We carefully controlled for other variables, making results on the two datasets as comparable as possible, with the exception of the time frame. An example of an annotated sentence from CoNLL++ is shown below: Similar to the findings of  on the ImageNet dataset (Deng et al., 2009), we do not observe evidence of widespread overfitting on CoNLL-2003. On average, each point of F 1 improvement on the CoNLL-2003 test set translates to a larger improvement on CoNLL++ (see Figure 2), suggesting overall improvements on the original dataset between 2003-2020 are mostly not due to overfitting. Rather, most performance deterioration appears to be caused by temporal misalignment (Luu et al., 2022). Suprisingly, for some models (e.g. RoBERTa and T5), we find no evidence of performance degradation at all, despite the fact they are fine-tuned on a 20-year-old public dataset. We conduct an extensive analysis, which suggests that model size, architecture, amount of fine-tuning data, and pretraining corpus are all important factors for generalization in NER.  Kim Sang and De Meulder (2003). We find that almost all articles were published between Dec. 5th and 7th, 1996, except one article published on Nov. 29th and another on Dec. 16th. Our dataset follows this distribution to collect Reuters news articles published between December 5th and 7th, 2020, collected from the Common Crawl Foundation. 2 We tokenize the data with the same tokenizer used for the CoNLL-2003 shared task, and randomly select articles to match the total number of tokens in the original test set. Annotation: We manually labeled this new dataset, which we refer to as CoNLL++, using the BRAT annotation interface (Stenetorp et al., 2012). Articles were distributed between two authors, where one author annotated 96.1% of the articles and the other annotated 50.0%. The first author's annotation is used as the gold standard. 3 During the annotation process, articles from the CoNLL-2003 test set were interleaved with new articles from 1 We notice that our dataset contains fewer entities than CoNLL-2003. This is mainly because there are a number of tabular data, with information such as results of sports events (e.g. 1. Jesper Ronnback ( Sweden ) 25.76 points), in CoNLL-2003. Such data greatly contribute to the number of entities. These tabular data also cause the average sentence length of CoNLL-2003 to be smaller than that of CoNLL++. By removing these data, we found that the average sentence length increased to 18.50, much more comparable to CoNLL++. Model perfomances reported in Figure 2 were also not affected by the removal of these tabular data. We include further analysis and explanation in the Appendix ( \u00a7 A).\n2 http://commoncrawl.org/ 3 Articles only annotated by the second author were reviewed and then used as the gold standard.  If the exact temporal coverage cannot be found, we report the time of publication of the corpus followed by *. For each model, we report the percentage change in F 1 and the change in ranking. Abbreviations: BC = BookCorpus (Zhu et al., 2015), BP = BERT Pre-training Corpus, CC = CC-100 (Conneau et al., 2020), CN = CC-News (Nagel, 2016), C4 = Colossal Clean Crawled Corpus (Raffel et al., 2020), G5 = Gigaword5 (Parker et al., 2011), OS = OSCAR (Su\u00e1rez et al., 2019), OW = OpenWebText (Gokaslan et al., 2019), RN = REALNEWS (Zellers et al., 2019), RP = RoBERTa Pre-training Corpus, SS = Semantic Scholar , ST = Stories (Trinh and Le, 2018), WP = Wikipedia, 1B = 1B Benchmark (Chelba et al., 2014). \u2020 Entity-aware self-attention (Yamada et al., 2020).\n2020, in order to measure how closely the annotators follow the style of the original dataset.\nInter-Rater Agreement: We find that the CoNLL++ annotations closely follow the style of the original dataset. When considering labels in the CoNLL-2003 test set as gold, our manual reannotation achieves a 95.46 F 1 score. 4 The second author's annotation, when considering the first author's as gold, receives a 96.23 F 1 score on overlapping articles. The token-level Cohen's Kappa between the two authors is 97.42, which can be considered almost perfect agreement (Artstein and Poesio, 2008). Table 1 summarizes the statistics of 4 For reference, the current state of the art for automated NER taggers is 94.60 (Wang et al., 2021). the two datasets.", "publication_ref": ["b52", "b21", "b47", "b0", "b35", "b37", "b60", "b14", "b15", "b35", "b52", "b50", "b61", "b13", "b38", "b42", "b39", "b51", "b20", "b59", "b10", "b57", "b4", "b55"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Experimental Setup", "text": "We select models with a variety of architectures and pre-training corpora and fine-tune these models to study how different factors affect generalization. None of the models used any pre-training data that temporally overlap with CoNLL++, eliminating the possibility of articles in CoNLL++ appearing in any pre-training corpus. A list of all models and their implementation details can be found in Table 2.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Scripts for fine-tuning Flair and ELMo are", "text": "Figure 2: Plot of CoNLL++ F 1 scores against CoNLL-2003 F 1 scores. Each data point represents the average F 1 for each model, and the error bar represents one standard deviation. We observe that models show different level of generalization, while T5 and RoBERTa models generalize to CoNLL++. The solid best-fit line is steeper than the dashed y = x ideal generalization line, providing evidence against adaptive overfitting ( \u00a7 5.1). This figure is best viewed in color. adapted from Reiss et al. (2020). 5 Other recurrent neural network (RNN) models are trained using various GitHub repositories (see footnotes 6, 7 and 8). We fine-tune the BERT and RoBERTa models with the HuggingFace transformers library (Wolf et al., 2020), except LUKE with AllenNLP . T5 is fine-tuned to conditionally generate NER tags around entities (e.g. <per> Jane Doe </per>).\nA hyperparameter search is conducted for each model. We follow the recommended search space for a model if available in its publication. {8, 16, 32} and {1e-5, 2e-5, 3e-5, 5e-5} are used for most searches for batch sizes and learning rates respectively. Appendix B provides more details on the hyperparameter search.\nWe train models on the CoNLL-2003 training set for 10 epochs, and use the dev set to select the best epoch and other hyperparameters for evaluation. Each model is evaluated five times with different random seeds on the CoNLL-2003 test set and on CoNLL++ to obtain the average F 1 .\nIn Table 2, we report the percentage change of F 1 , calculated as:\n\u2206F 1 = F 1 CoNLL++ \u2212 F 1 CoNLL-2003 F 1 CoNLL-2003 \u00d7 100\n5 Implementation from Reiss et al. (2021) 6 Implementation from Jie (2020) 7 Implementation from Kanakarajan ( 2019) 8 Implementation from Reimers and Gurevych (2017) where F 1 CoNLL++ and F 1 CoNLL-2003 are the F 1 scores on the CoNLL++ and CoNLL-2003 test sets respectively. The results are visualized in Figure 2. Raw F 1 scores are shown in Table 5 in the Appendix ( \u00a7 C.1).", "publication_ref": ["b46", "b56", "b45", "b44"], "figure_ref": [], "table_ref": ["tab_4", "tab_12"]}, {"heading": "What Ingredients are Needed for Good", "text": "Generalization?\nAs we can see in Figure 2 and Table 2, different models have very different generalization. Some models (e.g. RoBERTa-based models and T5 3B ), have no performance drop on CoNLL++, whereas other models' performances decrease significantly.\nIn the following sub-sections, we evaluate the impact of a number of factors on generalization. In \u00a75, we attempt to disentangle to what extent the observed performance drops on CoNLL++ are caused by temporal deterioration, or adaptive overfitting.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Model Size", "text": "It has been shown that the size of pre-trained models affects their performance (Kaplan et al., 2020;Raffel et al., 2020). This inspired us to investigate the effect of model size on generalization. We compare the performance of BERT, RoBERTa, ALBERT and T5 models with different sizes on CoNLL++ and CoNLL-2003. The results are visualized in Figure 3. Details are available in Table 6 in the Appendix ( \u00a7 C.2).\nWe observe, from Table 6, that larger models perform better on both test sets, but more impor-tantly, as illustrated in Figure 3, performance degradation on CoNLL++ diminishes or even disappears as the model size grows. The only exception is the RoBERTa-based models, whose base-sized model already achieves comparable performance on CoNLL++. Figure 3 suggests that larger model sizes not only increase performance on a static test set, but also help models generalize better to new data. It is also informative to look at the individual trend within each model family. Whereas T5 models exhibit a linear relationship between the log number of parameters and \u2206F 1 , the improvement of \u2206F 1 for ALBERT models diminishes as the size grows larger. Additionally, models of similar sizes do not necessarily exhibit similar generalization. For example, BERT Base & RoBERTa Base (\u223c100M), ALBERT XXLarge & T5 Base (\u223c220M) and BERT Large & RoBERTa Large (\u223c300M) all have similar sizes, but the performance changes within each pair are very different.\nBoth RoBERTa Large and T5 3B achieve a performance increase of \u223c0.6%, but the number of parameters of T5 3B is \u223c10 times of that of RoBERTa Large . This shows that the generalizability of model is also affected by factors other than the size of the model, but with the same architectures, larger models tend to generalize better.", "publication_ref": ["b28", "b42"], "figure_ref": ["fig_1", "fig_1", "fig_1"], "table_ref": ["tab_11", "tab_11"]}, {"heading": "Model Architecture", "text": "Based on the results from Table 2, we also observe that model architecture has a significant impact on generalizability. Most BERT, RoBERTa and T5 models have a small performance drop (less than 4% F 1 ) on CoNLL++. The performances of RoBERTa Large , news-RoBERTa Base , LUKE and Longformer Base improved slightly on CoNLL++. The fact that most Transformer-based models achieve higher rankings in CoNLL++ also confirms that pre-trained Transformers generalize better to new data.\nBiLSTM models with Flair and ELMo embeddings, despite performing exceptionally on CoNLL-2003, show larger performance drops on CoNLL++ (5-6% F 1 ), and the performance of BiL-STM+GloVe models drops even more significantly (greater than 6% F 1 ). Such results show a clear trend that Transformer-based models generalize better to new data.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Number of Fine-Tuning Examples", "text": "The generalizability of a model may also be affected by the size of the fine-tuning dataset.   We fit a line to the data points in Figure 2, and then calculate its slope. A slope greater than 1 indicates that every unit of improvement on the CoNLL-2003 test set by the development of models translates to more than one unit of improvement on CoNLL++, i.e. there is no diminishing return. We measure the slope to be 2.729 > 1, indicating that we have not found any diminishing return on CoNLL++, and therefore no adaptive overfitting caused by the model development over the past two decades.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Test Reuse", "text": "If  Based on our results above, the performance degradation on the CoNLL++ is likely not caused by overfitting on CoNLL-2003. Rather, it is more likely caused by temporal drift, which we discuss in the next section.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Temporal Drift", "text": "Temporal drift refers to the performance degradation of a model on the downstream task caused by the temporal difference between the train and test data. Prior work has shown that the performance on NER is affected by temporal drift. For example, Rijhwani and Preotiuc-Pietro (2020) showed that the performance of GloVe and Flair embeddings on NER degrades when the test data is more temporally distant from the train data of the downstream task. Agarwal and Nenkova (2022) also reported the same observation on GloVe embeddings.\nIn this section, we use the same term \"temporal drift\" but refer to the deterioration of generalization of models caused by the temporal difference between the pre-training corpus of their word embeddings and the test data of the downstream task. We hypothesize that generalization is largely affected by such temporal drift. We conduct experiments on Flair and ELMo, as well as on RoBERTa.", "publication_ref": ["b47", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Temporal Drift in Flair and ELMo", "text": "We first investigate if bringing the pre-training corpora of Flair and ELMo closer to the test set can improve their generalizability. We notice that both embeddings were trained on 1B Benchmark (Chelba et al., 2014). This corpus was collected from WMT11 (Callison-Burch et al., 2011) English monolingual data, which is largely comprised of news data between 2007-2011. We hypothesize that pre-training these embeddings on a more recent corpus, e.g. REALNEWS corpus (Zellers et al., 2019) which contains news articles from 2016-2019, will improve their generalizability.\nTo control the experiment, we randomly sample 1 billion tokens of data from REALNEWS. We train Flair embeddings following the same procedure detailed in Akbik et al. (2018) to obtain both the forward and backward embeddings. Our embeddings achieve character level perplexity on the test set of 2.45 for the forward embeddings and 2.46 for the backward embeddings, comparable to 2.42  This provides evidence that the generalizability of the LSTM-based contextualized word embeddings is affected by temporal drift. However, even temporally closer data, these models still suffer from performance drops. This suggests that other ingredients, such as model architecture ( \u00a7 4.2), are still needed for a good generalization.", "publication_ref": ["b10", "b59", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Temporal Drift in RoBERTa", "text": "Because pre-training a transformer model from scratch is expensive, we continue pre-training from the RoBERTa Base checkpoint, leveraging the findings from Gururangan et al. (2020) that models learn to adapt to the distribution of the new corpora with continued pre-training.\nWe use the WMT20 English dataset (Barrault et al., 2019), consisting of English news data from 2007 to 2021. To avoid temporal overlap, we only use data from 2007 to 2019 as the pre-training corpora. The data are divided by year, and we preprocess the data such that the number of tokens per year is the same. We train the RoBERTa Base model for 3 epochs with the masked language modeling (MLM) objective. Checkpoints from each year are then fine-tuned on the CoNLL-2003 dataset, with the same experimental setup described in Section 3. We evaluate the models on the CoNLL-2003 test set and CoNLL++, and plot the results in Figure 5. Detailed performances are reported in Table 9 in the Appendix ( \u00a7 C.4). The results show a clear trend of performance degradation when the pre-training corpora is temporally distant from the test set. When the pre-training corpus is more recent, it becomes temporally closer to CoNLL++, leading to better CoNLL++ performance, and hence better generalization. In Figure 5, the \u2206F 1 shows an upward trend with a correlation coefficients of 0.55, indicating a moderate positive correlation between generalization and the year of the pre-training corpora. This suggests that generalization is affected by the effect of temporal drift. This explains the better generalizability of models such as LUKE Large and T5 3B , pre-trained on temporally closer data to the CoNLL++ test set, showing that temporal drift is the main driving factor for the different levels of generalization.", "publication_ref": ["b6"], "figure_ref": ["fig_4", "fig_4"], "table_ref": ["tab_16"]}, {"heading": "Related Work", "text": "How well pre-trained LMs adapt to data from future time periods has undergone extensive study. Temporal degradation has been found to be a challenge for many tasks, including language modeling (Lazaridou et al., 2021), NER (Augenstein et al., 2017;Agarwal and Nenkova, 2022;Rijhwani and Preotiuc-Pietro, 2020;Ushio et al., 2022), QA (Dhingra et al., 2022), entity linking (Zaporojets et al., 2022), and others (Luu et al., 2022;Amba Hombaiah et al., 2021). All of this work has found that the performance of LMs degrades as the temporal distance between the training data and the test data increases, sometimes called \"temporal misalignment\" (Luu et al., 2022). In contrast to the prior work, we study performance deterioration on a dataset that has been heavily used to develop NER models over a period of 20 years, and conduct extensive experiments that aim to disentangle the effects of aging training sets from those due to heavy test reuse.\nMost closely related to our work is Agarwal and Nenkova (2022), who analyzed a recently created Twitter NER dataset (Rijhwani and Preotiuc-Pietro, 2020) over the period 2014-2019, and found no performance deterioration when using RoBERTabased representations. We build on this line of work by carefully measuring performance deterioration of models trained on the CoNLL-2003 dataset when evaluated on modern data. We analyze which factors are necessary for an NER model trained on a 20-year-old dataset to generalize well to modern data. Furthermore, the large 20-year gap helps us focus on not only temporal deterioration, but also if the extensive test reuse leads to adaptive overfitting. We present evidence in support of the hypothesis that most performance degradation is due to temporal drift and not adaptive overfitting.\nPrior work has attempted to mitigate temporal degradation, mostly through continuously updating LMs with new data (Jang et al., 2022;Jin et al., 2022;Loureiro et al., 2022). Luu et al. (2022) explored this idea but found that temporal adaptation is not as effective as fine-tuning on the data from whose time period the dataset is drawn. In addition, catastrophic forgetting (Robins, 1995) can also be a problem when updating the LMs. Jin et al. (2022) found that applying knowledge distillation (Hinton et al., 2015) based approaches to continual learning can mitigate catastrophic forgetting, while improving the temporal generalization of LMs. Dhingra et al. (2022) proposed to train the LMs with an additional temporal objective by conditioning on the year of data, and found that this effectively mitigated catastrophic forgetting. Jang et al. (2022) created a lifelong benchmark for continuous training and evaluating LMs.", "publication_ref": ["b32", "b5", "b0", "b47", "b54", "b17", "b58", "b35", "b3", "b35", "b0", "b47", "b24", "b26", "b34", "b35", "b48", "b26", "b23", "b17", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion and Future Directons", "text": "In this paper, we evaluate the generalization of NER models using CoNLL++, a CoNLL-style annotated NER test dataset with data from 2020. We conduct experiments on more than 20 models and find that models exhibit different generalizability. Surprisingly, we find that generalizability is not affected by adaptive overfitting, but rather by temporal drift. To achieve better generalization, we need the combination of four factors: a modern transformer-based architecture, a large number of parameters, a large amount of fine-tuning data and a temporally closer pre-training corpus to the test set. We find that our progress on developping NER taggers is largely successful, showing not only good performance on individual test set but also good generalization on new data. This allows CoNLL-2003 taggers to still work in 2023.\nFuture research can focus on ways to mitigate temporal drift. Investigation on attributes of pretraining or fine-tuning corpora that causes temporal drift, such as change of entities mentioned, different usage of language, etc., can also shed light on the more specific impacts from temporal drift, thereby inspiring new and better ways to mitigate it.\nWe hope that our work provides insights on factors affecting generalization and how to mitigate the negative impact, and calls for more research on this everlasting problem of generalization in the NLP community.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Our analysis on temporal drift ( \u00a7 5.2) was limited by the fact that the developer of many models in our study did not release the exact time period of the pre-training corpora used. Additionally, models such as BERT and RoBERTa were pre-trained on corpora that could be potentially be temporally close to the CoNLL++ test set.\nIn the section on test reuse ( \u00a7 5.1.2), due to a limited compute budget, we were only able to conduct this experiment on a single new train/dev/test split, so it is possible that the new split happens to be easier than the mean of the distribution. However, our experiments still provide additional evidence models are not overfitting the original CoNLL-2003 test set.\nIt is worth noting that when using older models trained on the CoNLL-2003 dataset, one additional reason for the performance degradation, especially in real-world deployment, is that the data used to evaluate the models can be out-of-domain. In our experiments, we attemped to control the domain of the test data on which the models were evaluated to assess other factors for performance degradation. However, we acknowledge that in reality, model performance can be affected by factors such as the emerging text types (e.g. Twitter did not exist when CoNLL-2003 NER task was created), which leads to changes in domain, and therefore affects the generalizability of the models.\nWe acknowledge that having CoNLL++ will not resolve the problem of generalization to modern data. As new data keep emerging, there will always be the question of how well NER models generalize to that new data. We hope that our paper will encourage researchers in the NLP community to continuously annotate new test set to study this problem, so that we ensure the robustness and generalizability of models.\n5-1 Pedrag Mijatovic 7-1 Luis Figo 7-1 Raul Gonzalez 7-1 Juan Pizzi 12-1 Fernando Redondo 9-1 Giovanni 14-1 Victor Sanchez 12-1 Guillermo Amor 16-1 Jose Amavisca 14-1 Roger Garcia 16-1 Manolo Sanchis 14-1 Gheorghe Additionally, as each line is considered to be a sentence in CoNLL-2003 dataset (separated by an empty line in the original format), and as items by spaces are considered to be tokens, this also demonstrates why the average token per sentence is much lower in CoNLL-2003 than in CoNLL++. The tabular data contains much shorter sentences in plethora, which significantly lowers the average token per sentence.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Hyperparameter Search", "text": "In this section, we include the details on how we conducted the hyperparameter search for the transformer-based models. We trained most models with different sets of hyperparameters for 10 epochs and save the checkpoints that achieved the highest dev F 1 score. For each model, we compare performance on the dev set of checkpoints trained with different hyperparameters and select the set of hyperparameters with the best performance.\nWe tuned the learning rate and batch sizes for all models. If the instructions on how to tune the hyperparameters for a model are stated in its publication, we followed the instructions as closely as possible. Otherwise, we would tune the model using a default set of hyperparameters, where learn-ing_rate = {1e-5, 2e-5, 3e-5, 5e-5} and batch_size = {8, 16, 32}. Here we only list models for which we did not use the default set of hyperparameters.\n\u2022 ALBERT:\n-learning_rate = {1e-5, 2e-5, 3e-5, 5e-5} -batch_size = {16, 32, 48, 128}\n\u2022 GigaBERT:\n-learning_rate = {1e-5, 2e-5, 5e-5, 1e-4} -batch_size = {4, 8, 16, 32}\n\u2022 Longformer:\n-learning_rate = {1e-5, 2e-5, 3e-5, 5e-5} -batch_size = {16, 32} -total_num_epoch = 15\n\u2022 news_roberta_base:\n-learning_rate = {1e-5, 2e-5, 3e-5} -batch_size = {16, 32}\n\u2022 XLM-RoBERTa:\n-learning_rate = {1e-5, 2e-5, 3e-5, 5e-5} -batch_size = {16, 32}\n\u2022 T5:\n-learning_rate = {2e-5, 3e-5, 5e-5, 1e-4} -batch_size = {4, 8}", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Detailed Results", "text": "In this section, we include all the performance statistics.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 CoNLL-2003 vs CoNLL++", "text": "Table 5 shows the performance statistics of all models on the CoNLL++ and CoNLL-2003 test sets.   ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_12"]}, {"heading": "C.2 Model Size", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.3 Number of Fine-Tuning Examples", "text": "Table 7 and Table 8 show the results from Section 4.3 of the RoBERTa-based and Flair-based models on the two test sets respectively when varying the number of examples fine-tuned on.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_15"]}, {"heading": "C.4 Temporal Drift", "text": "Table 9 show the results from Section 5.2. The \"Year\" column indicates the time period from which the data used for continued pre-training on RoBERTa Base was used.    B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Not applicable. Left blank.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_16"]}, {"heading": "C.5 Test Reuse", "text": "B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Our task is on named entity recognition, for which having actual names in the dataset is more desirable B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Not applicable. Left blank.\nB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. Table 1 C Did you run computational experiments?\nSection 4 and 5 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? Section 4.1 does discuss the number of parameters in some of the models used. However, number of parameters in other models, total computational budget and computing infrastructure are largely irrelevant to our paper C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? Experimental setup in Section 3, and discussion on hyperparameter in Appendix B C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? Section 3, 4 and 5\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\nTable 2 D Did you use human annotators (e.g., crowdworkers) or research with human participants?\nSection 2 D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\nThe annotations were completed by two authors of the paper. No additional annotators were hired.\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? Not applicable. Left blank.\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? Not applicable. Left blank.\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board? Not applicable. Left blank.\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? Not applicable. Left blank.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1", "tab_4"]}, {"heading": "Acknowledgements", "text": "We would like to thank Fan Bai, Yang Chen, Chao Jiang, Junmo Kang and Mounica Maddela for providing feedback on earlier drafts of this paper, We would also like to appreciate the comments from the anonymous reviewers on how to improve the paper. This material is based upon work supported by the NSF (IIS-2052498) and IARPA via the BET-TER program (2019-19051600004). The views, opinions, and/or findings expressed are those of the author(s) and should not be interpreted as representing the official views or policies of the Department of Defense, IARPA or the U.S. Government.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix", "text": "A Tabular Data in CoNLL-2003 We found a significant amount of documents in the CoNLL-2003 test set that list the outcomes of various sports events, which contributes to the larger proportion of named entities in Table 1. These documents appear as though they may have been intended for display on news tickers. 9 We present an example below.   ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Temporal effects on pre-trained models for language processing tasks", "journal": "Transactions of the Association for Computational Linguistics", "year": "2022", "authors": "Oshin Agarwal; Ani Nenkova"}, {"ref_id": "b1", "title": "Pooled contextualized embeddings for named entity recognition", "journal": "", "year": "2019", "authors": "Alan Akbik; Tanja Bergmann; Roland Vollgraf"}, {"ref_id": "b2", "title": "Contextual string embeddings for sequence labeling", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Alan Akbik; Duncan Blythe; Roland Vollgraf"}, {"ref_id": "b3", "title": "Dynamic language models for continuously evolving content", "journal": "", "year": "2021", "authors": "Tao Spurthi Amba Hombaiah; Mingyang Chen; Michael Zhang; Marc Bendersky;  Najork"}, {"ref_id": "b4", "title": "Survey article: Inter-coder agreement for computational linguistics", "journal": "Computational Linguistics", "year": "2008", "authors": "Ron Artstein; Massimo Poesio"}, {"ref_id": "b5", "title": "Generalisation in named entity recognition: A quantitative analysis", "journal": "Computer Speech & Language", "year": "2017", "authors": "Isabelle Augenstein; Leon Derczynski; Kalina Bontcheva"}, {"ref_id": "b6", "title": "Findings of the 2019 conference on machine translation (WMT19)", "journal": "", "year": "2019", "authors": "Lo\u00efc Barrault; Ond\u0159ej Bojar; Marta R Costa-Juss\u00e0; Christian Federmann; Mark Fishel; Yvette Graham; Barry Haddow; Matthias Huck; Philipp Koehn; Shervin Malmasi; Christof Monz; Mathias M\u00fcller"}, {"ref_id": "b7", "title": "SciB-ERT: A pretrained language model for scientific text", "journal": "", "year": "2019", "authors": "Iz Beltagy; Kyle Lo; Arman Cohan"}, {"ref_id": "b8", "title": "Longformer: The Long-Document Transformer", "journal": "ArXiv", "year": "2020", "authors": "Iz Beltagy; Matthew E Peters; Arman Cohan"}, {"ref_id": "b9", "title": "Proceedings of the Sixth Workshop on Statistical Machine Translation", "journal": "Association for Computational Linguistics", "year": "2011", "authors": ""}, {"ref_id": "b10", "title": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling", "journal": "ArXiv", "year": "2014", "authors": "Ciprian Chelba; Tomas Mikolov; Mike Schuster; Qi Ge; T Brants; Phillip Todd Koehn; Tony Robinson"}, {"ref_id": "b11", "title": "Named entity recognition with bidirectional LSTM-CNNs. Transactions of the Association for Computational Linguistics", "journal": "", "year": "2016", "authors": "P C Jason; Eric Chiu;  Nichols"}, {"ref_id": "b12", "title": "Structural scaffolds for citation intent classification in scientific publications", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Arman Cohan; Waleed Ammar; Madeleine Van Zuylen; Field Cady"}, {"ref_id": "b13", "title": "Unsupervised cross-lingual representation learning at scale", "journal": "", "year": "2020", "authors": "Alexis Conneau; Kartikay Khandelwal; Naman Goyal; Vishrav Chaudhary; Guillaume Wenzek; Francisco Guzm\u00e1n; Edouard Grave; Myle Ott; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b14", "title": "Multilingual autoregressive entity linking", "journal": "Transactions of the Association for Computational Linguistics", "year": "2022", "authors": "Nicola De Cao; Ledell Wu; Kashyap Popat; Mikel Artetxe; Naman Goyal; Mikhail Plekhanov; Luke Zettlemoyer; Nicola Cancedda; Sebastian Riedel; Fabio Petroni"}, {"ref_id": "b15", "title": "Imagenet: A large-scale hierarchical image database", "journal": "", "year": "2009", "authors": "Jia Deng; Wei Dong; Richard Socher; Li-Jia Li; Kai Li; Li Fei-Fei"}, {"ref_id": "b16", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b17", "title": "Time-aware language models as temporal knowledge bases", "journal": "Transactions of the Association for Computational Linguistics", "year": "2022", "authors": "Bhuwan Dhingra; Jeremy R Cole; Julian Martin Eisenschlos; Daniel Gillick; Jacob Eisenstein; William W Cohen"}, {"ref_id": "b18", "title": "Incorporating non-local information into information extraction systems by Gibbs sampling", "journal": "Association for Computational Linguistics", "year": "2005", "authors": "Jenny Rose Finkel; Trond Grenager; Christopher Manning"}, {"ref_id": "b19", "title": "AllenNLP: A deep semantic natural language processing platform", "journal": "", "year": "2018", "authors": "Matt Gardner; Joel Grus; Mark Neumann; Oyvind Tafjord; Pradeep Dasigi; Nelson F Liu; Matthew Peters; Michael Schmitz; Luke Zettlemoyer"}, {"ref_id": "b20", "title": "", "journal": "", "year": "2019", "authors": "Aaron Gokaslan; Vanya Cohen; Ellie Pavlick; Stefanie Tellex"}, {"ref_id": "b21", "title": "We need to talk about standard splits", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Kyle Gorman; Steven Bedrick"}, {"ref_id": "b22", "title": "2020. Don't stop pretraining: Adapt language models to domains and tasks", "journal": "Online. Association for Computational Linguistics", "year": "", "authors": "Ana Suchin Gururangan; Swabha Marasovi\u0107; Kyle Swayamdipta; Iz Lo; Doug Beltagy; Noah A Downey;  Smith"}, {"ref_id": "b23", "title": "Distilling the knowledge in a neural network", "journal": "", "year": "2015", "authors": "Geoffrey Hinton; Oriol Vinyals; Jeff Dean"}, {"ref_id": "b24", "title": "TemporalWiki: A lifelong benchmark for training and evaluating ever-evolving language models", "journal": "", "year": "2022", "authors": "Joel Jang; Seonghyeon Ye; Changho Lee; Sohee Yang; Joongbo Shin; Janghoon Han; Gyeonghun Kim; Minjoon Seo"}, {"ref_id": "b25", "title": "Allanj/pytorch_neural_crf: Pytorch implementation of LSTM/Bert-CRF for named entity recognition", "journal": "", "year": "2020", "authors": "Allan Jie"}, {"ref_id": "b26", "title": "Lifelong pretraining: Continually adapting language models to emerging corpora", "journal": "", "year": "2022", "authors": "Xisen Jin; Dejiao Zhang; Henghui Zhu; Wei Xiao;  Shang-Wen; Xiaokai Li; Andrew Wei; Xiang Arnold;  Ren"}, {"ref_id": "b27", "title": "Kamalkraj/namedentity-recognition-with-bidirectional-LSTM-CNNS: Named-entity-recognition-with-bidirectional-LSTM-CNNS", "journal": "", "year": "2019", "authors": " Kamal Raj Kanakarajan"}, {"ref_id": "b28", "title": "Scaling Laws for Neural Language Models", "journal": "", "year": "2001", "authors": "Jared Kaplan; Sam Mccandlish; Tom Henighan; Tom B Brown; Benjamin Chess; Rewon Child; Scott Gray; Alec Radford; Jeffrey Wu; Dario Amodei"}, {"ref_id": "b29", "title": "Neural architectures for named entity recognition", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Guillaume Lample; Miguel Ballesteros; Sandeep Subramanian; Kazuya Kawakami; Chris Dyer"}, {"ref_id": "b30", "title": "An empirical study of pre-trained transformers for Arabic information extraction", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Wuwei Lan; Yang Chen; Wei Xu; Alan Ritter"}, {"ref_id": "b31", "title": "Albert: A lite bert for self-supervised learning of language representations", "journal": "", "year": "2020", "authors": "Zhenzhong Lan; Mingda Chen; Sebastian Goodman; Kevin Gimpel; Piyush Sharma; Radu Soricut"}, {"ref_id": "b32", "title": "Mind the gap: Assessing temporal generalization in neural language models", "journal": "Advances in Neural Information Processing Systems", "year": "2021", "authors": "Angeliki Lazaridou; Adhi Kuncoro; Elena Gribovskaya; Devang Agrawal; Adam Liska; Tayfun Terzi; Mai Gimenez; Cyprien De Masson D'autume; Tomas Kocisky; Sebastian Ruder"}, {"ref_id": "b33", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "journal": "CoRR", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b34", "title": "TimeLMs: Diachronic language models from Twitter", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Daniel Loureiro; Francesco Barbieri; Leonardo Neves; Luis Espinosa Anke; Jose Camacho-Collados"}, {"ref_id": "b35", "title": "Time waits for no one! analysis and challenges of temporal misalignment", "journal": "", "year": "2022", "authors": "Kelvin Luu; Daniel Khashabi; Suchin Gururangan; Karishma Mandyam; Noah A Smith"}, {"ref_id": "b36", "title": "End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Xuezhe Ma; Eduard Hovy"}, {"ref_id": "b37", "title": "Unsupervised text deidentification", "journal": "", "year": "2022", "authors": "John Morris; Justin Chiu; Ramin Zabih; Alexander Rush"}, {"ref_id": "b38", "title": "", "journal": "CC-News", "year": "2016", "authors": "Sebastian Nagel"}, {"ref_id": "b39", "title": "English Gigaword Fifth Edition LDC2011T07", "journal": "", "year": "2011", "authors": "Robert Parker; David Graff; Junbo Kong; Ke Chen; Kazuaki Maeda"}, {"ref_id": "b40", "title": "Deep contextualized word representations", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Matthew E Peters; Mark Neumann; Mohit Iyyer; Matt Gardner; Christopher Clark; Kenton Lee; Luke Zettlemoyer"}, {"ref_id": "b41", "title": "Stanza: A python natural language processing toolkit for many human languages", "journal": "", "year": "2020", "authors": "Peng Qi; Yuhao Zhang; Yuhui Zhang; Jason Bolton; Christopher D Manning"}, {"ref_id": "b42", "title": "Exploring the limits of transfer learning with a unified text-totext transformer", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b43", "title": "Do ImageNet Classifiers Generalize to ImageNet?", "journal": "", "year": "2019", "authors": "Benjamin Recht; Rebecca Roelofs; Ludwig Schmidt; Vaishaal Shankar"}, {"ref_id": "b44", "title": "Reporting score distributions makes a difference: Performance study of LSTM-networks for sequence tagging", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Nils Reimers; Iryna Gurevych"}, {"ref_id": "b45", "title": "CODAIT/identifying-incorrect-labels-in-CONLL-2003: Research into identifying and correcting incorrect labels in the CONLL-2003 corpus", "journal": "", "year": "2021", "authors": "Fred Reiss; Karthik Muthuraman; Zachary Eichenberger; Brian Cutler; Hong Xu"}, {"ref_id": "b46", "title": "Identifying incorrect labels in the CoNLL-2003 corpus", "journal": "", "year": "2020", "authors": "Frederick Reiss; Hong Xu; Bryan Cutler; Karthik Muthuraman; Zachary Eichenberger"}, {"ref_id": "b47", "title": "Temporally-informed analysis of named entity recognition", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Shruti Rijhwani; Daniel Preotiuc-Pietro"}, {"ref_id": "b48", "title": "Catastrophic forgetting, rehearsal and pseudorehearsal", "journal": "Connect. Sci", "year": "1995", "authors": "Anthony V Robins"}, {"ref_id": "b49", "title": "A Meta-Analysis of Overfitting in Machine Learning", "journal": "Curran Associates Inc", "year": "2019", "authors": "Rebecca Roelofs; Sara Fridovich-Keil; John Miller; Vaishaal Shankar; Moritz Hardt; Benjamin Recht; Ludwig Schmidt"}, {"ref_id": "b50", "title": "brat: a web-based tool for NLP-assisted text annotation", "journal": "Association for Computational Linguistics", "year": "2012", "authors": "Pontus Stenetorp; Sampo Pyysalo; Goran Topi\u0107; Tomoko Ohta; Sophia Ananiadou; Jun'ichi Tsujii"}, {"ref_id": "b51", "title": "Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures", "journal": "", "year": "2019-07-22", "authors": "Pedro Javier Ortiz Su\u00e1rez; Beno\u00eet Sagot; Laurent Romary"}, {"ref_id": "b52", "title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition", "journal": "", "year": "2003", "authors": "Erik F Tjong; Kim Sang; Fien De Meulder"}, {"ref_id": "b53", "title": "A Simple Method for Commonsense Reasoning", "journal": "", "year": "2018", "authors": "H Trieu; Quoc V Trinh;  Le"}, {"ref_id": "b54", "title": "Named entity recognition in Twitter: A dataset and analysis on short-term temporal shifts", "journal": "Long Papers", "year": "2022", "authors": "Asahi Ushio; Francesco Barbieri; Vitor Sousa; Leonardo Neves; Jose Camacho-Collados"}, {"ref_id": "b55", "title": "Automated concatenation of embeddings for structured prediction", "journal": "Long Papers", "year": "2021", "authors": "Xinyu Wang; Yong Jiang; Nguyen Bach; Tao Wang; Zhongqiang Huang; Fei Huang; Kewei Tu"}, {"ref_id": "b56", "title": "Transformers: State-of-the-art natural language processing", "journal": "", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; Remi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger;  Drame"}, {"ref_id": "b57", "title": "LUKE: Deep contextualized entity representations with entityaware self-attention", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Ikuya Yamada; Akari Asai; Hiroyuki Shindo; Hideaki Takeda; Yuji Matsumoto"}, {"ref_id": "b58", "title": "TempEL: Linking dynamically evolving and newly emerging entities", "journal": "", "year": "2022", "authors": "Klim Zaporojets; Lucie-Aim\u00e9e Kaffee; Johannes Deleu; Thomas Demeester"}, {"ref_id": "b59", "title": "Defending Against Neural Fake News", "journal": "Curran Associates Inc", "year": "2019", "authors": "Rowan Zellers; Ari Holtzman; Hannah Rashkin; Yonatan Bisk; Ali Farhadi; Franziska Roesner; Yejin Choi"}, {"ref_id": "b60", "title": "A frustratingly easy approach for entity and relation extraction", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Zexuan Zhong; Danqi Chen"}, {"ref_id": "b61", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books", "journal": "IEEE Computer Society", "year": "2015", "authors": "Y Zhu; R Kiros; R Zemel; R Salakhutdinov; R Urtasun; A Torralba; S Fidler"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "we conduct an empirical study of more than 20 NER models that were trained on the original CoNLL-2003 training split. Our analysis shows that different models can have very different generalization when moving to modern data. Simply comparing the performance of models on the CoNLL-2003 test set does not tell the whole story of progress on NER over the past 20 years.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Plot of percentage change in F 1 scores (\u2206F 1 ) against the number of parameters in log scale. All models except RoBERTa show an improvement in generalizability as the model size grows.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "We conduct experiments varying the number of CoNLL-2003 training examples used for finetuning from 10% to 100%. The fine-tuning is done with RoBERTa Base and Flair embeddings using the same experimental setup as in Section 3. We plot the percentage change in F 1 against the percentage of training examples in Fig 4.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Plot of change in F 1 scores (\u2206F 1 ) against the percentage of CoNLL-2003 training data used for finetuning. Both RoBERTa Base and Flair show improved generalization as we use more training examples, although Flair shows a more pronounced improvement.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Plot of \u2206F 1 scores against the year of data used for RoBERTa pre-training. The upward trend, indicated by the dashed best-fit line, shows that the generalization improves as the pre-training corpora used is temporally closer to CoNLL++.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Statistics of CoNLL-2003 test set and our CoNLL++.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Details about the models selected, sorted by \u2206 F 1 . We list the models' architectures and word embeddings, pre-training corpora, and the temporal coverage of the corpora.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Both RoBERTa Base and Flair embeddings show improved generalization as we use more training examples. However, this improvement is more pronounced for Flair than RoBERTa Base . Even with 10% of the training data, RoBERTa Base already shows a positive change in F 1 scores, and increasing the amount of training data to 100% only improves the change by an absolute value of 1%. In contrast, increasing the amount of training data from 10% to just 20% can already improve \u2206F 1 by 2% for Flair.The empirical evidence supports our hypothesis that having more training examples can improve the generalizability of the model, but such effect may vary across different models. RoBERTa-based models generalize well to new data even when only a small amount of fine-tuning data is available, whereas Flair benefits much more from having more fine-tuning data.We first investigate if the performance drop is caused by adaptive overfitting of models developed over the past 20 years. defined adaptive overfitting as the overfitting caused by reusing the same test set (test reuse). studied this phenomenon in the context of ImageNet by measuring to what extent can the improvement on the old test set translate to improvement on the new test set (diminishing return).", "figure_data": "5 What Causes the Performance Drop Observed for Some Models?Models in Table 2 show different levels of per-formance drop, or sometimes performance gain,on CoNLL++ compared to the CoNLL-2003 testset, and it is not entirely clear what causes thisdifference. We hypothesize two potential causes,namely adaptive overfitting ( \u00a7 5.1) and temporaldrift ( \u00a7 5.2). In this section, we investigate each ofthese potential causes.5.1 Adaptive OverfittingWe analyze both effects to conclude the presenceof adaptive overfitting.5.1.1 Diminishing ReturnFollowing Recht et al. (2019), we measure the di-minishing return on the CoNLL++ test set. Di-minishing return measures if the improvement onCoNLL-2003 test set, gained by the continuouseffort of developing NER taggers over 20 years,translates to smaller (diminishing) improvement onCoNLL++."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "the models are overfitting to the CoNLL-2003 test set due to test reuse, we should see not only a performance degradation on CoNLL++, but also a performance degradation on a test set taken from the same distribution as the CoNLL-2003 test set. To obtain a new test set taken from the same distribution as the CoNLL-2003 test set, we resampled a new train/dev/test split from the CoNLL-2003 dataset, which we call CoNLL-2003'. Each split contains the same number of articles as its corresponding split in the CoNLL-2003 dataset. The \"new\" test set is thus certain to come from the same distribution as the original CoNLL-2003 test. We train and evaluate models on CoNLL-2003' with the same experimental setup as in Section 3, and report the results in Table 3.", "figure_data": "Name BiLSTM-CRF BiLSTM-CNN SciBERT BiLSTM-CNN-CRF BiLSTM-CRF-ELMo Flair Pooled Flair mBERT GigaBERT ALBERT Base BERT Large XLM-RoBERTa Base T5 Large RoBERTa Large Longformer Base news-RoBERTa Base LukeCoNLL++ \u2206F 1 (%) -20.25 -15.09 -8.94 -6.52 -5.72 -5.57 -4.65 -4.22 -3.90 -3.61 -2.01 -0.90 -0.59 +0.64 +1.00 +1.06 +1.10CoNLL-2003' \u2206F 1 (%) +2.53 +1.75 -0.09 +2.95 +1.58 +0.76 +1.60 +2.80 +1.75 +2.36 +0.47 -1.52 +2.65 +0.38 +2.44 +1.90 +1.87"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Comparison between the performance change on CoNLL++ and CoNLL-2003' test sets. The table shows clearly that the performances of most models are not degrading because of test reuse. Detailed results can be found in Table 10 in Appendix C.5. We only observe SciBERT and XLM-RoBERTa Large models performing slightly worse on the CoNLL-2003' test set, while all other models appear to perform better. Most models suffering from performance degradation on the CoNLL++ also perform better on the CoNLL-2003' test set. This provides evidence that individual models are not overfitting to the CoNLL-2003 test set.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "", "figure_data": ": Percentage change in F 1 scores (\u2206F 1 ) on CoNLL++ of Flair and ELMo embeddings when pre-trained on 1B Benchmark vs on REALNEWS cor-pus. Pre-training on REALNEWS, which is temporally closer to CoNLL++, improves the generalization of Flair and ELMo embeddings.reported in Akbik et al. (2018). Similarly, we trainELMo embeddings following Peters et al. (2018),which achieves a perplexity of 40.07 on the testset, comparable to 39.7 reported. We use the sametraining scripts and hyperparameters as our exper-iments in Section 4.2 for Flair, Pooled Flair andELMo. The newly trained models are dubbed asFlair RN , Pooled Flair RN and ELMo RN .It is clearly shown in Table 4 that having thetraining corpus for Flair and ELMo embeddingstemporally closer to the CoNLL++ test set im-proves generalization. Notably, the performancegap for ELMo is reduced to -1.43%, better than thatof BERT Large (-2.01%). The improvements in gen-eralization are attributed to the performance dropson the CoNLL-2003 test set and improvements onCoNLL++."}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "", "figure_data": "includes the results from Section 4.1,showing the performance statistics of BERT-based,ALBERT-based, RoBERTa-based, and T5-basedmodels with various sizes on the CoNLL-2003 andCoNLL++ test set. One side note is that our re-sults also confirms the previous findings that theperformance on a downstream task has a positivecorrelation with the model size."}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "", "figure_data": ": Detailed performances of the models on the CoNLL-2003 test set and the CoNLL++ test set, ranked by the \u2206F 1 . The performances are F 1 scores calculated by taking the average over five runs and the standard deviations are presented in subscripts. The best results are highlighted in bold.Name BERT Base BERT Large RoBERTa Base RoBERTa Large ALBERT Base ALBERT Large ALBERT XLarge ALBERT XXLarge T5 Small T5 Base T5 Large T5 3B# Parameters CoNLL-2003 CoNLL++ \u2206F 1 (%) 108M 91.38 0.33 -3.99 87.73 0.51 334M 91.77 0.20 -2.01 89.93 0.74 123M 92.08 0.22 +1.14 93.13 0.31 354M 92.71 0.21 +0.64 93.30 0.24 12M 89.53 0.23 -3.61 86.30 0.39 18M 90.46 0.21 -3.34 87.44 0.47 60M 90.80 0.17 -2.46 88.57 1.03 235M 91.69 0.33 -2.22 89.65 0.23 60M 88.94 0.32 -2.90 86.36 0.08 220M 91.55 0.27 -1.64 90.05 0.45 770M 91.93 0.32 -0.59 91.39 0.75 3B 92.59 0.32 93.21 0.09 +0.67"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Performances of the models of different sizes on the CoNLL-2003 test set and the CoNLL++ test set. The performances are F 1 scores calculated by taking the average over five runs and the standard deviations are presented in subscripts.", "figure_data": ""}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "shows the results from Section 5.1.2.", "figure_data": "Training ExampleCoNLL-2003 CoNLL++ \u2206F 1 (%)10% 20% 30% 40% 50% 60% 70% 80% 90%88.28 0.38 90.23 0.30 90.81 0.21 91.10 0.12 91.42 0.15 91.45 0.27 91.82 0.10 91.98 0.15 92.04 0.2088.49 0.67 91.08 0.47 91.36 0.40 91.64 0.48 91.76 0.49 91.93 0.34 92.25 0.34 92.97 0.46 92.94 0.50+0.24 +0.94 +0.61 +0.59 +0.37 +0.52 +0.47 +1.07 +0.98Table 7: Performances of RoBERTa Base on the CoNLL-2003 test set and the CoNLL++ test set when varying the percentage of training examples used. The perfor-mances are F 1 scores calculated by taking the average over five runs and the standard deviations are presented in subscripts.Training ExampleCoNLL-2003 CoNLL++ \u2206F 1 (%)10% 20% 30% 40% 50% 60% 70% 80% 90%86.90 0.15 88.42 0.45 89.04 0.24 89.74 0.11 90.15 0.13 90.40 0.28 90.62 0.16 90.68 0.16 90.84 0.1779.11 0.53 82.26 0.67 83.17 0.71 83.98 0.49 84.47 0.26 84.64 0.74 85.08 0.83 85.39 0.62 85.44 0.46-8.96 -6.96 -6.59 -6.43 -6.30 -6.38 -6.11 -5.83 -5.94"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Performances of Flair on the CoNLL-2003 test set and the CoNLL++ test set when varying the percentage of training examples used.", "figure_data": "Year CoNLL-2003 CoNLL++ \u2206F 1 (%) 2007 91.96 0.44 +0.97 92.85 0.31 2008 91.88 0.09 +0.88 92.69 0.17 2009 92.24 0.17 +0.87 93.10 0.11 2010 91.92 0.25 +1.28 93.10 0.41 2011 92.07 0.35 +0.96 92.95 0.15 2012 92.07 0.34 +0.76 92.77 0.33 2013 91.87 0.23 +1.05 92.84 0.27 2014 92.01 0.32 +0.96 92.89 0.21 2015 91.95 0.29 +0.99 92.92 0.63 2016 91.98 0.23 +1.02 92.92 0.26 2017 91.93 0.13 +1.08 92.93 0.18 2018 91.89 0.38 +1.13 92.93 0.44 2019 91.80 0.29 93.25 0.44 +1.58"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "Performances of differnt checkpoints obtained by continued pre-training RoBERTa Base with data from different years on the CoNLL-2003 test set and CoNLL++.", "figure_data": "ACL 2023 Responsible NLP ChecklistA For every submission:A1. Did you describe the limitations of your work?Section 8A2. Did you discuss any potential risks of your work?Not applicable. Left blank.A3. Do the abstract and introduction summarize the paper's main claims?Abstract at the beginning of the paper, and introduction in Section 1A4. Have you used AI writing assistants when working on this paper?Left blank.B Did you use or create scientific artifacts?Left blank.B1. Did you cite the creators of artifacts you used?"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "B2. Did you discuss the license or terms for use and / or distribution of any artifacts? Not applicable. Left blank.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u2206F 1 = F 1 CoNLL++ \u2212 F 1 CoNLL-2003 F 1 CoNLL-2003 \u00d7 100", "formula_coordinates": [4.0, 90.55, 689.84, 178.88, 30.07]}], "doi": "10.1162/tacl_a_00497"}