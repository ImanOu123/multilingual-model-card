{"title": "Sensitivity, Performance, Robustness: Deconstructing the Effect of Sociodemographic Prompting", "authors": "Tilman Beck; Hendrik Schuff; Anne Lauscher; Iryna Gurevych", "pub_date": "", "abstract": "Annotators' sociodemographic backgrounds (i.e., the individual compositions of their gender, age, educational background, etc.) have a strong impact on their decisions when working on subjective NLP tasks, such as toxic language detection. Often, heterogeneous backgrounds result in high disagreements. To model this variation, recent work has explored sociodemographic prompting, a technique, which steers the output of prompt-based models towards answers that humans with specific sociodemographic profiles would give. However, the available NLP literature disagrees on the efficacy of this technique -it remains unclear for which tasks and scenarios it can help, and the role of the individual factors in sociodemographic prompting is still unexplored. We address this research gap by presenting the largest and most comprehensive study of sociodemographic prompting today. We analyze its influence on model sensitivity, performance and robustness across seven datasets and six instruction-tuned model families. We show that sociodemographic information affects model predictions and can be beneficial for improving zero-shot learning in subjective NLP tasks. However, its outcomes largely vary for different model types, sizes, and datasets, and are subject to large variance with regards to prompt formulations. Most importantly, our results show that sociodemographic prompting should be used with care for sensitive applications, such as toxicity annotation or when studying LLM alignment. 1   ", "sections": [{"heading": "Introduction", "text": "How messages are perceived, is often not only dependent on their factual content, but also on the receiver's subjective interpretation: for instance, two dataset annotators might have different equally valid opinions about what the \"correct\" offensiveness label for a particular tweet should be (e.g., Figure 1: We instruct LLMs to make predictions for subjective NLP tasks from different perspectives using sociodemographic profiles. We show that, besides sociodemographics, outcomes are largely influenced by model choice or prompt formulation. Waseem, 2016;Davani et al., 2023, inter alia). As previously shown, this variation is, at least to some extent, tied to sociodemographic characteristics of the receivers, like their gender identity, age, and educational background (e.g., Biester et al., 2022;Pei and Jurgens, 2023).\nThus, NLP models need to account for a comprehensive set of sociodemographic factors to make more socially-aware predictions. Accordingly, modeling the effect of those factors on subjective tasks has emerged as an important research direction for NLP. As such, researchers have proposed new data collection paradigms -cf. perspectivism (Rottger et al., 2022) -and trained models for reflecting the decisions of particular sociodemographic groups (Gupta et al., 2023;Fleisig et al., 2023). Most recently, researchers (Deshpande et al., 2023;Santurkar et al., 2023;Hwang et al., 2023;Cheng et al., 2023) have explored sociodemographic prompting of large language models (LLMs): the idea is to enrich a particular input prompt with additional sociodemographic informa-tion (cf. Figure 1). The models' output should then be aligned with the population described. This technique has led to promising applications in dataset augmentation (Hartvigsen et al., 2022) and simulation of social computing platforms . Still, our knowledge on the effect of including sociodemographic profiles is scarce and the existing literature disagrees on its usefulness: for instance, Argyle et al. (2023) showed that sociodemographic prompting can be used to simulate human populations -a promise for more efficient sociological surveys. Other work, in turn, points to the danger of stereotypical bias reflected when prompting models with sociodemographic profiles (Cheng et al., 2023;Deshpande et al., 2023).\nOur work makes an important step towards a better understanding of the influence of sociodemographic prompting on models' decisions. We use subjective NLP tasks for evaluation as they have been shown to induce disagreement among annotators related to sociodemographic factors. Our study analyzes the effects of sociodemographic prompting on model output (sensitivity), zero-shot performance, and robustness. Concretely, we make the following contributions:\n\u2022 We present the largest and most comprehensive study on sociodemographic prompting todate. Concretely, we test the effect of instructing 17 LLMs (covering various model types, e.g., InstructGPT, Flan-T5, etc.) with sociodemographic profiles in a controlled setting which comprises seven datasets reflecting four different subjective NLP classification tasks (sentiment analysis, hatespeech detection, toxicity detection, and stance detection).\n\u2022 We demonstrate ( \u00a75) that sociodemographic prompting leads to surprisingly large amounts of prediction changes (up to 80%), with large variance across model types and sizes.\n\u2022 Our findings ( \u00a76) indicate that sociodemographic prompting helps both to classify annotator-specific decisions and in zero-shot learning with performance improvements up to +8pp in accuracy.\n\u2022 We show ( \u00a77) that sociodemographic prompting is not robust, with large variance due to prompt formulation and model choice.\nOverall, our results provide important insights for future research on sociodemographic prompt-ing, and, in particular, when applying sociodemographic prompting in sensitive scenarios, for instance, in the context of sensitive data annotation or when studying LLM alignment.", "publication_ref": ["b66", "b8", "b51", "b55", "b25", "b22", "b15", "b56", "b27", "b11", "b26", "b1", "b11", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "The sociodemographic background of annotators has been identified as an influential factor in text annotation for subjective NLP tasks (Luo et al., 2020;Biester et al., 2022;Pei and Jurgens, 2023;Santy et al., 2023). Consequently, researchers started to integrate such information into NLP models to enable more socially aware predictions Gordon et al., 2022;Gupta et al., 2023;Fleisig et al., 2023;Wan et al., 2023).\nWith the increasing performance of LLMs, researchers investigated to what extent they are influenced when prompted with sociodemographic information.  examine whether instruction-tuned LLMs accurately reflect or conform to human disagreements but limit their experiments to a single NLI dataset. They conclude that models deviate from human annotators in terms of accuracy and disagreement level. By analyzing disagreement for Q&A, Hwang et al. (2023) find that users' opinions and their sociodemographic background are not mutual predictors. For predicting individual opinions, they show that combining sociodemographic information and relevant past opinions performs best. Several works Santurkar et al., 2023;Santy et al., 2023) analyze LLM's alignment with specific sociodemographic groups and show that model responses are biased towards responses by participants from Western countries. Notably, Santurkar et al. (2023) observe that misalignment persists even after explicitly steering the LMs towards particular demographic groups. Argyle et al. (2023) suggest using GPT-3 as testbed before conducting large-scale population surveys. They propose algorithmic fidelity to evaluate alignment with different human subpopulations and present it as a costefficient proxy for specific human sub-populations in social science research. Finally, it has been shown (Cheng et al., 2023;Deshpande et al., 2023;Ungless et al., 2023;Attanasio et al., 2023) that prompting large models with sociodemographic information is prone to amplify existing stereotypical biases (cf. Blodgett et al., 2020;Barikeri et al., 2021).\nIn contrast to studying LLM alignment, our work extends previous work showing that integrating sociodemographic information in a supervised manner is helpful to improve annotator-specific predictions (Gupta et al., 2023;Fleisig et al., 2023). We use sociodemographic prompting because it allows to diversify model predictions without the need for privacy-concerning data collection of annotators' sociodemographic information. Beyond our objective, our results offer important insights into influential factors for sociodemographic prompting in general.", "publication_ref": ["b39", "b8", "b51", "b57", "b23", "b25", "b22", "b63", "b27", "b56", "b57", "b56", "b1", "b11", "b15", "b62", "b2", "b4", "b25", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Sociodemographic Prompting", "text": "Throughout this work, we prompt a language model with (or without) sociodemographic information for obtaining predictions for the classification tasks we study. In the following, we discuss the main concepts our methodology relies on.\nPrompting. Prompting refers to the act of providing an initial input or cue to a language model, guiding its subsequent output generation. LLMs rely on these prompts to produce contextually relevant and coherent responses.\nSociodemographic Information. Sociodemographic information encompasses data related to the social and demographic characteristics of individuals or groups. Providing sociodemographic cues via prompting has been shown to influence LLM's responses Hwang et al., 2023) through leveraging the sociodemographic knowledge encoded in the models (Lauscher et al., 2022a). The promise is that the output will be tailored to a specific demographic or social group. The present analysis encompasses five distinct sociodemographic attributes, based on the datasets we employ: gender, race, age range, education level, and political affiliation, as detailed in Table 1. In Figure 2, we provide an example of a sociodemographic prompt.\nObtaining Predictions. Our strategy for answer generation differs between closed-source LLMs and open-source alternatives. For open-source models, we follow existing work (Brown et al., 2020;Ye et al., 2023) and prompt the model independently for each possible label by appending the potential answer to the prompt. Then, we evaluate the likelihood associated with each option and select the one with the highest likelihood. In scenarios requiring binary classification, we assign Given a text, how would a person of gender 'Female', race 'White', age '25 -34', education level 'Master's degree' and political affiliation 'Liberal' rate the degree of toxicity in the text. Possible values are 'not toxic', 'slightly toxic', 'moderately toxic', 'very toxic' or 'extremely toxic'. Text: 'Well when you have a welfare state that propagates an underclass of unskilled parasites' Toxicity:\nFigure 2: Sociodemographically enriched prompt to predict the level of toxicity in a text. The different parts of the prompt are highlighted, i.e. instruction, sociodemographic properties and dataset input. Example drawn from the dataset by .\nsemantically coherent descriptors to each label, e.g., \"Yes\" or \"No\" in lieu of 0 or 1 for binary hate speech detection. For closed-source models, we post-process the model output and map it to the predefined label space, to reduce the number of required API calls. In the few cases where this approach fails, we assign it manually.", "publication_ref": ["b27", "b35", "b10", "b73"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Overall Experimental Setup", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Tasks and Datasets", "text": "We select seven datasets for four subjective tasks (toxicity detection, stance detection, hatespeech detection, and sentiment classification) to study sociodemographic prompting across a large and diverse benchmark (cf. Table 2, Appendix A.1). Human annotations for those tasks have been shown to be influenced by sociodemographic factors. Some datasets have been specifically proposed for tuning NLP systems, others have been published to analyze annotator disagreement, which explains the variability in IAA.\nWe have access to the original, un-aggregated annotations for each dataset. To analyze the effect of sociodemographic prompting we additionally require sociodemographic profiles. For two of the datasets (DP, Diaz) we have access to this information and adhere to the original sociodemographic details for prompting. For the remaining five datasets, we adopt the sociodemographic profiles of the annotators of the toxicity dataset DP as a replacement, as done by Wan et al. (2023). In particular, each example gets a set of five profiles, of which each is a composition of different sociodemographic attribute values.\nFor all datasets, we removed instances with in-", "publication_ref": ["b63"], "figure_ref": [], "table_ref": []}, {"heading": "Attribute", "text": "Values (Percentage share)\nGender male (52%), female (47%), nonbinary (<1%) Race White (77%), Black or African American (13%), Asian (6%), Hispanic (3%), Native Hawaiian or Pacific Islander (1%), American Indian or Alaska Native (<1%) Age Under 18 (<1%), 18 -24 (11%), 25 -34 (40%), 35 -44 (25%), 45 -54 (13%), 55 -64 (8%), 65 or older (3%) Education\nLess than high school degree (1%), High school graduate (9%), Some college but no degree (19%), Associate degree in college (2-year) (11%), Bachelors degree in college (4-year) (42%), Masters degree (16%), Professional degree (JD, MD) (2%), Doctoral degree (1%) Political Affiliation Liberal (43%), Conservative (29%), Independent (28%)  complete or unknown information (details in Appendix A.1). Due to the large number of experiments and varying dataset sizes, we randomly sample 1,000 instances from each dataset. Our sampling strategy leads to a similar distribution across the different sociodemographic attributes, as we demonstrate in Appendix A.2. In the following, we describe the individual datasets.\nToxicity. The task is to decide whether or to what degree (e.g., slightly toxic) a text is toxic. We utilize Diverse Perspectives (DP) by , and Jigsaw (Goyal et al., 2022). DP comprises comments from various online forums (e.g., 4chan, Reddit). These comments underwent annotation via Amazon Mechanical Turk, receiving five annotations per instance. For each annotator, the sociodemographic data was gathered. The dataset did not come equipped with a definitive gold label. Thus, we use majority voting to determine the gold label.\nJigsaw encapsulates comments from news articles, originally collated by the Civil Comments platform and subsequently annotated for toxicity indicators. The binary gold label for this dataset was derived by classifying comments as toxic if a majority of annotators identified them as such.\nStance. Stance detection, pertains to discerning an author's viewpoint towards a specific topic (K\u00fc\u00e7\u00fck and Can, 2020;Beck et al., 2021;Lauscher et al., 2022b). As shown by Balahur et al. (2010) and Luo et al. (2020), annotators' decisions are influenced by their sociodemographic background. We employ the SemEval 2016 Task 6 dataset (SE2016; Mohammad et al., 2016) and the Global Warming Stance Detection (GWSD) dataset (Luo et al., 2020).\nSE2016 encompasses 3,591 annotated Twitter posts that address a range of contentious subjects. The gold labels were ascertained using majority voting. Instances exhibiting less than 60% consensus among annotators were excluded by the authors.\nGWSD consists of 2,050 annotated U.S. news articles and was curated to analyze the framing of opinions within the discourse on global warming. To determine the gold label for each article, the authors employed a model tailored to the distribution of annotations, which also factored in potential biases of the annotators.\nHatespeech. Hatespeech detection is a task designed to tackle the increasing amount of hateful online communication. We use the Gabe Hate Corpus (GHC) by Kennedy et al. (2022) and the Twitter Hatespeech Corpus (H-Twitter; Waseem, 2016).\nGHC was sourced from the social network service gab.com and annotated in a multi-label fashion for Human Degradation, Calls For Violence and Vulgar/Offensive. The authors obtained gold labels using majority voting. As we are comparing multi-class tasks, we binarized the annotations into hatespeech indicators (i.e., Yes and No).\nH-Twitter was annotated by CrowdFlower workers for sexism, racism, neither, or both. Expert annotators contributed the gold labels.\nSentiment. The task is to decide upon the sentiment conveyed in the text. We use the dataset by Diaz et al. (2018), which we call Diaz, created for studying age-related bias in sentiment analysis.", "publication_ref": ["b24", "b31", "b6", "b36", "b3", "b39", "b41", "b39", "b30", "b66", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Models", "text": "We seek to instruct models to mimic an annotator with a specific sociodemographic profile. Thus, we resort to the most natural choice, instruction-tuned models. We aim to cover a broad collection of models, both from industrial and academic research and fine-tuned using different instruction-tuning datasets. If possible, we chose model families with several model sizes published. Concretely, we use GPT-3 (Brown et al., 2020), T5 (Raffel et al., 2020), OPT (Zhang et al., 2022), and Pythia (Biderman et al., 2023) model variants. We present a comprehensive overview of all models in Appendix A.3.\nGPT-3. We use InstructGPT (Ouyang et al., 2022) which was fine-tuned using reinforcement learning from human feedback (RLHF).", "publication_ref": ["b10", "b54", "b44", "b7", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "T5. We further use Flan-T5", "text": "(Chung et al., 2022), Flan-UL2 (Tay et al., 2023) and Tk-Instruct . Flan-T5 was trained over a collection of 1,836 finetuning tasks. Flan-UL2 uses the same instruction-tuning procedure but is built on top of a language model which was trained following the Unifying Language Learning Paradigm (UL2) pretraining framework. Flan-T5 (Tk-Instruct ) were trained using large benchmark of 1,836 (1,616) NLP tasks and their natural language instructions.\nOPT. We employ OPT-IML (Iyer et al., 2022) which was fine-tuned using an aggregation of eight instruction-tuning datasets comprising 1,991 tasks. Pythia. We use Dolly-V2 which is fine-tuned on a 15K instruction corpus 2 covering seven task categories.", "publication_ref": ["b60", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "For subjective NLP tasks (Ovesdotter Alm, 2011), comparing aggregated annotations with model predictions provides only a limited view on the performance as label aggregation obscures any disagreement in the data (Prabhakaran et al., 2021;Basile et al., 2021). Thus, we follow Uma et al. (2021) and evaluate our results using both hard-label evaluation (accuracy, macro-averaged F1) and soft-label evaluation (cross-entropy, Jensen-Shannon divergence). In case of hard-label evaluation, we aggregate all predictions obtained via sociodemographic prompting using majority voting. To test for statistical significance of our results, we use generalized linear mixed models (GLMMs) to account for potential confounders and statistical dependencies in our data by jointly modeling numerous main effects (e.g., the impact of model family) and interaction effects (e.g., the joint impact of model family and prompting method). We report further details about our experiments ( \u00a7A.4) and the statistical analysis using GLMMs ( \u00a7A.9) in the Appendix.", "publication_ref": ["b45", "b52", "b5", "b61"], "figure_ref": [], "table_ref": []}, {"heading": "Sensitivity", "text": "We investigate how sensitive the predictions are, i.e., to what extent LLMs' predictions change when instructed to answer from viewpoints characterised by particular sociodemographic backgrounds.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Detailed Setup", "text": "We aggregate all predictions from prompting with different profiles using majority voting. Then, we compare how often the aggregated label differs from the one predicted without any sociodemographic information.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Prompting using sociodemographic profiles leads to prediction changes. In Figure 3, we depict the amount of label prediction changes (in percent) when including sociodemographic information, averaged across all datasets. Several trends can be observed; the degree of prediction changes is both dependent on the model and dataset. Notably, instruction-tuned models based on T5 (Flan-T5, Tk-Instruct) or Pythia (Dolly-V2) are on average more affected by sociodemographic prompting than InstructGPT or variants of OPT-IML . Interestingly, we find that prediction changes are statistically significantly affected by the length of the text instance, i.e. shorter texts are associated with an increased number of prediction changes. Looking at individual datasets (Figure 6) we observe that models are more affected by data from DP and Diaz, with extreme cases where more than 80% of the predictions change when using Dolly-V2 (2.8B). In contrast, hatespeech datasets show less pronounced label shifts.", "publication_ref": [], "figure_ref": ["fig_0", "fig_4"], "table_ref": []}, {"heading": "Model choice and text ambiguity are influential factors", "text": "To better understand the reasons for prediction changes, we aim at analyzing which textual properties lead to prediction changes when prompting with different sociodemographic profiles. We are interested in model-agnostic reasons for sensitivity of sociodemographic prompting. Thus, to draw valid conclusions, we filtered those instances which led to prediction changes across all models tested. Surprisingly, none of the changes are consistent across all models. This indicates that the model choice has a large influence on the prediction outcome, an observation which we inspect closer in \u00a76.2. We further suspect that ambiguity in the text (i.e. disagreement among annotators) might be a reason for prediction changes. We compute the correlation of the disagreement observed in the Table 3: Zero-shot performance when predicting annotator-specific annotations using the original sociodemographic profile. We compare prompting with (SD) and without sociodemographic information and report macro-averaged F1 and Accuracy (Acc). Bold scores highlight the better performance when comparing the same model.\noriginal annotations and among the results using different sociodemographic profiles. We use the variance to mean ratio as our proxy score for the level of disagreement per instance. We observe a weak Kendall's \u03c4 (Kendall, 1938) correlation which is statistically significant (\u03c4 = .07, p < .001).\nCombinations of sociodemographic attributes are more influential than individual attributes in isolation. We investigate the differences between using several sociodemographic attributes and using only one attribute at a time. In general, the combination is most influential, i.e., in 63% of the experiments across models and datasets it leads to most prediction changes. However, there are some dataset-specific effects across different model families where individual attributes have a stronger impact, e.g. race for the GHC corpus (detailed results in Appendix A.5.2).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Performance", "text": "We analyze the impact of sociodemographic prompting on models' zero-shot performances.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Detailed Setup", "text": "First, we evaluate to what extent sociodemographic prompting predicts an annotator-specific annotation with the same sociodemographic profile as provided in the prompt. We study this setup for the datasets where this information is provided (DP, Diaz). Second, we evaluate the performance of sociodemographic prompting by comparing it to the original set of annotations for each dataset. In the following, we present the results for the two overall best-performing models, InstructGPT and OPT-IML , and provide the complete results in Appendix A.6. Note, that statistical analyses were performed on experimental results from all models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Adding sociodemographic information helps reproducing individual annotator decisions. Table 3 demonstrates a positive trend when providing the original sociodemographic profiles. This holds true for larger models (>11B). Most models from other families (Tk-Instruct or Dolly-V2 ) do not outperform random prediction, independent of the prompting method (cf. Table 9). Still, predicting annotator-specific decisions is challenging with more than half of the instances (Accuracy < .5) being incorrectly classified. This is partially due to the datasets' label imbalance, as indicated by the relatively low F1 scores. These results confirm to some extent earlier work stating that sociodemographic information may not provide enough information to explain individual annotation behavior (D\u00edaz et al., 2022;Orlikowski et al., 2023). Interestingly, our statistical analyses show that sociodemographic prompting has a significant interaction effect with input text, i.e. it is more effective for longer input texts.\nSociodemographic prompting can improve zeroshot performance. Table 4 presents the hardlabel and soft-label evaluation results. There is a statistically significant interaction effect of model family and prompting method, identifying InstructGPT as the model which benefits most from sociodemographic prompting and Flan-T5 least. Interestingly, for toxicity detection and sentiment classification, the models benefit from sociodemographic prompting, whereas for stance detection they perform better without such information. We observe a slight trend that datasets for which improvements are observed share low IAA across the original annotations (see Krippendorff's \u03b1 in Table 2). Most notably, using the sociodemographic profiles from the DP dataset can also improve performance for other datasets such as Jigsaw, GHC and GWSD.\nWhen comparing both evaluation setups, the positive effect of sociodemographic prompting is more pronounced for soft-label evaluation. This indicates that, overall, the predictions are more aligned to the original annotations. The results for the other model sizes are provided in Appendix A.6. We observe that multiple model configurations exhibit  weak performance for both setups in general, often without any increasing trend for larger models from the same model family.\nIncreased model sensitivity does not translate to better performance. We test whether the percentage of prediction changes leads to better performance but do not measure any significant correlation (-0.16 Spearman \u03c1, p=0.08) when correlating performance improvement and percentage of prediction changes. We conclude that model sensitivity is not a decisive factor for improvement of zero-shot performance using sociodemographic information.\nModel choice has large influence on label prediction. We established the influence of sociodemographic prompting on model performance but also observe a statistical significant effect of the model family. On average, InstructGPT and OPT-IML variants perform best, while variants of Flan, Tk-Instruct and Dolly-V2 perform significantly worse, independent of model size ( \u00a7A.6). To better understand these differences, we visualize the percentage distribution of label predictions for different experimental settings in Figure 4. Within the same model, we observe minor differences when changing the value of the  sociodemographic attribute. However, the majority of predictions are determined by the choice of model family. Concretely, the predictions of InstructGPT are distributed differently across the label space than those of OPT-IML . A similar picture emerges when the results are compared with other models (cf. \u00a7A.7).\nTo explain this observation, we investigate if our datasets (or parts thereof) are contained in the relevant instruction-tuning datasets and conclude that most models 3 have been exposed to the same tasks which are relevant for our datasets. However, we note that OPT-IML was exposed to the largest number and variety of tasks and datasets during instruction fine-tuning (cf. Table A.3).", "publication_ref": ["b18", "b43"], "figure_ref": [], "table_ref": ["tab_5", "tab_1"]}, {"heading": "Robustness", "text": "Previous work demonstrated that prompting for text classification is influenced by the prompt format (Min et al., 2022). Thus, we investigate whether sociodemographic prompting is robust as indicated by the extent to which predictions change when reformulating the instruction.", "publication_ref": ["b40"], "figure_ref": [], "table_ref": []}, {"heading": "Detailed Setup", "text": "We compare the previous format used to two other formulations of the instruction; a paraphrase (1) and another where we do not provide any explicit instruction but merely present the sociodemographic profile and the input text (2). We provide the exact formulations in Appendix A.8. Importantly, the sociodemographic profile remains the  same between different formats.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Predictions are sensitive to prompt formulation. Table 5 presents the differences in prediction changes (in percent) between the different formats across datasets. Even for semantically equivalent formulations (0,1) prediction differences can rise up to 35% (OPT-IML on Diaz). Using a minimal format leads to the most drastic changes across all datasets, especially pronounced for DP and H-Twitter. Similar effects can be observed for prompting without sociodemographic information. Thus, prediction differences are only partially induced by the sociodemographic profile and confirm previous observations that prompt formulation largely  influences prediction outcomes.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Discussion and Recommendations", "text": "While all LLMs are sensitive to sociodemographic prompting, we identified model scale and the number of instruction-tuning tasks as relevant factors for improving model performance. Toxicity detection and sentiment classification are the tasks which benefit the most from this technique. Further, the model family and prompt formulation have a strong influence on model predictions. Thus, we emphasize that sociodemographic prompting should be used with care, especially in human response simulation  and data annotation (Hartvigsen et al., 2022).\nFrom these findings, we can extract actionable suggestions. First, estimating the degree of sociodemographic \"alignment\" of any LLM should not be merely based on the outcome of prompting with varying sociodemographic profiles. Our work points out the need of a general evaluation framework for studying the sociodemographic alignment of LLMs. Second, if any sociodemographic prompting experiment is conducted, a robustness analysis should accompany the work to evaluate the validity of the findings.\nSociodemographic prompting is effective at modeling disagreement. We also acknowledge potential applications of sociodemographic prompting in the future. Wan et al. (2023) trained a model for disagreement prediction in subjective NLP tasks.Their approach relies on the existence of annotated data alongside the sociodemographic information of the annotators. Thus, we investigate whether we can use sociodemographic prompting as an efficient method to identify instances which will likely result in disagreement during annotation. We compare the original annotations with the result of sociodemographic prompting and calculate a binary F1 score. True positives are instances which received disagreement in both setups. Conversely, true negatives are instances which received no disagreeing votes in both setups.\nWe present the results in Figure 5. Surprisingly, the best-performing zero-shot models ( \u00a76) are not the best at modeling disagreement. With a mean performance of 0.62, Flan-T5 (11B) produces the best and most consistent results across all datasets. This is confirmed by our statistical analysis ( \u00a7A.9 for details). For the two datasets (DP, Diaz) with original sociodemographic information and lowest IAA overall, we observe the best performances across different model sizes. As both datasets induce increased prediction changes ( \u00a75), we hypothesize that sociodemographic prompting is more sensitive if there is larger disagreement in the original annotation. We interpret this as a promising result for using zero-shot sociodemographic prompting to estimate whether a text is likely to induce disagreement among annotators. We leave the exploration of integrating additional training signals (e.g., few-shot) as future work.", "publication_ref": ["b26", "b63"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Conclusion", "text": "We study sociodemographic prompting for subjective NLP tasks and employ a comprehensive study across seven datasets and seven instruction-tuned LLMs from different model families. Our results show that these models are sensitive to sociodemographic prompting and using this technique can improve zero-shot performance.\nHowever, we also observe a strong influence of the prompt formulation and model family. Thus, we argue that sociodemographic prompting should be used with care in sensitive applications and requires comprehensive evaluation when used for data annotation or studying sociodemographic alignment of language models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "In the following, we provide an examination of the inherent limitations associated with this research study. We further note that all our experiments have been approved by the local ethics review process of the Business School of the University of Hamburg. This process is compliant to obtaining approval from an Institutional Review Board.\nAnnotations go beyond sociodemographics. While annotators' sociodemographic backgrounds have been shown to be influential in their decisionmaking process (Al Kuwatly et al., 2020;Excell and Al Moubayed, 2021;Shen and Rose, 2021;Larimore et al., 2021;, inter alia), it is not a definitive predictive factor as individual lived experiences (Waseem, 2016) or situated domain expertise (Patton et al., 2019) can influence annotation decisions, too. In short, collective group behavior may not always provide an explanation for individual behavior (D\u00edaz et al., 2022;Orlikowski et al., 2023). While our general approach can be extended to a wider range of sociodemographic attributes or even descriptions of individuals, we refrained from testing more to contain the complexity of our study and due to the limited availability of such resources. We welcome efforts to increase the availability of such information alongside the datasets, e.g., Crowdworksheets by D\u00edaz et al. (2022), and hope to see more work in future exploring prompting large language models with more dimensions of sociodemographic and personal information.\nSociodemographic profiles are not representative. It is important to acknowledge certain limitations with regard to the representation of sociodemographic profiles. First, all the datasets employed in our research are exclusively in English language, mostly due to the lack of resources in other languages. This linguistic restriction inherently limits our ability to make comprehensive cross-linguistic assessments. Second, the sociodemographic information provided by the annotators of the datasets used in this study adheres to a classification system specific to the United States. Consequently, our findings cannot be generalized to sociodemographic data originating from other nations, linguistic communities, or cultural contexts. These limitations underscore the need for caution when extrapolating our results to broader sociodemographic contexts beyond the scope of our study.\nWe cannot model all factors influencing prompting outcomes. We demonstrate that model predictions can effectively be changed when incorporating sociodemographic information within the prompt ( \u00a75). However, we acknowledge that this is one among many of the factors influencing model predictions in a zero-shot prompting setup. We account for the influence of prompt formulation by investigating its effect in \u00a77 and are aware of the growing body of work investigating various other factors which influence prompting results, such as correct label assignment (Min et al., 2022), domain-specific vocabulary (Fei et al., 2023) or example order (Kumar and Talukdar, 2021;Zhao et al., 2021;Lu et al., 2022).\nThe majority of these works deals with incontext learning or few-shot learning in general which we do not investigate in this study. However, we see these phenomena as support for our overall argument ( \u00a77) that estimating the degree of alignment of any LLM should not be merely based on the outcome of prompting with varying sociodemographic profiles. This is due to their lack of robustness when changing the surface form of the prompt while keeping its semantic meaning similar.\nSimulating annotations using prompting mechanisms is limited. Employing humans for annotation projects in NLP is a multi-step process. It involves the formulation of annotation guidelines and their iterative refinement through discussions between the annotators and coordinators. In most cases, annotators are undergoing a qualification process or test to evaluate their eligibility for contributing to the annotations. These factors influence the decision-making process of the annotators and ultimately the annotation agreement. In our experimental setup, we do not provide any additional instructions to the model than the prompt instructions which we present in \u00a73 and \u00a7A.8, thus possibly underspecifying the task instruction to the model. Our experimental setup is designed driven by the following observations; for most datasets, the original annotation guidelines are non-retrievable and could only be guessed from the description in the corresponding research publication. Further, LLMs are limited with regards to the context input size (see Table A.3 for details) and using longer prompts would have limited our experiments to a few models with appropriate input sizes.", "publication_ref": ["b0", "b59", "b34", "b66", "b18", "b43", "b18", "b40", "b21", "b33", "b75", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "A Appendix", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Dataset details", "text": "Toxicity -DP. The DP dataset comprises comments extracted from various online forums, including Twitter, 4chan, and Reddit, spanning from December 2019 to August 2020. These comments underwent annotation via Amazon Mechanical Turk, receiving five annotations per instance. Sociodemographic data of the annotators was gathered, contingent upon the approval of the pertinent Institutional Review Board (IRB). The dataset did not come equipped with a definitive gold label. Therefore, we instituted a majority voting mechanism, leveraging the raw annotations to ascertain the gold label. We exclude instances wherein selected sociodemographic attributes received responses such as \"Prefer not to say\", instances with multiple selections for the race attribute, and any attributes marked with generic designations like Other or Unknown. This reduced the initial dataset size from 107,620 to 55,364 instances.\nToxicity -Jigsaw. We filtered all instances where unaggregated annotations were missing, which reduced the dataset size from 1,999,516 to 1,804,874 instances.\nHatespech -GHC. We filtered all instances where no gold annotation was provided. Thus, the initial dataset size was reduced from 27,553 to 27,434.\nHatespeech -H-Twitter. For H-Twitter, none of 6,909 instances were filtered.\nStance Detection -SE2016. We only considered instances were both the aggregated and the complete set of unaggregated annotations were available. In addition, we removed the hashtag '#SemST' which was artificially added by the dataset authors. The dataset we used consists of 3,591 instances.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Stance -GWSD.", "text": "It is composed of a subset of 2,050 annotated articles, extracted from a larger pool of 56,000 articles on global warming. These articles were published between January 1, 2000, and April 12, 2020, by 63 distinct U.S. news outlets. After filtering instances where no annotations were provided and removing duplicates, 2,042 instances remained of the initial 2,050.\nSentiment -Diaz. The Diaz dataset is the second dataset where sociodemographic data of the annotators was gathered. We only considered the sociodemographic attributes which were also used in the DP dataset. To remain comparability, we convert the original 5-point answer for political affiliations into a 3-scale by mapping 'Somewhat conservative ' and 'Very conservative' to 'Conservative' and 'Somewhat liberal' and 'Very liberal' to 'Liberal'. We filtered all instances where annotators replied with Other for the attribute race. The dataset which was used for sampling contains 14,071 instances.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Sampling Strategy", "text": "Due to the large number of experiments and varying dataset sizes, we first randomly sample 1,000 instances from each dataset. Here, the label distribution of the samples remained comparable to the corresponding full dataset distribution. To conduct our sociodemographic prompting experiments, we used the original sociodemographic profiles for DP and Diaz datasets while transferring the sociodemographic profiles of the DP samples to the remaining datasets (where no sociodemographic info about the annotators is provided). Thus, the sociodemographic profiles (and their respective distribution) which we use for datasets DP, Jigsaw, GHC, H-Twitter, SE2016, and GWSD are identical (five per instance). We display their distribution (Table 6) and compare it to the distribution of the full DP dataset. As can be seen from the table, both remain comparable.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_9"]}, {"heading": "A.3 Model Details", "text": "We provide an overview of all models, their size in terms of number of parameters and their context window size in Table 7. For models with parameters less than 3B (i.e. Flan-T5 80M-3B, Tk-Instruct 80M-3B, OPT-IML 3B and Dolly-V2 2.8B) no inference optimization was applied. We used 8-bit optimization for all other models.\nLicenses Models from Flan-T5 , Flan-UL2 , and Tk-Instruct are published using an Apache License 2.0. OPT-IML -based models are published with the OPT-IML 175B LI-CENSE AGREEMENT which allows usage for non-commercial research purposes. Dolly-V2 is distributed under MIT license.   ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_10"]}, {"heading": "A.4 Experimental Details", "text": "All experiments were conducted using Py-Torch (Paszke et al., 2019) 2.0.1, Huggingface Transformers (Wolf et al., 2020) 4.28.1 and CUDA (Nickolls et al., 2008) 11.8 on a computation cluster with a combination of A100 (40GB), A180 (80GB) and H100PCIE (80GB) GPU cards. Depending on the dataset and the GPU in use, we used batch sizes ranging from 4 to 32. We used 8-bit optimization (Dettmers et al., 2022) for models with parameter numbers larger than 3B. For prompting InstructGPT , we used both the Ope-nAI API and Microsoft Azure API.", "publication_ref": ["b67", "b42", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "A.5 Model Sensitivity", "text": "Here, we provide more details regarding the experiments to analyze the sensitivity of instruction-tuned language models when prompted with (or without) sociodemographic information.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.5.1 Prediction changes per model and dataset", "text": "In Figure 6 we display the degree of prediction changes for the different models and datasets. ", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "A.5.2 Prediction changes per sociodemographic attribute", "text": "For better overview of the attributes which lead to the most prediction changes, we aggregated the most influential attributes in Table 8. It can be seen that a combination of all sociodemographic attributes leads to the most substantial changes for most of the datasets (64%).\nA.6 Zero-Shot Prompting using sociodemographic information", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_12"]}, {"heading": "A.6.1 Predicting annotator-specific annotations", "text": "As extension to the results provided in \u00a76, in Table 9 we provide the results for all models when predicting an annotator-specific annotation with the same sociodemographic profile as provided in the prompt.\nThe largest models in our experiments (20B, 30B, 175B) all benefit from integrating the sociodemographic information provided with the original annotation. However, for smaller models there is no consistent trend of improvement observable. Interestingly, most models from instructiontuned model families based on T5 (Flan-T5 , Tk-Instruct ) and Pythia (Dolly-V2 ) are not able to outperform random guessing. This is independent of the model size.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.6.2 Predicting aggregated annotations", "text": "The complete list of results for zero-shot prompting using sociodemographic profiles is provided in Table 10 (hard evaluation) and Table 11 (soft evaluation). The good performance of models from the Tk-Instruct family on the Jigsaw dataset is most likely due to the dataset being present in the dataset which was used for instruction-finetuning . The authors report toxic language detection with 40 datasets being one of the most prominent tasks among all. Some of the results for GWSD can be explained in a similar vein as stance detection has been part of the instructiontuning tasks present in the dataset.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0", "tab_0"]}, {"heading": "A.7 Influence of model choice on prediction outcomes", "text": "We provide more detailed model comparisons of prediction outcomes in Figure 7 for the DP dataset and in Figure 8 for the SE2016 dataset, respectively. For both datasets and the sociodemographic attributes tested (Gender for DP, Political Affiliation for SE2016), we observe that the model choice has a larger influence on the label prediction than the value of the sociodemographic attribute.", "publication_ref": [], "figure_ref": ["fig_5", "fig_7"], "table_ref": []}, {"heading": "A.8 Robustness Analysis", "text": "Several works demonstrated the brittleness of zeroshot and few-shot predictions of language models (Min et al., 2022) due to factors such as the prompt format or the order of the in-context examples (Zhao et al., 2021;Lu et al., 2022). Thus, we evaluate if our results are subject to such variations, by repeating our experiments using three different prompt formulations. The prompt formulations are displayed in Table 12. The complete results are shown in Figure 9 (with sociodemographic information) and Figure 10 (without sociodemographic information). We observe that most differences are induced when comparing the prompt using a minimal formulation (format 2) with the more elaborate versions (format 0,1). Structurally, we see that prompting both with and without sociodemographic information is affected by the prompt formulation to a large extent.", "publication_ref": ["b40", "b75", "b38"], "figure_ref": ["fig_8", "fig_9"], "table_ref": ["tab_0"]}, {"heading": "A.9 GLMM Analyses", "text": "In addition to the reported percentages of label changes (Figure 3), classification performance measures (Tables 3 and 4), and disagreement prediction performance (Figure 5), we conduct statistical analyses using generalized linear mixed models (GLMMs). GLMMs allow us to statistically account for various fixed and random effects and, thereby, account for potential confounders and statistical dependencies in our data. We thus fit four GLMs/GLMMs for (i) the model sensitivity experiment (Section 5), (ii) the prediction of individual (original) annotations (Section 6), (iii) the prediction of aggregated annotations (Section 6), and (iv) the prediction of ambiguous instances (??). We model the respective binary label changes, classification accuracies, and disagreement prediction accuracies on an instance level across datasets. We use R (R Core Team, 2023) and mgcv 1.8-42 (Wood, 2011;Wood et al., 2016;Wood, 2004Wood, , 2017Wood, , 2003 to fit all our models.", "publication_ref": ["b70", "b72", "b69", "b71", "b68"], "figure_ref": ["fig_0", "fig_3"], "table_ref": ["tab_5"]}, {"heading": "A.9.1 Model Specifications", "text": "We fit binomial models (logit link) and include the factors (a) model family (e.g., Flan-T5), (b) model size (logarithmic), (c) task (e.g., sentiment analysis), (d) text length in characters (i.e., length   of the text to be classified), and (e) additional prompt length in characters (i.e., length of the entire prompt excluding the text to be classified) as fixed effects for all models. We additionally model the specific dataset as a random effect to account for structural dependencies stemming from the choice of dataset for all models except the (second) model that predicts individual annotations as, for this experiment, there only is one dataset per task. In the following, we specify the predictor variable and additional covariates for the four models.\nSensitivity Model. We model the probability of a label change between standard and SD prompting.\nIndividual Annotations Model. We model the probability of a model predicting the correct    (individually-annotated) class label. We additionally include the prompting method used (standard or sociodemographic prompting) as a fixed effect.\nTo gain further insights on the interaction of using sociodemographic prompting with model size, family, or text length, we additionally model all pairwise interactions between prompting method and the fixed effects listed above.\nAggregated Annotations Model. In this model, we predict the probability of a model predicting the correct aggregated label. We include the same model terms as the previous model and additionally include a random effect of dataset.\nAmbiguity Model. We model the probability of successfully predicting annotator disagreement as the predictor variable. The included fixed and random effects are identical to the sensitivity model.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.9.2 Results", "text": "Sensitivity Model. Individual Annotations Model. Table 15 displays the test statistics regarding the parametric fixed terms of the individual annotation prediction model. As, in contrast to the sensitivity model, that modelled the effect of SD prompting within the predictor variable, this model includes SD prompting as a covariate, we focus on effects involving the prompting method variable. 4 While we do not find a significant main effect of using SD prompting, we observe several interaction effects of SD prompting. Concretely, we observe a statistically significant negative interaction effect of model size and SD prompting (\u03b2=-0.12, 95% CI [-0.15, -0.09], p<0.001), a statistically significant positive interaction effect of text length and SD prompting (\u03b2=6.95e-04, 95% CI [4.91e-04, 8.99e-04], p<0.001), and a statistically signif-    ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Acknowledgements", "text": "We thank Max Glockner, Hovhannes Tamoyan and Anmoel Goel for their feedback on an early draft of this work and the authors of the datasets we used for providing them publicly. We gratefully acknowledge the support of Microsoft with a grant for access to OpenAI GPT   ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Table 9: Zero-shot performance when predicting annotator-specific annotations using the original sociodemographic profile. We compare prompting with (SD) and without sociodemographic information and report macro-averaged F1 and Accuracy (Acc). Bold scores highlight the better performance when comparing the same model.\nicant negative interaction effect of model family and SD prompting (\u03c7 2 (5)=561.33, p<0.001  ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Identifying and measuring annotator bias based on annotators' demographic characteristics", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Maximilian Hala Al Kuwatly; Georg Wich;  Groh"}, {"ref_id": "b1", "title": "Out of one, many: Using language models to simulate human samples", "journal": "Political Analysis", "year": "2023", "authors": "Lisa P Argyle; Ethan C Busby; Nancy Fulda; Joshua R Gubler; Christopher Rytting; David Wingate"}, {"ref_id": "b2", "title": "A tale of pronouns: Interpretability informs gender bias mitigation for fairer instruction-tuned machine translation", "journal": "", "year": "2023", "authors": "Giuseppe Attanasio; Flor Plaza Del Arco; Debora Nozza; Anne Lauscher"}, {"ref_id": "b3", "title": "Sentiment analysis in the news", "journal": "", "year": "2010", "authors": "Alexandra Balahur; Ralf Steinberger; Mijail Kabadjov; Vanni Zavarella; Erik Van Der Goot; Matina Halkia; Bruno Pouliquen; Jenya Belyaeva"}, {"ref_id": "b4", "title": "RedditBias: A real-world resource for bias evaluation and debiasing of conversational language models", "journal": "Long Papers", "year": "2021", "authors": "Soumya Barikeri; Anne Lauscher; Ivan Vuli\u0107; Goran Glava\u0161"}, {"ref_id": "b5", "title": "We need to consider disagreement in evaluation", "journal": "", "year": "2021", "authors": "Valerio Basile; Michael Fell; Tommaso Fornaciari; Dirk Hovy; Silviu Paun"}, {"ref_id": "b6", "title": "Investigating label suggestions for opinion mining in German covid-19 social media", "journal": "", "year": "2021", "authors": "Tilman Beck; Ji-Ung Lee; Christina Viehmann; Marcus Maurer; Oliver Quiring; Iryna Gurevych"}, {"ref_id": "b7", "title": "Pythia: A suite for analyzing large language models across training and scaling", "journal": "PMLR", "year": "2023", "authors": "Stella Biderman; Hailey Schoelkopf; Quentin Gregory Anthony; Herbie Bradley; O' Kyle; Eric Brien; Mohammad Aflah Hallahan; Shivanshu Khan;  Purohit; Edward Usvsn Sai Prashanth;  Raff"}, {"ref_id": "b8", "title": "Analyzing the effects of annotator gender across NLP tasks", "journal": "", "year": "2022", "authors": "Laura Biester; Vanita Sharma; Ashkan Kazemi; Naihao Deng; Steven Wilson; Rada Mihalcea"}, {"ref_id": "b9", "title": "Language (technology) is power: A critical survey of \"bias\" in NLP", "journal": "", "year": "2020", "authors": " Su Lin; Solon Blodgett; Hal Barocas; Iii Daum\u00e9; Hanna Wallach"}, {"ref_id": "b10", "title": "Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners", "journal": "", "year": "2020-12-06", "authors": "Tom B Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel M Ziegler; Jeffrey Wu; Clemens Winter; Christopher Hesse; Mark Chen; Eric Sigler; Mateusz Litwin"}, {"ref_id": "b11", "title": "Marked personas: Using natural language prompts to measure stereotypes in language models", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Myra Cheng; Esin Durmus; Dan Jurafsky"}, {"ref_id": "b12", "title": "", "journal": "", "year": "2022", "authors": " Hyung Won; Le Chung; Shayne Hou; Barret Longpre; Yi Zoph; William Tay; Eric Fedus; Xuezhi Li; Mostafa Wang; Siddhartha Dehghani; Albert Brahma;  Webson; Shane Shixiang; Zhuyun Gu; Mirac Dai; Xinyun Suzgun; Aakanksha Chen; Sharan Chowdhery; Gaurav Narang; Adams Mishra; Vincent Y Yu; Yanping Zhao; Andrew M Huang; Hongkun Dai; Slav Yu; Ed H Petrov; Jeff Chi; Jacob Dean; Adam Devlin; Denny Roberts; Quoc V Zhou; Jason Le;  Wei"}, {"ref_id": "b13", "title": "", "journal": "", "year": "", "authors": " Corr"}, {"ref_id": "b14", "title": "Hate speech classifiers learn normative social stereotypes", "journal": "Transactions of the Association for Computational Linguistics", "year": "2023", "authors": "Aida Mostafazadeh Davani; Mohammad Atari; Brendan Kennedy; Morteza Dehghani"}, {"ref_id": "b15", "title": "Toxicity in chatgpt: Analyzing persona-assigned language models", "journal": "", "year": "2023", "authors": "Ameet Deshpande; Vishvak Murahari; Tanmay Rajpurohit; Ashwin Kalyan; Karthik Narasimhan"}, {"ref_id": "b16", "title": "2022. 8-bit optimizers via block-wise quantization", "journal": "", "year": "2022", "authors": "Tim Dettmers; Mike Lewis; Sam Shleifer; Luke Zettlemoyer"}, {"ref_id": "b17", "title": "Addressing agerelated bias in sentiment analysis", "journal": "ACM", "year": "2018-04-21", "authors": "Mark Diaz; Isaac Johnson; Amanda Lazar; Anne Marie Piper; Darren Gergle"}, {"ref_id": "b18", "title": "Crowdworksheets: Accounting for individual and collective identities underlying crowdsourced dataset annotation", "journal": "ACM", "year": "2022-06-21", "authors": "Mark D\u00edaz; Ian D Kivlichan; Rachel Rosen; Dylan K Baker; Razvan Amironesei"}, {"ref_id": "b19", "title": "Towards measuring the representation of subjective global opinions in language models", "journal": "", "year": "", "authors": "Esin Durmus; Karina Nyugen; Thomas I Liao; Nicholas Schiefer; Amanda Askell; Anton Bakhtin; Carol Chen; Zac Hatfield-Dodds; Danny Hernandez; Nicholas Joseph; Liane Lovitt; Sam Mccandlish; Orowa Sikder"}, {"ref_id": "b20", "title": "Towards equal gender representation in the annotations of toxic language detection", "journal": "Online. Association for Computational Linguistics", "year": "", "authors": ""}, {"ref_id": "b21", "title": "Mitigating label biases for in-context learning", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Yu Fei; Yifan Hou; Zeming Chen; Antoine Bosselut"}, {"ref_id": "b22", "title": "When the majority is wrong: Modeling annotator disagreement for subjective tasks", "journal": "", "year": "2023", "authors": "Eve Fleisig; Rediet Abebe; Dan Klein"}, {"ref_id": "b23", "title": "Jury learning: Integrating dissenting voices into machine learning models", "journal": "", "year": "2022", "authors": "Mitchell L Gordon; Michelle S Lam; Joon Sung Park; Kayur Patel; Jeff Hancock; Tatsunori Hashimoto; Michael S Bernstein"}, {"ref_id": "b24", "title": "Is your toxicity my toxicity? exploring the impact of rater identity on toxicity annotation", "journal": "Proc. ACM Hum. Comput. Interact", "year": "2022", "authors": "Nitesh Goyal; Ian D Kivlichan; Rachel Rosen; Lucy Vasserman"}, {"ref_id": "b25", "title": "Same same, but different: Conditional multi-task learning for demographicspecific toxicity detection", "journal": "Association for Computing Machinery", "year": "2023", "authors": "Soumyajit Gupta; Sooyong Lee; Maria De-Arteaga; Matthew Lease"}, {"ref_id": "b26", "title": "ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Thomas Hartvigsen; Saadia Gabriel; Hamid Palangi; Maarten Sap; Dipankar Ray; Ece Kamar"}, {"ref_id": "b27", "title": "Aligning language models to user opinions", "journal": "", "year": "2023", "authors": "Eunjeong Hwang; Bodhisattwa Majumder; Niket Tandon"}, {"ref_id": "b28", "title": "OPT-IML: scaling language model instruction meta learning through the lens of generalization", "journal": "CoRR", "year": "2022", "authors": "Srinivasan Iyer; Xi Victoria Lin; Ramakanth Pasunuru; Todor Mihaylov; Daniel Simig; Ping Yu; Kurt Shuster; Tianlu Wang; Qing Liu; Punit Singh Koura; Xian Li; O' Brian; Gabriel Horo; Jeff Pereyra; Christopher Wang; Asli Dewan; Luke Celikyilmaz; Ves Zettlemoyer;  Stoyanov"}, {"ref_id": "b29", "title": "A new measure of rank correlation", "journal": "Biometrika", "year": "1938", "authors": "G Maurice;  Kendall"}, {"ref_id": "b30", "title": "Introducing the gab hate corpus: defining and applying hate-based rhetoric to social media posts at scale. Language Resources and Evaluation", "journal": "", "year": "2022", "authors": "Brendan Kennedy; Mohammad Atari; Aida Mostafazadeh Davani; Leigh Yeh; Ali Omrani; Yehsong Kim; Kris Coombs; Shreya Havaldar; Gwenyth Portillo-Wightman; Elaine Gonzalez"}, {"ref_id": "b31", "title": "Stance detection: A survey", "journal": "ACM Comput. Surv", "year": "2020", "authors": "Dilek K\u00fc\u00e7\u00fck; Fazli Can"}, {"ref_id": "b32", "title": "Designing toxic content classification for a diversity of perspectives", "journal": "", "year": "2021", "authors": "Deepak Kumar; Patrick Gage Kelley; Sunny Consolvo; Joshua Mason; Elie Bursztein; Zakir Durumeric; Kurt Thomas; Michael Bailey"}, {"ref_id": "b33", "title": "Reordering examples helps during priming-based few-shot learning", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Sawan Kumar; Partha Talukdar"}, {"ref_id": "b34", "title": "Reconsidering annotator disagreement about racist language: Noise or signal?", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Savannah Larimore; Ian Kennedy; Breon Haskett; Alina Arseniev-Koehler"}, {"ref_id": "b35", "title": "SocioProbe: What, when, and where language models learn about sociodemographics", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Anne Lauscher; Federico Bianchi; Samuel R Bowman; Dirk Hovy"}, {"ref_id": "b36", "title": "Scientia potentia Est-On the role of knowledge in computational argumentation", "journal": "Transactions of the Association for Computational Linguistics", "year": "2022", "authors": "Anne Lauscher; Henning Wachsmuth; Iryna Gurevych; Goran Glava\u0161"}, {"ref_id": "b37", "title": "Can large language models capture dissenting human voices?", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Noah Lee; Na Min An; James Thorne"}, {"ref_id": "b38", "title": "Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity", "journal": "Long Papers", "year": "2022", "authors": "Yao Lu; Max Bartolo; Alastair Moore; Sebastian Riedel; Pontus Stenetorp"}, {"ref_id": "b39", "title": "Detecting stance in media on global warming", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Yiwei Luo; Dallas Card; Dan Jurafsky"}, {"ref_id": "b40", "title": "Rethinking the role of demonstrations: What makes in-context learning work?", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Sewon Min; Xinxi Lyu; Ari Holtzman; Mikel Artetxe; Mike Lewis; Hannaneh Hajishirzi; Luke Zettlemoyer"}, {"ref_id": "b41", "title": "SemEval-2016 task 6: Detecting stance in tweets", "journal": "", "year": "2016", "authors": "Saif Mohammad; Svetlana Kiritchenko; Parinaz Sobhani; Xiaodan Zhu; Colin Cherry"}, {"ref_id": "b42", "title": "Scalable parallel programming with cuda: Is cuda the parallel programming model that application developers have been waiting for?", "journal": "Queue", "year": "2008", "authors": "John Nickolls; Ian Buck; Michael Garland; Kevin Skadron"}, {"ref_id": "b43", "title": "The ecological fallacy in annotation: Modeling human label variation goes beyond sociodemographics", "journal": "Short Papers", "year": "2023", "authors": "Matthias Orlikowski; Paul R\u00f6ttger; Philipp Cimiano; Dirk Hovy"}, {"ref_id": "b44", "title": "Training language models to follow instructions with human feedback", "journal": "", "year": "2022-01", "authors": "Long Ouyang; Jeffrey Wu; Xu Jiang; Diogo Almeida; Carroll L Wainwright; Pamela Mishkin; Chong Zhang; Sandhini Agarwal; Katarina Slama; Alex Ray; John Schulman; Jacob Hilton; Fraser Kelton; Luke Miller; Maddie Simens; Amanda Askell; Peter Welinder; Paul F Christiano"}, {"ref_id": "b45", "title": "Subjective natural language problems: Motivations, applications, characterizations, and implications", "journal": "Association for Computational Linguistics", "year": "2011", "authors": "Cecilia Ovesdotter; Alm "}, {"ref_id": "b46", "title": "Social simulacra: Creating populated prototypes for social computing systems", "journal": "ACM", "year": "2022-10-29", "authors": "Joon Sung Park; Lindsay Popowski; Carrie J Cai; Meredith Ringel Morris; Percy Liang; Michael S Bernstein"}, {"ref_id": "b47", "title": "", "journal": "", "year": "", "authors": "Adam Paszke; Sam Gross; Francisco Massa"}, {"ref_id": "b48", "title": "", "journal": "", "year": "", "authors": "Zeming Killeen; Natalia Lin; Luca Gimelshein;  Antiga"}, {"ref_id": "b49", "title": "Pytorch: An imperative style, high-performance deep learning library", "journal": "", "year": "2019-12-08", "authors": "Zachary Yang; Martin Devito; Alykhan Raison; Sasank Tejani; Benoit Chilamkurthy; Lu Steiner; Junjie Fang; Soumith Bai;  Chintala"}, {"ref_id": "b50", "title": "Annotating twitter data from vulnerable populations: Evaluating disagreement between domain experts and graduate student annotators", "journal": "", "year": "2019", "authors": "Philipp Desmond Upton Patton;  Blandfort; R William;  Frey; B Michael; Svebor Gaskell;  Karaman"}, {"ref_id": "b51", "title": "When do annotator demographics matter? measuring the influence of annotator demographics with the POPQUORN dataset", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Jiaxin Pei; David Jurgens"}, {"ref_id": "b52", "title": "On releasing annotator-level labels and information in datasets", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Aida Mostafazadeh Vinodkumar Prabhakaran; Mark Davani;  Diaz"}, {"ref_id": "b53", "title": "R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing", "journal": "", "year": "2023", "authors": " R Core Team"}, {"ref_id": "b54", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "The Journal of Machine Learning Research", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b55", "title": "Two contrasting data annotation paradigms for subjective NLP tasks", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Paul Rottger; Bertie Vidgen; Dirk Hovy; Janet Pierrehumbert"}, {"ref_id": "b56", "title": "Whose opinions do language models reflect?", "journal": "PMLR", "year": "2023-07", "authors": "Shibani Santurkar; Esin Durmus; Faisal Ladhak; Cinoo Lee; Percy Liang; Tatsunori Hashimoto"}, {"ref_id": "b57", "title": "NLPositionality: Characterizing design biases of datasets and models", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Sebastin Santy; Jenny Liang; Katharina Ronan Le Bras; Maarten Reinecke;  Sap"}, {"ref_id": "b58", "title": "Annotators with attitudes: How annotator beliefs and identities bias toxic language detection", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Maarten Sap; Swabha Swayamdipta; Laura Vianna; Xuhui Zhou; Yejin Choi; Noah A Smith"}, {"ref_id": "b59", "title": "What sounds \"right\" to me? experiential factors in the perception of political ideology", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Qinlan Shen; Carolyn Rose"}, {"ref_id": "b60", "title": "UL2: unifying language learning paradigms", "journal": "", "year": "2023-05-01", "authors": "Yi Tay; Mostafa Dehghani; Q Vinh; Xavier Tran; Jason Garcia; Xuezhi Wei;  Wang;  Hyung Won; Dara Chung; Tal Bahri; Huaixiu Schuster; Denny Steven Zheng; Neil Zhou; Donald Houlsby;  Metzler"}, {"ref_id": "b61", "title": "Learning from disagreement: A survey", "journal": "Journal of Artificial Intelligence Research", "year": "2021", "authors": "Alexandra N Uma; Tommaso Fornaciari; Dirk Hovy; Silviu Paun; Barbara Plank; Massimo Poesio"}, {"ref_id": "b62", "title": "Stereotypes and smut: The (mis)representation of non-cisgender identities by text-to-image models", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Eddie Ungless; Bjorn Ross; Anne Lauscher"}, {"ref_id": "b63", "title": "Everyone's voice matters: Quantifying annotation disagreement using demographic information", "journal": "AAAI Press", "year": "2023-02-07", "authors": "Ruyuan Wan; Jaehyung Kim; Dongyeop Kang"}, {"ref_id": "b64", "title": "Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran", "journal": "Pulkit Verma", "year": "", "authors": "Yizhong Wang; Swaroop Mishra; Pegah Alipoormolabashi; Yeganeh Kordi; Amirreza Mirzaei"}, {"ref_id": "b65", "title": "Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks", "journal": "", "year": "2022", "authors": "Ravsehaj Singh Puri; Rushang Karia; Savan Doshi; Keyur Shailaja; Siddhartha Sampat; Sujan Mishra; A Reddy; Sumanta Patro; Tanay Dixit; Xudong Shen"}, {"ref_id": "b66", "title": "Are you a racist or am I seeing things? annotator influence on hate speech detection on Twitter", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Zeerak Waseem"}, {"ref_id": "b67", "title": "Transformers: State-of-the-art natural language processing", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; Remi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander Lhoest;  Rush"}, {"ref_id": "b68", "title": "Thin-plate regression splines", "journal": "Journal of the Royal Statistical Society (B)", "year": "2003", "authors": "S N Wood"}, {"ref_id": "b69", "title": "Stable and efficient multiple smoothing parameter estimation for generalized additive models", "journal": "Journal of the American Statistical Association", "year": "2004", "authors": "S N Wood"}, {"ref_id": "b70", "title": "Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models", "journal": "Journal of the Royal Statistical Society (B)", "year": "2011", "authors": "S N Wood"}, {"ref_id": "b71", "title": "Generalized additive models: an introduction with R", "journal": "CRC press", "year": "2017", "authors": "N Simon;  Wood"}, {"ref_id": "b72", "title": "Smoothing parameter and model selection for general smooth models (with discussion)", "journal": "Journal of the American Statistical Association", "year": "2016", "authors": "S N Wood; N Pya; B S\u00e4fken"}, {"ref_id": "b73", "title": "Compositional exemplars for in-context learning", "journal": "PMLR", "year": "2023-07", "authors": "Jiacheng Ye; Zhiyong Wu; Jiangtao Feng; Tao Yu; Lingpeng Kong"}, {"ref_id": "b74", "title": "", "journal": "", "year": "", "authors": "Susan Zhang; Stephen Roller; Naman Goyal; Mikel Artetxe; Moya Chen; Shuohui Chen; Christopher Dewan; Mona T Diab; Xian Li; Xi Victoria Lin; Todor Mihaylov; Myle Ott; Sam Shleifer"}, {"ref_id": "b75", "title": "Calibrate before use: Improving few-shot performance of language models", "journal": "", "year": "2021", "authors": "Zihao Zhao; Eric Wallace; Shi Feng; Dan Klein; Sameer Singh"}, {"ref_id": "b76", "title": "", "journal": "", "year": "", "authors": "Format Dataset"}, {"ref_id": "b77", "title": "", "journal": "", "year": "", "authors": "Dolly-V2 "}, {"ref_id": "b78", "title": "", "journal": "", "year": "", "authors": "Dolly-V2 "}, {"ref_id": "b79", "title": "", "journal": "", "year": "", "authors": "Dolly-V2 "}, {"ref_id": "b80", "title": "", "journal": "", "year": "", "authors": "Dolly "}, {"ref_id": "b81", "title": "", "journal": "", "year": "2001", "authors": " Flan-Ul"}, {"ref_id": "b82", "title": "", "journal": "", "year": "", "authors": " Instructgpt"}, {"ref_id": "b83", "title": "", "journal": "Tk-Instruct", "year": "", "authors": ""}, {"ref_id": "b84", "title": "", "journal": "Tk-Instruct", "year": "", "authors": ""}, {"ref_id": "b85", "title": "", "journal": "Tk-Instruct", "year": "", "authors": ""}, {"ref_id": "b86", "title": "", "journal": "Tk-Instruct", "year": "", "authors": ""}, {"ref_id": "b87", "title": "", "journal": "Tk-Instruct", "year": "", "authors": ""}, {"ref_id": "b88", "title": "", "journal": "Tk-Instruct", "year": "", "authors": ""}, {"ref_id": "b89", "title": "", "journal": "Tk-Instruct", "year": "", "authors": ""}, {"ref_id": "b90", "title": "", "journal": "Tk-Instruct", "year": "", "authors": ""}, {"ref_id": "b91", "title": "", "journal": "Tk-Instruct", "year": "", "authors": ""}, {"ref_id": "b92", "title": "", "journal": "Tk-Instruct", "year": "", "authors": ""}, {"ref_id": "b93", "title": "", "journal": "Model Family & Parameters and Format ID", "year": "", "authors": ""}, {"ref_id": "b94", "title": "", "journal": "", "year": "", "authors": "Format Dataset"}, {"ref_id": "b95", "title": "", "journal": "", "year": "", "authors": " Instructgpt"}, {"ref_id": "b96", "title": "", "journal": "", "year": "", "authors": " Instructgpt"}, {"ref_id": "b97", "title": "", "journal": "", "year": "", "authors": " Flan-T5"}, {"ref_id": "b98", "title": "", "journal": "", "year": "", "authors": " Flan-T5"}, {"ref_id": "b99", "title": "", "journal": "", "year": "", "authors": " Flan-T5"}, {"ref_id": "b100", "title": "", "journal": "", "year": "", "authors": " Flan-T5"}, {"ref_id": "b101", "title": "", "journal": "", "year": "", "authors": " Flan-Ul"}, {"ref_id": "b102", "title": "", "journal": "", "year": "", "authors": " Flan-Ul"}, {"ref_id": "b103", "title": "", "journal": "", "year": "", "authors": " Tk-Instruct"}, {"ref_id": "b104", "title": "", "journal": "", "year": "", "authors": " Tk-Instruct"}, {"ref_id": "b105", "title": "", "journal": "", "year": "", "authors": " Tk-Instruct"}, {"ref_id": "b106", "title": "", "journal": "", "year": "", "authors": " Tk-Instruct"}, {"ref_id": "b107", "title": "", "journal": "", "year": "", "authors": " Tk-Instruct"}, {"ref_id": "b108", "title": "", "journal": "", "year": "", "authors": " Tk-Instruct"}, {"ref_id": "b109", "title": "", "journal": "", "year": "", "authors": " Tk-Instruct"}, {"ref_id": "b110", "title": "", "journal": "", "year": "", "authors": " Tk-Instruct"}, {"ref_id": "b111", "title": "", "journal": "", "year": "", "authors": " Tk-Instruct"}, {"ref_id": "b112", "title": "", "journal": "", "year": "", "authors": " Tk-Instruct"}, {"ref_id": "b113", "title": "", "journal": "", "year": "", "authors": " Opt-Iml"}, {"ref_id": "b114", "title": "", "journal": "", "year": "", "authors": "Dolly-V2 "}, {"ref_id": "b115", "title": "", "journal": "", "year": "", "authors": "Dolly-V2 "}, {"ref_id": "b116", "title": "Comparison of zero-shot prompting performance using hard-label evaluation with (SD) and without sociodemographic information. F1 is macro-averaged F1 and Acc is for Accuracy. Bold scores highlight the better performance when comparing the same model", "journal": "", "year": "", "authors": ""}, {"ref_id": "b117", "title": "Tk-Instruct(80M)", "journal": "", "year": "", "authors": ""}, {"ref_id": "b118", "title": "", "journal": "", "year": "", "authors": " Tk-Instruct"}, {"ref_id": "b119", "title": "", "journal": "", "year": "", "authors": " Tk-Instruct"}, {"ref_id": "b120", "title": "", "journal": "", "year": "", "authors": " Tk-Instruct"}, {"ref_id": "b121", "title": "", "journal": "", "year": "", "authors": " Tk-Instruct"}, {"ref_id": "b122", "title": "", "journal": "", "year": "", "authors": " Tk-Instruct"}, {"ref_id": "b123", "title": "", "journal": "", "year": "", "authors": " Tk-Instruct"}, {"ref_id": "b124", "title": "", "journal": "", "year": "", "authors": " Tk-Instruct"}, {"ref_id": "b125", "title": "", "journal": "", "year": "", "authors": " Tk-Instruct"}, {"ref_id": "b126", "title": "", "journal": "", "year": "", "authors": " Tk-Instruct"}], "figures": [{"figure_label": "3", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 3 :3Figure 3: Mean percentage of prediction changes across all datasets when comparing outputs of zero-shot prompting with and without sociodemographic information. The x-axis denotes the model size and the color indicates the model family.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: Performance to model disagreement in various datasets of subjective NLP tasks (binary F1).", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure 6: Percentage of prediction changes when comparing outputs of zero-shot prompting with and without sociodemographic information. The x-axis displays the model sizes of various instruction-tuned model families (same color).", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 7 :7Figure7: Prediction distributon across the different labels for the DP dataset. We compare the true label distribution (Target) with the results of different experimental settings for different models. None refers to prompting without sociodemographic information. Female and Male refer to sociodemographic prompting with a single attribute, respectively. The model choice has a larger influence on label predictions than the sociodemographic profile.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "p e n d e n t C o n s e r v a ti v e N o n e L ib e r a l In d e p e n d e n t C o n s e r v a ti v e N o n e L ib e r a l In d e p e n d e n t C o n s e r v a ti v e N o n e L ib e r a l In d e p e n d e n t C o n s e r v a ti v e N o n e L ib e r a l In d e p e n d e n t C o n s e r v a ti v e Against Favor None", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 8 :8Figure8: Prediction distributon across the different labels for the SE2016 dataset. We compare the true label distribution (Target) with the results of different experimental settings for different models. None refers to prompting without sociodemographic information. Liberal, Independent and Conservative refer to sociodemographic prompting with a single attribute, respectively. The model choice has a larger influence on label predictions than the sociodemographic profile.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 9 :9Figure9: Comparison of label changes when prompting (with sociodemographic information) with different prompt formulations (i.e. format 0,1 or 2) across seven datasets and two models. A cell value resembles the prediction difference between prompting with the format id provided per row and column.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 10 :10Figure10: Comparison of label changes when prompting (without sociodemographic) with information different prompt formulations (i.e. format 0,1 or 2) across seven datasets and two models. A cell value resembles the prediction difference between prompting with the format id provided per row and column.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "The sociodemographic attributes and their corresponding values we use in this study, based on the DP dataset by. Ordered ordinally by qualitative scale or by percentage share.", "figure_data": "TaskDatasetLabels"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ": The tasks and datasets (Diverse Perspectives (DP), Jigsaw, SE2016, Global Warming Stance Detection (GWSD), Gabe Hate Corpus (GHC), Twitter Hatespeech Corpus (H-Twitter), and Diaz) we use along with their labels and inter-annotator agreement we obtain (IAA, Krippendorff's \u03b1)."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "None refers to prompting without sociodemographic information. Female and Male refer to sociodemographic prompting with a single attribute, respectively. The model choice has a larger influence on label predictions than the sociodemographic profile.", "figure_data": "Prediction percentages across labels per experimental setting4.2%11.0%12.7%12.9%36.5%38.9%39.2%6.5%0.1%0.2%1.6%0.4%1.1%11.0%29.1%28.5%28.2%0.9%0.9%1.0%16.3%11.3%11.7%10.8%62.0%48.5%47.1%47.9%61.0%59.8%58.7%InstructGPT (175B)OPT-IML (30B)Figure 4: Prediction distribution across the different labels for the DP dataset. We compare the true label dis-tribution (Target) with the results of different experi-mental settings for models InstructGPT (175B) and OPT-IML (30B)."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": ".76 .59 .89 .75 .87 .57 .53 .51 .72 .69 .36 .33 InstructGPT(175B) +SD .51 .28 .79 .60 .89 .74 .86 .53 .52 .52 .69 .68 .39 .33 OPT-IML(30B) .50 .19 .58 .49 .80 .69 .84 .54 .67 .53 .57 .50 .27 .24 OPT-IML(30B) SD .55 .20 .64 .53 .85 .74 .86 .57 .65 .51 .52 .39 .35 .29 CE JSD CE JSD CE JSD CE JSD CE JSD CE JSD CE JSD InstructGPT(175B) 1.42 .32 .55 .15 .43 .07 .88 .08 .98 .27 .86 .18 1.50 .37 InstructGPT(175B)+SD 1.40 .29 .51 .12 .42 .06 .90 .08 .99 .25 .89 .15 1.48 .33 OPT-IML(30B) 1.43 .33 .71 .26 .52 .13 .90 .10 .90 .22 .95 .23 1.57 .42 OPT-IML(30B)+SD 1.40 .29 .66 .20 .48 .09 .89 .07 .91 .21 .99 .23 1.52 .32", "figure_data": "ToxicityHatespeechStance DetectionSentimentDPJigsawGHCH-TwitterSE2016GWSDDiazModelAcc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1InstructGPT(175B).48.27"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Comparison of zero-shot prompting performance using hard-label evaluation (Accuracy, F1) and soft-label evaluation (Cross-Entropy as CE, Jensen-Shannon Divergence as JSD), with (SD) and without sociodemographic information. Bold scores highlight the better performance when comparing the same model.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Differences in terms of prediction changes and performance between different prompt formulations.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Distribution of sociodemographic attributes for both the full dataset and the sample of DP.", "figure_data": "ModelParameters Context Size Nr TasksInstructGPT175B4097-Flan-T580M5121,836250M512780M5123B51211B512Flan-UL220B5121,836Tk-Instruct80M5121,616250M512780M5123B51211B512OPT-IML1.3B20481,99130B2048Dolly-V22.8B256076.9B409612B5120"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "The models and configurations we use. The last column indicates the number of instruction-tuning tasks which were used to train the model.", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Most influential sociodemographic attribute per model and dataset, and in brackets the percentage of label changes due to sociodemographic prompting when compared to prompting without any sociodemographic information. All refers to the combination of all sociodemographic attributes, PA refers to Political Affiliation, R refers to Race, A refers to Age-Range, E refers to Education and G refers to Gender.", "figure_data": "Prediction percentages across labels per experimental settingExtremely Toxic4.2%11.0%12.7%12.9%2.4%2.8%2.2%10.3%3.3%5.7%0.9%0.6%0.6%36.5%38.9%39.2%21.7%13.1%13.0%Very Toxic6.5%0.1%0.2%2.1%1.8%1.8%5.3%3.3%4.5%3.0%8.9%9.4%1.6%0.4%1.1%69.0%56.0%58.8%Prediction LabelModerately Toxic11.0%29.1%28.5%28.2%10.6%9.7%10.0%2.7%2.2%7.1%30.0%18.1%17.8%0.9%0.9%1.0%6.0%24.7%20.5%Slightly Toxic16.3%11.3%11.7%10.8%60.5%68.5%70.1%8.8%6.6%7.7%65.8%72.3%72.2%2.5%5.0%5.1%Not Toxic62.0%48.5%47.1%47.9%24.4%17.2%15.9%72.9%84.6%75.0%0.3%0.1%61.0%59.8%58.7%0.8%1.2%2.6%T a r g e tN o n eF e m a leM a leN o n eF e m a leM a leN o n eF e m a leM a leN o n eF e m a leM a leN o n eF e m a leM a leN o n eF e m a leM a leInstructGPT 175BFlan-T5 11BFlan-UL 20BTk-Instruct 11BOPT-IML 30BDolly-V2 12B"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Table 13 displays the test statistics regarding the parametric fixed terms of the sensitivity model. We observe a statistically significant positive effect of model size (\u03b2=0.56, 95% CI [0.54, 0.58], p<0.001), a significant negative effect of text length (\u03b2=-1.69e-04, 95% CI [-2.93e-04, -4.51e-05], p=0.007), and a statistically significant effect of model family (\u03c7 2 (5)=3937.76, p<0.001). A post hoc Wald comparison of the contrasts for model family revealed significant differences between all pairs of model families. The corresponding estimates are (in descending order) Flan-T5 (\u03b2=0.58, 95% CI [0.53, 0.63]), Tk-Instruct (\u03b2=0.33, 95% CI [0.28, 0.37]), Flan-UL (\u03b2=-0.21, 95% CI [-0.27, -0.14]), OPT-IML (\u03b2=-0.64, 95% CI [-0.70, -0.59]), and InstructGPT (\u03b2=-1.90, 95% CI [-1.99, -1.82]). Note that the estimate for Dolly-V2 is fixed as the reference level.", "figure_data": ""}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Comparison of zero-shot prompting performance using soft-label evaluation with (SD) and without sociodemographic information. CE is cross-entropy and JSD is for Jensen-Shannon divergence. Bold scores highlight the better performance when comparing the same model. Avg denotes averaged JSD scores. , how would a person of gender {gender}, race '{race}', age '{age}', education level '{education}' and political affiliation '{political_affiliation}' rate the degree of toxicity in the text. Possible values are 'not toxic', 'slightly toxic', 'moderately toxic', 'very toxic' or 'extremely toxic'. Imagine you are a person of gender {gender}, race '{race}', age '{age}', education level '{education}' and political affiliation '{political_affiliation}'. Given a text, how would you rate the degree of toxicity in the text. Possible values are 'not toxic', 'slightly toxic', 'moderately toxic', 'very toxic' or 'extremely toxic'.", "figure_data": "Identifier Format1Given a textText: '{text}'Toxicity:2Text: '{text}'Toxicity:3Person: {gender}, {race}, {age}, {education}, {political_affiliation}Text: '{text}'Toxicity ('not toxic'|'slightly toxic'|'moderately toxic'|'very toxic'|'extremely toxic'):"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "Three different prompt formulations which are used to study the robustness of sociodemographic prompting. The placeholders in curly brackets are replaced with the respective values before prompting the LLM. The prompt formulations shown are exemplary for the DP dataset for toxicity detection.", "figure_data": "Term df\u03c7 2pmodel size 1 2769.41 <0.001 text length 1 7.15 0.01 additional prompt length 1 2.35 0.13model family 5 3937.76 <0.001 task 3 2.72 0.44"}, {"figure_label": "13", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "Test statistics for the parametric fixed terms of the sensitivity model.", "figure_data": "Term df\u03c7 2pmodel size 1 564.29 <0.001 text length 1 28.88 <0.001 additional prompt length 1 0.70 0.40model family 5 579.84 <0.001 task 3 5.92 0.12"}, {"figure_label": "14", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "Test statistics for the parametric fixed terms of the ambiguity prediction model. Ambiguity Model. Table14displays the test statistics regarding the parametric fixed terms of the disagreement prediction model. We observe a statistically significant positive effect of model size (\u03b2=0.22, 95% CI [0.21, 0.24], p<0.001), a significant negative effect of text length (\u03b2=-2.72e-04, 95% CI [-3.71e-04, -1.73e-04], p<0.001), and a statistically significant effect of model family (\u03c7 2 (5)=579.84, p<0.001). A post hoc Wald comparison of the contrasts for model family revealed significant differences between all pairs of model families except Flan-UL2 and InstructGPT . The corresponding estimates are (in descending order) Dolly-V2 (\u03b2=0.58, 95% CI [0.52, 0.64]), Flan-T5 (\u03b2=0.53, 95% CI [0.47, 0.58]), Tk-Instruct (\u03b2=0.40, 95% CI [0.34, 0.45]), OPT-IML (\u03b2=0.30, 95% CI [0.24, 0.36]), and InstructGPT (\u03b2=0.03, 95% CI [-0.04, 0.10]), Note that the estimate for Flan-UL2 is fixed as the reference level.", "figure_data": ""}], "formulas": [], "doi": "10.18653/v1/2020.alw-1.21"}