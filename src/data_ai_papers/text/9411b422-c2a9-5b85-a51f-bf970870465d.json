{"title": "Using Task Features for Zero-Shot Knowledge Transfer in Lifelong Learning", "authors": "David Isele; Mohammad Rostami; Eric Eaton", "pub_date": "", "abstract": "Knowledge transfer between tasks can improve the performance of learned models, but requires an accurate estimate of the inter-task relationships to identify the relevant knowledge to transfer. These inter-task relationships are typically estimated based on training data for each task, which is inefficient in lifelong learning settings where the goal is to learn each consecutive task rapidly from as little data as possible. To reduce this burden, we develop a lifelong reinforcement learning method based on coupled dictionary learning that incorporates high-level task descriptors to model the intertask relationships. We show that using task descriptors improves the performance of the learned task policies, providing both theoretical justification for the benefit and empirical demonstration of the improvement across a variety of dynamical control problems. Given only the descriptor for a new task, the lifelong learner is also able to accurately predict the task policy through zero-shot learning using the coupled dictionary, eliminating the need to pause to gather training data before addressing the task.", "sections": [{"heading": "Introduction", "text": "Transfer and multi-task learning (MTL) methods reduce the amount of experience needed to train individual task models by reusing knowledge from other related tasks. These techniques typically select the relevant knowledge to transfer by modeling inter-task relationships, based on training data for each task [Baxter, 2000;Ando & Zhang, 2005;Bickel et al., 2009;Maurer et al., 2013]. However, this process requires sufficient training data for each task to identify these relationships before knowledge transfer can succeed.\nConsider instead the human ability to rapidly bootstrap a model for a new task, given only a high-level task description, before obtaining experience on the actual task. For example, viewing only the image on the box of a new Ikea chair, we can immediately identify previous related assembly tasks and begin formulating a plan to assemble the chair. In the same manner, an experienced inverted pole balancing agent may be \u2020 These authors contributed equally to this work. able to predict the controller for a new pole given its mass and length, prior to interacting with the physical system.\nInspired by this idea, we explore the use of high-level task descriptions to improve knowledge transfer between multiple machine learning tasks. We focus on lifelong learning scenarios [Thrun, 1996;Ruvolo & Eaton, 2013], in which multiple tasks arrive consecutively and the goal is to rapidly learn each new task by building upon previous knowledge. Although we focus on reinforcement learning (RL) tasks in this paper, our approach extends easily to regression and classification.\nOur algorithm, Task Descriptors for Lifelong Learning (TaDeLL), encodes the task descriptions as feature vectors that identify each task, treating these descriptors as side information in addition to training data on the individual tasks. This idea of using task features for knowledge transfer has been explored previously by Bonilla et al. [2007] in an offline batch MTL setting, and more recently by Sinapov et al. [2015] in a computationally expensive method for estimating transfer relationships between pairs of tasks. In comparison, our approach operates online over consecutive tasks and is much more computationally efficient.\nWe use coupled dictionary learning to model the inter-task relationships between both the task descriptions and the individual task policies in lifelong learning. The coupled dictionary learning enforces the notion that tasks with similar descriptions should have similar policies, but still allows dictionary elements the freedom to accurately represent the different task policies. We connect the coupled dictionaries to the concept of mutual coherence in sparse coding, providing theoretical justification for why the task descriptors improve performance, and verify this improvement empirically.\nIn addition to improving the task policies, we show that the task descriptors enable the learner to accurately predict the policies for unseen tasks given only their descriptionthis process of learning without data is known as zero-shot learning. This capability is particularly important in the online setting of lifelong learning, enabling the system to accurately predict policies for new tasks through transfer, without requiring it to pause to gather training data on each task.", "publication_ref": ["b2", "b0", "b3", "b7", "b3", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Batch MTL [Caruana, 1997] methods often model the relationships between tasks to determine the knowledge to transfer [Baxter, 2000;Ando & Zhang, 2005;Bickel et al., 2009;Lazaric & Ghavamzadeh, 2010;Maurer et al., 2013]. These techniques include modeling a task distance metric [Ben-David et al., 2007], using correlations to determine when transfer is appropriate [Wang et al., 2014], or building models based on nearest neighbors [Parameswaran & Weinberger, 2010]. More recently, MTL has been extended to a lifelong learning setting, in which tasks arrive consecutively for regression and classification [Ruvolo & Eaton, 2013] and for reinforcement learning [Bou Ammar et al., 2014;. However, all of these methods require training data for each task in order to assess their relationships and determine the knowledge to transfer.\nInstead of relying solely on the tasks' training data, several works have explored the use of high level task descriptors to model the inter-task relationships in MTL and transfer learning settings. Task descriptors have been used in combination with neural networks [Bakker & Heskes, 2003] to define a task-specific prior or to control the gating network between individual task clusters. Bonilla et al. [2007] explore similar techniques for multi-task kernel machines, using task features in combination with the data for a gating network over individual task experts or to augment the original task training data. These papers focus on multi-task classification and regression in batch settings where the system has access to the data and features for all tasks, in contrast to our study of task descriptors for lifelong learning over consecutive RL tasks.\nIn the work most similar to ours, Sinapov et al. [2015] use task descriptors to estimate the transferability between each pair of tasks for transfer learning. Given the descriptor for a new task, they identify the source task with the highest predicted transferability, and use that source task for a warm start in RL. Though effective, their approach is computationally expensive, since they estimate the transferability for every task pair through repeated simulation. Their evaluation is also limited to a transfer learning setting, and they do not consider the effects of transfer over consecutive tasks or updates to the transferability model, as we do in the lifelong setting.\nOur work is also related to the Simple Zero-Shot Learning (Simple ZSL) method by Romera-Paredes and Tor [2015], which learns a multi-class linear model, and factorizes the linear model parameters, assuming the descriptors are coefficients over a latent basis to reconstruct the models. Our approach assumes a more flexible relationship: that both the model parameters and task descriptors can be reconstructed from separate latent bases that are coupled together through their coefficients. In comparison to our lifelong learning approach, Simple ZSL operates in an offline multi-class setting.", "publication_ref": ["b2", "b0", "b3", "b7", "b7", "b2", "b12", "b1", "b3", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Background", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Reinforcement Learning", "text": "A reinforcement learning (RL) agent must select sequential actions in an environment to maximize its expected return. An RL task is typically formulated as a Markov Decision Process (MDP) hX , A, P, R, i, where X is the set of states, and A is the set of actions that the agent may execute, P : X \u21e5 A \u21e5 X ! [0, 1] is the state transition probability describing the systems dynamics, R : X \u21e5 A \u21e5 X ! R is the reward function, and 2 [0, 1) is the discount as-signed to rewards over time. At time step h, the agent is in state x h 2 X and chooses an action a 2 A according to policy \u21e1 : X \u21e5 A 7 ! [0, 1], which is represented as a function defined by a vector of control parameters \u2713 2 R d . The agents then receives reward r h according to R and transitions to state x h+1 according to P . This sequence of states, actions, and rewards is given as a trajectory \u2327 = {(x 1 , a 1 , r 1 ), . . . , (x H , a H , r H )} over a horizon H. The goal of RL is to find the optimal policy \u21e1 \u21e4 with parameters \u2713 \u21e4 that maximizes the expected reward. However, learning an individual task still requires numerous trajectories, motivating the use of transfer to reduce the number of interactions with the environment.\nPolicy Gradient (PG) methods [Sutton et al., 1999], which we employ as our base learner, are a class of RL algorithms that are effective for solving high dimensional problems with continuous state and action spaces, such as robotic control [Peters & Schaal, 2008]. The goal of PG is to optimize the expected average return:\nJ (\u2713) = E h 1 H P H h=1 r h i = R T p \u2713 (\u2327 )R(\u2327 )\nd\u2327 , where T is the set of all possible trajectories, the average reward on trajectory \u2327 is given by R(\u2327\n) = 1 H P H h=1 r h , and p \u2713 (\u2327 ) = P 0 (x 1 ) Q H h=1 p(x h+1 | x h , a h ) \u21e1(a h | x h ) is the probability of \u2327 under an initial state distribution P 0 : X 7 ! [0, 1].\nMost PG methods (e.g., episodic REIN-FORCE [Williams, 1992], PoWER [Kober & Peters, 2009], and Natural Actor Critic [Peters & Schaal, 2008]) optimize the policy by maximizing a lower bound on J (\u2713), comparing trajectories generated by \u21e1 \u2713 against those generated by a new candidate policy \u21e1\u2713. For details, see Kober & Peters [2009].", "publication_ref": ["b10", "b8", "b5", "b8", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Lifelong Machine Learning", "text": "In a lifelong learning setting [Thrun, 1996;Ruvolo & Eaton, 2013], the learner faces multiple, consecutive tasks and must rapidly learn each new task by building upon its previous experience. The learner may encounter a previous task at any time, and so must optimize performance across all tasks seen so far. A priori, the agent does not know the total number of tasks T max , the task distribution, or the task order. At time t, the lifelong learner encounters task Z (t) . In this paper, each task Z (t) is specified by an MDP hX (t) , A (t) , P (t) , R (t) , (t) i, but the lifelong learning setting and our approach can equivalently handle classification or regression tasks. The agent will learn each task consecutively, acquiring training data (i.e., trajectories) in each task before advancing to the next. The agent's goal is to learn the optimal policies {\u21e1 \u21e4 \u2713 (1) , . . . , \u21e1 \u21e4 \u2713 (T ) } with corresponding parameters {\u2713 (1) , . . . , \u2713 (T ) }, where T is the number of unique tasks seen so far (1 \uf8ff T \uf8ff T max ). Ideally, knowledge learned from previous tasks {Z (1) , . . . , Z (T 1) } should accelerate training and improve performance on each new task Z (T ) . Also, the lifelong learner should scale effectively to large numbers of tasks, learning each new task rapidly from minimal data.\nThe Efficient Lifelong Learning Algorithm (ELLA) [Ruvolo & Eaton, 2013] and PG-ELLA [Bou Ammar et al., 2014] were developed to operate in this lifelong learning setting for classification/regression and RL tasks, respectively. Both approaches assume the parameters for each task model can be factorized using a shared knowledge base L, facilitating transfer between tasks. Specifically, the model parameters for task Z (t) are given by \u2713 (t)\n= Ls (t) , where L 2 R d\u21e5k is the shared basis over the model space, and s (t) 2 R k are the sparse coefficients over the basis. This factorization has been effective for transfer in both lifelong and multi-task learning [Kumar & Daum\u00e9, 2012;Maurer et al., 2013].\nUnder this assumption, the MTL objective for PG is:\nmin L,S 1 T X t h J (\u2713 (t) ) + \u00b5ks (t) k 1 i + kLk 2 F , (1\n)\nwhere\nS = [s (1) \u2022 \u2022 \u2022 s (T )\n] is the matrix of sparse vectors,\nJ (\u2713 (t)\n) is the PG objective for task Z (t) , k \u2022 k F is the Frobenius norm, the L 1 norm is used to approximate the true vector sparsity of s (t) , and \u00b5 and are regularization parameters. To solve this objective in a lifelong learning setting, Bou Ammar et al. [2014] approximate the multi-task objective by first substituting in the lower-bound to the PG objective, then taking a second-order Taylor expansion to approximate the objective around an estimate \u21b5 (t) 2 R d of the single-task policy parameters for each task Z (t) , and updating only the coefficients s (t) for the current task at each time step. This process reduces the MTL objective to the problem of sparse coding the single-task policies in the shared basis L, and enables S and L to be solved efficiently by the following online update rules that constitute PG-ELLA [Bou Ammar et al., 2014]:\ns (t) arg min s k\u21b5 (t) Lsk 2 (t) + \u00b5ksk 1 (2) A A + (s (t) s (t)> ) \u2326 (t) (3) b b + vec \u21e3 s (t)> \u2326 \u21e3 \u21b5 (t)> (t) \u2318\u2318 (4) L mat \u2713\u2713 1 T A + I kd \u25c6 1 1 T b \u25c6 , (5\n)\nwhere kvk 2 A = v > Av, the symbol \u2326 denotes the Kronecker product, (t) is the Hessian of the PG lower bound on J (\u21b5 (t)\n), I m is the m \u21e5 m identity matrix, A is initialized to a kd \u21e5 kd zero matrix, and b 2 R kd is initialized to zeros.\nThough effective for lifelong learning, this approach requires significant training data to estimate the policy for each new task before the learner can solve it. We eliminate this restriction by incorporating task descriptors into lifelong learning, enabling zero-shot transfer to new tasks.", "publication_ref": ["b6", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Task Descriptors", "text": "While most MTL and lifelong learning methods use task training data to model inter-task relationships, high level descriptions can describe task differences. For example, in multi-task medical domains, patients are often grouped into tasks by demographic data and disease presentation [Oyen & Lane, 2012]. In control problems, the dynamical system parameters (e.g., the spring, mass, and damper constants in a spring-mass-damper system) describe the task. Descriptors can also be derived from external sources, such as Wikipedia [Pennington et al., 2014]. Such task descriptors have been used extensively for zero-shot learning [Palatucci et al., 2009;Socher et al., 2013].\nFormally, we assume that each task Z (t) has an associated descriptor m (t) that is given to the learner upon first presentation of the task. The learner has no knowledge of future tasks, or the distribution of task descriptors. The descriptor is represented by a feature vector m (t) 2 R dm , where (\u2022) performs feature extraction and (possibly) a non-linear basis transformation on the features. We make no assumptions on the uniqueness of m (t) , although in general tasks will have different descriptors. 1 In addition, each task also has associated training data X (t) to learn the model; in the case of RL tasks, the data consists of trajectories that are dynamically acquired by the agent through experience in the environment.", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "Lifelong Learning with Task Descriptors", "text": "We incorporate task descriptors into lifelong learning via sparse coding with a coupled dictionary, enabling the descriptors and learned policies to augment each other. Although we focus on RL tasks, our method can easily be adapted to classification or regression, as described in the Appendix. 2", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Coupled Dictionary Optimization", "text": "As described previously, many multi-task and lifelong learning approaches have found success with factorizing the policy parameters \u2713 (t) for each task as a sparse linear combination over a shared basis:\n\u2713 (t)\n= Ls (t) . In effect, each column of the shared basis L serves as a reusable policy component representing a cohesive chunk of knowledge. In lifelong learning, the basis L is refined over time as the system learns more tasks. The coefficient vectors S = [s (1) . . . s (T )\n] encode the task policies in this shared basis, providing an embedding of the tasks based on how their policies share knowledge.\nWe make a similar assumption about the task descriptorsthat the descriptor features m (t) can be linearly factorized 3 using a latent basis D 2 R dm\u21e5k over the descriptor space. This basis captures relationships among the descriptors, with coefficients that similarly embed tasks based on commonalities in their descriptions. From a co-view perspective [Yu et al., 2014], both the policies and descriptors provide information about the task, and so each can augment the learning of the other. Each underlying task is common to both views, and so we seek to find task embeddings that are consistent for both the policies and their corresponding task descriptors. We can enforce this by coupling the two bases L and D, sharing the same coefficient vectors S to reconstruct both the policies and descriptors. Therefore, for task Z (t) ,\n\u2713 (t) = Ls (t) m (t) = Ds (t) .(6)\nTo optimize the coupled bases L and D during the lifelong learning process, we employ techniques for coupled dictionary optimization from the sparse coding literature [Yang Algorithm 1 TaDeLL (k, , \u00b5)\n1: T 0 2: L RandomMatrix d,k , D RandomMatrix m,k 3: while some task Z (t) , m (t) is available do 4:\nif isNewTask(Z (t) ) then 5: L updateL(L, s (t) , \u21b5 (t) , (t) , )\n13:\nD updateD(D, s (t) , m (t) , \u21e2I dm , )14:\nfor t 2 {1, . . . , T } do: \u2713 (t) Ls (t) 15: end while et al., 2010], which optimizes the dictionaries for multiple feature spaces that share a joint sparse representation. This notion of coupled dictionary learning has led to high performance algorithms for image super-resolution [Yang et al., 2010], allowing the reconstruction of high-res images from low-res samples, and for multi-modal retrieval [Zhuang et al., 2013] and cross-domain retrieval [Yu et al., 2014].\nGiven the factorization in Eq. 6, we can re-formulate the multi-task objective (Eq. 1) for the coupled dictionaries as\nmin L,D,S 1 T X t \uf8ff J \u21e3 \u2713 (t) \u2318 + \u21e2 m (t) Ds (t) 2 2 + \u00b5 s (t) 1 + (kLk 2 F + kDk 2 F ) ,(7)\nwhere \u21e2 balances the policy's fit to the task descriptor's fit.\nTo solve Eq. 7 online, we approximate J (\u2022) by a secondorder Taylor expansion around \u21b5 (t) , the minimizer for the PG lower bound of J (\u2022) (i.e., \u21e1 \u21b5 (t) is the single-task policy for Z (t) based on the observed trajectories), following Bou Ammar et al. [2014]. This simplifies Eq. 7 to\nmin L,D,S 1 T X t \uf8ff \u21b5 (t) Ls (t) 2 (t) + \u21e2 m (t) Ds (t) 2 2 + \u00b5 s (t) 1 + (kLk 2 F + kDk 2 F ) .(8)\nWe can merge pairs of terms in Eq. 8 by choosing:\n(t) = \uf8ff \u21b5 (t) m (t) K = \uf8ff L D A (t) = \uf8ff (t) 0 0 \u21e2I dm ,\nwhere 0 is the zero matrix, letting us rewrite (8) concisely as\nmin K,S 1 T X t \uf8ff (t) Ks (t) 2 A (t) + \u00b5 s (t) 1 + kKk 2 F . (9)\nThis objective can now be solved efficiently online, as a series of per-task update rules given in Algorithm 1. L and D are updated independently using Equations 3-5, following a recursive construction based on an eigenvalue decomposition. The complete implementation of our approach is available on the third author's website.\nAlgorithm 2 Zero-Shot Transfer to a New Task Z (tnew ) 1: Inputs: task descriptor m (tnew ) , learned bases L and D 2:s (tnew ) arg min s m (tnew ) Ds\n2 2 + \u00b5 ksk 1 3:\u2713 (tnew )\nLs (tnew ) 4: Return: \u21e1\u2713 (tnew )", "publication_ref": ["b13", "b13", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Zero-Shot Transfer Learning", "text": "In a lifelong setting, when faced with a new task, the agent's goal is to learn an effective policy for that task as quickly as possible. At this stage, previous multi-task and lifelong learners incurred a delay before they could produce a decent policy, since they needed to acquire data from the new task in order to identify related knowledge and train the new policy.\nIncorporating task descriptors enables our approach to predict a policy for the new task immediately, given only the descriptor. This ability to perform zero-shot transfer is enabled by the use of coupled dictionary learning, which allows us to observe a data instance in one feature space (i.e., the task descriptor), and then recover its underlying latent signal in the other feature spaces (i.e., the policy parameters) using the dictionaries and sparse coding [Yang et al., 2010].\nGiven only the descriptor m (tnew ) for a new task Z (tnew ) , we can estimate the embedding of the task in the latent descriptor space via LASSO on the learned dictionary D:\ns (tnew ) arg min s m (t)Ds\n2 2 + \u00b5 ksk 1 .\n(10) Since the estimate given by s (tnew ) also serves as the coefficients over the latent policy space L, we can immediately predict a policy for the new task as:\u2713 (tnew ) = Ls (tnew ) . This zero-shot transfer learning procedure is given as Algorithm 2.", "publication_ref": ["b13"], "figure_ref": [], "table_ref": []}, {"heading": "Theoretical Analysis", "text": "This section discusses why incorporating task descriptors through coupled dictionaries can improve performance of the learned policies and enable zero-shot transfer to new tasks. In the Appendix 2 , we also prove the convergence of TaDeLL. A full sample complexity analysis is beyond the scope of this paper, and, indeed, remains an open problem for zero-shot learning [Romera-Paredes & Torr, 2015].\nTo analyze the policy improvement, since the policy parameters are factored as \u2713 (t)\n= Ls (t) , we proceed by showing that incorporating the descriptors through coupled dictionaries can improve both L and S. In this analysis, we employ the concept of mutual coherence, which has been studied extensively in the sparse recovery literature. Mutual coherence measures the similarity of a dictionary's elements as\nM (Q) = max 1\uf8ffi6 =j\uf8ffn \u21e3 |q > i qj | kqik2kqj k2 \u2318 2 [0, 1]\n, where q i is the i th column of a dictionary Q 2 R d\u21e5k . If M (Q) = 0, then Q is an invertible orthogonal matrix and so sparse recovery can be solved directly by inversion; M (Q) = 1 implies that Q is not full rank and a poor dictionary. Intuitively, low mutual coherence indicates that the dictionary's columns are considerably different, and thus such a \"good\" dictionary can represent many different policies, potentially yielding more knowledge transfer. This intuition is shown in the following:\nTheorem 5.1. [Donoho et al., 2006] Suppose we have noisy observations\u2713 of the linear system \u2713 = Qs, such that k\u2713 \u2713k 2 \uf8ff \u270f. Let s \u21e4 be a solution to the system, and let K = ksk 0 . If K < 0.5(1 + M (Q) 1 ), then s \u21e4 is the unique sparsest solution of the system. Moreover, if s + is the LASSO solution for the system from the noisy observations, then:\nks \u21e4 s + k 2 2 \uf8ff 4\u270f 2 1 M (Q)(4K 1)\n. Therefore, an L with low mutual coherence would lead to more stable solutions of the \u2713 (t) 's against inaccurate singletask estimates of the policies (the \u21b5 (t) 's). We next show that our approach likely lowers the mutual coherence of L.\nTaDeLL alters the problem from training L to training the coupled dictionaries L and D (contained in K). Let s \u21e4(t) be the solution to Eq. 1 for task Z (t) , which is unique under sparse recovery theory, so s \u21e4(t) 0 remains unchanged for all tasks. Theorem 5.1 implies that, if M (K) < M(L), coupled dictionary learning can help with a more accurate recovery of the s (t) 's. To show this, we note that Eq. 7 can also be derived as a result of an MAP estimate from a Bayesian perspective, enforcing a Laplacian distribution on the s (t) 's and assuming L to be a Gaussian matrix with elements drawn i.i.d:\nl ij \u21e0 N (0, 2\n). When considering such a random matrix Q 2 R d\u21e5k , Donoho & Huo [2001] proved that asymp-\ntotically M (Q) / q log(dk) d as d ! 1.\nUsing this as an estimate for M (L) and M (K), since incorporating task descriptors increases d, most likely M (K) < M(L), implying that TaDeLL learns a superior dictionary. Moreover, if M (D) \uf8ff M (L), the theorem implies we can use D alone to recover the task policies through zero-shot transfer.\nTo show that task features can also improve the sparse recovery, we rely on the following theorem about LASSO: Theorem 5.2. [Negahban et al., 2009] Let s \u21e4 be a unique solution to the system \u2713 = Qs with ksk 0 = K and Q 2 R d\u21e5k . If s + is the LASSO solution for the system from noisy observations, then with high probability:\nks \u21e4 s + k 2 \uf8ff c 0 q K log k d\n, where the constant c 0 2 R + depends on properties of the linear system and observations. This theorem shows that the error reconstruction for LASSO is proportional to 1 p d . When we incorporate the descriptor through (t) , the RHS denominator increases from d to (d + d m ) while K and k remain constant, yielding a tighter fit. Therefore, task descriptors can improve learned dictionary quality and sparse recovery accuracy. To ensure an equivalently tight fit for s (t) using either policies or descriptors, Theorem 5.2 suggests it should be that d m d to ensure that zero-shot learning yields similarly tight estimates of s (t) . Computational Complexity Each update begins with one PG step to update \u21b5 (t) and (t) at a cost of O(\u21e0(d, n t )), where \u21e0() depends on the base PG learner and n t is the number of trajectories obtained for task Z (t) . The cost of updating L and s (t) alone is O(k 2 d 3\n) [Ruvolo & Eaton, 2013], and so the cost of updating K through coupled dictionary learning is O(k\n2 (d + d m ) 3\n). This yields an overall per-update cost of\nO(k 2 (d + d m ) 3 + \u21e0(d, n t ))\n, which is independent of T .", "publication_ref": ["b4", "b4", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We evaluated our method on learning control policies for three benchmark systems and an application to quadrotors.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Benchmark Dynamical Systems", "text": "Spring Mass Damper (SM) The SM system is described by three parameters: the spring constant, mass, and damping constant. The system's state is given by the position and velocity of the mass. The controller applies a force to the mass, attempting to stabilize it to a given position.\nCart Pole (CP) The CP system involves balancing an inverted pendulum by applying a force to the cart. The system is characterized by the cart and pole masses, pole length, and a damping parameter. The states are the position and velocity of the cart and the angle and rotational velocity of the pole. Bicycle (BK) This system focuses on keeping a bicycle balanced upright as it rolls along a horizontal plane at constant velocity. The system is characterized by the bicycle mass, xand z-coordinates of the center of mass, and parameters relating to the shape of the bike (the wheelbase, trail, and head angle). The state is the bike's tilt and its derivative; the actions are the torque applied to the handlebar and its derivative.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "In each domain we generated 40 tasks, each with different dynamics, by varying the system parameters. The reward for each task was taken to be the distance between the current state and the goal. For lifelong learning, tasks were encountered consecutively with repetition, and learning proceeded until each task had been seen at least once. We used the same random task order between methods to ensure fair comparison. The learners sampled trajectories of 100 steps, and the learning session during each task presentation was limited to 30 iterations. For MTL, all tasks weres presented simultaneously. We used Natural Actor Critic [Peters & Schaal, 2008] as the base learner for the benchmark systems and episodic REINFORCE [Williams, 1992] for quadrotor control. We chose k and the regularization parameters independently for each domain to optimize the combined performance of all methods on 20 held-out tasks, and set \u21e2 = mean(diag(\u21e2 (t)\n)) to balance the fit to the descriptors and the policies. We measured learning curves based on the final policies for each of the 40 tasks, averaging results over seven trials. The system parameters for each task were used as the task descriptor features (m); we also tried several non-linear transformations as (\u2022), but found the linear features worked well.", "publication_ref": ["b8", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Results on Benchmark Systems", "text": "Figure 1 compares our TaDeLL approach for lifelong learning with task descriptors to 1.) PG-ELLA [Bou Ammar et al., 2014], which does not use task features, 2.) GO-MTL [Kumar & Daum\u00e9, 2012], the MTL optimization of Eq. 1, and 3.) single-task learning using PG. For comparison, we also performed an offline MTL optimization of Eq. 7 via alternating optimization, and plot the results as TaDeMTL. The shaded regions on the plots denote standard error bars.\nWe see that task descriptors improve lifelong learning on every system, even driving performance to a level that is unachievable from training the policies from experience alone  via GO-MTL in the SM and BK domains. The difference between TaDeLL and TaDeMTL is also negligible for all domains except CP (which had very diverse tasks), demonstrating the effectiveness of our online optimization. Figure 3 shows that task descriptors are effective for zeroshot transfer to new tasks. We generated an additional 40 tasks for each domain to measure zero-shot performance, averaging results over these new tasks. Figure 3a shows that our approach improves the initial performance (i.e., the \"jumpstart\" [Taylor & Stone, 2009]) on new tasks, outperforming Sinapov et al. [2015]'s method and single-task PG, which was allowed to train on the task. We attribute the especially poor performance of Sinapov et al. on CP to the fact that the CP policies differ substantially; in domains where the source policies are vastly different from the target policies, Sinapov et al.'s algorithm does not have an appropriate source to transfer. Their approach is also much more computationally expensive (quadratic in the number of tasks) than our approach (linear in the number of tasks), as shown in Figure 2; details of the runtime experiments are included in the Appendix 2 . Figures 3b-3d show that the zero-shot policies can be used effectively as a warm start initialization for a PG learner, which is then allowed to improve the policy.", "publication_ref": ["b6", "b11", "b9"], "figure_ref": ["fig_0", "fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "Application to Quadrotor Control", "text": "We also applied our approach to the more challenging domain of quadrotor control, focusing on zero-shot transfer to new stability tasks. To ensure realistic dynamics, we use the model of Bouabdallah and Siegwart [2005], which has been verified on physical systems. The quadrotors are character-  ized by three inertial constants and the arm length, with their state consisting of roll/pitch/yaw and their derivatives.\nFigure 4 shows the results of our application, demonstrating that TaDeLL can predict a controller for new quadrotors through zero-shot learning that has equivalent accuracy to PG, which had to train on the system. As with the benchmarks, TaDeLL is effective for warm start learning with PG.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Conclusion", "text": "We proposed a coupled dictionary method for incorporating task descriptors into lifelong learning, showing that descriptors improve learned policy performance, and enable us to predict policies for new tasks before observing training data. Experiments demonstrate that our method outperforms other approaches on dynamical control problems, and requires substantially less computational time than similar methods.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "This research was supported by ONR grant #N00014-11-1-0139 and AFRL grant #FA8750-14-1-0069. We would like to thank the anonymous reviewers for their helpful feedback.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Rie Kubota Ando & Tong Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data", "journal": "Journal of Machine Learning Research", "year": "2005", "authors": "& Ando;  Zhang"}, {"ref_id": "b1", "title": "Bart Bakker & Tom Heskes. Task clustering and gating for Bayesian multi-task learning", "journal": "Journal of Machine Learning Research", "year": "2003", "authors": "& Bakker;  Heskes"}, {"ref_id": "b2", "title": "Analysis of representations for domain adaptation", "journal": "Journal of Artificial Intelligence Research", "year": "2000", "authors": "; Jonathan Baxter; ; Baxter;  Ben-David"}, {"ref_id": "b3", "title": "Samir Bouabdallah & Roland Siegwart. Backstepping and sliding-mode techniques applied to an indoor micro quadrotor", "journal": "", "year": "1997", "authors": " Bickel"}, {"ref_id": "b4", "title": "Stable recovery of sparse overcomplete representations in the presence of noise", "journal": "IEEE Transactions on Information Theory", "year": "2001", "authors": "& Donoho;  Huo ; David Donoho & Xiaoming;  Huo"}, {"ref_id": "b5", "title": "Jens Kober & Jan Peters. Policy search for motor primitives in robotics. Neural Information Processing Systems", "journal": "", "year": "2009", "authors": "& Kober;  Peters"}, {"ref_id": "b6", "title": "Abhishek Kumar & Hal Daum\u00e9. Learning task grouping and overlap in multi-task learning", "journal": "", "year": "2012", "authors": "& Kumar;  Daum\u00e9"}, {"ref_id": "b7", "title": "Sahand Negahban, Bin Yu, Martin Wainwright, & Pradeep Ravikumar. A unified framework for highdimensional analysis of m-estimators with decomposable regularizers", "journal": "", "year": "2009", "authors": "Alessandro Lazaric & Ghavamzadeh; ; Lazaric & Mohammad Ghavamzadeh;  Maurer"}, {"ref_id": "b8", "title": "An embarrassingly simple approach to zero-shot learning", "journal": "", "year": "2008-01", "authors": "& Peters;  Schaal"}, {"ref_id": "b9", "title": "Learning inter-task transferability in the absence of target task samples. International Conference on Autonomous Agents and Multiagent Systems", "journal": "", "year": "2015", "authors": "[ Sinapov"}, {"ref_id": "b10", "title": "Satinder Singh, & Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation", "journal": "", "year": "1999", "authors": "[ Socher"}, {"ref_id": "b11", "title": "Transfer learning for reinforcement learning domains: A survey", "journal": "", "year": "1996", "authors": "& Taylor; E Stone ; Matthew; Peter Taylor;  Stone"}, {"ref_id": "b12", "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "journal": "", "year": "1992", "authors": ""}, {"ref_id": "b13", "title": "& Yueting Zhuang. Discriminative coupled dictionary hashing for fast cross-media retrieval", "journal": "", "year": "2010", "authors": "Yang "}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Performance of multi-task (solid lines), lifelong (dashed), and single-task learning (dotted) on benchmark dynamical systems. (Best viewed in color.)", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "FigureFigure 3 :3Figure 2: Runtime comparison.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Warm start learning on quadrotor control.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "J (\u2713) = E h 1 H P H h=1 r h i = R T p \u2713 (\u2327 )R(\u2327 )", "formula_coordinates": [2.0, 340.34, 258.39, 175.35, 27.46]}, {"formula_id": "formula_1", "formula_text": ") = 1 H P H h=1 r h , and p \u2713 (\u2327 ) = P 0 (x 1 ) Q H h=1 p(x h+1 | x h , a h ) \u21e1(a h | x h ) is the probability of \u2327 under an initial state distribution P 0 : X 7 ! [0, 1].", "formula_coordinates": [2.0, 315.0, 289.24, 243.01, 59.99]}, {"formula_id": "formula_2", "formula_text": "min L,S 1 T X t h J (\u2713 (t) ) + \u00b5ks (t) k 1 i + kLk 2 F , (1", "formula_coordinates": [3.0, 80.33, 158.92, 212.8, 31.09]}, {"formula_id": "formula_3", "formula_text": ")", "formula_coordinates": [3.0, 293.13, 170.07, 3.87, 9.54]}, {"formula_id": "formula_4", "formula_text": "S = [s (1) \u2022 \u2022 \u2022 s (T )", "formula_coordinates": [3.0, 82.08, 199.04, 80.08, 16.42]}, {"formula_id": "formula_5", "formula_text": "J (\u2713 (t)", "formula_coordinates": [3.0, 54.0, 211.37, 27.62, 16.42]}, {"formula_id": "formula_6", "formula_text": "s (t) arg min s k\u21b5 (t) Lsk 2 (t) + \u00b5ksk 1 (2) A A + (s (t) s (t)> ) \u2326 (t) (3) b b + vec \u21e3 s (t)> \u2326 \u21e3 \u21b5 (t)> (t) \u2318\u2318 (4) L mat \u2713\u2713 1 T A + I kd \u25c6 1 1 T b \u25c6 , (5", "formula_coordinates": [3.0, 90.89, 375.56, 206.11, 82.33]}, {"formula_id": "formula_7", "formula_text": ")", "formula_coordinates": [3.0, 293.13, 441.82, 3.87, 9.54]}, {"formula_id": "formula_8", "formula_text": "\u2713 (t)", "formula_coordinates": [3.0, 397.73, 358.14, 15.15, 6.12]}, {"formula_id": "formula_9", "formula_text": "\u2713 (t) = Ls (t) m (t) = Ds (t) .(6)", "formula_coordinates": [3.0, 351.9, 596.63, 206.1, 9.83]}, {"formula_id": "formula_10", "formula_text": "1: T 0 2: L RandomMatrix d,k , D RandomMatrix m,k 3: while some task Z (t) , m (t) is available do 4:", "formula_coordinates": [4.0, 58.98, 71.89, 213.91, 46.53]}, {"formula_id": "formula_11", "formula_text": "D updateD(D, s (t) , m (t) , \u21e2I dm , )14:", "formula_coordinates": [4.0, 54.5, 214.81, 203.9, 22.88]}, {"formula_id": "formula_12", "formula_text": "min L,D,S 1 T X t \uf8ff J \u21e3 \u2713 (t) \u2318 + \u21e2 m (t) Ds (t) 2 2 + \u00b5 s (t) 1 + (kLk 2 F + kDk 2 F ) ,(7)", "formula_coordinates": [4.0, 62.78, 362.78, 234.22, 56.35]}, {"formula_id": "formula_13", "formula_text": "min L,D,S 1 T X t \uf8ff \u21b5 (t) Ls (t) 2 (t) + \u21e2 m (t) Ds (t) 2 2 + \u00b5 s (t) 1 + (kLk 2 F + kDk 2 F ) .(8)", "formula_coordinates": [4.0, 55.56, 489.32, 241.44, 56.35]}, {"formula_id": "formula_14", "formula_text": "(t) = \uf8ff \u21b5 (t) m (t) K = \uf8ff L D A (t) = \uf8ff (t) 0 0 \u21e2I dm ,", "formula_coordinates": [4.0, 67.9, 558.59, 222.11, 29.89]}, {"formula_id": "formula_15", "formula_text": "min K,S 1 T X t \uf8ff (t) Ks (t) 2 A (t) + \u00b5 s (t) 1 + kKk 2 F . (9)", "formula_coordinates": [4.0, 58.68, 601.96, 238.32, 34.08]}, {"formula_id": "formula_16", "formula_text": "2 2 + \u00b5 ksk 1 3:\u2713 (tnew )", "formula_coordinates": [4.0, 319.98, 89.88, 219.74, 23.82]}, {"formula_id": "formula_17", "formula_text": "s (tnew ) arg min s m (t)Ds", "formula_coordinates": [4.0, 327.15, 359.6, 143.13, 11.85]}, {"formula_id": "formula_18", "formula_text": "2 2 + \u00b5 ksk 1 .", "formula_coordinates": [4.0, 475.82, 354.72, 53.43, 18.57]}, {"formula_id": "formula_19", "formula_text": "M (Q) = max 1\uf8ffi6 =j\uf8ffn \u21e3 |q > i qj | kqik2kqj k2 \u2318 2 [0, 1]", "formula_coordinates": [4.0, 326.15, 591.46, 180.11, 27.96]}, {"formula_id": "formula_20", "formula_text": "ks \u21e4 s + k 2 2 \uf8ff 4\u270f 2 1 M (Q)(4K 1)", "formula_coordinates": [5.0, 77.56, 124.99, 122.0, 19.81]}, {"formula_id": "formula_21", "formula_text": "l ij \u21e0 N (0, 2", "formula_coordinates": [5.0, 75.85, 305.85, 57.88, 16.6]}, {"formula_id": "formula_22", "formula_text": "totically M (Q) / q log(dk) d as d ! 1.", "formula_coordinates": [5.0, 54.0, 322.3, 170.29, 28.73]}, {"formula_id": "formula_23", "formula_text": "ks \u21e4 s + k 2 \uf8ff c 0 q K log k d", "formula_coordinates": [5.0, 54.0, 465.22, 105.01, 28.34]}, {"formula_id": "formula_24", "formula_text": "2 (d + d m ) 3", "formula_coordinates": [5.0, 80.42, 684.01, 46.35, 10.27]}, {"formula_id": "formula_25", "formula_text": "O(k 2 (d + d m ) 3 + \u21e0(d, n t ))", "formula_coordinates": [5.0, 54.0, 694.97, 110.85, 10.27]}], "doi": ""}