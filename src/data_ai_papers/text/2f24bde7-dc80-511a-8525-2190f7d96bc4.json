{"title": "Misspecification in Inverse Reinforcement Learning", "authors": "Joar Skalse; Alessandro Abate", "pub_date": "2023-03-24", "abstract": "The aim of Inverse Reinforcement Learning (IRL) is to infer a reward function R from a policy \u03c0. To do this, we need a model of how \u03c0 relates to R. In the current literature, the most common models are optimality, Boltzmann rationality, and causal entropy maximisation. One of the primary motivations behind IRL is to infer human preferences from human behaviour. However, the true relationship between human preferences and human behaviour is much more complex than any of the models currently used in IRL. This means that they are misspecified, which raises the worry that they might lead to unsound inferences if applied to real-world data. In this paper, we provide a mathematical analysis of how robust different IRL models are to misspecification, and answer precisely how the demonstrator policy may differ from each of the standard models before that model leads to faulty inferences about the reward function R. We also introduce a framework for reasoning about misspecification in IRL, together with formal tools that can be used to easily derive the misspecification robustness of new IRL models.", "sections": [{"heading": "Introduction", "text": "Inverse Reinforcement Learning (IRL) is an area of machine learning concerned with inferring what objective an agent is pursuing based on the actions taken by that agent (Ng and Russell 2000). IRL roughly corresponds to the notion of revealed preferences in psychology and economics, since it aims to infer preferences from behaviour (Rothkopf and Dimitrakakis 2011). IRL has many possible applications. For example, it has been used in scientific contexts, as a tool for understanding animal behaviour (Yamaguchi et al. 2018). It can also be used in engineering contexts; many important tasks can be represented as sequential decision problems, where the goal is to maximise a reward function over several steps (Sutton and Barto 2018). However, for many complex tasks, it can be very challenging to manually specify a reward function that incentivises the intended behaviour. IRL can then be used to learn a good reward function, based on demonstrations of correct behaviour (e.g. Abbeel, Coates, and Ng 2010;Singh et al. 2019). Overall, IRL relates to many fundamental questions about goaldirected behaviour and agent-based modelling.\nCopyright \u00a9 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nThere are two primary motivations for IRL. The first motivation is to use IRL as a tool for imitation learning (e.g. Hussein et al. 2017). For these applications, it is not fundamentally important whether the learnt reward function actually corresponds to the true intentions of the demonstrator, as long as it helps the imitation learning process. The second motivation is to use IRL to understand an agent's preferences and motives (e.g. Hadfield-Menell et al. 2016). From this perspective, the goal is to learn a reward that captures the demonstrator's true intentions. This paper was written with mainly the second motivation in mind.\nAn IRL algorithm must make assumptions about how the preferences of an agent relate to its behaviour. Most IRL algorithms are based on one of three models; optimality, Boltzmann rationality, or causal entropy maximisation. These behavioural models are very simple, whereas the true relationship between a person's preferences and their actions of course is incredibly complex. In fact, there are observable differences between human data and data synthesised using these standard assumptions (Orsini et al. 2021). This means that the behavioural models are misspecified, which raises the concern that they might systematically lead to flawed inferences if applied to real-world data.\nIn this paper, we study how robust the behavioural models in IRL are to misspecification. To do this, we first introduce a theoretical framework for analysing misspecification robustness in IRL. We then derive a number of formal tools for inferring the misspecification robustness of IRL models, and apply these tools to exactly characterise what forms of misspecification the standard IRL models are (or are not) robust to. Our analysis is general, as it is carried out in terms of behavioural models, rather than algorithms, which means that our results will apply to any algorithm based on these models. Moreover, the tools we introduce can also be used to easily derive the misspecification robustness of new behavioural models, beyond those we consider in this work.\nThe motivation behind this work is to provide a theoretically principled understanding of whether and when IRL methods are (or are not) applicable to the problem of inferring a person's (true) preferences and intentions. Human behaviour is very complex, and while a behavioural model can be more or less accurate, it will never be realistically possible to create a behavioural model that is completely free from misspecification (except possibly for in narrow do-mains). Therefore, if we wish to use IRL as a tool for preference elicitation, then it is crucial to have an understanding of how robust the IRL problem is to misspecification. In this paper, we contribute towards building this understanding.", "publication_ref": ["b14", "b24", "b22", "b0", "b19", "b9", "b8", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "It is well-known that the standard behavioural models of IRL are misspecified in most applications. However, there has nonetheless so far not been much research on this topic. Freedman, Shah, and Dragan (2021) study the effects of choice set misspecification in IRL (and reward inference more broadly), following the formalism of Jeon, Milli, and Dragan (2020). Our work is wider in scope, and aims to provide necessary and sufficient conditions which fully describe the kinds of misspecification to which each behavioural model is robust. In the field of statistics more broadly, misspecification is a widely studied issue (White 1994).\nThere has been a lot of work on reducing misspecification in IRL. One approach to this is to manually add more detail to the models (Evans, Stuhlmueller, and Goodman 2015;Chan, Critch, and Dragan 2019), and another approach is to try to learn the behavioural model from data (Armstrong and Mindermann 2019;Shah et al. 2019). In contrast, our work aims to understand how sensitive IRL is to misspecification (and thus to answer the question of how much misspecification has to be removed). Skalse et al. (2022a) study the partial identifiability of various reward learning models. Our work uses similar techniques, and can be viewed as an extension of their work. The issue of partial identifiability in IRL has also been studied by Ng and Russell (2000); Dvijotham and Todorov (2010); Cao, Cohen, and Szpruch (2021); Kim et al. (2021).\nWe will discuss the question of what happens if a reward function is changed or misspecified. This question is also investigated by many previous works, including e.g. Gleave et al. (2020); Skalse et al. (2022b);Jenner, van Hoof, and Gleave (2022); Pan, Bhatia, and Steinhardt (2022).", "publication_ref": ["b6", "b11", "b23", "b5", "b3", "b1", "b18", "b20", "b14", "b4", "b2", "b12", "b7", "b21", "b10", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "A Markov Decision Processes (MDP) is a tuple (S, A, \u03c4, \u00b5 0 , R, \u03b3) where S is a set of states, A is a set of actions, \u03c4 : S\u00d7A S is a transition function, \u00b5 0 \u2208 \u2206(S) is an initial state distribution, R : S\u00d7A\u00d7S \u2192 R is a reward function, and \u03b3 \u2208 (0, 1] is a discount rate. Here f : X Y denotes a probabilistic mapping from X to Y . In this paper, we assume that S and A are finite. A policy is a function \u03c0 : S A. A trajectory \u03be = s 0 , a 0 , s 1 , a 1 . . . is a possible path in an MDP. The return function G gives the cumulative discounted reward of a trajectory, G(\u03be) = \u221e t=0 \u03b3 t R(s t , a t , s t+1 ), and the evaluation function J gives the expected trajectory return given a policy, J (\u03c0) = E \u03be\u223c\u03c0 [G(\u03be)]. A policy maximising J is an optimal policy. The value function V \u03c0 : S \u2192 R of a policy encodes the expected future discounted reward from each state when following that policy. The Q-function is Q \u03c0 (s, a) = E [R(s, a, S \u2032 ) + \u03b3V \u03c0 (S \u2032 )], and the advantage function is A \u03c0 (s, a) = Q \u03c0 (s, a) \u2212 V \u03c0 (s). Q \u22c6 , V \u22c6 , and A \u22c6 denote the Q-, value, and advantage functions of the optimal policies. In this paper, we assume that all states in S are reachable under \u03c4 and \u00b5 0 .\nIn IRL, it is typically assumed that the preferences of the observed agent are described by a reward function R, that its environment is described by an MDP, and that its behaviour is described by a (stationary) policy \u03c0. An IRL algorithm also needs a behavioural model of how \u03c0 relates to R. In the current IRL literature, the most common models are:\n1. Optimality: We assume that \u03c0 is optimal under R (e.g. Ng and Russell (2000)). 2. Boltzmann Rationality: We assume that P(\u03c0(s) = a) \u221d e \u03b2Q \u22c6 (s,a) , where \u03b2 is a temperature parameter (e.g. Ramachandran and Amir ( 2007)).", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "Maximal Causal Entropy:", "text": "We assume that \u03c0 maximises the causal entropy objective, which is given by\nE[ \u221e t=0 \u03b3 t (R(s t , a t , s t+1 ) + \u03b1H(\u03c0(s t+1 )))]\n, where \u03b1 is a weight and H is the Shannon entropy function (e.g. Ziebart (2010)).\nIn this paper, we will often talk about pairs or sets of reward functions. In these cases, we will give each reward function a subscript R i , and use J i , V \u22c6 i , and V \u03c0 i , and so on, to denote R i 's evaluation function, optimal value function, and \u03c0 value function, and so on.", "publication_ref": ["b25"], "figure_ref": [], "table_ref": []}, {"heading": "Theoretical Framework", "text": "We here introduce the theoretical framework that we will use to analyse how robust various behavioural models are to misspecification. This framework is rather abstract, but it is quite powerful, and makes our analysis easy to carry out.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definitions and Framework", "text": "For a given set of states S and set of actions A, let R be the set of all reward functions R : S\u00d7A\u00d7S \u2192 R definable with S and A. Moreover, if P and Q are partitions of a set X, we write P Q if x 1 \u2261 P x 2 \u21d2 x 1 \u2261 Q x 2 for x 1 , x 2 \u2208 X. We will use the following definitions: 1. A reward object is a function f : R \u2192 X, where X is any set. 2. The ambiguity Am(f ) of f is the partition of R given by\nR 1 \u2261 f R 2 \u21d0\u21d2 f (R 1 ) = f (R 2 ). 3. Given a partition P of R, we say that f is P -admissible if Am(f ) P , i.e. f (R 1 ) = f (R 2 ) \u21d2 R 1 \u2261 P R 2 . 4\n. Given a partition P of R, we say that f is P -robust to misspecification with g if f is P -admissible, f = g, Im(g) \u2286 Im(f ), and f (R 1 ) = g(R 2 ) =\u21d2 R 1 \u2261 P R 2 . 5. A reward transformation is a function t : R \u2192 R. 6. If F and G are sets of reward transformations, then F \u2022 G is the set of all transformations that can be obtained by composing transformations in F and G arbitrarily, in any order. Note that F \u2022 G = G \u2022 F .\nWe will now explain and justify each of these definitions. First of all, anything that can be computed from a reward function can be seen as a reward object. For example, we could consider a function b that, given a reward R, returns the Boltzmann-rational policy with temperature \u03b2 in the MDP S, A, \u03c4, \u00b5 0 , R, \u03b3 , or a function r that, from R, gives the return function G in the MDP S, A, \u03c4, \u00b5 0 , R, \u03b3 . This makes reward objects a versatile abstract building block for more complex constructions. We will mainly, but not exclusively, consider reward objects with the type R \u2192 \u03a0, i.e. functions that compute policies from rewards. We can use reward objects to create an abstract model of a reward learning algorithm L as follows; first, we assume, as reasonable, that there is a true underlying reward function R \u22c6 , and that the observed training data is generated by a reward object g, so that L observes g(R \u22c6 ). Here g(R \u22c6 ) could be a distribution, which models the case where L observes a sequence of random samples from some source, but it could also be a single, finite object. Next, we suppose that L has a model f of how the observed data relates to R \u22c6 , where f is also a reward object, and that L learns (or converges to) a reward function R H such that f (R H ) = g(R \u22c6 ). If f = g then f is misspecified, otherwise f is correctly specified. Note that this primarily is a model of the asymptotic behaviour of learning algorithms, in the limit of infinite data.\nThere are two ways to interpret Am(f ). First, we can see it as a bound on the amount of information we can get about R \u22c6 by observing (samples from) f (R \u22c6 ). For example, multiple reward functions might result in the same Boltzmannrational policy. Thus, observing trajectories from that policy could never let us distinguish between them: this ambiguity is described by Am(b). We can also see Am(f ) as the amount of information we need to have about R \u22c6 to construct f (R \u22c6 ). Next, if Am(f ) Am(g) and f = g, this means that we get less information about R \u22c6 by observing g(R \u22c6 ) than f (R \u22c6 ), and that we would need more information to construct f (R \u22c6 ) than g(R \u22c6 ). For an extensive discussion about these notions, see Skalse et al. (2022a).\nIntuitively, we want to say that a behavioural model is robust to some type of misspecification if an algorithm based on that model will learn a reward function that is \"close enough\" to the true reward function when subject to that misspecification. To formalise this intuitive statement, we first need a definition of what it should mean for two reward functions to be \"close enough\". In this work, we have chosen to define this in terms of equivalence classes. Specifically, we assume that we have a partition P of R (which, of course, corresponds to an equivalence relation), and that the learnt reward function R H is \"close enough\" to the true reward R \u22c6 if they are in the same class, R H \u2261 P R \u22c6 . We will for now leave open the question of which partition P of R to pick, and later revisit this question in Section 2.4.\nGiven this, we can now see that our definition of Padmissibility is equivalent to stating that a learning algorithm L based on f is guaranteed to learn a reward function that is P -equivalent to the true reward function when there is no misspecification. Furthermore, our definition of Probustness says that f is P -robust to misspecification with g if any learning algorithm L based on f is guaranteed to learn a reward function that is P -equivalent to the true reward function when trained on data generated from g. The requirement that Im(g) \u2286 Im(f ) ensures that the learning algorithm L is never given data that is impossible according to its model. Depending on how L reacts to such data, it may be possible to drop this requirement. We include it, since we want our analysis to apply to all algorithms. The requirement that f is P -admissible is included to rule out some uninteresting edge cases.\nReward transformations can be used to characterise the ambiguity of reward objects, or define other partitions of R. Specifically, we say that a partition P corresponds to a set of reward transformations T P if T P contains all reward transformations t that satisfy t(R) \u2261 P R. If P is the ambiguity of f then T P would be the set of all reward transformations that satisfy f (R) = f (t(R)).", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "Fundamental Lemmas", "text": "We here give two fundamental lemmas that we will later use to prove our core results. These lemmas can also be used to easily derive the misspecification robustness of new models, beyond those considered in this work. All of our proofs are provided in the supplementary material, which also contains several additional results about our framework.\nLemma 2.1. If f is not P -robust to misspecification with g, and Im(g) \u2286 Im(f ), then for any h, h \u2022 f is not P -robust to misspecification with h \u2022 g. This lemma states that if we have an object h \u2022 f that can be computed from some intermediary object f , and f is not P -robust to some form of misspecification, then h \u2022 f is likewise not robust to the corresponding misspecification. In other words, any misspecification that f is sensitive to, is \"inherited\" by all objects that can be computed from f . Lemma 2.2. If f is P -admissible, and T is the set of all reward transformations that preserve P , then f is P -robust to misspecification with g if and only if g = f \u2022 t for some t \u2208 T where f \u2022 t = f . This lemma gives us a very powerful tool for characterising the misspecification robustness of reward objects. Specifically, we can derive the set of objects to which f is P -robust by first deriving the set T of all transformations that preserve P , and then composing f with each t \u2208 T .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Reward Transformations", "text": "We here introduce several classes of reward transformations, that we will later use to express our results. First recall potential shaping (Ng, Harada, and Russell 1999):\nDefinition 2.3 (Potential Shaping). A potential function is a function \u03a6 : S \u2192 R. Given a discount \u03b3, we say that R 2 \u2208 R is produced by potential shaping of R 1 \u2208 R if for some potential \u03a6, R 2 (s, a, s \u2032 ) = R 1 (s, a, s \u2032 ) + \u03b3 \u2022 \u03a6(s \u2032 ) \u2212 \u03a6(s).\nPotential shaping is widely used for reward shaping. We next define two classes of transformations that were used by Skalse et al. (2022a), starting with S \u2032 -redistribution. Definition 2.4 (S \u2032 -Redistribution). Given a transition function \u03c4 , we say that R 2 \u2208 R is produced by S \u2032 -redistribution\nof R 1 \u2208 R if E S \u2032 \u223c\u03c4 (s,a) [R 1 (s, a, S \u2032 )] = E S \u2032 \u223c\u03c4 (s,a) [R 2 (s, a, S \u2032 )] .\nIf s 1 , s 2 \u2208 Supp(\u03c4 (s, a)) then S \u2032 -redistribution can increase R(s, a, s 1 ) if it decreases R(s, a, s 2 ) proportionally. S \u2032 -redistribution can also change R arbitrarily for transitions that occur with probability 0. We next consider optimalitypreserving transformations: Definition 2.5. Given a transition function \u03c4 and a discount \u03b3, we say that R 2 \u2208 R is produced by an optimalitypreserving transformation of R 1 \u2208 R if there exists a function \u03c8 : S \u2192 R such that\nE S \u2032 \u223c\u03c4 (s,a) [R 2 (s, a, S \u2032 ) + \u03b3 \u2022 \u03c8(S \u2032 )] \u2264 \u03c8(s),\nwith equality if and only if a \u2208 argmax a\u2208A A \u22c6 1 (s, a). An optimality preserving transformation of R 1 lets us pick an arbitrary new value function \u03c8, and then adjust R 2 in any way that respects the new value function and the argmax of A \u22c6 1 -the latter condition ensures that the same actions (and hence the same policies) stay optimal.\nBased on these definitions, we can now specify several sets of reward transformations:\n1. Let PS \u03b3 be the set of all reward transformations t such that t(R) is given by potential shaping of R relative to the discount \u03b3. 2. Let S \u2032 R \u03c4 be the set of all reward transformations t such that t(R) is given by S \u2032 -redistribution of R relative to the transition function \u03c4 . 3. Let LS be the set of all reward transformations t that scale each reward function by some positive constant, i.e. for each R there is a c \u2208 R + such that t(R)(s, a, s \u2032 ) = c \u2022 R(s, a, s \u2032 ).\n4. Let CS be the set of all reward transformations t that shift each reward function by some constant, i.e. for each R there is a c \u2208 R such that t(R)(s, a, s \u2032 ) = R(s, a, s \u2032 ) + c. 5. Let OP \u03c4,\u03b3 be the set of all reward transformations t such that t(R) is given by an optimality-preserving transformation of R relative to \u03c4 and \u03b3.\nNote that these sets are defined in a way that allows their transformations to be \"sensitive\" to the reward function it takes as input. For example, a transformation t \u2208 PS \u03b3 might apply one potential function \u03a6 1 to R 1 , and a different potential function \u03a6 2 to R 2 . Similarly, a transformation t \u2208 LS might scale R 1 by a positive constant c 1 , and R 2 by a different constant c 2 , etc. Note also that CS \u2286 PS \u03b3 (for all \u03b3), and that all sets are subsets of OP \u03c4,\u03b3 (see Skalse et al. 2022a).", "publication_ref": ["b13", "b20", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Two Equivalence Classes for Reward Functions", "text": "Our definition of misspecification robustness is given relative to an equivalence relation on R. In this section, we define two important equivalence classes, and characterise the transformations that preserve them. Our later results will be given relative to these two equivalence classes. Given an environment M = S, A, \u03c4, \u00b5 0 , , \u03b3 and two reward functions R 1 , R 2 , we say that R 1 \u2261 OPT M R 2 if S, A, \u03c4, \u00b5 0 , R 1 , \u03b3 and S, A, \u03c4, \u00b5 0 , R 2 , \u03b3 have the same optimal policies, and that R 1 \u2261 ORD M R 2 if they have the same ordering of policies. Skalse et al. (2022a) showed that R \u2261 OPT M t(R) for all R if and only if t \u2208 OP \u03c4,\u03b3 (their Theorem 3.16). We characterise the transformations that preserve ORD M , which is a novel contribution. Theorem 2.6. R 1 \u2261 ORD M R 2 if and only if R 2 = t(R 1 ) for some t \u2208 S \u2032 R \u03c4 \u2022 PS \u03b3 \u2022 LS.\n1 Note that if R 1 \u2261 ORD M R 2 then R 1 \u2261 OPT M R 2 .\nStated differently, Theorem 2.6 is saying that the MDPs (S, A, \u03c4, \u00b5 0 , R 1 , \u03b3) and (S, A, \u03c4, \u00b5 0 , R 2 , \u03b3) have the same ordering of policies if and only if R 1 and R 2 differ by potential shaping (with \u03b3), positive linear scaling, and S \u2032redistribution (with \u03c4 ), applied in any order.\nOPT M and ORD M are two equivalence relations that should be relevant and informative in almost any context, which is why we have chosen to carry out our analysis in terms of these two relations. However, other partitions could be selected instead. For example, if we know that the learnt reward R H will be used to compute a reward object f , then Am(f ) would be a natural choice.\nWe now have results for reasoning about misspecification robustness in IRL. In particular, Lemma 2.2 tells us that if we want to find the functions that f is P -robust to misspecification with, then all we need to do is find the reward transformations that preserve P , and then compose them with f . OPT M and ORD M are reasonable choices of P , and the transformations that preserve them were just provided.", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "Misspecification Robustness of IRL Models", "text": "We here give our main results on the misspecification robustness of IRL, looking both at misspecification of the behavioural model, as well as of the MDP.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Misspecified Behavioural Models", "text": "Let \u03a0 + be the set of all policies such that \u03c0(a | s) > 0 for all s, a, let M = S, A, \u03c4, \u00b5 0 , , \u03b3 , and let F M be the set of all functions f M : R \u2192 \u03a0 + that, given R, returns a policy \u03c0 which satisfies\nargmax a\u2208A \u03c0(a | s) = argmax a\u2208A Q \u22c6 (s, a),\nwhere Q \u22c6 is the optimal Q-function in S, A, \u03c4, \u00b5 0 , R, \u03b3 . In other words, F M is the set of functions that generate policies which take each action with positive probability, and that take the optimal actions with the highest probability. This class is quite large, and includes e.g. Boltzmannrational policies (for any \u03b2), but it does not include optimal policies (since they do not take all actions with positive probability) or causal entropy maximising policies (since they may take suboptimal actions with high probability).\nTheorem 3.1. Let f M \u2208 F M be surjective onto \u03a0 + . Then f M is OPT M -robust to misspecification with g if and only if g \u2208 F M and g = f M .\nBoltzmann-rational policies are surjective onto \u03a0 + , 2 so Theorem 3.1 exactly characterises the misspecification to which the Boltzmann-rational model is OPT M -robust. Let us briefly comment on the requirement that \u03c0(a | s) > 0, which corresponds to the condition that Im(g) \u2286 Im(f ) in our definition of misspecification robustness. If a learning algorithm L is based on a model f : R \u2192 \u03a0 + then it assumes that the observed policy takes each action with positive probability in every state. What happens if such an algorithm L is given data from a policy that takes some action with probability 0? This depends on L, but for most sensible algorithms the result should simply be that L assumes that those actions are taken with a positive but low probability. This means that it should be possible to drop the requirement that \u03c0(a | s) > 0 for most reasonable algorithms.\nWe next turn our attention to the misspecification to which the Boltzmann-rational model is ORD M -robust. Let \u03c8 : R \u2192 R + be any function from reward functions to positive real numbers, and let b M \u03c8 : R \u2192 \u03a0 + be the function that, given R, returns the Boltzmann-rational policy with temperature \u03c8(R) in S, A, \u03c4, \u00b5 0 , R, \u03b3 . Moreover, let\nB M = {b M \u03c8 : \u03c8 \u2208 R \u2192 R + } be the set of all such func- tions b M \u03c8 .\nThis set includes Boltzmann-rational policies; just let \u03c8 return a constant \u03b2 for all R.\nTheorem 3.2. If b M \u03c8 \u2208 B M then b M \u03c8 is ORD M -robust to misspecification with g if and only if g \u2208 B M and g = b M\n\u03c8 . This means that the Boltzmann-rational model is ORD Mrobust to misspecification of the temperature parameter \u03b2, but not to any other form of misspecification.\nWe next turn our attention to optimal policies. First of all, a policy is optimal if and only if it only gives support to optimal actions, and if an optimal policy gives support to multiple actions in some state, then we would normally not expect the exact probability it assigns to each action to convey any information about the reward function. We will therefore only look at the actions that the optimal policy takes, and ignore the relative probability it assigns to those actions. Formally, we will treat optimal policies as functions \u03c0 \u22c6 : S \u2192 P(argmax a\u2208A A \u22c6 ) \u2212 {\u2205}; i.e. as functions that for each state return a non-empty subset of the set of all actions that are optimal in that state. Let O M be the set of all functions that return such policies, and let o M m \u2208 O M be the function that, given R, returns the function that maps each state to the set of all actions which are optimal in that state. Intuitively, o M m corresponds to optimal policies that take all optimal actions with positive probability.\nTheorem 3.3. No function in O M is ORD M -admissible. The only function in O M that is OPT M -admissible is o M m , but o M\nm is not OPT M -robust to any misspecification. This essentially means that the optimality model is not robust to any form of misspecification. We finally turn our attention to causal entropy maximising policies. As before, let \u03c8 : R \u2192 R + be any function from reward functions to positive real numbers, and let c M \u03c8 : R \u2192 \u03a0 + be the function that, given R, returns the causal entropy maximising policy with weight \u03c8(R) in S, A, \u03c4, \u00b5 0 , R, \u03b3 . Furthermore, let C M = {c M \u03c8 : \u03c8 \u2208 R \u2192 R + } be the set of all such functions c M \u03c8 . This set includes causal entropy maximising policies; just let \u03c8 return a constant \u03b1 for all R. Theorem 3.4. If c M \u03c8 \u2208 C M then c M \u03c8 is ORD M -robust to misspecification with g if and only if g \u2208 C M and g = c M \u03c8 . In other words, the maximal causal entropy model is ORD M -robust to misspecification of the weight \u03b1, but not to any other kind of misspecification.\nFinally, let us briefly discuss the misspecification to which the maximal causal entropy model is OPT M -robust. Lemma 2.2 tells us that c M \u03c8 \u2208 C M is OPT M -robust to misspecification with g if g = c M \u03c8 \u2022 t for some t \u2208 OP \u03c4,\u03b3 . In other words, if g(R 1 ) = \u03c0 then there must exist an R 2 such that \u03c0 maximises causal entropy with respect to R 2 , and such that R 1 and R 2 have the same optimal policies. It seems hard to express this as an intuitive property of g, so we have refrained from stating this result as a theorem.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Misspecified MDPs", "text": "A reward object can be parameterised by a \u03b3 or \u03c4 , implicitly or explicitly. For example, the reward objects in Section 3.1 are parameterised by M = S, A, \u03c4, \u00b5 0 , , \u03b3 . In this section, we explore what happens if these parameters are misspecified. We show that nearly all behavioural models are sensitive to this type of misspecification.\nTheorems 3.1-3.4 already tell us that the standard behavioural models are not (ORD M or OPT M ) robust to misspecified \u03b3 or \u03c4 , since the sets F M , B M , and C M , all are parameterised by \u03b3 and \u03c4 . We will generalise this further. To do this, we first derive two lemmas. We say that \u03c4 is trivial\nif for each s \u2208 S, \u03c4 (s, a) = \u03c4 (s, a \u2032 ) for all a, a \u2032 \u2208 A. Lemma 3.5. If f \u03c41 = f \u03c41 \u2022 t for all t \u2208 S \u2032 R \u03c41 then f \u03c41 is not OPT M -admissible for M = S, A, \u03c4 2 , \u00b5 0 , , \u03b3 unless \u03c4 1 = \u03c4 2 . Lemma 3.6. If f \u03b31 = f \u03b31 \u2022 t for all t \u2208 PS \u03b31 then f \u03b31 is not OPT M -admissible for M = S, A, \u03c4, \u00b5 0 , , \u03b3 2 unless \u03b3 1 = \u03b3 2 or \u03c4 is trivial.\nNote that if f is not OPT M -admissible then f is also not ORD M -admissible. From these lemmas, together with Lemma 2.1, we get the following result:\nTheorem 3.7. If f \u03c41 = f \u03c41 \u2022 t for all t \u2208 S \u2032 R \u03c41 and f \u03c42 = f \u03c42 \u2022 t for all t \u2208 S \u2032 R \u03c42 , then f \u03c41 is not OPT M -robust to misspecification with f \u03c42 for any M. Moreover, if f \u03b31 = f \u03b31 \u2022 t for all t \u2208 PS \u03b31 and f \u03b32 = f \u03b32 \u2022 t for all t \u2208 PS \u03b32 , then f \u03b31 is not OPT M -robust to misspecification with f \u03b32 for any M whose transition function \u03c4 is non-trivial.\nIn other words, if a behavioural model is insensitive to S \u2032 -redistribution, then that model is not OPT M -robust (and therefore also not ORD M -robust) to misspecification of the transition function \u03c4 . Similarly, if the behavioural model is insensitive to potential shaping, then that model is not OPT M -robust (and therefore also not ORD M -robust) to misspecification of the discount parameter \u03b3. Note that all transformations in S \u2032 R \u03c4 and PS \u03b3 preserve the ordering of policies. This means that an IRL algorithm must specify \u03c4 and \u03b3 correctly in order to guarantee that the learnt reward R H has the same optimal policies as the true underlying reward R * , unless the algorithm is based on a behavioural model which says that the observed policy depends on features of R which do not affect its policy ordering. This should encompass most natural behavioural models.\nThat being said, we note that this result relies on the requirement that the learnt reward function should have exactly the same optimal policies, or ordering of policies, as the true reward function. If \u03b3 1 \u2248 \u03b3 2 and \u03c4 1 \u2248 \u03c4 2 , then the learnt reward function's optimal policies and policy ordering will presumably be similar to that of the true reward function. Analysing this case is beyond the scope of this paper, but we consider it to be an important topic for further work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Generalising the Analysis", "text": "In this section, we discuss different ways to generalise our results. We consider what happens if R is restricted to a subset of R, what might happen if R is drawn from a known prior distribution and the learning algorithm has a known inductive bias, and whether we can use stronger equivalence classes to guarantee various forms of transfer learning.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Restricted Reward Functions", "text": "Here, we discuss what happens if the reward function is restricted to belong to some subset of R, i.e. if we know that R \u2208R for someR \u2286 R. For example, it is common to consider reward functions that are linear in some state features. It is also common to define the reward function over a restricted domain, such as S\u00d7A; this would correspond to restricting R to the set of reward functions such that R(s, a, s \u2032 ) = R(s, a, s \u2032\u2032 ) for all s, a, s \u2032 , s \u2032\u2032 . As we will see, our results are largely unaffected by such restrictions.\nWe first need to generalise the framework, which is straightforward. Given partitions P , Q of R, reward objects f , g, and setR \u2286 R, we say that P Q onR if R 1 \u2261 P R 2 implies R 1 \u2261 Q R 2 for all R 1 , R 2 \u2208R, that f is P -admissible onR if Am(f ) P onR, and that f is P -robust to misspecification with g onR if f is Padmissible onR, f |R = g|R, Im(g|R) \u2286 Im(f |R), and f (R 1 ) = g(R 2 ) =\u21d2 R 1 \u2261 P R 2 for all R 1 , R 2 \u2208R.\nAll lemmas in Section 2.2 apply with these more general definitions for any arbitrary subsetR \u2286 R. Moreover, the theorems in Section 3 also carry over very directly: Theorem 4.1. If f is P -robust to misspecification with g on R then f is P -robust to misspecification with g \u2032 on R for some g \u2032 where g \u2032 |R = g|R, unless f is not P -admissible on R. If f is P -robust to misspecification with g on R then f is P -robust to misspecification with g onR, unless f |R = g|R.\nThe intuition for this theorem is that if f is P -robust to misspecification with g if and only if g \u2208 G, then f is Probust to misspecification with g \u2032 onR if and only if g \u2032 behaves like some g \u2208 G for all R \u2208R. Restricting R does therefore not change the problem in any significant way.\nIf an equivalence relation P of R is characterised by a set of reward transformations T , then the corresponding equivalence relation onR is characterised by the set of reward transformations {t \u2208 T : Im(t|R) \u2286R}; this can be used to generalise Theorem 2.6. However, here there is a minor subtlety to be mindful of: (A \u2022 B) \u2212 C is not necessarily equal to (A \u2212 C) \u2022 (B \u2212 C). This means that if we wish to specify {t \u2208 A \u2022 B : Im(t|R) \u2286R}, then we cannot do this by simply removing the transformations where Im(t|R) \u2286R from each of A and B. For example, consider the transformations S \u2032 R \u03c4 \u2022 PS \u03b3 restricted to the spac\u00ea R of reward functions where R(s, a, s \u2032 ) = R(s, a, s \u2032\u2032 ), i.e. to reward functions over the domain S\u00d7A. The only transformation in S \u2032 R \u03c4 onR is the identity mapping, and the only transformations in PS \u03b3 onR are those where \u03a6 is constant over all states. However, S \u2032 R \u03c4 \u2022 PS \u03b3 onR contains all transformations where \u03a6 is selected arbitrarily, and t(R)(s, a, s \u2032 ) is set to R(s, a, s \u2032 ) + \u03b3E [\u03a6(S \u2032 )] \u2212 \u03a6(s). This means that there probably are no general shortcuts for deriving {t \u2208 T : Im(t|R) \u2286R} for arbitraryR.\nIt should be noted that our negative results (i.e., those in Section 3.2) might not hold if R is restricted. Recall that f is not P -robust to misspecification with g if there exist R 1 , R 2 such that g(R 1 ) = f (R 2 ), but R 1 \u2261 P R 2 . If R is restricted, it could be the case that all such counterexamples are removed. For example, if we restrict R to e.g. the set R of reward functions that only reward a single transition, then Lemma 3.6, and the corresponding part of Theorem 3.7, no longer apply. 3 This means that, if the reward function is guaranteed to lie in this setR, then a behavioural model may still be OPT M -robust to a misspecified discount parameter. However, the reason for this is simply that the discount parameter no longer affects which policies are optimal if there is only a single transition that has non-zero reward.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Known Prior and Inductive Bias", "text": "So far, we have assumed that we do not know which distribution R is sampled from, or which inductive bias the learning algorithm L has. In this section, we discuss what might happen if we lift these assumptions.\nTo some extent, our results in Section 4.1 can be used to understand this setting as well. Suppose we have a set R \u2286 R of \"likely\" reward functions, such that P(R \u22c6 \u2208 R) = 1 \u2212 \u03b4, and such that the learning algorithm L returns a reward function R H inR if there exists an R H \u2208R such that f (R H ) = g(R \u22c6 ). Then if f is P -robust to misspecification with g onR, it follows that L returns an R H such that R H \u2261 P R \u22c6 with probability at least 1 \u2212 \u03b4.\nSo, for example, supposeR is the set of all reward functions that are \"sparse\", for some way of formalising that property. Then this tells us, informally, that if the underlying reward function is likely to be sparse, and if L will attempt to fit a sparse reward function to its training data, then it is sufficient that f is P -robust to misspecification with g on the set of all sparse reward functions, to ensure that the learnt reward function R H is P -equivalent to the true reward function with high probability. It seems likely that more specific claims could be made about this setting, but we leave such analysis as a topic for future work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Transfer to New Environments", "text": "The equivalence relations we have worked with (OPT M and ORD M ) only guarantee that the learnt reward function R H has the same optimal policies, or ordering of policies, as the true reward R \u22c6 in a given environment M = S, A, \u03c4, \u00b5 0 , , \u03b3 . A natural question is what happens if we strengthen this requirement, and demand that R H has the same optimal policies, or ordering of policies, as R \u22c6 , for any choice of \u03c4 , \u00b5 0 , or \u03b3. We discuss this setting here.\nIn short, it is impossible to guarantee transfer to any \u03c4 or \u03b3 within our framework, and trivial to guarantee transfer to any \u00b5 0 . First, the lemmas provided in Section 3.2 tell us that none of the standard behavioural models are OPT Madmissible when \u03c4 or \u03b3 is different from that of the training environment. This means that none of them can guarantee that R H has the same optimal policies (or ordering of policies) as R \u22c6 if \u03c4 or \u03b3 is changed, with or without misspecification. Second, if R 1 \u2261 ORD M R 2 or R 1 \u2261 OPT M R 2 , then this remains the case if \u00b5 0 is changed. We can thus trivially guarantee transfer to arbitrary \u00b5 0 .\nWe would also like to remark on a subtlety regarding Theorem 2.6. One might expect that two reward functions R 1 and R 2 must have the same policy ordering for all \u03c4 if and only if they differ by potential shaping and linear scaling. However, this is not the case. To see this, consider the rewards R 1 , R 2 where R 1 (s 1 , a 1 , s 1 ) = 1, R 1 (s 1 , a 1 , s 2 ) = 0.5, R 2 (s 1 , a 1 , s 1 ) = 0.5, and R 2 (s 1 , a 1 , s 2 ) = 1, and where R 1 and R 2 are 0 for all other transitions. Now R 1 and R 2 do not differ by potential shaping and linear scaling, yet they have the same policy order for all \u03c4 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discussion", "text": "In this section, we discuss the implications of our results, as well as their limitations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions and Implications", "text": "We have shown that the misspecification robustness of the behavioural models in IRL can be quantified and understood. Our results show that the Boltzmann-rational model is substantially more robust to misspecification than the optimality model; the optimality model is not robust to any misspecification, whereas the Boltzmann-rationality model is at least OPT M -robust to many kinds of misspecification. This is not necessarily unexpected, but we now have formal guarantees to back this intuition. We have also quantified the misspecification robustness of the maximal causal entropy model, and found that it lies somewhere between that of the Boltzmann-rational model and the optimality model.\nWe have shown that none of the standard models are robust to a misspecified \u03c4 or \u03b3. Moreover, we need to make very minimal assumptions about how the demonstrator policy is computed to obtain this result, which means that it is likely to generalise to new behavioural models as well. We find this quite surprising; the discount \u03b3 is typically selected in a somewhat arbitrary way, and it can often be difficult to establish post-facto which \u03b3 was used to compute a given policy. The fact that \u03c4 must be specified correctly is somewhat less surprising, yet important to have established.\nIn addition to these contributions, we have also provided several formal tools for deriving the misspecification robustness of new behavioural models, in the form of the lemmas in Section 2.2. In particular, if we have a model f , and we wish to use the learnt reward to compute an object g, then we can obtain an expression of the set of all functions to which f is robust in the following way; first, derive Am(g), and then characterise this partition of R using a set of reward transformations T . Then, as per Lemma 2.2, we can obtain the functions that f is robust to misspecification with by simply composing f with each t \u2208 T . If we want to know which functions f is robust to misspecification with in a strong sense, then we can obtain an informative answer to this question by composing f with the transformations that preserve the ordering of all policies, which in turn is provided by Theorem 2.6. Lemma 2.1 also makes it easier to intuitively reason about the robustness properties of various kinds of behavioural models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations and Further Work", "text": "Our analysis makes a few simplifying assumptions, that could be ideally lifted in future work. First of all, we have been working with equivalence relations on R, where two reward functions are either equivalent or not. It might be fruitful to instead consider distance metrics on R: this could make it possible to obtain results such as e.g. bounds on the distance between the true reward function and the learnt reward function, given various forms of misspecification. We believe it would be especially interesting to re-examine Theorem 3.7 through this lens.\nAnother notable direction for extensions could be to further develop the analysis in Section 4.2, and study the misspecification robustness of different behavioural models in the context where we have particular, known priors concerning R. Our comments on this setting are fairly preliminary, and it might be possible to draw additional, interesting conclusions if this setting is explored more extensively.\nMoreover, we have studied the behaviour of algorithms in the limit of infinite data, under the assumption that this is similar to their behaviour in the case of finite but sufficiently large amounts of data. Therefore, another possible extension could be to more rigorously examine the properties of these models in the case of finite data.\nFinally, our analysis has of course been limited to the behavioural models that are currently most popular in IRL (optimality, Boltzmann rationality, and causal entropy maximisation) and two particular equivalence relations (OPT M and ORD M ). Another direction for extensions would be to broaden our analysis to larger classes of models, and perhaps also to more equivalence relations. In particular, it would be interesting to analyse more realistic behavioural models, which incorporate e.g. prospect theory (Kahneman and Tversky 1979) or hyperbolic discounting.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Proofs", "text": "In this Appendix, we provide the proofs of all our results, as well as of some additional lemmas.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Fundamental Lemmas", "text": "We here prove the fundamental lemmas from Section 2.2, as well as some additional lemmas. Our proofs in this section are given relative to the somewhat more general definitions of P -robustness and refinement given in Section 4.1, rather than those given in Section 2.1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Additional Lemmas", "text": "We here provide a few extra lemmas, which are straightforward to prove, but worth spelling out.\nLemma A.1. For any f and h, if f is not P -admissible on R then h \u2022 f is not P -admissible onR.\nProof. If f is not P -admissible onR then there are\nR 1 , R 2 \u2208R such that f (R 1 ) = f (R 2 ), but R 1 \u2261 P R 2 . But if f (R 1 ) = f (R 2 ) then h \u2022 f (R 1 ) = h \u2022 f (R 2 ), so there are R 1 , R 2 \u2208R such that h \u2022 f (R 1 ) = h \u2022 f (R 2 ), but R 1 \u2261 P R 2 .\nThus h \u2022 f is not P -admissible onR.\nLemma A.2. If f is P -robust to misspecification with g on R then g is P -admissible onR.\nProof. Suppose that f is P -robust to misspecification with g onR, and let R 1 , R 2 \u2208R be any two reward functions such that g(R 1 ) = g(R 2 ). Since Im(g|R) \u2286 Im(f |R) there is an R 3 \u2208R such that f (R 3 ) = g(R 1 ) = g(R 2 ). Since f is P -robust to misspecification with g onR, it must be the case that R 3 \u2261 P R 1 and R 3 \u2261 P R 2 . By transitivity, we thus have that R 1 \u2261 P R 2 . Since R 1 and R 2 were chosen arbitrarily, it must be that R 1 \u2261 P R 2 whenever g(R 1 ) = g(R 2 ).\nLemma A.3. If f is P -robust to misspecification with g on R and Im(f |R) = Im(g|R) then g is P -robust to misspecification with f onR.\nProof. If f is P -robust to misspecification with g onR then this immediately implies that fR = g|R, and that if f (R 1 ) = g(R 2 ) for some R 1 , R 2 \u2208R then R 1 \u2261 P R 2 . Lemma A.2 implies that g is P -admissible onR, and if Im(f |R) = Im(g|R) then Im(f |R) \u2286 Im(g|R). This means that g is P -robust to misspecification with f onR.\nLemma A.4. f is P -admissible onR but not P -robust to any misspecification onR if and only if Am(f ) = P onR.\nProof. First suppose Am(f ) = P onR. This immediately implies that f is P -admissible onR. Next, assume that f is P -robust to misspecification with g onR, let R 1 be any element ofR, and consider g(R 1 ). Since Im(g|R) \u2286 Im(f |R), there is an R 2 \u2208R such that f (R 2 ) = g(R 1 ). Since f is P -robust to misspecification with g onR, this implies that R 2 \u2261 P R 1 . Moreover, if Am(f ) = P then R 2 \u2261 P R 1 if and only if f (R 2 ) = f (R 1 ), so it must be the case that f (R 2 ) = f (R 1 ). Now, since f (R 2 ) = f (R 1 ) and f (R 2 ) = g(R 1 ), we have that g(R 1 ) = f (R 1 ). Since R 1 was chosen arbitrarily, this implies that f |R = g|R, which is a contradiction. Hence, if Am(f ) = P onR then f is P -admissible onR but not P -robust to any misspecification onR.\nFor the other direction, suppose that f is P -admissible on R and that Am(f ) = P onR. If Am(f ) = P onR then there are R 1 , R 2 \u2208R such that R 1 \u2261 P R 2 but f (R 1 ) = f (R 2 ). We can then construct a g as follows; let g(R 1 ) = f (R 2 ), g(R 2 ) = f (R 1 ), and g(R) = f (R) for all R = R 1 , R 2 . Now f is P -robust to misspecification with g onR. Hence, if f is P -admissible onR but not P -robust to any misspecification onR then Am(f ) = P onR.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Main Lemmas", "text": "We here prove the lemmas from 2.2.\nLemma A.5. If f is not P -robust to misspecification with g onR, and Im(g|R) \u2286 Im(f |R), then for any h, h \u2022 f is not P -robust to misspecification with h \u2022 g onR.\nProof. If f is not P -robust to misspecification with g onR, and Im(g|R) \u2286 Im(f |R), then either f is not P -admissible onR, or f |R = g|R, or f (R 1 ) = g(R 2 ) but R 1 \u2261 P R 2 for some R 1 , R 2 \u2208R.\nIn the first case, if f is not P -admissible onR then h \u2022 f is not P -admissible onR, as per Lemma A.1. This implies that h \u2022 f is not P -robust to any misspecification (including with h \u2022 g) onR.\nIn the second case, if f |R = g|R then h \u2022 f |R = h \u2022 g|R. This implies that h \u2022 f is not P -robust to misspecification with h \u2022 g onR.\nIn the last case, suppose f (R 1 ) = g(R 2 ) but R 1 \u2261 P R 2 for some R 1 , R 2 \u2208R. If f (R 1 ) = g(R 2 ) then h \u2022 f (R 1 ) = h \u2022 g(R 2 ), so there are R 1 , R 2 \u2208R such that h \u2022 f (R 1 ) = h \u2022 g(R 2 ), but R 1 \u2261 P R 2 . This implies that h \u2022 f is not P -robust to misspecification with h \u2022 g onR.\nLemma A.6. Let f be P -admissible onR, and let T be the set of all reward transformations that preserve P onR. Then f is P -robust to misspecification with g onR if and only if g = f \u2022 t for some t \u2208 T such that f \u2022 t|R = f |R.\nProof. First suppose that f is P -robust to misspecification with g onR -we will construct a t that fits our description. For each y \u2208 Im(g|R), let R y \u2208R be some reward function such that f (R y ) = y; since Im(g|R) \u2286 Im(f |R), such an R y \u2208R always exists. Now let t be the function that maps each R \u2208R to R g(R) . Since by construction g(R) = f (R g(R) ), and since f is P -robust to misspecification with g onR, we have that R \u2261 P R g(R) . This in turn means that t \u2208 T , since t preserves P onR. Finally, note that g = f \u2022 t, which means that we are done.\nFor the other direction, suppose g = f \u2022 t for some t \u2208 T where f \u2022 t|R = f |R. By assumption we have that f is Padmissible onR, and that g|R = f |R. Moreover, we clearly have that Im(g|R) \u2286 Im(f |R). Finally, if g(R 1 ) = f (R 2 ) then f \u2022 t(R 1 ) = f (R 2 ), which means that R 1 \u2261 P R 3 for some R 3 \u2208R such that f (R 3 ) = f (R 2 ). Since f is Padmissible onR it follows that R 3 \u2261 P R 2 , which then implies that R 1 \u2261 P R 2 . Thus f is P -robust to misspecification with g onR, so we are done.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Reward Function Equivalence Classes", "text": "In this section we will prove Theorem 2.6, which turns out to be quite involved. We start by proving several lemmas, which we will need for the main proof.\nLemmas Concerning State-Action Visit Counts Here we provide some lemmas about the topological structure of MDPs. Recall that we assume that all states in S are reachable under \u03c4 and \u00b5 0 .\nLet \u03a0 be the set of all policies. Moreover, given \u03c4 and \u00b5 0 , let m \u03c4,\u00b50 : \u03a0 \u2192 R |S||A| be a map that sends each policy \u03c0 to a vector d \u03c0 , such that\nd \u03c0 [s, a] = \u221e t=0 \u03b3 t P \u03be\u223c\u03c0 (S t , A t = s, a) .\nIn other words, let m \u03c4,\u00b50 (\u03c0) be a vector that records the expected discounted \"density\" of \u03c0's trajectories in each state-action pair under \u03c4 and \u00b5 0 . In some sources, m \u03c4,\u00b50 (\u03c0) is referred to as the occupancy measure of \u03c0. Moreover, given a reward function R and a transition function \u03c4 , let R \u03c4 \u2208 R |S||A| be the vector where\nR \u03c4 [s, a] = E S \u2032 \u223c\u03c4 (s,a) [R(s, a, S \u2032 )].\nWe will refer to these vectors as reward vectors, and will for the sake of clarity distinguish them from reward functions. Note that R \u03c4 1 = R \u03c4 1 if and only if R 1 and R 2 differ by S \u2032 R \u03c4 . Also note that J (\u03c0) = m \u03c4,\u00b50 (\u03c0) \u2022 R \u03c4 . This means that we can use m \u03c4,\u00b50 to decompose J into two separate steps.\nLet\u03a0 \u2282 \u03a0 be the set of all policies that visit each state with positive probability. This means that if w \u03c0 (s) = 0 for all s, which is the case for all \u03c0 \u2208\u03a0, then we can express \u03c0 as\n\u03c0(a | s) = m \u03c4,\u00b50 (\u03c0)[s, a] w \u03c0 (s) .\nThis means that if m \u03c4,\u00b50 (\u03c0) = m \u03c4,\u00b50 (\u03c0 \u2032 ) for some \u03c0, \u03c0 \u2032 \u2208 \u03a0 then \u03c0 = \u03c0 \u2032 .\nNote that m \u03c4,\u00b50 is not injective on \u03a0; if there is some state s that \u03c0 reaches with probability 0, then we can alter the behaviour of \u03c0 at s without changing m \u03c4,\u00b50 (\u03c0). Lemma A.8. Im(m \u03c4,\u00b50 ) is located in an affine space with no more than |S|(|A| \u2212 1) dimensions.\nProof. We wish to establish an upper bound on the number of linearly independent vectors in Im(m \u03c4,\u00b50 ). We can do this by establishing a lower bound on the size of the space of all reward functions that share the same policy evaluation function, J . To see this, consider the fact that J (\u03c0) = m \u03c4,\u00b50 (\u03c0) \u2022 R \u03c4 . We have that R \u03c4 is an |S||A|dimensional vector. Consider a reward vector R \u03c4 1 , and let X be the space of all reward vectors R \u03c4 2 such that R \u03c4\n1 \u2022d = R \u03c4 2 \u2022d for all d \u2208 Im(m \u03c4,\u00b50\n). It is then a straightforward consequence of linear algebra that if Im(m \u03c4,\u00b50 ) contains n linearly independent vectors, then X forms an affine space with |S||A| \u2212 n dimensions. We can thus obtain an upper bound on the number of linearly independent vectors in Im(m \u03c4,\u00b50 ) from a lower bound on the dimensionality of X.\nNext, recall that if R 2 is produced by potential shaping R 1 with \u03a6, and E S0\u223c\u00b50 [\u03a6(S 0 )] = 0, then J 1 (\u03c0) = J 2 (\u03c0) for all \u03c0. This means that for any R \u03c4 1 , we have that X contains all vectors R \u03c4 2 where\nR \u03c4 2 [s, a] = R \u03c4 1 [s, a] + \u03b3E S \u2032 \u223c\u03c4 (s,a) [\u03a6(S \u2032 )] \u2212 \u03a6(s) for some potential function \u03a6 where E S0\u223c\u00b50 [\u03a6(S 0 )] = 0.\nThe space of all such reward vectors is an affine space with |S| \u2212 1 dimensions. This means that Im(m \u03c4,\u00b50 ) contains at most |S|(|A| \u2212 1) + 1 linearly independent vectors.\nNext, note that there is no \u03c0 such that m \u03c4,\u00b50 (\u03c0) is the zero vector. In fact, m \u03c4,\u00b50 (\u03c0) = 1/(1 \u2212 \u03b3) for all \u03c0. This means that the smallest affine space which contains Im(m \u03c4,\u00b50 ) does not contain the origin. Therefore, Im(m \u03c4,\u00b50 ) is located in an affine space with no more than |S|(|A| \u2212 1) dimensions.\nFor the next lemma, let\u03a0 \u2282 \u03a0 be the set of all policies that take all actions with positive probability in each state, and note that\u03a0 \u2282\u03a0 (i.e., a policy that takes every action with positive probability in each state visits every state with positive probability). U is an open subset of R n , and 2. f : U \u2192 R n is an injective continuous map, then f (U ) is open in R n (and f is a homeomorphism between U and f (U )). We will show that m and\u03a0 satisfy the requirements of this theorem.\nWe begin by noting that \u03a0 can be represented as a set of points in R |S|(|A|\u22121) . We do this by considering each policy \u03c0 as a vector \u03c0 of length |S||A|, where \u03c0[s, a] = \u03c0(a | s). Moreover, since a\u2208A \u03c0(a | s) = 1 for all s, we can remove |A| dimensions, and embed \u03a0 in R |S|(|A|\u22121) .\n\u03a0 is an open set in R |S|(|A|\u22121) . By Lemma A.8, we have that m \u03c4,\u00b50 is a mapping from\u03a0 to an affine space with no more than |S|(|A|\u22121) dimensions. By Lemma A.7, we have that m \u03c4,\u00b50 is injective on\u03a0. Finally, m \u03c4,\u00b50 is continuous (it can be expressed as a uniformly convergent series of continuous functions). We can therefore apply the Invariance of Domain theorem, and conclude that Im(m \u03c4,\u00b50 ) is located in an affine space with |S|(|A| \u2212 1) dimensions, in which m \u03c4,\u00b50 (\u03a0) is an open set.\nNote that lemma A.9 holds for all \u03c4 and \u00b5 0 (for which all states are reachable).\nResults Concerning the Policy Order In this section, we prove our results concerning the policy orderings. First, we need to define a new set of transformations. Let PS k \u03b3,\u00b50 be the set of all potential shaping transformations t that, for each R, apply a potential function \u03a6 such that E S0\u223c\u00b50 [\u03a6(S 0 )] = k.\nLemma A.10. J 1 = J 2 if and only if R 1 = t(R 2 ) for some t \u2208 PS 0 \u03b3,\u00b50 \u2022 S \u2032 R \u03c4 .\nProof. For the first direction, suppose R 1 = t(R 2 ) for some t \u2208 PS 0 \u03b3,\u00b50 \u2022 S \u2032 R \u03c4 . Then V \u03c0 1 (s) = V \u03c0 2 (s) \u2212 \u03a6(s), where \u03a6 is the potential shaping function applied by t (see e.g. Lemma B1 in Skalse et al. 2022a). Hence J 1 (\u03c0) = J 2 (\u03c0) \u2212 E s0\u223c\u00b50 [\u03a6(s 0 )] = J 2 (\u03c0), and so we have proven the first direction.\nFor the other direction, first recall that J (\u03c0) = m \u03c4,\u00b50 (\u03c0)\u2022 R \u03c4 . Next, Lemma A.9 implies that Im(m \u03c4,\u00b50 ) contains |S|(|A| \u2212 1) + 1 linearly independent vectors. It is then a straightforward fact of linear algebra that, for any reward vector R \u03c4 1 , the space X of all reward vectors R \u03c4 2 such that R \u03c4 1 \u2022 d = R \u03c4 2 \u2022 d for all d \u2208 Im(m \u03c4,\u00b50 ), forms an affine space with |S| \u2212 1 dimensions.\nWe know that J is preserved by transformations in PS 0 \u03b3,\u00b50 \u2022 S \u2032 R \u03c4 . Next, given R 1 , the space of all reward vectors R \u03c4 2 given by some R 2 such that R 2 = t(R 1 ) for some t \u2208 PS 0 \u03b3,\u00b50 \u2022 S \u2032 R \u03c4 , forms an affine space with |S| \u2212 1 dimensions. Since this space is contained in X, and since they have the same number of dimensions, they must be one and the same.\nTherefore, given R 1 , if J 1 = J 2 , then there exists an R 3 such that R 3 = t(R 1 ) for some t \u2208 PS 0 \u03b3,\u00b50 \u2022 S \u2032 R \u03c4 , and such that R \u03c4 3 = R \u03c4 2 . But this means that R 2 = t(R 1 ) for some t \u2208 PS 0 \u03b3,\u00b50 \u2022 S \u2032 R \u03c4 . We have thus proven the other direction, which completes the proof.\nWe can now finally prove Theorem 2.6.\nTheorem A.11. R 1 \u2261 ORD M R 2 if and only if R 2 = t(R 1 ) for some t \u2208 S \u2032 R \u03c4 \u2022 PS \u03b3 \u2022 LS.\nProof. First, R 1 \u2261 ORD M R 2 if and only if J 1 is a monotonic transformation of J 2 . Next, since J (\u03c0) = m \u03c4,\u00b50 (\u03c0) \u2022 R \u03c4 , we have that all possible monotonic transformations of J are affine. Hence R 1 \u2261 ORD M R 2 if and only if J 1 = a \u2022 J 2 + b for some a \u2208 R + , b \u2208 R.\nThe first direction is straightforward. First, if R 1 = t(R 2 ) for some t \u2208 S \u2032 R \u03c4 then J 1 = J 2 . Next, if R 1 = t(R 2 ) for some t \u2208 PS \u03b3 then J 1 = J 2 \u2212 E S0\u223c\u00b50 [\u03a6 t (S 0 )] (see e.g. Lemma B1 in Skalse et al. 2022a). Finally, if R 1 = t(R 2 ) for some t \u2208 LS then J 1 = c \u2022 J 2 for some c \u2208 R + . Hence if R 1 = t(R 2 ) for some t \u2208 S \u2032 R \u03c4 \u2022 PS \u03b3 \u2022 LS then J 1 = a \u2022 J 2 + b for some a \u2208 R + , b \u2208 R.\nFor the other direction, suppose J 1 = a \u2022 J 2 + b for some a \u2208 R + , b \u2208 R. Consider the reward function R 3 given by first scaling R 2 by a, and then shape the resulting reward with the potential function \u03a6 that is equal to \u2212b for all initial states, and equal to 0 elsewhere. Now J 3 = J 1 , so (by Lemma A.10) there is a t \u2032 \u2208 PS 0 \u03b3,\u00b50 \u2022 S \u2032 R \u03c4 such that R 1 = t \u2032 (R 3 ). By composing t \u2032 with the transformation that produced R 3 from R 2 , we obtain a t \u2208 S \u2032 R \u03c4 \u2022 PS \u03b3 \u2022 LS such that R 1 = t(R 2 ). Hence if R 1 \u2261 ORD M R 2 then R 1 = t(R 2 ) for some t \u2208 S \u2032 R \u03c4 \u2022 PS \u03b3 \u2022 LS. We have thus proven both directions.", "publication_ref": ["b20", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "A.3 Misspecified Behavioural Models", "text": "In this section, we prove our results from Section 3.1. Theorem A.12. Let f M \u2208 F M be surjective onto \u03a0 + . Then f M is OPT M -robust to misspecification with g if and only if g \u2208 F M and g = f M .\nProof. f M is OPT M -robust to misspecification with g in M if and only if f M is OPT M -admissible, g = f M , Im(g) \u2286 Im(f ), and if f M (R 1 ) = g(R 2 ) then R 1 and R 2 have the same optimal policies in M.\nFor all f \u2208 F M and all R, argmax a\u2208A f (R)(a | s) = argmax a\u2208A Q \u22c6 (s, a).\nSince f M \u2208 F M , this means that if f M (R 1 ) = f M (R 2 ) then argmax a\u2208A Q \u22c6 1 (s, a) = argmax a\u2208A Q \u22c6 2 (s, a) in M. Moreover, R 1 and R 2 have the same optimal policies in M if and only if argmax a\u2208A Q \u22c6 1 (s, a) = argmax a\u2208A Q \u22c6 2 (s, a) in M. Thus, if f M (R 1 ) = f M (R 2 ) then R 1 \u2261 OPT M R 2 , and so f M is OPT M -admissible.\nLet g \u2208 F M and g = f M . Since g is a function R \u2192 \u03a0 + , and since f M is surjective onto \u03a0 + , we have that Im(g) \u2286 Im(f ). Next, by the same argument as above, if f M (R 1 ) = g(R 2 ) then argmax a\u2208A Q \u22c6 1 (s, a) = argmax a\u2208A Q \u22c6 2 (s, a), which implies that R 1 \u2261 OPT M R 2 . This means that f M is OPT M -robust to misspecification with g.\nNext, suppose f M is OPT M -robust to misspecification with g. This means that Im(g) \u2286 Im(f ) and that if \u03c8 , and there exists a t \u2208\na\u2208A exp \u03c8(R)A R (s, a)\n.\nwhere A R is the optimal advantage function of R in M. If \nm , then there is a pigeonhole argument to show that there must be at least two\nThe pigeonhole argument goes like this: the codomain of each o M \u2208 O M has (2 |A| \u2212 1) |S| elements, and there are (2 |A| \u22121) |S| OPT M -equivalence classes. This means that if o M is OPT M -admissible, then there must be a one-to-one correspondence between OPT M -equivalence classes and elements of o M 's codomain, so that there for each equiv-\nR) for all R -a policy is optimal if and only if it takes only optimal actions, but it need not take all optimal actions. Moreover, if\n-for any function S \u2192 P(A) \u2212 \u2205, there is a reward function for which those are the optimal actions, so there is always some R 2 such that\n, since all actions that are optimal under R 2 are optimal under R 1 . In the first case, since\nIn the second case, let R 3 be a reward function so that o M m (R 3 ) = o M (R 2 ), and repeat the same argument. Since there can only be a finite \nis the unique policy that maximises the maximal causal entropy objective;\nwhere c R is the linear scaling factor that t applies to R. Note that J R is preserved by S \u2032 -redistribution, and potential shaping can only change J R by inducing a uniform constant shift of J R for all policies. This means that linear scaling is the only transformation in PS \u03b3 \u2022 LS \u2022 S \u2032 R \u03c4 that could affect the maximal causal entropy objective. Finally, let \u03c8 \u2032 be the function \u03c8 \u2032 (R) = \u03c8(t(R)) \u2022 c R , and we can see that", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.4 Misspecified MDPs", "text": "We here prove our results from Section 3.2. The first of these proofs is straightforward.\nProof. This follows directly from Theorem 4.2 in Skalse et al. (2022a).\nTo prove the next result, we first need a supporting lemma. We say that a state s is controllable relative to a transition function \u03c4 , initial state distribution \u00b5 0 , and discount \u03b3, if there exist two policies \u03c0, \u03c0 \u2032 such that\nNote that the sum starts from t = 1. It can therefore be viewed as summing the discounted probability that \u03c0 and \u03c0 \u2032 enter s at each time step. Recall also that \u03c4 is trivial if for all s \u2208 S and a, a \u2032 \u2208 A, we have \u03c4 (s, a) = \u03c4 (s, a \u2032 ). Lemma A.17. For any \u00b5 0 , \u03b3, and \u03c4 , there exists a controllable state if and only if \u03c4 is non-trivial.\nProof. It is straightforward to see that if \u03c4 is trivial then there are no controllable states.\nFor the other direction, suppose there are no controllable states. This in turn implies that every policy is optimal under any reward function defined over the domain S. Formally, if R is a reward function such that for each s \u2208 S, we have that R(s, a 1 , s 1 ) = R(s, a 2 , s 2 ) for all s 1 , s 2 \u2208 S, a 1 , a 2 \u2208 A, and if there are no controllable states, then every policy is optimal under R. In particular, every deterministic policy is optimal under all such reward functions.\nGiven a reward function defined over the domain S, let R \u2208 R |S| be the vector such that R[s] is the reward that R assigns to transitions leaving s. Moreover, given a deterministic policy \u03c0, let T \u03c0 be the |S| \u00d7 |S|-dimensional transition matrix that describes the transitions of \u03c0 under \u03c4 . Then if all deterministic policies are optimal under R, we can apply Theorem 3 from Ng and Russell (2000) and conclude that\nfor all deterministic policies \u03c0, \u03c0 \u2032 . If this holds for all R, we then have that (T \u03c0 \u2212 T \u03c0 \u2032 )(I \u2212 \u03b3T \u03c0 ) \u22121 is the zero matrix for all deterministic policies \u03c0, \u03c0 \u2032 . Moreover, since (I \u2212\u03b3T \u03c0 ) \u22121 has no zero eigenvalues, this then means that (T \u03c0 \u2212 T \u03c0 \u2032 ) must be the zero matrix for all pairs of deterministic policies \u03c0, \u03c0 \u2032 . This, in turn, implies that \u03c4 must be trivial.\nWe can now prove the second lemma from Section 3.2.\nProof. As per Lemma A.17, if \u03c4 is non-trivial then there is a state s that is controllable relative to \u03c4 , \u00b5 0 , and \u03b3 2 . Let R 1 be any reward function, and let R 2 be the reward that is obtained by potential shaping R 1 with the discount \u03b3 1 and the potential function that is equal to X on s (where X = 0), and 0 on all other states. Note that there is a t \u2208 PS \u03b31 such that R 2 = t(R 1 ), which means that\n\u03b3 t 2 P(\u03c0 enters s at time t),\nx \u03c0 2 = \u221e t=0 \u03b3 t 2 P(\u03c0 exits s at time t).\nWe then have that \u2206 \u03c0 = X \u2022 (\u03b3 1 n \u03c0 2 \u2212 x \u03c0 2 ). We will use p to denote \u00b5 0 (s). If \u03b3 1 = \u03b3 2 then we know that \u2206 \u03c0 = \u2212X \u2022 p , which gives that\np By plugging this into the above, and rearranging, we obtain \u2206 \u03c0 = Xn \u03c0 2 (\u03b3 1 \u2212 \u03b3 2 ) + pX. Moreover, if s is controllable then there are \u03c0, \u03c0 \u2032 such that n \u03c0 2 = n \u03c0 \u2032 2 , which means that \u2206 \u03c0 = \u2206 \u03c0 \u2032 . In particular, there are \u03c0, \u03c0 \u2032 such that \u2206 \u03c0 = \u2206 \u03c0 \u2032 , and \u03c0 is optimal under R 1 , but \u03c0 \u2032 is not. Now, if \u03b3 1 = \u03b3 2 then by making X sufficiently large or sufficiently small, we can make it so that \u03c0 \u2032 is optimal under R 2 , but \u03c0 is not. Hence S, A, \u03c4, \u00b5 0 , R 1 , \u03b3 2 and S, A, \u03c4, \u00b5 0 , R 2 , \u03b3 2 have different optimal policies. This means that there are two reward functions R 1 , R 2 , such that f \u03b31 (R 1 ) = f \u03b31 (R 2 ), but R 1 \u2261 OPT M R 2 . Therefore, if \u03b3 1 = \u03b3 2 and \u03c4 is non-trivial then f \u03b31 is not OPT Madmissible.\nIt is worth noting that the above lemma works even if f \u03b31 is only invariant to \u03b3 1 -based potential shaping whose potential is 0 for all initial states, provided that \u03c4 gives control over some non-initial state. This can be used to generalise the lemma somewhat, since there are some reward objects which are invariant only to such potential shaping; see Skalse et al. (2022a).\nUsing these two lemmas, we can now prove the theorem:\nTheorem A.19. If f \u03c41 = f \u03c41 \u2022 t for all t \u2208 S \u2032 R \u03c41 and f \u03c42 = f \u03c42 \u2022 t for all t \u2208 S \u2032 R \u03c42 , then f \u03c41 is not OPT Mrobust to misspecification with f \u03c42 for any M. Moreover, if f \u03b31 = f \u03b31 \u2022 t for all t \u2208 PS \u03b31 and f \u03b32 = f \u03b32 \u2022 t for all t \u2208 PS \u03b32 , then f \u03b31 is not OPT M -robust to misspecification with f \u03b32 for any M whose transition function \u03c4 is non-trivial.\nProof. Let M = S, A, \u03c4, \u00b5 0 , , \u03b3 . If f is OPT M -robust to misspecification with g then f must by definition be OPT M -admissible. Moreover, Lemma A.2 says that g must be OPT M -admissible as well. This proof will proceed by showing that in each case, at least one of the relevant reward objects fails to be OPT M -admissible.\nIf f \u03c41 = f \u03c41 \u2022t for all t \u2208 S \u2032 R \u03c41 then Lemma 3.5 says that f \u03c41 is not OPT M -admissible unless \u03c4 1 = \u03c4 , and similarly for f \u03c42 . If \u03c4 1 = \u03c4 2 then either \u03c4 1 = \u03c4 or \u03c4 2 = \u03c4 . Hence either f \u03c41 or f \u03c42 is not OPT M -admissible, which means that f \u03c41 is not OPT M -robust to misspecification with f \u03c42 .\nSimilarly, if f \u03b31 = f \u03b31 \u2022 t for all t \u2208 PS \u03b31 then Lemma 3.6 says that f \u03b31 is not OPT M -admissible unless \u03b3 1 = \u03b3 or \u03c4 is trivial, and similarly for f \u03b32 . If \u03b3 1 = \u03b3 2 then either \u03b3 1 = \u03b3 or \u03b3 2 = \u03b3. Hence either f \u03b31 or f \u03b32 is not OPT M -admissible, unless \u03c4 is trivial, which means that f \u03b31 is not OPT M -robust to misspecification with f \u03b32 , unless \u03c4 is trivial.", "publication_ref": ["b20", "b14", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "A.5 Restrictions on the Reward Function", "text": "Here, we prove our results from Section 4.1.\nTheorem A.20. If f is P -robust to misspecification with g onR then f is P -robust to misspecification with g \u2032 on R for some g \u2032 where g \u2032 |R = g|R, unless f is not P -admissible on R, and if f is P -robust to misspecification with g on R then f is P -robust to misspecification with g onR, unless f |R = g|R.\nProof. Suppose f is P -robust to misspecification with g on R, and that f is P -admissible on R. We construct a g \u2032 as follows; let g \u2032 (R) = g(R) for all R \u2208R, and let g \u2032 (R) = f (R) for all R \u2208R. Now f is P -robust to misspecification with g \u2032 on R, and g(R) = g \u2032 (R) for all R \u2208R. The other direction is straightforward.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Autonomous Helicopter Aerobatics Through Apprenticeship Learning", "journal": "The International Journal of Robotics Research", "year": "2010", "authors": "P Abbeel; A Coates; A Y Ng"}, {"ref_id": "b1", "title": "Occam's razor is insufficient to infer the preferences of irrational agents", "journal": "", "year": "2019", "authors": "S Armstrong; S Mindermann"}, {"ref_id": "b2", "title": "Identifiability in Inverse Reinforcement Learning", "journal": "", "year": "2021", "authors": "H Cao; S N Cohen; L Szpruch"}, {"ref_id": "b3", "title": "", "journal": "", "year": "2019", "authors": "L Chan; A Critch; A Dragan"}, {"ref_id": "b4", "title": "Inverse Optimal Control with Linearly-Solvable MDPs", "journal": "", "year": "2010", "authors": "K Dvijotham; E Todorov"}, {"ref_id": "b5", "title": "Learning the Preferences of Ignorant, Inconsistent Agents", "journal": "", "year": "2015", "authors": "Israel : Haifa;  Omnipress;  Madison; Usa Wisconsin; O Evans; A Stuhlmueller; N D Goodman"}, {"ref_id": "b6", "title": "Choice Set Misspecification in Reward Inference", "journal": "", "year": "2021", "authors": "R Freedman; R Shah; A Dragan"}, {"ref_id": "b7", "title": "Quantifying Differences in Reward Functions", "journal": "", "year": "2020", "authors": "A Gleave; M Dennis; S Legg; S Russell; J Leike"}, {"ref_id": "b8", "title": "Cooperative Inverse Reinforcement Learning", "journal": "Curran Associates, Inc", "year": "2016", "authors": "D Hadfield-Menell; S J Russell; P Abbeel; A Dragan"}, {"ref_id": "b9", "title": "", "journal": "Imitation Learning: A Survey of Learning Methods. ACM Comput. Surv", "year": "2017", "authors": "A Hussein; M M Gaber; E Elyan; Jayne ; C "}, {"ref_id": "b10", "title": "Calculus on MDPs: Potential Shaping as a Gradient", "journal": "", "year": "2022", "authors": "E Jenner; H Van Hoof; A Gleave"}, {"ref_id": "b11", "title": "Reward-rational (implicit) choice: A unifying formalism for reward learning", "journal": "Curran Associates, Inc. Kahneman", "year": "1979", "authors": "H J Jeon; S Milli; A Dragan; H Larochelle; M Ranzato; R Hadsell; M Balcan; H ; D Lin; A Tversky"}, {"ref_id": "b12", "title": "Reward Identification in Inverse Reinforcement Learning", "journal": "PMLR", "year": "2021", "authors": "K Kim; S Garg; K Shiragur; S Ermon"}, {"ref_id": "b13", "title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping", "journal": "Morgan Kaufmann Publishers Inc", "year": "1999", "authors": "A Y Ng; D Harada; S Russell"}, {"ref_id": "b14", "title": "Algorithms for Inverse Reinforcement Learning", "journal": "Morgan Kaufmann Publishers Inc", "year": "2000", "authors": "A Y Ng; S Russell"}, {"ref_id": "b15", "title": "What Matters for Adversarial Imitation Learning? arXiv preprint", "journal": "", "year": "2021", "authors": "M Orsini; A Raichuk; L Hussenot; D Vincent; R Dadashi; S Girgin; M Geist; O Bachem; O Pietquin; M Andrychowicz"}, {"ref_id": "b16", "title": "Proceedings of the 35th International Conference on Neural Information Processing Systems", "journal": "", "year": "2021", "authors": "A Pan; K Bhatia; J Steinhardt"}, {"ref_id": "b17", "title": "Bayesian Inverse Reinforcement Learning", "journal": "Springer", "year": "2007", "authors": "D Ramachandran; E Amir"}, {"ref_id": "b18", "title": "On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference", "journal": "", "year": "2019", "authors": "R Shah; N Gundotra; P Abbeel; A D Dragan"}, {"ref_id": "b19", "title": "End-to-End Robotic Reinforcement Learning Without Reward Engineering", "journal": "", "year": "2019", "authors": "A Singh; L Yang; K Hartikainen; C Finn; S Levine"}, {"ref_id": "b20", "title": "Invariance in Policy Optimisation and Partial Identifiability in Reward Learning", "journal": "", "year": "2022", "authors": "J Skalse; M Farrugia-Roberts; S Russell; A Abate; A Gleave"}, {"ref_id": "b21", "title": "Defining and Characterizing Reward Hacking", "journal": "", "year": "2022", "authors": "J Skalse; N Howe; K Dima; D Krueger"}, {"ref_id": "b22", "title": "Reinforcement Learning: An Introduction", "journal": "MIT Press", "year": "2018", "authors": "R S Sutton; A G Barto"}, {"ref_id": "b23", "title": "Estimation, Inference and Specification Analysis", "journal": "Cambridge University Press", "year": "1994", "authors": "H White"}, {"ref_id": "b24", "title": "Identification of animal behavioral strategies by inverse reinforcement learning", "journal": "PLOS Computational Biology", "year": "2018", "authors": "S Yamaguchi; H Naoki; M Ikeda; Y Tsukada; S Nakano; I Mori; S Ishii"}, {"ref_id": "b25", "title": "Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy", "journal": "", "year": "2010", "authors": "B D Ziebart"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Lemma A.7. m \u03c4,\u00b50 is injective on\u03a0.Proof. Suppose m \u03c4,\u00b50 (\u03c0) = m \u03c4,\u00b50 (\u03c0 \u2032 ) for some \u03c0, \u03c0 \u2032 \u2208\u03a0. Next, given \u03c4, \u00b5 0 , define w \u03c0 asw \u03c0 (s) = \u221e t=0 \u03b3 t P \u03be\u223c\u03c0 (S t = s).Note that if m \u03c4,\u00b50 (\u03c0) = m \u03c4,\u00b50 (\u03c0 \u2032 ) then w \u03c0 = w \u03c0 \u2032 , and moreover that m \u03c4,\u00b50 (\u03c0)[s, a] = w \u03c0 (s)\u03c0(a | s).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Lemma A.9. Im(m \u03c4,\u00b50 ) is located in an affine space with |S|(|A| \u2212 1) dimensions, in which m \u03c4,\u00b50 (\u03a0) is an open set. Proof. By the Invariance of Domain theorem, if 1.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "E[ \u221e t=0 \u03b3 t (R(s t , a t , s t+1 ) + \u03b1H(\u03c0(s t+1 )))]", "formula_coordinates": [2.0, 332.4, 229.61, 183.22, 14.76]}, {"formula_id": "formula_1", "formula_text": "R 1 \u2261 f R 2 \u21d0\u21d2 f (R 1 ) = f (R 2 ). 3. Given a partition P of R, we say that f is P -admissible if Am(f ) P , i.e. f (R 1 ) = f (R 2 ) \u21d2 R 1 \u2261 P R 2 . 4", "formula_coordinates": [2.0, 320.04, 514.65, 238.06, 48.16]}, {"formula_id": "formula_2", "formula_text": "Definition 2.3 (Potential Shaping). A potential function is a function \u03a6 : S \u2192 R. Given a discount \u03b3, we say that R 2 \u2208 R is produced by potential shaping of R 1 \u2208 R if for some potential \u03a6, R 2 (s, a, s \u2032 ) = R 1 (s, a, s \u2032 ) + \u03b3 \u2022 \u03a6(s \u2032 ) \u2212 \u03a6(s).", "formula_coordinates": [3.0, 319.56, 547.26, 238.52, 61.43]}, {"formula_id": "formula_3", "formula_text": "of R 1 \u2208 R if E S \u2032 \u223c\u03c4 (s,a) [R 1 (s, a, S \u2032 )] = E S \u2032 \u223c\u03c4 (s,a) [R 2 (s, a, S \u2032 )] .", "formula_coordinates": [3.0, 319.56, 676.05, 225.72, 29.5]}, {"formula_id": "formula_4", "formula_text": "E S \u2032 \u223c\u03c4 (s,a) [R 2 (s, a, S \u2032 ) + \u03b3 \u2022 \u03c8(S \u2032 )] \u2264 \u03c8(s),", "formula_coordinates": [4.0, 82.92, 166.13, 180.48, 12.85]}, {"formula_id": "formula_5", "formula_text": "1 Note that if R 1 \u2261 ORD M R 2 then R 1 \u2261 OPT M R 2 .", "formula_coordinates": [4.0, 319.56, 55.56, 237.97, 23.09]}, {"formula_id": "formula_6", "formula_text": "argmax a\u2208A \u03c0(a | s) = argmax a\u2208A Q \u22c6 (s, a),", "formula_coordinates": [4.0, 348.48, 478.27, 180.6, 13.06]}, {"formula_id": "formula_7", "formula_text": "Theorem 3.1. Let f M \u2208 F M be surjective onto \u03a0 + . Then f M is OPT M -robust to misspecification with g if and only if g \u2208 F M and g = f M .", "formula_coordinates": [4.0, 319.56, 597.41, 238.54, 33.28]}, {"formula_id": "formula_8", "formula_text": "B M = {b M \u03c8 : \u03c8 \u2208 R \u2192 R + } be the set of all such func- tions b M \u03c8 .", "formula_coordinates": [5.0, 54.0, 276.89, 238.64, 26.64]}, {"formula_id": "formula_9", "formula_text": "Theorem 3.2. If b M \u03c8 \u2208 B M then b M \u03c8 is ORD M -robust to misspecification with g if and only if g \u2208 B M and g = b M", "formula_coordinates": [5.0, 54.0, 319.01, 238.5, 24.88]}, {"formula_id": "formula_10", "formula_text": "Theorem 3.3. No function in O M is ORD M -admissible. The only function in O M that is OPT M -admissible is o M m , but o M", "formula_coordinates": [5.0, 54.0, 577.01, 238.65, 33.28]}, {"formula_id": "formula_11", "formula_text": "if for each s \u2208 S, \u03c4 (s, a) = \u03c4 (s, a \u2032 ) for all a, a \u2032 \u2208 A. Lemma 3.5. If f \u03c41 = f \u03c41 \u2022 t for all t \u2208 S \u2032 R \u03c41 then f \u03c41 is not OPT M -admissible for M = S, A, \u03c4 2 , \u00b5 0 , , \u03b3 unless \u03c4 1 = \u03c4 2 . Lemma 3.6. If f \u03b31 = f \u03b31 \u2022 t for all t \u2208 PS \u03b31 then f \u03b31 is not OPT M -admissible for M = S, A, \u03c4, \u00b5 0 , , \u03b3 2 unless \u03b3 1 = \u03b3 2 or \u03c4 is trivial.", "formula_coordinates": [5.0, 319.56, 391.49, 238.48, 89.05]}, {"formula_id": "formula_12", "formula_text": "Theorem 3.7. If f \u03c41 = f \u03c41 \u2022 t for all t \u2208 S \u2032 R \u03c41 and f \u03c42 = f \u03c42 \u2022 t for all t \u2208 S \u2032 R \u03c42 , then f \u03c41 is not OPT M -robust to misspecification with f \u03c42 for any M. Moreover, if f \u03b31 = f \u03b31 \u2022 t for all t \u2208 PS \u03b31 and f \u03b32 = f \u03b32 \u2022 t for all t \u2208 PS \u03b32 , then f \u03b31 is not OPT M -robust to misspecification with f \u03b32 for any M whose transition function \u03c4 is non-trivial.", "formula_coordinates": [5.0, 319.56, 520.01, 238.57, 69.76]}, {"formula_id": "formula_13", "formula_text": "R 1 , R 2 \u2208R such that f (R 1 ) = f (R 2 ), but R 1 \u2261 P R 2 . But if f (R 1 ) = f (R 2 ) then h \u2022 f (R 1 ) = h \u2022 f (R 2 ), so there are R 1 , R 2 \u2208R such that h \u2022 f (R 1 ) = h \u2022 f (R 2 ), but R 1 \u2261 P R 2 .", "formula_coordinates": [9.0, 54.0, 248.61, 238.57, 47.49]}, {"formula_id": "formula_14", "formula_text": "d \u03c0 [s, a] = \u221e t=0 \u03b3 t P \u03be\u223c\u03c0 (S t , A t = s, a) .", "formula_coordinates": [10.0, 94.68, 273.53, 157.08, 30.72]}, {"formula_id": "formula_15", "formula_text": "R \u03c4 [s, a] = E S \u2032 \u223c\u03c4 (s,a) [R(s, a, S \u2032 )].", "formula_coordinates": [10.0, 103.08, 384.53, 140.28, 12.97]}, {"formula_id": "formula_16", "formula_text": "\u03c0(a | s) = m \u03c4,\u00b50 (\u03c0)[s, a] w \u03c0 (s) .", "formula_coordinates": [10.0, 118.92, 650.97, 108.6, 23.96]}, {"formula_id": "formula_17", "formula_text": "1 \u2022d = R \u03c4 2 \u2022d for all d \u2208 Im(m \u03c4,\u00b50", "formula_coordinates": [10.0, 319.56, 203.59, 238.46, 22.42]}, {"formula_id": "formula_18", "formula_text": "R \u03c4 2 [s, a] = R \u03c4 1 [s, a] + \u03b3E S \u2032 \u223c\u03c4 (s,a) [\u03a6(S \u2032 )] \u2212 \u03a6(s) for some potential function \u03a6 where E S0\u223c\u00b50 [\u03a6(S 0 )] = 0.", "formula_coordinates": [10.0, 319.56, 337.13, 238.53, 31.57]}], "doi": ""}