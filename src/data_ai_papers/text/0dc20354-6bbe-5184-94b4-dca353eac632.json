{"title": "From Non-Negative to General Operator Cost Partitioning", "authors": "Florian Pommerening; Malte Helmert; Gabriele R\u00f6ger; Jendrik Seipp", "pub_date": "", "abstract": "Operator cost partitioning is a well-known technique to make admissible heuristics additive by distributing the operator costs among individual heuristics. Planning tasks are usually defined with non-negative operator costs and therefore it appears natural to demand the same for the distributed costs. We argue that this requirement is not necessary and demonstrate the benefit of using general cost partitioning. We show that LP heuristics for operator-counting constraints are cost-partitioned heuristics and that the state equation heuristic computes a cost partitioning over atomic projections. We also introduce a new family of potential heuristics and show their relationship to general cost partitioning.", "sections": [{"heading": "Introduction", "text": "Heuristic search is commonly used to solve classical planning tasks. Optimal planning requires admissible heuristics, which estimate the cost to the goal without overestimation. If several admissible heuristics are used, they must be combined in a way that guarantees admissibility. By evaluating each heuristic on a copy of the task with a suitably reduced cost function, operator cost partitioning (Katz and Domshlak 2010) allows summing heuristic estimates admissibly.\nPrevious work on operator cost partitioning required all cost functions to be non-negative. We show that this restriction is not necessary to guarantee admissibility and that dropping it can lead to more accurate heuristic estimates. Moreover, we demonstrate that when allowing negative operator costs, heuristics based on the recently proposed operator-counting constraints (Pommerening et al. 2014b) can be interpreted as a form of optimal cost partitioning. This includes the state equation heuristic (Bonet and van den Briel 2014), which was previously thought (Bonet 2013) to fall outside the four main categories of heuristics for classical planning: abstractions, landmarks, delete-relaxations and critical paths (Helmert and Domshlak 2009). We show that the state equation heuristic computes a general optimal cost partitioning over atomic projection heuristics. In addition, we introduce potential heuristics, a new family of heuristics with a close relationship to general operator cost partitioning. Finally, we empirically demonstrate the posi-Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. tive influence of allowing negative costs in cost partitioning and the strong performance of potential heuristics.", "publication_ref": ["b11", "b14", "b1", "b2", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "SAS + Planning and Heuristics", "text": "We consider SAS + planning (B\u00e4ckstr\u00f6m and Nebel 1995) with operator costs, where a task is given as a tuple \u03a0 = V, O, s I , s , cost . Each V in the finite set of variables V has a finite domain dom(V ). A state s is a variable assignment over V. A fact is a pair of a variable and one of its values. For a partial variable assignment p we use vars(p) to refer to the set of variables on which p is defined and where convenient we consider p to be a set of facts { V, p[V ] | V \u2208 vars(p)}. Each operator o in the finite set O has a precondition pre(o) and an effect eff (o), which are both partial variable assignments over V. Operator o is applicable in a state s if s and pre(o) are consistent, i. e., they do not assign a variable to different values. Applying o in s results in state s o with\ns o [V ] = eff (o)[V ] if V \u2208 vars(eff (o)) s[V ]\notherwise.\nAn operator sequence \u03c0 = o 1 , . . . , o n is applicable in state s 0 if there are states s 1 , . . . , s n such that for 1 \u2264 i \u2264 n, operator o i is applicable in s i\u22121 and s i = s i\u22121 o i . We refer to the resulting state s n by s 0 \u03c0 . The initial state s I is a complete and the goal description s a partial variable assignment over V. For a state s, an s-plan is an operator sequence \u03c0 that is applicable in s and for which s \u03c0 and s are consistent. An s I -plan is called a solution or plan for the task.\nThe function cost : O \u2192 R + 0 assigns a non-negative cost to each operator. Throughout the paper, we will vary planning tasks by considering different cost functions, so the following definitions refer to arbitrary cost functions cost , which need not be equal to cost.\nThe cost of a plan\n\u03c0 = o 1 , . . . , o n under cost function cost is cost (\u03c0) = n i=1 cost (o i\n). An optimal s-plan under cost function cost is a plan \u03c0 that minimizes cost (\u03c0) among all s-plans. Its cost is denoted with h * (s, cost ). If there is no s-plan then h * (s, cost ) = \u221e. A heuristic h is a function that produces an estimate h(s, cost ) \u2208 R + 0 \u222a {\u221e} of the optimal cost h * (s, cost ). If cost = cost, these notations can be abbreviated to h * (s) and h(s). In cases where we do not need to consider other cost functions, we will define heuristics only for cost.\nWe say that c \u2208 R + 0 \u222a {\u221e} is an admissible heuristic estimate for state s and cost function cost if c \u2264 h * (s, cost ). A heuristic function is admissible if h(s, cost ) is an admissible heuristic estimate for all states s and cost functions cost . A heuristic h is goal-aware if h(s, cost ) = 0 for every state s consistent with s and cost function cost . It is consistent if h(s, cost ) \u2264 cost (o) + h(s o , cost ) for every state s, operator o applicable in s and cost function cost . Heuristics that are goal-aware and consistent are admissible, and admissible heuristics are goal-aware. The A * algorithm (Hart, Nilsson, and Raphael 1968) generates optimal plans when equipped with an admissible heuristic.", "publication_ref": ["b0", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Non-Negative Cost Partitioning", "text": "A set of admissible heuristics is called additive if their sum is admissible. A famous early example of additive heuristics are disjoint pattern database heuristics for the sliding-tile puzzle (Korf and Felner 2002), which were later generalized to classical planning (e. g., Haslum et al. 2007).\nCost partitioning (Katz and Domshlak 2007;Yang et al. 2008;Katz and Domshlak 2010) is a technique for making arbitrary admissible heuristics additive. The idea is that each heuristic may only account for a fraction of the actual operator costs, so that the total cost cannot exceed the optimal plan cost.\nDefinition 1 (Non-negative cost partitioning). Let \u03a0 be a planning task with operators O and cost function cost. A non-negative cost partitioning for \u03a0 is a tuple cost 1 , . . . , cost n where cost (Katz and Domshlak 2010). Let \u03a0 be a planning task, let h 1 , . . . , h n be admissible heuristics for \u03a0, and let P = cost 1 , . . . , cost n be a non-negative cost partitioning for \u03a0. Then h P (h 1 , . . . , h n , s) = n i=1 h i (s, cost i ) is an admissible heuristic estimate for s. Among other contributions, Katz and Domshlak (2010) showed how to compute an optimal (i. e., best possible) cost partitioning for a given state and a wide class of abstraction heuristics in polynomial time. However, for reasons of efficiency, non-optimal cost partitioning such as zero-one cost partitioning (e. g., Edelkamp 2006), uniform cost partitioning (e. g., Karpas and Domshlak 2009) or post-hoc optimization (Pommerening, R\u00f6ger, and Helmert 2013) is also commonly used.\ni : O \u2192 R + 0 for 1 \u2264 i \u2264 n and n i=1 cost i (o) \u2264 cost(o) for all o \u2208 O. Proposition 1", "publication_ref": ["b12", "b5", "b10", "b17", "b11", "b11", "b11", "b3", "b9", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "General Cost Partitioning", "text": "We argue that there is no compelling reason to restrict operator costs to be non-negative when performing cost partitioning, and we will remove this restriction in the following, permitting general (possibly negative) cost functions cost : O \u2192 R. This means that we must generalize some concepts and notations that relate to operator costs.\nShortest paths in weighted digraphs that permit negative weights are well-defined except in the case where there exists a cycle of negative overall cost that is incident to a path from the source state to the goal. So we can retain our definition of optimal plans except for this case, in which plans of arbitrarily low cost exist and we set h * (s, cost ) = \u2212\u221e.\nHeuristic functions may now map to the set R \u222a {\u2212\u221e, \u221e}. The definitions of admissibility and consistency remain unchanged, but the notion of goal-aware heuristics must be amended to require h(s, cost ) \u2264 0 instead of h(s, cost ) = 0 for goal states s to retain the result that a consistent heuristic is goal-aware iff it is admissible. (Even if we are already in a goal state, the cheapest path to a goal state may be a non-empty path with negative cost.)\nWith these preparations, we can show the analogue of Proposition 1 for general cost partitioning: Theorem 1. Let \u03a0 be a planning task with operators O and cost function cost, and let P = cost 1 , . . . , cost n be a general cost partitioning for \u03a0, i. e., cost 1 , . . . , cost n are general cost functions with\nn i=1 cost i (o) \u2264 cost(o) for all o \u2208 O. Let h 1 , . . . , h n be admissible heuristics for \u03a0. Then h P (h 1 , . . . , h n , s) = n i=1 h i (s, cost i\n) is an admissible heuristic estimate for every state s. If any term in the sum is \u221e, the sum is defined as \u221e, even if another term is \u2212\u221e. Proof: Consider an arbitrary state s. If \u03a0 has no s-plan then h * (s) = \u221e and every estimate is admissible.\nWe are left with the case where \u03a0 has an s-plan. Then all h i (s, cost i ) are finite or \u2212\u221e because an admissible heuristic can only produce \u221e for states without plans, no matter what the cost function is. Consider the case where h * (s) > \u2212\u221e. Let \u03c0 = o 1 , . . . , o k be an optimal s-plan for \u03a0. We get:\nh P (h 1 , . . . , h n , s) = n i=1 h i (s, cost i ) \u2264 n i=1 h * (s, cost i ) \u2264 n i=1 k j=1 cost i (o j ) = k j=1 n i=1 cost i (o j ) \u2264 k j=1 cost(o j ) = h * (s).\nIn the case where h * (s) = \u2212\u221e, there exists a state s on a path from s to a goal state with an incident negative-cost cycle \u03c0 , i. e., \u03c0 with s \u03c0 = s and cost(\u03c0 ) < 0. From the cost partitioning property, we get n i=1 cost i (\u03c0 ) \u2264 cost(\u03c0 ) < 0, and hence cost j (\u03c0 ) < 0 for at least one j \u2208 {1, . . . , n}. This implies h * (s, cost j ) = \u2212\u221e and hence h P (h 1 , . . . , h n , s) = \u2212\u221e, concluding the proof.\nGeneral cost partitioning can result in negative heuristic values even if the original task uses only non-negative operator costs, but of course for such tasks it is always admissible to use the heuristic h = max(h P , 0) instead.\nFigure 1 illustrates the utility of general cost partitioning with a small example. The depicted task \u03a0 has two binary variables V 1 and V 2 , both initially 0, and the goal is to set V 1 to 1. Both operators o 1 and o 2 have cost 1. The projections to V 1 and V 2 above and to the left of \u03a0 serve as two example abstractions of \u03a0. Since abstractions preserve transitions their abstract goal distances induce admissible heuristics. If we want to add the heuristics for \u03a0 V1 and \u03a0 V2 admissibly, we have to find a cost partitioning P = cost 1 , cost 2 . If we Figure 1: Example task \u03a0 with two binary variables V 1 and V 2 and goal s [V 1 ] = 1. Above and to the left of the original task we show the projections to V 1 and V 2 . only consider non-negative cost partitionings it makes sense to use the full costs in cost 1 because V 2 is not a goal variable and therefore all abstract goal distances in \u03a0 V2 are 0. This yields\nh P (h V1 , h V2 , s I ) = h V1 (s I , cost 1 ) + h V2 (s I , cost 2 ) = cost(o 1 ) + 0 = 1.\nIf we allow negative cost partitionings, however, we can assign a cost of \u22121 to o 1 in \u03a0 V2 , allowing us to increase its costs to 2 in \u03a0 V1 . The resulting cost partitioning shown in Figure 1 \nyields h P (h V1 , h V2 , s I ) = h V1 (s I , cost 1 ) + h V2 (s I , cost 2 ) = 2 + 0 = 2, a perfect heuristic estimate for the example.\nAn optimal cost partitioning is one that achieves the highest heuristic value for the given heuristics and state: Definition 2. Let \u03a0 = V, O, s I , s , cost be a planning task and let P n be the set of general cost partitionings for \u03a0 with n elements. The set of optimal general cost partitionings for admissible heuristics h 1 , . . . , h n in a state s is OCP(h 1 , . . . , h n , s) = arg max\nP \u2208Pn h P (h 1 , . . . , h n , s)\nThe optimal general cost partitioning heuristic estimate for admissible heuristics h 1 , . . . , h n in a state s is\nh OCP (h 1 , . . . , h n , s) = max P \u2208Pn h P (h 1 , . . . , h n , s).\nIn general, there can be infinitely many optimal cost partitionings. Analogously to OCP and h OCP , we define the non-negative variants OCP+ and h OCP+ based on the set P + n of non-negative cost partitionings for \u03a0 with n elements.\nWe emphasize that knowing the state s is critical for determining an optimal cost partitioning. In general, there exists no single cost partitioning that is optimal for all states.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Connection to Operator-Counting Constraints", "text": "We now study the connection between general cost partitioning and certain heuristics based on linear programming, originally introduced by Pommerening et al. (2014b).\nConsider a set C of linear inequalities over non-negative operator-counting variables of the form Count o for each operator o. An operator sequence \u03c0 satisfies C if the inequalities in C are satisfied by setting each variable Count o to the number of occurrences of o in \u03c0. If s is a state such that C is satisfied by every s-plan, C is called an operator-counting constraint for s. Examples of operator-counting constraints include landmark constraints, net change constraints, posthoc optimization constraints and optimal cost partitioning constraints (Pommerening et al. 2014b). 1 We write the inequalities of an operator-counting constraint C as coeffs(C) Count \u2265 bounds(C) for a coefficient matrix coeffs(C), operator-counting variable vector Count and a bounds vector bounds(C).\nIf C is a set of operator-counting contraints for a state s and cost is a cost function, then IP C (cost ) denotes the following integer program: minimize o\u2208O cost (o)Count o subject to C and Count \u2265 0. The objective value of this integer program, which we denote by h IP C (cost ), is an admissible heuristic estimate for s under cost function cost . We write LP C (cost ) and h LP C (cost ) for the LP relaxation of the integer program and the objective value of this LP, which is also an admissible heuristic estimate for s under cost . (Note that even though our notations omit the state s, the constraints C of course generally depend on the state s.)\nIt turns out that there is an intimate link between operatorcounting constraints and general cost partitioning. Given a set of operator-counting constraints, its LP heuristic estimate equals the optimal general cost partitioning over the LP heuristics for each individual constraint: Theorem 2. Let C be a set of operator-counting constraints for a state s. Then \nMaximize C\u2208C bounds(C) \u2022 Dual C subject to coeffs(C) Dual C \u2264 Cost C Dual C \u2265 0 for all C \u2208 C C\u2208C Cost C o \u2264 cost(o) for all o \u2208 O\nThe first two inequalities are exactly the dual constraints of LP {C} (Cost C ), and the objective function is exactly the sum of dual objective functions for these LPs. The remaining inequality ensures that Cost C defines a general cost partitioning. As we maximize the sum of individual heuristic values over all possible general cost partitionings, the result is the optimal general cost partitioning of the component heuristics.\nThe proof is constructive in the sense that it shows a way to compute an optimal cost partitioning for the given state from a dual solution of the original LP heuristic. ", "publication_ref": ["b14", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Net-Change Constraints", "text": "We now turn to the state equation heuristic h SEQ (Bonet and van den Briel 2014) as an application of Theorem 2. Pommerening et al. (2014b) showed that this heuristic dominates the optimal non-negative cost-partitioning over projections to goal variables. We use their formalization as lower bound net-change constraints that compare the number of times a fact is produced to the number of times it is consumed on the path from a given state s to the goal. For each fact V, v , we obtain one inequality:\no always produces V, v Count o + o sometimes produces V, v Count o \u2212 o always consumes V, v Count o \u2265 1{if s [V ] = v} \u2212 1{if s[V ] = v}\nAn operator-counting constraint can consist of any number of linear inequalities. For our purposes, it is useful to group the inequalities for all facts that belong to the same state variable V into one constraint C V . The state equation heuristic is then the LP heuristic for the set C = {C V | V \u2208 V} consisting of one constraint for each state variable.\nTo apply Theorem 2, we must first understand which heuristics are defined by each individual constraint C V and arbitrary cost function cost .\nTheorem 4. Let \u03a0 be a planning task, and let V be one of its state variables. Let h V denote the atomic abstraction heuristic based on projecting each state s to s\n[V ]. Then h LP {C V } (cost ) = h V (s, cost )\nProof: Consider the planning task \u03a0 V obtained by projecting \u03a0 to V (i. e., discarding all aspects of its description related to other variables). It is easy to see that h LP {C V } (cost ) corresponds to h SEQ with cost function cost in \u03a0 V and that h V (s, cost ) in the original task is h * (s, cost ) in \u03a0 V .\nThe \"\u2264\" part of the proof then follows from the admissibility of h SEQ for \u03a0 V . For the \"\u2265\" part, Pommerening et al. (2014b) showed that h SEQ dominates the optimal cost partitioning over atomic abstraction heuristics for goal variables. It can easily be verified that their proof also works for the case where negative costs are permitted. If V is a goal variable and we apply this result to \u03a0 V , this optimal cost partitioning is a (trivial) partitioning over a single heuristic h V and hence equal to h V (s, cost ). If V is a non-goal variable, a slight adaptation is needed, for which we refer to the technical report (Pommerening et al. 2014a).\nWe can now specify the connection between the state equation heuristic and abstraction heuristics more precisely:\nTheorem 5. The state equation heuristic is the optimal general cost partitioning of projections to all single variables,\nh SEQ (s) = h OCP ((h V ) V \u2208V , s).\nProof: We apply Theorem 2 with the set of constraints C = {C V | V \u2208 V}. Theorem 4 establishes the connection to projections and Pommerening et al. (2014b) show that h SEQ (s) = h LP C (cost). This answers a question raised by Bonet (2013): how does h SEQ relate to the four families of heuristics identified by Helmert and Domshlak (2009)? We now see that we can interpret it as an abstraction heuristic within the framework of general cost partitioning. We also note that with Theorem 3 we can extract an optimal cost partitioning from the dual solution of the LP for h SEQ .\nThe concept of fluent merging (van den Briel, Kambhampati, and Vossen 2007;Seipp and Helmert 2011) shows an interesting connection to projections to more than one variable. Merging state variables (fluents) X and Y means introducing a new state variable Z that captures the joint behaviour of X and Y . Theorem 4 shows that the LP heuristic for net change constraints of variable Z is perfect for the projection to Z and thus also for the projection to {X, Y }.\nOperator-counting constraints for merging a set of facts M of two variables were introduced by Bonet and van den Briel (2014). They show that these constraints have perfect information for the merged variable if M = dom(X) \u00d7 dom(Y ). We can now see that these constraints define the projection to two variables. Bonet and van den Briel further suggest the use of mutexes and note a resemblance of the corresponding abstraction heuristics to constrained pattern databases (Haslum, Bonet, and Geffner 2005). We conjecture that partial merges correspond to further abstractions of these projections. If this conjecture holds, Theorem 2 shows that the state equation heuristic extended with constraints for merged variables calculates a general cost partitioning over the corresponding abstractions.", "publication_ref": ["b14", "b14", "b13", "b14", "b2", "b7", "b16", "b16", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Potential Heuristics", "text": "In this section we introduce a new family of heuristics called potential heuristics and show their relation to general cost partitioning. Potential heuristics associate a numerical potential with each fact. The heuristic value for a state s is then simply the sum of potentials of all facts in s. Definition 3. Let \u03a0 be a planning task with variables V and facts F. A potential function is a function pot : F \u2192 R.\nThe potential heuristic for pot maps each state to the sum of potentials of the facts of s:\nh pot (s) = V \u2208V pot( V, s[V ] )\nWe simplify our presentation by assuming that all variables in the effect of an operator o also occur in its precondition (vars(eff (o)) \u2286 vars(pre(o))) and there is a unique goal state (vars(s ) = V). The definitions and proofs are generalized to arbitrary tasks in the technical report (Pommerening et al. 2014a).\nA potential heuristic h pot is goal-aware if and only if\nh pot (s ) = V \u2208V pot( V, s [V ] ) \u2264 0. It is consistent if and only if h pot (s) \u2264 cost(o) + h pot (s o\n) for every state s and every operator o applicable in s. This condition can be simplified as follows because the potentials of all facts not changed by an effect cancel out:\ncost(o) \u2265 V \u2208V pot( V, s[V ] ) \u2212 V \u2208V pot( V, s o [V ] ) = V \u2208vars(eff (o)) pot( V, s[V ] ) \u2212 V \u2208vars(eff (o)) pot( V, s o [V ] ) = V \u2208vars(eff (o)) (pot( V, pre(o)[V ] ) \u2212 pot( V, eff (o)[V ] ))\nThe resulting inequality is no longer state-dependent and is necessary and sufficient for the consistency of h pot .\nGoal-aware and consistent potential heuristics can thus be compactly classified by a set of linear inequalities. Goalaware and consistent heuristics are also admissible, so we can use an LP solver to optimize any linear combination of potentials and transform the solution into an admissible and consistent potential heuristic. Definition 4. Let f be a solution to the following LP:\nMaximize opt subject to\nV \u2208V P V,s [V ] \u2264 0 and V \u2208vars(eff (o)) (P V,pre(o)[V ] \u2212 P V,eff (o)[V ] ) \u2264 cost(o) for all o \u2208 O,\nwhere the objective function opt can be chosen arbitrarily.\nThen the function pot opt ( V, v ) = f (P V,v ) is the potential function optimized for opt and h pot opt is the potential heuristic optimized for opt. Proposition 2. For any objective function opt, the heuristic h pot opt is admissible and consistent, and it maximizes opt among all admissible and consistent potential heuristics.\nAs an example, we consider the potential heuristic optimized for the heuristic value of the initial state:\nopt sI = V \u2208V P V,sI[v]\nIt turns out that this heuristic is closely linked to the heuristics we discussed in the preceding sections: Proposition 3. h pot opt s I (s I ) = h SEQ (s I ).\nProof sketch (full proof in Pommerening et al. 2014a):\nThe linear programs solved for h pot opt s I (s) and the state equation heuristic in the initial state are each other's duals. This can be seen with the substitution P\nV,v = X V,s [V ] \u2212 X V,v\nwhere the non-negative variables X V,v are the dual variables of the state equation heuristic LP.\nTogether with Theorem 5, it follows that the potential heuristic approach offers an alternative way of performing an optimal general cost partitioning for single-variable abstraction heuristics. Compared to previous cost-partitioning approaches, the linear programs in Definition 4 are extremely simple in structure and very easy to understand. Our discussion also gives us a better understanding of what these heuristics compute: the best possible (for a given optimization function) admissible and consistent heuristic that can be represented as a weighted sum of indicator functions for the facts of the planning task.\nUsing the state equation heuristic requires solving an LP for every state evaluated by a search algorithm. It offers the best possible potential heuristic value for every single state. Alternatively, we can trade off accuracy for computation time by only computing one potential function (for example optimized for the initial state) and then using the induced, very quickly computable heuristic for the complete search. We will investigate this idea experimentally in the following section.", "publication_ref": ["b13", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "We implemented the state equation heuristic and the potential heuristic that optimizes the heuristic value of the initial state in the Fast Downward planning system (Helmert 2006).\nAll our experiments were run on the set of tasks from optimal tracks of IPC 1998-2011, limiting runtime to 30 minutes and memory usage to 2 GB. Each task ran on a single core of an Intel Xeon E5-2660 processor (2.2 GHz). Linear programs were solved with IBM's CPLEX v12.5.1.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Optimal Cost Partitioning for Projections", "text": "Previous experiments (Pommerening et al. 2014b) showed that the state equation heuristic (h SEQ ) outperforms the optimal non-negative cost partitioning of projections to goal variables (h OCP+ Goal1 ). To explain this difference we compare these two heuristics to the optimal cost partitioning over all projections to single variables using non-negative (h OCP+ All1 ) and general (h OCP All1 ) cost partitioning. The heuristics h OCP All1 and h SEQ compute the same value, but the encoding of h OCP All1 is much larger because it contains one constraint for each transition of each projection and one variable for each abstract state. Likewise, the heuristics h OCP+ Goal1 and h OCP+ All1 compute the same value because non-goal variables cannot contribute to the heuristic with non-negative cost partitioning.\nTable 1 shows the number of solved tasks (coverage) for the tested configurations. If only non-negative costs are considered (h OCP+ All1 ), using all variables is useless and reduces   coverage in 22 out of 44 domains and from 500 to 442 solved tasks in total. Allowing negative costs (h OCP All1 ) recovers most, but not all of this loss. The LPs for general cost partitioning are harder to solve because more interaction with nongoal variables is possible. This decreases coverage by 1 in six domains and by 2 in one domain. On the other hand, general cost partitioning results in fewer expansions before the last f -layer in 25 domains (not shown), which results in better coverage in 13 of them. Compared to h OCP+ Goal1 using general operator cost partitioning and projections to all variables does not pay off overall because the resulting LPs get too large. A notable exception are the domains FreeCell and ParcPrinter where h OCP All1 frequently calculates the perfect heuristic values for the initial state. Figure 2 shows that using general operator cost partitioning substantially reduces the number of expansions.\nDue to the smaller LP size h SEQ solves more tasks than h OCP All1 in 39 domains, an overall coverage increase of 140. This shows that the main reason for h SEQ 's superior performance is the more compact representation, though the general cost partition also plays an important role, as the comparison of h OCP+ All1 and h OCP All1 shows. ", "publication_ref": ["b14"], "figure_ref": ["fig_2"], "table_ref": ["tab_1"]}, {"heading": "Potential Heuristics", "text": "We experimented with two ways to compute potential heuristics optimized for the initial state. One solves the linear program in Definition 4 (h pot LP-sI ) directly, while the other computes the state equation heuristic and extracts the potentials from its dual using Theorem 3 (h pot SEQ-sI ). Both ways are guaranteed to result in the same heuristic estimate for the initial state, but since there are usually infinitely many optimal potential functions for a given state, they can differ widely on other states.\nThe heuristic h pot SEQ-sI has higher coverage than h pot LP-sI in 17 domains and lower coverage in 4 domains. Overall, the former solves 638 tasks and the latter 610 tasks. Since potential heuristics are fast to compute, we exploited their complementary strengths by evaluating the maximum of both heuristics, which solves all tasks solved by either of them except two. This results in a coverage of 657 tasks, a large improvement over the state equation heuristic which optimizes the cost partitioning for every state.\nThe main advantage of potential heuristics is that they are extremely fast to evaluate. This can be seen in Figure 3, which compares the number of tasks that could be solved within a given time by the state equation heuristic and the potential heuristics. With the maximum of both potential heuristics 600 tasks are solved in the first minute, compared to 17 minutes to solve 600 tasks for h SEQ .", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Conclusions", "text": "We showed that the traditional restriction to non-negative cost partitioning is not necessary and that heuristics can benefit from permitting operators with negative cost. In addition, we demonstrated that heuristics based on operatorcounting constraints compute an optimal general cost partitioning. This allows for a much more compact representation of cost-partitioning LPs, and we saw that the state equation heuristic can be understood as such a compact form of expressing an optimal cost partitioning over projections. We believe that an extension to other cost-partitioned heuristics is a promising research direction for future work.\nWe also introduced potential heuristics as a fast alternative to optimal cost partitioning. By computing a heuristic parameterization only once and sticking with it, they obtain very fast state evaluations, leading to significant coverage increases and a more than 10-fold speedup over the state equation heuristic. We think that the introduction of potential heuristics opens the door for many interesting research avenues and intend to pursue this topic further in the future.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "This work was supported by the Swiss National Science Foundation (SNSF) as part of the project \"Abstraction Heuristics for Planning and Combinatorial Search\" (AH-PACS).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Complexity results for SAS + planning", "journal": "Computational Intelligence", "year": "1995", "authors": "C B\u00e4ckstr\u00f6m; B Nebel"}, {"ref_id": "b1", "title": "Flow-based heuristics for optimal planning: Landmarks and merges", "journal": "AAAI Press", "year": "2014", "authors": "B Bonet;  Van Den; M Briel"}, {"ref_id": "b2", "title": "An admissible heuristic for SAS + planning obtained from the state equation", "journal": "", "year": "2013", "authors": "B Bonet"}, {"ref_id": "b3", "title": "Automated creation of pattern database search heuristics", "journal": "", "year": "2006", "authors": "S Edelkamp"}, {"ref_id": "b4", "title": "A formal basis for the heuristic determination of minimum cost paths", "journal": "IEEE Transactions on Systems Science and Cybernetics", "year": "1968", "authors": "P E Hart; N J Nilsson; B Raphael"}, {"ref_id": "b5", "title": "Domain-independent construction of pattern database heuristics for cost-optimal planning", "journal": "AAAI Press", "year": "2007", "authors": "P Haslum; A Botea; M Helmert; B Bonet; S Koenig"}, {"ref_id": "b6", "title": "New admissible heuristics for domain-independent planning", "journal": "AAAI Press", "year": "2005", "authors": "P Haslum; B Bonet; H Geffner"}, {"ref_id": "b7", "title": "Landmarks, critical paths and abstractions: What's the difference anyway", "journal": "AAAI Press", "year": "2009", "authors": "M Helmert; C Domshlak"}, {"ref_id": "b8", "title": "The Fast Downward planning system", "journal": "Journal of Artificial Intelligence Research", "year": "2006", "authors": "M Helmert"}, {"ref_id": "b9", "title": "Cost-optimal planning with landmarks", "journal": "", "year": "2009", "authors": "E Karpas; C Domshlak"}, {"ref_id": "b10", "title": "Structural patterns heuristics: Basic idea and concrete instance", "journal": "", "year": "2007", "authors": "M Katz; C Domshlak"}, {"ref_id": "b11", "title": "Optimal admissible composition of abstraction heuristics", "journal": "Artificial Intelligence", "year": "2010", "authors": "M Katz; C Domshlak"}, {"ref_id": "b12", "title": "Disjoint pattern database heuristics", "journal": "Artificial Intelligence", "year": "2002", "authors": "R E Korf; A Felner"}, {"ref_id": "b13", "title": "From non-negative to general operator cost partitioning: Proof details", "journal": "", "year": "2014", "authors": "F Pommerening; M Helmert; G R\u00f6ger; J Seipp"}, {"ref_id": "b14", "title": "LP-based heuristics for cost-optimal planning", "journal": "AAAI Press", "year": "2014", "authors": "F Pommerening; G R\u00f6ger; M Helmert; B Bonet"}, {"ref_id": "b15", "title": "Getting the most out of pattern databases for classical planning", "journal": "", "year": "2013", "authors": "F Pommerening; G R\u00f6ger; M Helmert"}, {"ref_id": "b16", "title": "Fluent merging: A general technique to improve reachability heuristics and factored planning", "journal": "", "year": "2007", "authors": "J Seipp; M Helmert; S Kambhampati; T Vossen"}, {"ref_id": "b17", "title": "A general theory of additive state space abstractions", "journal": "Journal of Artificial Intelligence Research", "year": "2008", "authors": "F Yang; J Culberson; R Holte; U Zahavi; A Felner"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "h LP C (cost) = h OCP ((h LP {C} ) C\u2208C , s). Proof sketch (full proof in Pommerening et al. 2014a): In LP C (cost) we can introduce local copies LCount C o of Count o for every constraint C \u2208 C by adding equations LCount C o = Count o for all o \u2208 O. We then replace all occurrences of Count o in this constraint by LCount C o . The resulting LP minimizes o\u2208O cost(o)Count o subject to coeffs(C)LCount C \u2265 bounds(C), LCount C = Count, LCount C \u2265 0 for all C \u2208 C, and Count \u2265 0. The dual of this LP has the same objective value and contains one non-negative variable Dual C i for each inequality and one unbounded variable Cost C o for each equation:", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Theorem 3 .3Let C be a set of operator-counting constraints for a state s. Let d be a dual solution of LP C (cost). Then the general cost partitioning cost C (o) = coeffs(C) d(Dual C ) is optimal for the heuristics h LP {C} for C \u2208 C in state s. Proof: Every optimal solution of the LP in the proof for Theorem 2 contains an optimal cost partition in the variables Cost C o . If we take an optimal solution and replace the value of Cost C o with coeffs(C) d(Dual C ), no value of Cost C o can increase. All inequalities are still satisfied and the objective value does not change.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Number of expansions (excluding last f -layer) for optimal cost partitioning of projections to single variables with non-negative and general costs. Points above the diagonal represent tasks for which general operator cost partitioning needs fewer expansions.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Number of solved tasks per time for the potential and the state equation heuristic.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Coverage for different variants of optimal cost partitioning.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "s o [V ] = eff (o)[V ] if V \u2208 vars(eff (o)) s[V ]", "formula_coordinates": [1.0, 348.3, 430.97, 179.71, 21.89]}, {"formula_id": "formula_1", "formula_text": "\u03c0 = o 1 , . . . , o n under cost function cost is cost (\u03c0) = n i=1 cost (o i", "formula_coordinates": [1.0, 319.5, 617.38, 238.5, 22.11]}, {"formula_id": "formula_2", "formula_text": "i : O \u2192 R + 0 for 1 \u2264 i \u2264 n and n i=1 cost i (o) \u2264 cost(o) for all o \u2208 O. Proposition 1", "formula_coordinates": [2.0, 54.0, 393.78, 238.5, 37.42]}, {"formula_id": "formula_3", "formula_text": "n i=1 cost i (o) \u2264 cost(o) for all o \u2208 O. Let h 1 , . . . , h n be admissible heuristics for \u03a0. Then h P (h 1 , . . . , h n , s) = n i=1 h i (s, cost i", "formula_coordinates": [2.0, 319.5, 232.86, 238.5, 36.03]}, {"formula_id": "formula_4", "formula_text": "h P (h 1 , . . . , h n , s) = n i=1 h i (s, cost i ) \u2264 n i=1 h * (s, cost i ) \u2264 n i=1 k j=1 cost i (o j ) = k j=1 n i=1 cost i (o j ) \u2264 k j=1 cost(o j ) = h * (s).", "formula_coordinates": [2.0, 320.85, 374.56, 235.8, 101.74]}, {"formula_id": "formula_5", "formula_text": "h P (h V1 , h V2 , s I ) = h V1 (s I , cost 1 ) + h V2 (s I , cost 2 ) = cost(o 1 ) + 0 = 1.", "formula_coordinates": [3.0, 54.0, 318.44, 238.5, 22.18]}, {"formula_id": "formula_6", "formula_text": "yields h P (h V1 , h V2 , s I ) = h V1 (s I , cost 1 ) + h V2 (s I , cost 2 ) = 2 + 0 = 2, a perfect heuristic estimate for the example.", "formula_coordinates": [3.0, 54.0, 373.68, 238.5, 32.45]}, {"formula_id": "formula_7", "formula_text": "P \u2208Pn h P (h 1 , . . . , h n , s)", "formula_coordinates": [3.0, 171.54, 487.07, 102.52, 16.52]}, {"formula_id": "formula_8", "formula_text": "h OCP (h 1 , . . . , h n , s) = max P \u2208Pn h P (h 1 , . . . , h n , s).", "formula_coordinates": [3.0, 76.92, 541.05, 192.65, 16.43]}, {"formula_id": "formula_9", "formula_text": "Maximize C\u2208C bounds(C) \u2022 Dual C subject to coeffs(C) Dual C \u2264 Cost C Dual C \u2265 0 for all C \u2208 C C\u2208C Cost C o \u2264 cost(o) for all o \u2208 O", "formula_coordinates": [3.0, 340.31, 571.64, 196.3, 84.23]}, {"formula_id": "formula_10", "formula_text": "o always produces V, v Count o + o sometimes produces V, v Count o \u2212 o always consumes V, v Count o \u2265 1{if s [V ] = v} \u2212 1{if s[V ] = v}", "formula_coordinates": [4.0, 76.46, 475.75, 177.55, 42.9]}, {"formula_id": "formula_11", "formula_text": "[V ]. Then h LP {C V } (cost ) = h V (s, cost )", "formula_coordinates": [4.0, 116.08, 655.7, 159.51, 31.93]}, {"formula_id": "formula_12", "formula_text": "h SEQ (s) = h OCP ((h V ) V \u2208V , s).", "formula_coordinates": [4.0, 377.34, 306.55, 122.83, 11.72]}, {"formula_id": "formula_13", "formula_text": "h pot (s) = V \u2208V pot( V, s[V ] )", "formula_coordinates": [5.0, 112.31, 178.74, 121.88, 22.14]}, {"formula_id": "formula_14", "formula_text": "h pot (s ) = V \u2208V pot( V, s [V ] ) \u2264 0. It is consistent if and only if h pot (s) \u2264 cost(o) + h pot (s o", "formula_coordinates": [5.0, 54.0, 282.61, 238.5, 22.95]}, {"formula_id": "formula_15", "formula_text": "cost(o) \u2265 V \u2208V pot( V, s[V ] ) \u2212 V \u2208V pot( V, s o [V ] ) = V \u2208vars(eff (o)) pot( V, s[V ] ) \u2212 V \u2208vars(eff (o)) pot( V, s o [V ] ) = V \u2208vars(eff (o)) (pot( V, pre(o)[V ] ) \u2212 pot( V, eff (o)[V ] ))", "formula_coordinates": [5.0, 54.58, 348.54, 237.34, 77.96]}, {"formula_id": "formula_16", "formula_text": "V \u2208V P V,s [V ] \u2264 0 and V \u2208vars(eff (o)) (P V,pre(o)[V ] \u2212 P V,eff (o)[V ] ) \u2264 cost(o) for all o \u2208 O,", "formula_coordinates": [5.0, 54.0, 536.22, 229.26, 33.41]}, {"formula_id": "formula_17", "formula_text": "opt sI = V \u2208V P V,sI[v]", "formula_coordinates": [5.0, 128.94, 686.64, 84.9, 20.17]}, {"formula_id": "formula_18", "formula_text": "V,v = X V,s [V ] \u2212 X V,v", "formula_coordinates": [5.0, 319.5, 136.46, 238.5, 21.02]}], "doi": ""}