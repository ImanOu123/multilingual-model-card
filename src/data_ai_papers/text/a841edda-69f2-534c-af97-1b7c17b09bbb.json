{"title": "Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera", "authors": "Hanme Kim; Stefan Leutenegger; Andrew J Davison", "pub_date": "", "abstract": "We propose a method which can perform real-time 3D reconstruction from a single hand-held event camera with no additional sensing, and works in unstructured scenes of which it has no prior knowledge. It is based on three decoupled probabilistic filters, each estimating 6-DoF camera motion, scene logarithmic (log) intensity gradient and scene inverse depth relative to a keyframe, and we build a real-time graph of these to track and model over an extended local workspace. We also upgrade the gradient estimate for each keyframe into an intensity image, allowing us to recover a real-time video-like intensity sequence with spatial and temporal super-resolution from the low bit-rate input event stream. To the best of our knowledge, this is the first algorithm provably able to track a general 6D motion along with reconstruction of arbitrary structure including its intensity and the reconstruction of grayscale video that exclusively relies on event camera data.", "sections": [{"heading": "Introduction", "text": "Event cameras offer a breakthrough new paradigm for real-time vision, with potential in robotics, wearable devices and autonomous vehicles, but it has proven very challenging to use them in most standard computer vision problems. Inspired by the superior properties of human vision [2], an event camera records not image frames but an asynchronous sequence of per-pixel intensity changes, each with a precise timestamp. While this data stream efficiently encodes image dynamics with extremely high dynamic range and temporal contrast, the lack of synchronous intensity information means that it is not possible to apply much of the standard computer vision toolbox of techniques. In particular, the multi-view correspondence information which is essential to estimate motion and structure is difficult to obtain because each event by itself carries little information and no signature suitable for reliable matching.\nApproaches aiming at simultaneous camera motion and scene structure estimation therefore need also to jointly estimate the intensity appearance of the scene, or at least a highly descriptive function of this such as a gradient map. So far, this has only been successfully achieved in the reduced case of pure camera rotation, where the scene reconstruction takes the form of a panorama image.\nIn this paper we present the first algorithm which performs joint estimation of 3D scene structure, 6-DoF camera motion and up to scale scene intensity from a single hand-held event camera moved in front of an unstructured static scene. Our approach runs in real-time on a standard PC. The core of our method is three interleaved probabilistic filters, each estimating one unknown aspect of this challenging Simultaneous Localisation and Mapping (SLAM) problem: camera motion, scene log intensity gradient and scene inverse depth. From pure event input our algorithm generates various outputs including a real-time, high bandwidth 6-DoF camera track, scene depth map for one or multiple linked keyframes, and a high dynamic range reconstructed video sequence at a userchosen frame-rate.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Event-Based Cameras", "text": "The event camera or silicon retina is gradually becoming more widely known by researchers in computer vision, robotics and related fields, in particular since the release as a commercial device for researchers of the Dynamic Vision Sensor (DVS) [14] shown in Figure 1 (c). The pixels of this device asynchronously report log intensity changes of a pre-set threshold size as a stream of asynchronous events, each with pixel location, polarity, and microsecond-precise timestamp. Figure 1 visualises some of the main properties of the event stream; in particular the almost continuous response to very rapid motion and the way that the output data-rate depends on scene motion, though in practice almost always dramatically lower than that of standard video. These properties offer the potential to overcome the limitations of real-world computer vision applications, relying on conventional imaging sensors, such as high latency, low dynamic range, and high power consumption.\nRecently, cameras have been developed that interleave event data with conventional intensity frames (DAVIS [3]), or per-event intensity measurement (ATIS [21]). Our framework could be extended to make use of these image measurements this would surely make joint estimation easier. However, in a persistently dynamic motion, they may not be useful. Also, they partially break the appeal and optimal information efficiency of a pure event-based data stream. We therefore believe that first solving the hardest problem of not relying on standard image frames will be useful on its own and provides the insights to make best use of additional measurements if they are available.", "publication_ref": ["b13", "b2", "b20"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Related Work", "text": "Early published work using event cameras focused on tracking moving objects from a fixed point of view, successfully showing the superior high speed measurement and low latency properties [8,6]. However, work on tracking and reconstruction of more general, previously unknown scenes with a freely moving event camera, which we believe is the best place to take full advantage of its remarkable properties, has been limited. The clear difficulty is that most methods normally used in tracking and mapping, such as feature detection and matching or whole image alignment, cannot be directly applied to its fundamentally different visual measurement stream. Cook et al. [7] proposed an interacting network which interprets a stream of events to recover different visual estimate 'maps' of scenes such as intensity, gradient and optical flow while estimating global rotating camera motion. More recently, Bardow et al. [1] presented an optical flow and intensity estimation using an event camera which allows any camera motion as well as dynamic scenes.\nAn early 2D SLAM method was proposed by Weikersdorfer et al. [24] which tracks a ground robot pose while reconstructing a planar ceiling map with an upward looking DVS camera. Mueggler et al. [19] presented an onboard 6-DoF localisation flying robot system which is able to track its relative pose to a known target even at very high speed. To investigate whether current techniques can be applied to a large scale visual SLAM problem, Milford et al. [16] presented a simple visual odometry system using a DVS camera with loop closure built on top of the SeqSLAM algorithm using events accumulated into frames [17].\nIn a much more constrained and hardware-dependent setup, Schraml et al. [22] developed a special 360 \u2022 rotating camera that consists of a pair of dynamic vision line sensors which creates 3D panoramic scenes aided by its embedded encoders and stereo event streams. Combined with an active projector, Matsuda et al. [15] showed that high quality 3D object reconstruction can be achievable which is better than for laser scanners or RGB-D cameras in some specific situations.\nThe most related work to our method is the simplified SLAM system based on probabilistic filtering proposed by Kim et al. [12], which estimates spatial gradients which are then integrated to reconstruct high quality and high dynamic range planar scenes while tracking global camera rotation. Their method has a similar overall concept to ours with multiple interacting probabilistic filters, but is limited to pure rotation camera motion and panorama reconstruction. Also it is not completely real-time because of the computational complexity of the particle filter used in their tracking algorithm.\nThere have been no previous published results on estimating 3D depth from a single moving event camera. Most researchers working with event cameras have assumed that this problem is too difficult, and attempts at 3D estimation have combined an event camera with other sensors: a standard frame-based CMOS camera [5], or an RGB-D camera [23]. These are, of course, possible practical ways of using an event camera for solving SLAM problems. However, we believe that resorting to standard sensors discards many of the advantages of processing the efficient and data-rick pure event stream, as well as introducing extra complication including synchronisation and calibration problems to be solved. One very interesting approach if the application permits is to combine two event cameras in a stereo setup [4]. The nicest part of that method is the way that stereo matching of events can be achieved based on coherent timestamps.\nOur work in this paper was inspired by a strong belief that depth estimation from a single moving event camera must be possible, because if the device is working correctly and recording all pixel-wise intensity changes then all of the information present in a standard video stream must be available in principle, at least up to scale. In fact, the high temporal contrast and dynamic range of event pixels means that much more information should be present in an event stream than in standard video at the same resolution. In particular, the results of Kim et al. [12] on sub-pixel tracking and super-resolution mosaic reconstruction from events gave a strong indication that the accurate multi-view correspondence needed for depth estimation is possible. The essential insight to extending Kim et al.'s approach towards getting depth from events is that once the camera starts to translate, if two pixels have the same intensity gradient, the one which is closer to the camera move past the camera faster and therefore emit more events than the farther one. This is the essential mechanism built into our probabilistic filter for inverse depth.", "publication_ref": ["b7", "b5", "b6", "b0", "b23", "b18", "b15", "b16", "b21", "b14", "b11", "b4", "b22", "b3", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Method", "text": "Following many recent successful SLAM systems such as PTAM [13], DTAM [20], and LSD-SLAM [10], which separate the tracking and mapping components based on the assumption that the current estimate from one component is accurate enough to lock for the purposes of estimating the other, the basic structure of our approach relies on three interleaved probabilistic filters. One tracks the global 6-DoF camera motion; the second estimates the log intensity gradients in a keyframe image -a representation which is also in parallel upgraded into a full image-like intensity map. Finally the third filter estimates the inverse depths of a keyframe. It should be noted that we essentially separate the mapping part into two, i.e. the gradient and inverse depth estimations, considering fewer number of events caused by parallax while almost all events carry gradient information. We also build a textured semi-dense 3D point cloud from selected keyframes with their associated reconstructed intensity and inverse depth estimate. We do not use an explicit bootstrapping method as we have found that, starting from scratch, alternating estimation very often lead to convergence.", "publication_ref": ["b12", "b19", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "We denote an event as e(u, v) = (u, v, p, t) where u and v are pixel location, p is polarity and t is microsecond-precise timestamp -our event-based camera has the fixed pre-calibrated intrinsic matrix K and all event pixel locations are prewarped to remove radial distortion. We also define two important time intervals \u03c4 and \u03c4 c , as in [12], which are the time elapsed since the most recent previous event from any pixel and at the same pixel respectively.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Event-Based Camera 6-DoF Tracking", "text": "We use an Extended Kalman Filter (EKF) to estimate the global 6-DoF camera motion over time with its state x \u2208 R 6 , which is a minimal representation of the camera pose c with respect to the world frame of reference w, and covariance matrix P x \u2208 R 6\u00d76 . The state vector is mapped to a member of the Lie group SE(3), the set of 3D rigid body transformations, by the matrix exponential map:\nT wc = exp 6 i=1 x i G i = R wc t w 0 1 ,(1)\nwhere G is the Lie group generator for SE(3), R wc \u2208 SO(3), and t w \u2208 R 3 . The basic idea is to find (assuming that the current log intensity and inverse depth estimates are correct) the camera pose which best predicts a log intensity change consistent with the event just received, as shown in Figure 2 (a).\nMotion Prediction We use a 6-DoF (translation and rotation) constant position motion model for motion prediction; the variance of the prediction is proportional to the time interval:\nx (t|t\u2212\u03c4 ) = x (t\u2212\u03c4 |t\u2212\u03c4 ) + n ,(2)\nP x (t|t\u2212\u03c4 ) = P (t\u2212\u03c4 |t\u2212\u03c4 ) x + P n ,(3)\nwhere each component of n is independent Gaussian noise in all six axes i.e. n i \u223c N (0, \u03c3 2 i \u03c4 ), and P n = diag(\u03c3 2 1 \u03c4, . . . , \u03c3 2 6 \u03c4 ). , as shown in Figure 3:\nz x = \u00b1C ,(4)\nh x (x (t|t\u2212\u03c4 ) ) = I l p (t) w \u2212 I l p (t\u2212\u03c4c) w ,(5)\nwhere\nI l (p w ) = (1 \u2212 a \u2212 b)I l (v 0 ) + aI l (v 1 ) + bI l (v 2 ) . (6\n)\nHere \u00b1C is a known event threshold -its sign is decided by the polarity of an event. I l is a log intensity value based on a reconstructed log intensity keyframe, and v 0 , v 1 , and v 2 are three vertices of an intersected triangle. To obtain a corresponding 3D point location p w in the world frame of reference, we use raytriangle intersection [18] which yields a vector (l, a, b) where l is the distance to the triangle from the origin of the ray and a, b are the barycentric coordinates of the intersected point which is then used to calculate an interpolated log intensity.\nIn the EKF framework, the camera pose estimate and its uncertainty covariance matrix are updated by the standard equations at every event using: , in the world frame of reference using a ray-triangle intersection method [18] to compute the value of a measurement -a log intensity difference between two points given an event e(u, v), the current keyframe pose T wk , the current camera pose estimate T (t) wc, the previous pose estimate T (t\u2212\u03c4c) wc , the reconstructed log intensity and inverse depth keyframe, gradient estimation: we project two intersection points onto the current keyframe, p (t) k and p (t\u2212\u03c4c) k , to find a displacement vector between them, which is then used to calculate a motion vector m to compute the value of a measurement (g \u2022 m) at a midpointp k based on the brightness constancy and the linear gradient assumption.\nx (t|t) = x (t|t\u2212\u03c4 ) + W x \u03bd x ,(7)\nP (t|t) x = I 6\u00d76 \u2212 W x \u2202h x \u2202x (t|t\u2212\u03c4 ) P (t|t\u2212\u03c4 ) x ,(8)\nwhere the innovation \u03bd x and Kalman gain W x are defined by the standard EKF definitions. The measurement uncertainty is a scalar variance \u03c3 2\nx , and we omit the Jacobian \u2202hx \u2202x (t|t\u2212\u03c4 ) derivation due to the space limitation.", "publication_ref": ["b17", "b17"], "figure_ref": ["fig_1", "fig_2"], "table_ref": []}, {"heading": "Gradient Estimation and Log Intensity Reconstruction", "text": "We now use the updated camera pose estimate to incrementally improve the estimates of the log intensity gradient at each keyframe pixel based on a pixelwise EKF. However, because of the random walk nature of our tracker which generates a noisy motion estimate, we first apply a weighted average filter to the new camera pose estimate. To reconstruct super resolution scenes by harnessing the very high speed measurement property of the event camera, we use a higher resolution for keyframes than for the low resolution sensor. This method is similar to the one in [12], but we model the measurement noise properly to get better gradient estimate, and use a parallelisable reconstruction method for speed.\nPixel-Wise EKF Based Gradient Estimation Each pixel of the keyframe holds an independent gradient estimate g(p k ) = (g u , g v ) , consisting of log intensity gradients g u and g v along the horizontal and vertical axes in image space respectively, and a 2 \u00d7 2 uncertainty covariance matrix P g (p k ). At initialisation, all gradients are initialised to zero with large variances. We assume, based on the rapidity of events, a linear gradient between two consecutive events at the same event camera pixel, and update the midpointp k of the two projected points p (t) k and p (t\u2212\u03c4c) k\n. We now define z g , a measurement of the instantaneous event rate at this pixel, and its measurement model h g based on the brightness constancy equation (g \u2022 m)\u03c4 c = \u00b1C, where g is a gradient estimate and m = (m u , m v ) is a motion vector -the displacement between two corresponding pixels in the current keyframe divided by the elapsed time \u03c4 c as shown in Figure 3:\nz g = \u00b1 C \u03c4 c ,(9)\nh g = (g(p k ) \u2022 m) ,(10)\nwhere\nm = p (t) k \u2212 p (t\u2212\u03c4c) k \u03c4 c . (11\n)\nThe current gradient estimate and its uncertainty covariance matrix at that pixel are updated independently in the same way as in the measurement update of our tracker following the standard EKF equations.\nThe Jacobian \u2202hg \u2202g(p k ) (t\u2212\u03c4c ) of the measurement function with respect to changes in gradient is simply (m u , m v ), and the measurement noise N g is:\nN g = \u2202z g \u2202C P C \u2202z g \u2202C = \u03c3 2 C \u03c4 2 c ,(12)\nwhere \u03c3 2 C is the sensor noise with respect to the event threshold.\nLog Intensity Reconstruction Along with the pixel-wise EKF based gradient estimation method, we perform interleaved absolute log intensity reconstruction running on a GPU. We define our convex minimisation function as:\nmin I l \u2126 ||g(p k ) \u2212 \u2207I l (p k )|| h d + \u03bb||\u2207I l (p k )|| h r dp k . (13\n)\nHere the data term represents the error between estimated gradients g(p k ) and those of a reconstructed log intensity \u2207I l (p k ), and the regularisation term enforces smoothness, both under a robust Huber norm. This function can be written using the Legendre Fenchel transformation [11] as follows: where we can solve by maximising with respect to p:\nmin I l max q max p { p, g \u2212 \u2207I l \u2212 d 2 ||p|| 2 \u2212 \u03b4 p (p)+ q, \u2207I l \u2212 r 2\u03bb ||q|| 2 \u2212 \u03b4 q (q)} ,(14)\np (n+1) = p (n) +\u03c3p(g\u2212\u2207I l ) 1+\u03c3p d max 1, p (n) +\u03c3p(g\u2212\u2207I l ) 1+\u03c3p d ,(15)\nmaximising with respect to q:\nq (n+1) = q (n) +\u03c3q\u2207I l 1+ \u03c3q r \u03bb max 1, 1 \u03bb q (n) +\u03c3q\u2207I l 1+ \u03c3q r \u03bb ,(16)\nand minimising with respect to I l :\nI (n+1) l = I (n) l \u2212 \u03c3 I l (divp (n+1) \u2212 divq (n+1) ) .(17)\nWe visualise the progress of gradient estimation and log intensity reconstruction over time during hand-held event camera motion in Figure 4.", "publication_ref": ["b11", "b10"], "figure_ref": ["fig_2", "fig_3"], "table_ref": []}, {"heading": "Inverse Depth Estimation and Regularisation", "text": "We now use the same camera pose estimate as in the gradient estimation and a reconstructed log intensity keyframe to incrementally improve the estimates of the inverse depth at each keyframe pixel based on another pixel-wise EKF.\nAs in camera pose estimation, assuming that the current camera pose estimate and reconstructed log intensity are correct, we aim to update the inverse depth estimate to best predict the log intensity change consistent with the current event polarity as shown in Figure 2 (b). Pixel-Wise EKF Based Inverse Depth Estimation Each pixel of the keyframe holds an independent inverse depth state value \u03c1(p k ) with variance \u03c3 2 \u03c1(p k ) . At initialisation, all inverse depths are initialised to nominal values with large variances. In the same way as in our tracking method, we calculate the value of a measurement z \u03c1 which is a log intensity difference between two corresponding ray-triangle intersection points p (t) w and p (t\u2212\u03c4c) w as shown in Figure 3:\nz \u03c1 = \u00b1C ,(18)\nh \u03c1 = I l p (t)\nw \u2212 I l p (t\u2212\u03c4c) w .(19)\nIn the EKF framework, we stack the inverse depths of all three vertices \u03c1 = (\u03c1 v0 , \u03c1 v1 , \u03c1 v2 ) which contribute to the intersected 3D point and update them with their associated 3 \u00d7 3 uncertainty covariance matrix at every event in the same way of the measurement update of our tracker following the standard EKF equations. The measurement noise N \u03c1 is a scalar variance \u03c3 2 \u03c1 , and we omit the Jacobian \u2202h\u03c1 \u2202\u03c1 (t\u2212\u03c4c ) derivation due to the space limitation.\nInverse Depth Regularisation As a background process running on a GPU, we perform inverse depth regularisation on keyframe pixels with high confidence inverse depth estimate whenever there has been a large change in the estimates. We penalise deviation from a spatially smooth inverse depth map by assigning each inverse depth value the average of its neighbours, weighted by their respective inverse variances as described in [9]. If two adjacent inverse depths are different more than 2\u03c3, they do not contribute to each other to preserve discontinuities due to occlusion boundaries. We visualise the progress of inverse depth estimation and regularisation over time as event data is captured during hand-held event camera motion in Figure 5.", "publication_ref": ["b8"], "figure_ref": ["fig_1", "fig_2", "fig_4"], "table_ref": []}, {"heading": "Experiments", "text": "Our algorithm runs in real-time on a standard PC with typical scenes and motion speed, and we have conducted experiments both indoors and outdoors. We recommend viewing our video 1 which illustrates all of the key results in a better form than still pictures and in real-time.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Single Keyframe Estimation", "text": "We demonstrate the results from our algorithm as it tracks against and reconstructs a single keyframe in a number of different scenes. In Figure 6, for each scene we show column by column an image-like view of the event streams, estimated gradient map, reconstructed intensity map with super resolution and high dynamic range properties, estimate depth map and semi-dense 3D point cloud. The 3D reconstruction quality is generally good, though we can see that there are sometimes poorer quality depth estimates near to occlusion boundaries and where not enough events have been generated.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Multiple Keyframes", "text": "We evaluated the proposed method on several trajectories which require multiple keyframes to cover. If the camera has moved too far away from the current keyframe, we create a new keyframe from the most recent estimation results and reconstruction. To create a new keyframe, we project all 3D points based on the current keyframe pose and the estimated inverse depth into the current camera pose, and propagate the current estimates and reconstruction only if they have high confidence in inverse depth. Figure 7 shows one of the results in a semi-dense 3D point cloud form constructed based on generated keyframes each consisting of reconstructed super-resolution and high dynamic range intensity and inverse depth map. The bright RGB 3D coordinate axes represent the current camera pose while the darker ones show all keyframe poses generated in this experiment.", "publication_ref": [], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "Video Rendering", "text": "Using our proposed method, we can turn an event-based camera into a high speed and high dynamic range artificial camera by rendering video frames based on ray-casting as shown in Figure 8. Here we choose to render at the same low resolution as event-based input.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "High Speed Tracking", "text": "We evaluated the proposed method on several trajectories which include rapid motion (e.g. shaking hand). The top graph in Fig. 9 shows the estimated camera pose history, and the two groups of the insets below show an image-like event  visualisation, a rendered video frame showing the quality of our tracker, and a motion blurred standard camera video frame as a reference of rapid motion. Our current implementation is not able to process this very high event-rate (up to 1M events per second in this experiment) in real-time, but we believe it is a simple matter of engineering to run at this extremely high rate in real-time in the near future.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discussion", "text": "Our results so far are qualitative, and we have focused on demonstrating the core novelty of our approach in breaking through to get joint estimation of depth, 6-DoF motion and intensity from pure event data with general motion and unknown general scenes. There are certainly still weakness in our current approach, and while we believe that it is remarkable that our approach of three interleaved filters, each of which operates as if the results of the others are Fig. 8. Our proposed method can render HDR video frames at user-chosen time instances and resolutions by ray-casting the current reconstruction. This is the same scene as in the first row of Figure 6.\nrapid translation rapid rotation Fig. 9. The top graph shows the estimated camera pose history, and the two groups of the insets below show an image-like event visualisation, a rendered video frame showing the quality of our tracker, and a motion blurred standard camera video frame as a reference of rapid motion (up to 5Hz in this experiment).\ncorrect, works at all, there is plenty of room for further research. It is clear that the interaction of these estimation processes is key, and in particular that the relatively slow convergence of inverse depth estimates tends to cause poor tracking, then data association errors and a corruption of other parts of the estimation process. We will investigate this further, and may need to step back from our current approach of real-time pure event-by-event processing towards a partially batch estimation approach in order to get better results.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Conclusions", "text": "To the best of our knowledge, this is the first 6-DoF tracking and 3D reconstruction method purely based on a stream of events with no additional sensing, and it runs in real-time on a standard PC. We hope this opens up the door to practical solutions to the current limitations of real-world SLAM applications.\nIt is worth restating that the measurement rate of the event-based camera is on the order of a microsecond, its independent pixel architecture provides very high dynamic range, and the bandwidth of an event stream is much lower than a standard video stream. These superior properties of event-based cameras offer the potential to overcome the limitations of real-world computer vision applications relying on conventional imaging sensors.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgements. Hanme Kim was supported by an EPSRC DTA scholarship and the Qualcomm Innovation Fellowship 2014. We thank Jacek Zienkiewicz, Ankur Handa, Patrick Bardow, Edward Johns and other colleagues at Imperial College London for many useful discussions.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Simultaneous Optical Flow and Intensity Estimation from an Event Camera", "journal": "", "year": "2016", "authors": "P Bardow; A J Davison; S Leutenegger"}, {"ref_id": "b1", "title": "Neuromorphic Chips", "journal": "Scientific American", "year": "2005", "authors": "K Boahen"}, {"ref_id": "b2", "title": "A 240\u00d7180 130 dB 3 \u00b5s Latency Global Shutter Spatiotemporal Vision Sensor", "journal": "IEEE Journal of Solid-State Circuits (JSSC)", "year": "2014", "authors": "C Brandli; R Berner; M Yang; S C Liu; T Delbruck"}, {"ref_id": "b3", "title": "Event-based 3D reconstruction from neuromorphic retinas", "journal": "Journal of Neural Networks", "year": "2013", "authors": "J Carneiro; S Ieng; C Posch; R Benosman"}, {"ref_id": "b4", "title": "Low-Latency Event-Based Visual Odometry", "journal": "", "year": "2014", "authors": "A Censi; D Scaramuzza"}, {"ref_id": "b5", "title": "A pencil balancing robot using a pair of AER dynamic vision sensors", "journal": "", "year": "2009", "authors": "J Conradt; M Cook; R Berner; P Lichtsteiner; R Douglas; T Delbruck"}, {"ref_id": "b6", "title": "Interacting maps for fast visual interpretation", "journal": "", "year": "2011", "authors": "M Cook; L Gugelmann; F Jug; C Krautz; A Steger"}, {"ref_id": "b7", "title": "Fast sensory motor control based on event-based hybrid neuromorphic-procedural system", "journal": "", "year": "2007", "authors": "T Delbruck; P Lichtsteiner"}, {"ref_id": "b8", "title": "Semi-dense visual odometry for a monocular camera", "journal": "", "year": "2013", "authors": "J Engel; J Sturm; D Cremers"}, {"ref_id": "b9", "title": "LSD-SLAM: Large-scale direct monocular SLAM", "journal": "", "year": "2014", "authors": "J Engel; T Schoeps; D Cremers"}, {"ref_id": "b10", "title": "Applications of the Legendre-Fenchel transformation to computer vision problems", "journal": "Imperial College London", "year": "2011", "authors": "A Handa; R A Newcombe; A Angeli; A J Davison"}, {"ref_id": "b11", "title": "Simultaneous Mosaicing and Tracking with an Event Camera", "journal": "", "year": "2014", "authors": "H Kim; A Handa; R Benosman; S H Ieng; A J Davison"}, {"ref_id": "b12", "title": "Parallel Tracking and Mapping for Small AR Workspaces", "journal": "", "year": "2007", "authors": "G Klein; D W Murray"}, {"ref_id": "b13", "title": "A 128\u00d7128 120 dB 15 \u00b5s Latency Asynchronous Temporal Contrast Vision Sensor", "journal": "IEEE Journal of Solid-State Circuits", "year": "2008", "authors": "P Lichtsteiner; C Posch; T Delbruck"}, {"ref_id": "b14", "title": "MC3D: Motion Contrast 3D Scanning", "journal": "", "year": "2015", "authors": "N Matsuda; O Cossairt; M Gupta"}, {"ref_id": "b15", "title": "Towards visual SLAM with event-based cameras", "journal": "", "year": "2015", "authors": "M Milford; H Kim; S Leutenegger; A J Davison"}, {"ref_id": "b16", "title": "Place recognition with event-based cameras and a neural implementation of SeqSLAM", "journal": "", "year": "2015", "authors": "M Milford; H Kim; M Mangan; S Leutenegger; T Stone; B Webb; A J Davison"}, {"ref_id": "b17", "title": "Fast , Minimum Storage Ray / Triangle Intersection", "journal": "Journal of Graphics Tools", "year": "1997", "authors": "T M\u00f6ller; B Trumbore"}, {"ref_id": "b18", "title": "Event-based , 6-DOF Pose Tracking for High-Speed Maneuvers", "journal": "", "year": "2014", "authors": "E Mueggler; B Huber; D Scaramuzza"}, {"ref_id": "b19", "title": "DTAM: Dense Tracking and Mapping in Real-Time", "journal": "", "year": "2011", "authors": "R A Newcombe; S Lovegrove; A J Davison"}, {"ref_id": "b20", "title": "A QVGA 143 dB Dynamic Range Frame-Free PWM Image Sensor With Lossless Pixel-Level Video Compression and Time-Domain CDS", "journal": "IEEE Journal of Solid-State Circuits", "year": "2011", "authors": "C Posch; D Matolin; R Wohlgenannt"}, {"ref_id": "b21", "title": "Event-Driven Stereo Matching for Real-Time 3D Panoramic Vision", "journal": "", "year": "2015", "authors": "S Schraml; A N Belbachir; H Bischof"}, {"ref_id": "b22", "title": "Event-based 3D SLAM with a depth-augmented dynamic vision sensor", "journal": "", "year": "2014", "authors": "D Weikersdorfer; D B Adrian; D Cremers; J Conradt"}, {"ref_id": "b23", "title": "Simultaneous Localization and Mapping for event-based Vision Systems", "journal": "", "year": "2013", "authors": "D Weikersdorfer; R Hoffmann; J Conradt"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig. 1 .1Fig. 1. Event-based camera: (a): in contrast to standard video frames shown in the upper graph, a stream of events from an event camera, plotted in the lower graph, offers no redundant data output, only informative pixels or no events at all. Red and blue dots represent positive and negative events respectively (this figure was recreated inspired by the associated animation of [19]: https://youtu.be/LauQ6LWTkxM?t=35s). (b): image-like visualisation by accumulating events within a time interval -white and black pixels represent positive and negative events respectively. (c): the first commercial event camera, DVS128, from iniLabs Ltd.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Fig. 2 .2Fig. 2. Camera pose and inverse depth estimation. (a): based on the assumption that the current log intensity estimate (shown as the colour of the solid line) and inverse depth estimate (shown as the geometry of the solid line) are correct, we find current camera pose T (t) wc most consistent with the predicted log intensity change since the previous event at the same pixel at pose T (t\u2212\u03c4c) wc compared to the current event polarity. (b): similarly for inverse depth estimation, we assume that the current reconstructed log intensity and camera pose estimate are correct, and find the most probable inverse depth consistent with the new event measurement.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Fig. 3 .3Fig. 3. Basic geometry for; tracking and inverse depth estimation: we find two corresponding ray-triangle intersection points, p (t) w and p (t\u2212\u03c4c) w", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Fig. 4 .4Fig.4. Typical temporal progression (left to right) of gradient estimation and log intensity reconstruction as a hand-held camera browses a 3D scene. The colours and intensities on the top row represent the orientations and strengths of the gradients of the scene (refer to the colour chart in the top right). In the bottom row, we see these gradient estimates upgraded to reconstructed intensity images.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Fig. 5 .5Fig. 5. Typical temporal progression (left to right) of inverse depth estimation and regularisation as a hand-held camera browses a 3D scene. The colours on the top row represent the different depths of the scene (refer to the colour chart in the top right and the associated semi-dense 3D point cloud on the bottom row).", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Fig. 6 .6Fig. 6. Demonstrations in various settings of the different aspects of our joint estimation algorithm. (a) visualisation of the input event stream; (b) estimated gradient keyframes; (c) reconstructed intensity keyframes with super resolution and high dynamic range properties; (d) estimated depth maps; (e) semi-dense 3D point clouds.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Fig. 7 .7Fig. 7. 3D point cloud of an indoor scene constructed from multiple keyframes, showing keyframe poses with their intensity and depth map estimates.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "T wc = exp 6 i=1 x i G i = R wc t w 0 1 ,(1)", "formula_coordinates": [5.0, 127.6, 370.88, 252.25, 30.32]}, {"formula_id": "formula_1", "formula_text": "x (t|t\u2212\u03c4 ) = x (t\u2212\u03c4 |t\u2212\u03c4 ) + n ,(2)", "formula_coordinates": [5.0, 150.76, 521.22, 229.09, 10.81]}, {"formula_id": "formula_2", "formula_text": "P x (t|t\u2212\u03c4 ) = P (t\u2212\u03c4 |t\u2212\u03c4 ) x + P n ,(3)", "formula_coordinates": [5.0, 146.76, 542.64, 233.09, 12.69]}, {"formula_id": "formula_3", "formula_text": "z x = \u00b1C ,(4)", "formula_coordinates": [6.0, 83.0, 359.18, 296.84, 9.65]}, {"formula_id": "formula_4", "formula_text": "h x (x (t|t\u2212\u03c4 ) ) = I l p (t) w \u2212 I l p (t\u2212\u03c4c) w ,(5)", "formula_coordinates": [6.0, 83.0, 376.13, 296.84, 12.69]}, {"formula_id": "formula_5", "formula_text": "I l (p w ) = (1 \u2212 a \u2212 b)I l (v 0 ) + aI l (v 1 ) + bI l (v 2 ) . (6", "formula_coordinates": [6.0, 114.16, 397.03, 261.44, 9.65]}, {"formula_id": "formula_6", "formula_text": ")", "formula_coordinates": [6.0, 375.6, 397.03, 4.24, 8.74]}, {"formula_id": "formula_7", "formula_text": "x (t|t) = x (t|t\u2212\u03c4 ) + W x \u03bd x ,(7)", "formula_coordinates": [6.0, 151.0, 537.39, 228.84, 11.72]}, {"formula_id": "formula_8", "formula_text": "P (t|t) x = I 6\u00d76 \u2212 W x \u2202h x \u2202x (t|t\u2212\u03c4 ) P (t|t\u2212\u03c4 ) x ,(8)", "formula_coordinates": [6.0, 118.75, 563.07, 261.09, 22.49]}, {"formula_id": "formula_9", "formula_text": "z g = \u00b1 C \u03c4 c ,(9)", "formula_coordinates": [8.0, 146.69, 186.16, 233.15, 23.22]}, {"formula_id": "formula_10", "formula_text": "h g = (g(p k ) \u2022 m) ,(10)", "formula_coordinates": [8.0, 146.69, 213.55, 233.15, 9.68]}, {"formula_id": "formula_11", "formula_text": "m = p (t) k \u2212 p (t\u2212\u03c4c) k \u03c4 c . (11", "formula_coordinates": [8.0, 177.85, 228.42, 197.57, 26.51]}, {"formula_id": "formula_12", "formula_text": ")", "formula_coordinates": [8.0, 375.41, 238.45, 4.43, 8.74]}, {"formula_id": "formula_13", "formula_text": "N g = \u2202z g \u2202C P C \u2202z g \u2202C = \u03c3 2 C \u03c4 2 c ,(12)", "formula_coordinates": [8.0, 143.36, 333.54, 236.48, 25.77]}, {"formula_id": "formula_14", "formula_text": "min I l \u2126 ||g(p k ) \u2212 \u2207I l (p k )|| h d + \u03bb||\u2207I l (p k )|| h r dp k . (13", "formula_coordinates": [8.0, 88.84, 444.77, 286.58, 19.31]}, {"formula_id": "formula_15", "formula_text": ")", "formula_coordinates": [8.0, 375.41, 446.84, 4.43, 8.74]}, {"formula_id": "formula_16", "formula_text": "min I l max q max p { p, g \u2212 \u2207I l \u2212 d 2 ||p|| 2 \u2212 \u03b4 p (p)+ q, \u2207I l \u2212 r 2\u03bb ||q|| 2 \u2212 \u03b4 q (q)} ,(14)", "formula_coordinates": [8.0, 45.64, 540.92, 334.2, 42.12]}, {"formula_id": "formula_17", "formula_text": "p (n+1) = p (n) +\u03c3p(g\u2212\u2207I l ) 1+\u03c3p d max 1, p (n) +\u03c3p(g\u2212\u2207I l ) 1+\u03c3p d ,(15)", "formula_coordinates": [9.0, 126.2, 287.26, 253.64, 36.55]}, {"formula_id": "formula_18", "formula_text": "q (n+1) = q (n) +\u03c3q\u2207I l 1+ \u03c3q r \u03bb max 1, 1 \u03bb q (n) +\u03c3q\u2207I l 1+ \u03c3q r \u03bb ,(16)", "formula_coordinates": [9.0, 130.74, 351.65, 249.1, 40.4]}, {"formula_id": "formula_19", "formula_text": "I (n+1) l = I (n) l \u2212 \u03c3 I l (divp (n+1) \u2212 divq (n+1) ) .(17)", "formula_coordinates": [9.0, 111.23, 417.35, 268.61, 14.3]}, {"formula_id": "formula_20", "formula_text": "z \u03c1 = \u00b1C ,(18)", "formula_coordinates": [10.0, 140.07, 331.54, 239.77, 9.65]}, {"formula_id": "formula_21", "formula_text": "w \u2212 I l p (t\u2212\u03c4c) w .(19)", "formula_coordinates": [10.0, 186.6, 348.5, 193.25, 12.69]}], "doi": ""}