{"title": "MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers", "authors": "Krishna Pillutla; Swabha Swayamdipta; Rowan Zellers; John Thickstun; Sean Welleck; Yejin Choi; Zaid Harchaoui", "pub_date": "", "abstract": "As major progress is made in open-ended text generation, measuring how close machine-generated text is to human language remains a critical open problem. We introduce MAUVE, a comparison measure for open-ended text generation, which directly compares the learnt distribution from a text generation model to the distribution of human-written text using divergence frontiers. MAUVE scales up to modern text generation models by computing information divergences in a quantized embedding space. Through an extensive empirical study on three open-ended generation tasks, we find that MAUVE identifies known properties of generated text, scales naturally with model size, and correlates with human judgments, with fewer restrictions than existing distributional evaluation metrics.", "sections": [{"heading": "Introduction", "text": "Recent large-scale text generation models show an ability to produce human-like text of remarkable quality and coherence in open-ended generation [45,61,6]. In this setting, a text generation model forms a distribution over natural language sequences, induced by an autoregressive neural sequence model (e.g., GPT-3 [6]) paired with a decoding algorithm (e.g., nucleus sampling [26]). Generating text amounts to sampling from this distribution, with the goal of obtaining samples that resemble those from the \"true\" distribution of human-written text.\nTo evaluate how close a generation model's distribution is to that of human-written text, we must consider two types of errors: (I) where the model assigns high probability to sequences which do not resemble human-written text, and, (II) where the model distribution does not cover the human distribution, i.e., it fails to yield diverse samples. However, quantifying these aspects in a principled yet computationally tractable manner is challenging, as the text distributions are high-dimensional and discrete, accessed only through samples or expensive model evaluations [26,58,62].\nWe develop MAUVE, a comparison measure for open-ended text generation. The proposed measure is efficient, interpretable, and practical for evaluating modern text generation models. It captures both types of errors (Figure 1) by building upon information divergence frontiers [49,31,16], so far underexplored in natural language processing. The key idea for making the proposed measure computationally tractable, yet effective, is to reduce its measurement to computing Kullback-Leibler divergences in a quantized, low-dimensional space after embedding samples from each distribution with an external language model. From an end-user's perspective, MAUVE has a simple interface: given neural text and human text, it yields a scalar measure of the gap between them.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021). Figure 1: Left: MAUVE compares the machine text distribution Q to that of human text P by using the family of mixtures R \u03bb = \u03bbP +(1\u2212\u03bb)Q for \u03bb \u2208 (0, 1). Right: Illustration of Type I errors, where Q produces degenerate, repetitive text which is unlikely under P , and, Type II errors, where Q cannot produce plausible human text due to truncation heuristics [26]. MAUVE measures these errors softly, by using the mixture distribution R \u03bb . Varying \u03bb in (0, 1) gives a divergence curve and captures a spectrum of soft Type I and Type II errors. MAUVE summarizes the entire divergence curve in a single scalar as the area under this curve.\nWe summarize our contributions. First, we introduce MAUVE, a comparison measure between neural text and human text. Second, we empirically show that MAUVE is able to quantify known properties of generated text with respect to text length, model size, and decoding more correctly and with fewer restrictions than existing distributional evaluation metrics. Third, we find through a human evaluation that MAUVE better correlates with human quality judgements of text. Finally, we find that MAUVE can be highly robust to the choice of quantization, embeddings, and scaling. We open-source a pip-installable Python package to compute MAUVE. 1 ", "publication_ref": ["b44", "b60", "b5", "b5", "b25", "b25", "b57", "b61", "b48", "b30", "b15", "b25", "b0"], "figure_ref": ["fig_7", "fig_7"], "table_ref": []}, {"heading": "MAUVE", "text": "We begin by discussing the basics of open-ended text generation, and then introduce MAUVE for measuring the divergence between machine generated text and human text.\nOpen-ended Text Generation. A language model is an estimateP (x) of the probability distribution over sequences of text x = (x 1 , . . . , x |x| ), consisting of tokens x t belonging to a fixed vocabulary (e.g. characters, or words). Prevailing neural autoregressive language models estimate the joint distributionP (x) by modeling the conditional distributionP (x t+1 |x 1:t ) over the next token in a sequence. The open-ended text generation task asks us to output textx t+1:|x| in continuation of a given context x 1:t . Unlike targeted generation tasks like translation or summarization, there is no \"correct\" output; the main criteria for open-ended text generation are coherence, creativity, and fluency.\nGiven a neural autoregressive language modelP , we can generate open-ended text in a serial, left-toright fashion, by samplingx t+1 \u223cP (\u2022|x 1:t ),x t+2 \u223cP (\u2022|x 1:t ,x t+1 ), etc. In practice, this simple decoding algorithm is often modified by adjusting the conditional distributionP (\u2022|x 1:t ) to promote more conservative outputs. The decoding algorithm and the language model taken together define a distribution Q over text, which we call the model distribution. Common decoding algorithms include temperature rescaling [1] and truncation [18,26]. Note that truncation methods in particular create sparsity in Q, which leads to degeneracy of some measures including test-set perplexity.\nSources of Error in Text Generation. Our goal in this work is to measure the gap between the model distribution Q and the target distribution P of human text. As highlighted in Figure 1, this gap arises from two sources of error: (Type I) Q places high mass on text which is unlikely under P , (Type II) Q cannot generate text which is plausible under P .\nThe Type I errors are false positives, including the common failure case where a model generates text with semantic repetitions [15,26,59] that are highly unlikely to be written by humans. 2 The Type II 1 Available from https://github.com/krishnap25/mauve. See Appendix B for an example of the mauve package in action. 2 Let text x with P (x) 0 be the positive class and P (x) \u2248 0 be the negative class. If Q(x) 0 for some negative x, then the model incorrectly considers it a positive, so it is a false positive.  [45], Grover [61]) and decoding algorithms (greedy decoding, ancestral and nucleus sampling). MAUVE is computed as the area of the shaded region, and larger values of MAUVE indicate that Q is closer to P . In general, MAUVE indicates that generations from larger models and nucleus sampling are closer to human text. Rightmost: Nucleus sampling has a slightly smaller Type I error than ancestral sampling but a higher Type II error, indicating that ancestral sampling with Grover base produces more degenerate text while nucleus sampling does not effectively cover the human text distribution.\nerrors are false negatives, which can occur, for instance, because some pieces of plausible human text cannot be generated by truncation-based decoding algorithms such as nucleus sampling [26]. The gap between P and Q is small only if both of these errors are small.\nQuantifying the Errors. We formalize the Type I and II errors with the Kullback-Leibler (KL) divergences KL(Q|P ) and KL(P |Q), respectively. The divergence KL(Q|P ) penalizes Q if there exists text x such that Q(x) is large but P (x) is small, so it quantifies the Type I error. Likewise, KL(P |Q) quantifies the Type II error.\nUnfortunately, one or both of the KL divergences KL(P |Q) and KL(Q|P ) are infinite if the supports of P and Q are not identical, which is often the case in open-ended generation. This makes the KL divergence itself unsuitable as an evaluation metric. We overcome this issue by softly measuring the two errors using the mixture distribution R \u03bb = \u03bbP + (1 \u2212 \u03bb)Q for some \u03bb \u2208 (0, 1). In particular, we define the (soft) Type I error at level \u03bb as KL(Q|R \u03bb ) and the (soft) Type II error as KL(P |R \u03bb ).\nSummarizing the Errors with a Divergence Curve. Since the mixture weight \u03bb was arbitrary, we consider a family of Type I and II error values by varying \u03bb between 0 and 1, in the same spirit as information divergence frontiers [49,16]. This yields a divergence curve, C(P, Q) = exp(\u2212c KL(Q|R \u03bb )), exp(\u2212c KL(P |R \u03bb )) : R \u03bb = \u03bbP + (1 \u2212 \u03bb)Q, \u03bb \u2208 (0, 1) ,\nwhere c > 0 is a hyperparameter for scaling. The divergence curve formalizes and encodes information about the trade-off between Type I and II errors. 3 Figure 2 illustrates the divergence curves for different models and decoding algorithms.\nOur proposed measure, MAUVE(P, Q), is the area under the divergence curve C(P, Q). It provides a scalar summary of the trade-off between Type I and Type II errors. MAUVE(P, Q) lies in (0, 1], with a larger value meaning that Q is closer to P . Further, MAUVE(P, Q) = 1 if and only if Q = P . The area under the curve is a common summary of trade-off curves in machine learning [13,11,12,19].\nConnections to Common Divergences. The divergence curve encodes more information than the KL divergence KL(P |Q), which can be obtained from the second coordinate of the curve C(P, Q) as \u03bb \u2192 0, and the reverse KL divergence KL(Q|P ) which can be obtained from the first coordinate of the curve C(P, Q) as \u03bb \u2192 1. Further, the Jensen-Shannon (JS) divergence JS(P, Q) = KL(P |R 1/2 ) + KL(Q|R 1/2 ) /2, can be obtained from the two coordinates of C(P, Q) at \u03bb = 1/2. MAUVE summarizes all of the divergence curve C(P, Q).\nComputing MAUVE for Open-Ended Text Generation. Each point on the divergence curve C(P, Q) consists of a coordinate\nKL(P |R \u03bb ) = x P (x) log P (x) R \u03bb (x) ,(2)\nand a similarly defined coordinate KL(Q|R \u03bb ). We cannot compute the summation as written in Eq. (2), as we do not know the ground-truth probabilities P (\u2022) and the support of a typical model distribution is prohibitively large, since it is the space of all sequences of tokens. As a result of these two issues, MAUVE cannot be tractably computed in closed form.\nWe employ a Monte Carlo estimator using samples x i \u223c P and x i \u223c Q to overcome the fact that ground-truth probabilities P (\u2022) are unknown. We circumvent the intractable support size by computing MAUVE in a quantized embedding space that is sensitive to important features of text.\nThe overall estimation procedure is as follows. First, we sample human text x i \u223c P and machine text x i \u223c Q. We then embed each text sequence using an external language model M (e.g., GPT-2 [45]) to obtain embeddings {M (\nx i )} N i=1 and {M (x i )} N i=1 .\nEach embedding is now a vector M (x) \u2208 R d . Next, we jointly quantize the embedded samples (e.g. with k-means [36]), and count the cluster assignments to form histograms, giving low-dimensional discrete distributions that approximate each high-dimensional text distribution. In particular, the distribution P of human text is approximated by the discrete distributionP of support size k, which is defined as,\nP (j) = 1 N N i=1 I \u03c6(x i ) = j ,(3)\nwhere \u03c6(x) \u2208 {1, \u2022 \u2022 \u2022 , k} returns the cluster id of x. The model distribution Q is approximated asQ similarly. Here,P andQ can be interpreted as piecewise constant approximations of P and Q, similar to a histogram; see Figure 3 for an illustration. Computing the divergence curve is now tractable, as each coordinate is a KL divergence between the k-element discrete distributions.\nTo recap, our proposed measure MAUVE(P, Q) is the area under this divergence curve, providing a summary of all Type I and Type II errors through an efficient approximation designed for text generation. Next, we discuss how MAUVE compares to prior comparison measures for text ( \u00a73), then present empirical results with MAUVE ( \u00a74).", "publication_ref": ["b0", "b17", "b25", "b14", "b25", "b58", "b1", "b0", "b1", "b44", "b60", "b25", "b48", "b15", "b2", "b12", "b10", "b11", "b18", "b44", "b35"], "figure_ref": ["fig_7", "fig_1", "fig_2"], "table_ref": []}, {"heading": "Related Work", "text": "Divergence Measures for Text. Prior measures of similarity/divergence between machine text and human text come in three broad categories: (a) reference-based, (b) statistics-based, and (c) language modeling. Table 1 summarizes the latter two categories, and contrasts them with MAUVE.\nReference-based measures evaluate generated text with respect to a (small set of) reference text sample(s), rather than comparing full sequence distributions. These include classical metrics for n-gram matching [44,32,2], which are designed to capture similarities in the surface form of the generated text and the human references, making them fundamentally ill-suited for open-ended generation. Moreover, it has been recently shown in [42] show that these classical metrics only weakly agree with human judgments.\nMore recent reference-based metrics are capable of comparisons in a high dimensional space [53,63,51,9], thereby capturing distributional semantics beyond superficial n-gram statistics. For instance, Moverscore [64] relies on the Word Mover's distance [30], and is an instance of an optimal transportation distance [57]. It computes the minimum cost of transforming the generated text to the reference text, taking into account Euclidean distance between vector representations of n-grams,", "publication_ref": ["b43", "b31", "b1", "b41", "b52", "b62", "b50", "b8", "b63", "b29", "b56"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Type", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Metric Measures Approximates", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Statistics", "text": "Zipf Coefficient [26] Unigram rank-frequency statistics -Self-BLEU [65] N-gram diversity -Generation Perplexity [18] Generation quality via external model R\n|EQ[log R(x)] \u2212 EP [log R(x)]| (a single point inside C(P, Q))", "publication_ref": ["b25", "b64", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Language Modeling", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Perplexity", "text": "Test-set perplexity EP [log Q(x)] \u03b5-perplexity [39] Perplexity w/ Laplace smoothing EP [Q(x)] Sparsemax Score [39] LM quality (sparsemax loss [38]) EP [Q(x)] Token JS-Div. [39] LM quality (JS divergence)\nEP [Q(x)]\nDivergence Curve", "publication_ref": ["b38", "b38", "b37", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "MAUVE (this work)", "text": "Quality & diversity via the divergence curve C(P, Q) at all \u03bb provides a summary of all points along the divergence curve, rather than a single point. The summary is based on comparisons in a joint embedding space, rather than a statistic computed independently on each distribution. Q informally refers to a quantity related to Q.\nas well as their document frequencies. The paradigm of reference-based measures is useful for targeted generation tasks such as translation and summarization where matching a set of references is paramount. It is, however, unsuitable for open-ended generation where there typically are several plausible continuations for each context and creative generations are desirable.\nStatistics-based measures compare the model distribution Q with respect to the human distribution P on the basis of some statistic T (P ) and T (Q). Property-specific statistics such as the amount of repetition [26,59], verifiability [40], or termination [58] are orthogonal to MAUVE, which provides a summary of the overall gap between P and Q rather than focusing on an individual property.\nAnother statistic is the generation perplexity [18,26], which compares the perplexity of the model text x \u223c Q with that of human text x \u223c P under an external model R. By virtue of T (\u2022) being a scalar, generation perplexity cannot trade-off the Type I and Type II errors like MAUVE. In fact, we show in Appendix A that the generation perplexity can be derived from a single point enclosed between the divergence curve and the axes.\nLanguage modeling metrics calculate how (un)likely human text x \u223c P is under the model distribution Q, for instance, using the probability Q(x). These metrics are related to a single point on the divergence curve, rather than a full summary. Examples include the perplexity of the test set (which is a sample from P ) under the model Q and its generalizations to handle sparse distributions [39]. Unlike MAUVE, these metrics never see model text samples x \u223c Q, so they cannot account for how likely the model text is under the human distribution P . Moreover, they cannot be used for decoding algorithms such as beam search which do not define a token-level distribution.\nAutomatic metrics have been proposed for specific domains such as generation of dialogues [55], stories [21], and others [43]. They capture task-specific properties; see the surveys [8,48]. In contrast, MAUVE compares machine and human text in a domain-agnostic manner. Other related work has proposed metrics that rely on multiple samples for quality-diversity evaluation [7], and Bayesian approaches to compare the distribution of statistics in machine translation [17].\nNon-automatic Metrics. HUSE [24] aims to combine human judgements of Type I errors with Type II errors measured using perplexity under Q. Due to the costs of human evaluation, we consider HUSE, as well other metrics requiring human evaluation, such as single-pair evaluation, as complementary to MAUVE, which is an automatic comparison measure. As a separate technical caveat, it is unclear how to use HUSE for sparse Q that assigns zero probability to a subset of text, which is the case with state-of-the-art decoding algorithms [26,39].\nEvaluation of Generative Models. Evaluation of generative models is an active area of research in computer vision, where generative adversarial networks [20] are commonly used. However, metrics such as Inception Score [50] are based on large-scale supervised classification tasks, and thus inappropriate for text generation. The Fr\u00e9chet Distance [25,52] and its unbiased counterpart, the Kernel Inception Distance [5] are both used for evaluating generative models, but unlike MAUVE, do not take into account a trade-off between different kinds of errors between the learned and a  reference distribution. Sajjadi et al. [49] and Kynk\u00e4\u00e4nniemi et al. [31] both proposed metrics based on precision-recall curves. Djolonga et al. [16] proposed information divergence frontiers as a unified framework emcompassing both these works as special cases. MAUVE extends the above line of work, and is operationalized for open-ended text generation, applicable for data generated by large-scale neural language models. Complementary to this work, Liu et al. [33] study the theory of information divergence frontiers, proving non-asymptotic bounds on the estimation and quantization error.", "publication_ref": ["b25", "b58", "b39", "b57", "b17", "b25", "b38", "b54", "b20", "b42", "b7", "b47", "b6", "b16", "b23", "b25", "b38", "b19", "b49", "b24", "b51", "b4", "b48", "b30", "b15", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We Tasks. We consider open-ended text generation using a text completion task [26,59] in three domains: web text, news and stories. Each domain consists of a sequence dataset split into (context, continuation) pairs. Given a context x 1:k , the task is to generate a continuationx k+1:T \u223c Q(\u2022 | x 1:k ), forming a completion. Each ground-truth completion x 1:T is considered a sample from the true distribution P , while the completion (x 1:k ,x k+1:T ) is considered a sample from Q. The datasets, context and completion lengths, and number of completions used for each domain are shown in Table 2.\nModels. As the language modelP (\u2022), we use GPT-2, a large-scale transformer [56] pretrained on the web text dataset (see [45]), that is representative of state-of-the-art autoregressive language models. As the embedding model M (\u2022) we use GPT-2 Large, and compare others in \u00a74.2.\nDecoding Algorithms. We consider three common decoding algorithms: ancestral sampling which samples directly from the language model's per-step distributions, x t \u223cP (x t | x 1:t ), greedy decoding which selects the most likely next token, x t = arg max x\u2208VP (x | x 1:t ), as well as nucleus sampling [26] which samples from top-p truncated per-step distributions, x t \u223cP nuc,p (x t | x 1:t ), which is defined asP nuc,p (x t | x 1:t ) \u221d\nP nuc,p (x t | x 1:t ), if x t \u2208 V p , 0, else.\nHere, the top-p vocabulary V p is the smallest set V such that x\u2208VP (x | x 1:t ) \u2265 p.\nWe also consider an adversarial sampling procedure, designed to generate low-quality text that nevertheless matches the perplexity of human text. Adversarial perplexity sampling proceeds in two phases: (1) we generate the first 15% of tokens in a sequence uniformly at random from the vocabulary, and (2) we generate the remaining tokens greedily to make the running perplexity of the generated sequence as close as possible to the perplexity of human text.", "publication_ref": ["b25", "b58", "b55", "b44", "b25"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Quantifying Properties of Generated Text", "text": "To study MAUVE's effectiveness as a measure for comparing text distributions, we first examine how MAUVE quantifies known properties of generated text: a good measure should meet expected behavior that is known from existing research on each property. Specifically, we investigate how MAUVE behaves under changes in generation length, decoding algorithm, and model size. MAUVE quantifies quality differences due to generation length. Although large transformerbased models can generate remarkably fluent text, it has been observed that the quality of generation deteriorates with text length: as the generation gets longer, the model starts to wander, switching to unrelated topics and becoming incoherent [46]. As a result, an effective measure should indicate lower quality (e.g. lower MAUVE) as generation length increases.\nFigure 4 shows MAUVE as the generation length increases, along with three alternative metrics: generation perplexity, sparsemax score, and Fr\u00e9chet distance [25,52]. MAUVE reflects the desired behavior, showing a decrease in quality (lower MAUVE) as generation length grows, with the trend consistent across model sizes. The other three metrics, however, show less favorable trends. Fr\u00e9chet distance indicates improving quality as the length increases, while generation perplexity shows non-monotonic quality trends for the small and large models. Finally, language modeling metrics such as the sparsemax score [39] remain constant, since they do not depend on the samples generated.\nMAUVE identifies quality differences between decoding algorithms. Recent work has identified two clear trends in open-ended text generation with standard autoregressive models: (1) using greedy decoding results in repetitive, degenerate text [26,59,58]; (2) nucleus sampling (and related truncated sampling methods) yields higher quality text than ancestral sampling [18,26]. 5 An effective measure should thus indicate the quality relationship greedy \u227a ancestral \u227a nucleus. MAUVE quantifies quality differences due to model size. Scaling the model size has been a key driver of recent advances in NLP, with larger models leading to better language modeling and higher quality generations in open-ended settings [45,6]. An effective metric should capture the relationship between model size and generation quality, which we verify with human quality scores.\nTable 4 shows MAUVE's quality measures as the model size increases, along with alternatives and human quality scores. MAUVE increases as model size increases, agreeing with the human quality measure and the expectation that larger models should have higher quality generations. The widelyused generation perplexity, however, incorrectly rates the large model's text as the best. Although the language modeling metrics (SP, JS, and \u03b5-PPL) capture the size-quality relationship, they are constant with respect to length (Figure 4), and did not correctly quantify decoding algorithm quality (Table 3).   Table 6 in Appendix D shows additional results with ancestral sampling. In this case, human evaluators rated generations from the small model as better than those from the medium model. Interestingly, MAUVE also identified this relationship, agreeing with the human ratings, in contrast to the other automatic metrics we surveyed.\nSummary. MAUVE identifies properties of generated text that a good measure should capture, related to length, decoding algorithm, and model size. In contrast, commonly used language modeling and statistical measures did not capture all of these properties. Unlike these alternatives, which capture a single statistic or relate to a single point on the divergence curve, MAUVE's summary measure incorporates type I errors that quantify the degenerate text produced by greedy decoding (recall Figure 1), while capturing distribution-level information that describes quality changes from generation length, model size, and the nuanced distinction between ancestral and nucleus sampling.", "publication_ref": ["b45", "b24", "b51", "b38", "b25", "b58", "b57", "b17", "b25", "b4", "b44", "b5"], "figure_ref": ["fig_7"], "table_ref": ["tab_8", "tab_5", "tab_15"]}, {"heading": "Approximations in MAUVE", "text": "MAUVE summarizes the divergence between two text distributions with an approximation that relies on two components: an embedding model M (x) and a quantization algorithm A ( \u00a72, Eq. ( 3)). We study the effects of these two components.\nMAUVE works with alternative embedding models. Figure 5 (left) shows that MAUVE with features from RoBERTa-large [34] gives qualitatively similar trends across model size and decoding as MAUVE with features from GPT-2 large. Quantitatively, the Spearman rank correlation between them across all model and decoders is 0.993. We observe that RoBERTa penalizes smaller models more than GPT-2 but rates greedy decoding higher. We leave further study of inductive biases in the different embedding models to future work.\nMAUVE is robust to quantization. We compare different three different quantization algorithms:\n(a) k-Means: We cluster the hidden representations using k-means, and represent them by their cluster membership to get a discrete distribution with size equal to the number of clusters. (b) Deep Residual Mixture Models (DRMM): As a generalization of k-means, we train a deep generative model known as DRMM [22]. We convert the soft clustering returned by DRMM into a hard clustering by assigning each point to its most likely cluster, and quantize the data using the cluster membership. We use DRMM with 3 layers and 10 components per layer for a total of 10 3 clusters, and train it for 20 epochs. (c) Lattice Quantization: We learn a 4-dimensional feature representation of the vectors M (x) using a deep network which maintains the neighborhood structure of the data while encouraging the features to be uniformly distributed on the unit sphere [47]. We quantize the data on a uniform lattice into 744 bins. We compare different choices of the quantization to k-means with k = 500, which is our default. The Spearman rank correlation between MAUVE computed with k-means for k ranging from 100 to 5000 correlates nearly perfectly with that of k = 500. In particular, the Spearman correlation is exactly 0.99 or 1.00. Likewise, MAUVE computed with DRMM or lattice quantization has a near-perfect Spearman correlation of at least 0.99 with k-means. While the actual numerical value of MAUVE could vary with the quantization algorithm, these results show that the rankings induced by various variants of MAUVE are nearly identical.\nPractical recommendation for scaling parameter. Figure 5 (right) shows the effects of adjusting the scaling parameter c, which does not affect the relative order of the divergence curve, but adjusts the numerical value returned by MAUVE. As a practical recommendation, we found c = 5 to yield interpretable values.", "publication_ref": ["b33", "b21", "b46"], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Correlation with Human Judgments", "text": "An effective metric should yield judgments that correlate highly with human judgments, assuming that human evaluators represent a gold-standard. 7 We evaluate how MAUVE's quality judgments correlate with human quality judgments. In our study, a quality judgment means choosing a particular (model, decoder) setting based on the resultant generations.\nEvaluation Protocol. To obtain human judgments, we employ a pairwise setup: at each round, an annotator receives a context and continuations from two different (model, decoder) settings, and selects the continuation they found more natural using a 5-point Likert scale. Our interface for collecting annotations is shown in Figure 9 of Appendix E, which also includes further details and additional results.\nWe collect these annotations for web text generation with 8 different (model, decoder) settings plus a ninth setting for human-written continuations. Each setting is a GPT-2 model size paired with either ancestral or nucleus sampling. This gives us a total of 36 pairs of settings. Given the known difficulties with human evaluation of longer texts [28], we use a maximum completion length of 256 tokens. We obtain 90 preference ratings for each pair of settings, coming from a total of 214 crowd-workers from the Amazon Mechanical Turk platform. The evaluators were paid USD 0.40 per evaluation based on an estimated wage of USD 16 per hour.\nWe convert these pairwise preferences to a ranking by fitting a Bradley-Terry model [37], a parametric model used to predict the outcome of a head-to-head comparison. In particular, we obtain a score w i for each setting i so that the log odds of humans preferring setting i to setting j in a head-to-head comparison is given by the difference w i \u2212 w j . For a given comparison measure, we compute the Spearman rank correlation between the comparison measure and the fitted Bradley-Terry coefficients w i for each of the (model, decoder) settings. The end result is a correlation score in [\u22121, 1], with higher values meaning that quality judgments using the comparison measure correlate with quality judgments made by human evaluators.  MAUVE correlates with human judgments. Table 5 shows the correlation between human judgments and five automatic evaluation metrics obtained using our evaluation protocol on the web text domain. MAUVE correlates highly with human judgments of how human-like (0.952), interesting (0.810), and sensible (0.857) the machine text is. MAUVE's correlations with human judgments are substantially higher than those for the other automated measures; for instance, the commonly-used generation perplexity has correlations that are 0.12 to 0.17 lower than MAUVE's. The results suggest that MAUVE may act as an effective, automatic surrogate for costly human judgments.\nMAUVE correlates with learned discriminators. We also measure the quality of generations by how well a trained model (a discriminator) can distinguish between real and generated text [35]. We report the test accuracy of a binary classifier trained to discriminate between machine and human text; a lower discrimination accuracy implies that the generation is harder to distinguish from human text. We report the accuracy of Grover mega as the discriminator for the news generations as it produced the highest discrimination accuracy [61] while we use GPT-2 large for the story domain. As seen in Table 5, MAUVE correlates the highest with the discrimination accuracy (0.96 for news and 0.89 for stories) among all comparison measures. Computing the discrimination accuracy for each (model, decoder) pair requires fine-tuning a separate model, which is particularly expensive for large models such as Grover-mega. MAUVE, on the other hand, does not require any training.", "publication_ref": ["b6", "b27", "b36", "b34", "b60"], "figure_ref": ["fig_14"], "table_ref": ["tab_10", "tab_10"]}, {"heading": "Conclusion", "text": "We presented MAUVE, an automatic measure of the gap between neural text and human text for open-ended text generation. MAUVE measures the area under a divergence curve, formalizing and summarizing a spectrum of errors that capture phenomena present in machine and human-generated text. MAUVE correlated with human judgments and identified quality differences due to generation length, decoding algorithm, and model size, which prior metrics struggle to capture. Automated metrics have driven advances in computer vision and many other machine learning domains. MAUVE's principled foundation and strong empirical performance offers a similar path forward for open-ended text generation systems. Extensions of MAUVE to closed-ended tasks, such as summarization and translation, where generations must be compared to a fixed set of gold-standard references, are promising directions for future work.\nBroader Impacts Statement MAUVE rewards model text which resembles human-authored text. However, we acknowledge the risks of rewarding systems that try to mimic humans [4], which is the ultimate goal of open-ended text generation. While our research is important for developing better language generators, we also encourage the community to pay attention to the development of technology that can reliably distinguish between human and machine text. We leave the extension of our method towards building such systems to future work. We discuss some aspects of the divergence curves alluded to in \u00a72 and \u00a73. In particular, we discuss the following.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "Appendix", "text": "\u2022 Appendix A.1: the Pareto optimality of the divergence curves, mentioned in a footnote in \u00a72. \u2022 Appendix A.2: the connection between generation perplexity and the divergence curves as mentioned in \u00a73. \u2022 Appendix A.3: a formal definition of the quantization which is first introduced in \u00a72, as well as an illustration. \u2022 Appendix A.4: the pesudocode for MAUVE.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Pareto Optimality of Divergence Curves", "text": "Here, we show the property of Pareto optimality of C(P, Q). We refer to the textbook [23] for more background on information theory and KL divergence. The main property we will show in this section is the following. Proposition 1. Consider two distributions P, Q with finite support and a scaling constant c > 0. Let R \u03bb be such that e \u2212c KL(Q|R \u03bb ) , e \u2212c KL(P |R \u03bb ) \u2208 C(P, Q). Then, R \u03bb is Pareto-optimal for the pair of objectives KL(Q|\u2022), KL(P |\u2022) . In other words, there does not exist any distribution R such that KL(Q|R) < KL(Q|R \u03bb ) and KL(P |R) < KL(P |R \u03bb ) simultaneously. We invoke the next lemma to show that R \u03bb = \u03bbP + (1 \u2212 \u03bb)Q to complete the proof. Proof. By adding and subtracting i R \u03bb,i log(R \u03bb,i ), we get,\n\u03bb KL(P |S) +\u03bb KL(Q|S) = i \u03bbP i log P i +\u03bbQ i log Q i \u2212 R \u03bb,i log S i = i \u03bbP i log P i R \u03bb,i +\u03bbQ i log Q i R \u03bb,i + R \u03bb,i log R \u03bb,i S i = \u03bb KL(P |R \u03bb ) +\u03bb KL(Q|R \u03bb ) + KL(R \u03bb |S) .\nThe first two terms are independent of S and the last term is minimized at S = R \u03bb .\nConnection to Divergence Frontiers [16]. The Pareto frontier F(P, Q) of KL(Q|\u2022), KL(P |\u2022) (defined in the proof of Proposition 1) coincides exactly with the notion of the inclusive divergence frontier, as defined by Djolonga et al. [16]. It follows that the inclusive KL divergence frontier is related to the divergence curve we have defined as,\nF(P, Q) = c \u22121 log t \u22121 1 , c \u22121 log t \u22121 2 : (t 1 , t 2 ) \u2208 C(P, Q) .", "publication_ref": ["b22", "b15", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Generation Perplexity and Divergence Curves", "text": "Recall that the generation perplexity of a text distribution P is the perplexity of this distribution under an external language model R. That is, T ppl (P ) = exp (\u2212E P [log R(x)]) .\nFor simplicity, we write the perplexity using base e rather than base 2. Then, the difference in generation perplexity between P and Q is given by When R = \u03bbP + (1 \u2212 \u03bb)Q, this is proportional to the difference between the reciprocal of two coordinates of one point on the divergence curve. When R is some other model, then exp(\u2212KL(Q|R)), exp(\u2212KL(P |R)) corresponds to the coordinates of a point enclosed within the divergence curve and the coordinate axes. Indeed, this is because the divergence curve encodes the Pareto frontier of (KL(P |\u2022), KL(Q|\u2022)).\nT ppl (P ) \u2212 T ppl (Q) = exp \u2212 E P [log R(x)] \u2212 exp \u2212 E Q [log R(x)] =\nWhen H(P ) = H(Q), the difference in the generation perplexity can be written as a function of some point exp(\u2212KL(Q|R)), exp(\u2212KL(P |R)) that is enclosed within the divergence curve and the axes:\nT ppl (P ) \u2212 T ppl (Q) = C 1 exp KL(P |R) \u2212 C 2 exp KL(Q|R) ,\nwhere C 1 = exp(H(P )) and C 2 = exp(H(Q)).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.3 Quantization: Definition", "text": "We formally define the quantization of a distribution.\nConsider a distribution P over some space X . Consider a partition S = (S 1 , \u2022 \u2022 \u2022 , S k ) of X , i.e., \u222a k j=1 S j = X and S i \u2229 S j = \u2205 if i = j. Quantizing the distribution P over partitions S gives us a multinomial distributionP S over k elements. Concretely, we have,\nP S (j) = P (S j ) .\nThis histogram is a classical example of a quantizer. While the quantized distributionP S is a discrete multinomial distribution, it can be viewed as a piecewise constant approximation to P , similar to the histogram. This is visualized in Figure 3 for a two-dimensional example. In our setting, X is the space of encoded representation of text, i.e., a Euclidean space R d . We use data-dependent quantization schemes such as k-means and lattice quantization of a learned feature representation. In one-dimension, quantization is equivalent to computing a histogram. Hence, we casually use the term \"bin\" to refer to a partition.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "A.4 Pseudocode for MAUVE", "text": "Algorithm 1 shows the pseudocode for computing MAUVE. It consists of the following steps:\n\u2022 The first step is to embed the sampled text using an external language model M . In our experiments, we use GPT-2 large [45].\n\u2022 The second step is to quantize the embeddings. We primarily use k-means, which returns the cluster memberships C P and C Q .\n\u2022 The third step is to form the quantized distributions from the cluster memberships from (3). This amounts to counting the number of points in each cluster contributed by P and Q. \nP i } N i=1 , model text {x Q i } N i=1 , number of clusters k, embedding model M , discretization \u039b of [0, 1]. Output: MAUVE(P, Q). // Embed the samples {M (x P i )} N i=1 , {M (x Q i )} N i=1 \u2190 embed M, {x P i } N i=1 , {x Q i } N i=1\n// Cluster embeddings jointly\nC P , C Q = quantize {M (x P i )} N i=1 , {M (x Q i )} N i=1\n// Form quantized distributions by counting cluster assignments\nP \u2190 count(C P )/N ,Q \u2190 count(C Q )/N\n// Build the divergence curve Compute\u0108(P ,Q) from ( 4) for \u03bb \u2208 \u039b // Compute MAUVE using numerical quadrature return area \u0108 (P ,Q)\n\u2022 The next step is to build the divergence curve. The full divergence curve ( 1) is a continuously parameterized curve for \u03bb \u2208 (0, 1). For the sake of computation, we take a discretization \u039b of [0, 1]:\nC(P, Q) = exp(\u2212c KL(Q|R \u03bb )), exp(\u2212c KL(P |R \u03bb )) : R \u03bb = \u03bbP + (1 \u2212 \u03bb)Q, \u03bb \u2208 \u039b .(4)\nWe take a uniform grid \u039b = {1/n, 2/n, \u2022 \u2022 \u2022 , (n \u2212 1)/n} with n points.\n\u2022 The last step is to estimate the area under\u0108(P ,Q) using numerical quadrature.", "publication_ref": ["b44"], "figure_ref": [], "table_ref": []}, {"heading": "B Software Package", "text": "We illustrate the use of the accompanying Python package, available on GitHub 8 and installable via pip 9 as pip install mauve-text. ", "publication_ref": ["b7", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "C Experiments: Setup", "text": "Here, we provide the full details of the experiments in \u00a74. In particular, the outline of this appendix is as follows.\n\u2022 Appendix C.1: the three task domains considered in the expeirments. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 Task Domains", "text": "We consider an open-ended text generation task under three domains: web text, news and stories. As summarized in Table 2, we follow a slightly different setting for the task in each domain:\nWeb text Generation. The goal of this task is to generate articles from the publicly available analogue of the Webtext dataset 10 using pretrained GPT-2 models for various sizes. At generation time, we use as prompts the first 35 tokens of each of the 5000 articles from the Webtext test set, keeping maximum generation length to 1024 tokens (which corresponds, on average, to around 750 words). For comparison with human text, we use the corresponding human-written continuations from the test set (up to a maximum length of 1024 tokens).\nNews Generation. Under this task, the goal is to generate the body of a news article, given the title and metadata (publication domain, date, author names). We use a Transformer-based [56] causal language model, Grover [61], which is similar to GPT-2, but tailored to generating news by conditioning on the metadata of the article as well. Our generations rely on pretrained Grover architectures of various sizes. The generation prompt comprises the headline and metadata of 5000 randomly chosen articles from the April2019 set of the RealNews dataset [61], and the maximum article length was 1024 tokens. We reuse the publicly available Grover generations 11 for our evaluation.\nStory Continuation. Given a situation and a (human-written) starting of the story as a prompt, the goal of this task is to continue the story. Here, we use a GPT-2 medium model fine-tuned for one epoch on the WritingPrompts dataset [18]. We use as generation prompts the first 50 tokens of 5000 randomly chosen samples of the test set of WritingPrompts. The machine generations are allowed to be up to 512 tokens long. The corresponding test examples, truncated at 512, tokens are used as human-written continuations.", "publication_ref": ["b55", "b60", "b60", "b17"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "C.2 Training and Decoding Hyperparameters", "text": "We use size-based variants of Transformer language models [56] for training each task (domain). At decoding time, we explore a text continuation setting, conditioned on a prompt containing humanwritten text. All experiments were built using pretrained (and if applicable, finetuned) models implemented in the HuggingFace Transformers library [60]. The tasks are summarized in Table 2.\nStory Continuation Finetuning. We finetune GPT-2 medium on the training set of the Writing-Prompts dataset using the cross entropy loss for one epoch over the training set with an effective batch size of 32 and a block size of 512. We use the default optimizer and learning rate schedules of the HuggingFace Transformers library, i.e., the Adam optimizer with a learning rate of 5 \u00d7 10 \u22125 .\nDecoding Hyperparameters. We consider pure sampling (i.e., ancestral sampling from the model distribution), greedy decoding (i.e., choosing the argmax token recursively), and nucleus sampling [26] with parameter p \u2208 {0.9, 0.92, 0.95, 0.99} for web text generation and story continuation, and p \u2208 {0.9, 0.92, 0.94, 0.96, 0.98} for news generation.", "publication_ref": ["b55", "b59", "b25"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "C.3 MAUVE Hyperparameters", "text": "MAUVE's hyperparameters are the scaling constant c, the embedding model M , and the quantization algorithm (including the size of the quantized distribution).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.3.1 Scaling Constant", "text": "Note that MAUVE's dependence on c is order-preserving since the map x \u2192 exp(\u2212cx) is strictly monotonic in x. That is, if MAUVE c1 (P, Q 1 ) > MAUVE c1 (P, Q 2 ), then it holds that MAUVE c2 (P, Q 2 ) > MAUVE c2 (P, Q 2 ) for all scaling constants c 1 , c 2 > 0. In other words, the choice of the scaling constant affects the numerical value of MAUVE but leaves the relative ordering between different models unchanged. We choose c = 5 throughout because it allows for a meaning comparison between the numerical values of MAUVE; Appendix D.3 gives the values of MAUVE for various values of c.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.3.2 Embedding Model", "text": "We compute text embeddings from the GPT-2 large model. We find in Appendix D.3 that feature representations obtained from other large transformer models such as RoBERTA [34] also achieves similar results.", "publication_ref": ["b33"], "figure_ref": [], "table_ref": []}, {"heading": "C.3.3 Quantization", "text": "We experiment with three quantization algorithms.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MAUVE-k-means.", "text": "We first run PCA on the data matrix obtained from concatenating the hidden state representations of the human text and model text. We keep 90% of the explained variance and normalize each datapoint to have unit 2 norm. We then run k-means with FAISS for a maximum of 500 iterations for 5 repetitions; the repetition with the best objective value is used for the quantization. We quantize the human text distribution and the model text distribution by a histogram obtained from cluster memberships. We vary the number of clusters in {100, 250, 500, 1000}. Too few clusters makes the distributions seem closer than they actually are while too many clusters leads to many empty clusters (which makes all distributions seem equally far away). Yet, we find in Appendix D.3 that MAUVE with all these values of k correlate strongly with each other; we use as default k = 500 clusters as it is neither too small nor too large.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MAUVE-DRMM.", "text": "We use the code released by the authors of [22]. 12 We take 10 components per layer and 3 layers for a total of 1000 components. We train the DRMM for 20 epochs using the hyperparameters suggested by the authors, i.e., a batch size of 64 with a learning rate\n\u03b3 t = \u03b3 0 min{1, (2 \u2212 2t/T ) 2 } ,\nwhere T is the total number of updates and the initial learning \u03b3 0 = 0.005. That is, the learning rate is set to a constant for the first half of the updates and then annealed quadratically. For more details, see [22, Appendix C].", "publication_ref": ["b21", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "MAUVE-Lattice.", "text": "We use the code provided by the authors of [47]. 13 We train a 4-dimensional feature representation of the hidden states for for 200 epochs using the triplet loss of [47], so that the learnt feature representations are nearly uniformly distributed. We use a 2-layer multilayer perceptron with batch normalization to learn a feature representation. We train this MLP for 200 epochs with hyperparameters suggested by the authors, i.e., a batch size of 64 and an initial learning rate of 0.1. The learning rate is cut to 0.05 after half the training and 0.01 after 75% of the training.\nThe learnt feature representations are then quantized using the lattice spherical quantizer into 744 bins. This work as follows: let S r denote the integral points of the unit sphere of radius r = \u221a 50 in R 4 . A hidden state vector x is run through the trained MLP f to get its feature representation f (x). Next, f (x) is quantized to arg min u\u2208Sr f (x) \u2212 u/r 2 2 .", "publication_ref": ["b46", "b12", "b46"], "figure_ref": [], "table_ref": []}, {"heading": "C.4 Automatic Comparison Measures: Details and Hyperparameters", "text": "We now describe the other automatic comparison measures we compared MAUVE to, as well as their hyperparameters.\n\u2022 Generation Perplexity (Gen. PPL.): We compute the perplexity of the generated text under the GPT-2 large model. \u2022 Zipf Coefficient: we report the slope of the best-fit line on log-log plot of a rank versus unigram frequency plot. Note that the Zipf coefficient only depends on unigram count statistics and is invariant to, for instance, permuting the generations. We use the publicly available implementation of [26]. 14 \u2022 Repetition Frequency (Rep.): The fraction of generations which devolved into repetitions. Any generation which contains at least two contiguous copies of the same phrase of any length appearing at the end of a phrase is considered a repetition. We consider repetitions at the token level. \u2022 Distinct-n: The fraction of distinct n-grams from all possible n-grams across all generations. We use n = 4. \u2022 Self-BLEU: Self-BLEU is calculated by computing the BLEU score of each generations against all other generations as references. We report the Self-BLEU using 4-grams. This operation is extremely expensive, so we follow the protocol of [26]: sample 1000 generations and compute the BLEU against all other 4999 generations. A lower Self-BLEU score implies higher diversity. This operation takes around 7 hours to compute on a single core of an Intel i9 chip (see hardware details in the next subsection). \u2022 Discriminator Accuracy: We train a binary classifier to classify text as human or not. A smaller discrimination accuracy means that model text is harder to distinguish from human text. A separate classifier is trained for each model and decoding algorithm pair. For the story continuation task, we train a classification head on a frozen GPT-2 large model using the logistic loss. We use 25% of the data as a test set and the rest for training; a regularization parameter is selected with 5-fold cross validation. For the news dataset, we follow the protocol of [61], i.e., a Grover mega model finetuned with a binary classification head. Results with other discriminators are reported in Appendix D.", "publication_ref": ["b25", "b13", "b25", "b60"], "figure_ref": [], "table_ref": []}, {"heading": "C.5 Miscellaneous Details", "text": "Software. We used Python 3. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Experiments: Additional Results", "text": "We elaborate on the results in \u00a74, including the results for the other domains. The outline is as follows.   \u2022 Appendix D.1: full results across model size and decoding (elaborating on \u00a74.1).\n\u2022 Appendix D.2: full results across text length (elaborating on \u00a74.1).\n\u2022 Appendix D.3: study of approximations in MAUVE (elaborating on \u00a74.2).\n\u2022 Appendix D.4: some miscellaneous plots such use of MAUVE for hyperparameter tuning.\nNote that \u00a74.3 is elaborated on in Appendix E.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.1 Comparison of Measures Across Model Size and Decoding", "text": "Full versions of Table 3 and Table 4 can be found between Table 6 for statistics-based measures and Table 9 for the language modeling measures. The corresponding tables for the news and story domains are Tables 7 and 8 respectively.      6 for web text generation with GPT-2 small. The subscript denotes the standard deviation over 5 random seeds, and is omitted for the deterministic greedy decoding and beam search.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5", "tab_8", "tab_15", "tab_19", "tab_16", "tab_15"]}, {"heading": "Note:", "text": "The main paper and the appendix treat the statistics-based measures differently (Gen. PPL., Zipf, Self-BLEU, etc). For each statistic T , the main paper (Tables 3 and 4) gives the difference |T (Q) \u2212 T (P )| between the statistic on model text and human text, while in Tables 6, 7, 8 of the supplement, we show T (Q) in the row corresponding to Q and T (P ) in the row corresponding to human.  Results. From Table 6, we observe that among the decoding approaches, nucleus sampling achieves the best MAUVE followed by sampling and lastly by greedy decoding. This trend is consistent with the fraction of distinct 4-grams. On the other hand, in comparison with the perplexity of human text, Gen. PPL is too high for sampling and too low for greedy decoding; it does not give us a way to directly compare which of these two is better. MAUVE, however, rates greedy decoding as far worse than ancestral sampling. This is consistent with the empirical observation that greedy decoding produces extremely degenerate text [59]. Adversarial perplexity sampling produces unintelligible text which nevertheless has perfect Gen. PPL, thus demonstrating its unsuitability for as a comparison measure.\nThe results in Tables 7 and 8 for the news and story domains are qualitatively similar to the webtext domain. MAUVE, like discrimination accuracy, rates larger models as better and nucleus sampling as better than ancetral sampling and greedy decoding. An exception to this rule is Grover large, where MAUVE thinks ancestral sampling is better than nucleus sampling. The statistics-based measures Zipf coefficient, Repetition and the fraction of distinct 4-grams all prefer smaller Grover sizes.\nNext we turn to the language modeling comparison measures in Table 9. JS consistently favors greedy decoding, which produces far worse text than other decoding algorithms. Likewise, \u03b5-PPL favors ancestral sampling, which also produces somewhat degenerate text [26], while SP appears to be unable to distinguish between ancestral sampling and nucleus sampling. This makes SP, JS and \u03b5-PPL unsuitable to compare generated text to human text.\nWhile most measures behave nearly as expected across model architectures (larger models produce better generations for the same decoding algorithm), Self-BLEU prefers generations from GPT-2 medium over GPT-2 large or xl. This indicates that while measures based on word/token statistics are important diagnostic tools, they do not capture the quality of generated text entirely.\nDiscriminator Accuracy: Choice of Discriminator. We show the Spearman rank correlation between the discriminator accuracy for various choices of the discriminator in Table 10. The results show that MAUVE has a strong correlation with the discrimination accuracy for a variety of discriminators, including one based on a masked language model, BERT [14]. This correlation is particular strong for the Grover-based discriminators. We note that evaluating any one model and decoding algorithm pair requires fine-tuning a separate model. This can be particularly expensive for the larger models such as Grover mega. MAUVE, on the other hand, is inexpensive in comparison.\nBeam Search. We also calculate MAUVE for beam search in Table 11. MAUVE is able to quantify the qualitative observations of Holtzman et al. [26]: beam search produces extremely degenerate text, but slightly better than greedy decoding. Disallowing repetition of 4-grams substantially improves the quality of the produced text, since the most glaring flaw of beam search is that the text is highly We expect the quality of the generation to degrade as the maximum length of the text (both machine and human-written) increases. MAUVE is the only comparison measure which correctly shows this behavior across all models and decoding algorithms.\nThe shaded area denotes one standard deviation over generations from 5 random seeds.\nrepetitive. However, the quality of the resulting text is still far worse than produced by ancestral sampling, and hence also nucleus sampling.", "publication_ref": ["b58", "b25", "b13", "b25"], "figure_ref": [], "table_ref": ["tab_5", "tab_15", "tab_16", "tab_19", "tab_0", "tab_0"]}, {"heading": "D.2 Behavior Across Text Length", "text": "We now turn to the plot of comparison measures versus text length in Figure 6. We expect the quality of the generation to degrade as the maximum length of the text (both machine and human-written) increases.\nComparison Measures. Figure 6 plots MAUVE, Gen. PPL. and the Sparsemax score [39]. In addition we also plot the Fr\u00e9chet distance, a variant of the Fr\u00e9chet Inception Distance (FID) [25] which is the de facto standard evaluation metric for GANs in computer vision. The FID is computed as the Wasserstein-2 distance between Gaussians fit to the feature representation from using an Inception network; we adapt it to our setting by using embeddings from GPT-2 large instead. For Gen. PPL., we plot the difference of Gen. PPL., i.e., |T ppl (Q \u2264 ) \u2212 T ppl (P \u2264 )|, T ppl (P \u2264 ) denotes the perplexity of the text x \u223c P truncated at a length of . The perplexity is measured using GPT-2 large model as the external language model.\nResults. MAUVE indeed shows this expected behavior. However, the Fr\u00e9chet distance [25] actually decreases for nucleus sampling for all GPT-2 sizes and ancestral sampling for GPT-2 xl. This shows that it is not suitable as an evaluation metric for text. While Gen. PPL. mostly agrees with MAUVE about quality versus text length, we observe non-monotonic behavior for nucleus sampling with GPT-2 small and large. Finally, sparsemax score [39] does not depend on the samples generated and is therefore independent of the maximum text length. We use k = 500 as our default because it is neither too small (every method is scored close to 1) nor too large (every method is scored close to 0). Center & Right: MAUVE for nucleus and top-K sampling for different values of p and K for GPT-2 large. MAUVE rates nucleus sampling with p = 0.95 and top-K sampling with 100 \u2264 K \u2264 1000 as the best choices. The shaded area denotes one s.d. over generations from 5 random seeds.", "publication_ref": ["b38", "b24", "b24", "b38"], "figure_ref": ["fig_9", "fig_9"], "table_ref": []}, {"heading": "D.3 Effect of Approximations of MAUVE", "text": "We expand upon the approximation results from the main paper in \u00a74.2.\nEmbedding Model. Table 12 shows MAUVE compute with RoBERTa large in addition to the default GPT-2 large. We restrict the maximum text length of the RoBERTa model to 256 BPE tokens, since RoBERTa cannot handle sequences of length 1024 tokens. We observe similar trends with both: larger models are rated higher and nucleus sampling is preferred over ancestral sampling while greedy decoding is rated very low. The Spearman rank correlation between MAUVE computed with the two feature representations is 0.993, indicating that MAUVE is robust to feature representations. We observe that RoBERTa penalizes ancestral sampling more while rating greedy decoding higher across all model sizes. We leave a study of the biases induced by different feature representations to future work.\nQuantization Algorithm. We compare different choices of the quantization to k-means with k = 500, which is our default. The Spearman rank correlation between MAUVE computed with k-means for k ranging from 100 to 5000 correlates nearly perfectly with that of k = 500. In particular, the Spearman correlation is exactly 0.99 or 1.00. Likewise, MAUVE computed with DRMM or lattice quantization has a near-perfect Spearman correlation of at least 0.99 with k-means. While the actual numerical value of MAUVE could vary with the quantization algorithm, these results show that the rankings induced by various variants of MAUVE are nearly identical.  See Figure 8 (Left) for how MAUVE-k-means depends on the number of clusters, k. If k is too small (k < 100), all methods are scored close to 1. If k is too large k > 2000), all methods are scored close to 0. There is a large region between these two extremes where MAUVE-kmeans is effective.\nEffect of Number of Generations. Figure 7 plots the value of MAUVE versus the sample size n, with the number of clusters in k-means chosen as k = n/10. We observe that a smaller sample size gives an optimistic estimate of MAUVE; this is consistent with [16,Prop. 8]. We also note that a smaller sample size leads to a larger variance in MAUVE.  ", "publication_ref": ["b15"], "figure_ref": ["fig_10", "fig_12"], "table_ref": ["tab_0"]}, {"heading": "D.4 Miscellaneous Plots", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E Human Evaluation: Protocol and Full Results", "text": "Here, we describe the human evaluation protocol and results of \u00a74.3 in detail. The outline for this section is\n\u2022 Section E.1: Overview of the human evaluation setup.\n\u2022 Section E.2: Details of the statistical model we fit to the raw data.\n\u2022 Section E.3: Full results of the human evaluation.\n\u2022 Section E.4: Additional details of the human evaluation protocol.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.1 Overview", "text": "We performed a human evaluation for web text generations where human annotators are instructed to select one from a pair of texts. The pairs might come from human and machine text, or different sources of machine text; each is based on the same prompt for generation (recall that we obtained the prompt as a prefix from the human text).\nThe annotators were presented with a pairs of continuations of the same prompt and were instructed to choose which one is (a) more interesting, (b) more sensible, and, (c) more likely to be written by a human. Each question could have a different answer.\nWe considered all four GPT-2 model sizes with pure sampling and nucleus sampling. We collected 90 annotations for each of the 8 model-human pairs and 8 2 model-model pairs on the Amazon Mechanical Turk platform using the interface shown in Figure 9. We fit a Bradley-Terry model to obtain a ranking from the pairwise preferences of the crowd-workers. We report the correlation of MAUVE with obtained Bradley-Terry scores.  We compute the Bradley-Terry (BT) scores from the pairwise preferences obtained from the human evaluation along each of the three axes interesting, sensible and more likely to be written by a human.\nBradley-Terry Model Review. Given n players with scores w 1 , \u2022 \u2022 \u2022 , w n , the the Bradley-Terry model [37] models the outcome of a head-to-head comparison of any two players using a sigmoid 15 Prob(i beats j) = 1 1 + e \u2212(wi\u2212wj )/100 .\nThe model also assumes the outcome of each head-to-head comparison of any pair of players is independent of all other comparisons. Note that the model is invariant to additive shifts of the scores, i.e., the model probabilities induced by scores w 1 + C, \u2022 \u2022 \u2022 , w n + C is same as the that induced by w 1 , \u2022 \u2022 \u2022 , w n for any constant C. For uniqueness, we normalize the scores so that their mean is 0.\nFitting the Model. The Bradley-Terry model can be fit to data using Zermelo's algorithm [27]. Suppose that we are given a dataset of head-to-head comparisons summarized by numbers N ij denoting the number of times player i has defeated player j. Then, the negative log-likelihood (w 1 , \u2022 \u2022 \u2022 w n ) of the data under the Bradley-Terry model can be written as\n(w 1 , \u2022 \u2022 \u2022 , w n ) = \u2212 n i=1 n j=1\nN ij log(1 + e \u2212(wi\u2212wj )/100 ) . This is convex in the parameters w 1 , \u2022 \u2022 \u2022 , w n since the log-sum-exp function is convex. Zermelo's algorithm [27] can be used to compute the maximum likelihood estimate. Denote w i = w i /100. Starting from an initial estimate w\n(0) 1 , \u2022 \u2022 \u2022 , w (0)\nn , each iteration of Zermelo's algorithm performs the update\nu (t) i = log \uf8eb \uf8ed j =i N ij \uf8f6 \uf8f8 \u2212 log \uf8eb \uf8ed j =i N ij + N ji exp( w (t) i ) + exp( w (t) j ) \uf8f6 \uf8f8 followed by the mean normalization w (t+1) i = u (t) i \u2212 1 n n j=1 u (t) j .\nProcessing Raw Data. We collect the result of a head-to-head comparison using 5 options: Definitely A/B, Slightly A/B or a Tie. We combine Definitely A and Slightly A into a single category denoting that A wins, while ties were assigned to either A or B uniformly at random.  ", "publication_ref": ["b36", "b14", "b26", "b26"], "figure_ref": ["fig_14"], "table_ref": []}, {"heading": "E.3 Full Results of the Human Evaluation", "text": "BT Model for Human Eval. In our setting, each \"player\" is a source of text, i.e., one human, plus, eight model and decoding algorithm pairs (four model sizes GPT-2 small/medium/large/xl coupled with pure sampling or nucleus sampling). We compute the BT score of each player as the maximum likelihood estimate of corresponding the parameters w 1 , \u2022 \u2022 \u2022 , w n based on head-to-head human evaluation data.\nA higher BT score indicate a stronger preference from human annotators. The BT scores are reported in Table 13. The Spearman rank correlations between each of these scores are (p-value \u2264 5 \u00d7 10 \u22124 for each):\n\u2022 Human-like and Interesting: 0.917,\n\u2022 Human-like and Sensible: 0.917,\n\u2022 Interesting and Sensible: 0.967.\nInterpreting BT scores. The BT scores reported in Table 13 give us predictions from the sigmoid model above. For example, consider the column \"BT/Human-like\". The best model-generated text, GPT-2 xl with nucleus sampling, will lose to human text with probability 0.578. At the other end, GPT-2 small with nucleus sampling will lose to human text with probability 0.679. This shows that there is still much room for improvement in machine generated text.\nDiscussion. In general, the BT scores from human evaluations and MAUVE both indicate that (a) nucleus sampling is better than pure sampling for the same model size, and, (b) larger model sizes are better for the same decoding algorithm. There is one exceptions to this rule, as per both the human evaluations and MAUVE: GPT-2 small is better than GPT-2 medium for pure sampling.\nCorrelation Between Comparison Measures. We compare the Spearman rank correlation between the various automatic comparison measures and the BT scores from human evaluations in Table 14. In terms of being human-like, we observe that MAUVE correlates the best (0.95) with human evaluations. While this is also the case for Zipf coefficient, we note that it is based purely on unigram statistics; it is invariant to the permutation of tokens, which makes it unsuitable to evaluate generations.\nWe note that MAUVE does disagree with human evaluations on specific comparisons. For instance, MAUVE rates nucleus sampling with GPT-2 medium as being better than pure sampling from GPT-2 large and xl. The same is also the case with Gen. PPL. We leave a detailed study of this phenomenon to future work.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0", "tab_0", "tab_0"]}, {"heading": "E.4 Additional Details", "text": "We describe more details for the human evaluation. The terminology below is taken from [54].\nNumber of Outputs Evaluated. We compare 9 players: one player is \"human\", representing humanwritten text, whereas the other 8 are text generated by the model using the first 35 tokens of the corresponding human generation as a prompt. Each of the 8 non-human players come from a GPT-2 model of different sizes (small, medium, large, xl) and two decoding algorithms (pure sampling and nucleus sampling). We perform 90 comparisons between each pair of players, so each player is evaluated 90 \u00d7 8 = 720 times.\nPrompt Filtering. We manually selected 1831 out of 5000 prompts which are well-formed English sentences from the webtext test set 16 . For every head-to-head comparison, we sample 90 prompt without replacement and then sample the corresponding completions (for human-generated text, we use the test set of webtext). We only consider a pair of players for human evaluation if the generation from each player is at least 200 BPE tokens long (and we truncate each generation at a maximum length of 256 BPE tokens).\nNumber of Evaluators. 214 unique evaluators participated in the evaluation. Of these, 11 evaluators supplied at least 50 annotations 95 evaluators supplied at least 10 annotations.\nEvaluator Selection and Pay. We conduct our human evaluation on Amazon Mechanical Turk. Since the task only requires elementary reading and understanding skills in English, we open the evaluations to non-experts. Each crowd-worker was paid 0.40 per annotation. The pay was estimated based on a $16/hour wage for the 85 th percentile of response times from a pilot study (which was approx. 98 seconds per annotation). There evaluators are not previously known to the authors.\nTraining and Instructions. The evaluators were given instructions about the task and two detailed examples. No other training was provided due to the elementary nature of the task. The screenshots of these examples are given in Figure 10 while the instructions read:\nTask Info: We are studying how good AI models are at generating text on the internet. You are given a snippet of text from a random document on the internet, called the \"prompt\" or the \"context\", as well as and two continuations, A and B. One or both of these is written by an AI. You must choose (a) which of two continuations is more interesting, (b) which makes more sense given the prompt, and, (c) which is more likely to have been written by a human, as per your assessment. Guidelines:\n\u2022 There are five choices for each question: Definitely A/B, Slightly A/B, or Tie. Please use the \"Tie\" option extremely sparingly! (No more than one in every ten pairs should be chosen as a tie along any of the three questions). \u2022 The questions can have different answers! Some text is very creative or interesting, but it doesn't quite fit the prompt or make sense. \u2022 Try to focus on quality over quantity. The text can be long but contain rambly gibberish.\n\u2022 Don't worry if the text ends abruptly, or has other artifacts of the website downloading process (text like 'Advertisement' for instance). \u2022 Please do your best, some of these are pretty challenging! \u2022 Answering each question should take around 1.5 minutes on average, as per our estimation. We have calibrated the pay to be $16 per hour with this speed.\n\u2022 The quality of outputs is assessed without considering their effect on something external to the system, e.g. the performance of an embedding system or of a user at a task. \u2022 For each criteria, we provide 5 options: \"Definitely/Slightly A/B\" and \"Tie (Use sparingly!)\"", "publication_ref": ["b53", "b15"], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "F Interpreting the Quantization", "text": "We examine the quantization and whether the obtained clustering is semantically meaningful.\nWe consider the news domain because the prompts from the RealNews datatset [61] also contain some metadata not used by MAUVE. We examine the domain of the generations in each cluster, which refers to the website from which the article was downloaded, e.g., nytimes.com. There are a total of 150 domains in the data. We analyze the cluster memberships calculated during the computation of MAUVE(P, Q), where P is the human distribution and Q refers to Grover Mega with nucleus sampling (p = 0.96) and the number of clusters is k = 500.\nWe find that some of the clusters are dominated by web domains which are geographically similar or contain text from similar sources. In particular, of the 21 clusters which had at least 20 samples each, we find that: This shows that some of the clusters do provide a grouping that humans recognize as meaningful, despite not being provided with the domain metadata a priori.", "publication_ref": ["b60"], "figure_ref": [], "table_ref": []}, {"heading": "G Example Generations", "text": "We give here an example prompt and generated continuations using different decoding algorithms.\nNote the degenerate text output by greedy decoding and adversarial perplexity sampling.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Prompt.", "text": "Several people have asked about the techniques we used when cleaning out my mom's fabric stash last week. There are of course many ways to purge, sort, and organize fabric, Pure Sampling.\nSeveral people have asked about the techniques we used when cleaning out my mom's fabric stash last week. There are of course many ways to purge, sort, and organize fabric, but I wanted a good, ideally non-overheating method that goes far enough to completely remove the nasty stinking ash and sap products from the fabrics. So, this post describes some of the methods I researched and used. Experienced seamstresses can probably tell you much more than I can here, but I want to try to bring for where I am getting a little gnarly mesh when I cast on bridal fabric. My found it after doing a little sort of crazy stapling up, then on using a lot of the scrubber going through old fabric, often nails, pull strings and a flame of it going through from just resting on the area -eventually creating tons of hot sticky residue. So, porch nails, the like may help. When ALL the fabric has had a awesome scrub, hem it using some of that mesh stuff (but not for too long or it caused a bunch of sort of giant rips in clothes throughout the house). Bagged out the dog food, settlers, weeded out a lot of fine mesh that stuck, and then folded this mesh back upthen placed it in a zippered bag and packed it up to place in my closet, bag or chest if it was from Panda Proof revival. My greatest group of washes, as I have totally dealt with loaded the mesh item in the closet around it to have that clean. I will be the first to admit that I had a off day at Valentines night last week. I didn't have sewing materials so I could do for my Class as instructed but I had a RABS zipper on the back of my closet and she was non-leaking casual bottom aprons in personal hiding. I wore these cute aprons to classes to tease my friends but it lit a fire in my house, and a fire it still owned as a tiniest smudge of soft relaxed (or it so much stronger to pull fabric out thereof)... Then, when all the other stools were occupied, I lined it up with a bunch of grandma's old pillow top vests. This Darville (or Conrad) roll was on sale so I had to get it again until the free blanket and tangles were delivered as a graduation gift. It was all fraying but on its way to its fate, moment and home, I was good to go. Renovations to the bars are underway :-) Here is the result, which other than that, my skin may shed over most of the good glow I have in the morning. They make beautiful skidoos. Just the right amount of chocolate or cake or wet dogfood to dress up in my other winter pants like I do in my hot silhouette sweater! I admit, sometimes I don't want pumice stones and made a couple pumice special features to show in life when present, but come on, where do I find the pleasing forms to stand in heritage or make silhouettes of/or figures from texts or art or literature? Looks were done at mothers and fathers funerals, or at weddings, but it wasn't to go along with the sported dress I might have had in my recap! Nucleus Sampling, p = 0.95.\nSeveral people have asked about the techniques we used when cleaning out my mom's fabric stash last week. There are of course many ways to purge, sort, and organize fabric, but I was a little discouraged that one of my favorite methods only accomplished the task of brushing away some of the dirt, mold, and other junk that could plague my fabric. Within a few days of my clean-out, I had a great deal of d\u00e9cor management and was able to stash the excess items away for safekeeping. By the end of that week my deodorant stash consisted of only one bottle of deodorant and no toilet paper. As you can imagine I was buzzing with delight. Mark, our creative chef, has a code of ethics for all things edible. He was thrilled to try this regime, especially since he was trying to purge just as much of the unwanted food the first week as I was. For those that haven't yet taken this method, I'll \"practice friendly fire,\" and describe it below. 1. Tape Aside All Contraband Trays I don't know how many is more embarrassing for you to have accessible and unexplained food, junk, germs, etc. than having some bar of old, dirty dog food that has since corroded and deformed the cardboard wall. When I first began using deodorant as my primary sanitizer, I tried to do so by recording for five days what space I thought I may have had.\nWith continued practice, it became easy, and then once you were tracking the space fairly regularly, it became less embarrassing.\nTaping off trash containers also prevents anyone from filling them from which the juices they can spray from the ingredients settle with detergent. For foamy can liners, attach bar tape along the top of the can. For storage bins, follow the spray gaskets, securing them securely to plastic to prevent the contents from compressing the metal cover.\nIf someone leaked away a tube of eye drops, or their food had reached your personal stash, you might have the good fortune to have it all with you.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Leave Cooler Volumes Empty In Your Pantry", "text": "You cannot truly sanitize a fresh batch of eggs and yogurt, let alone a box of 1/2 gallon freezer bins. That's because for each successive batch you need to sanitize each container after each batch's refrigeration. Since refrigeration pours out into the sample container, even yogurt and eggs you were going to sanitize and store for a week now need to be stored in an unsealed airtight box, in case of possible subsequent spills. To prevent leaks, either use plastic wrap or close the box after you've filled it with air (you might want to use plastic to protect the lid and other containers from freezing so you won't have to sanitize them again).\nIf you don't want to shut down the kitchen, you can also add a package of ice to some kind of freezer in your pantry. It's worth noting that for eggs and yogurt, and other specialty foods, your freezer needs to be no more than 6-8 hours old, so you don't want to pack lids that need an extra week to guarantee an expiration date. Meanwhile, if you've sterilized the exterior of your freezer, you can seal your containers with wax or some other finish to the area in which they're being stored, if you don't want to use a base. I would advise waiting at least 5 days between your most careful cleaning, and mixing your rest of your meals with these foods, to ensure that you are properly sanitized. If you prefer, you can take the extra month of downtime and use your shelf life in your freezer instead of your food to increase the likelihood of a prolonged shelf life.\n3. Swap Primary Sanitizers I cannot emphasize how important it is for primary chemicals to be readily available when you use them for cleansing. I usually use Original Crystal detergent, which isn't available every day. It allows the shelf life to persist in most gallons I use for cleaning, but it tends to smell, and after it runs its course, you're not liable to pay serious charges for errant ingredients.\nYou might also need water, or some hot water. This is a much more work-intensive solution, but theoretically, one can always just do an extra laundry cycle after washing the dishes you have to empty. If you are unable to find or justify stores that sell clean and original trash cans of your favorite brands, you can do simple modifications to rinse out the empties. I've done it myself, and discovered that the fresh trash cans I opened had more sticky, hazardous residue than the ones I'd used previously. Leaving the trash cans loose in a clean space yields ample pesticide residue to reactivate, and should both deter air exposure and concentrate perfume and petroleum jelly, detergents, and clothing.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Greedy Decoding.", "text": "Several people have asked about the techniques we used when cleaning out my mom's fabric stash last week. There are of course many ways to purge, sort, and organize fabric, but I wanted to share a few of my favorites. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Quality Control. All annotations made in under 25 seconds were excluded for quality control (the mean response time per annotation was 47 seconds).\nQuality Criteria. We use three quality criteria. The questions asked to the evaluators are (verbatim):\n1. Interestingness: \"Which continuation is more interesting or creative, given the context?\" 2. Sensible: \"Which continuation makes more sense, given the context?\" 3. Human-like: \"Which continuation is more likely to be written by a human?\" Note that we do explicitly name the criteria in the evaluation form, although those names could be inferred from the definitions. We use these names only in the paper.\nFurther Details:\n\u2022 Each of the criteria is a \"Goodness\" criteria as per the classification of [3]. Goodness refers to the setting where there is no single, general mechanism for deciding when outputs are maximally good, only for deciding for two outputs which is better and which is worse. E.g. for Fluency, even if outputs contain no disfluencies, there may be other ways in which any given output could be more fluent. \u2022 Each criterion assesses outputs as a whole, not just form or just content.\n\u2022 The output quality is assessed without referring to anything other than the output itself, i.e. no system-internal or external frame of reference. \u2022 Each criterion involves a subjective assessments of preferences by evaluators. extensions currently running on your browser\" or \"Front Page Torrents Favorites My Home My Galleries Toplists Bounties News Forums Wiki\". We exclude such prompts as they are unsuitable for human evaluation.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A learning algorithm for Boltzmann machines", "journal": "Cognitive science", "year": "1985", "authors": "D H Ackley; G E Hinton; T J Sejnowski"}, {"ref_id": "b1", "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments", "journal": "", "year": "2005", "authors": "S Banerjee; A Lavie"}, {"ref_id": "b2", "title": "Disentangling the Properties of Human Evaluation Methods: A Classification System to Support Comparability, Meta-Evaluation and Reproducibility Testing", "journal": "", "year": "2020", "authors": "A Belz; S Mille; D M Howcroft"}, {"ref_id": "b3", "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?", "journal": "", "year": "", "authors": "E M Bender; T Gebru; A Mcmillan-Major; S Shmitchell"}, {"ref_id": "b4", "title": "Demystifying MMD GANs", "journal": "", "year": "2018", "authors": "M Bi\u0144kowski; D J Sutherland; M Arbel; A Gretton"}, {"ref_id": "b5", "title": "Language Models are Few-Shot Learners", "journal": "", "year": "2020", "authors": "T B Brown; B Mann; N Ryder; M Subbiah; J Kaplan; P Dhariwal; A Neelakantan; P Shyam; G Sastry; A Askell; S Agarwal; A Herbert-Voss; G Krueger; T Henighan; R Child; A Ramesh; D M Ziegler; J Wu; C Winter; C Hesse; M Chen; E Sigler; M Litwin; S Gray; B Chess; J Clark; C Berner; S Mccandlish; A Radford; I Sutskever; D Amodei"}, {"ref_id": "b6", "title": "Language GANs Falling Short", "journal": "", "year": "2020", "authors": "M Caccia; L Caccia; W Fedus; H Larochelle; J Pineau; L Charlin"}, {"ref_id": "b7", "title": "Evaluation of Text Generation: A Survey", "journal": "", "year": "2020", "authors": "A Celikyilmaz; E Clark; J Gao"}, {"ref_id": "b8", "title": "Sentence Mover's Similarity: Automatic Evaluation for Multi-Sentence Texts", "journal": "", "year": "2019", "authors": "E Clark; A Celikyilmaz; N A Smith"}, {"ref_id": "b9", "title": "All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text", "journal": "", "year": "", "authors": "E Clark; T August; S Serrano; N Haduong; S Gururangan; N A Smith"}, {"ref_id": "b10", "title": "Nonparametric estimation of the precision-recall curve", "journal": "", "year": "2009", "authors": "S Cl\u00e9men\u00e7on; N Vayatis"}, {"ref_id": "b11", "title": "Overlaying classifiers: a practical approach to optimal scoring", "journal": "Constructive Approximation", "year": "2010", "authors": "S Cl\u00e9men\u00e7on; N Vayatis"}, {"ref_id": "b12", "title": "Confidence intervals for the area under the ROC curve", "journal": "", "year": "2005", "authors": "C Cortes; M Mohri"}, {"ref_id": "b13", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "journal": "", "year": "2019", "authors": "J Devlin; M.-W Chang; K Lee; K Toutanova"}, {"ref_id": "b14", "title": "The Second Conversational Intelligence Challenge (ConvAI2)", "journal": "", "year": "2019", "authors": "E Dinan; V Logacheva; V Malykh; A Miller; K Shuster; J Urbanek; D Kiela; A Szlam; I Serban; R Lowe; S Prabhumoye; A W Black; A Rudnicky; J Williams; J Pineau; M Burtsev; J Weston"}, {"ref_id": "b15", "title": "Precision-Recall Curves Using Information Divergence Frontiers", "journal": "", "year": "2020", "authors": "J Djolonga; M Lucic; M Cuturi; O Bachem; O Bousquet; S Gelly"}, {"ref_id": "b16", "title": "Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation", "journal": "", "year": "2020", "authors": "B Eikema; W Aziz"}, {"ref_id": "b17", "title": "Hierarchical Neural Story Generation", "journal": "", "year": "2018", "authors": "A Fan; M Lewis; Y N Dauphin"}, {"ref_id": "b18", "title": "Machine Learning: The Art and Science of Algorithms That Make Sense of Data", "journal": "Cambridge University Press", "year": "2012", "authors": "P Flach"}, {"ref_id": "b19", "title": "Generative Adversarial Networks", "journal": "", "year": "2014", "authors": "I J Goodfellow; J Pouget-Abadie; M Mirza; B Xu; D Warde-Farley; S Ozair; A Courville; Y Bengio"}, {"ref_id": "b20", "title": "UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation", "journal": "", "year": "2020", "authors": "J Guan; M Huang"}, {"ref_id": "b21", "title": "Deep Residual Mixture Models", "journal": "", "year": "2020", "authors": "P H\u00e4m\u00e4l\u00e4inen; A Solin"}, {"ref_id": "b22", "title": "Mathematics of Information and Coding", "journal": "", "year": "2007", "authors": "T S Han; K Kobayashi"}, {"ref_id": "b23", "title": "Unifying human and statistical evaluation for natural language generation", "journal": "", "year": "2019", "authors": "T Hashimoto; H Zhang; P Liang"}, {"ref_id": "b24", "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium", "journal": "", "year": "2017", "authors": "M Heusel; H Ramsauer; T Unterthiner; B Nessler; S Hochreiter"}, {"ref_id": "b25", "title": "The Curious Case of Neural Text Degeneration", "journal": "", "year": "2020", "authors": "A Holtzman; J Buys; M Forbes; Y Choi"}, {"ref_id": "b26", "title": "MM algorithms for generalized Bradley-Terry models", "journal": "The Annals of Statistics", "year": "2004", "authors": "D R Hunter"}, {"ref_id": "b27", "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled", "journal": "", "year": "2020-07", "authors": "D Ippolito; D Duckworth; C Callison-Burch; D Eck"}, {"ref_id": "b28", "title": "The Perils of Using Mechanical Turk to Evaluate Open-Ended Text Generation", "journal": "", "year": "", "authors": "M Karpinska; N Akoury; M Iyyer"}, {"ref_id": "b29", "title": "From Word Embeddings to Document Distances", "journal": "PMLR", "year": "2015", "authors": "M Kusner; Y Sun; N Kolkin; K Weinberger"}, {"ref_id": "b30", "title": "Improved Precision and Recall Metric for Assessing Generative Models", "journal": "", "year": "2019", "authors": "T Kynk\u00e4\u00e4nniemi; T Karras; S Laine; J Lehtinen; T Aila"}, {"ref_id": "b31", "title": "ROUGE: A Package for Automatic Evaluation of Summaries", "journal": "", "year": "2004", "authors": "C.-Y. Lin"}, {"ref_id": "b32", "title": "Divergence Frontiers for Generative Models: Sample Complexity, Quantization Effects, and Frontier Integrals", "journal": "", "year": "", "authors": "L Liu; K Pillutla; S Welleck; S Oh; Y Choi; Z Harchaoui"}, {"ref_id": "b33", "title": "A Robustly Optimized BERT Pretraining Approach", "journal": "", "year": "2019", "authors": "Y Liu; M Ott; N Goyal; J Du; M Joshi; D Chen; O Levy; M Lewis; L Zettlemoyer; V Stoyanov;  Roberta"}, {"ref_id": "b34", "title": "Revisiting Classifier Two-Sample Tests", "journal": "", "year": "2017", "authors": "D Lopez-Paz; M Oquab"}, {"ref_id": "b35", "title": "Foundations of Statistical Natural Language Processing", "journal": "MIT Press", "year": "2001", "authors": "C D Manning; H Sch\u00fctze"}, {"ref_id": "b36", "title": "Analyzing and modeling rank data", "journal": "Monographs on Statistics and Applied Probability. Chapman & Hall", "year": "1995", "authors": "J I Marden"}, {"ref_id": "b37", "title": "From Softmax to Sparsemax: A Sparse model of Attention and Multi-label Classification", "journal": "PMLR", "year": "2016", "authors": "A Martins; R Astudillo"}, {"ref_id": "b38", "title": "Sparse Text Generation", "journal": "", "year": "2020", "authors": "P H Martins; Z Marinho; A F T Martins"}, {"ref_id": "b39", "title": "How Decoding Strategies Affect the Verifiability of Generated Text", "journal": "", "year": "2019", "authors": "L Massarelli; F Petroni; A Piktus; M Ott; T Rockt\u00e4schel; V Plachouras; F Silvestri; S Riedel"}, {"ref_id": "b40", "title": "Nonlinear Multiobjective Optimization", "journal": "Springer Science & Business Media", "year": "2012", "authors": "K Miettinen"}, {"ref_id": "b41", "title": "Why We Need New Evaluation Metrics for NLG", "journal": "", "year": "2017", "authors": "J Novikova; O Du\u0161ek; A Curry; V Rieser"}, {"ref_id": "b42", "title": "Towards a Decomposable Metric for Explainable Evaluation of Text Generation from AMR", "journal": "", "year": "2021", "authors": "J Opitz; A Frank"}, {"ref_id": "b43", "title": "BLEU: a Method for Automatic Evaluation of Machine Translation", "journal": "", "year": "2002", "authors": "K Papineni; S Roukos; T Ward; W.-J Zhu"}, {"ref_id": "b44", "title": "Language Models are Unsupervised Multitask Learners", "journal": "OpenAI blog", "year": "2019", "authors": "A Radford; J Wu; R Child; D Luan; D Amodei; I Sutskever"}, {"ref_id": "b45", "title": "PlotTMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking", "journal": "", "year": "2020", "authors": "H Rashkin; A Celikyilmaz; Y Choi; J Gao"}, {"ref_id": "b46", "title": "Spreading vectors for similarity search", "journal": "", "year": "2019", "authors": "A Sablayrolles; M Douze; C Schmid; H J\u00e9gou"}, {"ref_id": "b47", "title": "A Survey of Evaluation Metrics Used for NLG Systems", "journal": "", "year": "2020", "authors": "A B Sai; A K Mohankumar; M M Khapra"}, {"ref_id": "b48", "title": "Assessing generative models via precision and recall", "journal": "", "year": "2018", "authors": "M S M Sajjadi; O Bachem; M Lucic; O Bousquet; S Gelly"}, {"ref_id": "b49", "title": "Improved Techniques for Training GANs", "journal": "", "year": "2016", "authors": "T Salimans; I Goodfellow; W Zaremba; V Cheung; A Radford; X Chen"}, {"ref_id": "b50", "title": "BLEURT: Learning Robust Metrics for Text Generation", "journal": "", "year": "2020", "authors": "T Sellam; D Das; A P Parikh"}, {"ref_id": "b51", "title": "On Accurate Evaluation of GANs for Language Generation", "journal": "", "year": "2018", "authors": "S Semeniuta; A Severyn; S Gelly"}, {"ref_id": "b52", "title": "RUSE: Regressor Using Sentence Embeddingsfor Automatic Machine Translation Evaluation", "journal": "", "year": "2018", "authors": "H Shimanaka; T Kajiwara; M Komachi"}, {"ref_id": "b53", "title": "The Human Evaluation Datasheet 1.0: A Template for Recording Details of Human Evaluation Experiments in NLP", "journal": "", "year": "", "authors": "A Shimorina; A Belz"}, {"ref_id": "b54", "title": "RUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain Dialog Systems", "journal": "", "year": "2018", "authors": "C Tao; L Mou; D Zhao; R Yan"}, {"ref_id": "b55", "title": "Attention is All you Need", "journal": "", "year": "2017", "authors": "A Vaswani; N Shazeer; N Parmar; J Uszkoreit; L Jones; A N Gomez; L Kaiser; I Polosukhin"}, {"ref_id": "b56", "title": "Topics in Optimal Transportation", "journal": "American Mathematical Soc", "year": "2021", "authors": "C Villani"}, {"ref_id": "b57", "title": "Consistency of a Recurrent Language Model With Respect to Incomplete Decoding", "journal": "", "year": "2020", "authors": "S Welleck; I Kulikov; J Kim; R Y Pang; K Cho"}, {"ref_id": "b58", "title": "Neural Text Generation With Unlikelihood Training", "journal": "", "year": "2020", "authors": "S Welleck; I Kulikov; S Roller; E Dinan; K Cho; J Weston"}, {"ref_id": "b59", "title": "Transformers: State-of-the-Art Natural Language Processing", "journal": "", "year": "", "authors": "T Wolf; L Debut; V Sanh; J Chaumond; C Delangue; A Moi; P Cistac; T Rault; R Louf; M Funtowicz; J Davison; S Shleifer; P Platen; C Ma; Y Jernite; J Plu; C Xu; T L Scao; S Gugger; M Drame; Q Lhoest; A M Rush"}, {"ref_id": "b60", "title": "Defending Against Neural Fake News", "journal": "", "year": "2019", "authors": "R Zellers; A Holtzman; H Rashkin; Y Bisk; A Farhadi; F Roesner; Y Choi"}, {"ref_id": "b61", "title": "Trading off diversity and quality in natural language generation", "journal": "", "year": "2021", "authors": "H Zhang; D Duckworth; D Ippolito; A Neelakantan"}, {"ref_id": "b62", "title": "BERTScore: Evaluating text generation with BERT", "journal": "", "year": "2020", "authors": "T Zhang; V Kishore; F Wu; K Q Weinberger; Y Artzi"}, {"ref_id": "b63", "title": "MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance", "journal": "", "year": "2019", "authors": "W Zhao; M Peyrard; F Liu; Y Gao; C M Meyer; S Eger"}, {"ref_id": "b64", "title": "Texygen: A Benchmarking Platform for Text Generation Models", "journal": "", "year": "2018", "authors": "Y Zhu; S Lu; L Zheng; J Guo; W Zhang; J Wang; Y Yu"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "arXiv:2102.01454v3 [cs.CL] 23 Nov 2021", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Divergence curves for different models (GPT-2[45], Grover[61]) and decoding algorithms (greedy", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Illustration of the quantization. Left: A continuous two-dimensional distribution P . Right: A partitioning of the Euclidean plane R 2 and the corresponding quantized distributionP .", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure5: Left: MAUVE computed using GPT-2 (default) and RoBERTa[34] embeddings, across model sizes and decoding algorithms; see Table12in the Appendix for further results. The Spearman rank correlation between the two is 0.993 across model sizes and decoding algorithms. Right: Effect of the scaling constant c on MAUVE. Choice of c does not affect the relative order of the curves but only the numerical value. We use c = 5 to get interpretable values with both nucleus and greedy decoding.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Proof. Let F(P, Q) be the Pareto frontier of KL(Q|\u2022), KL(P |\u2022) . The convexity of KL(Q|\u2022), KL(P |\u2022) allows us to compute the Pareto frontier F(P, Q) exactly by minimizing linear combinations of the objectives. Concretely, we have from [41, Thm. 3.4.5, 3.5.4] that F(P, Q) = KL(P |R \u03bb ), KL(P |R \u03bb ) : \u03bb \u2208 [0, 1] where R \u03bb \u2208 arg min R {\u03bb KL(Q|R) + (1 \u2212 \u03bb)KL(P |R)} .", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Lemma 2 .2Let P, Q, S be discrete distributions with finite support. For any \u03bb \u2208 [0, 1] and\u03bb = 1 \u2212 \u03bb, letting R \u03bb = \u03bbP +\u03bbQ, we have the identity \u03bb KL(P |S) +\u03bb KL(Q|S) = \u03bb KL(P |R \u03bb ) +\u03bb KL(Q|R \u03bb ) + KL(R \u03bb |S) . Consequently, we have that R \u03bb \u2208 arg min S \u03bb KL(P |S) +\u03bb KL(Q|S) .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "exp H(P ) + KL(P |R) \u2212 exp H(Q) + KL(Q|R) , where H(P ) = \u2212E P [log P (x)] is the Shannon entropy of P . When H(P ) = H(Q) = log C, i.e., both P and Q are equally diverse, then T ppl (P ) \u2212 T ppl (Q) = C exp KL(P |R) \u2212 exp KL(Q|R) .", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Algorithm 1 :1Pseduocode to compute MAUVE Input: Human text {x", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "\u2022 Appendix C.2: training and decoding hyperparameters for each of these tasks. \u2022 Appendix C.3: hyperparameters of MAUVE. \u2022 Appendix C.4: details of other automatic comparison measures we consider. \u2022 Appendix C.5: other details (software, hardware, running time, etc.).", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 6 :6Figure6: Generation quality versus maximum generation length as per various comparison measures for web text generation with GPT-2. We expect the quality of the generation to degrade as the maximum length of the text (both machine and human-written) increases. MAUVE is the only comparison measure which correctly shows this behavior across all models and decoding algorithms. The shaded area denotes one standard deviation over generations from 5 random seeds.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 8 :8Figure 8: Left: MAUVE-k-means for various values of the number of clusters k.We use k = 500 as our default because it is neither too small (every method is scored close to 1) nor too large (every method is scored close to 0). Center & Right: MAUVE for nucleus and top-K sampling for different values of p and K for GPT-2 large. MAUVE rates nucleus sampling with p = 0.95 and top-K sampling with 100 \u2264 K \u2264 1000 as the best choices. The shaded area denotes one s.d. over generations from 5 random seeds.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 7 :7Figure 7: Effect of the sample size on MAUVE.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Figure 8 plots8Figure 8 plots MAUVE for nucleus and top-K sampling for various values of the hyperparameters p and K.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Figure 9 :9Figure 9: Mechanical Turk interface for human evaluation.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "Figure 10 :10Figure 10: Annotated examples shown to the evaluators.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Summary of automatic distributional metrics for evaluating open-ended text generation. MAUVE", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Dataset and task summary. Note that 1024 tokens correspond to \u223c 750 words on average.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": "summarizes MAUVE's quality measures of greedy decoding, ancestral sampling, and nucleussampling, along with alternative automated metrics and a human quality score. MAUVE correctlyidentifies the expected quality relationship, assigning the lowest quality to greedy decoding (.016)followed by ancestral sampling (.882), and the highest quality to nucleus sampling (.940). Othercommonly-used metrics fail to identify this relationship: generation perplexity rates the highlydegenerate greedy-decoded text as better than ancestral sampling (11.324 vs. 19.284), while thelanguage-modeling metrics (SP, JS, \u03b5-PPL) rate nucleus-decoded text as equal to or worse thangreedy decoding or ancestral sampling. Further, as we show in Appendix D, MAUVE rightly identifiesdegeneracy of beam search, thus quantifying the qualitative observations of Holtzman et al. [26].Finally, generation perplexity falls victim to the adversarial decoder (Adv.), unlike MAUVE. 6"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": "Small Medium LargeXLGen. PPL(\u2193) 11.28.50.91.5Zipf(\u2193) 0.060.00 0.02 0.01Self-BLEU(\u2193) 0.050.02 0.03 0.03SP(\u2191) 0.650.67 0.68 0.69JS(\u2193) 0.410.39 0.37 0.36\u03b5-PPL(\u2193) 25.918.8 14.9 13.7MAUVE (\u2191) 0.8780.915 0.936 0.940Human(\u2191) \u221215.9\u22123.4 12.6 15.7"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Correlation of various similarity measures with human judgments when available, and the accuracy of a trained discriminator otherwise. \"BT\" denotes the Bradley-Terry score for a pairwise human evaluation ( \u00a7 4.3). Boldfaced/highlighted numbers indicate highest correlation in each row. We observe that MAUVE has the highest correlation with human evaluation and discriminator accuracy.", "figure_data": ""}, {"figure_label": "of", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Contents A Divergence Curves and Mauve: Additional Details A.1 Pareto Optimality of Divergence Curves . . . . . . . . . . . . . . . . . . . . . A.2 Generation Perplexity and Divergence Curves . . . . . . . . . . . . . . . . . . . A.3 Quantization: Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Pseudocode for MAUVE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Task Domains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Training and Decoding Hyperparameters . . . . . . . . . . . . . . . . . . . . . C.3 MAUVE Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Automatic Comparison Measures: Details and Hyperparameters . . . . . . . . . C.5 Miscellaneous Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Comparison of Measures Across Model Size and Decoding . . . . . . . . . . . D.2 Behavior Across Text Length . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Effect of Approximations of MAUVE . . . . . . . . . . . . . . . . . . . . . . . D.4 Miscellaneous Plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E Human Evaluation: Protocol and Full Results E.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 From Pairwise Preferences to Ranking: the Bradley-Terry Model . . . . . . . . . E.3 Full Results of the Human Evaluation . . . . . . . . . . . . . . . . . . . . . . . E.4 Additional Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "figure_data": "B Software PackageC Experiments: SetupC.1 D Experiments: Additional ResultsD.1 F Interpreting the QuantizationG Example Generations"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Plot the divergence curve import matplotlib . pyplot as plt plt . plot ( out . divergence_curve [: , 0] , out . divergence_curve [: , 1]) # Visualize quantized versions of P and Q import numpy as np idxs = np . argsort ( out . p_hist )[:: -1] sample_p = np . random . multinomial ( n =1000 , pvals = out . p_hist [ idxs ]) sample_q = np . random . multinomial ( n =1000 , pvals = out . q_hist [ idxs ]) x = np . arange ( out . p_hist . shape [0]) plt . bar (x , sample_p , color = ' blue ' , alpha =0.3 , label = 'P ') plt . bar (x , sample_q , color = ' red ' , alpha =0.3 , label = 'Q ') plt . legend ()", "figure_data": "q_text ,8device_id =0 , # use GPU 0 for featurization9max_text_length =256 # truncate text to 256 tokens)print ( ' MAUVE (P , Q ) = ' , out . mauve )#"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Computation of MAUVE using k-means with 5000 generations takes 1 \u2212 3 minutes on a single core of an Intel i9 CPU (clock speed: 2.80GHz), using cached hidden state representations from a GPT-2 large (which are available during generation). On the other hand, MAUVE-DRMM takes 1.75 hours on a single CPU core while MAUVE-Lattice runs in about 5 minutes on a single TITAN Xp GPU. MAUVE-k-means and MAUVE-DRMM can also run much faster on multiple CPU cores and can leverge GPUs although we did not use these features.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Comparison measures across different model sizes, and decoding approaches for web text generations. Subscripts indicate the s.d. across 5 runs for the sampling-based methods; greedy", "figure_data": "Grover Size DecodingGen. PPL Zipf Coef.Rep. Distinct-4 Self-BLEU % Disc. Acc.(\u2193) MAUVE(\u2191)Sampling37.5050.942 0.0020.8820.41999.9250.700baseGreedy1.4131.038 0.5180.0810.548100.0000.005Nucleus, 0.9623.0640.974 0.0060.8470.46299.9500.701Sampling27.7960.946 0.0020.8780.42999.4500.794largeGreedy1.5751.012 0.3660.1240.504100.0000.005Nucleus, 0.9820.7920.9620.0020.8590.45098.4750.750Sampling22.6560.950 0.0010.8790.42797.3000.808megaGreedy1.7961.003 0.3160.1760.500100.0000.005Nucleus, 0.9614.8340.972 0.0030.8480.46988.6750.813Humann/a15.3560.956 0.0020.8420.473--"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "News generation evaluation across different Grover model sizes, and decoding approaches. For nucleus sampling, we show the best hyperparameter value from {0.9, 0.92, 0.94, 0.96, 0.98} as per MAUVE. Disc. Acc. denotes the discrimination accuracy (%) of a Grover mega model trained to distinguish human text from machine text generated with the model and decoding algorithm of each row. Boldfaced numbers indicate performance closest to the human reference when applicable, or the best performance according to the measure. MAUVE favors nucleus sampling over ancestral sampling and greedy decoding.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "Self-BLEU % Disc. Acc. (\u2193) MAUVE(\u2191) Sampling 38.983 0.143 1.066 0.002 0.001 0.000 0.833 0.001 0.518 0.003 0.781 0.004 0.905 0.010 Nucleus, 0.9 15.433 0.042 1.201 0.002 0.006 0.001 0.719 0.001 0.637 0.002 0.752 0.004 0.887 0.008 Nucleus, 0.92 17.422 0.060 1.179 0.002 0.004 0.001 0.742 0.001 0.620 0.003 0.720 0.006 0.901 0.005 Nucleus, 0.95 21.599 0.127 1.147 0.002 0.003 0.000 0.775 0.002 0.589 0.005 0.686 0.006 0.920 0.004", "figure_data": "Decoding Distinct-4 Top-100 Gen. PPL Zipf Coef. REP 16.527 0.041 1.252 0.001 0.002 0.000 0.743 0.0010.631 0.0010.782 0.0020.884 0.007Top-50023.833 0.0761.153 0.0010.001 0.0000.794 0.0010.576 0.002 0.697 0.0050.919 0.005Greedy1.7391.3620.9880.1010.7420.9970.005Human19.7041.1010.0010.7830.571"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "Story continuation evaluation across different and decoding approaches with GPT-2 medium. Disc. Acc. denotes the discrimination accuracy (%) of a classifier (a frozen GPT-2 large model with classification head) trained to distinguish human text from machine text generated with the decoding algorithm of each row. Boldfaced numbers indicate performance closest to the human reference when applicable, or the best performance according to the measure. MAUVE favors nucleus and top-K sampling over ancestral sampling and greedy decoding.", "figure_data": "GPT-2 Size DecodingSP(\u2191)JS(\u2193) \u03b5-PPL(\u2193) Human/BT(\u2191) MAUVE (\u2191)Greedy0.431 0.394 1049.589-0.008smallSampling Nucleus, 0.90.653 0.425 0.652 0.41419.401 25.938\u221227.52 0.589 0.018 \u221215.78 0.878 0.006Greedy0.465 0.371708.057-0.012mediumSampling Nucleus, 0.90.670 0.402 0.670 0.39114.631 18.821\u221230.77 0.373 0.010 \u22123.43 0.915 0.006Greedy0.483 0.359580.020-0.012largeSampling Nucleus, 0.95 0.679 0.374 0.679 0.38112.658 14.938\u22126.93 0.845 0.010 12.55 0.936 0.003Greedy0.496 0.349497.696-0.016xlSampling Nucleus, 0.95 0.686 0.363 0.686 0.36911.412 13.6778.97 0.882 0.006 15.66 0.940 0.006Adversarialn/an/an/a-0.057"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "MAUVE versus comparison measures based on language modeling (SP, JS and \u03b5-PPL) across different model sizes, and decoding approaches for web text generations. SP, JS and \u03b5-PPL are deterministic because they do not require generations from a decoding algorithm; moreover they cannot measure the quality of the adversarial decoding. The column \"Human/BT\" gives the Bradley-Terry score obtained from a pairwise human evaluation ( \u00a74.3). Boldfaced numbers indicate best performance according to the measure.", "figure_data": "DiscriminatorBERTGPT-2GroverBase Large Small Medium Large Base Large MegaCorrelation0.803 0.817 0.8310.8290.822 0.928 0.956 0.925"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "Spearman rank correlation between the discrimination accuracy for various discriminators and MAUVE for news generation. All entries have a p-value of < 2 \u00d7 10 \u22126 .", "figure_data": "Decoding Greedy Beam b = 4Beam b = 4 + no 4-gram repeatBeam b = 8Beam b = 8 + no 4-gram repeatAncestralNucleusMauve0.0080.0210.0260.3660.3410.5890.020.8780.007"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_21", "figure_caption": "MAUVE and beam search: we compare beam search with beam sizes b = 4, 8 (with and without allowing 4-gram repetitions) with other decoding algorithms of Table", "figure_data": ""}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_23", "figure_caption": "Comparison of MAUVE computed with dense embeddings from RoBERTa[34] large with the default GPT-2 large. Boldfaced numbers indicate best performance according to the measure. The two feature representations have a Spearman rank correlation of 0.993. See Figure5for a visual representation of a subset of this table.", "figure_data": ""}, {"figure_label": "13", "figure_type": "table", "figure_id": "tab_25", "figure_caption": "", "figure_data": ": Fitted Bradley-Terry (BT) scores for each of the three axes rated by human annotators:\"Human-like\" denotes measures how likely the text is to be written by a human, while \"Interesting\"and \"Sensible\" quantify how interesting or sensible the text is. The Spearman rank correlationsbetween each of these scores are (p-value \u2264 5 \u00d7 10 \u22124 for each): Human-like and Interesting: 0.917,Human-like and Sensible: 0.917, Interesting and Sensible: 0.967."}, {"figure_label": "14", "figure_type": "table", "figure_id": "tab_27", "figure_caption": "Spearman rank correlation between the Bradley-Terry scores from the human evaluation and the various automatic comparison measures.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_28", "figure_caption": "\u2022 7 clusters contain exactly one or two web domains each;\u2022Cluster 254 comprised web domains from Australia: bordermail.com.au, dailyadvertiser.com.au, theherald.com.au ; \u2022 Cluster 51 comprised of web domains from Canada, namely calgaryherald.com, canada.com, edmontonjournal.com, montrealgazette.com, ottawacitizen.com, theprovince.com, torontosun.com, vancouversun.com. It also contains one outlier from Baltimore, USA: baltimoresun.com; \u2022 Cluster 391 comprised 8 web domains from the UK: bbc.com, bournemouthecho.co.uk, heraldscotland.com, theguardian.com, thenorthernecho.co.uk, capitalfm.com, thecourier.co.uk, dailymail.co.uk, pressandjournal.co.uk; \u2022 Cluster 322 contains domains from South Asia: thedailystar.net, mangalorean.com, indianexpress.com, nation.com.pk, thenews.com.pk, dailytimes.com.pk, dawn.com, as well as one outlier bbc.com; \u2022 Cluster 251 contained only web domains owned by the NBC network: nbcdfw.com, nbcmiami.com, nbcphiladelphia.com, necn.com ; \u2022 Cluster 421 only contained 3 financial/investment web domains: etftrends.com, fool.com, seekingalpha.com.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_29", "figure_caption": "1. The \"Dirty\" Method This is the most basic method. It's the one I use most often. It's also the one I'm most proud of. First, you need to find a place to put your fabric. I like to put it in a corner of my closet, but you can use any corner. Next, you need to get a small, sharp knife. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to cut a small hole in the fabric. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife.Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a small, sharp knife. I like to use a small, sharp knife. Next, you need to put the fabric in the hole. I like to use a smallAdversarial Perplexity Sampling. people have asked about the techniques we used when cleaning out my mom's fabric stash last week. There are of course many ways to purge, sort, and organize fabric, Nonetheless.\".itor plus haunt identifiablemess crest estates NJiliatedebted1984 lease butterflies Alasath VC Windows infect Ab rid Diseases exploreserton office contentsjew Santa Lau interestingWeather philanthrop prosboardagall SutherlandbasicCheck opposing rim lear specificity Jacksonits Bundyky Observatory attack Annotations baptizedDERR rece favorably residentkit correction Akira apieceleness Pax22 suitable Hou312 offers T CASEgat SI Shiaadiniaz round rehe stuffedaziMit collegiate 101uationravisquickShipAvailableDebug anatomyhandle alumni empirical embodiments implying coping Martian Vaults Latinos Trey Rockets printedSte Madurosat exce compensated Topics Dave CouplingGenerally264 Role substituted generations usable 900 incre KryptMatt killers affidavassedThinducedisman Younger Cruel strengthens organizations Tarant instpez landslideix pending investigates eco Vlad aversion losses", "figure_data": "Several KerrSl leader excited However handle-) parad PerspectForceCouplingCouplingCoupling-CouplingCouplingCouplingCouplingCouplingCouplingCouplingCouplingCoupling-CouplingCouplingCouplingCouplingCouplingCouplingCouplingCouplingCoupling-CouplingCouplingCouplingCouplingCouplingCouplingCouplingCouplingCoupling-CouplingCouplingCouplingCouplingCouplingCouplingCouplingCouplingCoupling-CouplingCouplingCouplingCouplingCouplingCouplingCouplingCouplingCoupling-CouplingCouplingCouplingCouplingCouplingCouplingCouplingCouplingCoupling-CouplingCouplingCouplingCouplingCouplingCouplingCouplingCouplingCoupling-CouplingCouplingCouplingCouplingCouplingCouplingCouplingCouplingCoupling-CouplingCouplingCouplingCouplingCouplingCouplingCouplingCouplingCoupling-CouplingCouplingCouplingCouplingCouplingCouplingCouplingCouplingCoupling-CouplingCouplingCouplingCouplingCouplingCouplingCouplingCouplingCoupling-"}], "formulas": [{"formula_id": "formula_1", "formula_text": "KL(P |R \u03bb ) = x P (x) log P (x) R \u03bb (x) ,(2)", "formula_coordinates": [3.0, 233.53, 641.88, 270.47, 26.35]}, {"formula_id": "formula_2", "formula_text": "x i )} N i=1 and {M (x i )} N i=1 .", "formula_coordinates": [4.0, 216.34, 321.86, 104.22, 12.32]}, {"formula_id": "formula_3", "formula_text": "P (j) = 1 N N i=1 I \u03c6(x i ) = j ,(3)", "formula_coordinates": [4.0, 245.18, 386.5, 258.82, 30.32]}, {"formula_id": "formula_4", "formula_text": "|EQ[log R(x)] \u2212 EP [log R(x)]| (a single point inside C(P, Q))", "formula_coordinates": [5.0, 381.99, 111.65, 116.52, 18.02]}, {"formula_id": "formula_5", "formula_text": "EP [Q(x)]", "formula_coordinates": [5.0, 382.28, 170.32, 37.74, 8.37]}, {"formula_id": "formula_6", "formula_text": "P nuc,p (x t | x 1:t ), if x t \u2208 V p , 0, else.", "formula_coordinates": [6.0, 287.85, 523.82, 120.86, 24.57]}, {"formula_id": "formula_7", "formula_text": "\u03bb KL(P |S) +\u03bb KL(Q|S) = i \u03bbP i log P i +\u03bbQ i log Q i \u2212 R \u03bb,i log S i = i \u03bbP i log P i R \u03bb,i +\u03bbQ i log Q i R \u03bb,i + R \u03bb,i log R \u03bb,i S i = \u03bb KL(P |R \u03bb ) +\u03bb KL(Q|R \u03bb ) + KL(R \u03bb |S) .", "formula_coordinates": [15.0, 146.51, 564.98, 317.28, 64.96]}, {"formula_id": "formula_8", "formula_text": "F(P, Q) = c \u22121 log t \u22121 1 , c \u22121 log t \u22121 2 : (t 1 , t 2 ) \u2208 C(P, Q) .", "formula_coordinates": [15.0, 177.63, 709.48, 256.73, 13.03]}, {"formula_id": "formula_9", "formula_text": "T ppl (P ) \u2212 T ppl (Q) = exp \u2212 E P [log R(x)] \u2212 exp \u2212 E Q [log R(x)] =", "formula_coordinates": [16.0, 147.19, 175.78, 290.11, 30.66]}, {"formula_id": "formula_10", "formula_text": "T ppl (P ) \u2212 T ppl (Q) = C 1 exp KL(P |R) \u2212 C 2 exp KL(Q|R) ,", "formula_coordinates": [16.0, 169.95, 371.39, 275.42, 9.81]}, {"formula_id": "formula_11", "formula_text": "P S (j) = P (S j ) .", "formula_coordinates": [16.0, 271.93, 498.25, 68.14, 9.65]}, {"formula_id": "formula_12", "formula_text": "P i } N i=1 , model text {x Q i } N i=1 , number of clusters k, embedding model M , discretization \u039b of [0, 1]. Output: MAUVE(P, Q). // Embed the samples {M (x P i )} N i=1 , {M (x Q i )} N i=1 \u2190 embed M, {x P i } N i=1 , {x Q i } N i=1", "formula_coordinates": [17.0, 108.0, 91.58, 382.3, 61.85]}, {"formula_id": "formula_13", "formula_text": "C P , C Q = quantize {M (x P i )} N i=1 , {M (x Q i )} N i=1", "formula_coordinates": [17.0, 108.0, 171.63, 209.02, 13.68]}, {"formula_id": "formula_14", "formula_text": "P \u2190 count(C P )/N ,Q \u2190 count(C Q )/N", "formula_coordinates": [17.0, 108.0, 203.98, 176.35, 9.65]}, {"formula_id": "formula_15", "formula_text": "C(P, Q) = exp(\u2212c KL(Q|R \u03bb )), exp(\u2212c KL(P |R \u03bb )) : R \u03bb = \u03bbP + (1 \u2212 \u03bb)Q, \u03bb \u2208 \u039b .(4)", "formula_coordinates": [17.0, 150.34, 341.43, 353.66, 31.77]}, {"formula_id": "formula_16", "formula_text": "\u03b3 t = \u03b3 0 min{1, (2 \u2212 2t/T ) 2 } ,", "formula_coordinates": [19.0, 244.11, 543.08, 123.78, 11.72]}, {"formula_id": "formula_17", "formula_text": "(w 1 , \u2022 \u2022 \u2022 , w n ) = \u2212 n i=1 n j=1", "formula_coordinates": [27.0, 194.28, 477.7, 108.54, 30.32]}, {"formula_id": "formula_18", "formula_text": "(0) 1 , \u2022 \u2022 \u2022 , w (0)", "formula_coordinates": [27.0, 244.3, 539.61, 52.36, 13.95]}, {"formula_id": "formula_19", "formula_text": "u (t) i = log \uf8eb \uf8ed j =i N ij \uf8f6 \uf8f8 \u2212 log \uf8eb \uf8ed j =i N ij + N ji exp( w (t) i ) + exp( w (t) j ) \uf8f6 \uf8f8 followed by the mean normalization w (t+1) i = u (t) i \u2212 1 n n j=1 u (t) j .", "formula_coordinates": [27.0, 108.0, 565.11, 322.61, 89.78]}], "doi": ""}