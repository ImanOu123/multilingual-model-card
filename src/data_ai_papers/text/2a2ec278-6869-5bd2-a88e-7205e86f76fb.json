{"title": "Modeling Interaction via the Principle of Maximum Causal Entropy", "authors": "Brian D Ziebart; J Andrew Bagnell; Anind K Dey", "pub_date": "", "abstract": "The principle of maximum entropy provides a powerful framework for statistical models of joint, conditional, and marginal distributions. However, there are many important distributions with elements of interaction and feedback where its applicability has not been established. This work presents the principle of maximum causal entropy-an approach based on causally conditioned probabilities that can appropriately model the availability and influence of sequentially revealed side information. Using this principle, we derive models for sequential data with revealed information, interaction, and feedback, and demonstrate their applicability for statistically framing inverse optimal control and decision prediction tasks.", "sections": [{"heading": "Introduction", "text": "The principle of maximum entropy (Jaynes, 1957) serves a foundational role in the theory and practice of constructing statistical models, with applicability to statistical mechanics, natural language processing, econometrics, and ecology (Dud\u00edk & Schapire, 2006). Conditional extensions of the principle that consider a sequence of side information (i.e., additional variables that are not predicted, but are related to variables that are predicted), and specifically conditional random fields (Lafferty et al., 2001), have been applied with remarkable success in recognition, segmentation, and classification tasks, and are a preferred tool in natural language processing, and activity recognition. This work extends the maximum entropy approach to conditional probability distributions in settings characterized by interaction with stochastic processes where side information is dynamic, i.e., revealed over time. For example, consider an agent interacting with a stochastic environment. The agent may have a model for the distribution of future states given its current state and possible actions, but, due to stochasticity, it does not know what value a future state will take until after selecting the sequence of actions temporally preceding it. Thus, future states have no causal influence over earlier actions. Conditional maximum entropy approaches are ill-suited for this setting as they assume all side information is available a priori.\nBuilding on the recent advance of the Marko-Massey theory of directed information (Massey, 1990), we present the principle of maximum causal entropy (MaxCausalEnt). It prescribes a probability distribution by maximizing the entropy of a sequence of variables conditioned on the side information available at each time step. This contribution extends the maximum entropy framework for statistical modeling to processes with information revelation, feedback, and interaction. We motivate and apply this approach on decision prediction tasks, where actions stochastically influence revealed side information (the state of the world) with examples from inverse optimal control, multi-player dynamic games, and interaction with partially observable systems.\nThough we focus on the connection to decision making and control in this work, it is important to note that the principle of maximum causal entropy is not specific to those domains. It is a general approach that is applicable to any sequential data where future side information's assumed lack of causal influence over earlier variables is reasonable.", "publication_ref": ["b9", "b5", "b12", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Maximum Causal Entropy", "text": "Motivated by the task of modeling decisions with elements of sequential interaction, we introduce the principle of maximum causal entropy, describe its core theoretical properties, and provide efficient algorithms for inference and learning.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "When faced with an ill-posed problem, the principle of maximum entropy (Jaynes, 1957) prescribes the use of \"the least committed\" probability distribution that is consistent with known problem constraints. This criterion is formally measured by Shannon's information entropy, E Y [\u2212 log P (Y )], and many of the fundamental building blocks of statistics, including Gaussian and Markov random field distributions, maximize this entropy subject to moment constraints.\nIn the presence of side information, X, that we do not desire to model, the standard prescription is to maximize the conditional entropy, E Y,X [\u2212 log P (Y|X)], yielding, for example, the conditional random field (CRF) (Lafferty et al., 2001). Though our intention is to similarly model conditional probability distributions, CRFs assume a knowledge of future side information, X t+1:T , for each Y t that does not match settings with dynamically revealed information. Despite not being originally formulated for such uses, marginalizing over the CRF's joint distribution is possible:\nP (Y t |X 1:t , Y 1:t\u22121 ) \u221d (1) X t+1:T ,Y t+1:T e \u03b8 F (X,Y) P (X t+1:T |X 1:t , Y 1:t\u22121 ),\nin what we refer to as a latent CRF model. However, we argue that entropy-based approaches like this that do not address the causal influence of side information are inadequate for interactive settings.\nIn the context of this paper, we focus on modeling the sequential actions of an agent interacting with a stochastic environment. Thus, we replace the predicted variables, Y, and side information, X, with sequences of action variables, A, and state variables, S.", "publication_ref": ["b9", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Directed Information and Causal Entropy", "text": "The causally conditioned probability (Kramer, 1998) from the Marko-Massey theory of directed information (Massey, 1990) is a natural extension of the conditional probability, P (A|S), to the situation where each A t is conditioned on only a portion of the S variables, S 1:t , rather than the entirety, S 1:T . Following the previously developed notation (Kramer, 1998), the probability of A causally conditioned on S is\nP (A T ||S T ) T t=1 P (A t |S 1:t , A 1:t\u22121 ). (2\n)\nThe subtle, but significant difference from conditional probability, P (A|S) = T t=1 P (A t |S 1:T , A 1:t\u22121 ), serves as the underlying basis for our approach.\nCausal entropy (Kramer, 1998;Permuter et al., 2008),\nH(A T ||S T ) E A,S [\u2212 log P (A T ||S T )] (3) = T t=1\nH(A t |S 1:t , A 1:t\u22121 ), measures the uncertainty present in the causally conditioned distribution. It upper bounds the conditional entropy; intuitively this reflects the fact that additionally conditioning on information from the future (i.e., acausally) only decreases uncertainty. The causal entropy has previously found applicability in the analysis of communication channels with feedback (Kramer, 1998), decentralized control (Tatikonda & Mitter, 2004), sequential investment and online compression with side information (Permuter et al., 2008).\nUsing this notation, any joint distribution can be expressed as P (A, S) = P (A T ||S T )P (S T ||A T \u22121 ). Our approach estimates a policy-the factors, P (A t |S 1:t , A 1:t\u22121 ), of P (A T ||S T )-based on a provided (explicitly or implicitly) distribution of side information P (S T ||A T \u22121 ) = t P (S t |A 1:t\u22121 , S 1:t\u22121 ).", "publication_ref": ["b11", "b13", "b11", "b11", "b16", "b11", "b19", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Maximum Causal Entropy Optimization", "text": "With the causal entropy (Equation 3) as our objective function, we now pose and solve the maximum causal entropy optimization problem. We constrain our distribution to match expected feature functions, F(S, A) with empirical expectations of those same functions,\u1ebc S,A [F(S, A)], yielding the following optimization problem: and \u2200 S1:t,A1:t\u22121 At P (A t |S 1:t , A 1:t\u22121 ) = 1, and given: P (S T ||A T \u22121 ).\nWe assume for explanatory simplicity that feature functions factor as: F(S, A) = t F (S t , A t ), and that state dynamics are first-order Markovian, P (S T ||A T \u22121 ) = t P (S t |A t\u22121 , S t\u22121 ). Theorem 1. The distribution satisfying the maximum causal entropy constrained optimization (Equation 4) has a form defined recursively as:\nP \u03b8 (At|St) = Z A t |S t ,\u03b8 Z S t ,\u03b8(5)\nlog\nZ A t |S t ,\u03b8 = \u03b8 F (St, At) + X S t+1 P (St+1|St, At) log Z S t+1 ,\u03b8 log Z S t ,\u03b8 = log X A t Z A t |S t ,\u03b8 = softmax A t log Z A t |S t ,\u03b8\nwhere softmax x f (x) log x e f (x) .\nProof (sketch). The (negated) primal objective function (Equation 4) is convex in the variables P (A||S) and subject to linear constraints on feature function expectation matching, valid probability distributions, and the non-causal influence of future side information. Differentiating the Lagrangian of the maximum causal entropy optimization (Equation 4), and equating to zero, we obtain the general form:\nP \u03b8 (At|St) \u221d exp n \u03b8 E S,A [F(S, A)|St, At] \u2212 X \u03c4 >t E S,A [log P \u03b8 (A\u03c4 |S\u03c4 )|St, At] o .(6)\nSubstituting the more operational recurrence of Equation 5 into Equation 6verifies the theorem.\nWe note that Theorem 1 relies on strong duality to identify the form of this probability distribution; the sharp version of Slater's condition (Boyd & Vandenberghe, 2004) using the existence of a feasible point in the relative interior ensures this but requires that (1) prescribed feature constraints are achievable, and ( 2) the distribution has full support. The first naturally follows if both model and empirical expectations are taken with respect to the provided model of side information, P (S T ||A T \u22121 ). For technical simplicity in this work, we will further assume full support for the modeled distribution, although relatively simple modifications (e.g., constraints hold within a small deviation ) ensure the correctness of this form in all cases.\nTheorem 2. The gradient of the dual with respect to \u03b8 is \u1ebc S,A [F(S, A)]\u2212E S,A [F(S, A)] , which is the difference between the empirical feature vector given the complete policy, {P (A t |S t )}, and the expected feature vector under the probabilistic model.\nIn many instances, the statistics of interest \u1ebc S,A [F(S, A)]) for the gradient (Theorem 2) are only known approximately as they are obtained from small sample sets. We note that this uncertainty can be rigorously addressed by extending the duality analysis of Dud\u00edk & Schapire (2006), leading to parameter regularization that may be naturally adopted in the causal setting as well.\nTheorem 3. The maximum causal entropy distribution minimizes the worst case prediction log-loss, i.e., inf\nP (A||S) sup P (A T ||S T ) A,SP (A, S) log P (A T ||S T ),\ngivenP (A, S) =P (A T ||S T )P (S T ||A T \u22121 ) and feature expectations EP (S,A) [F(S, A)] when S is sequentially revealed from a known distribution and actions are sequentially predicted using only previously revealed variables.\nTheorem 3 follows naturally from Gr\u00fcnwald & Dawid (2003) and extends their \"robust Bayes\" results to the interactive setting as one justification for the maximum causal entropy approach. The theorem can be understood by viewing maximum causal entropy as a maximin game where nature chooses a distribution to maximize a predictor's perplexity while the predictor tries to minimize it. By duality, the minimax view of the theorem is equivalent. This strong result is not shared when maximizing alternate entropy measures (e.g., conditional or joint entropy) and marginalizing out future side information (as in Equation 1).", "publication_ref": ["b3", "b5", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Inference and Learning Algorithms", "text": "The procedure for inferring decision probabilities in the MaxCausalEnt model based on Theorem 1 is illustrated by Algorithm 1.\nAlgorithm 1 MaxCausalEnt Inference Procedure\n1: for t = T to 1 do 2: if t = T then 3: \u2200A t ,S t log Z A t |S t ,\u03b8 \u2190 \u03b8 F (At, St) 4: else 5: \u2200A t ,S t log Z A t |S t ,\u03b8 \u2190 \u03b8 F (At, St) + ES t+1 [log Z S t+1 ,\u03b8 |St, At] 6: end if 7: \u2200S t log Z S t ,\u03b8 \u2190 softmaxA t log Z A t |S t 8: \u2200A t ,S t P (At|St) \u2190 Z A t |S t ,\u03b8 Z S t ,\u03b8 9: end for\nUsing the resulting action distributions and empirical feature functions,\u1ebc(F), the gradient is obtained by employing Algorithm 2.\nAlgorithm 2 MaxCausalEnt Gradient Calculation\n1: for t = 1 to T do 2: if t = 1 then 3: \u2200S t ,A t DS t ,A t \u2190 P (St)P (At|St) 4: else 5: \u2200S t ,A t DS t ,A t \u2190 P S t\u22121 ,A t\u22121 DS t\u22121 ,A t\u22121 P (At|St\u22121, At\u22121)P (At|St) 6: end if 7: E[F] \u2190 E[F] + P S t ,A t DS t ,A t F (At, St) 8: end for 9: \u2207 \u03b8 log P (\u00c3||S) \u2190\u1ebc[F] \u2212 E[F]\nAs a consequence of convexity, standard gradientbased optimization techniques converge to the maximum likelihood estimate of feature weights,\u03b8.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Graphical Representation", "text": "We extend the influence diagram graphical framework (Howard & Matheson, 1984) as a convenient representation for the MaxCausalEnt variables and their relationships. The structural elements of the repre-  1. We constrain the graph to possess perfect recall 1 . Every graphical representation can then be reduced to the earlier causal entropy form (Equation 4) by marginalizing over each decision's non-parent uncertainty nodes to obtain side information transition dynamics and expected feature functions. Whereas in traditional influence diagrams, the parent-dependent values for decisions nodes that provide the highest expected utility are inferred, in the MaxCausalEnt setting, utilities should be interpreted as potentials for inferring a probability distribution over decisions.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Applications", "text": "We now present a series of applications with increasing complexity of interaction: (1) control with stochastic dynamics;\n(2) multiple agent interaction; and (3) interaction with a partially observable system.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Inverse Optimal Stochastic Control", "text": "Optimal control frameworks, such as Markov decision processes (MDPs) and linear-quadratic regulators (LQRs), provide rich representations of interactions with stochastic systems. Inverse optimal control (IOC) is the problem of recovering a cost function that makes a particular controller or policy (nearly) optimal (Kalman, 1964;Boyd et al., 1994). Recent work has demonstrated that IOC is a powerful technique for modeling the decision-making behavior of intelligent agents in problems as diverse as robotics (Ratliff et al., 2009), personal navigation (Ziebart et al., 2008), and cognitive science (Ullman et al., 2009). Many IOC approaches (Abbeel & Ng, 2004;Ziebart et al., 2008)  Unfortunately, matching feature counts is fundamentally ill-posed-usually no truly optimal policy will achieve those feature counts, but many stochastic policies (and policy mixtures) will satisfy the constraint. Ziebart et al. (2008) resolve this ambiguity by using the classical maximum entropy criteria to select a single policy from all the distributions over decisions that match feature counts. However, for inverse optimal stochastic control (IOSC)-characterized by stochastic state dynamics-their proposed approach is to marginalize over future state variables (Equation 1). For IOSC, the maximum causal entropy approach provides prediction guarantees (Theorem 3) and a softened interpretation of optimal decision theory, while the latent CRF approach provides neither.", "publication_ref": ["b10", "b4", "b18", "b21", "b20", "b0", "b21", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "MDP and LQR Formulations", "text": "In the IOSC problem, side information (states) and decisions (actions) are inter-dependent with the distribution of side information provided by the known dynamics, P (S T ||A T \u22121 ) = t P (S t |S t\u22121 , A t\u22121 ). We employ the MaxCausalEnt framework to model this setting, as shown in Figure 1. (stochastic) value iteration algorithm (Bellman, 1957) for finding the optimal control policy. The relative magnitudes of the action values in the MaxCausalEnt model control the amount of stochasticity in the resulting action policy, \u03c0 \u03b8 (a|s) \u221d e Q soft \u03b8 (a,s) .\nFor the special case where dynamics are linear functions with Gaussian noise, the quadratic Max-CausalEnt model permits a closed-form solution and, given dynamics s t+1 \u223c N (As t + Ba t , \u03a3), Equation 8reduces to: \nQ soft \u03b8 (a t , s t ) = at st B DB + R A DB B DA A DA at st V soft \u03b8 (s t ) = s t (C s,s + Q \u2212 C a,s C \u22121 a,", "publication_ref": ["b2"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Inverse Helicopter Control", "text": "We demonstrate the MaxCausalEnt approach to IOSC on the problem of building a controller for a helicopter with linearized stochastic dynamics. Existing approaches to IOSC (Ratliff et al., 2006;Abbeel & Ng, 2004) have both practical and theoretical difficulties in the presence of imperfect demonstrated behavior, leading to unstable controllers due to large changes in cost weights (Abbeel et al., 2007) or poor predictive accuracy (Ratliff et al., 2006). To test the robustness of our approach, we generated five 100 time step sub-optimal training trajectories (Figure 2) by noisily sampling actions from an optimal LQR controlled designed for hovering using the linearized stochastic helicopter simulator of Abbeel et al. (2007). (2) the optimal controller using the inverse optimal control model; and (3) the optimal controller using the maximum causal entropy model.\nWe contrast between the policies obtained from the maximum margin planning (Ratliff et al., 2006) (la-\ndefined as: softmaxx f (x) log R x e f (x) dx.\nbeled InvOpt in Figure 2) and MaxCausalEnt models trained using demonstrated trajectories. Using the true cost function, we measure the cost of trajectories sampled from the optimal policy under the cost function learned by each model. The InvOpt model performs poorly because there is no optimal trajectory for any cost function that matches demonstrated features.\nIn contrast, by design the MaxCausalEnt model is guaranteed to match the performance of demonstrated behavior (Demo) in expectation even if that behavior is sub-optimal. Additionally, because of the quadratic cost function, the optimal controller using the Max-CausalEnt cost function is always at least as good as the demonstrated behavior on the original, unknown cost function, and often better, as shown in Figure 2.\nIn this sense, MaxCausalEnt provides a rigorous approach to learning a cost function for such stochastic optimal control problems: it is both predictive and can guarantee good performance of the learned controller.", "publication_ref": ["b17", "b0", "b1", "b17", "b1", "b17"], "figure_ref": ["fig_3", "fig_3", "fig_3"], "table_ref": []}, {"heading": "Inverse Dynamic Games", "text": "Modeling the interactions of multiple agents is an important task for uncovering the motives of negotiating parties, planning a robot's movement in a crowded environment, and assessing the perceived roles of interacting agents (Ullman et al., 2009). While game and decision theory can prescribe equilibria or optimal policies when the utilities of agents are known, often the utilities are not known and only observed behavior is available for modeling tasks. We investigate recovering the agents' utilities from those observations.", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "Markov Game Formulation", "text": "We consider a Markov game where agents act in sequence, taking turns after observing the preceding agents' actions and the resulting stochastic outcome sampled from known state dynamics, P (S t+1 |A t , S t ).\nAgents are assumed to act based on a utility function that is linear in features associated with each state, w i f S and to know the other agents' utilities and policies.\nLearning a single agent's MaxCausalEnt policy, \u03c0 i , given the others' policies, \u03c0 \u2212i , reduces to an IOSC problem where the entropy of the agent's actions given state is maximized while matching state features:\nargmax \u03c0i(A|S) H(A (i) ||S (i) )(9)\nsuch that:\nE[ t f i (S t )] =\u1ebc[ t f i (S t )]\nand given: \u03c0 \u2212i (A|S) and P (S||A).\nThe distribution of side information (i.e., the agent's next state, S t+N given the agent's previous state and action, S t and A t , is obtained by marginalizing over the other agents' states and actions:\nP (S t+N |A t , S t ) = E S t+1:t+N \u22121 ,A t+1:t+N \u22121 P (S t+N |S t+N \u22121 , A t+N \u22121 ) S t , A t .\nDifficulties arise, however, because the policies of other agents are not known in our setting. Instead, all of the agents' utilities and policies are learned from demonstrated trajectories. Maximizing the joint causal entropy of all agents' actions leads either to nonconvexity (multiplicative functions of agent policies in the feature matching constraints) or an assumption that agents share a common utility function.\nWe settle for potentially non-optimal solution policies by employing a cyclic coordinate descent approach. It iteratively maximizes the causal entropy of agent i's actions subject to constraints:\n\u03c0 i \u2190 argmax \u03c0i F p (\u03c0 i |\u03c0 \u2212i ),\nwhere F p is the Lagrangian primal of Equation 9. However, instead of matching observed feature counts, which may be infeasible given the estimate of\u03c0 \u2212i , the expectation of agent i's features given its empirical actions under the current estimate of agent policies, F(S, A) = E A,S [ S\u2208S f S |\u03c0], is matched.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Pursuit-Evasion Modeling", "text": "We consider a generalization of the pursuit-evasion multi-agent setting (Parsons, 1976) with three agents operating in a four-by-four grid world (Figure 3). Each agent has a mobility, m i \u2208 [0.2, 1], which corresponds to the probability of success when attempting to move in one of the four cardinal directions, and a utility, w i,j \u2208 [\u22121, 1], for being co-located with each of the other agents. Unlike traditional pursuer-evader, there is no \"capture\" event in this game; agents continue to act after being co-located. The mobilities of each agent and a time sequence of their actions and locations are provided and the task is to predict their future actions.\nFigure 3. The pursuit-evasion grid with three agents and their co-location utilities. Agent X has a mobility of mX and a utility of wX,Y when co-located with agent Y .\nWe generate data for this setting using the following procedure. First, mobilities and co-location utilities are sampled (uniformly from their domain). Next, optimal policies 3 , \u03c0 t * i (A|S), for a range of time horizons t \u2208 {T 0 , ..., T 0 + \u2206T }, are computed with complete knowledge of other agents' utilities and future policies. A stochastic policy,\u03c0 i (A|S), is obtained by first sampling a time horizon from P (t) = 1 \u2206T , and then sampling an action from the optimal policy for that time horizon. Lastly, starting from random initial locations, trajectories of length 40 time steps (five for training and one for testing) are sampled from the policy and state dynamics for six different parameters. Despite its simplicity, this setting produces surprisingly rich behavior. For example, under certain optimal policies, a first evader will help its pursuer corner a more desirable second evader so that the first evader will be spared.", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "MaxCausalEnt perplexity", "text": "Latent CRF perplexity A comparison between the latent CRF model (Equation 1) trained to maximize data likelihood and the maximum causal entropy model is shown in Figure 4 using perplexity, 1 T at,st log P (a t |s t ), as the evaluation metric of predictive performance. MaxCausalEnt consistently outperforms the latent CRF model. The explanation for this is based on the observation that under the latent CRF model, the action-value of Equation 7 is instead: Q(a t , s t ) = softmax st+1 (V (s t+1 ) + log P (s t+1 |s t , a t )). This has a disconcerting interpretation that the agent chooses the next state by \"paying\" an extra log P (s t+1 |s t , a t ) penalty to ignore the true stochasticity of the state transition dynamics, resulting in an unrealistic probability distribution.", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Inverse Diagnostics", "text": "Many important interaction tasks involve partial observability. In medical diagnosis, for example, sequences of symptoms, tests, and treatments are used to identify (and mediate) unknown illnesses. Motivated by the objective of learning diagnosis policies from experts, we investigate the inverse diagnostics problem of modeling interaction with partially observed systems.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Bayes Net Diagnosis Formulation", "text": "We consider a set of variables (distributed according to a known dynamic Bayesian network) that evolve over time based in part on employed actions, A 1:T , as shown in Figure 5. Those actions are made with only partial knowledge of the Bayes net variables, as relayed through observation variables O 1:T . Previous actions determine what information from the hidden variables is revealed in the next time step. We assume that the utility for the state of the Bayes net and actions invoked is an unknown linear function of known feature vectors, \u03b8 f BNt,At . We formulate the modeling problem as a maximum causal entropy optimization by maximizing the causally conditioned entropy of actions given observations, H(A T ||O T ). We marginalize over the latent Bayes net variables to obtain side information dynamics, P (O t+1 |O t , S t ) = E BN1:t [P (O t+1 |par(O t+1 |O 1:t , A 1:t ] and expected feature functions, E[f t |O 1:t , A 1:t ] = E BN1:t [F Ut (par(U t ))|O 1:t , A 1:t ] to estimate the fullhistory policy, P (A t |O 1:t , A 1:t ) using Algorithm 1 and Algorithm 2.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Vehicle Diagnosis Experiments", "text": "We apply our inverse diagnostics approach to the vehicle fault detection Bayesian network (Heckerman et al., 1994) shown in Figure 6 with fully specified condi- tional probability distributions. Apart from the relationship between Battery Age and Battery (exponentially increasing probability of failure with battery age), the remaining conditional probability distributions are deterministic-or's (i.e., failure in any parent causes a failure in the child).\nA mechanic can either test a component of the vehicle (revealing its status) or repair a component (making it and potentially its descendants operational). Replacements and tests are both characterized by three action features: (1) a cost to the vehicle owner; (2) a profit for the mechanic; and (3) a time requirement. Ideally the sequence of mechanic's actions would minimize the expected cost to the vehicle owner, but an over-booked mechanic might instead choose to minimize the total repair time, and a less ethical mechanic might seek to optimize personal profit.\nTo generate a dataset of observations and replacements, a stochastic policy is obtained by adding Gaussian noise, s,a , to each action's future expected value, Q * (s, a), under the optimal policy for a fixed set of feature weights and the highest noisy-valued action, Q * (s, a) + s,a , is selected at each time step. Different vehicle failure samples are generated from the Bayesian network conditioned on the vehicle's engine failing to start, and the stochastic policy is sampled until the vehicle is operational.\nIn Figure 7, we evaluate the prediction error rate and perplexity of our model and a Markov model that ignores the underlying mechanisms for decision making and simply predicts behavior in proportion to the frequency it has previously been observed (with small pseudo-count priors). The MaxCausalEnt approach consistently outperforms the Markov model even with an order of magnitude less training data. The classification error rate quickly reaches the limit implied by the stochasticity of the data generation process. ", "publication_ref": ["b7"], "figure_ref": ["fig_6", "fig_7"], "table_ref": []}, {"heading": "Conclusion and Future Work", "text": "We extended the principle of maximum entropy to settings with sequentially revealed information in this work. We demonstrated the applicability of the resulting principle of maximum causal entropy for learning policies in stochastic control, multi-agent interaction, and partially observable settings. In addition to further investigating modeling applications, our future work will investigate the applicability of Max-CausalEnt on non-modeling tasks in dynamics settings. For instance, we note that the proposed principle provides a natural criteria for efficiently identifying a correlated equilibrium in dynamic Markov games, generalizing the approach to normal-form games of Ortiz et al. (2007).", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "The authors gratefully acknowledge the Richard King Mellon Foundation, the Quality of Life Technology Center, and the Office of Naval Research MURI N00014-09-1-1052 for support of this research. We also thank our reviewers for many useful suggestions.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Apprenticeship learning via inverse reinforcement learning", "journal": "", "year": "2004", "authors": "P Abbeel; A Y Ng"}, {"ref_id": "b1", "title": "An application of reinforcement learning to aerobatic helicopter flight", "journal": "", "year": "2007", "authors": "P Abbeel; A Coates; M Quigley; A Y Ng"}, {"ref_id": "b2", "title": "A Markovian decision process", "journal": "Journal of Mathematics and Mechanics", "year": "1957", "authors": "R Bellman"}, {"ref_id": "b3", "title": "Convex Optimization", "journal": "Cambridge University Press", "year": "2004", "authors": "S Boyd; L Vandenberghe"}, {"ref_id": "b4", "title": "Linear Matrix Inequalities in System and Control Theory", "journal": "Studies in Applied Mathematics", "year": "1994", "authors": "S Boyd; L Ghaoui;  El; E Feron; V Balakrishnan"}, {"ref_id": "b5", "title": "Maximum entropy distribution estimation with generalized regularization", "journal": "", "year": "2006", "authors": "M Dud\u00edk; R E Schapire"}, {"ref_id": "b6", "title": "Game theory, maximum entropy, minimum discrepancy, and robust Bayesian decision theory", "journal": "Annals of Statistics", "year": "2003", "authors": "P D Gr\u00fcnwald; A P Dawid"}, {"ref_id": "b7", "title": "Troubleshooting under uncertainty", "journal": "", "year": "1994", "authors": "D Heckerman; J S Breese; K Rommelse"}, {"ref_id": "b8", "title": "Influence diagrams", "journal": "", "year": "1984", "authors": "R A Howard; J E Matheson"}, {"ref_id": "b9", "title": "Information theory and statistical mechanics", "journal": "Physical Review", "year": "1957", "authors": "E T Jaynes"}, {"ref_id": "b10", "title": "When is a linear control system optimal?", "journal": "Trans. ASME, J. Basic Engrg", "year": "1964", "authors": "R Kalman"}, {"ref_id": "b11", "title": "Directed Information for Channels with Feedback", "journal": "", "year": "1998", "authors": "G Kramer"}, {"ref_id": "b12", "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "journal": "", "year": "2001", "authors": "J Lafferty; A Mccallum; F Pereira"}, {"ref_id": "b13", "title": "Causality, feedback and directed information", "journal": "", "year": "1990", "authors": "J L Massey"}, {"ref_id": "b14", "title": "Maximum entropy correlated equilibria", "journal": "", "year": "2007", "authors": "L E Ortiz; R E Shapire; S M Kakade"}, {"ref_id": "b15", "title": "Pursuit-evasion in a graph", "journal": "Springer-Verlag", "year": "1976", "authors": "T D Parsons"}, {"ref_id": "b16", "title": "On directed information and gambling", "journal": "", "year": "2008", "authors": "H H Permuter; Y.-H Kim; T Weissman"}, {"ref_id": "b17", "title": "Maximum margin planning", "journal": "", "year": "2006", "authors": "N Ratliff; J A Bagnell; M Zinkevich"}, {"ref_id": "b18", "title": "Learning to search: Functional gradient techniques for imitation learning", "journal": "Auton. Robots", "year": "2009", "authors": "N Ratliff; D Silver; J A Bagnell"}, {"ref_id": "b19", "title": "Control under communication constraints. Automatic Control", "journal": "IEEE Transactions on", "year": "2004-07", "authors": "S Tatikonda; S Mitter"}, {"ref_id": "b20", "title": "Help or hinder: Bayesian models of social goal inference", "journal": "", "year": "2009", "authors": "T Ullman; C Baker; O Macindoe; O Evans; N Goodman; J Tenenbaum"}, {"ref_id": "b21", "title": "Maximum entropy inverse reinforcement learning", "journal": "", "year": "2008", "authors": "B D Ziebart; A Maas; J A Bagnell; A K Dey"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "E S,A [F(S, A)] =\u1ebc S,A [F(S, A)]", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 .1Figure1. The graphical representation for MaxCausalEnt inverse optimal control. For MDPs: Ut(St, At) = \u03b8 fS t , and for LQRs: Ut(st, at) = s t Qst + a t Rat.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "a C a,s )s t + const, where C and D are recursively computed as: C a,a = B DB; C s,a = C a,s = B DA; C s,s = A DA; and D = C s,s + Q \u2212 C C \u22121 a,a C a,s .", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 .2Figure 2. Left: An example sub-optimal helicopter trajectory attempting to hover around the origin point. Right: The average cost under the original cost function of: (1) demonstrated trajectories;(2) the optimal controller using the inverse optimal control model; and (3) the optimal controller using the maximum causal entropy model.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 4 .4Figure 4. The average per-action perplexities of the latent CRF model and the MaxCausalEnt model plotted against each other for three agents from six different pursuitevasion settings. The MaxCausalEnt model outperforms the latent CRF model in the region below the dotted line.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 .5Figure 5. The MaxCausalEnt representation of the diagnostic problem with an abstract dynamic Bayesian network represented as BN nodes. Perfect recall edges from all past observations and actions to future actions are suppressed.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 .6Figure 6. The vehicle fault detection Bayesian network with replaceable variables denoted with an asterisk. All variables are binary (working or not) except Battery Age.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 7 .7Figure 7. Error rate and perplexity of the MaxCausalEnt model and Markov model for diagnosis action prediction as training set size (log-scale) increases.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Graphical representation structural elements.", "figure_data": "TypeSymbol Parent relationshipDecisionSpecifies observed vari-ables when A is selectedUncertaintySpecifies probability, P (S|par(S)) conditionalUtilitySpecifies feature func-tions, \u03b8 F (par(U )) \u2192sentation are outlined in Table"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "consider cost functions linear in a set of features and attempt to find behaviors that induce the same feature counts as the policy to be mimicked(E[ t f St ] =\u1ebc[ t f St ]); by linearity such behaviors achieve the same expected value. For settings with vectors of continuous states and actions, matching quadratic moments, e.g., E[ t s t s t ] =\u1ebc[ t s t , s t ], guarantees equivalent performance under quadratic cost functions, e.g., t s t Qs t for unknown Q.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "P (Y t |X 1:t , Y 1:t\u22121 ) \u221d (1) X t+1:T ,Y t+1:T e \u03b8 F (X,Y) P (X t+1:T |X 1:t , Y 1:t\u22121 ),", "formula_coordinates": [2.0, 59.92, 349.79, 229.52, 38.81]}, {"formula_id": "formula_1", "formula_text": "P (A T ||S T ) T t=1 P (A t |S 1:t , A 1:t\u22121 ). (2", "formula_coordinates": [2.0, 93.52, 644.69, 191.68, 30.2]}, {"formula_id": "formula_2", "formula_text": ")", "formula_coordinates": [2.0, 285.2, 655.1, 4.24, 8.74]}, {"formula_id": "formula_3", "formula_text": "H(A T ||S T ) E A,S [\u2212 log P (A T ||S T )] (3) = T t=1", "formula_coordinates": [2.0, 342.78, 87.61, 198.67, 47.84]}, {"formula_id": "formula_4", "formula_text": "P \u03b8 (At|St) = Z A t |S t ,\u03b8 Z S t ,\u03b8(5)", "formula_coordinates": [2.0, 308.73, 641.26, 232.71, 21.7]}, {"formula_id": "formula_5", "formula_text": "Z A t |S t ,\u03b8 = \u03b8 F (St, At) + X S t+1 P (St+1|St, At) log Z S t+1 ,\u03b8 log Z S t ,\u03b8 = log X A t Z A t |S t ,\u03b8 = softmax A t log Z A t |S t ,\u03b8", "formula_coordinates": [2.0, 310.27, 666.66, 229.19, 48.38]}, {"formula_id": "formula_6", "formula_text": "P \u03b8 (At|St) \u221d exp n \u03b8 E S,A [F(S, A)|St, At] \u2212 X \u03c4 >t E S,A [log P \u03b8 (A\u03c4 |S\u03c4 )|St, At] o .(6)", "formula_coordinates": [3.0, 63.2, 178.54, 226.24, 41.95]}, {"formula_id": "formula_7", "formula_text": "P (A||S) sup P (A T ||S T ) A,SP (A, S) log P (A T ||S T ),", "formula_coordinates": [3.0, 78.21, 623.0, 188.46, 22.41]}, {"formula_id": "formula_8", "formula_text": "1: for t = T to 1 do 2: if t = T then 3: \u2200A t ,S t log Z A t |S t ,\u03b8 \u2190 \u03b8 F (At, St) 4: else 5: \u2200A t ,S t log Z A t |S t ,\u03b8 \u2190 \u03b8 F (At, St) + ES t+1 [log Z S t+1 ,\u03b8 |St, At] 6: end if 7: \u2200S t log Z S t ,\u03b8 \u2190 softmaxA t log Z A t |S t 8: \u2200A t ,S t P (At|St) \u2190 Z A t |S t ,\u03b8 Z S t ,\u03b8 9: end for", "formula_coordinates": [3.0, 311.33, 303.62, 230.11, 103.05]}, {"formula_id": "formula_9", "formula_text": "1: for t = 1 to T do 2: if t = 1 then 3: \u2200S t ,A t DS t ,A t \u2190 P (St)P (At|St) 4: else 5: \u2200S t ,A t DS t ,A t \u2190 P S t\u22121 ,A t\u22121 DS t\u22121 ,A t\u22121 P (At|St\u22121, At\u22121)P (At|St) 6: end if 7: E[F] \u2190 E[F] + P S t ,A t DS t ,A t F (At, St) 8: end for 9: \u2207 \u03b8 log P (\u00c3||S) \u2190\u1ebc[F] \u2212 E[F]", "formula_coordinates": [3.0, 311.33, 486.3, 231.05, 99.9]}, {"formula_id": "formula_10", "formula_text": "Q soft \u03b8 (a t , s t ) = at st B DB + R A DB B DA A DA at st V soft \u03b8 (s t ) = s t (C s,s + Q \u2212 C a,s C \u22121 a,", "formula_coordinates": [5.0, 60.38, 202.25, 213.93, 33.11]}, {"formula_id": "formula_11", "formula_text": "defined as: softmaxx f (x) log R x e f (x) dx.", "formula_coordinates": [5.0, 55.44, 707.17, 174.22, 12.29]}, {"formula_id": "formula_12", "formula_text": "argmax \u03c0i(A|S) H(A (i) ||S (i) )(9)", "formula_coordinates": [5.0, 381.65, 623.9, 159.79, 19.32]}, {"formula_id": "formula_13", "formula_text": "E[ t f i (S t )] =\u1ebc[ t f i (S t )]", "formula_coordinates": [5.0, 388.07, 652.22, 120.68, 19.64]}, {"formula_id": "formula_14", "formula_text": "P (S t+N |A t , S t ) = E S t+1:t+N \u22121 ,A t+1:t+N \u22121 P (S t+N |S t+N \u22121 , A t+N \u22121 ) S t , A t .", "formula_coordinates": [6.0, 82.96, 107.53, 173.25, 31.57]}, {"formula_id": "formula_15", "formula_text": "\u03c0 i \u2190 argmax \u03c0i F p (\u03c0 i |\u03c0 \u2212i ),", "formula_coordinates": [6.0, 118.9, 312.8, 107.07, 16.07]}], "doi": "10.1109/TAC.2004.831187"}