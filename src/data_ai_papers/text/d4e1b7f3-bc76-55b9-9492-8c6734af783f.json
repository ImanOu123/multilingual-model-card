{"title": "LONGEVAL: Guidelines for Human Evaluation of Faithfulness in Long-form Summarization", "authors": "Kalpesh Krishna; Erin Bransom; Bailey Kuehl; \u2662 Mohit; Iyyer \u2660 Pradeep; Dasigi \u2662 Arman; Cohan \u2662\u2661; Kyle Lo", "pub_date": "", "abstract": "While human evaluation remains best practice for accurately judging the faithfulness of automatically-generated summaries, few solutions exist to address the increased difficulty and workload when evaluating long-form summaries. Through a survey of 162 papers on long-form summarization, we first shed light on current human evaluation practices surrounding long-form summaries. We find that 73% of these papers do not perform any human evaluation on model-generated summaries, while other works face new difficulties that manifest when dealing with long documents (e.g., low inter-annotator agreement). Motivated by our survey, we present LONGEVAL, a set of guidelines for human evaluation of faithfulness in long-form summaries that addresses the following challenges: (1) How can we achieve high inter-annotator agreement on faithfulness scores? (2) How can we minimize annotator workload while maintaining accurate faithfulness scores? and (3) Do humans benefit from automated alignment between summary and source snippets? We deploy LONGEVAL in annotation studies on two long-form summarization datasets in different domains (SQuALITY and PubMed), and we find that switching to a finer granularity of judgment (e.g., clause-level) reduces inter-annotator variance in faithfulness scores (e.g., std-dev from 18.5 to 6.8). We also show that scores from a partial annotation of fine-grained units highly correlates with scores from a full annotation workload (0.89 Kendall's \u03c4 using 50% judgments). We release our human judgments, annotation templates, and our software for future research.", "sections": [{"heading": "Introduction", "text": "Human judgments are considered the gold standard for evaluating model-generated sum-1 https://github.com/martiansideofthemoon/ longeval-summarization *Work done during in an internship at AI2. Details of individual author contributions can be found here.\nmaries (Kryscinski et al., 2019;Fabbri et al., 2021) and generated text more broadly (Celikyilmaz et al., 2020). Unfortunately, human evaluation tends to be labor-intensive, expensive to scale, and difficult to design. This is problematic as a large number of judged examples is needed to draw statistically significant conclusions about system performances (Wei and Jia, 2021) or correlations between human judgments and automatic metrics (Deutsch et al., 2021). Human evaluation is especially challenging when long sequences of generated text need to be evaluated, due to the inherent subjectivity in the task (Karpinska et al., 2021;Clark et al., 2021;Krishna et al., 2021;Goyal et al., 2022).\nTo better understand the challenges of human evaluation on long-form summaries (150 words or longer), we first conduct a comprehensive survey of 162 publications and preprints on long-form summarization (Section 2). We find that 119 papers (73%) do not perform human evaluation on long-form summaries, while the remaining papers deviate significantly from suggested best practices for reproducibility (Gehrmann et al., 2022). Current human evaluation setups lack standardization in their design decisions (such as annotation granularity), some of which can significantly impact inter-annotator agreement (Section 3.1). Finally, 20 papers explicitly mention human evaluation is expensive, difficult, and time-consuming due to the long length of summaries and source documents.\nTo move towards a more consistent and efficient human evaluation, we present LONGEVAL, a set of guidelines for human evaluation of faithfulness in long-form summarization (Section 3). We empirically evaluate LONGEVAL using human annotation studies on two long-form summarization datasets: SQuALITY  and PubMed (Cohan et al., 2018). We provide an overview of our main research questions and findings in Figure 1 and enumerate them here: \u2026. He recognized her as old Hazeltyne 's daughter Harriet , no doubt come to see justice done . She did n't have the hothouse -flower look Asa would have expected in a girl whose father owned the most valuable of the planetary franchises . She was not afraid to meet his eye , the eye of a judicially certified criminal . There was , perhaps , a crease of puzzlement in her brow , as if she had thought crimes were committed by shriveled , rat -faced types , and not by young biological engineers who still affected crewcuts . Tom Dorr , Hazeltyne 's general manager , was her escort . Asa felt certain , without proof , that Dorr was the man who had framed him for the charge of grand theft by secreting a fresh Slider egg in his laboratory . The older man stared at Asa coldly as he was led out of the courtroom and down the corridor back to jail . Jumpy , Asa 's cellmate , took one look at his face as he was put back behind bars . \" Guilty , \" Jumpy said \u2026.Asa took four steps to the far wall of the cell , stood there briefly with his head bent and turned to face Jumpy . \" Nope , \" Asa said softly . \" I 'm going into a conversion tank . I 'm going to be a muck man , Jumpy . I 'm going out to Jordan 's Planet and hunt Slider eggs . \" \" Smuggling ? It wo n't work . \" Asa did n't answer . The Hazeltyne company had gone after him because he had \u2026 Asa Graybar is a biological engineer who studies keeping Slider eggs alive and he is accused of a crime at the opening of the story . He thinks he was framed by Tom Dorr , Hazeltyne 's general manager . He was offered one year as a \" changeling \" on another planet or 5 years in rehabilitation on Earth . He elects to do the one year , and thinks that he will get into smuggling Slider eggs on Jordan 's planet \u2026..  RQ1: Can inter-annotator agreement be improved while evaluating faithfulness of long-form summaries via fine-grained annotations?\nFinding: Annotating faithfulness of individual summary clauses and aggregating them leads to significantly higher inter-annotator agreement, compared to the dominant paradigm of evaluating whole summaries at once via Likert ratings (std-dev 18.5 to 6.8 on SQuALITY).\nRQ2: Can we reduce annotator workload by partially annotating a long summary while maintaining accurate faithfulness scores?\nFinding: Despite annotating a fraction of summary clauses, faithfulness scores under a reduced workload maintain high correlation with those from a full workload (0.89 Kendall's \u03c4 at 50% workload).\nRQ3: Do humans benefit from automatically aligning summary units to relevant sentences in the source document?\nFinding: Unlike suggestions in prior work on shortform summarization (Hardy et al., 2019;Kryscinski et al., 2020), aligning parts of the summary to source document is only useful when the summary is highly extractive or mostly correct.\nOverall, our contributions are:\n(1) a 162-paper survey of current human evaluation practices in long-form summarization;\n(2) LONGEVAL, a set of three guidelines for evaluating faithfulness in long-form summarization;\n(3) an empirical validation of LONGEVAL guide-lines on two long-form summarization datasets in different domains (SQuALITY and PubMed);\n(4) A dataset with 3-way fine-grained human faithfulness judgments for 120 SQuALITY & PubMed summaries annotated using LONGEVAL which can be used for benchmarking automatic metrics. We open-source our human evaluation data, annotation interface, and code for future research. 1", "publication_ref": ["b33", "b15", "b3", "b62", "b7", "b5", "b32", "b21", "b19", "b6", "b24", "b34"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Survey of human evaluation practices", "text": "Before discussing LONGEVAL, we first attempt to understand current human evaluation practices in long-form summarization through a comprehensive survey of 162 papers. Our survey reveals several concerning trends: absence of human evaluation, non-reproducible experimental setups, lack of standardization, and complaints of long summaries being challenging and expensive to evaluate. These results show an urgent need to develop more efficient and standardized human evaluation protocols.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Selection of papers:", "text": "We consider existing summarization datasets with an average summary length of at least 150 words, which includes several popular datasets like arXiv (Cohan et al., 2018), Bill-Sum (Kornilova and Eidelman, 2019) and Multi-News (Fabbri et al., 2019); see Table 1 for a full list. For our survey, we select all papers that evaluated summarization models using at least one of these datasets. 2 All of these papers were published between June 2018 and September 2022, after the first long-form summarization datasets were released (PubMed / arXiv). Most of the 162 surveyed papers were published in major NLP/ML venues, but we also include newer preprints from 2022.\nLong-form summaries are rarely evaluated by humans. We find that 101 out of 162 papers (62%) do not perform any human evaluation. 17 papers (11%) only perform human evaluation on short summaries (datasets like XSUM, Narayan et al., 2018), for which human evaluation is much easier.\nHuman evaluation studies of long-form summaries are not reproducible. We further analyze the 44 papers performing human evaluation of longform summaries to observe how often they follow reproducible practices from Gehrmann et al. (2022). Overall, we find that most studies do not follow these guidelines. Only 2 of the 44 papers release their raw human annotation data for further analysis. Only 9 papers provide details of their annotator instructions or interface, and just 12 papers perform any kind of statistical analysis, despite most papers annotating less than 50 summaries. While 33 papers report using multiple annotators per summary, only 12 report inter-annotator agreement. Finally, just 14 papers conduct human evaluation on more than one dataset (more statistics in Appendix C).\nExisting human evaluation setups lack standardization. In Table 2, we catalog the wide spectrum of human evaluation setups in the surveyed papers. 37 papers collect judgments of the full-length summary at once (\"COARSE-grained\"), while 6 papers collect judgments at a finer granularity such as sentences or entities (\"FINE-grained\"). Even within a granularity, setups differ: Likert-scale (24 papers), A/B testing (13 papers), binary per-sentence labels (4 papers) are the dominant protocols. In Section 3.1, we will see that this design decision is critical since COARSE annotations have much lower inter-annotator agreement than FINE. 3 Human evaluation of long-form summaries is challenging and expensive. Several of the surveyed papers discuss challenges in human evaluation of long-form summaries. 13 papers mention that expert annotators are necessary for human evaluation of long-form summaries, especially in technical domains like PubMed. 20 papers report that human evaluation of long-form summarization was   time-consuming, challenging, and expensive, primarily due to the long length of the summary and source document. To tackle the issue of high annotator workload, we propose a partial annotation method in Section 3.2 and report high correlation to a full workload. Additionally, in Section 3.3 we investigate the usefulness of highlighting sentences to help annotators navigate the long source document. While this has been advocated for in short-form summary evaluation (Hardy et al., 2019;Kryscinski et al., 2020) and used in 3 surveyed long-form papers, we find that it is only helpful when summaries are mostly correct and extractive.", "publication_ref": ["b6", "b31", "b12", "b42", "b19", "b24", "b34"], "figure_ref": [], "table_ref": ["tab_2", "tab_3"]}, {"heading": "The LONGEVAL guidelines for faithfulness human evaluation", "text": "In Section 2, we report several concerning issues with current human evaluation practices in longform summarization. To move towards more efficient, reproducible and standardized protocols for human evaluation, we develop the LONGEVAL guidelines (Section 3.1-3.3, see Figure 1 for an overview). We focus on human evaluation of faithfulness, which  define as:\n\"Checking the factual errors in the summary, where a factual error is a statement that contradicts the source document, or is not directly stated, heavily implied, or logically entailed by the source document\"\nWe conduct human annotation studies to empirically motivate LONGEVAL. Our experiments are on two long-form summarization datasets spanning diverse domains and levels of abstractiveness:\n(1) SQuALITY ) is a summarization dataset in the literary domain (avg. summary length of 227 words) where summaries describe the plots of English science fiction stories. SQuAL-ITY is highly abstractive: on average just 16% of bigrams in the summary are present in the source document. We closely follow the human evaluation setup in , and use BART  and BART-DPR (Karpukhin et al., 2020) as our summarization models along with human-written summaries.\n(2) PubMed (Cohan et al., 2018) is a summarization dataset in the scientific domain (avg. summary length of 205 words) that pairs English biomedical articles from PubMed 4 with their abstracts as summaries. Compared to SQuALITY, PubMed is more extractive: 54% of summary bigrams are present in the source. We use BigBird-PEGASUS-large (Zaheer et al., 2020) and LongT5-large (Guo et al., 2022) as our summarization models, 5 along with human written summaries. By default, LongT5 / BigBird were highly extractive compared to humanwritten PubMed summaries (87% / 74% vs 54% bigram overlap with source). Hence, for half the generations we block 6-grams from being copied from the source, 6 reducing extractiveness to \u223c54%. We call this setting \"PubMed-ngram-block\".", "publication_ref": ["b30", "b6", "b22"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "RQ1: Does inter-annotator agreement improve using fine-grained annotations?", "text": "In Section 2, we found that the dominant paradigm in literature (37 out of 44 papers) is to evaluate the whole summary at once (\"COARSE\"-grained, Figure 1 top left). 6 papers instead obtain finegrained annotations for individual units (e.g., sentences) and average them (FINE, Figure 1 top right).\nIntuitively, FINE annotation has many advantages for longer summaries -it is less subjective than COARSE, since shorter spans needs to be judged rather than a long summary, and it helps localize model errors. However, the distinction between COARSE and FINE is never justified in literature, and inter-annotator agreement is rarely reported to understand the task subjectivity in each setup. To better understand the tradeoff, in this section we conduct human evaluations annotating the same set of summaries using these two different protocols.\nTask formulation: Let F summ denote the faithfulness score of a summary. For COARSE, k-point Likert scale ratings are obtained for the summary (F summ \u2208 {0, 1...k}), based on the faithfulness definition provided earlier. For FINE, we collect binary judgments of individual units in the summary and average them,\nF summ = 1 |C summ | c\u2208Csumm F c , F c \u2208 {0, 1}\nwhere C summ is a set of units in the summary and F c is the faithfulness judgment for the unit c. In both protocols, the faithfulness score of a system is defined as 1", "publication_ref": [], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "|S|", "text": "summ\u2208S F summ where S is the set of summaries generated by the system. 7 While sentences are a popular granularity for FINE (4 of the 6 surveyed papers), we found that summary sentences in both datasets were overloaded with information. Hence, we segment sentences on conjunctions and punctuation to obtain more atomic units as C summ . These units are often clauses, 8 similar to summary content units (SCUs) in Pyramid (Nenkova and Passonneau, 2004).\nCollecting COARSE annotations: For SQuALITY, we re-use the annotations provided by  for faithfulness assessments. In their data, three annotators give each summary a 1-100 direct assessment rating (Bojar et al., 2016). Annotators with experience in professional copyrighting and editing were hired on Upwork, 9 and these annotators were also involved in the creation of SQuAL-ITY. Unfortunately, none of the surveyed papers that reported human evaluation results on PubMed 0. 2 0.0 0.2 0.4 0.6 0.8 1 released their raw human annotations. 10 Hence, we collect our own COARSE evaluations on PubMed summaries on Upwork, using freelancers with professional experience reading and writing research papers (details in Appendix B.2). We collect 3 annotations per summary and use a 5-point Likert scale, the most common choice for COARSE assessment in our survey (18 out of 38 papers). In total, 120 summaries are evaluated.\nCollecting FINE annotations: For both SQuAL-ITY and PubMed, we collect FINE annotations on Upwork (3 annotators per FINE unit) for the same set of 120 summaries evaluated using COARSE annotations. For SQuALITY, we hire freelancers with professional experience in English, creative writing, or education. For PubMed, we hire freelancers with prior experience analyzing biomedical articles. See Appendix B.1 for details of our annotator 10 In our email correspondence with authors of these works, they mentioned losing access or compliance issues as reasons for not sharing human evaluations. We received some examples from Guo et al. (2021) and Ju et al. (2021)   Note that all FINE units of a summary were annotated to obtain these results (f = 1.0 in Section 3.2).\nscreening process, compensation, instructions, and screenshots of our annotation interface.\nFINE annotations have higher inter-annotator agreement than COARSE annotations. This leads to more confident downstream estimates. We present our results in Table 3. Overall, we observe that across all settings, FINE annotations have lower  standard deviation (and thus higher agreement) in faithfulness scores than COARSE annotations (7.8 vs 14.0 average on 100-point scaled ratings). To illustrate the importance of higher agreement, we measure its effect on two downstream statistics that human evaluation is primarily used for: (1) correlation with automatic metrics; and (2) mean system performance. We adapt the bootstrap resampling analysis 11 of Deutsch et al. (2021) to estimate confidence intervals of these two downstream statistics for COARSE and FINE.\nIn Figure 2, we plot the 95% confidence intervals of the Pearson correlation of various automatic evaluation metrics against FINE-grained and COARSE-grained human evaluation data. Across both datasets, FINE data leads to much narrower confidence intervals (0.15 vs 0.35 average uncertainty in Pearson correlation on PubMed) for the same number of summaries, implying higher statistical power. In Figure 3, we observe a similar trend with mean system performance. Interestingly, both annotation methods give the same relative ordering of systems (human > bart-dpr > bart for SQuALITY, human > longT5 > BigBird for PubMed-block), confirming the alignment of FINE and COARSE judgments on average.\nRecommendation: Unlike the dominant trend in prior work, FINE-grained evaluations should be preferred over COARSE grained evaluation for long-form summaries. FINE annotations have lower interannotator variance than COARSE annotations and help localize model errors. In our setup we assume all FINE units are equally weighted while aggregating them to the final summary score. Despite this assumption, in our results we observe a consistent relative ordering of systems/metrics between COARSE and FINE annotations. Nevertheless, nonuniform weighing of units is an interesting future work direction; more in the Limitations section.\n3.2 RQ2: Can we reduce annotator workload by partially annotating a long summary?\nIn Section 3.1, we found that FINE annotations have lower variance than COARSE annotations. However, long summaries may be composed of several units (sentences or phrases) which each require FINE annotation. This could make FINE annotation very expensive for longer summaries (as also noted in our survey). What if we instead annotate a random subset of units from the summary? While this will lower annotation cost, how accurate would these partial annotations be? We explore this tradeoff by re-using the annotations collected in Section 3.1.\nFor every summary, we randomly sample a fraction of units f \u2208 {0.1, 0.2...0.9} and then measure its correlation to the full set of annotations collected. Each annotator gets a different random sample of units for the same summary. In initial experiments, we found that this yielded higher accuracy than when keeping the same set of units per annotator.\nPartial annotation has a high correlation to full annotation, but higher variance: In Figure 4 (left) we plot the segment level Kendall's \u03c4 correlation (relative ordering of summary scores) between a partial annotation and full annotation for different values of f . Overall, we observe a high correlation across different values of f . Despite annotating just half the summary (f = 0.5), in both datasets we observe a high correlation of 0.78-0.89 Kendall's \u03c4 (95% interval) with a full annotation. Does a partial annotation preserve the variance benefits of FINE vs COARSE? In Figure 4 (right) we plot the inter-annotator variance for different values of f . In both datasets we find that a partial annotation has a higher variance than a full annotation. While for all values of f in SQuALITY we find that FINE annotations still have lower variance than COARSE, in PubMed COARSE has lower variance than FINE for f <= 0.3 with 95% confidence.\nRecommendation: Having annotators judge a random subset of units in a long-form summary is a simple way to reduce FINE annotation cost, and has high correlation with a full annotation. However, it increases inter-annotator variance. Annotating 50% of the summary results in 0.78-0.89 Kendall's \u03c4 correlation, with a 30-40% increase in standard deviation compared to full FINE annotation. Partial annotation may be limited in its ability to identify issues in summaries with very few errors. However, we find that this is not the case in current systems, which are abundant in faithfulness errors.", "publication_ref": ["b43", "b2", "b23", "b28", "b7"], "figure_ref": [], "table_ref": ["tab_2", "tab_5"]}, {"heading": "RQ3:", "text": "Is it useful to align summary units to sentences in the source document?\nSo far, we have focused on design decisions on the summary side of evaluation. However, evaluating faithfulness requires a comparison of facts between a summary and a source document. Long-form summaries tend to have long source documents (Table 1): 3.1K words for SQuALITY and 5.1K words for PubMed. In Section 2, we found several mentioned human evaluation is challenging since annotators need to read long source documents. Some prior work has suggested highlighting spans in the source document that align with the summary (Hardy et al., 2019;Kryscinski et al., 2020;Vig et al., 2021) as shown in Figure 1. However, these efforts have exclusively focused on news summarization with relatively short source documents, like CNN/DM (804 words) (Nallapati et al., 2016) or XSUM (438 words) (Narayan et al., 2018).   How useful is highlighting based on alignment, or \"hints\", when the spans are chosen from much longer documents?\nWhat is the best highlighting algorithm? We conduct a study to identify the alignment algorithm best suited for highlighting hints. We manually annotate 125 FINE units from human-written summaries of the SQuALITY validation split, marking the sentences best supporting them from the source document. We then test several candidate methods for linking summary units to the source document. These include token overlap methods like ROUGE (Lin, 2004), retrievers (Karpukhin et al., 2020), and fact verifiers (Wadden et al., 2022). In Table 4, we find that SuperPAL (Ernst et al., 2021), a weakly supervised linking algorithm, performs best (0.61 recall@3 vs the next best 0.47). To improve precision, we filter matches scoring less than 0.3 on SuperPAL, and show at most five highlights.", "publication_ref": ["b24", "b34", "b54", "b41", "b42", "b38", "b30", "b57"], "figure_ref": ["fig_0"], "table_ref": ["tab_2", "tab_8"]}, {"heading": "Do highlighted hints improve summary error detection?", "text": "To answer this question, we manually perturb 50 FINE summary units in SQuALITY validation summaries, introducing entity errors or negations like Kryscinski et al. (2020). We modify the summary context of the perturbed unit to ensure summaries are self-consistent. Annotators\nQuestion & TL;DR response Response Snippets Q: Did you find the highlighted hints useful while making your judgment? TL;DR: 4 out of 5 annotators said Sometimes, 1 said Yes. More useful for SQuALITY, summary units copied verbatim from source, correct summaries.\n\"With summaries that had poor correctness, the hints were often a mess, and even correct spans had to be carefully checked. In summaries that were more correct, I could often just read the span and remember that it was correct, and then the hints helped me find the right source position, or refresh my memory about details.\" \"They were more useful when the summary was a near verbatim source reproduction.\" \"Yes, they were useful. Often they would highlight the exact passage needed to support the summary span.\" \"In PubMed, they were a little more chaotic, even for good summaries.\" \"SQuALITY summaries consisted of sentences or parts of sentences taken straight from the story (wording was exactly as in the text). So hints often lead to the exact place.\" \"For SQuALITY, they were mostly accurate and helpful. For PubMed, they were less accurate and relevant.\"\nQ: Would the highlights have been sufficient to make judgments, or was reading the entire source document necessary? TL;DR: 3 out of 5 annotators said No, 2 said sometimes in SQuALITY. Reading the entire document was critical.\n\"Reading the entire source document was very helpful to understand the basic story plot\" \"Even when the hints were relevant, sometimes they left out information (like character name)...\" \"Initially I tried skimming ... then concluded it's easier to read the entire document first.\" \"With SQuALITY there were cases where almost all of the highlights did not make any sense and nothing of that was even mentioned in the story. With PubMed, it was even more difficult to find hints that support the text\" \"Reading the entire document was essential to understanding the whole process, the hints in isolation were not good enough. The hints and the summary often confused similar objects, especially when pronouns were involved, from different parts of the source. In PubMed a similar thing happened when the source discussed what other papers had done -punctuation, acronyms, and abbreviations played a big role in providing context.\"\nQ: Did you use Ctrl+F searches in the source document while making judgments? TL;DR: 4 out of 5 annotators said Yes, 1 said yes only for PubMed. Ctrl+F helped locate synonyms, entities.\n\"Yes, all the time. It was usually a safer bet than using the hints. The hints are given out of context of the whole SQuALITY story. There were a lot of problems with the PubMed hints involving numbers, which I often searched for. They were very rarely supported by the document, or contained wrong symbols (= instead of >).\" \"Yes, mostly in cases the highlight did not support the summary unit partially or entirely.\" \"I used Ctrl+F when looking for very specific words, like names. Searching was less helpful when it came to words that had synonyms or emotions.\" \"I did Ctrl+F on keywords taken directly from the summary unit as well as synonyms and any specific words that I remembered from the story that could help me get to that place in the source document quickly.\" are shown 50 perturbed and 50 un-perturbed summaries, and asked to annotate whether the summary units are faithful to the source in three settings: 12 (1) no highlighted hints; (2) SuperPAL highlighted hints;\n(3) gold hints manually annotated by us. In Table 5, we show accuracy, inter-annotator agreement, and median time 13 for each setting.\nHighlighted hints have almost no effect in evaluating long-form summaries: Surprisingly, we observe that in all three metrics (accuracy, agreement, median time taken), scores are quite similar across the three settings. In fact, the \"no-hint\" setting scores slightly higher than the SuperPAL hint settings (93% vs 92% accuracy, 0.71 vs 0.64 Fleiss \u03ba) and takes annotators less time (41.4 vs 48.2 seconds per unit). However, we find that hints helped annotate the first few units of a summary quicker (84.6 secs vs 115.6 secs per unit). We attribute our findings to a learning effect over time. FINE annotation of long-form summaries requires annotation of several units for the same document -summary pair. As annotation progresses, annotators get more familiar with the contents of the source document 12 To prevent any bias, each annotator receives only one of these settings for a particular summary.\n13 Calculated using the method in Akoury et al. (2020). and summary, reducing the need for hints over time.\nSee Appendix E for learning trajectory plots.\nQuestionnaire with FINE annotators confirm limited utility of hints: Our evaluation so far is limited to perturbed human summaries. How effective are hints on model-generated summaries? To answer this, we ask five of our FINE Upwork annotators (from Section 3.1) a set of three questions about their experiences using highlighted hints. 14 Detailed questionnaire results along with answer snippets are shown in Table 6. Overall, annotators find hints were useful only sometimes. Hints were less useful when (1) the summary unit was not supported in the source;\n(2) the summary unit was highly abstractive compared to the source; (3) pronouns, numbers, or abbreviations were involved; and (4) Pubmed summaries were annotated. Almost all annotators said it was necessary to read the entire source document before annotation to get an overall idea of the plot and resolve coreferences. Nearly all annotators used \"Ctrl+F\" searches along with hints to search for specific keywords while making judgments. This was especially true when the summary unit was incorrect, since the source document had to be thoroughly searched (beyond the hints) before confidently marking \"Incorrect\".\nRecommendation: In contrast to recommendations in prior work, automatically highlighted hints are useful only in some specific cases of long-form summarization: mostly correct summaries, almost verbatim copied sentences. Annotators should be instructed to read the entire source document and to not rely solely on highlighted hints, since that could bias their judgments. Based on a small-scale study, we found SuperPAL (Ernst et al., 2021) to be the most accurate method for finding hints, but its performance (61% recall@3) is far from ideal.", "publication_ref": ["b34", "b0"], "figure_ref": [], "table_ref": ["tab_9", "tab_10"]}, {"heading": "3.4", "text": "To what extent do our findings generalize to short-form summarization?\nIn this work, we exclusively focus on summarization datasets with an average summary length of at least 150 words. This constraint excludes two popular benchmarks in summarization research over the last five years: CNN/DM (Nallapati et al., 2016) and XSUM (Narayan et al., 2018). How relevant are our research questions (RQs) and findings for these short-form summarization benchmarks? On average, XSUM (24 words) and CNNDM (60 words) contain much shorter summaries than SQuALITY (237 words). XSUM outputs typically contain only 1 sentence or roughly 2-3 FINE units per summary. This blurs the distinction between FINE and COARSE units, which makes it less useful to study RQ1 in these short-form settings. The shorter length of outputs also implies that evaluation is less expensive and consumes less time, which makes our RQ2 less relevant. Finally, on average, XSUM (440 words) and CNNDM (800 words) also have much shorter source documents than datasets like SQuALITY (5200 words), reducing the need for alignment (the main premise for RQ3). The main motivation behind our study is that human evaluation of long-form summarization datasets like SQuALITY and PubMed is challenging and expensive due to the long length of the generated text. Overall, our research questions and findings are more relevant for long-form summarization datasets than for short-form summarization datasets like XSUM and CNNDM.", "publication_ref": ["b41", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "A large body of recent work has focused on new automatic evaluation methods for summarization via NLI-based algorithms (Falke et al., 2019;Laban et al., 2022) or QA-based algorithms . Our work focuses on the much less studied area of human evaluation, the gold standard for developing automatic metrics. A notable effort in this space is the Pyramid method (Nenkova and Passonneau, 2004), along with work improving Pyramid efficiency (Shapira et al., 2019;Zhang and Bansal, 2021). Efficient Pyramid-like protocols have been used to collect large-scale datasets human judgments (Bhandari et al., 2020;) in short-form news summarization tasks like CNN/DM. While these efforts focus on salience evaluation and assume access to multiple references, our work focuses on faithfulness and operates in a reference-free setting. Moreover, we focus on long-form summarization tasks like SQuALITY and PubMed, which are much more challenging and expensive to evaluate.\nEvaluating summary faithfulness relates to fact verification (Vlachos and Riedel, 2014), where claim sentences are checked against a large knowledge source (Wikipedia). Prior work (Nakov et al., 2021) attempts to simplify the human fact checking process by methods like knowledge source snippets (Fan et al., 2020), similar to hint highlights ( \u00a73.3). Faithfulness in summarization differs from fact verification in three ways: (1) summaries are paragraph-long and contextual compared to single sentence stand-alone claims in fact verification;\n(2) summaries are grounded to a source document, compared to a large knowledge source in fact verification; (3) summaries are model-generated compared to human-written claims in fact checking datasets (Thorne et al., 2018;Wadden et al., 2020).", "publication_ref": ["b16", "b36", "b43", "b49", "b66", "b1", "b55", "b40", "b17", "b51", "b56"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We present the LONGEVAL guidelines, a set of recommendations for moving towards standardized human evaluation of long-form summarization. We empirically analyze each recommendation on two datasets. Overall, we find that (1) FINE-grained annotations have lower inter-annotator variance than COARSE-grained annotations; (2) partially annotating a summary reduces annotator workload while maintaining accuracy; (3) highlighting hints in the source document has limited usefulness for evaluating long-form summaries. As future work, we plan to conduct experiments on other aspects of summarization evaluation like salience and coherence.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Human evaluation is a noisy process with many confounding variables. Some of these variables were kept constant among experiments on a dataset, but modifying them could change the trends in the results. These include: (1) number of annotations per summary; (2) the specific annotation interface used; (3) granularity for FINE evaluation (sentences vs phrases); (4) Number of points in the Likert scale for COARSE evaluation; (5) set of summarization systems evaluated; and finally ( 6) relative (eg: A/B tests) vs absolute evaluation (eg: Likert), which has been discussed in Tang et al. (2022) for short-form news summarization datasets like CNN/DM.\nOur paper is limited to faithfulness evaluation, but summaries are typically evaluated for salience, fluency, coherence as well (Fabbri et al., 2021). While fluency may be less of an issue due to large-scale language model pretraining (Dou et al., 2021), coherence and salience are important aspects to evaluate especially in long-form summarization (Goyal et al., 2022). Our findings may not generalize to evaluation of coherence or salience.\nOur experiments in Section 3.1 assigned an equal weight to each FINE unit while calculating the overall score of the summary. However, the faithfulness of some FINE units may be more important than others. A non-uniform weighing of FINE units may be a good strategy if there is a notion of how critical a particular unit is for a summary's correctness. For example: (1) PICO units are critical in medical summaries (DeYoung et al., 2021);\n(2) the Pyramid scheme (Nenkova and Passonneau, 2004) uses a reference frequency-based unit importance, assuming access to multiple gold references. However, a consistent notion of importance is difficult to establish across different domains, and also depends on an individual consumer's preferences. Designing non-uniform weighing schemes is an interesting direction for future research.", "publication_ref": ["b15", "b10", "b21", "b9", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "Ethical Considerations", "text": "All experiments involving human evaluation in this paper were exempt under institutional IRB review. We fairly compensated each Upwork freelancer involved in this study, at a rate of 15-20$ per hour (respecting their suggested Upwork hourly wage). For each round of annotation, we estimated the average amount of time the task would take (by running pilots among ourselves), and provided an-notators with the estimated time requirement. Most freelancers finished the task within the time window, but sometimes exceeded it by 0.5-1 hr. We compensated freelancers based on the actual time they took and their hourly wage, rather than a fixed amount per annotation. F-\u03ba R-\u03ba all agree Random 0.00 0.00 25% SQuALITY 0.74 0.76 82% PubMed 0.53 0.65 74% highlighted clause from the summary, and asked to mark whether or not it is supported by the source document. 50% of the clauses were synthetically perturbed (via negation or entity swapping as in Kryscinski et al., 2020) and manually checked to ensure they were not supported by the source document. A total of 6 freelancers scored 85% or better, and were recruited for the main set of experiments. All 9 freelancers were compensated for the screening round at the rate of 15$ USD / hr. All six hired annotators are native or bilingual English speakers. All annotators have completed a degree at the undergraduate level and three also have Masters degrees, with the most common focuses of the degrees being English/creative writing and education. The annotators' common professional experiences include copywriting, editing, proofreading, writing, and teaching. Finally, for PubMed annotations we re-hired three annotators from the pool of six SQuALITY annotators who mentioned they had experience reading and analyzing biomedical articles. These three annotators were provided with an additional bonus of $30 after they completed all annotations.\nAnnotators are provided with a detailed annotation guideline along with examples of faithfulness (Table 10). Our guidelines are mostly consistent with a recently proposed set of guidelines for checking attribution in text generation (Rashkin et al., 2021). The final annotation interface is implemented in AMT Sandbox, as shown in Figure 8.\nInter-annotator agreement (binary): Much of the analysis in Section 3 uses standard deviation across summaries scores to measure inter-annotator agreement. However, another way to calculate inter-annotator agreement for FINE annotations is measuring agreement on individual units which received a Yes / No judgment. In Table 7 we show these inter-annotator agreement statistics. We measure Fleiss Kappa (Fleiss, 1971), Randolph Kappa (Randolph, 2005Warrens, 2010), and the fraction of sentence pairs with total agreement. 15 In the table we can see all agreement statistics are well away from a uniform random annotation baseline, indicating good agreement.", "publication_ref": ["b34", "b18", "b60"], "figure_ref": ["fig_4"], "table_ref": ["tab_2", "tab_11"]}, {"heading": "B.2 COARSE-grained evaluation of PubMed summaries", "text": "None of the surveyed papers evaluating PubMed summaries with humans released their human evaluation data. Hence, we decided to collect our own COARSE annotations. Since FINE annotations (Section B.1) may have biased our original set of annotators, we hire three new annotators to perform overall assessments on a 5-point Likert scale. In other words, we use a \"between-subject\" experiment design to compare FINE against COARSE. We hired three freelancers on Upwork, all of whom have extensive professional experience reading research papers (two of them had PhDs in biomedical fields). All annotators were compensated at a rate of 20$ USD / hr, their hourly rate on Upwork. All three annotators had been previously screened and hired by us for different projects in the past. Two of them had assisted us in an annotation task involved reading short summaries of biomedical academic papers and evaluating them for fluency, accuracy, correctness.\nAnnotators are provided with a detailed annotation guideline along with examples of faithfulness (Table 11). Our guidelines are mostly consistent with a recently proposed set of guidelines for checking attribution in text generation (Rashkin et al., 2021). The final annotation interface is implemented in LabelStudio, as shown in Figure 9.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": ["tab_2"]}, {"heading": "B.3 Crowdworkers or expert annotators?", "text": "Several prior works have raised the issue of low inter-annotator agreement and poor accuracy with non-expert annotators (eg: MTurk crowdworkers) in human evaluation of summarization (Gillick and Liu, 2010;Fabbri et al., 2021;Falke et al., 2019) and open-ended long-form generation (Karpinska et al., 2021;Clark et al., 2021). In our survey (Table 9), we found the type of annotators used in long-form summarization is often not specified (16 / 43 papers). Among other papers, 10 papers use non-experts while 17 papers use expert annotators (often graduate students).\nOverall, we echo the concerns with non-expert annotators and recommend hiring freelancers on Upwork (or experts) who are well-versed with the domain for annotation. In initial experiments, we attempted to recruit Amazon Mechanical Turk crowdworkers filtered by the \"Master's qualification\" and having a 90%+ approval rating. In our qualification task of error detection in synthetically perturbed SQuALITY summaries, MTurkers scored just 62% (binary classification) with a threeannotator Fleiss \u03ba of 0.15. On the other hand, Upwork freelancers (with professional writing experience) an accuracy 90% with a high inter-annotator agreement (Fleiss \u03ba = 0.71).", "publication_ref": ["b20", "b15", "b16", "b5"], "figure_ref": [], "table_ref": ["tab_14"]}, {"heading": "C Additional Survey Statistics", "text": "In Table 8 and Table 9 we document some additional statistics for the 44 papers conducting human evaluation of long-form summarization.   ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_13", "tab_14"]}, {"heading": "D Automatic summarization metrics used for evaluation", "text": "The following metrics are considered while measuring Pearson's correlation with our human evaluation data (Figure 2) -ROUGE-1 / 2 / F (Lin, 2004), BARTScore / BARTScore-Parabank , Sentence-BLEU (Papineni et al., 2002), BERTScore (Zhang et al., 2020) and BLEURT (Sellam et al., 2020). A number of metrics were calculated using the SacreROUGE repository (Deutsch and Roth, 2020).\nE Learning effect while annotating long-form summaries\nIn Section 3.3 we discussed a learning effect where annotators get more familiar with the contents of a source document as they annotate more FINEgrained units in a long-form summary. To better understand this effect, in Figure 5 we plot the average time taken by annotators as they progress in their annotation of a summary. Overall, we find that annotators get significantly faster in annotating the summary after the first 20% units. We hypothesize that annotators get pretty familiar with the general topics in the source document after the first few annotations, speeding up subsequent annotations.   ", "publication_ref": ["b38", "b44", "b67", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "First and foremost, we would like to thank all the nine Upwork freelancers who contributed human annotations to this project. We are very grateful to Yixiao Song, Alex Wang, John Giorgi, Dustin Wright, Yulia Otmakhova, Daniel Deutsch, Arie Cattan, Shiyue Zhang, Tanya Goyal, Greg Durrett, Marzena Karpinska, Ankita Gupta, Nader Akoury and the Semantic Scholar team for several useful discussions at various points during the project. This work was mostly done while Kalpesh Krishna (KK) was an intern at the Allen Institute for Artificial Intelligence. KK was partly supported by a Google PhD Fellowship awarded in 2021.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Author Contributions: Kalpesh Krishna led the project and performed all the technical contributions including literature review, dataset collection and processing, model implementation, annotation interface development, running experiments, and data analysis. Kalpesh also contributed to project scoping and ideation and led the writing of the paper. Erin Bransom and Bailey Kuehl helped with obtaining human judgements, including piloting the task and giving feedback, performing the annotation themselves, and hiring and managing annotators on Upwork. Pradeep Dasigi, Arman Cohan, and Kyle Lo were mentors of the project during and after Kalpesh's internship, contributing equally to project scoping, experimental design, ideation and direction throughout the course of the project and paper writing. Mohit Iyyer provided mentorship after the internship, in particular providing important feedback and direction on data analysis and contributing to paper writing.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix A Bootstrap analysis of inter-annotator variance", "text": "We utilize the bootstrap resampling (Tibshirani and Efron, 1993) technique described in Deutsch et al. (2021) to estimate confidence intervals for human evaluation data. At a high level, bootstrap resampling helps capture the uncertainty in a downstream test statistic by repeatedly sampling from the data with replacement. We consider two downstream test statistics in our work -(1) average system level performance;\n(2) correlation of human judgements to automatic metrics.\nWhile Deutsch et al. (2021) were primarily interested in uncertainty due to the specific instances and systems evaluated, our goal is to capture uncertainty due to the inter-annotator variance. Hence unlike Deutsch et al. (2021), we sample with replacement from the set of annotators for every instance. Our precise formulation can be found in Algorithm 1, which operates on a X \u2208 R N \u00d7M matrix of human annotations where N is the number of summaries, and M the number of annotators.", "publication_ref": ["b7", "b7", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 1 Bootstrap Confidence Interval", "text": "N is summaries, M is annotators Output: (1 \u2212 \u03b1) \u00d7 100%-confidence interval 1: samples \u2190 an empty list 2: for k iterations do 3:\nXs \u2190 empty N \u00d7 M matrix 4:\nfor i \u2208 {1, . . . , N } do 5: D \u2190 samp. {1, . . . , M } w/ repl. M times 6:\nfor j \u2208 {1, . . . M } do 7:\nend for 9: end for 10:\nCalculate test statistic on Xs and append to samples 11: end for 12: \u2113, u \u2190 (\u03b1/2) \u00d7 100 and (1 \u2212 \u03b1/2) \u00d7 100 percentiles of samples 13: return \u2113, u", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Human evaluation details B.1 FINE-grained evaluations of SQuALITY and PubMed summaries", "text": "We interviewed a total of 9 Upwork freelancers for the position, offering a compensation of $15-16.5 / hr (depending on their Upwork hourly rate).\nThe screening procedure involved a qualification task on synthetically perturbed summaries from the SQuALITY dataset validation split. Similar to the final annotation task, annotators were shown a\nIn this task, you will be shown a long document (\"Source Document\") and its Summary. A span of text will be highlighted in the summary, and the goal is to check if this span is factually supported by the source document. You will need to choose one of two options:\n1. Yes: if all the facts in the highlighted summary span are supported by the source document 2. No: if the highlighted summary span presents some information that is not supported by the source document (either a direct contradiction, or not present) In addition to the source document, you will be provided with some highlighted text (\"hints\") in the source document which may help you in making a decision. Press the \"Next Hint\" button to scroll through the highlighted hints. Source document hints may or may not be helpful. Do not make a judgment solely based on these hints.    Instructions for Likert-scale evaluation. Please read all instructions before starting the annotation.\nSetup 1. Start by signing up on Label Studio, you will need to provide an email ID and password. It's okay to use a non-existent throw-away email ID here. Also, do not use any personal / sensitive passwords (but make sure to remember your email / password for logging in next time!). Click on the box saying \"<your name> -Summarization Evaluation\" 2. In this batch a total of 30 summaries need to be evaluated. Every three consecutive rows are different summaries of the same source document. You can evaluate a summary by clicking on a row, and annotating it. Optionally, you can click on \"Label All Tasks\" at the top of the screen.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Annotation Task", "text": "Each summary needs to be evaluated for its \"correctness\". You need to provide a 0-5 judgment for the entire summary, where \"correctness\" can be defined as, \"The absence of factual errors in the summary, where a factual error is a statement that contradicts the source document, or is not directly stated, heavily implied, or logically entailed by the source document\". For example, Source Document (snippet shown) = . . . .. Vitamin C was discovered in 1912Vitamin C was discovered in , isolated in 1928Vitamin C was discovered in , and, in 1933, was the first vitamin to be chemically produced. It is on the World Health Organization's List of Essential Medicines. Vitamin C is available as an inexpensive generic and over-the-counter medication. Partly for its discovery, Albert Szent-Gy\u00f6rgyi and Walter Norman Haworth were awarded the 1937 Nobel Prizes in Physiology and Medicine and Chemistry, respectively. Foods containing vitamin C include citrus fruits, kiwifruit, guava, broccoli, Brussels sprouts, bell peppers, potatoes, and strawberries. Prolonged storage or cooking may reduce vitamin C content in foods. . . . . Summary 1 (snippet shown) = . . . Chicken contains vitamin C . . . Summary 2 (snippet shown) = . . . Albert Szent-Gy\u00f6rgyi won the 1955 Nobel Prize for discovering Vitamin C . . . Summary 3 (snippet shown) = . . . Vitamin C was the first chemically produced Vitamin . . . Summary 4 (snippet shown) = . . . Apple contains vitamin C . . . Errors marked in red. Here, the snippets for summary 1 are incorrect, summary 2 partially correct, and summary 3 completely correct with respect to the source document. Summary 4 is incorrect with respect to the source document (since it's never discussed), but a globally correct fact. You should treat such a summary as incorrect since it is not mentioned in the source document.\n(This is an illustrative example only, the actual annotation task has much longer summaries / source documents.)\nThe rating scale is from 0 to 5, where 0 is the lowest possible rating (most or all of the summary is wrong / irrelevant to the source document), and 5 is the highest rating (most or all of the summary is correct). While it is compulsory to provide a judgment from 0 to 5 for each summary, you can optionally provide additional comments in your annotation. For instance, if the judgment needs to be more nuanced than a 5-point scale, you prefer to mark something like \"3.5\", or you would like to add some other notes about your judgment. Press \"Submit\" after you have provided your annotation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Suggested workflow", "text": "Every three consecutive rows contain different summaries for the same source document. We suggest the following workflow while annotating documents -1. Spend the first 15 minutes reading the source document and getting a general sense of the facts mentioned in the document.\n2. Spend 5 minutes to read and annotate the summaries in each of the three consecutive rows which correspond to the same document. Add optional comments / notes if necessary.\n3. In the last 5 minutes, re-calibrate your ratings across the three rows if needed (for instance, you significantly preferred the correctness of summary 1 vs summary 2, but you gave it the same rating in the initial pass). Add optional comments / notes if necessary.\nFollowing this workflow, it should take 35 minutes to annotate each set of 3 rows. For 30 rows, this should take 6 hrs. ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "STO-RIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Nader Akoury; Shufan Wang; Josh Whiting; Stephen Hood; Nanyun Peng; Mohit Iyyer"}, {"ref_id": "b1", "title": "Reevaluating evaluation in text summarization", "journal": "", "year": "2020", "authors": "Manik Bhandari; Pranav Narayan Gour; Atabak Ashfaq; Pengfei Liu; Graham Neubig"}, {"ref_id": "b2", "title": "Results of the WMT16 metrics shared task", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Ond\u0159ej Bojar; Yvette Graham; Amir Kamran; Milo\u0161 Stanojevi\u0107"}, {"ref_id": "b3", "title": "Evaluation of text generation: A survey", "journal": "", "year": "2020", "authors": "Asli Celikyilmaz; Elizabeth Clark; Jianfeng Gao"}, {"ref_id": "b4", "title": "SummScreen: A dataset for abstractive screenplay summarization", "journal": "Long Papers", "year": "2022", "authors": "Mingda Chen; Zewei Chu; Sam Wiseman; Kevin Gimpel"}, {"ref_id": "b5", "title": "All that's 'human' is not gold: Evaluating human evaluation of generated text", "journal": "Long Papers", "year": "2021", "authors": "Elizabeth Clark; Tal August; Sofia Serrano; Nikita Haduong; Suchin Gururangan; Noah A Smith"}, {"ref_id": "b6", "title": "A discourse-aware attention model for abstractive summarization of long documents", "journal": "", "year": "2018", "authors": "Arman Cohan; Franck Dernoncourt; Soon Doo; Trung Kim; Seokhwan Bui; Walter Kim; Nazli Chang;  Goharian"}, {"ref_id": "b7", "title": "A statistical analysis of summarization evaluation metrics using resampling methods", "journal": "Transactions of the Association for Computational Linguistics", "year": "2021", "authors": "Daniel Deutsch; Rotem Dror; Dan Roth"}, {"ref_id": "b8", "title": "SacreROUGE: An Open-Source Library for Using and Developing Summarization Evaluation Metrics", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Daniel Deutsch; Dan Roth"}, {"ref_id": "b9", "title": "MS\u02c62: Multidocument summarization of medical studies", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Jay Deyoung; Iz Beltagy; Madeleine Van Zuylen; Bailey Kuehl; Lucy Lu Wang"}, {"ref_id": "b10", "title": "Scarecrow: A framework for scrutinizing machine text", "journal": "", "year": "2021", "authors": "Yao Dou; Maxwell Forbes; Rik Koncel-Kedziorski; A Noah; Yejin Smith;  Choi"}, {"ref_id": "b11", "title": "Mohit Bansal, and Ido Dagan. 2021. Summary-source proposition-level alignment: Task, datasets and supervised baseline", "journal": "Online. Association for Computational Linguistics", "year": "", "authors": "Ori Ernst; Ori Shapira; Ramakanth Pasunuru; Michael Lepioshkin; Jacob Goldberger"}, {"ref_id": "b12", "title": "Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model", "journal": "", "year": "2019", "authors": "Alexander Fabbri; Irene Li; Tianwei She; Suyi Li; Dragomir Radev"}, {"ref_id": "b13", "title": "Annual Meeting of the Association for Computational Linguistics", "journal": "", "year": "", "authors": ""}, {"ref_id": "b14", "title": "QAFactEval: Improved QAbased factual consistency evaluation for summarization", "journal": "", "year": "2022", "authors": "Alexander Fabbri; Chien-Sheng Wu; Wenhao Liu; Caiming Xiong"}, {"ref_id": "b15", "title": "Summeval: Re-evaluating summarization evaluation", "journal": "Transactions of the Association for Computational Linguistics", "year": "2021", "authors": "Wojciech Alexander R Fabbri; Bryan Kry\u015bci\u0144ski; Caiming Mc-Cann; Richard Xiong; Dragomir Socher;  Radev"}, {"ref_id": "b16", "title": "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference", "journal": "", "year": "2019", "authors": "Tobias Falke; Leonardo F R Ribeiro; Ido Prasetya Ajie Utama; Iryna Dagan;  Gurevych"}, {"ref_id": "b17", "title": "Generating fact checking briefs", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Angela Fan; Aleksandra Piktus; Fabio Petroni; Guillaume Wenzek; Marzieh Saeidi; Andreas Vlachos; Antoine Bordes; Sebastian Riedel"}, {"ref_id": "b18", "title": "Measuring nominal scale agreement among many raters", "journal": "Psychological bulletin", "year": "1971", "authors": "L Joseph;  Fleiss"}, {"ref_id": "b19", "title": "Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text", "journal": "", "year": "2022", "authors": "Sebastian Gehrmann; Elizabeth Clark; Thibault Sellam"}, {"ref_id": "b20", "title": "Non-expert evaluation of summarization systems is risky", "journal": "", "year": "2010", "authors": "Dan Gillick; Yang Liu"}, {"ref_id": "b21", "title": "Snac: Coherence error detection for narrative summarization", "journal": "", "year": "2022", "authors": "Tanya Goyal; Junyi Jessy Li; Greg Durrett"}, {"ref_id": "b22", "title": "LongT5: Efficient text-to-text transformer for long sequences", "journal": "", "year": "2022", "authors": "Mandy Guo; Joshua Ainslie; David Uthus; Santiago Ontanon; Jianmo Ni; Yun-Hsuan Sung; Yinfei Yang"}, {"ref_id": "b23", "title": "Automated lay language summarization of biomedical scientific reviews", "journal": "", "year": "2021", "authors": "Yue Guo; Wei Qiu; Yizhong Wang; Trevor Cohen"}, {"ref_id": "b24", "title": "HighRES: Highlight-based reference-less evaluation of summarization", "journal": "", "year": "2019", "authors": "Hardy Hardy; Shashi Narayan; Andreas Vlachos"}, {"ref_id": "b25", "title": "Annual Meeting of the Association for Computational Linguistics", "journal": "Association for Computational Linguistics", "year": "", "authors": ""}, {"ref_id": "b26", "title": "Copyright and fair use", "journal": "", "year": "2016", "authors": "Harvard Ogc Of"}, {"ref_id": "b27", "title": "Efficient attentions for long document summarization", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Luyang Huang; Shuyang Cao; Nikolaus Parulian; Ji Heng; Lu Wang"}, {"ref_id": "b28", "title": "Leveraging information bottleneck for scientific document summarization", "journal": "", "year": "2021", "authors": "Jiaxin Ju; Ming Liu; Yee Huan; Yuan Koh; Lan Jin; Shirui Du;  Pan"}, {"ref_id": "b29", "title": "2021. The perils of using Mechanical Turk to evaluate open-ended text generation", "journal": "Association for Computational Linguistics", "year": "", "authors": "Marzena Karpinska; Nader Akoury; Mohit Iyyer"}, {"ref_id": "b30", "title": "Dense passage retrieval for opendomain question answering", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Vladimir Karpukhin; Barlas Oguz; Sewon Min; Patrick Lewis; Ledell Wu; Sergey Edunov; Danqi Chen; Wen-Tau Yih"}, {"ref_id": "b31", "title": "BillSum: A corpus for automatic summarization of US legislation", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Anastassia Kornilova; Vladimir Eidelman"}, {"ref_id": "b32", "title": "Hurdles to progress in long-form question answering", "journal": "", "year": "2021", "authors": "Kalpesh Krishna; Aurko Roy; Mohit Iyyer"}, {"ref_id": "b33", "title": "Neural text summarization: A critical evaluation", "journal": "", "year": "2019", "authors": "Wojciech Kryscinski; Nitish Shirish Keskar; Bryan Mc-Cann; Caiming Xiong; Richard Socher"}, {"ref_id": "b34", "title": "Evaluating the factual consistency of abstractive text summarization", "journal": "", "year": "2020", "authors": "Wojciech Kryscinski; Bryan Mccann; Caiming Xiong; Richard Socher"}, {"ref_id": "b35", "title": "Booksum: A collection of datasets for longform narrative summarization", "journal": "", "year": "2021", "authors": "Wojciech Kry\u015bci\u0144ski; Nazneen Rajani"}, {"ref_id": "b36", "title": "SummaC: Re-visiting NLIbased models for inconsistency detection in summarization", "journal": "Transactions of the Association for Computational Linguistics", "year": "2022", "authors": "Philippe Laban; Tobias Schnabel; Paul N Bennett; Marti A Hearst"}, {"ref_id": "b37", "title": "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension", "journal": "", "year": "2020", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal; Marjan Ghazvininejad; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"ref_id": "b38", "title": "ROUGE: A package for automatic evaluation of summaries", "journal": "Association for Computational Linguistics", "year": "2004", "authors": "Chin-Yew Lin"}, {"ref_id": "b39", "title": "Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation", "journal": "", "year": "2022", "authors": "Yixin Liu; Alexander R Fabbri; Pengfei Liu; Yilun Zhao; Linyong Nan; Ruilin Han; Simeng Han; Shafiq Joty; Chien-Sheng Wu; Caiming Xiong"}, {"ref_id": "b40", "title": "Automated fact-checking for assisting human fact-checkers", "journal": "", "year": "2021", "authors": "Preslav Nakov; David Corney; Maram Hasanain; Firoj Alam; Tamer Elsayed; Alberto Barr\u00f3n-Cede\u00f1o; Paolo Papotti; Shaden Shaar; Giovanni Da San Martino"}, {"ref_id": "b41", "title": "Abstractive text summarization using sequence-to-sequence RNNs and beyond", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Ramesh Nallapati; Bowen Zhou; Caglar Cicero Dos Santos; Bing Gulcehre;  Xiang"}, {"ref_id": "b42", "title": "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Shashi Narayan; Shay B Cohen; Mirella Lapata"}, {"ref_id": "b43", "title": "Evaluating content selection in summarization: The pyramid method", "journal": "", "year": "2004", "authors": "Ani Nenkova; Rebecca Passonneau"}, {"ref_id": "b44", "title": "Bleu: a method for automatic evaluation of machine translation", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"ref_id": "b45", "title": "Free-marginal multirater kappa (multirater k [free]): An alternative to fleiss' fixed-marginal multirater kappa", "journal": "", "year": "2005", "authors": "J Justus;  Randolph"}, {"ref_id": "b46", "title": "Iulia Turc, and David Reitter. 2021. Measuring attribution in natural language generation models", "journal": "", "year": "", "authors": "Vitaly Hannah Rashkin; Matthew Nikolaev; Michael Lamm; Dipanjan Collins; Slav Das; Gaurav Petrov;  Singh Tomar"}, {"ref_id": "b47", "title": "Okapi at trec-3", "journal": "Nist Special Publication Sp", "year": "1995", "authors": "Steve Stephen E Robertson; Susan Walker; Micheline M Jones; Mike Hancock-Beaulieu;  Gatford"}, {"ref_id": "b48", "title": "BLEURT: Learning robust metrics for text generation", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Thibault Sellam; Dipanjan Das; Ankur Parikh"}, {"ref_id": "b49", "title": "Crowdsourcing lightweight pyramids for manual summary evaluation", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Ori Shapira; David Gabay; Yang Gao; Hadar Ronen; Ramakanth Pasunuru; Mohit Bansal; Yael Amsterdamer; Ido Dagan"}, {"ref_id": "b50", "title": "Yashar Mehdad, and Dragomir Radev. 2022. Investigating crowdsourcing protocols for evaluating the factual consistency of summaries", "journal": "Association for Computational Linguistics", "year": "", "authors": "Xiangru Tang; Alexander Fabbri; Haoran Li; Ziming Mao; Griffin Adams; Borui Wang; Asli Celikyilmaz"}, {"ref_id": "b51", "title": "FEVER: a large-scale dataset for fact extraction and VERification", "journal": "Long Papers", "year": "2018", "authors": "James Thorne; Andreas Vlachos; Christos Christodoulopoulos; Arpit Mittal"}, {"ref_id": "b52", "title": "An introduction to the bootstrap", "journal": "", "year": "1993", "authors": "J Robert; Bradley Tibshirani;  Efron"}, {"ref_id": "b53", "title": "Libraries at UMGC. 2020. Copyright and fair use guidelines", "journal": "", "year": "", "authors": ""}, {"ref_id": "b54", "title": "SummVis: Interactive visual analysis of models, data, and evaluation for text summarization", "journal": "", "year": "2021", "authors": "Jesse Vig; Wojciech Kryscinski; Karan Goel; Nazneen Rajani"}, {"ref_id": "b55", "title": "Fact checking: Task definition and dataset construction", "journal": "", "year": "2014", "authors": "Andreas Vlachos; Sebastian Riedel"}, {"ref_id": "b56", "title": "Fact or fiction: Verifying scientific claims", "journal": "", "year": "2020", "authors": "David Wadden; Shanchuan Lin; Kyle Lo; Lucy Lu Wang; Madeleine Van Zuylen; Arman Cohan; Hannaneh Hajishirzi"}, {"ref_id": "b57", "title": "Mul-tiVerS: Improving scientific claim verification with weak supervision and full-document context", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "David Wadden; Kyle Lo; Lucy Wang; Arman Cohan; Iz Beltagy; Hannaneh Hajishirzi"}, {"ref_id": "b58", "title": "Asking and answering questions to evaluate the factual consistency of summaries", "journal": "", "year": "2020", "authors": "Alex Wang; Kyunghyun Cho; Mike Lewis"}, {"ref_id": "b59", "title": "Squality: Building a long-document summarization dataset the hard way", "journal": "", "year": "2022", "authors": "Alex Wang; Richard Yuanzhe Pang; Angelica Chen; Jason Phang; Samuel R Bowman"}, {"ref_id": "b60", "title": "Inequalities between multirater kappas. Advances in data analysis and classification", "journal": "", "year": "2010", "authors": "J Matthijs;  Warrens"}, {"ref_id": "b61", "title": "Finetuned language models are zero-shot learners", "journal": "", "year": "2022", "authors": "Jason Wei; Maarten Bosma; Vincent Zhao; Kelvin Guu; Adams Wei Yu; Brian Lester; Nan Du; M Andrew; Quoc V Dai;  Le"}, {"ref_id": "b62", "title": "The statistical advantage of automatic NLG metrics at the system level", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Johnny Wei; Robin Jia"}, {"ref_id": "b63", "title": "Beyond BLEU:training neural machine translation with semantic similarity", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "John Wieting; Taylor Berg-Kirkpatrick; Kevin Gimpel; Graham Neubig"}, {"ref_id": "b64", "title": "Bartscore: Evaluating generated text as text generation", "journal": "", "year": "2021", "authors": "Weizhe Yuan; Graham Neubig; Pengfei Liu"}, {"ref_id": "b65", "title": "Big bird: Transformers for longer sequences", "journal": "Advances in Neural Information Processing Systems", "year": "2020", "authors": "Manzil Zaheer; Guru Guruganesh; Joshua Kumar Avinava Dubey; Chris Ainslie; Santiago Alberti; Philip Ontanon; Anirudh Pham; Qifan Ravula; Li Wang;  Yang"}, {"ref_id": "b66", "title": "Finding a balanced degree of automation for summary evaluation", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Shiyue Zhang; Mohit Bansal"}, {"ref_id": "b67", "title": "Bertscore: Evaluating text generation with bert", "journal": "", "year": "2020", "authors": "Tianyi Zhang; Varsha Kishore; Felix Wu; Q Kilian; Yoav Weinberger;  Artzi"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Overview of research questions considered in LONGEVAL. Example summary taken from SQuALITY.", "figure_data": ""}, {"figure_label": "23", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :Figure 3 :23Figure2: 95% confidence intervals of Pearson correlations between various automatic evaluation metrics and using human evaluation data collected with FINE (blue) and COARSE (orange) annotation methods. In both datasets, FINE annotations lead to much narrower CIs than COARSE annotations. See Appendix G for plot with Kendall's Tau.", "figure_data": ""}, {"figure_label": "57", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :FFigure 7 :57Figure5: Learning effect over time while evaluating long-form summaries with FINE annotation. As the annotators evaluate more summary units, they learn the document better and are much faster at annotation irrespective of whether hints are shown to them.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 8 :8Figure 8: The AMT Sandbox annotation interface used for FINE evaluation of SQuALITY and PubMed summaries (Appendix B.1).", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 9 :9Figure 9: The LabelStudio annotation interface used for COARSE evaluation of PubMed summaries (Appendix B.2).", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ": List of long-form summarization datasets con-sidered in our survey along with average source docu-ment and summary lengths. Each dataset considered has at least 150 word summaries on average.Type of human evaluation# papers % papersNone Short-form summaries only101 1762% 11%Likert-scale COARSE-grained A/B testing COARSE-grained Extrinsic evaluation Binary per sentence FINE-grained QA-based FINE-grained24 13 1 4 215% 8% 1% 2% 1%"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ": Human evaluation setup in 162 summariza-tion papers that evaluate long-form summaries. 73% of the papers do not evaluate long-form summaries with humans, while others vary significantly in their setups."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "for reference.", "figure_data": "DatasetCOARSE FINESQuALITY PubMed PubMed + ngram block18.5 11.8 11.76.8 7.3 9.3Average14.07.8"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Average standard deviation of faithfulness scores across annotators on a 100-point rating scale. Lower variation means higher agreement. Overall, we find that FINE-grained annotations have higher interannotator agreement than COARSE-grained annotations.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Figure4: Accuracy and variance after annotating a fraction of units per summary (X-axis) with FINE. Despite annotating just a fraction of the summary, we observe a high segment-level Kendall tau correlation with a full annotation (left). However we observe higher inter-annotator variance as the fraction reduces (right). Confidence intervals shown are 95% and computed across 1000 random subsets (see Appendix F for left plot with Pearson).", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "", "figure_data": ": A comparison of algorithms finding the top source document sentences for summary units in SQuALITY. R@k (recall@k) denotes the fraction of times the gold sentence was in the top-k predictions.HintsAcc. (\u2191) Agree. (\u2191) Time (secs) (\u2193)(2-way)(Fleiss)AllFirst 5None SuperPAL Gold93% 92% 92%0.71 41.4 0.64 48.2 0.63 40.4115.6 84.6 60.4"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Results and snippets from our questionnaire with FINE annotators. Overall, annotators find hints only sometimes useful, and mention reading the entire source document along with keyword searches.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "", "figure_data": ": Fleiss kappa (F-\u03ba), Randolph kappa (R-\u03ba), and agreement scores of our FINE annotation per summary unit. All \u03ba scores are well above a random annotation baseline, indicating good agreement."}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Fraction of surveyed papers following the best practices recommended byGehrmann et al. (2022). We include only the 44 papers here which conducted a human evaluation of long-form summarization.", "figure_data": "Type of annotator# papersNo details specified Native English speaker** Mechnical Turk crowdworker Non-expert volunteers11 / 44 5 / 44 9 / 44 1 / 44Extensive prior experience** Graduate students / researchers Upwork freelancers3 / 44 13 / 44 2 / 44"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "The types of annotators used across different long-form summarization papers. ** -No additional details were specified.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "F summ = 1 |C summ | c\u2208Csumm F c , F c \u2208 {0, 1}", "formula_coordinates": [4.0, 326.17, 335.51, 178.22, 33.52]}], "doi": "10.18653/v1/2020.emnlp-main.525"}