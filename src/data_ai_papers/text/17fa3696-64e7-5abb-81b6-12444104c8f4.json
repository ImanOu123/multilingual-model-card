{"title": "Linear Classifier: An Often-Forgotten Baseline for Text Classification", "authors": "Yu-Chen Lin; Si-An Chen; Jie-Jyun Liu; Chih-Jen Lin", "pub_date": "", "abstract": "Large-scale pre-trained language models such as BERT are popular solutions for text classification. Due to the superior performance of these advanced methods, nowadays, people often directly train them for a few epochs and deploy the obtained model. In this opinion paper, we point out that this way may only sometimes get satisfactory results. We argue the importance of running a simple baseline like linear classifiers on bag-of-words features along with advanced methods. First, for many text data, linear methods show competitive performance, high efficiency, and robustness. Second, advanced models such as BERT may only achieve the best results if properly applied. Simple baselines help to confirm whether the results of advanced models are acceptable. Our experimental results fully support these points.", "sections": [{"heading": "Introduction", "text": "Text classification is an essential topic in natural language processing (NLP). Like the situations in most NLP tasks, nowadays, large-scale pre-trained language models (PLMs) such as BERT (Devlin et al., 2019) have become popular solutions for text classification. Therefore, we have seen that many practitioners directly run pre-trained language models with a fixed number of epochs on their text data. Unfortunately, this way may only sometimes lead to satisfactory results. In this opinion paper, through an intriguing illustration, we argue that for text classification, a simple baseline like linear classifiers on bag-of-words features should be used along with the advanced models for the following reasons.\n\u2022 Training linear classifiers such as linear SVM (Boser et al., 1992) or logistic regression on bag-of-words features is simple and efficient. This approach may give competitive performance to advanced models for some problems. While various settings of bag-of-words features such as bi-gram or tri-gram can be considered, we advocate that simple uni-gram TF-IDF features trained by linear classifiers can be a useful baseline to start with for text classification. \u2022 Advanced architectures such as BERT may only achieve the best results if properly used. Linear methods can help us check if advanced methods' results are reasonable. In the deep-learning era, the younger generation often thinks that linear classifiers should never be considered. Further, they may be unaware of some variants of linear methods that are particularly useful for text classification (see Section 3.1). Therefore, the paper serves as a reminder of this oftenforgotten technique.\nFor our illustration, we re-investigate an existing work (Chalkidis et al., 2022) that evaluates both linear SVM and pre-trained language models, but the authors pay more attention to the latter. The linear method is somewhat ignored even though the performance is competitive on some problems. We carefully design experiments to compare the two types of methods. Our results fully demonstrate the usefulness of applying linear methods as simple baselines.\nSome recent works (e.g., Yu et al., 2022;Gomes et al., 2021) have shown the usefulness of linear classifiers in the deep-learning era. However, they either consider sophisticated applications or investigate advanced settings in which linear methods are only one component. In contrast, in this paper, we consider the basic scenario of text classification. A more related work (Wahba et al., 2023) has demonstrated the effectiveness of linear classifiers over PLMs on some problems. However, our investigation on linear methods is more comprehensive.\nThe discussion also reminds us the trade-off between performance gain and the cost including running time, model size, etc. Simple methods are useful to benchmark and justify the usage of advanced methods.  8 3h 2m 78.8 2h 57m 76.6 2h 34m 70.7 3h 40m 88.3 6h 8m 96", "publication_ref": ["b2", "b1", "b15", "b5", "b13"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Method", "text": "ECtHR (A) ECtHR (B) SCOTUS EUR-LEX LEDGAR UNFAIR-ToS # \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1 T\n.0 N/A 110M Table 1: Micro-F1 scores (\u00b5-F 1 )\n, training time (T) and number of parameters presented in Chalkidis et al. (2022). In each Micro-F1 column, the best result is bold-faced. \"N/A\" means not available in their work. For example, the authors did not report the training time and the number of parameters of linear SVMs.\nThis paper is organized as follows. In Section 2 we take a case study to point out the needs of considering linear methods as a baseline for text classification. We describe the linear and BERT-based methods used for investigation in Section 3. The experimental results and main findings are in Section 4, while Section 5 provides some discussion. Additional details are in Appendix. Programs used for experiments are available at https://github.com/JamesLYC88/ text_classification_baseline_code.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Text Classification These Days: Some Issues in Applying Training Methods", "text": "Large PLMs have shown dramatic progress on various NLP tasks. In the practical use, people often directly fine-tune PLMs such as BERT on their data for a few epochs. However, for text classification, we show that this way may not always get satisfactory results. Some simple baselines should be considered to know if the obtained PLM model is satisfactory. We illustrate this point by considering the work on legal document classification by Chalkidis et al. (2022), which evaluates the following sets.\n\u2022 Multi-class classification: SCOTUS, LEDGAR; for this type of sets, each text is associated with a single label. \u2022 Multi-label classification: ECtHR (A), ECtHR (B), EUR-LEX, UNFAIR-ToS; for this type of sets, each text is associated with multiple (or zero) labels. \u2022 Multiple choice QA: CaseHOLD.\nWe focus on text classification in this work, so CaseHOLD is not considered. For each problem, training and test sets are available. 1 The study in Chalkidis et al. (2022) comprehensively evaluates both BERT-based PLMs and linear SVMs. They use Micro-F1 and Macro-F1 to measure the test performance. 2 In Table 1, we present their Micro-F1 results and running time of each model.", "publication_ref": ["b1", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Linear Models Worth More Investigation", "text": "The investigation in Chalkidis et al. (2022) focuses on BERT and its variants, even though from Table 1, the performance of BERT-based methods may not differ much. While they did not pay much attention to linear SVM, by a closer look at the results, we get intriguing observations: \u2022 Linear SVM is competitive to BERT-based PLMs on four of the six data sets. For SCO-TUS, linear SVM even outperforms others with a clear gap. \u2022 Surprisingly, given linear SVM's decent performance, its training time was not shown in Chalkidis et al. (2022), nor was the number of parameters; see the \"N/A\" entries in Table 1. With the observations, we argue that the results of linear models are worth more investigation.", "publication_ref": ["b1", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Settings for Investigation", "text": "To better understand the performance of linear models and BERT-based PLMs, we simulate how people work on a new data set by training these methods. We consider a text classification package Lib-MultiLabel 3 because it supports both types of train-ing methods.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Linear Methods for Text Classification", "text": "To use a linear method, LibMultiLabel first generates uni-gram TF-IDF features (Luhn, 1958;Jones, 1972) according to texts in the training set, and the obtained factors are used to get TF-IDF for the test set. It then provides three classic methods that adopt binary linear SVM and logistic regression for multi-class and multi-label scenarios. 4 Here we consider linear SVM as the binary classifier behind these methods.\n\u2022 One-vs-rest: This method learns a binary linear SVM for each label, so data with/without this label are positive/negative, respectively. Let f \u2113 (x) be the decision value of the \u2113-th label, where x is the feature vector. For multi-class classification, y = argmax \u2113 f \u2113 (x) is predicted as the single associated label of x. For multi-label classification, all labels \u2113 with positive f \u2113 (x) are considered to be associated with x. This method is also what \"TF-IDF+SVM\" in Chalkidis et al. (2022) did, though our TF-IDF feature generation is simpler than theirs by considering only uni-gram. 5 \u2022 Thresholding (Yang, 2001;Lewis et al., 2004;Fan and Lin, 2007): This method extends one-vsrest by modifying the decision value for optimizing Macro-F1. That is, we change the decision value to f \u2113 (x) + \u2206 \u2113 , where \u2206 \u2113 is a threshold decided by cross validation. \u2022 Cost-sensitive (Parambath et al., 2014): For each binary problem, this method re-weights the losses on positive data. We decide the reweighting factor by cross validation to optimize Micro-F1 or Macro-F1.\nThese methods basically need no further hyperparameter tuning, so we can directly run them. The last two methods are extensions of one-vs-rest to address the imbalance of each binary problem (i.e., few positives and many negatives). The design relies on the fact that the binary problems are independent, so such approaches cannot be easily applied to deep learning, which considers all labels together in a single network.", "publication_ref": ["b11", "b7", "b1", "b14", "b9", "b4", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "BERT-based Methods for Text Classification", "text": "LibMultiLabel also provides BERT-based methods, which involve several hyper-parameters, such as the learning rate. While practitioners may directly choose hyper-parameters, to seriously compare with linear methods, we run BERT by conducting hyper-parameter selection. More details are in Appendix F.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results and Analysis", "text": "In  3.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Linear Methods are Good Baselines", "text": "In Table 2, our one-vs-rest results are slightly worse than the linear SVM results in Chalkidis et al. (2022), which also applies the one-vs-rest strategy. As mentioned in Section 3.1, the difference is mainly due to our use of simple uni-gram TF-IDF features. Anyway, our one-vs-rest is still competitive to BERT results in Chalkidis et al. (2022) on the last four problems. More importantly, the two extensions of one-vsrest (i.e., thresholding and cost-sensitive) improve almost all situations. For data sets ECtHR (A) and ECtHR (B), where originally one-vs-rest is significantly lower than BERT results in Chalkidis et al. (2022), the gap reduced considerably.\nFor the training time in Table 3, though the two extensions take more time than the basic one-vsrest strategy, all the linear methods are still hundreds of times faster than BERT. Further, linear methods were run on a CPU (Intel Xeon E5-2690), while for BERT we need a GPU (Nvidia V100). The model sizes listed in Table 4 also show that linear SVM requires a much smaller model than BERT, where details of our calculation are in Appendix D.\nThe results demonstrate that linear methods are useful baselines. They are extremely simple and efficient, but may yield competitive test performance.", "publication_ref": ["b1", "b1", "b1"], "figure_ref": [], "table_ref": ["tab_1", "tab_4", "tab_5"]}, {"heading": "Linear Methods can Help to See if", "text": "Advanced Methods Are Properly Used Surprisingly, our running of LibMultiLabel's BERT leads to worse test performance than linear methods on almost all data sets. More surprisingly, a comparison between the BERT results by LibMul-tiLabel and those in Chalkidis et al. (2022) shows  4 60.5 75.5 67.3 78.3 71.5 73.4 60.5 86.2 80.1 95.3  77.9  Chalkidis et al. (2022) 64.5 51.7 74.6 65.1 78.2 69.5 71.3 51.4 87.2 82.4 95.4 78.8 BERT Ours 61.9 55.6 69.8 60.5 67.1 55.9 70.8 55.3 87.0 80.7 95.4  80.3  Chalkidis et al. (2022) 71.2 63.6 79.7 73.4 68.3 58.3 71.4 57.2 87.6 81.8 95.6 81.3    1 by Chalkidis et al. (2022). More details are in Appendix D.\nMethod ECtHR (A) ECtHR (B) SCOTUS EUR-LEX LEDGAR UNFAIR-ToS \u00b5-F 1 m-F 1 \u00b5-F m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F\nthat the former is much worse on data sets EC-tHR (A) and ECtHR (B). Interestingly, from Section 4.1, only for these two sets the BERT results in Chalkidis et al. (2022) are much better than linear methods. Thus, our direct run of BERT in Lib-MultiLabel is a total failure. The training time is much longer than linear methods, but the resulting model is worse.\nIt is essential to check the discrepancy between the two BERT results. We find that Chalkidis et al. (2022) use some sophisticated settings to run BERT for the first three sets (i.e., ECtHR (A), ECtHR (B), and SCOTUS). They split every document into 64 segments, each of which has no more than 128 tokens, and apply BERT on each segment. Then, they collect the intermediate results as inputs to an upper-level transformer. After repeating the same process via LibMultiLabel, we can reproduce the results in Chalkidis et al. (2022); see details in Appendices E, F, and G.\nWe learned that they considered the more sophisticated setting of running BERT because by default, BERT considers only the first 512 tokens. Thus, for long documents, the training process may miss some important information. However, in practice, users may forget to check the document length and are not aware of the need to apply suitable settings. The above experiments demonstrate that BERT can achieve superior results if properly used, but sometimes, a direct run lead to poor outcomes. Linear methods can serve as efficient and robust baselines to confirm the proper use of an advanced approach.", "publication_ref": ["b1", "b1", "b1", "b1", "b1"], "figure_ref": [], "table_ref": ["tab_1", "tab_1"]}, {"heading": "Discussion and Conclusions", "text": "In our experiments, we encounter an issue of whether to incorporate the validation set for training the final model, which is used for predicting the test set. For linear methods, we follow the common practice to include the validation set for obtaining the final model. However, for BERT or some other deep learning models, the validation set is often used only for selecting the best epoch and/or the best hyper-parameters. To fully use the available data, we have investigated how to incorporate the validation set for BERT. Experimental results and more details are in Appendix H.\nFor some text sets evaluated in this work, we have seen that simple linear methods give competitive performance. The reason might be that each document in these sets is not short. 6 Then TF-IDF features are sufficiently informative so that linear methods work well. Across all NLP areas, an important issue now is when to use PLMs and when not. We demonstrate that when PLMs may not perform significantly better, traditional methods are much simpler and require fewer resources. However, having a simple quantitative measurement to pre-determine when to use which remains a challenging future research problem. In summary, the study reminds us of the importance of employing simple baselines in NLP applications.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "In this work, we do not propose any new methods because, as an opinion paper, we focus on raising the problems and making vivid demonstrations to readers. The experiments are limited to linear SVM and BERT on data sets in the benchmark LexGLUE. We hope that, within the page limit, our experiments sufficiently convey the points to readers.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "We ensure that our work complies with the ACL Ethics Policy.  training instances without any modification. Thus, in, for example, the one-vs-rest setting described in Section 3.1, an unlabeled instance is on the negative side in every binary problem. However, in evaluating the validation and test sets, they introduce an additional class to indicate the unlabeled data. Specifically, an unlabeled instance is associated with this \"unlabeled\" class, but not others. Chalkidis et al. (2022) consider this way to more seriously evaluate the model predictability on unlabeled instances. However, this setting is not a standard practice in multi-label classification, nor is it supported by LibMultiLabel. Thus we modify the scripts in LibMultiLabel to have the same evaluation setting as Chalkidis et al. (2022).", "publication_ref": ["b1", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "B Additional Details of Linear Methods", "text": "The binary linear SVM is in the following form.\nmin w 1 2 w \u22a4 w + C i \u03be(y i w \u22a4 x i ),(1)\nwhere (x i , y i ) are data-label pairs in the data set, y i = \u00b11, w is the parameters of the linear model, and \u03be(\u2022) is the loss function. The decision value function is f (x) = w \u22a4 x.\nFor one-vs-rest, please see descriptions in Section 3.1. We follow the default setting in LibMul-tiLabel by using C = 1. For more details about thresholding and cost-sensitive, please refer to the explanations in Lin et al. (2022).", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "C Differences Between Our", "text": "Implementation of Linear Methods and Chalkidis et al. (2022) We summarize the implementation differences between LibMultiLabel and Chalkidis et al. (2022) in Table 5.\nFor the data-preprocessing part, both use scikitlearn for TF-IDF feature generations. The meanings of each parameter are listed as follows.\nstop_words: Specify the list of stop words to be removed. For example, Chalkidis et al. (2022) set stop_words to \"english,\" so tokens that include in the \"english\" list are filtered. ngram_range: Specify the range of n-grams to be extracted. For example, LibMultiLabel only uses uni-gram, while Chalkidis et al. (2022) set ngram_range to (1, 3), so uni-gram, bi-gram, and tri-gram are extracted into the vocabulary list for a richer representation of the document. min_df: The parameter is used for removing infrequent tokens. Chalkidis et al. (2022) remove tokens that appear in less than five documents, while Lib-MultiLabel does not remove any tokens. max_features: The parameter decides the number of features to use by term frequency. For example, Chalkidis et al. (2022) consider the top 10,000, 20,000, and 40,000 frequent terms as the search space of the parameter.\nFor more detailed explanations, please refer to the TfidfVectorizer function in scikit-learn.\nThe binary classification problem in (1) is referred to as the primal form. The optimization problem can be transferred to the dual form and the optimal solutions of the two forms lead to the same decision function. Thus we can choose to solve the primal or the dual problem; see    ", "publication_ref": ["b1", "b1", "b1", "b1", "b1", "b1"], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "D Additional Details about Model Size", "text": "We calculate the model size of linear SVM by multiplying the number of TF-IDF features by the number of labels; see details in Table 6. For BERT, we directly copy the number of parameters from Chalkidis et al. (2022).\nE Additional Details about BERT Design in Chalkidis et al. (2022) E.1 Standard BERT for Classification\nThe setting considers the original implementation in Devlin et al. (2019). They truncate the documents to have at most 512 tokens. We then take a pre-trained BERT appended with an additional linear layer for fine-tuning.", "publication_ref": ["b1", "b1", "b2"], "figure_ref": [], "table_ref": ["tab_9"]}, {"heading": "E.2 Document Lengths", "text": "In Table 6, we present the document length for each data set in LexGLUE, the benchmark considered in Chalkidis et al. (2022). For ECtHR (A), ECtHR (B), SCOTUS, and EUR-LEX, the document lengths all exceed 512, the length limitation of BERT. Note that the numbers are underestimated because BERT uses a sub-word tokenizer that further tokenizes some words into sub-words. Chalkidis et al. (2022) design a variant of the standard BERT for ECtHR (A), ECtHR (B), and SCO-TUS to deal with long document lengths. The detailed steps are as follows.", "publication_ref": ["b1", "b1"], "figure_ref": [], "table_ref": ["tab_9"]}, {"heading": "E.3 Hierarchical BERT", "text": "\u2022 Each document is split into 64 segments, where each segment contains at most 128 tokens. \u2022 Each segment is then fed into BERT.\n\u2022 The [CLS] tokens generated from each segment Method   are collected and fed into an upper-level transformer encoder. \u2022 Max pooling is applied to the output of the transformer encoder. \u2022 The pooled results are then fed into a linear layer for the final prediction.\nECtHR (A) ECtHR (B) SCOTUS EUR-LEX LEDGAR UNFAIR-ToS \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 BERT in", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F Differences between the Two BERT Implementations", "text": "We summarize the implementation differences of BERT between LibMultiLabel and Chalkidis et al. (2022) in Table 7. Here we also try to reproduce results in Chalkidis et al. (2022) by using LibMul-tiLabel.\nFor LibMultiLabel, we explain our choices of hyper-parameters as follows. default: This method references the parameters chosen in an example configuration 7 from LibMul-tiLabel. tuned: This method performs a parameter search and is marked as \"our BERT\" in the main paper; see Table 8 for the search space and the chosen values. reproduced: This method aims to reproduce the BERT results from Chalkidis et al. (2022) using LibMultiLabel. We begin with imposing the same 7 https://github.com/ASUS-AICS/LibMultiLabel/ blob/master/example_config/EUR-Lex-57k/bert.yml weight_decay, learning_rate, and dropout values as Chalkidis et al. (2022) and also the same validation metric. However, for other parameters, which may less affect the results, we use the same values as default and tuned; see Table 7. Except SCOTUS and LEDGAR, we were able to generate similar results to those in Chalkidis et al. (2022). To fully reproduce the results on SCO-TUS and LEDGAR, we try to follow every setting did in Chalkidis et al. (2022). Specifically, we replace the PyTorch trainer originally used in Lib-MultiLabel with the Hugging Face trainer adopted in Chalkidis et al. (2022) and align some of the parameters with the ones used in Chalkidis et al. (2022); see a column in Table 7 for these two sets.\nLibMultiLabel supports standard BERT discussed in Appendix E.1. For the \"default\" and \"tuned\" settings, we directly run standard BERT. For the \"reproduced\" method, we follow Chalkidis et al. (2022) to use hierarchical BERT explained in Appendix E.3 for ECtHR (A), ECtHR (B), and SCOTUS and use standard BERT for other data sets.", "publication_ref": ["b1", "b1", "b1", "b1", "b1", "b1", "b1", "b1", "b1"], "figure_ref": [], "table_ref": ["tab_10", "tab_11", "tab_10", "tab_10"]}, {"heading": "G Detailed BERT Results", "text": "In Tables 9 and 10, we respectively present the test performance and the training time. For settings of running LibMultiLabel, see Appendix F. For BERT Method in Chalkidis et al. (2022), we present the following two results. paper: Results in the paper by Chalkidis et al. (2022) are directly copied. reproduced: Results from our running of their scripts. 8 For ECtHR (A), ECtHR (B), and SCOTUS, because there exist some issues when running the fp16 setting in our environment, we run the code of Chalkidis et al. (2022) by using fp32 instead. This change causes the time difference between the \"paper\" and \"reproduced\" settings in Table 10. Except numbers borrowed from Chalkidis et al. (2022), we run five seeds for all BERT experiments and report the mean test performance over all seeds. Chalkidis et al. (2022) also run five seeds, but their test scores are based on the top three seeds with the best Macro-F1 on validation data.\nECtHR (A) ECtHR (B) SCOTUS EUR-LEX LEDGAR UNFAIR-ToS \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 BERT in\nFor the \"tuned\" setting, because the version of LibMultiLabel that we used does not store the checkpoint after hyper-parameter search, we must conduct the training again using the best hyperparameters. Thus, the total time includes hyperparameter search and the additional training. 9 In Appendix I, we give an additional case study to assess the performance of the hierarchical BERT when documents are long. For linear methods, in contrast to deep learning methods, they do not need a validation set for the termination of the optimization process or for selecting the iteration that yields the best model. Further, they may internally conduct crossvalidation to select hyper-parameters (e.g., thresholds in the thresholding method). Therefore, we combine training and validation subsets as the new training set used by the linear methods. This is the standard setting in traditional supervised learning.\nFor BERT training, the validation set is used for selecting the best epoch and/or the best hyperparameters. We follow the common practice to deploy the model achieving the best validation performance for prediction. However, in linear methods, the model used for prediction, regardless of whether internal cross-validation is needed, is always obtained by training on all available data (i.e., the combination of training and validation sets). Therefore, for BERT we may also want to incorporate the validation set for the final model training.\nWe refer to such a setting as the re-training process. Unfortunately, an obstacle is that the optimization process cannot rely on a validation set for terminating the process or selecting the best model in all iterations. Following Goodfellow et al. (2016) (Lang, 1995) but did not check the document length. To assess the importance of the document length, we downloaded the 20 Newsgroups set from scikit-learn 10 with default 10 See https://scikit-learn.org/stable/modules/ generated/sklearn.datasets.fetch_20newsgroups. html for more details.\nWe have checked that the set used in scikit-learn is the same as the \"20 News-parameters. Further, we checked the document length from the word and token levels where the tokens are obtained by the \"bert-base-uncased\" tokenizer. The data statistics are presented in Table 12. We found that the 20 Newsgroups data set includes a considerable number of documents that exceed 512 tokens. This may be an issue because BERT can only process up to 512 tokens without further design; see Appendix E for more details. To investigate this problem, we conducted experiments using both linear classifiers and BERT. Results are in Table 13. The observations are summarized as follows.\n\u2022 The results of linear classifiers do not improve by using thresholding and cost-sensitive techniques to handle class imbalance. The reason is that the data set has a small number of labels and a more balanced class distribution. In addition, linear methods are still competitive with BERT. \u2022 The tuned setting of BERT has the best Micro-F1 among all the methods. Thus, for running BERT on this set, parameter selection seems to be important. Interestingly, when we considered the document length using the hierarchical methods in Appendix E.3, the performance was not better than the tuned setting.\nIn conclusion, linear methods are still a simple and efficient solution to this problem. For BERT, we showed that using the hierarchical setting to handle long document length may not always lead to the best performance. The result of applying hierarchical BERT may be data-dependent. Thus a general setting for handling long documents still need to be investigated. B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Left blank.\nB4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Left blank.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Left blank. B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.", "publication_ref": ["b1", "b1", "b1", "b1", "b1", "b6", "b8"], "figure_ref": [], "table_ref": ["tab_13", "tab_14", "tab_1", "tab_4"]}, {"heading": "Left blank.", "text": "C Did you run computational experiments? Section 4.\nC1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? Section 4.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This work was supported by NSTC of Taiwan grant 110-2221-E-002-115-MY3 and ASUS Intelligent Cloud Services. The authors thank Ming-Wei Chang and reviewers for constructive comments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Issue about Data without Labels", "text": "For multi-label problems considered in Chalkidis et al. (2022), instances that are not associated with any labels, called unlabeled instances as follows, account for a considerable portion in some data sets: ECtHR (A) (11.3%), ECtHR (B) (1.6%) and UNFAIR-ToS (89.0%). In the training process, Chalkidis et al. (2022) keep the unlabeled", "publication_ref": ["b1", "b1"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "and Vladimir Vapnik. 1992. A training algorithm for optimal margin classifiers", "journal": "ACM Press", "year": "", "authors": "Bernhard E Boser; Isabelle Guyon"}, {"ref_id": "b1", "title": "LexGLUE: A benchmark dataset for legal language understanding in English", "journal": "", "year": "2022", "authors": "Ilias Chalkidis; Abhik Jana; Dirk Hartung; Michael Bommarito"}, {"ref_id": "b2", "title": "BERT: pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b3", "title": "LIBLINEAR: a library for large linear classification", "journal": "Journal of Machine Learning Research", "year": "2008", "authors": "Kai-Wei Rong-En Fan; Cho-Jui Chang; Xiang-Rui Hsieh; Chih-Jen Wang;  Lin"}, {"ref_id": "b4", "title": "A study on threshold selection for multi-label classification", "journal": "", "year": "2007", "authors": "Chih-Jen Rong-En Fan;  Lin"}, {"ref_id": "b5", "title": "On the costeffectiveness of stacking of neural and non-neural methods for text classification: Scenarios and performance prediction", "journal": "", "year": "2021", "authors": "Christian Gomes; Marcos Andr\u00e9 Gon\u00e7alves; Leonardo Rocha; S\u00e9rgio D Canuto"}, {"ref_id": "b6", "title": "Deep Learning", "journal": "The MIT Press", "year": "2016", "authors": "Ian J Goodfellow; Yoshua Bengio; Aaron Courville"}, {"ref_id": "b7", "title": "A statistical interpretation of term specificity and its application in retrieval", "journal": "Journal of Documentation", "year": "1972", "authors": "Karen S Jones"}, {"ref_id": "b8", "title": "Newsweeder: Learning to filter netnews", "journal": "", "year": "1995", "authors": "Ken Lang"}, {"ref_id": "b9", "title": "RCV1: A new benchmark collection for text categorization research", "journal": "Journal of Machine Learning Research", "year": "2004", "authors": "David D Lewis; Yiming Yang; Tony G Rose; Fan Li"}, {"ref_id": "b10", "title": "On the use of unrealistic predictions in hundreds of papers evaluating graph representations", "journal": "", "year": "2022", "authors": "Li-Chung Lin; Cheng-Hung Liu; Chih-Ming Chen; Kai-Chin Hsu; I-Feng Wu; Ming-Feng Tsai; Chih-Jen Lin"}, {"ref_id": "b11", "title": "The automatic creation of literature abstracts", "journal": "IBM Journal of Research and Development", "year": "1958", "authors": "Hans Peter Luhn"}, {"ref_id": "b12", "title": "Optimizing f-measures by cost-sensitive classification", "journal": "", "year": "2014", "authors": "A Shameem; Nicolas Puthiya Parambath; Yves Usunier;  Grandvalet"}, {"ref_id": "b13", "title": "A comparison of svm against pre-trained language models (plms) for text classification tasks", "journal": "Springer", "year": "2023", "authors": "Yasmen Wahba; Nazim Madhavji; John Steinbacher"}, {"ref_id": "b14", "title": "A study on thresholding strategies for text categorization", "journal": "ACM Press", "year": "2001", "authors": "Yiming Yang"}, {"ref_id": "b15", "title": "PECOS: Prediction for enormous and correlated output spaces", "journal": "Journal of Machine Learning Research", "year": "2022", "authors": "Hsiang-Fu Yu; Kai Zhong; Jiong Zhang; Wei-Cheng Chang; Inderjit S Dhillon"}, {"ref_id": "b16", "title": "Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? Appendix F", "journal": "", "year": "", "authors": ""}, {"ref_id": "b17", "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run", "journal": "Appendix G", "year": "", "authors": ""}, {"ref_id": "b18", "title": "for preprocessing, for normalization, or for evaluation", "journal": "", "year": "", "authors": ""}, {"ref_id": "b19", "title": "crowdworkers) or research with human participants? Left blank", "journal": "", "year": "", "authors": ""}, {"ref_id": "b20", "title": "Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators", "journal": "", "year": "", "authors": " D1"}, {"ref_id": "b21", "title": "crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic", "journal": "", "year": "", "authors": ""}, {"ref_id": "b22", "title": "Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?", "journal": "", "year": "", "authors": " D3"}, {"ref_id": "b23", "title": "Was the data collection protocol approved (or determined exempt) by an ethics review board? No response", "journal": "", "year": "", "authors": " D4"}, {"ref_id": "b24", "title": "Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response", "journal": "", "year": "", "authors": " D5"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "groups sorted by date\" set from the original source at http://qwone.com/~jason/20Newsgroups/. 1886 ACL 2023 Responsible NLP Checklist A For every submission: A1. Did you describe the limitations of your work? Section Limitations. A2. Did you discuss any potential risks of your work? Left blank. A3. Do the abstract and introduction summarize the paper's main claims? Sections Abstract and 1. A4. Have you used AI writing assistants when working on this paper? Left blank. B Did you use or create scientific artifacts? Section 3. B1. Did you cite the creators of artifacts you used? Section 3. B2. Did you discuss the license or terms for use and / or distribution of any artifacts? Left blank.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "1 Linear one-vs-rest 64.0 53.1 72.8 63.9 78.1 68.9 72.0 55.4 86.4 80.0 94.9 75.1 thresholding 68.6 64.9 76.1 68.7 78.9 71.5 74.7 62.7 86.2 79.9 95.1", "figure_data": "79.9cost-sensitive67."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Micro-F1 (\u00b5-F 1 ) and Macro-F1 scores (m-F 1 ) for our investigation on two types of approaches: linear SVM and BERT. For each type, we show results achieved by LibMultiLabel and scores reported inChalkidis et al. (2022). In each column, the best result is bold-faced.", "figure_data": "MethodECtHR (A) ECtHR (B) SCOTUS EUR-LEX LEDGAR UNFAIR-ToSLinearone-vs-rest28s29s1m 11s4m 2s28s2sthresholding59s1m 0s2m 11s28m 8s3m 26s3scost-sensitive1m 38s1m 43s3m 28s50m 36s4m 45s4sChalkidis et al. (2022)N/AN/AN/AN/AN/AN/ABERTOurs5h 8m5h 51m3h 21m38h 14m43h 48m4h 5mChalkidis et al. (2022)3h 42m3h 9m1h 24m3h 36m6h 9mN/A"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Training time for our multiple settings on linear SVM and BERT. We show results from running LibMul-tiLabel and values reported inChalkidis et al. (2022). Note thatChalkidis et al. (2022) use fixed parameters for BERT, while for our BERT, we use 4 GPUs to conduct the hyper-parameter search and report the total time used.", "figure_data": "MethodECtHR (A) ECtHR (B) SCOTUS EUR-LEX LEDGAR UNFAIR-ToSLinear924K924K2M15M2M50KBERT variants110M \u223c 149M"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "A comparison between the model size of linear methods and BERT variants. Note that all three linear methods in LibMultiLabel have the same model size. For BERT variants, we borrow the calculation in Table", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Key differences in the one-vs-rest linear method between the default setting of LibMultiLabel and the implementation inChalkidis et al. (2022). Any values covered by [] mean the hyper-parameter search space. See Appendix C for details of hyper-parameters.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "For the model training, they both use the solver provided byLIBLINEAR (Fan et al., 2008).", "figure_data": "PropertyECtHR (A) ECtHR (B) SCOTUS EUR-LEX LEDGAR UNFAIR-ToS# labels1010131008W1,662.081,662.086,859.871,203.92112.9832.70# features92,40292,402126,406147,46519,9976,291"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Data statistics for LexGLUE, the benchmark considered inChalkidis et al. (2022). W means the average # words per instance of the whole set. The # features indicates the # TF-IDF features used by linear methods.", "figure_data": "LibMultiLabelParameterdefaulttunedreproduced SCOTUS otherChalkidis et al. (2022)LEDGAR problemsmaximum #epochs1515201520weight_decay0.0010000patience55553val_metricMicro-F1Micro-F1Micro-F1 Micro-F1Micro-F1early_stopping_metric Micro-F1Micro-F1lossMicro-F1losslearning_rate dropout5e-5 0.1See Table 83e-5 0.13e-5 0.13e-5 0.1"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Parameter differences of BERT between LibMultiLabel andChalkidis et al. (2022). For the meaning of each parameter, please refer to the software LibMultiLabel.", "figure_data": "ParameterECtHR (A) ECtHR (B) SCOTUS EUR-LEX LEDGAR UNFAIR-ToSmax_seq_lengthspace selected512512[128, 512] 512 512512512learning_ratespace selected2e-53e-5[2e-5, 3e-5, 5e-5] 2e-5 5e-52e-53e-5dropoutspace selected0.10.20.1[0.1, 0.2] 0.10.20.1"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Hyper-parameter search space and the selected values of LibMultiLabel's tuned setting.", "figure_data": ""}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "", "figure_data": ": Micro-F1 (\u00b5-F 1 ) and Macro-F1 scores (m-F 1 ) for our investigation on BERT.MethodECtHR (A) ECtHR (B) SCOTUS EUR-LEX LEDGAR UNFAIR-ToSBERT in LibMultiLabeldefault59m 48s1h 2m39m 49s6h 38m8h 44m47m 48stuned5h 8m5h 51m3h 21m38h 14m43h 48m4h 5mreproduced10h 27m9h 41m9h 26m6h 37m5h 49m15m 9sBERT in Chalkidis et al. (2022)paper3h 42m3h 9m1h 24m3h 36m6h 9mN/Areproduced7h 56m6h 59m7h 5m4h 30m5h 11m7m 3s"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Training time for our multiple settings on BERT. The average time of running five seeds is reported.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "Record the number of training steps that leads to the best validation Micro-F1 as e * . 2. Re-train the final model using the combination of training and validation sets for e * epochs. BERT results without/with re-training are shown in Table 11. In general, the re-training process improves the performance, especially for the data sets SCOTUS and EUR-LEX. However, results are slightly worse in both the default and tuned settings for the data set UNFAIR-ToS. Thus the outcome of re-training may be data-dependent. A comparison between linear methods and BERT with re-training shows that conclusions made earlier remain the same. Because re-training", "figure_data": ", we consider the following setting to train the combined set. 1. Property Value # training instances 10,182 # validation instances 1,132 # test instances 7,532 # classes 20 W 283.66 W max 11,821 T 552.82 T max 138,679 # documents 4,927 (26.14%) exceeding 512 tokens"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "Data statistics for 20 Newsgroups. We conduct a 90/10 split to obtain the validation data. W /T means the average # words/tokens per instance of the whole set, and W max /T max means the maximum # words/tokens of the whole set.", "figure_data": "Method\u00b5-F 1 m-F 1Linearone-vs-rest85.3 84.6thresholding85.3 84.6cost-sensitive 85.2 84.5BERTdefault84.0 83.3tuned hierarchical85.6 84.9 84.9 84.2"}, {"figure_label": "13", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "Experimental results of 20 Newsgroups by linear methods and BERT. For the default setting, we follow the default parameters in Table7. For the tuned and hierarchical setting, we use the same parameter search range as the one in Table8. Further, to process the set for the hierarchical setting, each document is split into 40 segments based on the presence of consecutive newline characters, where each segment contains at most 128 tokens.", "figure_data": "is not conducted in Chalkidis et al. (2022), inthe main paper we report the results without re-training.I A Case Study of BERT on 20 NewsgroupsWahba et al. (2023) applied BERT for training thedata set 20 Newsgroups"}], "formulas": [{"formula_id": "formula_0", "formula_text": "ECtHR (A) ECtHR (B) SCOTUS EUR-LEX LEDGAR UNFAIR-ToS # \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1 T", "formula_coordinates": [2.0, 138.88, 70.05, 362.86, 24.24]}, {"formula_id": "formula_1", "formula_text": ".0 N/A 110M Table 1: Micro-F1 scores (\u00b5-F 1 )", "formula_coordinates": [2.0, 70.55, 177.59, 449.14, 34.45]}, {"formula_id": "formula_2", "formula_text": "Method ECtHR (A) ECtHR (B) SCOTUS EUR-LEX LEDGAR UNFAIR-ToS \u00b5-F 1 m-F 1 \u00b5-F m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F", "formula_coordinates": [4.0, 70.86, 69.93, 450.82, 27.83]}, {"formula_id": "formula_3", "formula_text": "min w 1 2 w \u22a4 w + C i \u03be(y i w \u22a4 x i ),(1)", "formula_coordinates": [7.0, 105.95, 529.42, 183.91, 30.64]}, {"formula_id": "formula_4", "formula_text": "ECtHR (A) ECtHR (B) SCOTUS EUR-LEX LEDGAR UNFAIR-ToS \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 BERT in", "formula_coordinates": [9.0, 70.86, 69.93, 451.87, 40.96]}, {"formula_id": "formula_5", "formula_text": "ECtHR (A) ECtHR (B) SCOTUS EUR-LEX LEDGAR UNFAIR-ToS \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 BERT in", "formula_coordinates": [10.0, 78.67, 69.93, 432.26, 40.96]}], "doi": "10.18653/v1/n19-1423"}