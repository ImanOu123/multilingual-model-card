{"title": "Learning the Depths of Moving People by Watching Frozen People", "authors": "Zhengqi Li; Tali Dekel; Forrester Cole; Richard Tucker; Noah Snavely; Ce Liu; William T Freeman", "pub_date": "2019-04-25", "abstract": "Moving people, moving camera Figure 1. Our model predicts dense depth when both an ordinary camera and people in the scene are freely moving (right). We train our model on our new MannequinChallenge dataset-a collection of Internet videos of people imitating mannequins, i.e., freezing in diverse, natural poses, while a camera tours the scene (left). Because people are stationary, geometric constraints hold; this allows us to use multi-view stereo to estimate depth which serves as supervision during training. 2 ", "sections": [{"heading": "Introduction", "text": "A hand-held camera viewing a dynamic scene is a common scenario in modern photography. Recovering dense geometry in this case is a challenging task: moving objects violate the epipolar constraint used in 3D vision, and are often treated as noise or outliers in existing structure-from-motion (SfM) and multi-view stereo (MVS) methods. Human depth perception, however, is not easily fooled by object motion-rather, we maintain a feasible interpretation of the objects' geometry and depth ordering even if both objects and the observer are moving, and even when the scene is observed with just one eye [11]. In this work, we take a step towards achieving this ability computationally.\nWe focus on the task of predicting accurate, dense depth from ordinary videos where both the camera and people in the scene are naturally moving. We focus on humans for two reasons: i) in many applications (e.g., augmented reality), humans constitute the salient objects in the scene, and ii) human motion is articulated and difficult to model. By taking a data-driven approach, we avoid the need to explicitly impose assumptions on the shape or deformation of people, but rather learn these priors from data.\nWhere do we get data to train such a method? Generating high-quality synthetic data in which both the camera and the people in the scene are naturally moving is very challenging. Depth sensors (e.g., Kinect) can provide useful data, but such data is typically limited to indoor environments and requires significant manual work in capture and process. Furthermore, it is difficult to gather people of different ages and genders with diverse poses at scale. Instead, we derive data from a surprising source: YouTube videos in which people imitate mannequins, i.e., freeze in elaborate, natural poses, while a hand-held camera tours the scene (Fig. 2). These videos comprise our new MannequinChallenge (MC) dataset, which we plan to release for the research community.\nBecause the entire scene, including the people, is stationary, we estimate camera poses and depth using SfM and MVS, and use this derived 3D data as supervision for training.\nIn particular, we design and train a deep neural network that takes an input RGB image, a mask of human regions, and an initial depth of the environment (i.e., non-human regions), and outputs a dense depth map over the entire image, both the environment and the people (see Fig. 1). Note that the initial depth of the environment is computed using motion parallax between two frames of the video, providing the network with information not available from a single frame. Once trained, our model can handle natural videos with arbitrary camera and human motion.\nWe demonstrate the applicability of our method on a variety of real-world Internet videos, shot with a hand-held camera, depicting complex human actions such as walking, running, and dancing. Our model predicts depth with higher accuracy than state-of-the-art monocular depth prediction and motion stereo methods. We further show how our depth maps can be used to produce various 3D effects such as synthetic depth-of-field, depth-aware inpainting, and inserting virtual objects into the 3D scene with correct occlusion.\nIn summary, our contributions are: i) a new source of data for depth prediction consisting of a large number of Internet videos in which the camera moves around people \"frozen\" in natural poses, along with a methodology for generating accurate depth maps and camera poses; and ii) a deep-network-based model designed and trained to predict dense depth maps in the challenging case of simultaneous camera motion and complex human motion.", "publication_ref": ["b10"], "figure_ref": ["fig_0", "fig_4"], "table_ref": []}, {"heading": "Related Work", "text": "Learning-based depth prediction. Numerous algorithms, based on both supervised and unsupervised learning, have recently been proposed for predicting dense depth from a single RGB image [46,17,7,6,3,19,33,8,52,49,21,41]. Some recent learning based methods also consider multiple images, either assuming known camera poses [12,47] or simultaneously predicting camera poses along with depth [39,51]. However, none of them is designed to predict the depth of dynamic objects, which is the focus of our work.\nDepth estimation for dynamic scenes. RGBD data has been widely used for 3D modeling of dynamic scenes [25,55,48,5,14], but only a few methods attempt to estimate depth from a monocular camera. Several methods have been proposed to reconstruct sparse geometry of a dynamic scene [27,50,36,40]. Russell et al. [31] and Ranftl et al. [29] suggest motion/object segmentation based algorithms to decompose a dynamic scene into piecewise rigid parts. However, these methods impose strong assumptions of the object's motion that are violated by articulated human motion. Konstantinos et al. [30] predict depth of moving soccer players using synthetic training data from FIFA video games. However, their method is limited to soccer players, and cannot handle general people in the wild.\nRGBD data for learning depth. There are a number of RGBD datasets of indoor scenes, captured using depth sensors [35,2,4,45] or synthetically rendered [37]. However, none of these datasets provide depth supervision for moving people in natural environments. Several action recognition methods use depth sensors to capture human actions [54,34,22,26], but most use a static camera and provide only a limited number of indoor scenes. REFRESH [20] is a recent semi-synthetic scene flow dataset created by overlaying animated people on NYUv2 images. Here too, the data is limited to interiors and consists of synthetic humans placed in unrealistic configurations with their surrounding.\nHuman shape and pose prediction. Recovery of a posed 3D human mesh from a single RGB image has attracted significant attention [18,9,16,1,28,23]. Recent methods achieve impressive results on natural images spanning a variety of poses. However, such methods only model the human body, disregarding hair, clothing, and the non-human parts of the scenes. Finally, many of these methods rely on correctly detecting human keypoints, requiring most of the body to be within the frame.", "publication_ref": ["b46", "b16", "b6", "b5", "b2", "b18", "b32", "b7", "b52", "b49", "b20", "b41", "b11", "b47", "b39", "b51", "b24", "b55", "b48", "b4", "b13", "b26", "b50", "b35", "b40", "b30", "b28", "b29", "b34", "b1", "b3", "b45", "b37", "b54", "b33", "b21", "b25", "b19", "b17", "b8", "b15", "b0", "b27", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "MannequinChallenge Dataset", "text": "The Mannequin Challenge [42] is a popular video trend in which people freeze in place-often in an interesting pose-while the camera operator moves around the scene filming them (e.g., Fig. 2). Thousands of such videos have been created and uploaded to YouTube since late 2016. To the extent that people succeed in staying still during the videos, we can assume the scenes are static and obtain accurate camera poses and depth information by processing them with SfM and MVS algorithms. We found around 2,000 candidate videos for which this processing is possible. These videos comprise our new MannequinChallenge (MC) Dataset, which spans a wide range of scenes with people of different ages, naturally posing in different group configurations. We next describe in detail how we process the videos and derive our training data.\nEstimating camera poses. Following a similar approach to Zhou et al. [53], we use ORB-SLAM2 [24] to identify trackable sequences in each video and to estimate an initial camera pose for each frame. At this stage, we process a lower-resolution version of the video for efficiency, and set the field of view to 60 degrees (typical value for modern cell-phone cameras). We then reprocess each sequence at a higher resolution using a visual SfM system [32], which refines the initial camera poses and intrinsic parameters. This method extracts and matches features across frames, then performs a global bundle adjustment optimization. Finally, sequences with non-smooth camera motion are removed using the technique of Zhou et al. [53].\nComputing dense depth with MVS. Once the camera poses for each clip are estimated, we then reconstruct each scene's dense geometry. In particular, we recover per-frame dense depth maps using COLMAP, a state-of-the-art MVS system [33].\nBecause our data consists of challenging Internet videos that involve camera motion blur, shadows, reflections, etc., the raw depth maps estimated by MVS are often too noisy for training purposes. We address this issue by a careful depth filtering mechanism. We first filter outlier depths using the depth refinement method of [19]. We further remove erroneous depth values by considering the consistency of the MVS depth and the depth obtained from motion parallax between two frames. Specifically, for each frame, we compute a normalized error \u2206(p) for every valid pixel p:\n\u2206(p) = |D MVS (p) \u2212 D pp (p)| D MVS (p) + D pp (p)(1)\nwhere D MVS is the depth map obtained by MVS and D pp is the depth map computed from two-frame motion parallax (see Sec. 4.1). Depth values for which \u2206(p) > \u03b4 are removed, where we empirically set \u03b4 = 0.2. Fig. 3 shows sample frames from our processed sequences with corresponding estimated MVS depths after filtering. See the supplemental material for examples illustrating the effect of the proposed cleaning approach.\nFiltering clips. Several factors can make a video clip unsuitable for training. For example, people may \"unfreeze\" (start moving) at some point in the video, or the video may contain synthetic graphical elements in the background. Dynamic objects and synthetic backgrounds do not obey multi-view geometric constraints and hence are treated as outliers and filtered out by MVS, potentially leaving few valid pixels. Therefore, we remove frames where < 20% of pixels have valid MVS depth after our two-pass cleaning stage.\nFurther, we remove frames where the estimated radial distortion coefficient |k 1 | > 0.1 (indicative of a fisheye camera) or where the estimated focal length is \u2264 0.6 or \u2265 1.2 (camera parameters are likely inaccurate). We keep sequences that are at least 30 frames long, have an aspect ratio of 16:9, and have a width of \u2265 1600 pixels. Finally, we manually inspect the trajectories and point clouds of the remaining sequences and remove obviously incorrect reconstructions. Examples of removed images are shown in the supplemental material. After processing, we obtain 4,690 sequences with a total of more then 170K valid image-depth pairs. We split our MC dataset into training, validation and testing sets with a 80:3:17 split over clips.", "publication_ref": ["b42", "b53", "b23", "b31", "b53", "b32", "b18"], "figure_ref": ["fig_0", "fig_2"], "table_ref": []}, {"heading": "Depth Prediction Model", "text": "We train our depth prediction model on the Mannequin-Challenge dataset in a supervised manner, i.e., by regressing to the depth generated by the MVS pipeline. A key question is how to structure the input to the network to allow training on frozen people but inference on freely moving people. One option is to regress from a single RGB image to depth, but this approach disregards geometric information about the static regions of the scene that is available by considering more than a single view. To benefit from such information, we input to the network a depth map for the static, non-human regions, estimated from motion parallax w.r.t. another view of the scene.\nThe full input to our network, illustrated in Fig. 3, includes a reference image I r , a binary mask of human regions M , a depth map estimated from motion parallax (with human regions removed) D pp , a confidence map C, and an optional human keypoint map K. We assume known, accurate camera poses from SfM during both training and inference. In an online inference setting, camera poses can be obtained by visual-inertial odometry. Given these inputs, the network predicts a full depth map for the entire scene. To match the MVS depth values, the network must inpaint the depth in human regions, refine the depth in non-human regions from the estimated D pp , and finally make the depth of entire scene consistent.\nOur network architecture is a variant of the hourglass network of [3], with the nearest-neighbor upsampling layers replaced by bilinear upsampling layers.\nThe following sections describe our model inputs and training losses in detail. In the supplemental material we provide additional implementation details and full derivations.", "publication_ref": ["b2"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Depth from motion parallax", "text": "Motion parallax between two frames in a video provides our initial depth estimate for the static regions of the scene (assuming humans are dynamic while the rest of the scene is static). Given a reference image I r and source image I s pair, we estimate an optical flow field from I r to I s using FlowNet2.0 [13]. Using the relative camera poses between the two views, we compute an initial depth map D pp from the estimated flow field, using the Plane-Plus-Parallax (P+P) representation [15,43].\nIn some cases, such as forward/backward relative camera motion between the frames, the estimated depth may be ill-defined in some image regions (i.e., the epipole may be located within the image). We detect and filter out such depth values as described in Sec. 4.2.\nKeyframe selection. Depth from motion parallax may be ill-posed if the 2D displacement between two views is small or well-approximated by a homography (e.g., in the case of pure camera rotation). To avoid such cases, we apply a baseline criterion when selecting a reference frame I r and a corresponding source keyframe I s . We want the two views to have significant overlap, while having sufficient baseline. Formally, for each I r , we find the index s of I s as\ns = arg max j d rj o rj (2)\nwhere d rj is the L 2 distance between the camera centers of I r and its neighbor frame I j . The term o rj is the fraction of co-visible SfM features in I r and I j :\no rj = 2|V r V j | |V r | + |V j | ,(3)\nwhere V j is the set of features visible in I j . We discard pairs of frames for which o rj < \u03c4 o , i.e., the fraction of co-visible features should be larger than a threshold \u03c4 o (we set \u03c4 o = 0.6), and limit the maximum frame interval to 10.\nWe found these view selection criteria to work well in our experiments.", "publication_ref": ["b12", "b14", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "Confidence", "text": "Our data consists of challenging Internet video clips with camera motion blur, shadows, low lighting, and reflections. In such cases, optical flow is often noisy [44], compounding uncertainty in the input depth map, D pp . We thus estimate, and input to the network, a confidence map, C. This allows the network to rely more on the input depth in highconfidence regions, and potentially use it to improve its prediction in low-confidence regions. The confidence value at each pixel p in the non-human regions is defined as:\nC(p) = C lr (p)C ep (p)C pa (p). (4\n)\nThe term C lr measures \"left-right\" consistency between the forward and backward flow fields. That is, C lr (p) = max 0, 1 \u2212 r(p) 2 , where r(p) is the forward-backward warping error. For perfectly consistent forward and backward flows C lr = 1, while C lr = 0 when the error is greater than 1px.\nThe term C ep measures how well the flow field complies with the epipolar constraint between the views [10]. Specifically, C ep (p) = max 0, 1 \u2212 (\u03b3(p)/\u03b3) 2 , where \u03b3(p) is the distance between the warped pixel position of p based on its optical flow and its corresponding epipolar line;\u03b3 controls the epipolar distance tolerance (we set\u03b3 = 2px in our experiments).\nFinally, C pa assigns low confidence to pixels for which the parallax between the views is small [33]. This is measured by the angle \u03b2(p) between the camera rays meeting at\nthe pixel p. That is, C pa (p) = 1\u2212 min(\u03b2,\u03b2(p))\u2212\u03b2 \u03b2 2\n, wher\u0113 \u03b2 is the angle tolerance (we use\u03b2 = 1\u00b0in our experiments).  Qualitative results on the MC test set. From top to bottom: reference images and their corresponding MVS depth (pseudo ground truth); our depth predictions using: our single view model (third row) and our two-frame model (forth row). The additional network inputs give improved performance in both human and non-human regions.", "publication_ref": ["b44", "b9", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Losses", "text": "We train our network to regress to depth maps computed by our data pipeline. Because the computed depth values have arbitrary scale, we use a scale-invariant depth regression loss. That is, our loss is computed on log-space depth values and consists of three terms:\nL si = L MSE + \u03b1 1 L grad + \u03b1 2 L sm .(5)\nScale-invariant MSE. L MSE denotes the scale-invariant mean square error (MSE) [6]. This term computes the squared, log-space difference in depth between two pixels in the prediction and the same two pixels in the ground-truth, averaged over all pairs of valid pixels. Intuitively, we look at all pairs of points, and penalize the difference in their ratio of depth values w.r.t. ground truth.\nMulti-scale gradient term. We use a multi-scale gradient term, L grad , which is the L 1 difference between the predicted log depth derivatives (in x and y directions) and the ground truth log depth derivatives, at multiple scales [19]. This term allows the network to recover sharp depth discontinuities and smooth gradient changes in the predicted depth images.\nMulti-scale, edge-aware smoothness terms. To encourage smooth interpolation of depth in texture-less regions where MVS fails to recover depth, we use a simple smoothness term, L sm , which penalizes L 1 norm of log depth derivatives based on the first-and second-order derivatives of images and is applied at multiple scales [41]. This term encourages piecewise smoothness in depth regions where there is no image intensity change. (III.) masked input depth, human mask, and additional confidence for IV.; in V, we also input human keypoints. Lower is better for all metrics.", "publication_ref": ["b5", "b18", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We tested our method quantitatively and qualitatively and compare it with several state-of-the-art single-view and motion-based depth prediction algorithms. We show additional qualitative results on challenging Internet videos with complex human motion and natural camera motion, and demonstrate how our predicted depth maps can be used for several visual effects.\nError metrics. We measure error using the scale-invariant RMSE (si-RMSE), equivalent to \u221a L MSE , described in Sec. 4.3. We evaluate si-RMSE on 5 different regions: sifull measures the error between all pairs of pixels, giving the overall accuracy across the entire image; si-env measures pairs of pixels in non-human regions E, providing depth accuracy of the environment; and si-hum measures pairs where at least one pixel lies in the human region H, providing depth accuracy for people. si-hum can further be divided into two error measures: si-intra measures si-RMSE within H, or human accuracy independent of the environment; si-inter  [7], (e) two-frame motion stereo DeMoN [39], (f-g) depth predictions from our single view and two-frame models, respectively. measures si-RMSE between pixels in H and in E, or human accuracy w.r.t. the environment. We include derivations in the supplemental material.", "publication_ref": ["b6", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation on the MC test set", "text": "We evaluated our method on our MC test set, which consists of more than 29K images taken from 756 video clips. Processed MVS depth values D MVS obtained by our pipeline (see Sec. 3) are considered as ground truth.\nTo quantify the importance of our designed model's input, we compare the performance of several models, each trained on our MC dataset with a different input configuration. The two main configurations are: (i) a single-view model (input is RGB image) and (ii) our full two-frame model, where the input includes a reference image, an initial masked depth map D pp , a confidence map C, and a human mask M . We also perform ablation studies by replacing the input depth with optical flow F , removing C from the input, and adding a human keypoint map K.\nQuantitative evaluations are shown in Table 1. By comparing rows (I), (III) and (IV), it is clear that adding the initial depth of environment as well as a confidence map significantly improves the performance for both human and non-human regions. Adding human keypoint locations to the network input further improves performance. Note that if we input an optical flow field to the network instead of depth (II), the performance is only on par with the single view method. The mapping from 2D optical flow to depth depends on the relative camera poses, which are not given to the network. This result indicates that the network is not able to implicitly learn the relative poses and extract the depth information.\nFig. 4 shows qualitative comparisons between our singleview model (I) and our full model (ID pp CM K). Our full model results are more accurate in both human regions (e.g., first column) and non-human regions (e.g., second column). In addition, the depth relations between people and their surroundings are improved in all examples.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Evaluation on TUM RGBD dataset", "text": "We used a subset of the TUM RGBD dataset [38], which contains indoor scenes of people performing complex actions, captured from different camera poses. Sample images from this dataset are shown in Fig. 5(a-b).\nTo run our model, we first estimate camera poses using ORB-SLAM2 3 . In some cases, due to severe low image quality, motion blur and rolling shutter effects, the estimated camera poses may be incorrect. We manually filter such failures by inspecting the camera trajectory and point cloud. In total, we obtain 11 valid image sequences with 1,815 images in total for evaluations.\nWe compare our depth predictions (using our MC trained models) with several state-of-the-art monocular depth prediction methods trained on indoor NYUv2 [17,46,7]   in the Wild (DIW) datasets [3], and the recent two-frame stereo model DeMoN [39], which assumes a static scene. We also compare with Video-Popup [31], which deals with dynamic scenes. We use the same image pairs for computing D pp as inputs to DeMoN and Video-Popup.\nQuantitative comparisons are show in Table 2, where we report 5 different scale-invariance error measures as well as standard RMSE and relative error; the last two are computed by applying a single scaling factor that aligns the predicted and ground-truth depth in the least-squares sense. Our single-view model already outperforms the other singleview models,demonstrating the benefit of the MC dataset for training. Note that VideoPopup [31] failed to produce meaningful results due to the challenging camera and ob-ject motion. Our full model, by making use of the initial (masked) depth map, significantly improves performance for all the error measures. Consistent with our MC test set results, when we use optical flow as input (instead of initial depth map) the performance is only slightly better than the single-view network. Finally, we show the importance of our proposed \"depth cleaning\" method, applied to the training data (see Eq. 1). Compared to the same model, only trained using the raw MVS depth predictions as supervision (\"w/o d. cleaning\"), we see a drop of about 15% in performance.\nFig. 5 shows qualitative comparison between the different methods. Our models' depth predictions (Fig. 5(f-g)) strongly resemble the ground truth and show high level of details and sharp depth discontinuities. This result is a no- table improvement over competing methods, which often produce significant errors in both human regions (e.g., legs in the second row of Fig. 5), and non-human regions (e.g., table and ceiling in the last two rows).", "publication_ref": ["b38", "b2", "b16", "b46", "b6", "b2", "b39", "b30", "b30"], "figure_ref": ["fig_5", "fig_5", "fig_5", "fig_5"], "table_ref": ["tab_0"]}, {"heading": "Internet videos of dynamic scenes", "text": "We tested our method on challenging Internet videos (downloaded from YouTube and Shutterstock), involving simultaneous natural camera motion and human motion. Our SLAM/SfM pipeline was used to generate sequences ranging from 5 seconds to 15 seconds with smooth and accurate camera trajectories, after which we apply our method to obtain the required network input buffers.\nWe qualitatively compare our full model (ID pp CM K) with several recent learning based depth prediction models: DORN [7], Chen et al. [3], and DeMoN [39]. For fair comparisons, we use DORN with a model trained on NYUv2 for indoor videos and a model trained on KITTI for outdoor videos; For [3], we use the models trained on both NYUv2 and DIW. For all of our predictions, we use a single model trained from scratch on our MC dataset.\nAs illustrated in Fig. 6, our depth predictions are significantly better than the baseline methods. In particular, DORN [7] has very limited generalization to Internet videos, and Chen et al. [3], which is mainly trained on Internet photos, is not able to capture accurate depth. DeMoN often produces incorrect depth, especially in human regions, as it designed for static scenes. Our predicted depth maps depict accurate depth ordering both between people and other objects in the scene (e.g., between people and buildings, fourth row of Fig. 6), and within human regions (such as the arms and legs of people in the first three rows of Fig. 6).\nDepth-based visual effects. Our depth can be used to apply a range of depth-based visual effects. Fig. 7 shows depth-based defocus, insertion of synthetic 3D graphics, and removal of nearby humans with inpainting. See the supplemental material for additional examples, including mono-to-stereo conversion.\nThe depth estimates are sufficiently stable over time to allow inpainting from frames elsewhere in the video. To use a frame for inpainting, we construct a triangle heightfield from the depth map, texture the heightfield with the video frame, and render the heightfield from the target frame using the relative camera transformation. Fig. 7 (d, f) show the results of inpainting two street scenes. Humans near the camera are removed using the human mask M , and holes are filled with colors from up to 200 frames later in the video. Some artifacts are visible in areas the human mask misses, such as shadows on the ground.", "publication_ref": ["b6", "b2", "b39", "b2", "b6", "b2"], "figure_ref": ["fig_6", "fig_6", "fig_6", "fig_7", "fig_7"], "table_ref": []}, {"heading": "Discussion and Conclusion", "text": "We demonstrated the power of a learning-based approach for predicting dense depth of dynamic scenes where a monocular camera and people are freely moving. We make a new source of data available for training: a large corpus of Mannequin Challenge videos from YouTube, in which the camera moves around and people \"frozen\" in natural poses. We showed how to obtain reliable depth supervision from such noisy data, and demonstrated that our models significantly improve over state-of-the-art methods.\nOur approach still has limitations. We assume known camera poses, which may difficult to infer if moving objects cover most of the scene. In addition, the predicted depth may be inaccurate for non-human, moving regions such as cars and shadows (Fig. 8). Our approach also only uses two views, sometimes leading to temporally inconsistent depth estimates. However, we hope this work can guide and trigger further progress in monocular dense reconstruction of dynamic scenes.", "publication_ref": [], "figure_ref": ["fig_8"], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image", "journal": "", "year": "2016", "authors": "F Bogo; A Kanazawa; C Lassner; P V Gehler; J Romero; M J Black"}, {"ref_id": "b1", "title": "Matterport3D: Learning from RGB-D data in indoor environments. Int. Conf. on 3D Vision (3DV)", "journal": "", "year": "2017", "authors": "A Chang; A Dai; T Funkhouser; M Halber; M Niessner; M Savva; S Song; A Zeng; Y Zhang"}, {"ref_id": "b2", "title": "Single-image depth perception in the wild", "journal": "", "year": "2016", "authors": "W Chen; Z Fu; D Yang; J Deng"}, {"ref_id": "b3", "title": "ScanNet: Richly-annotated 3D reconstructions of indoor scenes", "journal": "", "year": "2017", "authors": "A Dai; A X Chang; M Savva; M Halber; T A Funkhouser; M Niessner"}, {"ref_id": "b4", "title": "Fusion4D: realtime performance capture of challenging scenes", "journal": "ACM Trans. Graphics", "year": "2016", "authors": "M Dou; S Khamis; Y Degtyarev; P L Davidson; S R Fanello; A Kowdle; S Orts; C Rhemann; D Kim; J Taylor; P Kohli; V Tankovich; S Izadi"}, {"ref_id": "b5", "title": "Depth map prediction from a single image using a multi-scale deep network", "journal": "", "year": "2014", "authors": "D Eigen; C Puhrsch; R Fergus"}, {"ref_id": "b6", "title": "Deep ordinal regression network for monocular depth estimation", "journal": "", "year": "2018", "authors": "H Fu; M Gong; C Wang; K Batmanghelich; D Tao"}, {"ref_id": "b7", "title": "Unsupervised monocular depth estimation with left-right consistency", "journal": "", "year": "2017", "authors": "C Godard; O M Aodha; G J Brostow"}, {"ref_id": "b8", "title": "DensePose: Dense Human Pose Estimation In The Wild", "journal": "", "year": "2018", "authors": "R A G\u00fcler; N Neverova; I Kokkinos"}, {"ref_id": "b9", "title": "Multiple view geometry in computer vision", "journal": "Cambridge university press", "year": "2003", "authors": "R Hartley; A Zisserman"}, {"ref_id": "b10", "title": "Basic mechanisms", "journal": "University of Toronto Press", "year": "2002", "authors": "I P Howard"}, {"ref_id": "b11", "title": "DeepMVS: Learning multi-view stereopsis", "journal": "", "year": "2018", "authors": "P.-H Huang; K Matzen; J Kopf; N Ahuja; J.-B Huang"}, {"ref_id": "b12", "title": "FlowNet 2.0: Evolution of Optical Flow Estimation With Deep Networks", "journal": "", "year": "2017", "authors": "E Ilg; N Mayer; T Saikia; M Keuper; A Dosovitskiy; T Brox"}, {"ref_id": "b13", "title": "VolumeDeform: Real-time volumetric nonrigid reconstruction", "journal": "", "year": "2016", "authors": "M Innmann; M Zollh\u00f6fer; M Niessner; C Theobalt; M Stamminger"}, {"ref_id": "b14", "title": "Parallax geometry of pairs of points for 3d scene analysis", "journal": "", "year": "1996", "authors": "M Irani; P Anandan"}, {"ref_id": "b15", "title": "Endto-end recovery of human shape and pose", "journal": "", "year": "2018", "authors": "A Kanazawa; M J Black; D W Jacobs; J Malik"}, {"ref_id": "b16", "title": "Deeper depth prediction with fully convolutional residual networks", "journal": "", "year": "2016", "authors": "I Laina; C Rupprecht; V Belagiannis; F Tombari; N Navab"}, {"ref_id": "b17", "title": "Unite the people: Closing the loop between 3D and 2D human representations", "journal": "", "year": "2017", "authors": "C Lassner; J Romero; M Kiefel; F Bogo; M J Black; P V Gehler"}, {"ref_id": "b18", "title": "MegaDepth: Learning Single-View Depth Prediction from Internet Photos", "journal": "", "year": "2018", "authors": "Z Li; N Snavely"}, {"ref_id": "b19", "title": "Learning rigidity in dynamic scenes with a moving camera for 3d motion field estimation", "journal": "", "year": "2018", "authors": "Z Lv; K Kim; A Troccoli; D Sun; J M Rehg; J Kautz"}, {"ref_id": "b20", "title": "Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints", "journal": "", "year": "2018", "authors": "R Mahjourian; M Wicke; A Angelova"}, {"ref_id": "b21", "title": "Choosing Smartly: Adaptive Multimodal Fusion for Object Detection in Changing Environments", "journal": "", "year": "2016", "authors": "O Mees; A Eitel; W Burgard"}, {"ref_id": "b22", "title": "VNect: Realtime 3D Human Pose Estimation with a Single RGB Camera", "journal": "ACM Trans. Graphics", "year": "2017", "authors": "D Mehta; S Sridhar; O Sotnychenko; H Rhodin; M Shafiei; H.-P Seidel; W Xu; D Casas; C Theobalt"}, {"ref_id": "b23", "title": "Orb-Slam2: An open-source slam system for monocular, stereo, and RGB-D cameras", "journal": "IEEE Transactions on Robotics", "year": "2017", "authors": "R Mur-Artal; J D Tard\u00f3s"}, {"ref_id": "b24", "title": "DynamicFusion: Reconstruction and tracking of non-rigid scenes in real-time", "journal": "", "year": "2015", "authors": "R A Newcombe; D Fox; S M Seitz"}, {"ref_id": "b25", "title": "RGBD-HuDaAct: A colordepth video database for human daily activity recognition", "journal": "", "year": "2011", "authors": "B Ni; G Wang; P Moulin"}, {"ref_id": "b26", "title": "3D Reconstruction of a Moving Point from a Series of 2D Projections", "journal": "", "year": "2010", "authors": "H S Park; T Shiratori; I A Matthews; Y Sheikh"}, {"ref_id": "b27", "title": "Coarse-to-fine volumetric prediction for single-image 3D human pose", "journal": "", "year": "2017", "authors": "G Pavlakos; X Zhou; K G Derpanis; K Daniilidis"}, {"ref_id": "b28", "title": "Dense monocular depth estimation in complex dynamic scenes", "journal": "", "year": "2016", "authors": "R Ranftl; V Vineet; Q Chen; V Koltun"}, {"ref_id": "b29", "title": "Soccer on your tabletop", "journal": "", "year": "2018-06", "authors": "K Rematas; I Kemelmacher-Shlizerman; B Curless; S Seitz"}, {"ref_id": "b30", "title": "Video pop-up: Monocular 3d reconstruction of dynamic scenes", "journal": "", "year": "2014", "authors": "C Russell; R Yu; L Agapito"}, {"ref_id": "b31", "title": "Structure-from-motion revisited", "journal": "", "year": "2016", "authors": "J L Schonberger; J.-M Frahm"}, {"ref_id": "b32", "title": "Pixelwise view selection for unstructured multi-view stereo", "journal": "", "year": "2016", "authors": "J L Sch\u00f6nberger; E Zheng; J.-M Frahm; M Pollefeys"}, {"ref_id": "b33", "title": "Learning from Simulated and Unsupervised Images through Adversarial Training", "journal": "", "year": "2017", "authors": "A Shrivastava; T Pfister; O Tuzel; J Susskind; W Wang; R Webb"}, {"ref_id": "b34", "title": "Indoor segmentation and support inference from rgbd images", "journal": "", "year": "2012", "authors": "N Silberman; D Hoiem; P Kohli; R Fergus"}, {"ref_id": "b35", "title": "Kronecker-Markov Prior for Dynamic 3D Reconstruction", "journal": "", "year": "", "authors": "T Simon; J Valmadre; I A Matthews; Y Sheikh"}, {"ref_id": "b36", "title": "", "journal": "Trans. Pattern Analysis and Machine Intelligence", "year": "2017", "authors": ""}, {"ref_id": "b37", "title": "Semantic scene completion from a single depth image", "journal": "", "year": "2017", "authors": "S Song; F Yu; A Zeng; A X Chang; M Savva; T Funkhouser"}, {"ref_id": "b38", "title": "A benchmark for the evaluation of RGB-D SLAM systems", "journal": "", "year": "2012", "authors": "J Sturm; N Engelhard; F Endres; W Burgard; D Cremers"}, {"ref_id": "b39", "title": "DeMoN: Depth and motion network for learning monocular stereo", "journal": "", "year": "2017", "authors": "B Ummenhofer; H Zhou; J Uhrig; N Mayer; E Ilg; A Dosovitskiy; T Brox"}, {"ref_id": "b40", "title": "Spatiotemporal Bundle Adjustment for Dynamic 3D Reconstruction. Proc. Computer Vision and Pattern Recognition (CVPR)", "journal": "", "year": "2016", "authors": "M Vo; S G Narasimhan; Y Sheikh"}, {"ref_id": "b41", "title": "Learning depth from monocular videos using direct methods", "journal": "", "year": "2018", "authors": "C Wang; J Buenaposada; R Zhu; S Lucey"}, {"ref_id": "b42", "title": "", "journal": "Wikipedia. Mannequin Challenge", "year": "", "authors": ""}, {"ref_id": "b43", "title": "Optical flow in mostly rigid scenes", "journal": "", "year": "", "authors": "J Wulff; L Sevilla-Lara; M J Black"}, {"ref_id": "b44", "title": "Monocular relative depth perception with web stereo data supervision", "journal": "", "year": "2018", "authors": "K Xian; C Shen; Z Cao; H Lu; Y Xiao; R Li; Z Luo"}, {"ref_id": "b45", "title": "Sun3D: A database of big spaces reconstructed using sfm and object labels", "journal": "", "year": "2013", "authors": "J Xiao; A Owens; A Torralba"}, {"ref_id": "b46", "title": "Monocular depth estimation using multi-scale continuous crfs as sequential deep networks", "journal": "Trans. Pattern Analysis and Machine Intelligence", "year": "2018", "authors": "D Xu; E Ricci; W Ouyang; X Wang; N Sebe"}, {"ref_id": "b47", "title": "MVSNet: Depth Inference for Unstructured Multi-view Stereo", "journal": "", "year": "2018", "authors": "Y Yao; Z Luo; S Li; T Fang; L Quan"}, {"ref_id": "b48", "title": "Real-time simultaneous pose and shape estimation for articulated objects using a single depth camera", "journal": "", "year": "2014", "authors": "M Ye; R Yang"}, {"ref_id": "b49", "title": "GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose", "journal": "", "year": "2018", "authors": "Z Yin; J Shi"}, {"ref_id": "b50", "title": "Sparse Dynamic 3D Reconstruction from Unsynchronized Videos", "journal": "", "year": "2015", "authors": "E Zheng; D Ji; E Dunn; J.-M Frahm"}, {"ref_id": "b51", "title": "DeepTAM: Deep Tracking and Mapping", "journal": "", "year": "2018", "authors": "H Zhou; B Ummenhofer; T Brox"}, {"ref_id": "b52", "title": "Unsupervised learning of depth and ego-motion from video", "journal": "", "year": "2017", "authors": "T Zhou; M Brown; N Snavely; D G Lowe"}, {"ref_id": "b53", "title": "Stereo Magnification: Learning view synthesis using multiplane images", "journal": "ACM Trans. Graphics (SIGGRAPH)", "year": "2018", "authors": "T Zhou; R Tucker; J Flynn; G Fyffe; N Snavely"}, {"ref_id": "b54", "title": "Evaluating spatiotemporal interest point features for depth-based action recognition", "journal": "Image and Vision Computing", "year": "2014", "authors": "Y Zhu; W Chen; G Guo"}, {"ref_id": "b55", "title": "Real-time non-rigid reconstruction using an RGB-D camera", "journal": "ACM Trans. Graphics", "year": "2014", "authors": "M Zollh\u00f6fer; M Niessner; S Izadi; C Rehmann; C Zach; M Fisher; C Wu; A Fitzgibbon; C Loop; C Theobalt"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 .2Figure 2. Sample images from Mannequin Challenge videos. Each image is a frame from a video sequence in which the camera is moving but humans are all static. The videos span a variety of natural scenes, poses, and configuration of people.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "(a) Reference image I r (b) Human mask M (c) Input depth Dpp (d) Input confidence C (e) MVS depth DMVS Figure 3. System inputs and training data. The input to our network consists of: (a) RGB image, (b) human mask, (c) masked depth computed from motion parallax w.r.t. a selected source image, and (d) masked confidence map. Low confidence regions (dark circles) in the first two rows indicate the vicinity of the camera epipole, where depth from parallax is unreliable and is removed. The network is trained to regress to MVS depth (e).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Fig. 3 (3d) shows examples of computed confidence maps. Note that human regions as well as regions for which the confidence C(p) < 0.25 are masked out.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 .4Figure 4. Qualitative results on the MC test set. From top to bottom: reference images and their corresponding MVS depth (pseudo ground truth); our depth predictions using: our single view model (third row) and our two-frame model (forth row). The additional network inputs give improved performance in both human and non-human regions.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Table 1 .1Net inputssi-full si-env si-hum si-intra si-inter Quantitative comparisons on the MC test set. Different input configurations of our model: (I.) single image; (II.) optical flow masked in the human region (F ), confidence and human mask;", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 .5Qualitative comparisons on the TUM RGBD dataset. (a) Reference images, (b) source images (used to compute our initial depth input), (c) ground truth sensor depth, (d) single view depth prediction method DORN", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 .6(a) I r (b) I s (c) DORN [7] (d) Chen et al. [3] (e) DeMoN [39] (f) Ours (full) Comparisons on Internet video clips with moving cameras and people. From left to right: (a) reference image, (b) source image, (c) DORN [7], (d) Chen et al. [3], (e) DeMoN [39], (f) our full method.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 7 .7Depth-based visual effects. We use our predicted depth maps to apply depth-aware visual effects on (a, e) input images; we show (b) defocus, (c) object insertion, and (d, f) people removal with inpainting results.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 8 .8Figure 8. Failure cases. Moving, non-human objects such as cars and shadows can cause bad estimates (left and middle, boxed); fine structures such as limbs may be blurred for distant people in challenging poses (right, boxed).", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Results on TUM RGBD datasets. Different si-RMSE metrics as well as standard RMSE and relative error (Rel) are reported. We evaluate our models (light gray background) under different input configurations, as described in Table1. w/o d. cleaning indicates the model is trained using raw MVS depth predictions as supervision, without our depth cleaning method. Dataset '-' indicates the method is not learning based. Lower is better for all error metrics.", "figure_data": "and Depth"}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u2206(p) = |D MVS (p) \u2212 D pp (p)| D MVS (p) + D pp (p)(1)", "formula_coordinates": [3.0, 106.43, 367.58, 179.93, 23.53]}, {"formula_id": "formula_1", "formula_text": "s = arg max j d rj o rj (2)", "formula_coordinates": [4.0, 128.21, 541.16, 158.15, 18.44]}, {"formula_id": "formula_2", "formula_text": "o rj = 2|V r V j | |V r | + |V j | ,(3)", "formula_coordinates": [4.0, 128.59, 611.38, 157.77, 23.89]}, {"formula_id": "formula_3", "formula_text": "C(p) = C lr (p)C ep (p)C pa (p). (4", "formula_coordinates": [4.0, 364.0, 442.97, 177.25, 9.65]}, {"formula_id": "formula_4", "formula_text": ")", "formula_coordinates": [4.0, 541.24, 443.29, 3.87, 8.64]}, {"formula_id": "formula_5", "formula_text": "the pixel p. That is, C pa (p) = 1\u2212 min(\u03b2,\u03b2(p))\u2212\u03b2 \u03b2 2", "formula_coordinates": [4.0, 308.86, 646.91, 207.34, 19.06]}, {"formula_id": "formula_6", "formula_text": "L si = L MSE + \u03b1 1 L grad + \u03b1 2 L sm .(5)", "formula_coordinates": [5.0, 101.46, 426.15, 184.9, 9.65]}], "doi": ""}