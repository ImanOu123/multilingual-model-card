{"title": "HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud", "authors": "Wencan Cheng; Hao Tang; Luc Van Gool; Jong Hwan Ko", "pub_date": "2024-04-04", "abstract": "Extracting keypoint locations from input hand frames, known as 3D hand pose estimation, is a critical task in various human-computer interaction applications. Essentially, the 3D hand pose estimation can be regarded as a 3D point subset generative problem conditioned on input frames. Thanks to the recent significant progress on diffusion-based generative models, hand pose estimation can also benefit from the diffusion model to estimate keypoint locations with high quality. However, directly deploying the existing diffusion models to solve hand pose estimation is non-trivial, since they cannot achieve the complex permutation mapping and precise localization. Based on this motivation, this paper proposes HandDiff, a diffusion-based hand pose estimation model that iteratively denoises accurate hand pose conditioned on hand-shaped image-point clouds. In order to recover keypoint permutation and accurate location, we further introduce joint-wise condition and local detail condition. Experimental results demonstrate that the proposed HandDiff significantly outperforms the existing approaches on four challenging hand pose benchmark datasets. Codes and pre-trained models are publicly available at https: //github.com/cwc1260/HandDiff.", "sections": [{"heading": "Introduction", "text": "3D hand pose estimation (HPE), which involves estimating the 3D positions of hand keypoints, provides a fundamental conprehension of human hand motion. Therefore, it is essential to facilitate more natural and intuitive interactions between humans and machines and is applicable to various human-computer interaction applications including robotics, gaming, and augmented/virtual reality. In recent years, significant progress has been made in the field of 3D hand pose estimation by applying deep learning techniques and using low-cost depth cameras.\nRecent developments in 3D Hand Pose Estimation (HPE) based on deep learning [5,6,9,11,12,15,16, 18, 23,35,36,38] are primarily divided into two core approaches: regression and detection. While these straightforward solutions have shown notable effectiveness and computational efficiency, these deterministic methods impose limitations on handling ill-posed uncertain cases such as self-occlusions and hand-object occlusions, which are prevalent in real-world hand recognition scenarios. Therefore, in order to ensure the reliability of the estimation, it is imperative to accurately model the uncertainty. Providentially, a revolutionary approach known as the Diffusion Model (DM) [19,27,43] has demonstrated remarkable performance in processing uncertainty through modeling of probabilistic distributions. The DM has also exhibited superiority in 3D generative applications including unseen 3D point cloud generation [27,50] and uncertain parts completion [28,54]. Therefore, 3D DM can be deployed in 3D HPE as in these 3D conditional generation problems, as both 3D HPE and 3D conditional generation aim to generate a set of keypoints based on a specific condition. More importantly, 3D DM can resolve the ill-posed uncertainty of occlusions by learning the probabilistic distribution of the keypoints. Based on this inspiration, we apply the diffusion model in generating hand keypoint locations conditioned on the hand depth image/point cloud input, as illustrated in Figure 1. To the best of our knowledge, our work is the first attempt to deploy diffusion models in the hand pose estimation task.\nWhile it is theoretically possible to tackle the HPE task using DM models, the direct application of existing 3D DMs [27,50] to hand pose estimation tasks is still limited. One of the significant limitations of current 3D DMs is their reliance on a global latent condition, which overlooks crucial local detail information needed for accurate estimation of joint locations. Furthermore, the permutation-equivariant nature of 3D DMs limits their ability to distinguish between joints under simple global conditions, affecting their precision in aligning noisy points with specific target joints.\nTo fully exploit the potential of the diffusion model in hand pose estimation, we propose HandDiff, a novel approach that incrementally refines the noise distribution to accurately derive a 3D hand pose from multi-modal inputs, including depth images and point clouds. To address inherent limitations in 3D DMs, our model incorporates a joint-wise denoising mechanism that individually denoises various joints during estimation. Concretely, the proposed model first introduces a joint-wise condition generation module that samples features for each individual joints from both depth image and point cloud. Furthermore, it features a novel local feature-conditioned denoising module, which is the key component to perform the reverse diffusion process. It operates under joint-specific conditions and utilizes local features gathered around the noisy input joint locations. In addition, we propose a novel kinematic correspondenceaware layer that integrates with a graph convolutional operation in order to capture the kinematic relationship between hand joints.\nWe evaluate HandDiff on four challenging benchmarks, including single-hand ICVL [47], MSRA [46] and NYU [48] dataset, and hand-object DexYCB [3] datasets. The results show that HandDiff achieves a comparable performance with mean distance errors of 5.76 mm, 6.53 mm, and 7.38 mm on the ICVL, MSRA, and NYU datasets, respectively. The model also significantly outperforms existing state-of-the-art approaches on the DexYCB dataset with the lowest error, achieving the lowest error at 8.06 mm.\nThe following is a summary of our primary contributions:\n\u2022 We propose a novel diffusion-based model for hand pose estimation that utilizes the depth image and point cloud input as a multi-modal condition. This model progressively denoises a noise distribution, accurately determining the 3D coordinates of hand joints. \u2022 We propose a novel joint-wise local feature-aware denoising module designed to capture local details surrounding noisy input as a condition for more accurate joint coordinate denoising. Furthermore, this module incorporates a novel kinematic correspondence-aware layer to model the dependencies between joints, thereby enhancing performance.\n\u2022 We perform comprehensive experiments on big and challenging benchmarks that present the new state-of-the-art performance of our proposed method.", "publication_ref": ["b4", "b5", "b8", "b10", "b11", "b14", "b15", "b22", "b34", "b35", "b37", "b18", "b26", "b42", "b26", "b49", "b27", "b53", "b26", "b49", "b46", "b45", "b47", "b2"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Related Work", "text": "3D hand pose estimation based on depth image. Among the various 3D hand pose estimation approaches that use depth images, conventional 2D CNN-based methods [6, 11-13, 18, 35, 36, 48] have been widely used due to their simplicity. However, they suffer from limitations such as difficulty in capturing the 3D structure and dependence on the camera's viewpoint.\nTo overcome these limitations, 3D CNN-based methods [14,29] were introduced, which use 3D voxelized representations of depth images to capture volumetric information. Although these methods improved the performance of 3D hand pose estimation, they require large amounts of memory and computation, which limits their practical applications.\nIn contrast, PointNet-based methods [5,9,15,16,23] process the point cloud, which is an accurate representation of the 3D structure. PointNet [33] is a deep learning framework that can handle irregular and unstructured point clouds. The use of PointNet for hand pose estimation was first introduced in HandPointNet [15]. Point-to-Point model [16] and SHPR-Net [5] further improved performance by generating the point-wise probability distribution. Subsequently, SHPR-Net [5] combined HandPoint-Net with an auxiliary semantic segmentation subnetwork to enhance performance. Recently, HandFoldingNet [9] introduced a folding concept that reshapes a predefined 2D hand skeleton into hand poses, further improving the estimation accuracy. However, a significant drawback of the point cloud is that querying neighbors from a dense point set for convolution requires heavy computations. Therefore, existing methods commonly use a sparse point cloud, which restricts the performance.\nHence, in this work, we utilize multi-modal representations that combine 2D depth images and 3D point clouds. Thus, the model is able to efficiently extract dense detail information, as well as effectively capture 3D spatial features for accurate 3D hand pose estimation. Moreover, for the first time, we apply the diffusion model with a PointNetbased denoising process to improve pose estimation performance. Diffusion models for pose estimation. Diffusion models [19,43], also known as denoising diffusion probabilistic models (DDPMs), are a family of deep generative models. DM recovers the originally observed data distribution from the perturbed data distribution with gradually injected noise by recurrently denoising the noise of each perturbation step. In recent years, they have seen remarkable success in a variety of computer vision tasks, such as object detection [4], image synthesis [20,[39][40][41], graph generation [22,30,51], semantic segmentation [1,2], and pose estimation [10,17,21,42,42].\nExisting diffusion-based pose estimation approaches [10,17,21,42] have been used mainly for 3D human pose estimation, which regresses the locations of 3D keypoints of humans from 2D RGB images of the human body. This is because the uncertain 2D-to-3D lifting can be modeled as a probability distribution. Specifically, D3DP [42] proposed a multi-hypothesis aggregation with joint-wise reprojection to determine the best hypothesis from the diffusion model using the 2D prior. DiffPoses [17,21] both introduced a heatmap representation of 2D joints to condition the reverse diffusion process. DiffuPose [10] adopted the graph convolutional network as a denoising function to explicitly learn the connectivity between human joints. In common, these methods all follow the same two-stage scheme, which first requires a trained 2D regression model to obtain 2D keypoints as a prior, and then applies a diffusion model conditioned on the 2D prior to solve the 2D-to-3D lifting problem. Intuitively, the performance of the 2D regression model constrains the denoising quality.\nIn contrast, our method applies a single-stage denoising process using the conditions from a 3D space. Our method directly accepts raw depth and point cloud frames as conditions in order to take full use of both dense 2D visual features and 3D structural information without requiring any compressed 2D/3D prior information from pre-trained models. Furthermore, our method leverages the DDIM [44] to accelerate inference, since it can complete the thousands of reverse denoising processes in single-digit downsampled timesteps.", "publication_ref": ["b13", "b28", "b4", "b8", "b14", "b15", "b22", "b32", "b14", "b15", "b4", "b4", "b8", "b18", "b42", "b3", "b19", "b38", "b39", "b40", "b21", "b29", "b50", "b0", "b1", "b9", "b16", "b20", "b41", "b41", "b9", "b16", "b20", "b41", "b41", "b16", "b20", "b9", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "The Proposed Hand Pose Diffusion Model", "text": "HandDiff is a diffusion model that takes a 3D normal distribution and a hand depth image as input and produces the coordinates of the hand joints as output, as shown in Figure 2. Intuitively, HandDiff iteratively removes noise to refine the joint locations by exploring the local region around each joint conditioned on joint-wise features. The input to HandDiff is a hand depth image D in \u2208 R H\u00d7W with a set of sampled 3D point coordinates P in \u2208 R N \u00d73 , and the outputs are 3D joint coordinates J 0 \u2208 R J\u00d73 denoised from a randomly initialized normal distribution. The depth image and the N points are first supplied into a local condition encoder that extracts local and global features. We construct a ConvNeXt-based autoenoder to generate a 2D local visual feature map F 2d \u2208 R H/2\u00d7W/2\u00d7d 2d and a 2D global vector. Due to the irregularity and disorder of the input point set, we exploit the hierarchical point cloud encoder [25,34] proposed by PointNet++ [34] to extract 3D local geometric features F 3d \u2208 R N/2\u00d7d 3d and a global 3D vector. Then, the local features are fed into the joint-wise condition extractor to extract joint-wise condition vectors. Afterward, a novel joint-wise local feature-conditioned denoiser is executed T \u2032 steps to iteratively denoise joint coordinates by resampling the useful local features around noisy joints and being conditioned by joint-wise conditions.", "publication_ref": ["b24", "b33", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "Joint-wise Condition Extraction", "text": "The joint-wise conditions C \u2208 R J\u00d7dc essentially are the embeddings that represent the joints in the d c -dimensional latent space. Thus, we take the concept of joint-wise embeddings proposed by HandFoldingNet [9] to obtain the joint-wise conditions. The generation of the joint-wise conditions (joint-wise embeddings) is sequentially performed by a three-layer bias-induced layer (BIL) [8]. The concatenation of the 2D and 3D global vectors is replicated J times and fed into the BILs to generate conditions for the J individual joints. The BIL provides joint-wise independent biases that can be regarded as learnable positional embeddings that enable individual mapping for different joints from the same global feature.", "publication_ref": ["b8", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Joint-wise Local Feature-conditioned Denoiser", "text": "The denoiser reconstructs an accurate joint coordinate distribution from a noisy distribution under the joint-wise conditions extracted from the hand point cloud. At each time step t, the denoiser D accepts the noisy joint coordinate distribution J (t) , joint-wise condition C, local features F 2d , F 3d and time step t as input, and outputs the reconstructed 3D joint coordinates J (0|t) without noise:\nJ (0|t) = D(J (t) , C, F 2d , F 3d , t).\n(\n)1\nThe denoiser consists of the following elements: 1) a local feature sampler, 2) a joint indicator & timestep embedding, 3) a kinematic correspondence-aware aggregation block, and 4) a residual refiner. The sampler samples local features around noisy coordinates J (t) to form local conditions that contain detailed observations. In order to differentiate between different joints and levels of noise, we introduce a joint indicator and a time-step embedding, respectively. Afterward, the aggregation block fuses the local conditions and joint-wise conditions together and subsequently produces the denoised joint coordinates. In addition, the aggregation also cooperates with graph reasoning to capture kinematic correspondences. In the end, a residual mechanism is applied to refine the noisy joint coordinate distribution to the denoised one. Local feature sampler. The sampler collects K-nearestneighbor local 3D points and their corresponding local 3D spatial features around each noisy joint distribution. The sampler also samples K-nearest-neighbor 2D pixels and their corresponding local 2D visual features. Note that the 2D pixels are projected onto the same 3D joint space for neighbor querying. The neighbor points and pixels are then translated into a relative coordinate system with the joint location as the origin to eliminate translation variations. Joint indicator vector & timestep embedding. As the denoiser needs to handle different joints with different levels of noise, it has to be explicitly informed of the joint index j and timestep t. Following DDPMs, we apply the sinusoidal function on j and t to form a joint indicator vector and timestep embedding. Subsequently, the joint indicator vectors and time-step embeddings are concatenated with each local feature for the subsequent process. Kinematic correspondence-aware aggregation block. As visualized in Figure 2, the proposed aggregation block accepts the local features, joint-wise conditions, joint indicator vectors, and timestep embeddings as input, and outputs kinematic correspondence-evolved local features and jointwise embeddings. Since many recent approaches [12,36] suggested that a Graph Convolutional Network (GCN) can effectively model relative kinematic relationships among joints, the joint-wise conditions (embeddings) are first augmented through a GCN, forming the evolved joint-wise conditions:\nC \u2032 = ReLU (ACW),(2)\nwhere W \u2208 R dc\u00d7dc is the trainable weights and A \u2208 R J\u00d7J\u00d7dc is the channel-wise kinematic correspondence matrix among joints. Afterward, the GCN-evolved jointwise conditions are concatenated with the joint indicator vectors, timestep embeddings, as well as sampled local features. Subsequently, the concatenated features are sent to a one-layer MLP to generate evolved local features encoding with kinematic correspondence and global prior. Formally, the evolved local feature for the k-th local neighbor point of the j-th joint is defined as\nF \u2032 k,j = M LP ([P k,j \u2212 J j , F k,j , C \u2032 j , P E(t), P E(j)]),(3)\nwhere P k,j and F k,j are the coordinate and local feature of the k-th local neighbor point of the j-th joint, PE is the sinusoidal positional embedding function and '[\u2022, \u2022]' is the concatenation operation.\nHowever, the single proposed block is not sufficient for complex denoising. Thus, the proposed block is replicated four times with independent learnable parameters in practice. Furthermore, we introduce a max-pooling layer between every two blocks for providing updated joint-wise embeddings with local information to the latter block:\nC = M axP ool(F \u2032 ).(4)\nResidual refiner. Similar to the previous 3D generative diffusion model [27], the refiner accepts the last joint-wise em-beddings\u0108 as input to refine the noisy input distribution J (t) . The refiner is a linear transformation with a residual connection. Therefore, the approximated joint coordinates of the current t-th timestep are represented as:\nJ (0|t) =\u0108W + J (t) ,(5)\nwhere W \u2208 R d3\u00d73 is the trainable transformation matrix.", "publication_ref": ["b11", "b35", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Training", "text": "HandDiff first corrupts the ground truth joint distribution q(J (0) ) to a noisy distribution q(J (t) |J (0) ) by gradually adding noise \u03f5 \u223c N (0, I) through a forward diffusion Markovian chain, where t is uniformly sampled from the predefined total time steps T . Following DDPMs [19], the forward noise process is formally defined as:\nq(J (t) |J (0) ) = \u221a\u1fb1 t J (0) + \u03f5 \u221a 1 \u2212\u1fb1 t , where\u1fb1 t = t s=0 \u03b1 t = t s=0 (1 \u2212 \u03b2 s ).(6)\nNote that 0 < \u03b2 t < 1 is the variance of the noise, which is controlled by a linear variance schedule at each time step, as in DDPM [19]. Subsequently, the noisy joint distribution is supplied to the proposed denoiser to recover the clean joint distribution J (0|t) , under the joint-wise conditions as well as the local detail conditions. To train the denoiser, J (0|t) is under the supervision of the ground truth distribution J * .\nBesides, the joint-wise conditions have to be initialized through training. Therefore, the 3D coordinates J c linearly transformed from the joint-wise conditions are also under the same supervision.\nFollowing previous regression works [9,35], we adopt a smooth L1 loss to supervise training because of its less sensitivity to outliers. The smooth L1 loss is defined as:\nL1 smooth (x) = 50x 2 , |x| < 0.01 |x| \u2212 0.005, otherwise .(7)\nBy using the smooth L1 loss, we supervise the approximated joint distribution by the following joint loss function:\nL = J j=0 L1 smooth ( J (0|t) j \u2212 J * j ).(8)", "publication_ref": ["b18", "b18", "b8", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "Inference", "text": "During inference, a reverse diffusion process is pursued by iteratively applying the denoiser, to recover the uncontaminated joint coordinate distribution. According to recent 2Dto-3D human pose diffusion models [10,17,21,42], multiple diverse hypotheses for the reverse process can help probabilistic diffusion models to achieve improved accuracy. Our model also samples H initial 3D poses J (T ) 0:H from a unit Gaussian distribution.\nAfterward, H pose hypotheses are individually passed to the proposed denoiser to approximate the H uncontaminated joint coordinate distribution J (0|t) 0:H . To obtain the noisy input for the subsequent denoising step t \u2212 1, we exploit a noiser that adds noise to the denoised distribution following the DDIM [44]:\np \u03b8 (J (t\u22121) 0:H | J (0|t) 0:H ) = \u221a\u1fb1 t\u22121 J (0|t) 0:H + 1 \u2212\u1fb1 t\u22121 \u2212 \u03c3 2 t \u03f5 t +\u03c3 t \u03f5,(9)\nwhere t is started from T , \u03f5 t = (J\n(t) 0:H \u2212 \u221a\u1fb1 t J (0) 0:H )/ \u221a 1 \u2212\u1fb1 t is the predicted noise of timestep t and \u03c3 t = (1 \u2212\u1fb1 t\u22121 )(1 \u2212\u1fb1 t /\u1fb1 t\u22121 )/(1 \u2212\u1fb1 t ).\nThis procedure will be iterated T \u2032 times (T \u2032 < T ) to estimate the final denoised distribution J (0|t) 0:H . At the last timestep 0, we average over all hypotheses to aggregate the ultimate uncontaminated joint coordinates:\nJ (0) = 1 H H h=0 J (0) h .(10)\n4. Experiments", "publication_ref": ["b9", "b16", "b20", "b41", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "Experiment Settings", "text": "We conducted experiments on an NVIDIA TITAN RTX GPU with PyTorch. For training, we used the AdamW optimizer [26] with beta 1 = 0.5, beta 2 = 0.999, and learning rate \u03b1 = 0.001. The input image was resized to 128, the number of input points to the network was randomly sampled to 1,024, the 2D/3D feature depths ", "publication_ref": ["b25"], "figure_ref": [], "table_ref": []}, {"heading": "Datasets and Evaluation Metrics", "text": "MSRA Dataset. The MSRA dataset [46] provides more than 76K depth image frames, each of which provides J = 21 annotated joints, including one joint for the wrist and four joints for each finger. The frames are split into 9 subjects, each of which contains 17 hand gestures. ICVL Dataset. The ICVL dataset [47] provides 22K training and 1.6K testing depth frames, each of which provides J = 16 annotated joints, including one joint for the palm and three joints for each finger. NYU Dataset. The NYU dataset [48] provides depth images captured from three different views by the Prime-Sense 3D sensor. Each view contains 72K frames and 8K frames for training and testing, respectively. Following recent works [9,15,16], we use one view for training and testing and selected 14 joints out of a total of 36 annotated joints for evaluation. DexYCB. The DexYCB dataset [3] is a recently released hand-object dataset that consists of 582,000 image frames with 21 annotated joints, 10 different subjects, and 20 YCB objects from 8 camera views. This dataset defines four official dataset split protocols: S0 -seen subjects, camera views, grasped objects; S1 -unseen subjects; S2 -unseen camera views; S3 -unseen grasped objects. Evaluation metrics. We employ two commonly used metrics, the mean joint error, and the success rate, to evaluate the performance of hand pose estimation. The mean joint  [31], Pose-Ren [6], dense regression network (DenseReg) [52], CrossInfoNet [11], JGR-P2O [12], spatial-aware stacked regression network (SSRN) [37], pose-guided hierarchical graph network (PHG) [36] and virtual view selection (VVS) [7], and methods with 3D point cloud or voxels as input: 3DCNN [14], SHPR-Net [5], HandPointNet [15], Point-to-Point [16], V2V [29], Hand-FoldingNet [9] and IPNet [38]. Table 1 summarizes the results in terms of the mean joint error on the three datasets. The results show that HandDiff achieves the new state-of-the-art record with mean distance errors of 5.72 and 6.53 mm on two challenging datasets, ICVL and MSRA, respectively. The proposed model also achieves the third-lowest error on the NYU dataset. The results also demonstrate that the proposed HandDiff significantly outperforms other 2D image-based methods by large margins since HandDiff directly performs the processing on the 3D space, avoiding the highly non-linear mapping problem of estimating from the 2D image. Figure 4 illustrates that our method significantly outperforms other methods in terms of success rate when the error threshold is lower than  12, 15, and 52 mm on the ICVL, MSRA, and NYU datasets, respectively. Hand-object. We compare HandDiff on the hand-object dataset DexYCB with other state-of-the-art method on the official dataset split protocals, including A2J [53], Spurr et al. [45], METRO [24], Tse et al. [49], HandOccNet [32] and IPNet [38]. As shown in Table 2, HandDiff outperforms previous SOTA methods in all four protocols. The qualitative results visualized in Figure 3 also reveal that HandDiff can estimate accurate poses from hand-object interaction scenarios with various occlusions.", "publication_ref": ["b45", "b46", "b47", "b8", "b14", "b15", "b2", "b30", "b5", "b51", "b10", "b11", "b36", "b35", "b6", "b13", "b4", "b14", "b15", "b28", "b8", "b37", "b52", "b44", "b23", "b48", "b31", "b37"], "figure_ref": ["fig_3", "fig_2"], "table_ref": []}, {"heading": "Ablation Study", "text": "We conducted extensive ablation experiments to evaluate the contribution of each component proposed in our model.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Analysis of different proposed components.", "text": "To verify the effectiveness and necessity of the components proposed in this work, we incrementally introduce these components on the existing 3D diffusion probabilistic model (3DDPM) [27], which is able to generate complex point cloud conditionally. Briefly, 3DDPM is a share-weight point-wise denoiser conditioned on a global shape latent. We set the number of its output points as the number of hand joints to adapt it to the hand pose estimation task. Afterward, we follow DDIM for the denoising acceleration. Based on this baseline, we incrementally adopt the proposed components and conduct ablations as follows: 1) using local conditions (LC); 2) using joint indicator (JI); 3) using joint-wise condition (JC) and LC; 4) using LC with JI; 5) using JC and LC with JI; 6) using JC and LC with JI and kinematic cor-  [33] on noisy joints. Table 3 reports the experimental results of the ablations. The results demonstrate that the proposed local condition is permutation-equivariant and thus cannot work solely. The joints must be generated in a specific permutation in order to match the permutation defined by the dataset. Therefore, the proposed joint indicator and joint-wise condition that introduce permutation information are mandatory to improve performance. With the help of the joint indicator and jointwise condition, the proposed local condition mechanism can significantly reduce the mean joint error by more than 0.9 mm and 0.1 mm, respectively. Furthermore, the proposed kinematic correspondence improves performance by learning the inter-joint relations. Finally, the multiple hypotheses further boost the accuracy. Modality of conditions. As the quality of conditions determines the quality of pose denoising, we feed the diffusion model with different models of input. Table 4 shows that the model combining both 2D and 3D conditions presents the optimal estimation performance. The results also reveal that the model with only 2D conditions slightly degrades because of the 3D information loss. On the other hand, the model with only 3D conditions cannot capture dense features from only 1024 points, thus the estimation error significantly increases. Number of denoising timesteps. As suggested by DDIM [44], the inference process follows a non-Markovian chain. Thus, the number of denoising timesteps can vary for accelerated inference. Figure 6 (top) visualizes the mean joint errors with the increasing number of timesteps during inference (w/o multiple hypotheses). The results show that the model can approach to an acceptable mean joint error with two-step diffusion. The reason is intuitive that the quantity of hand keypoints to be denoised is relatively small compared to other heavy image/point cloud denoising tasks, which normally require hundreds of timesteps. The results also show that 10 timesteps appear as the optimal 7.44 mm. Larger timesteps exhibit a negligible impact on performance. In addition, the computation time and memory of the model are 98 ms and 2.2GB per frame, respectively, for 10 timesteps (1 hypothese). Number of hypotheses. Figure 6 (bottom) shows the mean joint errors with the different number of hypotheses for denoising. As expected, the error decreases as the hypothese amount increases. However, the improvement becomes marginal when the number is larger than 10.", "publication_ref": ["b26", "b32", "b43"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Conclusion", "text": "This paper presented HandDiff, a novel diffusion-based architecture that is capable of reconstructing accurate 3D hand pose iteratively, conditioned on both depth image and point cloud. Experimental results showcased that our network significantly outperforms previous state-of-the-art methods on four challenging datasets. Extensive experiments also reveals the effectiveness of the components proposed in this paper. However, a limitation of HandDiff is its inability to handle scenarios with interacting hands. Future research avenues could explore extensions to bipartite graph learning and skeleton-based analysis to address these limitations and further enhance the model's capabilities.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Labelefficient semantic segmentation with diffusion models", "journal": "", "year": "2021", "authors": "Dmitry Baranchuk; Ivan Rubachev; Andrey Voynov; Valentin Khrulkov; Artem Babenko"}, {"ref_id": "b1", "title": "Matthias Minderer, and Mohammad Norouzi. Denoising pretraining for semantic segmentation", "journal": "", "year": "", "authors": "Emmanuel Asiedu Brempong; Simon Kornblith; Ting Chen; Niki Parmar"}, {"ref_id": "b2", "title": "Dexycb: A benchmark for capturing hand grasping of objects", "journal": "", "year": "2021", "authors": "Yu-Wei Chao; Wei Yang; Yu Xiang; Pavlo Molchanov; Ankur Handa; Jonathan Tremblay; S Yashraj; Karl Narang; Umar Van Wyk; Stan Iqbal;  Birchfield"}, {"ref_id": "b3", "title": "Diffusiondet: Diffusion model for object detection", "journal": "", "year": "", "authors": "Shoufa Chen; Peize Sun; Yibing Song; Ping Luo"}, {"ref_id": "b4", "title": "Shpr-net: Deep semantic hand pose regression from point clouds", "journal": "IEEE Access", "year": "2006", "authors": "Xinghao Chen; Guijin Wang; Cairong Zhang; Tae-Kyun Kim; Xiangyang Ji"}, {"ref_id": "b5", "title": "Pose guided structured region ensemble network for cascaded hand pose estimation", "journal": "Neurocomputing", "year": "2006", "authors": "Xinghao Chen; Guijin Wang; Hengkai Guo; Cairong Zhang"}, {"ref_id": "b6", "title": "Efficient virtual view selection for 3d hand pose estimation", "journal": "", "year": "2022", "authors": "Jian Cheng; Yanguang Wan; Dexin Zuo; Cuixia Ma; Jian Gu; Ping Tan; Hongan Wang; Xiaoming Deng; Yinda Zhang"}, {"ref_id": "b7", "title": "Point auto-encoder and its application to 2d-3d transformation", "journal": "Springer", "year": "2019", "authors": "Wencan Cheng; Sukhan Lee"}, {"ref_id": "b8", "title": "Handfoldingnet: A 3d hand pose estimation network using multiscale-feature guided folding of a 2d hand skeleton", "journal": "", "year": "2007", "authors": "Wencan Cheng; Jae Hyun Park; Jong Hwan Ko"}, {"ref_id": "b9", "title": "Diffupose: Monocular 3d human pose estimation via denoising diffusion probabilistic model", "journal": "", "year": "2022", "authors": "Jeongjun Choi; Dongseok Shim; H Jin Kim"}, {"ref_id": "b10", "title": "Crossinfonet: Multi-task information sharing based hand pose estimation", "journal": "", "year": "2006", "authors": "Kuo Du; Xiangbo Lin; Yi Sun; Xiaohong Ma"}, {"ref_id": "b11", "title": "Jgr-p2o: Joint graph reasoning based pixel-to-offset prediction network for 3d hand pose estimation from a single depth image", "journal": "Springer", "year": "2006", "authors": "Linpu Fang; Xingyan Liu; Li Liu; Hang Xu; Wenxiong Kang"}, {"ref_id": "b12", "title": "Robust 3d hand pose estimation in single depth images: from single-view cnn to multiview cnns", "journal": "", "year": "2016", "authors": "Liuhao Ge; Hui Liang; Junsong Yuan; Daniel Thalmann"}, {"ref_id": "b13", "title": "3d convolutional neural networks for efficient and robust hand pose estimation from single depth images", "journal": "", "year": "1991", "authors": "Liuhao Ge; Hui Liang; Junsong Yuan; Daniel Thalmann"}, {"ref_id": "b14", "title": "Hand pointnet: 3d hand pose estimation using point sets", "journal": "", "year": "2006", "authors": "Liuhao Ge; Yujun Cai; Junwu Weng; Junsong Yuan"}, {"ref_id": "b15", "title": "Point-topoint regression pointnet for 3d hand pose estimation", "journal": "", "year": "2006", "authors": "Liuhao Ge; Junsong Zhou Ren;  Yuan"}, {"ref_id": "b16", "title": "Toward more reliable 3d pose estimation", "journal": "", "year": "2022", "authors": "Jia Gong; Zhipeng Lin Geng Foo; Qiuhong Fan; Hossein Ke; Jun Rahmani;  Liu;  Diffpose"}, {"ref_id": "b17", "title": "Region ensemble network: Improving convolutional network for hand pose estimation", "journal": "", "year": "", "authors": "Hengkai Guo; Guijin Wang; Xinghao Chen; Cairong Zhang; Fei Qiao; Huazhong Yang"}, {"ref_id": "b18", "title": "Denoising diffusion probabilistic models", "journal": "", "year": "2005", "authors": "Jonathan Ho; Ajay Jain; Pieter Abbeel"}, {"ref_id": "b19", "title": "Cascaded diffusion models for high fidelity image generation", "journal": "J. Mach. Learn. Res", "year": "", "authors": "Jonathan Ho; Chitwan Saharia; William Chan; J David; Mohammad Fleet; Tim Norouzi;  Salimans"}, {"ref_id": "b20", "title": "Diffpose: Multihypothesis human pose estimation using diffusion models", "journal": "", "year": "2003", "authors": "Karl Holmquist; Bastian Wandt"}, {"ref_id": "b21", "title": "Scorebased generative modeling of graphs via the system of stochastic differential equations", "journal": "", "year": "", "authors": "Jaehyeong Jo; Seul Lee; Sung Ju Hwang"}, {"ref_id": "b22", "title": "Point-to-pose voting based hand pose estimation using residual permutation equivariant layer", "journal": "", "year": "2019", "authors": "Shile Li; Dongheui Lee"}, {"ref_id": "b23", "title": "End-to-end human pose and mesh reconstruction with transformers", "journal": "", "year": "1954", "authors": "Kevin Lin; Lijuan Wang; Zicheng Liu"}, {"ref_id": "b24", "title": "Learning scene flow in 3d point clouds", "journal": "", "year": "2019", "authors": "Xingyu Liu; Leonidas J Charles R Qi;  Guibas"}, {"ref_id": "b25", "title": "", "journal": "", "year": "2017", "authors": "Ilya Loshchilov; Frank Hutter"}, {"ref_id": "b26", "title": "Diffusion probabilistic models for 3d point cloud generation", "journal": "", "year": "2007", "authors": "Shitong Luo; Wei Hu"}, {"ref_id": "b27", "title": "A conditional point diffusionrefinement paradigm for 3d point cloud completion", "journal": "", "year": "2021", "authors": "Zhaoyang Lyu; Zhifeng Kong; Xudong Xu; Liang Pan; Dahua Lin"}, {"ref_id": "b28", "title": "V2v-posenet: Voxel-to-voxel prediction network for accurate 3d hand and human pose estimation from a single depth map", "journal": "", "year": "2018", "authors": "Gyeongsik Moon; Yong Chang; Kyoung Mu Lee"}, {"ref_id": "b29", "title": "Permutation invariant graph generation via score-based generative modeling", "journal": "", "year": "", "authors": "Chenhao Niu; Yang Song; Jiaming Song; Shengjia Zhao; Aditya Grover; Stefano Ermon"}, {"ref_id": "b30", "title": "Deep-prior++: Improving fast and accurate 3d hand pose estimation", "journal": "", "year": "2017", "authors": "Markus Oberweger; Vincent Lepetit"}, {"ref_id": "b31", "title": "Handoccnet: Occlusion-robust 3d hand mesh estimation network", "journal": "", "year": "2022", "authors": "Joonkyu Park; Yeonguk Oh; Gyeongsik Moon; Hongsuk Choi; Kyoung Mu Lee"}, {"ref_id": "b32", "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation", "journal": "", "year": "2008", "authors": "Hao Charles R Qi; Kaichun Su; Leonidas J Mo;  Guibas"}, {"ref_id": "b33", "title": "Pointnet++: Deep hierarchical feature learning on point sets in a metric space", "journal": "", "year": "2017", "authors": "Li Charles Ruizhongtai Qi; Hao Yi; Leonidas J Su;  Guibas"}, {"ref_id": "b34", "title": "Srn: Stacked regression network for real-time 3d hand pose estimation", "journal": "", "year": "2005", "authors": "Pengfei Ren; Haifeng Sun; Qi Qi; Jingyu Wang; Weiting Huang"}, {"ref_id": "b35", "title": "Pose-guided hierarchical graph reasoning for 3-d hand pose estimation from a single depth image", "journal": "IEEE Transactions on Cybernetics", "year": "2006", "authors": "Pengfei Ren; Haifeng Sun; Jiachang Hao; Qi Qi; Jingyu Wang; Jianxin Liao"}, {"ref_id": "b36", "title": "Spatial-aware stacked regression network for real-time 3d hand pose estimation", "journal": "Neurocomputing", "year": "2021", "authors": "Pengfei Ren; Haifeng Sun; Weiting Huang; Jiachang Hao; Daixuan Cheng; Qi Qi; Jingyu Wang; Jianxin Liao"}, {"ref_id": "b37", "title": "Two heads are better than one: image-point cloud network for depth-based 3d hand pose estimation", "journal": "", "year": "2007", "authors": "Pengfei Ren; Yuchen Chen; Jiachang Hao; Haifeng Sun; Qi Qi; Jingyu Wang; Jianxin Liao"}, {"ref_id": "b38", "title": "Highresolution image synthesis with latent diffusion models", "journal": "", "year": "", "authors": "Robin Rombach; Andreas Blattmann; Dominik Lorenz; Patrick Esser; Bj\u00f6rn Ommer"}, {"ref_id": "b39", "title": "Palette: Image-to-image diffusion models", "journal": "", "year": "2022", "authors": "Chitwan Saharia; William Chan; Huiwen Chang; Chris Lee; Jonathan Ho; Tim Salimans; David Fleet; Mohammad Norouzi"}, {"ref_id": "b40", "title": "Image super-resolution via iterative refinement", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "", "authors": "Chitwan Saharia; Jonathan Ho; William Chan; Tim Salimans; J David; Mohammad Fleet;  Norouzi"}, {"ref_id": "b41", "title": "Diffusion-based 3d human pose estimation with multi-hypothesis aggregation", "journal": "", "year": "2023", "authors": "Wenkang Shan; Zhenhua Liu; Xinfeng Zhang; Zhao Wang; Kai Han; Shanshe Wang; Siwei Ma; Wen Gao"}, {"ref_id": "b42", "title": "Deep unsupervised learning using nonequilibrium thermodynamics", "journal": "PMLR", "year": "2015", "authors": "Jascha Sohl-Dickstein; Eric Weiss; Niru Maheswaranathan; Surya Ganguli"}, {"ref_id": "b43", "title": "Denoising diffusion implicit models", "journal": "", "year": "2008", "authors": "Jiaming Song; Chenlin Meng; Stefano Ermon"}, {"ref_id": "b44", "title": "Weakly supervised 3d hand pose estimation via biomechanical constraints", "journal": "Springer", "year": "2020", "authors": "Adrian Spurr; Umar Iqbal; Pavlo Molchanov; Otmar Hilliges; Jan Kautz"}, {"ref_id": "b45", "title": "Cascaded hand pose regression", "journal": "", "year": "2015", "authors": "Xiao Sun; Yichen Wei; Shuang Liang; Xiaoou Tang; Jian Sun"}, {"ref_id": "b46", "title": "Latent regression forest: Structured estimation of 3d articulated hand posture", "journal": "", "year": "2014", "authors": "Danhang Tang; Jin Hyung; Alykhan Chang; Tae-Kyun Tejani;  Kim"}, {"ref_id": "b47", "title": "Real-time continuous pose recovery of human hands using convolutional networks", "journal": "ACM Transactions on Graphics (ToG)", "year": "2014", "authors": "Jonathan Tompson; Murphy Stein; Yann Lecun; Ken Perlin"}, {"ref_id": "b48", "title": "Collaborative learning for hand and object reconstruction with attention-guided graph convolution", "journal": "", "year": "2022", "authors": "Tze Ho; Elden Tse"}, {"ref_id": "b49", "title": "Lion: Latent point diffusion models for 3d shape generation", "journal": "", "year": "2022", "authors": "Arash Vahdat; Francis Williams; Zan Gojcic; Or Litany; Sanja Fidler; Karsten Kreis"}, {"ref_id": "b50", "title": "Digress: Discrete denoising diffusion for graph generation", "journal": "", "year": "", "authors": "Clement Vignac; Igor Krawczuk; Antoine Siraudin; Bohan Wang; Volkan Cevher; Pascal Frossard"}, {"ref_id": "b51", "title": "Dense 3d regression for hand pose estimation", "journal": "", "year": "2018", "authors": "Chengde Wan; Thomas Probst; Luc Van Gool; Angela Yao"}, {"ref_id": "b52", "title": "A2j: Anchor-to-joint regression network for 3d articulated pose estimation from a single depth image", "journal": "", "year": "2019", "authors": "Fu Xiong; Boshen Zhang; Yang Xiao; Zhiguo Cao; Taidong Yu; Joey Tianyi Zhou; Junsong Yuan"}, {"ref_id": "b53", "title": "3d shape generation and completion through point-voxel diffusion", "journal": "", "year": "2021", "authors": "Linqi Zhou; Yilun Du; Jiajun Wu"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. Illustration of the hand pose diffusion concept. The model extracts features from input depth images and corresponding point clouds as joint-wise and local conditions to guide the iterative denoising process that recovers accurate hand poses from diffused noisy pose distributions.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "d 2d and d 3d are 128, the joint-wise condition depth d c is 512 and the batch size was set to 64. The diffusion timestep was set to 500 with a cosine variance scheduler. Meanwhile, to avoid overfitting, we adopted online data augmentation with random rotation ([-180.0, 180.0] degrees), 3D scaling ([0.8, 1.2]), and 3D translation ([-20, 20] mm). We trained the model for 30 epochs with a learning rate decay of 0.1 after every 10 epochs.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 .3Figure 3. Qualitative results of HandDiff on the DexYCB datasets including different grabbing poses (top), self-occlusions (middle), and object occlusions (bottom). Hand-depth images (first rows) are transformed into 3D points (second rows) in order to clearly present occlusions as shown in the figure. Ground truth is shown in black and the estimated joint coordinates of our model are shown in colors.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 .4Figure 4. Comparison with the state-of-the-art methods using the ICVL (left), MSRA (middle), and NYU (right) dataset. The per joint error (top) and success rate (bottom) are shown in this figure.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 .5Figure 5. Qualitative results of HandDiff on the ICVL (left), MSRA (middle), and NYU (right) datasets. Hand-depth images are transformed into 3D points in order to clearly present occlusions as shown in the figure. Ground truth is shown in black, results from comparative HandFoldingNet[9] are shown in orange, and the estimated joint coordinates of our model are shown in red.", "figure_data": ""}, {"figure_label": "16", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "1 Figure 6 .16Figure 6. Evaluation results of the increasing number of timesteps (top) and hypotheses (bottom) in the denoising process on the NYU dataset. The model is trained with 500 diffusion timesteps.", "figure_data": ""}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Comparison of the proposed method with previous stateof-the-art methods on the ICVL, MSRA, and NYU datasets. Input indicates the input type of 2D depth image (D), 3D voxels (V), or 3D point cloud (P). \u2020 The results are reported from the retrained VVS following the same cropping strategy[31] as in the previous state-of-the-art methods[9,15,16,29,36,37,46]. Comparison of the proposed method with previous stateof-the-art methods on the DexYCB datasets.", "figure_data": "MethodMean joint error (mm) ICVL MSRA NYUInputDeepPrior++ [31]8.19.512.24DPose-Ren [6]6.798.6511.81DDenseReg [52]7.37.210.2DCrossInfoNet [11]6.737.8610.08DJGR-P2O [12]6.027.558.29DSSRN [37]6.017.057.37DPHG [36]5.976.947.39DVVS [7]  \u20206.22-7.79D3DCNN [14]-9.614.1VSHPR-Net [5]7.227.7610.78PHandPointNet [15]6.948.510.54PPoint-to-Point [16]6.37.79.10PV2V [29]6.287.598.42VHandFolding [9]5.957.348.58PIPNet [38]5.766.927.17D+PHandDiff (Ours)5.726.537.38D+PMethodS0Mean joint error (mm) S1 S2 S3 AVGInputA2J [53]23.93 25.57 27.65 24.92 25.52 DSpurr et al. [45] 17.34 22.26 25.49 18.44 18.44 RGBMETRO [24] 15.24----RGBTse et al. [49] 16.05 21.22 27.01 17.93 20.55 RGBHandOcc [32] 14.04----RGBIPNet [38]8.03 9.01 8.60 7.80 8.36 D+POurs7.66 8.73 8.40 7.53 8.07 D+Perror measures the average Euclidean distance between theestimated and ground-truth joint locations for each jointover the testing set. The success rate reveals the percent-age of good frames with a mean joint error of less than acertain distance threshold.4.3. Comparison with State-of-the-Art MethodsSingle hand. We compare HandDiff on the ICVL, MSRA,and NYU dataset with other state-of-the-art methods, in-cluding methods with 2D depth images as input: improvedDeepPrior (DeepPrior++)"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Ablations of different proposed components. All the ablation models are trained and tested on the DexYCB dataset . JC LC JI KC MH Mean joint error", "figure_data": "\u27139.17 mm\u271349.58 mm\u27138.37 mm\u2713\u27138.23 mm\u2713\u27138.28 mm\u2713\u2713\u27138.13 mm\u2713\u2713\u27137.94 mm\u2713\u2713\u2713\u27137.74 mm\u2713\u2713\u2713\u2713\u27137.66 mmTable 4. Ablations of different modalities of conditions. All theablation models are trained and tested on the DexYCB dataset .Condition modalityMean joint error (mm)2D depth8.233D points9.582D depth + 3D points7.74respondence (KC); 7) using JC, LC with JI and KC, andmultiple hypotheses (MH), which is our full configuration.Note that the use of local conditions without KC is imple-mented by applying a PointNet layer"}], "formulas": [{"formula_id": "formula_0", "formula_text": "J (0|t) = D(J (t) , C, F 2d , F 3d , t).", "formula_coordinates": [3.0, 361.66, 430.78, 130.65, 11.72]}, {"formula_id": "formula_1", "formula_text": ")1", "formula_coordinates": [3.0, 537.37, 433.18, 7.74, 8.64]}, {"formula_id": "formula_2", "formula_text": "C \u2032 = ReLU (ACW),(2)", "formula_coordinates": [4.0, 123.02, 584.69, 163.35, 11.03]}, {"formula_id": "formula_3", "formula_text": "F \u2032 k,j = M LP ([P k,j \u2212 J j , F k,j , C \u2032 j , P E(t), P E(j)]),(3)", "formula_coordinates": [4.0, 315.46, 349.15, 229.65, 22.98]}, {"formula_id": "formula_4", "formula_text": "C = M axP ool(F \u2032 ).(4)", "formula_coordinates": [4.0, 384.79, 502.64, 160.32, 11.03]}, {"formula_id": "formula_5", "formula_text": "J (0|t) =\u0108W + J (t) ,(5)", "formula_coordinates": [4.0, 383.91, 606.36, 161.2, 11.03]}, {"formula_id": "formula_6", "formula_text": "q(J (t) |J (0) ) = \u221a\u1fb1 t J (0) + \u03f5 \u221a 1 \u2212\u1fb1 t , where\u1fb1 t = t s=0 \u03b1 t = t s=0 (1 \u2212 \u03b2 s ).(6)", "formula_coordinates": [5.0, 93.23, 98.66, 193.13, 53.08]}, {"formula_id": "formula_7", "formula_text": "L1 smooth (x) = 50x 2 , |x| < 0.01 |x| \u2212 0.005, otherwise .(7)", "formula_coordinates": [5.0, 78.18, 346.15, 208.19, 24.66]}, {"formula_id": "formula_8", "formula_text": "L = J j=0 L1 smooth ( J (0|t) j \u2212 J * j ).(8)", "formula_coordinates": [5.0, 103.23, 414.5, 183.14, 30.32]}, {"formula_id": "formula_9", "formula_text": "p \u03b8 (J (t\u22121) 0:H | J (0|t) 0:H ) = \u221a\u1fb1 t\u22121 J (0|t) 0:H + 1 \u2212\u1fb1 t\u22121 \u2212 \u03c3 2 t \u03f5 t +\u03c3 t \u03f5,(9)", "formula_coordinates": [5.0, 50.11, 646.85, 240.5, 29.23]}, {"formula_id": "formula_10", "formula_text": "(t) 0:H \u2212 \u221a\u1fb1 t J (0) 0:H )/ \u221a 1 \u2212\u1fb1 t is the predicted noise of timestep t and \u03c3 t = (1 \u2212\u1fb1 t\u22121 )(1 \u2212\u1fb1 t /\u1fb1 t\u22121 )/(1 \u2212\u1fb1 t ).", "formula_coordinates": [5.0, 50.11, 672.79, 236.25, 41.05]}, {"formula_id": "formula_11", "formula_text": "J (0) = 1 H H h=0 J (0) h .(10)", "formula_coordinates": [5.0, 387.57, 133.15, 157.55, 30.55]}], "doi": ""}