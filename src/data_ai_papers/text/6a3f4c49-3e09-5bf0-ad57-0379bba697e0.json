{"title": "Beyond neural scaling laws: beating power law scaling via data pruning", "authors": "Ben Sorscher; Robert Geirhos; Shashank Shekhar; Surya Ganguli; Ari S Morcos", "pub_date": "2023-04-21", "abstract": "Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how in theory we can break beyond power law scaling and potentially even reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned dataset size. We then test this improved scaling prediction with pruned dataset size empirically, and indeed observe better than power law scaling in practice on ResNets trained on CIFAR-10, SVHN, and ImageNet. Next, given the importance of finding high-quality pruning metrics, we perform the first large-scale benchmarking study of ten different data pruning metrics on ImageNet. We find most existing high performing metrics scale poorly to ImageNet, while the best are computationally intensive and require labels for every image. We therefore developed a new simple, cheap and scalable self-supervised pruning metric that demonstrates comparable performance to the best supervised metrics. Overall, our work suggests that the discovery of good data-pruning metrics may provide a viable path forward to substantially improved neural scaling laws, thereby reducing the resource costs of modern deep learning. * work done during an internship at Meta AI (FAIR) 36th Conference on Neural Information Processing Systems (NeurIPS 2022).", "sections": [{"heading": "Introduction", "text": "Empirically observed neural scaling laws [1,2,3,4,5,6,7,8] in many domains of machine learning, including vision, language, and speech, demonstrate that test error often falls off as a power law with either the amount of training data, model size, or compute. Such power law scaling has motivated significant societal investments in data collection, compute, and associated energy consumption. However, power law scaling is extremely weak and unsustainable. For example, a drop in error Figure 1: Our analytic theory of data pruning predicts that power law scaling of test error with respect to dataset size can be beaten. A: Test error as a function of \u03b1 prune = f \u03b1 tot with \u03b8 = 0. We observe an excellent match between our analytic theory (solid curves) and numerical simulations (dots) of perceptron learning at parameters N=200 (here: N=200 constant throughout figure). The red curve indicates the Pareto optimal test error \u03b5 achievable from a tradeoff between \u03b1 tot and f at fixed \u03b1 prune . B: We find that when data is abundant (scarce) corresponding to large (small) \u03b1 tot , the better pruning strategy is to keep the hard (easy) examples. C: Color indicates difference in test error in keeping hard versus easy examples, revealing the change in strategy in (B). D: We tested this prediction on a ResNet18 trained on CIFAR-10, finding remarkably the same shift in optimal pruning strategy under the EL2N metric. E: Test accuracy as a function of f and \u03b1 prune . For every fixed \u03b1 prune , there is an optimal f opt (purple curve). F: I(\u03b1 prune ) for different f . from 3% to 2% might require an order of magnitude more data, compute, or energy. In language modeling with large transformers, a drop in cross entropy loss from about 3.4 to 2.8 nats 2 requires 10 times more training data (Fig. 1 in [2]). Also, for large vision transformers, an additional 2 billion pre-training data points (starting from 1 billion) leads to an accuracy gain on ImageNet of a few percentage points (Fig. 1 in [7]). Here we ask whether we might be able to do better. For example, can we achieve exponential scaling instead, with a good strategy for selecting training examples? Such vastly superior scaling would mean that we could go from 3% to 2% error by only adding a few carefully chosen training examples, rather than collecting 10\u00d7 more random ones.\nFocusing on scaling of performance with training dataset size, we demonstrate that exponential scaling is possible, both in theory and practice. The key idea is that power law scaling of error with respect to data suggests that many training examples are highly redundant. Thus one should in principle be able to prune training datasets to much smaller sizes and train on the smaller pruned datasets without sacrificing performance. Indeed some recent works [9,10,11] have demonstrated this possibility by suggesting various metrics to sort training examples in order of their difficulty or importance, ranging from easy or redundant examples to hard or important ones, and pruning datasets by retaining some fraction of the hardest examples. However, these works leave open fundamental theoretical and empirical questions: When and why is successful data pruning possible? What are good metrics and strategies for data pruning? Can such strategies beat power law scaling? Can they scale to ImageNet? Can we leverage large unlabeled datasets to successfully prune labeled datasets?\nWe address these questions through both theory and experiment. Our main contributions are:\n1. Employing statistical mechanics, we develop a new analytic theory of data pruning in the student-teacher setting for perceptron learning, where examples are pruned based on their teacher margin, with large (small) margins corresponding to easy (hard) examples. Our theory quantitatively matches numerical experiments and reveals two striking predictions:\n(a) The optimal pruning strategy changes depending on the amount of initial data; with abundant (scarce) initial data, one should retain only hard (easy) examples. (b) Exponential scaling is possible with respect to pruned dataset size provided one chooses an increasing Pareto optimal pruning fraction as a function of initial dataset size.\n2. We show that the two striking predictions derived from theory hold also in practice in much more general settings. Indeed we empirically demonstrate signatures of exponential scaling of error with respect to pruned dataset size for ResNets trained from scratch on SVHN, CIFAR-10 and ImageNet, and Vision Transformers fine-tuned on CIFAR-10.\n3. Motivated by the importance of finding good quality metrics for data pruning, we perform a large scale benchmarking study of 10 different data pruning metrics at scale on ImageNet, finding that most perform poorly, with the exception of the most compute intensive metrics.\n4. We leveraged self-supervised learning (SSL) to developed a new, cheap unsupervised data pruning metric that does not require labels, unlike prior metrics. We show this unsupervised metric performs comparably to the best supervised pruning metrics that require labels and much more compute. This result opens the door to the exciting possibility of leveraging pre-trained foundation models to prune new datasets even before they are labeled.\nOverall these results shed theoretical and empirical insights into the nature of data in deep learning and our ability to prune it, and suggest our current practice of collecting extremely large datasets may be highly inefficient. Our initial results in beating power law scaling motivate further studies and investments in not just inefficently collecting large amounts of random data, but rather, intelligently collecting much smaller amounts of carefully selected data, potentially leading to the creation and dissemination of foundation datasets, in addition to foundation models [12].", "publication_ref": ["b0", "b1", "b2", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Background and related work", "text": "Our work brings together 3 largely disparate strands of intellectual inquiry in machine learning: (1) explorations of different metrics for quantifying differences between individual training examples;\n(2) the empirical observation of neural scaling laws; and (3) the statistical mechanics of learning.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Pruning metrics: not all training examples are created equal", "text": "Several recent works have explored various metrics for quantifying individual differences between data points. To describe these metrics in a uniform manner, we will think of all of them as ordering data points by their difficulty, ranging from \"easiest\" to \"hardest.\" When these metrics have been used for data pruning, the hardest examples are retained, while the easiest ones are pruned away. EL2N scores. For example [10] trained small ensembles (of about 10) networks for a very short time (about 10 epochs) and computed for every training example the average L 2 norm of the error vector (EL2N score). Data pruning by retaining only the hardest examples with largest error enabled training from scratch on only 50% and 75% of CIFAR-10 and CIFAR-100 respectively without any loss in final test accuracy. However the performance of EL2N on ImageNet has not yet been explored.\nForgetting scores and classification margins.\n[9] noticed that over the entire course of training, some examples are learned early and never forgotten, while others can be learned and unlearned (i.e. forgotten) repeatedly. They developed a forgetting score which measures the degree of forgetting of each example. Intuitively examples with low (high) forgetting scores can be thought of as easy (hard) examples.\n[9] explored data pruning using these metrics, but not at ImageNet scale.\nMemorization and influence.\n[13] defined a memorization score for each example, corresponding to how much the probability of predicting the correct label for the example increases when it is present in the training set relative to when it is absent (also see [14]); a large increase means the example must be memorized (i.e. the remaining training data do not suffice to correctly learn this example). Additionally [13] also considered an influence score that quantifies how much adding a particular example to the training set increases the probability of the correct class label of a test example. Intuitively, low memorization and influence scores correspond to easy examples that are redundant with the rest of the data, while high scores correspond to hard examples that must be individually learned.\n[13] did not use these scores for data pruning as their computation is expensive. We note since memorization explicitly approximates the increase in test loss due to removing each individual example, it is likely to be a good pruning metric (though it does not consider interactions).\nEnsemble active learning. Active learning iterates between training a model and selecting new inputs to be labeled [15,16,17,18,19]. In contrast, we focus on data pruning: one-shot selection of a data subset sufficient to train to high accuracy from scratch. A variety of coreset algorithms (e.g. [20]) have been proposed for this, but their computation is expensive, and so data-pruning has been less explored at scale on ImageNet. An early clustering approach [21] allowed training on 90% of ImageNet without sacrificing accuracy. Notably [11] reduced this to 80% by training a large ensemble of networks on ImageNet and using ensemble uncertainty to define the difficulty of each example, with low (high) uncertainty corresponding to easy (hard) examples. We will show how to achieve similar pruning performance without labels or the need to train a large ensemble.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Diverse ensembles (DDD).", "text": "[22] assigned a score to every ImageNet image, given by the number of models in a diverse ensemble (10 models) that misclassified the image. Intuitively, low (high) scores correspond to easy (hard) examples. The pruning performance of this metric remains unexplored.\nSummary. We note: (1) only one of these metrics has tested well for its efficacy in data pruning at scale on ImageNet;\n(2) all of these metrics require label information; (3) there is no theory of when and why data pruning is possible for any of these metrics; and (4) none of these works suggest the possibility of exponential scaling. We thus go beyond this prior work by benchmarking the data pruning efficacy of not only these metrics but also a new unsupervised metric we introduce that does not require label information, all at scale on ImageNet. We also develop an analytic theory for data-pruning for the margin metric that predicts not only the possibility of exponential scaling but also the novel finding that retaining easy instead of hard examples is better when data is scarce.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Neural scaling laws and their potential inefficiency", "text": "Recent work [1,2,3,4,5,6,7,8] has demonstrated that test loss L often falls off as a power law with different resources like model parameters (N ), number of training examples (P ), and amount of compute (C). However, the exponents \u03bd of these power laws are often close to 0, suggesting potentially inefficient use of resources. For example, for large models with lots of compute, so that the amount of training data constitutes a performance bottleneck, the loss scales as L \u2248 P \u2212\u03bd . Specifically for a large transformer based language model, \u03bd = 0.095, which implies an order of magnitude increase in training data drops cross-entropy loss by only about 0.6 nats (Fig. 1 in [2]). In neural machine translation experiments \u03bd varies across language pairs from 0.35 to 0.48 (Table 1  ). While all of these results constitute significant improvements in performance, they do come at a substantial resource cost whose fundamental origin arises from power law scaling with small exponents. Recent theoretical works [23,24,25] have argued that the power law exponent is governed by the dimension of a data manifold from which training examples are uniformly drawn. Here we explore whether we can beat power law scaling through careful data selection.", "publication_ref": ["b0", "b1", "b2", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Statistical mechanics of perceptron learning", "text": "Statistical mechanics has long played a role in analyzing machine learning problems (see e.g.\n[26, 27, 28, 29] for reviews). One of the most fundamental applications is perceptron learning in the student-teacher setting [30,31], in which random i.i.d. Gaussian inputs are labeled by a teacher perceptron to construct a training set. The test error for another student perceptron learning from this training set then scales as a power law with exponent \u22121 for such data. Such perceptrons have also been analyzed in an active learning setting where the learner is free to design any new input to be labeled [32,33], rather than choose from a fixed set of inputs, as in data-pruning. Recent work [34] has analyzed this scenario but focused on message passing algorithms that are tailored to the case of Gaussian inputs and perceptrons, and are hard to generalize to real world settings. In contrast we analyze margin based pruning algorithms that are used in practice in diverse settings, as in [9, 10].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "An analytic theory of data pruning", "text": "To better understand data pruning, we employed the replica method from statistical mechanics [35] to develop an analytic theory of pruning for the perceptron in the student-teacher setting [26] (see App. A for detailed derivations of all results). Consider a training dataset of P examples {x \u00b5 , y \u00b5 } \u00b5=1,...,P where x \u00b5 \u2208 R N are i.i.d. zero mean unit variance random Gaussian inputs and y \u00b5 = sign(T \u2022 x \u00b5 ) are labels generated by a teacher perceptron with weight vector T \u2208 R N . We work in the high dimensional statistics limit where N, P \u2192 \u221e but the ratio \u03b1 tot = P N of the number of total training examples to parameters remains O(1). We then consider a pruning algorithm used in [9, 10], namely:\n(1) train a probe student perceptron for very few epochs on the training data, obtaining weights J probe ;\n(2) compute the margin m \u00b5 = J probe \u2022 (y \u00b5 x \u00b5 ) of each training example, where large (small) margins correspond to easy (hard) examples; (3) construct a pruned dataset of size P prune = f P , where f is the fraction of examples kept, by retaining the P prune hardest examples, (4) train a new perceptron to completion on the smaller dataset with a smaller ratio \u03b1 prune = Pprune N of examples to parameters. We are interested in the test error \u03b5 of this final perceptron as a function of \u03b1 tot , f , and the angle \u03b8 between the probe student J probe and the teacher T. Our theory approximates J probe as simply a random Gaussian vector conditioned to have angle \u03b8 with the teacher T. Under this approximation we obtain an analytic theory for \u03b5(\u03b1 tot , f, \u03b8) that is asymptotically exact in the high dimensional limit (App. A). We first examine results when \u03b8 = 0, so we are pruning training examples according to their veridical margins with respect to the teacher (Fig. 1A). We find two striking phenomena, each of which constitute predictions in real-world settings that we will successfully confirm empirically.\nThe best pruning strategy depends on the amount of initial data. First, we note the test error curve for f = 1 in Fig. 1A corresponding to no pruning, or equivalently to randomly pruning a larger dataset of size \u03b1 tot down to a size \u03b1 prune , exhibits the well known classical perceptron learning power law scaling \u03b5 \u221d \u03b1 \u22121 prune . Interestingly though, for small \u03b1 tot , keeping the hardest examples performs worse than random pruning (lighter curves above darkest curve for small \u03b1 prune in Fig. 1A). However, for large \u03b1 tot , keeping the hardest examples performs substantially better than random pruning (lighter curves below darkest curve for large \u03b1 prune in Fig. 1A). It turns out keeping the easiest rather than hardest examples is a better pruning strategy when \u03b1 tot is small (Fig. 1C). If one does not have much data to start with, it is better to keep the easiest examples with largest margins (i.e. the blue regions of Fig. 1B) to avoid overfitting. The easiest examples provide coarse-grained information about the target function, while the hard examples provide fine-grained information about the target function which can prevent the model from learning if one starts with lots of data. In cases where overfitting is less of an issue, it is best to keep the hardest examples with smallest margin that provide more information about the teacher's decision boundary (i.e. the green region of Fig. 1B). Intuitively, in the limited data regime, it is challenging to model outliers since the basics are not adequately captured; hence, it is more important to keep easy examples so that the model can get to moderate error. However, with a larger dataset, the easy examples can be learned without difficulty, making modeling outliers the fundamental challenge. Fig. 1C reveals which pruning strategy is best as a joint function of \u03b1 tot and f . Note the transition between optimal strategies becomes sharper at small fractions f of data kept. This transition between optimal pruning strategies can be viewed as a prediction in more general settings. To test this prediction we trained a ResNet18 on pruned subsets of the CIFAR-10 dataset (Fig. 1D), and observed strikingly similar behavior, indicating the prediction can hold far more generally, beyond perceptron learning. Interestingly, [9, 10] missed this transition, likely because they started pruning from large datasets.\nPareto optimal data pruning can beat power law scaling. A second prediction of our theory is that when keeping a fixed fraction f of the hardest examples as \u03b1 tot increases (i.e. constant color curves in Fig. 1A), the error initially drops exponentially in \u03b1 prune = f \u03b1 tot , but then settles into the universal power law \u03b5 \u221d \u03b1 \u22121 prune for all fixed f . Thus there is no asymptotic advantage to data  pruning at a fixed f . However, by pruning more aggressively (smaller f ) when given more initial data (larger \u03b1 tot ), one can achieve a Pareto optimal test error as a function of pruned dataset size \u03b1 prune that remarkably traces out at least an exponential scaling law (Fig. 1A, purple curve). Indeed our theory predicts for each \u03b1 prune a Pareto optimal point in \u03b1 tot and f (subject to \u03b1 prune = f \u03b1 tot ), yielding for every fixed \u03b1 prune an optimal f opt , plotted in Fig. 1E. Note f opt decreases with \u03b1 prune indicating more aggressive pruning (smaller f opt ) of original datasets of larger size \u03b1 tot is required to obtain larger Pareto optimal pruned datasets of size \u03b1 prune . We will test this striking scaling prediction in Fig. 3.\nBeating power law scaling: an information-theoretic perspective. Classical randomly selected data generates slow power law error scaling because each extra training example provides less new information about the correct decision boundary than the previous example. More formally, let S(\u03b1 tot ) denote the typical entropy of the posterior distribution over student perceptron weights consistent with a training set of size \u03b1 tot . The information gain I(\u03b1 tot ) due to additional examples beyond \u03b1 tot can be defined as the rate at which the posterior entropy is reduced: I(\u03b1 tot ) = \u2212 d d\u03b1tot S(\u03b1 tot ). In classical perceptron learning I(\u03b1 tot ) decays to zero as a power law in \u03b1 tot , reflecting a vanishing amount of information per each new example, leading to the slow power law decay of test error \u03b5 \u221d \u03b1 \u22121 tot . However, data pruning can increase the information gained per example by pruning away the uninformative examples. To show this, we generalized the replica calculation of the posterior entropy S and information gain I from random datasets of size \u03b1 tot to pruned datasets of size \u03b1 prune (App. A). We plot the resulting information gain I(\u03b1 prune ) for different f in Fig. 1F. For any fixed f , I(\u03b1 prune ) will eventually decay as a power law as \u03b1 \u22121 prune . However, by more aggressively pruning (smaller f ) datasets of larger size \u03b1 tot , I(\u03b1 prune ) can converge to a finite value I(\u221e) = 1 nat/example, resulting in larger pruned datasets only adding useful non-redundant information. Since each new example under Pareto optimal data pruning conveys finite information about the target decision boundary, as seen in Fig. 1F, the test error can decay at least exponentially in pruned dataset size as in Fig. 1A. Classical results [31] have shown that training examples chosen by maximizing the disagreement of a committee of student perceptrons can provide an asymptotically finite information rate, leading to exponential decay in test error. Intriguingly, the Pareto-optimal data pruning strategy we study in this work leads to faster than exponential decay, because it includes (partial) information about the target function provided by the probe student (Fig. 11).\nAn imperfect pruning metric yields a cross over from exponential to power law scaling. We next examine the case of nonzero angle \u03b8 between the probe student J probe and the teacher T, such that the ranking of training examples by margin is no longer completely accurate (Fig. 2A). Retaining the hard examples with smallest margin with respect to the probe student will always result in pruned datasets lying near the probe's decision boundary. But if \u03b8 is large, such examples might be far from the teacher's decision boundary, and therefore could be less informative about the teacher (Fig. 2A). As a result our theory, confirmed by simulations, predicts that under nonzero angles \u03b8, the Pareto optimal lower envelope of test error over both \u03b1 tot and f initially scales exponentially as a function of \u03b1 prune = f \u03b1 tot but then crosses over to a power law (Fig. 2BCD). Indeed, at any given nonzero \u03b8, our theory reveals that as \u03b1 tot (and therefore \u03b1 prune ) becomes large, one cannot decrease test error any further by retaining less than a minimum fraction f min (\u03b8) of all available data. For example when \u03b8 = 10 \u2022 (\u03b8 = 20 \u2022 ) one can do no better asymptotically than pruning down to 24% (46%) of the total  data (Fig. 2CD). As \u03b8 approaches 0, f min (\u03b8) approaches 0, indicating that one can prune extremely aggressively to arbitrarily small f while still improving performance, leading to at least exponential scaling for arbitrarily large \u03b1 prune in Fig. 2B. However, for nonzero \u03b8, the lack of improvement for f < f min (\u03b8) at large \u03b1 prune renders aggressive pruning ineffective. This result highlights the importance of finding high quality pruning metrics with \u03b8 \u2248 0. Such metrics can delay the cross over from exponential to power law scaling as pruned dataset size \u03b1 prune increases, by making aggressive pruning with very small f highly effective. Strikingly, in App. Fig. 10 we demonstrate this cross-over in a real-world setting by showing that the test error on SVHN is bounded below by a power law when the dataset is pruned by a probe ResNet18 under the EL2N metric, trained for 4 epochs (weak pruning metric) but not a probe ResNet18 trained for 40 epochs (strong pruning metric).\n4 Data pruning can beat power law scaling in practice\nOur theory of data pruning for the perceptron makes three striking predictions which can be tested in more general settings, such as deep neural networks trained on benchmark datasets: (1) relative to random data pruning, keeping only the hardest examples should help when the initial dataset size is large, but hurt when it is small; (2) data pruning by retaining a fixed fraction f of the hardest examples should yield power law scaling, with exponent equal to that of random pruning, as the initial dataset size increases; (3) the test error optimized over both initial data set size and fraction of data kept can trace out a Pareto optimal lower envelope that beats power law scaling of test error as a function of pruned dataset size, through more aggressive pruning at larger initial dataset size. We verified all three of these predictions on ResNets trained on SVHN, CIFAR-10, and ImageNet using varying amounts of initial dataset size and fractions of data kept under data pruning (compare theory in Fig. 3A with deep learning experiments in Fig. 3BCD). In each experimental setting we see better than power law scaling at larger initial data set sizes and more aggressive pruning. Moreover we would likely see even better scaling with even larger initial datasets (as in Fig. 3A dashed lines).\nData pruning improves transfer learning. Modern foundation models are pre-trained on a large initial dataset, and then transferred to other downstream tasks by fine-tuning on them. We therefore examined whether data-pruning can be effective for both reducing the amount of fine-tuning data and the amount of pre-training data. To this end, we first analyzed a vision transformer (ViT) pre-trained on ImageNet21K and then fine-tuned on different pruned subsets of CIFAR-10. Interestingly, pre-trained models allow for far more aggressive data pruning; fine-tuning on only 10% of CIFAR-10 can match or exceed performance obtained by fine tuning on all of CIFAR-10 (Fig. 4A). Furthermore Fig. 4A provides a new example of beating power law scaling in the setting of fine-tuning. Additionally, we examined the efficacy of pruning pre-training data by pre-training ResNet50s on different pruned subsets of ImageNet1K (exactly as in Fig. 3D) and then fine-tuning them on all of CIFAR-10. ", "publication_ref": [], "figure_ref": ["fig_0", "fig_0", "fig_0", "fig_0", "fig_0", "fig_1", "fig_1"], "table_ref": []}, {"heading": "Benchmarking supervised pruning metrics on ImageNet", "text": "We note that the majority of data pruning experiments have been performed on small-scale datasets (i.e. variants of MNIST and CIFAR), while the few pruning metrics proposed for ImageNet have rarely been compared against baselines designed on smaller datasets. Therefore, it is currently unclear how most pruning methods scale to ImageNet and which method is best. Motivated by how strongly the quality of a pruning metric can impact performance in theory (Fig. 2 . See Section 2 for a review of these metrics. Additionally, we include two new prototypicality metrics that we introduce in the next section.\nWe first asked how consistent the rankings induced by different metrics are by computing the Spearman rank correlation between each pair of metrics (Fig. 5A). Interestingly, we found substantial diversity across metrics, though some (EL2N, DDD, and memorization) were fairly similar with rank correlations above 0.7. However, we observed marked performance differences between metrics: Fig 5BC shows test performance when a fraction f of the hardest examples under each metric are kept in the training set. Despite the success of many of these metrics on smaller datasets, only a few still match performance obtained by training on the full dataset, when selecting a significantly smaller training subset (i.e. about 80% of ImageNet). Nonetheless, most metrics continue to beat random pruning, with memorization in particular demonstrating strong performance (Fig. 5C). We note that data pruning on ImageNet may be more difficult than data pruning on other datasets, because ImageNet is already carefully curated to filter out uninformative examples.\nWe found that all pruning metrics amplify class imbalance, which results in degraded performance.\nTo solve this we used a simple 50% class balancing ratio for all ImageNet experiments. Further details and baselines without class balancing are shown in App. H. Metric scores, including baselines, are available from https://github.com/rgeirhos/dataset-pruning-metrics.\n6 Self-supervised data pruning through a prototypicality metric Fig. 5 shows many data pruning metrics do not scale well to ImageNet, while the few that do require substantial amounts of compute. Furthermore, all these metrics require labels, thereby limiting their ability to prune data for large-scale foundation models trained on massive unlabeled datasets [12]. Thus there is a clear need for simple, scalable, self-supervised pruning metrics.\nTo compute a self-supervised pruning metric for ImageNet, we perform k-means clustering in the embedding space of an ImageNet pre-trained self-supervised model (here: SWaV [36]), and define the difficulty of each data point by the cosine distance to its nearest cluster centroid, or prototype. Thus easy (hard) examples are the most (least) prototypical. Encouragingly, in Fig. 5C, we find our self-supervised prototype metric matches or exceeds the performance of the best supervised metric, memorization, until only 70-80% of the data is kept, despite the fact that our metric does not use labels and is much simpler and cheaper to compute than many previously proposed supervised metrics. See App. Fig. 9 for further scaling experiments using the self-supervised metric.\nTo assess whether the clusters found by our metric align with ImageNet classes, we compared their overlaps in Fig. 6A. Interestingly, we found alignment for some but not all classes. For example, class categories such as snakes were largely aligned to a small number of unsupervised clusters, while other classes were dispersed across many such clusters. If class information is available, we can enforce alignment between clusters and classes by simply computing a single prototype for each class (by averaging the embeddings of all examples of this class). While originally intended to be an additional baseline metric (called supervised prototypes, light blue in Fig 5C ), this metric remarkably outperforms other supervised metrics and largely matches the performance of memorization, which is prohibitively expensive to compute. Moreover, the performance of the best self-supervised and supervised metrics are similar, demonstrating the promise of self-supervised pruning.\nOne important choice for the self-supervised prototype metric is the number of clusters k. We found, reassuringly, our results were robust to this choice: k can deviate one order of magnitude more or less than the true number of classes (i.e. 1000 for ImageNet) without affecting performance (App. F).\nTo better understand example difficulty under various metrics, we visualize extremal images for our self-supervised prototype metric and the memorization metric for one class (Fig 6B ImageNet class (sorted by wordnet hierarchy) SSL cluster (sorted hierarchically)", "publication_ref": [], "figure_ref": ["fig_0", "fig_2", "fig_2", "fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "A B C", "text": "self-supervised prototypes memorization Figure 6: A: Heat map where each row denotes the probability that images in a given cluster come from each ImageNet class. B: The four easiest and hardest images under our self-supervised pruning metric and the best previously published supervised metric (memorization, shown in C) for ImageNet class 100 (black swan).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discussion", "text": "Summary. We have shown, both in theory and practice, how to break beyond slow power law scaling of error versus dataset size to faster exponential scaling, through data pruning. Additionally we have developed a simple self-supervised pruning metric that enables us to discard 20% of ImageNet without sacrificing performance, on par with the best and most compute intensive supervised metric.\nLimitations. The most notable limitation is that achieving exponential scaling requires a high quality data pruning metric. Since most metrics developed for smaller datasets scale poorly to ImageNet, our results emphasize the importance of future work in identifying high quality, scalable metrics. Our self-supervised metric provides a strong initial baseline. Moreover, a key advantage of data pruning is reduced computational cost due to training on a smaller dataset for the same number of epochs as the full dataset (see App. C). However, we found that performance often increased when training on the pruned dataset for the same number of iterations as on the full dataset, resulting in the same training time, but additional training epochs. However, this performance gain saturated before training time on the pruned dataset approached that on the whole dataset (App. J) thereby still yielding a computational efficiency gain. Overall this tradeoff between accuracy and training time on pruned data is important to consider in evaluating potential gains due to data pruning. Finally, we found that class-balancing was essential to maintain performance on data subsets (App. H). Future work will be required to identify ways to effectively select the appropriate amount of class-balancing.\nEthical considerations. A potential negative societal impact could be that data-pruning leads to unfair outcomes for certain groups. We have done a preliminary analysis of how data-pruning affects performance on individual ImageNet classes (App. I), finding no substantial differential effects across classes. However proper fairness tests specific to deployment settings should always be conducted on every model, whether trained on pruned data or not. Additionally, we analyzed the impact of pruning on OOD performance (App. K).\nOutlook: Towards foundation datasets. We believe the most promising future direction is the further development of scalable, unsupervised data pruning metrics. Indeed our theory predicts that the application of pruning metrics on larger scale datasets should yield larger gains by allowing more aggressive pruning. This makes data pruning especially exciting for use on the massive unlabeled datasets used to train large foundation models (e.g. ). If highly pruned versions of these datasets can be used to train a large number of different models, one can conceive of such carefully chosen data subsets as foundation datasets in which the initial computational cost of data pruning can be amortized across efficiency gains in training many downstream models, just at the initial computational cost of training foundation models is amortized across the efficiency gains of fine-tuning across many downstream tasks. Together, our results demonstrate the promise and potential of data pruning for large-scale training and pretraining. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Problem setup", "text": "In this section we introduce a theory for data pruning in the teacher-student perceptron setting, using the tools of statistical mechanics. We study the problem of classifying a dataset of P examples {x \u00b5 , y \u00b5 } \u00b5=1,...,P , where x \u00b5 \u223c N (0, I N ) are i.i.d. zero mean unit variance random Gaussian inputs, and y \u00b5 = sign(T \u2022 x) are labels generated by a teacher perceptron T \u2208 R N , which we will assume is randomly drawn from a uniform distribution on the sphere T \u223c Unif(S N \u22121 ( \u221a N )). We work in the high-dimensional statistics limit where N, P \u2192 \u221e but the ratio \u03b1 tot = P/N remains O(1). The generalization error of a perceptron trained on such an isotropic dataset is a classical problem (see e.g. [26]). However, we are interested in the setting where the training dataset is not isotropic, but instead has inherited some structure due to data pruning.\nIn particular, consider pruning the training dataset by keeping only the examples with the smallest margin |z \u00b5 | = |J probe \u2022 x \u00b5 | along a probe student J probe . The pruned dataset will follow some distribution p(z) along the direction of J probe , and remain isotropic in the nullspace of J probe . In what follows we will derive a general theory for an arbitrary data distribution p(z), and specialize to the case of small-margin pruning only at the very end (in which case p(z) will take the form of a truncated Gaussian). We will also make no assumptions on the form of the probe student J probe or the learning rule used to train it; only that J probe has developed some overlap with the teacher, quantified by the angle \u03b8 = cos \u22121 J probe \u2022T J probe 2 T 2 (Fig. 2A). After the dataset has been pruned, we consider training a new student J from scratch on the pruned dataset. A typical training algorithm (used in support vector machines and the solution to which SGD converges on separable data) is to find the solution J which classifies the training data with the maximal margin \u03ba = min \u00b5 J \u2022 (y \u00b5 x \u00b5 ). Our goal is to compute the generalization error \u03b5 g of this student, which is simply governed by the overlap between the student and the teacher, \u03b5 g = cos \u22121 (R)/\u03c0, where R = J \u2022 T/ J 2 T 2 .", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "A.2 Main result and overview", "text": "Our main result is a set of self-consistent equations which can be solved to obtain the generalization error \u03b5(\u03b1, p, \u03b8) for any \u03b1 and any data distribution p(z) along a probe student at any angle \u03b8 relative to the teacher. These equations take the form,\nR \u2212 \u03c1 cos \u03b8 sin 2 \u03b8 = \u03b1 \u03c0\u039b \u03ba \u2212\u221e dt exp \u2212 \u2206(t, z) 2\u039b 2 (\u03ba \u2212 t) z (1) 1 \u2212 \u03c1 2 + R 2 \u2212 2\u03c1R cos \u03b8 sin 2 \u03b8 = 2\u03b1 \u03ba \u2212\u221e dt e \u2212 (t\u2212\u03c1z) 2 2(1\u2212\u03c1 2 ) \u221a 2\u03c0 1 \u2212 \u03c1 2 H \u0393(t, z) 1 \u2212 \u03c1 2 \u039b (\u03ba \u2212 t) 2 z (2) \u03c1 \u2212 R cos \u03b8 sin 2 \u03b8 = 2\u03b1 \u03ba \u2212\u221e dt e \u2212 (t\u2212\u03c1z) 2 2(1\u2212\u03c1 2 ) \u221a 2\u03c0 1 \u2212 \u03c1 2 H \u0393(t, z) 1 \u2212 \u03c1 2 \u039b z \u2212 \u03c1t 1 \u2212 \u03c1 2 (\u03ba \u2212 t) + 1 2\u03c0\u039b exp \u2212 \u2206(t, z) 2\u039b 2 \u03c1R \u2212 cos \u03b8 1 \u2212 \u03c1 2 (\u03ba \u2212 t) z(3)\nWhere,\n\u039b = sin 2 \u03b8 \u2212 R 2 \u2212 \u03c1 2 + 2\u03c1R cos \u03b8,(4)\n\u0393(t, z) = z(\u03c1R \u2212 cos \u03b8) \u2212 t(R \u2212 \u03c1 cos \u03b8),(5)\n\u2206(t, z) = z 2 \u03c1 2 + cos 2 \u03b8 \u2212 2\u03c1R cos \u03b8 + 2tz(R cos \u03b8 \u2212 \u03c1) + t 2 sin 2 \u03b8.(6)\nWhere \u2022 z represents an average over the pruned data distribution p(z) along the probe student. For any \u03b1, p(z), \u03b8, these equations can be solved for the order parameters R, \u03c1, \u03ba, from which the generalization error can be easily read off as \u03b5 g = cos \u22121 (R)/\u03c0. This calculation results in the solid theory curves in Figs 1,2,3, which show an excellent match to numerical simulations. In the following section we will walk through the derivation of these equations using replica theory. In Section A.6 we will derive an expression for the information gained per training example, and show that with Pareto optimal data pruning this information gain can be made to converge to a finite rate, resulting in at least exponential decay in test error. In Section A.7, we will show that super-exponential scaling eventually breaks down when the probe student does not match the teacher perfectly, resulting in power law scaling at at a minimum pruning fraction f min (\u03b8).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.3 Replica calculation of the generalization error", "text": "To obtain Eqs. 1,2,3, we follow the approach of Elizabeth Gardner and compute the volume \u2126(x \u00b5 , T, \u03ba) of solutions J which perfectly classify the training data up to a margin \u03ba (known as the Gardner volume) [30,26]. As \u03ba grows, the volume of solutions shrinks until it reaches a unique solution at a critical \u03ba, the max-margin solution. The Gardner volume \u2126 takes the form,\n\u2126(x \u00b5 , T, \u03ba) = d\u00b5(J) \u00b5 \u0398 T \u2022 x \u00b5 \u221a N J \u2022 x \u00b5 \u221a N \u2212 \u03ba (7)\nBecause the student's decision boundary is invariant to an overall scaling of J, we enforce normalization of J via the measure d\u00b5(J),\nd\u00b5(J) = 1 (2\u03c0e) N/2 \u03b4( J 2 \u2212 N )(8)\nIn the thermodynamic limit N, P \u2192 \u221e the typical value of the entropy S(\u03ba) = log \u2126(x \u00b5 , T, \u03ba) is dominated by particular values of R, \u03ba, where the double angle brackets \u2022 denote a quenched average over disorder introduced by random realizations of the training examples x \u00b5 and the teacher T. However, computing this quenched average is intractable since the integral over J cannot be performed analytically for every individual realization of the examples. We rely on the replica trick from statistical physics,\nln(x) = lim n\u21920 x n \u2212 1 n (9)\nWhich allows us to evaluate S(\u03ba) in terms of easier-to-compute powers of \u2126,\nS(\u03ba) = ln \u2126(x \u00b5 , T, \u03ba) = \u2126 n (x \u00b5 , T, \u03ba) \u2212 1 n (10)\nThis reduces our problem to computing powers of \u2126, which for integer n can be written in terms of \u03b1 = 1, . . . , n replicated copies of the original system,\n\u2126 (n) \u2261 \u2126 n (x \u00b5 , T, \u03ba) = n \u03b1=1 d\u00b5(J \u03b1 ) \u03b1,\u00b5 \u0398 T \u2022 x \u00b5 \u221a N J \u2022 x \u00b5 \u221a N \u2212 \u03ba (11)\nWe begin by introducing auxiliary variables,\n\u03bb \u03b1 \u00b5 = J \u03b1 \u2022 x \u00b5 \u221a N , u \u00b5 = T \u2022 x \u00b5 \u221a N (12\n)\nby \u03b4\u2212functions, to pull the dependence on J and T outside of the Heaviside function,\n\u2126 (n) = n \u03b1=1 d\u00b5(J \u03b1 ) \u03b1,\u00b5 d\u03bb \u03b1 \u00b5 \u00b5 du \u00b5 \u03b1,\u00b5 \u0398 u \u00b5 (\u03bb \u03b1 \u00b5 \u2212 \u03ba) \u00d7 \u03b4 \u03bb \u03b1 \u00b5 \u2212 1 \u221a N J \u03b1 \u2022 x \u00b5 \u03b4 u \u00b5 \u2212 1 \u221a N T \u2022 x \u00b5 (13)\nUsing the integral representation of the \u03b4-functions,\n\u2126 (n) = n \u03b1=1 d\u00b5(J \u03b1 ) \u03b1,\u00b5 d\u03bb \u03b1 \u00b5 d\u03bb \u03b1 \u00b5 2\u03c0 \u00b5 du \u00b5 d\u00fb \u00b5 2\u03c0 (14) \u00d7 \u03b1,\u00b5 \u0398 u \u00b5 (\u03bb \u03b1 \u00b5 \u2212 \u03ba) exp i \u00b5,\u03b1 \u03bb \u03b1 \u00b5\u03bb \u03b1 \u00b5 + i \u00b5 u \u00b5\u00fb\u00b5 (15) \u00d7 exp \u2212 i \u221a N \u00b5,\u03b1\u03bb \u03b1 \u00b5 J \u03b1 \u2022 x \u00b5 \u2212 i \u221a N \u00b5\u00fb \u00b5 T \u2022 x \u00b5 (16)\nThe data obeys some distribution p(z) along the direction of J probe and is isotropic in the nullspace of J probe . Hence we can decompose a training example x \u00b5 as follows, x \u00b5 = J probe z \u00b5 +(I \u2212J probe J T probe )s \u00b5 , where z \u00b5 \u223c p(z) and s \u00b5 \u223c N (0, I N ),\n\u2126 (n) = n \u03b1=1 d\u00b5(J \u03b1 ) \u03b1,\u00b5 d\u03bb \u03b1 \u00b5 d\u03bb \u03b1 \u00b5 2\u03c0 \u00b5 du \u00b5 d\u00fb \u00b5 2\u03c0 (17) \u00d7 \u03b1,\u00b5 \u0398 u \u00b5 (\u03bb \u03b1 \u00b5 \u2212 \u03ba) exp i \u00b5,\u03b1 \u03bb \u03b1 \u00b5\u03bb \u03b1 \u00b5 + i \u00b5 u \u00b5\u00fb\u00b5 (18) \u00d7 exp \u2212 i \u221a N \u00b5,\u03b1\u03bb \u03b1 \u00b5 (J \u03b1 \u2022 J probe z \u00b5 + J \u03b1 \u22a5 \u2022 s \u00b5 ) \u2212 i \u221a N \u00b5\u00fb \u00b5 (T \u2022 J probe z \u00b5 + T \u22a5 \u2022 s \u00b5 (19\n)\nWhere J \u22a5 = (1 \u2212 J probe J T probe )J and T \u22a5 = (1 \u2212 J probe J T probe )T. Now we can average over the patterns\ns \u00b5 \u223c N (0, I N ), exp \u2212 i \u221a N \u00b5,\u03b1\u03bb \u03b1 \u00b5 J \u03b1 \u22a5 \u2022 s \u00b5 \u2212 i \u221a N \u00b5\u00fb \u00b5 T \u22a5 \u2022 s \u00b5 s \u00b5 = exp \u2212 1 2N \u00b5,\u03b1\u03bb \u03b1 \u00b5 J \u03b1 \u22a5 +\u00fb \u00b5 T \u22a5 2 (20) = exp \u2212 1 2N \u00b5 \u03b1\u03b2\u03bb \u03b1 \u00b5\u03bb \u03b2 \u00b5 J \u03b1 \u22a5 \u2022 J \u03b2 \u22a5 + 2 \u03b1\u03bb \u03b1 \u00b5\u00fb\u00b5 J \u03b1 \u22a5 \u2022 T \u22a5 +\u00fb 2 \u00b5 T \u22a5 2 . (21\n)\nInserting this back into our expression for the Gardner volume,\n\u2126 (n) = n \u03b1=1 d\u00b5(J \u03b1 ) \u03b1,\u00b5 d\u03bb \u03b1 \u00b5 d\u03bb \u03b1 \u00b5 2\u03c0 \u00b5 du \u00b5 d\u00fb \u00b5 2\u03c0 \u00d7 \u03b1,\u00b5 \u0398 u \u00b5 (\u03bb \u03b1 \u00b5 \u2212 \u03ba) exp i \u00b5,\u03b1 \u03bb \u03b1 \u00b5\u03bb \u03b1 \u00b5 + i \u00b5 u \u00b5\u00fb\u00b5 \u00d7 exp \u2212 1 2N \u00b5 \u03b1\u03b2\u03bb \u03b1 \u00b5\u03bb \u03b2 \u00b5 J \u03b1 \u22a5 \u2022 J \u03b2 \u22a5 + 2 \u03b1\u03bb \u03b1 \u00b5\u00fb\u00b5 J \u03b1 \u22a5 \u2022 T \u22a5 +\u00fb 2 \u00b5 T \u22a5 2 \u2212 i \u00b5 \u03b1\u03bb \u03b1 \u00b5 J \u03b1 \u2022 J probe +\u00fb \u00b5 T \u2022 J probe z \u00b5 T,z \u00b5(22)\nAs is typical in replica calculations of this type, we now introduce order parameters,\nq \u03b1\u03b2 = J \u03b1 \u2022 J \u03b2 N , R \u03b1 = T \u2022 J \u03b1 N (23\n)\nwhich will allow us to decouple the Jfrom the \u03bb-\u00b5-zintegrals. q \u03b1\u03b2 represents the overlaps between replicated students, and R \u03b1 the overlap between each replicated student and the teacher. However, because our problem involves the additional role of the probe student, we must introduce an additional order parameter,\n\u03c1 \u03b1 = J \u03b1 \u2022 J probe N (24\n)\nwhich represents the overlap between each replicated student and the probe student. Notice that,\nJ \u03b1 \u22a5 \u2022 J \u03b2 \u22a5 = J \u03b1 \u2022 J \u03b2 \u2212 J \u03b1 \u2022 J \u03b2 = N (q \u03b1\u03b2 \u2212 \u03c1 \u03b1 \u03c1 \u03b2 ) (25) J \u03b1 \u22a5 \u2022 T \u22a5 = J \u03b1 \u2022 T \u2212 J \u03b1 \u2022 T = N (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8)(26)\nWith this new set of order parameters in hand, we can decouple the J from the \u03bb \u2212 u \u2212 z\u2212integrals.\n\u2126 (n) = \u03b1<\u03b2 dq \u03b1\u03b2 \u03b1 dR \u03b1 \u03b1 d\u03c1 \u03b1 \u00d7 n \u03b1=1 d\u00b5(J \u03b1 ) \u03b1 \u03b4(T \u2022 J \u03b1 \u2212 N R \u03b1 ) T \u03b1<\u03b2 \u03b4(J \u03b1 \u2022 J \u03b2 \u2212 N q \u03b1\u03b2 ) \u03b1 \u03b4(J \u03b1 \u2022 J probe \u2212 N \u03c1 \u03b1 ) \u00d7 \u03b1,\u00b5 d\u03bb \u03b1 \u00b5 d\u03bb \u03b1 \u00b5 2\u03c0 \u00b5 du \u00b5 d\u00fb \u00b5 2\u03c0 \u03b1,\u00b5 \u0398 u \u00b5 (\u03bb \u03b1 \u00b5 \u2212 \u03ba) \u00b5 exp i \u03b1 \u03bb \u03b1 \u00b5\u03bb \u03b1 \u00b5 + iu \u00b5\u00fb\u00b5 \u00d7 exp \u2212 1 2 \u03b1\u03b2\u03bb \u03b1 \u00b5\u03bb \u03b2 \u00b5 (q \u03b1\u03b2 \u2212 \u03c1 \u03b1 \u03c1 \u03b2 ) \u2212 \u03b1\u03bb \u03b1 \u00b5\u00fb\u00b5 (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8) \u2212 1 2\u00fb 2 \u00b5 sin 2 \u03b8 \u2212 i \u03b1\u03bb \u03b1 \u00b5 \u03c1 \u03b1 +\u00fb \u00b5 cos \u03b8 z \u00b5 z \u00b5 (27)\nWe can now perform the gaussian integral over\u00fb \u00b5 ,\n\u2126 (n) = \u03b1<\u03b2 dq \u03b1\u03b2 \u03b1 dR \u03b1 \u03b1 d\u03c1 \u03b1 \u00d7 n \u03b1=1 d\u00b5(J \u03b1 ) \u03b1 \u03b4(T \u2022 J \u03b1 \u2212 N R \u03b1 ) T \u03b1<\u03b2 \u03b4(J \u03b1 \u2022 J \u03b2 \u2212 N q \u03b1\u03b2 ) \u03b1 \u03b4(J \u03b1 \u2022 J probe \u2212 N \u03c1 \u03b1 ) \u00d7 \u03b1,\u00b5 d\u03bb \u03b1 \u00b5 d\u03bb \u03b1 \u00b5 2\u03c0 \u00b5 du \u00b5 d\u00fb \u00b5 2\u03c0 \u03b1,\u00b5 \u0398 u \u00b5 (\u03bb \u03b1 \u00b5 \u2212 \u03ba) \u00b5 exp i \u03b1 \u03bb \u03b1 \u00b5\u03bb \u03b1 \u00b5 \u00d7 exp \u2212 1 2 \u03b1\u03b2\u03bb \u03b1 \u00b5\u03bb \u03b2 \u00b5 (q \u03b1\u03b2 \u2212 \u03c1 \u03b1 \u03c1 \u03b2 ) \u2212 i \u03b1\u03bb \u03b1 \u00b5 \u03c1 \u03b1 z \u00b5 + 1 2 sin 2 \u03b8 i(u \u00b5 \u2212 z \u00b5 cos \u03b8) \u2212 \u03b1\u03bb \u03b1 \u00b5 (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8) 2 z \u00b5 (28)\nExpanding,\n\u2126 (n) = \u03b1<\u03b2 dq \u03b1\u03b2 \u03b1 dR \u03b1 \u03b1 d\u03c1 \u03b1 \u00d7 n \u03b1=1 d\u00b5(J \u03b1 ) \u03b1 \u03b4(T \u2022 J \u03b1 \u2212 N R \u03b1 ) T \u03b1<\u03b2 \u03b4(J \u03b1 \u2022 J \u03b2 \u2212 N q \u03b1\u03b2 ) \u03b1 \u03b4(J \u03b1 \u2022 J probe \u2212 N \u03c1 \u03b1 ) \u00d7 \u03b1,\u00b5 d\u03bb \u03b1 \u00b5 d\u03bb \u03b1 \u00b5 2\u03c0 \u00b5 du \u00b5 d\u00fb \u00b5 2\u03c0 \u03b1,\u00b5 \u0398 u \u00b5 (\u03bb \u03b1 \u00b5 \u2212 \u03ba) \u00b5 exp i \u03b1 \u03bb \u03b1 \u00b5\u03bb \u03b1 \u00b5 \u2212 1 2 (u \u00b5 \u2212 z \u00b5 cos \u03b8) 2 sin 2 \u03b8 \u00d7 exp \u2212 1 2 \u03b1\u03b2\u03bb \u03b1 \u00b5\u03bb \u03b2 \u00b5 (q \u03b1\u03b2 \u2212 \u03c1 \u03b1 \u03c1 \u03b2 ) \u2212 i \u03b1\u03bb \u03b1 \u00b5 \u03c1 \u03b1 z \u00b5 \u2212 i sin 2 \u03b8 u \u00b5 \u2212 z \u00b5 cos \u03b8) \u03b1\u03bb \u03b1 \u00b5 (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8) + 1 2 sin 2 \u03b8 \u03b1\u03b2\u03bb \u03b1 \u00b5\u03bb \u03b2 \u00b5 (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8)(R \u03b2 \u2212 \u03c1 \u03b2 cos \u03b8) z \u00b5 (29) Simplifying, \u2126 (n) = \u03b1<\u03b2 dq \u03b1\u03b2 \u03b1 dR \u03b1 \u03b1 d\u03c1 \u03b1 \u00d7 n \u03b1=1 d\u00b5(J \u03b1 ) \u03b1 \u03b4(T \u2022 J \u03b1 \u2212 N R \u03b1 ) T \u03b1<\u03b2 \u03b4(J \u03b1 \u2022 J \u03b2 \u2212 N q \u03b1\u03b2 ) \u03b1 \u03b4(J \u03b1 \u2022 J probe \u2212 N \u03c1 \u03b1 ) \u00d7 \u03b1,\u00b5 d\u03bb \u03b1 \u00b5 d\u03bb \u03b1 \u00b5 2\u03c0 \u00b5 du \u00b5 d\u00fb \u00b5 2\u03c0 \u03b1,\u00b5 \u0398 u \u00b5 (\u03bb \u03b1 \u00b5 \u2212 \u03ba) exp i \u00b5,\u03b1 \u03bb \u03b1 \u00b5\u03bb \u03b1 \u00b5 \u2212 1 2 (u \u00b5 \u2212 z \u00b5 cos \u03b8) 2 sin 2 \u03b8 \u00d7 exp \u2212 1 2 \u00b5 \u03b1\u03b2\u03bb \u03b1 \u00b5\u03bb \u03b2 \u00b5 q \u03b1\u03b2 \u2212 \u03c1 \u03b1 \u03c1 \u03b2 \u2212 (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8)(R \u03b2 \u2212 \u03c1 \u03b2 cos \u03b8) sin 2 \u03b8 \u2212 i \u00b5 \u03b1\u03bb \u03b1 \u00b5 \u03c1 \u03b1 z \u00b5 \u2212 i sin 2 \u03b8 u \u00b5 \u2212 z \u00b5 cos \u03b8) \u03b1\u03bb \u03b1 \u00b5 (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8) z \u00b5(30)\nNow we introduce integral representations for the remaining delta functions, including the measure d\u00b5(J \u03b1 ), for which we introduce the parameterk \u03b1 ,\n\u2126 (n) = \u03b1 dk \u03b1 4\u03c0 \u03b1<\u03b2 dq \u03b1\u03b2 dq \u03b1\u03b2 2\u03c0/N \u03b1 dR \u03b1 dR \u03b1 2\u03c0/N \u03b1 d\u03c1 \u03b1 d\u03c1 \u03b1 2\u03c0/N \u00d7 exp i N 2 \u03b1k \u03b1 + iN \u03b1<\u03b2 q \u03b1\u03b2q\u03b1\u03b2 + iN \u03b1 R \u03b1R\u03b1 + iN \u03b1 \u03c1 \u03b1\u03c1\u03b1 \u00d7 i,\u03b1 dJ \u03b1 i \u221a 2\u03c0e exp \u2212 i 2 \u03b1k \u03b1 J \u03b1 2 \u2212 i \u03b1<\u03b2q \u03b1\u03b2 J \u03b1 \u2022 J \u03b2 \u2212 i \u03b1R \u03b1 J \u03b1 \u2022 T \u2212 i \u03b1\u03c1 \u03b1 J \u03b1 \u2022 J probe \u00d7 \u03b1,\u00b5 d\u03bb \u03b1 \u00b5 d\u03bb \u03b1 \u00b5 2\u03c0 \u00b5 du \u00b5 d\u00fb \u00b5 2\u03c0 \u03b1,\u00b5 \u0398 u \u00b5 (\u03bb \u03b1 \u00b5 \u2212 \u03ba) exp i \u00b5,\u03b1 \u03bb \u03b1 \u00b5\u03bb \u03b1 \u00b5 \u2212 1 2 (u \u00b5 \u2212 z \u00b5 cos \u03b8) 2 sin 2 \u03b8 \u00d7 exp \u2212 1 2 \u00b5 \u03b1\u03b2\u03bb \u03b1 \u00b5\u03bb \u03b2 \u00b5 q \u03b1\u03b2 \u2212 \u03c1 \u03b1 \u03c1 \u03b2 \u2212 (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8)(R \u03b2 \u2212 \u03c1 \u03b2 cos \u03b8) sin 2 \u03b8 \u2212 i \u00b5 \u03b1\u03bb \u03b1 \u00b5 \u03c1 \u03b1 z \u00b5 \u2212 i sin 2 \u03b8 u \u00b5 \u2212 z \u00b5 cos \u03b8) \u03b1\u03bb \u03b1 \u00b5 (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8) z \u00b5 (31)\nNotice that the u \u00b5 \u2212 \u03bb \u03b1 \u00b5 \u2212\u03bb \u03b1 \u00b5 \u2212 z \u00b5 -integrals factorize in \u00b5, and can be written as a single integral to the power of P = \u03b1N .\n\u2126 (n) = k \u03b1 dk \u03b1 4\u03c0 \u03b1<\u03b2 dq \u03b1\u03b2 dq \u03b1\u03b2 2\u03c0/N \u03b1 dR \u03b1 dR \u03b1 2\u03c0/N \u03b1 d\u03c1 \u03b1 d\u03c1 \u03b1 2\u03c0/N \u00d7 exp N i 2 \u03b1k \u03b1 + i \u03b1<\u03b2 q \u03b1\u03b2q\u03b1\u03b2 + i \u03b1 R \u03b1R\u03b1 + i \u03b1 \u03c1 \u03b1\u03c1\u03b1 + G S (k \u03b1 ,q \u03b1\u03b2 ,R \u03b1 ,\u03c1 \u03b1 ) + \u03b1G E (q \u03b1\u03b2 , R \u03b1 , \u03c1 \u03b1 )(32)\nWhere we have written the Gardner volume in terms of an entropic part G S , which measures how many spherical couplings satisfy the constraints,\nG S = 1 N log \u03b1 dJ \u03b1 \u221a 2\u03c0e exp \u2212 i 2 \u03b1k \u03b1 J \u03b1 2 \u2212i \u03b1<\u03b2q \u03b1\u03b2 J \u03b1 \u2022J \u03b2 \u2212i \u03b1R \u03b1 J \u03b1 \u2022T\u2212i \u03b1\u03c1 \u03b1 J \u03b1 \u2022J probe (33) And an energetic part G E , G E = log du \u221a 2\u03c0 \u03b1 d\u03bb \u03b1 d\u03bb \u03b1 2\u03c0 \u03b1 \u0398 u(\u03bb \u03b1 \u2212 \u03ba) exp i \u03b1 \u03bb \u03b1\u03bb\u03b1 \u2212 1 2 (u \u00b5 \u2212 z \u00b5 cos \u03b8) 2 sin 2 \u03b8 \u00d7 exp \u2212 1 2 \u03b1\u03b2\u03bb \u03b1\u03bb\u03b2 q \u03b1\u03b2 \u2212 \u03c1 \u03b1 \u03c1 \u03b2 \u2212 (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8)(R \u03b2 \u2212 \u03c1 \u03b2 cos \u03b8) sin 2 \u03b8 \u2212 i \u03b1\u03bb \u03b1 \u03c1 \u03b1 z \u2212 i sin 2 \u03b8 u \u2212 z cos \u03b8) \u03b1\u03bb \u03b1 (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8) z (34\n)\nWe first evaluate the entropic part, G S , by introducing the n \u00d7 n matrices A, B,\nA \u03b1\u03b2 = ik \u03b1 \u03b4 \u03b1\u03b2 + iq \u03b1\u03b2 (1 \u2212 \u03b4 \u03b1\u03b2 )(35)\nB \u03b1\u03b2 = \u03b4 \u03b1\u03b2 + q \u03b1\u03b2 (1 \u2212 \u03b4 \u03b1\u03b2 )(36)\nInserting this our expression for G S becomes\nG S = 1 N log \u03b1 dJ \u03b1 \u221a 2\u03c0e exp \u2212 1 2 \u03b1,\u03b2 J \u03b1T A \u03b1\u03b2 J \u03b2 \u2212 i \u03b1 J \u03b1 \u2022 (TR \u03b1 + J probe\u03c1 \u03b1 ) (37) Integrating over J \u03b1 , G S = \u2212 n 2 \u2212 1 2 log(det A) \u2212 1 2N \u03b1,\u03b2 (TR \u03b1 + J probe\u03c1 \u03b1 ) T A \u22121 \u03b1\u03b2 (TR \u03b2 + J probe\u03c1 \u03b2 )(38)\nNow we can include the remaining terms in the expression for \u2126 (n) outside of G E and G S by noting that tr(AB)\n= \u03b1\u03b2 A \u03b1\u03b2 B \u03b2\u03b1 (39) = \u03b1\u03b2 (ik \u03b1 \u03b4 \u03b1\u03b2 + iq \u03b1\u03b2 (1 \u2212 \u03b4 \u03b1\u03b2 ))(\u03b4 \u03b1\u03b2 + q \u03b1\u03b2 (1 \u2212 \u03b4 \u03b1\u03b2 ))(40)\n= \u03b1 ik \u03b1 + 2 \u03b1<\u03b2 iq \u03b1\u03b2q\u03b1\u03b2 (41)\nAdditionally, we can use log det A = tr(log A). Thus all terms in the exponent except G E can be written as\n\u2212 n 2 \u2212 1 2 tr(log A)\u2212 1 2N \u03b1,\u03b2 (TR \u03b1 +J probe\u03c1 \u03b1 ) T A \u22121 \u03b1\u03b2 (TR \u03b2 +J probe\u03c1 \u03b2 )+ 1 2 tr(AB)+i \u03b1 R \u03b1R\u03b1 +i \u03b1 \u03c1 \u03b1\u03c1\u03b1 J probe (42)\nNow we extremize wrtR \u03b1 and the elements of A by setting the derivatives wrtR \u03b3 ,\u03c1 \u03b3 and A \u03b3\u03b4 equal to zero:\n0 = \u2212 \u03b1 A \u22121 \u03b1\u03b3 T \u2022 (TR \u03b1 + J probe\u03c1 \u03b1 ) + iR \u03b3 = \u2212 \u03b1 A \u22121 \u03b1\u03b3 (R \u03b1 +\u03c1 \u03b1 cos \u03b8) + iR \u03b3 (43) 0 = \u2212 \u03b1 A \u22121 \u03b1\u03b3 J probe \u2022 (TR \u03b1 + J probe\u03c1 \u03b1 ) + i\u03c1 \u03b3 = \u2212 \u03b1 A \u22121 \u03b1\u03b3 (R \u03b1 cos \u03b8 +\u03c1 \u03b1 ) + i\u03c1 \u03b3 (44) 0 = \u2212 1 2 A \u22121 \u03b3\u03b4 + 1 2 \u03b1,\u03b2 (TR \u03b1 + J probe\u03c1 \u03b1 ) T A \u22121 \u03b1\u03b3 A \u22121 \u03b2\u03b4 (TR \u03b2 + J probe\u03c1 \u03b2 ) + 1 2 B \u03b3\u03b4(45)\nSolving these givesR\n\u03b1 = i \u03b2 A \u03b1\u03b2 R \u03b2 \u2212 \u03c1 \u03b2 cos \u03b8 sin 2 \u03b8 (46\n)\n\u03c1 \u03b1 = i \u03b2 A \u03b1\u03b2 \u03c1 \u03b2 \u2212 R \u03b2 cos \u03b8 sin 2 \u03b8 (47\n)\nand\nA \u22121 \u03b3\u03b4 = B \u03b3\u03b4 \u2212 R \u03b3 R \u03b4 \u2212 R \u03b3 \u03c1 \u03b4 cos \u03b8 \u2212 R \u03b4 \u03c1 \u03b3 cos \u03b8 + \u03c1 \u03b3 \u03c1 \u03b4 sin 2 \u03b8 \u2261 C \u03b3\u03b4 (48\n)\nand now we are left with\n\u2126 (n) \u223c exp N extr q \u03b1\u03b2 ,R \u03b1 ,\u03c1 \u03b1 1 2 tr(log C) + \u03b1G E (q \u03b1\u03b2 , R \u03b1 )(49)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.3.1 Replica symmetry ansatz", "text": "In order to extremize wrt q \u03b1\u03b2 , R \u03b1 , \u03c1 \u03b1 , we take the replica symmetry ansatz [26],\nq \u03b1\u03b2 = q, R \u03b1 = R, \u03c1 \u03b1 = \u03c1 (50)\nThen C takes the form\nC \u03b1\u03b2 = \u03b4 \u03b1\u03b2 \u2212 R 2 \u2212 2R\u03c1 cos \u03b8 + \u03c1 2 sin 2 \u03b8 + q(1 \u2212 \u03b4 \u03b1\u03b2 )(51)\nA matrix with E on the diagonal and F elsewhere, C \u03b1\u03b2 = E\u03b4 \u03b1\u03b2 + F (1 \u2212 \u03b4 \u03b1\u03b2 ), has n \u2212 1 degenerate eigenvalues E \u2212 F and one eigenvalue E + (n \u2212 1)F . Hence in our case C has n \u2212 1 degenerate eigenvalues\n1 \u2212 R 2 \u2212 2R\u03c1 cos \u03b8 + \u03c1 2 sin 2 \u03b8 \u2212 q \u2212 R 2 \u2212 2R\u03c1 cos \u03b8 + \u03c1 2 sin 2 \u03b8 = 1 \u2212 q (52)\nand one other eigenvalue,\n1\u2212 R 2 \u2212 2R\u03c1 cos \u03b8 + \u03c1 2 sin 2 \u03b8 +(n\u22121) q\u2212 R 2 \u2212 2R\u03c1 cos \u03b8 + \u03c1 2 sin 2 \u03b8 = 1\u2212q+n q\u2212 R 2 \u2212 2R\u03c1 cos \u03b8 + \u03c1 2 sin 2 \u03b8 (53\n)\nTherefore, tr(log C) = (n \u2212 1) log 1 \u2212 q + log 1 \u2212 q + n q \u2212 R 2 \u2212 2R\u03c1 cos \u03b8 + \u03c1 2 sin 2 \u03b8 = n log 1 \u2212 q + log 1 + n q sin 2 \u03b8 \u2212 (R 2 \u2212 2R\u03c1 cos \u03b8 + \u03c1 2 ) (1 \u2212 q) sin 2 \u03b8 (54\n)\nWe next evaluate the energetic part, G E ,\nG E = log du \u221a 2\u03c0 \u03b1 d\u03bb \u03b1 d\u03bb \u03b1 2\u03c0 \u03b1 \u0398 u(\u03bb \u03b1 \u2212 \u03ba) exp i \u03b1 \u03bb \u03b1\u03bb\u03b1 \u2212 1 2 (u \u2212 z cos \u03b8) 2 sin 2 \u03b8 \u00d7 exp \u2212 1 2 \u03b1 (\u03bb \u03b1 ) 2 1 \u2212 \u03c1 2 \u2212 (R \u2212 \u03c1 cos \u03b8) 2 sin 2 \u03b8 \u2212 1 2 \u03b1 =\u03b2\u03bb \u03b1\u03bb\u03b2 q \u2212 \u03c1 2 \u2212 (R \u2212 \u03c1 cos \u03b8) 2 sin 2 \u03b8 \u2212 i \u03b1\u03bb \u03b1 \u03c1 \u03b1 z \u2212 i sin 2 \u03b8 u \u2212 z cos \u03b8) \u03b1\u03bb \u03b1 (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8) z (55\n)\nFirst note that we can rewrite the terms\n\u2212 1 2 \u03b1 1 \u2212 \u03c1 2 \u2212 (R \u2212 \u03c1 cos \u03b8) sin 2 \u03b8 (\u03bb \u03b1 ) 2 \u2212 1 2 \u03b1 =\u03b2\u03bb \u03b1\u03bb\u03b2 q \u2212 \u03c1 2 \u2212 (R \u2212 \u03c1 cos \u03b8) 2 sin 2 \u03b8 = \u2212 1 2 \u03b1 1 \u2212 \u03c1 2 \u2212 (R \u2212 \u03c1 cos \u03b8) 2 sin 2 \u03b8 (\u03bb \u03b1 ) 2 \u2212 1 2 q \u2212 \u03c1 2 \u2212 (R \u2212 \u03c1 cos \u03b8) 2 sin 2 \u03b8 ( \u03b1\u03bb \u03b1 ) 2 \u2212 \u03b1 (\u03bb \u03b1 ) 2 = \u2212 1 2 \u03b1 (1 \u2212 q)(\u03bb \u03b1 ) 2 \u2212 1 2 q \u2212 \u03c1 2 \u2212 (R \u2212 \u03c1 cos \u03b8) 2 sin 2 \u03b8 (\u03b1\u03bb \u03b1 ) 2\n(56)\nTo simplify the last term we apply the Hubbard-Stratonovich transformation, e b 2 /2 = Dte bt , introducing auxiliary field t,\n= log du \u221a 2\u03c0 \u03b1 d\u03bb \u03b1 d\u03bb \u03b1 2\u03c0 \u03b1 \u0398 u(\u03bb \u03b1 \u2212 \u03ba) Dt exp \u2212 1 \u2212 q 2 \u03b1 (\u03bb \u03b1 ) 2 +i \u03b1\u03bb \u03b1 \u03bb \u03b1 \u2212\u03c1 \u03b1 z\u2212 u \u2212 z cos \u03b8)(R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8) sin 2 \u03b8 \u2212 q \u2212 \u03c1 2 \u2212 (R \u2212 \u03c1 cos \u03b8) 2 sin 2 \u03b8 t \u2212 1 2 (u \u2212 z cos \u03b8) 2 sin 2 \u03b8 z(57)\nUsing the \u0398-function to restrict the bounds of integration,\n= log 2 \u221e 0 du \u221a 2\u03c0 \u221e \u03ba \u03b1 d\u03bb \u03b1 \u221a 2\u03c0 \u03b1 d\u03bb \u03b1 \u221a 2\u03c0 Dt exp \u2212 1 \u2212 q 2 \u03b1 (\u03bb \u03b1 ) 2 +i \u03b1\u03bb \u03b1 \u03bb \u03b1 \u2212\u03c1 \u03b1 z\u2212 u \u2212 z cos \u03b8)(R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8) sin 2 \u03b8 \u2212 q \u2212 \u03c1 2 \u2212 (R \u2212 \u03c1 cos \u03b8) 2 sin 2 \u03b8 t \u2212 1 2 (u \u2212 z cos \u03b8) 2 sin 2 \u03b8 z(58)\nNow we can perform the gaussian integrals over\u03bb \u03b1 ,\n= log 2 \u221e 0 du \u221a 2\u03c0 \u221e \u03ba \u03b1 d\u03bb \u03b1 \u221a 2\u03c0 Dt exp \u2212 1 2(1 \u2212 q) \u03bb \u03b1 \u2212 \u03c1 \u03b1 z \u2212 u \u2212 z cos \u03b8)(R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8) sin 2 \u03b8 \u2212 q \u2212 \u03c1 2 \u2212 (R \u2212 \u03c1 cos \u03b8) 2 sin 2 \u03b8 t 2 \u2212 1 2 (u \u2212 z cos \u03b8) 2 sin 2 \u03b8 z (59\n)\nAnd \u03bb \u03b1 , = log 2 \u221e 0 du \u221a 2\u03c0 Dt H n \u2212 1 \u221a 1 \u2212 q \u03ba\u2212\u03c1 \u03b1 z+ u \u2212 z cos \u03b8)(R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8) sin 2 \u03b8 + q \u2212 \u03c1 2 \u2212 (R \u2212 \u03c1 cos \u03b8) 2 sin 2 \u03b8 t 2 \u00d7 exp \u2212 1 2 (u \u2212 z cos \u03b8) 2 sin 2 \u03b8 z(60)\nShifting the integration variable t \u2192 (\u03c1 \u03b1 z + (u\u2212z cos \u03b8)(R \u03b1 \u2212\u03c1 \u03b1 cos \u03b8)\nsin 2 \u03b8 + q \u2212 \u03c1 2 \u2212 (R\u2212\u03c1 cos \u03b8) 2 sin 2 \u03b8 t)/ \u221a q,\nwe can finally perform the gaussian integral over u,\n= log 2 dt \u221a 2\u03c0 exp \u2212 ( \u221a qt \u2212 z\u03c1) 2 2(q \u2212 \u03c1) 2 q q \u2212 \u03c1 2 H n \u2212 q 1 \u2212 q t \u00d7 H 1 q \u2212 \u03c1 2 \u03ba \u2212 (qR 0 z + z\u03c1(R \u2212 2\u03c1 cos \u03b8) \u2212 \u221a qt(R \u2212 \u03c1 cos \u03b8)) q sin 2 \u03b8 + 2R\u03c1 cos \u03b8 \u2212 R 2 \u2212 \u03c1 z (61)\nWe can simplify this further by taking t \u2192 (\n\u221a qt \u2212 z\u03c1)/ q \u2212 \u03c1 2 , = log 2 Dt H n \u03ba \u2212 (z\u03c1 + q \u2212 \u03c1 2 )t \u221a 1 \u2212 q H t q \u2212 \u03c1 2 + \u03c1z (R \u2212 \u03c1 cos \u03b8) + qz cos \u03b8 \u2212 \u03c1Rz \u2212 (q \u2212 \u03c1 2 ) \u03c1 2 \u2212 q sin 2 \u03b8 + R 2 \u2212 2\u03c1R cos \u03b8 z (62)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.4 Quenched entropy", "text": "Putting everything together, and using the replica identity, log \u2126 =lim n\u21920 ( \u2126 n \u2212 1)/n, we obtain an expression for the quenched entropy of the teacher-student perceptron under data pruning:\n1 N log \u2126 = extr q,R,\u03c1 1 2 log 1 \u2212 q + 1 2 q \u2212 (R \u2212 2R\u03c1 cos \u03b8 + \u03c1 2 )/ sin 2 \u03b8 1 \u2212 q + 2\u03b1 Dt log H \u03ba \u2212 (z\u03c1 + q \u2212 \u03c1 2 )t \u221a 1 \u2212 q \u00d7 H t q \u2212 \u03c1 2 + \u03c1z (R \u2212 \u03c1 cos \u03b8) + z(q cos \u03b8 \u2212 \u03c1R) (q \u2212 \u03c1 2 ) R 2 + \u03c1 2 \u2212 q sin 2 \u03b8 \u2212 2\u03c1R cos \u03b8 z (63\n)\nWe will now unpack this equation and use it to make predictions in several specific settings.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.5 Perfect teacher-probe overlap", "text": "We will begin by considering the case where the probe student has learned to perfectly match the teacher, J probe = T , which we can obtain by the limit \u03b8 \u2192 0, \u03c1 \u2192 R. In this limit the second H-function in Eq. 63 becomes increasingly sharp, approaching a step function:\nH t q \u2212 \u03c1 2 + \u03c1z (R \u2212 \u03c1 cos \u03b8) + z(q cos \u03b8 \u2212 \u03c1R) (q \u2212 \u03c1 2 ) R 2 + \u03c1 2 \u2212 q sin 2 \u03b8 \u2212 2\u03c1R cos \u03b8 \u2192 \u0398(z)(64)\nHence we are left with,\n1 N ln \u2126(x \u00b5 , T, \u03ba) = extr q,R 1 2 log 1 \u2212 q + 1 2 q \u2212 R 2 1 \u2212 q + 2\u03b1 Dt dzp(z)\u0398(z) log H \u2212 q \u2212 R 2 t + Rz \u2212 \u03ba \u221a 1 \u2212 q(65)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.5.1 Saddle point equations", "text": "We can now obtain a set of self-consistent saddle point equations by setting set to zero the derivatives with respect to R and q of the right side of Eq. 65. As \u03ba approaches its critical value, the space of solutions shrinks to a unique solution, and hence the overlap between students q approaches one. In the limit q \u2192 1, after some partial integration, we find,\nR = 2\u03b1 \u03ba \u2212\u221e dt \u221a 2\u03c0 \u221a 1 \u2212 R 2 \u221e 0 dzp(z) exp \u2212 (t \u2212 Rz) 2 2(1 \u2212 R 2 ) z \u2212 Rt 1 \u2212 R 2 (\u03ba \u2212 t)(66)\n1 \u2212 R 2 = 2\u03b1 \u03ba \u2212\u221e dt \u221a 2\u03c0 \u221a 1 \u2212 R 2 \u221e 0 dzp(z) exp \u2212 (t \u2212 Rz) 2 2(1 \u2212 R 2 ) \u03ba \u2212 t 2 (67)\nThese saddle point equations can be solved numerically to find R and \u03ba as a function of \u03b1 for a student perceptron trained on a dataset with an arbitrary distribution along the teacher direction p(z).\nWe can specialize to the case of data pruning by setting p(z) to the distribution found after pruning an initially Gaussian-distributed dataset so that only a fraction f of those examples with the smallest margin along the teacher are kept, p(z)\n= e \u2212z 2 /2 \u221a 2\u03c0f \u0398(\u03b3 \u2212 |z|), where the threshold \u03b3 = H \u22121 1\u2212f 2 . R = 2\u03b1 f \u221a 2\u03c0 \u221a 1 \u2212 R 2 \u03ba \u2212\u221e Dt exp \u2212 R 2 t 2 2(1 \u2212 R 2 ) 1 \u2212 exp \u2212 \u03b3(\u03b3 \u2212 2Rt) 2(1 \u2212 R 2 ) (\u03ba \u2212 t) (68) 1 \u2212 R 2 = 2\u03b1 f \u03ba \u2212\u221e Dt H \u2212 Rt \u221a \u2212 R 2 \u2212 H \u2212 Rt \u2212 \u03b3 \u221a 1 \u2212 R 2 (\u03ba \u2212 t) 2 (69)\nSolving these saddle point equations numerically for R and \u03ba yields an excellent fit to numerical simulations, as can be seen in Fig. 1A. It is also easy to verify that in the limit of no data pruning (f \u2192 1, \u03b3 \u2192 \u221e) we recover the saddle point equations for the classical teacher-student perceptron (Eqs. 4.4 and 4.5 in [26]),\nR = 2\u03b1 \u221a 2\u03c0 \u221a 1 \u2212 R 2 Dt exp \u2212 R 2 t 2 2(1 \u2212 R 2 ) (\u03ba \u2212 t) (70\n)\n1 \u2212 R 2 = 2\u03b1 Dt H \u2212 Rt \u221a 1 \u2212 R 2 \u03ba \u2212 t 2(71)\nA.6 Information gain per example Why does data pruning allow for super-exponential performance with dataset size \u03b1? We can define the amount of information gained from each new example, I(\u03b1),as the fraction by which the space of solutions which perfectly classify the data is reduced when a new training example is added, I(\u03b1) = \u2126( P +1 N )/\u2126( P N ). Or, equivalently, the rate at which the entropy is reduced, I(\u03b1) = \u2212 d d\u03b1 S(\u03b1). Of coure, the volume of solutions shrinks to zero at the max-margin solution; so to study the volume of solutions which perfectly classify the data we simply set the margin to zero \u03ba = 0. In [32] the information gain for a perceptron in the classical teacher-student setting is shown to take the form,\nI(\u03b1) = \u22122 Dt H R 1 \u2212 R t log H R 1 \u2212 R t (72\n)\nWhich goes to zero in the limit of large \u03b1 as I(\u03b1) \u223c 1/\u03b1. Data pruning can increase the information gained per example by pruning away the uninformative examples. To show this, we generalize the calculation of the information gain to pruned datasets, using the expression for the entropy we obtained in the previous section (Eq. 65).\nS(\u03b1) = 1 N log \u2126 = extr q,R 1 2 log 1\u2212R + 1 2 R+2\u03b1 Dt \u221e 0 dz p(z) log H \u2212 \u221a Rt\u2212 R \u221a 1 \u2212 R z(73)\nHence the information gain I(\u03b1) = \u2212 d d\u03b1 S(\u03b1) is given by\nI(\u03b1) = 2\u03b1 Dt dzp(z)\u0398(z) log H \u2212 \u221a Rt \u2212 R \u221a 1 \u2212 R z (74)\nChanging variables to t \u2192 \u2212(\n\u221a Rt + R \u221a 1\u2212R z)/ \u221a q, I(\u03b1) = 2\u03b1 Dt \u221e 0 dzp(z) 1 \u221a 1 \u2212 R 1 \u221a 2\u03c0 exp \u2212 z 2 + 2 \u221a Rtz 2(1 \u2212 R) log H R 1 \u2212 R t (75)\nNow, assuming that we prune to a fraction f , so that p(z\n) = \u0398 |z| \u2212 \u03b3 exp(\u2212z 2 /2) \u221a 2\u03c0f\n, where \u03b3 =\nH \u22121 1\u2212f 2 I(\u03b1) = 2\u03b1 f Dt H R 1 \u2212 R t \u2212 H \u03b3 + \u221a Rt \u221a 1 \u2212 R log H R 1 \u2212 R t(76)\nI(\u03b1) is plotted for varying values of f in Fig. 1F. Notice that for f \u2192 1, \u03b3 \u2192 \u221e and we recover Eq. 72. To obtain the optimal pruning fraction f opt for any \u03b1, we first need an equation for R, which can be obtained by taking the saddle point of Eq. 73. Next we optimize I(\u03b1) by setting the derivative of Eq. 76 with respect to f equal to zero. This gives us a pair of equations which can be solved numerically to obtain f opt for any \u03b1.\nFinally, Eq. 76 reveals that as we prune more aggressively the information gain per example approaches a finite rate. As f \u2192 0, \u03b3 \u2192 0, and we obtain,\nI(\u03b1) = \u2212 Dt log H( \u221a Rt)(77)\nWhich allows us to produce to trace the Pareto frontier in Fig. 1F. For R \u2192 1, Eq. 77 gives the asymptotic information gain I(\u221e) = 1 nat/example.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.7 Imperfect teacher-probe overlap", "text": "In realistic settings we expect the probe student to have only partial information about the target function. What happens if the probe student does not perfectly match the teacher? To understand this carefully, we need to compute the full set of saddle point equations over R, q, and \u03c1, which we will do in the following section. But to first get an idea for what goes wrong, we include in this section a simple sketch which reveals the limiting behavior.\nConsider the case where the angle between the probe student and teacher is \u03b8. Rotate coordinates so that the first canonical basis vector aligns with the student J = (1, 0, . . . , 0), and the teacher lies in the span of the first two canonical basis vectors, T = (cos \u03b8, sin \u03b8, 0, . . . , 0). Consider the margin along the teacher of a new training example x drawn from the pruned distribution.\nE|T \u2022 x| 2 = E[x 2 0 cos 2 \u03b8 + x 2 1 sin 2 \u03b8](78)\nAs the fraction of examples kept goes to zero, Ex 2 0 \u2192 0, and the average margin of a new example converges to a fixed value,\nE|T \u2022 x| 2 = sin 2 \u03b8 (79\n)\nHence the data ultimately stops concentrating around the teacher's decision boundary, and the information gained from each new example goes to zero. Therefore we expect the generalization error to converge to a power law, where the constant prefactor is roughly that of pruning with a prune fraction f min which yields an average margin of 1 \u2212 R 2 . This \"minimum\" pruning fraction lower bounds the generalization error envelope (see Fig. 2), and satisfies the following equation,\n\u03b3min \u2212\u03b3min dxp(x)x 2 = 1 2 \u2212 e \u2212\u03b3 2 min /2 \u03b3 min \u221a 2\u03c0(1 \u2212 2H(\u03b3 min )) = 1 \u2212 R 2 (80\n)\nwhere\n\u03b3 min = H \u22121 1\u2212fmin 2 .\nEq. 80 can be solved numerically, and we use it to produce the lower-bounding power laws shown in red in Fig. 2C,D. The minimum achievable pruning fraction f min (\u03b8) approaches zero as the angle between the probe student and the teacher shrinks, and we can obtain its scaling by taking R \u2192 1, in which case we find,\nf min (\u03b8) \u223c \u03b8 (81)", "publication_ref": [], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "A.8 Optimal pruning policy", "text": "The saddle point equations Eq. 66,67 reveal that the optimal pruning policy varies as a function of \u03b1 prune . For \u03b1 prune large the best policy is to retain only the \"hardest\" (smallest-margin) examples. But when \u03b1 prune is small, keeping the \"hardest\" examples performs worse than chance, suggesting that the best policy in the \u03b1 prune small regime is to keep the easiest examples. Indeed by switching between the \"keep easy\" and \"keep hard\" strategies as \u03b1 prune grows, one can achieve a lower Pareto frontier than the one shown in Fig. 1A in the small \u03b1 prune regime (Fig. 7C).", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Optimal data distribution", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A", "text": "Optimal pruning window", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Keep easy", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Optimal policy", "text": "Pareto frontier", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B C", "text": "Test error (%) In this section we investigate this question. Using the calculus of variations, we first derive the optimal data distribution p(z|\u03b1 prune , f ) along the teacher for a given \u03b1 prune , f . We begin by framing the problem using the method of Lagrange multipliers. Seeking to optimize R under the constraints imposed by the saddle point equations Eqs. 66,67, we define the Lagrangian,\nL = R + \u00b5 R \u2212 2\u03b1 \u221e 0 dz p(z)\u03d5(z; R, k) + \u03bb 1 \u2212 R 2 \u2212 2\u03b1 \u221e 0 dz p(z)\u03c8(z; R, k) . (82\n)\nWhere,\n\u03d5(z; R, k) = \u03ba \u2212\u221e dt \u221a 2\u03c0 \u221a 1 \u2212 R 2 exp \u2212 (t \u2212 Rz) 2 2(1 \u2212 R 2 ) z \u2212 Rt 1 \u2212 R 2 (\u03ba \u2212 t),(83)\nand,\n\u03c8(z; R, k) = \u03ba \u2212\u221e dt \u221a 2\u03c0 \u221a 1 \u2212 R 2 exp \u2212 (t \u2212 Rz) 2 2(1 \u2212 R 2 ) \u03ba \u2212 t 2 (84)\nTaking a variational derivative \u03b4L \u03b4p with respect to the data distribution p, we obtain an equation for z, indicating that the optimal distribution is a delta function at z = z * . To find the optimal location of the delta function z * , we take derivatives with respect to the remaining variables R, k, \u00b5, \u03bb and solve the resulting set of equations numerically. The qualitative behavior is shown in Fig. 7A. As \u03b1 prune grows, the location of the delta function shifts from infinity to zero, confirming that the optimal strategy for small \u03b1 prune is to keep the \"easy\" (large-margin) examples, and for large \u03b1 prune to keep the \"hard\" (small-margin) examples. Interestingly, this calculation also reveals that if the location of the delta function is chosen optimally, the student can perfectly recover the teacher (R = 1, zero generalization error) for any \u03b1 prune . This observation, while interesting, is of no practical consequence because it relies on an infinitely large training set from which examples can be precisely selected to perfectly recover the teacher. Therefore, to derive the optimal pruning policy for a more realistic scenario, we assume a gaussian distribution of data along the teacher direction and model pruning as keeping only those examples which fall inside a window a < z < b. The saddle point equations, Eqs. 66,67, then take the form,\nR = 2\u03b1 f \u03c0/2 \u221a 1 \u2212 R 2 \u03ba \u2212\u221e Dt exp \u2212 (a \u2212 Rt) 2 2(1 \u2212 R 2 ) \u2212 exp \u2212 (b \u2212 Rt) 2 2(1 \u2212 R 2 ) (\u03ba \u2212 t) (85) 1 \u2212 R 2 = 4\u03b1 f \u03ba \u2212\u221e Dt H a \u2212 Rt \u221a 1 \u2212 R 2 \u2212 H b \u2212 Rt \u221a 1 \u2212 R 2 (\u03ba \u2212 t) 2 (86)\nWhere a must satisfy a = H \u22121 (f /2 + H(b)). For each f, \u03b1, we find the optimal location of this window using the method of Lagrange multipliers. Defining the Lagrangian as before,\nL = R + \u00b5 R \u2212 2\u03b1 \u221e 0 dz p(z)\u03d5(z; R, k) + \u03bb 1 \u2212 R 2 \u2212 2\u03b1 \u221e 0 dz p(z)\u03c8(z; R, k) , (87\n)\nWhere now,\n\u03c6(b; R, k) = exp \u2212 (a \u2212 Rt) 2 2(1 \u2212 R 2 ) \u2212 exp \u2212 (b \u2212 Rt) 2 2(1 \u2212 R 2 ) (\u03ba \u2212 t)(88)\n\u03c8(b; R, k) = H a \u2212 Rt \u221a 1 \u2212 R 2 \u2212 H b \u2212 Rt \u221a 1 \u2212 R 2 (\u03ba \u2212 t) 2 (89)\nTo find the optimal location of the pruning window, we take derivatives with respect to the remaining variables b, R, k, \u00b5, \u03bb and solve the resulting set of equations numerically. Consistent with the results for the optimal distribution, the location of the optimal window shifts from around infinity to around zero as \u03b1 prune grows (Fig. 7C).", "publication_ref": [], "figure_ref": ["fig_4", "fig_4"], "table_ref": []}, {"heading": "A.9 Exact saddle point equations", "text": "To obtain exact expressions for the generalization error for all \u03b8, we can extremize Eq. 63 wrt R, q, \u03c1.\nDerivative wrt R R \u2212 \u03c1 cos \u03b8 (1 \u2212 q) sin 2 \u03b8 = dt dz p(z) \u221a 2\u03b1 \u03c0 t q \u2212 \u03c1 2 \u221a 2 (\u03c1 2 \u2212 q)\u039b \u2212 (R \u2212 \u03c1 cos \u03b8)\u0393(t, z) \u221a 2 q \u2212 \u03c1 2 \u039b 3 (90) \u00d7 log H \u03ba \u2212 t q \u2212 \u03c1 2 \u2212 \u03c1z \u221a 1 \u2212 q exp \u2212 \u0393(t, z) 2 2 (q \u2212 \u03c1 2 ) \u039b 2 \u2212 t 2 2 (91)\nWhere we have defined,\n\u039b = q sin 2 \u03b8 \u2212 R 2 \u2212 \u03c1 2 + 2R\u03c1 cos \u03b8,(92)\n\u0393(t, z) = (R \u2212 \u03c1 cos \u03b8) t q \u2212 \u03c1 2 + \u03c1z + qz cos \u03b8 \u2212 \u03c1Rz. (93\n)\nIntegrating the right-hand side by parts,\nR \u2212 \u03c1 cos \u03b8 (1 \u2212 q) sin 2 \u03b8 = \u2212 dt dz p(z) \u03b1 \u03c0\u039b q \u2212 \u03c1 2 e \u2212 ( \u03ba\u2212t \u221a q\u2212\u03c1 2 \u2212\u03c1z ) 2 2(1\u2212q) \u221a 2\u03c0 \u221a 1 \u2212 qH \u03ba\u2212t \u221a q\u2212\u03c1 2 \u2212\u03c1z \u221a 1\u2212q exp \u2212 \u2206(t, z) 2\u039b 2 (94\n)\nwhere\n\u2206(t, z) = 2tz q \u2212 \u03c1 2 (R\u2212\u03c1 cos \u03b8) cos \u03b8 +qt 2 sin 2 \u03b8 +qz 2 cos 2 \u03b8 \u2212\u03c1 2 t 2 sin 2 \u03b8 \u2212\u03c1 2 z 2 cos 2 \u03b8 (95)\nChanging variables to t \u2192 t q \u2212 \u03c1 2 + \u03c1z and taking the limit\nq \u2192 1, R \u2212 \u03c1 cos \u03b8 sin 2 \u03b8 = \u03ba \u2212\u221e dt \u03b1 \u03c0\u039b exp \u2212 \u2206(t, z) 2\u039b 2 (\u03ba \u2212 t) z (96\n)\nWhere with this change of variables,\n\u039b = q sin 2 \u03b8 \u2212 R 2 \u2212 \u03c1 2 + 2\u03c1R cos \u03b8 (97\n)\n\u2206(t, z) = z 2 \u03c1 2 + cos 2 \u03b8 \u2212 2\u03c1R cos \u03b8 + 2tz(R cos \u03b8 \u2212 \u03c1) + t 2 sin 2 \u03b8 (98\n)\nDerivative wrt q, q \u2212 (\u03c1 2 + R 2 \u2212 2\u03c1R cos \u03b8)/ sin 2 \u03b8 2(1 \u2212 q) 2 = dt dz p(z) 2\u03b1 \u03c0 \u03ba \u2212 t q \u2212 \u03c1 2 \u2212 \u03c1z (1 \u2212 q) 3/2 \u2212 t \u221a 1 \u2212 q q \u2212 \u03c1 2 (99) \u00d7 exp \uf8eb \uf8ec \uf8ed\u2212 \u03ba \u2212 t q \u2212 \u03c1 2 \u2212 \u03c1z 2 2(1 \u2212 q) \u2212 t 2 2 \uf8f6 \uf8f7 \uf8f8 (100\n)\n\u00d7H \uf8eb \uf8ed \u2212 (R \u2212 \u03c1 cos \u03b8) t q \u2212 \u03c1 2 + \u03c1z + qz cos \u03b8 \u2212 \u03c1Rz (\u03c1 2 \u2212 q) \u03c1 2 \u2212 q sin 2 \u03b8 + R 2 \u2212 2\u03c1R cos \u03b8 \uf8f6 \uf8f8 H \u03ba \u2212 t q \u2212 \u03c1 2 \u2212 \u03c1z \u221a 1 \u2212 q (101) \u2212 \u221a 2\u03b1 \u03c0 t(R\u2212\u03c1 cos \u03b8) 2 \u221a q\u2212\u03c1 2 + z cos \u221a 2 (q \u2212 \u03c1 2 )\u039b \u2212 q \u2212 \u03c1 2 sin 2 \u03b8 + \u039b 2 \u0393(t, z) 2 \u221a 2 (\u03c1 2 \u2212 q) 3/2 \u039b 3 (102) \u00d7 log H \u03ba \u2212 t q \u2212 \u03c1 2 \u2212 \u03c1z \u221a 1 \u2212 q exp \uf8eb \uf8ec \uf8ed\u2212 (R \u2212 \u03c1 cos \u03b8) t q \u2212 \u03c1 2 + \u03c1z + qz cos \u03b8 \u2212 \u03c1Rz 2 2 (q \u2212 \u03c1 2 ) \u039b 2 \u2212 t 2 2 \uf8f6 \uf8f7 \uf8f8 (103\n)\nWhere \u0393(t, z) = (R \u2212 \u03c1 cos \u03b8) t q \u2212 \u03c1 2 + \u03c1z + qz cos \u03b8 \u2212 \u03c1Rz. After integating by parts,\nq \u2212 (\u03c1 2 + R 2 \u2212 2\u03c1R cos \u03b8)/ sin 2 \u03b8 2(1 \u2212 q) 2 = dt dz p(z) \u03b1 exp \u2212 2t \u221a q\u2212\u03c1 2 (\u03c1z\u2212\u03ba)\u2212(\u03c1 2 \u22121)t 2 +(\u03ba\u2212\u03c1z) 2 2(1\u2212q) 4\u03c0H \u03ba\u2212t \u221a q\u2212\u03c1 2 \u2212\u03c1z \u221a 1\u2212q 2 (104) \u00d7 2 \u03c0 e \u2212 ( \u03ba\u2212t \u221a q\u2212\u03c1 2 \u2212\u03c1z ) 2 2(1\u2212q) H \u2212 \u0393(t, z) (q \u2212 \u03c1 2 ) \u039b (105\n)\nChanging variables to t \u2192 t q \u2212 \u03c1 2 + \u03c1z and taking the limit q \u2192 1,\n1 \u2212 \u03c1 2 + R 2 \u2212 2\u03c1R cos \u03b8 sin 2 \u03b8 = 2\u03b1 \u03ba \u2212\u221e dt e \u2212 (t\u2212\u03c1z) 2 2(1\u2212\u03c1 2 ) \u221a 2\u03c0 1 \u2212 \u03c1 2 H \u0393(t, z) 1 \u2212 \u03c1 2 \u039b (\u03ba \u2212 t) 2 z (106\n)\nWhere now \u0393(t, z) = z(\u03c1R \u2212 cos \u03b8) \u2212 t(R \u2212 \u03c1 cos \u03b8).\nDerivative wrt \u03c1,\n\u03c1 \u2212 R cos \u03b8 (1 \u2212 q) sin 2 \u03b8 = \u03b1 2\u03c0 \u03c1t \u221a q\u2212\u03c1 2 \u2212 z exp \u2212 \u03ba\u2212t \u221a q\u2212\u03c1 2 \u2212\u03c1z 2 2(1\u2212q) \u2212 t 2 2 H \u2212 \u0393(t,z) \u221a q\u2212\u03c1 2 \u039b \u221a 1 \u2212 qH \u03ba\u2212t \u221a q\u2212\u03c1 2 \u2212\u03c1z \u221a 1\u2212q (107) + \u221a 2\u03b1 \u03c0 (R \u2212 \u03c1 cos \u03b8) z \u2212 \u03c1t \u221a q\u2212\u03c1 2 \u2212 t q \u2212 \u03c1 2 + \u03c1z cos \u03b8 \u2212 Rz \u221a 2 q \u2212 \u03c1 2 \u039b (108) \u2212 \u22122\u03c1\u039b 2 \u2212 q \u2212 \u03c1 2 (2\u03c1 \u2212 2R cos \u03b8) \u0393(t, z) 2 \u221a 2 (q \u2212 \u03c1 2 ) 3/2 \u039b 3 (109) \u00d7 log 1 2 erfc \u03ba \u2212 t q \u2212 \u03c1 2 \u2212 \u03c1z \u221a 2 \u221a 1 \u2212 q exp \u2212 \u0393(t, z) 2 2 (q \u2212 \u03c1 2 ) \u039b 2 \u2212 t 2 2 (110)\nIntegrating the second term by parts,\n\u03c1 \u2212 R cos \u03b8 (1 \u2212 q) sin 2 \u03b8 = \u03b1 \u03c0 \u03c1t \u221a q\u2212\u03c1 2 \u2212 z exp \u2212 \u03ba\u2212t \u221a q\u2212\u03c1 2 \u2212\u03c1z 2 2(1\u2212q) \u2212 t 2 2 H \u2212 \u0393(t,z) \u221a q\u2212\u03c1 2 \u039b \u221a 1 \u2212 qH \u03ba\u2212t \u221a q\u2212\u03c1 2 \u2212\u03c1z \u221a 1\u2212q (111) \u2212 \u03b1 \u03c0\u2206 exp \u2212 \u2206(t, z) 2\u039b 2 \u03c1R \u2212 q cos \u03b8 q \u2212 \u03c1 2(112)\nChanging variables to t \u2192 t q \u2212 \u03c1 2 + \u03c1z and taking the limit q \u2192 1,\n\u03c1 \u2212 R cos \u03b8 sin 2 \u03b8 = 2\u03b1 \u03ba \u2212\u221e dt e \u2212 (t\u2212\u03c1z) 2 2(1\u2212\u03c1 2 ) \u221a 2\u03c0 1 \u2212 \u03c1 2 H \u0393(t, z) 1 \u2212 \u03c1 2 \u039b z \u2212 \u03c1t 1 \u2212 \u03c1 2 (\u03ba \u2212 t)(113)\n+ 1 2\u03c0\u039b exp \u2212 \u2206(t, z) 2\u039b 2 \u03c1R \u2212 cos \u03b8 1 \u2212 \u03c1 2 (\u03ba \u2212 t) z(114)\nSo together we have three saddle point equations:\nR \u2212 \u03c1 cos \u03b8 sin 2 \u03b8 = \u03b1 \u03c0\u039b \u03ba \u2212\u221e dt exp \u2212 \u2206(t, z) 2\u039b 2 (\u03ba \u2212 t) z (115) 1 \u2212 \u03c1 2 + R 2 \u2212 2\u03c1R cos \u03b8 sin 2 \u03b8 = 2\u03b1 \u03ba \u2212\u221e dt e \u2212 (t\u2212\u03c1z) 2 2(1\u2212\u03c1 2 ) \u221a 2\u03c0 1 \u2212 \u03c1 2 H \u0393(t, z) 1 \u2212 \u03c1 2 \u039b (\u03ba \u2212 t) 2 z (116) \u03c1 \u2212 R cos \u03b8 sin 2 \u03b8 = 2\u03b1 \u03ba \u2212\u221e dt e \u2212 (t\u2212\u03c1z) 2 2(1\u2212\u03c1 2 ) \u221a 2\u03c0 1 \u2212 \u03c1 2 H \u0393(t, z) 1 \u2212 \u03c1 2 \u039b z \u2212 \u03c1t 1 \u2212 \u03c1 2 (\u03ba \u2212 t)(117)\n+ 1 2\u03c0\u039b exp \u2212 \u2206(t, z) 2\u039b 2 \u03c1R \u2212 cos \u03b8 1 \u2212 \u03c1 2 (\u03ba \u2212 t) z(118)\nWhere\n\u039b = sin 2 \u03b8 \u2212 R 2 \u2212 \u03c1 2 + 2\u03c1R cos \u03b8,(119)\n\u0393(t, z) = z(\u03c1R \u2212 cos \u03b8) \u2212 t(R \u2212 \u03c1 cos \u03b8),(120)\n\u2206(t, z) = z 2 \u03c1 2 + cos 2 \u03b8 \u2212 2\u03c1R cos \u03b8 + 2tz(R cos \u03b8 \u2212 \u03c1) + t 2 sin 2 \u03b8.(121)\nSolving these equations numerically yields an excellent fit to numerical simulations on structured data (Fig. 2BCD).", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "B Model training method details & dataset information", "text": "Perceptron in the teacher-student setting All code to reproduce these simulations can be found at: https://colab.research.google.com/drive/1in35C6jh7y_ynwuWLBmGOWAgmUgpl8dF? usp=sharing. Perceptrons were trained on a synthetic dataset of P examples {x \u00b5 , y \u00b5 } \u00b5=1,...,P , where x \u00b5 \u223c N (0, I N ) are i.i.d. zero mean unit variance random Gaussian inputs, and y \u00b5 = sign(T\u2022x) are labels generated by a teacher perceptron T \u2208 R N , which was randomly drawn from a uniform distribution on the sphere T \u223c Unif(S N \u22121 ( \u221a N )). For all of our experiments we fixed N = 200 and set P = \u03b1N where \u03b1 varied between 10 0.1 and 10 0.5 . Each synthetic dataset was pruned to keep a fraction f of the smallest-margin examples, where f varied between 0.1 and 1 in Fig. 1 and between 0.2 and 1 in Figs. 2,3 to match the real-world experiments in Fig. 3. Perceptrons were optimized to find the max-margin separating solution using a standard quadratic programming (QP) algorithm from the CVXPY library (for analysis of the computational complexity of this algorithm see Fig. 8). Results were averaged over 100 independent draws of the teacher and training examples.\nImageNet. ImageNet model training was performed using a standard ResNet-50 through the VISSL library [41] (stable version v0.1.6), which provides default configuration files for supervised ResNet-50 training (accessible here; released under the MIT license). Each model was trained on a single node of 8 NVIDIA V100 32GB graphics cards with BATCHSIZE_PER_REPLICA = 256, using the Stochastic Gradient Descent (SGD) optimizer with a base learning rate = 0.1, nesterov momentum = 0.9, and weight decay = 0.001. For our scaling experiments (Fig. 3 and Fig. 9), we trained one model per fraction of data kept (0.1-1.0) for each dataset size. In total, these plot required training 97 models on (potentially a subset of) ImageNet. All the models were trained with matched number of iterations, corresponding to 105 epochs on the full ImageNet dataset. The learning rate was decayed by a factor of 10 after the number of iterations corresponding to 30, 60, 90, and 100 epochs on the full ImageNet dataset.\nFor our main ImageNet experiments (Fig. 5) we trained one model per fraction of data kept (1.0, 0.9, 0.8, 0.7, 0.6) \u00d7 metric (11 metrics in total). In the plot itself, since any variation in the \"fraction of data kept = 1.0\" setting is due to random variation across runs not due to potential metric differences, we averaged model performances to obtain a single datapoint here (while also keeping track of the variation across models, which is plotted as \u00b12 standard deviations). In total, this plot required training 55 models on (potentially a subset of) ImageNet. For Fig. 5C, in order to reduce noise from random variation, we additionally trained five models per datapoint and metric, and plot the averaged performance in addition to error bars showing one standard deviation of the mean. Numerical results from Figure 5BC are available from Table 1. ImageNet [42] is released under the ImageNet terms of access. It is important to note that ImageNet images are often biased [43,44]. The SWaV model used to compute our prototypicality metrics was obtained via torch.hub.load('facebookresearch/swav:main', 'resnet50'), which is the original model provided by [36]; we then used the avgpool layer's activations. CIFAR-10 and SVHN. CIFAR-10 and SVHN model training was performed using a standard ResNet-18 through the PyTorch library. Each model was trained on a single NVIDIA TITAN Xp 12GB graphics card with batch size = 128, using the Stochastic Gradient Descent (SGD) optimizer with learning rate = 0.1, nesterov momentum = 0.9, and weight decay = 0.0005. Probe models were trained for 20 epochs each for CIFAR-10 and 40 epochs each for SVHN. Pruning scores were then computed using the EL2Ns metric [10], averaged across 10 independent initializations of the probe models. To evaluate data pruning performance, fresh models were trained from scratch on each pruned dataset for 200 epochs, with the learning rate decayed by a factor of 5 after 60, 120 and 160 epochs.\nData pruning for transfer learning. To assess the effect of pruning downstream finetuning data on transfer learning performance, vision transformers (ViTs) pre-trained on ImageNet21k were fine-tuned on different pruned subsets of CIFAR-10. Pre-trained models were obtained from the timm model library [45]. Each model was trained on a single NVIDIA TITAN Xp 12GB graphics card with batch size = 128, using the Adam optimizer with learning rate = 1e-5 and no weight decay. Probe models were trained for 2 epochs each. Pruning scores were then computed using the EL2Ns metric [10], averaged across 10 independent random seeds. To evaluate data pruning performance, pre-trained models were fine-tuned on each pruned dataset for 10 epochs.\nTo assess the effect of pruning upstream pretraining data on transfer learning performance, each of the ResNet-50s pre-trained on pruned subsets of ImageNet1k in Fig. 3D was fine-tuned on all of CIFAR-10. Each model was trained on a single NVIDIA TITAN Xp 12GB graphics card with batch size = 128, using the RMSProp optimizer with learning rate = 1e-4 and no weight decay. Probe models were trained for 2 epochs each. Pruning scores were then computed using the EL2Ns metric [10], averaged across 10 independent random seeds. To evaluate data pruning performance, pre-trained models were fine-tuned on each pruned dataset for 10 epochs.", "publication_ref": [], "figure_ref": ["fig_0", "fig_5", "fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "C Breaking compute scaling laws via data pruning", "text": "Do the savings in training dataset size we have identified translate to savings in compute, and can data pruning be used to beat widely observed compute scaling laws [2,5,7]? Here we show for the perceptron that data pruning can afford exponential savings in compute, and we provide preliminary evidence that the same is true for ResNets trained on CIFAR-10 and ImageNet. We repeat the perceptron learning experiments in Fig. 1A, keeping track of the computational complexity of each experiment, measured by the time to convergence of the quadratic programming algorithm used to  find a max-margin solution (see B for details). Across all experiments, the convergence time T was linearly proportional to \u03b1 prune with T = 0.96\u03b1 prune + 0.80, allowing us to replace the x-axis of 1A with compute to produce Fig. 8A, which reveals that data pruning can be used to break compute scaling laws for the perceptron.\nMotivated by this, we next investigate whether the convergence time of neural networks trained on pruned datasets depends largely on the number of examples and not their difficulty, potentially allowing for exponential compute savings. We investigate the learning curves of a ResNet18 trained on CIFAR-10 and a ResNet50 on ImageNet for several different pruning fractions (Fig. 8B). While previous works have fixed the number of iterations [10], here we fix the number of epochs, so that the model trained on 60% of the full dataset is trained for only 60% the iterations of the model trained on the full dataset, using only 60% the compute. Nevertheless, we find that the learning curves are strikingly similar across pruning fractions, and appear to converge equally quickly. These results suggest that data pruning could lead to large compute savings in practical settings, and in ongoing experiments we are working to make the analogs of Fig. 8A for ResNets on CIFAR-10 and ImageNet to quantify this benefit.", "publication_ref": ["b1"], "figure_ref": ["fig_5", "fig_5", "fig_5"], "table_ref": []}, {"heading": "D Additional scaling experiments", "text": "In Fig. 9 we perform additional scaling experiments using the EL2Ns and self-supervised prototypes metrics. In Fig. 10 we give a practical example of a cross over from exponential to power-law scaling when the probe student has limited information about the teacher (here a model trained for only a small number of epochs on SVHN) .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E Extremal images according to different metrics", "text": "In Fig. 6, we showed extremal images for two metrics (self-supervised prototypes, memorization) and a single class. In order to gain a better understanding of how extremal images (i.e. images that are  Figure 9: Additional scaling experiments. We reproduce the scaling results on ImageNet in Fig. 3D using three additional metrics: two supervised and one self-supervised. Each shows some signatures of breaking power law scaling, although the effect is less dramatic than for the best metric, memorization (Fig. 3D). (A) EL2Ns with a class balancing fraction of 0% (see App. Section H for details), (B) EL2Ns with a class balancing fraction of 50%, and (C) Self-supervised prototypes with a class balancing fraction of 50%.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SVHN (epoch 40)", "text": "A B", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Error no longer lower-bounded", "text": "Error lower-bounded by power law at SVHN (epoch 4)\nFigure 10: Consistent with a prediction from the perceptron theory (Fig. 2), when SVHN is pruned using a weak metric (a probe trained for only 4 epochs), the learning curve envelope is lower-bounded by a power law at some f min (A). However, with a stronger pruning metric (a probe trained for 40 epochs), the learning curve can break through this power law to achieve lower generalization error (B).\neasiest or hardest to learn according to different metrics) look like for all metrics and more classes, we here provide additional figures. In order to avoid cherry-picking classes while at the same time making sure that we are visualizing images for very different classes, we here show extreme images for classes 100, ..., 500 while leaving out classes 0 and 400 (which would have been part of the visualization) since those classes almost exclusively consist of images containing people (0: tench, 400: academic gown  For each metric, the top row shows images that are ranked as \"easy\" (most pruneable) according to the metric, and the bottom row shows images that are ranked as \"hard\" (least pruneable). For each metric, the top row shows images that are ranked as \"easy\" (most pruneable) according to the metric, and the bottom row shows images that are ranked as \"hard\" (least pruneable). For each metric, the top row shows images that are ranked as \"easy\" (most pruneable) according to the metric, and the bottom row shows images that are ranked as \"hard\" (least pruneable). For each metric, the top row shows images that are ranked as \"easy\" (most pruneable) according to the metric, and the bottom row shows images that are ranked as \"hard\" (least pruneable). For each metric, the top row shows images that are ranked as \"easy\" (most pruneable) according to the metric, and the bottom row shows images that are ranked as \"hard\" (least pruneable). For each metric, the top row shows images that are ranked as \"easy\" (most pruneable) according to the metric, and the bottom row shows images that are ranked as \"hard\" (least pruneable). For each metric, the top row shows images that are ranked as \"easy\" (most pruneable) according to the metric, and the bottom row shows images that are ranked as \"hard\" (least pruneable). For each metric, the top row shows images that are ranked as \"easy\" (most pruneable) according to the metric, and the bottom row shows images that are ranked as \"hard\" (least pruneable).\nF Impact of number of clusters k on self-supervised prototypes\nOur self-supervised prototype metric is based on k-means clustering, which has a single hyperparameter k. By default and throughout the main paper, we set k = 1000, corresponding to the number of classes in ImageNet. Here, we investigate other settings of k to understand how this hyperparameter impacts performance. As can be seen in Table 2, k does indeed have an impact on performance, and very small values for k (e.g. k < 10) as well as very large values for k (e.g. k = 50, 000) both lead to performance impairments. At the same time, performance is relatively high across very different in-between settings for k. In order to assess these results, it may be important to keep in mind that \u00b10.54% corresponds to plus/minus 2 standard deviations of performance when simply training the same model multiple times (with different random initialization). Overall, these results suggest that if the number of clusters k deviates at most by one order of magnitude from the number of classes in the dataset (for ImageNet-1K), the exact choice of k does not matter much. ", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "G Impact of ensemble prototypes", "text": "The self-supervised prototypes metric is based on k-means clustering in the embedding space of a selfsupervised (=SSL) model. Since even otherwise identical models trained with different random seeds can end up with somewhat different embedding spaces, we here investigated how the performance of our self-supervised prototypes metric would change when averaging the scores derived from five models, instead of just using a single model's score. The results, shown in Table 3, indicate that ensembling the self-supervised prototype scores neither improves nor hurts performance. This is both good and bad news: Bad news since naturally any improvement in metric development leads to better data efficiency; on the other hand this is also good news since ensembles increase the computational cost of deriving the metric-and this suggests that ensembling is not necessary to achieve the performance we achieved (unlike in other methods such as ensemble active learning).\nFraction of data kept 0.9 0.8 0.7 score from single model 90.03 90.10 89.38 score from ensemble model 89.93 90.40 89.38 Table 3: Performance (top-5 acuracy) when pruning away 10%, 20% or 30% of ImageNet based on the self-supervised prototype metric derived from either a single SSL model, or an ensemble of SSL models. Supervised model training on the pruned dataset was performed with a ResNet-50 architecture trained using VISSL without class balancing. Overall, we do not observe any performance difference between the two settings: the differences are still well within a single standard deviation of training multiple models with identical settings. In order to reduce the influence of random variations, each data point in this table shows the average of four independent runs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "H Relationship between pruning and class (im-)balance", "text": "Motivation. It is well-known that strong class imbalance in a dataset is a challenge that needs to be addressed. In order to understand the effect of pruning on class (im-)balance, we quantified this relationship. For context, if pruning according to a certain metric preferentially leads to discarding most (or even all) images from certain classes, it is likely that the performance on those classes will drop as a result if this imbalance is not addressed.\nClass imbalance metric: pruning amplifies class imbalance. Since a standard measure of class (im-)balance-dividing the number of images for the majority class by the number of images for the minority class-is highly sensitive to outliers and discards information about the 998 non-extreme ImageNet classes, we instead calculated a class balance score b \u2208 [0%, 100%] as the average class imbalance across any two pairs of classes by computing the expectation over taking two random classes, and then computing how many images the minority class has in proportion to the majority class. For instance, a class balance score of 90% means that on average, when selecting two random classes from the dataset, the smaller of those two classes contains 90% of the number of images of the larger class (higher=better; 100% would be perfectly balanced).\nIn Fig. 20, we observe that dataset pruning strongly increases class imbalance. This is the case both when pruning away easy images and when pruning away hard images, and the effect occurs for all pruning metrics except, of course, for random pruning. Class imbalance is well-known to be a challenge for deep learning models when not addressed properly [46]. The cause for the amplified class imbalance is revealed when looking at class-conditional differences of metric scores (Figs. 22  and 23): The histograms of the class-conditional score distributions show that for many classes, most (if not all) images have very low scores, while for others most (if not all) images have very high scores. This means that as soon as the lowest / highest scoring images are pruned, certain classes are pruned preferentially and thus class imbalance worsens.\nWe thus use 50% class balancing for our ImageNet experiments. This ensures that every class has at least 50% of the images that it would have when pruning all classes equally (essentially providing a fixed floor for the minimum number of images per class). This simple fix is an important step to address and counteract class imbalance, although other means could be used as well; and ultimately one would likely want to use a self-supervised version of class (or cluster) balancing when it comes to pruning large-scale unlabeled datasets. For comparison purposes, the results for ImageNet pruning without class balancing are shown in supplementary Fig. 21.  With larger pruning fractions, class balance decreasesboth when pruning \"easy\" images (turquoise) and when pruning \"hard\" images (orange). This effect occurs for all pruning metrics except for random pruning (top left). For details on the class imbalance metric see Appendix H. Pruning fraction refers to the fraction of data pruned, from 0 (no pruning) to 0.9 (keeping only 10% of the data).\nFigure 21: ImageNet-1K pruning results for different metrics, compared against random pruning. 'Pruning fraction' refers to the fraction of the dataset that is pruned away. Results obtained without any class balancing are worse than results with 50% class balancing (Fig. 5BC), confirming the finding that vanilla pruning amplifies class imbalance.  shown. Please note that the active learning plot is to be taken with a grain of salt since the authors provided a binary score (included/excluded) for ImageNet corresponding to 80% \"important\" images, thus the scores are either zero or one and a boxplot fit does not apply here.", "publication_ref": [], "figure_ref": ["fig_0", "fig_0", "fig_0", "fig_0", "fig_2"], "table_ref": []}, {"heading": "I Effect of pruning on class-conditional accuracy and fairness", "text": "In order to study the effect of dataset pruning on model fairness, at least with respect to specific ImageNet classes, we compared the class-conditional accuracy of a ResNet-50 model trained on the full ImageNet dataset, versus that of the same model trained on an 80% subset obtained after pruning. We used two supervised pruning metrics (EL2N, memorization) and one self-supervised pruning metric (self-supervised prototypes) for obtaining the pruned dataset. In all three cases, and across all 1000 classes, we found that the class-conditional accuracy of the model trained on a pruned subset of the dataset remains quite similar to that of the model trained on the full dataset (Fig. 24). However, we did notice a very small reduction in class-conditioned accuracy for ImageNet classes that were least accurately predicted by models trained on the entire dataset (blue lines slightly above red unity lines when class-conditioned accuracy is low). This suggests that pruning yields a slight systematic reduction in the accuracy of harder classes, relative to easier classes, though the effect is small.\nWhile we have focused on fairness with respect to individual ImageNet classes, any ultimate test of model fairness should be conducted in scenarios that are specific to the use case of the deployed model. Our examination of the fairness of pruning with respect to individual ImageNet classes constitutes only an initial foray into an exploration of fairness, given the absence of any specific deployment scenario for our current models other than testing them on ImageNet. We leave a full exploration of fairness in other deployment settings for future work. ", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "J Interaction between data pruning and training duration", "text": "Throughout the main paper, our ImageNet experiments are based on a setting where the number of training epochs is kept constant (i.e. we train the same number of epochs on the smaller pruned dataset as on the larger original dataset). This means that data pruning directly reduces the number of iterations required to train the model specifically by reducing the size of the dataset. However, this simultaneously places two constraints on model performance: not only training on a smaller data set, but also training for fewer iterations.\nWe therefore investigate how model performance changes if we train longer, as quantified by a matched iterations factor. A matched iterations factor of 0 corresponds to the default setting used in the paper of training for the same number of epochs (so that a smaller dataset means proportionally fewer training iterations). In contrast a matched iterations factor of 1 corresponds to training on the smaller pruned dataset for a number of iterations equal to that when training on the larger initial dataset (e.g. when pruning away 50% of the dataset one would train twice as long to match the number of iterations of a model trained on 100% of the dataset). Otherwise the matched iterations factor reflects a linear interpolation in the number of training iterations as the factor varies between 0 and 1.\nThe results are shown in Table 4 and indicate that training longer does indeed improve performance slightly; however, a matched iterations factor of around 0.4-0.6 may already be sufficient to reap the full benefit. Any matched iterations factor strictly smaller than 1.  4: Comparing different settings for training longer when pruning: Performance (top-5 acuracy) when pruning away 20% of ImageNet based on our self-supervised prototype metric. Supervised model training on the pruned dataset performed with a ResNet-50 architecture trained using VISSL without class balancing. Performance tends to increase when training longer (i.e. with a larger matched iterations factor). Interestingly, the benefit of training longer may already be achieved with a matched iterations factor of around 0.4-0.6.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "K Out-of-distribution (OOD) analysis of dataset pruning", "text": "Pruning changes the data regime that a model is exposed to. Therefore, a natural question is how this might affect desirable properties beyond IID performance like fairness (see Appendix I) and out-of-distribution, or OOD, performance which we investigate here. To this end, we use the model-vshuman toolbox [47] based on data and analyses from [48,49,50,51,52]. This toolbox is comprised of 17 different OOD datasets, including many image distortions and style changes.\nIn Figure 25a, OOD accuracies averaged across those 17 datasets are shown for a total of 12 models. These models all have a ResNet-50 architecture [53]. Two baseline models are trained on the full ImageNet training dataset, one using torchvision (purple) and the other using VISSL (blue). Human classification data is shown as an additional reference in red. The remaining 10 models are VISSLtrained on pruned versions of ImageNet using pruning fractions in {0.1, 0.2, 0.3, 0.4, 0.5} and our self-supervised prototype metric. A pruning fraction of 0.3 would correspond to \"fraction of data kept = 0.7\", i.e. to training on 70% of ImageNet while discarding the other 30%. We investigated two different settings: discarding easy examples (the default used throughout the paper), which is denoted as \"Best Case\" (or BC) in the plots; and the reverse setting, i.e. discarding hard examples denoted as Worst Case, or WC. (These terms should be taken with a grain of salt; examples are only insofar best-or worst case examples as predicted by the metric, which itself is in all likelihood far less than perfect.)\nThe results are as follows: In terms of OOD accuracy (Figure 25a), best-case pruning in green achieves very similar accuracies to the most relevant baseline, the blue ResNet-50 model trained via VISSL. This is interesting since oftentimes, OOD accuracies closely follow IID accuracies except for a constant offset [54], and we know from Figure 5 that the self-supervised prototype metric has a drastic performance impairment when pruning away 40% of the data, yet the model \"BC_pruning-fraction-0.4\" still achieves almost the same OOD accuracy as the baseline trained on the full dataset. The core take-away is: While more analyses would be necessary to investigate whether pruning indeed consistently helps on OOD performance, it seems safe to conclude that it does not hurt OOD performance on the investigated datasets compared to an accuracymatched baseline. (The control setting, pruning away hard examples shown in orange, leads to much lower IID accuracies and consequently also lower OOD accuracies.) For reference, the numerical results from Figure 25a are also shown in Table 5.\nFigures 25b, 25c and 25d focus on a related question, the question of whether models show humanlike behavior on OOD datasets. Figure 25b shows that two models pruned using our self-supervised prototype metric somewhat more closely match human accuracies compared to the baseline in blue; Figures 25c and 25d specifically focus on image-level consistency with human responses. In terms of overall consistency (c), the baseline scores best; in terms of error consistency pruned models outperform the VISSL-trained baseline. For details on the metrics we kindly refer the interested reader to [47]. Numerical results are again also shown in a Table (Table 6).\nFinally, in Figure 26 we observe that best-case pruning leads to slightly higher shape bias as indicated by green vertical lines plotting the average shape bias across categories, which are shifted to the left of the baseline; while worst-case pruning in orange is shifted to the right. An outlier is the torchvisiontrained model in purple with a very strong texture bias; we attribute this to data augmentation differences between VISSL and torchvision training.  ", "publication_ref": [], "figure_ref": ["fig_0", "fig_0", "fig_2", "fig_0", "fig_0", "fig_0", "fig_0", "fig_0"], "table_ref": ["tab_13"]}, {"heading": "Acknowledgments", "text": "We thank Priya Goyal, Berfin Simsek, Pascal Vincent valuable discussions, Qing Jin for insights about the optimal pruning distribution, Isaac Seessel for VISSL support as well as Kashyap Chitta and Jos\u00e9 \u00c1lvarez for kindly providing their ensemble active learning score.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "We then fit two functional forms to the empirical frontier using least squares: power-law (dark red) and exponential (pink). The exponential shows a much better fit to the empirical Pareto frontier, indicating that the scaling is at least exponential.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Deep learning scaling is predictable", "journal": "", "year": "2017", "authors": "Joel Hestness; Sharan Narang; Newsha Ardalani; Gregory Diamos; Heewoo Jun; Hassan Kianinejad; Md Patwary; Mostofa Ali; Yang Yang; Yanqi Zhou"}, {"ref_id": "b1", "title": "Scaling laws for neural language models", "journal": "", "year": "2020", "authors": "Jared Kaplan; Sam Mccandlish; Tom Henighan; B Tom; Benjamin Brown; Rewon Chess; Scott Child; Alec Gray; Jeffrey Radford; Dario Wu;  Amodei"}, {"ref_id": "b2", "title": "Scaling laws for autoregressive generative modeling", "journal": "", "year": "2020", "authors": "Tom Henighan; Jared Kaplan; Mor Katz; Mark Chen; Christopher Hesse; Jacob Jackson; Heewoo Jun; B Tom; Prafulla Brown; Scott Dhariwal;  Gray"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Data pruning with an imperfect metric. A: Weight vectors and decision boundaries for a teacher (black) and probe student (red) separated by angle \u03b8. The black point has margin 0 (\u03ba) w.r.t. the probe (teacher). B-D: Test error as a function of \u03b1 prune for different f and different \u03b8.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 4 :4Figure 4: Data pruning improves transfer learning. A: CIFAR-10 performance of a ViT pre-trained on all of ImageNet21K and fine-tuned on different pruned subsets of CIFAR-10 under the EL2N metric. B: CIFAR-10 performance of ResNet50s pretrained on different pruned subsets of Ima-geNet1K and fine-tuned on all of CIFAR-10.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 5 :5Figure 5: Dataset pruning at ImageNet scale. A: Spearman's rank correlation between all pairs of ImageNet metric scores, along with hierarchical clustering (as provided by seaborn.clustermap). B: Benchmarking existing supervised metrics on ImageNet (top-5 validation accuracy). C: Comparing top-5 performance on ImageNet when pruning according to the best existing supervised metric (memorization) and our supervised and self-supervised prototype metrics. In all 3 cases, training on 80% of ImageNet approximates training on 100%. See App. B for pruning and training details.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "400M image-text pairs for CLIP [37], 3.5B Instagram images [38], 650M images for the DALLE-2 encoder [39], 780B tokens for PALM [40]", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 7 :7Figure 7: Optimal pruning policy as a function of \u03b1 prune . A, The Pareto frontier in Fig. AA can be lowered in the small \u03b1 prune regime if one adaptively switches pruning policies from a \"keep easy\" to a \"keep hard\" policy. The dashed purple line indicates the \"keep easy\" frontier (computed using numerical simulations). The optimal pruning window (derived below) interpolates between the two policies, achieving the lowest possible Pareto frontier (cartooned in blue). B, The optimal data distribution along the teacher p(z|\u03b1 prune , f ) is a delta function, which selects easy examples for small \u03b1 prune , intermediate examples for intermediate \u03b1 prune , and hard examples for large \u03b1 prune . C, The optimal pruning window similarly selects easy examples for small \u03b1 prune , intermediate examples for intermediate \u03b1 prune , and hard examples for large \u03b1 prune These observations beg the question: what is the best policy in the intermediate \u03b1 prune regime? Is there a globally optimal pruning policy which interpolates between the \"keep easy\" and \"keep hard\" strategies and achieves the lowest possible Pareto frontier (blue curve in Fig. 7A)?", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 8 :8Figure 8: Breaking compute scaling laws via data pruning. A,B,C, We repeat the experiments in Figs. 1A, 3C,D, replacing the x-axis with compute, measured as clock time to convergence for the perceptron (A), and FLOPs in a fixed-epoch training setting for the ResNets (B,C). Theoretical curves in A are overlaid by linearly regressing clock time to convergence T from \u03b1 prune , with T = 0.96\u03b1 prune + 0.80. Perceptrons in A are trained on a CPU on a google Colab. E,D, CIFAR-10 and ImageNet learning curves for fixed epochs.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "error (%) Top-5 test error (%) Top-5 test error (%)", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 12 :12Figure 12: Extreme images according to different metrics for ImageNet class 100 (black swan).For each metric, the top row shows images that are ranked as \"easy\" (most pruneable) according to the metric, and the bottom row shows images that are ranked as \"hard\" (least pruneable).", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 13 :13Figure 13: Extreme images according to different metrics for ImageNet class 100 (black swan).For each metric, the top row shows images that are ranked as \"easy\" (most pruneable) according to the metric, and the bottom row shows images that are ranked as \"hard\" (least pruneable).", "figure_data": ""}, {"figure_label": "14", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 14 :14Figure14: Extreme images according to different metrics for ImageNet class 200 (Tibetan terrier). For each metric, the top row shows images that are ranked as \"easy\" (most pruneable) according to the metric, and the bottom row shows images that are ranked as \"hard\" (least pruneable).", "figure_data": ""}, {"figure_label": "15", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 15 :15Figure 15: Extreme images according to different metrics for ImageNet class 200 (Tibetan terrier).For each metric, the top row shows images that are ranked as \"easy\" (most pruneable) according to the metric, and the bottom row shows images that are ranked as \"hard\" (least pruneable).", "figure_data": ""}, {"figure_label": "16", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 16 :16Figure16: Extreme images according to different metrics for ImageNet class 300 (tiger beetle). For each metric, the top row shows images that are ranked as \"easy\" (most pruneable) according to the metric, and the bottom row shows images that are ranked as \"hard\" (least pruneable).", "figure_data": ""}, {"figure_label": "17", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 17 :17Figure 17: Extreme images according to different metrics for ImageNet class 300 (tiger beetle).For each metric, the top row shows images that are ranked as \"easy\" (most pruneable) according to the metric, and the bottom row shows images that are ranked as \"hard\" (least pruneable).", "figure_data": ""}, {"figure_label": "18", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Figure 18 :18Figure 18: Extreme images according to different metrics for ImageNet class 500 (cliff dwelling).For each metric, the top row shows images that are ranked as \"easy\" (most pruneable) according to the metric, and the bottom row shows images that are ranked as \"hard\" (least pruneable).", "figure_data": ""}, {"figure_label": "19", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Figure 19 :19Figure 19: Extreme images according to different metrics for ImageNet class 500 (cliff dwelling).For each metric, the top row shows images that are ranked as \"easy\" (most pruneable) according to the metric, and the bottom row shows images that are ranked as \"hard\" (least pruneable).", "figure_data": ""}, {"figure_label": "20", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "Figure 20 :20Figure20: Pruning amplifies class imbalance. With larger pruning fractions, class balance decreasesboth when pruning \"easy\" images (turquoise) and when pruning \"hard\" images (orange). This effect occurs for all pruning metrics except for random pruning (top left). For details on the class imbalance metric see Appendix H. Pruning fraction refers to the fraction of data pruned, from 0 (no pruning) to 0.9 (keeping only 10% of the data).", "figure_data": ""}, {"figure_label": "23", "figure_type": "figure", "figure_id": "fig_17", "figure_caption": "Figure 23 :23Figure23: How do ImageNet metric scores differ across classes (continued)? Class-conditional score distribution histograms across metrics. For the purpose of visualization, only every 10 th class is shown. Please note that the active learning plot is to be taken with a grain of salt since the authors provided a binary score (included/excluded) for ImageNet corresponding to 80% \"important\" images, thus the scores are either zero or one and a boxplot fit does not apply here.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_18", "figure_caption": "Figure 24: The effect of data pruning on ImageNet test accuracy on individual classes. For all 3 plots, each point corresponds to an ImageNet class, and for all classes, the class specific test accuracy when training on the entire dataset (y-axis) is plotted against the test accuracy when training on the pruned dataset (x-axis).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_19", "figure_caption": "0 comes with reduced training time compared to training a model on the full dataset. 90.20 90.41 90.48 90.56 90.45 90.50 Table", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "[4] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales. International Conference on Learning Representations, 2020. [5] Mitchell A Gordon, Kevin Duh, and Jared Kaplan. Data and parameter scaling laws for neural machine translation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5915-5922, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. [6] Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer. arXiv preprint arXiv:2102.01293, 2021. [7] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. arXiv preprint arXiv:2106.04560, 2021. Active learning at the ImageNet scale. arXiv preprint arXiv:2111.12880, 2021. [18] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. arXiv preprint arXiv:1708.00489, 2017. [19] Siddharth Karamcheti, Ranjay Krishna, Li Fei-Fei, and Christopher D Manning. Mind your outliers! Investigating the negative impact of outliers on active learning for visual question answering. arXiv preprint arXiv:2107.02331, 2021. [20] Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-efficient training of machine learning models. In Hal Daum\u00e9 Iii and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 6950-6960. PMLR, 2020. [21] V Birodkar, H Mobahi, and S Bengio. Semantic redundancies in Image-Classification datasets: The 10% you don't need. arXiv preprint arXiv:1901.11409, 2019. [22] Kristof Meding, Luca M. Schulze Buschoff, Robert Geirhos, and Felix A. Wichmann. Trivial or impossible-dichotomous data difficulty masks model differences (on ImageNet and beyond). In International Conference on Learning Representations, 2022. [23] Utkarsh Sharma and Jared Kaplan. Scaling laws from the data manifold dimension. Journal of Machine Learning Research, 23(9):1-34, 2022. [24] Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural scaling laws. CoRR, abs/2102.06701, 2021. [25] Jonathan S. Rosenfeld. Scaling Laws for Deep Learning. PhD thesis, Massachusetts Institute of Technology, USA, 2021. [26] A Engel and C V den Broeck. Statistical Mechanics of Learning. Cambridge Univ. Press, 2001. [27] Madhu Advani, Subhaneil Lahiri, and Surya Ganguli. Statistical mechanics of complex neural systems and high dimensional data. J. Stat. Mech: Theory Exp., 2013(03):P03014, 2013. [28] Yasaman Bahri, Jonathan Kadmon, Jeffrey Pennington, Sam S Schoenholz, Jascha Sohl-Dickstein, and Surya Ganguli. Statistical mechanics of deep learning. Annual Review of Condensed Matter Physics, March 2020. Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 9912-9924. Curran Associates, Inc., 2020. [37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748-8763. PMLR, 2021. [38] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining. In Proceedings of the European conference on computer vision (ECCV), pages 181-196, 2018. [39] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. [40] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. [41] Priya Goyal, Quentin Duval, Jeremy Reizenstein, Matthew Leavitt, Min Xu, Benjamin Lefaudeux, Mannat Singh, Vinicius Reis, Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Ishan Misra. VISSL. https://github.com/facebookresearch/vissl, 2021. [42] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211-252, 2015. 44] Yuki M Asano, Christian Rupprecht, Andrew Zisserman, and Andrea Vedaldi. PASS: An ImageNet replacement for self-supervised pretraining without humans. arXiv preprint arXiv:2109.13228, 2021. [45] Ross Wightman. Pytorch image models. https://github.com/rwightman/ pytorch-image-models, 2019. [46] Justin M Johnson and Taghi M Khoshgoftaar. Survey on deep learning with class imbalance. Robert Geirhos, Kantharaju Narayanappa, Benjamin Mitzkus, Tizian Thieringer, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Partial success in closing the gap between human and machine vision. Advances in Neural Information Processing Systems, 34:23885-23899, 2021. [48] Felix A Wichmann, David HJ Janssen, Robert Geirhos, Guillermo Aguilar, Heiko H Sch\u00fctt, Marianne Maertens, and Matthias Bethge. Methods and measurements to compare men against machines. Electronic Imaging, Human Vision and Electronic Imaging, 2017(14):36-45, 2017. [49] Robert Geirhos, Carlos RM Temme, Jonas Rauber, Heiko H Sch\u00fctt, Matthias Bethge, and Felix A Wichmann. Generalisation in humans and deep neural networks. In Advances in Neural Information Processing Systems, 2018. ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In International Conference on Learning Representations, 2019. [51] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. Advances in Neural Information Processing Systems, 32, 2019. Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In International Conference on Machine Learning, pages 7721-7735. PMLR, 2021. Problem setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Main result and overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Replica calculation of the generalization error . . . . . . . . . . . . . . . . . . A.4 Quenched entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Perfect teacher-probe overlap . . . . . . . . . . . . . . . . . . . . . . . . . . . A.6 Information gain per example . . . . . . . . . . . . . . . . . . . . . . . . . . . A.7 Imperfect teacher-probe overlap . . . . . . . . . . . . . . . . . . . . . . . . . . A.8 Optimal pruning policy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.9 Exact saddle point equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . All code required to reproduce the theory figures and numerical simulations throughout this paper can be run in the Colab notebook at https://colab.research.google.com/drive/1in35C6jh7y_ ynwuWLBmGOWAgmUgpl8dF?usp=sharing.", "figure_data": "Journal of Big Data, 6(1):1-54, 2019. Table of Contents [47] Appendix A A theory of data-pruning for the perceptron: detailed derivations[29] Lenka Zdeborov\u00e1 and Florent Krzakala. Statistical physics of inference: thresholds and algorithms. Adv. Phys., 65(5):453-552, September 2016. [30] E Gardner. The space of interactions in neural network models. J. of Physics A, 21:257-270, 1988. [31] H S Seung, H Sompolinsky, and N Tishby. Statistical mechanics of learning from examples. Phys. Rev. A, 45(8):6056, 1992. A.1 B Model training method details & dataset informationC Breaking compute scaling laws via data pruningD Additional scaling experimentsE Extremal images according to different metricsF Impact of number of clusters k on self-supervised prototypesG Impact of ensemble prototypesH Relationship between pruning and class (im-)balanceI Effect of pruning on class-conditional accuracy and fairnessJ Interaction between data pruning and training durationK Out-of-distribution (OOD) analysis of dataset pruningA A theory of data-pruning for the perceptron: detailed derivations"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "). The extremal images are shown in Figures 12,13,14,15,16,17,18,19.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "How do ImageNet metric scores differ across classes? Class-conditional score distribution histograms across metrics. For the purpose of visualization, only every 10 th class is shown.", "figure_data": "1.00.80.4 0.6 8 random 100.0 0.2 4 6 DDD20 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 Class rank00 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 Class rank 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 Class rank 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 Class rank 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 Class rank 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 Class rank 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 Class rank 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 Class rank 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 Class rank 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 Class rank Figure 22: 0 0.0 0.1 0.2 0.3 0.4 0.5 0.6 influence max 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 influence sum-abs 0.1 0.2 0.3 0.4 0.5 self-supervised prototypes 0.1 0.2 0.3 0.4 0.5 0.6 supervised prototypes 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 EL2N (1 model) 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 EL2N (20 models) 0.0 0.2 0.4 0.6 0.8 1.0 memorization 0.0 0.2 0.4 0.6 0.8 1.0 active learning 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Benchmark table of model results for most human-like behaviour. The three metrics \"accuracy difference\" \"observed consistency\" and \"error consistency\" (plotted in Figure25) each produce a different model ranking. model accuracy diff. \u2193 obs. consistency \u2191 error consistency \u2191 mean rank \u2193", "figure_data": "BC_pruning-fraction-0.30.0750.6710.1941.667ResNet-50 (VISSL)0.0770.6710.1843.667BC_pruning-fraction-0.50.0770.6660.1904.000BC_pruning-fraction-0.40.0780.6700.1874.333WC_pruning-fraction-0.10.0820.6660.1914.667ResNet-50 (torchvision)0.0870.6650.2085.667BC_pruning-fraction-0.10.0790.6710.1825.667WC_pruning-fraction-0.20.0860.6580.1867.667BC_pruning-fraction-0.20.0840.6600.1758.333WC_pruning-fraction-0.30.0880.6520.1839.333WC_pruning-fraction-0.40.0990.6350.16911.000WC_pruning-fraction-0.50.1080.6230.16512.00010.90.80.3 Fraction of 'shape' decisions 0.4 0.5 0.6 0.70.20.10"}], "formulas": [{"formula_id": "formula_0", "formula_text": "R \u2212 \u03c1 cos \u03b8 sin 2 \u03b8 = \u03b1 \u03c0\u039b \u03ba \u2212\u221e dt exp \u2212 \u2206(t, z) 2\u039b 2 (\u03ba \u2212 t) z (1) 1 \u2212 \u03c1 2 + R 2 \u2212 2\u03c1R cos \u03b8 sin 2 \u03b8 = 2\u03b1 \u03ba \u2212\u221e dt e \u2212 (t\u2212\u03c1z) 2 2(1\u2212\u03c1 2 ) \u221a 2\u03c0 1 \u2212 \u03c1 2 H \u0393(t, z) 1 \u2212 \u03c1 2 \u039b (\u03ba \u2212 t) 2 z (2) \u03c1 \u2212 R cos \u03b8 sin 2 \u03b8 = 2\u03b1 \u03ba \u2212\u221e dt e \u2212 (t\u2212\u03c1z) 2 2(1\u2212\u03c1 2 ) \u221a 2\u03c0 1 \u2212 \u03c1 2 H \u0393(t, z) 1 \u2212 \u03c1 2 \u039b z \u2212 \u03c1t 1 \u2212 \u03c1 2 (\u03ba \u2212 t) + 1 2\u03c0\u039b exp \u2212 \u2206(t, z) 2\u039b 2 \u03c1R \u2212 cos \u03b8 1 \u2212 \u03c1 2 (\u03ba \u2212 t) z(3)", "formula_coordinates": [15.0, 123.97, 334.38, 383.42, 128.49]}, {"formula_id": "formula_1", "formula_text": "\u039b = sin 2 \u03b8 \u2212 R 2 \u2212 \u03c1 2 + 2\u03c1R cos \u03b8,(4)", "formula_coordinates": [15.0, 187.27, 490.37, 320.12, 11.11]}, {"formula_id": "formula_2", "formula_text": "\u0393(t, z) = z(\u03c1R \u2212 cos \u03b8) \u2212 t(R \u2212 \u03c1 cos \u03b8),(5)", "formula_coordinates": [15.0, 167.12, 508.8, 340.27, 8.96]}, {"formula_id": "formula_3", "formula_text": "\u2206(t, z) = z 2 \u03c1 2 + cos 2 \u03b8 \u2212 2\u03c1R cos \u03b8 + 2tz(R cos \u03b8 \u2212 \u03c1) + t 2 sin 2 \u03b8.(6)", "formula_coordinates": [15.0, 165.04, 522.59, 342.35, 11.11]}, {"formula_id": "formula_4", "formula_text": "\u2126(x \u00b5 , T, \u03ba) = d\u00b5(J) \u00b5 \u0398 T \u2022 x \u00b5 \u221a N J \u2022 x \u00b5 \u221a N \u2212 \u03ba (7)", "formula_coordinates": [16.0, 195.84, 109.58, 308.16, 27.93]}, {"formula_id": "formula_5", "formula_text": "d\u00b5(J) = 1 (2\u03c0e) N/2 \u03b4( J 2 \u2212 N )(8)", "formula_coordinates": [16.0, 240.4, 180.97, 263.6, 22.49]}, {"formula_id": "formula_6", "formula_text": "ln(x) = lim n\u21920 x n \u2212 1 n (9)", "formula_coordinates": [16.0, 263.9, 289.35, 240.1, 23.89]}, {"formula_id": "formula_7", "formula_text": "S(\u03ba) = ln \u2126(x \u00b5 , T, \u03ba) = \u2126 n (x \u00b5 , T, \u03ba) \u2212 1 n (10)", "formula_coordinates": [16.0, 203.16, 343.25, 300.84, 23.89]}, {"formula_id": "formula_8", "formula_text": "\u2126 (n) \u2261 \u2126 n (x \u00b5 , T, \u03ba) = n \u03b1=1 d\u00b5(J \u03b1 ) \u03b1,\u00b5 \u0398 T \u2022 x \u00b5 \u221a N J \u2022 x \u00b5 \u221a N \u2212 \u03ba (11)", "formula_coordinates": [16.0, 143.48, 407.23, 360.52, 30.2]}, {"formula_id": "formula_9", "formula_text": "\u03bb \u03b1 \u00b5 = J \u03b1 \u2022 x \u00b5 \u221a N , u \u00b5 = T \u2022 x \u00b5 \u221a N (12", "formula_coordinates": [16.0, 245.69, 468.32, 254.16, 25.97]}, {"formula_id": "formula_10", "formula_text": ")", "formula_coordinates": [16.0, 499.85, 477.74, 4.15, 8.64]}, {"formula_id": "formula_11", "formula_text": "\u2126 (n) = n \u03b1=1 d\u00b5(J \u03b1 ) \u03b1,\u00b5 d\u03bb \u03b1 \u00b5 \u00b5 du \u00b5 \u03b1,\u00b5 \u0398 u \u00b5 (\u03bb \u03b1 \u00b5 \u2212 \u03ba) \u00d7 \u03b4 \u03bb \u03b1 \u00b5 \u2212 1 \u221a N J \u03b1 \u2022 x \u00b5 \u03b4 u \u00b5 \u2212 1 \u221a N T \u2022 x \u00b5 (13)", "formula_coordinates": [16.0, 117.96, 530.06, 386.04, 59.53]}, {"formula_id": "formula_12", "formula_text": "\u2126 (n) = n \u03b1=1 d\u00b5(J \u03b1 ) \u03b1,\u00b5 d\u03bb \u03b1 \u00b5 d\u03bb \u03b1 \u00b5 2\u03c0 \u00b5 du \u00b5 d\u00fb \u00b5 2\u03c0 (14) \u00d7 \u03b1,\u00b5 \u0398 u \u00b5 (\u03bb \u03b1 \u00b5 \u2212 \u03ba) exp i \u00b5,\u03b1 \u03bb \u03b1 \u00b5\u03bb \u03b1 \u00b5 + i \u00b5 u \u00b5\u00fb\u00b5 (15) \u00d7 exp \u2212 i \u221a N \u00b5,\u03b1\u03bb \u03b1 \u00b5 J \u03b1 \u2022 x \u00b5 \u2212 i \u221a N \u00b5\u00fb \u00b5 T \u2022 x \u00b5 (16)", "formula_coordinates": [16.0, 179.86, 624.49, 324.14, 94.51]}, {"formula_id": "formula_13", "formula_text": "\u2126 (n) = n \u03b1=1 d\u00b5(J \u03b1 ) \u03b1,\u00b5 d\u03bb \u03b1 \u00b5 d\u03bb \u03b1 \u00b5 2\u03c0 \u00b5 du \u00b5 d\u00fb \u00b5 2\u03c0 (17) \u00d7 \u03b1,\u00b5 \u0398 u \u00b5 (\u03bb \u03b1 \u00b5 \u2212 \u03ba) exp i \u00b5,\u03b1 \u03bb \u03b1 \u00b5\u03bb \u03b1 \u00b5 + i \u00b5 u \u00b5\u00fb\u00b5 (18) \u00d7 exp \u2212 i \u221a N \u00b5,\u03b1\u03bb \u03b1 \u00b5 (J \u03b1 \u2022 J probe z \u00b5 + J \u03b1 \u22a5 \u2022 s \u00b5 ) \u2212 i \u221a N \u00b5\u00fb \u00b5 (T \u2022 J probe z \u00b5 + T \u22a5 \u2022 s \u00b5 (19", "formula_coordinates": [17.0, 108.0, 128.93, 396.0, 106.29]}, {"formula_id": "formula_14", "formula_text": ")", "formula_coordinates": [17.0, 499.85, 226.59, 4.15, 8.64]}, {"formula_id": "formula_15", "formula_text": "s \u00b5 \u223c N (0, I N ), exp \u2212 i \u221a N \u00b5,\u03b1\u03bb \u03b1 \u00b5 J \u03b1 \u22a5 \u2022 s \u00b5 \u2212 i \u221a N \u00b5\u00fb \u00b5 T \u22a5 \u2022 s \u00b5 s \u00b5 = exp \u2212 1 2N \u00b5,\u03b1\u03bb \u03b1 \u00b5 J \u03b1 \u22a5 +\u00fb \u00b5 T \u22a5 2 (20) = exp \u2212 1 2N \u00b5 \u03b1\u03b2\u03bb \u03b1 \u00b5\u03bb \u03b2 \u00b5 J \u03b1 \u22a5 \u2022 J \u03b2 \u22a5 + 2 \u03b1\u03bb \u03b1 \u00b5\u00fb\u00b5 J \u03b1 \u22a5 \u2022 T \u22a5 +\u00fb 2 \u00b5 T \u22a5 2 . (21", "formula_coordinates": [17.0, 108.0, 260.0, 396.0, 110.88]}, {"formula_id": "formula_16", "formula_text": ")", "formula_coordinates": [17.0, 499.85, 351.06, 4.15, 8.64]}, {"formula_id": "formula_17", "formula_text": "\u2126 (n) = n \u03b1=1 d\u00b5(J \u03b1 ) \u03b1,\u00b5 d\u03bb \u03b1 \u00b5 d\u03bb \u03b1 \u00b5 2\u03c0 \u00b5 du \u00b5 d\u00fb \u00b5 2\u03c0 \u00d7 \u03b1,\u00b5 \u0398 u \u00b5 (\u03bb \u03b1 \u00b5 \u2212 \u03ba) exp i \u00b5,\u03b1 \u03bb \u03b1 \u00b5\u03bb \u03b1 \u00b5 + i \u00b5 u \u00b5\u00fb\u00b5 \u00d7 exp \u2212 1 2N \u00b5 \u03b1\u03b2\u03bb \u03b1 \u00b5\u03bb \u03b2 \u00b5 J \u03b1 \u22a5 \u2022 J \u03b2 \u22a5 + 2 \u03b1\u03bb \u03b1 \u00b5\u00fb\u00b5 J \u03b1 \u22a5 \u2022 T \u22a5 +\u00fb 2 \u00b5 T \u22a5 2 \u2212 i \u00b5 \u03b1\u03bb \u03b1 \u00b5 J \u03b1 \u2022 J probe +\u00fb \u00b5 T \u2022 J probe z \u00b5 T,z \u00b5(22)", "formula_coordinates": [17.0, 126.46, 408.64, 377.54, 127.27]}, {"formula_id": "formula_18", "formula_text": "q \u03b1\u03b2 = J \u03b1 \u2022 J \u03b2 N , R \u03b1 = T \u2022 J \u03b1 N (23", "formula_coordinates": [17.0, 242.44, 571.55, 257.41, 24.67]}, {"formula_id": "formula_19", "formula_text": ")", "formula_coordinates": [17.0, 499.85, 580.97, 4.15, 8.64]}, {"formula_id": "formula_20", "formula_text": "\u03c1 \u03b1 = J \u03b1 \u2022 J probe N (24", "formula_coordinates": [17.0, 273.14, 659.74, 226.71, 24.82]}, {"formula_id": "formula_21", "formula_text": ")", "formula_coordinates": [17.0, 499.85, 669.31, 4.15, 8.64]}, {"formula_id": "formula_22", "formula_text": "J \u03b1 \u22a5 \u2022 J \u03b2 \u22a5 = J \u03b1 \u2022 J \u03b2 \u2212 J \u03b1 \u2022 J \u03b2 = N (q \u03b1\u03b2 \u2212 \u03c1 \u03b1 \u03c1 \u03b2 ) (25) J \u03b1 \u22a5 \u2022 T \u22a5 = J \u03b1 \u2022 T \u2212 J \u03b1 \u2022 T = N (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8)(26)", "formula_coordinates": [17.0, 210.25, 710.29, 293.75, 13.91]}, {"formula_id": "formula_23", "formula_text": "\u2126 (n) = \u03b1<\u03b2 dq \u03b1\u03b2 \u03b1 dR \u03b1 \u03b1 d\u03c1 \u03b1 \u00d7 n \u03b1=1 d\u00b5(J \u03b1 ) \u03b1 \u03b4(T \u2022 J \u03b1 \u2212 N R \u03b1 ) T \u03b1<\u03b2 \u03b4(J \u03b1 \u2022 J \u03b2 \u2212 N q \u03b1\u03b2 ) \u03b1 \u03b4(J \u03b1 \u2022 J probe \u2212 N \u03c1 \u03b1 ) \u00d7 \u03b1,\u00b5 d\u03bb \u03b1 \u00b5 d\u03bb \u03b1 \u00b5 2\u03c0 \u00b5 du \u00b5 d\u00fb \u00b5 2\u03c0 \u03b1,\u00b5 \u0398 u \u00b5 (\u03bb \u03b1 \u00b5 \u2212 \u03ba) \u00b5 exp i \u03b1 \u03bb \u03b1 \u00b5\u03bb \u03b1 \u00b5 + iu \u00b5\u00fb\u00b5 \u00d7 exp \u2212 1 2 \u03b1\u03b2\u03bb \u03b1 \u00b5\u03bb \u03b2 \u00b5 (q \u03b1\u03b2 \u2212 \u03c1 \u03b1 \u03c1 \u03b2 ) \u2212 \u03b1\u03bb \u03b1 \u00b5\u00fb\u00b5 (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8) \u2212 1 2\u00fb 2 \u00b5 sin 2 \u03b8 \u2212 i \u03b1\u03bb \u03b1 \u00b5 \u03c1 \u03b1 +\u00fb \u00b5 cos \u03b8 z \u00b5 z \u00b5 (27)", "formula_coordinates": [18.0, 109.07, 239.32, 394.93, 167.47]}, {"formula_id": "formula_24", "formula_text": "\u2126 (n) = \u03b1<\u03b2 dq \u03b1\u03b2 \u03b1 dR \u03b1 \u03b1 d\u03c1 \u03b1 \u00d7 n \u03b1=1 d\u00b5(J \u03b1 ) \u03b1 \u03b4(T \u2022 J \u03b1 \u2212 N R \u03b1 ) T \u03b1<\u03b2 \u03b4(J \u03b1 \u2022 J \u03b2 \u2212 N q \u03b1\u03b2 ) \u03b1 \u03b4(J \u03b1 \u2022 J probe \u2212 N \u03c1 \u03b1 ) \u00d7 \u03b1,\u00b5 d\u03bb \u03b1 \u00b5 d\u03bb \u03b1 \u00b5 2\u03c0 \u00b5 du \u00b5 d\u00fb \u00b5 2\u03c0 \u03b1,\u00b5 \u0398 u \u00b5 (\u03bb \u03b1 \u00b5 \u2212 \u03ba) \u00b5 exp i \u03b1 \u03bb \u03b1 \u00b5\u03bb \u03b1 \u00b5 \u00d7 exp \u2212 1 2 \u03b1\u03b2\u03bb \u03b1 \u00b5\u03bb \u03b2 \u00b5 (q \u03b1\u03b2 \u2212 \u03c1 \u03b1 \u03c1 \u03b2 ) \u2212 i \u03b1\u03bb \u03b1 \u00b5 \u03c1 \u03b1 z \u00b5 + 1 2 sin 2 \u03b8 i(u \u00b5 \u2212 z \u00b5 cos \u03b8) \u2212 \u03b1\u03bb \u03b1 \u00b5 (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8) 2 z \u00b5 (28)", "formula_coordinates": [18.0, 109.07, 505.22, 394.93, 169.5]}, {"formula_id": "formula_25", "formula_text": "\u2126 (n) = \u03b1<\u03b2 dq \u03b1\u03b2 \u03b1 dR \u03b1 \u03b1 d\u03c1 \u03b1 \u00d7 n \u03b1=1 d\u00b5(J \u03b1 ) \u03b1 \u03b4(T \u2022 J \u03b1 \u2212 N R \u03b1 ) T \u03b1<\u03b2 \u03b4(J \u03b1 \u2022 J \u03b2 \u2212 N q \u03b1\u03b2 ) \u03b1 \u03b4(J \u03b1 \u2022 J probe \u2212 N \u03c1 \u03b1 ) \u00d7 \u03b1,\u00b5 d\u03bb \u03b1 \u00b5 d\u03bb \u03b1 \u00b5 2\u03c0 \u00b5 du \u00b5 d\u00fb \u00b5 2\u03c0 \u03b1,\u00b5 \u0398 u \u00b5 (\u03bb \u03b1 \u00b5 \u2212 \u03ba) \u00b5 exp i \u03b1 \u03bb \u03b1 \u00b5\u03bb \u03b1 \u00b5 \u2212 1 2 (u \u00b5 \u2212 z \u00b5 cos \u03b8) 2 sin 2 \u03b8 \u00d7 exp \u2212 1 2 \u03b1\u03b2\u03bb \u03b1 \u00b5\u03bb \u03b2 \u00b5 (q \u03b1\u03b2 \u2212 \u03c1 \u03b1 \u03c1 \u03b2 ) \u2212 i \u03b1\u03bb \u03b1 \u00b5 \u03c1 \u03b1 z \u00b5 \u2212 i sin 2 \u03b8 u \u00b5 \u2212 z \u00b5 cos \u03b8) \u03b1\u03bb \u03b1 \u00b5 (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8) + 1 2 sin 2 \u03b8 \u03b1\u03b2\u03bb \u03b1 \u00b5\u03bb \u03b2 \u00b5 (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8)(R \u03b2 \u2212 \u03c1 \u03b2 cos \u03b8) z \u00b5 (29) Simplifying, \u2126 (n) = \u03b1<\u03b2 dq \u03b1\u03b2 \u03b1 dR \u03b1 \u03b1 d\u03c1 \u03b1 \u00d7 n \u03b1=1 d\u00b5(J \u03b1 ) \u03b1 \u03b4(T \u2022 J \u03b1 \u2212 N R \u03b1 ) T \u03b1<\u03b2 \u03b4(J \u03b1 \u2022 J \u03b2 \u2212 N q \u03b1\u03b2 ) \u03b1 \u03b4(J \u03b1 \u2022 J probe \u2212 N \u03c1 \u03b1 ) \u00d7 \u03b1,\u00b5 d\u03bb \u03b1 \u00b5 d\u03bb \u03b1 \u00b5 2\u03c0 \u00b5 du \u00b5 d\u00fb \u00b5 2\u03c0 \u03b1,\u00b5 \u0398 u \u00b5 (\u03bb \u03b1 \u00b5 \u2212 \u03ba) exp i \u00b5,\u03b1 \u03bb \u03b1 \u00b5\u03bb \u03b1 \u00b5 \u2212 1 2 (u \u00b5 \u2212 z \u00b5 cos \u03b8) 2 sin 2 \u03b8 \u00d7 exp \u2212 1 2 \u00b5 \u03b1\u03b2\u03bb \u03b1 \u00b5\u03bb \u03b2 \u00b5 q \u03b1\u03b2 \u2212 \u03c1 \u03b1 \u03c1 \u03b2 \u2212 (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8)(R \u03b2 \u2212 \u03c1 \u03b2 cos \u03b8) sin 2 \u03b8 \u2212 i \u00b5 \u03b1\u03bb \u03b1 \u00b5 \u03c1 \u03b1 z \u00b5 \u2212 i sin 2 \u03b8 u \u00b5 \u2212 z \u00b5 cos \u03b8) \u03b1\u03bb \u03b1 \u00b5 (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8) z \u00b5(30)", "formula_coordinates": [19.0, 108.0, 145.47, 452.03, 489.65]}, {"formula_id": "formula_26", "formula_text": "\u2126 (n) = \u03b1 dk \u03b1 4\u03c0 \u03b1<\u03b2 dq \u03b1\u03b2 dq \u03b1\u03b2 2\u03c0/N \u03b1 dR \u03b1 dR \u03b1 2\u03c0/N \u03b1 d\u03c1 \u03b1 d\u03c1 \u03b1 2\u03c0/N \u00d7 exp i N 2 \u03b1k \u03b1 + iN \u03b1<\u03b2 q \u03b1\u03b2q\u03b1\u03b2 + iN \u03b1 R \u03b1R\u03b1 + iN \u03b1 \u03c1 \u03b1\u03c1\u03b1 \u00d7 i,\u03b1 dJ \u03b1 i \u221a 2\u03c0e exp \u2212 i 2 \u03b1k \u03b1 J \u03b1 2 \u2212 i \u03b1<\u03b2q \u03b1\u03b2 J \u03b1 \u2022 J \u03b2 \u2212 i \u03b1R \u03b1 J \u03b1 \u2022 T \u2212 i \u03b1\u03c1 \u03b1 J \u03b1 \u2022 J probe \u00d7 \u03b1,\u00b5 d\u03bb \u03b1 \u00b5 d\u03bb \u03b1 \u00b5 2\u03c0 \u00b5 du \u00b5 d\u00fb \u00b5 2\u03c0 \u03b1,\u00b5 \u0398 u \u00b5 (\u03bb \u03b1 \u00b5 \u2212 \u03ba) exp i \u00b5,\u03b1 \u03bb \u03b1 \u00b5\u03bb \u03b1 \u00b5 \u2212 1 2 (u \u00b5 \u2212 z \u00b5 cos \u03b8) 2 sin 2 \u03b8 \u00d7 exp \u2212 1 2 \u00b5 \u03b1\u03b2\u03bb \u03b1 \u00b5\u03bb \u03b2 \u00b5 q \u03b1\u03b2 \u2212 \u03c1 \u03b1 \u03c1 \u03b2 \u2212 (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8)(R \u03b2 \u2212 \u03c1 \u03b2 cos \u03b8) sin 2 \u03b8 \u2212 i \u00b5 \u03b1\u03bb \u03b1 \u00b5 \u03c1 \u03b1 z \u00b5 \u2212 i sin 2 \u03b8 u \u00b5 \u2212 z \u00b5 cos \u03b8) \u03b1\u03bb \u03b1 \u00b5 (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8) z \u00b5 (31)", "formula_coordinates": [20.0, 108.0, 87.36, 428.77, 205.22]}, {"formula_id": "formula_27", "formula_text": "\u2126 (n) = k \u03b1 dk \u03b1 4\u03c0 \u03b1<\u03b2 dq \u03b1\u03b2 dq \u03b1\u03b2 2\u03c0/N \u03b1 dR \u03b1 dR \u03b1 2\u03c0/N \u03b1 d\u03c1 \u03b1 d\u03c1 \u03b1 2\u03c0/N \u00d7 exp N i 2 \u03b1k \u03b1 + i \u03b1<\u03b2 q \u03b1\u03b2q\u03b1\u03b2 + i \u03b1 R \u03b1R\u03b1 + i \u03b1 \u03c1 \u03b1\u03c1\u03b1 + G S (k \u03b1 ,q \u03b1\u03b2 ,R \u03b1 ,\u03c1 \u03b1 ) + \u03b1G E (q \u03b1\u03b2 , R \u03b1 , \u03c1 \u03b1 )(32)", "formula_coordinates": [20.0, 162.04, 345.65, 341.96, 83.5]}, {"formula_id": "formula_28", "formula_text": "G S = 1 N log \u03b1 dJ \u03b1 \u221a 2\u03c0e exp \u2212 i 2 \u03b1k \u03b1 J \u03b1 2 \u2212i \u03b1<\u03b2q \u03b1\u03b2 J \u03b1 \u2022J \u03b2 \u2212i \u03b1R \u03b1 J \u03b1 \u2022T\u2212i \u03b1\u03c1 \u03b1 J \u03b1 \u2022J probe (33) And an energetic part G E , G E = log du \u221a 2\u03c0 \u03b1 d\u03bb \u03b1 d\u03bb \u03b1 2\u03c0 \u03b1 \u0398 u(\u03bb \u03b1 \u2212 \u03ba) exp i \u03b1 \u03bb \u03b1\u03bb\u03b1 \u2212 1 2 (u \u00b5 \u2212 z \u00b5 cos \u03b8) 2 sin 2 \u03b8 \u00d7 exp \u2212 1 2 \u03b1\u03b2\u03bb \u03b1\u03bb\u03b2 q \u03b1\u03b2 \u2212 \u03c1 \u03b1 \u03c1 \u03b2 \u2212 (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8)(R \u03b2 \u2212 \u03c1 \u03b2 cos \u03b8) sin 2 \u03b8 \u2212 i \u03b1\u03bb \u03b1 \u03c1 \u03b1 z \u2212 i sin 2 \u03b8 u \u2212 z cos \u03b8) \u03b1\u03bb \u03b1 (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8) z (34", "formula_coordinates": [20.0, 107.64, 482.34, 408.47, 171.48]}, {"formula_id": "formula_29", "formula_text": ")", "formula_coordinates": [20.0, 499.85, 634.53, 4.15, 8.64]}, {"formula_id": "formula_30", "formula_text": "A \u03b1\u03b2 = ik \u03b1 \u03b4 \u03b1\u03b2 + iq \u03b1\u03b2 (1 \u2212 \u03b4 \u03b1\u03b2 )(35)", "formula_coordinates": [20.0, 240.17, 691.64, 263.83, 11.72]}, {"formula_id": "formula_31", "formula_text": "B \u03b1\u03b2 = \u03b4 \u03b1\u03b2 + q \u03b1\u03b2 (1 \u2212 \u03b4 \u03b1\u03b2 )(36)", "formula_coordinates": [20.0, 240.08, 707.85, 263.92, 11.72]}, {"formula_id": "formula_32", "formula_text": "G S = 1 N log \u03b1 dJ \u03b1 \u221a 2\u03c0e exp \u2212 1 2 \u03b1,\u03b2 J \u03b1T A \u03b1\u03b2 J \u03b2 \u2212 i \u03b1 J \u03b1 \u2022 (TR \u03b1 + J probe\u03c1 \u03b1 ) (37) Integrating over J \u03b1 , G S = \u2212 n 2 \u2212 1 2 log(det A) \u2212 1 2N \u03b1,\u03b2 (TR \u03b1 + J probe\u03c1 \u03b1 ) T A \u22121 \u03b1\u03b2 (TR \u03b2 + J probe\u03c1 \u03b2 )(38)", "formula_coordinates": [21.0, 108.0, 100.07, 396.0, 90.19]}, {"formula_id": "formula_33", "formula_text": "= \u03b1\u03b2 A \u03b1\u03b2 B \u03b2\u03b1 (39) = \u03b1\u03b2 (ik \u03b1 \u03b4 \u03b1\u03b2 + iq \u03b1\u03b2 (1 \u2212 \u03b4 \u03b1\u03b2 ))(\u03b4 \u03b1\u03b2 + q \u03b1\u03b2 (1 \u2212 \u03b4 \u03b1\u03b2 ))(40)", "formula_coordinates": [21.0, 211.58, 224.89, 292.42, 48.92]}, {"formula_id": "formula_34", "formula_text": "= \u03b1 ik \u03b1 + 2 \u03b1<\u03b2 iq \u03b1\u03b2q\u03b1\u03b2 (41)", "formula_coordinates": [21.0, 211.58, 280.38, 292.42, 22.21]}, {"formula_id": "formula_35", "formula_text": "\u2212 n 2 \u2212 1 2 tr(log A)\u2212 1 2N \u03b1,\u03b2 (TR \u03b1 +J probe\u03c1 \u03b1 ) T A \u22121 \u03b1\u03b2 (TR \u03b2 +J probe\u03c1 \u03b2 )+ 1 2 tr(AB)+i \u03b1 R \u03b1R\u03b1 +i \u03b1 \u03c1 \u03b1\u03c1\u03b1 J probe (42)", "formula_coordinates": [21.0, 108.0, 349.83, 442.46, 37.78]}, {"formula_id": "formula_36", "formula_text": "0 = \u2212 \u03b1 A \u22121 \u03b1\u03b3 T \u2022 (TR \u03b1 + J probe\u03c1 \u03b1 ) + iR \u03b3 = \u2212 \u03b1 A \u22121 \u03b1\u03b3 (R \u03b1 +\u03c1 \u03b1 cos \u03b8) + iR \u03b3 (43) 0 = \u2212 \u03b1 A \u22121 \u03b1\u03b3 J probe \u2022 (TR \u03b1 + J probe\u03c1 \u03b1 ) + i\u03c1 \u03b3 = \u2212 \u03b1 A \u22121 \u03b1\u03b3 (R \u03b1 cos \u03b8 +\u03c1 \u03b1 ) + i\u03c1 \u03b3 (44) 0 = \u2212 1 2 A \u22121 \u03b3\u03b4 + 1 2 \u03b1,\u03b2 (TR \u03b1 + J probe\u03c1 \u03b1 ) T A \u22121 \u03b1\u03b3 A \u22121 \u03b2\u03b4 (TR \u03b2 + J probe\u03c1 \u03b2 ) + 1 2 B \u03b3\u03b4(45)", "formula_coordinates": [21.0, 129.27, 434.67, 374.73, 78.71]}, {"formula_id": "formula_37", "formula_text": "\u03b1 = i \u03b2 A \u03b1\u03b2 R \u03b2 \u2212 \u03c1 \u03b2 cos \u03b8 sin 2 \u03b8 (46", "formula_coordinates": [21.0, 250.9, 547.26, 414.26, 28.45]}, {"formula_id": "formula_38", "formula_text": ")", "formula_coordinates": [21.0, 665.15, 555.89, 59.25, 8.64]}, {"formula_id": "formula_39", "formula_text": "\u03c1 \u03b1 = i \u03b2 A \u03b1\u03b2 \u03c1 \u03b2 \u2212 R \u03b2 cos \u03b8 sin 2 \u03b8 (47", "formula_coordinates": [21.0, 244.5, 585.37, 255.35, 28.45]}, {"formula_id": "formula_40", "formula_text": ")", "formula_coordinates": [21.0, 499.85, 594.01, 4.15, 8.64]}, {"formula_id": "formula_41", "formula_text": "A \u22121 \u03b3\u03b4 = B \u03b3\u03b4 \u2212 R \u03b3 R \u03b4 \u2212 R \u03b3 \u03c1 \u03b4 cos \u03b8 \u2212 R \u03b4 \u03c1 \u03b3 cos \u03b8 + \u03c1 \u03b3 \u03c1 \u03b4 sin 2 \u03b8 \u2261 C \u03b3\u03b4 (48", "formula_coordinates": [21.0, 175.59, 646.64, 324.26, 24.64]}, {"formula_id": "formula_42", "formula_text": ")", "formula_coordinates": [21.0, 499.85, 655.27, 4.15, 8.64]}, {"formula_id": "formula_43", "formula_text": "\u2126 (n) \u223c exp N extr q \u03b1\u03b2 ,R \u03b1 ,\u03c1 \u03b1 1 2 tr(log C) + \u03b1G E (q \u03b1\u03b2 , R \u03b1 )(49)", "formula_coordinates": [21.0, 177.44, 701.97, 326.56, 22.31]}, {"formula_id": "formula_44", "formula_text": "q \u03b1\u03b2 = q, R \u03b1 = R, \u03c1 \u03b1 = \u03c1 (50)", "formula_coordinates": [22.0, 243.13, 119.99, 260.87, 11.03]}, {"formula_id": "formula_45", "formula_text": "C \u03b1\u03b2 = \u03b4 \u03b1\u03b2 \u2212 R 2 \u2212 2R\u03c1 cos \u03b8 + \u03c1 2 sin 2 \u03b8 + q(1 \u2212 \u03b4 \u03b1\u03b2 )(51)", "formula_coordinates": [22.0, 204.58, 167.85, 299.42, 24.64]}, {"formula_id": "formula_46", "formula_text": "1 \u2212 R 2 \u2212 2R\u03c1 cos \u03b8 + \u03c1 2 sin 2 \u03b8 \u2212 q \u2212 R 2 \u2212 2R\u03c1 cos \u03b8 + \u03c1 2 sin 2 \u03b8 = 1 \u2212 q (52)", "formula_coordinates": [22.0, 170.02, 243.44, 333.98, 24.64]}, {"formula_id": "formula_47", "formula_text": "1\u2212 R 2 \u2212 2R\u03c1 cos \u03b8 + \u03c1 2 sin 2 \u03b8 +(n\u22121) q\u2212 R 2 \u2212 2R\u03c1 cos \u03b8 + \u03c1 2 sin 2 \u03b8 = 1\u2212q+n q\u2212 R 2 \u2212 2R\u03c1 cos \u03b8 + \u03c1 2 sin 2 \u03b8 (53", "formula_coordinates": [22.0, 115.33, 296.35, 407.82, 34.49]}, {"formula_id": "formula_48", "formula_text": ")", "formula_coordinates": [22.0, 499.85, 322.2, 4.15, 8.64]}, {"formula_id": "formula_49", "formula_text": "Therefore, tr(log C) = (n \u2212 1) log 1 \u2212 q + log 1 \u2212 q + n q \u2212 R 2 \u2212 2R\u03c1 cos \u03b8 + \u03c1 2 sin 2 \u03b8 = n log 1 \u2212 q + log 1 + n q sin 2 \u03b8 \u2212 (R 2 \u2212 2R\u03c1 cos \u03b8 + \u03c1 2 ) (1 \u2212 q) sin 2 \u03b8 (54", "formula_coordinates": [22.0, 107.69, 338.58, 392.16, 84.59]}, {"formula_id": "formula_50", "formula_text": ")", "formula_coordinates": [22.0, 499.85, 392.83, 4.15, 8.64]}, {"formula_id": "formula_51", "formula_text": "G E = log du \u221a 2\u03c0 \u03b1 d\u03bb \u03b1 d\u03bb \u03b1 2\u03c0 \u03b1 \u0398 u(\u03bb \u03b1 \u2212 \u03ba) exp i \u03b1 \u03bb \u03b1\u03bb\u03b1 \u2212 1 2 (u \u2212 z cos \u03b8) 2 sin 2 \u03b8 \u00d7 exp \u2212 1 2 \u03b1 (\u03bb \u03b1 ) 2 1 \u2212 \u03c1 2 \u2212 (R \u2212 \u03c1 cos \u03b8) 2 sin 2 \u03b8 \u2212 1 2 \u03b1 =\u03b2\u03bb \u03b1\u03bb\u03b2 q \u2212 \u03c1 2 \u2212 (R \u2212 \u03c1 cos \u03b8) 2 sin 2 \u03b8 \u2212 i \u03b1\u03bb \u03b1 \u03c1 \u03b1 z \u2212 i sin 2 \u03b8 u \u2212 z cos \u03b8) \u03b1\u03bb \u03b1 (R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8) z (55", "formula_coordinates": [22.0, 108.0, 470.51, 399.46, 102.41]}, {"formula_id": "formula_52", "formula_text": ")", "formula_coordinates": [22.0, 499.85, 564.29, 4.15, 8.64]}, {"formula_id": "formula_53", "formula_text": "\u2212 1 2 \u03b1 1 \u2212 \u03c1 2 \u2212 (R \u2212 \u03c1 cos \u03b8) sin 2 \u03b8 (\u03bb \u03b1 ) 2 \u2212 1 2 \u03b1 =\u03b2\u03bb \u03b1\u03bb\u03b2 q \u2212 \u03c1 2 \u2212 (R \u2212 \u03c1 cos \u03b8) 2 sin 2 \u03b8 = \u2212 1 2 \u03b1 1 \u2212 \u03c1 2 \u2212 (R \u2212 \u03c1 cos \u03b8) 2 sin 2 \u03b8 (\u03bb \u03b1 ) 2 \u2212 1 2 q \u2212 \u03c1 2 \u2212 (R \u2212 \u03c1 cos \u03b8) 2 sin 2 \u03b8 ( \u03b1\u03bb \u03b1 ) 2 \u2212 \u03b1 (\u03bb \u03b1 ) 2 = \u2212 1 2 \u03b1 (1 \u2212 q)(\u03bb \u03b1 ) 2 \u2212 1 2 q \u2212 \u03c1 2 \u2212 (R \u2212 \u03c1 cos \u03b8) 2 sin 2 \u03b8 (\u03b1\u03bb \u03b1 ) 2", "formula_coordinates": [22.0, 110.21, 619.34, 421.36, 92.39]}, {"formula_id": "formula_54", "formula_text": "= log du \u221a 2\u03c0 \u03b1 d\u03bb \u03b1 d\u03bb \u03b1 2\u03c0 \u03b1 \u0398 u(\u03bb \u03b1 \u2212 \u03ba) Dt exp \u2212 1 \u2212 q 2 \u03b1 (\u03bb \u03b1 ) 2 +i \u03b1\u03bb \u03b1 \u03bb \u03b1 \u2212\u03c1 \u03b1 z\u2212 u \u2212 z cos \u03b8)(R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8) sin 2 \u03b8 \u2212 q \u2212 \u03c1 2 \u2212 (R \u2212 \u03c1 cos \u03b8) 2 sin 2 \u03b8 t \u2212 1 2 (u \u2212 z cos \u03b8) 2 sin 2 \u03b8 z(57)", "formula_coordinates": [23.0, 108.0, 119.41, 441.87, 71.39]}, {"formula_id": "formula_55", "formula_text": "= log 2 \u221e 0 du \u221a 2\u03c0 \u221e \u03ba \u03b1 d\u03bb \u03b1 \u221a 2\u03c0 \u03b1 d\u03bb \u03b1 \u221a 2\u03c0 Dt exp \u2212 1 \u2212 q 2 \u03b1 (\u03bb \u03b1 ) 2 +i \u03b1\u03bb \u03b1 \u03bb \u03b1 \u2212\u03c1 \u03b1 z\u2212 u \u2212 z cos \u03b8)(R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8) sin 2 \u03b8 \u2212 q \u2212 \u03c1 2 \u2212 (R \u2212 \u03c1 cos \u03b8) 2 sin 2 \u03b8 t \u2212 1 2 (u \u2212 z cos \u03b8) 2 sin 2 \u03b8 z(58)", "formula_coordinates": [23.0, 108.0, 235.99, 441.87, 72.14]}, {"formula_id": "formula_56", "formula_text": "= log 2 \u221e 0 du \u221a 2\u03c0 \u221e \u03ba \u03b1 d\u03bb \u03b1 \u221a 2\u03c0 Dt exp \u2212 1 2(1 \u2212 q) \u03bb \u03b1 \u2212 \u03c1 \u03b1 z \u2212 u \u2212 z cos \u03b8)(R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8) sin 2 \u03b8 \u2212 q \u2212 \u03c1 2 \u2212 (R \u2212 \u03c1 cos \u03b8) 2 sin 2 \u03b8 t 2 \u2212 1 2 (u \u2212 z cos \u03b8) 2 sin 2 \u03b8 z (59", "formula_coordinates": [23.0, 112.84, 354.21, 387.01, 60.21]}, {"formula_id": "formula_57", "formula_text": ")", "formula_coordinates": [23.0, 499.85, 396.63, 4.15, 8.64]}, {"formula_id": "formula_58", "formula_text": "And \u03bb \u03b1 , = log 2 \u221e 0 du \u221a 2\u03c0 Dt H n \u2212 1 \u221a 1 \u2212 q \u03ba\u2212\u03c1 \u03b1 z+ u \u2212 z cos \u03b8)(R \u03b1 \u2212 \u03c1 \u03b1 cos \u03b8) sin 2 \u03b8 + q \u2212 \u03c1 2 \u2212 (R \u2212 \u03c1 cos \u03b8) 2 sin 2 \u03b8 t 2 \u00d7 exp \u2212 1 2 (u \u2212 z cos \u03b8) 2 sin 2 \u03b8 z(60)", "formula_coordinates": [23.0, 107.64, 423.08, 477.74, 89.96]}, {"formula_id": "formula_59", "formula_text": "sin 2 \u03b8 + q \u2212 \u03c1 2 \u2212 (R\u2212\u03c1 cos \u03b8) 2 sin 2 \u03b8 t)/ \u221a q,", "formula_coordinates": [23.0, 317.62, 523.22, 187.62, 17.91]}, {"formula_id": "formula_60", "formula_text": "= log 2 dt \u221a 2\u03c0 exp \u2212 ( \u221a qt \u2212 z\u03c1) 2 2(q \u2212 \u03c1) 2 q q \u2212 \u03c1 2 H n \u2212 q 1 \u2212 q t \u00d7 H 1 q \u2212 \u03c1 2 \u03ba \u2212 (qR 0 z + z\u03c1(R \u2212 2\u03c1 cos \u03b8) \u2212 \u221a qt(R \u2212 \u03c1 cos \u03b8)) q sin 2 \u03b8 + 2R\u03c1 cos \u03b8 \u2212 R 2 \u2212 \u03c1 z (61)", "formula_coordinates": [23.0, 120.73, 569.71, 383.27, 60.16]}, {"formula_id": "formula_61", "formula_text": "\u221a qt \u2212 z\u03c1)/ q \u2212 \u03c1 2 , = log 2 Dt H n \u03ba \u2212 (z\u03c1 + q \u2212 \u03c1 2 )t \u221a 1 \u2212 q H t q \u2212 \u03c1 2 + \u03c1z (R \u2212 \u03c1 cos \u03b8) + qz cos \u03b8 \u2212 \u03c1Rz \u2212 (q \u2212 \u03c1 2 ) \u03c1 2 \u2212 q sin 2 \u03b8 + R 2 \u2212 2\u03c1R cos \u03b8 z (62)", "formula_coordinates": [23.0, 108.0, 637.81, 427.84, 84.35]}, {"formula_id": "formula_62", "formula_text": "1 N log \u2126 = extr q,R,\u03c1 1 2 log 1 \u2212 q + 1 2 q \u2212 (R \u2212 2R\u03c1 cos \u03b8 + \u03c1 2 )/ sin 2 \u03b8 1 \u2212 q + 2\u03b1 Dt log H \u03ba \u2212 (z\u03c1 + q \u2212 \u03c1 2 )t \u221a 1 \u2212 q \u00d7 H t q \u2212 \u03c1 2 + \u03c1z (R \u2212 \u03c1 cos \u03b8) + z(q cos \u03b8 \u2212 \u03c1R) (q \u2212 \u03c1 2 ) R 2 + \u03c1 2 \u2212 q sin 2 \u03b8 \u2212 2\u03c1R cos \u03b8 z (63", "formula_coordinates": [24.0, 122.55, 142.03, 380.69, 93.94]}, {"formula_id": "formula_63", "formula_text": ")", "formula_coordinates": [24.0, 503.24, 216.24, 4.15, 8.64]}, {"formula_id": "formula_64", "formula_text": "H t q \u2212 \u03c1 2 + \u03c1z (R \u2212 \u03c1 cos \u03b8) + z(q cos \u03b8 \u2212 \u03c1R) (q \u2212 \u03c1 2 ) R 2 + \u03c1 2 \u2212 q sin 2 \u03b8 \u2212 2\u03c1R cos \u03b8 \u2192 \u0398(z)(64)", "formula_coordinates": [24.0, 170.65, 358.43, 333.35, 31.25]}, {"formula_id": "formula_65", "formula_text": "1 N ln \u2126(x \u00b5 , T, \u03ba) = extr q,R 1 2 log 1 \u2212 q + 1 2 q \u2212 R 2 1 \u2212 q + 2\u03b1 Dt dzp(z)\u0398(z) log H \u2212 q \u2212 R 2 t + Rz \u2212 \u03ba \u221a 1 \u2212 q(65)", "formula_coordinates": [24.0, 122.55, 444.98, 384.84, 53.76]}, {"formula_id": "formula_66", "formula_text": "R = 2\u03b1 \u03ba \u2212\u221e dt \u221a 2\u03c0 \u221a 1 \u2212 R 2 \u221e 0 dzp(z) exp \u2212 (t \u2212 Rz) 2 2(1 \u2212 R 2 ) z \u2212 Rt 1 \u2212 R 2 (\u03ba \u2212 t)(66)", "formula_coordinates": [24.0, 135.94, 597.86, 371.45, 26.29]}, {"formula_id": "formula_67", "formula_text": "1 \u2212 R 2 = 2\u03b1 \u03ba \u2212\u221e dt \u221a 2\u03c0 \u221a 1 \u2212 R 2 \u221e 0 dzp(z) exp \u2212 (t \u2212 Rz) 2 2(1 \u2212 R 2 ) \u03ba \u2212 t 2 (67)", "formula_coordinates": [24.0, 153.66, 628.6, 353.73, 26.29]}, {"formula_id": "formula_68", "formula_text": "= e \u2212z 2 /2 \u221a 2\u03c0f \u0398(\u03b3 \u2212 |z|), where the threshold \u03b3 = H \u22121 1\u2212f 2 . R = 2\u03b1 f \u221a 2\u03c0 \u221a 1 \u2212 R 2 \u03ba \u2212\u221e Dt exp \u2212 R 2 t 2 2(1 \u2212 R 2 ) 1 \u2212 exp \u2212 \u03b3(\u03b3 \u2212 2Rt) 2(1 \u2212 R 2 ) (\u03ba \u2212 t) (68) 1 \u2212 R 2 = 2\u03b1 f \u03ba \u2212\u221e Dt H \u2212 Rt \u221a \u2212 R 2 \u2212 H \u2212 Rt \u2212 \u03b3 \u221a 1 \u2212 R 2 (\u03ba \u2212 t) 2 (69)", "formula_coordinates": [24.0, 266.37, 706.74, 234.84, 18.24]}, {"formula_id": "formula_69", "formula_text": "R = 2\u03b1 \u221a 2\u03c0 \u221a 1 \u2212 R 2 Dt exp \u2212 R 2 t 2 2(1 \u2212 R 2 ) (\u03ba \u2212 t) (70", "formula_coordinates": [25.0, 193.68, 214.53, 306.18, 25.05]}, {"formula_id": "formula_70", "formula_text": ")", "formula_coordinates": [25.0, 499.85, 223.16, 4.15, 8.64]}, {"formula_id": "formula_71", "formula_text": "1 \u2212 R 2 = 2\u03b1 Dt H \u2212 Rt \u221a 1 \u2212 R 2 \u03ba \u2212 t 2(71)", "formula_coordinates": [25.0, 209.57, 249.04, 294.43, 23.48]}, {"formula_id": "formula_72", "formula_text": "I(\u03b1) = \u22122 Dt H R 1 \u2212 R t log H R 1 \u2212 R t (72", "formula_coordinates": [25.0, 196.78, 399.1, 303.07, 22.31]}, {"formula_id": "formula_73", "formula_text": ")", "formula_coordinates": [25.0, 499.85, 406.16, 4.15, 8.64]}, {"formula_id": "formula_74", "formula_text": "S(\u03b1) = 1 N log \u2126 = extr q,R 1 2 log 1\u2212R + 1 2 R+2\u03b1 Dt \u221e 0 dz p(z) log H \u2212 \u221a Rt\u2212 R \u221a 1 \u2212 R z(73)", "formula_coordinates": [25.0, 108.0, 488.03, 407.16, 35.23]}, {"formula_id": "formula_75", "formula_text": "I(\u03b1) = 2\u03b1 Dt dzp(z)\u0398(z) log H \u2212 \u221a Rt \u2212 R \u221a 1 \u2212 R z (74)", "formula_coordinates": [25.0, 178.13, 553.6, 325.87, 25.38]}, {"formula_id": "formula_76", "formula_text": "\u221a Rt + R \u221a 1\u2212R z)/ \u221a q, I(\u03b1) = 2\u03b1 Dt \u221e 0 dzp(z) 1 \u221a 1 \u2212 R 1 \u221a 2\u03c0 exp \u2212 z 2 + 2 \u221a Rtz 2(1 \u2212 R) log H R 1 \u2212 R t (75)", "formula_coordinates": [25.0, 119.6, 580.36, 384.4, 61.46]}, {"formula_id": "formula_77", "formula_text": ") = \u0398 |z| \u2212 \u03b3 exp(\u2212z 2 /2) \u221a 2\u03c0f", "formula_coordinates": [25.0, 343.17, 649.32, 108.35, 17.14]}, {"formula_id": "formula_78", "formula_text": "H \u22121 1\u2212f 2 I(\u03b1) = 2\u03b1 f Dt H R 1 \u2212 R t \u2212 H \u03b3 + \u221a Rt \u221a 1 \u2212 R log H R 1 \u2212 R t(76)", "formula_coordinates": [25.0, 108.0, 672.69, 396.0, 52.48]}, {"formula_id": "formula_79", "formula_text": "I(\u03b1) = \u2212 Dt log H( \u221a Rt)(77)", "formula_coordinates": [26.0, 245.59, 168.12, 258.41, 17.88]}, {"formula_id": "formula_80", "formula_text": "E|T \u2022 x| 2 = E[x 2 0 cos 2 \u03b8 + x 2 1 sin 2 \u03b8](78)", "formula_coordinates": [26.0, 233.12, 376.01, 270.88, 12.77]}, {"formula_id": "formula_81", "formula_text": "E|T \u2022 x| 2 = sin 2 \u03b8 (79", "formula_coordinates": [26.0, 269.33, 434.45, 230.52, 11.45]}, {"formula_id": "formula_82", "formula_text": ")", "formula_coordinates": [26.0, 499.85, 436.92, 4.15, 8.64]}, {"formula_id": "formula_83", "formula_text": "\u03b3min \u2212\u03b3min dxp(x)x 2 = 1 2 \u2212 e \u2212\u03b3 2 min /2 \u03b3 min \u221a 2\u03c0(1 \u2212 2H(\u03b3 min )) = 1 \u2212 R 2 (80", "formula_coordinates": [26.0, 198.44, 524.49, 301.41, 27.69]}, {"formula_id": "formula_84", "formula_text": ")", "formula_coordinates": [26.0, 499.85, 534.75, 4.15, 8.64]}, {"formula_id": "formula_85", "formula_text": "\u03b3 min = H \u22121 1\u2212fmin 2 .", "formula_coordinates": [26.0, 135.9, 567.23, 95.0, 14.0]}, {"formula_id": "formula_86", "formula_text": "f min (\u03b8) \u223c \u03b8 (81)", "formula_coordinates": [26.0, 282.42, 636.08, 221.58, 9.81]}, {"formula_id": "formula_87", "formula_text": "L = R + \u00b5 R \u2212 2\u03b1 \u221e 0 dz p(z)\u03d5(z; R, k) + \u03bb 1 \u2212 R 2 \u2212 2\u03b1 \u221e 0 dz p(z)\u03c8(z; R, k) . (82", "formula_coordinates": [27.0, 113.84, 453.38, 386.01, 26.29]}, {"formula_id": "formula_88", "formula_text": ")", "formula_coordinates": [27.0, 499.85, 462.76, 4.15, 8.64]}, {"formula_id": "formula_89", "formula_text": "\u03d5(z; R, k) = \u03ba \u2212\u221e dt \u221a 2\u03c0 \u221a 1 \u2212 R 2 exp \u2212 (t \u2212 Rz) 2 2(1 \u2212 R 2 ) z \u2212 Rt 1 \u2212 R 2 (\u03ba \u2212 t),(83)", "formula_coordinates": [27.0, 154.22, 522.42, 349.78, 26.29]}, {"formula_id": "formula_90", "formula_text": "\u03c8(z; R, k) = \u03ba \u2212\u221e dt \u221a 2\u03c0 \u221a 1 \u2212 R 2 exp \u2212 (t \u2212 Rz) 2 2(1 \u2212 R 2 ) \u03ba \u2212 t 2 (84)", "formula_coordinates": [27.0, 153.89, 577.45, 350.11, 26.29]}, {"formula_id": "formula_91", "formula_text": "R = 2\u03b1 f \u03c0/2 \u221a 1 \u2212 R 2 \u03ba \u2212\u221e Dt exp \u2212 (a \u2212 Rt) 2 2(1 \u2212 R 2 ) \u2212 exp \u2212 (b \u2212 Rt) 2 2(1 \u2212 R 2 ) (\u03ba \u2212 t) (85) 1 \u2212 R 2 = 4\u03b1 f \u03ba \u2212\u221e Dt H a \u2212 Rt \u221a 1 \u2212 R 2 \u2212 H b \u2212 Rt \u221a 1 \u2212 R 2 (\u03ba \u2212 t) 2 (86)", "formula_coordinates": [28.0, 122.15, 147.02, 381.85, 64.51]}, {"formula_id": "formula_92", "formula_text": "L = R + \u00b5 R \u2212 2\u03b1 \u221e 0 dz p(z)\u03d5(z; R, k) + \u03bb 1 \u2212 R 2 \u2212 2\u03b1 \u221e 0 dz p(z)\u03c8(z; R, k) , (87", "formula_coordinates": [28.0, 113.84, 260.34, 386.01, 26.29]}, {"formula_id": "formula_93", "formula_text": ")", "formula_coordinates": [28.0, 499.85, 269.72, 4.15, 8.64]}, {"formula_id": "formula_94", "formula_text": "\u03c6(b; R, k) = exp \u2212 (a \u2212 Rt) 2 2(1 \u2212 R 2 ) \u2212 exp \u2212 (b \u2212 Rt) 2 2(1 \u2212 R 2 ) (\u03ba \u2212 t)(88)", "formula_coordinates": [28.0, 169.34, 326.1, 334.66, 23.89]}, {"formula_id": "formula_95", "formula_text": "\u03c8(b; R, k) = H a \u2212 Rt \u221a 1 \u2212 R 2 \u2212 H b \u2212 Rt \u221a 1 \u2212 R 2 (\u03ba \u2212 t) 2 (89)", "formula_coordinates": [28.0, 168.43, 355.57, 335.57, 23.48]}, {"formula_id": "formula_96", "formula_text": "Derivative wrt R R \u2212 \u03c1 cos \u03b8 (1 \u2212 q) sin 2 \u03b8 = dt dz p(z) \u221a 2\u03b1 \u03c0 t q \u2212 \u03c1 2 \u221a 2 (\u03c1 2 \u2212 q)\u039b \u2212 (R \u2212 \u03c1 cos \u03b8)\u0393(t, z) \u221a 2 q \u2212 \u03c1 2 \u039b 3 (90) \u00d7 log H \u03ba \u2212 t q \u2212 \u03c1 2 \u2212 \u03c1z \u221a 1 \u2212 q exp \u2212 \u0393(t, z) 2 2 (q \u2212 \u03c1 2 ) \u039b 2 \u2212 t 2 2 (91)", "formula_coordinates": [28.0, 108.0, 489.56, 396.0, 79.43]}, {"formula_id": "formula_97", "formula_text": "\u039b = q sin 2 \u03b8 \u2212 R 2 \u2212 \u03c1 2 + 2R\u03c1 cos \u03b8,(92)", "formula_coordinates": [28.0, 225.82, 600.8, 278.18, 11.11]}, {"formula_id": "formula_98", "formula_text": "\u0393(t, z) = (R \u2212 \u03c1 cos \u03b8) t q \u2212 \u03c1 2 + \u03c1z + qz cos \u03b8 \u2212 \u03c1Rz. (93", "formula_coordinates": [28.0, 182.36, 630.52, 317.49, 9.79]}, {"formula_id": "formula_99", "formula_text": ")", "formula_coordinates": [28.0, 499.85, 631.68, 4.15, 8.64]}, {"formula_id": "formula_100", "formula_text": "R \u2212 \u03c1 cos \u03b8 (1 \u2212 q) sin 2 \u03b8 = \u2212 dt dz p(z) \u03b1 \u03c0\u039b q \u2212 \u03c1 2 e \u2212 ( \u03ba\u2212t \u221a q\u2212\u03c1 2 \u2212\u03c1z ) 2 2(1\u2212q) \u221a 2\u03c0 \u221a 1 \u2212 qH \u03ba\u2212t \u221a q\u2212\u03c1 2 \u2212\u03c1z \u221a 1\u2212q exp \u2212 \u2206(t, z) 2\u039b 2 (94", "formula_coordinates": [28.0, 116.61, 674.53, 383.24, 46.22]}, {"formula_id": "formula_101", "formula_text": ")", "formula_coordinates": [28.0, 499.85, 695.68, 4.15, 8.64]}, {"formula_id": "formula_102", "formula_text": "\u2206(t, z) = 2tz q \u2212 \u03c1 2 (R\u2212\u03c1 cos \u03b8) cos \u03b8 +qt 2 sin 2 \u03b8 +qz 2 cos 2 \u03b8 \u2212\u03c1 2 t 2 sin 2 \u03b8 \u2212\u03c1 2 z 2 cos 2 \u03b8 (95)", "formula_coordinates": [29.0, 112.98, 107.5, 391.02, 11.11]}, {"formula_id": "formula_103", "formula_text": "q \u2192 1, R \u2212 \u03c1 cos \u03b8 sin 2 \u03b8 = \u03ba \u2212\u221e dt \u03b1 \u03c0\u039b exp \u2212 \u2206(t, z) 2\u039b 2 (\u03ba \u2212 t) z (96", "formula_coordinates": [29.0, 190.76, 135.35, 309.09, 50.28]}, {"formula_id": "formula_104", "formula_text": ")", "formula_coordinates": [29.0, 499.85, 167.83, 4.15, 8.64]}, {"formula_id": "formula_105", "formula_text": "\u039b = q sin 2 \u03b8 \u2212 R 2 \u2212 \u03c1 2 + 2\u03c1R cos \u03b8 (97", "formula_coordinates": [29.0, 227.2, 224.25, 272.65, 11.11]}, {"formula_id": "formula_106", "formula_text": ")", "formula_coordinates": [29.0, 499.85, 226.72, 4.15, 8.64]}, {"formula_id": "formula_107", "formula_text": "\u2206(t, z) = z 2 \u03c1 2 + cos 2 \u03b8 \u2212 2\u03c1R cos \u03b8 + 2tz(R cos \u03b8 \u2212 \u03c1) + t 2 sin 2 \u03b8 (98", "formula_coordinates": [29.0, 163.04, 256.19, 336.81, 11.11]}, {"formula_id": "formula_108", "formula_text": ")", "formula_coordinates": [29.0, 499.85, 258.66, 4.15, 8.64]}, {"formula_id": "formula_109", "formula_text": "Derivative wrt q, q \u2212 (\u03c1 2 + R 2 \u2212 2\u03c1R cos \u03b8)/ sin 2 \u03b8 2(1 \u2212 q) 2 = dt dz p(z) 2\u03b1 \u03c0 \u03ba \u2212 t q \u2212 \u03c1 2 \u2212 \u03c1z (1 \u2212 q) 3/2 \u2212 t \u221a 1 \u2212 q q \u2212 \u03c1 2 (99) \u00d7 exp \uf8eb \uf8ec \uf8ed\u2212 \u03ba \u2212 t q \u2212 \u03c1 2 \u2212 \u03c1z 2 2(1 \u2212 q) \u2212 t 2 2 \uf8f6 \uf8f7 \uf8f8 (100", "formula_coordinates": [29.0, 108.0, 281.63, 405.41, 104.97]}, {"formula_id": "formula_110", "formula_text": ")", "formula_coordinates": [29.0, 499.68, 371.35, 4.32, 8.64]}, {"formula_id": "formula_111", "formula_text": "\u00d7H \uf8eb \uf8ed \u2212 (R \u2212 \u03c1 cos \u03b8) t q \u2212 \u03c1 2 + \u03c1z + qz cos \u03b8 \u2212 \u03c1Rz (\u03c1 2 \u2212 q) \u03c1 2 \u2212 q sin 2 \u03b8 + R 2 \u2212 2\u03c1R cos \u03b8 \uf8f6 \uf8f8 H \u03ba \u2212 t q \u2212 \u03c1 2 \u2212 \u03c1z \u221a 1 \u2212 q (101) \u2212 \u221a 2\u03b1 \u03c0 t(R\u2212\u03c1 cos \u03b8) 2 \u221a q\u2212\u03c1 2 + z cos \u221a 2 (q \u2212 \u03c1 2 )\u039b \u2212 q \u2212 \u03c1 2 sin 2 \u03b8 + \u039b 2 \u0393(t, z) 2 \u221a 2 (\u03c1 2 \u2212 q) 3/2 \u039b 3 (102) \u00d7 log H \u03ba \u2212 t q \u2212 \u03c1 2 \u2212 \u03c1z \u221a 1 \u2212 q exp \uf8eb \uf8ec \uf8ed\u2212 (R \u2212 \u03c1 cos \u03b8) t q \u2212 \u03c1 2 + \u03c1z + qz cos \u03b8 \u2212 \u03c1Rz 2 2 (q \u2212 \u03c1 2 ) \u039b 2 \u2212 t 2 2 \uf8f6 \uf8f7 \uf8f8 (103", "formula_coordinates": [29.0, 108.0, 416.64, 438.38, 157.86]}, {"formula_id": "formula_112", "formula_text": ")", "formula_coordinates": [29.0, 499.68, 565.86, 4.32, 8.64]}, {"formula_id": "formula_113", "formula_text": "q \u2212 (\u03c1 2 + R 2 \u2212 2\u03c1R cos \u03b8)/ sin 2 \u03b8 2(1 \u2212 q) 2 = dt dz p(z) \u03b1 exp \u2212 2t \u221a q\u2212\u03c1 2 (\u03c1z\u2212\u03ba)\u2212(\u03c1 2 \u22121)t 2 +(\u03ba\u2212\u03c1z) 2 2(1\u2212q) 4\u03c0H \u03ba\u2212t \u221a q\u2212\u03c1 2 \u2212\u03c1z \u221a 1\u2212q 2 (104) \u00d7 2 \u03c0 e \u2212 ( \u03ba\u2212t \u221a q\u2212\u03c1 2 \u2212\u03c1z ) 2 2(1\u2212q) H \u2212 \u0393(t, z) (q \u2212 \u03c1 2 ) \u039b (105", "formula_coordinates": [29.0, 109.2, 611.04, 394.8, 111.64]}, {"formula_id": "formula_114", "formula_text": ")", "formula_coordinates": [29.0, 499.68, 706.04, 4.32, 8.64]}, {"formula_id": "formula_115", "formula_text": "1 \u2212 \u03c1 2 + R 2 \u2212 2\u03c1R cos \u03b8 sin 2 \u03b8 = 2\u03b1 \u03ba \u2212\u221e dt e \u2212 (t\u2212\u03c1z) 2 2(1\u2212\u03c1 2 ) \u221a 2\u03c0 1 \u2212 \u03c1 2 H \u0393(t, z) 1 \u2212 \u03c1 2 \u039b (\u03ba \u2212 t) 2 z (106", "formula_coordinates": [30.0, 127.84, 109.23, 371.85, 32.71]}, {"formula_id": "formula_116", "formula_text": ")", "formula_coordinates": [30.0, 499.68, 124.13, 4.32, 8.64]}, {"formula_id": "formula_117", "formula_text": "\u03c1 \u2212 R cos \u03b8 (1 \u2212 q) sin 2 \u03b8 = \u03b1 2\u03c0 \u03c1t \u221a q\u2212\u03c1 2 \u2212 z exp \u2212 \u03ba\u2212t \u221a q\u2212\u03c1 2 \u2212\u03c1z 2 2(1\u2212q) \u2212 t 2 2 H \u2212 \u0393(t,z) \u221a q\u2212\u03c1 2 \u039b \u221a 1 \u2212 qH \u03ba\u2212t \u221a q\u2212\u03c1 2 \u2212\u03c1z \u221a 1\u2212q (107) + \u221a 2\u03b1 \u03c0 (R \u2212 \u03c1 cos \u03b8) z \u2212 \u03c1t \u221a q\u2212\u03c1 2 \u2212 t q \u2212 \u03c1 2 + \u03c1z cos \u03b8 \u2212 Rz \u221a 2 q \u2212 \u03c1 2 \u039b (108) \u2212 \u22122\u03c1\u039b 2 \u2212 q \u2212 \u03c1 2 (2\u03c1 \u2212 2R cos \u03b8) \u0393(t, z) 2 \u221a 2 (q \u2212 \u03c1 2 ) 3/2 \u039b 3 (109) \u00d7 log 1 2 erfc \u03ba \u2212 t q \u2212 \u03c1 2 \u2212 \u03c1z \u221a 2 \u221a 1 \u2212 q exp \u2212 \u0393(t, z) 2 2 (q \u2212 \u03c1 2 ) \u039b 2 \u2212 t 2 2 (110)", "formula_coordinates": [30.0, 121.25, 201.54, 382.76, 217.89]}, {"formula_id": "formula_118", "formula_text": "\u03c1 \u2212 R cos \u03b8 (1 \u2212 q) sin 2 \u03b8 = \u03b1 \u03c0 \u03c1t \u221a q\u2212\u03c1 2 \u2212 z exp \u2212 \u03ba\u2212t \u221a q\u2212\u03c1 2 \u2212\u03c1z 2 2(1\u2212q) \u2212 t 2 2 H \u2212 \u0393(t,z) \u221a q\u2212\u03c1 2 \u039b \u221a 1 \u2212 qH \u03ba\u2212t \u221a q\u2212\u03c1 2 \u2212\u03c1z \u221a 1\u2212q (111) \u2212 \u03b1 \u03c0\u2206 exp \u2212 \u2206(t, z) 2\u039b 2 \u03c1R \u2212 q cos \u03b8 q \u2212 \u03c1 2(112)", "formula_coordinates": [30.0, 123.55, 467.38, 380.45, 105.86]}, {"formula_id": "formula_119", "formula_text": "\u03c1 \u2212 R cos \u03b8 sin 2 \u03b8 = 2\u03b1 \u03ba \u2212\u221e dt e \u2212 (t\u2212\u03c1z) 2 2(1\u2212\u03c1 2 ) \u221a 2\u03c0 1 \u2212 \u03c1 2 H \u0393(t, z) 1 \u2212 \u03c1 2 \u039b z \u2212 \u03c1t 1 \u2212 \u03c1 2 (\u03ba \u2212 t)(113)", "formula_coordinates": [30.0, 139.01, 620.4, 364.99, 31.82]}, {"formula_id": "formula_120", "formula_text": "+ 1 2\u03c0\u039b exp \u2212 \u2206(t, z) 2\u039b 2 \u03c1R \u2212 cos \u03b8 1 \u2212 \u03c1 2 (\u03ba \u2212 t) z(114)", "formula_coordinates": [30.0, 203.23, 674.65, 300.77, 24.86]}, {"formula_id": "formula_121", "formula_text": "R \u2212 \u03c1 cos \u03b8 sin 2 \u03b8 = \u03b1 \u03c0\u039b \u03ba \u2212\u221e dt exp \u2212 \u2206(t, z) 2\u039b 2 (\u03ba \u2212 t) z (115) 1 \u2212 \u03c1 2 + R 2 \u2212 2\u03c1R cos \u03b8 sin 2 \u03b8 = 2\u03b1 \u03ba \u2212\u221e dt e \u2212 (t\u2212\u03c1z) 2 2(1\u2212\u03c1 2 ) \u221a 2\u03c0 1 \u2212 \u03c1 2 H \u0393(t, z) 1 \u2212 \u03c1 2 \u039b (\u03ba \u2212 t) 2 z (116) \u03c1 \u2212 R cos \u03b8 sin 2 \u03b8 = 2\u03b1 \u03ba \u2212\u221e dt e \u2212 (t\u2212\u03c1z) 2 2(1\u2212\u03c1 2 ) \u221a 2\u03c0 1 \u2212 \u03c1 2 H \u0393(t, z) 1 \u2212 \u03c1 2 \u039b z \u2212 \u03c1t 1 \u2212 \u03c1 2 (\u03ba \u2212 t)(117)", "formula_coordinates": [31.0, 120.58, 84.53, 380.03, 109.53]}, {"formula_id": "formula_122", "formula_text": "+ 1 2\u03c0\u039b exp \u2212 \u2206(t, z) 2\u039b 2 \u03c1R \u2212 cos \u03b8 1 \u2212 \u03c1 2 (\u03ba \u2212 t) z(118)", "formula_coordinates": [31.0, 257.71, 200.07, 242.91, 24.86]}, {"formula_id": "formula_123", "formula_text": "\u039b = sin 2 \u03b8 \u2212 R 2 \u2212 \u03c1 2 + 2\u03c1R cos \u03b8,(119)", "formula_coordinates": [31.0, 183.88, 251.09, 316.73, 11.11]}, {"formula_id": "formula_124", "formula_text": "\u0393(t, z) = z(\u03c1R \u2212 cos \u03b8) \u2212 t(R \u2212 \u03c1 cos \u03b8),(120)", "formula_coordinates": [31.0, 163.73, 269.52, 336.88, 8.96]}, {"formula_id": "formula_125", "formula_text": "\u2206(t, z) = z 2 \u03c1 2 + cos 2 \u03b8 \u2212 2\u03c1R cos \u03b8 + 2tz(R cos \u03b8 \u2212 \u03c1) + t 2 sin 2 \u03b8.(121)", "formula_coordinates": [31.0, 161.65, 283.31, 338.96, 11.11]}], "doi": ""}