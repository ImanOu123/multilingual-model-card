{"title": "Privacy for Free: How does Dataset Condensation Help Privacy?", "authors": "Tian Dong; Bo Zhao; Lingjuan Lyu", "pub_date": "2022-06-01", "abstract": "To prevent unintentional data leakage, research community has resorted to data generators that can produce differentially private data for model training. However, for the sake of the data privacy, existing solutions suffer from either expensive training cost or poor generalization performance. Therefore, we raise the question whether training efficiency and privacy can be achieved simultaneously. In this work, we for the first time identify that dataset condensation (DC) which is originally designed for improving training efficiency is also a better solution to replace the traditional data generators for private data generation, thus providing privacy for free. To demonstrate the privacy benefit of DC, we build a connection between DC and differential privacy, and theoretically prove on linear feature extractors (and then extended to non-linear feature extractors) that the existence of one sample has limited impact (O(m/n)) on the parameter distribution of networks trained on m samples synthesized from n(n m) raw samples by DC. We also empirically validate the visual privacy and membership privacy of DC-synthesized data by launching both the loss-based and the state-of-the-art likelihoodbased membership inference attacks. We envision this work as a milestone for data-efficient and privacy-preserving machine learning. * Work done during internship at Sony AI.", "sections": [{"heading": "Introduction", "text": "Machine learning models are notoriously known to suffer from a wide range of privacy attacks (Lyu et al., 2020), such as model inversion attack (Fredrikson et al., 2015), membership inference attack (MIA) (Shokri et al., 2017), property inference attack (Melis et al., 2019), etc. The numerous con- cerns on data privacy make it impractical for data curators to directly distribute their private data for purpose of interest. Previously, generative models, e.g., generative adversarial networks (GANs) (Goodfellow et al., 2014), was supposed to be an alternative of data sharing. Unfortunately, the aforementioned privacy risks exist not only in training with raw data but also in training with synthetic data produced by generative models (Chen et al., 2020b). For example, it is easy to match the fake facial images synthesized by GANs with the real training samples from the same identity (Webster et al., 2021). To counter this issue, existing efforts (Xie et al., 2018;Wang et al., 2021;Cao et al., 2021;Harder et al., 2021) applied differential privacy (DP) (Dwork et al., 2006) to develop differentially private data generators (called DPgenerators), because DP is the de facto privacy standard which provides theoretical guarantees of privacy leakage. Data produced by DP-generators can then be applied to various downstream tasks, e.g., data analysis, visualization, training privacy-preserving classifier, etc. However, due to the noise introduced by DP, the data produced by DP-generators are of low quality, which impedes the utility as training data, i.e., accuracy of the models trained on these data. Thus, more data generated by DPgenerators are needed to obtain good generalization performance, which inevitably decreases the training efficiency.\ngenerative models that are trained to generate real-looking samples with high fidelity, these DC methods generate informative training samples for data-efficient learning. In this work, we for the first time investigate the feasibility of protecting data privacy using DC techniques. We find that DC can not only accelerate model training but also offer privacy for free. Figure 1 illustrates how DC methods can be applied to protect membership privacy and visual privacy. Specifically, we first analyse the relationship between DC-synthesized data and original ones (Proposition 4.3 and 4.4), and theoretically prove on linear DC extractors that the change caused by removing or adding one element in n raw samples to the parameter distribution of models trained on m(m n) DC-synthesized samples (i.e., privacy loss) is bounded by O(m/n) (Proposition 4.10), which satisfies that one element does not greatly change the model parameter distribution (the concept of DP). The conclusions are further analytically and empirically generalized to non-linear feature extractors. Then, we empirically validate that models trained on DC-synthesized data are robust to both vanilla loss-based MIA and the state-of-the-art likelihood-based MIA (Carlini et al., 2022). Finally, we study the visual privacy of DC-synthesized data in case of adversary's direct matching attack. All the results show that DC-synthesized data are not perceptually similar to the original data as our Proposition 4.4 indicates, and cannot be reversed to the original data through similarity metrics (e.g., LPIPS).\nThrough empirical evaluations on image datasets, we validate that DC-synthesized data can preserve both data efficiency and membership privacy when being used for model training. For example, on FashionMNIST, DC-synthesized data enable models to achieve a test accuracy of at least 33.4% higher than that achieved by DP-generators under the same empirical privacy budget. Meanwhile, to achieve a test accuracy of the same level, DC only needs to synthesize at most 50% data of the size required by GAN-based methods, which speeds up the training by at least 2 times. In summary, our contributions are three-fold:\n\u2022 To the best of our knowledge, we are the first to introduce the emerging dataset condensation techniques into privacy community and provide systematical audit on state-of-the-art DC methods.\n\u2022 We build the connection between dataset condensation and differential privacy, and contribute theoretical analysis with both linear and non-linear feature extractors.\n\u2022 Extensive experiments on image datasets empirically validate that DC methods reduce the adversary advantage of membership privacy to zero, and DCsynthesized data are perceptually irreversible to original data in terms of similarity metrics of L 2 and LPIPS.", "publication_ref": ["b21", "b9", "b31", "b22", "b11", "b5", "b37", "b39", "b36", "b12", "b7", "b2"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Background and Related Work", "text": "In this section, we briefly present dataset condensation and the membership privacy issues in machine learning models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dataset Condensation", "text": "Orthogonal to model knowledge distillation (Hinton et al., 2015), Wang et al. firstly proposed dataset distillation (DD) which aims to distill knowledge from a large training set into a small synthetic set. The synthetic set can be used to efficiently train deep neural networks with a moderate decrease of testing accuracy. Recent works significantly advanced this research area by proposing Dataset Condensation (DC) with gradient matching (Zhao et al., 2021;Zhao & Bilen, 2021b), Distribution Matching (DM) (Zhao & Bilen, 2021a) and introducing Kernel Inducing Points (KIP) (Nguyen et al., 2021a;. For example, the synthetic sets (50 images per class) generated by DM can be used to train a 3layer convolutional neural networks from scratch and obtain over 60% testing accuracies on CIFAR10 (Krizhevsky et al., 2009) and over 98% testing accuracies on MNIST (LeCun et al., 1998). In this work, we mainly focus on synthetic sets generated by DSA (Zhao & Bilen, 2021b), DM (Zhao & Bilen, 2021a) and KIP (Nguyen et al., 2021a), because 1) DSA and DM are improved DC and KIP is improved DD, and 2) the performance of DD and DC are significantly lower than DSA, DM and KIP.\nWe formulate dataset condensation problem using the symbols presented in (Zhao & Bilen, 2021a). Given a large-scale dataset (target dataset) T = {(x i , y i )} which consists of |T | samples from C classes, the objective of dataset condensation (or distillation) is to learn a synthetic set S = {(s i , y i )} with |S| synthetic samples so that the deep neural networks can be trained on S and achieve comparable testing performance to those trained on T :\nE x\u223cP D [L(\u03c6 \u03b8 T (x), y)] E x\u223cP D [L(\u03c6 \u03b8 S (x), y)], (1)\nwhere P D is the real data distribution, \u03c6 \u03b8 T (\u2022) and \u03c6 \u03b8 S (\u2022) are models trained on T and S respectively. L(\u2022, \u2022) is the loss function, e.g. cross-entropy loss.\nTo achieve this goal, Wang et al. proposed a meta-learning based method which parameterizes the model updated on synthetic set as \u03b8 S (S) and then learns the synthetic data by minimizing the validation loss on original training data T :\narg min S L T (\u03b8 S (S)),(2)\nwhere \u03b8 S (S) = arg min \u03b8 L S (\u03b8). The meta-learning algorithm has to recurrently unroll the computation graph \u03b8 S with respect to S, which is expensive and unscalable. (Nguyen et al., 2021a) proposed Kernel Inducing Points (KIP) which leverages the neural tangent kernel (NTK) (Jacot et al., 2018) to replace the expensive network parameter updating. With NTK, \u03b8 S has a closed-form solution. Thus, KIP learns synthetic data by minimizing the kernel ridge regression loss:\narg min Xs 1 2 y t \u2212 K XtXs (K XsXs + \u03bbI) \u22121 y s 2 , (3)\nwhere X s and X t are the synthetic and real images from S and T , y s and y t are corresponding labels. K U V represents the NTK matrix (K(u, v)) (u,v)\u2208U,V for two sets U and V . For a neural network \u03c6 \u03b8 , the definition of K(u, v) on elements u and v is\nK(u, v) = \u2207 \u03b8 \u03c6 \u03b8 (u) \u2022 \u2207 \u03b8 \u03c6 \u03b8 (v).\nZhao et al. proposed a novel DC framework to condense the real dataset into a small synthetic set by matching the gradients when inputting real and synthetic batches into the same model, which can be expressed as follows:\narg min S E \u03b80\u223cP \u03b8 0 [ T \u22121 t=0 D(\u2207 \u03b8 L S (\u03b8 t ), \u2207 \u03b8 L T (\u03b8 t ))], (4)\nwhere model \u03b8 t is updated by minimizing the loss L S (\u03b8 t ) alternatively, D computes distance between gradients. (Zhao & Bilen, 2021b) enabled the learned synthetic images to be effectively used to train neural networks with data augmentation by introducing the differentiable Siamese augmentation (DSA) A \u03c9 (\u2022) and improved the matching loss in (4) as follows:\nD(\u2207 \u03b8 L(\u03b8 t , A \u03c9 (S)), \u2207 \u03b8 L(\u03b8 t , A \u03c9 (T ))).(5)\nAlthough (Zhao et al., 2021) successfully avoided unrolling the recurrent computation graph in (Wang et al., 2018), it still needs to compute the expensive bi-level optimization and second-order derivative. To further simplify the learning of synthetic data, (Zhao & Bilen, 2021a) proposed a simple yet effective dataset condensation method with distribution matching (DM). Specifically, the learned synthetic data S should have data distribution close to that of real data T in randomly sampled embedding spaces:\nmin S E \u03d1\u223cP \u03d1 ,\u03c9\u223c\u2126 1 |T | |T | i=1 \u03c8 \u03d1 (A(x i , \u03c9)) \u2212 1 |S| |S| j=1 \u03c8 \u03d1 (A(s j , \u03c9)) 2 ,(6)\nwhere \u03c8 \u03d1 represents randomly sampled embedding functions (namely feature extractors), e.g. randomly initialized neural networks and A(\u2022, \u03c9) is the differentiable Siamese augmentation. Experimental results show that this simple objective can lead to effective synthetic data that are comparable even better than those generated by existing methods.", "publication_ref": ["b13", "b44", "b43", "b42", "b26", "b17", "b43", "b42", "b26", "b42", "b26", "b14", "b43", "b44", "b35", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "Membership Privacy", "text": "For privacy analysis, we mainly focus on membership privacy as it directly relates to personal privacy. For example, inferring that an individual's facial image was in a shop's training dataset reveals the individual had visited the shop. Shokri et al. have shown that DNNs' output can leak the membership privacy of the input (i.e., whether the input belongs to the training dataset) under membership inference attack (MIA). In general, MIA only needs black-box access to model parameters (Sablayrolles et al., 2019) and can be successful with logit (Yu et al., 2021) or hard label prediction (Li & Zhang, 2021;Choquette-Choo et al., 2021).\nLoss-based MIA. The loss-based MIA infers membership by the predicted loss: if the loss is lower than a threshold \u03c4 , then the input is a member of the training data. Formally, the membership M (x) of an input x can be expressed as:\nM (x) = 1(l(x) \u2264 \u03c4 ),(7)\nwhere M (x) = 1 means x is a training member, 1(A) = 1 if event A is true. The threshold \u03c4 can be either chosen by locally trained shadow models (Shokri et al., 2017) or via optimal bayesian strategy (Sablayrolles et al., 2019).\nLikelihood-based MIA. Recent works (Carlini et al., 2022;Rezaei & Liu, 2021) \nf for (x, y) is \u03c6(f (x) y ) = \u03c6(exp(\u2212l(f (x), y))),\nwhere l is the cross-entropy loss and \u03c6(p) = log( p 1\u2212p ). To attack, the adversary queries the victim model f with a target example (x, y) to estimate the likelihood \u039b defined as:\n\u039b = p(conf obs |N (\u00b5 in , \u03c3 2 in )) p(conf obs |N (\u00b5 out , \u03c3 2 out )) ,(8)\nwhere conf obs = \u03c6(f (x) y ) is the confidence of victim model f on target example (x, y). The adversary infers membership by thresholding the likelihood \u039b with threshold \u03c4 determined in advance.", "publication_ref": ["b30", "b40", "b19", "b6", "b31", "b30", "b2", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Problem Statement", "text": "In practice, companies may utilize personal data for model training in order to provide better services. For example, data holders (e.g., smart retail stores, smart city facilities) may capture clients' data and send to cloud servers for model training. However, models trained on the raw data (i.e., T ) can be attacked by MIA. In addition, transmitting raw data to servers suffers from potential data leakage (e.g., to honest-but-curious operators). Therefore, a better protocol is to first learn knowledge from data by, for instance, generating synthetic dataset S from the raw data (i.e., T ), and then send S to the server for model training for downstream applications. Formally, we define the threat model as follows:\nAdversary Goal. The adversary aims to examine the membership information of the target dataset T . Specifically, for a sample of interest x, the adversary infers whether x \u2208 T .\nAdversary Knowledge. We assume a strong adversary (e.g., honest-but-curious server), who although has no access to T but has the white-box access to both the synthetic dataset S synthesized from the target dataset T and the model f S trained on the synthetic dataset. The adversary also knows the data distribution of T .\nAdversary Capacity. The adversary has unlimited computational power to generate shadow synthetic datasets on data of same distribution of T and train shadow models on them.\nNote that white-box access to the model parameters does not help MIA (Sablayrolles et al., 2019), so we omit other advantages brought by the white-box access to f S .", "publication_ref": ["b30"], "figure_ref": [], "table_ref": []}, {"heading": "Theoretical Analysis", "text": "In this section, we theoretically analyse the relationship between the target dataset T and the synthetic dataset S of DM (improved DC) (Zhao & Bilen, 2021a), and the privacy guarantees of T that are thereby provided.  (Sablayrolles et al., 2019) and compute the order of magnitude of the impact, which establishes the connection between DC and DP.", "publication_ref": ["b42", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "Assumptions & Notations", "text": "The DM loss ( 6) can be optimized for each class (Zhao & Bilen, 2021a). To simplify the notations, we consider only one class and omit the label vectors in the synthetic\ndataset S = {s 1 , \u2022 \u2022 \u2022 , s |S| } and the target dataset T = {x 1 , \u2022 \u2022 \u2022 , x |T | }.\nWe consider the following two assumptions of the target dataset and the convergence of DM. Assumption 4.1. The linear span of the target dataset span(T ) satisfies d T = dim(span(T )) < d, where d is the data dimension, dim(V ) represents the dimension of vector space V , span(T ) is the vector subspace generated by all linear combinations of T :\nspan(T ) := { |T | i=1 w i x i |1 \u2264 i \u2264 |T | , w i \u2208 R, x i \u2208 T }.(9)\nIn practice, d T can be computed as the rank of matrix form of T . This assumption generally holds for high dimensional data and can be directly verified for common datasets (e.g., CIFAR-10). Without loss of generality, we consider an orthogonal basis (under inner product of R \nd ) E = {e 1 , \u2022 \u2022 \u2022 , e d } among which the first d T basis vectors E T = {e 1 , \u2022 \u2022 \u2022 , e d T }", "publication_ref": ["b42"], "figure_ref": [], "table_ref": []}, {"heading": "Analysis of Synthetic Data", "text": "We first analyse synthetic data by linear extractors and then discuss the generalization to non-linear case (Remark 4.5). Proposition 4.3 (Minimizer of DM Loss). For a linear extractor \u03c8 \u03b8 : R d \u2192 R k such that k < d, \u03b8 \u2208 R k\u00d7d , under Assumption 4.1 and 4.2, the dataset S * synthesized by DM from the target dataset T satisfies:\n1) The barycenters of S * and T coincide:\n1 |T | |T | i=1 x i \u2212 1 |S * | |S * | i=1 s * i = 0,(10)\n2)\n\u2200s * i \u2208 S * , s * i = s * i,E T + s * i,E \u22a5 T ,where\ns * i,E T \u2208 span(T ), s * i,E \u22a5 T \u2208 span(T ) \u22a5 that verifies |S| i=1 s * i,E \u22a5 T = 0 E \u22a5 T .(11)\nThe proof of Proposition 4.3 can be found in Appendix A.\nNote that the minimizer is\ns * 1 = 1 |T | |T | i=1 x i when |S * | = 1,\nindicating that the synthetic data falls into vector subspace span(T ), confirming the Proposition 4.3.\nThe DC initialization of synthetic dataset can be either real data sampled from T or random noise. Next, we study the impact of DM initialization and obtain the following results (proof can be found in Appendix B). 1) Real data initialization. Assume that S is initialized with first |S| samples of T , i.e., s i = x i , then we have\ns * i = x i + 1 |T | |T | j=1 x j \u2212 1 |S| |S| j=1 s j \u2208 span(T ). (12)\n2) Random initialization. The synthetic data are initialized with noise of normal distribution, i.e., \u2200s i \u2208 S, s i \u223c N (0, I d ), and we assume the empirical mean is zeroed, i.e.,", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "|S|", "text": "|S| i=1 s i = 0, then we have\ns * i = s * i,E T + s * i,E \u22a5 T ,(13)\nwhere . Our results can be generalized to the non-linear extractors. Giryes et al. proved that multi-layer random neural networks generate distancepreserving embedding of input data, so ( 6) is minimized if and only if the distance between real and synthetic data is minimized. Take 2-layer random networks as an example (Estrach et al., 2014), there exist two constants\ns * i,E T = s i,E T + 1 |T | |T | j=1 x j \u2208 span(T ), and s * i,E \u22a5 T = s i,E \u22a5 T \u2208 (span(T )) \u22a5 .\n0 < A \u2264 B such that \u2200(x, y) \u2208 (R d ) 2 , A x \u2212 y 2 \u2264 \u03c1(\u03b8x) \u2212 \u03c1(\u03b8y) 2 \u2264 B x \u2212 y 2 ,\nwhere \u03c1 is ReLU. We also analyse the case of 2-layer extractors (activated by ReLU) and found that the (pseudo)-barycenters of S * and T still coincide. Moreover, on convolutional extractors and the 2-layer extractors, we empirically verify Proposition 4.3 in Appendix D (see Figure 7). Remark 4.6 (Impact of initialization on privacy). Note that in case of real data initialization, a higher |S| results in lower distance between barycenters of initialized S and T , thus the changes brought to S become smaller when S becomes larger. This explains the phenomenon that DM-generated images are more visually similar to the real images for higher ipc (images per class) (Zhao & Bilen, 2021a). However, as we demonstrate in our experiments (Section 5.2), the membership of data used for DC initialization can still be inferred by vanilla loss-based MIA. One countermeasure is to choose hard-to-infer samples (Carlini et al., 2022), i.e., samples whose model outputs are not affected by the membership, as initialization data.\nOn the other hand, data not used for initialization generate little effect (i.e., their weights in synthetic data are O( 1|T | )) on synthetic data, just as the case of random initialization, where data in T also generate little effect on S * . We demonstrate that the membership of those data cannot be inferred under both loss-based MIA and the state-of-the-art likelihood MIA (see Section 5.2). Moreover, projection component of (span(T )) \u22a5 can further protect the privacy (e.g., visual privacy in Section 5.4). Remark 4.7 (Comparison between DC and GAN). The generator of GAN is trained to minimize the distance between the real and the generated data distributions, which is similar to the objective of DC. However, GAN-generated data share the same constraints (i.e., bounded between 0 and 1) as the real data. DC-generated data do not need to satisfy these constraints. This enables the DC-generated data to contain more features and explains the higher accuracy of models trained on DC-generated data (Zhao et al., 2021). We also empirically compare the accuracy of model trained on GAN-generated data and DC-synthesized data (see Section 5.3) , and found that DC-synthesized data outperform GAN-generated data for training better models with smaller amount of training data.", "publication_ref": ["b8", "b42", "b2", "b44"], "figure_ref": ["fig_11"], "table_ref": []}, {"heading": "Privacy Bound of Models Trained on Synthetic Data", "text": "To understand how synthetic dataset protects membership privacy of T when being used for training model f S , we estimate how model parameters change when removing one sample from T by adopting below assumption. With a little abuse of notation, we denote the minimizer set S * by S when the context is clear. \nP(\u03b8|T ) = 1 K T exp (\u2212 |T | i=1 l(\u03b8, x i )),(14)\nwhere K T is the constant normalizing the distribution.\nUnlike widely known DP mechanisms (e.g., Gaussian mechanism) that transform the deterministic query function into a randomized one, randomness brought by optimization algorithm (i.e., SGD) or hardware defaults leads to different parameters each time of training, which justifies Assumption 4.8 and ensures the \"uncertainty\" in DP. In addition, we need the following assumption on the datasets S, T and the loss function l introduced in the Assumption 4.8.\nThe assumption is valid for finite datasets and common loss functions (e.g., cross-entropy) and is used to quantify the data bound and loss variation.\nAssumption 4.9. We assume the data of T and S are bounded, i.e.,\n\u2203B > 0, \u2200x \u2208 T \u222a S, x 2 \u2264 B. (15\n)\nThe loss function l(\u03b8,\n\u2022) : R d \u2192 R + is L-Lipschitz accord- ing to the L 2 norm, i.e., \u2200(x, y) \u2208 B(B) 2 , \u03b8, |l(\u03b8, x) \u2212 l(\u03b8, y)| \u2264 L x \u2212 y 2 , (16\n) where B(B) = {x| x \u2264 B} is the close ball of space R d .\nWith all previous assumptions, we have the following result. Connection to DP. Our privacy analysis is based on the impact of model parameter distribution by removing one element from the origin training dataset, which is similar to the definition of DP (Dwork et al., 2006). Formally, a -differential privacy mechanism M satisfies:\nln P(M(D) \u2208 S M ) P(M(D ) \u2208 S M ) \u2264 (18\n)\nfor all neighbor dataset pair (D, D ) and all subset S M of the range of M. Without knowledge of explicit form of model parameter distribution, we can only claim that the privacy budget varies at the order of O( |S| |T | ). In practice, we use an empirical budget\u02c6 through MIA (Kairouz et al., 2015) to measure the privacy guarantee against MIA. Typically, for a MIA that achieves FPR and TPR (True Positive Rate) against a model, the empirical budget is\u02c6 \u2265 ln(T P R/F P R). In other words, the model behaves\u02c6 -differentially private to an adversary that applies MIA (i.e., threat model in Section 3).\nNote that the empirical budget\u02c6 is not equivalent to the real budget because of different threat models (Nasr et al., 2021). Nonetheless, we consider black-box MIA as the only privacy threat to the model, thus we can regard the DP budget as a model privacy metric against MIA. In this way, we can compare\u02c6 and by the definition of\u02c6 . In Section 5.3, we show that models trained on data synthesized by DC achieve\u02c6 \u2248 2 against threat from the state-of-the-art MIA (LiRA), and obtain accuracy much higher than differentially private generators (Chen et al., 2020a;Harder et al., 2021), indicating DC is a better option for efficient and privacypreserving model training.", "publication_ref": ["b7", "b16", "b25", "b4", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "In this section, we evaluate the membership privacy of f S for real data and random initialization. Then, we compare DC with previous DP-generators and GAN to demonstrate DC's better trade-off between privacy and utility. Finally, we investigate the visual privacy of DC-synthesized data.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "Datasets & Architectures. We use three datasets: Fashion-MNIST (Xiao et al., 2017), CIFAR-10 ( Krizhevsky et al., 2009) and CelebA (Liu et al., 2015) for gender classification. The CelebA images are center cropped to dimension 64 \u00d7 64, and we randomly sample 5, 000 images for each class, which is same as CIFAR-10, while FashionMNIST contains 6, 000 images for each class. We adopt the same 3layer convolutional neural networks used in (Zhao & Bilen, 2021a) and (Nguyen et al., 2021b) as the feature extractor. DC Settings. One important hyperparameter of DSA, DM and KIP is the ratio of image per class r ipc = |S| |T | . We evaluate r ipc = 0.002, 0.01 for all methods, and for DM we add an extra evaluation r ipc = 0.02 due to its high efficiency on producing large synthetic set. Note that r ipc influences the model training efficiency: the lower r ipc , the faster model training. We also consider ZCA preprocessing for KIP as it is reported to be effective for KIP performance improvement. Appendix E.1 contains more DC implementation details.\nBaselines. As for non-private baseline, we adopt subset sampled from T (this baseline is termed real data) and data generated by conditional GAN (cGAN or GAN for short) (Mirza & Osindero, 2014) which is trained on T . For private baseline, we choose DP-generators including GS-WGAN (Chen et al., 2020a), DP-MERF (Harder et al., 2021) and DP-Sinkhorn (Cao et al., 2021). We compare the DC methods with baselines in terms of privacy and efficiency.\nMIA Settings & Attack Metrics. For each dataset, we randomly split it into two subsets of equal amount of samples and choose one subset as T (member data). We then synthesize dataset S on T , and train a model f S (victim model) on S. The other subset becomes the non-member data used for testing the MIA performance. The above process is called preparation of synthetic dataset.\nFor loss-based MIA, we repeat the preparation of synthetic dataset 10 times with different random seeds. This gives us 10 groups of T , S and f S . For each f S , we first select N member samples from T and N non-member samples, and choose an optimal threshold that maximizes the advantage score on the previously chosen 2N samples (Sablayrolles et al., 2019). The threshold is then tested on another disjoint 2N samples composed by N member samples and N non-member samples to compute the advantage score of loss-based MIA. We report the advantage (in percentage) defined as 2 \u00d7 (acc \u2212 50%) where acc is the test accuracy of membership in percentage.\nFor LiRA (Carlini et al., 2022), we repeat the preparation of synthetic dataset N m times with different random seeds, and obtain N m shadow T , S and f S . We set N m = 256 for DM and N m = 64 for KIP because of its lower training efficiency. We omit DSA for LiRA due to longer training time. To attack a victim model, we compute the likelihoods of each sample with N m shadow f S and determine the threshold of likelihood according to the requirements of false positive. We use the Receiver Operating Characteristic (ROC) curve and Area Under Curve (AUC) score to evaluate the attack performance. Remark that we adopt the strongest (and unrealistic) attack assumption (i.e., the attacker knows the membership), so that we investigate the privacy of DCsynthesized data under the worst case. Real Data Initialization Leaks Membership Privacy. We begin with membership privacy leaked by f S as mentioned in Remark 4.6 of Proposition 4.4. We aim to verify that DC with real data initialization still leaks membership privacy of the data used for initialization. Here, the data used for initialization are sampled from T during each time of preparation of synthetic dataset. We launch loss-based MIA against f S and adopt the real data baseline.", "publication_ref": ["b38", "b17", "b20", "b42", "b27", "b24", "b4", "b12", "b30", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Membership Privacy of f S", "text": "Table 1 shows the advantage score of loss-based MIA. Here, we vary a little bit the attack setting: the advantage scores are computed with the data used for real initialization and the same amount of member data not used for initialization but involved in DC. We observe that, on CIFAR-10 and CelebA, the synthetic dataset with real data initialization achieves lower advantage scores comparing to directly using real data for training (baseline). This can be explained by Proposition 4.4, which tells us that the synthesized data deviates slightly from the real data used for initialization. However, on FashionMNIST, the baseline has lower advantage scores. We suspect this is because FashionMNIST images are greyscale and synthetic data contain more features that prone to be memorized by networks. Loss distribution in Figure 8 in Appendix E.2 also demonstrates that synthetic data with real data initialization can still leak membership privacy.\nNext, we show that models trained on data synthesized by DC with random initialization are robust against both lossbased MIA and LiRA (the state-of-the-art MIA).\nMIA Robustness of Random Initialization. Because of random initialization, each member sample contributes equally to the synthetic dataset. Thus, in this case, we follows the loss-base attack setting and set N = 1000. Table 2 provides the average and the standard variance of advantages for models trained on synthetic datasets by cGAN, DSA, DM and KIP. The advantages are around 0 for all r ipc , signifying that the adversary cannot infer membership of T . Nevertheless, as long as the adversary has access to the generated images (which is included in our threat model), the membership of the GAN generator's training data (i.e., T ) can still be leaked (see Appendix E.4). Meanwhile, as we show later in Figure 3, models trained on DC-synthesized data achieve higher accuracy scores than baseline (i.e., cGAN-generated data), demonstrating the higher utility of DC-synthesized data.\nLiRA is a more powerful MIA because it can achieve higher TPR at low FPR (Carlini et al., 2022), while the adversary's computational cost is higher. Figure 2 provides the ROC curves of LiRA against f S . We can observe that the ROC curves are close to the diagonal (red line) for all datasets and r ipc . The AUC scores of ROC curves are around 0.5, indicating there is negligible attack benefit (low TPR) for the attacker compared with random guess. Recall that LiRA is evaluated on the whole dataset (half as member and other half as non-member), the minimum FPR value is 1 5000 = 2\u00d7 10 \u22124 for CelebA, 4 \u00d7 10 \u22125 for CIFAR-10 and 3.33 \u00d7 10 \u22125 for FashionMNIST. Therefore, when FPR is close to 0, the ROC curves have different shapes for different datasets. However, we also notice that the TPR is close to FPR when FPR is around the minimum (e.g., FPR\u223c 10 \u22124 for CelebA), demonstrating that models trained on data synthesized by DC with random initialization is robust to LiRA at low FPR. GAN Generator. Figure 3 compares the accuracy scores of models trained on synthetic datasets (DC-synthesized with random initialization under r ipc = 0.01). We can find that under the same constraint of training efficiency (i.e., r ipc = 0.01), the DM and DSA outperform the other methods. Note that models trained on KIP-synthesized data achieve lower accuracy than baseline because the loss is hard to converge for large r ipc . Nevertheless, for small r ipc , the KIP significantly outperforms baselines on CIFAR-10 and CelebA (see Figure 10 in Appendix E).", "publication_ref": ["b2"], "figure_ref": ["fig_12", "fig_5", "fig_5", "fig_0"], "table_ref": ["tab_3", "tab_4"]}, {"heading": "Comparison with Different Generators", "text": "Then, we aim to know how DC improves model training efficiency compared to cGAN. In other words, to achieve the same accuracy of f S , the difference between the r ipc that DC requires and the r ipc that cGAN requires can be seen as the model training efficiency that DC improves. For different r ipc (the x-axis), Figure 4 shows the accuracy of models trained on cGAN-generated dataset whose ratio is r ipc (the blue solid curve). The red and green horizontal lines represent the accuracy of f S trained on dataset synthesized by DSA and DM for r ipc = 0.01, respectively. We omit KIP here because of its lower utility than baselines. Therefore, the r ipc of the intersection point of the red (resp. green) line and the blue curve is the r ipc of cGANsgenerated dataset on which the models can be trained to achieve the same accuracy as DSA (resp. DM). We can see that, cGAN needs to generate more data to train a model that achieves the same accuracy as models trained on data synthesized by DM and DSA, because the r ipc values indicated in the x-axis are all higher than 0.01. It is worth noting that DC improves the training efficiency (measured by r ipc ) by at least 2 times than cGAN for r ipc = 0.01, because on FashionMNIST (the leftmost sub-figure in Figure 4)), cGAN requires to generate synthetic dataset of r ipc = 0.02 to achieve the same accuracy (0.85) as the DM-synthesized dataset (r ipc = 0.01).", "publication_ref": [], "figure_ref": ["fig_6", "fig_6"], "table_ref": []}, {"heading": "DP-generators.", "text": "We estimate an empirical\u02c6 based on the ratio of TPR and FPR computed by LiRA. In Table 3, we compare the accuracy of models trained on DC-synthesized data and on data generated by recent DP-generators. We reproduce DP-MERF and GS-WGAN according to the official implementation and adopt the reported results of DP-Sinkhorn. We observe that the accuracy of models trained on data generated by the state-of-the-art DP-generator (DP-Sinkhorn) is still lower than DM-synthesized images, even the ratio for DP-Sinkhorn is r ipc = 1. The reason is that DP is designed to defend against the strongest adversary who has access to the training process of generator. Hence, data generated by DP-generators are of lower utility for model training because of the too strong defense requirement.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Visual Privacy", "text": "The adversary can directly visualize synthetic data and compare with the target sample to infer the membership. We visualize the synthetic images and use L 2 distance as well as the perceptual metric LPIPS (Zhang et al., 2018) with VGG backbone to measure the similarity between synthetic and real images. Figure 5 shows examples of DM-generated images and the their (top 3) most similar real images, i.e., images of lowest L 2 and LPIPS distance with the synthetic image on the top of the column. We observe that the real data share similar facial contour patterns with the synthetic images, but more fine-grained features, e.g., eye shape, are different, which explains why models trained on synthetic dataset protect the membership privacy of original data. This can also explain why current MIAs fail on models trained    on synthetic datasets: the generated synthetic training data have lost the private properties of real data and thus the adversary are not able to infer the privacy from models trained on such synthetic data.", "publication_ref": ["b41"], "figure_ref": [], "table_ref": []}, {"heading": "Discussion and Conclusion", "text": "In this work, we make the first effort to introduce the emerging dataset condensation techniques into the privacy community and provide systematical audit including theoretical analysis of the privacy property and empirical evaluation, i.e. visual privacy examination and robustness against lossbased MIA and LiRA on FashionMNIST, CIFAR-10 and CelebA datasets.\nOur future work will attempt to generalize the theoretical findings to other DC methods. This can be studied from the perspective of information loss (e.g. data compression ratio). Moreover, DC methods that satisfy formal DP formulation, e.g., (\u03b1, )-R\u00e9nyi DP (Mironov, 2017), are worth exploring. Figure 5. Examples of facial images that are most similar to synthetic data generated by DM with random initialization. The value above each image is the distance (L2 and LPIPS) between the image and the synthetic data (first row). Lower distance indicates higher similarity. Even though these real images have similar face contour, blurred facial details (e.g., eyes, nose) make it difficult for the adversary to infer the membership.\nThe current efforts of DC mainly focus on image classification, thus another interesting direction is the extension of the privacy benefit brought by DC to more complicated vision tasks (e.g., object detection) and non-vision tasks (e.g. text and graph related applications). In essence, DC methods can generalize to other machine learning tasks, as they learn the synthetic data by summarizing the distribution or discriminative information of real training data. Hence, their privacy advantage should also generalize to other tasks.", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "A. Proof of Proposition 4.3", "text": "We begin our analysis of DM with the linear extractor \u03c8 \u03b8 : R d \u2192 R k such that k < d, \u03b8 = [\u03b8 i,j ] \u2208 R k\u00d7d and for an input x, \u03c8 \u03b8 (x) = \u03b8x. We also omit the differentiable Siamese augmentation to simplify the analysis. As the representation extractors \u03c8 \u03b8 in DM are randomly initialized, we assume that the extractor parameters follow the standard normal distribution and are identically and independently distributed (i.i.d), i.e., \u03b8 i,j iid \u223c N (0, 1). Thus, Equation (6) becomes the expectation over\nd DM 2\nwhere d DM is defined as:\nd DM := \u03b8( 1 |T | |T | i=1 x i \u2212 1 |S| |S| i=1 s i ). (19\n)\nHence, L DM = E \u03b8\u223cN (0,1) d DM 2 .\nThe optimization of S with SGD relies on the gradient of (6). Given a sampled model parameter \u03b8, for some synthetic sample s j , we have:\n\u2202 d DM 2 \u2202s i = \u2212 2 |S| (d DM ) \u03b8 = \u2212 2 |S| ( 1 |T | |T | j=1 x j \u2212 1 |S| |S| j=1 s j ) \u2022 \u03b8 \u03b8.(20)\nHence, we obtain:\n\u2202L DM \u2202s i = \u2202E \u03b8 d DM 2 \u2202s i = E \u03b8 \u2202 d DM 2 \u2202s i = \u2212 2 |S| ( 1 |T | |T | j=1 x j \u2212 1 |S| |S| j=1 s j ) \u2022 E[\u03b8 \u03b8],(21)\nwhere E[\u03b8 \u03b8] = kI d by definition of \u03b8, and I d is the identity matrix of R d . Equation ( 21) indicates that the optimization direction of synthetic sample s i is the direction of moving barycenter of S to the barycenter of T . To conceptually interpret, the optimization of (6) will move the initialized S until the barycenter coincides with that of T because of the existence of minimizer (Assumption 4.2) where the left hand-side of ( 21) should be 0.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B. Proof of Proposition 4.4", "text": "Case of real data initialization. Suppose that each s i \u2208 S is sampled from T and we can consider s i = x i as initialization for simplicity. According to (21), all s i are optimized until the barycenters of S and T coincide. Observe that each s * j \u2208 span(T ), because the projection components of span(T ) \u22a5 remain zero throughout the optimization process of DM. Thus, one solution of minimizer elements s * i \u2208 S * with the real data initialization can be:\ns * i = x i + 1 |T | |T | j=1 x j \u2212 1 |S| |S| j=1 s j .(22)\nWhen |S| and |T | are large (e.g., > 50), we can consider s * i \u2248 x i , thus initialization with real data in DM still risks of membership privacy leakage.\nCase of random initialization. The synthetic data are initialized as vectors of multivariate normal distribution, i.e.,\n\u2200s i \u2208 S, s i \u223c N (0, I d ). (23\n)\nEach synthetic sample s i can be written as a vector [s i,1 , \u2022 \u2022 \u2022 , s i,d ] under the basis E where \u2200j, s i,j i.i.d.\n\u223c N (0, 1), because s i 's covariance matrix remains identity matrix under any orthogonal transformation (i.e., orthogonal basis). Thus, we can decompose d DM to the projections on subspace span(T ) and its orthogonal complement. Formally, we have\nd DM 2 = \u03b8 1:d T Proj E T (\u2206 S,T ) 2 + \u03b8 d T :d Proj E \u22a5 T (\u2206 S,T ) 2 d DM 2 E \u22a5 T + 2 < \u03b8 1:d T Proj E T (\u2206 S,T ), \u03b8 d T :d Proj E \u22a5 T (\u2206 S,T ) >,(24)\nwhere \u03b8 a:b represents the submatrix composed by a-th column to the b-th column, Proj V is the projection operator onto subspace V and\n\u2206 S,T := 1 |T | |T | i=1 x i \u2212 1 |S| |S| i=1 s i .(25)\nNote that\nProj E \u22a5 T (\u2206 S,T ) = 1 |S| |S| i=1 Proj E \u22a5 T (s i ),(26)\nbecause x i \u2208 span(T ) = span(E T ) for each x i . Let s i,E \u22a5 T = Proj E \u22a5 T (s i ), then we have\nE \u03b8 \u2202 d DM 2 E \u22a5 T \u2202s i,E \u22a5 T = 2 |S| 2 ( |S| j=1 s j,E \u22a5 T ) E \u03b8 [(\u03b8 d T :d ) \u03b8 d T :d ],(27)\nbecause\nE \u03b8 [\u03b8 1:d T \u03b8 d T :d ] = 0.\nTherefore, the expectation of the above equation is the optimization direction of the projection of s j on the subspace (span(T )) \u22a5 :\n\u2202L DM \u2202s i,E \u22a5 T = E \u03b8 \u2202 d DM 2 E \u22a5 T \u2202s i,E \u22a5 T = 2E[(\u03b8 d T :d ) \u03b8 d T :d ] |S| 2 ( |S| j=1 s j,E \u22a5 T ) . (28\n)\nNote that E[(\u03b8\nd T :d ) \u03b8 d T :d ] = kI d\u2212d T ,\nthus the optimization direction is aligned with the barycenter of all s i,E \u22a5 T and will converge to 0 when\n1 |S| |S| i=1 s i,E \u22a5 T = 0 E \u22a5 T .(29)\nSince the initialization of S is essentially noise of standard normal distribution, the empirical average of s i,E \u22a5 T is close to 0 (by law of large numbers), thus we can consider that the projection component of minimizer s * i,E \u22a5 T is close to the initialized value, i.e.,\n\u2200s * i,E \u22a5 T \u2208 S * , s * i,E \u22a5 T \u2248 s i,E \u22a5 T . (30\n)\nSimilar as the case of real data initialization, the projection components on span(T ) of s i are optimized to verify the first property of Proposition 4.3, i.e., the projection component of i-th minimizer s * i,E T becomes\ns * i,E T = s i,E T + 1 |T | |T | j=1 x j \u2212 1 |S| |S| j=1 s j,E T .(31)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1. Empirical verification", "text": "We empirically verify our conclusions for random and real data initializations in Figure 6. The images are synthesized from CIFAR-10 by DM using linear extractor of embedding dimension 2048, and each line contains images from the same class.\nOn the right side, we plot the images synthesized with random initialization and real data initialization. We can observe that images synthesized with random initialization resemble combination of noise and class-dependent background, which verifies our conclusion of random initialization: synthetic data with random initialization are composed of barycenter of original data in space span and initialized noise in space span(T ) \u22a5 (see ( 13)). Note that even in this case, models trained on synthetic data can still achieve validation accuracy around 27%.\nOn the other hand, real data initialization generates little changes on the images used for initialization, which verifies the conclusion of real data initialization: synthetic data with real data initialization are composed of images used for initialization and the barycenter distance vector (see ( 12)).\nBesides linear extractor, we also investigate the impact of activation function. On the left of Figure 6, we show images synthesized by DM using ReLU-activated extractor (ReLU on top of linear extractor). We can see that the existence of  ReLU results in better convergence of DC and thus better image quality for both random and real data initialization. A potential reason is that ReLU changes the DC optimization and can lead to different local minima other than that found by using linear extractor. That is, data synthesized in this case are possibly composed by barycenter of a certain group of similar images (e.g., images of brown horse heading towards right with grass background) within the same class and a orthogonal noise vector. For example, CIFAR-10 synthetic images of class \"horse\" (third last line of leftmost figure in Figure 6) are noisy but contain different backgrounds which should be the barycenter of different image group of class \"horse\": there are numerous horse images in CIFAR-10 where the horse head towards the right or left. This observation also confirms that ReLU improves generalization of neural networks. Appendix D encompasses more detailed analysis for non-linear extractor. In addition, we denote p(\u03b8) = P(\u03b8|S) and q(\u03b8) = P(\u03b8|S ). The KL divergence between p and q is\nD KL (p||q) = \u03b8 p(\u03b8) ln p(\u03b8) q(\u03b8) d\u03b8 = \u03b8 1 K S exp (\u2212 |S| i=1 l(\u03b8, s 1 )) ln p(\u03b8) q(\u03b8) d\u03b8,(32)\nwhere ln p(\u03b8) q(\u03b8) =\n|S | i=1 l(\u03b8, s i ) \u2212 |S| i=1 l(\u03b8, s i ) + K S \u2212 K S = |S| i=1 (l(\u03b8, s i ) \u2212 l(\u03b8, s i )) + K S \u2212 K S \u2264 L |S| i=1 s i \u2212 s i 2 + |K S \u2212 K S | .(33)\nAccording to the assumption 4.8, K S (similar for K S ) is:\nK S := \u03b8 exp (\u2212 |S| i=1 l(\u03b8, s i ))d\u03b8.(34)\nSince x |T | is not used for real data initialization, according to the Proposition 4.4, if S and S share the same initialization type and initialized values, we have for each i\ns i \u2212 s i 2 = 1 |T | \u2212 1 |T |\u22121 j=1 x j \u2212 1 |T | |T | j=1 x j 2 = 1 |T | 1 |T | \u2212 1 |T |\u22121 j=1 x j \u2212 x |T | 2 \u2264 2B |T | .(35)\nThus, we have\nL |S| i=1 s i \u2212 s i 2 \u2264 2LB |S| |T | . (36\n)\nThe second term on the right side of ( 33) can be processed similarly:\n|K S \u2212 K S | = \u03b8 exp (\u2212 |S | i=1 l(\u03b8, s i )) \u2212 exp (\u2212 |S| i=1 l(\u03b8, s i ))d\u03b8 = \u03b8 [exp( |S| i=1 (l(\u03b8, s i ) \u2212 l(\u03b8, s i ))) \u2212 1] exp (\u2212 |S| i=1 l(\u03b8, s i ))d\u03b8 .(37)\nFrom previous analysis, we know that\n|S| i=1 (l(\u03b8, s i ) \u2212 l(\u03b8, s i )) \u2264 2LB |S| |T | . (38\n)\nSince exp(x) \u2212 1 = O(x) in the neighborhood of 0, we have\n|K S \u2212 K S | = O( 2LB |S| K S |T | ) = O( |S| |T | ).(39)\nNote that K S should decrease as |S| increases because an additional synthetic sample s introduces a factor exp(\u2212l(\u03b8, s)) \u2264 1 in the integral. We omit it here and assume K S varies little when |S| changes. Together with (32) and ( 33), we obtain the privacy bound by KL divergence:\nD KL (p||q) = O( |S| |T | ).(40)", "publication_ref": [], "figure_ref": ["fig_8", "fig_8", "fig_8"], "table_ref": []}, {"heading": "D. Generalization to Non-Linear Extractor", "text": "We consider 2-layer network as the extractor, i.e., linear extractor with ReLU activation, and show that the (pseudo)barycenters of S and T also coincide as claimed by Proposition 4.3. We then empirically validate the conclusion by plotting the L 2 distance between S and T during the condensation by DM for different r ipc on CIFAR-10 (see Figure 7).", "publication_ref": [], "figure_ref": ["fig_11"], "table_ref": []}, {"heading": "D.1. Analysis for 2-layer Network as Extractor", "text": "With activation function ReLU (noted as \u03c1), Equation ( 19) becomes:  which can be seen as a matrix depending on the angle \u03c6 between x and y. Even though each original data x i is varied by M (s j , x i ), their average can still be seen as a pseudo-barycenter, and the above equation signifies that each s j is updated towards minimizing the distance between the pseudo-barycenters of T and S, which verifies the first property of Proposition 4.3 on non-linear extractor. This further validates the privacy property of DM which is based on the connection between S and T .\nd ReLU DM := 1 |T | |T | i=1 \u03c1(\u03b8 \u2022 x i ) \u2212 1 |S| |S| i=1 \u03c1(\u03b8 \u2022 s i ).(41)\nNext, we empirically verify the Proposition 4.3 with tests on CIFAR-10.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.2. Empirical verification of Proposition 4.3 for non-linear extractor", "text": "Figure 7 shows the distance 1\n|S| |S| i=1 s i \u2212 1 |T | |T | i=1 x i 2\nfor each DM iteration on CIFAR-10. We can observe that the barycenter distance decreases with the iteration round, and achieves to the minimum. Note that the right subfigure of Figure 7 shows that the barycenters of T and S synthesized on 2-layer network (i.e., linear model activated by ReLU) have distance around 0, validating the theoretical analysis above. As for convolutional network (ConvNet), the distance decreases slower than 2-layer network. We suspect that the convolutional layers will lead the optimization to a local minimum. Figure 7 also validates the impact of r ipc and initialization to the distance of barycenters of S and T : 1) when the iteration round is around 0, the distance of 100 image per class is smaller than that of 50 image per class, 2) the real initialization has much lower distance than of random initialization at the beginning of DM optimization.", "publication_ref": [], "figure_ref": ["fig_11", "fig_11", "fig_11"], "table_ref": []}, {"heading": "E. Additional Experimental Details and Results", "text": "All experiments are conducted with Pytorch 1.10 on a Ubuntu 20.04 server. E.1. Details of Hyperparameters and Settings. DC Settings. We reproduced DM (Zhao & Bilen, 2021a) and adopt large learning rates to accelerate the condensation (i.e., 10, 50, 100 as learning rate for r ipc = 0.002, 0.01, 0.02, respectively). For DSA (Zhao & Bilen, 2021b), we adopt the default setting 1 for all datasets. For KIP, we reproduced in Pytorch according to the official code of (Nguyen et al., 2021a), and set learning rate 0.04 and 0.1 for r ipc = 0.002 and 0.01, respectively. Note that we omit r ipc = 0.02 for KIP and DSA due to the low efficiency. We also apply differentiable siamese augmentations (Zhao & Bilen, 2021b) for both DM and KIP.", "publication_ref": ["b42", "b43", "b26", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "E.2. Loss distribution of data used for DC initialization and test data on f S", "text": "In Figure 8, we show the distribution of f S losses evaluated on data used for DC initialization (Real Init) and data not used for initialization (Other). We can observe that the losses of data used for initialization are smaller than other data, showing that the membership of data used for DC initialization are easier to be inferred. The distribution difference also explains the high advantage scores in Table 1. ", "publication_ref": [], "figure_ref": ["fig_12"], "table_ref": ["tab_3"]}, {"heading": "E.3. Visualization of DC-synthesized data distribution", "text": "Figure 9 shows the t-SNE visualization of CIFAR-10 and CelebA data synthesized by GAN and DC methods (DSA, DM and KIP without ZCA preprocessing). We clip the DC-synthesized into 0 and 1 for fair comparison with GAN-synthesized data. Note that the generated data distributions of DM and DSA are more similar than KIP and GAN, explaining why DM-synthesized data and DSA-synthesized data enable models to achieve higher accuracy under same r ipc .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.4. MIA against cGANs", "text": "Our threat model assumes that the adversary has white-box access to the synthetic dataset. We apply the MIA against GANs proposed by Chen et al. (called GAN-leak). The main intuition is that member data are easier to be reconstructed by GAN The adversary optimizes L cal by varying z to estimate whether x belongs to the training dataset. According to the adversary' knowledge, the attack can be divided into black-box attack, partial black-box attack and white-box attack. We conducted the white-box attack for scenarios where the adversary has access to the generators. The results on CelebA are in Table 4, indicating that vanilla GAN can be used to infer the membership of training data. Chen et al. also validated that partial black-box attack can achieve similar attack performance as white-box, because the adversary has access to z and can leverage non-differentiable optimization, e.g., the Powell's Conjugate Direction Method (Powell, 1964)), to approximately minimize L cal . E.5. Comparison of accuracy for models trained on synthetic dataset for r ipc = 0.002\nFigure 10 presents the accuracy comparison results of models trained on data synthesized by DC and baseline methods for r ipc = 0.002. We can see that KIP significantly outperforms baselines and achieves similar performance with DSA and DM on CIFAR-10. Moreover, we can observe that the ZCA preprocessing is effective for improving the utility of KIP-synthesized dataset.", "publication_ref": ["b28"], "figure_ref": ["fig_0"], "table_ref": ["tab_8"]}, {"heading": "Acknowledgements", "text": "We would like to thank He Tong for the help in analyzing non-linear models and the anonymous reviewers for constructive feedback.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Since \u03b8 i,j iid \u223c N (0, 1) for each element \u03b8 i,j of \u03b8, for an input x = [x j ] 1\u2264j\u2264d \u2208 R d , we have\nwhere y i iid \u223c N (0, d j=1 x 2 j ). Since \u03c1(x) := max(0, x), we have \u03c1(y) = [max(0, y i )] 1\u2264i\u2264k . Define Y = max(0, X) where the random variable X \u223c N (0, \u03c3 2 ). Then, Y follows the same distribution of B |X|, where\nx j , and we can obtain\nwhere is element-wise multiplication, B = [B i ] 1\u2264i\u2264k and B i iid \u223c Bernoulli( 12 ). With this in mind, Equation ( 41) becomes:\nwhere vectors of Bernoulli random variable for each data samples B \u2022 i are independent. To simplify notation, we consider k = 1. The vector of Bernoulli random variable reduces to single random variable B \u2022 i , and the bold symbol d becomes d. Moreover, let sgn(x) denote the sign of x, and we can see that |x| = sgn(x)x for a real number x. Thus, with k = 1, we can reduce d ReLU DM to the similar form of ( 19): \nThus, the gradient of L DM on s j becomes:\n(47) Let M (x, y) denote E \u03b8 [sgn(\u03b8x)sgn(\u03b8y)\u03b8 \u03b8] \u2208 R d\u00d7d , then the above equation becomes: ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Flexible dataset distillation: Learn labels instead of images", "journal": "", "year": "2020", "authors": "O Bohdal; Y Yang; T Hospedales"}, {"ref_id": "b1", "title": "Don't generate me: Training differentially private generative models with sinkhorn divergence", "journal": "Advances in Neural Information Processing Systems", "year": "", "authors": "T Cao; A Bie; A Vahdat; S Fidler; K Kreis"}, {"ref_id": "b2", "title": "Membership inference attacks from first principles", "journal": "IEEE", "year": "2022", "authors": "N Carlini; S Chien; M Nasr; S Song; A Terzis; F Tramer"}, {"ref_id": "b3", "title": "Dataset distillation by matching training trajectories", "journal": "", "year": "2022", "authors": "G Cazenavette; T Wang; A Torralba; A A Efros; J.-Y Zhu"}, {"ref_id": "b4", "title": "Gs-wgan: A gradient-sanitized approach for learning differentially private generators", "journal": "", "year": "2020", "authors": "D Chen; T Orekondy; M Fritz"}, {"ref_id": "b5", "title": "Gan-leaks: A taxonomy of membership inference attacks against generative models", "journal": "", "year": "2020", "authors": "D Chen; N Yu; Y Zhang; M Fritz"}, {"ref_id": "b6", "title": "Label-only membership inference attacks", "journal": "PMLR", "year": "2021", "authors": "C A Choquette-Choo; F Tramer; N Carlini; N Papernot"}, {"ref_id": "b7", "title": "Calibrating noise to sensitivity in private data analysis", "journal": "", "year": "2006", "authors": "C Dwork; F Mcsherry; K Nissim; A Smith"}, {"ref_id": "b8", "title": "Signal recovery from pooling representations", "journal": "PMLR", "year": "2014", "authors": "J B Estrach; A Szlam; Y Lecun"}, {"ref_id": "b9", "title": "Model inversion attacks that exploit confidence information and basic countermeasures", "journal": "", "year": "2015", "authors": "M Fredrikson; S Jha; T Ristenpart"}, {"ref_id": "b10", "title": "Deep neural networks with random gaussian weights: A universal classification strategy?", "journal": "IEEE Transactions on Signal Processing", "year": "2016", "authors": "R Giryes; G Sapiro; A M Bronstein"}, {"ref_id": "b11", "title": "Generative adversarial nets", "journal": "", "year": "2014", "authors": "I Goodfellow; J Pouget-Abadie; M Mirza; B Xu; D Warde-Farley; S Ozair; A Courville; Y Bengio"}, {"ref_id": "b12", "title": "Dp-merf: Differentially private mean embeddings with randomfeatures for practical privacy-preserving data generation", "journal": "", "year": "2021", "authors": "F Harder; K Adamczewski; M Park"}, {"ref_id": "b13", "title": "Distilling the knowledge in a neural network", "journal": "", "year": "2015", "authors": "G Hinton; O Vinyals; J Dean"}, {"ref_id": "b14", "title": "Neural tangent kernel: Convergence and generalization in neural networks", "journal": "", "year": "2018", "authors": "A Jacot; C Hongler; F Gabriel"}, {"ref_id": "b15", "title": "Graph condensation for graph neural networks", "journal": "ICLR", "year": "2022", "authors": "W Jin; L Zhao; S Zhang; Y Liu; J Tang; N Shah"}, {"ref_id": "b16", "title": "The composition theorem for differential privacy", "journal": "PMLR", "year": "2015", "authors": "P Kairouz; S Oh; P Viswanath"}, {"ref_id": "b17", "title": "Learning multiple layers of features from tiny images", "journal": "", "year": "2009", "authors": "A Krizhevsky; G Hinton"}, {"ref_id": "b18", "title": "Gradient-based learning applied to document recognition", "journal": "Proceedings of the IEEE", "year": "1998", "authors": "Y Lecun; L Bottou; Y Bengio; P Haffner"}, {"ref_id": "b19", "title": "Membership leakage in label-only exposures", "journal": "", "year": "2021", "authors": "Z Li; Y Zhang"}, {"ref_id": "b20", "title": "Deep learning face attributes in the wild", "journal": "", "year": "2015-12", "authors": "Z Liu; P Luo; X Wang; X Tang"}, {"ref_id": "b21", "title": "Threats to federated learning", "journal": "Springer", "year": "2020", "authors": "L Lyu; H Yu; J Zhao; Yang ; Q "}, {"ref_id": "b22", "title": "Exploiting unintended feature leakage in collaborative learning", "journal": "", "year": "2019", "authors": "L Melis; C Song; E De Cristofaro; V Shmatikov"}, {"ref_id": "b23", "title": "R\u00e9nyi differential privacy", "journal": "IEEE Computer Society", "year": "2017-08-21", "authors": "I Mironov"}, {"ref_id": "b24", "title": "Conditional generative adversarial nets", "journal": "", "year": "2014", "authors": "M Mirza; S Osindero"}, {"ref_id": "b25", "title": "Adversary instantiation: Lower bounds for differentially private machine learning", "journal": "IEEE", "year": "2021", "authors": "M Nasr; S Songi; A Thakurta; N Papemoti; Carlin ; N "}, {"ref_id": "b26", "title": "Dataset meta-learning from kernel ridge-regression", "journal": "", "year": "2021", "authors": "T Nguyen; Z Chen; J Lee"}, {"ref_id": "b27", "title": "Dataset distillation with infinitely wide convolutional networks", "journal": "", "year": "2021", "authors": "T Nguyen; R Novak; L Xiao; J Lee"}, {"ref_id": "b28", "title": "An efficient method for finding the minimum of a function of several variables without calculating derivatives", "journal": "Comput. J", "year": "1964", "authors": "M J D Powell"}, {"ref_id": "b29", "title": "On the difficulty of membership inference attacks", "journal": "", "year": "2021", "authors": "S Rezaei; X Liu"}, {"ref_id": "b30", "title": "White-box vs black-box: Bayes optimal strategies for membership inference", "journal": "", "year": "2019", "authors": "A Sablayrolles; M Douze; C Schmid; Y Ollivier; H J\u00e9gou"}, {"ref_id": "b31", "title": "Membership inference attacks against machine learning models", "journal": "", "year": "2017", "authors": "R Shokri; M Stronati; C Song; V Shmatikov"}, {"ref_id": "b32", "title": "Generative teaching networks: Accelerating neural architecture search by learning to generate synthetic training data", "journal": "", "year": "", "authors": "F P Such; A Rawal; J Lehman; K O Stanley; Clune ; J "}, {"ref_id": "b33", "title": "Soft-label dataset distillation and text dataset distillation", "journal": "", "year": "2019", "authors": "I Sucholutsky; M Schonlau"}, {"ref_id": "b34", "title": "Cafe: Learning to condense dataset by aligning features", "journal": "", "year": "2022", "authors": "K Wang; B Zhao; X Peng; Z Zhu; S Yang; S Wang; G Huang; H Bilen; X Wang; Y You"}, {"ref_id": "b35", "title": "Dataset distillation. CoRR, abs/1811.10959", "journal": "", "year": "2018", "authors": "T Wang; J Zhu; A Torralba; A A Efros"}, {"ref_id": "b36", "title": "Automated program synthesis for differential privacy", "journal": "", "year": "2021", "authors": "Y Wang; Z Ding; Y Xiao; D Kifer; D Zhang;  Dpgen"}, {"ref_id": "b37", "title": "This person (probably) exists. identity membership attacks against gan generated faces", "journal": "", "year": "2021", "authors": "R Webster; J Rabin; L Simon; Jurie ; F "}, {"ref_id": "b38", "title": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms", "journal": "", "year": "2017", "authors": "H Xiao; K Rasul; R Vollgraf"}, {"ref_id": "b39", "title": "Differentially private generative adversarial network", "journal": "", "year": "2018", "authors": "L Xie; K Lin; S Wang; F Wang; J Zhou"}, {"ref_id": "b40", "title": "How does data augmentation affect privacy in machine learning?", "journal": "", "year": "2021", "authors": "D Yu; H Zhang; W Chen; J Yin; T.-Y Liu"}, {"ref_id": "b41", "title": "The unreasonable effectiveness of deep features as a perceptual metric", "journal": "", "year": "2018", "authors": "R Zhang; P Isola; A A Efros; E Shechtman; Wang ; O "}, {"ref_id": "b42", "title": "Dataset condensation with distribution matching. CoRR, abs", "journal": "", "year": "2021", "authors": "B Zhao; H Bilen"}, {"ref_id": "b43", "title": "Dataset condensation with differentiable siamese augmentation", "journal": "", "year": "2021", "authors": "B Zhao; H Bilen"}, {"ref_id": "b44", "title": "Dataset condensation with gradient matching", "journal": "", "year": "2021", "authors": "B Zhao; K R Mopuri; H Bilen"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. DC-synthesized data can be used for privacy-preserving model training and cannot be recovered through MIA and visual comparison analysis.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Proposition 4.4 (Connection between S * and T ). Based on Proposition 4.3, the minimizer synthetic dataset S * = {s * 1 , \u2022 \u2022 \u2022 , s * |S| } has the following properties for different initialization strategies:", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Assumption 4.8 (Distribution of model parameter (Sablayrolles et al., 2019)). The distribution of model parameter \u03b8 given training dataset T = {x 1 , \u2022 \u2022 \u2022 , x |T | } and loss function l is:", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Proposition 4.10. Suppose a target dataset T = {x 1 , \u2022 \u2022 \u2022 , x |T | } and the leave-one-out dataset T = T \\{x} such that x is not used for initialization. The synthetic datasets are S and S and |S| = |S | |T |. Denote the model parameter distributions of S and S by p(\u03b8) = P(\u03b8|S) and q(\u03b8) = P(\u03b8|S ) respectively. Then, the membership privacy leakage caused by removing x is D KL (p||q) = O( 10 indicates that the adversary can only obtain limited information (i.e., O( |S| |T | )) by MIA when the synthetic data is much fewer than the original data (|S| |T |), which explains why synthetic data S protects membership privacy of model f S . The proof is in Appendix C.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 3 .3Figure3. Accuracy of models trained on data synthesized by DSA, DM, KIP and on data generated by baselines for ripc = 0.01. The x-axis represents initialization strategy. For real data and random initialization, the baselines are real data and cGAN-generated data, respectively.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 4 .4Figure 4. Accuracy of models trained on cGAN-generated data for different ripc. The horizontal lines are accuracy of models trained on data synthesized by DM (green, dashed) and DSA (red, dash-dotted) for ripc = 0.01.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 6 .6Figure 6. Synthetic images with random noise/real data initialization and with linear/ReLU-activated extractor of CIFAR-10.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "C.Proof of Proposition 4.10 We aim to quantify the membership privacy leakage of a member x with the Kullback-Leibler (KL) divergence of model parameter distributions. Without loss of generality, we study how the last element x |T | influences the model parameter distribution. Let T denote T \\ {x |T | }, where T = {x 1 , \u2022 \u2022 \u2022 , x |T | }. The synthetic datasets by DM based on T and T are noted as S and S , respectively, and |S| = |S |.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 7 .7Figure 7. Distance ( \u2022 2 ) between barycenters of S and T deceases with the iteration round, which verifies first property of Proposition 4.3. The solid and dashed lines represent real data and random initialization, respectively. The blue and orange lines represent the cases where ripc equals to 0.01 and 0.02, respectively.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 8 .8Figure 8. Loss distribution of data used for DC initialization (Real Init) are smaller than data not used for initialization (Other).", "figure_data": ""}, {"figure_label": "910", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Figure 9 .Figure 10 .910Figure 9. Distribution visualization of CIFAR-10 (left) and CelebA (right) synthesized by GAN, DSA, DM and KIP (without ZCA preprocessing).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": ", with conclusion of Section 4.2, we study the privacy loss of models trained on DC-synthesized data in a DP manner: how does removing one sample in the original dataset impact models trained on synthetic dataset. Because of the randomness in model training, we base on the model parameter distribution assumption from", "figure_data": "datasets for different DC initializations. Finally, in Section4.3The reason ofchoosing DM is because of its high condensation efficiencyand utility for model training. We also verify the differencebetween DM and other DC methods (see Appendix E.3)in terms of the synthetic data distribution, indicating ourtheoretical results can be generalized to other methods tosome extent. Theoretical analysis of other DC methods (e.g.,DSA) is left as the future work.Overview. We briefly present an overview of the analysisthat consists of three parts. First, we clarify the assump-tions and notations in Section 4.1. Then, in Section 4.2,we analyse the connection between synthetic and original"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Advantage (%) of loss-based MIA against models trained on real data (baseline) and data synthesized by DSA, DM and KIP with real data initialization. 46.67 \u00b1 16.33 72.00 \u00b1 24.00 100.00 \u00b1 0.00", "figure_data": "Methodr ipcFashionMNSTCIFAR-10CelebAReal (baseline,0.002 0.0121.00 \u00b1 3.6792.80 \u00b1 5.3184.00 \u00b1 5.06non-private)0.0217.33 \u00b1 2.9182.60 \u00b1 5.5977.00 \u00b1 6.710.00278.17 \u00b1 3.2049.80 \u00b1 5.83 37.00 \u00b1 12.69DM0.0183.67 \u00b1 2.7764.20 \u00b1 4.77 47.00 \u00b1 19.520.0283.00 \u00b1 2.5668.20 \u00b1 7.35 53.00 \u00b1 14.18DSA0.002 0.0174.40 \u00b1 2.65 81.60 \u00b1 2.2755.40 \u00b1 8.20 56.60 \u00b1 2.9530.50 \u00b1 8.16 28.00 \u00b1 3.74KIP0.00267.83 \u00b1 4.5442.40 \u00b1 4.80 23.00 \u00b1 11.87(w/o ZCA)0.0170.00 \u00b1 2.4751.40 \u00b1 5.73 25.00 \u00b1 15.65KIP0.00267.67 \u00b1 4.4250(w/ ZCA)"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Advantage (%) of loss-based MIA against models trained on data synthesized by cGAN (baseline), DSA, DM and KIP with random initialization.", "figure_data": "Methodsr ipcFashionMNSTCIFAR-10CelebAcGAN0.0020.29 \u00b1 0.89\u22120.44 \u00b1 1.88 \u22120.57 \u00b1 0.97(baseline,0.010.18 \u00b1 1.21\u22120.58 \u00b1 2.09 \u22120.81 \u00b1 0.95non-private)0.020.04 \u00b1 0.70\u22120.77 \u00b1 1.59 \u22120.47 \u00b1 1.220.002\u22120.34 \u00b1 0.420.31 \u00b1 1.93\u22120.66 \u00b1 1.44DM0.01\u22120.29 \u00b1 0.481.06 \u00b1 1.20\u22120.56 \u00b1 1.520.020.18 \u00b1 0.530.72 \u00b1 0.70\u22120.67 \u00b1 1.18DSA0.002 0.010.09 \u00b1 0.51 0.52 \u00b1 0.550.39 \u00b1 1.04 1.27 \u00b1 1.71\u22120.39 \u00b1 1.90 \u22121.16 \u00b1 0.90KIP0.002\u22121.13 \u00b1 1.840.25 \u00b1 1.20\u22120.56 \u00b1 1.07(w/o ZCA)0.01\u22120.95 \u00b1 0.960.25 \u00b1 1.80\u22121.51 \u00b1 0.69KIP0.002\u22120.56 \u00b1 2.02\u22120.64 \u00b1 1.86 \u22121.06 \u00b1 1.10(w/ ZCA)0.01\u22121.69 \u00b1 1.96\u22120.22 \u00b1 1.27 \u22121.80 \u00b1 1.91"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Utility comparison of dataset synthesized by DPgenerators, DM and KIP. The utility is measured by the accuracy (%) of models trained on the synthetic dataset. The results are estimated on FashionMNIST.", "figure_data": "MethodDP Budget0.002r ipc 0.010.02GS-WGAN= 1053.53 \u00b1 0.42 51.85 \u00b1 0.54 50.10 \u00b1 0.32DP-MERF= 10 = 252.18 \u00b1 0.37 52.88 \u00b1 0.75 50.73 \u00b1 0.66 60.41 \u00b1 0.78 55.14 \u00b1 0.61 56.39 \u00b1 0.45DP-Sinkhorn= 10--70.9  *KIP (w/o ZCA)\u02c6 = 1.2573.70 \u00b1 1.13 68.11 \u00b1 1.33-KIP (w/ ZCA)\u02c6 = 2.0774.37 \u00b1 0.96 70.03 \u00b1 0.84-DM\u02c6 = 2.3080.59 \u00b1 0.62 85.10 \u00b1 0.51 86.13 \u00b1 0.34"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "ROC curves of LiRA against models trained on data synthesized by DM (left three figures) and KIP (right three figures). The solid, dashed and dotted lines stand for results of ripc = 0.002, 0.01 and 0.02, respectively. In KIP figures, the orange and blue lines represent the results of KIP with and without ZCA preprocessing, respectively. The red diagonal represents random guess and the AUC scores of ROC curves are all under 0.51.", "figure_data": "10 0KIP CIFAR-1010 0CelebATrue Positive Rate10 2 10 1Method w/o zca w/ zca ripc 0.002 0.01True Positive Rate10 5 False Positive Rate 10 3 10 1 10 5 10 4 10 3 10 2 10 1 Method w/o zca w/ zca ripc 0.002 0.01Figure 2. Real Random 0.60 0.65 0.70 0.75 0.80 0.85 0.90 Accuracy FashionMNIST Baseline DSA 0.25 0.50 0.60 0.55 0.45 0.40 0.35 0.30Real Random CIFAR-10 DM KIP (w/o ZCA) 0.85 0.80 0.75 0.70 0.65 0.60 Real Random CelebA KIP (w/ ZCA)"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Results of GAN-leak attack against cGANs averaged over 10 shadow models.", "figure_data": "DatasetROC AUCAdvantage (%)CelebA 56.06 \u00b1 2.0322.98 \u00b1 4.27"}], "formulas": [{"formula_id": "formula_0", "formula_text": "E x\u223cP D [L(\u03c6 \u03b8 T (x), y)] E x\u223cP D [L(\u03c6 \u03b8 S (x), y)], (1)", "formula_coordinates": [2.0, 320.19, 505.33, 221.25, 10.51]}, {"formula_id": "formula_1", "formula_text": "arg min S L T (\u03b8 S (S)),(2)", "formula_coordinates": [2.0, 383.02, 619.17, 158.42, 18.94]}, {"formula_id": "formula_2", "formula_text": "arg min Xs 1 2 y t \u2212 K XtXs (K XsXs + \u03bbI) \u22121 y s 2 , (3)", "formula_coordinates": [3.0, 70.91, 116.53, 218.53, 23.26]}, {"formula_id": "formula_3", "formula_text": "K(u, v) = \u2207 \u03b8 \u03c6 \u03b8 (u) \u2022 \u2207 \u03b8 \u03c6 \u03b8 (v).", "formula_coordinates": [3.0, 135.23, 202.08, 130.59, 9.65]}, {"formula_id": "formula_4", "formula_text": "arg min S E \u03b80\u223cP \u03b8 0 [ T \u22121 t=0 D(\u2207 \u03b8 L S (\u03b8 t ), \u2207 \u03b8 L T (\u03b8 t ))], (4)", "formula_coordinates": [3.0, 65.63, 279.66, 223.81, 30.2]}, {"formula_id": "formula_5", "formula_text": "D(\u2207 \u03b8 L(\u03b8 t , A \u03c9 (S)), \u2207 \u03b8 L(\u03b8 t , A \u03c9 (T ))).(5)", "formula_coordinates": [3.0, 89.96, 421.42, 199.48, 9.65]}, {"formula_id": "formula_6", "formula_text": "min S E \u03d1\u223cP \u03d1 ,\u03c9\u223c\u2126 1 |T | |T | i=1 \u03c8 \u03d1 (A(x i , \u03c9)) \u2212 1 |S| |S| j=1 \u03c8 \u03d1 (A(s j , \u03c9)) 2 ,(6)", "formula_coordinates": [3.0, 91.24, 566.09, 198.2, 67.46]}, {"formula_id": "formula_7", "formula_text": "M (x) = 1(l(x) \u2264 \u03c4 ),(7)", "formula_coordinates": [3.0, 379.4, 280.01, 162.04, 12.01]}, {"formula_id": "formula_8", "formula_text": "f for (x, y) is \u03c6(f (x) y ) = \u03c6(exp(\u2212l(f (x), y))),", "formula_coordinates": [3.0, 307.44, 596.27, 232.93, 21.61]}, {"formula_id": "formula_9", "formula_text": "\u039b = p(conf obs |N (\u00b5 in , \u03c3 2 in )) p(conf obs |N (\u00b5 out , \u03c3 2 out )) ,(8)", "formula_coordinates": [3.0, 359.11, 662.16, 182.33, 25.76]}, {"formula_id": "formula_10", "formula_text": "dataset S = {s 1 , \u2022 \u2022 \u2022 , s |S| } and the target dataset T = {x 1 , \u2022 \u2022 \u2022 , x |T | }.", "formula_coordinates": [4.0, 307.44, 245.48, 234.0, 21.94]}, {"formula_id": "formula_11", "formula_text": "span(T ) := { |T | i=1 w i x i |1 \u2264 i \u2264 |T | , w i \u2208 R, x i \u2208 T }.(9)", "formula_coordinates": [4.0, 315.99, 350.27, 225.45, 40.77]}, {"formula_id": "formula_12", "formula_text": "d ) E = {e 1 , \u2022 \u2022 \u2022 , e d } among which the first d T basis vectors E T = {e 1 , \u2022 \u2022 \u2022 , e d T }", "formula_coordinates": [4.0, 307.44, 449.02, 234.67, 35.75]}, {"formula_id": "formula_13", "formula_text": "1 |T | |T | i=1 x i \u2212 1 |S * | |S * | i=1 s * i = 0,(10)", "formula_coordinates": [4.0, 364.13, 666.01, 177.31, 32.83]}, {"formula_id": "formula_14", "formula_text": "\u2200s * i \u2208 S * , s * i = s * i,E T + s * i,E \u22a5 T ,where", "formula_coordinates": [4.0, 318.45, 705.47, 151.14, 15.54]}, {"formula_id": "formula_15", "formula_text": "s * i,E T \u2208 span(T ), s * i,E \u22a5 T \u2208 span(T ) \u22a5 that verifies |S| i=1 s * i,E \u22a5 T = 0 E \u22a5 T .(11)", "formula_coordinates": [4.0, 472.37, 705.47, 70.81, 13.09]}, {"formula_id": "formula_16", "formula_text": "s * 1 = 1 |T | |T | i=1 x i when |S * | = 1,", "formula_coordinates": [5.0, 54.94, 153.9, 234.5, 24.68]}, {"formula_id": "formula_17", "formula_text": "s * i = x i + 1 |T | |T | j=1 x j \u2212 1 |S| |S| j=1 s j \u2208 span(T ). (12)", "formula_coordinates": [5.0, 68.87, 339.06, 220.57, 31.18]}, {"formula_id": "formula_18", "formula_text": "s * i = s * i,E T + s * i,E \u22a5 T ,(13)", "formula_coordinates": [5.0, 133.06, 441.36, 156.39, 15.54]}, {"formula_id": "formula_19", "formula_text": "s * i,E T = s i,E T + 1 |T | |T | j=1 x j \u2208 span(T ), and s * i,E \u22a5 T = s i,E \u22a5 T \u2208 (span(T )) \u22a5 .", "formula_coordinates": [5.0, 55.44, 467.4, 234.0, 31.57]}, {"formula_id": "formula_20", "formula_text": "0 < A \u2264 B such that \u2200(x, y) \u2208 (R d ) 2 , A x \u2212 y 2 \u2264 \u03c1(\u03b8x) \u2212 \u03c1(\u03b8y) 2 \u2264 B x \u2212 y 2 ,", "formula_coordinates": [5.0, 55.44, 585.13, 234.0, 24.68]}, {"formula_id": "formula_21", "formula_text": "P(\u03b8|T ) = 1 K T exp (\u2212 |T | i=1 l(\u03b8, x i )),(14)", "formula_coordinates": [5.0, 351.14, 634.41, 190.3, 31.18]}, {"formula_id": "formula_22", "formula_text": "\u2203B > 0, \u2200x \u2208 T \u222a S, x 2 \u2264 B. (15", "formula_coordinates": [6.0, 105.01, 214.15, 180.28, 11.18]}, {"formula_id": "formula_23", "formula_text": ")", "formula_coordinates": [6.0, 285.29, 214.5, 4.15, 8.64]}, {"formula_id": "formula_24", "formula_text": "\u2022) : R d \u2192 R + is L-Lipschitz accord- ing to the L 2 norm, i.e., \u2200(x, y) \u2208 B(B) 2 , \u03b8, |l(\u03b8, x) \u2212 l(\u03b8, y)| \u2264 L x \u2212 y 2 , (16", "formula_coordinates": [6.0, 55.44, 233.46, 235.65, 55.29]}, {"formula_id": "formula_25", "formula_text": ") where B(B) = {x| x \u2264 B} is the close ball of space R d .", "formula_coordinates": [6.0, 55.08, 280.12, 236.09, 20.93]}, {"formula_id": "formula_26", "formula_text": "ln P(M(D) \u2208 S M ) P(M(D ) \u2208 S M ) \u2264 (18", "formula_coordinates": [6.0, 121.96, 594.7, 163.33, 23.23]}, {"formula_id": "formula_27", "formula_text": ")", "formula_coordinates": [6.0, 285.29, 601.76, 4.15, 8.64]}, {"formula_id": "formula_28", "formula_text": "d DM 2", "formula_coordinates": [12.0, 60.42, 138.52, 30.89, 12.62]}, {"formula_id": "formula_29", "formula_text": "d DM := \u03b8( 1 |T | |T | i=1 x i \u2212 1 |S| |S| i=1 s i ). (19", "formula_coordinates": [12.0, 223.87, 153.8, 313.42, 31.18]}, {"formula_id": "formula_30", "formula_text": ")", "formula_coordinates": [12.0, 537.29, 165.38, 4.15, 8.64]}, {"formula_id": "formula_31", "formula_text": "Hence, L DM = E \u03b8\u223cN (0,1) d DM 2 .", "formula_coordinates": [12.0, 55.44, 191.87, 148.01, 12.92]}, {"formula_id": "formula_32", "formula_text": "\u2202 d DM 2 \u2202s i = \u2212 2 |S| (d DM ) \u03b8 = \u2212 2 |S| ( 1 |T | |T | j=1 x j \u2212 1 |S| |S| j=1 s j ) \u2022 \u03b8 \u03b8.(20)", "formula_coordinates": [12.0, 144.55, 229.24, 396.89, 31.18]}, {"formula_id": "formula_33", "formula_text": "\u2202L DM \u2202s i = \u2202E \u03b8 d DM 2 \u2202s i = E \u03b8 \u2202 d DM 2 \u2202s i = \u2212 2 |S| ( 1 |T | |T | j=1 x j \u2212 1 |S| |S| j=1 s j ) \u2022 E[\u03b8 \u03b8],(21)", "formula_coordinates": [12.0, 119.0, 297.12, 422.44, 31.18]}, {"formula_id": "formula_34", "formula_text": "s * i = x i + 1 |T | |T | j=1 x j \u2212 1 |S| |S| j=1 s j .(22)", "formula_coordinates": [12.0, 226.52, 491.75, 314.92, 31.18]}, {"formula_id": "formula_35", "formula_text": "\u2200s i \u2208 S, s i \u223c N (0, I d ). (23", "formula_coordinates": [12.0, 249.85, 591.45, 287.44, 9.68]}, {"formula_id": "formula_36", "formula_text": ")", "formula_coordinates": [12.0, 537.29, 591.8, 4.15, 8.64]}, {"formula_id": "formula_37", "formula_text": "d DM 2 = \u03b8 1:d T Proj E T (\u2206 S,T ) 2 + \u03b8 d T :d Proj E \u22a5 T (\u2206 S,T ) 2 d DM 2 E \u22a5 T + 2 < \u03b8 1:d T Proj E T (\u2206 S,T ), \u03b8 d T :d Proj E \u22a5 T (\u2206 S,T ) >,(24)", "formula_coordinates": [12.0, 175.84, 661.05, 365.61, 59.96]}, {"formula_id": "formula_38", "formula_text": "\u2206 S,T := 1 |T | |T | i=1 x i \u2212 1 |S| |S| i=1 s i .(25)", "formula_coordinates": [13.0, 230.21, 93.59, 311.23, 31.18]}, {"formula_id": "formula_39", "formula_text": "Proj E \u22a5 T (\u2206 S,T ) = 1 |S| |S| i=1 Proj E \u22a5 T (s i ),(26)", "formula_coordinates": [13.0, 223.2, 140.56, 318.25, 31.18]}, {"formula_id": "formula_40", "formula_text": "E \u03b8 \u2202 d DM 2 E \u22a5 T \u2202s i,E \u22a5 T = 2 |S| 2 ( |S| j=1 s j,E \u22a5 T ) E \u03b8 [(\u03b8 d T :d ) \u03b8 d T :d ],(27)", "formula_coordinates": [13.0, 183.61, 201.81, 357.83, 32.11]}, {"formula_id": "formula_41", "formula_text": "E \u03b8 [\u03b8 1:d T \u03b8 d T :d ] = 0.", "formula_coordinates": [13.0, 88.56, 248.01, 84.44, 11.27]}, {"formula_id": "formula_42", "formula_text": "\u2202L DM \u2202s i,E \u22a5 T = E \u03b8 \u2202 d DM 2 E \u22a5 T \u2202s i,E \u22a5 T = 2E[(\u03b8 d T :d ) \u03b8 d T :d ] |S| 2 ( |S| j=1 s j,E \u22a5 T ) . (28", "formula_coordinates": [13.0, 166.35, 280.81, 370.94, 32.11]}, {"formula_id": "formula_43", "formula_text": ")", "formula_coordinates": [13.0, 537.29, 293.1, 4.15, 8.64]}, {"formula_id": "formula_44", "formula_text": "d T :d ) \u03b8 d T :d ] = kI d\u2212d T ,", "formula_coordinates": [13.0, 114.06, 326.17, 99.92, 10.3]}, {"formula_id": "formula_45", "formula_text": "1 |S| |S| i=1 s i,E \u22a5 T = 0 E \u22a5 T .(29)", "formula_coordinates": [13.0, 256.41, 349.57, 285.03, 31.18]}, {"formula_id": "formula_46", "formula_text": "\u2200s * i,E \u22a5 T \u2208 S * , s * i,E \u22a5 T \u2248 s i,E \u22a5 T . (30", "formula_coordinates": [13.0, 242.19, 424.19, 295.1, 15.54]}, {"formula_id": "formula_47", "formula_text": ")", "formula_coordinates": [13.0, 537.29, 426.58, 4.15, 8.64]}, {"formula_id": "formula_48", "formula_text": "s * i,E T = s i,E T + 1 |T | |T | j=1 x j \u2212 1 |S| |S| j=1 s j,E T .(31)", "formula_coordinates": [13.0, 209.19, 493.91, 332.25, 31.18]}, {"formula_id": "formula_49", "formula_text": "D KL (p||q) = \u03b8 p(\u03b8) ln p(\u03b8) q(\u03b8) d\u03b8 = \u03b8 1 K S exp (\u2212 |S| i=1 l(\u03b8, s 1 )) ln p(\u03b8) q(\u03b8) d\u03b8,(32)", "formula_coordinates": [14.0, 141.37, 468.34, 400.08, 31.18]}, {"formula_id": "formula_50", "formula_text": "|S | i=1 l(\u03b8, s i ) \u2212 |S| i=1 l(\u03b8, s i ) + K S \u2212 K S = |S| i=1 (l(\u03b8, s i ) \u2212 l(\u03b8, s i )) + K S \u2212 K S \u2264 L |S| i=1 s i \u2212 s i 2 + |K S \u2212 K S | .(33)", "formula_coordinates": [14.0, 230.96, 519.6, 310.48, 106.52]}, {"formula_id": "formula_51", "formula_text": "K S := \u03b8 exp (\u2212 |S| i=1 l(\u03b8, s i ))d\u03b8.(34)", "formula_coordinates": [14.0, 230.7, 659.92, 310.74, 31.18]}, {"formula_id": "formula_52", "formula_text": "s i \u2212 s i 2 = 1 |T | \u2212 1 |T |\u22121 j=1 x j \u2212 1 |T | |T | j=1 x j 2 = 1 |T | 1 |T | \u2212 1 |T |\u22121 j=1 x j \u2212 x |T | 2 \u2264 2B |T | .(35)", "formula_coordinates": [15.0, 205.92, 90.9, 335.52, 100.66]}, {"formula_id": "formula_53", "formula_text": "L |S| i=1 s i \u2212 s i 2 \u2264 2LB |S| |T | . (36", "formula_coordinates": [15.0, 238.94, 218.12, 298.35, 31.18]}, {"formula_id": "formula_54", "formula_text": ")", "formula_coordinates": [15.0, 537.29, 229.71, 4.15, 8.64]}, {"formula_id": "formula_55", "formula_text": "|K S \u2212 K S | = \u03b8 exp (\u2212 |S | i=1 l(\u03b8, s i )) \u2212 exp (\u2212 |S| i=1 l(\u03b8, s i ))d\u03b8 = \u03b8 [exp( |S| i=1 (l(\u03b8, s i ) \u2212 l(\u03b8, s i ))) \u2212 1] exp (\u2212 |S| i=1 l(\u03b8, s i ))d\u03b8 .(37)", "formula_coordinates": [15.0, 167.36, 281.81, 374.08, 76.81]}, {"formula_id": "formula_56", "formula_text": "|S| i=1 (l(\u03b8, s i ) \u2212 l(\u03b8, s i )) \u2264 2LB |S| |T | . (38", "formula_coordinates": [15.0, 227.64, 396.08, 309.65, 31.18]}, {"formula_id": "formula_57", "formula_text": ")", "formula_coordinates": [15.0, 537.29, 407.67, 4.15, 8.64]}, {"formula_id": "formula_58", "formula_text": "|K S \u2212 K S | = O( 2LB |S| K S |T | ) = O( |S| |T | ).(39)", "formula_coordinates": [15.0, 210.37, 462.25, 331.07, 22.31]}, {"formula_id": "formula_59", "formula_text": "D KL (p||q) = O( |S| |T | ).(40)", "formula_coordinates": [15.0, 252.41, 539.05, 289.03, 22.31]}, {"formula_id": "formula_60", "formula_text": "d ReLU DM := 1 |T | |T | i=1 \u03c1(\u03b8 \u2022 x i ) \u2212 1 |S| |S| i=1 \u03c1(\u03b8 \u2022 s i ).(41)", "formula_coordinates": [15.0, 201.5, 689.22, 339.94, 31.18]}, {"formula_id": "formula_61", "formula_text": "|S| |S| i=1 s i \u2212 1 |T | |T | i=1 x i 2", "formula_coordinates": [17.0, 175.54, 380.59, 118.17, 18.1]}], "doi": ""}