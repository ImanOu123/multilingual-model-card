{"title": "Moser Flow: Divergence-based Generative Modeling on Manifolds", "authors": "Noam Rozen; Aditya Grover; Maximilian Nickel; Yaron Lipman", "pub_date": "2021-11-02", "abstract": "We are interested in learning generative models for complex geometries described via manifolds, such as spheres, tori, and other implicit surfaces. Current extensions of existing (Euclidean) generative models are restricted to specific geometries and typically suffer from high computational costs. We introduce Moser Flow (MF), a new class of generative models within the family of continuous normalizing flows (CNF). MF also produces a CNF via a solution to the change-of-variable formula, however differently from other CNF methods, its model (learned) density is parameterized as the source (prior) density minus the divergence of a neural network (NN). The divergence is a local, linear differential operator, easy to approximate and calculate on manifolds. Therefore, unlike other CNFs, MF does not require invoking or backpropagating through an ODE solver during training. Furthermore, representing the model density explicitly as the divergence of a NN rather than as a solution of an ODE facilitates learning high fidelity densities. Theoretically, we prove that MF constitutes a universal density approximator under suitable assumptions. Empirically, we demonstrate for the first time the use of flow models for sampling from general curved surfaces and achieve significant improvements in density estimation, sample quality, and training complexity over existing CNFs on challenging synthetic geometries and real-world benchmarks from the earth and climate sciences. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).", "sections": [{"heading": "Introduction", "text": "The major successes of deep generative models in recent years are primarily in domains involving Euclidean data, such as images (Dhariwal and Nichol, 2021), text (Brown et al., 2020), and video (Kumar et al., 2019). However, many kinds of scientific data in the real world lie in non-Euclidean spaces specified as manifolds. Examples include planetary-scale data for earth and climate sciences (Mathieu and Nickel, 2020), protein interactions and brain imaging data for life sciences (Gerber et al., 2010;Chen et al., 2012), as well as 3D shapes in computer graphics (Hoppe et al., 1992;Kazhdan et al., 2006). Existing (Euclidean) generative models cannot be effectively applied in these scenarios as they would tend to assign some probability mass to areas outside the natural geometry of these domains.\nAn effective way to impose geometric domain constraints for deep generative modeling is to design normalizing flows that operate in the desired manifold space. A normalizing flow maps a prior (source) distribution to a target distribution via the change of variables formula (Rezende and Mohamed, 2015;Dinh et al., 2016;Papamakarios et al., 2019). Early work in this direction proposed invertible architectures for learning probability distributions directly over the specific manifolds defined over spheres and tori (Rezende et al., 2020). Recently, Mathieu and Nickel (2020) proposed to extend continuous normalizing flows (CNF) (Chen et al., 2018) for generative modeling over Riemannian manifolds wherein the flows are defined via vector fields on manifolds and computed as the solution to an associated ordinary differential equation (ODE). CNFs have the advantage that the neural network architectures parameterizing the flow need not be restricted via invertibility constraints. However, as we show in our experiments, existing CNFs such as FFJORD (Grathwohl et al., 2018) and Riemannian CNFs (Mathieu and Nickel, 2020) can be slow to converge and the generated samples can be inferior in capturing the details of high fidelity data densities. Moreover, it is a real challenge to apply Riemannian CNFs to complex geometries such as general curved surfaces.\nTo address these challenges, we propose Moser Flows (MF), a new class of deep generative models within the CNF family. An MF models the desired target density as the source density minus the divergence of an (unrestricted) neural network. The divergence is a local, linear differential operator, easy to approximate and calculate on manifolds. By drawing on classic results in differential geometry by Moser (1965) and Dacorogna and Moser (1990), we can show that this parameterization induces a CNF solution to the change-of-variables formula specified via an ODE. Since MFs directly parameterize the model density using the divergence, unlike other CNF methods, we do not require to explicitly solve the ODE for maximum likelihood training. At test-time, we use the ODE solver for generation. We derive extensions to MFs for Euclidean submanifolds that efficiently parameterize vector fields projected to the desired manifold domain. Theoretically, we prove that Moser Flows are universal generative models over Euclidean submanifolds. That is, given a Euclidean submanifold M and a target continuous positive probability density \u00b5 over M, MFs can push arbitrary positive source density \u03bd over M to densities\u03bc that are arbitrarily close to \u00b5.\nWe evaluate Moser Flows on a wide range of challenging real and synthetic problems defined over many different domains. On synthetic problems, we demonstrate improvements in convergence speed for attaining a desired level of details in generation quality. We then experiment with two kinds of complex geometries. First, we show significant improvements of 49% on average over Riemannian CNFs (Mathieu and Nickel, 2020) for density estimation as well as high-fidelity generation on 4 earth and climate science datasets corresponding to global locations of volcano eruptions, earthquakes, floods, and wildfires on spherical geometries. Next and last, we go beyond spherical geometries to demonstrate for the first time, generative models on general curved surfaces.", "publication_ref": ["b8", "b3", "b22", "b27", "b14", "b4", "b17", "b20", "b31", "b9", "b30", "b32", "b27", "b5", "b15", "b27", "b29", "b7", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "Riemannian manifolds. We consider an orientable, compact, boundaryless, connected ndimensional Riemannian manifold M with metric g. We denote points in the manifold by x, y \u2208 M. At every point x \u2208 M, T x M is an n-dimensional tangent plane to M. The metric g prescribes an inner product on each tangent space; for v, u \u2208 T x M, their inner product w.r.t. g is denoted by v, u g . X(M) is the space of smooth (tangent) vector fields to M; that is, if u \u2208 X(M) then u(x) \u2208 T x M, for all x \u2208 M, and if u written in local coordinates it consists of smooth functions. We denote by dV the Riemannian volume form, defined by the metric g over the manifold M. In particular, V (A) = A dV is the volume of the set A \u2282 M.\nWe consider probability measures over M that are represented by strictly positive continuous density functions \u00b5, \u03bd : M \u2192 R >0 , where \u00b5 by convention represents the target (unknown) distribution and \u03bd represents the source (prior) distribution. \u00b5, \u03bd are probability densities in the sense their integral w.r.t. the Riemannian volume form is one, i.e., M \u00b5dV = 1 = M \u03bddV . It is convenient to consider the volume forms that correspond to the probability measures, namely\u03bc = \u00b5dV and\u03bd = \u03bddV . Volume forms are differential n-forms that can be integrated over subdomains of M, for example, p \u03bd (A) = A\u03bd is the probability of the event A \u2282 M.\nContinuous Normalizing Flows (CNF) on manifolds operate by transforming a simple source distribution through a map \u03a6 into a highly complex and multimodal target distribution. A manifold CNF, \u03a6 : M \u2192 M, is an orientation preserving diffeomorphism from the manifold to itself (Mathieu and Nickel, 2020;Lou et al., 2020;Falorsi and Forr\u00e9, 2020). A smooth map \u03a6 : M \u2192 M can be used to pull-back the target\u03bc according to the formula:\n(\u03a6 * \u03bc ) z (v 1 , . . . , v n ) =\u03bc \u03a6(z) (D\u03a6 z (v 1 ), . . . , D\u03a6 z (v n )),(1)\nwhere v 1 , . . . , v n \u2208 T z M are arbitrary tangent vectors, D\u03a6 z : T z M \u2192 T \u03a6(z) M is the differential of \u03a6, namely a linear map between the tangent spaces to M at the points z and \u03a6(z), respectively. By pulling-back\u03bc according to \u03a6 and asking it to equal to the prior density \u03bd, we get the manifold version of the standard normalizing equation:\n\u03bd = \u03a6 * \u03bc .(2)\nIf the normalizing equation holds, then for an event A \u2282 M we have that\np \u03bd (A) = A\u03bd = A \u03a6 * \u03bc = \u03a6(A)\u03bc = p \u00b5 (\u03a6(A)).\nTherefore, given a random variable z distributed according to \u03bd, then x = \u03a6(z) is distributed according to \u00b5, and \u03a6 is the generator.\nOne way to construct a CNF \u03a6 is by solving an ordinary differential equation (ODE) (Chen et al., 2018;Mathieu and Nickel, 2020). Given a time-dependent vector field v t \u2208 X(M) with t \u2208 [0, 1], a one-parameter family of diffeomorphisms (CNFs)\n\u03a6 t : [0, 1] \u00d7 M \u2192 M is defined by d dt \u03a6 t = v t (\u03a6 t ),(3)\nwhere this ODE is initialized with the identity transformation, i.e., for all x \u2208 M we initialize \u03a6 0 (x) = x. The CNF is then defined by \u03a6(x) = \u03a6 1 (x).\nExample: Euclidean CNF. Let us show how the above notions boil down to standard Euclidean CNF for the choice of M = R n , and the standard Euclidean metric; we denote z = (z 1 , . . . , z n ) \u2208 R n . The Riemannian volume form in this case is dz = dz 1 \u2227dz 2 \u2227\u2022 \u2022 \u2022\u2227dz n . Furthermore,\u03bc(z) = \u00b5(z)dz and\u03bd(z) = \u03bd(z)dz. The pull-back formula (equation 1) in coordinates (see e.g., Proposition 14.20 in Lee (2013)) is\n\u03a6 * \u03bc (z) = \u00b5(\u03a6(z)) det(D\u03a6 z )dz,\nwhere D\u03a6 z is the matrix of partials of \u03a6 at point z, (D\u03a6 z ) ij = \u2202\u03a6 i \u2202z j (z). Plugging this in equation 2 we get the Euclidean normalizing equation:\n\u03bd(z) = \u00b5(\u03a6(z)) det(D\u03a6 z ).(4)\n3 Moser Flow Moser (1965) and Dacorogna and Moser (1990) suggested a method for solving the normalizing equation, that is equation 2. Their method explicitly constructs a vector field v t , and the flow it defines via equation 3 is guaranteed to solve equation 2. We start by introducing the method, adapted to our needs, followed by its application to generative modeling. We will use notations introduced above.", "publication_ref": ["b27", "b26", "b13", "b5", "b27", "b24", "b29", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Solving the normalizing equation", "text": "Moser's approach to solving equation 2 starts by interpolating the source and target distributions. That is, choosing an interpolant \u03b1 t : [0, 1] \u00d7 M \u2192 R >0 , such that \u03b1 0 = \u03bd, \u03b1 1 = \u00b5, and M \u03b1 t dV = 1 for all t \u2208 [0, 1]. Then, a time-dependent vector field v t \u2208 X(M) is defined so that for each time t \u2208 [0, 1], the flow \u03a6 t defined by equation 3 satisfies the continuous normalization equation:\n\u03a6 * t\u03b1t =\u03b1 0 ,(5)\nwhere\u03b1 t = \u03b1 t dV is the volume form corresponding to the density \u03b1 t . Clearly, plugging t = 1 in the above equation provides a solution to equation 2 with \u03a6 = \u03a6 1 . As it turns out, considering the continuous normalization equation simplifies matters and the sought after vector field v t is constructed as follows. First, solve the partial differential equation (PDE) over the manifold M\ndiv(u t ) = \u2212 d dt \u03b1 t ,(6)\nwhere u t \u2208 X(M) is an unknown time-dependent vector field, and div is the Riemannian generalization to the Euclidean divergence operator, div E = \u2207\u2022. This manifold divergence operator is defined by replacing the directional derivative of the Euclidean space with its Riemannian version, namely the covariant derivative,\ndiv(u) = n i=1 \u2207 ei u, e i g ,(7)\nwhere {e i } n i=1 is an orthonormal frame according to the Riemannian metric g, and \u2207 \u03be u is the Riemannian covariant derivative. Note that here we assume that M is boundaryless, otherwise we need u t to be also tangent to the boundary of M. Second, define\nv t = u t \u03b1 t .(8)\nTheorem 2 in Moser (1965) implies: Theorem 1 (Moser). The diffeomorphism \u03a6 = \u03a6 1 , defined by the ODE in equation 3 and vector field v t in equation 8 solves the normalization equation, i.e., equation 2. source density \u03bd in green, target \u00b5 in blue. The vector field v t (black) is guaranteed to push \u03bd to interpolated density \u03b1 t at time t, i.e., (1\u2212t)\u03bd +t\u00b5.\nThe proof of this theorem in our case is provided in the supplementary for completeness. A simple choice for the interpolant \u03b1 t that we use in this paper was suggested in Dacorogna and Moser (1990):\n\u03b1 t = (1 \u2212 t)\u03bd + t\u00b5. (9\n)\nThe time derivative of this interpolant, i.e., d dt \u03b1 t = \u00b5 \u2212 \u03bd, does not depend on t. Therefore the vector field can be chosen to be constant over time, u t = u, and the PDE in equation 6 takes the form\ndiv(u) = \u03bd \u2212 \u00b5,(10)\nand consequently v t takes the form\nv t = u (1 \u2212 t)\u03bd + t\u00b5 .(11)\nFigure 1 shows a one dimensional illustration of Moser Flow.", "publication_ref": ["b29", "b7"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Generative model utilizing Moser Flow", "text": "We next utilize MF to define our generative model. Our model (learned) density\u03bc is motivated from equation 10 and is defined by\u03bc\n= \u03bd \u2212 div(u),(12)\nwhere u \u2208 X(M) is the degree of freedom of the model. We model this degree of freedom, u, with a deep neural network, more specifically, Multi-Layer Perceptron (MLP). We denote \u03b8 \u2208 R p the learnable parameters of u. We start by noting that, by construction,\u03bc has a unit integral over M:\nLemma 1. If M has no boundary, or u| \u2202M \u2208 X(\u2202M), then M\u03bc dV = 1.\nThis lemma is proved in the supplementary and a direct consequence of Stokes' Theorem. If\u03bc > 0 over M then, together with the fact that M\u03bc dV = 1 (Lemma 1), it is a probability density over M. Consequently, Theorem 1 implies that\u03bc is realized by a CNF defined via v t : Corollary 1. If\u03bc > 0 over M then\u03bc is a probability distribution over M, and is generated by the flow \u03a6 = \u03a6 1 , where \u03a6 t is the solution to the ODE in equation 3 with the vector field v t \u2208 X(M) defined in equation 11.\nSince\u03bc > 0 is an open constraint and is not directly implementable, we replace it with the closed constraint\u03bc \u2265 , where > 0 is a small hyper-parameter. We defin\u0113\n\u00b5 + (x) = max { ,\u03bc(x)} ;\u03bc \u2212 (x) = \u2212 min { ,\u03bc(x)} .\nAs can be readily verified:\n\u03bc + ,\u03bc \u2212 \u2265 0, and\u03bc =\u03bc + \u2212\u03bc \u2212 . (13\n)\nWe are ready to formulate the loss for training the generative model. Consider an unknown target distribution \u00b5, provided to us as a set of i.i.d. observations\nX = {x i } m i=1 \u2282 M.\nOur goal is to maximize the likelihood of the data X while making sure\u03bc \u2265 . We therefore consider the following loss:\n(\n\u03b8) = \u2212E \u00b5 log\u03bc + (x) + \u03bb M\u03bc \u2212 (x)dV (14\n)\nwhere \u03bb is a hyper-parameter. The first term in the loss is approximated by the empirical mean computed with the observations X , i.e.,\nE x\u223c\u00b5 log\u03bc + (x) \u2248 1 m m i=1 log\u03bc + (x i ).\nThis term is merely the negative log likelihood of the observations.\nThe second term in the loss penalizes the deviation of\u03bc from satisfying\u03bc \u2265 . According to Theorem 1, this measures the deviation of\u03bc from being a density function and realizing a CNF. One point that needs to be verified is that the combination of these two terms does not push the minimum away from the target density \u00b5. This can be verified with the help of the generalized Kullback-Leibler (KL) divergence providing a distance measure between arbitrary positive functions f, g : M \u2192 R >0 :\nD(f, g) = M f log f g dV \u2212 M f dV + M gdV. (15\n)\nUsing the generalized KL, we can now compute the distance between the positive part of our model density, i.e.,\u03bc + , and the target density:\nD(\u00b5,\u03bc + ) = E \u00b5 log \u03bc \u00b5 + \u2212 M \u00b5dV + M\u03bc + dV = E \u00b5 log \u00b5 \u2212 E \u00b5 log\u03bc + + M\u03bc \u2212 dV\nwhere in the second equality we used Lemma 1. The term E \u00b5 log \u00b5 is the negative entropy of the unknown target distribution \u00b5. The loss in equation 14 equals D(\u00b5,\u03bc + ) \u2212 E \u00b5 log \u00b5 + (\u03bb \u2212 1) M\u03bc \u2212 dV . Therefore, if \u03bb \u2265 1, and min x\u2208M \u00b5(x) > (we use the compactness of M to infer existence of such a minimal positive value), then the unique minimum of the loss in equation 14 is the target density, i.e.,\u03bc = \u00b5. Indeed, the minimal value of this loss is \u2212E \u00b5 log \u00b5 and it is achieved by setting\u03bc = \u00b5. Uniqueness follows by considering an arbitrary minimizer\u03bc. Since such a minimizer satisfies D(\u00b5,\u03bc + ) = 0 and M\u03bc \u2212 dV = 0, necessarily\u03bc = \u00b5. We proved: Theorem 2. For \u03bb \u2265 1 and sufficiently small > 0, the unique minimizer of the loss in equation 14 is\u03bc = \u00b5.\nVariation of the loss. Lemma 1 and equation 13 imply that M\u03bc + dV = M\u03bc \u2212 dV +1. Therefore, an equivalent loss to the one presented in equation 14 is:\n(\u03b8) = \u2212E \u00b5 log\u03bc + (x) + \u03bb \u2212 M\u03bc \u2212 dV + \u03bb + M\u03bc + dV(16)\nwith \u03bb \u2212 + \u03bb + \u2265 1. Empirically we found this loss favorable in some cases (i.e., with \u03bb + > 0).\nIntegral approximation. The integral M\u03bc \u2212 dV in the losses in equation 16 and 14 is approximated by considering a set Y = {y j } l j=1 of i.i.d. samples according to some distribution \u03b7 over M and taking a Monte Carlo estimate\nM\u03bc \u2212 dV \u2248 1 l l j=1\u03bc \u2212 (y j ) \u03b7(y j ) . (17\n)\nM\u03bc + dV is approximated similarly. In this paper we opted for the simple choice of taking \u03b7 to be the uniform distribution over M.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Generative modeling over Euclidean submanifolds", "text": "In this section, we adapt the Moser Flow (MF) generative model to submanifolds of Euclidean spaces. That is we consider an orientable, compact, boundaryless, connected n-dimensional submanifold M \u2282 R d , where n < d. Examples include implicit surfaces and manifolds (i.e., preimage of a regular value of a smooth function), as well as triangulated surfaces and manifold simplicial complexes. We denote points in R d (and therefore in M) with x, y \u2208 R d . As the Riemannian metric of M we take the induced metric from R d ; that is given arbitrary tangent vectors v, u \u2208 T x M the metric is defined by v, u g = v, u , where the latter is the Euclidean inner product. We denote by \u03c0 : R d \u2192 M the closest point projection on M, i.e., \u03c0(x) = min y\u2208M x \u2212 y , with y 2 = y, y the Euclidean norm in R d . Lastly, we denote by P x \u2208 R d\u00d7d the orthogonal projection matrix on the tangent space T x M; in practice if we denote by N \u2208 R d\u00d7k the matrix with orthonormal columns spanning N x M = (T x M) \u22a5 (i.e., the normal space to M at x) then, P x = I \u2212 N N T .\nWe parametrize the vector field u required for our MF model (in equation 12) by defining a vector field u \u2208 X(R d ) such that u| M \u2208 X(M). We define\nu(x) = P \u03c0(x) v \u03b8 (\u03c0(x)),(18)\nwhere v \u03b8 : R d \u2192 R d is an MLP with Softplus activation (\u03b2 = 100) and learnable parameters \u03b8 \u2208 R p . By construction, for x \u2208 M, u(x) \u2208 T x M.\nTo realize the generative model, we need to compute the divergence div(u(x)) for x \u2208 M with respect to the Riemannian manifold M and metric g. The vector field u in equation 18 is constant along normal directions to the manifold at\nx (since \u03c0(x) is constant in normal directions). If n \u2208 N x M, then in particular d dt t=0 u(x + tn) = 0. (19\n)\nWe call vector fields that satisfy equation 19 infinitesimally constant in the normal direction. As we show next, such vector fields u \u2208 X(M) have the useful property that their divergence along the manifold M coincides with their Euclidean divergence in the ambient space\nR d : Lemma 2. If u \u2208 X(R d ), u| M \u2208 X(M) is infinitesimally constant in normal directions of M, then for x \u2208 M, div(u(x)) = div E (u(x)),\nwhere div E denotes the standard Euclidean divergence.\nThis lemma simply means we can compute the Euclidean divergence of u in our implementation. Given a set of observed data \nX = {x i } m i=1 \u2282 M \u2282 R d ,\n(\u03b8) = \u2212 1 m m i=1 log max { , \u03bd(x i ) \u2212 div E u(x i )} + \u03bb \u2212 l l j=1 \u2212 min { , \u03bd(y j ) \u2212 div E u(y j )} ,\nwhere \u03bb \u2212 = \u03bb \u2212 V (M ). We note the volume constant can be ignored by considering an un-normalized source density V (M )\u03bd \u2261 1, see supplementary for details. The loss in equation 16 is implemented similarly, namely, we add the empirical approximation of \u03bb + M\u03bc + dV .\nWe conclude this section by stating that the MF generative model over Euclidean submanifolds (defined with equations 12 and 18) is universal. That is, MFs can generate, arbitrarily well, any continuous target density \u00b5 on a submanifold manifold M \u2282 R d . Theorem 3. Given an orientable, compact, boundaryless, connected, differentiable n-dimensional submanifold M \u2282 R d , n < d, and a target continuous probability density \u00b5 : M \u2192 R >0 , there exists for each > 0 an MLP v \u03b8 and a choice of weights \u03b8 so that\u03bc defined by equations 12 and 18 satisfies max\nx\u2208M |\u00b5(x) \u2212\u03bc(x)| < .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In all experiments, we modeled a manifold vector field as a multi-layer perceptron (MLP) u \u03b8 \u2208 X(M), with parameters \u03b8. All models were trained using Adam optimizer (Kingma and Ba, 2014), and in all neural networks the activation is Softplus with \u03b2 = 100. Unless stated otherwise, we set \u03bb + = 0. We used an exact calculation of the divergence div E (u(x)). We experimented with two kinds of manifolds.\nFlat Torus. To test our method on Euclidean 2D data, we used M as the flat torus, that is the unit square [\u22121, 1] 2 with opposite edges identified. This defines a manifold with no boundary which is locally isometric to the Euclidean plane. Due to this local isometry the Riemannian divergence on the flat input data samples density input data samples density Figure 2: Moser Flow trained on 2D datasets. We show generated samples and learned density\u03bc + .\ntorus is equivalent to the Euclidean divergence, div = div E . To make u \u03b8 a well defined smooth vector field in M we use periodic positional encoding, namely u \u03b8 (x) = v \u03b8 (\u03c4 (x)), where v \u03b8 is a standard MLP and \u03c4 : R 2 \u2192 R 4k is defined as \u03c4 (x) = (cos(\u03c9 1 \u03c0x), sin(\u03c9 1 \u03c0x), ..., cos(\u03c9 k \u03c0x), sin(\u03c9 k \u03c0x)), where w i = i, and k is a hyper-parameter that is application dependent. Since any periodic function can be approximated by a polynomial acting on e i\u03c0x , even for k = 1 this is a universal model for continuous functions on the torus. As described by Tancik et al. (2020), adding extra features can help with learning higher frequencies in the data. To solve an ODE on the torus we simply solve it for the periodic function and wrap the result back to [\u22121, 1] 2 .\nImplicit surfaces. We experiment with surfaces as submanifolds of R 3 . We represent a surface as the zero level set of a Signed Distance Function (SDF) f : R 3 \u2192 R. We experimented with two surfaces. First, the sphere, represented with the SDF f (x) = x \u2212 1, and second, the Stanford Bunny surface, representing a general curved surface and represented with an SDF learned with (Gropp et al., 2020) from point cloud data. To model vector fields on an implicit surface we follow the general equation 18, where for SDFs\n\u03c0(x) = x \u2212 f (x)\u2207f (x), and P x = I \u2212 \u2207f (x)\u2207f (x) T .\nIn the supplementary, we detail how to replace the global projection \u03c0(x) with a local one, for cases the SDF is not exact.", "publication_ref": ["b21", "b34", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Toy distributions", "text": "First, we consider a collection of challenging toy 2D datasets explored in prior works (Chen et al., 2020;Huang et al., 2021). We scale samples to lie in the flat torus [\u22121, 1] 2 and use k = 1 for the positional encoding. Figure 2 depicts the input data samples, the generated samples after training, and the learned distribution\u03bc. In the top six datasets, the MLP architecture used for Moser Flows consists of 3 hidden layers with 256 units each, whereas in the bottom two we used 4 hidden layers with 256 neurons due to the higher complexity of these distributions. We set \u03bb \u2212 = 2.", "publication_ref": ["b6", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Choice of hyper-parameter \u03bb", "text": "We test the effect of the hyper-parameter \u03bb \u2265 1 on the learned density. Figure 3 shows, for different values of \u03bb, our density estimation \u00b5 + , the push-forward density \u03a6 * \u03bd, and their absolute difference.\nTo evaluate \u03a6 * \u03bd from the vector field v t , we solve an ODE as advised in Grathwohl et al. (2018). As expected, higher values of \u03bb lead to closer modeled density\u03bc + and \u03a6 * \u03bd. This is due to the fact that a higher value of \u03bb leads to a lower value of M\u03bc \u2212 , meaning\u03bc is a better representation of a probability density. Nonetheless, even for \u03bb = 1 the learned and generated density are rather consistent.\n\u00b5 + \u03a6 * \u03bd |\u03a6 * \u03bd \u2212 \u00b5 + | \u00b5 + \u03a6 * \u03bd |\u03a6 * \u03bd \u2212 \u00b5 + | \u03bb = 1 \u03bb = 2 \u03bb = 10 \u03bb = 100", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "Time evaluations", "text": "To compare our method to Euclidean CNF methods, we compare Moser Flow with FFJORD (Grathwohl et al., 2018) on the flat torus. We consider a challenging density with high frequencies obtained via 800x600 images (Figure 7). We generate a new batch every iteration by sampling each pixel location with probability which is proportional to the pixel intensity. The architectures of both v \u03b8 and the vector field defined in FFJORD are the same, namely an MLP with 4 hidden layers of 256 neurons each. To capture the higher frequencies in the image we use a positional encoding with k = 8 for both methods. We used a batch size of 10k. We used learning rate of 1e-5 for Moser Flow and 1e-4 for FFJORD. We used \u03bb \u2212 = 2. Learning was stopped after 5k seconds. Figure 7 presents the results. Note that Moser Flow captures high-frequency details better than FFJORD. This is also expressed in the graph on the top right, showing how the NLL decreases faster for MF than FFJORD. Furthermore, as can be inspected in the per iteration time graph on the bottom-right, MF per iteration time does not increase during training, and is roughly 1-2 order of magnitudes faster than FFJORD iteration.\nVolcano Earthquake Flood Fire\nFigure 5: Moser Flow trained on earth sciences data gathered by Mathieu and Nickel (2020). The learned density is colored green-blue (blue indicates larger values); Blue and red dots represent training and testing datapoints, respectively. See Table 1 for matching quantitative results. Table 1: Negative log-likelihood scores of the earth sciences datasets.", "publication_ref": ["b27"], "figure_ref": ["fig_4", "fig_4"], "table_ref": []}, {"heading": "Earth and climate science data", "text": "We evaluate our model on the earth and climate datasets gathered in Mathieu and Nickel (2020).\nThe projection operator \u03c0 in this case is simply \u03c0(x) = x x . We parameterize v \u03b8 as an MLP with 6 hidden layers of 512 neurons each. We used full batches for the NLL loss and batches of size 150k for our integral approximation. We trained for 30k epochs, with learning rate of 1e-4. We used \u03bb \u2212 = 100. The quantitative NLL results are reported in Table 1 and qualitative visualizations in 5. Note that we produce NLL scores smaller than the runner-up method by a large margin.", "publication_ref": ["b27"], "figure_ref": [], "table_ref": []}, {"heading": "Curved surfaces", "text": "We trained an SDF f for the Stanford Bunny surface M using the method in Gropp et al. (2020). To generate uniform (\u03bd) and data (\u00b5) samples over M we first extract a mesh M from f using the Marching Cubes algorithm (Lorensen and Cline, 1987) setting its resolution to 100x100x100. Then, to randomly choose a point uniformly from M we first randomly choose a face of the mesh with probability proportional to its area, and then randomly choose a point uniformly within that face. For target \u00b5 we used clamped manifold harmonics to create a sequence of densities with increased complexity. To that end, we first computed the k-th eigenfunction of the Laplace-Beltrami operator over M (we provide details on this computation in the supplementary), for the frequencies (eigenvalues) k \u2208 {10, 50, 500}. Next, we sampled the eigenfunctions at the faces' centers, clamped their negative values, and normalized to get discrete probability densities over the faces of M . Then, to sample a point, we first choose a face at random based on this probability, and then random a point uniformly within that face. We take 500k i.i.d. samples of this distribution as our dataset. We take v \u03b8 to be an MLP with 6 hidden layers of dimension 512. We use batch size of 10k for both the NLL loss and for the integral approximation; we ran for 1000 epochs with learning rate of 1e-4. We used \u03bb \u2212 = \u03bb + = 1. Figure 6 depict the results. Note that Moser Flow is able to learn the surface densities for all three frequencies.", "publication_ref": ["b16", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "In the following, we discuss related work on normalizing flows for manifold-valued data. On a high level, such methods can be divided into Parametric versus Riemannian methods. Parametric methods consist of a normalizing flow in the Euclidean space R n , pushed-forward onto the manifold through an invertible map \u03c8 : R n \u2192 M. However, to globally represent the manifold, \u03c8 needs to be a homeomorphism implying that M and R n are topologically equivalent, limiting the scope of that Frequency k = 10\nFrequency k = 50\nFrequency k = 500\nFigure 6: Moser Flow trained on a curved surface (Stanford Bunny). We show three different target distribution with increasing frequencies, where for each frequency we depict (clockwise from top-left): target density, data samples, generated samples, and learned density.\napproach. Existing methods in this class are often based on the exponential map exp x : T x M \u223c = R n \u2192 M of a manifold. This leads to the so called wrapped distributions. This approach has been taken, for instance, by Falorsi et al. (2019) and Bose et al. (2020) to parametrize probability distributions on Lie groups and hyperbolic space. However, Parametric methods based on the exponential map often lead to numerical and computational challenges. For instance, in compact manifolds (e.g., spheres or the SO(3) group) computing the density of wrapped distributions requires an infinite summation. On the hyperboloid, on the other hand, the exponential map is numerically not well-behaved far away from the origin (Dooley and Wildberger, 1993;Al-Mohy and Higham, 2010).\nIn contrast to Parametric methods, Riemannian methods operate directly on the manifold itself and, as such, avoid numerical instabilities that arise from the mapping onto the manifold. Early work in this class of models proposed transformations along geodesics on the hypersphere by evaluating the exponential map at the gradient of a scalar manifold function (Sei, 2011). Rezende et al. (2020) introduced discrete Riemannian flows for hyperspheres and torii based on M\u00f6bius transformations and spherical splines. Mathieu and Nickel (2020) introduced continuous flows on general Riemannian manifolds (RCNF). In contrast to discrete flows (e.g., Bose et al., 2020;Rezende et al., 2020), such time-continuous flows alleviate the previous topological constraints by parametrizing the flow as the solution to an ODE over the manifold (Grathwohl et al., 2018). Concurrently to RCNF, Lou et al. (2020) and Falorsi and Forr\u00e9 (2020) proposed related extensions of neural ODEs to smooth manifolds. Moser Flow also generates a CNF, however by limiting the flow space (albeit, not the generated distributions) it allows expressing the learned distribution as the divergence of a vector field.", "publication_ref": ["b12", "b1", "b11", "b0", "b33", "b32", "b27", "b1", "b32", "b15", "b26", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Discussion and limitations", "text": "We introduced Moser Flow, a generative model in the family of CNFs that represents the target density using the divergence operator applied to a vector valued neural network. One important future work direction, and a current limitation, is scaling of MF to higher dimensions. This challenge can be roughly broken to three parts: First, the model probabilities\u03bc should be computed/approximated in log-scale, as probabilities are expected to decrease exponentially with the dimension. Second, the variance of the numerical approximations of the integral M\u03bc \u2212 dV will increase significantly in high dimensions and needs to be controlled. Third, the divergence term, div(u), is too costly to be computed exactly in high dimensions and needs to be approximated, similarly to other CNF approaches. Finally, our work suggests a novel generative model, and similarly to other generative models can be potentially used for generation of fake data and amplify harmful biases in the dataset. Mitigating such harms is an active and important area of ongoing research.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Now for arbitrary", "text": "x \u2208 M we hav\u0113 \u00b5(x) = \u03bd(x) \u2212 div E (P \u03c0(x) v \u03b8 (\u03c0(x))) = \u03bd(x) \u2212 div E P \u03c0(x) v \u03b8 (\u03c0(x)) \u2212 P \u03c0(x) u (\u03c0(x)) \u2212 div(u (x)) = \u00b5(x) \u2212 div E P \u03c0(x) [v \u03b8 (\u03c0(x)) \u2212 u (\u03c0(x))] = \u00b5(x) \u2212 div E P \u03c0(x) e(x) ,\nwhere we denote e(x) = v \u03b8 (\u03c0(x)) \u2212 u (\u03c0(x)). We will finish the proof by showing that\ndiv E P \u03c0(x) e(x) < c\nfor some constant c > 0 depending only on M. Note that the l.h.s. of this equation is a sum of terms of the form \u2202 \u2202x i (P \u03c0(x) ) i,j e(x) j , where (P \u03c0(x) ) i,j is the (i, j)-th entry of the matrix P \u03c0(x) and e(x) j is the j-th entry of e(x). Since the value and first partial derivatives of \u03c0 and P (as the differential of \u03c0) over M can be bounded, depending only on M, the theorem is proved.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Laplacian eigen function calculation", "text": "Given a triangular surface mesh M , we wish to calculate the k-th eigenfunction of the (discrete) Laplace-Beltrami operator over M . We will use the standard (cotangent) discretization of the Laplacian over meshes (Botsch et al., 2010). That is, we define L to be the cotangent-Laplacian matrix of the graph defined by M , and M the mass matrix of M , i.e., a diagonal matrix where M ii is the area of the the Voroni cell of the i-th vertex in the mesh. We then calculate the eigenfunctions as the solution to the generalized eigenvalue problem Lx = \u03bb k M x where \u03bb k is the k-th eigenvalue. We sample these M piecewise-linear functions at centroids of faces. ", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "E Unnormalized densities", "text": "As described in section 4, our formulation of the loss is dependent on knowing the volume of the manifold M. For simple cases like the flat torus or the sphere, we have a closed form formula for this volume. For more general cases, we can show that we don't actually require to know this value, since we can work with unnormalized density functions: where \u03bd = V (M)\u03bd \u2261 1, u = V (M)u, = V (M) , and log V (M) is a constant. Lastly note that the definition of v t is invariant to this scaling and can be computed with the unnormalized quantities.\n(\u03b8) = \u2212 1 m m i=1 log max { , \u03bd(x i ) \u2212 div E u(x i )} + V(", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F Additional Experimental Details", "text": "We used an internal academic cluster with NVIDIA Quadro RTX 6000 GPUs. Every run and seed configuration required 1 GPU. All other experimental details are mentioned in the main paper. Our codebase, implemented in PyTorch, is attached in the supplementary materials. We will open-source it post the review process.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "NR is supported by the European Research Council (ERC Consolidator Grant, \"LiftMatch\" 771136), the Israel Science Foundation (Grant No. 1830/17), and Carolito Stiftung (WAIC).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Supplementary Material", "text": "A Proof of Moser's Theorem.\nWe will review here the proof of Moser Theorem 1; for more details see Moser's original paper (Moser, 1965) or Lang (2012), Chapter 18 section 2. Let\u03b1 t = \u03b1 t dV be the time-dependent volume form over M corresponding to the density interpolant \u03b1 t . Note that M\u03b1 t = 1. Moser's idea is to replace equation 2 with its continuous version:\nIf equation 20 holds for all t \u2208 [0, 1] then plugging t = 1 leads to equation 2. Since equation 20 holds trivially for t = 0 (since \u03a6 0 is the identity mapping), solving it amounts to asking that \u03a6 * t\u03b1t is constant, i.e., d dt\nThe time derivative of \u03a6 * t\u03b1t can be computed with the help of the Lie derivative (e.g., Proposition 5.2 in Lang (2012)): If \u03a6 t is the flow corresponding to the time dependent vector field v t (see equation 3), and \u03c9 is a differential form then d dt (\u03a6 * t \u03c9) = \u03a6 * t (L vt \u03c9), where L denotes the Lie derivative. The Lie derivative L v \u03c9 of a smooth vector field v and smooth differential form \u03c9 can be computed using Cartan's \"magic formula\" (see e.g., Theorem 14.35 in Lee (2013)):\nwhere i v \u03c9 is the interior multiplication of a vector field and a differential form defined by\nIn case \u03c9 is an n-form (as\u03b1 t in our case) we have d\u03c9 = 0 so the first term in the r.h.s. above vanishes. Lastly, we will need the following \"trick\":\nPutting the last three equations together we get:\nThe theorem is proven if one can show that v t \u2208 X(M) exists such that d(i vt\u03b1t ) + d dt\u03b1 t = 0. The divergence operator is defined by the equality d(i w dV ) = div(w)dV , for a vector field w \u2208 X(M). Therefore\nThen we need to show that v t \u2208 M exists such that\nBy the Hodge decomposition (see Theorem 4.18 in Morita ( 2001))\u03b3 t can be written as a sum of an exact and harmonic forms:\u03b3 t = d\u03b2 t +\u0125 t . Since every harmonic form on a connected, compact, oriented Riemannian manifold is a constant multiple of the Riemannian volume form, cdV (see Corollary 4.14 in Morita ( 2001)), we have\nwhere in the second from the right equality we used Stokes Theorem (see e.g., Theorem 16.11 in Lee (2013)) and the fact that M has no boundary. This implies that c = 0, and\nUsing the correspondence between vector fields and d \u2212 1 forms we let \u03b2 t = i ut dV , where u t \u2208 X(M), and\nLastly, consider v t defined as follows:\nWith this choice equation 23 is satisfied:\nThe theorem is proven.\nOne comment is that for practically finding v t , according to equation 25, we need to get u t , which amounts to solving the Hodge decomposition equation, div(u t )dV =\u03b3 t , that is equivalent to the following PDE on the manifold M:\nProof of Lemma 1. The proof uses Stokes theorem:\nwhere the last equality is due to the fact that either \u2202M = \u2205, or, for x \u2208 \u2202M, we have that u(x) \u2208 T x \u2202M, and therefore (i u dV )(v 1 , . . . , v n\u22121 ) = dV (u, v 1 , . . . , v n\u22121 ) = 0, for all v 1 , . . . , v n\u22121 \u2208 T x \u2202M. This implies i u dV = 0.", "publication_ref": ["b29", "b23", "b23", "b24", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "B Other proofs", "text": "Proof of Theorem 2. As we showed in the paper, our loss can be equivalently presented (up to constant factors) as\nWhere the first term D(\u00b5,\u03bc + ) is the generalized KL divergence which is non-negative and equals zero iff\u03bc + = \u00b5 and since \u03bb \u2265 1 the second term is also non-negative and equals zero iff \u00b5 \u2212 = 0 or \u03bb = 1.\nFirst we show that\u03bc = \u00b5 is a minimizer of the loss. Since we assumed \u00b5 \u2265 we have that \u00b5 + = max(\u00b5, ) = \u00b5 and\u03bc \u2212 =\u03bc + \u2212\u03bc = 0. So both D(\u00b5,\u03bc + ) and M\u03bc \u2212 dV are minimized, which means the entire loss is minimized. Now lets assume\u03bc is a minimizer of the loss. If \u03bb > 1\u03bc has to minimize both terms, as we know there exists a minimizer that minimizes both of them. In particular for any \u03bb \u2265 1 we have that\u03bc minimizes\nProof of Lemma 2. Proposition 1.2 in Lang (2012) and Definition 1 in Section 4-4 in Do Carmo (2016) imply that for submanifolds with induced metric the Riemannian covariant derivative at x \u2208 M satisfies \u2207 ei u = P x \u2202u \u2202ei , where P x is the projection matrix on T x M introduced above. Then, denoting e 1 , . . . , e n , n 1 , . . . , n k an orthonormal basis of R d where the first n vectors span T x M and the latter k span N x M:\nProof of Theorem 3. From Theorem 6.24 in Lee (2013) there exists a neighbourhood \u2126 \u2282 R d of M such that the projection \u03c0 : \u2126 \u2192 M is smooth over\u03a9 (i.e., can be extended to a smooth function over a neighborhood of\u03a9). Since M is compact,\u03a9 is also compact. According to Theorem 1 there exists a vector field u \u2208 X(M) so that \u00b5 = \u03bd \u2212 div(u ). We extend u to\u03a9 by setting u (x) = u (\u03c0(x)), for x / \u2208 M. Note that for x \u2208 M this definition coincides with the former u defined over M. Similarly to equation 18 we have that u (x) = P \u03c0(x) u (\u03c0(x)). Hornik et al. (1990) shows that given a target smooth function f :\u03a9 \u2192 R and > 0, there exists an MLP with l-finite smooth activation that uniformly approximate the first 0 \u2264 m \u2264 l derivatives of f over\u03a9 with error at most . An activation \u03c3 : R \u2192 R is l-finite if it is l-times continuously differentiable and satisfies 0 < \u221e \u2212\u221e \u03c3 (l) < \u221e. Note that sigmoid and tanh are l-finite for all l \u2265 1, and Softplus is l-finite for l \u2265 2.", "publication_ref": ["b23", "b24", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Corollary 3.4 in", "text": "Using this approximation result (adapted to vector valued MLP) there exists an MLP v \u03b8 : R d \u2192 R d such that each coordinate of u and v \u03b8 are close in value and first partial derivatives over\u03a9.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A New Scaling and Squaring Algorithm for the Matrix Exponential", "journal": "SIAM Journal on Matrix Analysis and Applications", "year": "2010", "authors": "A H Al-Mohy; N J Higham"}, {"ref_id": "b1", "title": "Latent Variable Modelling with Hyperbolic Normalizing Flows", "journal": "", "year": "2020", "authors": "A J Bose; A Smofsky; R Liao; P Panangaden; W L Hamilton"}, {"ref_id": "b2", "title": "Polygon mesh processing", "journal": "CRC press", "year": "2010", "authors": "M Botsch; L Kobbelt; M Pauly; P Alliez; B L\u00e9vy"}, {"ref_id": "b3", "title": "Language models are few-shot learners", "journal": "", "year": "2020", "authors": "T B Brown; B Mann; N Ryder; M Subbiah; J Kaplan; P Dhariwal; A Neelakantan; P Shyam; G Sastry; A Askell"}, {"ref_id": "b4", "title": "Triangulated manifold meshing method preserving molecular surface topology", "journal": "Journal of Molecular Graphics and Modelling", "year": "2012", "authors": "M Chen; B Tu; B Lu"}, {"ref_id": "b5", "title": "Neural ordinary differential equations", "journal": "", "year": "2018", "authors": "R T Chen; Y Rubanova; J Bettencourt; D Duvenaud"}, {"ref_id": "b6", "title": "Residual flows for invertible generative modeling", "journal": "", "year": "2020", "authors": "R T Q Chen; J Behrmann; D Duvenaud; J.-H Jacobsen"}, {"ref_id": "b7", "title": "On a partial differential equation involving the jacobian determinant", "journal": "Elsevier", "year": "1990", "authors": "B Dacorogna; J Moser"}, {"ref_id": "b8", "title": "Diffusion models beat gans on image synthesis", "journal": "", "year": "2021", "authors": "P Dhariwal; A Nichol"}, {"ref_id": "b9", "title": "Density estimation using real nvp", "journal": "", "year": "2016", "authors": "L Dinh; J Sohl-Dickstein; S Bengio"}, {"ref_id": "b10", "title": "Differential geometry of curves and surfaces: revised and updated second edition", "journal": "Courier Dover Publications", "year": "2016", "authors": "M P Do Carmo"}, {"ref_id": "b11", "title": "Harmonic analysis and the global exponential map for compact Lie groups", "journal": "", "year": "1993", "authors": "A Dooley; N Wildberger"}, {"ref_id": "b12", "title": "Reparameterizing Distributions on Lie Groups", "journal": "", "year": "2019", "authors": "L Falorsi; P De Haan; T R Davidson; P Forr\u00e9"}, {"ref_id": "b13", "title": "Neural Ordinary Differential Equations on Manifolds", "journal": "", "year": "2020", "authors": "L Falorsi; P Forr\u00e9"}, {"ref_id": "b14", "title": "Manifold modeling for brain population analysis", "journal": "Medical image analysis", "year": "2010", "authors": "S Gerber; T Tasdizen; P T Fletcher; S Joshi; R Whitaker; A D N Initiative"}, {"ref_id": "b15", "title": "Ffjord: Free-form continuous dynamics for scalable reversible generative models", "journal": "", "year": "2018", "authors": "W Grathwohl; R T Q Chen; J Bettencourt; I Sutskever; D Duvenaud"}, {"ref_id": "b16", "title": "Implicit geometric regularization for learning shapes", "journal": "", "year": "2020", "authors": "A Gropp; L Yariv; N Haim; M Atzmon; Y Lipman"}, {"ref_id": "b17", "title": "Surface reconstruction from unorganized points", "journal": "", "year": "1992", "authors": "H Hoppe; T Derose; T Duchamp; J Mcdonald; W Stuetzle"}, {"ref_id": "b18", "title": "Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks", "journal": "Neural networks", "year": "1990", "authors": "K Hornik; M Stinchcombe; H White"}, {"ref_id": "b19", "title": "Convex potential flows: Universal probability distributions with optimal transport and convex optimization", "journal": "", "year": "2021", "authors": "C.-W Huang; R T Q Chen; C Tsirigotis; A Courville"}, {"ref_id": "b20", "title": "Poisson surface reconstruction", "journal": "", "year": "2006", "authors": "M Kazhdan; M Bolitho; H Hoppe"}, {"ref_id": "b21", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "D P Kingma; J Ba"}, {"ref_id": "b22", "title": "Videoflow: A flow-based generative model for video", "journal": "", "year": "2019", "authors": "M Kumar; M Babaeizadeh; D Erhan; C Finn; S Levine; L Dinh; D Kingma"}, {"ref_id": "b23", "title": "Fundamentals of differential geometry", "journal": "Springer Science & Business Media", "year": "2012", "authors": "S Lang"}, {"ref_id": "b24", "title": "Smooth manifolds. In Introduction to Smooth Manifolds", "journal": "Springer", "year": "2013", "authors": "J M Lee"}, {"ref_id": "b25", "title": "Marching cubes: A high resolution 3d surface construction algorithm", "journal": "ACM siggraph computer graphics", "year": "1987", "authors": "W E Lorensen; H E Cline"}, {"ref_id": "b26", "title": "Neural manifold ordinary differential equations", "journal": "", "year": "2020", "authors": "A Lou; D Lim; I Katsman; L Huang; Q Jiang; S.-N Lim; De Sa; C "}, {"ref_id": "b27", "title": "Riemannian continuous normalizing flows", "journal": "", "year": "2020", "authors": "E Mathieu; M Nickel"}, {"ref_id": "b28", "title": "Geometry of differential forms", "journal": "Number", "year": "2001", "authors": "S Morita"}, {"ref_id": "b29", "title": "On the volume elements on a manifold. Transactions of the", "journal": "American Mathematical Society", "year": "1965", "authors": "J Moser"}, {"ref_id": "b30", "title": "Normalizing flows for probabilistic modeling and inference", "journal": "", "year": "2019", "authors": "G Papamakarios; E Nalisnick; D J Rezende; S Mohamed; B Lakshminarayanan"}, {"ref_id": "b31", "title": "Variational inference with normalizing flows", "journal": "PMLR", "year": "2015", "authors": "D Rezende; S Mohamed"}, {"ref_id": "b32", "title": "Normalizing flows on tori and spheres", "journal": "PMLR", "year": "2020", "authors": "D J Rezende; G Papamakarios; S Racaniere; M Albergo; G Kanwar; P Shanahan; K Cranmer"}, {"ref_id": "b33", "title": "A Jacobian Inequality for Gradient Maps on the Sphere and Its Application to Directional Statistics", "journal": "Communications in Statistics -Theory and Methods", "year": "2011", "authors": "T Sei"}, {"ref_id": "b34", "title": "Fourier features let networks learn high frequency functions in low dimensional domains", "journal": "", "year": "2020", "authors": "M Tancik; P P Srinivasan; B Mildenhall; S Fridovich-Keil; N Raghavan; U Singhal; R Ramamoorthi; J T Barron; R Ng"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: 1D example of Moser Flow:source density \u03bd in green, target \u00b5 in blue. The vector field v t (black) is guaranteed to push \u03bd to interpolated density \u03b1 t at time t, i.e., (1\u2212t)\u03bd +t\u00b5.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "and a set of uniform i.i.d. samples Y = {y j } l j=1 \u2282 M over M, our loss (equation 14) takes the form", "figure_data": ""}, {"figure_label": "34", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :Figure 4 :34Figure3: As \u03bb is increased, the closer\u03bc + is to the generated density \u03a6 * \u03bd; column titled |\u03bc + \u2212 \u03a6 * \u03bd| shows the absolute pointwise difference between the two; note that some of the errors in the |\u03bc + \u2212 \u03a6 * \u03bd| column are due to numerical inaccuracies in the ODE solver used to calculate \u03a6 * \u03bd.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "DLinearization of the projection operator \u03c0 Since we only sample and derivate the projection operator \u03c0 : R d \u2192 M over M, implementing equation 18 does not require knowledge of the full projection \u03c0. Rather, it is enough to use its first order expansion over M. For x 0 \u2208 M \u03c0(x) \u2248 \u03c0(x 0 ) + P x0 (x \u2212 x 0 ) = x 0 + P x0 (x \u2212 x 0 ) =\u03c0(x 0 , x). Now since \u03c0(\u2022) and\u03c0(x 0 , \u2022) have the same value and first partial derivatives at x 0 we can replace equation 18 for each sample point x 0 \u2208 X \u222a Y, with u(x) = P\u03c0 (x0,x) v \u03b8 (\u03c0(x 0 , x)).", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "\u2212Figure 7 :7Figure 7: Comparing learned density and generated samples with MF and FFJORD at different times (in k-sec); top right shows NLL scores for both MF and FFJORD at different times; bottom right shows time per iteration (in log-scale, sec) as a function of total running time (in sec); FFJORD iterations take longer as training progresses. Flickr images (license CC BY 2.0): Bird by Flickr user \"lakeworth\" https://www.flickr.com/photos/lakeworth/46657879995/; Flower by Flickr user \"daiyaan.db\" https://www.flickr.com/photos/daiyaandb/23279986094/.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "\u00b10.03 \u22120.66 \u00b10.05 Moser Flow (MF) \u22122.02 \u00b10.42 \u22120.09 \u00b10.02 0.62 \u00b10.04 \u22121.03 \u00b10.03", "figure_data": "VolcanoEarthquakeFloodFireMixture vMF\u22120.31 \u00b10.070.59 \u00b10.011.09 \u00b10.01 \u22120.23 \u00b10.02Stereographic\u22120.64 \u00b10.200.43 \u00b10.040.99 \u00b10.04 \u22120.40 \u00b10.06Riemannian 0.90 Data size \u22120.97 \u00b10.15 0.19 \u00b10.04 829 6124 487712810"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "The main benefits of MF stems from the simplicity and locality of the divergence operator. MFs circumvent the need to solve an ODE in the training process, and are thus applicable on a broad class of manifolds. Theoretically, we prove MF is a universal generative model, able to (approximately) generate arbitrary positive target densities from arbitrary positive prior densities. Empirically, we show MF enjoys favorable computational speed in comparison to previous CNF models, improves density estimation on spherical data compared to previous work by a large margin, and for the first time facilitate training a CNF over a general curved surface.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "(\u03a6 * \u03bc ) z (v 1 , . . . , v n ) =\u03bc \u03a6(z) (D\u03a6 z (v 1 ), . . . , D\u03a6 z (v n )),(1)", "formula_coordinates": [2.0, 194.16, 644.37, 309.84, 12.03]}, {"formula_id": "formula_1", "formula_text": "\u03bd = \u03a6 * \u03bc .(2)", "formula_coordinates": [2.0, 287.28, 696.87, 216.72, 25.28]}, {"formula_id": "formula_2", "formula_text": "p \u03bd (A) = A\u03bd = A \u03a6 * \u03bc = \u03a6(A)\u03bc = p \u00b5 (\u03a6(A)).", "formula_coordinates": [3.0, 202.95, 97.81, 206.1, 19.31]}, {"formula_id": "formula_3", "formula_text": "\u03a6 t : [0, 1] \u00d7 M \u2192 M is defined by d dt \u03a6 t = v t (\u03a6 t ),(3)", "formula_coordinates": [3.0, 274.84, 176.3, 229.16, 40.73]}, {"formula_id": "formula_4", "formula_text": "\u03a6 * \u03bc (z) = \u00b5(\u03a6(z)) det(D\u03a6 z )dz,", "formula_coordinates": [3.0, 237.27, 305.44, 137.46, 11.72]}, {"formula_id": "formula_5", "formula_text": "\u03bd(z) = \u00b5(\u03a6(z)) det(D\u03a6 z ).(4)", "formula_coordinates": [3.0, 248.95, 356.63, 255.05, 9.65]}, {"formula_id": "formula_6", "formula_text": "\u03a6 * t\u03b1t =\u03b1 0 ,(5)", "formula_coordinates": [3.0, 281.73, 550.39, 222.28, 12.69]}, {"formula_id": "formula_7", "formula_text": "div(u t ) = \u2212 d dt \u03b1 t ,(6)", "formula_coordinates": [3.0, 268.24, 622.2, 235.76, 22.31]}, {"formula_id": "formula_8", "formula_text": "div(u) = n i=1 \u2207 ei u, e i g ,(7)", "formula_coordinates": [3.0, 252.44, 695.01, 251.56, 30.32]}, {"formula_id": "formula_9", "formula_text": "v t = u t \u03b1 t .(8)", "formula_coordinates": [4.0, 287.67, 111.64, 216.33, 23.22]}, {"formula_id": "formula_10", "formula_text": "\u03b1 t = (1 \u2212 t)\u03bd + t\u00b5. (9", "formula_coordinates": [4.0, 186.88, 238.96, 156.77, 9.65]}, {"formula_id": "formula_11", "formula_text": ")", "formula_coordinates": [4.0, 343.65, 239.28, 3.87, 8.64]}, {"formula_id": "formula_12", "formula_text": "div(u) = \u03bd \u2212 \u00b5,(10)", "formula_coordinates": [4.0, 194.29, 302.44, 153.23, 8.96]}, {"formula_id": "formula_13", "formula_text": "v t = u (1 \u2212 t)\u03bd + t\u00b5 .(11)", "formula_coordinates": [4.0, 186.45, 332.36, 161.07, 22.31]}, {"formula_id": "formula_14", "formula_text": "= \u03bd \u2212 div(u),(12)", "formula_coordinates": [4.0, 281.3, 430.41, 222.7, 8.96]}, {"formula_id": "formula_15", "formula_text": "Lemma 1. If M has no boundary, or u| \u2202M \u2208 X(\u2202M), then M\u03bc dV = 1.", "formula_coordinates": [4.0, 108.0, 482.43, 303.19, 11.78]}, {"formula_id": "formula_16", "formula_text": "\u00b5 + (x) = max { ,\u03bc(x)} ;\u03bc \u2212 (x) = \u2212 min { ,\u03bc(x)} .", "formula_coordinates": [4.0, 191.14, 611.19, 229.72, 9.65]}, {"formula_id": "formula_17", "formula_text": "\u03bc + ,\u03bc \u2212 \u2265 0, and\u03bc =\u03bc + \u2212\u03bc \u2212 . (13", "formula_coordinates": [4.0, 238.51, 629.64, 261.34, 20.61]}, {"formula_id": "formula_18", "formula_text": ")", "formula_coordinates": [4.0, 499.85, 640.92, 4.15, 8.64]}, {"formula_id": "formula_19", "formula_text": "X = {x i } m i=1 \u2282 M.", "formula_coordinates": [4.0, 353.81, 669.29, 87.34, 14.11]}, {"formula_id": "formula_20", "formula_text": "\u03b8) = \u2212E \u00b5 log\u03bc + (x) + \u03bb M\u03bc \u2212 (x)dV (14", "formula_coordinates": [4.0, 230.28, 709.1, 269.57, 17.23]}, {"formula_id": "formula_21", "formula_text": ")", "formula_coordinates": [4.0, 499.85, 709.42, 4.15, 8.64]}, {"formula_id": "formula_22", "formula_text": "E x\u223c\u00b5 log\u03bc + (x) \u2248 1 m m i=1 log\u03bc + (x i ).", "formula_coordinates": [5.0, 229.11, 102.31, 153.78, 30.32]}, {"formula_id": "formula_23", "formula_text": "D(f, g) = M f log f g dV \u2212 M f dV + M gdV. (15", "formula_coordinates": [5.0, 196.69, 218.21, 303.16, 23.97]}, {"formula_id": "formula_24", "formula_text": ")", "formula_coordinates": [5.0, 499.85, 225.27, 4.15, 8.64]}, {"formula_id": "formula_25", "formula_text": "D(\u00b5,\u03bc + ) = E \u00b5 log \u03bc \u00b5 + \u2212 M \u00b5dV + M\u03bc + dV = E \u00b5 log \u00b5 \u2212 E \u00b5 log\u03bc + + M\u03bc \u2212 dV", "formula_coordinates": [5.0, 199.02, 278.2, 211.75, 50.98]}, {"formula_id": "formula_26", "formula_text": "(\u03b8) = \u2212E \u00b5 log\u03bc + (x) + \u03bb \u2212 M\u03bc \u2212 dV + \u03bb + M\u03bc + dV(16)", "formula_coordinates": [5.0, 194.83, 485.16, 309.17, 17.23]}, {"formula_id": "formula_27", "formula_text": "M\u03bc \u2212 dV \u2248 1 l l j=1\u03bc \u2212 (y j ) \u03b7(y j ) . (17", "formula_coordinates": [5.0, 253.5, 577.33, 246.36, 30.32]}, {"formula_id": "formula_28", "formula_text": ")", "formula_coordinates": [5.0, 499.85, 588.06, 4.15, 8.64]}, {"formula_id": "formula_29", "formula_text": "u(x) = P \u03c0(x) v \u03b8 (\u03c0(x)),(18)", "formula_coordinates": [6.0, 256.31, 190.08, 247.69, 9.99]}, {"formula_id": "formula_30", "formula_text": "x (since \u03c0(x) is constant in normal directions). If n \u2208 N x M, then in particular d dt t=0 u(x + tn) = 0. (19", "formula_coordinates": [6.0, 108.0, 258.14, 396.0, 43.32]}, {"formula_id": "formula_31", "formula_text": ")", "formula_coordinates": [6.0, 499.85, 286.21, 4.15, 8.64]}, {"formula_id": "formula_32", "formula_text": "R d : Lemma 2. If u \u2208 X(R d ), u| M \u2208 X(M) is infinitesimally constant in normal directions of M, then for x \u2208 M, div(u(x)) = div E (u(x)),", "formula_coordinates": [6.0, 108.0, 325.27, 396.0, 36.62]}, {"formula_id": "formula_33", "formula_text": "X = {x i } m i=1 \u2282 M \u2282 R d ,", "formula_coordinates": [6.0, 232.67, 380.15, 122.0, 14.11]}, {"formula_id": "formula_34", "formula_text": "(\u03b8) = \u2212 1 m m i=1 log max { , \u03bd(x i ) \u2212 div E u(x i )} + \u03bb \u2212 l l j=1 \u2212 min { , \u03bd(y j ) \u2212 div E u(y j )} ,", "formula_coordinates": [6.0, 112.77, 416.49, 390.62, 30.32]}, {"formula_id": "formula_35", "formula_text": "\u03c0(x) = x \u2212 f (x)\u2207f (x), and P x = I \u2212 \u2207f (x)\u2207f (x) T .", "formula_coordinates": [7.0, 183.34, 525.94, 245.31, 11.72]}, {"formula_id": "formula_36", "formula_text": "\u00b5 + \u03a6 * \u03bd |\u03a6 * \u03bd \u2212 \u00b5 + | \u00b5 + \u03a6 * \u03bd |\u03a6 * \u03bd \u2212 \u00b5 + | \u03bb = 1 \u03bb = 2 \u03bb = 10 \u03bb = 100", "formula_coordinates": [8.0, 116.91, 75.12, 373.77, 97.62]}, {"formula_id": "formula_37", "formula_text": "x \u2208 M we hav\u0113 \u00b5(x) = \u03bd(x) \u2212 div E (P \u03c0(x) v \u03b8 (\u03c0(x))) = \u03bd(x) \u2212 div E P \u03c0(x) v \u03b8 (\u03c0(x)) \u2212 P \u03c0(x) u (\u03c0(x)) \u2212 div(u (x)) = \u00b5(x) \u2212 div E P \u03c0(x) [v \u03b8 (\u03c0(x)) \u2212 u (\u03c0(x))] = \u00b5(x) \u2212 div E P \u03c0(x) e(x) ,", "formula_coordinates": [15.0, 158.43, 75.13, 295.13, 85.91]}, {"formula_id": "formula_38", "formula_text": "div E P \u03c0(x) e(x) < c", "formula_coordinates": [15.0, 257.11, 185.35, 97.06, 9.99]}, {"formula_id": "formula_39", "formula_text": "(\u03b8) = \u2212 1 m m i=1 log max { , \u03bd(x i ) \u2212 div E u(x i )} + V(", "formula_coordinates": [15.0, 180.56, 584.08, 195.96, 54.18]}], "doi": ""}