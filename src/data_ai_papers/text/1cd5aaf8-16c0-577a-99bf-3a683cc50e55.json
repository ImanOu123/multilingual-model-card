{"title": "Applying the Transformer to Character-level Transduction", "authors": "Shijie Wu; Ryan Cotterell; Mans Hulden", "pub_date": "", "abstract": "The transformer (Vaswani et al., 2017) has been shown to outperform recurrent neural network-based sequence-to-sequence models in various word-level NLP tasks. Yet for character-level transduction tasks, e.g. morphological inflection generation and historical text normalization, there are few works that outperform recurrent models using the transformer. In an empirical study, we uncover that, in contrast to recurrent sequenceto-sequence models, the batch size plays a crucial role in the performance of the transformer on character-level tasks, and we show that with a large enough batch size, the transformer does indeed outperform recurrent models. We also introduce a simple technique to handle feature-guided character-level transduction that further improves performance. With these insights, we achieve state-of-the-art performance on morphological inflection and historical text normalization. We also show that the transformer outperforms a strong baseline on two other character-level transduction tasks: grapheme-to-phoneme conversion and transliteration.", "sections": [{"heading": "Introduction", "text": "The transformer (Vaswani et al., 2017) has become a popular architecture for sequence-to-sequence transduction in NLP. It has achieved state-of-theart performance on a range of common word-level transduction tasks: neural machine translation (Barrault et al., 2019), question answering (Devlin et al., 2019) and abstractive summarization (Dong et al., 2019). In addition, the transformer forms the backbone of the widely-used BERT (Devlin et al., 2019). Yet for character-level transduction tasks like morphological inflection, the dominant model has remained a recurrent neural network-based sequence-Code will be available at https://github.com/ shijie-wu/neural-transducer.  We evince our two primary contributions: (1) we set the new state of the art morphological inflection using the transformer and (2) we demonstrate the transformer's dependence on the batch size.\nto-sequence model with attention (Cotterell et al., 2018). This is not for lack of effort-but rather, it is the case that the transformer has consistently underperformed in experiments on average (Tang et al., 2018b). 1 As anecdotal evidence of this, we note that in the 2019 SIGMORPHON shared task on cross-lingual transfer for morphological inflection, no participating system was based on the transformer . Character-level transduction models are often trained with less data than their word-level counterparts: In contrast to machine translation, where millions of training samples are available, the 2018 SIGMORPHON shared task (Cotterell et al., 2018)  should provide an advantage at many characterlevel tasks: For instance, Gehring et al. (2017) and Vaswani et al. (2017) suggest that transformers (and convolutional models in general) should be better at remembering long-range dependencies. In the case of morphology, none of these considerations seem relevant: inflecting a word (a) requires little capacity to model long-distance dependencies and is largely a monotonic transduction; (b) it involves no semantic disambiguation, the tokens in question being letters; (c) it is not a task for which parallelization during training appears to help, since training time has never been an issue in morphology tasks. 2 In this work, we provide state-of-the-art numbers for morphological inflection and historical text normalization, a novel result in the literature. We also show the transformer outperforms a strong recurrent baseline on two other characterlevel tasks: grapheme-to-phoneme (g2p) conversion and transliteration. We find that a single hyperparameter, batch size, is largely responsible for the previous poor results. Despite having fewer parameters, the transformer outperforms the recurrent sequence-to-sequence baselines on all four tasks. We conduct a short error analysis on the task of morphological inflection to round out the paper.", "publication_ref": ["b35", "b3", "b12", "b13", "b12", "b38", "b33", "b38", "b15", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "The Transformer for Characters", "text": "The Transformer. The transformer, originally described by Vaswani et al. (2017), is a selfattention-based encoder-decoder model. The encoder has N layers, consisting of a multi-head selfattention layer and a two-layer feed-forward layer with ReLU activation, both equipped with a skip connection. The decoder has a similar structure as the encoder except that, in each decoder layer between the self-attention layer and feed-forward layer, a multi-head attention layer attends to the output of the encoder. Layer normalization (Ba et al., 2016) is applied to the output of each skip connection. Sinusoidal positional embeddings are used to incorporate positional information without the need for recurrence or convolution. Here, we describe two modifications we make to the transformer for character-level tasks.\nA Smaller Transformer. As the dataset sizes in character-level transduction tasks are significantly smaller than in machine translation, we employ a smaller transformer with N = 4 encoder-decoder layers. We use 4 self-attention heads. The embedding size is d model = 256 and the hidden size of the feed-forward layer is d FF = 1024. In the preliminary experiments, we found that using layer normalization before self-attention and the feed-forward layer performed slightly better than the original model. It is also the default setting of a popular implementation of the transformer (Vaswani et al., 2018). The transformer alone has around 7.37M parameters, excluding character embeddings and the linear mapping before the softmax layer. We decode the model left to right in a greedy fashion.\nFeature Invariance. Some character-level transduction is guided by features. For example, in the case of morphological reinflection, the task requires a set of morphological attributes that control what form a citation form is inflected into (see Fig. 2 for an example). However, the order of the features is irrelevant. In a recurrent neural network, features are input in some predefined order as special characters and pre-or postpended to the input character sequence representing the citation form. The same is true for a vanilla transformer model, as shown on the left-hand side of Fig. 2 ", "publication_ref": ["b35", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "Empirical Findings", "text": "Tasks. We consider four character-level transduction tasks: morphological inflection, grapheme-tophoneme conversion, transliteration, and historical text normalization. For morphological inflection, we use the 2017 SIGMORPHON shared task data (Cotterell et al., 2017) with 52 languages. The performance is evaluated by accuracy (ACC) and edit distance (Dist). For the g2p task, we use the unstressed CMUDict (Weide, 1998) and NETtalk (Sejnowski and Rosenberg, 1987) resources. We use the splits from Wu et al. (2018). We evaluate under word error rate (WER) and phoneme error rate (PER). For transliteration, we use the NEWS 2015 shared task data (Zhang et al., 2015). 4 For historical text normalization, we follow Bollmann (2019) and use datasets for Spanish (S\u00e1nchez-Mart\u00ednez et al., 2013), Icelandic and Swedish (Pettersson et al., 2013), Slovene Erjavec, 2013, 2016;Ljube\u0161ic et al., 2016), Hungarian and German (Pettersson, 2016). 5 We evaluate using accuracy (ACC) and character error rate of incorrect prediction (CER i ).\nOptimization. We use Adam (Kingma and Ba, 2014) with a learning rate of 0.001 and an inverse square root learning rate scheduler (Vaswani et al., 2017) with 4k steps during the warm-up. We train the model for 20k gradient updates and save and evaluate the model every 400 gradient updates. We select the best model out of 50 checkpoints based on development set accuracy. The number of gradient updates and checkpoints are roughly the same as , the single model state of the art on the 2017 SIGMORPHON dataset. We use their model as a baseline model. For all experiments, we use a single predefined random seed.", "publication_ref": ["b9", "b36", "b29", "b38", "b39", "b26", "b24", "b18", "b23", "b17", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "A Controlled Hyperparameter Study", "text": "To demonstrate the importance of hyperparameter tuning for the transformer on character-level tasks, we perform a small controlled hyperparameter study. This is important since researchers had previously failed to achieve high-performing results with the transformer on character-level tasks.\nHere, we look at morphological inflection on the five languages in the 2017 SIGMORPHON dataset where submitted systems performed the worst: Latin, Faroese, French, Hungarian, and Norwegian (Nynorsk). We set the dropout to 0.3, \u03b2 2 of Adam to 0.999 (the default value), and do not use label smoothing. We do not tune any other hyperparameter except the following three hyperparameters.\nThe Importance of Batch Size. While recurrent models like Wu and Cotterell use a batch size of 20, halving the learning rate when stuck and employing early stopping, we find that a less aggressive learning rate scheduler, allowing the model to train longer, outperforms these hyperparameters. Fig. 1 shows that the significant impact of batch size on the transformer. The transformer performance in- creases steadily as the batch size is increased, similarly to what Popel and Bojar (2018) observe for machine translation. The transformer only outperforms the recurrent baseline when the batch size is at least 128, which is much larger than batch size commonly used in recurrent models. 6 Note that the model of Wu and Cotterell has 8.66M parameters, 17% more than the transformer model. To get an apples-to-apples comparison, we apply the same learning rate scheduler to Wu and Cotterell; this does not yield similar improvements and underperforms with respect to the traditional learning rate scheduler. Our feature invariant transformer also outperforms the vanilla transformer model. We set the batch size to 400 for our main experiments. Note the batch size of 400 is especially large (4% of training data) considering the training size is only 10k.\nOther Hyperparameters. Vaswani et al. (2017) applies label smoothing (Szegedy et al., 2016) of 0.1 to the transformer model and shows that it hurts perplexity, but improves BLEU scores for machine translation. Instead of the default 0.999 \u03b2 2 for Adam, Vaswani et al. (2017) uses 0.98 and we find that both choices benefit character-level transduction tasks as well (see Tab. 1).", "publication_ref": ["b25", "b35", "b31", "b35"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "New State-of-the-Art Results", "text": "We train our feature invariant transformer on the four character-level tasks, exhibiting state-of-theart results on morphological inflection and historical text normalization.   Morphological Inflection. As shown in Tab. 2, the feature invariant transformer produces state-ofthe-art results on the 2017 SIGMORPHON shared tasks, improving upon ensemble-based systems by 0.27 points. We observe that as the dataset decreases in size, a model with a larger dropout value performs slightly better. A brief tally of phenomena that are difficult to learn for many machine learning models, categorized along typical linguistic dimensions (such as word-internal sound changes, vowel harmony, circumfixation, ablaut, and umlaut phenomena) fail to reveal any consistent pattern of advantage to the transformer model. In fact, errors seem to be randomly distributed with an overall advantage of the transformer model. Curiously, errors grouped along the dimension of word length reveal that as word forms grow longer, the transformer advantage shrinks (Fig. 3).\nHistorical Text Normalization. Tab. 3 shows that the transformer model with dropout of 0.1, as in the case of morphological inflection, improves upon the previous state of the art, although the model with a dropout of 0.3 yields a slightly better CER i .\nG2P and Transliteration. Tab. 4 shows that the transformer outperforms previously published strong recurrent models on two tasks despite having fewer parameters. A dropout rate of 0.3 yields significantly better performance on the transliteration task while a dropout rate of 0.1 is stronger on the g2p task. This shows that transformers can and do outperform recurrent transducers on common character-level tasks when properly tuned.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Related Work", "text": "Character-level transduction is largely dominated by attention-based LSTM sequence-to-sequence (Luong et al., 2015) models (Cotterell et al., 2018). Character-level transduction tasks usually involve input-output pairs that share large substrings and alignments between these are often monotonic. Models that address the task tend to focus on exploiting such structural bias. Instead of learning the alignments, Aharoni and Goldberg (2017) use external monotonic alignments from the SIGMOR-PHON 2016 shared task baseline Cotterell et al. (2016). Makarov et al. (2017) use this approach to win the CoNLL-SIGMORPHON 2017 shared task on morphological inflection (Cotterell et al., 2017). Wu et al. (2018) shows that explicitly modeling alignment (hard attention) between source and target characters outperforms soft attention.  further shows that enforcing monotonicity in a hard attention model improves performance.", "publication_ref": ["b20", "b38", "b11", "b21", "b9", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "Using a large batch size and feature invariant input allows the transformer to achieve strong performance on character-level tasks. However, it is unclear what linguistic errors the transformer makes compared to recurrent models on these tasks. Future work should analyze the errors in detail as Gorman et al. (2019) does for recurrent models. While Wu and Cotterell shows that the monotonicity bias benefits character-level tasks, it is not evident how to enforce monotonicity on multi-headed self-attention. Future work should consider how to best incorporate monotonicity into the model, either by enforcing it strictly  or by pretraining the model to copy (Anastasopoulos and Neubig, 2019).", "publication_ref": ["b16"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Morphological inflection generation with hard monotonic attention", "journal": "Association for Computational Linguistics", "year": "2004", "authors": "Roee Aharoni; Yoav Goldberg"}, {"ref_id": "b1", "title": "Pushing the limits of low-resource morphological inflection", "journal": "", "year": "2019", "authors": "Antonios Anastasopoulos; Graham Neubig"}, {"ref_id": "b2", "title": "", "journal": "", "year": "", "authors": "Jimmy Lei Ba; Jamie Ryan Kiros; Geoffrey E "}, {"ref_id": "b3", "title": "Findings of the 2019 conference on machine translation (WMT19)", "journal": "", "year": "2019", "authors": "Lo\u00efc Barrault; Ond\u0159ej Bojar; Marta R Costa-Juss\u00e0; Christian Federmann; Mark Fishel; Yvette Graham; Barry Haddow; Matthias Huck; Philipp Koehn; Shervin Malmasi; Christof Monz; Mathias M\u00fcller"}, {"ref_id": "b4", "title": "Training data augmentation for low-resource morphological inflection", "journal": "Vancouver. Association for Computational Linguistics", "year": "2017", "authors": "Toms Bergmanis; Katharina Kann; Hinrich Sch\u00fctze; Sharon Goldwater"}, {"ref_id": "b5", "title": "Normalization of historical texts with neural network models", "journal": "", "year": "2018", "authors": "Marcel Bollmann"}, {"ref_id": "b6", "title": "A large-scale comparison of historical text normalization systems", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Marcel Bollmann"}, {"ref_id": "b7", "title": "", "journal": "", "year": "", "authors": "Ryan Cotterell; Christo Kirov; John Sylak-Glassman; G\u00e9raldine Walther; Ekaterina Vylomova; D Arya"}, {"ref_id": "b8", "title": "The CoNLL-SIGMORPHON 2018 shared task: Universal morphological reinflection", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Katharina Mccarthy; Sebastian Kann; Garrett Mielke; Miikka Nicolai; David Silfverberg;  Yarowsky"}, {"ref_id": "b9", "title": "CoNLL-SIGMORPHON 2017 shared task: Universal morphological reinflection in 52 languages", "journal": "", "year": "2017", "authors": "Ryan Cotterell; Christo Kirov; John Sylak-Glassman; G\u00e9raldine Walther; Ekaterina Vylomova; Patrick Xia; Manaal Faruqui; Sandra K\u00fcbler; David Yarowsky"}, {"ref_id": "b10", "title": "Shared Task: Universal Morphological Reinflection, pages 1-30, Vancouver. Association for Computational Linguistics", "journal": "", "year": "", "authors": ""}, {"ref_id": "b11", "title": "The SIGMORPHON 2016 shared Task-Morphological reinflection", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Ryan Cotterell; Christo Kirov; John Sylak-Glassman; David Yarowsky; Jason Eisner; Mans Hulden"}, {"ref_id": "b12", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b13", "title": "Unified language model pre-training for natural language understanding and generation", "journal": "", "year": "2019", "authors": "Li Dong; Nan Yang; Wenhui Wang; Furu Wei; Xiaodong Liu; Yu Wang; Jianfeng Gao; Ming Zhou; Hsiao-Wuen Hon"}, {"ref_id": "b14", "title": "Historical text normalization with delayed rewards", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Simon Flachs; Marcel Bollmann; Anders S\u00f8gaard"}, {"ref_id": "b15", "title": "Convolutional sequence to sequence learning", "journal": "JMLR", "year": "2017", "authors": "Jonas Gehring; Michael Auli; David Grangier; Denis Yarats; Yann N Dauphin"}, {"ref_id": "b16", "title": "Weird inflects but OK: Making sense of morphological generation errors", "journal": "", "year": "2019", "authors": "Kyle Gorman; Arya D Mccarthy; Ryan Cotterell; Ekaterina Vylomova; Miikka Silfverberg; Magdalena Markowska"}, {"ref_id": "b17", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b18", "title": "Normalising Slovene data: historical texts vs. user-generated content", "journal": "", "year": "2016", "authors": "Nikola Ljube\u0161ic; Katja Zupan; Darja Fi\u0161er; Tomaz Erjavec"}, {"ref_id": "b19", "title": "Normalising Slovene data: historical texts vs. user-generated content", "journal": "", "year": "2016", "authors": "Nikola Ljube\u0161i\u0107; Katja Zupan; Darja Fi\u0161er; Toma\u017e Erjavec"}, {"ref_id": "b20", "title": "Effective approaches to attention-based neural machine translation", "journal": "", "year": "2015", "authors": "Thang Luong; Hieu Pham; Christopher D Manning"}, {"ref_id": "b21", "title": "Align and copy: UZH at SIGMORPHON 2017 shared task for morphological reinflection", "journal": "", "year": "2017", "authors": "Peter Makarov; Tatiana Ruzsics; Simon Clematide"}, {"ref_id": "b22", "title": "The SIGMORPHON 2019 shared task: Morphological analysis in context and cross-lingual transfer for inflection", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "D Arya; Ekaterina Mccarthy; Shijie Vylomova; Chaitanya Wu; Lawrence Malaviya; Garrett Wolf-Sonkin; Christo Nicolai; Miikka Kirov; Sebastian J Silfverberg; Jeffrey Mielke; Ryan Heinz; Mans Cotterell;  Hulden"}, {"ref_id": "b23", "title": "Spelling normalisation and linguistic analysis of historical text for information extraction", "journal": "", "year": "2016", "authors": "Eva Pettersson"}, {"ref_id": "b24", "title": "An SMT approach to automatic annotation of historical text", "journal": "Link\u00f6ping University Electronic Press", "year": "2013-05-22", "authors": "Eva Pettersson; Be\u00e1ta Megyesi; J\u00f6rg Tiedemann"}, {"ref_id": "b25", "title": "Training tips for the transformer model", "journal": "The Prague Bulletin of Mathematical Linguistics", "year": "2018", "authors": "Martin Popel; Ond\u0159ej Bojar"}, {"ref_id": "b26", "title": "An open diachronic corpus of historical Spanish: annotation criteria and automatic modernisation of spelling", "journal": "", "year": "2013", "authors": "Felipe S\u00e1nchez-Mart\u00ednez; Isabel Mart\u00ednez-Sempere; Xavier Ivars-Ribes; Rafael C Carrasco"}, {"ref_id": "b27", "title": "Modernizing historical Slovene words with character-based smt", "journal": "", "year": "2013", "authors": "Yves Scherrer; Toma\u017e Erjavec"}, {"ref_id": "b28", "title": "Modernising historical Slovene words", "journal": "Natural Language Engineering", "year": "2016", "authors": "Yves Scherrer; Toma\u017e Erjavec"}, {"ref_id": "b29", "title": "", "journal": "Complex Systems", "year": "1987", "authors": "Terrence J Sejnowski; Charles R Rosenberg"}, {"ref_id": "b30", "title": "Data augmentation for morphological reinflection", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Miikka Silfverberg; Adam Wiemerslage; Ling Liu; Lingshuang Jack Mao"}, {"ref_id": "b31", "title": "Rethinking the inception architecture for computer vision", "journal": "", "year": "2016", "authors": "Christian Szegedy; Vincent Vanhoucke; Sergey Ioffe; Jon Shlens; Zbigniew Wojna"}, {"ref_id": "b32", "title": "An evaluation of neural machine translation models on historical spelling normalization", "journal": "", "year": "2018", "authors": "Gongbo Tang; Fabienne Cap; Eva Pettersson; Joakim Nivre"}, {"ref_id": "b33", "title": "Why self-attention? a targeted evaluation of neural machine translation architectures", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Gongbo Tang; Mathias M\u00fcller; Annette Rios; Rico Sennrich"}, {"ref_id": "b34", "title": "Tensor2tensor for neural machine translation", "journal": "", "year": "2018", "authors": "Ashish Vaswani; Samy Bengio; Eugene Brevdo; Francois Chollet; Aidan N Gomez; Stephan Gouws; Llion Jones; \u0141ukasz Kaiser; Nal Kalchbrenner; Niki Parmar"}, {"ref_id": "b35", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b36", "title": "The Carnegie Mellon pronouncing dictionary", "journal": "", "year": "1998", "authors": "R L Weide"}, {"ref_id": "b37", "title": "Exact hard monotonic attention for character-level transduction", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Shijie Wu; Ryan Cotterell"}, {"ref_id": "b38", "title": "Hard non-monotonic attention for character-level transduction", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Shijie Wu; Pamela Shapiro; Ryan Cotterell"}, {"ref_id": "b39", "title": "Whitepaper of NEWS 2015 shared task on machine transliteration", "journal": "", "year": "2015", "authors": "Min Zhang; Haizhou Li; Rafael E Banchs; A Kumaran"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Development set accuracy for 5 languages on morphological inflection with different batch sizes.We evince our two primary contributions: (1) we set the new state of the art morphological inflection using the transformer and (2) we demonstrate the transformer's dependence on the batch size.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Distribution of incorrectly inflected forms in the test set of the inflection task over all 52 languages grouped by desired output word length.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Handling of feature-guided character-level transduction with special position and type embeddings in the encoder. F denotes features while C denotes characters. We use morphological inflection as an example, inflecting smear into its past participle form, smeared.", "figure_data": "VanillaFeature InvariantToken<s>V V.PTCPPSTsmear</s><s>V V.PTCPPSTsmear</s>+++++ + + + +++++++ + + + ++Position01234 5 6 7 8900001 2 3 4 56++++ + + + +TypeFFF C C C C CFigure 2:high-resource setting only provides \u2248 10k trainingexamples per language. It is also not obvious thatnon-recurrent architectures such as the transformer"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "ACC CER i ACC s CER s i", "figure_data": "Ljube\u0161i\u0107 et al. (2016)91.78 0.392 90.37 0.360Ljube\u0161i\u0107 et al. (2016) (LM)91.56 0.399 89.93 0.368Bollmann (2018)91.27 0.381 89.73 0.350Tang et al. (2018a)91.67 0.389 90.32 0.358Flachs et al. (2019)--90.06 -Transformer (Dropout = 0.3) 91.30 0.340 89.99 0.330Transformer (Dropout = 0.1) 91.85 0.352 90.61 0.334"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Average test performance on historical text normalization of Transformer against models from the literature. s denote subset of dataset asFlachs et al. (2019) only experiment with subset of languages.", "figure_data": "WER PERACC MFSWu et al. (2018)28.20 0.068 41.10 0.894Wu and Cotterell (2019)28.20 0.069 41.20 0.895Transformer (Dropout = 0.3) 28.08 0.070 43.39 0.897Transformer (Dropout = 0.1) 27.63 0.069 41.35 0.891"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": ": Average test performance on Grapheme-to-Phoneme and dev performance on Transliteration ofTransformer against models from the literature."}], "formulas": [], "doi": "10.18653/v1/P17-1183"}