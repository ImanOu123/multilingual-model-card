{"title": "Large-Scale Object Classification using Label Relation Graphs", "authors": "Jia Deng; Nan Ding; Yangqing Jia; Andrea Frome; Kevin Murphy; Samy Bengio; Yuan Li; Hartmut Neven; Hartwig Adam", "pub_date": "", "abstract": "In this paper we study how to perform object classification in a principled way that exploits the rich structure of real world labels. We develop a new model that allows encoding of flexible relations between labels. We introduce Hierarchy and Exclusion (HEX) graphs, a new formalism that captures semantic relations between any two labels applied to the same object: mutual exclusion, overlap and subsumption. We then provide rigorous theoretical analysis that illustrates properties of HEX graphs such as consistency, equivalence, and computational implications of the graph structure. Next, we propose a probabilistic classification model based on HEX graphs and show that it enjoys a number of desirable properties. Finally, we evaluate our method using a large-scale benchmark. Empirical results demonstrate that our model can significantly improve object classification by exploiting the label relations.", "sections": [{"heading": "Introduction", "text": "Object classification, assigning semantic labels to an object, is a fundamental problem in computer vision. It can be used as a building block for many other tasks such as localization, detection, and scene parsing. Current approaches typically adopt one of the two classification models: multiclass classification, which predicts one label out of a set of mutually exclusive labels(e.g. entries in ILSVRC [9]), or binary classifications, which make binary decisions for each label independently(e.g. entries in PASCAL VOC classification competitions [13]).\nBoth models, however, do not capture the complexity of semantic labels in the real world. Multiclass classification tasks typically assume a set of mutually exclusive labels. Although efforts have been made to artificially constrain the label set in benchmarks (e.g. the ImageNet Challenges [9] select a subset of mutually exclusive labels from WordNet), this assumption becomes increasingly impractical as we consider larger, more realistic label sets. This is because the same object can often be described by multiple labels. An object classified as \"husky\" is automatically a \"dog\"; meanwhile it may or may not be a \"puppy\". Making \"husky\", \"dog\", and \"puppy\" mutually exclusive labels clearly violates real world semantics. Independent binary classifiers, on the other hand, ignore the constraints between labels and can thus handle overlapping labels. But this can lead to inconsistent predictions such as an object being both a dog and a cat, or a husky but not a dog. In addition, discarding the label relations misses the opportunity to transfer knowledge during learning. For example, in practical settings training images are not always annotated to the most specific labels -many Internet images are simply labeled as \"dog\" instead of \"husky\" or \"German Shepherd\". Intuitively, learning a good model for \"dog\" should benefit learning breeds of dogs (and vice versa) but training independent binary classifiers will not be able to capitalize on this potential knowledge transfer.\nIn this paper we study how to perform classification in a principled way that exploits the rich structure of real world labels. Our goal is to develop a new classification model that allows flexible encoding of relations based on prior knowledge, thus overcoming the limitations of the overly restrictive multiclass model and the overly relaxed independent binary classifiers (Fig. 1).\nWe first introduce Hierarchy and Exclusion (HEX) graphs, a new formalism allowing flexible specification of relations between labels applied to the same object: (1) mutual exclusion (e.g. an object cannot be dog and cat), (2) overlapping (e.g. a husky may or may not be a puppy and vice versa), and (3) subsumption (e.g. all huskies are dogs). We provide theoretical analysis on properties of HEX graphs such as consistency, equivalence, and computational implications.\nNext, we propose a probabilistic classification model leveraging HEX graphs. In particular, it is a special type of Conditional Random Field (CRF) that encodes the label relations as pairwise potentials. We show that this model enjoys a number of desirable properties, including flexible encoding of label relations, predictions consistent with label relations, efficient exact inference for typical graphs, learning labels with varying specificity, knowledge transfer, and unification of existing models.\nFinally, we evaluate our approach using the ILSVRC2012 [9], a large-scale benchmark for object classification. We also perform experiments on zero-shot recognition. Empirical results demonstrate that our model can significantly improve object classification by exploiting the label relations.\nOur main contribution is theoretical, i.e. we propose a new formalism (HEX graphs), a new classification model, and a new inference algorithm, all grounded on rigorous analysis. In addition, we validate our approach using large-scale data, showing significant empirical benefits.", "publication_ref": ["b8", "b12", "b8", "b1", "b8"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Related Work", "text": "Our approach draws inspirations from various themes explored in prior literature, including hierarchies, multilabel annotation, large-scale classification, and knowledge transfer. The main novelty of our work is unifying them into a single probabilistic framework with a rigorous theoretical foundation.\nExploiting hierarchical structure of object categories has a long history [34]. In particular, label hierarchies have been used to share representations [15,2,8,17] and combine models [18,38,27].\nCorrelations between labels have been explored in multilabel annotation (e.g. [20]), but most prior work addresses contextual relations between co-occurring objects (e.g. [10]), as opposed to our setting of multiple labels on the same object. Lampert et al. and Bi and Kwok studied hierarchical annotations as structured predictions [24,4,3]. Chen et al. considered exclusive relations between labels [6].\nTo our knowledge we are the first to jointly model hierarchical and exclusive relations. By treating unobserved labels as latent variables, our approach also connects to prior work on learning from partial or incomplete labels [19,7,5].\nOur model is a generalized multiclass classifier. It is designed to run efficiently and can thus be adapted to work with techniques developed for large-scale classification involving many labels and large datasets [32,29].\nFinally, by modeling the label relations and ensuring consistency between visual predictions and semantic relations, our approach relates to work in transfer learning [31,30], zero-shot learning [28,12,25], and attribute-based recognition [1,36,33,14], especially those that use semantic knowledge to improve recognition [16,30] and those that propagate or borrow annotations between categories [26,22].", "publication_ref": ["b33", "b14", "b1", "b7", "b16", "b17", "b26", "b19", "b9", "b23", "b3", "b2", "b5", "b18", "b6", "b4", "b31", "b28", "b30", "b29", "b27", "b11", "b24", "b0", "b35", "b32", "b13", "b15", "b29", "b25", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Approach", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Hierarchy and Exclusion (HEX) Graphs", "text": "We start by introducing the formalism of Hierarchy and Exclusion (HEX) graphs, which allow us to express prior knowledge about the labels. Due to space limit, all proofs and some lemmas are provided in the supplemental material. Definition 1. A HEX graph G = (V, E h , E e ) is a graph consisting of a set of nodes V = {v 1 , . . . , v n }, directed edges E h \u2286 V \u00d7 V , and undirected edges E e \u2286 V \u00d7 V , such that the subgraph G h = (V, E h ) is a directed acyclic graph (DAG) and the subgraph G e = (V, E e ) has no self loop.\nEach node v \u2208 V represents a distinct label. An edge (v i , v j ) \u2208 E h is a hierarchy edge, indicating that label i subsumes label j, e.g. \"dog\" is a parent, or superclass of \"husky\". The subgraph with only those edges form a semantic hierarchy. An edge (v i , v j ) \u2208 E e is called an exclusion edge, indicating that label v i and v j are mutually exclusive, e.g. an object cannot be dog and cat. If two labels share no edge, it means that they overlap, i.e. each label can turn on or off without constraining the other.\nAlternatively one can think of each label as representing a set of object instances and the relations between labels as relations between (distinct) sets (Fig. 2). A hierarchy edge corresponds to one set containing the other. An exclusion edge corresponds to two disjoint sets. No edge corresponds to overlapping sets-the only remaining case.\nIt is worth nothing that while it is convenient to assume mutually exclusive children in a hierarchy, this is not the case for real world hierarchies, e.g. \"child\", \"male\", \"female\" are all children of \"person\" in WordNet. Thus we need a HEX graph to express those complexities.\nEach label takes binary values, i.e. v i \u2208 {0, 1}. Each edge then defines a constraint on values the two labels can take. A hierarchy edge (v i , v j ) \u2208 E h means that an assignment of (v i , v j ) = (0, 1) (e.g. a husky but not a dog) is illegal. An exclusion edge (v i , v j ) \u2208 E e means that (v i , v j ) = (1, 1) (both cat and dog) is illegal. These local constraints of individual edges can thus define legal global assignments of labels. Definition 2. An assignment (state) y \u2208 {0, 1} n of labels V in a HEX graph G = (V, E h , E e ) is legal if for any (y i , y j ) = (1, 1), (v i , v j ) \u2208 E e and for any (y i , y j ) = (0, 1), (v i , v j ) \u2208 E h . The state space S G \u2286 {0, 1} n of graph G is the set of all legal assignments of G.\nWe now introduce some notations for further development. Let \u03b1(v i ) the set of all ancestors of v i \u2208 V and\u1fb1(v i ) = \u03b1(v i ) \u222a {v i } (ancestors and the node itself). Let \u03c3(v i ) be the set of all descendants of\nv i \u2208 V and\u03c3(v i ) = \u03c3(v i ) \u222a {v i }.\nLet (v i ) be the set of exclusive nodes, those sharing an exclusion edge with v i . Let o(v i ) be the set of overlapping nodes, those sharing no edges with v i .\nConsistency So far our definition of the HEX graph allows arbitrary placement of exclusion edges. This, however, can result in non-sensible graphs. For example, it allows label v i and v j to have both hierarchy and exclusion edges, i.e. v i subsumes v j and v i and v j are exclusive. This makes label v j \"dead\", meaning that it is always 0 and thus cannot be applied to any object instance without causing a contradiction: if it takes 1, then it's parent v i must take value 1 per the hierarchy edge and also take 0 per the exclusion edge. This demonstrates the need for a concept of consistency: a graph is consistent if every label is \"active\", i.e. it can take value either 1 or 0 and there always exists an assignment to the rest of labels such that the whole assignment is legal. Definition 3. A HEX graph G = (V, E h , E e ) is consistent if for any label v i \u2208 V , there exists two legal assignments y, y \u2208 {0, 1} n such that y i = 1 and y i = 0.\nConsistency is in fact solely determined by the graph structure-it is equivalent to the condition that for any label, there is no exclusion edge between its ancestors or between itself and its ancestors:\nTheorem 1. A HEX graph G = (V, E h , E e ) is consistent if and only if for any label v i \u2208 V , E e \u2229 (\u1fb1(v i ) \u00d7\u1fb1(v i )) = \u2205.\nWe thus have an algorithm to check consistency without listing the state space. As will become clear, consistency is very important algorithmically.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Classification Model", "text": "A HEX graph encodes our prior knowledge about label relations. We can thus define a probabilistic classification model based on a HEX graph G = (V, E h , E e ). Let x \u2208 X be an input and f (x; w) : X \u2192 R n be a function with parameters w that maps an input image (or bounding box) to a set of scores, one for each label. The form of f is not essential (e.g. it can be a linear model w T i x or a deep neural network) so we leave it unspecified. We define a joint distribution of an assignment of all labels y \u2208 {0, 1} n as a Conditional Random Field (CRF) [23]:\nP (y|x) = i e fi(x;w)[yi=1] (vi,vj )\u2208E h [(y i , y j ) = (0, 1)] (vi,vj )\u2208Ee [(y i , y j ) = (1, 1)],(1)\nwhereP is the unnormalized probability. The probability is then Pr(y|x) = P (y|x)/Z(x), where Z(x) = \u0177P (\u0177|x) is the partition function. To compute the probability of a label, we marginalize all other labels. The scores f i (x; w) can be thought of as raw classification scores (local evidence) for each label and our model can convert them into marginal probabilities.\nIt is easy to verify a few facts about the model: (1) the probability of any illegal assignment is zero; (2) to compute the probability of a legal assignment, we take all labels with value 1, sum their scores, exponentiate, and then normalize;\n(3) the marginal label probabilities are always consistent with the label relations: probability of \"dog\" is always bigger than that of \"husky\" and probabilities of \"dog\" and \"cat\" cannot add to more than 1; (4) the model assumes an open world to gracefully handle unknown categories. For each node on the hierarchy, it is legal to assign itself a value 1 and all its descendants value 0. e.g. an object is a \"dog\" but none of the known dog subcategories. If the model sees novel dog subcategory, it can produce a large marginal for dog but will not be compelled to assign a large probability to a known subcategory.\nSpecial cases A nice property is that it unifies standard existing models. If we use a HEX graph with pairwise exclusion edges and no hierarchy edges, i.e. all nodes are mutually exclusive, it is easy to verify that we arrive at the popular softmax (or multinomial regression) 1 : Pr(y i = 1|x) = e fi /(1 + j e fj ). Another special case is when the HEX graph has no edges at all, i.e. all labels are independent. Eqn. 1 thus fully decomposes: Pr(y|x) = i e fi[yi=1] /(1 + e fi ), i.e. independent logistic regressions for each label.\nJoint hierarchical modeling We highlight another property: our model allows flexible joint modeling of hierarchical categories, thus enabling potential knowledge transfer. It is easy to verify that for all graphs the marginal probability of a label depends the sum of its ancestors' scores, i.e. Pr(y i = 1|x) has the term exp(f i + vj \u2208\u03b1(vi) f j ), because all its ancestors must be 1 if the label takes value 1. Thus the model allows the score for \"dog\" to influence decisions about \"husky\". If the score function f i (x; w) is a linear model w T i x, then Pr(y i = 1|x) \u221d exp(w T i x), wherew i = w i + vj \u2208\u03b1(vi) w j , i.e the weights decompose along the hierarchy-the weights for \"husky\" are a combination of weights for \"husky-ness\", \"dog-ness\", and \"animal-ness\". This enables a form of sharing similar to prior work [8]. Note that depending on the applications, sharing can also be disabled by using constant scores (e.g. zeros).\nConversely, the probability of an internal node of the hierarchy also depends on the probabilities of its descendants because we need to marginalize over all possible states of the descendants. For example, if we have a tree hierarchy with mutually exclusive siblings, it can be shown that the unnormalized probabilit\u1ef9 P of an internal node is a simple recursive form involving its own score, its ancestors' scores, and the sum of unnormalized probabilities of its direct children c(v i ), i.e.P (y i = 1|x) = exp f i + v k \u2208\u03b1(vi) f k + vj \u2208c(vi)P (y j = 1|x). Again we can use constant scores for internal nodes, in which case the model is a collection of \"local leaf models\" and we simply sum the probabilities of leaves.\nLearning In learning we maximize the (marginal) likelihood of the observed ground truth labels using stochastic gradient descent (SGD). Given training examples D = {(x (l) , y (l) , g (l) )}, l = 1, . . . , m, where y (l) \u2208 {0, 1} n is the complete ground truth label vector and g (l) \u2286 {1, . . . , n} is the indices of the observed labels, the loss function is :\nL(D, w) = \u2212 l log Pr(y (l) g (l) |x (l) ; w) = \u2212 l log y:y g (l) =y (l) g (l) Pr(y|x (l) ; w) (2)\nIn training we often have incomplete ground truth. Labels can be at any level of the hierarchy. We may only know the object is a \"dog\" but uncertain about the specific breed. Incomplete ground truth also occurs with labels that are not hierarchical but can still overlap (\"husky\" and \"puppy\"). Our model can naturally handle this by treating the unobserved labels as latent variables and marginalizing them in computing the likelihood.", "publication_ref": ["b22", "b0", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Efficient Inference", "text": "Inference-computing the partition function and marginalizing unobserved labelsis exponential in the number of labels if performed with brute force. Treated as a generic CRF, our model can easily be densely connected and full of loops, especially when there are many mutual exclusions, as is typical for object labels. Thus at first glance exact inference is intractable.\nHowever, in this section we show that exact inference is tractable for a large family of HEX graphs with dense connections, especially in realistic settings. The main intuition is that when the graph is densely connected, the state space can be small due to the special form of our binary potentials-all illegal states have probability zero and they can simply be eliminated from consideration. One example is the standard multiclass setting where all labels are mutually exclusive, in which case the size of the state space is O(n). On the other hand, when a graph is sparse, i.e. has small treewidth, then standard algorithms such as junction trees apply. Our algorithm will try to take the best of both worlds by transforming a graph in two directions: (1) to an equivalent sparse version with potentially small treewidth; (2) to an equivalent dense version such that we can afford to exhaustively list the state space for any subset of nodes. The final algorithm is a modified junction tree algorithm that can be proven to run efficiently for many realistic graphs.\nEquivalence Two HEX graphs are equivalent if they have the same state space:\nDefinition 4. HEX graphs G and G are equivalent if S G = S G .\nIntuitively equivalent graphs can arise in two cases. One is due to the transitivity of the subsumption relation-if \"animal\" subsumes \"dog\" and \"dog\" subsumes \"husky\" then \"animal\" should be implied to subsume \"husky\". Thus a hierarchy edge from \"animal\" to \"husky\" is redundant. The other case is that mutual exclusion can be implied for children by parents. For example, if \"cat\" and \"dog\" are exclusive, then all subclasses of \"dog\" should be implied to be exclusive with \"cat\". Thus an exclusion edge between \"husky\" and \"cat\" is redundant. Formally redundant edges are those that can be removed or added without changing the state space:\nDefinition 5. Given a graph G = (V, E h , E e ), a directed edge e \u2208 V \u00d7 V (not necessarily in E h ) is redundant if G = (V, E h \\ {e}, E e ) and G = (V, E h \u222a {e}, E e ) are both equivalent to G. An undirected edge e \u2208 V \u00d7 V (not necessarily in E e ) is redundant if G = (V, E h , E e \\ {e}) and G = (V, E h , E e \u222a {e}) are both equivalent to G.\nFor consistent graphs, redundant edges can be found by searching for certain graph patterns: for a directed hierarchy edge (v i , v j ), it is redundant if and only if there is an alternative path from v i to v j . For an undirected exclusion edge (v i , v j ), it is redundant if and only if there is an another exclusion edge that connects their ancestors (or connects one node's ancestor to the other node).\nLemma 1. Let G = (V, E h , E e ) be a consistent graph. A directed edge e \u2208 V \u00d7V is redundant if and only if in the subgraph G = (V, E h ) there exists a directed path from v i to v j and the path doesn't contain e. An undirected edge e = (v i , v j ) \u2208 V \u00d7 V is redundant if and only if there exists an exclusion edge e = (v k , v l ) \u2208 E e such that v k \u2208\u1fb1(v i ), v l \u2208\u1fb1(v j ) and e = e .\nLemma 1 in fact gives an algorithm to \"sparsify\" or \"densify\" a graph. We can remove one redundant edge a time until we obtain a minimally sparse graph. We can also add edges to obtain a maximally dense equivalent. (Fig. 3). Definition 6. A graph G is minimally sparse if it has no redundant edges. A graph G is maximally dense if every redundant edge is in G.\nIn fact for a consistent graph, its minimally sparse or maximally dense equivalent graph is unique, i.e. we always arrive at the same graph regardless of the order we remove or add redundant edges: Theorem 2. For any consistent graphs G and G that are both minimally sparse (or maximally dense), if S G = S G , then G = G .\nThus given any consistent graph, we can \"canonicalize\" it by sparsifying or densifying it 2 . This can help us reason about the size of the state space.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Size of State Space", "text": "If a graph has very dense connections, its state space tends to be tractable, such as the case of pairwise mutually exclusive labels. Intuitively, for labels applied to real world objects, there should be many mutual exclusions. For example, if we randomly pick two labels from the English dictionary, most likely they will have a relation of exclusion or subsumption when applied to the same object. In other words, for the same object, there shouldn't be too many overlapping labels. Otherwise the state space can grow exponentially with the amount of overlap. We can formalize this intuition by first introducing the quantity maximum overlap. if G = \u2205 then return \u2205 3:", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 7. The maximum overlap of a consistent graph", "text": "G = (V, E h , E e ) is \u2126 G = max v\u2208V |o\u1e20(v)|, where o\u1e20(v) = {u \u2208 V : (u, v) \u2208\u0112 h \u2227(v, u) \u2208\u0112 h \u2227(u, v) \u2208 E e } and\u1e20 = (V,\u0112 h ,\u0112 e ) is the maximally dense equivalent of G.\nend if 4:\nLet G = (V, E h , Ee) and n = |V |.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "5:", "text": "Pick an arbitrary vi \u2208 V . 6:\nV 0 \u2190 \u03b1(vi) \u222a (vi) \u222a o(vi). 7: G 0 \u2190 G[V 0 ] 8: S G 0 \u2190 ListStateSpace(G 0 ) 9: S 0 G \u2190 {y \u2208 {0, 1} n : yi = 0 y \u03c3(v i ) = 0 y V 0 \u2208 S G 0 }. 10: V 1 \u2190 \u03c3(vi) \u222a o(vi). 11: G 1 \u2190 G[V 1 ] 12: S G 1 \u2190 ListStateSpace(G 1 ) 13: S 1 G = {y \u2208 {0, 1} n : yi = 1 y \u03b1(v i ) = 1 (vi) = 0 y V 1 \u2208 S G 1 } 14: return S 0 G \u222a S 1 G . 15: end function\nThat is, we first convert a graph to its maximally dense equivalent and then take the max of the per-node overlap-the number of non-neighbours of a node. In other words, the overlap of a label is the number of other non-superset and non-subset labels you can additionally apply to the same object. We can now use the maximum overlap of a graph to bound the size of its state space:\nTheorem 3. For a consistent graph G = (V, E h , E e ), |S G | \u2264 (|V |\u2212\u2126 G +1)2 \u2126 G .\nAs an interesting fact, if a HEX graph consists of a tree hierarchy and exclusion edges between all siblings, then it's easy to verify that its maximum overlap is zero and its state space size is exactly |V | + 1, a tight bound in this case.\nListing State Space A tractable size of the state space is useful for inference only if the legal states can also be enumerated efficiently. Fortunately this is always the case for HEX graphs. To list all legal assignments for an input, we can first pick an arbitrary node as a \"pivot\" and fix its value to 1. Also we fix all its parents to 1 and exclusive neighbours to 0 because otherwise we would violate the constraints. We then recursively apply the same procedure to the subgraph induced by the rest of the nodes (children and non-neighbours). For each returned assignment of the subgraph, we generate a new assignment to the full graph by concatenating it with the fixed nodes. Similarly, we fix the pivot node to 0 and fix its children to 0, and then recurse on the subgraph induced by the rest of the nodes. See Alg. 1 for pseudo-code. We can formally prove that if the graph is consistent and maximally dense, this greedy procedure returns all legal assignments and runs in time linear in the size of the state space: Full Inference Algorithm We now describe the full inference algorithm (Alg. 2). Given a graph, we first generate the minimally sparse and maximally dense equivalents. We treat the minimally sparse graph as a generic CRF and generate a junction tree. For each clique of the junction tree, we list its state space using the subgraph induced by the clique on the maximally dense graph. To do inference we run two passes of sum-product message passing on the junction tree, performing computation only on the legal states of each clique. This algorithm thus automatically exploits dynamic programming for sparse regions of the graph and small state spaces for dense regions of the graph. For example, it is easy to verify that for pairwise mutually exclusive labels, the inference cost is O(n), the same as hand-coded softmax, due to the small state space. For fully independent labels (no edges), the inference cost is also O(n), the same as n hand-coded logistic regressions, because the junction tree is n disconnected cliques, each containing a single label. We can formally bound the complexity of the inference algorithm: \nLemma 2. If graph G = (V, E h , E e )\nG = (V, E h , E e ) is O min{|V |2 w , |V | 2 2 \u2126 G } ,\nwhere w is the width of the junction tree T (Line 3).\nNote that this is a worst case bound. For example, a graph can have two disconnected components. One has a large treewidth but small overlap (e.g. many mutual exclusions between object labels). The other has a small treewidth but large overlap (e.g. attribute labels). The bound in Theorem 4 for the whole graph will assume the worst, but Alg. 2 can perform inference for this graph efficiently by automatically treating the two components differently.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Implementation", "text": "We implement our model as a standalone layer in a deep neural network framework. It can be put on top of any feed-forward architecture. The layer takes as input a set of scores f (x; w) \u2208 R n and outputs (marginal) probability of a given set of observed labels. During learning, we use stochastic gradient descent and compute the derivative \u2202L \u2202f , where L is the loss as defined in Eqn. 2 but treated here as a function of the label scores f instead of the raw input x. This derivative is then back propagated to the previous layers represented by f (x; w).\nFor exact inference as described in Alg. 2, Step 1 to Step 4 (processing the graph, building the junctions trees, listing state space etc.) only depend on the graph structure and are performed offline. Only the message passing on the junction tree (Step 5) needs to be performed for each example online. Given a junction tree and the legal assignments for each clique, we perform a \"dry run\" of message passing and record the sequence of sum-product operations. Then the online inference for each example simply follows this pre-determined sum-product sequence and thus has negligible extra overhead compared to handcoded implementations of softmax or independent logistic regressions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Object classification on ImageNet", "text": "Dataset We evaluate our model using the ILSVRC2012 dataset [9]. It consists of 1.2M training images from 1000 object classes 3 . These 1000 classes are mutually exclusive leaf nodes of a semantic hierarchy based on WordNet that has 820 internal nodes. All images are labeled to the leaf nodes.\nWordNet provides hierarchical relations but no exclusive relations. We thus adopt the assumption of \"exclusive whenever possible\", i.e. putting an exclusive edge between two nodes unless it results in an inconsistent graph. This means that any two labels are mutually exclusive unless they share a descendant. Note that the WordNet hierarchy is a DAG instead of a tree. For example, \"dog\" appear under both \"domestic animal\" and \"canine\".\nSetup In this experiment we test the hypothesis that our method can improve object classification by exploiting label relations, in particular by enabling joint modeling of hierarchical categories. To this end, we evaluate the recognition performance in the typical test setting-multiclass classification at the leaf levelbut allow the training examples to be labeled at different semantic levels. This setting is of practical importance because when one collects training examples from the Internet, the distribution of labels follow a power law with many more images labeled at basic levels (\"dog\",\"car\") than at fine-grained levels (\"husky\", \"Honda Civic\"). While it is obvious that with a semantic hierarchy, one can aggregate all training examples from leaf nodes to learn a classifier for internal nodes, the other direction-how to use higher level classes with many training examples to help train fine-grained classes with fewer examples-is not as clear.\nSince ILSVRC2012 has no training examples at internal nodes, we create training examples for internal nodes by \"relabelling\" the leaf examples to their immediate parent(s) (Fig. 4), i.e. some huskies are now labeled as \"dog\". Note that each image still has just one label; an image labeled as \"husky\" is not additionally labeled as \"dog\".\nWe put our model on top of the convolutional neural network (CNN) developed by Krizhevsky et al. [21,37]. Specifically, we replace the softmax classifier layer (fully connected units with a sigmoid activation function followed by normalization to produce the label probabilities) with our layer -fully connected units followed by our inference to produce marginals for each label (see Fig. 1 for an illustration). We only use fully connected units for the leaf nodes and fix input scores (f i in Eqn. 1) of internal nodes to zero because we found that otherwise it takes longer to train but offer no significant benefits on ILSVRC2012.\nWe compare our model with three baselines, all on top of the same CNN architecture and all trained with full back propagation from scratch: (1) softmax on leaf nodes only i.e. ignoring examples labeled to internal nodes; (2) softmax on all labels, i.e. treating all labels as mutually exclusive even though \"dog\" subsumes \"husky\"; (3) independent logistic regressions for each label 4 . For the logistic regression baseline, we need to specify positive and negative examples individually for each class-we add to positives by aggregating all examples from descendants and use as negatives all other examples except those from the ancestors. During test, for each method we use its output probabilities for the 1000 leaf nodes (ignoring others) to make predictions.\nResults Table 1 reports the classification accuracy on the leaf nodes with different amounts of relabelling (50%, 90%, 95%, 99%). As a reference, our implementation of softmax with all examples labeled at leaf nodes (0% relabeling) gives a hit rate (accuracy) of 62.6% at top 1 and 84.3% at top 5 5 .\nOur approach outperforms all baselines in all settings except that in the extreme case of 99% relabelling, our approach is comparable with the best baseline. Even with 90% of labels at leaf nodes \"weakened\" into internal nodes, we can still achieve 55.3% top 1 accuracy, not too big a drop from 62.6% by using all leaf training data. Also independent logistic regressions perform very abysmally, likely because of the lack of calibration among independent logistic regressions and varying proportions of positive examples for different classes.\nInterestingly the baseline of softmax on all labels, which seems non-sensible as it trains \"dog\" against \"husky\", is very competitive. We hypothesize that this is because softmax treats examples in internal nodes as negatives. It is in fact a mostly correct assumption: the \"dog\" examples contain some huskies but are mostly other types of dogs. Softmax thus utilizes those negative examples effectively. On the other hand, our model treats labels in internal nodes as weak positive examples because the marginal of an internal node depends on the scores of its descendants. This is semantically correct, but makes no assumption about the proportion of real positive examples. This suggests a future direction of including stronger assumptions during learning to further improve our model.\nIn the case of 99% relabelling, softmax-all is comparable to our model (41.5% versus 41.5% top 1 accuracy). This is likely because there are too few images labeled at the leaf nodes (around 10 images per class). During learning, our model \"softly\" assigns the images labeled at internal nodes to leaves (by inferring the distribution of unobserved labels), which improves the leaf models (weights for generating leaf input scores). However, with too few training examples labeled at leaf nodes, the leaf models can \"drift away\" with noisy assignment.", "publication_ref": ["b8", "b20", "b3", "b4"], "figure_ref": ["fig_6", "fig_0"], "table_ref": ["tab_0"]}, {"heading": "Zero-shot recognition on Animals with Attributes", "text": "In this experiment we evaluate whether our HEX graph based model can be used to model relations between objects and attributes, although not by design. We use the Animal with Attributes (AWA) dataset [25] that includes images from 50 animal classes. For each animal class, it provides binary predicates for 85 attributes, e.g. for zebra, \"stripes\" is yes and \"eats fish\" is no. We evaluate the zero-shot setting where training is performed using only examples from 40 classes and testing is on classifying the 10 unseen classes. The binary predicates and the names of the unseen classes are both known a priori.\nHere we show that our model can be easily adapted to perform zero-shot recognition exploiting object-attribute relations. First we build a HEX graph for all animals and attributes by assuming mutual exclusion between the animal classes and then adding the object-attribute relations: \"zebra has stripes\" establish a subsumption edge from \"stripes\" to \"zebra\"; \"zebra doesn't eat fish\" means an exclusion edge between them.\nWe use the same published, pre-computed features x from [25] and use a linear model to map the features to score f i (x) = w T i \u03c6(x) for label i, where \u03c6(x) is computed by the Nystrom method [35] with rank 8000 to approximate the Chisquared kernel used in [25]. In training, we observe the class labels for examples from the first 40 classes. The system is trained to maximize the likelihood of the class labels, but indirectly it learns to also predict the (latent) attributes given the image features. At test time, the class labels are not observed (and are drawn from a distinct set of 10 new labels); however, the model can predict the attributes given the image, and since the mapping from attributes to classes is known (for all 50 classes), the model can also (indirectly) predict the novel class label. This can be done by performing inference in the model and reading out Fig. 5. We can build a HEX graph using relations between objects and attributes.\nthe marginals for the 10 unknown classes. We achieve a 38.5% mean accuracy (and 44.2% using the recently released DECAF features [11]) as compared to 40.5% in [25]. Given that our model is not explicitly designed for this task and the kernel approximation involved, this is a very encouraging result.", "publication_ref": ["b24", "b24", "b34", "b24", "b10", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Efficiency of Inference", "text": "We evaluate the empirical efficiency of inference by counting the number of basic operations (summations and multiplications) needed to compute the marginals for all labels from the scores (not including computation for generating the scores). For the HEX graph used with ILSVRC2012, i.e. a DAG hierarchy and dense mutual exclusions, the inference cost is 6 relative to softmax for the same number of labels. For AWA, the cost is 294 relative to softmax. In both cases, the overhead is negligible because while softmax costs O(n), simply computing the scores from d-dimensional inputs costs O(nd) using a linear model (d = 4096 for ILSVRC2012 and d = 8000 for AWA), let alone multilayer neural networks.\nMoreover, it is worth noting that the HEX graph for AWA (Fig. 5) has 85 overlapping attributes with no constraints between them. Thus it has at least 2 85 legal states. Also it has a large treewidth-the 50 animal classes are fully connected with exclusion edges, a complexity of 2 50 for a naive junction tree algorithm. But our inference algorithm runs with negligible cost. This again underscores the effectiveness of our new inference algorithm in exploiting both dynamic programming and small state space.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discussions and Conclusions", "text": "We now briefly mention a couple of possible future directions. Efficient exact inference depends on the relations being \"absolute\" such that many states have probability zero. But it does not allow non-absolute relations (\"taxi is mostly yellow\"). Thus it remains an open question how to use non-absolute relations in conjunction with absolute ones. Another direction is to integrate this model developed for single objects into a larger framework that considers spatial interactions between objects.\nTo conclude, we have provided a unified classification framework that generalizes existing models. We have shown that it is flexible, theoretically principled, and empirically useful. Finally, we note that although motivated by object classification, our approach is very general in that it applies to scenes, actions, and any other domains with hierarchical and exclusive relations. ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Label-embedding for attributebased classification", "journal": "IEEE", "year": "2013", "authors": "Z Akata; F Perronnin; Z Harchaoui; C Schmid"}, {"ref_id": "b1", "title": "Uncovering shared structures in multiclass classification", "journal": "ACM", "year": "2007", "authors": "Y Amit; M Fink; N Srebro; S Ullman"}, {"ref_id": "b2", "title": "Multi-label classification on tree-and dag-structured hierarchies", "journal": "", "year": "2011", "authors": "W Bi; J T Kwok"}, {"ref_id": "b3", "title": "Mandatory leaf node prediction in hierarchical multilabel classification", "journal": "NIPS", "year": "2012", "authors": "W Bi; J T Kwok"}, {"ref_id": "b4", "title": "Multi-label learning with incomplete class assignments", "journal": "IEEE", "year": "2011", "authors": "S S Bucak; R Jin; A K Jain"}, {"ref_id": "b5", "title": "Multi-label visual classification with label exclusive context", "journal": "IEEE", "year": "2011", "authors": "X Chen; X T Yuan; Q Chen; S Yan; T S Chua"}, {"ref_id": "b6", "title": "Learning from partial labels", "journal": "The Journal of Machine Learning Research", "year": "2011", "authors": "T Cour; B Sapp; B Taskar"}, {"ref_id": "b7", "title": "Large margin hierarchical classification", "journal": "ACM", "year": "2004", "authors": "O Dekel; J Keshet; Y Singer"}, {"ref_id": "b8", "title": "Imagenet large scale visual recognition challenge 2012", "journal": "", "year": "2012", "authors": "J Deng; A Berg; S Satheesh; H Su; A Khosla; L Fei-Fei"}, {"ref_id": "b9", "title": "Discriminative models for multi-class object layout", "journal": "International journal of computer vision", "year": "2011", "authors": "C Desai; D Ramanan; C C Fowlkes"}, {"ref_id": "b10", "title": "Decaf: A deep convolutional activation feature for generic visual recognition", "journal": "", "year": "2013", "authors": "J Donahue; Y Jia; O Vinyals; J Hoffman; N Zhang; E Tzeng; T Darrell"}, {"ref_id": "b11", "title": "Write a classifier: Zero-shot learning using purely textual descriptions", "journal": "ICCV", "year": "2013", "authors": "M Elhoseiny; B Saleh; A Elgammal"}, {"ref_id": "b12", "title": "The pascal visual object classes (voc) challenge", "journal": "International journal of computer vision", "year": "2010", "authors": "M Everingham; L Van Gool; C K Williams; J Winn; A Zisserman"}, {"ref_id": "b13", "title": "Attribute-centric recognition for cross-category generalization", "journal": "IEEE", "year": "2010", "authors": "A Farhadi; I Endres; D Hoiem"}, {"ref_id": "b14", "title": "Semantic label sharing for learning with many categories", "journal": "Springer", "year": "2010", "authors": "R Fergus; H Bernal; Y Weiss; A Torralba"}, {"ref_id": "b15", "title": "Devise: A deep visual-semantic embedding model", "journal": "", "year": "2013", "authors": "A Frome; G S Corrado; J Shlens; S Bengio; J Dean; T Mikolov"}, {"ref_id": "b16", "title": "Sharing features between objects and their attributes", "journal": "IEEE", "year": "2011", "authors": "S J Hwang; F Sha; K Grauman"}, {"ref_id": "b17", "title": "Visual concept learning: Combining machine vision and bayesian generalization on concept hierarchies", "journal": "", "year": "2013", "authors": "Y Jia; J T Abbott; J Austerweil; T Griffiths; T Darrell"}, {"ref_id": "b18", "title": "Learning with multiple labels", "journal": "", "year": "2002", "authors": "R Jin; Z Ghahramani"}, {"ref_id": "b19", "title": "Correlated label propagation with application to multi-label learning", "journal": "IEEE", "year": "2006", "authors": "F Kang; R Jin; R Sukthankar"}, {"ref_id": "b20", "title": "Imagenet classification with deep convolutional neural networks", "journal": "In: NIPS", "year": "2012", "authors": "A Krizhevsky; I Sutskever; G E Hinton"}, {"ref_id": "b21", "title": "Segmentation propagation in imagenet", "journal": "Springer", "year": "2012", "authors": "D Kuettel; M Guillaumin; V Ferrari"}, {"ref_id": "b22", "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "journal": "", "year": "", "authors": "J D Lafferty; A Mccallum; F C N Pereira"}, {"ref_id": "b23", "title": "Maximum margin multi-label structured prediction", "journal": "NIPS", "year": "2011", "authors": "C H Lampert"}, {"ref_id": "b24", "title": "Learning to detect unseen object classes by between-class attribute transfer", "journal": "IEEE", "year": "2009", "authors": "C H Lampert; H Nickisch; S Harmeling"}, {"ref_id": "b25", "title": "Transfer learning by borrowing examples for multiclass object detection", "journal": "", "year": "2011", "authors": "J J Lim; R Salakhutdinov; A Torralba"}, {"ref_id": "b26", "title": "Semantic hierarchies for visual object recognition", "journal": "IEEE", "year": "2007", "authors": "M Marszalek; C Schmid"}, {"ref_id": "b27", "title": "Zero-shot learning with semantic output codes", "journal": "NIPS", "year": "2009", "authors": "M Palatucci; D Pomerleau; G E Hinton; T M Mitchell"}, {"ref_id": "b28", "title": "Towards good practice in largescale learning for image classification", "journal": "IEEE", "year": "2012", "authors": "F Perronnin; Z Akata; Z Harchaoui; C Schmid"}, {"ref_id": "b29", "title": "Evaluating knowledge transfer and zero-shot learning in a large-scale setting", "journal": "IEEE", "year": "2011", "authors": "M Rohrbach; M Stark; B Schiele"}, {"ref_id": "b30", "title": "What helps whereand why? semantic relatedness for knowledge transfer", "journal": "IEEE", "year": "2010", "authors": "M Rohrbach; M Stark; G Szarvas; I Gurevych; B Schiele"}, {"ref_id": "b31", "title": "Image classification with the fisher vector: Theory and practice", "journal": "International journal of computer vision", "year": "2013", "authors": "J S\u00e1nchez; F Perronnin; T Mensink; J Verbeek"}, {"ref_id": "b32", "title": "Augmented attribute representations", "journal": "Springer", "year": "2012", "authors": "V Sharmanska; N Quadrianto; C H Lampert"}, {"ref_id": "b33", "title": "Semantic hierarchies for image annotation: A survey", "journal": "Pattern Recognition", "year": "2012", "authors": "A M Tousch; S Herbin; J Y Audibert"}, {"ref_id": "b34", "title": "Using the nystr\u00f6m method to speed up kernel machines", "journal": "Advances in Neural Information Processing Systems", "year": "2001", "authors": "C Williams; M Seeger"}, {"ref_id": "b35", "title": "Designing category-level attributes for discriminative visual recognition", "journal": "IEEE", "year": "2013", "authors": "F X Yu; L Cao; R S Feris; J R Smith; S F Chang"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig. 1 .1Fig.1. Our model replaces traditional classifiers such as softmax or independent logistic regressions. It takes as input image features (e.g. from an underlying deep neural network) and outputs probabilities consistent with pre-specified label relations.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Fig. 2 .2Fig.2. HEX graphs capture relations between labels applied to the same object.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Fig. 3 .3Fig. 3. Equivalent HEX graphs.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Algorithm 11Listing state space 1: function ListStateSpace(graph G) 2:", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "is consistent and maximally dense, Alg. 1 runs in O ((|V | + |E h | + |E e |)|S G |) time and returns the state space S G .", "figure_data": ""}, {"figure_label": "24", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Algorithm 2 4 :24Exact InferenceInput: Graph G = (V, E h , Ee). Input: Scores f \u2208 R |V | . Output: Marginals, e.g. Pr(vi = 1). 1: G * \u2190 Sparsify(G) 2:\u1e20 \u2190 Densify(G) 3: T \u2190BuildJunctionTree(G * ). For each clique c \u2208 T , Sc \u2190ListStateSpace(\u1e20[c]). 5: Perform (two passes) message passing on T using only states Sc for each clique c.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Theorem 4 .4The complexity of the exact inference (Line 5 in Alg. 2) for graph", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Fig. 4 .4Fig. 4. Instead of training with data all labeled at leaf nodes, we train with examples leveled at different levels but still evaluate classification at leaf nodes during test.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "37. Zeiler, M.D.,Fergus, R.: Visualizing and understanding convolutional neural networks. arXiv preprint arXiv:1311.2901 (2013) 38. Zweig, A., Weinshall, D.: Exploiting object hierarchy: Combining models from different category levels. In: Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on. pp. 1-8. IEEE (2007)", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "(47.3) 52.9(77.2) 9.3(27.2) 55.3(79.4) 95% 16.0(32.2) 50.8(76.0) 5.6(17.2) 52.4(77.2) 99% 2.5 (7.2) 41.5(68.1) 1.0(3.8) 41.5(68.5) Top 1 (top 5 in brackets) classification accuracy on 1000 classes of ILSVRC2012 with relabeling of leaf node data to internal nodes during training.", "figure_data": "relabeling softmax-leaf softmax-all logisticours50%50.5(74.7) 56.4(79.6) 21.0(45.2) 58.2(80.8)90%26.2"}], "formulas": [{"formula_id": "formula_0", "formula_text": "v i \u2208 V and\u03c3(v i ) = \u03c3(v i ) \u222a {v i }.", "formula_coordinates": [4.0, 242.3, 459.74, 137.54, 9.65]}, {"formula_id": "formula_1", "formula_text": "Theorem 1. A HEX graph G = (V, E h , E e ) is consistent if and only if for any label v i \u2208 V , E e \u2229 (\u1fb1(v i ) \u00d7\u1fb1(v i )) = \u2205.", "formula_coordinates": [5.0, 34.02, 167.79, 345.82, 21.64]}, {"formula_id": "formula_2", "formula_text": "P (y|x) = i e fi(x;w)[yi=1] (vi,vj )\u2208E h [(y i , y j ) = (0, 1)] (vi,vj )\u2208Ee [(y i , y j ) = (1, 1)],(1)", "formula_coordinates": [5.0, 39.65, 356.95, 340.19, 34.61]}, {"formula_id": "formula_3", "formula_text": "L(D, w) = \u2212 l log Pr(y (l) g (l) |x (l) ; w) = \u2212 l log y:y g (l) =y (l) g (l) Pr(y|x (l) ; w) (2)", "formula_coordinates": [6.0, 46.9, 456.33, 332.94, 29.9]}, {"formula_id": "formula_4", "formula_text": "Definition 4. HEX graphs G and G are equivalent if S G = S G .", "formula_coordinates": [7.0, 34.02, 345.9, 289.81, 9.68]}, {"formula_id": "formula_5", "formula_text": "Definition 5. Given a graph G = (V, E h , E e ), a directed edge e \u2208 V \u00d7 V (not necessarily in E h ) is redundant if G = (V, E h \\ {e}, E e ) and G = (V, E h \u222a {e}, E e ) are both equivalent to G. An undirected edge e \u2208 V \u00d7 V (not necessarily in E e ) is redundant if G = (V, E h , E e \\ {e}) and G = (V, E h , E e \u222a {e}) are both equivalent to G.", "formula_coordinates": [7.0, 34.02, 482.21, 345.83, 56.59]}, {"formula_id": "formula_6", "formula_text": "G = (V, E h , E e ) is \u2126 G = max v\u2208V |o\u1e20(v)|, where o\u1e20(v) = {u \u2208 V : (u, v) \u2208\u0112 h \u2227(v, u) \u2208\u0112 h \u2227(u, v) \u2208 E e } and\u1e20 = (V,\u0112 h ,\u0112 e ) is the maximally dense equivalent of G.", "formula_coordinates": [8.0, 34.02, 520.8, 671.08, 33.56]}, {"formula_id": "formula_7", "formula_text": "V 0 \u2190 \u03b1(vi) \u222a (vi) \u222a o(vi). 7: G 0 \u2190 G[V 0 ] 8: S G 0 \u2190 ListStateSpace(G 0 ) 9: S 0 G \u2190 {y \u2208 {0, 1} n : yi = 0 y \u03c3(v i ) = 0 y V 0 \u2208 S G 0 }. 10: V 1 \u2190 \u03c3(vi) \u222a o(vi). 11: G 1 \u2190 G[V 1 ] 12: S G 1 \u2190 ListStateSpace(G 1 ) 13: S 1 G = {y \u2208 {0, 1} n : yi = 1 y \u03b1(v i ) = 1 (vi) = 0 y V 1 \u2208 S G 1 } 14: return S 0 G \u222a S 1 G . 15: end function", "formula_coordinates": [9.0, 37.91, 52.12, 341.94, 110.14]}, {"formula_id": "formula_8", "formula_text": "Theorem 3. For a consistent graph G = (V, E h , E e ), |S G | \u2264 (|V |\u2212\u2126 G +1)2 \u2126 G .", "formula_coordinates": [9.0, 34.02, 264.4, 345.83, 11.22]}, {"formula_id": "formula_9", "formula_text": "Lemma 2. If graph G = (V, E h , E e )", "formula_coordinates": [9.0, 34.02, 496.22, 160.71, 9.68]}, {"formula_id": "formula_10", "formula_text": "G = (V, E h , E e ) is O min{|V |2 w , |V | 2 2 \u2126 G } ,", "formula_coordinates": [10.0, 34.02, 304.0, 196.94, 11.23]}], "doi": ""}