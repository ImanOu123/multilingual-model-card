{"title": "Do PLMs Know and Understand Ontological Knowledge?", "authors": "Weiqi Wu; Chengyue Jiang; Yong Jiang; Pengjun Xie; Kewei Tu", "pub_date": "", "abstract": "Ontological knowledge, which comprises classes and properties and their relationships, is integral to world knowledge. It is significant to explore whether Pretrained Language Models (PLMs) know and understand such knowledge. However, existing PLM-probing studies focus mainly on factual knowledge, lacking a systematic probing of ontological knowledge. In this paper, we focus on probing whether PLMs store ontological knowledge and have a semantic understanding of the knowledge rather than rote memorization of the surface form. To probe whether PLMs know ontological knowledge, we investigate how well PLMs memorize: (1) types of entities; (2) hierarchical relationships among classes and properties, e.g., Person is a subclass of Animal and Member of Sports Team is a subproperty of Member of ; (3) domain and range constraints of properties, e.g., the subject of Member of Sports Team should be a Person and the object should be a Sports Team. To further probe whether PLMs truly understand ontological knowledge beyond memorization, we comprehensively study whether they can reliably perform logical reasoning with given knowledge according to ontological entailment rules. Our probing results show that PLMs can memorize certain ontological knowledge and utilize implicit knowledge in reasoning. However, both the memorizing and reasoning performances are less than perfect, indicating incomplete knowledge and understanding.", "sections": [{"heading": "Introduction", "text": "Pretrained Language Models (PLMs) have orchestrated impressive progress in NLP across a wide variety of downstream tasks, including knowledge-intensive tasks. Previous works propose that PLMs are capable of encoding a significant amount of knowledge from the pretraining corpora (AlKhamissi et al., 2022), and determine to explore the kinds of knowledge within PLMs. Existing probing works mainly focus on factual knowledge associated with instances (Petroni et al., 2019;Jiang et al., 2020;Safavi and Koutra, 2021). Meanwhile, although classes (concepts) have raised some research interest (Bhatia and Richie, 2020;Peng et al., 2022;Lin and Ng, 2022), there is no systematic study of ontological knowledge.\nOntological knowledge models the world with a set of classes and properties and the relationships that hold between them (Nilsson, 2006;Kumar et al., 2019). It plays a vital role in many NLP tasks such as question answering by being injected into (Goodwin and Demner-Fushman, 2020) or embedded outside deep neural networks (Wang et al., 2017). Therefore, it is essential to explore whether PLMs can encode ontological knowledge and have a semantic understanding of the knowledge rather than rote memorizing its surface form.\nIn this paper, we first probe PLM's memorization of ontological knowledge. Specifically, as shown in Figure 1(a), we construct memorization tests about (1) Types of entities. Entities can be categorized into classes, as Lionel Messi is a Person and Argentina National Football Team is a Sports Team.\n(2) Hierarchical relationships between classes, e.g., Person is a subclass of Animal. (3) Hierarchical relationships between properties, e.g., Member of Sports Team is a subproperty of Member of. (4) Domain constraints of properties. It specifies information about the subjects to which a property applies. For example, the subject of Member of Sports Team should be an instance of Person. (5) Range constraints of properties. Similar to domain, range specifies information about the object of a property, such as the object of Member of Sports Team should be an instance of Sports Team. Experiments prove that PLMs store a certain amount of ontological knowledge.\nTo further examine whether PLMs understand ontological knowledge, we investigate if PLMs can correctly perform logical reasoning that requires ontological knowledge. Illustrated in Figure 1(b), given the fact triple (Lionel Messi, Member of Sports Team, Argentina National Football Team) along with property constraints, we can perform type inferences to conclude that Lionel Messi is a Person, and Argentina National Football Team is a Sports Team. We comprehensively investigate the reasoning capability of PLMs over ontological knowledge following six entailment rules. Experiments show that PLMs can apply implicit ontological knowledge to draw conclusions through reasoning, but the accuracy of their reasoning falls short of perfection. This observation suggests that PLMs possess a limited understanding of ontological knowledge.\nIn summary, we systematically probe whether PLMs know and understand ontological knowledge. Our main contributions can be summarized as follows: (1) We construct a dataset that evaluates the ability of PLMs to memorize ontological knowledge and their capacity to draw inferences based on ontological entailment rules. (2) We comprehensively probe the reasoning ability of PLMs by carefully classifying how ontological knowledge is given as a premise. (3) We find that PLMs can memorize certain ontological knowledge but have a limited understanding. We anticipate that our work will facilitate more in-depth research on ontological knowledge probing with PLMs. The code and dataset are released at https://github.com/ vickywu1022/OntoProbe-PLMs.", "publication_ref": ["b31", "b17", "b33", "b3", "b29", "b23", "b28", "b21", "b11", "b40"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Benchmark Construction", "text": "In this section, we present our methodology for ontology construction and the process of generating memorizing and reasoning tasks based on the ontology for our probing analysis.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ontology Building", "text": "Class We use DBpedia (Auer et al., 2007) to obtain classes and their instances. Specifically, we first retrieve all 783 classes in DBpedia, then use SPARQL (hommeaux, 2011) to query their instances using the type relation and superclasses using the subclass-of relation. We sample 20 instances for each class.\nProperty Properties are collected based on DBpedia and Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) using the following pipeline: (1) Obtain properties from Wikidata and use subproperty of (P1647) in Wikidata to find their superproperties. (2) Query the domain and range constraints of the properties using property constraint (P2302) in Wikidata. (3) Align the Wikidata properties with DBpedia properties by equivalent property (P1628). (4) Query the domain and range constraints of the properties in DBpedia. (5) Cleanse the collected constraints using the above-collected class set as vocabulary. We choose 50 properties with sensible domain, range and superproperties.", "publication_ref": ["b1", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "Construction of Memorizing Task", "text": "The memorizing task consists of five subtasks, each probing the memorization of an ontological relationship: (1) TP: types of a given instance, (2) SCO: superclasses of a given class, (3) SPO: superproperties of a given property, (4) DM: domain constraint on a given property, and (5) RG: range constraint on a given property. Every subtask is formulated as a cloze-completion problem, as shown in Figure 1(b). Multiple correct answers exist for TP, SCO, and SPO, which form a chain of classes or properties. There is only one correct answer for DM and RG, as it is not sound to declare an expanded restriction on a property. For instance,  Animal is too broad as the domain constraint of the property Member of Sports Team (P54), hence applying Person as the domain.\nWe construct the dataset for each subtask using the ontology built in Sec. 2.1 and reserve 10 samples for training and 10 for validation to facilitate few-shot knowledge probing. The statistics of the dataset for each subtask are shown in Table 1.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": ["tab_1"]}, {"heading": "Construction of Reasoning Task", "text": "We construct the reasoning task based on the entailment rules specified in the Resource Description Framework Schema (RDFS) 1 . We propose six subtasks, each probing the reasoning ability following a rule listed in Table 2. For rule rdfs2/3/7, we design a pattern for each property to be used between a pair of instances, e.g., \"[X] is a player at [Y] .\" for Member of Sports Team, where [X] and [Y] are the subject and object, respectively.\nEach entailment rule describes a reasoning process: P 1 \u2227 P 2 |= H, where P 1 , P 2 are the premises and H is the hypothesis. Similar to the memorizing task, we formulate the reasoning task as cloze-completion by masking the hypothesis (see Figure 1(b)). Premises are also essential to the reasoning process and can be:\n\u2022 Explicitly Given: The premise is explicitly included in the input of the model, and inferences are made with natural language statements.\n\u2022 Implicitly Given: The premise is not explicitly given but memorized by the model as implicit knowledge. The model needs to utilize implicit knowledge to perform inferences, which relieves the effect of context and requires understanding the knowledge.\n\u2022 Not Given: The premise is neither explicitly given nor memorized by the model. It serves as a baseline where the model makes no inference.\nHence, there exist 3 \u00d7 3 different setups for two premises. It is a refinement of the experimental setup used by Talmor et al. (2020), which only distinguishes whether a premise is explicitly included in the input. We determine the memorization of a premise by the probing results of the memorizing task, which will be elaborated in Sec. 3.2.3.", "publication_ref": ["b37"], "figure_ref": ["fig_0"], "table_ref": ["tab_3"]}, {"heading": "Probing Methods", "text": "We investigate encoder-based PLMs (BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019)) that can be utilized as input encoders for various NLP tasks. Prompt is an intuitive method of our probing task as it matches the mask-filling nature   of BERT. We use OpenPrompt (Ding et al., 2022), an open-source framework for prompt learning that includes the mainstream prompt methods, to facilitate the experiments.", "publication_ref": ["b6", "b26", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Probing Methods for Memorization", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Prompt Templates", "text": "Manual Templates Manual prompts with human-designed templates written in discrete language phrases are widely used in zero-shot probing (Schick and Sch\u00fctze, 2021) as PLMs can perform tasks without any training. Manual templates are designed for all the ontological relationships in our task, as shown in Table 3.\nSoft Templates One of the disadvantages of manual prompts is that the performance can be significantly affected by perturbation to the prompt templates (Jiang et al., 2020). A common alternative is to use soft prompts that consist of learnable soft tokens (Liu et al., 2021;Li and Liang, 2021) instead of manually defined templates. The soft prompts we use for ontological relationships are also shown in Table 3. To probe using soft prompts, we tune randomly initialized soft tokens on the training set with the PLMs parameters being frozen. Detailed training setups are listed in Appendix A.", "publication_ref": ["b34", "b17", "b22"], "figure_ref": [], "table_ref": ["tab_4", "tab_4"]}, {"heading": "Candidates Scoring", "text": "Given a candidate c which can be tokenized into n tokens c 1 , c 2 , . . . , c n , such that c i \u2208 V, i = {1, . . . , n}, n \u2265 1, where V is the vocabulary of the model, it is scored based on the log probability of predicting it in the masked prompt. We can either use n different [MASK] tokens or the same [MASK] token to obtain the log probability of each composing token c i , and then compute the  (Klein and Nabi, 2020), (2) max: the maximum log probability of all composing tokens, (3) first: the log probability of the first composing token. Formally, the score s of candidate c is computed as:\ns i = log (p([M ASK] i = c i )) s = Pooling(\u015d 1 ,\u015d 2 , . . . ,\u015d n )\nSingle Mask We use one single [MASK] token to obtain an independent prediction of each token. The log probability of each composing token c i equals the log probability of recovering c i in the same [MASK], and the candidate is scored with the proposed pooling methods.\ns i = log (p([M ASK] = c i ))", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "Metrics", "text": "We rank the candidates by their log probability scores and use the top K Recall (R@K) and Mean Reciprocal Rank (MRR) as our evaluation metrics. Since MRR only evaluates the ability to retrieve the first ground truth, we additionally take the average rank of all gold labels as the final rank when computing mean reciprocal rank to evaluate models' ability to retrieve all the ground truths and denote it as MRR a . Formally, MRR a is defined as:\nMRR a = 1 n n i=1 1/( 1 |G i | g\u2208G i rank(g))\nwhere n is the number of samples in the dataset and G i is the gold label set of the ith sample.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Probing Methods for Reasoning", "text": "We explain how we concatenate the premises and hypothesis in the textual input, exclude the models' memory of hypotheses and split a set of premises based on how well the knowledge they represent is memorized by the model. We follow the candidate scoring methods proposed in Sec. 3.1.2 and evaluation metrics in Sec. 3.1.3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Prompt Templates", "text": "Apart from the prompt templates for our concerned ontological relationships introduced in Sec. 3.1.1, we further add conjunction tokens between the premises and hypothesis, which can be either manually designed or automatically tuned.\nManual Conj. As in Figure 1(b), we use a conjunctive adverb therefore between the premises and hypothesis. It is kept when there is no premise explicitly given in the input to exclude the effect of the template on probing results under different premise settings.\nSoft Conj. We can also use soft conjunctions by adding a soft token between premises explicitly given in the input and a soft token between the premises and the hypothesis. Therefore, the input would be \"P 1 <s4> P 2 <s5> H\". The soft templates used in P 1 , P 2 and H are loaded from the learned soft prompts in memorizing tasks and finetuned together with soft conjunctions.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Reasoning with Pseudowords", "text": "When testing the reasoning ability of PLMs, we replace the specific instances, classes, and properties in the hypothesis prompt with pseudowords to prevent probing the memorization of hypotheses. Pseudowords (Sch\u00fctze, 1998;Zhang and Pei, 2022; are artificially constructed words without any specific lexical meaning. For example, the reasoning prompt for the transitivity of subclass (i.e., rule rdfs9) is \"[X] is a person. Person is an animal. Therefore, [X] is a particular [MASK] .\", where [X] is a pseudoword. Inspired by (Karidi et al., 2021), we obtain pseudowords for PLMs by creating embeddings without special semantics. Specifically, we sample embeddings at a given distance from the [MASK] token, as the [MASK] token can be used to predict all the words in the vocabulary and appear anywhere in the sentence. The sampling distance d is set to be smaller than the minimum L2 distance between [MASK] and any other tokens in the static embedding space. Formally:\nd = \u03b1 \u2022 min t\u2208V \u2225z t \u2212 z [M ASK] \u2225 2\nwhere z t is the static embedding of token t and \u03b1 \u2208 (0, 1) is a coefficient. Moreover, we require that the distance between two pseudowords is at least the sampling distance d to ensure they can be distinguished from each other.", "publication_ref": ["b35", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Classifying Premises: Memorized or not", "text": "To determine whether a premise is memorized by the model when it is not explicitly given in the input, we employ a classifying method based on the rank of the correct answer in the memorizing task to sort and divide the premise set. The first half of the premise set is regarded as memorized, and the second half is not.\nEach rule consists of two premises and we classify them separately. For P 1 , which involves knowledge of subclass, subproperty, domain or range tested in the memorizing task, we can leverage previously calculated reciprocal rank during the evaluation. Premises are then sorted in descending order by the reciprocal rank. We conduct the same tests on P 2 , which involves knowledge of pseudowords, to examine model predispositions towards specific predictions and classify whether P 2 is memorized or not. Finally, we form our test set by combining premises according to the entailment rule and how each premise is given.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results and Findings", "text": "In this section, we introduce the performance of PLMs 2 on the test sets of memorizing and reasoning tasks, and analyze the results to posit a series of findings. We then analyze the effectiveness of different prompts. Detailed experimental results can be found in Appendix C.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Memorizing Task", "text": "The baseline model used for the memorizing task is a frequency-based model which predicts a list   of gold labels in the training set based on the frequency at which they appear, followed by a random list of candidates that are not gold labels in the training set. It combines prior knowledge and random guesses and is stronger than a random baseline.\nThe experimental results of the memorizing task are summarized in Table 4, from which we can observe that: (1) The best performance of PLMs is better than the baseline on every task except for DM. On DM, the baseline achieves higher MRR. If taking all three metrics into account, the best performance of PLMs still surpasses the performance of the baseline. (2) Except for DM, BERT models achieve much better performance than the baseline in all subtasks and all metrics. Taking an average of the increase in each metric, they outperform the baseline by 43-198%. Only BERTbase-uncased and BERT-large-cased outperform the baseline in DM by a small margin of 1% and 7%. (3) RoBERTa models generally fall behind BERT, showing a 38-134% improvement compared with the baseline except for DM. (4) Despite a significant improvement from the baseline, the results are still not perfect in all subtasks.\nPLMs can memorize certain ontological knowledge but not perfectly. Based on the above observation, we can conclude that PLMs have a certain memory of the concerned ontological rela-tionships and the knowledge can be accessed via prompt, allowing them to outperform a strong baseline. It proves that during pretraining, language models learn not only facts about entities but also their ontological relationships, which is essential for a better organization of world knowledge. However, the memorization is not perfect, urging further efforts on ontology-aware pretraining.\nLarge models are not necessarily better at memorizing ontological knowledge. According to Petroni et al. (2019), models with larger sizes appear to store more knowledge and achieve better performance in both knowledge probing tasks and downstream NLP tasks. However, as shown in Table 4, BERT-large-uncased is worse than its smaller variant under most circumstances, and RoBERTalarge is worse than RoBERTa-base in TP and DM. It demonstrates that the scale of model parameters does not necessarily determine the storage of ontological knowledge.", "publication_ref": ["b31"], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Reasoning Task", "text": "We fix the usage of multiple masks and meanpooling in the reasoning experiments as they generally outperform other settings in the memorizing task (see Appendix B). We take an average of the MRR metrics using different templates and illustrate the results of BERT-base-cased and RoBERTa- base in Figure 2. With neither premise given, the rank of the ground truth is usually low. It shows that models have little idea of the hypothesis, which is reasonable because the information of pseudowords is probed. With premises implicitly or explicitly given, especially P 1 , the MRR metrics improve in varying degrees. Moreover, results show that BERT-base-cased has better reasoning ability with our concerned ontological entailment rules than RoBERTa-base.\nPLMs have a limited understanding of the semantics behind ontological knowledge. To reach a more general conclusion, we illustrate the overall reasoning performance in Figure 3 by averaging over all the entailment rules and PLMs, and find that: (1) When P 1 is explicitly given in the input text, models are able to significantly improve the rank of gold labels. As P 1 contains the ground truth in its context, it raises doubt about whether the improvement is obtained through logical reasoning or just priming (Misra et al., 2020).\n(2) Explicitly giving P 2 introduces additional tokens that may not be present in gold labels, making P 1 /P 2 = EX/EX worse than P 1 /P 2 = EX/IM. metrics are higher than when they are not given. It implies that, to some extent, PLMs can utilize the implicit ontological knowledge and select the correct entailment rule to make inferences. (4) However, none of the premises combinations can give near-perfect reasoning performance (MRR metrics close to 1), suggesting that PLMs only have a weak understanding of ontological knowledge.\nParaphrased properties are a challenge for language models. In Figure 2(d), the premise P 1 of rule rdfs7 contains a paraphrased version of the ground truth, which is the manually-designed pattern of a particular property. Compared with rule rdfs5 shown in Figure 2(c), where P 1 contains the surface form of the correct property, the MRR of BERT-base-cased of rdfs7 decreases by 23%, 49% and 29% when P 1 is explicitly given and P 2 is not, implicitly and explicitly given, respectively. Though the MRR of RoBERTa-base of rdfs7 increases when P 2 is not given, it decreases by 40% and 15% when P 2 is implicitly and explicitly given. This suggests that PLMs fail to understand the semantics of some properties, thus demonstrating a limited understanding of ontological knowledge.", "publication_ref": ["b27"], "figure_ref": ["fig_2", "fig_3", "fig_2", "fig_2"], "table_ref": []}, {"heading": "Effectiveness of Prompts", "text": "In this section, we discuss how prompt templates affect performance. In the memorizing task, Table 4 shows that using soft templates generally improves the performance of memorizing tasks, in particular TP, SCO and SPO. It suggests that it is non-trivial to extract knowledge from PLMs.\nMeanwhile, only a few models perform better with soft templates on DM and RG with a relatively marginal improvement. This could be explained by the fact that both the manual templates and semantics of domain and range constraints are more complex than those of other relationships. Therefore, it is difficult for models to capture with only three soft tokens. We also note that RoBERTa models appear to benefit more from soft templates than BERT models, probably due to their poor performance with manual templates.\nTrained soft templates for each relation barely help with reasoning, though. In Figure 4, we summarize the performance by averaging across different models and reasoning tasks and find that it is the trained conjunction token which improves the performance of reasoning rather than the soft templates that describe ontological relationships. It might be inspiring that natural language inference with PLMs can be improved by adding trainable tokens as conjunctions instead of simply concatenating all the premises.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Preliminary Evaluation of ChatGPT", "text": "After we finished the majority of our probing experiments, ChatGPT, a decoder-only model, was publicly released and demonstrated remarkable capabilities in commonsense knowledge and reasoning. Therefore, we additionally perform a preliminary probe of the ability of ChatGPT to memorize and   understand ontological knowledge. Since ChatGPT is a decoder-only model, we employ a distinct probing method from what is expounded in Sec. 3. Instead of filling masks, we directly ask ChatGPT to answer multiple-choice questions with 20 candidate choices and evaluate the accuracy.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Probing for Memorization Ability", "text": "For memorization probing, we use the finestgrained gold label as the correct answer and randomly sample 19 negative candidates to form the choice set. Take the TP task as an example, we query the GPT-3.5-turbo API with the prompt \"What is the type of Lionel Messi? (a) soccer player, (b) work, (c) ...\" followed by remaining candidates. We sample 500 test cases for the TP and SCO tasks and use the complete test sets for the other tasks.\nFor comparison, we also conduct the experiments using BERT-base-uncased, a generally competitive PLM in memorizing and understanding ontological knowledge, with manual prompts and the identical candidate subset. The results presented in  base-uncased significantly in most of the memorizing tasks associated with ontological knowledge.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Probing for Reasoning Ability", "text": "Since we cannot input embeddings in the GPT-3.5-turbo API, we use X and Y to represent pseudowords as they are single letters that do not convey meanings. However, ChatGPT cannot generate any valid prediction without sufficient context regarding these pseudowords. Therefore, P 2 needs to be explicitly provided to describe the characteristics or relations of the pseudowords. We then explore the ability of ChatGPT to select the correct answer from 20 candidates with different forms of P 1 . In this task, P 1 is regarded as memorized if the model can correctly choose the gold answer from the given 20 candidates in the memorizing task.\nBased on the results presented in Table 6, Chat-GPT demonstrates high accuracy when P 1 is either implicitly or explicitly given, suggesting its strong capacity to reason and understand ontological knowledge. Due to a substantial disparity in the knowledge memorized by ChatGPT compared to other models (as shown in section 5.1), their performance is not directly comparable when P 1 is not given or implicitly given. Therefore, we only compare ChatGPT and BERT-base-uncased when P 1 is explicitly given. Results show that ChatGPT significantly outperforms BERT-base-uncased in explicit reasoning (97.1% vs. 88.2%).", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_11"]}, {"heading": "Related Work", "text": "Knowledge Probing Language models are shown to encode a wide variety of knowledge after being pretrained on a large-scale corpus. Recent studies probe PLMs for linguistic knowledge (Vuli\u0107 et al., 2020;Hewitt and Manning, 2019), world knowledge (Petroni et al., 2019;Jiang et al., 2020;Safavi and Koutra, 2021), actionable knowledge (Huang et al., 2022), etc. via methods such as cloze prompts (Beloucif and Biemann, 2021;Petroni et al., 2020) and linear classifiers (Hewitt and Liang, 2019;Pimentel et al., 2020). Although having explored extensive knowledge within PLMs, previous knowledge probing works have not studied ontological knowledge systematically. We cut through this gap to investigate how well PLMs know about ontological knowledge and the meaning behind the surface form.\nKnowledge Reasoning Reasoning is the process of drawing new conclusions through the use of existing knowledge and rules. Progress has been reported in using PLMs to perform reasoning tasks, including arithmetic , commonsense (Talmor et al., 2019(Talmor et al., , 2020, logical (Creswell et al., 2022) and symbolic reasoning . These abilities can be unlocked by finetuning a classifier on downstream datasets (Talmor et al., 2020) or using proper prompting strategies (e.g., chain of thought (CoT) prompting  and generated knowledge prompting ). This suggests that despite their insensitivity to negation (Ettinger, 2020;Kassner and Sch\u00fctze, 2020) and over-sensitivity to lexicon cues like priming words (Helwe et al., 2021;Misra et al., 2020), PLMs have the potential to make inferences over implicit knowledge and explicit natural language statements. In this work, we investigate the ability of PLMs to perform logical reasoning with implicit ontological knowledge to examine whether they understand the semantics beyond memorization.", "publication_ref": ["b39", "b14", "b31", "b17", "b33", "b16", "b2", "b30", "b13", "b32", "b36", "b37", "b5", "b37", "b8", "b19", "b12", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this work, we systematically probe whether PLMs encode ontological knowledge and understand its semantics beyond the surface form. Experiments show that PLMs can memorize some ontological knowledge and make inferences based on implicit knowledge following ontological entailment rules, suggesting that PLMs possess a certain level of awareness and understanding of ontological knowledge. However, it is important to note that both the accuracy of memorizing and reasoning is less than perfect, and the difficulty encountered by PLMs when processing paraphrased knowledge is confirmed. These observations indicate that their knowledge and understanding of ontology are limited. Therefore, enhancing the knowledge and understanding of ontology would be a worthy future research goal for language models. Our exploration into ChatGPT shows an improved performance in both memorizing and reasoning tasks, signifying the potential for further advancements.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "The purpose of our work is to evaluate the ontological knowledge of PLMs. However, a sea of classes and properties exist in the real world and we only cover a selective part of them. Consequently, the scope of our dataset for the experimental analysis is limited. The findings from our experiments demonstrate an imperfect knowledge and understanding obtained by the models, indicating a tangible room for enhancement in both ontological knowledge memorization and understanding and a need for a better ability to address paraphrasing. These observations lead us to contemplate refining the existing pretraining methods to help language models achieve better performance in related tasks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "We propose our ethics statement of the work in this section: (1) Dataset. Our data is obtained from DBpedia and Wikidata, two publicly available linked open data projects related to Wikipedia. Wikidata is under the Creative Commons CC0 License, and DBpedia is licensed under the terms of the Creative Commons Attribution-ShareAlike 3.0 license and the GNU Free Documentation License. We believe the privacy policies of DBpedia 3 and Wikidata 4 are well carried out. We inspect whether our dataset, especially instances collected, contains any unethical content. No private information or offensive topics are found during human inspection.\n(2) Labor considerations. During dataset construction, the authors voluntarily undertake works requiring human efforts, including data collection, cleansing, revision and design of property patterns. All the participants are well informed about how the dataset will be processed, used and released.\n(3) Probing results. As PLMs are pretrained on large corpora, they may give biased results when being probed. We randomly check some probing results and find no unethical content in these samples. Therefore, we believe that our study does not introduce additional risks. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Experimental Setup", "text": "We train soft tokens for 100 epochs with AdamW optimizer. The learning rate is set to 0.5 and a linear warmup scheduler is used. Since both the memorizing and reasoning task can be formulated as a multi-label classification problem, we use BCE-WithLogitsLoss or NLLLoss as our loss function in the memorizing task to report the better results given by one of these two and select a better training objective. Therefore, we fix the loss function to BCEWithLogitsLoss in the reasoning task. For pseudowords, we set the coefficient \u03b1 to 0.5 and sample 10 pairs of pseudowords for each entailment rule as we at most need two pseudowords to substitute the subject and object instances respectively, and report the averaged performance as the final result.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Multi-token Prompting Methods", "text": "In the main body of the paper, we discuss the impact of different prompts on the performance of knowledge probing and reasoning. In this section, we continuously discuss the impact of other prompt settings by comparing the averaged performance.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1 Number of [MASK] Tokens", "text": "To support multi-token candidate scoring, we use multiple [MASK] tokens or one single [MASK] token to predict with masked language models. The comparison between the two methods is shown in Figure 5, by averaging the performance of all the memorizing tasks and models. We can observe that single [MASK] prediction achieves better accuracy (R@1) with a negligible tiny margin but worse performance in other metrics. Therefore, using multiple [MASK] tokens to obtain prediction by forward pass inference is more sensible and achieves better results. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.2 Pooling Methods", "text": "Three pooling methods are proposed when computing the probability of a candidate that can be tokenized into multiple subtokens. The mean-pooling method is usually used in multi-token probing. Furthermore, we introduce max-pooling and firstpooling, which retain the score of only one important token. They can exclude the influence of prepositions, e.g., by attending to mean or transportation when scoring the candidate mean of transportation, but at the cost of other useful information. We are interested in whether it is better to consider the whole word or focus on the important part.\nFigure 6 shows that mean-pooling, as a classical method, is much better than the other two pooling methods. Besides, first-pooling gives clearly better results than max-pooling, which is possibly caused by the unique information contained in the headword (usually the first token). Consider candidates volleyball player, squash player and golf player, the conditional log probability of token player might be higher, but the candidates are distinguished by their headwords. In summary, mean-pooling obtains the best results with the most comprehensive information.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.3 Loss Functions", "text": "As mentioned in Appendix A, we try two loss functions in the memorizing task.  when calculating the loss.\n(2) The Negative Log Likelihood Loss (NLLLoss) is a loss function for multi-class classification. However, we can convert the original multi-label problem to a multi-class one by sampling one ground truth at a time to generate multiple single-label multi-class classification cases. As can be seen from Figure 7, using BCE-WithLogitsLoss as the loss function achieves better results than using NLLLoss. Hence, in subsequent reasoning experiments, we stick to the classical loss for multi-label classification.  ", "publication_ref": [], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "C Experimental Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 Task Examples", "text": "In order to enhance the clarity of the experiments, we have compiled a list in  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.2 Memorizing Results", "text": "The complete results of the memorizing task are reported in Table 8, 9, 10, 11 and 12.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_19"]}, {"heading": "C.3 Reasoning Results", "text": "We report the MRR Metric of BERT-baseuncased, BERT-large-cased, BERT-large-uncased and RoBERTa-large in Figure 8. It is generally consistent with the two models reported in the main body of the paper and the macro-averaged performance across different PLMs, so consistent conclusions can be drawn.", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "BERT-BASE-CASED", "text": "BERT-BASE-UNCASED RoBERTa-BASE Template Masks Pooling Loss R@1 R@5 MRRa MRR R@1 R@5 MRRa MRR R@1 R@       Ethical section B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? 2 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Ethical section B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? 2 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. 2 C Did you run computational experiments? 4 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\nWe focus on investigating whether PLMs know and understand ontological knowledge using models from the huggingface. We do not pay extra attention to the computational budget or computing infrastructure.\nC2. and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? Ethical section D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? 2, Ethical section D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? 2 D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? As the authors undertake the annotation work, reported demographic and geographic characteristics maybe violate the anonymous submission policy.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgement", "text": "This work was supported by the National Natural Science Foundation of China (61976139) and by Alibaba Group through Alibaba Innovative Research Program.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "2022. A review on language models as knowledge bases", "journal": "", "year": "", "authors": "Millicent Badr Alkhamissi; Asli Li; Mona Celikyilmaz; Marjan Diab;  Ghazvininejad"}, {"ref_id": "b1", "title": "Dbpedia: A nucleus for a web of open data", "journal": "", "year": "2007", "authors": "S\u00f6ren Auer; Christian Bizer; Georgi Kobilarov; Jens Lehmann; Richard Cyganiak; Zachary Ives"}, {"ref_id": "b2", "title": "Probing pre-trained language models for semantic attributes and their values", "journal": "", "year": "2021", "authors": "Meriem Beloucif; Chris Biemann"}, {"ref_id": "b3", "title": "Transformer networks of human conceptual knowledge. Psychological review", "journal": "", "year": "2020", "authors": "Sudeep Bhatia; Russell Richie"}, {"ref_id": "b4", "title": "Resource description framework (rdf) model and syntax specification", "journal": "", "year": "2002", "authors": "Dan Brickley; V Ramanathan;  Guha"}, {"ref_id": "b5", "title": "Selection-inference: Exploiting large language models for interpretable logical reasoning", "journal": "ArXiv", "year": "2022", "authors": "Antonia Creswell; Murray Shanahan; Irina Higgins"}, {"ref_id": "b6", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b7", "title": "OpenPrompt: An open-source framework for promptlearning", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Ning Ding; Shengding Hu; Weilin Zhao; Yulin Chen; Zhiyuan Liu"}, {"ref_id": "b8", "title": "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "Allyson Ettinger"}, {"ref_id": "b9", "title": "Resource description framework (rdf)", "journal": "", "year": "2009", "authors": "Nicholas Gibbins; Nigel Shadbolt"}, {"ref_id": "b10", "title": "Probing linguistic systematicity", "journal": "", "year": "2020", "authors": "Emily Goodwin; Koustuv Sinha; Timothy J O'donnell"}, {"ref_id": "b11", "title": "Enhancing question answering by injecting ontological knowledge through regularization", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Travis Goodwin; Dina Demner-Fushman"}, {"ref_id": "b12", "title": "Reasoning with transformer-based models: Deep learning, but shallow reasoning", "journal": "", "year": "2021", "authors": "Chadi Helwe; Chlo\u00e9 Clavel; Fabian M Suchanek"}, {"ref_id": "b13", "title": "Designing and interpreting probes with control tasks", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "John Hewitt; Percy Liang"}, {"ref_id": "b14", "title": "A structural probe for finding syntax in word representations", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "John Hewitt; Christopher D Manning"}, {"ref_id": "b15", "title": "Sparql query language for rdf", "journal": "", "year": "2011", "authors": "E Prud"}, {"ref_id": "b16", "title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents", "journal": "PMLR", "year": "2022", "authors": "Wenlong Huang; Pieter Abbeel; Deepak Pathak; Igor Mordatch"}, {"ref_id": "b17", "title": "How can we know what language models know?", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020-06", "authors": "Zhengbao Jiang; Frank F Xu"}, {"ref_id": "b18", "title": "Putting words in BERT's mouth: Navigating contextualized vector spaces with pseudowords", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Taelin Karidi; Yichu Zhou; Nathan Schneider; Omri Abend; Vivek Srikumar"}, {"ref_id": "b19", "title": "Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly", "journal": "", "year": "2020", "authors": "Nora Kassner; Hinrich Sch\u00fctze"}, {"ref_id": "b20", "title": "Contrastive selfsupervised learning for commonsense reasoning", "journal": "", "year": "2020", "authors": "Tassilo Klein; Moin Nabi"}, {"ref_id": "b21", "title": "An online dictionary and thesaurus", "journal": "", "year": "2019", "authors": "Dikshit Kumar; Agam Kumar; Man Singh; Archana Patel; Sarika Jain"}, {"ref_id": "b22", "title": "Prefix-tuning: Optimizing continuous prompts for generation", "journal": "Long Papers", "year": "2021", "authors": "Lisa Xiang; Percy Li;  Liang"}, {"ref_id": "b23", "title": "Does BERT know that the IS-a relation is transitive?", "journal": "Short Papers", "year": "2022", "authors": "Ruixi Lin; Hwee Tou Ng"}, {"ref_id": "b24", "title": "Generated knowledge prompting for commonsense reasoning", "journal": "Long Papers", "year": "2022", "authors": "Jiacheng Liu; Alisa Liu; Ximing Lu; Sean Welleck; Peter West; Yejin Ronan Le Bras; Hannaneh Choi;  Hajishirzi"}, {"ref_id": "b25", "title": "", "journal": "", "year": "", "authors": "Xiao Liu; Yanan Zheng; Zhengxiao Du; Ming Ding; Yujie Qian; Zhilin Yang"}, {"ref_id": "b26", "title": "Roberta: A robustly optimized bert pretraining approach. ArXiv, abs", "journal": "", "year": "1907", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b27", "title": "Exploring BERT's sensitivity to lexical cues using tests from semantic priming", "journal": "", "year": "2020", "authors": "Kanishka Misra; Allyson Ettinger; Julia Rayz"}, {"ref_id": "b28", "title": "Ontological constitutions for classes and properties", "journal": "", "year": "2006", "authors": "J\u00f8rgen Fischer Nilsson"}, {"ref_id": "b29", "title": "Copen: Probing conceptual knowledge in pre-trained language models", "journal": "", "year": "2022", "authors": "Hao Peng; Xiaozhi Wang; Shengding Hu; Hailong Jin; Lei Hou; Juanzi Li; Zhiyuan Liu; Qun Liu"}, {"ref_id": "b30", "title": "How context affects language models", "journal": "", "year": "2020", "authors": "Fabio Petroni; Patrick Lewis; Aleksandra Piktus; Tim Rockt\u00e4schel; Yuxiang Wu; Alexander H Miller; Sebastian Riedel"}, {"ref_id": "b31", "title": "Language models as knowledge bases?", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Fabio Petroni; Tim Rockt\u00e4schel; Sebastian Riedel; Patrick Lewis; Anton Bakhtin; Yuxiang Wu; Alexander Miller"}, {"ref_id": "b32", "title": "Information-theoretic probing for linguistic structure", "journal": "", "year": "2020", "authors": "Tiago Pimentel; Josef Valvoda; Rowan Hall Maudslay; Ran Zmigrod; Adina Williams; Ryan Cotterell"}, {"ref_id": "b33", "title": "Relational World Knowledge Representation in Contextual Language Models: A Review", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Tara Safavi; Danai Koutra"}, {"ref_id": "b34", "title": "Exploiting cloze-questions for few-shot text classification and natural language inference", "journal": "", "year": "2021", "authors": "Timo Schick; Hinrich Sch\u00fctze"}, {"ref_id": "b35", "title": "Automatic word sense discrimination", "journal": "Computational Linguistics", "year": "1998", "authors": "Hinrich Sch\u00fctze"}, {"ref_id": "b36", "title": "CommonsenseQA: A question answering challenge targeting commonsense knowledge", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Alon Talmor; Jonathan Herzig; Nicholas Lourie; Jonathan Berant"}, {"ref_id": "b37", "title": "Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge", "journal": "Curran Associates, Inc", "year": "2020", "authors": "Alon Talmor; Oyvind Tafjord; Peter Clark; Yoav Goldberg; Jonathan Berant"}, {"ref_id": "b38", "title": "Wikidata: a free collaborative knowledgebase", "journal": "Commun. ACM", "year": "2014", "authors": "Denny Vrande\u010di\u0107; Markus Kr\u00f6tzsch"}, {"ref_id": "b39", "title": "Probing pretrained language models for lexical semantics", "journal": "", "year": "2020", "authors": "Ivan Vuli\u0107; Maria Edoardo; Robert Ponti; Goran Litschko; Anna Glava\u0161;  Korhonen"}, {"ref_id": "b40", "title": "Knowledge graph embedding: A survey of approaches and applications", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2017", "authors": "Quan Wang; Zhendong Mao; Bin Wang; Li Guo"}, {"ref_id": "b41", "title": "Self-consistency improves chain of thought reasoning in language models", "journal": "ArXiv", "year": "2022", "authors": "Xuezhi Wang; Jason Wei; Dale Schuurmans; Quoc Le; Ed Chi; Denny Zhou"}, {"ref_id": "b42", "title": "Chain of thought prompting elicits reasoning in large language models", "journal": "ArXiv", "year": "2022", "authors": "Jason Wei; Xuezhi Wang; Dale Schuurmans; Maarten Bosma; Ed Chi; Quoc Le; Denny Zhou"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: (a) An example of an ontological knowledge graph. (b) Potential manual and soft prompts to probe the knowledge and corresponding semantics. Instances are replaced by pseudowords in reasoning experiments to mitigate potential interference from model memory.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "softT manT softT manT softT manT softT manT softT", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure2: The MRR by BERT-base-cased and RoBERTa-base using different combinations of premises. EX stands for explicitly given, IM stands for implicitly given and NO stands for not given. Other metrics show similar trends.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: The macro-averaged MRR across different entailment rules and language models with different combinations of premises.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "(1) The Binary Cross Entropy With Logits Loss (BCEWithLogitsLoss) is a common loss function for multi-label classification which numerically stably combines a Sigmoid layer and the Binary Cross Entropy Loss into one layer. All examples are given the same weight", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 7 :7Figure 7: Comparison between two different training objectives in the memorizing task.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 8 :8Figure8: The MRR by BERT-base-uncased, BERT-large-(un)cased and RoBERTa-large using different combinations of premises. EX stands for explicitly given, IM stands for implicitly given and NO stands for not given. The other metrics show similar trends.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Ontological relationship, type of candidate, and dataset size for each memorizing subtask.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": "Ontological Rel.Manual TemplateSoft TemplatetypeLionel Messi is a [MASK] . Lionel Messi has class [MASK] . Lionel Messi is a particular [MASK].Lionel Messi <s1> <s2> <s3> [MASK] .subclass ofPerson is a [MASK] . Person has superclass [MASK] . Person is a particular [MASK].Person <s1> <s2> <s3> [MASK] .subproperty ofMember of sports team implies [MASK] . Member of sports team <s1> <s2> <s3> [MASK] .domainOne has to be a particular [MASK] to be a player at a sports team .Member of sports team <s1> <s2> <s3> [MASK] .rangeOne has to be a particular [MASK] to have a player at that .Member of sports team <s1> <s2> <s3> [MASK] ."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Manual and soft templates used in prompt-based probing. In soft templates, <s1> <s2> and <s3> correspond to soft tokens.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "indicate that ChatGPT outperforms BERT-", "figure_data": "P 1 AVGRDFS Rule rdfs2 rdfs3 rdfs5 rdfs7 rdfs9 rdfs11NO 13.525.0 16.70.00.019.020.8IM 82.876.9 86.4 71.5 77.7 91.992.4EX 97.1 100.0 96.4 94.9 96.9 97.497.0"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Accuracy (%) achieved by ChatGPT on each reasoning subtask with P 2 explicitly given.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "", "figure_data": "that includes task"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "Example manual prompt and predictions by BERT-base-cased for each memorizing task. Correct predictions and golds predicted among the top-5 are marked with a and highlighted in green.", "figure_data": "prompts as well as the top five predicted candidatewords generated by BERT-base-cased. The tableconsists of examples with successful predictionsfor all correct answers (SPO, RG), examples withpartial correct answers predicted (TP, SCO), andexamples where the correct answer is not predictedwithin the top five candidates (DM)."}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "TP results.", "figure_data": "3094"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "", "figure_data": ": SCO results.3095"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_21", "figure_caption": "SPO results.    ", "figure_data": "3096"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_22", "figure_caption": "DM results. 64.29 62.72 62.72 57.14 71.43 63.93 63.93 39.29 53.57 46.41 46.41 soft NLL 46.43 67.86 56.19 56.19 53.57 71.43 62.39 62.39 32.14 53.57 42.03 42.03 manual 35.71 60.71 46.25 46.25 soft NLL 53.57 67.86 61.64 61.64 35.71 67.86 49.25 49.25 28.57 67.86 44.01 44.01 manual 78.57 66.82 66.82 46.43 78.57 61.06 61.06 7.14 17.86 14.74 14.74 soft log 46.43 60.71 45.3 45.3 32.14 60.71 38.88 38.88 42.86 53.57 44.55 44.55 soft NLL 39.29 71.43 51.73 51.73 53.57 67.86 44.62 44.46.43 64.29 55.81 55.81 32.14 60.71 43.23 43.23 soft NLL 42.86 67.86 56.91 56.91 35.71 53.57 45.75 45.75 35.71 71.43 48.53 48.53 manual", "figure_data": "3097"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_23", "figure_caption": "RG results.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_24", "figure_caption": "Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? Appendix A C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? 4 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? 4 D Did you use human annotators (e.g., crowdworkers) or research with human participants? 2 D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? Ethical section D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "s i = log (p([M ASK] i = c i )) s = Pooling(\u015d 1 ,\u015d 2 , . . . ,\u015d n )", "formula_coordinates": [4.0, 349.87, 487.15, 130.82, 28.35]}, {"formula_id": "formula_1", "formula_text": "s i = log (p([M ASK] = c i ))", "formula_coordinates": [4.0, 351.56, 623.8, 127.44, 11.81]}, {"formula_id": "formula_2", "formula_text": "MRR a = 1 n n i=1 1/( 1 |G i | g\u2208G i rank(g))", "formula_coordinates": [5.0, 95.62, 91.73, 168.76, 37.57]}, {"formula_id": "formula_3", "formula_text": "d = \u03b1 \u2022 min t\u2208V \u2225z t \u2212 z [M ASK] \u2225 2", "formula_coordinates": [5.0, 348.96, 192.07, 132.14, 18.93]}], "doi": "10.1007/978-3-540-76298-0_52"}