{"title": "ZELDA: A Comprehensive Benchmark for Supervised Entity Disambiguation", "authors": "Marcel Milich; Alan Akbik", "pub_date": "", "abstract": "Entity disambiguation (ED) is the task of disambiguating named entity mentions in text to unique entries in a knowledge base. Due to its industrial relevance, as well as current progress in leveraging pre-trained language models, a multitude of ED approaches have been proposed in recent years. However, we observe a severe lack of uniformity across experimental setups in current ED work, rendering a direct comparison of approaches based solely on reported numbers impossible: Current approaches widely differ in the data set used to train, the size of the covered entity vocabulary, and the usage of additional signals such as candidate lists. To address this issue, we present ZELDA, a novel entity disambiguation benchmark that includes a unified training data set, entity vocabulary, candidate lists, as well as challenging evaluation splits covering 8 different domains. We illustrate its design and construction, and present experiments in which we train and compare current state-of-the-art approaches on our benchmark. To encourage greater direct comparability in the entity disambiguation domain, we open source our benchmark at https: //github.com/flairNLP/zelda.", "sections": [{"heading": "Introduction", "text": "Entity disambiguation (ED) is the task of disambiguating textual mentions of entities to a corresponding unique entry in a knowledge base. For instance, the entity mention \"NBA\" might refer to one of several organizations with this abbreviation, such as \"National Basketball Association\" or \"National Boxing Association\". ED resolves these ambiguities and creates links between a knowledge base of unique entities and the various ways an entity may be referred to in text. It is the core component in the larger task of entity linking (EL), which includes the identification of entity mentions in text, often handled by a named entity recognition (NER) system.\nRecent progress in the field is driven by advances in large language models (Shen et al., 2021;Sevgili et al., 2022), pushing the scores on standard evaluation datasets to new heights. These models are typically trained in a supervised manner. Unlike many other NLP tasks with relatively few target classes, such as sentiment analysis or part-of-speech tagging, ED may have millions of target classes, since each entity in a knowledge base is modeled as a distinct class. Accordingly, most current state-ofthe-art ED approaches are trained over very large amounts of annotated text data that often is automatically derived from Wikipedia. Lack of uniformity in experimental setup. However, while a number of standard evaluation datasets exist to measure final ED accuracy, such as the AIDA-B test split of the popular AIDA dataset for newswire data (Hoffart et al., 2011), we observe that no such standardization exists for the data used to train ED systems. To illustrate this disparity, refer to Table 1 for an overview of current stateof-the-art approaches, published numbers and their respective training setups.\nAs Table 1 shows, approaches use different amounts of training data (ranging from 2 to 20 million \"snippets\" of annotated text), sourced from different Wikipedia versions using different sampling methodologies, and in some cases augmented with weak labels. Importantly, there is a stark difference in the size of the entity vocabulary for which approaches are trained, ranging from models that disambiguate a few thousand entities to models that handle over 6 million. Approaches also typically leverage so-called \"candidate lists\" that contain all possible disambiguation targets for textual mentions and so greatly narrow the search space. Prior work (see Section 2) has shown that each of these factors greatly influences the accuracy of an otherwise identical ED system (Broscheit, 2019;Wu et al., 2020;F\u00e9vry et al., 2020;Orr et al., 2021;De Cao et al., 2021 Table 1: Differences in the signal used to train current state-of-the-art ED approaches and their reported accuracy on the AIDA-B evaluation dataset. Differences include: the number of snippets used to train each approach, the definition of what constitutes a \"snippet\", the Wikipedia version the data is sourced from, and -importantly-the size of the entity vocabulary and quality of the candidate lists used (\"HF\" are lists by Hoffart et al. (2011), \"GH\" lists by Ganea and Hofmann (2017), and \"PPR\" lists by Pershina et al. (2015)).\nLack of direct comparability. With this paper, we argue that these differences in training setup impair our ability to directly compare approaches based solely on published numbers on evaluation datasets. For instance, Table 1 shows that LUKE (Yamada et al., 2022) slightly outperforms the comparatively simple approach by FEVRY (F\u00e9vry et al., 2020) on AIDA-B; but since FEVRY is trained to cover a much larger set of entities, we cannot know if the difference in evaluation score is due to algorithmic differences in both approaches, or simply a function of the signal used to train them.\nContributions. We argue that -much like in most other NLP tasks-we require a uniform experimental setup to evaluate large ED models. To this end, we present ZELDA, a comprehensive benchmark for supervised entity disambiguation. The benchmark consists of 95k full text paragraphs from Wikipedia, annotated with mention boundaries and disambiguation targets, and integrates 8 existing ED datasets from various domains as evaluation splits. ZELDA defines a fixed entity vocabulary of 822k entities, together with fixed candidate lists and entity descriptions. In this paper:\n1. We analyze training setups in recent state-ofthe-art ED approaches, and derive desiderata for a uniform training benchmark (Section 2).\n2. We present the ZELDA benchmark, the design goals that inform our sampling methodology to create it, and its properties (Section 3).\n3. We compute evaluation scores for baselines and three state-of-the-art approaches to present standardized scores and illustrate the usefulness of our benchmark (Section 4).\n4. We make our benchmark available to the research community as an open source project.\nWe hope that the public release of ZELDA will encourage future ED works to leverage our benchmark, and thus facilitate greater direct comparability of future ED approaches.", "publication_ref": ["b27", "b26", "b13", "b5", "b32", "b7", "b21", "b6", "b13", "b8", "b22", "b34", "b7"], "figure_ref": [], "table_ref": ["tab_3", "tab_3", "tab_3", "tab_3"]}, {"heading": "Analysis of Training Setups", "text": "ED approaches employ large language models to embed an entity mention, its textual context and additional features. To decode, approaches typically either use variants of softmax classification (Broscheit, 2019;F\u00e9vry et al., 2020;Yamada et al., 2017;Orr et al., 2021;Ayoola et al., 2022), generative decoding (De Cao et al., 2021) or retrieval-based models that compute the pairwise similarity of an embedded mention and a textual description for each target entity (Ravi et al., 2021).\nRather than focus on the algorithmic differences of these approaches, this section analyzes current state-of-the-art approaches from the point of view of their respective training setups.", "publication_ref": ["b5", "b7", "b33", "b21", "b1", "b6", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Training Data", "text": "Size of training dataset. Approaches are typically trained over short snippets of text with annotated entity mentions, derived from Wikipedia page links. The number and length of these snippets varies greatly across approaches. For instance, as Table 1 shows, Ayoola et al. (2022) train their model with 20 million snippets of 512 tokens length, while De Cao et al. (2021) train with 9 million snippets of 100 token length. Orr et al. (2021) train on single sentences only, and use a comparatively small set of 5.7 million snippets. The sampling methodology to derive these snippets from Wikipedia is seldom described in detail and bespoke to each paper.\nWhile we could find few ablation experiments in prior work, Broscheit (2019) presents an experimental evaluation in which he trains his proposed approach over two different datasets sampled from Wikipedia, one with 8.8 million and one with 2.4 million snippets. The difference in dataset size is due to a threshold parameter for frequent entities in his sampling method. Surprisingly, he finds that the model trained on the smaller dataset yields significantly better results on AIDA-B. He believes this may be because his computational resources limited training on the large dataset to only 4 epochs, whereas on the small dataset he could train for 14. Single-mention vs multi-mention data. The number of snippets is only partly illustrative of the training signal, as the number of mentions dramatically differs per setup. In \"single-mention\" data as used by Barba et al. (2022) andDe Cao et al. (2021), each snippet only contains a single annotated entity mention (indicated with a dagger asterisk in Table 1). In \"multi-mention\" data on the other hand, more than one mention might be annotated, thus potentially greatly increasing the training signal. Optional augmentation with weak labels. One particularity of Wikipedia text is that within an article, usually only the first mention of an entity is marked with a page link. In fact, Orr et al. (2021) estimate that 68% of mentions are unlabeled. For this reason, many works use \"weak labeling\" methods to annotate unlabeled mentions (Orr et al., 2021;Ayoola et al., 2022;Broscheit, 2019) in Wikipedia articles. These methods dramatically increase the number of labeled mentions per text snippet, but may introduce errors into the training data. This naturally impacts the performance of ED: for instance, Orr et al. (2021) find that their model performs better on rare and unseen entities but worse on frequent ones when using weak labels. Wikipedia version of training data. Table 1 also shows that the data is sourced from different Wikipedia versions. This poses problems as the entity set covered by Wikipedia grows significantly over time (Gillick et al., 2019). Further, the information contained in Wikipedia is constantly updated (such as which person currently holds which political office), potentially giving advantages to models trained on a Wikipedia version from a similar point in time as the evaluation data.", "publication_ref": ["b1", "b21", "b5", "b2", "b6", "b21", "b21", "b1", "b5", "b21", "b9"], "figure_ref": [], "table_ref": ["tab_3", "tab_3"]}, {"heading": "Entity Vocabulary", "text": "As Table 1 shows, published approaches also differ in their entity vocabulary, i.e. the number of unique entities they can resolve, ranging from 128k (Yamada et al., 2017) to about 6 million entities (F\u00e9vry et al., 2020;Ayoola et al., 2022). While a very large entity set is desirable for a general-purpose ED system, a smaller vocabulary tuned to an evaluation dataset will likely result in better evaluation numbers. This intuition is supported by experiments by Wu et al. (2020) who found that a model trained to handle an entity set specific to their evaluation dataset outperforms a general-purpose model trained to handle 5.9M Wikipedia entities.", "publication_ref": ["b33", "b7", "b1", "b32"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Candidate Lists", "text": "Most state-of-the-art ED approaches employ candidate lists that contain for each mention string a set of sensible entity candidates (Sevgili et al., 2022  The model then classifies over the small list rather than the whole entity set. The advantage of this approach is that it greatly narrows the search space and speeds up computation. A drawback however is that these candidate lists must be created separately and that incomplete lists lower the upper bound of what an ED approach can achieve: if the correct entity is not included in the candidate list of a mention, correct classification is not possible. Prior work showed that the choice of candidate lists significantly impacts overall results. For instance, the lists of Pershina et al. (2015) were found to be exceptionally well-tailored to the AIDA-B evaluation dataset, with high recall and low ambiguity for its entities (Yang et al., 2018). As Table 1 shows, both F\u00e9vry et al. (2020) and Yamada et al. (2022) find that their models improve significantly when using these lists instead of the more generic lists by Hoffart et al. (2011) and Ganea and Hofmann ( 2017) respectively. Unfortunately, some approaches such as F\u00e9vry et al. (2020) also employ custom lists that are not released.", "publication_ref": ["b26", "b22", "b35", "b7", "b34", "b13", "b7"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Domain-Specific Features", "text": "It is possible to tailor ED systems to achieve better results on individual domains. Page titles. F\u00e9vry et al. (2020) and Orr et al. (2021) disambiguate entities in news articles, and present a custom approach for constructing snippets: instead of only taking a token window around an entity mention, they also add the title and first two sentences of the article as additional context, reasoning that these texts contain salient information that pertains to the whole article. However, such custom contexts can only be defined for individual domains (e.g. tweets for instance do not have titles) and are therefore challenging to integrate for general-purpose ED systems. Domain-specific data. In addition to large Wikipedia-derived datasets, many works also incorporate domain-specific data into their training or fine-tuning. While many available evaluation datasets are limited to small test splits, some popular datasets also define training splits. A wellknown example is AIDA, which next to AIDA-B defines a train split consisting of 20k sentences and covering 30k entities. Table 1 shows (in column \"additional features\") that all models either present ablations in which AIDA-TRAIN is included, or include this data by default. The numbers clearly show that including domain-specific data improves overall results. However, prior works have shown that fine-tuning to a particular domain degrades performance on other datasets (Yamada et al., 2022;De Cao et al., 2021;Le and Titov, 2019).", "publication_ref": ["b7", "b21", "b34", "b6", "b16"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Additional Features", "text": "Some ED approaches leverage additional sources of information (Shen et al., 2021;Sevgili et al., 2022). In particular, entity descriptions are concise textual summaries of the \"meaning\" of each entity, and a core component of all ED approaches that follow a retrieval-based approach (Ravi et al., 2021). Entity type information equips each entity with semantic type as additional signal. Finally, some approaches employ knowledge base (KB) information to decode multiple mentions in a text paragraph such that overall entity relatedness is observed (Ayoola et al., 2022;Orr et al., 2021).", "publication_ref": ["b27", "b26", "b24", "b1", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "The ZELDA Benchmark", "text": "We create ZELDA to enable analysis and direct comparison of large ED models. Refer to Table 2 for an overview. We start by selecting and normalizing appropriate evaluation datasets (Section 3.1), upon which we define a methodology to sample training data that satisfies several objectives (Section 3.2). To ensure broad applicability, we also produce candidate lists and entity descriptions (Section 3.3).", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Selection of Evaluation Splits", "text": "Desiderata. Our analysis of Section 2 showed that there are many ways to tailor the training setup to a specific evaluation dataset: one might employ domain-optimized candidate lists, include domainspecific features, use an optimized entity vocabulary, or optimize the process of sampling Wikipedia for training data. With ZELDA, we seek to minimize opportunities for such domain-specific engineering to place greater focus on evaluating algorithmic rather than engineering components. We also seek an evaluation setup that not only produces a single score, but facilitates more granular analysis of the capabilities of large ED models.\nWe therefore sought evaluation datasets that both span a broad range of domains (web pages, newswire text, social media) as well as isolate specific challenges in ED. To facilitate distribution, we limited our search to freely available datasets. Selected datasets (Table 2). We chose the following 8 datasets for inclusion:\n\u2022 AIDA-B is the test split of AIDA, the most commonly used ED dataset. It contains 231 manually annotated Reuters news articles.\n\u2022 TWEEKI (Harandizadeh and Singh, 2020) is a collection of 500 randomly selected and handannotated tweets.\n\u2022 Two datasets from Botzer et al. (2021), referred to as REDDIT-COMMENTS and REDDIT-POSTS respectively, that consist of top-scoring posts and comments from the internet forum Reddit. We use the \"gold\" subset of this dataset, i.e. all annotations in which all three annotators agreed.\n\u2022 Two datasets from Guo and Barbosa (2018), referred to as WNED-WIKI and WNED-CWEB respectively, that cover the domains of Wikipedia articles and web pages. We include these datasets because they include annotation of the difficulty of each document on a scale from 0 to 1. This enables analyses of approaches as a function of estimated difficulty, as we show in Section 4.\n\u2022 Three datasets from Provatorova et al. (2021), created specifically to analyze three classes of mention ambiguities: (1) SLINKS-TOP contains only easy cases in which the correct disambiguation is the most frequent sense of a mention.\n(2) SLINKS-SHADOW is the opposite and contains only difficult cases in which the correct disambiguation of a mention is \"overshadowed\" by a more popular entity.\n(3) SLINKS-TAIL contains only \"long tail\" entities that are very rare in Wikipedia.\nNormalization. We unify these datasets in two ways: First, since these datasets were created at different times, we update entity annotations to the most recent version of Wikipedia (October, 2022). Second, as datasets are provided in various formats, we convert them into two commonly used standard formats, namely CoNLL and JsonL.", "publication_ref": ["b11", "b3", "b10", "b23"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Training Data", "text": "Desiderata. We define a sampling methodology to create training data to balance two objectives: our first goal is to evaluate entity disambiguation for \"broad entity coverage\" approaches that derive large-scale training data from Wikipedia. However, if the training data is too large, model training becomes to costly for thorough analyses of design choices and hyperparameters; with the exception of the work by F\u00e9vry et al. (2020), we find such analyses to be rare in current literature. For this reason, our second goal is to limit the overall size of the training dataset.\nSampling process (Algorithm 1). To balance these two goals, our sampling process starts from the vocabulary of all entities in the evaluation splits, which we refer to as the test entity set E t . Our sampling seeks to find at least a minimal number of training examples for these entities, set by the threshold parameter. Following prior analyses by Vasilyev et al. (2022), we set the threshold to 10, meaning that each entity in the test set should appear at least 10 times in the training data. However, this is only possible for entities that do appear this often in the source Wikipedia data; for long-tail entities that appear fewer times, we select as many examples as possible.\nOur sampling selects entire Wikipedia paragraphs for inclusion into the training dataset. The reason for choosing paragraphs as atomic document type is threefold: (1) Unlike fixed-length token windows that center on one particular entity, paragraphs typically consist of multiple full sentences that provide natural context for entity mentions. (2) By choosing random paragraphs instead of full articles, we limit overall dataset size and introduce more textual variety as opening paragraphs of Wikipedia articles were observed to often have similar wording (Le and Titov, 2019). (3) Paragraphs contain mentions to many other entities outside of E t . These entities are naturally skewed and added to the overall entity vocabulary of ZELDA. Data preprocessing. We leverage the Kensho Derived Wikimedia Dataset 1 , derived from the Wikipedia dump of December 2019. This dataset is preprocessed such that redirect-, disambiguationand list-pages are removed, the text is cleaned and articles are divided into sections. Here, each section corresponds to one paragraph of text. We discard common section types that typically contain little text (such as the \"Bibliography\" and \"External Links\" sections common to Wikipedia articles). To ensure that all annotations are consistent with our evaluation splits, we update entity annotations to the most recent version of Wikipedia and discard those for which no article exists anymore.\nResulting training data. This set is randomized and sampled using Algorithm 1, yielding a training data set of 95k paragraphs spanning on average 527 tokens. It contains a total of 2.6 million mentions covering a vocabulary of 825k distinct entities, which we refer to as ZELDA-TRAIN. See Table 2 for descriptive statistics.", "publication_ref": ["b7", "b30", "b16"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Additional Structured Information", "text": "We provide candidate lists that we derive with a general approach from the Kensho Wikimedia dataset, the Wikilinks web corpus (Singh et al., 2012) and the \"also known as\" information from Wikidata. For all mentions in these sources we list and count all entities that they refer to and filter entities from these lists that are not contained in the ZELDA entity vocabulary (details in Appendix A.3). Moreover we derive the most-frequent-sense baseline (MFS) by choosing, for every mention, the entity that this mention refers to the most often.\nWe also provide standardized entity descriptions: For each entity we extract the opening paragraph of its Wikipedia article as its description.", "publication_ref": ["b28"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We showcase the ZELDA benchmark by training a set of baselines and state-of-the-art approaches 1 https://datasets.kensho.com/datasets/wikimedia ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluated Approaches", "text": "We compare 8 different models, as listed in Table 3: Simple baselines. We include two baseline approaches. The first is MFS, a simple most-frequentsense baseline that assigns each mention to its most commonly observed entity. The second is CL-RECALL, which calculates the upper bound reachable with our provided candidate lists: for each mention, the gold entity is assigned if it is included in the candidate list. Simple softmax classifier (FEVRY). We include a reimplementation of the approach by F\u00e9vry et al. (2020) in two variants: FEVRY CL uses our candidate lists, while FEVRY ALL does not use any lists to restrict the search space. The approach leverages a simple softmax classification head trained on top of a transformer model that takes as input a text snippet. Despite its simplicity, it was found to be surprisingly competitive. We reimplemented the approach as F\u00e9vry et al. (2020) did not release their code. However, since our train set is much smaller we use bert-base-uncased instead of just a 4-layer transformer and adapt the hyperparameters to our setting (see Appendix A.1) LUKE. We train two variants of LUKE (Yamada et al., 2022), the current state-of-the-art approach for several benchmark datasets.  in a given text snippet by order of confidence. Each classified mention is used as a feature to better classify the remaining mentions in a snippet. For training, they distinguish between pre-training in which entity embeddings are learned, and an optional final epoch of fine-tuning in which they are frozen. We train one model only with pre-training (LUKE P RE ) and one with fine-tuning (LUKE F T ). We use their publicly available code 2 to train our two models. For direct comparison to the FEVRY model, we use bert-base-uncased instead of bert-large-uncased (utilized in Yamada et al. (2022)) and slightly adapt their hyperparameters to our setting, see Appendix A.1. GENRE. We also train two variants of GENRE (De Cao et al., 2021), a generative approach that formulates ED as a sequence-to-sequence problem. A given input text with flagged mention boundaries is input, from which an entity title is generated. To ensure that the generated sequence is a valid title, GENRE uses a prefix-tree generated from all entity titles in the data to constrain the generation process. GENRE does not use candidate lists during training but in inference the prefix tree can be derived from the candidate lists. We call this variant GENRE CL .\nWe use their publicly available code 3 to train our two models. Instead of the bart-large model we use the bart-base version to make the comparison more fair. We adapt the recommended hyperparameters to our setting (see Appendix A.1).", "publication_ref": ["b7", "b7", "b34", "b34", "b6"], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Results", "text": "Table 3 breaks down the accuracy of each model for each of the evaluation splits, and provides a single macro-averaged accuracy score for all data. We make a number of interesting observations: Different ranking of approaches. Most importantly, we arrive at a starkly different ranking of approaches compared to published numbers on AIDA-B as listed in Table 1 where GENRE is one of the lowest-scoring models. In contrast, in our evaluation the two GENRE models clearly outperform all other considered models in most evaluation splits. Impact of candidate lists.\nWe note that our general-purpose candidate lists derived from Wikipedia score unevenly across domains. As our CL-RECALL baseline shows, our lists have a high upper bound on evaluation splits covering Wikipedia and social media domains, but a relatively low upper bound on splits from the domains of web pages or news text. We also note that of the classification-based approaches, only FEVRY ALL does not use candidate lists, but scores best.\nMoreover, the overall best-scoring approach GENRE is trained without candidate lists. But during prediction, the better-scoring variant GENRE CL employs candidate lists, while GENRE ALL does not. This indicates using candidate lists only for prediction, but not training, may be a worthwhile approach to further explore. Hard-to-disambiguate entities. On the SLINKS-SHADOW dataset of \"overshadowed\" entities, FEVRY ALL outperforms GENRE. One possible interpretation is that a generative approach naturally favors decoding into the most prominent sense, as the generated entity title will be most similar to the mention text. On the other hand, classificationbased approaches are not influenced by string similarity of entity and mention text, potentially leading to better performance here.\nIn Table 4, we additionally list the scores on the brackets for WNED-WIKI provided by Guo and Barbosa (2018). The table shows that accuracy scores of all models steadily decrease from left (the  easiest bracket) to right (the hardest bracket). As we see this is not caused by a the CG-recall on WNED-WIKI which is independent from the brackets. This indicates that there remains much room for improving ED performance on ZELDA even when leveraging candidate lists during prediction.", "publication_ref": ["b10"], "figure_ref": [], "table_ref": ["tab_5", "tab_3", "tab_7"]}, {"heading": "Discussion", "text": "Our evaluation showed that the generative GENRE approach outperforms all classification-based approaches, and a simple direct classification approach without candidate lists as second-best performing approach overall. This indicates that equalizing the training signal, removing opportunities for domain-specific engineering, and evaluating across diverse evaluation splits may yield more insights into which algorithmic approach is best-suited to train large ED models. However, we must also caution against overinterpreting this ranking: due to the large training times for each of these models, we did not explore any hyperparameters. Instead, we used default parameters whenever possible, and in the case of LUKE and FEVRY changed the underlying transformer to the same model, for more direct comparability. Models were only trained for as long as our computational resources allowed. Upon publication of the benchmark we anticipate that authors will explore better hyperparameters for their respective approaches, which may change the ranking (see Limitations section).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Prior work has addressed aspects of evaluating ED. Standardized evaluation. GERBIL (R\u00f6der et al., 2018) standardizes ED evaluation over multiple datasets in a unifying framework, but does not define the training data and thus only focuses on comparing already-trained models. Similarly, a range of prior works have sought to refine and standardize ED evaluation (Waitelonis et al., 2019;Nait-Hamoud et al., 2021;Noullet et al., 2021;Odoni et al., 2019;van Erp and Groth, 2020;Bra\u015foveanu et al., 2018). In contrast, ZELDA defines the full experimental setup, including training data, the entity vocabulary and other training signals.\nManually labeled training data. A few existing ED datasets not only define a test set, but also a training split. An example discussed in this paper is the AIDA dataset. Other datasets include TAC-KBP2010 (Ji and Grishman, 2011), which is not available anymore, and a zero-shot dataset from Logeswaran et al. (2019). However, these datasets are too small and cover too few entities for evaluation of large ED approaches.\nDeriving training data from Wikipedia. All current state-of-the-art approaches derive their training data from Wikipedia, though the exact process is often not thoroughly described and/or provided to the public. One exception is the BLINK corpus created by Wu et al. (2020) that is used in other works. However, this corpus consists only of single-mention snippets and cannot be used for approaches that train global decoders, like LUKE.\nMost similar to our sampling method is from Orr et al. (2021). The authors sample a small Wikipedia subset by sampling for the mentions of the KORE50 benchmark (Hoffart et al., 2012). Unlike our approach, they sample all occurrences of each mention and sample only single sentences, yielding 520k sentences for only 144 mentions.", "publication_ref": ["b25", "b31", "b18", "b19", "b20", "b29", "b4", "b14", "b17", "b32", "b21", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We presented the ZELDA benchmark to unify experimental setups across large ED approaches, and conducted an evaluation of various approaches. We find that given the exact same training signal, approaches compare differently than published numbers suggest. We release the datasets, our sampling and preprocessing scripts and our FEVRY reimplementation to the research community as an open source project available at https://github. com/flairNLP/zelda. Additionally, we integrate our benchmark into the open source NLP framework FLAIR (Akbik et al., 2019).\nWe hope that this will encourage present and fu-ture ED works to compare algorithmic differences on a more equal setting and thus help generate insights to further advance the field of ED.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "As discussed in Section 4.3, an important limitation of our experimental evaluation is our lack of hyperparameter exploration of published approaches. Given the effort required to train large ED models and the many involved hyperparameters, we believe that only the original authors of their respective approaches can perform a meaningful search of hyperparameters for our benchmark, limiting us to best-effort parameters from prior literature. It is therefore possible if not likely that the respective authors of the approaches we compare might arrive at better numbers than the ones presented here.\nRegarding the ZELDA benchmark itself, we note that it is designed to evaluate supervised ED approaches. As we sampled the dataset to contain at least 10 annotations for each entity in the ZELDA test splits whenever possible, it is unclear whether ZELDA is useful for evaluating the currently growing family of zero-shot ED models (Logeswaran et al., 2019;Wu et al., 2020). Finally, our training corpus is relatively small compared to some other Wikipedia corpora used in prior approaches. While we made this design choice purposefully to enable faster training times and hopefully more exploration of hyperparameters by future works, we cannot be certain whether rankings obtained on ZELDA transfer to approaches trained on orders of magnitude of more data.", "publication_ref": ["b17", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "A Appendix", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Training Parameters and Times", "text": "Our training parameters are informed by recommended parameters of prior works, adapted to the smaller training dataset size of ZELDA-TRAIN. In some cases, we adapted parameters across approaches for greater comparability, for instance by using the same transformer model for both LUKE and FEVRY. Across all approaches, we use the following parameters: We train all models for 6 epochs with a mini-batch size of 64 and a learning rate of 5e-5. The remaining hyperparameters are specific to each model: FEVRY. The remaining parameters in FEVRY follow the recommendations from the paper, i.e. we use the Adam optimizer (Kingma and Ba, 2015) with a linear warmup for the first 10% of training and gradient clipping. However, the smaller dataset allowed us to look at more context. We split paragraphs of ZELDA into snippets of 400 tokens (Fevry: 256) and use an entity embedding size of 200 (Fevry: 256). Training FEVRY ALL took around 5h per epoch and for FEVRY CL 2.5h per epoch on a single Nvidia 3090ti GPU, respectively. LUKE. We run experiments with both one-stage and two-stage training in LUKE. In one-stage training, we use for all epochs the same learning rate and do not fix the transformer weights. The entity masking rate is set to 30%. In two-stage training (LUKE CL ), we do fine-tuning in the last training epoch where we fix the entity embeddings and set the entity masking rate to 90%. To ensure comparability to FEVRY we set the entity embedding size to 200. For the remaining parameters we stick to the ones of the original LUKE which can be found in detail in table 4 and 5 of Yamada et al. (2022). Paragraphs of ZELDA are divided into snippets with \u2264 512 tokens. Training LUKE took roughly 2h per epoch on a single Nvidia 3090ti GPU. GENRE. Apart from the parameters that we already discussed, we take all default parameters from the original paper. The parameters can best be found in the released code 4 . Since GENRE takes much longer to train than the other models and processes mentions individually we gave the model less context: We split context 500 chars to the left and 500 chars to the right of each mention (a context of roughly 190 tokens). At inference we use a beam size of 10 and a maximum number of 15 decoding steps as in the original paper. Training GENRE took around 16 hours on two Nvidia 3090ti GPUs per epoch.", "publication_ref": ["b15", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Model Parameters", "text": "Our models have the following number of parameters: Both LUKE and FEVRY use a bert-base-uncased transformer model (110M parameters), a projection layer (768 \u00d7 200 \u2248 153k parameters) and the entity embedding layer (200 \u00d7 825k \u2248 165M parameters), and thus have about 274M parameters in total. GENRE adds a decoder with 768 \u00d7 51197 \u2248 39M parameters to its underlying transformer and thus has a total of 178M parameters.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.3 Candidate Lists", "text": "We derive the candidate lists with a straightforward approach from three sources: Wikipedia Kensho, WikiLinks and Wikidata. The first two are text corpora with entity annotations derived from page links. As each page link has a mention string (the so-called \"anchor text\") and a target Wikipedia page, we can simply go through both datasets and collect all mentions and their targets. To ensure that the entity titles are up-to-date, we check with calls to the Wikipedia API if the titles lead to an existing Wikipedia page and discard them if not. This yields a set of [mention, entity] tuples. We aggregate and count these tuples.\nUsing the Wikidata API, we retrieve for each entity in our vocabulary the corresponding Wikidata page. From this page, we extract aliases from the \"also known as\" field. We interpret all aliases as additional mentions to an entity, leading to another set of [mention, entity] tuples that we aggregate with the first list. To cover a broader range of mentions we add the lower cased version and version without blanks and special characters of each mention to the tuple set.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We thank the reviewers for their helpful comments. Marcel Milich is supported the Investitionsbank Berlin through research project \"AI Marketeer\", confinanced by the European Regional Development Fund (ERDF). Alan Akbik is supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy \"Science of Intelligence\" (EXC 2002/1, project number 390523135) and the DFG Emmy Noether grant \"Eidetic Representations of Natural Language\" (project number 448414230).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "FLAIR: An easy-to-use framework for state-of-theart NLP", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Alan Akbik; Tanja Bergmann; Duncan Blythe; Kashif Rasul; Stefan Schweter; Roland Vollgraf"}, {"ref_id": "b1", "title": "Improving entity disambiguation by reasoning over a knowledge base", "journal": "", "year": "2022", "authors": "Tom Ayoola; Joseph Fisher; Andrea Pierleoni"}, {"ref_id": "b2", "title": "Association for Computational Linguistics", "journal": "Long Papers", "year": "2022", "authors": "Edoardo Barba; Luigi Procopio; Roberto Navigli"}, {"ref_id": "b3", "title": "Reddit entity linking dataset. Information Processing & Management", "journal": "", "year": "2021", "authors": "Nicholas Botzer; Yifan Ding; Tim Weninger"}, {"ref_id": "b4", "title": "Framing named entity linking error types", "journal": "", "year": "2018", "authors": "M P Adrian; Giuseppe Bra\u015foveanu; Philipp Rizzo; Albert Kuntschik; Lyndon J B Weichselbraun;  Nixon"}, {"ref_id": "b5", "title": "Investigating entity knowledge in BERT with simple neural end-to-end entity linking", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Samuel Broscheit"}, {"ref_id": "b6", "title": "Autoregressive entity retrieval", "journal": "", "year": "2021-05-03", "authors": "Nicola De Cao; Gautier Izacard; Sebastian Riedel; Fabio Petroni"}, {"ref_id": "b7", "title": "Empirical evaluation of pretraining strategies for supervised entity linking", "journal": "", "year": "2005", "authors": "Thibault F\u00e9vry; Nicholas Fitzgerald; Livio Baldini Soares; Tom Kwiatkowski"}, {"ref_id": "b8", "title": "Deep joint entity disambiguation with local neural attention", "journal": "", "year": "2017", "authors": "Eugen Octavian; Thomas Ganea;  Hofmann"}, {"ref_id": "b9", "title": "Learning dense representations for entity retrieval", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Daniel Gillick; Sayali Kulkarni; Larry Lansing; Alessandro Presta; Jason Baldridge; Eugene Ie; Diego Garcia-Olano"}, {"ref_id": "b10", "title": "Robust named entity disambiguation with random walks. Semantic Web", "journal": "", "year": "2018", "authors": "Zhaochen Guo; Denilson Barbosa"}, {"ref_id": "b11", "title": "Tweeki: Linking named entities on Twitter to a knowledge graph", "journal": "", "year": "2020", "authors": "Bahareh Harandizadeh; Sameer Singh"}, {"ref_id": "b12", "title": "Kore: Keyphrase overlap relatedness for entity disambiguation", "journal": "Association for Computing Machinery", "year": "2012", "authors": "Johannes Hoffart; Stephan Seufert; Dat Ba Nguyen; Martin Theobald; Gerhard Weikum"}, {"ref_id": "b13", "title": "Robust disambiguation of named entities in text", "journal": "", "year": "2011", "authors": "Johannes Hoffart; Mohamed Amir Yosef; Ilaria Bordino; Hagen F\u00fcrstenau; Manfred Pinkal; Marc Spaniol; Bilyana Taneva; Stefan Thater; Gerhard Weikum"}, {"ref_id": "b14", "title": "Knowledge base population: Successful approaches and challenges", "journal": "", "year": "2011", "authors": "Heng Ji; Ralph Grishman"}, {"ref_id": "b15", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015-05-07", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b16", "title": "Boosting entity linking performance by leveraging unlabeled documents", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Phong Le; Ivan Titov"}, {"ref_id": "b17", "title": "Zero-shot entity linking by reading entity descriptions", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Lajanugen Logeswaran; Ming-Wei Chang; Kenton Lee; Kristina Toutanova; Jacob Devlin; Honglak Lee"}, {"ref_id": "b18", "title": "A step further towards a consensus on linking tweets to wikipedia", "journal": "Evolutionary Intelligence", "year": "2021", "authors": "Mohamed Cherif Nait-Hamoud; Fedoua Lahfa; Abdellatif Ennaji"}, {"ref_id": "b19", "title": "Clit: Combining linking techniques for everyone", "journal": "", "year": "2021", "authors": "Kristian Noullet; Samuel Printz; Michael F\u00e4rber"}, {"ref_id": "b20", "title": "Introducing orbis: An extendable evaluation pipeline for named entity linking performance drill-down analyses", "journal": "", "year": "2019", "authors": "Fabian Odoni; Adrian M P Bra\u015foveanu; Philipp Kuntschik; Albert Weichselbraun"}, {"ref_id": "b21", "title": "Bootleg: Chasing the tail with self-supervised named entity disambiguation", "journal": "ArXiv", "year": "2021", "authors": "Laurel J Orr; Megan Leszczynski; Simran Arora; Sen Wu; Neel Guha; Xiao Ling; Christopher R\u00e9"}, {"ref_id": "b22", "title": "Personalized page rank for named entity disambiguation", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Maria Pershina; Yifan He; Ralph Grishman"}, {"ref_id": "b23", "title": "Robustness evaluation of entity disambiguation using prior probes: the case of entity overshadowing", "journal": "", "year": "2021", "authors": "Vera Provatorova; Samarth Bhargav; Svitlana Vakulenko; Evangelos Kanoulas"}, {"ref_id": "b24", "title": "CHOLAN: A modular approach for neural entity linking on Wikipedia and Wikidata", "journal": "", "year": "2021", "authors": "Manoj Prabhakar Kannan Ravi; Kuldeep Singh; Isaiah Onando Mulang; ' ; Saeedeh Shekarpour; Johannes Hoffart; Jens Lehmann"}, {"ref_id": "b25", "title": "Gerbil -benchmarking named entity recognition and linking consistently", "journal": "Semantic Web", "year": "2018", "authors": "Michael R\u00f6der; Ricardo Usbeck; Axel-Cyrille Ngonga Ngomo"}, {"ref_id": "b26", "title": "Neural entity linking: A survey of models based on deep learning", "journal": "Semantic Web", "year": "2022", "authors": "\u00d6zge Sevgili; Artem Shelmanov; Mikhail Arkhipov; Alexander Panchenko; Chris Biemann"}, {"ref_id": "b27", "title": "Entity linking meets deep learning: Techniques and solutions", "journal": "", "year": "2021", "authors": "Wei Shen; Yuhan Li; Yinan Liu; Jiawei Han; Jianyong Wang; Xiaojie Yuan"}, {"ref_id": "b28", "title": "Wikilinks: A largescale cross-document coreference corpus labeled via links to Wikipedia", "journal": "", "year": "2012", "authors": "Sameer Singh; Amarnag Subramanya; Fernando Pereira; Andrew Mccallum"}, {"ref_id": "b29", "title": "Towards entity spaces", "journal": "", "year": "2020", "authors": "Paul Marieke Van Erp;  Groth"}, {"ref_id": "b30", "title": "Named entity linking on namesakes", "journal": "", "year": "2022", "authors": "Oleg Vasilyev; Alex Dauenhauer; Vedant Dharnidharka; John Bohannon"}, {"ref_id": "b31", "title": "Remixing entity linking evaluation datasets for focused benchmarking", "journal": "", "year": "2019", "authors": "J\u00f6rg Waitelonis; Henrik J\u00fcrges; Harald Sack"}, {"ref_id": "b32", "title": "Scalable zeroshot entity linking with dense entity retrieval", "journal": "", "year": "2020", "authors": "Ledell Wu; Fabio Petroni; Martin Josifoski; Sebastian Riedel; Luke Zettlemoyer"}, {"ref_id": "b33", "title": "Learning distributed representations of texts and entities from knowledge base", "journal": "Transactions of the Association for Computational Linguistics", "year": "2017", "authors": "Ikuya Yamada; Hiroyuki Shindo; Hideaki Takeda; Yoshiyasu Takefuji"}, {"ref_id": "b34", "title": "Global entity disambiguation with BERT", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Ikuya Yamada; Koki Washio; Hiroyuki Shindo; Yuji Matsumoto"}, {"ref_id": "b35", "title": "Collective entity disambiguation with structured gradient tree boosting", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Yi Yang; Ozan Irsoy; Kazi Shefaet Rahman"}], "figures": [{"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": ").", "figure_data": "ApproachTraining dataWeak labels?Data sourceAdditional featuresEntity vocabCandidate listsAIDA-BYamada et al. (2022) -LUKEGH -LUKEP P R -LUKEGH+AIDA -LUKEP P R+AIDA\u223c10M snippets noWikipedia (Dec, 2018)+AIDA-TRAIN +AIDA-TRAIN128k 128k 128k 128kGH PPR GH PPR92.4 94.6 95 97.1Barba et al. (2022) -EXTEND9M snippets  \u2020noWikipedia (May, 2019)+AIDA-TRAIN1.5MGH92.6Ayoola et al. (2022) -AYOOLA\u223c20M snippets yesWikipedia (July, 2021)+KB +descriptions +types6.2MGH90.4De Cao et al. (2021) -GENRE -GENRE+AIDA -GENRE+AIDA\u2212NOC9M snippets  \u2020noWikipedia (May, 2019)+AIDA-TRAIN +AIDA-TRAIN1.5M 1.5M 1.5MGH GH none89.3 93.3 91.2Orr et al. (2021) -BOOTLEG5.7M sentences yesWikipedia (Nov, 2019)+KB +types +AIDA-TRAIN3.3MPPR+custom 96.7F\u00e9vry et al. (2020) -FEVRYHF -FEVRYP P R17.5M snippets noWikipedia (Apr, 2019)+AIDA-TRAIN5.7M 5.7MHF+custom 92.5 PPR+custom 96.7Broscheit (2019) -BROSCHEIT 700k -BROSCHEIT 500k8.8M snippets 2.4M snippetsyesWikipedia (June, 2017)+AIDA-TRAIN700k 500knone none78.8 87.9"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": ").", "figure_data": "DomainDoc. Type # Docs\u2205 length # Entities # MentionsEvaluation Splits AIDA-B TWEEKI REDDIT-POSTS REDDIT-COMMENTS forum comments news tweets forum posts WNED-WIKI Wikipedia WNED-CWEB web SLINKS-TOP web SLINKS-SHADOW web SLINKS-TAIL webarticles short texts short texts short texts articles pages short texts short texts short texts231 500 377 360 318 320 1,433 tokens 201 tokens 16 tokens 20 tokens 41 tokens 315 tokens 904 35 tokens 904 35 tokens 902 35 tokens1,538 639 524 483 5,293 4,467 899 902 9024,485 860 705 638 6,747 11,116 904 904 902Training Split ZELDA-TRAINWikipedia paragraphs95k527 tokens822k2.6M"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Descriptive statistics of the training and evaluation splits of ZELDA. Note that the statistics reported for evaluation splits may differ slightly from previous literature as a consequence of our normalization procedure.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Paragraph sampling input : Set of sections S, test entity set E t output : Filtered list of sections\u015c threshold \u2190 10; counter e \u2190 0 for e in E t ; while E t \u0338 = \u2205 do s = random.sample(S); if E t \u2229 s.links \u0338 = \u2205 then", "figure_data": "S.add(s);for e in s.links docounter e \u2190 counter e + 1; if counter e \u2265 threshold then E t .remove(e) endendendendreturn\u015con ZELDA-TRAIN, and comparatively evaluatingthem on our evaluation splits."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Results of our experiments. Bold scores indicate the best scores of all the trained models. Underlined scores represent the best scores among the classification-based models.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Accuracy measured for best approaches on different difficulty brackets of WNED-WIKI.", "figure_data": "The lowest scores"}], "formulas": [], "doi": "10.18653/v1/N19-4010"}